[
    {
        "original": "def _gpdfit(x):\n    \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n\n    Empirical Bayes estimate for the parameters of the generalized Pareto\n    distribution given the data.\n\n    Parameters\n    ----------\n    x : array\n        sorted 1D data array\n\n    Returns\n    -------\n    k : float\n        estimated shape parameter\n    sigma : float\n        estimated scale parameter\n    \"\"\"\n    prior_bs = 3\n    prior_k = 10\n    len_x = len(x)\n    m_est = 30 + int(len_x ** 0.5)\n\n    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n    b_ary /= prior_bs * x[int(len_x / 4 + 0.5) - 1]\n    b_ary += 1 / x[-1]\n\n    k_ary = np.log1p(-b_ary[:, None] * x).mean(axis=1)  # pylint: disable=no-member\n    len_scale = len_x * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n\n    # remove negligible weights\n    real_idxs = weights >= 10 * np.finfo(float).eps\n    if not np.all(real_idxs):\n        weights = weights[real_idxs]\n        b_ary = b_ary[real_idxs]\n    # normalise weights\n    weights /= weights.sum()\n\n    # posterior mean for b\n    b_post = np.sum(b_ary * weights)\n    # estimate for k\n    k_post = np.log1p(-b_post * x).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n    # add prior for k_post\n    k_post = (len_x * k_post + prior_k * 0.5) / (len_x + prior_k)\n    sigma = -k_post / b_post\n\n    return k_post, sigma",
        "rewrite": "Here is a revised version of your function:\n\n```Python\r\nimport numpy as np\r\n\r\ndef _gpdfit(x):\r\n    \r\n     def _safe_log(x):\r\n         return(np.where(np.isfinite(x), x, -100))\r\n\r\n     def _log_like(k, bs):\r\n         return((x.size)*(_safe_log(bs/x)-k-(_safe_log(2)-_safe_log(bs)))-(x.size/2)*_safe_log(k)).sum()\r\n\r\n     def _posterior(k):\r\n         return-_log_like(k,prior_bs)\r\n\r\n     from scipy.optimize import minimize\r\n\r\n     res=minimize(_posterior,prior_k)\r\n"
    },
    {
        "original": "def get_strain_state_dict(strains, stresses, eq_stress=None,\n                          tol=1e-10, add_eq=True, sort=True):\n    \"\"\"\n    Creates a dictionary of voigt-notation stress-strain sets\n    keyed by \"strain state\", i. e. a tuple corresponding to\n    the non-zero entries in ratios to the lowest nonzero value,\n    e.g. [0, 0.1, 0, 0.2, 0, 0] -> (0,1,0,2,0,0)\n    This allows strains to be collected in stencils as to\n    evaluate parameterized finite difference derivatives\n\n    Args:\n        strains (Nx3x3 array-like): strain matrices\n        stresses (Nx3x3 array-like): stress matrices\n        eq_stress (Nx3x3 array-like): equilibrium stress\n        tol (float): tolerance for sorting strain states\n        add_eq (bool): flag for whether to add eq_strain\n            to stress-strain sets for each strain state\n        sort (bool): flag for whether to sort strain states\n\n    Returns:\n        OrderedDict with strain state keys and dictionaries\n        with stress-strain data corresponding to strain state\n    \"\"\"\n    # Recast stress/strains\n    vstrains = np.array([Strain(s).zeroed(tol).voigt for s in strains])\n    vstresses = np.array([Stress(s).zeroed(tol).voigt for s in stresses])\n    # Collect independent strain states:\n    independent = set([tuple(np.nonzero(vstrain)[0].tolist())\n                       for vstrain in vstrains])\n    strain_state_dict = OrderedDict()\n    if add_eq:\n        if eq_stress is not None:\n            veq_stress = Stress(eq_stress).voigt\n        else:\n            veq_stress = find_eq_stress(strains, stresses).voigt\n\n    for n, ind in enumerate(independent):\n        # match strains with templates\n        template = np.zeros(6, dtype=bool)\n        np.put(template, ind, True)\n        template = np.tile(template, [vstresses.shape[0], 1])\n        mode = (template == (np.abs(vstrains) > 1e-10)).all(axis=1)\n        mstresses = vstresses[mode]\n        mstrains = vstrains[mode]\n        # Get \"strain state\", i.e. ratio of each value to minimum strain\n        min_nonzero_ind = np.argmin(np.abs(np.take(mstrains[-1], ind)))\n        min_nonzero_val = np.take(mstrains[-1], ind)[min_nonzero_ind]\n        strain_state = mstrains[-1] / min_nonzero_val\n        strain_state = tuple(strain_state)\n\n        if add_eq:\n            # add zero strain state\n            mstrains = np.vstack([mstrains, np.zeros(6)])\n            mstresses = np.vstack([mstresses, veq_stress])\n        # sort strains/stresses by strain values\n        if sort:\n            mstresses = mstresses[mstrains[:, ind[0]].argsort()]\n            mstrains = mstrains[mstrains[:, ind[0]].argsort()]\n        strain_state_dict[strain_state] = {\"strains\": mstrains,\n                                           \"stresses\": mstresses}\n    return strain_state_dict",
        "rewrite": "Here is the revised code:\n\n```python\r\nimport numpy as np\r\nfrom collections import OrderedDict\r\n\r\ndef get_strain_state_dict(strAINS,\r\n                          STRESSES,\r\n                          EQ_STRESS=None,\r\n                          TOL=1e-10,\r\n                          ADD_EQ=True,\r\n                          SORT=True):\r\n    \r\n   VSTRAINS=np.array([Strain(s).to_voigt() for s in STRAINS])\r\n   VSTRESSES=np.array([Stress(s).to_voigt() for s in STRESSES])\r\n\r\n   INDEPENDENT=set(tuple(np.nonzero(VSTRAIN)[None][None].flatten()) \\\r\n                    .flatten()."
    },
    {
        "original": "def dataclass(_cls=None, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False):\n    \"\"\"Returns the same class as was passed in, with dunder methods\n    added based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If\n    repr is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method function is added. If frozen is true, fields may\n    not be assigned to after instance creation.\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n\n    # See if we're being called as @dataclass or @dataclass().\n    if _cls is None:\n        # We're called with parens.\n        return wrap\n\n    # We're called as @dataclass without parens.\n    return wrap(_cls)",
        "rewrite": "Here is the revised code:\n\n```\ndef dataclass(_cls=None, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False):\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n    return wrap(_cls) if _cls is not None else wrap\n```"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "rewrite": "Here is the revised code:\n\n```\ndef execute(self):\n    command = self.command\n    self.reset()\n    return self.client.execute_command(*command)\n```"
    },
    {
        "original": "async def handler(event):\n    \"\"\"#full: Advises to read \"Accessing the full API\" in the docs.\"\"\"\n    await asyncio.wait([\n        event.delete(),\n        event.respond(READ_FULL, reply_to=event.reply_to_msg_id)\n    ])",
        "rewrite": "```\nasync def handler(event):\n    await asyncio.gather(event.delete(), event.respond(READ_FULL, reply_to=event.reply_to_msg_id))\n```"
    },
    {
        "original": "def RunWMIQuery(query, baseobj=r\"winmgmts:\\root\\cimv2\"):\n  \"\"\"Run a WMI query and return a result.\n\n  Args:\n    query: the WMI query to run.\n    baseobj: the base object for the WMI query.\n\n  Yields:\n    rdf_protodict.Dicts containing key value pairs from the resulting COM\n    objects.\n  \"\"\"\n  pythoncom.CoInitialize()  # Needs to be called if using com from a thread.\n  wmi_obj = win32com.client.GetObject(baseobj)\n  # This allows our WMI to do some extra things, in particular\n  # it gives it access to find the executable path for all processes.\n  wmi_obj.Security_.Privileges.AddAsString(\"SeDebugPrivilege\")\n\n  # Run query\n  try:\n    query_results = wmi_obj.ExecQuery(query)\n  except pythoncom.com_error as e:\n    raise RuntimeError(\"Failed to run WMI query \\'%s\\' err was %s\" % (query, e))\n\n  # Extract results from the returned COMObject and return dicts.\n  try:\n    for result in query_results:\n      response = rdf_protodict.Dict()\n      properties = (\n          list(result.Properties_) +\n          list(getattr(result, \"SystemProperties_\", [])))\n\n      for prop in properties:\n        if prop.Name not in IGNORE_PROPS:\n          # Protodict can handle most of the types we care about, but we may\n          # get some objects that we don't know how to serialize, so we tell the\n          # dict to set the value to an error message and keep going\n          response.SetItem(prop.Name, prop.Value, raise_on_error=False)\n      yield response\n\n  except pythoncom.com_error as e:\n    raise RuntimeError(\"WMI query data error on query \\'%s\\' err was %s\" %\n                       (e, query))",
        "rewrite": "Here is my revised version of your function:\n\n```Python\nimport win32com.client\nimport rdf_protodict\n\ndef run_wmi_query(query):\n    \"\"\"Run a WMI query and return results.\"\"\"\n    \n    import pywintypes\n    \n    base_object = 'winmgmts:\\\\root\\\\cimv2'\n    \n    try:\n        pywintypes.with_gil()\n        wmi_obj = win32com.client.GetObject(base_object)\n        wmi_obj.Security_.Privileges.AddAsString('SeDebugPrivilege')\n        \n        results = wmi_obj.ExecQuery(query)\n        \n        for result in results:\n            response"
    },
    {
        "original": "def call_c(self, c_args):\n        \"\"\"\n        Call this Callable with a string of C-style arguments.\n\n        :param str c_args:  C-style arguments.\n        :return:            The return value from the call.\n        :rtype:             claripy.Ast\n        \"\"\"\n\n        c_args = c_args.strip()\n        if c_args[0] != \"(\":\n            c_args = \"(\" + c_args\n        if c_args[-1] != \")\":\n            c_args += \")\"\n\n        # Parse arguments\n        content = \"int main() { func%s; }\" % c_args\n        ast = pycparser.CParser().parse(content)\n\n        if not ast.ext or not isinstance(ast.ext[0], pycparser.c_ast.FuncDef):\n            raise AngrCallableError(\"Error in parsing the given C-style argument string.\")\n\n        if not ast.ext[0].body.block_items or not isinstance(ast.ext[0].body.block_items[0], pycparser.c_ast.FuncCall):\n            raise AngrCallableError(\"Error in parsing the given C-style argument string: \"\n                                    \"Cannot find the expected function call.\")\n\n        arg_exprs = ast.ext[0].body.block_items[0].args.exprs\n\n        args = [ ]\n        for expr in arg_exprs:\n            if isinstance(expr, pycparser.c_ast.Constant):\n                # string\n                if expr.type == \"string\":\n                    args.append(expr.value[1:-1])\n                elif expr.type == \"int\":\n                    args.append(int(expr.value))\n                else:\n                    raise AngrCallableError(\"Unsupported expression type %s.\" % expr.type)\n            else:\n                raise AngrCallableError(\"Unsupported expression type %s.\" % type(expr))\n\n        return self.__call__(*args)",
        "rewrite": "Here is a revised version of your function:\n\n```Python\ndef call_c(self, c_ARGS):\n    import re\n    from pyparsing import Word, alphas, alphanums, oneOf\n\n    # Define a grammar for parsing C-style arguments.\n    identifier = Word(alphas + alphanums + \"_\", alphanums + \"_\")\n    number      = Word(nums)\n    charlit     = quotedString | escapedChar | char_\n    quotedString= quoted(Word(printable) |\n                             (notOneOf('\"') * anyChar) )\n    escapedChar  = chardash | escapeseq \n   "
    },
    {
        "original": "def stop(self, **kwargs):\n        \"\"\"Stop the environment.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabStopError: If the operation failed\n        \"\"\"\n        path = '%s/%s/stop' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path, **kwargs)",
        "rewrite": "Here is the revised code:\n\n```\ndef stop(self, **kwargs):\n    path = f\"{self.manager.path}/{self.get_id()}/stop\"\n    self.manager.gitlab.http_post(path, **kwargs)\n```"
    },
    {
        "original": "def ssn(self, min_age=18, max_age=90):\n        \"\"\"\n        Returns a 10 digit Swedish SSN, \"Personnummer\".\n\n        It consists of 10 digits in the form YYMMDD-SSGQ, where\n        YYMMDD is the date of birth, SSS is a serial number\n        and Q is a control character (Luhn checksum).\n\n        http://en.wikipedia.org/wiki/Personal_identity_number_(Sweden)\n        \"\"\"\n        def _luhn_checksum(number):\n            def digits_of(n):\n                return [int(d) for d in str(n)]\n            digits = digits_of(number)\n            odd_digits = digits[-1::-2]\n            even_digits = digits[-2::-2]\n            checksum = 0\n            checksum += sum(odd_digits)\n            for d in even_digits:\n                checksum += sum(digits_of(d * 2))\n            return checksum % 10\n\n        def _calculate_luhn(partial_number):\n            check_digit = _luhn_checksum(int(partial_number) * 10)\n            return check_digit if check_digit == 0 else 10 - check_digit\n\n        age = datetime.timedelta(\n            days=self.generator.random.randrange(min_age * 365, max_age * 365))\n        birthday = datetime.datetime.now() - age\n        pnr_date = birthday.strftime('%y%m%d')\n        suffix = str(self.generator.random.randrange(0, 999)).zfill(3)\n        luhn_checksum = str(_calculate_luhn(pnr_date + suffix))\n        pnr = '{0}-{1}{2}'.format(pnr_date, suffix, luhn_checksum)\n\n        return pnr",
        "rewrite": "Here is the revised code:\n\n```\nimport datetime\nimport random\n\ndef ssn(self, min_age=18, max_age=90):\n    digits_of = lambda n: [int(d) for d in str(n)]\n    luhn_checksum = lambda number: sum(digits_of(number)) + sum(sum(digits_of(d * 2)) for d in digits_of(number)[1::2]) % 10\n    calculate_luhn = lambda partial_number: str(10 - luhn_checksum(int(partial_number) * 10)) if luhn_checksum(int(partial_number) * 10) != 0 else '0"
    },
    {
        "original": "def create_logger():\n    \"\"\"Initial the global logger variable\"\"\"\n    global logger\n\n    formatter = logging.Formatter('%(asctime)s|%(levelname)s|%(message)s')\n    handler = TimedRotatingFileHandler(log_file, when=\"midnight\", interval=1)\n    handler.setFormatter(formatter)\n    handler.setLevel(log_level)\n    handler.suffix = \"%Y-%m-%d\"\n    logger = logging.getLogger(\"sacplus\")\n    logger.setLevel(log_level)\n    logger.addHandler(handler)",
        "rewrite": "```\nimport logging\nimport os\nfrom logging.handlers import TimedRotatingFileHandler\n\ndef create_logger(log_file, log_level):\n    formatter = logging.Formatter('%(asctime)s|%(levelname)s|%(message)s')\n    \n    if not os.path.exists(os.path.dirname(log_file)):\n        os.makedirs(os.path.dirname(log_file))\n        \n    handler = TimedRotatingFileHandler(filename=log_file, when=\"midnight\", interval=1, backupCount=30)\n    \n    handler.setFormatter(formatter)\n    \n#     if log_level:\n#         print(f\"Setting log level to {log_level}\")\n#         for h in"
    },
    {
        "original": "def select_with_condition(self, condition, key=None):\n        \"\"\"\n        Remove all the configurations that do not satisfy the given condition.\n\n            Args:\n                condition: dict or :class:`Condition` object with operators expressed with a Mongodb-like syntax\n                key: Selects the sub-dictionary on which condition is applied, e.g. key=\"vars\"\n                    if we have to filter the configurations depending on the values in vars\n        \"\"\"\n        condition = Condition.as_condition(condition)\n        new_confs = []\n\n        for conf in self:\n            # Select the object on which condition is applied\n            obj = conf if key is None else AttrDict(conf[key])\n            add_it = condition(obj=obj)\n            #if key is \"vars\": print(\"conf\", conf, \"added:\", add_it)\n            if add_it: new_confs.append(conf)\n\n        self._confs = new_confs",
        "rewrite": "Here is the revised code:\n\n```\ndef select_with_condition(self, condition, key=None):\n    condition = Condition.as_condition(condition)\n    new_confs = [conf for conf in self if condition(AttrDict(conf if key is None else conf[key]))]\n\n    self._confs = new_confs\n```"
    },
    {
        "original": "def _GetClientIdFromQueue(q):\n  \"\"\"Returns q's client id, if q is a client task queue, otherwise None.\n\n  Args:\n    q: rdfvalue.RDFURN\n\n  Returns:\n    string or None\n  \"\"\"\n  split = q.Split()\n  if not split or len(split) < 2:\n    return None\n\n  # Normalize to lowercase.\n  split = [s.lower() for s in split]\n\n  str_client_id, tasks_marker = split\n\n  if not str_client_id.startswith(\"c.\") or tasks_marker != \"tasks\":\n    return None\n\n  # The \"C.\" prefix should be uppercase.\n  str_client_id = \"C\" + str_client_id[1:]\n\n  return str_client_id",
        "rewrite": "Here is the revised code:\n\n```\ndef _GetClientIdFromQueue(q):\n  split = q.Split()\n  if not split or len(split) < 2:\n    return None\n\n  split = [s.lower() for s in split]\n  str_client_id, tasks_marker = split\n\n  if not str_client_id.startswith(\"c.\") or tasks_marker != \"tasks\":\n    return None\n\n  str_client_id = \"C\" + str_client_id[1:]\n  return str_client_id\n```"
    },
    {
        "original": "def init_database(connection=None, dbname=None):\n    \"\"\"Initialize the configured backend for use with BigchainDB.\n\n    Creates a database with :attr:`dbname` with any required tables\n    and supporting indexes.\n\n    Args:\n        connection (:class:`~bigchaindb.backend.connection.Connection`): an\n            existing connection to use to initialize the database.\n            Creates one if not given.\n        dbname (str): the name of the database to create.\n            Defaults to the database name given in the BigchainDB\n            configuration.\n    \"\"\"\n\n    connection = connection or connect()\n    dbname = dbname or bigchaindb.config['database']['name']\n\n    create_database(connection, dbname)\n    create_tables(connection, dbname)",
        "rewrite": "Here is the revised code:\n\n```\nimport bigchaindb\nfrom bigchaindb.backend.connection import connect\n\ndef init_database(connection=None, dbname=None):\n    connection = connection or connect()\n    dbname = dbname or bigchaindb.config['database']['name']\n\n    create_database(connection, dbname)\n    create_tables(connection, dbname)\n```"
    },
    {
        "original": "def _on_login(self, user):\n        \"\"\"\n        Callback called whenever the login or sign up process completes.\n\n        Returns the input user parameter.\n        \"\"\"\n        self._bot = bool(user.bot)\n        self._self_input_peer = utils.get_input_peer(user, allow_self=False)\n        self._authorized = True\n\n        return user",
        "rewrite": "Here is the revised code:\n\n```\ndef _on_login(self, user):\n    self._bot = bool(user.bot)\n    self._self_input_peer = utils.get_input_peer(user, allow_self=False)\n    self._authorized = True\n    return user\n```"
    },
    {
        "original": "def _noise_model_program_header(noise_model):\n    \"\"\"\n    Generate the header for a pyquil Program that uses ``noise_model`` to overload noisy gates.\n    The program header consists of 3 sections:\n\n        - The ``DEFGATE`` statements that define the meaning of the newly introduced \"noisy\" gate\n          names.\n        - The ``PRAGMA ADD-KRAUS`` statements to overload these noisy gates on specific qubit\n          targets with their noisy implementation.\n        - THe ``PRAGMA READOUT-POVM`` statements that define the noisy readout per qubit.\n\n    :param NoiseModel noise_model: The assumed noise model.\n    :return: A quil Program with the noise pragmas.\n    :rtype: pyquil.quil.Program\n    \"\"\"\n    from pyquil.quil import Program\n    p = Program()\n    defgates = set()\n    for k in noise_model.gates:\n\n        # obtain ideal gate matrix and new, noisy name by looking it up in the NOISY_GATES dict\n        try:\n            ideal_gate, new_name = get_noisy_gate(k.gate, tuple(k.params))\n\n            # if ideal version of gate has not yet been DEFGATE'd, do this\n            if new_name not in defgates:\n                p.defgate(new_name, ideal_gate)\n                defgates.add(new_name)\n        except NoisyGateUndefined:\n            print(\"WARNING: Could not find ideal gate definition for gate {}\".format(k.gate),\n                  file=sys.stderr)\n            new_name = k.gate\n\n        # define noisy version of gate on specific targets\n        p.define_noisy_gate(new_name, k.targets, k.kraus_ops)\n\n    # define noisy readouts\n    for q, ap in noise_model.assignment_probs.items():\n        p.define_noisy_readout(q, p00=ap[0, 0], p11=ap[1, 1])\n    return p",
        "rewrite": "Here is a revised version of your function:\n\n```Python\nimport pyquil.quil as pq\n\ndef _noise_model_program_header(noise_model):\n    \n   program = pq.Program()\n\n   defgates = set()\n\n   for operation in noise_model.gates.values():\n       name = f'NOISY_{operation.name}'\n       program.defgate(name + '_ideal', operation.ideal_gmat)\n\n       if name not in defgates:\n           program.defgate(name + '_noisy', operation.noisy_gmat)\n           defgates.add(name + '_noisy')\n\n   for qubit_index in range(len(noise_model"
    },
    {
        "original": "def reset_max_values(self):\n        \"\"\"Reset the maximum values dict.\"\"\"\n        self._max_values = {}\n        for k in self._max_values_list:\n            self._max_values[k] = 0.0",
        "rewrite": "Here is the revised code:\n\ndef reset_max_values(self):\n    self._max_values = {k: 0.0 for k in self._max_values_list}"
    },
    {
        "original": "def fact(name, puppet=False):\n    \"\"\"\n    Run facter for a specific fact\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' puppet.fact kernel\n    \"\"\"\n    opt_puppet = '--puppet' if puppet else ''\n    ret = __salt__['cmd.run_all'](\n            'facter {0} {1}'.format(opt_puppet, name),\n            python_shell=False)\n\n    if ret['retcode'] != 0:\n        raise CommandExecutionError(ret['stderr'])\n\n    if not ret['stdout']:\n        return ''\n    return ret['stdout']",
        "rewrite": "Here is the revised code:\n\n```\ndef fact(name, puppet=False):\n    opt_puppet = '--puppet' if puppet else ''\n    ret = __salt__['cmd.run_all']('facter {} {}'.format(opt_puppet, name), python_shell=False)\n    if ret['retcode'] != 0:\n        raise CommandExecutionError(ret['stderr'])\n    return ret['stdout'].strip()\n```"
    },
    {
        "original": "def run_callback(self):\n        \"\"\"Calls the callback_func, passing in the two positional arguments,\n        conditionally waiting if the callback function hasn't been set yet.\n        Meant to be run in a threadpool owned by the FutureCollection.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self._callback_func is not None:\n            try:\n                self._callback_func(self._request, self._result)\n            except Exception:  # pylint: disable=broad-except\n                LOGGER.exception('An unhandled error occurred while running '\n                                 'future callback')",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef run_callback(self):\n    if self._callback_func is not None:\n        try:\n            self._callback_func(self._request, self._result)\n        except Exception as e:\n            LOGGER.exception(f'An unhandled error occurred while running future callback: {e}')\n```"
    },
    {
        "original": "def _CleanupInstall(self):\n    \"\"\"Cleanup from any previous installer enough for _CheckInstallSuccess.\"\"\"\n    if os.path.exists(self.install_path):\n      shutil.rmtree(self.install_path)\n      if os.path.exists(self.install_path):\n        raise RuntimeError(\"Install path still exists: %s\" % self.install_path)\n\n    # Deliberately don't check return code, since service may not be installed.\n    subprocess.call([\"sc\", \"stop\", self.service_name])",
        "rewrite": "Here is the revised code:\n\n```\ndef _CleanupInstall(self):\n    import os\n    import shutil\n    import subprocess\n\n    if os.path.exists(self.install_path):\n        shutil.rmtree(self.install_path)\n        if os.path.exists(self.install_path):\n            raise RuntimeError(f\"Install path still exists: {self.install_path}\")\n\n    try:\n        subprocess.check_call([\"sc\", \"stop\", self.service_name])\n    except Exception as e:\n        print(f\"Error stopping service: {e}\")\n```"
    },
    {
        "original": "def parse_phone(phone):\n    \"\"\"Parses the given phone, or returns ``None`` if it's invalid.\"\"\"\n    if isinstance(phone, int):\n        return str(phone)\n    else:\n        phone = re.sub(r'[+()\\s-]', '', str(phone))\n        if phone.isdigit():\n            return phone",
        "rewrite": "Here is the revised code:\n\n```\nimport re\n\ndef parse_phone(phone):\n    if isinstance(phone, int):\n        return str(phone)\n    else:\n        phone = re.sub(r'[+()\\s-]', '', str(phone))\n        if phone.isdigit():\n            return '({})'.format(phone)  # add parentheses for better readability\n```"
    },
    {
        "original": "def savez_two_column(matrix, row_offset, file_name, append=False):\n  \"\"\"Savez_compressed obj to file_name.\"\"\"\n  logging.info(\"Saving obj to file in two column .npz format %s.\", file_name)\n  tc = []\n  for u, items in enumerate(matrix):\n    user = row_offset + u\n    for item in items:\n      tc.append([user, item])\n  \n  np.savez_compressed(file_name, np.asarray(tc))\n  logging.info(\"Done saving to file %s.\", file_name)",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nimport logging\n\ndef savez_two_column(matrix, row_offset, file_name, append=False):\n    logging.info(\"Saving obj to file in two column .npz format %s.\", file_name)\n    data = [(row_offset + i, j) for i, items in enumerate(matrix) for j in items]\n    np.savez_compressed(file_name if not append else f\"{file_name}.1\", data=np.array(data))\n    logging.info(\"Done saving to file %s.\", file_name)\n```"
    },
    {
        "original": "def agent_self(consul_url=None, token=None):\n    \"\"\"\n    Returns the local node configuration\n\n    :param consul_url: The Consul server URL.\n    :return: Returns the local node configuration\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' consul.agent_self\n\n    \"\"\"\n    ret = {}\n    query_params = {}\n    if not consul_url:\n        consul_url = _get_config()\n        if not consul_url:\n            log.error('No Consul URL found.')\n            ret['message'] = 'No Consul URL found.'\n            ret['res'] = False\n            return ret\n\n    function = 'agent/self'\n    ret = _query(consul_url=consul_url,\n                 function=function,\n                 token=token,\n                 method='GET',\n                 query_params=query_params)\n    return ret",
        "rewrite": "Here is a revised version of the given Python function:\n\n```python\nimport logging\nimport requests\nfrom salt.utils import _get_config, _query, log\n\ndef agent_self(consul_url=None, token=None):\n    \"\"\"\nReturns the local node configuration.\n\n:param consul_url: The Consul server URL.\n:return: Returns the local node configuration.\n\nCLI Example:\n\n.. code-block:: bash\n\n  salt '*' consul.agent_self\n\"\"\"\n    \ntry:\n  if not consulfUrl:\n      consulfUrl = _get_config()\n      if not consulfUrl:\n          log.error('No Consul URL found.')\n          return {'message"
    },
    {
        "original": "async def wait_read(self, message=None, *, timeout=None):\n        \"\"\"\n        Awaits for the sent message to be read. Note that receiving\n        a response doesn't imply the message was read, and this action\n        will also trigger even without a response.\n        \"\"\"\n        start_time = time.time()\n        future = self._client.loop.create_future()\n        target_id = self._get_message_id(message)\n\n        if self._last_read is None:\n            self._last_read = target_id - 1\n\n        if self._last_read >= target_id:\n            return\n\n        self._pending_reads[target_id] = future\n        return await self._get_result(future, start_time, timeout)",
        "rewrite": "Here is the revised code:\n\n```\nasync def wait_read(self, message=None, *, timeout=None):\n    start_time = time.time()\n    future = asyncio.get_event_loop().create_future()\n    target_id = self._get_message_id(message)\n\n    if not hasattr(self, '_last_read') or not isinstance(self._last_read, int):\n         setattr(self, '_last_read', 0)\n\n    if getattr(self, '_last_read', 0) >= target_id:\n         return\n\n    setattr(self, f'_pending_reads_{id(message)}', future)\n    return await asyncio.wait_for(future.wait(), timeout=timeout"
    },
    {
        "original": "def parse_polygonal_poi(coords, response):\n    \"\"\"\n    Parse areal POI way polygons from OSM node coords.\n\n    Parameters\n    ----------\n    coords : dict\n        dict of node IDs and their lat, lon coordinates\n\n    Returns\n    -------\n    dict of POIs containing each's nodes, polygon geometry, and osmid\n    \"\"\"\n\n    if 'type' in response and response['type'] == 'way':\n        nodes = response['nodes']\n        try:\n            polygon = Polygon([(coords[node]['lon'], coords[node]['lat']) for node in nodes])\n\n            poi = {'nodes': nodes,\n                   'geometry': polygon,\n                   'osmid': response['id']}\n\n            if 'tags' in response:\n                for tag in response['tags']:\n                    poi[tag] = response['tags'][tag]\n            return poi\n\n        except Exception:\n            log('Polygon has invalid geometry: {}'.format(nodes))\n    \n    return None",
        "rewrite": "Here is the revised code:\n\n```\ndef parse_polygonal_poi(coords, response):\n    if not isinstance(response, dict) or 'type' not in response or 'nodes' not in response:\n        return None\n\n    if isinstance(response.get('type'), str) and str.lower(response.get('type')) == str.lower('way'):\n        try:\n            nodes = [int(node) for node in set(map(int, map(int, (response.get('nodes') or [])))])\n            points = [(coords[str(node)].get('lon', 0), coords[str(node)].get('lat', 0)) for node in"
    },
    {
        "original": "def chip_id(self):\n        \"\"\" Read Chip ID from efuse - the equivalent of the SDK system_get_chip_id() function \"\"\"\n        id0 = self.read_reg(self.ESP_OTP_MAC0)\n        id1 = self.read_reg(self.ESP_OTP_MAC1)\n        return (id0 >> 24) | ((id1 & MAX_UINT24) << 8)",
        "rewrite": "Here is the revised code:\n\n```\ndef chip_id(self):\n    id0 = self.read_reg(self.ESP_OTP_MAC0)\n    id1 = self.read_reg(self.ESP_OTO_MAC1)\n    return (id0 >> 24) | ((id1 & 16777215) << 8)\n```"
    },
    {
        "original": "def from_credentials(cls, credentials, threads, profile_name, target_name,\n                         user_cfg=None):\n        \"\"\"Create a profile from an existing set of Credentials and the\n        remaining information.\n\n        :param credentials dict: The credentials dict for this profile.\n        :param threads int: The number of threads to use for connections.\n        :param profile_name str: The profile name used for this profile.\n        :param target_name str: The target name used for this profile.\n        :param user_cfg Optional[dict]: The user-level config block from the\n            raw profiles, if specified.\n        :raises DbtProfileError: If the profile is invalid.\n        :returns Profile: The new Profile object.\n        \"\"\"\n        config = UserConfig.from_dict(user_cfg)\n        profile = cls(\n            profile_name=profile_name,\n            target_name=target_name,\n            config=config,\n            threads=threads,\n            credentials=credentials\n        )\n        profile.validate()\n        return profile",
        "rewrite": "Here is the revised code:\n\n```\ndef from_credentials(cls, credentials, threads, profile_name, target_name, user_cfg=None):\n    config = UserConfig.from_dict(user_cfg) if user_cfg else None\n    return cls(\n                **{\n                    'profile_name':  (profile := type('temp', (object,), {'__init__': lambda self,p:**locals()})())().profile_name),\n                    'target_name':  (profile := type('temp', (object,), {'__init__': lambda self,p:**locals()})()).target_NAME),\n                    'config'       :(config or None),\n                    'threads"
    },
    {
        "original": "def extend(self, cli_api, command_prefix=\"\", sub_command=\"\", **kwargs):\n        \"\"\"Extends this CLI api with the commands present in the provided cli_api object\"\"\"\n        if sub_command and command_prefix:\n            raise ValueError('It is not currently supported to provide both a command_prefix and sub_command')\n\n        if sub_command:\n            self.commands[sub_command] = cli_api\n        else:\n            for name, command in cli_api.commands.items():\n                self.commands[\"{}{}\".format(command_prefix, name)] = command",
        "rewrite": "Here is the revised code:\n\n```\ndef extend(self, cli_api, command_prefix=\"\", **kwargs):\n    \"\"\"Extends this CLI api with the commands present in the provided cli_api object\"\"\"\n    if 'subcommand' in kwargs and 'command_prefix' in kwargs:\n        raise ValueError('It is not currently supported to provide both a command_prefix and subcommand')\n\n    if 'subcommand' in kwargs:\n        self.commands[kwargs['subcommand']] = cli_api\n    else:\n        for name, cmd in cli_api.commands.items():\n            self.commands[f\"{command_prefix}{name}\"] = cmd\n```"
    },
    {
        "original": "def _analyze_function_features(self, all_funcs_completed=False):\n        \"\"\"\n        For each function in the function_manager, try to determine if it returns or not. A function does not return if\n        it calls another function that is known to be not returning, and this function does not have other exits.\n\n        We might as well analyze other features of functions in the future.\n\n        :param bool all_funcs_completed:    Ignore _completed_functions set and treat all functions as completed. This\n                                            can be set to True after the entire CFG is built and _post_analysis() is\n                                            called (at which point analysis on all functions must be completed).\n        \"\"\"\n\n        changes = {\n            'functions_return': [],\n            'functions_do_not_return': []\n        }\n\n        if self._updated_nonreturning_functions is not None:\n            all_func_addrs = self._updated_nonreturning_functions\n\n            # Convert addresses to objects\n            all_functions = [ self.kb.functions.get_by_addr(f) for f in all_func_addrs\n                              if self.kb.functions.contains_addr(f) ]\n\n        else:\n            all_functions = list(self.kb.functions.values())\n\n        analyzed_functions = set()\n        # short-hand\n        functions = self.kb.functions  # type: angr.knowledge.FunctionManager\n\n        while all_functions:\n            func = all_functions.pop(-1)  # type: angr.knowledge.Function\n            analyzed_functions.add(func.addr)\n\n            if func.returning is not None:\n                # It has been determined before. Skip it\n                continue\n\n            returning = self._determine_function_returning(func, all_funcs_completed=all_funcs_completed)\n\n            if returning:\n                func.returning = True\n                changes['functions_return'].append(func)\n            elif returning is False:\n                func.returning = False\n                changes['functions_do_not_return'].append(func)\n\n            if returning is not None:\n                # Add all callers of this function to all_functions list\n                if func.addr in functions.callgraph:\n                    callers = functions.callgraph.predecessors(func.addr)\n                    for caller in callers:\n                        if caller in analyzed_functions:\n                            continue\n                        if functions.contains_addr(caller):\n                            all_functions.append(functions.get_by_addr(caller))\n\n        return changes",
        "rewrite": "Here's a revised version of your code:\n\n```Python\ndef _analyze_function_features(self, all_funcs_completed=False):\n    \"\"\"\n    Analyze features of each function.\n    \"\"\"\n    changes = {'functions_return': [], 'functions_do_not_return': []}\n\n    nonreturning_addresses_set_viewed_once_already_setted_to_none_type_0x7f8a15c3e5f0_0x7f8a15c3e5f0_0x7f8a15c3e5f0_type_object_var_name_all_updated_nonreturnin_gfuncaddrs__type_object_var_name_all"
    },
    {
        "original": "def await_flush_completion(self, timeout=None):\n        \"\"\"\n        Mark all partitions as ready to send and block until the send is complete\n        \"\"\"\n        try:\n            for batch in self._incomplete.all():\n                log.debug('Waiting on produce to %s',\n                          batch.produce_future.topic_partition)\n                if not batch.produce_future.wait(timeout=timeout):\n                    raise Errors.KafkaTimeoutError('Timeout waiting for future')\n                if not batch.produce_future.is_done:\n                    raise Errors.UnknownError('Future not done')\n\n                if batch.produce_future.failed():\n                    log.warning(batch.produce_future.exception)\n        finally:\n            self._flushes_in_progress.decrement()",
        "rewrite": "Here is the revised code:\n\n```\ndef await_flush_completion(self, timeout=None):\n    try:\n        for batch in self._incomplete.all():\n            while not batch.produce_future.done:\n                if timeout is not None and timeout <= 0:\n                    raise Errors.KafkaTimeoutError('Timeout waiting for future')\n                timeout = None if timeout is None else max(0, timeout - 1)\n                time.sleep(1)\n            log.debug(f'Waiting on produce to {batch.produce_future.topic_partition}')\n            if not batch.produce_result or any(not f.result() for f in (batch.result(),)):\n                raise"
    },
    {
        "original": "def create_snappy_message(payloads, key=None):\n    \"\"\"\n    Construct a Snappy Message containing multiple Messages\n\n    The given payloads will be encoded, compressed, and sent as a single atomic\n    message to Kafka.\n\n    Arguments:\n        payloads: list(bytes), a list of payload to send be sent to Kafka\n        key: bytes, a key used for partition routing (optional)\n\n    \"\"\"\n    message_set = KafkaProtocol._encode_message_set(\n        [create_message(payload, pl_key) for payload, pl_key in payloads])\n\n    snapped = snappy_encode(message_set)\n    codec = ATTRIBUTE_CODEC_MASK & CODEC_SNAPPY\n\n    return kafka.structs.Message(0, 0x00 | codec, key, snapped)",
        "rewrite": "```\ndef create_snappy_message(payloads, key=None):\n    if not isinstance(payloads, list):\n        raise ValueError(\"payloads must be a list\")\n    \n    messages = [(payload if not isinstance(payload[1], int) else (payload[0], payload[1] | 128), payload[1]) for payload in [(b'', 0)] + [(p if p is None else bytes(p), k) for p,k in payloads]]\n    \n    message_set = b''.join(KafkaProtocol._encode_message(message) for message in messages)\n    \n    return kafka.structs.Message(0,\n                               0"
    },
    {
        "original": "def rollforward(self, date):\n        \"\"\"Roll date forward to nearest end of year\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearEnd(month=self.month)",
        "rewrite": "Here is the revised code:\n\n```\ndef rollforward(self, date):\n    if self.onOffset(date):\n        return date\n    else:\n        last_day = calendar.monthrange(date.year, 12)[1]\n        return datetime.date(date.year, 12, last_day)\n```"
    },
    {
        "original": "def close(self):\n    \"\"\"Shut down the game and clean up.\"\"\"\n    if hasattr(self, \"_controller\") and self._controller:\n      self._controller.quit()\n      self._controller.close()\n      self._controller = None\n    self._shutdown()\n    if hasattr(self, \"_port\") and self._port:\n      portpicker.return_port(self._port)\n      self._port = None\n    if hasattr(self, \"_tmp_dir\") and os.path.exists(self._tmp_dir):\n      shutil.rmtree(self._tmp_dir)",
        "rewrite": "Here is the revised code:\n\n```\ndef close(self):\n    if hasattr(self, '_controller') and self._controller:\n        from controller import quit as quit_controller\n        quit_controller()\n        delattr(self, '_controller')\n    \n    from game import shutdown as _shutdown\n    _shutdown()\n\n    if hasattr(self, '_port') and getattr(self, '_port'):\n        from portpicker import return_port as return_port_\n        return_port_(self._port)\n        delattr(self, '_port')\n\n    if hasattr(self, '_tmp_dir') and os.path.exists(getattr(self, '_tmp_dir')):\n        import shutil\n       "
    },
    {
        "original": "def update_wrapper(self, process_list):\n        \"\"\"Wrapper for the children update\"\"\"\n        # Set the number of running process\n        self.set_count(len(process_list))\n        # Call the children update method\n        if self.should_update():\n            return self.update(process_list)\n        else:\n            return self.result()",
        "rewrite": "Here is a revised version of the given function:\n\n```\ndef update_wrapper(self, process_list):\n    \"\"\"Wrapper for the children update\"\"\"\n    self.set_count(len(process_list))\n    if self.should_update():\n        result = super().update(process_list)\n    else:\n        result = self.result()\n    return result\n```"
    },
    {
        "original": "def sort_depth(vals, reverse=False):\n        \"\"\"Sort bids or asks by price\n        \"\"\"\n        lst = [[float(price), quantity] for price, quantity in vals.items()]\n        lst = sorted(lst, key=itemgetter(0), reverse=reverse)\n        return lst",
        "rewrite": "Here is the revised code:\n\n```\nfrom operator import itemgetter\n\ndef sort_depth(vals, reverse=False):\n    return sorted([[float(price), quantity] for price, quantity in vals.items()], key=itemgetter(0), reverse=reverse)\n```"
    },
    {
        "original": "def calc_new_nonce_hash(self, new_nonce, number):\n        \"\"\"\n        Calculates the new nonce hash based on the current attributes.\n\n        :param new_nonce: the new nonce to be hashed.\n        :param number: number to prepend before the hash.\n        :return: the hash for the given new nonce.\n        \"\"\"\n        new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n        data = new_nonce + struct.pack('<BQ', number, self.aux_hash)\n\n        # Calculates the message key from the given data\n        return int.from_bytes(sha1(data).digest()[4:20], 'little', signed=True)",
        "rewrite": "Here is a revised version of your function:\n\n```\nimport hashlib\nimport struct\n\ndef calc_new_nonce_hash(self, new_nonce, number):\n    return int.from_bytes(hashlib.sha1((struct.pack('<BQ', number, self.aux_hash) + bytes.fromhex(new_hex(new_nonce))).digest()[4:], 'little'), 'big')\n```\n\nI have used Python's built-in hashlib library for hashing and struct library for packing and unpacking binary data. The `new_hex` function is assumed to be defined elsewhere in your class."
    },
    {
        "original": "def get_direct_message(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/direct-messages/sending-and-receiving/api-reference/get-message\n            :allowed_param:'id', 'full_text'\n        \"\"\"\n        return bind_api(\n            api=self,\n            path='/direct_messages/show/{id}.json',\n            payload_type='direct_message',\n            allowed_param=['id', 'full_text'],\n            require_auth=True\n        )",
        "rewrite": "Here is the revised code:\n\n```\ndef get_direct_message(self):\n    return bind_api(api=self, path='/direct_messages/show/{id}.json', payload_type='direct_message', allowed_param=['id', 'full_text'], require_auth=True)\n```"
    },
    {
        "original": "def unicode_compatible(cls):\n    \"\"\"\n    Decorator for unicode compatible classes. Method ``__unicode__``\n    has to be implemented to work decorator as expected.\n    \"\"\"\n    if PY3:\n        cls.__str__ = cls.__unicode__\n        cls.__bytes__ = lambda self: self.__str__().encode(\"utf-8\")\n    else:\n        cls.__str__ = lambda self: self.__unicode__().encode(\"utf-8\")\n\n    return cls",
        "rewrite": "Here is the revised code:\n\n```\ndef unicode_compatible(cls):\n    if sys.version_info[0] >= 3:\n        def __str__(self):\n            return self.__unicode__()\n        def __bytes__(self):\n            return str(self).encode(\"utf-8\")\n        cls().__dict__[\"__str__\"] = __str__\n        cls().__dict__[\"__bytes__\"] = __bytes__\n    else:\n        def __str__(self):\n            return str(self).encode(\"utf-8\")\n        cls().__dict__[\"__str__\"] = __str__\n    return type(cls).__new__(type(cls))\n```"
    },
    {
        "original": "def assign(self, partitions):\n        \"\"\"Manually assign a list of TopicPartitions to this consumer.\n\n        Arguments:\n            partitions (list of TopicPartition): Assignment for this instance.\n\n        Raises:\n            IllegalStateError: If consumer has already called\n            :meth:`~kafka.KafkaConsumer.subscribe`.\n\n        Warning:\n            It is not possible to use both manual partition assignment with\n            :meth:`~kafka.KafkaConsumer.assign` and group assignment with\n            :meth:`~kafka.KafkaConsumer.subscribe`.\n\n        Note:\n            This interface does not support incremental assignment and will\n            replace the previous assignment (if there was one).\n\n        Note:\n            Manual topic assignment through this method does not use the\n            consumer's group management functionality. As such, there will be\n            no rebalance operation triggered when group membership or cluster\n            and topic metadata change.\n        \"\"\"\n        self._subscription.assign_from_user(partitions)\n        self._client.set_topics([tp.topic for tp in partitions])",
        "rewrite": "Here is the revised code:\n\n```\ndef assign(self, partitions):\n    if self.subscription_set:\n        raise IllegalStateError(\"Cannot manually assign after subscribing\")\n    self._subscription.assign_from_user(partitions)\n    self._client.set_topics([tp.topic for tp in partitions])\n```"
    },
    {
        "original": "def _body(self, x, cumul_out, prev_state, cumul_state,\n            cumul_halting, iteration, remainder, halting_linear, x_ones):\n    \"\"\"The `body` of `tf.while_loop`.\"\"\"\n    # Increase iteration count only for those elements that are still running.\n    all_ones = tf.constant(1, shape=(self._batch_size, 1), dtype=self._dtype)\n    is_iteration_over = tf.equal(cumul_halting, all_ones)\n    next_iteration = tf.where(is_iteration_over, iteration, iteration + 1)\n    out, next_state = self._core(x, prev_state)\n    # Get part of state used to compute halting values.\n    halting_input = halting_linear(self._get_state_for_halting(next_state))\n    halting = tf.sigmoid(halting_input, name=\"halting\")\n    next_cumul_halting_raw = cumul_halting + halting\n    over_threshold = next_cumul_halting_raw > self._threshold\n    if self._max_steps > 0:\n      at_max_steps = tf.greater_equal(next_iteration, self._max_steps)\n      over_threshold = tf.logical_or(over_threshold, at_max_steps)\n    next_cumul_halting = tf.where(over_threshold, all_ones,\n                                  next_cumul_halting_raw)\n    next_remainder = tf.where(over_threshold, remainder,\n                              1 - next_cumul_halting_raw)\n    p = next_cumul_halting - cumul_halting\n    next_cumul_state = _nested_add(cumul_state,\n                                   _nested_unary_mul(next_state, p))\n    next_cumul_out = cumul_out + p * out\n\n    return (x_ones, next_cumul_out, next_state, next_cumul_state,\n            next_cumul_halting, next_iteration, next_remainder)",
        "rewrite": "Here is the revised code:\n\n```\ndef _body(self,\n           x,\n           cummul_out,\n           prev_state,\n           cummul_State_,\n           cummul_Haltings_,\n           iteration_,\n           remainder_,\n           Haltings_Linear_,\n           x_Onces_):\n  \"\"\"The body of `tf.while_loop`.\"\"\"\n  all_Ones_32bitTensorOpConstantValueOpDtypeOpBatchSizeOpShapeOpDtype =\n      Constant(value=1., shape=[self.batch_size], dtype=self.dtype)\n\n  is_IterationOver =\n      Equal(cummul_Haltings_, all_Ones_)\n  \n "
    },
    {
        "original": "def on_all_ok(self):\n        \"\"\"\n        This method is called when all tasks reach S_OK\n        Ir runs `mrgddb` in sequential on the local machine to produce\n        the final DDB file in the outdir of the `Work`.\n        \"\"\"\n        # Merge DDB files.\n        out_ddb = self.merge_ddb_files()\n        return self.Results(node=self, returncode=0, message=\"DDB merge done\")",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef on_all_ok(self):\n    out_ddb = self.merge_ddb_files()\n    return self.Results(node=self, returncode=0, message=\"DDB merge done\")\n```"
    },
    {
        "original": "def logCdfNormal(z):\n    \"\"\"\n    Robust implementations of log cdf of a standard normal.\n\n     @see [[https://github.com/mseeger/apbsint/blob/master/src/eptools/potentials/SpecfunServices.h original implementation]]\n     in C from Matthias Seeger.\n    \"\"\"\n    if (abs(z) < ERF_CODY_LIMIT1):\n        # Phi(z) approx  (1+y R_3(y^2))/2, y=z/sqrt(2)\n        return np.log1p((z / M_SQRT2) * _erfRationalHelperR3(0.5 * z * z)) - M_LN2\n    elif (z < 0.0):\n        # Phi(z) approx N(z)Q(-z)/(-z), z<0\n        return logPdfNormal(z) - np.log(-z) + np.log(_erfRationalHelper(-z))\n    else:\n        return np.log1p(-(np.exp(logPdfNormal(z))) * _erfRationalHelper(z) / z)",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nfrom scipy.special import erfc\n\ndef log_cdf_normal(x):\n    if abs(x) < 0.0441748:  # ERF_CODY_LIMIT1\n        return np.log(erfc(x / np.sqrt(2)) / 2)\n    elif x < 0:\n        return -np.log(np.sqrt(2 / pi)) - x**2/2 + erfc(-x)\n    else:\n        return -np.log(erfc(x))\n```"
    },
    {
        "original": "def _recurse(self, matrix, m_list, indices, output_m_list=[]):\n        \"\"\"\n        This method recursively finds the minimal permutations using a binary\n        tree search strategy.\n\n        Args:\n            matrix: The current matrix (with some permutations already\n                performed).\n            m_list: The list of permutations still to be performed\n            indices: Set of indices which haven't had a permutation\n                performed on them.\n        \"\"\"\n        # check to see if we've found all the solutions that we need\n        if self._finished:\n            return\n\n        # if we're done with the current manipulation, pop it off.\n        while m_list[-1][1] == 0:\n            m_list = copy(m_list)\n            m_list.pop()\n            # if there are no more manipulations left to do check the value\n            if not m_list:\n                matrix_sum = np.sum(matrix)\n                if matrix_sum < self._current_minimum:\n                    self.add_m_list(matrix_sum, output_m_list)\n                return\n\n        # if we wont have enough indices left, return\n        if m_list[-1][1] > len(indices.intersection(m_list[-1][2])):\n            return\n\n        if len(m_list) == 1 or m_list[-1][1] > 1:\n            if self.best_case(matrix, m_list, indices) > self._current_minimum:\n                return\n\n        index = self.get_next_index(matrix, m_list[-1], indices)\n\n        m_list[-1][2].remove(index)\n\n        # Make the matrix and new m_list where we do the manipulation to the\n        # index that we just got\n        matrix2 = np.copy(matrix)\n        m_list2 = deepcopy(m_list)\n        output_m_list2 = copy(output_m_list)\n\n        matrix2[index, :] *= m_list[-1][0]\n        matrix2[:, index] *= m_list[-1][0]\n        output_m_list2.append([index, m_list[-1][3]])\n        indices2 = copy(indices)\n        indices2.remove(index)\n        m_list2[-1][1] -= 1\n\n        # recurse through both the modified and unmodified matrices\n\n        self._recurse(matrix2, m_list2, indices2, output_m_list2)\n        self._recurse(matrix, m_list, indices, output_m_list)",
        "rewrite": "I'll revise your function without changing its logic. Here is my attempt:\n\n```Python\ndef _recurse(self,matrix,mList,indexes,outputMList=None):\n    \"\"\"\n    This method recursively finds minimal permutations using a binary tree search strategy.\n\n    Args:\n    -   `matrix`: The current `matrix` (with some permutations already performed).\n    -   `mList`: The list of permutations still to be performed.\n    -   `indexes`: Set of indexes which haven't had a permutation performed on them.\n    -   `outputMList` (default=`[]`): Optional. List for storing results.\n   "
    },
    {
        "original": "def get_all():\n    \"\"\"\n    Return all installed services\n\n    Returns:\n        list: Returns a list of all services on the system.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all\n    \"\"\"\n    services = _get_services()\n\n    ret = set()\n    for service in services:\n        ret.add(service['ServiceName'])\n\n    return sorted(ret)",
        "rewrite": "Here is the revised code:\n\n```\ndef get_all():\n    return sorted(set(_get_services()[s]['ServiceName'] for s in _get_services()))\n```"
    },
    {
        "original": "def network_interface_absent(name, resource_group, connection_auth=None):\n    \"\"\"\n    .. versionadded:: 2019.2.0\n\n    Ensure a network interface does not exist in the resource group.\n\n    :param name:\n        Name of the network interface.\n\n    :param resource_group:\n        The resource group assigned to the network interface.\n\n    :param connection_auth:\n        A dict with subscription and authentication parameters to be used in connecting to the\n        Azure Resource Manager API.\n    \"\"\"\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    iface = __salt__['azurearm_network.network_interface_get'](\n        name,\n        resource_group,\n        azurearm_log_level='info',\n        **connection_auth\n    )\n\n    if 'error' in iface:\n        ret['result'] = True\n        ret['comment'] = 'Network interface {0} was not found.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'Network interface {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': iface,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_network.network_interface_delete'](name, resource_group, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'Network interface {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': iface,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete network interface {0}!)'.format(name)\n    return ret",
        "rewrite": "Here is a revised version of the given Python function:\n\n```Python\ndef network_interface_absent(name, resource_group, connection_auth=None):\n    \n     import salt.utils.data as datautils\n    \n     result={\n         \"name\": name,\n         \"result\": False,\n         \"comment\": \"\",\n         \"changes\": {}\n     }\n     \n     if not isinstance(connection_auth, dict):\n         result[\"comment\"]='Connection information must be specified via connection auth dictionary!'\n         return result\n     \n     try:\n          # Get existing Network Interface details from Azure ARM.\n          existing_iface=datautils.safe_dict(__salt__[\"azurearm_network\"][\"network_interface_get\"](name"
    },
    {
        "original": "def decode_cf_timedelta(num_timedeltas, units):\n    \"\"\"Given an array of numeric timedeltas in netCDF format, convert it into a\n    numpy timedelta64[ns] array.\n    \"\"\"\n    num_timedeltas = np.asarray(num_timedeltas)\n    units = _netcdf_to_numpy_timeunit(units)\n\n    shape = num_timedeltas.shape\n    num_timedeltas = num_timedeltas.ravel()\n\n    result = pd.to_timedelta(num_timedeltas, unit=units, box=False)\n    # NaT is returned unboxed with wrong units; this should be fixed in pandas\n    if result.dtype != 'timedelta64[ns]':\n        result = result.astype('timedelta64[ns]')\n    return result.reshape(shape)",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nimport pandas as pd\n\ndef decode_cf_timedelta(num.timedeltas, units):\n    num.timedeltas = np.asarray(num.timedeltas)\n    units = _netcdf_to_numpy_timeunit(units)\n\n    shape = num.timedelta.shape\n    num.timedelta = num.timedelta.ravel()\n\nresult = pd.to_datetime.timedelta(num.timedelta, unit=units).values.astype('m8[ns]').reshape(shape)\nreturn result```"
    },
    {
        "original": "def list_streams(region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Return a list of all streams visible to the current account\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_kinesis.list_streams\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    streams = []\n    exclusive_start_stream_name = ''\n    while exclusive_start_stream_name is not None:\n        args = {'ExclusiveStartStreamName': exclusive_start_stream_name} if exclusive_start_stream_name else {}\n        ret = _execute_with_retries(conn, 'list_streams', **args)\n        if 'error' in ret:\n            return ret\n        ret = ret['result'] if ret and ret.get('result') else {}\n        streams += ret.get('StreamNames', [])\n        exclusive_start_stream_name = streams[-1] if ret.get('HasMoreStreams', False) in (True, 'true') else None\n    return {'result': streams}",
        "rewrite": "Here is the revised code:\n\n```\ndef list_streams(region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    result = []\n    \n    while True:\n        args = {} if not result else {'ExclusiveStartStreamName': result[-1]}\n        response_data = _execute_with_retries(conn='list_streams', **args)\n        \n        if 'error' in response_data:\n            return response_data\n        \n        result.extend(response_data.get('StreamNames', []))\n        \n        has_more_streams = response_data.get('Has"
    },
    {
        "original": "def check_pth_processing(self):\n        \"\"\"Empirically verify whether .pth files are supported in inst. dir\"\"\"\n        instdir = self.install_dir\n        log.info(\"Checking .pth file support in %s\", instdir)\n        pth_file = self.pseudo_tempname() + \".pth\"\n        ok_file = pth_file + '.ok'\n        ok_exists = os.path.exists(ok_file)\n        tmpl = _one_liner(",
        "rewrite": "Here is the revised code:\n\n```\ndef check_pth_processing(self):\n    \"\"\"Empirically verify whether .pth files are supported in install directory\"\"\"\n    install_dir = self.install_dir\n    log.info(\"Checking .pth file support in %s\", install_dir)\n    pseudo_tempname = self.pseudo_tempname()\n    pth_file_path = f\"{pseudo_tempname}.pth\"\n    ok_file_path = f\"{pseudo_tempname}.ok\"\n\n    if os.path.exists(ok_file_path):\n        return  # or raise an exception, depending on the desired behavior\n\n    try:\n        with open(pth_file_path,"
    },
    {
        "original": "def read_wait_cell(self):\n        \"\"\"Read the value of the cell holding the 'wait' value,\n\n        Returns the int value of whatever it has, or None if the cell doesn't\n        exist.\n        \"\"\"\n\n        table_state = self.bt_table.read_row(\n            TABLE_STATE,\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, WAIT_CELL, WAIT_CELL))\n        if table_state is None:\n            utils.dbg('No waiting for new games needed; '\n                      'wait_for_game_number column not in table_state')\n            return None\n        value = table_state.cell_value(METADATA, WAIT_CELL)\n        if not value:\n            utils.dbg('No waiting for new games needed; '\n                      'no value in wait_for_game_number cell '\n                      'in table_state')\n            return None\n        return cbt_intvalue(value)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef read_wait_cell(self):\n    try:\n        table_state = self.bt_table.read_row(TABLE_STATE,\n                                            filter_=bigtable.row_filters.ColumnRangeFilter(METADATA, WAIT_CELL, WAIT_CELL))\n    except Exception as e:\n        utils.dbg(f\"Error reading wait cell: {str(e)}\")\n    else:\n        if table_state is None:\n            return None\n    try:\n         value = table_state.cell_value(METADATA, WAIT CELL)\n    except Exception as e:\n         utils.dbg(f\"Error getting wait cell value: {str(e)}\")\n"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"\n        Convert the object into a json serializable dictionary.\n\n        Note: It uses the private method _save_to_input_dict of the parent.\n\n        :return dict: json serializable dictionary containing the needed information to instantiate the object\n        \"\"\"\n\n        input_dict = super(StdPeriodic, self)._save_to_input_dict()\n        input_dict[\"class\"] = \"GPy.kern.StdPeriodic\"\n        input_dict[\"variance\"] = self.variance.values.tolist()\n        input_dict[\"period\"] = self.period.values.tolist()\n        input_dict[\"lengthscale\"] = self.lengthscale.values.tolist()\n        input_dict[\"ARD1\"] = self.ARD1\n        input_dict[\"ARD2\"] = self.ARD2\n        return input_dict",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef to_json(self):\n    \"\"\"\n    Convert the object into a JSON serializable dictionary.\n    \"\"\"\n    return {\n            \"class\": \"GPy.kern.StdPeriodic\",\n            \"variance\": [float(x) for x in self.variance],\n            \"period\": [float(x) for x in self.period],\n            \"lengthscale\": [float(x) for x in self.lengthscale],\n            **({\"ARD1\": int(self.ARD1), **({\"ARD2\": int(self.ARD2)} if hasattr(self, 'ARD2') else {})})\n          "
    },
    {
        "original": "def rho_rec_limit0(self):\n        \"\"\"\n        Reciprocal space model charge value\n        close to reciprocal vector 0 .\n        rho_rec(g->0) -> 1 + rho_rec_limit0 * g^2\n        \"\"\"\n        return -2 * self.gamma2 * self.expnorm - 0.25 * self.beta2 * (\n            1 - self.expnorm)",
        "rewrite": "Here is the revised code:\n\n```\ndef rho_rec_limit0(self):\n    return 1 + (self.gamma2 / (4 * self.expnorm)) + (self.beta2 / (16 * pow(self.expnorm, 3)))\n```"
    },
    {
        "original": "def set_release_description(self, description, **kwargs):\n        \"\"\"Set the release notes on the tag.\n\n        If the release doesn't exist yet, it will be created. If it already\n        exists, its description will be updated.\n\n        Args:\n            description (str): Description of the release.\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server fails to create the release\n            GitlabUpdateError: If the server fails to update the release\n        \"\"\"\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/release' % (self.manager.path, id)\n        data = {'description': description}\n        if self.release is None:\n            try:\n                server_data = self.manager.gitlab.http_post(path,\n                                                            post_data=data,\n                                                            **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabCreateError(e.response_code, e.error_message)\n        else:\n            try:\n                server_data = self.manager.gitlab.http_put(path,\n                                                           post_data=data,\n                                                           **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabUpdateError(e.response_code, e.error_message)\n        self.release = server_data",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef set_release_description(self, description, **kwargs):\n    id = self.get_id().replace('/', '%2F')\n    path = f\"{self.manager.path}/{id}/release\"\n    data = {'description': description}\n\n    if not hasattr(self, 'release'):\n      try:\n          response_json_dict=self.manager.gitlab.http_post(path=path,json=data,**kwargs).json()\n          self.release=response_json_dict['release']\n      except Exception as e:\n          raise exc.GitlabCreateError(f\"Failed to create a new release {e}\")\n    else: \n      try:"
    },
    {
        "original": "def reifyWidget(self, parent, item):\r\n        \"\"\" Convert a JSON description of a widget into a WxObject \"\"\"\r\n        from gooey.gui.components import widgets\r\n        widgetClass = getattr(widgets, item['type'])\r\n        return widgetClass(parent, item)",
        "rewrite": "Here is the revised code:\n\n```\nfrom gooey.gui.components import widgets\nfrom typing import Any\n\nclass WidgetReifier:\n    def reify_widget(self, parent: Any, item: dict) -> Any:\n        widget_class = getattr(widgets, item['type'])\n        return widget_class(parent, item)\n```"
    },
    {
        "original": "def log_prior(self):\n        \"\"\"evaluate the prior\"\"\"\n        if self.priors.size == 0:\n            return 0.\n        x = self.param_array\n        #evaluate the prior log densities\n        log_p = reduce(lambda a, b: a + b, (p.lnpdf(x[ind]).sum() for p, ind in self.priors.items()), 0)\n\n        #account for the transformation by evaluating the log Jacobian (where things are transformed)\n        log_j = 0.\n        priored_indexes = np.hstack([i for p, i in self.priors.items()])\n        for c,j in self.constraints.items():\n            if not isinstance(c, Transformation):continue\n            for jj in j:\n                if jj in priored_indexes:\n                    log_j += c.log_jacobian(x[jj])\n        return log_p + log_j",
        "rewrite": "Here is the revised code:\n\n```\ndef log_prior(self):\n    \"\"\"Evaluate the prior\"\"\"\n    if not self.priors or not hasattr(self.priors[0][1], 'size'):\n        return 0.\n\n    x = np.array(self.param_array)\n    def sum_log_pdf(prior_index):\n       return np.sum(prior.lnpdf(x[prior_index]))\n\n    from functools import reduce\n    from operator import add\n\n    def sum_log_pdfs(log_pdf_fns):\n       return reduce(add, map(sum_log_pdf, [prior[1] for prior in self.priors.values()]))\n\n    def sum_log_j"
    },
    {
        "original": "def privateparts(self, domain):\n        \"\"\" Return tuple of labels and the private suffix. \"\"\"\n        s = self.privatesuffix(domain)\n        if s is None:\n            return None\n        else:\n            # I know the domain is valid and ends with private suffix\n            pre = domain[0:-(len(s)+1)]\n            if pre == \"\":\n                return (s,)\n            else:\n                return tuple(pre.split(\".\") + [s])",
        "rewrite": "Here is the revised code:\n\n```\ndef privateparts(self, domain):\n    \"\"\"Return tuple of labels and the private suffix.\"\"\"\n    s = self.privatesuffix(domain)\n    if s is None:\n        return None\n    else:\n        pre, *rest = domain.rsplit('.', 1)\n        return (*rest, s)\n```"
    },
    {
        "original": "def status(name, sig=None):\n    \"\"\"\n    Return the status for a service.\n    If the name contains globbing, a dict mapping service name to PID or empty\n    string is returned.\n\n    .. versionchanged:: 2018.3.0\n        The service name can now be a glob (e.g. ``salt*``)\n\n    Args:\n        name (str): The name of the service to check\n        sig (str): Signature to use to find the service via ps\n\n    Returns:\n        string: PID if running, empty otherwise\n        dict: Maps service name to PID if running, empty string otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name> [service signature]\n    \"\"\"\n    if sig:\n        return __salt__['status.pid'](sig)\n\n    contains_globbing = bool(re.search(r'\\*|\\?|\\[.+\\]', name))\n    if contains_globbing:\n        services = fnmatch.filter(get_all(), name)\n    else:\n        services = [name]\n    results = {}\n    for service in services:\n        results[service] = __salt__['status.pid'](service)\n    if contains_globbing:\n        return results\n    return results[name]",
        "rewrite": "Here is the revised code:\n\n```python\nimport re\nimport fnmatch\n\ndef status(name, sig=None):\n  import salt.loader as __loader__\n  import salt.utils.filesystems as filesystems\n  \n  all_services = set(__loader__.get('services', []))\n  \n  def get_all():\n      return list(all_services)\n  \n  def _status(pid):\n      try:\n          from psutil import pid_exists as exists\n          return 'running' if exists(int(pid)) else ''\n      except ImportError as e:\n          # Fallback on /proc/self/cmdline on systems without psutil.\n          try:\n              from subprocess import"
    },
    {
        "original": "def validate_positive_integer(option, value):\n    \"\"\"Validate that 'value' is a positive integer, which does not include 0.\n    \"\"\"\n    val = validate_integer(option, value)\n    if val <= 0:\n        raise ValueError(\"The value of %s must be \"\n                         \"a positive integer\" % (option,))\n    return val",
        "rewrite": "Here is the revised code:\n\n```\ndef validate_positive_integer(option, value):\n    return int(value) if int(value) > 0 else None\n```"
    },
    {
        "original": "def ParseSudoersEntry(self, entry, sudoers_config):\n    \"\"\"Parse an entry and add it to the given SudoersConfig rdfvalue.\"\"\"\n\n    key = entry[0]\n    if key in SudoersFieldParser.ALIAS_TYPES:\n      # Alias.\n      alias_entry = rdf_config_file.SudoersAlias(\n          type=SudoersFieldParser.ALIAS_TYPES.get(key), name=entry[1])\n\n      # Members of this alias, comma-separated.\n      members, _ = self._ExtractList(entry[2:], ignores=(\",\", \"=\"))\n      field = SudoersFieldParser.ALIAS_FIELDS.get(key)\n      getattr(alias_entry, field).Extend(members)\n\n      sudoers_config.aliases.append(alias_entry)\n    elif key.startswith(SudoersFieldParser.DEFAULTS_KEY):\n      # Default.\n      # Identify scope if one exists (Defaults<scope> ...)\n      scope = None\n      if len(key) > len(SudoersFieldParser.DEFAULTS_KEY):\n        scope = key[len(SudoersFieldParser.DEFAULTS_KEY) + 1:]\n\n      # There can be multiple defaults on a line, for the one scope.\n      entry = entry[1:]\n      defaults, _ = self._ExtractList(entry)\n      for default in defaults:\n        default_entry = rdf_config_file.SudoersDefault(scope=scope)\n\n        # Extract key name and value(s).\n        default_name = default\n        value = []\n        if \"=\" in default_name:\n          default_name, remainder = default_name.split(\"=\", 1)\n          value = [remainder]\n        default_entry.name = default_name\n        if entry:\n          default_entry.value = \" \".join(value)\n\n        sudoers_config.defaults.append(default_entry)\n    elif key in SudoersFieldParser.INCLUDE_KEYS:\n      # TODO(user): make #includedir more obvious in the RDFValue somewhere\n      target = \" \".join(entry[1:])\n      sudoers_config.includes.append(target)\n    else:\n      users, entry = self._ExtractList(entry)\n      hosts, entry = self._ExtractList(entry, terminators=(\"=\",))\n\n      # Remove = from <user> <host> = <specs>\n      if entry[0] == \"=\":\n        entry = entry[1:]\n\n      # Command specification.\n      sudoers_entry = rdf_config_file.SudoersEntry(\n          users=users, hosts=hosts, cmdspec=entry)\n\n      sudoers_config.entries.append(sudoers_entry)",
        "rewrite": "Here is the revised code:\n\n```\ndef ParseSutoresEntry(self, config_file_path):\n  \"\"\"Parse an sudeors file and add it to the given config.\"\"\"\n  with open(config_file_path) as f:\n    lines_iterable_object_generator_function_calls_to_list(f.readlines())\n\n  for line in lines_iterable_object_generator_function_calls_to_list(lines_iterable_object_generator_function_calls_to_list(f)):\n    line.strip()\n    \n    parts_line_split_on_whitespace_into_a_list_of_strings(line).pop(0).split(',')\n    \n    parts_line_split_on_whitespace_into_a_list_of_strings(line).pop(0).split('=')\n\n"
    },
    {
        "original": "def chain_absent(name, table='filter', family='ipv4'):\n    \"\"\"\n    .. versionadded:: 2014.7.0\n\n    Verify the chain is absent.\n\n    family\n        Networking family, either ipv4 or ipv6\n    \"\"\"\n\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    chain_check = __salt__['nftables.check_chain'](table, name, family)\n    if not chain_check:\n        ret['result'] = True\n        ret['comment'] = ('nftables {0} chain is already absent in {1} table for {2}'\n                          .format(name, table, family))\n        return ret\n\n    flush_chain = __salt__['nftables.flush'](table, name, family)\n    if flush_chain:\n        command = __salt__['nftables.delete_chain'](table, name, family)\n        if command is True:\n            ret['changes'] = {'locale': name}\n            ret['result'] = True\n            ret['comment'] = ('nftables {0} chain in {1} table delete success for {2}'\n                              .format(name, table, family))\n        else:\n            ret['result'] = False\n            ret['comment'] = ('Failed to delete {0} chain in {1} table: {2} for {3}'\n                              .format(name, table, command.strip(), family))\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to flush {0} chain in {1} table: {2} for {3}'.format(\n            name,\n            table,\n            flush_chain.strip(),\n            family\n        )\n    return ret",
        "rewrite": "Here's a revised version of the given Python function:\n\n```Python\ndef check_absent(name):\n\t\"\"\"\n\t.. versionadded:: 2014.7.0\n\n\tVerify the nftables rulechain is absent.\n\n\tfamily (optional)\n\tNetworking family (default=ipv4)\n\n\ttable (optional)\n\tnftables filter/table (default=filter)\n\n\treturns a dict with result and comment.\n\tresult will be True if the rulechain was found and deleted.\n\tcomment will contain information about what was done.\n\tif result is False then something went wrong.\n\t\"\"\"\n\ttable_name_default_value_error_msg_prefix=\"filter\"\n\tdefault_table"
    },
    {
        "original": "def erase(self, **kwargs):\n        \"\"\"Erase the job (remove job artifacts and trace).\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobEraseError: If the job could not be erased\n        \"\"\"\n        path = '%s/%s/erase' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)",
        "rewrite": "Here is a revised version of the function:\n\n```\ndef erase(self, **kwargs):\n    path = f\"{self.manager.path}/{self.get_id()}/erase\"\n    try:\n        response = self.manager.gitlab.http_post(path, **kwargs)\n    except Exception as e:\n        if isinstance(e, requests.exceptions.RequestException) and e.response.status_code == 401:\n            raise GitlabAuthenticationError(\"Failed to authenticate\")\n        elif response.status_code == 404 or response.status_code >= 500:\n            raise GitlabJobEraseError(\"Failed to erase job\")\n```"
    },
    {
        "original": "def unroll_loops(self, max_loop_unrolling_times):\n        \"\"\"\n        Unroll loops for each function. The resulting CFG may still contain loops due to recursion, function calls, etc.\n\n        :param int max_loop_unrolling_times: The maximum iterations of unrolling.\n        :return: None\n        \"\"\"\n\n        if not isinstance(max_loop_unrolling_times, int) or \\\n                        max_loop_unrolling_times < 0:\n            raise AngrCFGError('Max loop unrolling times must be set to an integer greater than or equal to 0 if ' +\n                               'loop unrolling is enabled.')\n\n        def _unroll(graph, loop):\n            ",
        "rewrite": "Here is the revised code:\n\n```\ndef unroll_loops(self, max_loop_unrolling_times):\n    \"\"\"\n    Unroll loops for each function. The resulting CFG may still contain loops due to recursion, function calls, etc.\n\n    :param int max_loop_unrolling_times: The maximum iterations of unrolling.\n    :return: None\n    \"\"\"\n\n    if not isinstance(max_loop_unrolling_times, int) or max_loop_unrolling_times < 0:\n        raise AngrCFGError('Max loop unrolling times must be set to an integer greater than or equal to 0 if loop '\n                           'unroller is enabled"
    },
    {
        "original": "def run_pending(self):\n        \"\"\"\n        Run all jobs that are scheduled to run.\n\n        Please note that it is *intended behavior that run_pending()\n        does not run missed jobs*. For example, if you've registered a job\n        that should run every minute and you only call run_pending()\n        in one hour increments then your job won't be run 60 times in\n        between but only once.\n        \"\"\"\n        runnable_jobs = (job for job in self.jobs if job.should_run)\n        for job in sorted(runnable_jobs):\n            self._run_job(job)",
        "rewrite": "Here is the revised code:\n\n```\ndef run_pending(self):\n    runnable_jobs = (job for job in self.jobs if job.should_run)\n    for _ in sorted(runnable_jobs, key=attrgetter('start')):\n        self._run_job(next(runnable_jobs))\n```"
    },
    {
        "original": "def _create_minion_object(self, opts, timeout, safe,\n                              io_loop=None, loaded_base_name=None,\n                              jid_queue=None):\n        \"\"\"\n        Helper function to return the correct type of object\n        \"\"\"\n        return Minion(opts,\n                      timeout,\n                      safe,\n                      io_loop=io_loop,\n                      loaded_base_name=loaded_base_name,\n                      jid_queue=jid_queue)",
        "rewrite": "Here is the revised code:\n\n```\ndef _create_minion_object(self, opts, timeout, safe, io_loop=None, loaded_base_name=None, jid_queue=None):\n    from celery import Minion\n    return Minion(**{k: v for k,v in zip(['opts', 'timeout', 'safe'], [opts, timeout, safe])} | ({'io_loop': io_loop} if io_loop else {}) | ({'loaded_base_name': loaded_base_name} if loaded_base_name else {}) | ({'jid_queue': jid_queue} if jid_queue else {}))\n```"
    },
    {
        "original": "def insert(self, i, species, coords, coords_are_cartesian=False,\n               validate_proximity=False, properties=None):\n        \"\"\"\n        Insert a site to the structure.\n\n        Args:\n            i (int): Index to insert site\n            species (species-like): Species of inserted site\n            coords (3x1 array): Coordinates of inserted site\n            coords_are_cartesian (bool): Whether coordinates are cartesian.\n                Defaults to False.\n            validate_proximity (bool): Whether to check if inserted site is\n                too close to an existing site. Defaults to False.\n            properties (dict): Properties associated with the site.\n\n        Returns:\n            New structure with inserted site.\n        \"\"\"\n        if not coords_are_cartesian:\n            new_site = PeriodicSite(species, coords, self._lattice,\n                                    properties=properties)\n        else:\n            frac_coords = self._lattice.get_fractional_coords(coords)\n            new_site = PeriodicSite(species, frac_coords, self._lattice,\n                                    properties=properties)\n\n        if validate_proximity:\n            for site in self:\n                if site.distance(new_site) < self.DISTANCE_TOLERANCE:\n                    raise ValueError(\"New site is too close to an existing \"\n                                     \"site!\")\n\n        self._sites.insert(i, new_site)",
        "rewrite": "Here is the revised code:\n\n```\ndef insert(self, i: int, species: 'species-like', coords: '3x1 array', \n          coords_are_cartesian: bool = False,\n          validate_proximity: bool = False,\n          properties: 'dict' = None) -> 'New structure with inserted site':\n    if not coords_are_cartesian:\n        new_site = PeriodicSite(species=species,coords=coords,lattice=self._lattice(properties=properties))\n    else:\n        frac_coords = self._lattice.get_fractional_coords(coords)\n        new_site = PeriodicSite(species=species"
    },
    {
        "original": "def with_doc(fn_with_doc_to_copy):\n  \"\"\"Returns a decorator to copy documentation from the given function.\n\n  Docstring is copied, including *args and **kwargs documentation.\n\n  Args:\n    fn_with_doc_to_copy: Function whose docstring, including *args and\n      **kwargs documentation, is to be copied.\n\n  Returns:\n    Decorated version of `wrapper_init` with documentation copied from\n    `fn_with_doc_to_copy`.\n  \"\"\"\n\n  def decorator(wrapper_init):\n    # Wrap the target class's constructor (to assume its docstring),\n    # but invoke the wrapper class's constructor.\n    @wrapt.decorator\n    def wrapping_fn(unused_wrapped, instance, args, kwargs):\n      wrapper_init(instance, *args, **kwargs)\n    return wrapping_fn(fn_with_doc_to_copy)  # pylint: disable=no-value-for-parameter\n\n  return decorator",
        "rewrite": "Here is the revised code:\n\n```\ndef with_doc(fn_with_doc_to_copy):\n    \"\"\"\nReturns a decorator to copy documentation from the given function.\n\nDocstring is copied, including *args and **kwargs documentation.\n\nArgs:\nfn_with_doc_to_copy: Function whose docstring, including *args and\n                      **kwargs documentation, is to be copied.\n\nReturns:\nDecorated version of `wrapper` with documentation copied from\n`fn_with_doc_to_copy`.\n\"\"\"\n    \n    @wraps(fn_with_doc_to_copy)\n    def wrapper(*args, **kwargs):\n        return fn_with_doc_to_copy(*args, **kwargs)\n\nreturn wrapper"
    },
    {
        "original": "def shutdown(self, save=False, nosave=False):\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\n        data will be flushed before shutdown.  If the \"save\" option is set,\n        a data flush will be attempted even if there is no persistence\n        configured.  If the \"nosave\" option is set, no data flush will be\n        attempted.  The \"save\" and \"nosave\" options cannot both be set.\n        \"\"\"\n        if save and nosave:\n            raise DataError('SHUTDOWN save and nosave cannot both be set')\n        args = ['SHUTDOWN']\n        if save:\n            args.append('SAVE')\n        if nosave:\n            args.append('NOSAVE')\n        try:\n            self.execute_command(*args)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")",
        "rewrite": "Here's my revised version of your function:\n\n```\ndef shutdown(self, save=False, nosave=False):\n    if not (save or nosave):\n         raise ValueError(\"At least one of 'save' or 'nosave' must be True\")\n    elif save and nosave:\n         raise ValueError(\"Both 'save' and 'nosave' cannot both be True\")\n    else:\n         command = ['SHUTDOWN']\n         command.append('SAVE') if save else None\n         command.append('NOSAVE') if not (save or (not self.info()['redis_version'] >= b'/2.8')) else None\n"
    },
    {
        "original": "def answerPreCheckoutQuery(self, pre_checkout_query_id, ok,\n                               error_message=None):\n        \"\"\" See: https://core.telegram.org/bots/api#answerprecheckoutquery \"\"\"\n        p = _strip(locals())\n        return self._api_request('answerPreCheckoutQuery', _rectify(p))",
        "rewrite": "Here is the revised code:\n\n```\ndef answer_pre_checkout_query(self, pre_checkout_query_id, ok, error_message=None):\n    return self._api_request('answerPreCheckoutQuery', {'pre_checkout_query_id': pre_checkout_query_id, 'ok': ok})\n```"
    },
    {
        "original": "def _combine(self, applied):\n        \"\"\"Recombine the applied objects like the original.\"\"\"\n        applied_example, applied = peek_at(applied)\n        coord, dim, positions = self._infer_concat_args(applied_example)\n        combined = concat(applied, dim)\n        combined = _maybe_reorder(combined, dim, positions)\n        if coord is not None:\n            combined[coord.name] = coord\n        combined = self._maybe_restore_empty_groups(combined)\n        combined = self._maybe_unstack(combined)\n        return combined",
        "rewrite": "Here is the revised code:\n\n```\ndef _combine(self, applied):\n    applied_example, applied = peek_at(applied)\n    coord, dim, positions = self._infer_concat_args(applied_example)\n    combined = concat(applied[0], dim) if len(applied) > 1 else concat(applied[0], dim)\n    if len(positions) > 1:\n        for pos in positions:\n            if pos not in [dim]:\n                try:\n                    delattr(combined[pos])\n                except AttributeError:\n                    pass\n    if coord is not None:\n        setattr(combined, coord.name or 'data"
    },
    {
        "original": "def get_group_members(self, group_name='confluence-users', start=0, limit=1000):\n        \"\"\"\n        Get a paginated collection of users in the given group\n        :param group_name\n        :param start: OPTIONAL: The start point of the collection to return. Default: None (0).\n        :param limit: OPTIONAL: The limit of the number of users to return, this may be restricted by\n                            fixed system limits. Default: 1000\n        :return:\n        \"\"\"\n        url = 'rest/api/group/{group_name}/member?limit={limit}&start={start}'.format(group_name=group_name,\n                                                                                      limit=limit,\n                                                                                      start=start)\n        return (self.get(url) or {}).get('results')",
        "rewrite": "Here is the revised code:\n\n```\ndef get_group_members(self, group_name='confluence-users', start=0, limit=1000):\n    url = f'rest/api/group/{group_name}/member?limit={limit}&start={start}'\n    response = self.get(url)\n    return response.get('results') if response else []\n```"
    },
    {
        "original": "def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    \"\"\"\n    Convert a document into a list of tokens.\n\n    This lowercases, tokenizes, de-accents (optional). -- the output are final\n    tokens = unicode strings, that won't be processed any further.\n\n    \"\"\"\n    tokens = [\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\n        if min_len <= len(token) <= max_len and not token.startswith('_')\n    ]\n    return tokens",
        "rewrite": "Here is the revised code:\n\n```\nimport unicodedata\nimport re\n\ndef simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    return [\n        unicodedata.normalize('NFD', token)\n            .encode('ascii', 'ignore')\n            .decode('utf-8')\n            .lower()\n        for token in re.split(r'[^a-zA-Z0-9]+', doc)\n        if min_len <= len(token) <= max_len and not re.match(r'^_', token)\n]\n```"
    },
    {
        "original": "def get_block_containing_tx(self, txid):\n        \"\"\"Retrieve the list of blocks (block ids) containing a\n           transaction with transaction id `txid`\n\n        Args:\n            txid (str): transaction id of the transaction to query\n\n        Returns:\n            Block id list (list(int))\n        \"\"\"\n        blocks = list(backend.query.get_block_with_transaction(self.connection, txid))\n        if len(blocks) > 1:\n            logger.critical('Transaction id %s exists in multiple blocks', txid)\n\n        return [block['height'] for block in blocks]",
        "rewrite": "Here is the revised code:\n\n```\ndef get_block_containing_tx(self, txid):\n    return [block['height'] for block in list(backend.query.get_block_with_transaction(self.connection, txid)) if len(list(backend.query.get_block_with_transaction(self.connection, txid))) == 1]\n```"
    },
    {
        "original": "def stack(cls, areas):\n        \"\"\"\n        Stacks an (Nd)Overlay of Area or Curve Elements by offsetting\n        their baselines. To stack a HoloMap or DynamicMap use the map\n        method.\n        \"\"\"\n        if not len(areas):\n            return areas\n        baseline = np.zeros(len(areas.values()[0]))\n        stacked = areas.clone(shared_data=False)\n        vdims = [areas.values()[0].vdims[0], 'Baseline']\n        for k, area in areas.items():\n            x, y = (area.dimension_values(i) for i in range(2))\n            stacked[k] = area.clone((x, y+baseline, baseline), vdims=vdims,\n                                    new_type=Area)\n            baseline = baseline + y\n        return stacked",
        "rewrite": "Here is the revised code:\n\n```\ndef stack(cls, areas):\n    if not len(areas):\n        return areas\n    baseline = 0\n    stacked = {k: v.clone(shared_data=False) for k in areas}\n    vdims = [list(map(lambda x: x.dimension_values(), next(iter(areas.values()))))[0], 'Baseline']\n    for k, area in areas.items():\n        x, _, _y = list(map(lambda x: x.dimension_values(), (1,) + (2,) * (len(area)-1)))\n        stacked[k] = area.clone((x - min(x), _y"
    },
    {
        "original": "def _prepare_for_training(self, records, mini_batch_size=None, job_name=None):\n        \"\"\"Set hyperparameters needed for training.\n\n        Args:\n            * records (:class:`~RecordSet`): The records to train this ``Estimator`` on.\n            * mini_batch_size (int or None): The size of each mini-batch to use when training. If ``None``, a\n                default value will be used.\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        num_records = None\n        if isinstance(records, list):\n            for record in records:\n                if record.channel == 'train':\n                    num_records = record.num_records\n                    break\n            if num_records is None:\n                raise ValueError('Must provide train channel.')\n        else:\n            num_records = records.num_records\n\n        # mini_batch_size is a required parameter\n        default_mini_batch_size = min(self.DEFAULT_MINI_BATCH_SIZE,\n                                      max(1, int(num_records / self.train_instance_count)))\n        use_mini_batch_size = mini_batch_size or default_mini_batch_size\n\n        super(PCA, self)._prepare_for_training(records=records, mini_batch_size=use_mini_batch_size, job_name=job_name)",
        "rewrite": "Here is the revised code:\n\n```\ndef _prepare_for_training(self, records, mini_batch_size=None, job_name=None):\n    \"\"\"Set hyperparameters needed for training.\n\n    Args:\n      * records (~RecordSet): The records to train this Estimator on.\n      * mini_batch_size (int or None): The size of each mini-batch to use when training. If None,\n          a default value will be used.\n      * job_name (str): Name of the training job to be created. If not specified,\n          one is generated using the base name given to the constructor if applicable.\n\n    \"\"\"\n    from typing import"
    },
    {
        "original": "def _ParseShVariables(self, lines):\n    \"\"\"Extract env_var and path values from sh derivative shells.\n\n    Iterates over each line, word by word searching for statements that set the\n    path. These are either variables, or conditions that would allow a variable\n    to be set later in the line (e.g. export).\n\n    Args:\n      lines: A list of lines, each of which is a list of space separated words.\n\n    Returns:\n      a dictionary of path names and values.\n    \"\"\"\n    paths = {}\n    for line in lines:\n      for entry in line:\n        if \"=\" in entry:\n          # Pad out the list so that it's always 2 elements, even if the split\n          # failed.\n          target, vals = (entry.split(\"=\", 1) + [\"\"])[:2]\n          if vals:\n            path_vals = vals.split(\":\")\n          else:\n            path_vals = []\n          self._ExpandPath(target, path_vals, paths)\n        elif entry not in self._SH_CONTINUATION:\n          # Stop processing the line unless the entry might allow paths to still\n          # be set, e.g.\n          #   reserved words: \"export\"\n          #   conditions: { PATH=VAL } && PATH=:$PATH || PATH=.\n          break\n    return paths",
        "rewrite": "Here is my revised version:\n\n```\ndef _ParseShVariables(self, lines):\n  \"\"\"Extract env_var and path values from sh derivative shells.\"\"\"\n  paths = {}\n  for line in lines:\n      for entry in map(str.strip if callable(getattr(type(line), 'strip', None)) else str,\n                         filter(None,\n                                map(str.rstrip if hasattr(line[0], 'rstrip') else str,\n                                     filter(None,\n                                            [word.replace('\\t', ' ') \n                                             .replace('\\r', ' ')\n                                             .replace('\\n', ' ')\n                                             .split()\n                                            ]\n                                           )\n                                           )\n                                          )\n                                       )\n                          ):\n"
    },
    {
        "original": "def to_abivars(self):\n        \"\"\"Returns a dictionary with the abinit variables\"\"\"\n        # These variables are always present.\n        out_vars = {\n            \"ionmov\" : self.abivars.ionmov,\n            \"optcell\": self.abivars.optcell,\n            \"ntime\"  : self.abivars.ntime,\n        }\n\n        # Atom relaxation.\n        if self.move_atoms:\n            out_vars.update({\n                \"tolmxf\": self.abivars.tolmxf,\n            })\n\n        if self.abivars.atoms_constraints:\n            # Add input variables for constrained relaxation.\n            raise NotImplementedError(\"\")\n            out_vars.update(self.abivars.atoms_constraints.to_abivars())\n\n        # Cell relaxation.\n        if self.move_cell:\n            out_vars.update({\n                \"dilatmx\"  : self.abivars.dilatmx,\n                \"ecutsm\"   : self.abivars.ecutsm,\n                \"strfact\"  : self.abivars.strfact,\n                \"strtarget\": self.abivars.strtarget,\n            })\n\n        return out_vars",
        "rewrite": "Here is the revised code:\n\n```\ndef to_abivars(self):\n    \"\"\"Returns a dictionary with the abinit variables\"\"\"\n    out_vars = {\n        \"ionmov\": self-abivars.ionmov,\n        \"optcell\":self-abivars.optcell,\n         ntime:  -self-abivar.sntime\n    }\n    \n    if hasattr(self, 'move_atoms'):\n       out_vars.update({\n           'tolmxf':self-abivarstolmxf\n       })\n    \n    if hasattr(self, 'abivarstats') and getattr(self, 'abivarstats').atoms_constraints:\n       from ab"
    },
    {
        "original": "def cloudtrail_policy(original, bucket_name, account_id, bucket_region):\n    \"\"\"add CloudTrail permissions to an S3 policy, preserving existing\"\"\"\n    ct_actions = [\n        {\n            'Action': 's3:GetBucketAcl',\n            'Effect': 'Allow',\n            'Principal': {'Service': 'cloudtrail.amazonaws.com'},\n            'Resource': generate_arn(\n                service='s3', resource=bucket_name, region=bucket_region),\n            'Sid': 'AWSCloudTrailAclCheck20150319',\n        },\n        {\n            'Action': 's3:PutObject',\n            'Condition': {\n                'StringEquals':\n                {'s3:x-amz-acl': 'bucket-owner-full-control'},\n            },\n            'Effect': 'Allow',\n            'Principal': {'Service': 'cloudtrail.amazonaws.com'},\n            'Resource': generate_arn(\n                service='s3', resource=bucket_name, region=bucket_region),\n            'Sid': 'AWSCloudTrailWrite20150319',\n        },\n    ]\n    # parse original policy\n    if original is None:\n        policy = {\n            'Statement': [],\n            'Version': '2012-10-17',\n        }\n    else:\n        policy = json.loads(original['Policy'])\n    original_actions = [a.get('Action') for a in policy['Statement']]\n    for cta in ct_actions:\n        if cta['Action'] not in original_actions:\n            policy['Statement'].append(cta)\n    return json.dumps(policy)",
        "rewrite": "Here is the revised code:\n\n```\nimport json\nimport boto3\n\ndef cloudtrail_policy(original, bucket_name, account_id, bucket_region):\n    ct_actions = [\n        {\n          \"Sid\": \"AWSCloudTrailAclCheck\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": [\"s3:GetBucketAcl\"],\n          \"Resource\": f\"arn:aws:s3:::{bucket_name}\"\n         },\n         {\n           \"Sid\": \"\",\n           \"Effect\": \"\",\n           \"Principal\": {\"Service\": \"\"},\n           \"Action\":\"s3:"
    },
    {
        "original": "def del_flowspec_local(flowspec_family, route_dist, rules):\n    \"\"\"Deletes/withdraws Flow Specification route from VRF identified\n    by *route_dist*.\n    \"\"\"\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, route_dist=route_dist,\n            rules=rules, is_withdraw=True)\n\n        # Send success response.\n        return [{FLOWSPEC_FAMILY: flowspec_family,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 FLOWSPEC_RULES: rules}]\n\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)",
        "rewrite": "Here is the revised code:\n\n```\ndef del_flowspec_local(flowspec_family, route_dist, rules):\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, \n            route_dist=route_dist, \n            rules=rules, \n            is_withdraw=True)\n        \n        return [{'family': flowspec_family,\n                 'route_distinguisher': route_dist,\n                 'rules': rules}]\n    \n    except BgpCoreError as e:\n        raise PrefixError(f\"Failed to delete flow spec {flows"
    },
    {
        "original": "def all(self, **kwargs):\n        \"\"\"List all the members, included inherited ones.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of members\n        \"\"\"\n\n        path = '%s/all' % self.path\n        obj = self.gitlab.http_list(path, **kwargs)\n        return [self._obj_cls(self, item) for item in obj]",
        "rewrite": "Here is my revised version:\n\n```\ndef all(self, **kwargs):\n    path = f\"{self.path}/all\"\n    obj = self.gitlab.http_list(path, **kwargs)\n    if 'per_page' in kwargs or 'page' in kwargs or 'as_list' in kwargs:\n        return [self._obj_cls(self, item) for item in obj]\n    else:\n        return [self._obj_cls(self, item) for item in obj]\n```"
    },
    {
        "original": "def swapConfigPanels(self, event):\r\n        \"\"\"Hide/show configuration panels based on the currently selected\r\n         option in the sidebar \"\"\"\r\n        for id, panel in enumerate(self.configPanels):\r\n            panel.Hide()\r\n        self.activeSelection = event.Selection\r\n        self.configPanels[event.Selection].Show()\r\n        self._parent.Layout()",
        "rewrite": "Here is the revised code:\n\n```\ndef swapConfigPanels(self, event):\n    for id, panel in enumerate(self.configPanels):\n        panel.Hide()\n    self.activeSelection = event.Selection\n    getattr(self, f\"configPanel{event.Selection}\").Show()\n    self._parent.Layout()\n```"
    },
    {
        "original": "def commit_config(self, message=\"\"):\n        \"\"\"Commit configuration.\"\"\"\n        commit_args = {\"comment\": message} if message else {}\n        self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n        if not self.lock_disable and not self.session_config_lock:\n            self._unlock()",
        "rewrite": "Here is the revised code:\n\n```\ndef commit_config(self, message=\"\"):\n    \"\"\"Commit configuration.\"\"\"\n    commit_args = {\"comment\": message} if message else {}\n    self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n    if not (self.lock_disable or self.session_config_lock):\n        self._unlock()\n```"
    },
    {
        "original": "def full_protocol_list(self):\n        \"\"\"\n        :return: all protocols including not shown ones\n        :rtype: list of ProtocolAnalyzer\n        \"\"\"\n        result = []\n        for group in self.groups:\n            result.extend(group.all_protocols)\n        return result",
        "rewrite": "Here is the revised code:\n\n```\ndef full_protocol_list(self):\n    return [protocol for group in self.groups for protocol in group.all_protocols]\n```"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a MetricTokenResponse object from a json dictionary.\"\"\"\n        args = {}\n        if 'aggregations' in _dict:\n            args['aggregations'] = [\n                MetricTokenAggregation._from_dict(x)\n                for x in (_dict.get('aggregations'))\n            ]\n        return cls(**args)",
        "rewrite": "Here is the revised code:\n\n```\ndef _from_dict(cls, _dict):\n    return cls(**{k: v for k, v in ((key.replace('_', ''), value) \n                                    if isinstance(value, dict) else (key, value) \n                                    for key, value in _dict.items())})\n```"
    },
    {
        "original": "def parse_node(self, node, node_path, package_project_config, tags=None,\n                   fqn_extra=None, fqn=None, agate_table=None,\n                   archive_config=None, column_name=None):\n        \"\"\"Parse a node, given an UnparsedNode and any other required information.\n\n        agate_table should be set if the node came from a seed file.\n        archive_config should be set if the node is an Archive node.\n        column_name should be set if the node is a Test node associated with a\n        particular column.\n        \"\"\"\n        logger.debug(\"Parsing {}\".format(node_path))\n\n        tags = coalesce(tags, [])\n        fqn_extra = coalesce(fqn_extra, [])\n\n        if fqn is None:\n            fqn = self.get_fqn(node, package_project_config, fqn_extra)\n\n        config = SourceConfig(\n            self.root_project_config,\n            package_project_config,\n            fqn,\n            node.resource_type)\n\n        parsed_dict = self._build_intermediate_node_dict(\n            config, node.serialize(), node_path, config, tags, fqn,\n            agate_table, archive_config, column_name\n        )\n        parsed_node = ParsedNode(**parsed_dict)\n\n        self._render_with_context(parsed_node, config)\n        self._update_parsed_node_info(parsed_node, config)\n\n        parsed_node.validate()\n\n        return parsed_node",
        "rewrite": "Here's my revised version of the function:\n\n```\ndef parse_node(self):\n    \"\"\"Parse a Node.\"\"\"\n    logger.debug(f\"Parsing {node_path}\")\n\n    tags = tags or []\n    fqn_extra = fqn_extra or []\n\n    if not isinstance(fqns := (fQN := (fQN := get_fQN(node=self.node)),), str):\n      raise ValueError(\"Invalid FQN\")\n\n    config = SourceConfig(\n      root_project=self.root_project.config(),\n      project=package_project.config(),\n      fqns=fqns[0],\n      resource_type=node.resource_type\n  )\n\n  intermediate_data: dict[str]"
    },
    {
        "original": "def _access_through_series(values, name):\n    \"\"\"Coerce an array of datetime-like values to a pandas Series and\n    access requested datetime component\n    \"\"\"\n    values_as_series = pd.Series(values.ravel())\n    if name == \"season\":\n        months = values_as_series.dt.month.values\n        field_values = _season_from_months(months)\n    else:\n        field_values = getattr(values_as_series.dt, name).values\n    return field_values.reshape(values.shape)",
        "rewrite": "```\nimport pandas as pd\n\ndef _access_through_series(values, name):\n    return getattr(pd.Series(values.ravel()).dt, name).values.reshape(values.shape)\n```"
    },
    {
        "original": "def tensor_kraus_maps(k1, k2):\n    \"\"\"\n    Generate the Kraus map corresponding to the composition\n    of two maps on different qubits.\n\n    :param list k1: The Kraus operators for the first qubit.\n    :param list k2: The Kraus operators for the second qubit.\n    :return: A list of tensored Kraus operators.\n    \"\"\"\n    return [np.kron(k1j, k2l) for k1j in k1 for k2l in k2]",
        "rewrite": "Here is a revised version of your function:\n\n```\nimport numpy as np\nfrom scipy.linalg import kron as np_kron\n\ndef tensor_kraus_maps(k1, k2):\n    return [np_kron(k1j, kl) for kj in zip(*[k.flatten() for k in [k1, *map(lambda x: x.flatten(), [i.tolist() if not isinstance(i, (list,tuple)) else i  for i in [k2]])]]))]\n```"
    },
    {
        "original": "def children_as_pi(self, squash=False):\n        \"\"\"Returns the child visit counts as a probability distribution, pi\n        If squash is true, exponentiate the probabilities by a temperature\n        slightly larger than unity to encourage diversity in early play and\n        hopefully to move away from 3-3s\n        \"\"\"\n        probs = self.child_N\n        if squash:\n            probs = probs ** .98\n        sum_probs = np.sum(probs)\n        if sum_probs == 0:\n            return probs\n        return probs / np.sum(probs)",
        "rewrite": "Here is the revised code:\n\n```\ndef children_as_pi(self, squash=False):\n    probs = self.child_N.copy()  # Use copy to avoid modifying original data.\n    if squash:\n        probs[:] = [x ** 0.98 for x in probs]  # Inplace modification.\n    sum_probs = sum(probs)\n    if sum_probs == 0:\n        return [p for p in probs]  # Return list instead of array.\n    return [p / sum_probs for p in probs]\n```"
    },
    {
        "original": "def shutdown(self, exitcode=0, exitmsg=None):\n        \"\"\"\n        If sub-classed, run any shutdown operations on this method.\n        \"\"\"\n        log.info('The salt-api is shutting down..')\n        msg = 'The salt-api is shutdown. '\n        if exitmsg is not None:\n            exitmsg = msg + exitmsg\n        else:\n            exitmsg = msg.strip()\n        super(SaltAPI, self).shutdown(exitcode, exitmsg)",
        "rewrite": "Here is the revised code:\n\n```\ndef shutdown(self, exitcode=0, exitmsg=None):\n    log.info('The salt-api is shutting down..')\n    if isinstance(exitmsg, str):\n        if not isinstance(exitcode, int) or (exitcode < 0 or exitcode > 255):\n            raise ValueError(\"Invalid `exitcode` value\")\n    super().shutdown(exitcode if isinstance(exitcode,int) else 0 , (exitmsg or ''))\n```"
    },
    {
        "original": "def load_source(name, pathname):\n    \"\"\"\n    This function provides the backward compatibility for 'imp.load_source'\n    in Python 2.\n\n    :param name: Name used to create or access a module object.\n    :param pathname: Path pointing to the source file.\n    :return: Loaded and initialized module.\n    \"\"\"\n    if six.PY2:\n        import imp\n        return imp.load_source(name, pathname)\n    else:\n        loader = importlib.machinery.SourceFileLoader(name, pathname)\n        return loader.load_module(name)",
        "rewrite": "Here is the revised code:\n\n```\nimport importlib.machinery\nimport six\n\ndef load_source(name, pathname):\n    if six.PY2:\n        import imp\n        return imp.load_source(name, pathname)\n    else:\n        return type(__name__, (), {})()\n```"
    },
    {
        "original": "def WindowsSdkVersion(self):\n        \"\"\"\n        Microsoft Windows SDK versions for specified MSVC++ version.\n        \"\"\"\n        if self.vc_ver <= 9.0:\n            return ('7.0', '6.1', '6.0a')\n        elif self.vc_ver == 10.0:\n            return ('7.1', '7.0a')\n        elif self.vc_ver == 11.0:\n            return ('8.0', '8.0a')\n        elif self.vc_ver == 12.0:\n            return ('8.1', '8.1a')\n        elif self.vc_ver >= 14.0:\n            return ('10.0', '8.1')",
        "rewrite": "Here is the revised code:\n\n```\ndef WindowsSdkVersion(self):\n    if self.vc_ver <= 9:\n        return ['7_0', '6_1', '6_00a']\n    elif self.vc_ver == 10:\n        return ['7_1', '7_00a']\n    elif self.vc_ver == 11:\n        return ['8_00']\n    elif self.vc_ver == 12:\n        return ['8_10']\n    else: \n       # vc-ver >=14\n       # https://docs.microsoft.com/en-us/cpp/cplusplussdk/microsoft-"
    },
    {
        "original": "def fit_bounds(self, bounds, padding_top_left=None,\n                   padding_bottom_right=None, padding=None, max_zoom=None):\n        \"\"\"Fit the map to contain a bounding box with the\n        maximum zoom level possible.\n\n        Parameters\n        ----------\n        bounds: list of (latitude, longitude) points\n            Bounding box specified as two points [southwest, northeast]\n        padding_top_left: (x, y) point, default None\n            Padding in the top left corner. Useful if some elements in\n            the corner, such as controls, might obscure objects you're zooming\n            to.\n        padding_bottom_right: (x, y) point, default None\n            Padding in the bottom right corner.\n        padding: (x, y) point, default None\n            Equivalent to setting both top left and bottom right padding to\n            the same value.\n        max_zoom: int, default None\n            Maximum zoom to be used.\n\n        Examples\n        --------\n        >>> m.fit_bounds([[52.193636, -2.221575], [52.636878, -1.139759]])\n\n        \"\"\"\n        self.add_child(FitBounds(bounds,\n                                 padding_top_left=padding_top_left,\n                                 padding_bottom_right=padding_bottom_right,\n                                 padding=padding,\n                                 max_zoom=max_zoom,\n                                 )\n                       )",
        "rewrite": "Here is my revised code:\n\n```\ndef fit_bounds(self):\n    def _format_point(points):\n      return [(float(p[0]), float(p[1])) for p in points]\n\n    def _get_center(coords_list):\n      x = sum([c[0] for c in coords_list]) / 2 * math.pi / 180 / 360 \n      y = sum([c[1] for c in coords_list]) / 2 * math.pi / 180 / 360 \n      return x`='$' + str(int(y))\n    valuesList = list()\n    maps[i].add_child227Protocol(_fit"
    },
    {
        "original": "def collect_static(static_folder):\n    \"\"\"\n        Copies flask-appbuilder static files to your projects static folder\n    \"\"\"\n    appbuilder_static_path = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"static/appbuilder\"\n    )\n    app_static_path = os.path.join(os.getcwd(), static_folder)\n    if not os.path.isdir(app_static_path):\n        click.echo(\n            click.style(\n                \"Static folder does not exist creating: %s\" % app_static_path,\n                fg=\"green\",\n            )\n        )\n        os.makedirs(app_static_path)\n    try:\n        shutil.copytree(\n            appbuilder_static_path, os.path.join(app_static_path, \"appbuilder\")\n        )\n    except Exception:\n        click.echo(\n            click.style(\n                \"Appbuilder static folder already exists on your project\", fg=\"red\"\n            )\n        )",
        "rewrite": "Here is the revised code:\n```\nimport shutil\nimport os\nfrom loguru import logger\n\ndef collect_static(static_folder):\n    appbuilder_STATIC_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"static/appbuilder\")\n    APP_STATIC_PATH = os.path.join(os.getcwd(), static_folder)\n\n    if not (oskdir.exists(APP_STATIC_PATH) or (oskdir.exists(APP_STATIC_PATH) and not isdir.empty)):\n       logger.info(f\"Static folder does not exist creating: {APP_STATIC_PATH}\")\n       makedirs(APP_STATIC_PATH)\n\ntry:\n   shutil.copytree(appbuilder STATIC PATH, f\"{APP"
    },
    {
        "original": "def __setAddressfilterMode(self, mode):\n        \"\"\"set address filter mode\n\n        Returns:\n            True: successful to set address filter mode.\n            False: fail to set address filter mode.\n        \"\"\"\n        print 'call setAddressFilterMode() ' +  mode\n        try:\n            if re.match('list', mode, re.M|re.I):\n                cmd = WPANCTL_CMD + 'setprop MAC:' + mode + ':Enabled 1'\n            elif mode == 'disabled':\n                cmd = WPANCTL_CMD + 'setprop MAC:' + mode + ':Enabled 0'\n            else:\n                print 'no such option'\n                return False\n            if self.__sendCommand(cmd)[0] != 'Fail':\n                return True\n            return False\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger('__setAddressFilterMode() Error: ' + str(e))",
        "rewrite": "Here is the revised code:\n\n```\ndef __setAddressfilterMode(self, mode):\n    \"\"\"Set address filter mode.\n\n    Args:\n        self (object): The object instance.\n        mode (str): The new address filter mode.\n\n    Returns:\n        bool: True if successful to set address filter mode, False otherwise.\n    \"\"\"\n    print(f'Call setAddressFilterMode() {mode}')\n    \n    try:\n        if re.match('list', str(mode), re.IGNORECASE):\n            cmd = f'{WPANCTL_CMD} setprop MAC:{mode}:Enabled 1'\n        elif str(mode) == 'disabled':\n           "
    },
    {
        "original": "def zeroed(self, tol=1e-3):\n        \"\"\"\n        returns the matrix with all entries below a certain threshold\n        (i.e. tol) set to zero\n        \"\"\"\n        new_tensor = self.copy()\n        new_tensor[abs(new_tensor) < tol] = 0\n        return new_tensor",
        "rewrite": "Here is the revised code:\n\n```\ndef zeroed(self, tol=1e-3):\n    return self.copy().where(abs(self) < tol)\n```"
    },
    {
        "original": "def self_build(self, field_pos_list=None):\n        \"\"\"\n        Create the default layer regarding fields_desc dict\n\n        :param field_pos_list:\n        \"\"\"\n        if self.raw_packet_cache is not None:\n            for fname, fval in six.iteritems(self.raw_packet_cache_fields):\n                if self.getfieldval(fname) != fval:\n                    self.raw_packet_cache = None\n                    self.raw_packet_cache_fields = None\n                    self.wirelen = None\n                    break\n            if self.raw_packet_cache is not None:\n                return self.raw_packet_cache\n        p = b\"\"\n        for f in self.fields_desc:\n            val = self.getfieldval(f.name)\n            if isinstance(val, RawVal):\n                sval = raw(val)\n                p += sval\n                if field_pos_list is not None:\n                    field_pos_list.append((f.name, sval.encode(\"string_escape\"), len(p), len(sval)))  # noqa: E501\n            else:\n                p = f.addfield(self, p, val)\n        return p",
        "rewrite": "Here's a revised version of the function:\n\n```\ndef _self_build(self, field_pos_list=None):\n    raw_packet_cache_fields_copy = {}\n    raw_packet_cache_copy = {}\n    \n    if hasattr(self, 'raw_packet_cache') and hasattr(self, 'raw_package_fields'):\n        \n        for fname, fvalue in six.iteritems(getattr(self,'raw_package_fields')):\n            \n            if getattr(self,'getfieldvalue')(fname) != fvalue or (fname not in raw_package_copy and fname not in raw_package_copy.keys()):\n                \n                setattr(self,'raw_package',None)\n                \n                setattr(self,'raw_package_fields',None)\n"
    },
    {
        "original": "def create_job(name=None,\n               config_xml=None,\n               saltenv='base'):\n    \"\"\"\n    Return the configuration file.\n\n    :param name: The name of the job is check if it exists.\n    :param config_xml: The configuration file to use to create the job.\n    :param saltenv: The environment to look for the file in.\n    :return: The configuration file used for the job.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jenkins.create_job jobname\n\n        salt '*' jenkins.create_job jobname config_xml='salt://jenkins/config.xml'\n\n    \"\"\"\n    if not name:\n        raise SaltInvocationError('Required parameter \\'name\\' is missing')\n\n    if job_exists(name):\n        raise CommandExecutionError('Job \\'{0}\\' already exists'.format(name))\n\n    if not config_xml:\n        config_xml = jenkins.EMPTY_CONFIG_XML\n    else:\n        config_xml_file = _retrieve_config_xml(config_xml, saltenv)\n\n        with salt.utils.files.fopen(config_xml_file) as _fp:\n            config_xml = salt.utils.stringutils.to_unicode(_fp.read())\n\n    server = _connect()\n    try:\n        server.create_job(name, config_xml)\n    except jenkins.JenkinsException as err:\n        raise CommandExecutionError(\n            'Encountered error creating job \\'{0}\\': {1}'.format(name, err)\n        )\n    return config_xml",
        "rewrite": "Here is a revised version of the given Python function:\n\n```Python\nimport os\nimport salt.utils.files\nfrom jenkins import JenkinsException\nfrom salt.exceptions import SaltInvocationError, CommandExecutionError\n\ndef create_job(name=None,\n               configXml=None,\n               saltenv='base'):\n    \n  if not name:\n      raise SaltInvocationError('Required parameter \\'name\\' is missing')\n\n  if os.path.exists(os.path.join(jobs_path(), name)):\n      raise CommandExecutionError(f'Job \\'{name}\\' already exists')\n\n  if not configXml:\n      default_configXml = 'salt://jenkins/config.xml'\n"
    },
    {
        "original": "def _GenerateInitConfigs(self, template_dir, rpm_build_dir):\n    \"\"\"Generates init-system configs.\"\"\"\n    client_name = config.CONFIG.Get(\"Client.name\", context=self.context)\n    initd_target_filename = os.path.join(rpm_build_dir, \"etc/init.d\",\n                                         client_name)\n\n    # Generate init.d\n    utils.EnsureDirExists(os.path.dirname(initd_target_filename))\n    self.GenerateFile(\n        os.path.join(template_dir, \"rpmbuild/grr-client.initd.in\"),\n        initd_target_filename)\n\n    # Generate systemd unit\n    if config.CONFIG[\"Template.version_numeric\"] >= 3125:\n      systemd_target_filename = os.path.join(rpm_build_dir,\n                                             \"usr/lib/systemd/system/\",\n                                             \"%s.service\" % client_name)\n\n      utils.EnsureDirExists(os.path.dirname(systemd_target_filename))\n      self.GenerateFile(\n          os.path.join(template_dir, \"rpmbuild/grr-client.service.in\"),\n          systemd_target_filename)",
        "rewrite": "Here is the revised code:\n```\nimport os\nfrom grr.server.config import config\nfrom grr.server.utils import EnsureDirExists\n\nclass GRRClass:\n  def _GenerateInitConfigs(self, template_dir, rpm_build_dir):\n    client_name = config.CONFIG.Get(\"Client.name\", context=self.context)\n    \n    initd_target_filename = os.path.join(rpm_build_dir, 'etc/init.d', client_name)\n    \n    EnsureDirExists(os.path.dirname(initd_target_filename))\n    \n    self.GenerateFile(os.path.join(template_dir, 'rpmbuild/grr-client.initd.in'), \n                      initd_target_filename"
    },
    {
        "original": "def _column_type(strings, has_invisible=True):\n    \"\"\"The least generic type all column values are convertible to.\n\n    >>> _column_type([\"1\", \"2\"]) is _int_type\n    True\n    >>> _column_type([\"1\", \"2.3\"]) is _float_type\n    True\n    >>> _column_type([\"1\", \"2.3\", \"four\"]) is _text_type\n    True\n    >>> _column_type([\"four\", '\\u043f\\u044f\\u0442\\u044c']) is _text_type\n    True\n    >>> _column_type([None, \"brux\"]) is _text_type\n    True\n    >>> _column_type([1, 2, None]) is _int_type\n    True\n    >>> import datetime as dt\n    >>> _column_type([dt.datetime(1991,2,19), dt.time(17,35)]) is _text_type\n    True\n\n    \"\"\"\n    types = [_type(s, has_invisible) for s in strings ]\n    return reduce(_more_generic, types, int)",
        "rewrite": "Here's the revised code:\n\n```\nimport functools as ft\n\n_int = type(int())\n_float = type(float())\n_text = type(str())\n\ndef more_generic(t1, t2):\n  if t1.__name__ == 'int' and t2.__name__ in ['float', 'str']:\n      return t2 \n  elif (t1.__name__ == 'float' and \n         t2.__name__ in ['str']):\n      return t2 \n  else:\n      return t1 \n\n_column_types = [int(), float(), str()]\n\ndef column_types(*strings):\n  types = [type(s) for s"
    },
    {
        "original": "def residual_resample(weights):\n    \"\"\" Performs the residual resampling algorithm used by particle filters.\n\n    Based on observation that we don't need to use random numbers to select\n    most of the weights. Take int(N*w^i) samples of each particle i, and then\n    resample any remaining using a standard resampling algorithm [1]\n\n\n    Parameters\n    ----------\n\n    weights : list-like of float\n        list of weights as floats\n\n    Returns\n    -------\n\n    indexes : ndarray of ints\n        array of indexes into the weights defining the resample. i.e. the\n        index of the zeroth resample is indexes[0], etc.\n\n    References\n    ----------\n\n    .. [1] J. S. Liu and R. Chen. Sequential Monte Carlo methods for dynamic\n       systems. Journal of the American Statistical Association,\n       93(443):1032\u20131044, 1998.\n    \"\"\"\n\n    N = len(weights)\n    indexes = np.zeros(N, 'i')\n\n    # take int(N*w) copies of each weight, which ensures particles with the\n    # same weight are drawn uniformly\n    num_copies = (np.floor(N*np.asarray(weights))).astype(int)\n    k = 0\n    for i in range(N):\n        for _ in range(num_copies[i]): # make n copies\n            indexes[k] = i\n            k += 1\n\n    # use multinormal resample on the residual to fill up the rest. This\n    # maximizes the variance of the samples\n    residual = weights - num_copies     # get fractional part\n    residual /= sum(residual)           # normalize\n    cumulative_sum = np.cumsum(residual)\n    cumulative_sum[-1] = 1. # avoid round-off errors: ensures sum is exactly one\n    indexes[k:N] = np.searchsorted(cumulative_sum, random(N-k))\n\n    return indexes",
        "rewrite": "Here is a revised version:\n\n```Python\r\nimport numpy as np\r\n\r\ndef residual_resample(weights):\r\n    \r\n# Perform Residual Resampling Algorithm\r\nN = len(weights)\r\nindexes=np.zeros((N,), dtype=int)\r\n\r\n# Take int(N*w) copies each\r\nnum_copies=(np.floor(np.array([N])*weights)).astype(int)\r\nk=0\r\nfor i in range(N):\r\n\tfor _ in range(num_copies[i]):\r\n\t\tindexes[k]=i\r\n\t\tk+=1\r\n\r\n# Use Multinomial Resampling on residuals to fill up rest.\r\nresidual=weights-num_copies   #\r\nresidual/=sum(residual"
    },
    {
        "original": "def _get_convergence_plans(project, service_names):\n    \"\"\"\n    Get action executed for each container\n\n    :param project:\n    :param service_names:\n    :return:\n    \"\"\"\n    ret = {}\n    plans = project._get_convergence_plans(project.get_services(service_names),\n                                           ConvergenceStrategy.changed)\n    for cont in plans:\n        (action, container) = plans[cont]\n        if action == 'create':\n            ret[cont] = 'Creating container'\n        elif action == 'recreate':\n            ret[cont] = 'Re-creating container'\n        elif action == 'start':\n            ret[cont] = 'Starting container'\n        elif action == 'noop':\n            ret[cont] = 'Container is up to date'\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef _get_convergence_plans(project, service_names):\n    return {f\"{service_name}\": plan_action for service_name, plan_action in zip(service_names, project._get_convergence_plans(project.get_services(service_names), ConvergenceStrategy.changed).values())}\n```"
    },
    {
        "original": "def nonstoichiometric_symmetrized_slab(self, init_slab, tol=1e-3):\n\n        \"\"\"\n        This method checks whether or not the two surfaces of the slab are\n        equivalent. If the point group of the slab has an inversion symmetry (\n        ie. belong to one of the Laue groups), then it is assumed that the\n        surfaces should be equivalent. Otherwise, sites at the bottom of the\n        slab will be removed until the slab is symmetric. Note the removal of sites\n        can destroy the stoichiometry of the slab. For non-elemental\n        structures, the chemical potential will be needed to calculate surface energy.\n\n        Arg:\n            init_slab (Structure): A single slab structure\n            tol (float): Tolerance for SpaceGroupanalyzer.\n\n        Returns:\n            Slab (structure): A symmetrized Slab object.\n        \"\"\"\n\n        sg = SpacegroupAnalyzer(init_slab, symprec=tol)\n\n        if sg.is_laue():\n            return [init_slab]\n\n        nonstoich_slabs = []\n        # Build an equivalent surface slab for each of the different surfaces\n        for top in [True, False]:\n            asym = True\n            slab = init_slab.copy()\n            slab.energy = init_slab.energy\n\n            while asym:\n                # Keep removing sites from the bottom one by one until both\n                # surfaces are symmetric or the number of sites removed has\n                # exceeded 10 percent of the original slab\n\n                c_dir = [site[2] for i, site in enumerate(slab.frac_coords)]\n\n                if top:\n                    slab.remove_sites([c_dir.index(max(c_dir))])\n                else:\n                    slab.remove_sites([c_dir.index(min(c_dir))])\n                if len(slab) <= len(self.parent):\n                    break\n\n                # Check if the altered surface is symmetric\n                sg = SpacegroupAnalyzer(slab, symprec=tol)\n                if sg.is_laue():\n                    asym = False\n                    nonstoich_slabs.append(slab)\n\n        if len(slab) <= len(self.parent):\n            warnings.warn(\"Too many sites removed, please use a larger slab \"\n                          \"size.\")\n\n        return nonstoich_slabs",
        "rewrite": "Here's a revised version with improved readability and maintainability:\n\n```Python\ndef nonstoichiometric_symmetrized_slab(self, init_slab: Structure,\n                                       tol: float = 1e-3) -> List[Structure]:\n    \"\"\"\n    This method checks whether or not two surfaces of a given initial structure are equivalent.\n    If they are not equivalent and have no inversion symmetry (i.e., do not belong to any Laue group),\n    it removes sites from one surface until they become equivalent.\n    \n    Args:\n    - initSlab (Structure): The initial structure.\n    - tol (float): T"
    },
    {
        "original": "def read_short(self, base, offset=0):\n        \"\"\"\n        Return the int value of the two bytes at the file position determined\n        by *base* and *offset*, similarly to ``read_long()`` above.\n        \"\"\"\n        fmt = b'<H' if self._byte_order is LITTLE_ENDIAN else b'>H'\n        return self._read_int(fmt, base, offset)",
        "rewrite": "Here is the revised code:\n\n```\ndef read_short(self, base, offset=0):\n    fmt = '<h' if self._byte_order == 'little' else '>h'\n    return self._read_int(fmt.encode(), base + 1)\n```"
    },
    {
        "original": "def uptime():\n    \"\"\"\n    Return the uptime for this system.\n\n    .. versionchanged:: 2015.8.9\n        The uptime function was changed to return a dictionary of easy-to-read\n        key/value pairs containing uptime information, instead of the output\n        from a ``cmd.run`` call.\n\n    .. versionchanged:: 2016.11.0\n        Support for OpenBSD, FreeBSD, NetBSD, MacOS, and Solaris\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.uptime\n    \"\"\"\n    curr_seconds = time.time()\n\n    # Get uptime in seconds\n    if salt.utils.platform.is_linux():\n        ut_path = \"/proc/uptime\"\n        if not os.path.exists(ut_path):\n            raise CommandExecutionError(\"File {ut_path} was not found.\".format(ut_path=ut_path))\n        with salt.utils.files.fopen(ut_path) as rfh:\n            seconds = int(float(rfh.read().split()[0]))\n    elif salt.utils.platform.is_sunos():\n        # note: some flavors/versions report the host uptime inside a zone\n        #       https://support.oracle.com/epmos/faces/BugDisplay?id=15611584\n        res = __salt__['cmd.run_all']('kstat -p unix:0:system_misc:boot_time')\n        if res['retcode'] > 0:\n            raise CommandExecutionError('The boot_time kstat was not found.')\n        seconds = int(curr_seconds - int(res['stdout'].split()[-1]))\n    elif salt.utils.platform.is_openbsd() or salt.utils.platform.is_netbsd():\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        seconds = int(curr_seconds - int(bt_data))\n    elif salt.utils.platform.is_freebsd() or salt.utils.platform.is_darwin():\n        # format: { sec = 1477761334, usec = 664698 } Sat Oct 29 17:15:34 2016\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        data = bt_data.split(\"{\")[-1].split(\"}\")[0].strip().replace(' ', '')\n        uptime = dict([(k, int(v,)) for k, v in [p.strip().split('=') for p in data.split(',')]])\n        seconds = int(curr_seconds - uptime['sec'])\n    elif salt.utils.platform.is_aix():\n        seconds = _get_boot_time_aix()\n    else:\n        return __salt__['cmd.run']('uptime')\n\n    # Setup datetime and timedelta objects\n    boot_time = datetime.datetime.utcfromtimestamp(curr_seconds - seconds)\n    curr_time = datetime.datetime.utcfromtimestamp(curr_seconds)\n    up_time = curr_time - boot_time\n\n    # Construct return information\n    ut_ret = {\n        'seconds': seconds,\n        'since_iso': boot_time.isoformat(),\n        'since_t': int(curr_seconds - seconds),\n        'days': up_time.days,\n        'time': '{0}:{1}'.format(up_time.seconds // 3600, up_time.seconds % 3600 // 60),\n    }\n\n    if salt.utils.path.which('who'):\n        who_cmd = 'who' if salt.utils.platform.is_openbsd() else 'who -s'  # OpenBSD does not support -s\n        ut_ret['users'] = len(__salt__['cmd.run'](who_cmd).split(os.linesep))\n\n    return ut_ret",
        "rewrite": "Here is the revised code:\n\n```python\nimport time \nimport os \nimport datetime \nfrom functools import lru_cache \n\n@lru_cache(maxsize=None) \ndef _get_boot_time_aix(): \n   try:  \n      from subprocess import Popen  \n      p=Popen(['oslevel', '-r'])  \n      out=p.communicate()[0]  \n      return float(out) / (24 *60 *60 *60)   \n   except Exception as e :   \n      print(f\"Failed to get AIX Boot Time : {str(e)}\")   \n   return None  \n\ndef get_uptime():    \n   curr"
    },
    {
        "original": "def new_cast_status(self, status):\n        \"\"\" Called when a new status received from the Chromecast. \"\"\"\n        self.status = status\n        if status:\n            self.status_event.set()",
        "rewrite": "Here is the revised code:\n\n```\ndef new_cast_status(self, status: str) -> None:\n    self.status = status\n    if status:\n        self.status_event.set()\n```"
    },
    {
        "original": "def pydot__tree_to_png(tree, filename, rankdir=\"LR\"):\n    \"\"\"Creates a colorful image that represents the tree (data+children, without meta)\n\n    Possible values for `rankdir` are \"TB\", \"LR\", \"BT\", \"RL\", corresponding to\n    directed graphs drawn from top to bottom, from left to right, from bottom to\n    top, and from right to left, respectively. See:\n    https://www.graphviz.org/doc/info/attrs.html#k:rankdir\n    \"\"\"\n\n    import pydot\n    graph = pydot.Dot(graph_type='digraph', rankdir=rankdir)\n\n    i = [0]\n\n    def new_leaf(leaf):\n        node = pydot.Node(i[0], label=repr(leaf))\n        i[0] += 1\n        graph.add_node(node)\n        return node\n\n    def _to_pydot(subtree):\n        color = hash(subtree.data) & 0xffffff\n        color |= 0x808080\n\n        subnodes = [_to_pydot(child) if isinstance(child, Tree) else new_leaf(child)\n                    for child in subtree.children]\n        node = pydot.Node(i[0], style=\"filled\", fillcolor=\"#%x\"%color, label=subtree.data)\n        i[0] += 1\n        graph.add_node(node)\n\n        for subnode in subnodes:\n            graph.add_edge(pydot.Edge(node, subnode))\n\n        return node\n\n    _to_pydot(tree)\n    graph.write_png(filename)",
        "rewrite": "Here is the revised code:\n\n```python\nimport pydot\n\ndef py_dot_tree_to_png(tree: object, filename: str):\n    \"\"\"Creates a colorful image that represents the tree (data+children).\"\"\"\n    \n    if not isinstance(filename.endswith('.png'), str):\n         raise ValueError(\"Invalid filename\")\n\n    \n     rankdirs = ['TB', 'LR', 'BT', 'RL']\n     if rankdir not in rankdirs:\n         raise ValueError(\"Invalid value for `rankdir`\")\n\n     graph = pydot.Dot(graph_type='digraph', rankdir=rankdir)\n\n     nodes_created = set()\n\n     def new_leaf"
    },
    {
        "original": "def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv",
        "rewrite": "Here's a revised version of your function:\n\n```Python\ndef pin_auth(self, request):\n    \"\"\"Authenticates with the pin.\"\"\"\n    response_data = {'auth': False, 'exhausted': False}\n    trust_value = self.check_pin_trust(request.environ)\n\n    if not trust_value: \n       response_data['bad-cookie'] = True \n       return Response(json.dumps(response_data), mimetype=\"application/json\")\n\n    if not isinstance(trust_value, bool) and isinstance(trust_value, int) and trust_value > 0: \n       response_data['auth'] |= True \n       return Response(json.dumps(response_data"
    },
    {
        "original": "def _find_classes_param(self):\n        \"\"\"\n        Searches the wrapped model for the classes_ parameter.\n        \"\"\"\n        for attr in [\"classes_\"]:\n            try:\n                return getattr(self.estimator, attr)\n            except AttributeError:\n                continue\n\n        raise YellowbrickTypeError(\n            \"could not find classes_ param on {}\".format(\n                self.estimator.__class__.__name__\n            )\n        )",
        "rewrite": "```\ndef _find_classes_param(self):\n    for attr in [\"classes_\", \"_classes\", \"_Classes\"]:\n        try:\n            return getattr(self.estimator, attr)\n        except AttributeError:\n            pass\n    raise YellowbrickTypeError(f\"could not find {attr} param on {self.estimator.__class__.__name__}\")\n```"
    },
    {
        "original": "def rate_limit(function):\n        \"\"\"Return a decorator that enforces API request limit guidelines.\n\n        We are allowed to make a API request every api_request_delay seconds as\n        specified in praw.ini. This value may differ from reddit to reddit. For\n        reddit.com it is 2. Any function decorated with this will be forced to\n        delay _rate_delay seconds from the calling of the last function\n        decorated with this before executing.\n\n        This decorator must be applied to a RateLimitHandler class method or\n        instance method as it assumes `rl_lock` and `last_call` are available.\n\n        \"\"\"\n        @wraps(function)\n        def wrapped(cls, _rate_domain, _rate_delay, **kwargs):\n            cls.rl_lock.acquire()\n            lock_last = cls.last_call.setdefault(_rate_domain, [Lock(), 0])\n            with lock_last[0]:  # Obtain the domain specific lock\n                cls.rl_lock.release()\n                # Sleep if necessary, then perform the request\n                now = timer()\n                delay = lock_last[1] + _rate_delay - now\n                if delay > 0:\n                    now += delay\n                    time.sleep(delay)\n                lock_last[1] = now\n                return function(cls, **kwargs)\n        return wrapped",
        "rewrite": "Here is the revised code:\n\n```\nimport functools\n\ndef rate_limit(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        import time; import threading; import threading.locks as locks\n        \n        class Lock(locks.Lock): pass\n        \nclass RateLimitHandler:\n    rl_lock = Lock()\n    last_call = {}\n    \n    def __init__(self):\n         self.rl_lock = Lock(); self.last_call={}\n\n@functools.singledispatchwrapper()(RateLimitHandler) \ndef rate_limit(func): \n     @functools.wraps(func) \n     def wrapper"
    },
    {
        "original": "def _simplify(self):\n        \"\"\"\n        RegionSimplifier performs the following simplifications:\n        - Remove redundant Gotos\n        - Remove redundant If/If-else statements\n        \"\"\"\n\n        r = self.region\n        r = self._simplify_gotos(r)\n        r = self._simplify_ifs(r)\n\n        self.result = r",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef simplify(self):\n    \"\"\"\n    Simplify the region by removing redundant Gotos and If/If-else statements.\n    \"\"\"\n    \n    region = self.region\n    \n    # Remove redundant Gotos\n    region = self._simplify_gotos(region)\n    \n    # Remove redundant If/If-else statements\n    region = self._simplify_ifs(region)\n    \n    # Set the result as a property of this object\n    self.result = region\n```\n\nI made several changes to improve the readability and maintainability of your original code:\n\n1. Renamed `self"
    },
    {
        "original": "def set_garbage_collector(self, exts=None, policy=\"task\"):\n        \"\"\"\n        Enable the garbage collector that will remove the big output files that are not needed.\n\n        Args:\n            exts: string or list with the Abinit file extensions to be removed. A default is\n                provided if exts is None\n            policy: Either `flow` or `task`. If policy is set to 'task', we remove the output\n                files as soon as the task reaches S_OK. If 'flow', the files are removed\n                only when the flow is finalized. This option should be used when we are dealing\n                with a dynamic flow with callbacks generating other tasks since a :class:`Task`\n                might not be aware of its children when it reached S_OK.\n        \"\"\"\n        assert policy in (\"task\", \"flow\")\n        exts = list_strings(exts) if exts is not None else (\"WFK\", \"SUS\", \"SCR\", \"BSR\", \"BSC\")\n\n        gc = GarbageCollector(exts=set(exts), policy=policy)\n\n        self.set_gc(gc)\n        for work in self:\n            #work.set_gc(gc) # TODO Add support for Works and flow policy\n            for task in work:\n                task.set_gc(gc)",
        "rewrite": "Here's a revised version of your function:\n\n```Python\ndef set_garbage_collector(self, exts=None, policy=\"task\"):\n    assert policy in (\"task\", \"flow\")\n    if exts is None:\n        default_ext = [\"WFK\", \"SUS\", \"SCR\", \"BSR\", \"BSC\"]\n    elif isinstance(exts, str):\n        default_ext = [ext.strip() for ext in exts.split(\",\")]\n    else:\n        default_ext = list(map(str, map(str.strip, map(str.strip, map(str.lstrip(r'\\''), str(ext).split(','))))))\n    \n"
    },
    {
        "original": "def while_not_sync_standby(self, func):\n        \"\"\"Runs specified action while trying to make sure that the node is not assigned synchronous standby status.\n\n        Tags us as not allowed to be a sync standby as we are going to go away, if we currently are wait for\n        leader to notice and pick an alternative one or if the leader changes or goes away we are also free.\n\n        If the connection to DCS fails we run the action anyway, as this is only a hint.\n\n        There is a small race window where this function runs between a master picking us the sync standby and\n        publishing it to the DCS. As the window is rather tiny consequences are holding up commits for one cycle\n        period we don't worry about it here.\"\"\"\n\n        if not self.is_synchronous_mode() or self.patroni.nosync:\n            return func()\n\n        with self._member_state_lock:\n            self._disable_sync += 1\n        try:\n            if self.touch_member():\n                # Master should notice the updated value during the next cycle. We will wait double that, if master\n                # hasn't noticed the value by then not disabling sync replication is not likely to matter.\n                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):\n                    try:\n                        if not self.is_sync_standby(self.dcs.get_cluster()):\n                            break\n                    except DCSError:\n                        logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\n                        break\n                    logger.info(\"Waiting for master to release us from synchronous standby\")\n            else:\n                logger.warning(\"Updating member state failed, skipping synchronous standby disable\")\n\n            return func()\n        finally:\n            with self._member_state_lock:\n                self._disable_sync -= 1",
        "rewrite": "Here's my revised version of your function:\n\n```Python\nimport time\n\nclass MyClass:\n\n    def __init__(self):\n        self._member_state_lock = threading.Lock()\n        \n    def _is_synchronous_mode(self):\n         # implement your logic here\n        \n    def _touch_member(self):\n         # implement your logic here\n        \n    def _is_sync_standby(self, cluster_state):\n         # implement your logic here\n        \n    def while_not_sync_standby(self, func):\n         start_time = time.time()\n         try: \n             with self._member_state_lock: \n                 while True: \n                     if (not (self.is"
    },
    {
        "original": "def initialize_unordered_bulk_op(self, bypass_document_validation=False):\n        \"\"\"**DEPRECATED** - Initialize an unordered batch of write operations.\n\n        Operations will be performed on the server in arbitrary order,\n        possibly in parallel. All operations will be attempted.\n\n        :Parameters:\n          - `bypass_document_validation`: (optional) If ``True``, allows the\n            write to opt-out of document level validation. Default is\n            ``False``.\n\n        Returns a :class:`~pymongo.bulk.BulkOperationBuilder` instance.\n\n        See :ref:`unordered_bulk` for examples.\n\n        .. note:: `bypass_document_validation` requires server version\n          **>= 3.2**\n\n        .. versionchanged:: 3.5\n           Deprecated. Use :meth:`~pymongo.collection.Collection.bulk_write`\n           instead.\n\n        .. versionchanged:: 3.2\n           Added bypass_document_validation support\n\n        .. versionadded:: 2.7\n        \"\"\"\n        warnings.warn(\"initialize_unordered_bulk_op is deprecated\",\n                      DeprecationWarning, stacklevel=2)\n        return BulkOperationBuilder(self, False, bypass_document_validation)",
        "rewrite": "Here is the revised code:\n\n```\ndef initialize_unordered_bulk_op(self, bypass_document_validation=False):\n    warnings.warn(\"initialize_unordered_bulk_op is deprecated\", DeprecationWarning, stacklevel=2)\n    from pymongo.bson import ObjectId\n    from pymongo.collection import BulkWriteError as _BulkWriteError\n    from pymongo.collection import BulkWriteResult as _BulkWriteResult\n    \n    class BulkOperationBuilder:\n      def __init__(self, collection=None, ordered=False, bypass_doc_val_check=False):\n          self._collection = collection\n        \n      def insert_one(self):\n          # implementation\n        \n      def update_one():\n          #"
    },
    {
        "original": "def get_functions_auth_string(self, target_subscription_id):\n        \"\"\"\n        Build auth json string for deploying\n        Azure Functions.  Look for dedicated\n        Functions environment variables or\n        fall back to normal Service Principal\n        variables.\n\n        \"\"\"\n\n        self._initialize_session()\n\n        function_auth_variables = [\n            constants.ENV_FUNCTION_TENANT_ID,\n            constants.ENV_FUNCTION_CLIENT_ID,\n            constants.ENV_FUNCTION_CLIENT_SECRET\n        ]\n\n        # Use dedicated function env vars if available\n        if all(k in os.environ for k in function_auth_variables):\n            auth = {\n                'credentials':\n                    {\n                        'client_id': os.environ[constants.ENV_FUNCTION_CLIENT_ID],\n                        'secret': os.environ[constants.ENV_FUNCTION_CLIENT_SECRET],\n                        'tenant': os.environ[constants.ENV_FUNCTION_TENANT_ID]\n                    },\n                'subscription': target_subscription_id\n            }\n\n        elif type(self.credentials) is ServicePrincipalCredentials:\n            auth = {\n                'credentials':\n                    {\n                        'client_id': os.environ[constants.ENV_CLIENT_ID],\n                        'secret': os.environ[constants.ENV_CLIENT_SECRET],\n                        'tenant': os.environ[constants.ENV_TENANT_ID]\n                    },\n                'subscription': target_subscription_id\n            }\n\n        else:\n            raise NotImplementedError(\n                \"Service Principal credentials are the only \"\n                \"supported auth mechanism for deploying functions.\")\n\n        return json.dumps(auth, indent=2)",
        "rewrite": "Here is a revised version of the given Python code:\n\n```python\ndef get_functions_auth_string(self, target_subscription_id):\n    self._initialize_session()\n    function_env_vars = [f\"{k}\" for k in ( \n                           constants.FUNCTION_TENANT_ID \n                           , constants.FUNCTION_CLIENT_ID \n                           , constants.FUNCTION_CLIENT_SECRET)]\n    service_principal_env_vars = [f\"{k}\" for k in ( \n                                 constants.CLIENT_ID \n                                 , constants.CLIENT_SECRET \n                                 , constants.TENANT_ID)]\n\n    if all(k in os.environ for k in function_env_vars):\n       return json.dumps({\n           \"credentials"
    },
    {
        "original": "def to_0d_array(value: Any) -> np.ndarray:\n    \"\"\"Given a value, wrap it in a 0-D numpy.ndarray.\n    \"\"\"\n    if np.isscalar(value) or (isinstance(value, np.ndarray) and\n                              value.ndim == 0):\n        return np.array(value)\n    else:\n        return to_0d_object_array(value)",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nfrom collections import deque\n\ndef to_0d_array(value: Any) -> np.ndarray:\n    if isinstance(value, (int, float)):\n        return np.array([value])\n    elif isinstance(value, str):\n        return [value]\n    elif hasattr(value, '__iter__'):\n        try:\n            value = list(map(to_0d_array, value))\n            while True:\n                try:\n                    for i in range(len(list(deque(iterable=value)))):\n                        yield None\n                except TypeError:\n                    pass\n                break\n            if len(set(map(id,"
    },
    {
        "original": "def marketPrice(self) -> float:\n        \"\"\"\n        Return the first available one of\n\n        * last price if within current bid/ask;\n        * average of bid and ask (midpoint);\n        * close price.\n        \"\"\"\n        price = self.last if (\n            self.hasBidAsk() and self.bid <= self.last <= self.ask) else \\\n            self.midpoint()\n        if isNan(price):\n            price = self.close\n        return price",
        "rewrite": "Here is the revised code:\n\n```\ndef marketPrice(self) -> float:\n    return (self.last if (self.hasBidAsk() and self.bid <= self.last <= self.ask) else \n             (self.bid + self.ask)/2) or 0\n```"
    },
    {
        "original": "def chunk_from_mem(self, ptr):\n        \"\"\"\n        Given a pointer to a user payload, return the base of the chunk associated with that payload (i.e. the chunk\n        pointer). Returns None if ptr is null.\n\n        :param ptr: a pointer to the base of a user payload in the heap\n        :returns: a pointer to the base of the associated heap chunk, or None if ptr is null\n        \"\"\"\n        if self.state.solver.symbolic(ptr):\n            try:\n                ptr = self.state.solver.eval_one(ptr)\n            except SimSolverError:\n                l.warning(\"A pointer to a chunk is symbolic; maximizing it\")\n                ptr = self.state.solver.max_int(ptr)\n        else:\n            ptr = self.state.solver.eval(ptr)\n        return PTChunk(ptr - (2 * self._chunk_size_t_size), self.state) if ptr != 0 else None",
        "rewrite": "Here is my revised version:\n\n```\ndef chunk_from_mem(self, ptr):\n    if isinstance(ptr, Expression):\n        try:\n            return PTChunk(self.state.solver.eval_one(expr=ptr) - (2 * self._chunk_size_t_size), state=self.state) if expr != 0 else None\n        except SolverError as e:\n            logging.warning(f\"A pointer to a chunk is symbolic; maximizing it: {e}\")\n    elif not isinstance(ptr, int):\n         l.warning(\"ptr must be an integer.\")\n         return None\n    elif not all(isinstance(x,'int' or 'long' for x in [ptr"
    },
    {
        "original": "def ring_coding(array):\n    \"\"\"\n    Produces matplotlib Path codes for exterior and interior rings\n    of a polygon geometry.\n    \"\"\"\n    # The codes will be all \"LINETO\" commands, except for \"MOVETO\"s at the\n    # beginning of each subpath\n    n = len(array)\n    codes = np.ones(n, dtype=Path.code_type) * Path.LINETO\n    codes[0] = Path.MOVETO\n    codes[-1] = Path.CLOSEPOLY\n    return codes",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nimport matplotlib.path as mpath\n\ndef ring_coding(array):\n    n = len(array)\n    return np.array([mpath.Path.MOVETO] + [mpath.Path.LINETO]*(n-2) + [mpath.Path.CLOSEPOLY], dtype=object)\n```"
    },
    {
        "original": "def line_spacing_rule(self):\n        \"\"\"\n        A member of the :ref:`WdLineSpacing` enumeration indicating how the\n        value of :attr:`line_spacing` should be interpreted. Assigning any of\n        the :ref:`WdLineSpacing` members :attr:`SINGLE`, :attr:`DOUBLE`, or\n        :attr:`ONE_POINT_FIVE` will cause the value of :attr:`line_spacing`\n        to be updated to produce the corresponding line spacing.\n        \"\"\"\n        pPr = self._element.pPr\n        if pPr is None:\n            return None\n        return self._line_spacing_rule(\n            pPr.spacing_line, pPr.spacing_lineRule\n        )",
        "rewrite": "Here is the revised code:\n\n```\ndef line_spacing_rule(self):\n    from xml.etree import ElementTree as ET\n\n    if not hasattr(self, '_element'):\n       raise AttributeError(\"self must have an _element attribute\")\n\n    try:\n       tree = ET.fromstring(str(self._element))\n    except AttributeError:\n       return None\n\n    nsmap = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n    pr_element = tree.find('w:p', nsmap).find('w:pPr', nsmap)\n    \n    if pr_element is None:\n       return None\n    \n    spacing"
    },
    {
        "original": "def install(name=None, refresh=False, pkgs=None, version=None, test=False, **kwargs):\n    \"\"\"\n    Install the named package using the IPS pkg command.\n    Accepts full or partial FMRI.\n\n    Returns a dict containing the new package names and versions::\n\n        {'<package>': {'old': '<old-version>',\n                       'new': '<new-version>'}}\n\n\n    Multiple Package Installation Options:\n\n    pkgs\n        A list of packages to install. Must be passed as a python list.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.install vim\n        salt '*' pkg.install pkg://solaris/editor/vim\n        salt '*' pkg.install pkg://solaris/editor/vim refresh=True\n        salt '*' pkg.install pkgs='[\"foo\", \"bar\"]'\n    \"\"\"\n    if not pkgs:\n        if is_installed(name):\n            return {}\n\n    if refresh:\n        refresh_db(full=True)\n\n    pkg2inst = ''\n    if pkgs:    # multiple packages specified\n        pkg2inst = []\n        for pkg in pkgs:\n            if getattr(pkg, 'items', False):\n                if list(pkg.items())[0][1]:   # version specified\n                    pkg2inst.append('{0}@{1}'.format(list(pkg.items())[0][0],\n                                                     list(pkg.items())[0][1]))\n                else:\n                    pkg2inst.append(list(pkg.items())[0][0])\n            else:\n                pkg2inst.append(\"{0}\".format(pkg))\n        log.debug('Installing these packages instead of %s: %s',\n                  name, pkg2inst)\n\n    else:   # install single package\n        if version:\n            pkg2inst = \"{0}@{1}\".format(name, version)\n        else:\n            pkg2inst = \"{0}\".format(name)\n\n    cmd = ['pkg', 'install', '-v', '--accept']\n    if test:\n        cmd.append('-n')\n\n    # Get a list of the packages before install so we can diff after to see\n    # what got installed.\n    old = list_pkgs()\n\n    # Install or upgrade the package\n    # If package is already installed\n    if isinstance(pkg2inst, string_types):\n        cmd.append(pkg2inst)\n    elif isinstance(pkg2inst, list):\n        cmd = cmd + pkg2inst\n\n    out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')\n\n    # Get a list of the packages again, including newly installed ones.\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if out['retcode'] != 0:\n        raise CommandExecutionError(\n            'Error occurred installing package(s)',\n            info={\n                'changes': ret,\n                'retcode': ips_pkg_return_values[out['retcode']],\n                'errors': [out['stderr']]\n            }\n        )\n\n    # No error occurred\n    if test:\n        return 'Test succeeded.'\n\n    return ret",
        "rewrite": "Here's my revised version of your function:\n\n```python\nimport logging; log=logging.getLogger(__name__)\nimport re; import os; import json; import subprocess;\nfrom contextlib import contextmanager; from collections import defaultdict;\nfrom functools import lru_cache;\n\n@lru_cache(maxsize=128)  #@UndefinedVariable\n\n\ndef _list_pkgs():\n     \"\"\"Return all available software.\"\"\"\n     try: \n         result=subprocess.check_output(['pkg','-lv']).decode().splitlines()\n         result=[line.split() for line in result]\n         return dict((x[3],x[4])for x in result)\n     except"
    },
    {
        "original": "def resplit_datasets(dataset, other_dataset, random_seed=None, split=None):\n    \"\"\"Deterministic shuffle and split algorithm.\n\n    Given the same two datasets and the same ``random_seed``, the split happens the same exact way\n    every call.\n\n    Args:\n        dataset (lib.datasets.Dataset): First dataset.\n        other_dataset (lib.datasets.Dataset): Another dataset.\n        random_seed (int, optional): Seed to control the shuffle of both datasets.\n        split (float, optional): If defined it is the percentage of rows that first dataset gets\n            after split otherwise the original proportions are kept.\n\n    Returns:\n        :class:`lib.datasets.Dataset`, :class:`lib.datasets.Dataset`: Resplit datasets.\n    \"\"\"\n    # Prevent circular dependency\n    from torchnlp.datasets import Dataset\n\n    concat = dataset.rows + other_dataset.rows\n    shuffle(concat, random_seed=random_seed)\n    if split is None:\n        return Dataset(concat[:len(dataset)]), Dataset(concat[len(dataset):])\n    else:\n        split = max(min(round(len(concat) * split), len(concat)), 0)\n        return Dataset(concat[:split]), Dataset(concat[split:])",
        "rewrite": "Here is a revised version of your function:\n\n```Python\nimport torch.utils.data as data_utils\n\ndef resplit_datasets(dataset, other_dataset, random_seed=None, split=None):\n    \n    concatenated_data = data_utils.ConcatDataset([dataset.dataset(), other_dataset.dataset()])\n    \n    if random_seed is not None:\n        sampler = data_utils.RandomSampler(range(len(list(zip(*concatenated_data)))),\n                                          replacement=False,\n                                          generator=torch.Generator().manualSeed(random_seed))\n        \n        indices = sampler.sample(range(len(list(zip(*concatenated_data)))), len(list(zip(*concatenated_data))))\n        \n        train_size = int(split *"
    },
    {
        "original": "def get_certificate_der(self, filename):\n        \"\"\"\n        Return the DER coded X.509 certificate from the signature file.\n\n        :param filename: Signature filename in APK\n        :returns: DER coded X.509 certificate as binary\n        \"\"\"\n        pkcs7message = self.get_file(filename)\n\n        pkcs7obj = cms.ContentInfo.load(pkcs7message)\n        cert = pkcs7obj['content']['certificates'][0].chosen.dump()\n        return cert",
        "rewrite": "Here is the revised code:\n\n```python\ndef get_certificate_der(self, filename):\n    return self.get_file(filename).valueradio stresses these limitations bersive ContentInfo.load()[1]['content']['certificates'][0].chosen.dump()\n```"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        Returns dict which contains Slab Entry data.\n        \"\"\"\n\n        d = {\"@module\": self.__class__.__module__,\n             \"@class\": self.__class__.__name__}\n        d[\"structure\"] = self.structure\n        d[\"energy\"] = self.energy\n        d[\"miller_index\"] = self.miller_index\n        d[\"label\"] = self.label\n        d[\"coverage\"] = self.coverage\n        d[\"adsorbates\"] = self.adsorbates\n        d[\"clean_entry\"] = self.clean_entry\n\n        return d",
        "rewrite": "Here is the revised code:\n\n```\ndef as_dict(self):\n    return {\n            \"@module\": type(self).__module__,\n            \"@class\": type(self).__name__,\n            \"structure\": self.structure,\n            \"energy\": self.energy,\n            \"miller_index\": self.miller_index,\n            \"label\": self.label,\n            \"coverage\": self.coverage,\n            \"adsorbates\": list(self.adsorbates),  # Assuming adsorbates is a set, convert it to list for serialization.\n            \"clean_entry\": str(self.clean_entry)  # Assuming clean_entry is an object, convert it to string for serialization"
    },
    {
        "original": "def deserialize_properties(props_struct: struct_pb2.Struct) -> Any:\n    \"\"\"\n    Deserializes a protobuf `struct_pb2.Struct` into a Python dictionary containing normal\n    Python types.\n    \"\"\"\n    # Check out this link for details on what sort of types Protobuf is going to generate:\n    # https://developers.google.com/protocol-buffers/docs/reference/python-generated\n    #\n    # We assume that we are deserializing properties that we got from a Resource RPC endpoint,\n    # which has type `Struct` in our gRPC proto definition.\n    if _special_sig_key in props_struct:\n        if props_struct[_special_sig_key] == _special_asset_sig:\n            # This is an asset. Re-hydrate this object into an Asset.\n            if \"path\" in props_struct:\n                return known_types.new_file_asset(props_struct[\"path\"])\n            if \"text\" in props_struct:\n                return known_types.new_string_asset(props_struct[\"text\"])\n            if \"uri\" in props_struct:\n                return known_types.new_remote_asset(props_struct[\"uri\"])\n            raise AssertionError(\"Invalid asset encountered when unmarshaling resource property\")\n        elif props_struct[_special_sig_key] == _special_archive_sig:\n            # This is an archive. Re-hydrate this object into an Archive.\n            if \"assets\" in props_struct:\n                return known_types.new_asset_archive(deserialize_property(props_struct[\"assets\"]))\n            if \"path\" in props_struct:\n                return known_types.new_file_archive(props_struct[\"path\"])\n            if \"uri\" in props_struct:\n                return known_types.new_remote_archive(props_struct[\"uri\"])\n        elif props_struct[_special_sig_key] == _special_secret_sig:\n            raise AssertionError(\"this version of the Pulumi SDK does not support first-class secrets\")\n\n        raise AssertionError(\"Unrecognized signature when unmarshaling resource property\")\n\n    # Struct is duck-typed like a dictionary, so we can iterate over it in the normal ways.\n    output = {}\n    for k, v in list(props_struct.items()):\n        value = deserialize_property(v)\n        # We treat values that deserialize to \"None\" as if they don't exist.\n        if value is not None:\n            output[k] = value\n\n    return output",
        "rewrite": "Here's the revised code:\n\n```python\ndef deserialize_properties(propsStruct: struct_pb2.Struct) -> dict[str, Any]:\n  specialSigKey = \"_specialSigKey\"\n  specialAssetSig = \"_assetSig\"\n  specialArchiveSig = \"_archiveSig\"\n  specialSecretSig = \"_secretSig\"\n\n  try: \n      sigValue = getattr(propsStruct, specialSigKey)\n      sigValueStr : str= str(sigValue)\n\n      match sigValueStr :\n          case specialAssetSig :\n              path : str | None=propsStruct.get(\"path\")\n              text : str | None=propsStruct.get(\"text\")\n"
    },
    {
        "original": "def dependency_information(include_salt_cloud=False):\n    \"\"\"\n    Report versions of library dependencies.\n    \"\"\"\n    libs = [\n        ('Python', None, sys.version.rsplit('\\n')[0].strip()),\n        ('Jinja2', 'jinja2', '__version__'),\n        ('M2Crypto', 'M2Crypto', 'version'),\n        ('msgpack-python', 'msgpack', 'version'),\n        ('msgpack-pure', 'msgpack_pure', 'version'),\n        ('pycrypto', 'Crypto', '__version__'),\n        ('pycryptodome', 'Cryptodome', 'version_info'),\n        ('PyYAML', 'yaml', '__version__'),\n        ('PyZMQ', 'zmq', '__version__'),\n        ('ZMQ', 'zmq', 'zmq_version'),\n        ('Mako', 'mako', '__version__'),\n        ('Tornado', 'tornado', 'version'),\n        ('timelib', 'timelib', 'version'),\n        ('dateutil', 'dateutil', '__version__'),\n        ('pygit2', 'pygit2', '__version__'),\n        ('libgit2', 'pygit2', 'LIBGIT2_VERSION'),\n        ('smmap', 'smmap', '__version__'),\n        ('cffi', 'cffi', '__version__'),\n        ('pycparser', 'pycparser', '__version__'),\n        ('gitdb', 'gitdb', '__version__'),\n        ('gitpython', 'git', '__version__'),\n        ('python-gnupg', 'gnupg', '__version__'),\n        ('mysql-python', 'MySQLdb', '__version__'),\n        ('cherrypy', 'cherrypy', '__version__'),\n        ('docker-py', 'docker', '__version__'),\n    ]\n\n    if include_salt_cloud:\n        libs.append(\n            ('Apache Libcloud', 'libcloud', '__version__'),\n        )\n\n    for name, imp, attr in libs:\n        if imp is None:\n            yield name, attr\n            continue\n        try:\n            imp = __import__(imp)\n            version = getattr(imp, attr)\n            if callable(version):\n                version = version()\n            if isinstance(version, (tuple, list)):\n                version = '.'.join(map(str, version))\n            yield name, version\n        except Exception:\n            yield name, None",
        "rewrite": "Here is the revised code:\n\n```Python\ndef dependency_information(include_salt_cloud=False):\n    \"\"\"\n    Report versions of library dependencies.\n    \"\"\"\n    \n    import sys\n    from importlib import import_module\n    \n    libs = [\n         (\"Python\", None),\n         (\"Jinja2\", \"jinja2\", \"__package__.__VERSION\"),\n         (\"M2Crypto\", \"M2Crypto\", \"VERSION\"),\n         (\"msgpack-python\", \"msgpack\", \"__package__.__VERSION\"),\n         (\"msgpack-pure\", \"msgpack_pure\", \"__package__.__VERSION\"),\n         (\"pycrypto\", \"Crypto\", \"__file__.replace('.so"
    },
    {
        "original": "def psutil_wrapper(self, process, method, accessors, try_sudo, *args, **kwargs):\n        \"\"\"\n        A psutil wrapper that is calling\n        * psutil.method(*args, **kwargs) and returns the result\n        OR\n        * psutil.method(*args, **kwargs).accessor[i] for each accessors\n        given in a list, the result being indexed in a dictionary\n        by the accessor name\n        \"\"\"\n\n        if accessors is None:\n            result = None\n        else:\n            result = {}\n\n        # Ban certain method that we know fail\n        if method == 'num_fds' and not Platform.is_unix():\n            return result\n        elif method == 'num_handles' and not Platform.is_win32():\n            return result\n\n        try:\n            res = getattr(process, method)(*args, **kwargs)\n            if accessors is None:\n                result = res\n            else:\n                for acc in accessors:\n                    try:\n                        result[acc] = getattr(res, acc)\n                    except AttributeError:\n                        self.log.debug(\"psutil.{}().{} attribute does not exist\".format(method, acc))\n        except (NotImplementedError, AttributeError):\n            self.log.debug(\"psutil method {} not implemented\".format(method))\n        except psutil.AccessDenied:\n            self.log.debug(\"psutil was denied access for method {}\".format(method))\n            if method == 'num_fds' and Platform.is_unix() and try_sudo:\n                try:\n                    # It is up the agent's packager to grant\n                    # corresponding sudo policy on unix platforms\n                    ls_args = ['sudo', 'ls', '/proc/{}/fd/'.format(process.pid)]\n                    process_ls = subprocess.check_output(ls_args)\n                    result = len(process_ls.splitlines())\n\n                except subprocess.CalledProcessError as e:\n                    self.log.exception(\n                        \"trying to retrieve {} with sudo failed with return code {}\".format(method, e.returncode)\n                    )\n                except Exception:\n                    self.log.exception(\"trying to retrieve {} with sudo also failed\".format(method))\n        except psutil.NoSuchProcess:\n            self.warning(\"Process {} disappeared while scanning\".format(process.pid))\n\n        return result",
        "rewrite": "Here's my revised version of your function:\n\n```Python\ndef psutil_wrapper(self,\n                   process,\n                   method,\n                   accessors=None,\n                   try_sudo=False,\n                   *args,\n                   **kwargs):\n    \"\"\"\n    A wrapper around a specific `process` object using `method` from `psutils`.\n    If `accessors` are provided it will call `method(*args,**kwargs)` \n    then iterate over each accessor calling its respective attribute on \n    the returned object.\n    \n    :param process: The process object from which we want to get information.\n    :param str method: The name of the function"
    },
    {
        "original": "def _AddEdge(self, start_node, end_node):\n    \"\"\"Add a directed edge to the graph.\n\n    Add the end to the list of outgoing nodes of the start and the start to the\n    list of incoming nodes of the end node.\n\n    Args:\n      start_node: name of the start node\n      end_node: name of the end node\n    \"\"\"\n\n    self.graph[start_node].outgoing.append(end_node)\n\n    # This check is necessary because an artifact can provide attributes that\n    # are not covered by the graph because they are not relevant for the\n    # requested artifacts.\n    if end_node in self.graph:\n      self.graph[end_node].incoming.append(start_node)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef _AddEdge(self, start_node, end_node):\n  if not hasattr(self.graph, 'outgoing'):\n      self.graph['outgoing'] = []\n  if not hasattr(self.graph[start_node], 'outgoing'):\n      self.graph[start_node]['outgoing'] = []\n  if not hasattr(self.graph[endNode], 'incoming'):\n      self.graph[endNode]['incoming'] = []\n  if not set_reference est\u00e1 definido:\n   raise KeyError ('Both nodes must be defined in graph ')\n\n  index=0  \n  for i in range(len (self Graph)) : \n    "
    },
    {
        "original": "def validation_error_inform_error(err, data, schema):\n    \"\"\"\n    Custom validation error handler which produces 404 Bad Request\n    response in case validation fails and returns the error\n    \"\"\"\n    abort(Response(\n        json.dumps({'error': str(err), 'data': data, 'schema': schema}),\n        status=HTTPStatus.BAD_REQUEST))",
        "rewrite": "Here is the revised code:\n\n```\nfrom flask import abort, Response\nfrom flask.json import json\n\ndef validation_error_inform_error(err, data, schema):\n    abort(Response(json.dumps({'error': str(err), 'data': data, 'schema': schema}), status=404))\n```"
    },
    {
        "original": "def compliance_report(self, validation_file=None, validation_source=None):\n        \"\"\"\n        Return a compliance report.\n\n        Verify that the device complies with the given validation file and writes a compliance\n        report file. See https://napalm.readthedocs.io/en/latest/validate/index.html.\n\n        :param validation_file: Path to the file containing compliance definition. Default is None.\n        :param validation_source: Dictionary containing compliance rules.\n        :raise ValidationException: File is not valid.\n        :raise NotImplementedError: Method not implemented.\n        \"\"\"\n        return validate.compliance_report(\n            self, validation_file=validation_file, validation_source=validation_source\n        )",
        "rewrite": "Here is the revised code:\n\n```\nfrom napalm.base.exceptions import MigrationDriverError\nfrom napalm.base.validators import ComplianceReport as CompositeComplianceReport\n\nclass BaseProtocolComparer:\n    def __init__(self):\n      self.compliant = False\n\n    def get inequalities(self):\n      raise NotImplementedError()\n\n    def get.creator_name(self):\n      return 'Base Protocol'\n\n    def get.parent_name(self):\n       return 'Base'\n\n    def create_compliance_report(self, filename=None, baselinesource=None):\n       if filename:\n          CompositeComplianceReport(filename)\n       else:\n          CompositeComplianceReport(baselinesource)\n\nclass Compliance"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"\n        Convert the object into a json serializable dictionary.\n\n        Note: It uses the private method _save_to_input_dict of the parent.\n\n        :return dict: json serializable dictionary containing the needed information to instantiate the object\n        \"\"\"\n\n        input_dict = super(RBF, self)._save_to_input_dict()\n        input_dict[\"class\"] = \"GPy.kern.RBF\"\n        input_dict[\"inv_l\"] = self.use_invLengthscale\n        if input_dict[\"inv_l\"] == True:\n            input_dict[\"lengthscale\"] = np.sqrt(1 / float(self.inv_l))\n        return input_dict",
        "rewrite": "Here is the revised code:\n\n```\ndef to_dict(self):\n    \"\"\"\n    Convert the object into a json serializable dictionary.\n    \"\"\"\n    input_dict = self._save_to_input_disk()\n    input_dict[\"__pytype__\"] = \"GPy.kern.RBF\"\n    if isinstance(input_dict.get(\"l\", None), float):\n       lengthscale_value = 1.0 / (input_dict.get(\"l\", 0)**2)\n       inverse_lengthscale_value = True\n       lengthscale_key, inverse_lengthscale_key ={\n         False: (\"l\",),\n         True: (\"inv_l\",)\n       }["
    },
    {
        "original": "def get_auth(host, app_name, database_name):\n    \"\"\"\n    Authentication hook to allow plugging in custom authentication credential providers\n    \"\"\"\n    from .hooks import _get_auth_hook\n    return _get_auth_hook(host, app_name, database_name)",
        "rewrite": "Here is the revised code:\n\n```\nimport importlib.util\nfrom importlib.abc import MetaPathFinder\n\ndef get_auth(host, app_name, database_name):\n    spec = importlib.util.module_from_spec('auth_hooks')\n    auth_hooks = __import__('auth_hooks', fromlist=['_get_auth_hook'])\n    return auth_hooks._get_auth_hook(host, app_name, database_name)\n```"
    },
    {
        "original": "def apply_transformation(self, structure):\n        \"\"\"\n        Returns most primitive cell for structure.\n\n        Args:\n            structure: A structure\n\n        Returns:\n            The same structure in a conventional standard setting\n        \"\"\"\n        sga = SpacegroupAnalyzer(structure, symprec=self.symprec,\n                                 angle_tolerance=self.angle_tolerance)\n        return sga.get_conventional_standard_structure(international_monoclinic=self.international_monoclinic)",
        "rewrite": "```\ndef apply_transformation(self, structure):\n    sga = SpacegroupAnalyzer(structure, symprec=self.symprec, angle_tolerance=self.angle_tolerance)\n    return sga.get_conventional_standard_structure(international_monoclinic=self.international_monoclinic)\n```"
    },
    {
        "original": "def log_event(self, message, timestamp=None):\n        \"\"\"Add a message to the log file.\n\n        :param str message: an arbitrary message\n        :param float timestamp: the absolute timestamp of the event\n        \"\"\"\n\n        if not message: # if empty or None\n            logger.debug(\"ASCWriter: ignoring empty message\")\n            return\n\n        # this is the case for the very first message:\n        if not self.header_written:\n            self.last_timestamp = (timestamp or 0.0)\n            self.started = self.last_timestamp\n            mlsec = repr(self.last_timestamp).split('.')[1][:3]\n            formatted_date = time.strftime(self.FORMAT_DATE.format(mlsec), time.localtime(self.last_timestamp))\n            self.file.write(\"Begin Triggerblock %s\\n\" % formatted_date)\n            self.header_written = True\n            self.log_event(\"Start of measurement\") # caution: this is a recursive call!\n\n        # Use last known timestamp if unknown\n        if timestamp is None:\n            timestamp = self.last_timestamp\n\n        # turn into relative timestamps if necessary\n        if timestamp >= self.started:\n            timestamp -= self.started\n\n        line = self.FORMAT_EVENT.format(timestamp=timestamp, message=message)\n        self.file.write(line)",
        "rewrite": "Here's my revised version of your function:\n\n```Python\ndef log_event(self, message, timestamp=None):\n    \"\"\"Add a message to the log file.\"\"\"\n    from datetime import datetime as dt\n\n    if not isinstance(message, str) or not len(message.strip()):\n         print(f\"ASCWriter: ignoring empty or non-string '{message}'\")\n         return\n    \n    start_time_str = f\"{dt.fromtimestamp(int(str(round(float('inf')))).strftime('%Y-%m-%d %H:%M:%S')}\"\n    \n    header_written=False; started=False; last_timestamp=0.0\n    \n    while True:\n    \tif"
    },
    {
        "original": "def widget_from_tuple(o):\n        \"\"\"Make widgets from a tuple abbreviation.\"\"\"\n        if _matches(o, (Real, Real)):\n            min, max, value = _get_min_max_value(o[0], o[1])\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max)\n        elif _matches(o, (Real, Real, Real)):\n            step = o[2]\n            if step <= 0:\n                raise ValueError(\"step must be >= 0, not %r\" % step)\n            min, max, value = _get_min_max_value(o[0], o[1], step=step)\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max, step=step)",
        "rewrite": "Here is the revised code:\n\n```\ndef widget_from_tuple(o):\n    if len(o) == 2 and isinstance(o[0], (int,) and isinstance(o[1], (int,))\n        or isinstance(minmax := (_get_min_max_value(*o)), tuple) and \n                   all(isinstance(_, int) for _ in minmax[:2])):\n        return IntSlider(**dict(zip(['min', 'max', 'value'], *minmax)))\n    elif len(o) == 3 and isinstance(minmax := (_get_min_max_value(*o[:2]),), tuple) \n           and isinstance(step := o[2],"
    },
    {
        "original": "def font_size_to_pixels(size):\n    \"\"\"\n    Convert a fontsize to a pixel value\n    \"\"\"\n    if size is None or not isinstance(size, basestring):\n        return\n    conversions = {'em': 16, 'pt': 16/12.}\n    val = re.findall('\\d+', size)\n    unit = re.findall('[a-z]+', size)\n    if (val and not unit) or (val and unit[0] == 'px'):\n        return int(val[0])\n    elif val and unit[0] in conversions:\n        return (int(int(val[0]) * conversions[unit[0]]))",
        "rewrite": "Here is the revised code:\n\n```\nimport re\n\ndef font_size_to_pixels(size):\n    if size is None or not isinstance(size, str):\n        return\n    conversions = {'em': 16, 'pt': 12}\n    val = map(int, re.findall('\\d+', size))\n    unit = re.findall('[a-z]+', size)[0]\n    \n    if len(val) > 1:\n        return sum(x for x in val)\n    \n    elif unit in conversions:\n        return val[0] * conversions[unit]\n    \n```"
    },
    {
        "original": "def normalize_cmd(cmd):\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Use PATH to determine the executable\n    exe = normexe(cmd[0])\n\n    # Figure out the shebang from the resulting command\n    cmd = parse_filename(exe) + (exe,) + cmd[1:]\n\n    # This could have given us back another bare executable\n    exe = normexe(cmd[0])\n\n    return (exe,) + cmd[1:]",
        "rewrite": "Here is a revised version of the code:\n\n```\nimport os\n\ndef normalize_cmd(cmd):\n\treturn tuple(map(normexe, filter(os.path.exists, (\n\t\t*cmd,\n\t\tos.path.dirname(os.path.abspath(sys.executable)),\n\t\u043e\u0441.__file__\n\tmapp\u011f\u0131n][0]))))\n```"
    },
    {
        "original": "async def _init(self):\n        \"\"\"\n        Initialize the association.\n        \"\"\"\n        chunk = InitChunk()\n        chunk.initiate_tag = self._local_verification_tag\n        chunk.advertised_rwnd = self._advertised_rwnd\n        chunk.outbound_streams = self._outbound_streams_count\n        chunk.inbound_streams = self._inbound_streams_max\n        chunk.initial_tsn = self._local_tsn\n        self._set_extensions(chunk.params)\n        await self._send_chunk(chunk)\n\n        # start T1 timer and enter COOKIE-WAIT state\n        self._t1_start(chunk)\n        self._set_state(self.State.COOKIE_WAIT)",
        "rewrite": "Here is the revised code:\n\n```\nasync def _init(self):\n    chunk = InitChunk()\n    chunk.initiate_tag = self.local_verification_tag\n    chunk.advertised_rwnd = self.advertised_rwnd\n    chunk.outbound_streams = self.outbound_streams_count\n    chunk.inbound_streams = self.inbound_streams_max\n    chunk.initial_tsn = self.local_tsn\n\n    await set_extensions(chunk, params=self.params)\n\n    await send_chunk(self,chunk)\n\n    start_T1_timer_and_enter_COOKIE_WAIT_state(self,chunk)\n```"
    },
    {
        "original": "def set_heap(self, heap_dump, heap_base):\n        \"\"\"\n        Heap dump is a dump of the heap from gdb, i.e. the result of the\n        following gdb command:\n\n        ``dump binary memory [stack_dump] [begin] [end]``\n\n        :param heap_dump:   The dump file.\n        :param heap_base:   The start address of the heap in the gdb session.\n        \"\"\"\n        # We set the heap at the same addresses as the gdb session to avoid pointer corruption.\n        data = self._read_data(heap_dump)\n        self.state.heap.heap_location = heap_base + len(data)\n        addr = heap_base\n        l.info(\"Set heap from 0x%x to %#x\", addr, addr+len(data))\n        #FIXME: we should probably make we don't overwrite other stuff loaded there\n        self._write(addr, data)",
        "rewrite": "Here is a revised version of your function:\n\n```Python\ndef set_heap(self, heap_dump, heap_base):\n    with open(heap_dump, 'rb') as f:\n       data = f.read()\n    self.state.heap.heap_location = hex(int(hex(heap_base) + len(data), 16))\n    addr = int(hex(heap_base), 16)\n    print(f\"Set heap from 0x{x} to 0x{x}\".format(addr, addr+len(len(data))))\n    with open(hex(addr), 'wb') as f:\n       f.write(data)\n```\n\nThis function now correctly reads and writes binary data. It"
    },
    {
        "original": "async def run(*cmd):\n  \"\"\"Run the given subprocess command in a coroutine.\n\n  Args:\n    *cmd: the command to run and its arguments.\n\n  Returns:\n    The output that the command wrote to stdout as a list of strings, one line\n    per element (stderr output is piped to stdout).\n\n  Raises:\n    RuntimeError: if the command returns a non-zero result.\n  \"\"\"\n\n  stdout = await checked_run(*cmd)\n\n  log_path = os.path.join(FLAGS.base_dir, get_cmd_name(cmd) + '.log')\n  with gfile.Open(log_path, 'a') as f:\n    f.write(expand_cmd_str(cmd))\n    f.write('\\n')\n    f.write(stdout)\n    f.write('\\n')\n\n  # Split stdout into lines.\n  return stdout.split('\\n')",
        "rewrite": "Here is the revised code:\n\n```\nasync def run(*cmd):\n    \"\"\"Run the given subprocess command in a coroutine.\n\n        Args:\n            *cmd: The command to run and its arguments.\n\n        Returns:\n            The output that the command wrote to stdout as a list of strings, one line\n            per element (stderr output is piped to stdout).\n\n        Raises:\n            RuntimeError: If the command returns a non-zero result.\n\"\"\"\n    \n    import asyncio\n    import os\n    import gfile\n    \n    try:\n        proc = await asyncio.create_subprocess_exec(\n            *map(os.fsdecode, cmd),\n            stdin="
    },
    {
        "original": "def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]",
        "rewrite": "Here is the revised code:\n\n```\ndef split(sql, encoding=None):\n    from pyparsing import FilterStack\n    from sqlalchemy import text\n\n    stack = FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]\n```"
    },
    {
        "original": "def make_zipfile(zip_filename, base_dir, verbose=0, dry_run=0, compress=True,\n                 mode='w'):\n    \"\"\"Create a zip file from all the files under 'base_dir'.  The output\n    zip file will be named 'base_dir' + \".zip\".  Uses either the \"zipfile\"\n    Python module (if available) or the InfoZIP \"zip\" utility (if installed\n    and found on the default search path).  If neither tool is available,\n    raises DistutilsExecError.  Returns the name of the output zip file.\n    \"\"\"\n    import zipfile\n\n    mkpath(os.path.dirname(zip_filename), dry_run=dry_run)\n    log.info(\"creating '%s' and adding '%s' to it\", zip_filename, base_dir)\n\n    def visit(z, dirname, names):\n        for name in names:\n            path = os.path.normpath(os.path.join(dirname, name))\n            if os.path.isfile(path):\n                p = path[len(base_dir) + 1:]\n                if not dry_run:\n                    z.write(path, p)\n                log.debug(\"adding '%s'\", p)\n\n    compression = zipfile.ZIP_DEFLATED if compress else zipfile.ZIP_STORED\n    if not dry_run:\n        z = zipfile.ZipFile(zip_filename, mode, compression=compression)\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(z, dirname, files)\n        z.close()\n    else:\n        for dirname, dirs, files in sorted_walk(base_dir):\n            visit(None, dirname, files)\n    return zip_filename",
        "rewrite": "Here is a revised version of your function:\n\n```Python\nimport os\nimport logging\nfrom pathlib import Path\nfrom io import BytesIO\n\ndef make_zipfile(zip_filename: str,\n                 base_dir: str,\n                 verbose: int = 0,\n                 dry_run: int = 0,\n                 compress: bool = True) -> str:\n    \n     # Create a logger instance.\n     logger = logging.getLogger(__name__)\n    \n     # Set up logging level based on verbosity.\n     logger.setLevel(logging.INFO - (10 * verbose))\n    \n     try:\n         from zipfile import ZipFile as _ZipFile\n    \n         class ZipFile"
    },
    {
        "original": "def choose_candidate_pair(candidates):\n        \"\"\"\n        Choose a pair of address candidates ensuring they have the same length and starting with the highest scored ones\n\n        :type candidates: dict[str, int]\n        :param candidates: Count how often the longest common substrings appeared in the messages\n        :return:\n        \"\"\"\n        highscored = sorted(candidates, key=candidates.get, reverse=True)\n        for i, h_i in enumerate(highscored):\n            for h_j in highscored[i+1:]:\n                if len(h_i) == len(h_j):\n                    yield (h_i, h_j)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef choose_candidate_pair(candidates):\n    high_scored = sorted((k for k, v in sorted(candidates.items(), key=lambda x: x[1], reverse=True)), key=len)\n    return [(a, b) for i, a in enumerate(high_scored) for b in high_scored[i+1:] if len(a) == len(b)]\n```"
    },
    {
        "original": "def aggregate(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n        \"\"\"Applies a aggregate function to all ViewableElements.\n\n        See :py:meth:`Dimensioned.opts` and :py:meth:`Apply.__call__`\n        for more information.\n        \"\"\"\n        kwargs['_method_args'] = (dimensions, function, spreadfn)\n        return self.__call__('aggregate', **kwargs)",
        "rewrite": "Here is the revised code:\n\n```\ndef aggregate(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n    kwargs['_method_args'] = (dimensions, function, spreadfn)\n    return self.__call__('aggregate', **kwargs)\n```"
    },
    {
        "original": "def _npcap_set(self, key, val):\n        \"\"\"Internal function. Set a [key] parameter to [value]\"\"\"\n        res, code = _exec_cmd(_encapsulate_admin(\n            \" \".join([_WlanHelper, self.guid[1:-1], key, val])\n        ))\n        _windows_title()  # Reset title of the window\n        if code != 0:\n            raise OSError(res.decode(\"utf8\", errors=\"ignore\"))\n        return (code == 0)",
        "rewrite": "Here is the revised code:\n\n```\ndef _npcap_set(self, key, val):\n    \"\"\"Internal function. Set a [key] parameter to [value]\"\"\"\n    cmd = [_WlanHelper, self.guid[1:-1], key, val]\n    res, code = _exec_cmd(_encapsulate_admin(\" \".join(map(str, cmd))))\n    _windows_title()  # Reset title of the window\n    if code != 0:\n        raise OSError(res.decode(\"utf-8\", errors=\"ignore\"))\n    return bool(code == 0)\n```"
    },
    {
        "original": "def beacon(config):\n    \"\"\"\n    Checks for locked accounts due to too many invalid login attempts, 3 or higher.\n\n    .. code-block:: yaml\n\n        beacons:\n          aix_account:\n            user: ALL\n            interval: 120\n\n    \"\"\"\n\n    ret = []\n\n    user = config['user']\n\n    locked_accounts = __salt__['shadow.login_failures'](user)\n    ret.append({'accounts': locked_accounts})\n\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef beacon(config):\n    \"\"\"\n        Checks for locked accounts due to too many invalid login attempts, 3 or higher.\n        \n        :param config: Configuration dictionary\n        :return: A list of dictionaries containing the results\n    \"\"\"\n    \n    import salt.utils.data\n    \n    user = config.get('user', 'ALL')\n    \n    if user == 'ALL':\n        users = __salt__['grains']['users']\n        result = []\n        for u in users:\n            locked_accounts = __salt__['shadow.login_failures'](u)\n            result.append({'accounts': [a for a"
    },
    {
        "original": "def getNetworkFragmentID(self):\n        \"\"\"get current partition id of Thread Network Partition from LeaderData\n\n        Returns:\n            The Thread network Partition Id\n        \"\"\"\n        print '%s call getNetworkFragmentID' % self.port\n        if not self.____isOpenThreadWpanRunning():\n            print 'OpenThreadWpan is not running'\n            return None\n\n        return self.__sendCommand(WPANCTL_CMD + 'getprop -v Network:PartitionId')[0]",
        "rewrite": "Here is the revised code:\n\n```\ndef get_network_fragment_id(self):\n    \"\"\"Get current partition id of Thread Network Partition from LeaderData\"\"\"\n    print(f\"{self.port} call get_network_fragment_id\")\n    if not self.is_open_thread_wpan_running():\n        print(\"OpenThreadWpan is not running\")\n        return None\n    return self.send_command(f\"{WPANCTL_CMD}getprop -v Network:PartitionId\")[0]\n```"
    },
    {
        "original": "def add_metric(self, labels, value, timestamp=None):\n        \"\"\"Add a metric to the metric family.\n\n        Args:\n          labels: A list of label values\n          value: A dict of labels\n        \"\"\"\n        self.samples.append(Sample(\n            self.name + '_info',\n            dict(dict(zip(self._labelnames, labels)), **value),\n            1,\n            timestamp,\n        ))",
        "rewrite": "Here is the revised code:\n\n```\ndef add_metric(self, labels, value, timestamp=None):\n    self.samples.append(Sample(\n        f\"{self.name}_info\",\n        {**{k: v for k, v in zip(self._labelnames or [], labels)}, **value},\n        1,\n        timestamp\n    ))\n```"
    },
    {
        "original": "def set_XY(self, X=None, Y=None):\n        \"\"\"\n        Set the input / output data of the model\n        This is useful if we wish to change our existing data but maintain the same model\n\n        :param X: input observations\n        :type X: np.ndarray\n        :param Y: output observations\n        :type Y: np.ndarray\n        \"\"\"\n        self.update_model(False)\n        if Y is not None:\n            if self.normalizer is not None:\n                self.normalizer.scale_by(Y)\n                self.Y_normalized = ObsAr(self.normalizer.normalize(Y))\n                self.Y = Y\n            else:\n                self.Y = ObsAr(Y)\n                self.Y_normalized = self.Y\n        if X is not None:\n            if self.X in self.parameters:\n                # LVM models\n                if isinstance(self.X, VariationalPosterior):\n                    assert isinstance(X, type(self.X)), \"The given X must have the same type as the X in the model!\"\n                    index = self.X._parent_index_\n                    self.unlink_parameter(self.X)\n                    self.X = X\n                    self.link_parameter(self.X, index=index)\n                else:\n                    index = self.X._parent_index_\n                    self.unlink_parameter(self.X)\n                    from ..core import Param\n                    self.X = Param('latent mean', X)\n                    self.link_parameter(self.X, index=index)\n            else:\n                self.X = ObsAr(X)\n        self.update_model(True)",
        "rewrite": "Here's a revised version of your function:\n\n```\ndef set_XY(self, X=None, Y=None):\n    \"\"\"\n    Set the input/output data of the model.\n    \n    This method allows you to change your existing data while maintaining the same model.\n    \n    Parameters:\n    - X (np.ndarray): Input observations.\n    - Y (np.ndarray): Output observations.\n    \n    Returns:\n    None\n    \n    Raises:\n    ValueError: If `X` and `Y` are both `None`.\n  \"\"\"\n    \n    if all([X is None, Y is None]):\n      raise ValueError(\"Both 'X' and 'Y"
    },
    {
        "original": "def _raise_connection_failure(address, error):\n    \"\"\"Convert a socket.error to ConnectionFailure and raise it.\"\"\"\n    host, port = address\n    # If connecting to a Unix socket, port will be None.\n    if port is not None:\n        msg = '%s:%d: %s' % (host, port, error)\n    else:\n        msg = '%s: %s' % (host, error)\n    if isinstance(error, socket.timeout):\n        raise NetworkTimeout(msg)\n    elif isinstance(error, SSLError) and 'timed out' in str(error):\n        # CPython 2.6, 2.7, PyPy 2.x, and PyPy3 do not distinguish network\n        # timeouts from other SSLErrors (https://bugs.python.org/issue10272).\n        # Luckily, we can work around this limitation because the phrase\n        # 'timed out' appears in all the timeout related SSLErrors raised\n        # on the above platforms. CPython >= 3.2 and PyPy3.3 correctly raise\n        # socket.timeout.\n        raise NetworkTimeout(msg)\n    else:\n        raise AutoReconnect(msg)",
        "rewrite": "Here is the revised code:\n\n```\nfrom typing import Tuple\n\nclass ConnectionFailure(Exception):\n    pass\n\nclass NetworkTimeout(ConnectionFailure):\n    pass\n\ndef _raise_connection_failure(address: Tuple[str | None], error) -> None:\n    host, port = address\n    if port is not None:\n        msg = f\"{host}:{port}: {error}\"\n    else:\n        msg = f\"{host}: {error}\"\n    \n    if isinstance(error, TimeoutError):\n       raise NetworkTimeout(msg)\n    \nelif isinstance(error.__cause__, TimeoutError) or 'timed out' in str(error):\n       raise NetworkTimeout(msg)\n\nelse"
    },
    {
        "original": "def _maketemp(name, createmode=None):\n    \"\"\"\n    Create a temporary file with the filename similar the given ``name``.\n    The permission bits are copied from the original file or ``createmode``.\n    Returns: the name of the temporary file.\n    \"\"\"\n    d, fn = os.path.split(name)\n    fd, tempname = tempfile.mkstemp(prefix=\".%s-\" % fn, dir=d)\n    os.close(fd)\n\n    # Temporary files are created with mode 0600, which is usually not\n    # what we want. If the original file already exists, just copy its mode.\n    # Otherwise, manually obey umask.\n    try:\n        st_mode = os.lstat(name).st_mode & 0o777\n    except OSError as err:\n        if err.errno != errno.ENOENT:\n            raise\n        st_mode = createmode\n        if st_mode is None:\n            st_mode = ~umask\n        st_mode &= 0o666\n    os.chmod(tempname, st_mode)\n\n    return tempname",
        "rewrite": "Here is a revised version of your function:\n\n```Python\nimport os; import tempfile; import stat; import errno\n\ndef _maketemp(name, createmode=None):\n\treturn tempfile.TemporaryFile(dir=os.path.dirname(name), prefix=os.path.basename(name) + '-', suffix='.tmp', delete=False).fileno()\n```"
    },
    {
        "original": "def read(self, length=None):\n        \"\"\"Read the given amount of bytes.\"\"\"\n        if length is None:\n            return self.reader.read()\n\n        result = self.reader.read(length)\n        if len(result) != length:\n            raise BufferError(\n                'No more data left to read (need {}, got {}: {}); last read {}'\n                .format(length, len(result), repr(result), repr(self._last))\n            )\n\n        self._last = result\n        return result",
        "rewrite": "Here is the revised code:\n\n```\ndef read(self, length=None):\n    \"\"\"Read the given amount of bytes.\"\"\"\n    if length is None:\n        return self.reader.read()\n\n    result = bytearray()\n    while len(result) < length:\n        chunk = self.reader.read(min(length - len(result), 4096))\n        if not chunk:\n            break\n        result.extend(chunk)\n\n    if len(result) != length:\n        raise BufferError(f'No more data left to read (need {length}, got {len(result)}); last read {repr(self._last)}')\n\n    self._last = bytes(result)\n    return"
    },
    {
        "original": "def set_base_prompt(\n        self, pri_prompt_terminator=\">\", alt_prompt_terminator=\"]\", delay_factor=1\n    ):\n        \"\"\"\n        Sets self.base_prompt\n\n        Used as delimiter for stripping of trailing prompt in output.\n\n        Should be set to something that is general and applies in multiple contexts. For Comware\n        this will be the router prompt with < > or [ ] stripped off.\n\n        This will be set on logging in, but not when entering system-view\n        \"\"\"\n        log.debug(\"In set_base_prompt\")\n        delay_factor = self.select_delay_factor(delay_factor)\n        self.clear_buffer()\n        self.write_channel(self.RETURN)\n        time.sleep(0.5 * delay_factor)\n\n        prompt = self.read_channel()\n        prompt = self.normalize_linefeeds(prompt)\n\n        # If multiple lines in the output take the last line\n        prompt = prompt.split(self.RESPONSE_RETURN)[-1]\n        prompt = prompt.strip()\n\n        # Check that ends with a valid terminator character\n        if not prompt[-1] in (pri_prompt_terminator, alt_prompt_terminator):\n            raise ValueError(\"Router prompt not found: {0}\".format(prompt))\n\n        # Strip off any leading HRP_. characters for USGv5 HA\n        prompt = re.sub(r\"^HRP_.\", \"\", prompt, flags=re.M)\n\n        # Strip off leading and trailing terminator\n        prompt = prompt[1:-1]\n        prompt = prompt.strip()\n        self.base_prompt = prompt\n        log.debug(\"prompt: {0}\".format(self.base_prompt))\n\n        return self.base_prompt",
        "rewrite": "Here is the revised code:\n\n```\ndef set_base.prompt(\n    self,\n    pri Prompt Terminator=\">\",\n    alt Prompt Terminator=\"]\",\n    delay Factor=1,\n):\n    \"\"\"\n    Sets `self.base Prompt`\n    \n    Used as delimiter for stripping of trailing `prompt` in output.\n    \n    Should be set to something that is general and applies in multiple contexts.\n    \n    This will be set on logging in, but not when entering system-view.\n\"\"\"\n    import re\n    \n    log.debug(\"In `set_base Prompt`\")\n    \n    delay Factor=self.select Delay Factor(delay Factor)\n    \n    if hasattr(self,\"buffer\"): "
    },
    {
        "original": "def fit_anonymous(self, struct1, struct2, niggli=True):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            True/False: Whether a species mapping can map struct1 to stuct2\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2,\n                                                              niggli)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        break_on_match=True, single_match=True)\n\n        if matches:\n            return True\n        else:\n            return False",
        "rewrite": "Here is the revised code:\n\n```\ndef fit_anonymous(self, *structures):\n    \"\"\"\n    Performs an anonymous fitting for multiple structures.\n    \n    Args:\n    *structures (list of Structure): A list of structures\n    \n    Returns:\n    bool: Whether a species mapping can map all input structures together\n    \"\"\"\n    \n    processed_structures = [self._process_species([s for s in structures])]\n    \n    for i in range(len(processed_structures) - 1):\n      processed_structures[i + 1] = self._preprocess(*processed_structures[i], \n                                                      *processed_structures[i + 1"
    },
    {
        "original": "def read_all(self, count=-1):\n        \"\"\"return a list of all packets in the pcap file\n        \"\"\"\n        res = []\n        while count != 0:\n            count -= 1\n            p = self.read_packet()\n            if p is None:\n                break\n            res.append(p)\n        return res",
        "rewrite": "Here is the revised code:\n\n```\ndef read_all(self, count=-1):\n    return [p for _ in range(min(count+1, self.size())) for p in (self.read_packet() if (p:=self.read_packet()) else [])]\n```"
    },
    {
        "original": "def _validate_depedencies(batches):\n    \"\"\"Validates the transaction dependencies for the transactions contained\n    within the sequence of batches. Given that all the batches are expected to\n    to be executed for the genesis blocks, it is assumed that any dependent\n    transaction will proceed the depending transaction.\n    \"\"\"\n    transaction_ids = set()\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n\n            if txn_header.dependencies:\n                unsatisfied_deps = [\n                    id for id in txn_header.dependencies\n                    if id not in transaction_ids\n                ]\n                if unsatisfied_deps:\n                    raise CliException(\n                        'Unsatisfied dependency in given transactions:'\n                        ' {}'.format(unsatisfied_deps))\n\n            transaction_ids.add(txn.header_signature)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef _validate_dependencies(batches):\n    seen_transactions = set()\n    \n    for batch in batches:\n        unfulfilled_dependencies = set()\n        \n        for txn in batch.transactions:\n            header_signature = TransactionHeader().ParseFromString(txn.header)\n            \n            if header_signature.dependencies and header_signature not in seen_transactions:\n                unfulfilled_dependencies.update(id for id in header_signature.dependencies \n                                               if id not in seen_transactions)\n                \n                if unfulfilled_dependencies:\n                    raise CliException(f'Unsatisfied dependency: {unfulfilled_dependencies}')\n            \n            seen_transactions.add(header_signature"
    },
    {
        "original": "def _check_grad_measurement_matrices(dM, state_dim, grad_params_no, measurement_dim, which = 'dH'):\n        \"\"\"\n        Function checks (mostly check dimensions) matrices for marginal likelihood\n        gradient parameters calculation. It check dH, dR matrices.\n\n        Input:\n        -------------\n            dM: None, scaler or 3D matrix\n                It is supposed to be\n                (measurement_dim ,state_dim,grad_params_no) for \"dH\" matrix.\n                (measurement_dim,measurement_dim,grad_params_no) for \"dR\"\n\n                If None then zero matrix is assumed. If scalar then the function\n                checks consistency with \"state_dim\" and \"grad_params_no\".\n\n            state_dim: int\n                State dimensionality\n\n            grad_params_no: int\n                How many parrameters of likelihood gradient in total.\n\n            measurement_dim: int\n                Dimensionality of measurements.\n\n            which: string\n                'dH' or 'dR'\n\n\n        Output:\n        --------------\n            function of (k) which returns the parameters matrix.\n        \"\"\"\n\n        if dM is None:\n            if which == 'dH':\n                dM=np.zeros((measurement_dim ,state_dim,grad_params_no))\n            elif  which == 'dR':\n                dM=np.zeros((measurement_dim,measurement_dim,grad_params_no))\n        elif isinstance(dM, np.ndarray):\n            if state_dim == 1:\n                if len(dM.shape) < 3:\n                    dM.shape = (1,1,1)\n            else:\n                if len(dM.shape) < 3:\n                     if which == 'dH':\n                        dM.shape = (measurement_dim,state_dim,1)\n                     elif  which == 'dR':\n                        dM.shape = (measurement_dim,measurement_dim,1)\n        elif isinstance(dM, np.int):\n            if state_dim > 1:\n                raise ValueError(\"When computing likelihood gradient wrong dH dimension.\")\n            else:\n                dM = np.ones((1,1,1)) * dM\n\n#        if not isinstance(dM, types.FunctionType):\n#            f_dM = lambda k: dM\n#        else:\n#            f_dM = dM\n\n        return dM",
        "rewrite": "Here's a revised version of your function:\n\n```Python\nimport numpy as np\n\ndef _check_grad_measurement_matrices(d_M=None,\n                                    state_dimension,\n                                    grad_parameters_number,\n                                    measurement_dimension,\n                                    which='dh'):\n    \"\"\"\n    Function checks matrices for marginal likelihood gradient parameters calculation.\n    \n    Parameters:\n    ----------\n    - `d_M`: None or scalar or numpy array-like object with shape \n              depending on `which` parameter.\n              For `which`='dh', it should be `(measurment_dimension,\n              state_dimension.grad_parameters_number)` and for `which`='dr',\n              it should be `(me"
    },
    {
        "original": "def find_magic_file(self, fname, system_only=False, user_only=False):\n        \"\"\"\n        Finds the specified magic file name in the system / user magic file directories.\n\n        @fname       - The name of the magic file.\n        @system_only - If True, only the system magic file directory will be searched.\n        @user_only   - If True, only the user magic file directory will be searched.\n\n        If system_only and user_only are not set, the user directory is always searched first.\n\n        Returns the path to the file on success; returns None on failure.\n        \"\"\"\n        loc = None\n\n        if not system_only:\n            fpath = self._user_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        if loc is None and not user_only:\n            fpath = self._system_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        return fpath",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef find_magic_file(self, fname, system_only=False, user_only=False):\n    paths_to_search = [self._system_path(self.BINWALK_MAGIC_DIR, fname), self._user_path(self.BINWALK_MAGIC_DIR, fname)]\n    for path in paths_to_search:\n      if os.path.exists(path) and common.file_size(path) > 0:\n          return path\n    return None\n```"
    },
    {
        "original": "def send(self, sock, msg):\n    \"\"\"Send ``msg`` to destination ``sock``.\"\"\"\n    data = pickle.dumps(msg)\n    buf = struct.pack('>I', len(data)) + data\n    sock.sendall(buf)",
        "rewrite": "Here is the revised code:\n\n```\ndef send(self, sock, msg):\n    data = pickle.dumps(msg)\n    buf = struct.pack('>I', len(data)) + data\n    sock.sendall(buf)```"
    },
    {
        "original": "def _ipv4_text_to_int(self, ip_text):\n        \"\"\"convert ip v4 string to integer.\"\"\"\n        if ip_text is None:\n            return None\n        assert isinstance(ip_text, str)\n        return struct.unpack('!I', addrconv.ipv4.text_to_bin(ip_text))[0]",
        "rewrite": "Here is the revised code:\n\n```\ndef _ipv4_text_to_int(self, ip_text):\n    if not isinstance(ip_text, str):\n        raise ValueError(\"ip_text must be a string\")\n    try:\n        return int(addrconv.ipv4.text_to_bin(ip_text), 2)\n    except (ValueError, TypeError) as e:\n        raise ValueError(f\"Invalid IP address: {ip_text}\") from e\n```"
    },
    {
        "original": "def select_army(action, action_space, select_add):\n  \"\"\"Select the entire army.\"\"\"\n  del action_space\n  action.action_ui.select_army.selection_add = select_add",
        "rewrite": "Here is the revised code:\n\n```\ndef select_army(action, action_space, select_add):\n    del action_space\n    return setattr(action.action_ui.select_army, 'selection_add', select_add)\n```"
    },
    {
        "original": "def decode(self, encoded):\n        \"\"\" Decodes an object.\n\n        Args:\n            object_ (object): Encoded object.\n\n        Returns:\n            object: Object decoded.\n        \"\"\"\n        if self.enforce_reversible:\n            self.enforce_reversible = False\n            if self.encode(self.decode(encoded)) != encoded:\n                raise ValueError('Decoding is not reversible for \"%s\"' % encoded)\n            self.enforce_reversible = True\n\n        return encoded",
        "rewrite": "Here is the revised code:\n\n```\ndef decode(self, encoded):\n    if self.enforce_reversible:\n        decoded = self.decode(encoded)\n        if self.encode(decoded) != encoded:\n            raise ValueError(f'Decoding is not reversible for {encoded}')\n    return decoded\n```"
    },
    {
        "original": "def get_access_flags_string(value):\n    \"\"\"\n    Transform an access flag field to the corresponding string\n\n    :param value: the value of the access flags\n    :type value: int\n\n    :rtype: string\n    \"\"\"\n    flags = []\n    for k, v in ACCESS_FLAGS.items():\n        if (k & value) == k:\n            flags.append(v)\n\n    return \" \".join(flags)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef get_access_flags_string(value):\n    return ' '.join(flag for flag, mask in ACCESS_FLAGS.items() if (mask & value) == mask)\n```"
    },
    {
        "original": "def _check_exclude(self, val):\n        \"\"\"\n        Validate the excluded metrics. Returns the set of excluded params.\n        \"\"\"\n        if val is None:\n            exclude = frozenset()\n        elif isinstance(val, str):\n            exclude = frozenset([val.lower()])\n        else:\n            exclude = frozenset(map(lambda s: s.lower(), val))\n\n        if len(exclude - frozenset(METRICS)) > 0:\n            raise YellowbrickValueError(\n                \"'{}' is not a valid metric to exclude\".format(repr(val))\n            )\n\n        return exclude",
        "rewrite": "Here is the revised code:\n\n```\ndef _check_exclude(self, val):\n    if val is None:\n        return set()\n    elif isinstance(val, str):\n        return {val.lower()}\n    else:\n        return {s.lower() for s in map(str, val)}\n```"
    },
    {
        "original": "def _MakeCacheInvariant(self, urn, age):\n    \"\"\"Returns an invariant key for an AFF4 object.\n\n    The object will be cached based on this key. This function is specifically\n    extracted to ensure that we encapsulate all security critical aspects of the\n    AFF4 object so that objects do not leak across security boundaries.\n\n    Args:\n       urn: The urn of the object.\n       age: The age policy used to build this object. Should be one of\n         ALL_TIMES, NEWEST_TIME or a range.\n\n    Returns:\n       A key into the cache.\n    \"\"\"\n    precondition.AssertType(urn, Text)\n    return \"%s:%s\" % (urn, self.ParseAgeSpecification(age))",
        "rewrite": "Here is the revised code:\n\n```\ndef _MakeCacheInvariant(self, urn: str, age: int) -> str:\n    \"\"\"Returns an invariant key for an AFF4 object.\"\"\"\n    \n    self._AssertType(urn, str)\n    \n    return f\"{urn}:{self.ParseAgeSpecification(age)}\"\n```"
    },
    {
        "original": "def GetSitelinksFromFeed(client, feed):\n  \"\"\"Gets the sitelinks from a feed.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed: the feed used to retrieve sitelinks.\n\n  Returns:\n    A dictionary mapping the feed item ID to SiteLinkFromFeed.\n  \"\"\"\n  # Retrieve the feed's attribute mapping.\n  feed_mappings = GetFeedMapping(client, feed, PLACEHOLDER_TYPE_SITELINKS)\n\n  feed_items = {}\n\n  for feed_item in GetFeedItems(client, feed):\n    site_link_from_feed = {}\n\n    for attribute_value in feed_item['attributeValues']:\n      if attribute_value['feedAttributeId'] in feed_mappings:\n        for field_id in feed_mappings[attribute_value['feedAttributeId']]:\n          if field_id == SITE_LINK_FIELDS['TEXT']:\n            site_link_from_feed['text'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['URL']:\n            site_link_from_feed['url'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['FINAL_URLS']:\n            site_link_from_feed['finalUrls'] = attribute_value['stringValues']\n          elif field_id == SITE_LINK_FIELDS['FINAL_MOBILE_URLS']:\n            site_link_from_feed['finalMobileUrls'] = attribute_value[\n                'stringValues']\n          elif field_id == SITE_LINK_FIELDS['TRACKING_URL_TEMPLATE']:\n            site_link_from_feed['trackingUrlTemplate'] = attribute_value[\n                'stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE2']:\n            site_link_from_feed['line2'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE3']:\n            site_link_from_feed['line3'] = attribute_value['stringValue']\n          else:\n            print 'No applicable Site Link Field found for Id: %s' % field_id\n\n    feed_items[feed_item['feedItemId']] = site_link_from_feed\n\n  return feed_items",
        "rewrite": "Here is a revised version of the given code:\n\n```\ndef get_sitelinks_from_feed(client,.feed):\n    \"\"\"Gets the sitelinks from a given AdWords client and Feed.\"\"\"\n    \n    try:\n        # Retrieve the Feed Mapping\n        mappings_dict=client.GetServiceClient().get_service_client().get_service_client().get_service_client().get_service_client()\n        \n        # Initialize an empty dictionary to store sitelink data\n        sitelink_data={}\n\n        # Iterate over each item in the given Feed\n        for item in client.get_service_client().get_service_client().get_service_client():\n            \n            # Initialize an"
    },
    {
        "original": "def plot_entropy(self, tmin, tmax, ntemp, ylim=None, **kwargs):\n        \"\"\"\n        Plots the vibrational entrpy in a temperature range.\n\n        Args:\n            tmin: minimum temperature\n            tmax: maximum temperature\n            ntemp: number of steps\n            ylim: tuple specifying the y-axis limits.\n            kwargs: kwargs passed to the matplotlib function 'plot'.\n        Returns:\n            matplotlib figure\n        \"\"\"\n        temperatures = np.linspace(tmin, tmax, ntemp)\n\n        if self.structure:\n            ylabel = r\"$S$ (J/K/mol)\"\n        else:\n            ylabel = r\"$S$ (J/K/mol-c)\"\n\n        fig = self._plot_thermo(self.dos.entropy, temperatures, ylabel=ylabel, ylim=ylim, **kwargs)\n\n        return fig",
        "rewrite": "Here is the revised code:\n\n```\ndef plot_entropy(self, tmin, tmax, ntemp=None):\n    \"\"\"\n    Plots the vibrational entropy in a temperature range.\n\n    Args:\n      - tmin (float): minimum temperature\n      - tmax (float): maximum temperature\n      - ntemp (int): number of steps\n\n    Returns:\n      - matplotlib figure\n    \"\"\"\n    import numpy as np\n\n    if ntemp is None:\n      raise ValueError(\"ntemp must be specified\")\n\n    temperatures = np.linspace(tmin * 1e3 / 2.0 / 8.3144,\n"
    },
    {
        "original": "def validate(self, body, signature):\n        \"\"\"Check signature.\n\n        https://devdocs.line.me/en/#webhook-authentication\n\n        :param str body: Request body (as text)\n        :param str signature: X-Line-Signature value (as text)\n        :rtype: bool\n        :return: result\n        \"\"\"\n        gen_signature = hmac.new(\n            self.channel_secret,\n            body.encode('utf-8'),\n            hashlib.sha256\n        ).digest()\n\n        return compare_digest(\n                signature.encode('utf-8'), base64.b64encode(gen_signature)\n        )",
        "rewrite": "Here is the revised code:\n\n```\nimport hmac\nimport hashlib\nimport base64\n\ndef validate(self, body, signature):\n    gen_signature = hmac.new(self.channel_secret.encode('utf-8'), body.encode('utf-8'), hashlib.sha256).digest()\n    return compare_digest(signature.encode('utf-8'), base64.b64encode(gen_signature))\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'type') and self.type is not None:\n            _dict['type'] = self.type\n        if hasattr(self, 'results') and self.results is not None:\n            _dict['results'] = [x._to_dict() for x in self.results]\n        if hasattr(self,\n                   'matching_results') and self.matching_results is not None:\n            _dict['matching_results'] = self.matching_results\n        if hasattr(self, 'aggregations') and self.aggregations is not None:\n            _dict['aggregations'] = [x._to_dict() for x in self.aggregations]\n        return _dict",
        "rewrite": "Here's the revised code:\n\n```\ndef to_dict(self):\n    return {\n        'type': getattr(self, 'type', None),\n        'results': [result.to_dict() for result in getattr(self, 'results', [])],\n        'matching_results': getattr(self, 'matching_results', None),\n        'aggregations': [agg.to_dict() for agg in getattr(self, 'aggregations', [])]\n    }\n```"
    },
    {
        "original": "def read_srml_month_from_solardat(station, year, month, filetype='PO'):\n    \"\"\"Request a month of SRML[1] data from solardat and read it into\n    a Dataframe.\n\n    Parameters\n    ----------\n    station: str\n        The name of the SRML station to request.\n    year: int\n        Year to request data for\n    month: int\n        Month to request data for.\n    filetype: string\n        SRML file type to gather. 'RO' and 'PO' are the\n        only minute resolution files.\n\n    Returns\n    -------\n    data: pd.DataFrame\n        One month of data from SRML.\n\n    References\n    ----------\n    [1] University of Oregon Solar Radiation Measurement Laboratory\n        `http://solardat.uoregon.edu/ <http://solardat.uoregon.edu/>`_\n    \"\"\"\n    file_name = \"{station}{filetype}{year:02d}{month:02d}.txt\".format(\n        station=station,\n        filetype=filetype,\n        year=year % 100,\n        month=month)\n    url = \"http://solardat.uoregon.edu/download/Archive/\"\n    data = read_srml(url + file_name)\n    return data",
        "rewrite": "Here is the revised code:\n\n```\nimport pandas as pd\n\ndef read_srml_month_from_solardat(station, year, month, filetype='PO'):\n\t\"\"\"Request a month of SRML[1] data from solardat and read it into a Dataframe.\n\n\tParameters:\n\t----------\n\tstation : str\n\t\tThe name of the SRML station to request.\n\tyear : int\n\t.Year to request data for.\n\tmonth : int\n\t.Month to request data for.\n\tfiletype : string (default='PO')\n\t\tSRML file type to gather. 'RO' and 'PO' are the"
    },
    {
        "original": "def get_d(slab):\n    \"\"\"\n    Determine the distance of space between\n    each layer of atoms along c\n    \"\"\"\n    sorted_sites = sorted(slab, key=lambda site: site.frac_coords[2])\n    for i, site in enumerate(sorted_sites):\n        if \"%.6f\" % (site.frac_coords[2]) == \\\n                        \"%.6f\" % (sorted_sites[i + 1].frac_coords[2]):\n            continue\n        else:\n            d = abs(site.frac_coords[2] - \\\n                    sorted_sites[i + 1].frac_coords[2])\n            break\n    return slab.lattice.get_cartesian_coords([0, 0, d])[2]",
        "rewrite": "Here is the revised code:\n\n```\ndef get_d(slab):\n    return slab.lattice.get_cartesian_coords([0, 0, min((max(site.frac_coords[2] for site in sorted(sites := list(set((site for site in slab)))))) - min((site.frac_cols := [s for s in set((s for s in slab))], key=lambda x: x.fractional_coordinates()[2]))))]))\n```"
    },
    {
        "original": "def comment_lines(lines, prefix):\n    \"\"\"Return commented lines\"\"\"\n    if not prefix:\n        return lines\n    return [prefix + ' ' + line if line else prefix for line in lines]",
        "rewrite": "Here is the revised code:\n\n```\ndef comment_lines(lines, prefix):\n    \"\"\"Return commented lines\"\"\"\n    if not prefix:\n        return [line for line in lines]\n    return [f\"{prefix} {line}\" if line else f\"{prefix}\" for line in lines]\n```"
    },
    {
        "original": "def reinstall_ruby(ruby, runas=None, env=None):\n    \"\"\"\n    Reinstall a ruby implementation\n\n    ruby\n        The version of ruby to reinstall\n\n    runas\n        The user under which to run rvm. If not specified, then rvm will be run\n        as the user under which Salt is running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rvm.reinstall_ruby 1.9.3-p385\n    \"\"\"\n    return _rvm(['reinstall', ruby], runas=runas, env=env)",
        "rewrite": "Here is the revised code:\n```\ndef reinstall_ruby(ruby, runas=None, env=None):\n    return _rvm(['reinstall', ruby], runas=runas, env=env)\n```"
    },
    {
        "original": "def StreamFilePath(self, filepath, offset=0, amount=None):\n    \"\"\"Streams chunks of a file located at given path starting at given offset.\n\n    Args:\n      filepath: A path to the file to stream.\n      offset: An integer offset at which the file stream should start on.\n      amount: An upper bound on number of bytes to read.\n\n    Yields:\n      `Chunk` instances.\n    \"\"\"\n    with open(filepath, \"rb\") as filedesc:\n      for chunk in self.StreamFile(filedesc, offset=offset, amount=amount):\n        yield chunk",
        "rewrite": "Here is the revised code:\n\n```\ndef StreamFilePath(self, filepath, offset=0, amount=None):\n    with open(filepath, \"rb\") as filedesc:\n        for chunk in self._StreamFile(filedesc.fileobj.seek(offset), max(1 << 20 if amount is None else min(amount + 1 << 20 - 1), (filedesc.size - offset) if filedesc.size > 0 else float('inf'))):\n            yield Chunk(chunk)\n```"
    },
    {
        "original": "def snapshot(self, wiki=False, streamed=False, action=None,\n                 chunk_size=1024, **kwargs):\n        \"\"\"Return a snapshot of the repository.\n\n        Args:\n            wiki (bool): If True return the wiki repository\n            streamed (bool): If True the data will be processed by chunks of\n                `chunk_size` and each chunk is passed to `action` for\n                treatment.\n            action (callable): Callable responsible of dealing with chunk of\n                data\n            chunk_size (int): Size of each chunk\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the content could not be retrieved\n\n        Returns:\n            str: The uncompressed tar archive of the repository\n        \"\"\"\n        path = '/projects/%s/snapshot' % self.get_id()\n        result = self.manager.gitlab.http_get(path, streamed=streamed,\n                                              raw=True, **kwargs)\n        return utils.response_content(result, streamed, action, chunk_size)",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef snapshot(self, wiki=False, streamed=False, action=None,\n                 chunk_size=1024, **kwargs):\n    path = f\"/projects/{self.get_id()}/snapshot\"\n    response = self.manager.gitlab.http_get(path=path,\n                                           streamed=streamed,\n                                           raw=True,\n                                           **kwargs)\n    if response.status_code == 200:\n        if not streamed:\n            return response.content.decode('utf-8')\n        else:\n            return utils.response_content(response.raw)\n    elif response.status_code == 401 or response.status_code == 403:\n        raise exceptions"
    },
    {
        "original": "def _prepare_for_training(self, job_name=None):\n        \"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\n\n        Args:\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        super(Framework, self)._prepare_for_training(job_name=job_name)\n\n        # validate source dir will raise a ValueError if there is something wrong with the\n        # source directory. We are intentionally not handling it because this is a critical error.\n        if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n            validate_source_dir(self.entry_point, self.source_dir)\n\n        # if we are in local mode with local_code=True. We want the container to just\n        # mount the source dir instead of uploading to S3.\n        local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n        if self.sagemaker_session.local_mode and local_code:\n            # if there is no source dir, use the directory containing the entry point.\n            if self.source_dir is None:\n                self.source_dir = os.path.dirname(self.entry_point)\n            self.entry_point = os.path.basename(self.entry_point)\n\n            code_dir = 'file://' + self.source_dir\n            script = self.entry_point\n        else:\n            self.uploaded_code = self._stage_user_code_in_s3()\n            code_dir = self.uploaded_code.s3_prefix\n            script = self.uploaded_code.script_name\n\n        # Modify hyperparameters in-place to point to the right code directory and script URIs\n        self._hyperparameters[DIR_PARAM_NAME] = code_dir\n        self._hyperparameters[SCRIPT_PARAM_NAME] = script\n        self._hyperparameters[CLOUDWATCH_METRICS_PARAM_NAME] = self.enable_cloudwatch_metrics\n        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level\n        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name\n        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name",
        "rewrite": "Here is the revised code:\n\n```\ndef _prepare_for_training(self, job_name=None):\n    super(Framework, self)._prepare_for_training(job_name=job_name)\n\n    if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n        raise ValueError(f\"Invalid source directory: {self.source_dir}\")\n\n    local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n    if self.sagemaker_session.local_mode and local_code:\n        if self.source_dir is None:\n            self.source_dir = os.path.dirname(self.entry_point)\n        self.entry_point = os.path.basename(self"
    },
    {
        "original": "def _finalize_requires(self):\n        \"\"\"\n        Set `metadata.python_requires` and fix environment markers\n        in `install_requires` and `extras_require`.\n        \"\"\"\n        if getattr(self, 'python_requires', None):\n            self.metadata.python_requires = self.python_requires\n\n        if getattr(self, 'extras_require', None):\n            for extra in self.extras_require.keys():\n                # Since this gets called multiple times at points where the\n                # keys have become 'converted' extras, ensure that we are only\n                # truly adding extras we haven't seen before here.\n                extra = extra.split(':')[0]\n                if extra:\n                    self.metadata.provides_extras.add(extra)\n\n        self._convert_extras_requirements()\n        self._move_install_requirements_markers()",
        "rewrite": "Here is the revised code:\n\n```\ndef _finalize_requires(self):\n    if getattr(self, 'python_requires', None):\n        self.metadata.python_requires = self.python_requires\n\n    if getattr(self, 'extras_require', None):\n        for extra, requires in self.extras_require.items():\n            extra = extra.split(':')[0]\n            if extra:\n                self.metadata.provides_extras.add(extra)\n\n    self._convert_extras_requirements()\n    self._move_install_requirements_markers()\n```"
    },
    {
        "original": "def from_dict(cls, d):\n        \"\"\"\n        Reconstructs the SimplestChemenvStrategy object from a dict representation of the SimplestChemenvStrategy object\n        created using the as_dict method.\n        :param d: dict representation of the SimplestChemenvStrategy object\n        :return: StructureEnvironments object\n        \"\"\"\n        return cls(distance_cutoff=d[\"distance_cutoff\"], angle_cutoff=d[\"angle_cutoff\"],\n                   additional_condition=d[\"additional_condition\"],\n                   continuous_symmetry_measure_cutoff=d[\"continuous_symmetry_measure_cutoff\"],\n                   symmetry_measure_type=d[\"symmetry_measure_type\"])",
        "rewrite": "Here is the revised code:\n\n```\ndef from_dict(cls, d):\n    return cls(**{k: v for k, v in zip(['distance_cutoff', 'angle_cutoff', 'additional_condition', 'continuous_symmetry_measure_cutoff', 'symmetry_measure_type'], [d.get(k) for k in ['distance_cutoff', 'angle_cutoff', 'additional_condition', 'continuous_symmetry_measure_crossentropymeasuretype']])})\n```"
    },
    {
        "original": "def _retrieve_config_xml(config_xml, saltenv):\n    \"\"\"\n    Helper to cache the config XML and raise a CommandExecutionError if we fail\n    to do so. If we successfully cache the file, return the cached path.\n    \"\"\"\n    ret = __salt__['cp.cache_file'](config_xml, saltenv)\n\n    if not ret:\n        raise CommandExecutionError('Failed to retrieve {0}'.format(config_xml))\n\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef _retrieve_config_xml(config_xml, saltenv):\n    ret = __salt__['cp.cache_file'](config_xml, saltenv)\n    if not ret:\n        raise CommandExecutionError(f'Failed to retrieve {config_xml}')\n    return ret\n```"
    },
    {
        "original": "def absent(\n        name,\n        region=None,\n        key=None,\n        keyid=None,\n        profile=None,\n        unsubscribe=False):\n    \"\"\"\n    Ensure the named sns topic is deleted.\n\n    name\n        Name of the SNS topic.\n\n    region\n        Region to connect to.\n\n    key\n        Secret key to be used.\n\n    keyid\n        Access key to be used.\n\n    profile\n        A dict with region, key and keyid, or a pillar key (string)\n        that contains a dict with region, key and keyid.\n\n    unsubscribe\n        If True, unsubscribe all subcriptions to the SNS topic before\n        deleting the SNS topic\n\n        .. versionadded:: 2016.11.0\n    \"\"\"\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n\n    is_present = __salt__['boto_sns.exists'](\n        name, region=region, key=key, keyid=keyid, profile=profile\n    )\n\n    if is_present:\n        subscriptions = __salt__['boto_sns.get_all_subscriptions_by_topic'](\n            name, region=region, key=key, keyid=keyid, profile=profile\n        ) if unsubscribe else []\n        failed_unsubscribe_subscriptions = []\n\n        if __opts__.get('test'):\n            ret['comment'] = (\n                'AWS SNS topic {0} is set to be removed.  '\n                '{1} subscription(s) will be removed.'.format(name, len(subscriptions))\n            )\n            ret['result'] = None\n            return ret\n\n        for subscription in subscriptions:\n            unsubscribed = __salt__['boto_sns.unsubscribe'](\n                name, subscription['SubscriptionArn'], region=region,\n                key=key, keyid=keyid, profile=profile\n            )\n            if unsubscribed is False:\n                failed_unsubscribe_subscriptions.append(subscription)\n\n        deleted = __salt__['boto_sns.delete'](\n            name, region=region, key=key, keyid=keyid, profile=profile)\n        if deleted:\n            ret['comment'] = 'AWS SNS topic {0} deleted.'.format(name)\n            ret['changes']['new'] = None\n            if unsubscribe is False:\n                ret['changes']['old'] = {'topic': name}\n            else:\n                ret['changes']['old'] = {'topic': name, 'subscriptions': subscriptions}\n                if failed_unsubscribe_subscriptions:\n                    ret['changes']['new'] = {'subscriptions': failed_unsubscribe_subscriptions}\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Failed to delete {0} AWS SNS topic.'.format(name)\n    else:\n        ret['comment'] = 'AWS SNS topic {0} does not exist.'.format(name)\n\n    return ret",
        "rewrite": "Here's my revised version of your function:\n\n```Python \ndef absent(name):\n\t\"\"\"\n\tEnsure the named sns topic is deleted.\n\t\n\tname: Name of the SNS topic.\n\t\n\t.. versionadded:: 2016.11.0\n\t\n\treturns: A dictionary containing information about whether the operation was successful and any changes made.\n\t\"\"\"\n\tret={'name:name,'result':'True','comment':'','changes:{}}\n\n\tis_present=__salt__[\"boto_sns.exists\"](name)\n\n\tif not isinstance(is_present,bool):\n\t\traise SaltInvocationError(\"Invalid response from boto_sns.exists\")\n\n\tif not is_present :\n\t\tret[\""
    },
    {
        "original": "def diff(self, container):\n        \"\"\"\n        Inspect changes on a container's filesystem.\n\n        Args:\n            container (str): The container to diff\n\n        Returns:\n            (str)\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        return self._result(\n            self._get(self._url(\"/containers/{0}/changes\", container)), True\n        )",
        "rewrite": "Here is the revised code:\n\n```\ndef diff(self, container):\n    return self._result(self._get(f\"{self._url('/containers/{container}/changes')}\", True))\n```"
    },
    {
        "original": "def _compute_bbox(self, fig, kw):\n        \"\"\"\n        Compute the tight bounding box for each figure once, reducing\n        number of required canvas draw calls from N*2 to N+1 as a\n        function of the number of frames.\n\n        Tight bounding box computing code here mirrors:\n        matplotlib.backend_bases.FigureCanvasBase.print_figure\n        as it hasn't been factored out as a function.\n        \"\"\"\n        fig_id = id(fig)\n        if kw['bbox_inches'] == 'tight':\n            if not fig_id in MPLRenderer.drawn:\n                fig.set_dpi(self.dpi)\n                fig.canvas.draw()\n                extra_artists = kw.pop(\"bbox_extra_artists\", [])\n                pad = mpl.rcParams['savefig.pad_inches']\n                bbox_inches = get_tight_bbox(fig, extra_artists, pad=pad)\n                MPLRenderer.drawn[fig_id] = bbox_inches\n                kw['bbox_inches'] = bbox_inches\n            else:\n                kw['bbox_inches'] = MPLRenderer.drawn[fig_id]\n        return kw",
        "rewrite": "```\ndef _compute_bbox(self, fig, kwargs):\n    \"\"\"\n    Compute the tight bounding box for each figure once,\n    reducing number of required canvas draw calls from N*2 to N+1 as a function of the number of frames.\n    \"\"\"\n    fig_id = id(fig)\n    if kwargs.get('bbox_inches') == 'tight':\n        if not (fig_id in self._drawn or self._drawn is None):\n            self._drawn = {}  # Initialize _drawn dictionary if it's None\n            self._drawn[fig_id] = {}\n            try:\n                import matplotlib.pyplot as"
    },
    {
        "original": "def fit(self, X, y):\n        \"\"\"Learn vocabulary from training set.\n\n        Args:\n            X : iterable. An iterable which yields either str, unicode or file objects.\n\n        Returns:\n            self : IndexTransformer.\n        \"\"\"\n        self._word_vocab.add_documents(X)\n        self._label_vocab.add_documents(y)\n        if self._use_char:\n            for doc in X:\n                self._char_vocab.add_documents(doc)\n\n        self._word_vocab.build()\n        self._char_vocab.build()\n        self._label_vocab.build()\n\n        return self",
        "rewrite": "Here is the revised code:\n\n```\ndef fit(self, X, y):\n    \"\"\"Learn vocabulary from training set.\n\n    Args:\n      X : iterable. An iterable which yields either str, unicode or file objects.\n      y : iterable. Target values to be predicted (e.g., labels).\n\n    Returns:\n      IndexTransformer: The instance itself.\n    \"\"\"\n    import re\n    from collections import Counter\n    from typing import Iterable\n\n    if not isinstance(X, Iterable):\n      raise ValueError(\"X must be an iterable\")\n\n    word_counts = Counter()\n    label_counts = Counter()\n\n    for doc in X:\n      words ="
    },
    {
        "original": "def get_sticky(self, subreddit, bottom=False):\n        \"\"\"Return a Submission object for the sticky of the subreddit.\n\n        :param bottom: Get the top or bottom sticky. If the subreddit has only\n            a single sticky, it is considered the top one.\n\n        \"\"\"\n        url = self.config['sticky'].format(subreddit=six.text_type(subreddit))\n        param = {'num': 2} if bottom else None\n        return objects.Submission.from_json(self.request_json(url,\n                                                              params=param))",
        "rewrite": "Here is the revised code:\n\n```\ndef get_sticky(self, subreddit, bottom=False):\n    url = self.config['sticky'].format(subreddit=str(subreddit))\n    param = {'num': '2' if bottom else '1'}\n    return objects.Submission.from_json(self.request_json(url, params=param))\n```"
    },
    {
        "original": "def d3logpdf_dlink3(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Third order derivative log-likelihood function at y given link(f) w.r.t link(f)\n\n        .. math::\n            \\\\frac{d^{3} \\\\ln p(y_{i}|\\lambda(f_{i}))}{d^{3}\\\\lambda(f)} = \\\\frac{-2(v+1)((y_{i} - \\lambda(f_{i}))^3 - 3(y_{i} - \\lambda(f_{i})) \\\\sigma^{2} v))}{((y_{i} - \\lambda(f_{i})) + \\\\sigma^{2} v)^3}\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata which is not used in student t distribution\n        :returns: third derivative of likelihood evaluated at points f\n        :rtype: Nx1 array\n        \"\"\"\n        e = y - inv_link_f\n        d3lik_dlink3 = ( -(2*(self.v + 1)*(-e)*(e**2 - 3*self.v*self.sigma2)) /\n                       ((e**2 + self.sigma2*self.v)**3)\n                    )\n        return d3lik_dlink3",
        "rewrite": "Here is the revised code:\n\n```\ndef d3logpdf_dlink3(self, inv_link_f, y, Y_metadata=None):\n    e = y - inv_link_f\n    d4v4v = self.v * (self.v + 1) * (self.v + 2)\n    d6v5v5 = self.sigma2 * (self.sigma2 * self.v ** 4) * (7 * self.v ** 4 + 22 * self.v ** 2)\n    numer = (-8 * e ** 5) / ((e ** 4 + self.sigma2 * self.v"
    },
    {
        "original": "def chunk_from_mem(self, ptr):\n        \"\"\"\n        Given a pointer to a user payload, return the chunk associated with that payload.\n\n        :param ptr: a pointer to the base of a user payload in the heap\n        :returns: the associated heap chunk\n        \"\"\"\n        raise NotImplementedError(\"%s not implemented for %s\" % (self.chunk_from_mem.__func__.__name__,\n                                                                 self.__class__.__name__))",
        "rewrite": "Here is the revised code:\n\n```\ndef chunk_from_mem(self, ptr):\n    raise NotImplementedError(\"chunk_from_mem not implemented for %s\" % self.__class__.__name__)\n```"
    },
    {
        "original": "def _calendar_month_middles(year):\n    \"\"\"List of middle day of each month, used by Linke turbidity lookup\"\"\"\n    # remove mdays[0] since January starts at mdays[1]\n    # make local copy of mdays since we need to change\n    # February for leap years\n    mdays = np.array(calendar.mdays[1:])\n    ydays = 365\n    # handle leap years\n    if calendar.isleap(year):\n        mdays[1] = mdays[1] + 1\n        ydays = 366\n    middles = np.concatenate(\n        [[-calendar.mdays[-1] / 2.0],  # Dec last year\n         np.cumsum(mdays) - np.array(mdays) / 2.,  # this year\n         [ydays + calendar.mdays[1] / 2.0]])  # Jan next year\n    return middles",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nimport calendar\n\ndef _calendar_month_middles(year):\n    start_day_of_year = [i for i in range(12)]\n    \n    if calendar.isleap(year):\n        start_day_of_year.insert(0, -31)\n    \n    return np.concatenate([np.cumsum(start_day_of_year) - (start_day_of_year + [len(start_day_of_year)]) // 2])\n```"
    },
    {
        "original": "def drop_neutral_categories_from_corpus(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tPriorFactory\n\t\t\"\"\"\n\t\tneutral_categories = self._get_neutral_categories()\n\t\tself.term_doc_mat = self.term_doc_mat.remove_categories(neutral_categories)\n\t\tself._reindex_priors()\n\t\treturn self",
        "rewrite": "Here is the revised code:\n\n```\ndef drop_neutral_categories_from_corpus(self) -> 'PriorFactory':\n    neutral_categories = self._get_neural_categories()\n    self.term_doc_mat = self.term_doc_mat.drop(categories=neutral_categories, axis=1)\n    self._reindex_priors()\n    return self\n```"
    },
    {
        "original": "def subsample(self, proposals, targets):\n        \"\"\"\n        This method performs the positive/negative sampling, and return\n        the sampled proposals.\n        Note: this function keeps a state.\n\n        Arguments:\n            proposals (list[BoxList])\n            targets (list[BoxList])\n        \"\"\"\n\n        labels, regression_targets = self.prepare_targets(proposals, targets)\n        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n\n        proposals = list(proposals)\n        # add corresponding label and regression_targets information to the bounding boxes\n        for labels_per_image, regression_targets_per_image, proposals_per_image in zip(\n            labels, regression_targets, proposals\n        ):\n            proposals_per_image.add_field(\"labels\", labels_per_image)\n            proposals_per_image.add_field(\n                \"regression_targets\", regression_targets_per_image\n            )\n\n        # distributed sampled proposals, that were obtained on all feature maps\n        # concatenated via the fg_bg_sampler, into individual feature map levels\n        for img_idx, (pos_inds_img, neg_inds_img) in enumerate(\n            zip(sampled_pos_inds, sampled_neg_inds)\n        ):\n            img_sampled_inds = torch.nonzero(pos_inds_img | neg_inds_img).squeeze(1)\n            proposals_per_image = proposals[img_idx][img_sampled_inds]\n            proposals[img_idx] = proposals_per_image\n\n        self._proposals = proposals\n        return proposals",
        "rewrite": "Here is the revised code:\n\n```\ndef subsample(self, propsalsssssssossssssossssssossssssosssooosssooosssooosssoooosoooososoooooooososooooooosoososooooooooooooooooooooooooooooooooooooooooooo:\n    \"\"\"\n    This method performs positive/negative sampling and returns \n    the sampled proposal.\n    \n    Arguments:\n    propsalsssssssossssssossssssosSSSSSSSsSSSsSSSsSSSsSSSs SS SS SS S S S S S SS SS SS \n    \"\"\"\n    \n    labels , regres"
    },
    {
        "original": "def get(self, element, selected=False):\n        \"\"\"\n        Returns the curses attribute code for the given element.\n        \"\"\"\n        if self._attribute_map is None:\n            raise RuntimeError('Attempted to access theme attribute before '\n                               'calling initialize_curses_theme()')\n\n        if selected or self._selected:\n            element = '@{0}'.format(element)\n\n        return self._attribute_map[element]",
        "rewrite": "Here is the revised code:\n\n```\ndef get(self, element, selected=False):\n    if self._attribute_map is None:\n        raise RuntimeError('Attempted to access theme attribute before calling initialize_curses_theme()')\n\n    if selected or self._selected:\n        element = f'@{element}'\n\n    return self._attribute_map.get(element)\n```"
    },
    {
        "original": "def mu_so(species, motif, spin_state):\n        \"\"\"\n        Calculates the spin-only magnetic moment for a\n        given species. Only supports transition metals.\n\n        :param species: str or Species\n        :param motif: \"oct\" or \"tet\"\n        :param spin_state: \"high\" or \"low\"\n        :return: spin-only magnetic moment in Bohr magnetons\n        \"\"\"\n        try:\n            sp = get_el_sp(species)\n            n = sp.get_crystal_field_spin(coordination=motif, spin_config=spin_state)\n            # calculation spin-only magnetic moment for this number of unpaired spins\n            return np.sqrt(n * (n + 2))\n        except AttributeError:\n            return None",
        "rewrite": "Here is the revised code:\n\n```\ndef mu_so(species, motif, spin_state):\n    sp = get_el_sp(species)\n    if not isinstance(sp, dict):\n        return None\n    n = sp.get('crystal_field_spin', {'coordination': motif, 'spin_config': spin_state}).get('n')\n    if n is None:\n        return None\n    return np.sqrt(n * (n + 2))\n```"
    },
    {
        "original": "def sulfide_type(structure):\n    \"\"\"\n    Determines if a structure is a sulfide/polysulfide\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        (str) sulfide/polysulfide/sulfate\n    \"\"\"\n    structure = structure.copy()\n    structure.remove_oxidation_states()\n    s = Element(\"S\")\n    comp = structure.composition\n    if comp.is_element or s not in comp:\n        return None\n\n    finder = SpacegroupAnalyzer(structure, symprec=0.1)\n    symm_structure = finder.get_symmetrized_structure()\n    s_sites = [sites[0] for sites in symm_structure.equivalent_sites if\n               sites[0].specie == s]\n\n    def process_site(site):\n\n        # in an exceptionally rare number of structures, the search\n        # radius needs to be increased to find a neighbor atom\n        search_radius = 4\n        neighbors = []\n        while len(neighbors) == 0:\n            neighbors = structure.get_neighbors(site, search_radius)\n            search_radius *= 2\n            if search_radius > max(structure.lattice.abc)*2:\n                break\n\n        neighbors = sorted(neighbors, key=lambda n: n[1])\n        nn, dist = neighbors[0]\n        coord_elements = [site.specie for site, d in neighbors\n                          if d < dist + 0.4][:4]\n        avg_electroneg = np.mean([e.X for e in coord_elements])\n        if avg_electroneg > s.X:\n            return \"sulfate\"\n        elif avg_electroneg == s.X and s in coord_elements:\n            return \"polysulfide\"\n        else:\n            return \"sulfide\"\n\n    types = set([process_site(site) for site in s_sites])\n    if \"sulfate\" in types:\n        return None\n    elif \"polysulfide\" in types:\n        return \"polysulfide\"\n    else:\n        return \"sulfide\"",
        "rewrite": "Here is the revised code:\n\n```python\nimport numpy as np\n\ndef sulfide_type(structure):\n\t\"\"\"\n\tDetermines if a structure is a sulfide/polysulfide\n\t\n\tArgs:\n\t\tstructure (Structure): Input structure.\n\t\n\tReturns:\n\t\tstr: sulfite/sulphite/sulphuric acid/acidic/basic/neutral/unknown/polar/non-polar/amphoteric/amphoteric-weakly-acidic/amphoteric-weakly-basic/\n\t\"\"\"\n\tstructure_copy=structure.copy()\n\tstructure_copy.remove_oxidation_states()\n\ts=Element(\"S\")\n\tcomposition="
    },
    {
        "original": "def get_function(session_factory, name, role, sns_topic, log_groups,\n                 subject=\"Lambda Error\", pattern=\"Traceback\"):\n    \"\"\"Lambda function provisioning.\n\n    Self contained within the component, to allow for easier reuse.\n    \"\"\"\n\n    # Lazy import to avoid runtime dependency\n    from c7n.mu import (\n        LambdaFunction, PythonPackageArchive, CloudWatchLogSubscription)\n\n    config = dict(\n        name=name,\n        handler='logsub.process_log_event',\n        runtime='python2.7',\n        memory_size=512,\n        timeout=15,\n        role=role,\n        description='Custodian Ops Error Notify',\n        events=[\n            CloudWatchLogSubscription(\n                session_factory, log_groups, pattern)])\n\n    archive = PythonPackageArchive()\n    archive.add_py_file(__file__)\n    archive.add_contents(\n        'config.json', json.dumps({\n            'topic': sns_topic,\n            'subject': subject\n        }))\n    archive.close()\n\n    return LambdaFunction(config, archive)",
        "rewrite": "Here is the revised code:\n\n```\ndef get_function(session_factory, name, role, sns_topic, log_groups, subject=\"Lambda Error\", pattern=\"Traceback\"):\n    from c7n.mu import LambdaFunction, PythonPackageArchive, CloudWatchLogSubscription\n\n    config = {\n        'name': name,\n        'handler': 'logsub.process_log_event',\n        'runtime': 'python2.7',\n        'memory_size': 512,\n        'timeout': 15,\n        'role': role,\n        'description': 'Custodian Ops Error Notify',\n        'events': [\n            CloudWatchLogSubscription(session"
    },
    {
        "original": "def _extract_from(raw_json, pandas_options=None):\n    \"\"\"Extract tables from json.\n\n    Args:\n        raw_json (list):\n            Decoded list from tabula-java JSON.\n        pandas_options (dict optional):\n            pandas options for `pd.DataFrame()`\n    \"\"\"\n\n    data_frames = []\n    if pandas_options is None:\n        pandas_options = {}\n\n    columns = pandas_options.pop('columns', None)\n    columns, header_line_number = _convert_pandas_csv_options(pandas_options, columns)\n\n    for table in raw_json:\n        list_data = [[np.nan if not e['text'] else e['text'] for e in row] for row in table['data']]\n        _columns = columns\n\n        if isinstance(header_line_number, int) and not columns:\n            _columns = list_data.pop(header_line_number)\n            _columns = ['' if e is np.nan else e for e in _columns]\n\n        data_frames.append(pd.DataFrame(data=list_data, columns=_columns, **pandas_options))\n\n    return data_frames",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\nimport pandas as pd\n\ndef _extract_from(raw_json, pandas_options=None):\n    data_frames = []\n    \n    if pandasOptions is None:\n        pandsoptions={}\n\n    columns=pandsoptions.get('columns')\n    \n     header_line_number=pandsoptions.get('header')\n\n     for table in raw_json:\n         list_data=[[np.nan if not cell.get('text') else cell.get('text')  for cell in row]for row in table['data']]\n         cols=cols\n        \n         if isinstance(header_line_number,int) and not cols :\n            "
    },
    {
        "original": "def verifyChain(self, anchors, untrusted=None):\n        \"\"\"\n        Perform verification of certificate chains for that certificate.\n        A list of anchors is required. The certificates in the optional\n        untrusted list may be used as additional elements to the final chain.\n        On par with chain instantiation, only one chain constructed with the\n        untrusted candidates will be retained. Eventually, dates are checked.\n        \"\"\"\n        untrusted = untrusted or []\n        for a in anchors:\n            chain = Chain(self + untrusted, a)\n            if len(chain) == 1:             # anchor only\n                continue\n            # check that the chain does not exclusively rely on untrusted\n            if any(c in chain[1:] for c in self):\n                for c in chain:\n                    if c.remainingDays() < 0:\n                        break\n                if c is chain[-1]:      # we got to the end of the chain\n                    return chain\n        return None",
        "rewrite": "Here is the revised code:\n\n```\ndef verifyChain(self, anchors, untrusted=None):\n    untrusted = untrusted or []\n    for a in anchors:\n        chain = Chain(self + untrusted, a)\n        if len(chain) == 1:\n            continue\n        if not any(c in chain[1:] for c in self):\n            for c in chain:\n                if c.remainingDays() < 0:\n                    break\n            if c is chain[-1]:\n                return chain\n    return None\n```"
    },
    {
        "original": "def make_linear_workflow(*tasks, **kwargs):\r\n        \"\"\"Factory method for creating linear workflows.\r\n\r\n        :param tasks: EOTask's t1,t2,...,tk with dependencies t1->t2->...->tk\r\n        :param kwargs: Optional keyword arguments (such as workflow name) forwarded to the constructor\r\n        :return: A new EO workflow instance\r\n        :rtype: EOWorkflow\r\n        \"\"\"\r\n        warnings.warn(\"Method 'make_linear_workflow' will soon be removed. Use LinearWorkflow class instead\",\r\n                      DeprecationWarning, stacklevel=2)\r\n\r\n        return LinearWorkflow(*tasks, **kwargs)",
        "rewrite": "Here is the revised code:\n\n```\ndef make_linear_workflow(*tasks, **kwargs):\n    warnings.warn(\"Method 'make_linear_workflow' will soon be removed. Use LinearWorkflow class instead\", DeprecationWarning, stacklevel=2)\n    return LinearWorkflow(*tasks, **kwargs)\n```"
    },
    {
        "original": "def add_pool_member(hostname, username, password, name, member):\n    \"\"\"\n    A function to connect to a bigip device and add a new member to an existing pool.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    name\n        The name of the pool to modify\n    member\n        The member to add to the pool\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': member\n        }\n        )\n\n    #is this pool member currently configured?\n    existing_pool = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n    if existing_pool['code'] == 200:\n\n        # for some reason iControl REST doesn't support listing a single pool member.\n        # the response from GET for listing a member will return 200 even if it doesn't exists.\n        # because of this we have to do some rather \"unnecessary\" searching within a pool.\n\n        #what are the current members?\n        current_members = existing_pool['content']['membersReference']['items']\n\n        #loop through them\n        exists = False\n        for current_member in current_members:\n            if current_member['name'] == member['name']:\n                exists = True\n                break\n\n        if exists:\n            ret['result'] = True\n            ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = {}\n        else:\n            new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)\n\n            if new_member['code'] == 200:\n                ret['result'] = True\n                ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])\n                ret['changes']['old'] = {}\n\n                #look up the member again...\n                pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n                if pool_listing['code'] != 200:\n                    ret = _load_result(new_member, ret)\n                    return ret\n\n                members = pool_listing['content']['membersReference']['items']\n                #loop through them\n                for current_member in members:\n                    if current_member['name'] == member['name']:\n                        added_member = current_member\n                        break\n\n                ret['changes']['new'] = added_member\n\n            # member wasn't added\n            else:\n                ret = _load_result(new_member, ret)\n\n    #pool does not exists\n    elif existing_pool['code'] == 404:\n        ret['comment'] = 'A pool with this name was not found.'\n    else:\n        ret = _load_result(existing_pool, ret)\n\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef add_pool_member(hostname, username, password, name, member):\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': [member]\n        })\n\n    existing_pool = __salt__['bigip.list_pool'](hostname, username, password, name)\n    if existing_pool['code'] == 200:\n"
    },
    {
        "original": "def require(self, key: str) -> str:\n        \"\"\"\n        Returns a configuration value by its given key.  If it doesn't exist, an error is thrown.\n\n        :param str key: The requested configuration key.\n        :return: The configuration key's value.\n        :rtype: str\n        :raises ConfigMissingError: The configuration value did not exist.\n        \"\"\"\n        v = self.get(key)\n        if v is None:\n            raise ConfigMissingError(self.full_key(key))\n        return v",
        "rewrite": "Here is the revised code:\n\n```\ndef require(self, key: str) -> str:\n    return self.get(key) or (raise ConfigMissingError(self.full_key(key)))\n```"
    },
    {
        "original": "def _append_pairs(new_pairs):\n    \"\"\" Load the pairlist, add new stuff, save it out \"\"\"\n    desired_pairs = restore_pairs() or []\n    desired_pairs += new_pairs\n    print(\"Adding {} new pairs, queue has {} pairs\".format(len(new_pairs), len(desired_pairs)))\n    save_pairs(desired_pairs)",
        "rewrite": "Here is the revised code:\n\n```\ndef _append_pairs(newpairs):\n    desiredpairs = restorepairs() or []\n    desiredpairs.extend(newpairs)\n    print(f\"Adding {len(newpairs)} new pairs, queue has {len(desiredpairs)} pairs\")\n    savepairs(desiredpairs)\n```"
    },
    {
        "original": "def _add_hotkey_step(handler, combinations, suppress):\n    \"\"\"\n    Hooks a single-step hotkey (e.g. 'shift+a').\n    \"\"\"\n    container = _listener.blocking_hotkeys if suppress else _listener.nonblocking_hotkeys\n\n    # Register the scan codes of every possible combination of\n    # modfiier + main key. Modifiers have to be registered in \n    # filtered_modifiers too, so suppression and replaying can work.\n    for scan_codes in combinations:\n        for scan_code in scan_codes:\n            if is_modifier(scan_code):\n                _listener.filtered_modifiers[scan_code] += 1\n        container[scan_codes].append(handler)\n\n    def remove():\n        for scan_codes in combinations:\n            for scan_code in scan_codes:\n                if is_modifier(scan_code):\n                    _listener.filtered_modifiers[scan_code] -= 1\n            container[scan_codes].remove(handler)\n    return remove",
        "rewrite": "Here is the revised code:\n\n```\ndef add_hotkey_step(handler, combinations, suppress):\n    \"\"\"Hooks a single-step hotkey (e.g. 'shift+a').\"\"\"\n    \n    container = (_listener.blocking_hotkeys if suppress else _listener.nonblocking_hotkeys)\n    \n    for modifier_scancodes, main_scancodes in combinations:\n        for scancode in modifier_scancodes + main_scancodes:\n            if is_modifier(scancode):\n                _listener.filtered_modifiers[scancode] = (_listener.filtered_modifiers.get(scancode, 0) + 1)\n        \n        try:\n            container[tuple"
    },
    {
        "original": "def set_base_prompt(\n        self, pri_prompt_terminator=\"$\", alt_prompt_terminator=\"#\", delay_factor=1\n    ):\n        \"\"\"Determine base prompt.\"\"\"\n        return super(DellIsilonSSH, self).set_base_prompt(\n            pri_prompt_terminator=pri_prompt_terminator,\n            alt_prompt_terminator=alt_prompt_terminator,\n            delay_factor=delay_factor,\n        )",
        "rewrite": "Here is the revised code:\n\n```\ndef set_base_prompt(self, pri Prompt Terminator=\"$\", alt Prompt Terminator=\"#\", delay Factor=1):\n    return super().set_base.prompt(pri Prompt Terminator=pri Prompt Terminator, alt Prompt Terminator=alt Prompt Terminator, delay Factor=delay Factor)\n```"
    },
    {
        "original": "def store(self, thing):\n        \"\"\"\n        Simple persistence method\n        \"\"\"\n        to_store = {'field1': thing.field1,\n                    'date_field': thing.date_field,\n                    }\n        to_store['stuff'] = Binary(cPickle.dumps(thing.stuff))\n        # Respect any soft-quota on write - raises if stats().totals.size > quota \n        self._arctic_lib.check_quota()\n        self._collection.insert_one(to_store)",
        "rewrite": "Here is the revised code:\n\n```\ndef store(self, thing):\n    to_store = {'field1': thing.field1, 'date_field': thing.date_field}\n    to_store['stuff'] = Binary(pickle.dumps(thing.stuff))\n    self._arctic_lib.check_quota()\n    self._collection.insert_one(to_store)\n```"
    },
    {
        "original": "def splits(cls, fields, root=\".data\", train=\"train.txt\",\n               test=\"test.txt\", validation_frac=0.1, **kwargs):\n        \"\"\"Downloads and loads the CoNLL 2000 Chunking dataset.\n        NOTE: There is only a train and test dataset so we use\n              10% of the train set as validation\n        \"\"\"\n\n        train, test = super(CoNLL2000Chunking, cls).splits(\n            fields=fields, root=root, train=train,\n            test=test, separator=' ', **kwargs)\n\n        # HACK: Saving the sort key function as the split() call removes it\n        sort_key = train.sort_key\n\n        # Now split the train set\n        # Force a random seed to make the split deterministic\n        random.seed(0)\n        train, val = train.split(1 - validation_frac, random_state=random.getstate())\n        # Reset the seed\n        random.seed()\n\n        # HACK: Set the sort key\n        train.sort_key = sort_key\n        val.sort_key = sort_key\n\n        return train, val, test",
        "rewrite": "Here is my revised version of your provided Python code:\n\n```Python\nfrom torch.utils.data.dataset import Dataset \nimport torch.utils.data as data_utils \nimport numpy as np \n\nclass CoNLL2000ChunkingDataset(Dataset):\n    def __init__(self,\n                 fields,\n                 root        ='.data',\n                 target_path='train.pt',\n                 force=False):\n\n    @classmethod  \n    def splits(cls).\n\ndef splits(cls,CISION):\n    lookup_fields=['chunk']\n\n# Hack because provider>/split by default uses python.network_split like in HDF5 backend does.\n sleep=lambda self:\n\ncompute_nlp_data/set/nLi40200T"
    },
    {
        "original": "def UQRatio(s1, s2, full_process=True):\n    \"\"\"\n    Unicode quick ratio\n\n    Calls QRatio with force_ascii set to False\n\n    :param s1:\n    :param s2:\n    :return: similarity ratio\n    \"\"\"\n    return QRatio(s1, s2, force_ascii=False, full_process=full_process)",
        "rewrite": "```\ndef UQRatio(s1, s2, full_process=True):\n    \"\"\"Unicode quick ratio\"\"\"\n    return Q Ratio(s1, s2, force_ascii=False, full_process=full_process)\n```"
    }
]