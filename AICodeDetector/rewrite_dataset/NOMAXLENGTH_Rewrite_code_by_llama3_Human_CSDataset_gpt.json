[
    {
        "original": "def add_run(self, text=None, style=None):\n        \"\"\"\n        Append a run to this paragraph containing *text* and having character\n        style identified by style ID *style*. *text* can contain tab\n        (``\\\\t``) characters, which are converted to the appropriate XML form\n        for a tab. *text* can also include newline (``\\\\n``) or carriage\n        return (``\\\\r``) characters, each of which is converted to a line\n        break.\n        \"\"\"\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run",
        "rewrite": "Here is the revised code:\n\n```\ndef add_run(self, text=None, style=None):\n    r = self._p.add_r()\n    run = Run(r, self)\n    if text is not None:\n        run.text = text.replace('\\\\t', '\\t').replace('\\\\n', '\\n').replace('\\\\r', '\\n')\n    if style is not None:\n        run.style = style\n    return run\n```"
    },
    {
        "original": "def _read_execute_info(path, parents):\n  \"\"\"Read the ExecuteInfo.txt file and return the base directory.\"\"\"\n  path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n  if os.path.exists(path):\n    with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n      for line in f:\n        parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n        if len(parts) == 2 and parts[0] == \"executable\":\n          exec_path = parts[1].replace(\"\\\\\", \"/\")  # For windows compatibility.\n          for _ in range(parents):\n            exec_path = os.path.dirname(exec_path)\n          return exec_path",
        "rewrite": "Here is the revised code:\n\n```\nimport os\n\ndef read_execute_info(path, parents):\n    path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n    if os.path.exists(path):\n        with open(path, \"rb\") as f:\n            for line in f:\n                parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n                if len(parts) == 2 and parts[0] == \"executable\":\n                    exec_path"
    },
    {
        "original": "def _modify_eni_properties(eni_id, properties=None, vm_=None):\n    \"\"\"\n    Change properties of the interface\n    with id eni_id to the values in properties dict\n    \"\"\"\n    if not isinstance(properties, dict):\n        raise SaltCloudException(\n            'ENI properties must be a dictionary'\n        )\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute',\n              'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params[k] = v\n\n    result = aws.query(params,\n                       return_root=True,\n                       location=get_location(vm_),\n                       provider=get_provider(),\n                       opts=__opts__,\n                       sigver='4')\n\n    if isinstance(result, dict) and result.get('error'):\n        raise SaltCloudException(\n            'Could not change interface <{0}> attributes <\\'{1}\\'>'.format(\n                eni_id, properties\n            )\n        )\n    else:\n        return result",
        "rewrite": "Here is the revised code:\n\n```\ndef _modify_eni_properties(eni_id, properties=None, vm_=None):\n    if not isinstance(properties, dict):\n        raise SaltCloudException('ENI properties must be a dictionary')\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute', 'NetworkInterfaceId': eni_id}\n    params.update(properties)\n\n    result = aws.query(params, return_root=True, location=get_location(vm_), provider=get_provider(), opts=__opts__, sigver='"
    },
    {
        "original": "def data(self):\n        \"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs",
        "rewrite": "```\ndef data(self):\n    if self.sort:\n        return sorted(self.dataset, key=self.sort_key)\n    elif self.shuffle:\n        return [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n    else:\n        return self.dataset\n```"
    },
    {
        "original": "def Start(self):\n    \"\"\"This uploads the rules to the foreman and, thus, starts the hunt.\"\"\"\n    # We are already running.\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n      return\n\n    # Determine when this hunt will expire.\n    self.context.duration = self.runner_args.expiry_time\n\n    # When the next client can be scheduled. Implements gradual client\n    # recruitment rate according to the client_rate.\n    self.context.next_client_due = rdfvalue.RDFDatetime.Now()\n\n    self._CreateAuditEvent(\"HUNT_STARTED\")\n\n    # Start the hunt.\n    self.hunt_obj.Set(self.hunt_obj.Schema.STATE(\"STARTED\"))\n    self.hunt_obj.Flush()\n\n    if self.runner_args.add_foreman_rules:\n      self._AddForemanRule()",
        "rewrite": "Here is a revised version of the code with some improvements:\n\n```\ndef start(self):\n    if self.hunt_obj.get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n        return\n    \n    expiry_time = rdfvalue.RDFDatetime.Now() + rdfvalue.Duration.From(seconds=self.runner_args.expiry_time)\n    \n    due_time = rdfvalue.RDFDatetime.Now()\n    \n    client_query_frequency = rdfvalue.Duration.From(seconds=self.runner_args.client.query_frequency)\n    \n     while due_time "
    },
    {
        "original": "def _partition_spec(self, shape, partition_info):\n    \"\"\"Build magic (and sparsely documented) shapes_and_slices spec string.\"\"\"\n    if partition_info is None:\n      return ''  # Empty string indicates a non-partitioned tensor.\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec",
        "rewrite": "Here is the revised code:\n\n```\ndef _partition_spec(self, shape, partition_info):\n    if partition_info is None:\n        return ''\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec\n```"
    },
    {
        "original": "def connection_id_to_endpoint(self, connection_id):\n        \"\"\"\n        Get stored public key for a connection.\n        \"\"\"\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.uri\n            except KeyError:\n                return None",
        "rewrite": "Here is the revised code:\n\n```\ndef connection_id_to_endpoint(self, connection_id):\n    with self._connections_lock:\n        return self._connections.get(connection_id, {}).get('uri')\n```"
    },
    {
        "original": "def _ValidateAFF4Type(aff4_type):\n  \"\"\"Validates an AFF4 type.\"\"\"\n  if aff4_type is None:\n    return\n\n  if not isinstance(aff4_type, type):\n    raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n  if not issubclass(aff4_type, AFF4Object):\n    raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" %\n                    aff4_type)",
        "rewrite": "Here is the revised code:\n\n```\ndef validate_aff4_type(aff4_type):\n    if aff4_type is None:\n        return\n    if not isinstance(aff4_type, type):\n        raise TypeError(f\"aff4_type={aff4_type} must be a type\")\n    if not issubclass(aff4_type, AFF4Object):\n        raise TypeError(f\"aff4_type={aff4_type} must be a subclass of AFF4Object\")\n```"
    },
    {
        "original": "def KernelVersion():\n  \"\"\"Gets the kernel version as string, eg. \"5.1.2600\".\n\n  Returns:\n    The kernel version, or \"unknown\" in the case of failure.\n  \"\"\"\n  rtl_osversioninfoexw = RtlOSVersionInfoExw()\n  try:\n    RtlGetVersion(rtl_osversioninfoexw)\n  except OSError:\n    return \"unknown\"\n\n  return \"%d.%d.%d\" % (rtl_osversioninfoexw.dwMajorVersion,\n                       rtl_osversioninfoexw.dwMinorVersion,\n                       rtl_osversioninfoexw.dwBuildNumber)",
        "rewrite": "Here is the revised code:\n\n```\ndef get_kernel_version():\n    rtl_osversioninfoexw = RtlOSVersionInfoExw()\n    try:\n        RtlGetVersion(rtl_osversioninfoexw)\n        return f\"{rtl_osversioninfoexw.dwMajorVersion}.{rtl_osversioninfoexw.dwMinorVersion}.{rtl_osversioninfoexw.dwBuildNumber}\"\n    except OSError:\n        return \"unknown\"\n```"
    },
    {
        "original": "def guid_to_squid(guid):\n    \"\"\"\n    Converts a GUID   to a compressed guid (SQUID)\n\n    Each Guid has 5 parts separated by '-'. For the first three each one will be\n    totally reversed, and for the remaining two each one will be reversed by\n    every other character. Then the final compressed Guid will be constructed by\n    concatenating all the reversed parts without '-'.\n\n    .. Example::\n\n        Input:                  2BE0FA87-5B36-43CF-95C8-C68D6673FB94\n        Reversed:               78AF0EB2-63B5-FC34-598C-6CD86637BF49\n        Final Compressed Guid:  78AF0EB263B5FC34598C6CD86637BF49\n\n    Args:\n\n        guid (str): A valid GUID\n\n    Returns:\n        str: A valid compressed GUID (SQUID)\n    \"\"\"\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w\\w)(\\w\\w)-(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match is not None:\n        for index in range(1, 12):\n            squid += guid_match.group(index)[::-1]\n    return squid",
        "rewrite": "Here is the revised code:\n\n```\nimport re\n\ndef guid_to_squid(guid):\n    guid_pattern = re.compile(r'^([0-9A-Fa-f]{8})-([0-9A-Fa-f]{4})-([0-9A-Fa-f]{4})-([0-9A-Fa-f]{2})([0-9A-Fa-f]{2})-([0-9A-Fa-f]{2})([0-"
    },
    {
        "original": "def commit_config(self, message=\"\"):\n        \"\"\"Commit configuration.\"\"\"\n        commit_args = {\"comment\": message} if message else {}\n        self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n        if not self.lock_disable and not self.session_config_lock:\n            self._unlock()",
        "rewrite": "Here is the revised code:\n\n```\ndef commit_config(self, message=\"\"):\n    commit_args = {\"comment\": message} if message else {}\n    self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n    if not (self.lock_disable or self.session_config_lock):\n        self._unlock()\n```"
    },
    {
        "original": "def fetch_metric(self, cursor, results, tags):\n        \"\"\"\n        Because we need to query the metrics by matching pairs, we can't query\n        all of them together without having to perform some matching based on\n        the name afterwards so instead we query instance by instance.\n        We cache the list of instance so that we don't have to look it up every time\n        \"\"\"\n        if self.sql_name not in results:\n            self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n            return\n\n        tags = tags + self.tags\n\n        results_list = results[self.sql_name]\n        done_instances = []\n        for ndx, row in enumerate(results_list):\n            ctype = row[0]\n            cval = row[1]\n            inst = row[2]\n            object_name = row[3]\n\n            if inst in done_instances:\n                continue\n\n            if (self.instance != ALL_INSTANCES and inst != self.instance) or (\n                self.object_name and object_name != self.object_name\n            ):\n                done_instances.append(inst)\n                continue\n\n            # find the next row which has the same instance\n            cval2 = None\n            ctype2 = None\n            for second_row in results_list[: ndx + 1]:\n                if inst == second_row[2]:\n                    cval2 = second_row[1]\n                    ctype2 = second_row[0]\n\n            if cval2 is None:\n                self.log.warning(\"Couldn't find second value for {}\".format(self.sql_name))\n                continue\n            done_instances.append(inst)\n            if ctype < ctype2:\n                value = cval\n                base = cval2\n            else:\n                value = cval2\n                base = cval\n\n            metric_tags = list(tags)\n            if self.instance == ALL_INSTANCES:\n                metric_tags.append('{}:{}'.format(self.tag_by, inst.strip()))\n            self.report_fraction(value, base, metric_tags)",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef fetch_metric(self, cursor, results, tags):\n    if self.sql_name not in results:\n        return\n    \n    tags += self.tags\n    \n    for row in results[self.sql_name]:\n        ctype1, val1, inst1, objname1  = row\n        \n        if inst1 in [i[2] for i in results[self.sql_name][:results[self(sql_error).index(row)]]:\n             continue\n        \n         do_report=False"
    },
    {
        "original": "def apply(key, value):\n    \"\"\"\n    Set a single key\n\n    .. note::\n\n        This will strip comments from your config file\n    \"\"\"\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data, default_flow_style=False)",
        "rewrite": "```\ndef apply(key, value):\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    \n    data = values()\n    data[key] = value\n    \n    with open(os.fspath(path), 'w+', encoding='utf-8') as fp:\n        import yaml\n        yaml.safe_dump(data, fp, default_flow_style=False)\n```"
    },
    {
        "original": "def refresh_access_token(self, refresh_token):\n        \"\"\"\u5237\u65b0 access token\n\n        :param refresh_token: OAuth2 refresh token\n        :return: JSON \u6570\u636e\u5305\n        \"\"\"\n        res = self._get(\n            'sns/oauth2/refresh_token',\n            params={\n                'appid': self.app_id,\n                'grant_type': 'refresh_token',\n                'refresh_token': refresh_token\n            }\n        )\n        self.access_token = res['access_token']\n        self.open_id = res['openid']\n        self.refresh_token = res['refresh_token']\n        self.expires_in = res['expires_in']\n        return res",
        "rewrite": "Here is the revised code:\n\n```\ndef refresh_access_token(self, refresh_token):\n    res = self._get(\n        'sns/oauth2/refresh_token',\n        params={\n            'appid': self.app_id,\n            'grant_type': 'refresh_token',\n            'refresh_token': refresh_timer\n       }\n    )\n    self.access-token = res.get('access-token')\n    self.open_id = res.get('openid')\n    self.refresh-token = res.get('refresh-token')\n    self.expires"
    },
    {
        "original": "def is_connection_to_a_vcenter(service_instance):\n    \"\"\"\n    Function that returns True if the connection is made to a vCenter Server and\n    False if the connection is made to an ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('api_type = %s', api_type)\n    if api_type == 'VirtualCenter':\n        return True\n    elif api_type == 'HostAgent':\n        return False\n    else:\n        raise salt.exceptions.VMwareApiError(\n            'Unexpected api type \\'{0}\\' . Supported types: '\n            '\\'VirtualCenter/HostAgent\\''.format(api_type))",
        "rewrite": "Here is the revised code:\n\n```\ndef is_connection_to_a_vcenter(service_instance):\n    try:\n        api_type = service_instance.content.about.apiType\n    except (vim.fault.NoPermission, vim.fault.VimFault, vmodl.RuntimeFault) as exc:\n        log.exception(exc)\n        if isinstance(exc, vim.fault.NoPermission):\n            raise salt.exceptions.VMwareApiError(f'Not enough permissions. Required privilege: {exc.privilegeId}')\n        else:\n"
    },
    {
        "original": "def add_column(self, name, *, index=0, values=None):\n        \"\"\"\n        Adds a column to the table\n        :param str name: the name of the column\n        :param int index: the index at which the column should be added. Defaults to 0.\n        :param list values: a two dimension array of values to add to the column\n        \"\"\"\n        if name is None:\n            return None\n\n        params = {\n            'name': name,\n            'index': index\n        }\n        if values is not None:\n            params['values'] = values\n\n        url = self.build_url(self._endpoints.get('add_column'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef add_column(self, name, *, index=0, values=None):\n    if not isinstance(name, str) or not name:\n       raise ValueError(\"Column name must be a non-empty string\")\n    \n    params = {'name': name, 'index': index}\n    \n    if values is not None and not isinstance(values, list):\n       raise ValueError(\"Values must be a list\")\n    \n    if values is not None:\n       params['"
    },
    {
        "original": "def __get_stat_display(self, stats, layer):\n        \"\"\"Return a dict of dict with all the stats display.\n        stats: Global stats dict\n        layer: ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n        :returns: dict of dict\n            * key: plugin name\n            * value: dict returned by the get_stats_display Plugin method\n        \"\"\"\n        ret = {}\n\n        for p in stats.getPluginsList(enable=False):\n            if p == 'quicklook' or p == 'processlist':\n                # processlist is done later\n                # because we need to know how many processes could be displayed\n                continue\n\n            # Compute the plugin max size\n            plugin_max_width = None\n            if p in self._left_sidebar:\n                plugin_max_width = max(self._left_sidebar_min_width,\n                                       self.screen.getmaxyx()[1] - 105)\n                plugin_max_width = min(self._left_sidebar_max_width,\n                                       plugin_max_width)\n\n            # Get the view\n            ret[p] = stats.get_plugin(p).get_stats_display(args=self.args,\n                                                           max_width=plugin_max_width)\n\n        return ret",
        "rewrite": "Here is a revised version of the code:\n\n```Python\ndef __get_stat_display(self, stats, layer):\n    ret = {}\n\n    for p in (p for p in stats.getPluginsList(enable=False) if p not in ['quicklook', 'processlist']):\n        if p in self._left_sidebar:\n            max_width = min(max(self._left_sidebar_min_width, self.screen.getmaxyx()[1] - 105), \n                             self._leftSidebarMaxWidth)\n        else"
    },
    {
        "original": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)",
        "rewrite": "Here's a revised version of your function:\n\n```Python\ndef delete(self, obj_id: int | str, **: dict[str, Any] = {}) -> None:\n    if obj_id is None:\n        path = self.path\n    else:\n        obj_id = str(obj_id).replace('/', '%2F')  # ensure type conversion and escape slashes\n        path = f\"{self.path}/{obj_id}\"\n    self.gitlab.http_delete(path, kwargs)\n```"
    },
    {
        "original": "def _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call",
        "rewrite": "Here is the revised code:\n\n```\nfrom functools import wraps\nfrom typing import Callable\n\nclass LoginRequiredException(Exception):\n    pass\n\ndef _requires_login(func: Callable) -> Callable:\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    call.__doc__ += \":raises"
    },
    {
        "original": "def _build_next_request(self, verb, prior_request, prior_response):\n        \"\"\"Builds pagination-aware request object.\n\n        More details:\n          https://developers.google.com/api-client-library/python/guide/pagination\n\n        Args:\n            verb (str): Request verb (ex. insert, update, delete).\n            prior_request (httplib2.HttpRequest): Request that may trigger\n                paging.\n            prior_response (dict): Potentially partial response.\n\n        Returns:\n            httplib2.HttpRequest: HttpRequest or None. None is returned when\n                there is nothing more to fetch - request completed.\n        \"\"\"\n        method = getattr(self._component, verb + '_next')\n        return method(prior_request, prior_response)",
        "rewrite": "Here's a revised version of the code:\n\n```python\ndef _build_next_request(self, verb: str, prior_request, prior_response):\n    method = getattr(self._component, f\"{verbage}_next\")\n    return method(prior_request, prior_response)\n```"
    },
    {
        "original": "def _gpinv(probs, kappa, sigma):\n    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)  # pylint: disable=invalid-unary-operand-type\n        else:\n            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n        x *= sigma\n    else:\n        if np.abs(kappa) < np.finfo(float).eps:\n            x[ok] = -np.log1p(-probs[ok])  # pylint: disable=unsupported-assignment-operation, E1130\n        else:\n            x[ok] = (  # pylint: disable=unsupported-assignment-operation\n                np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n            )\n        x *= sigma\n        x[probs == 0] = 0\n        if kappa >= 0:\n            x[probs == 1] = np.inf  # pylint: disable=unsupported-assignment-operation\n        else:\n            x[probs == 1] = -sigma / kappa  # pylint: disable=unsupported-assignment-operation\n\n    return x",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\n\ndef _gpinv(probs, kappa, sigma):\n    if sigma <= 0:\n        return np.full_like(probs, np.nan)\n\n    ok = (proabs > 0) & (proabs < 1)\n\n    result = probs.copy()\n    \n    if(np.abs(kappa) < float_info.eps):\n       result[ok] -= probs[ok]\n       result[ok] /= -kappa \n"
    },
    {
        "original": "def _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    # Process kwargs which are split over both create and host_config\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    # All kwargs should have been consumed by this point, so raise\n    # error if any are left\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    # Fill in any kwargs which need processing by create_host_config first\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        # sort to make consistent for tests\n        create_kwargs['ports'] = [tuple(p.split('/', 1))\n                                  for p in sorted(port_bindings.keys())]\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [\n                v.get('bind') for v in volumes.values()\n            ]\n        else:\n            create_kwargs['volumes'] = [\n                _host_volume_from_bind(v) for v in volumes\n            ]\n    return create_kwargs",
        "rewrite": "Here is the revised code:\n\n```\ndef _create_container_args(kwargs):\n    create_kwargs = {}\n    host_config_kwargs = {}\n\n    for key in list(kwargs.keys()):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n        elif key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n"
    },
    {
        "original": "def _CollectArtifact(self, artifact, apply_parsers):\n    \"\"\"Returns an `CollectedArtifact` rdf object for the requested artifact.\"\"\"\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n      parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n      parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources,\n                                                   parser_factory):\n      for response in source_result_list:\n        action_result = rdf_artifacts.ClientActionResult()\n        action_result.type = response.__class__.__name__\n        action_result.value = response\n        artifact_result.action_results.append(action_result)\n        self.UpdateKnowledgeBase(response, artifact.provides)\n\n    return artifact_result",
        "rewrite": "Here is the revised code:\n\n```\ndef _CollectArtifact(self, artifact, apply_parsers):\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    parser_factory = parsers.ArtifactParserFactory(str(artifact.name)) if apply_parsers else None\n\n    for source_result_list in self._ProcessSources(artifact.sources, parser_factory):\n        for response in sourceResultList:\n            action_result = rdf_artifacts.ClientActionResult()\n            actionResult.type = response.__class__.__"
    },
    {
        "original": "def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):\n        \"\"\"\n        Moments match of the marginal approximation in EP algorithm\n\n        :param i: number of observation (int)\n        :param tau_i: precision of the cavity distribution (float)\n        :param v_i: mean/variance of the cavity distribution (float)\n        \"\"\"\n        sigma2_hat = 1./(1./self.variance + tau_i)\n        mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n        sum_var = self.variance + 1./tau_i\n        Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5*(data_i - v_i/tau_i)**2./sum_var)\n        return Z_hat, mu_hat, sigma2_hat",
        "rewrite": "Here is the revised code:\n\n```\ndef moments_match_ep(self, data_i, tau_i, v_i, Y_metadata=None):\n    sigma2_hat = 1 / (1 / self.variance + tau_i)\n    mu_hat = sigma2_chat * (data_I / self.variance + v_I)\n    sum_var = self.variance + 1 / tau_I\n    Z_har = 1 / np.sqrt(2 * np.pi * sum_var) * np.exp"
    },
    {
        "original": "def validate_ports_string(ports):\n        \"\"\" Validate that provided string has proper port numbers:\n            1. port number < 65535\n            2. range start < range end\n        \"\"\"\n        pattern = re.compile('^\\\\d+(-\\\\d+)?(,\\\\d+(-\\\\d+)?)*$')\n        if pattern.match(ports) is None:\n            return False\n\n        ranges = PortsRangeHelper._get_string_port_ranges(ports)\n        for r in ranges:\n            if r.start > r.end or r.start > 65535 or r.end > 65535:\n                return False\n        return True",
        "rewrite": "Here is the revised code:\n\n```\nimport re\n\nclass PortsRangeHelper:\n    class PortRange:\n        def __init__(self, start, end):\n            self.start = int(start)\n            self.end = int(end)\n\n    @staticmethod\n    def _get_string_port_ranges(port_str):\n        result = []\n        parts = port_str.split(',')\n        \n        for part in parts:\n            if '-' in part:\n                start, end = part.split('-')\n                result.append(PortsRangeHelper"
    },
    {
        "original": "def strongest_match(cls, overlay, mode, backend=None):\n        \"\"\"\n        Returns the single strongest matching compositor operation\n        given an overlay. If no matches are found, None is returned.\n\n        The best match is defined as the compositor operation with the\n        highest match value as returned by the match_level method.\n        \"\"\"\n        match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                          if op.mode == mode and (not op.backends or backend in op.backends)]\n        matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n        if matches == []: return None\n        else:             return sorted(matches)[0]",
        "rewrite": "Here's a revised version of your code:\n\n```\ndef strongest_match(cls, overlay, mode, backend=None):\n    return max(((op.match_level(overlay), op) for op in cls.definitions \n                if op.mode == mode and (not op.backends or backend in op.backends)), \n               default=None)\n```\n\nThis version uses a generator expression within the `max` function to find the maximum matching strength and its corresponding operation. The `default` parameter of `max` is set"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict",
        "rewrite": "```\ndef to_dict(self):\n    return {\n        **: getattr(self, attr) for attr in ('label', 'provenance_ids')\n        if hasattr(self, attr) and getattr(self, attr) is not None\n    }\n```"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)",
        "rewrite": "Here is the revised code:\n\n```\ndef _post_master_init(self, master):\n    mp_call = _metaproxy_call(self.opts, 'post_master_init')\n    return mp_call(self, master)\n```"
    },
    {
        "original": "def get_occupation(self, atom_index, orbital):\n        \"\"\"\n        Returns the occupation for a particular orbital of a particular atom.\n\n        Args:\n            atom_num (int): Index of atom in the PROCAR. It should be noted\n                that VASP uses 1-based indexing for atoms, but this is\n                converted to 0-based indexing in this parser to be\n                consistent with representation of structures in pymatgen.\n            orbital (str): An orbital. If it is a single character, e.g., s,\n                p, d or f, the sum of all s-type, p-type, d-type or f-type\n                orbitals occupations are returned respectively. If it is a\n                specific orbital, e.g., px, dxy, etc., only the occupation\n                of that orbital is returned.\n\n        Returns:\n            Sum occupation of orbital of atom.\n        \"\"\"\n\n        orbital_index = self.orbitals.index(orbital)\n        return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None])\n                for spin, d in self.data.items()}",
        "rewrite": "Here is the revised code:\n\n```\ndef get_occupation(self, atom_index: int, orbital: str) -> float:\n    orbital_index = self.orbitals.index(orbital)\n    return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None]) \n            for spin, d in self.data.items()}\n```"
    },
    {
        "original": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False",
        "rewrite": "Here's a revised version of your code:\n\n```python\ndef _process-range-request(self, environ, complete-length=None, accept-ranges=None):\n    from ..exceptions import RequestedRangeNotSatisfiable\n    \n    if accept_ranges is None:\n       return False\n    \n    self.headers[\"Accept-Ranges\"] = accept_ranges\n    \n    if not self._is-range-request-processable(environ) or complete-length is None:\n       return False\n    \n    parsed-range=parse-range-header(environ.get(\""
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n            _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict",
        "rewrite": "Here is the revised code:\n\n```\ndef to_dict(self):\n    result = {}\n    if hasattr(self, 'dialog_nodes') and self.dialog_nodes:\n        result['dialog_nodes'] = [model.to_dict() for model in self.dialog_nodes]\n    if hasattr(self, 'pagination') and self.pagination:\n        result['pagination'] = self.pagination.to_dict()\n    return result\n```"
    },
    {
        "original": "def on_consumer_cancelled(self, method_frame):\n        \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n        receiving messages.\n\n        :param pika.frame.Method method_frame: The Basic.Cancel frame\n        \"\"\"\n        _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n        if self._channel:\n            self._channel.close()",
        "rewrite": "```\ndef on_consumer_cancelled(self, method_frame):\n    _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n    if self._channel:\n        self.stop()\n```"
    },
    {
        "original": "def record(self, timestamp, *args, **kwargs):\n        \"\"\" add custom data to data store \"\"\"\n        if self.output_file is None:\n            return\n\n        data = {'datetime': timestamp}\n\n        # append all data\n        if len(args) == 1:\n            if isinstance(args[0], dict):\n                data.update(dict(args[0]))\n            elif isinstance(args[0], pd.DataFrame):\n                data.update(args[0][-1:].to_dict(orient='records')[0])\n\n        # add kwargs\n        if kwargs:\n            data.update(dict(kwargs))\n\n        data['datetime'] = timestamp\n        # self.rows.append(pd.DataFrame(data=data, index=[timestamp]))\n\n        new_data = {}\n        if \"symbol\" not in data.keys():\n            new_data = dict(data)\n        else:\n            sym = data[\"symbol\"]\n            new_data[\"symbol\"] = data[\"symbol\"]\n            for key in data.keys():\n                if key not in ['datetime', 'symbol_group', 'asset_class']:\n                    new_data[sym + '_' + str(key).upper()] = data[key]\n\n        new_data['datetime'] = timestamp\n\n        # append to rows\n        self.rows.append(pd.DataFrame(data=new_data, index=[timestamp]))\n\n        # create dataframe\n        recorded = pd.concat(self.rows, sort=True)\n\n        if \"symbol\" not in recorded.columns:\n            return\n\n\n        # group by symbol\n        recorded['datetime'] = recorded.index\n        data = recorded.groupby(['symbol', 'datetime'], as_index=False).sum()\n        data.set_index('datetime', inplace=True)\n\n        symbols = data['symbol'].unique().tolist()\n        data.drop(columns=['symbol'], inplace=True)\n\n\n        # cleanup:\n\n        # remove symbols\n        recorded.drop(['symbol'] + [sym + '_SYMBOL' for sym in symbols],\n                      axis=1, inplace=True)\n\n        # remove non-option data if not working with options\n        for sym in symbols:\n            try:\n                opt_cols = recorded.columns[\n                    recorded.columns.str.startswith(sym + '_OPT_')].tolist()\n                if len(opt_cols) == len(recorded[opt_cols].isnull().all()):\n                    recorded.drop(opt_cols, axis=1, inplace=True)\n            except Exception as e:\n                pass\n\n        # group df\n        recorded = recorded.groupby(recorded['datetime']).first()\n\n        # shift position\n        for sym in symbols:\n            recorded[sym + '_POSITION'] = recorded[sym + '_POSITION'\n                                                   ].shift(1).fillna(0)\n\n        # make this public\n        self.recorded = recorded.copy()\n\n        # cleanup columns names before saving...\n        recorded.columns = [col.replace('_FUT_', '_').replace(\n                            '_OPT_OPT_', '_OPT_') for col in recorded.columns]\n\n        # save\n        if \".csv\" in self.output_file:\n            recorded.to_csv(self.output_file)\n        elif \".h5\" in self.output_file:\n            recorded.to_hdf(self.output_file, 0)\n        elif (\".pickle\" in self.output_file) | (\".pkl\" in self.output_file):\n            recorded.to_pickle(self.output_file)\n\n        chmod(self.output_file)",
        "rewrite": "Here is the revised code:\n\n```\ndef record(self, timestamp, *_args=None, **_kwargs=None):\n    if self.output_file is None:\n         return\n\n    def _append_to_dict(main_dict, dict_to_append):\n         main_dict.update(dict_to_append)\n\n    def _transform_and_update(new_data_dict):\n         symbol_value=feature.get(\"field\")\n         tahmincimappingdict={\"asset_class\": \"\", \n                             '\u0421\u0421\u0420_UILayoutDirectional.swift\\:36| Ethics Committee': '', \n"
    },
    {
        "original": "def dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n        \"\"\"\n        derivative of logpdf wrt link_f param\n        .. math::\n\n        :param link_f: latent variables link(f)\n        :type link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: likelihood evaluated for this point\n        :rtype: float\n        \"\"\"\n        assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        val = np.log(y) - link_f\n        val_scaled = val/np.sqrt(self.variance)\n        val_scaled2 = val/self.variance\n        uncensored = (1-c)*(val_scaled2)\n        a = (1- stats.norm.cdf(val_scaled))\n        # llg(z) = 1. / (1 - norm_cdf(r / sqrt(s2))). * (1 / sqrt(2 * pi * s2). * exp(-1 / (2. * s2). * r. ^ 2));\n        censored = c*( 1./a) * (np.exp(-1.* val**2 /(2*self.variance)) / np.sqrt(2*np.pi*self.variance))\n        # censored = c * (1. / (1 - stats.norm.cdf(val_scaled))) * (stats.norm.pdf(val_scaled))\n        gradient = uncensored + censored\n        return gradient",
        "rewrite": "Here is the revised code:\n\n```\ndef dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n    assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n    c = np.zeros_like(y)\n    if Y_metadata is not None and 'censored' in Y_metadata:\n        c = Y_metadata['censored']\n\n    val = np.log(y) - link_f\n    val_scaled = val / np.sqrt"
    },
    {
        "original": "def _compare_by_version(path1, path2):\n    \"\"\"Returns the current/latest learned path.\n\n    Checks if given paths are from same source/peer and then compares their\n    version number to determine which path is received later. If paths are from\n    different source/peer return None.\n    \"\"\"\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None",
        "rewrite": "Here is the revised code:\n\n```\ndef _compare_by_version(path1, path2):\n    if path1.source == path2.source:\n        return max(path1, path2, key=lambda x: x.source_version_num)\n    return None\n```"
    },
    {
        "original": "def from_parts(cls, parts):\n        \"\"\"\n        Return content types XML mapping each part in *parts* to the\n        appropriate content type and suitable for storage as\n        ``[Content_Types].xml`` in an OPC package.\n        \"\"\"\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti",
        "rewrite": "Here is the revised code:\n\n```\ndef from_parts(cls, parts):\n    cti = cls()\n    cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n    cti._defaults['xml'] = CT.XML\n    for part in parts:\n        if not isinstance(part.content_type, str):\n            raise ValueError(\"content_type must be a string\")\n        if not isinstance(part.partname, str):\n            raise ValueError(\"partname must be a string\")\n        if"
    },
    {
        "original": "def ConfigureDatastore(config):\n  \"\"\"Guides the user through configuration of the datastore.\"\"\"\n  print(\"\\n\\n-=GRR Datastore=-\\n\"\n        \"For GRR to work each GRR server has to be able to communicate with\\n\"\n        \"the datastore. To do this we need to configure a datastore.\\n\")\n\n  existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n  if not existing_datastore or existing_datastore == \"FakeDataStore\":\n    ConfigureMySQLDatastore(config)\n    return\n\n  print(\"Found existing settings:\\n  Datastore: %s\" % existing_datastore)\n  if existing_datastore == \"SqliteDataStore\":\n    set_up_mysql = RetryBoolQuestion(\n        \"The SQLite datastore is no longer supported. Would you like to\\n\"\n        \"set up a MySQL datastore? Answering 'no' will abort config \"\n        \"initialization.\", True)\n    if set_up_mysql:\n      print(\"\\nPlease note that no data will be migrated from SQLite to \"\n            \"MySQL.\\n\")\n      ConfigureMySQLDatastore(config)\n    else:\n      raise ConfigInitError()\n  elif existing_datastore == \"MySQLAdvancedDataStore\":\n    print(\"  MySQL Host: %s\\n  MySQL Port: %s\\n  MySQL Database: %s\\n\"\n          \"  MySQL Username: %s\\n\" %\n          (grr_config.CONFIG.Get(\"Mysql.host\"),\n           grr_config.CONFIG.Get(\"Mysql.port\"),\n           grr_config.CONFIG.Get(\"Mysql.database_name\"),\n           grr_config.CONFIG.Get(\"Mysql.database_username\")))\n    if grr_config.CONFIG.Get(\"Mysql.client_key_path\"):\n      print(\"  MySQL client key file: %s\\n\"\n            \"  MySQL client cert file: %s\\n\"\n            \"  MySQL ca cert file: %s\\n\" %\n            (grr_config.CONFIG.Get(\"Mysql.client_key_path\"),\n             grr_config.CONFIG.Get(\"Mysql.client_cert_path\"),\n             grr_config.CONFIG.Get(\"Mysql.ca_cert_path\")))\n\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureMySQLDatastore(config)",
        "rewrite": "Here is the revised code:\n\n```\ndef configure_datastore(config):\n    print(\"\\n\\n-=GRR Datastore=-\\n\")\n    print(\n        *\"For GRR to work each GRR server has to be able to communicate with \"\n        \"*the datastore. To do this we need to configure a datastore.*\\n\")\n\n    existing_datastore = grr_config(CONFIG).Get('DataStore.implementation')\n\n    if notexistingdata store or existingdata store== 'FakeDAt"
    },
    {
        "original": "def _get_century_code(year):\n        \"\"\"Returns the century code for a given year\"\"\"\n        if 2000 <= year < 3000:\n            separator = 'A'\n        elif 1900 <= year < 2000:\n            separator = '-'\n        elif 1800 <= year < 1900:\n            separator = '+'\n        else:\n            raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n        return separator",
        "rewrite": "Here is the revised code:\n\n```\ndef get_century_code(year):\n    if 2000 <= year < 3000:\n        return 'A'\n    elif 1900 <= year < 2000:\n        return '-'\n    elif 1800 <= year < 1900:\n        return '+'\n    else:\n        raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n```"
    },
    {
        "original": "def bulk_build(jail, pkg_file, keep=False):\n    \"\"\"\n    Run bulk build on poudriere server.\n\n    Return number of pkg builds, failures, and errors, on error dump to CLI\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -N buildbox_group poudriere.bulk_build 90amd64 /root/pkg_list\n\n    \"\"\"\n    # make sure `pkg file` and jail is on file system\n    if not os.path.isfile(pkg_file):\n        return 'Could not find file {0} on filesystem'.format(pkg_file)\n    if not is_jail(jail):\n        return 'Could not find jail {0}'.format(jail)\n\n    # Generate command\n    if keep:\n        cmd = 'poudriere bulk -k -f {0} -j {1}'.format(pkg_file, jail)\n    else:\n        cmd = 'poudriere bulk -f {0} -j {1}'.format(pkg_file, jail)\n\n    # Bulk build this can take some time, depending on pkg_file ... hours\n    res = __salt__['cmd.run'](cmd)\n    lines = res.splitlines()\n    for line in lines:\n        if \"packages built\" in line:\n            return line\n    return ('There may have been an issue building packages dumping output: '\n            '{0}').format(res)",
        "rewrite": "Here's the revised code:\n\n```\ndef bulk_build(jail, pkg_file, keep=False):\n    \n      if not os.path.isfile(pkg_file):\n          raise FileNotFoundError('Could not find file {}'.format(pkg_file))\n      if not is_jail(jail):\n          raise ValueError('Could not find jail {}'.format(jail))\n\n      cmd = 'poudriere bulk {}-k -f {} -j {}'.format('-k' if keep else '', pkg_file, jail)\n\n      res = __salt"
    },
    {
        "original": "def copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error(\n            'The copy_snapshot function must be called with -f or --function.'\n        )\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A source_snapshot_id must be specified to copy a snapshot.')\n        return False\n\n    if 'description' not in kwargs:\n        kwargs['description'] = ''\n\n    params = {'Action': 'CopySnapshot'}\n\n    if 'source_region' in kwargs:\n        params['SourceRegion'] = kwargs['source_region']\n\n    if 'source_snapshot_id' in kwargs:\n        params['SourceSnapshotId'] = kwargs['source_snapshot_id']\n\n    if 'description' in kwargs:\n        params['Description'] = kwargs['description']\n\n    log.debug(params)\n\n    data = aws.query(params,\n                     return_url=True,\n                     location=get_location(),\n                     provider=get_provider(),\n                     opts=__opts__,\n                     sigver='4')\n    return data",
        "rewrite": "Here is the revised code:\n\n```\ndef copy_snapshot(**, **uong metod): \nif call != \"function\": \n  log.error(\"The copy_snapshot function must be called with -f or --function.\") \n  return False \n\nrequired_params= [\"source_region\", \"source_snaasename\"] \nfor param in required_params: \n  if param not in kuvvet: \n   lo (\"A {} musbe specified to coay a snaphot.\".format(param)) \n   reurn"
    },
    {
        "original": "def _left_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the left in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n      return r\n    elif not self._upper:\n      return 0\n    elif self._include_diagonal:\n      return r\n    else:\n      return r + 1",
        "rewrite": "Here is the revised code:\n\n```\ndef _left_zero_blocks(self, r):\n    if not self._include_off_diagonal:\n        return r\n    elif not self._upper:\n        return 0\n    else:\n        return r + (not self._include_diagonal)\n```"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "rewrite": "Here is the revised code:\n\n```\ndef execute(self):\n    command = self.reset() or self.command\n    return self.client.execute_command(command)\n```"
    },
    {
        "original": "def InferUserAndSubjectFromUrn(self):\n    \"\"\"Infers user name and subject urn from self.urn.\"\"\"\n    _, hunts_str, hunt_id, user, _ = self.urn.Split(5)\n\n    if hunts_str != \"hunts\":\n      raise access_control.UnauthorizedAccess(\n          \"Approval object has invalid urn %s.\" % self.urn,\n          requested_access=self.token.requested_access)\n\n    return (user, aff4.ROOT_URN.Add(\"hunts\").Add(hunt_id))",
        "rewrite": "Here is the revised code:\n\n```\ndef InferUserAndSubjectFromUrn(self):\n    _, hunts_str, hunt_id, user, _ = self.urn.split(5)\n    \n    if hunts_str != \"hunts\":\n        raise access_control.UnauthorizedAccess(\n            f\"Approval object has invalid urn {self.urn}\",\n            requested_access=self.token.requested_access)\n        \n    return user, aff4.ROOT_URN.Add(\"hunts\").Add(hunt_id)\n```"
    },
    {
        "original": "def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef _api_config_item(self, item):\n    response.content_type = 'application/json; charset=utf-8'\n    config_dict = self.config.as_dict()\n    if item not in config_dict:\n       abort(400, f\"Unknown configuration item {item}\")\n    try:\n       return json.dumps(config_dict[item])\n    except Exception as e:\n       abort(404, f\"Cannot get config item ({str(e)})\")\n```"
    },
    {
        "original": "async def get_response(self, message=None, *, timeout=None):\n        \"\"\"\n        Returns a coroutine that will resolve once a response arrives.\n\n        Args:\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\n                The message (or the message ID) for which a response\n                is expected. By default this is the last sent message.\n\n            timeout (`int` | `float`, optional):\n                If present, this `timeout` (in seconds) will override the\n                per-action timeout defined for the conversation.\n        \"\"\"\n        return await self._get_message(\n            message, self._response_indices, self._pending_responses, timeout,\n            lambda x, y: True\n        )",
        "rewrite": "Here is the revised code:\n\n```\nasync def get_response(self, message=None, *, timeout=None):\n    return await self._get_message(message, self._response_indices, self._pending_responses, timeout)\n```"
    },
    {
        "original": "def _extract_field_with_regex(self, field):\n        \"\"\" extract field from response content with regex.\n            requests.Response body could be json or html text.\n\n        Args:\n            field (str): regex string that matched r\".*\\(.*\\).*\"\n\n        Returns:\n            str: matched content.\n\n        Raises:\n            exceptions.ExtractFailure: If no content matched with regex.\n\n        Examples:\n            >>> # self.text: \"LB123abcRB789\"\n            >>> filed = \"LB[\\d]*(.*)RB[\\d]*\"\n            >>> _extract_field_with_regex(field)\n            abc\n\n        \"\"\"\n        matched = re.search(field, self.text)\n        if not matched:\n            err_msg = u\"Failed to extract data with regex! => {}\\n\".format(field)\n            err_msg += u\"response body: {}\\n\".format(self.text)\n            logger.log_error(err_msg)\n            raise exceptions.ExtractFailure(err_msg)\n\n        return matched.group(1)",
        "rewrite": "Here is the revised code:\n\n```\nimport re\nimport logging\nfrom exceptions import ExtractFailure\n\nclass MyClass:\n    def _extract_field_with_regex(self, field):\n        try:\n            return re.search(field, self.text).group(1)\n        except AttributeError as e:\n            err_msg = f\"Failed to extract data with regex! => {field}\\n\"\n            err_msg += f\"response body: {self.text}\\n\"\n            logging.error(err_msg)\n```"
    },
    {
        "original": "def get_accumulator_dir(cachedir):\n    \"\"\"\n    Return the directory that accumulator data is stored in, creating it if it\n    doesn't exist.\n    \"\"\"\n    fn_ = os.path.join(cachedir, 'accumulator')\n    if not os.path.isdir(fn_):\n        # accumulator_dir is not present, create it\n        os.makedirs(fn_)\n    return fn_",
        "rewrite": "```\nimport os\n\ndef get_accumulator_dir(cachedir):\n    accumulator_dir = os.path.join(cachedir, 'accumulator')\n    os.makedirs(accumulator_dir, exist_ok=True)\n    return accumulator_dir\n```"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "rewrite": "Here is the revised code:\n\n```\ndef differing_functions_with_consts(self):\n    return [(func_a, func_b) for func_a, func_b in self.function_matches \n           if not self.functions_probably_identical(func_a, func_b, check_consts=True)]\n```"
    },
    {
        "original": "def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt",
        "rewrite": "Here is the revised code:\n\n```\ndef single_frame_plot(obj):\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n    \nclass_obj = getattr(renderer, 'plotting_class', lambda x: None)(obj)\nplot_objs = [renderer.plot_options(obj)]\nsize_opts  = [renderer.size]\nplot_args  =[obj]+plot_objs+size_opts\n    \ntry:\n       "
    },
    {
        "original": "def __get_cpu(self):\n        \"\"\"Update and/or return the CPU using the psutil library.\"\"\"\n        # Never update more than 1 time per cached_time\n        if self.timer_cpu.finished():\n            self.cpu_percent = psutil.cpu_percent(interval=0.0)\n            # Reset timer for cache\n            self.timer_cpu = Timer(self.cached_time)\n        return self.cpu_percent",
        "rewrite": "Here is the revised code:\n\n```\ndef get_cpu(self):\n    if not hasattr(self, 'timer_cpu') or not hasattr(self, 'cpu_percent'):\n        self.timer_cpu = Timer(self.cached_time)\n        self.cpu_percent = psutil.cpu_percent(interval=0.0)\n    elif self.timer_cpu.finished():\n        self.cpu_percent = psutil cpu_percent(interval=0.0)\n        self.timer_cpu = Timer(self.cached_time)\n    return self.cpu_percent\n```"
    },
    {
        "original": "def text_of(relpath):\n    \"\"\"\n    Return string containing the contents of the file at *relpath* relative to\n    this file.\n    \"\"\"\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text",
        "rewrite": "```\nimport os\n\ndef text_of(relpath):\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, relpath)\n    with open(file_path) as f:\n        return f.read()\n```"
    },
    {
        "original": "def relativize(self, absolute_address, target_region_id=None):\n        \"\"\"\n        Convert an absolute address to the memory offset in a memory region.\n\n        Note that if an address belongs to heap region is passed in to a stack region map, it will be converted to an\n        offset included in the closest stack frame, and vice versa for passing a stack address to a heap region.\n        Therefore you should only pass in address that belongs to the same category (stack or non-stack) of this region\n        map.\n\n        :param absolute_address:    An absolute memory address\n        :return:                    A tuple of the closest region ID, the relative offset, and the related function\n                                    address.\n        \"\"\"\n\n        if target_region_id is None:\n            if self.is_stack:\n                # Get the base address of the stack frame it belongs to\n                base_address = next(self._address_to_region_id.irange(minimum=absolute_address, reverse=False))\n\n            else:\n                try:\n                    base_address = next(self._address_to_region_id.irange(maximum=absolute_address, reverse=True))\n\n                except StopIteration:\n                    # Not found. It belongs to the global region then.\n                    return 'global', absolute_address, None\n\n            descriptor = self._address_to_region_id[base_address]\n\n        else:\n            if target_region_id == 'global':\n                # Just return the absolute address\n                return 'global', absolute_address, None\n\n            if target_region_id not in self._region_id_to_address:\n                raise SimRegionMapError('Trying to relativize to a non-existent region \"%s\"' % target_region_id)\n\n            descriptor = self._region_id_to_address[target_region_id]\n            base_address = descriptor.base_address\n\n        return descriptor.region_id, absolute_address - base_address, descriptor.related_function_address",
        "rewrite": "Here is a revised version of your code:\n\n```\ndef relativize(self, absolute-address: int ,target-region-id: str=None) -> Tuple[str,int,int]:\n    if target-region-id is None:\n       if self.is_stack :\n           try :\n               base_ADDRESS=self._address-to-region-id.get_next_min(absolute-address)\n           except StopIteration :\n               raise SimRegionMapError(\"Address does not belong  any  known regions\")\n       else :\n           try :\n               base_ADDRESS=self._"
    },
    {
        "original": "def _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel, logits for per-class\n        predictions.\n\n    Raises:\n      IncompatibleShapeError: If images is not rank 4.\n      ValueError: If model_size is not one of 'basic' or 'large'.\n    \"\"\"\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n      raise base.IncompatibleShapeError(\n          \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n      if self._model_size == self.BASIC:\n        self._initializers[self.WEIGHTS] = identity_kernel_initializer\n      elif self._model_size == self.LARGE:\n        self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(\n            num_classes)\n      else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n      self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n      self._conv_modules = [\n          self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    elif self._model_size == self.LARGE:\n      self._conv_modules = [\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    else:\n      raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)",
        "rewrite": "Here is the revised code:\n\n```\ndef _build(self):\n  \tnum Classes=self_num_output_classses \n   \n  \tif len(images.get_shape()) !=4 :\n\t\traise base.IncompatibleShapeError(\"'imagesmust have shape[batch size,height,width.depth]\")\n   \n  \tif WEIGHTSNOT in _initiliazers :\n      \tif _modelszie== BASIC :\n           _initiliazers[WEIGHTS]=identity_kernel initializer \n       elif_modelsize= LARGE :\n           initiliazers[WEIGHT"
    },
    {
        "original": "def load(self):\n        \"\"\"\n        call this function after the file exists to populate properties\n        \"\"\"\n        # If we are given a string, open it up else assume it's something we\n        # can call read on.\n        if isinstance(self.specfile, str):\n            f = open(self.specfile, 'r')\n        else:\n            f = self.specfile\n\n        for line in f:\n            if self.v_regex.match(line):\n                self._pkg_version = self.v_regex.match(line).group(1)\n            if self.n_regex.match(line):\n                self._pkg_name = self.n_regex.match(line).group(1)\n        f.close()\n        self._loaded = True",
        "rewrite": "```\ndef load(self):\n    if isinstance(self.specfile, str):\n        with open(self.specfile, 'r') as f:\n            for line in f:\n                match = self.v_regex.match(line)\n                if match:\n                    self._pkg_version = match.group(1)\n                match = self.n_regex.match(line)\n                if match:\n                    self._pkg_name = match.group(1)\n    else:\n        for line in iter(self.specfile.readline, ''):\n            match ="
    },
    {
        "original": "def order_by_line_nos(objs, line_nos):\n    \"\"\"Orders the set of `objs` by `line_nos`\n    \"\"\"\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]",
        "rewrite": "```\ndef order_by_line_nos(objs, line_nos):\n    return [objs[i] for i in sorted(range(len(line_nos)), key=lambda k: line_nos[k])]\n```"
    },
    {
        "original": "def get_function_subgraph(self, start, max_call_depth=None):\n        \"\"\"\n        Get a sub-graph of a certain function.\n\n        :param start: The function start. Currently it should be an integer.\n        :param max_call_depth: Call depth limit. None indicates no limit.\n        :return: A CFG instance which is a sub-graph of self.graph\n        \"\"\"\n\n        # FIXME: syscalls are not supported\n        # FIXME: start should also take a CFGNode instance\n\n        start_node = self.get_any_node(start)\n\n        node_wrapper = (start_node, 0)\n        stack = [node_wrapper]\n        traversed_nodes = {start_node}\n        subgraph_nodes = set([start_node])\n\n        while stack:\n            nw = stack.pop()\n            n, call_depth = nw[0], nw[1]\n\n            # Get successors\n            edges = self.graph.out_edges(n, data=True)\n\n            for _, dst, data in edges:\n                if dst not in traversed_nodes:\n                    # We see a new node!\n                    traversed_nodes.add(dst)\n\n                    if data['jumpkind'] == 'Ijk_Call':\n                        if max_call_depth is None or (max_call_depth is not None and call_depth < max_call_depth):\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth + 1)\n                            stack.append(new_nw)\n                    elif data['jumpkind'] == 'Ijk_Ret':\n                        if call_depth > 0:\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth - 1)\n                            stack.append(new_nw)\n                    else:\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth)\n                        stack.append(new_nw)\n\n       #subgraph = networkx.subgraph(self.graph, subgraph_nodes)\n        subgraph = self.graph.subgraph(subgraph_nodes).copy()\n\n        # Make it a CFG instance\n        subcfg = self.copy()\n        subcfg._graph = subgraph\n        subcfg._starts = (start,)\n\n        return subcfg",
        "rewrite": "Here is the revised code:\n\n```\ndef get_function_subgraph(self, start, max_call_depth=None):\n    start_node= self.get_any_node(start)\n\n    node_wrapper= (start_node ,0 )\n    stack=[node_wrapper]\n    traversed_nodes={start_node}\n    SubGraphNodes=set([start_node])\n\n    while stack:\n       nw=stack.pop()\n       n ,call_Depth=nw[0] ,nw[1]\n\n       edges=self.graph.out_edges(n,data=True"
    },
    {
        "original": "def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        endpoint = instance.get('prometheus_url')\n        if endpoint is None:\n            raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n\n        metrics = [DEFAULT_METRICS, GO_METRICS]\n        metrics.extend(instance.get('metrics', []))\n\n        instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\n\n        return instance",
        "rewrite": "Here is the revised code:\n\n```\ndef _create_core_dns_instance(self, instance):\n    endpoint = instance.get('prometheus_url')\n    if not endpoint:\n        raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n    \n    metrics = [DEFAULT_METRICS, GO_METRICS]\n    metrics.extend(instance.get('metrics', []))\n    \n    instance.update({\n        'prometheus_url': endpoint,\n        'namespace': 'coredns',\n        'metrics': metrics\n    })\n"
    },
    {
        "original": "def exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n        \"\"\"Omits a scheduled operation from the schedule, if present.\n\n        Args:\n            scheduled_operation: The operation to try to remove.\n\n        Returns:\n            True if the operation was present and is now removed, False if it\n            was already not present.\n        \"\"\"\n        try:\n            self.scheduled_operations.remove(scheduled_operation)\n            return True\n        except ValueError:\n            return False",
        "rewrite": "```\ndef exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n    return self.scheduled_operations.remove(scheduled_operation) is None or True\n```"
    },
    {
        "original": "def server_show_libcloud(self, uuid):\n        \"\"\"\n        Make output look like libcloud output for consistency\n        \"\"\"\n        server_info = self.server_show(uuid)\n        server = next(six.itervalues(server_info))\n        server_name = next(six.iterkeys(server_info))\n        if not hasattr(self, 'password'):\n            self.password = None\n        ret = NovaServer(server_name, server, self.password)\n\n        return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef server_show_libcloud(self, uuid):\n    server_info = self.server_show(uuid)\n    server_name = next(iter(server_info))\n    server_data = server_info[server_name]\n    password = getattr(self, 'password') if hasattr(self, 'password') else None\n    return NovaServer(server_name, server_data, password)\n```"
    },
    {
        "original": "def extract(self, topic: str, parseNumbers=True) -> list:\n        \"\"\"\n        Extract items of given topic and return as list of objects.\n\n        The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n        Order, etc.\n        \"\"\"\n        cls = type(topic, (DynamicObject,), {})\n        results = [cls(**node.attrib) for node in self.root.iter(topic)]\n        if parseNumbers:\n            for obj in results:\n                d = obj.__dict__\n                for k, v in d.items():\n                    with suppress(ValueError):\n                        d[k] = float(v)\n                        d[k] = int(v)\n        return results",
        "rewrite": "Here is the revised code:\n\n```\ndef extract(self, topic: str, parseNumbers=True) -> list:\n    cls = type(topic, (DynamicObject,), {})\n    results = [cls(node.attrib) for node in self.root.iter(topic)]\n    if parseNumbers:\n        for obj in results:\n            d = obj.__dict__\n            for k, v in dict(d).items():  # iterate over a copy of items\n                try:\n                    d[k] = float(v)\n               "
    },
    {
        "original": "def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv",
        "rewrite": "Here's a revised version of your function:\n\n```\ndef pin_auth(self, request):\n    exhausted = False\n    auth = False\n    \n    if (trust := self.check_pin_trust(request.environ)) is None:\n       self._fail_pin_auth()\n       rv.delete_cookie(self.pin_cookie_name)\n    \n    elif trust: \n       auth=True\n    \n    elif self._failed_pin_AUTH > 10: \n       exhausted=True\n    \n    else: \n       entered.Pin=request.args.get(\"pin\")\n      "
    },
    {
        "original": "def set_lim(min, max, name):\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\n\n    Parameters\n    ----------\n    name: hashable\n        Any variable that can be used as a key for a dictionary\n\n    Raises\n    ------\n    KeyError\n        When no context figure is associated with the provided key.\n\n    \"\"\"\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale",
        "rewrite": "Here is the revised code:\n\n```\ndef set_lim(min_val, max_val, name):\n    scale = _context['scales'].setdefault(_get_attribute_dimension(name))\n    if not isinstance(scale, dict):\n        raise KeyError(f\"No context figure is associated with the provided key '{name}'\")\n    \n    scale.min = min_val\n    scale.max = max_val\n    \n```"
    },
    {
        "original": "def ParseMultiple(self, stats, unused_file_obj, unused_kb):\n    \"\"\"Identify the init scripts and the start/stop scripts at each runlevel.\n\n    Evaluate all the stat entries collected from the system.\n    If the path name matches a runlevel spec, and if the filename matches a\n    sysv init symlink process the link as a service.\n\n    Args:\n      stats: An iterator of StatEntry rdfs.\n      unused_file_obj: An iterator of file contents. Not needed as the parser\n        only evaluates link attributes.\n      unused_kb: Unused KnowledgeBase rdf.\n\n    Yields:\n      rdf_anomaly.Anomaly if the startup link seems wierd.\n      rdf_client.LinuxServiceInformation for each detected service.\n    \"\"\"\n    services = {}\n    for stat_entry in stats:\n      path = stat_entry.pathspec.path\n      runlevel = self.runlevel_re.match(os.path.dirname(path))\n      runscript = self.runscript_re.match(os.path.basename(path))\n      if runlevel and runscript:\n        svc = runscript.groupdict()\n        service = services.setdefault(\n            svc[\"name\"],\n            rdf_client.LinuxServiceInformation(\n                name=svc[\"name\"], start_mode=\"INIT\"))\n        runlvl = GetRunlevelsNonLSB(runlevel.group(1))\n        if svc[\"action\"] == \"S\" and runlvl:\n          service.start_on.append(runlvl.pop())\n          service.starts = True\n        elif runlvl:\n          service.stop_on.append(runlvl.pop())\n        if not stat.S_ISLNK(int(stat_entry.st_mode)):\n          yield rdf_anomaly.Anomaly(\n              type=\"PARSER_ANOMALY\",\n              finding=[path],\n              explanation=\"Startup script is not a symlink.\")\n    for svc in itervalues(services):\n      yield svc",
        "rewrite": "Here is a revised version of your Python function:\n\n```\ndef ParseMultiple(self, stats, _, __):\n    services = {}\n    \n    for stat_entry in stats:\n        path = stat_entry.pathspec.path\n        match_runlevel = self.runlevel_re.match(os.path.dirname(path))\n        match_runscript = self.runscript_re.match(os.path.basename(path))\n        \n        if match_runlevel and match_runscript:\n            svc_data = match_runscript.groupdict()\n            service_name = svc_data"
    },
    {
        "original": "def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs",
        "rewrite": "Here is the revised code:\n\n```\ndef get_ext_outputs(self):\n    all_outputs = []\n    ext_outputs = {}\n    paths = {self.bdist_dir: ''}\n\n    for base, dirs, files in sorted(os.walk(self.bdist_dir)):\n        for dir_ in dirs:\n            paths[os.path.join(base, dir_)] = os.path.relpath(os.path.join(base, dir_), self.bdist_dir) + '/'\n        \n    for base, _, files in sorted(os.walk(self"
    },
    {
        "original": "def base64_bytes(x):\n    \"\"\"Turn base64 into bytes\"\"\"\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(bytes_encode(x))",
        "rewrite": "```\nimport base64\n\ndef base64_bytes(x):\n    if x.endswith \"==\"):\n        x = x + \"=\" * ((4 - len(x) % 4) % 4)\n    return base64.b64decode(x.encode('utf-8'))\n```"
    },
    {
        "original": "def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node",
        "rewrite": "```\ndef fold(self, node):\n    node = self.generic_visit(node)\n    try:\n        return nodes.Const.from_untrusted(node.as_const(), lineno=node.lineno, environment=self.environment)\n    except nodes.Impossible:\n        return node\n```"
    },
    {
        "original": "def from_node(index, data, modify_index=None):\n        \"\"\"\n        >>> ClusterConfig.from_node(1, '{') is None\n        False\n        \"\"\"\n\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)",
        "rewrite": "```\ndef from_node(index, data, modify_index=None):\n    try:\n        data = json.loads(data)\n    except (TypeError, ValueError):\n        data = {} \n    if not isinstance(data, dict):\n        data = {}\n    return ClusterConfig(index, data, index if modify_index is None else modify_index)\n```"
    },
    {
        "original": "def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "rewrite": "```\ndef save_config(self, cmd=\"copy running-config startup-config\", confirm=True, confirm_response=\"y\"):\n    return super().save_config(cmd=cmd, confirm=confirm, confirm_response=confirm_response)\n```"
    },
    {
        "original": "def Validate(self):\n    \"\"\"GlobExpression is valid.\"\"\"\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n      raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)",
        "rewrite": "Here is the revised code:\n\n```\ndef validate(self):\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n        raise ValueError(f\"Only one ** is permitted per path: {self._value}\")\n```"
    },
    {
        "original": "def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        with self._lock:\n            if name not in self._gauges:\n                if self._registry._ignore_patterns and any(\n                    pattern.match(name) for pattern in self._registry._ignore_patterns\n                ):\n                    gauge = noop_metric\n                else:\n                    gauge = Gauge(name)\n                self._gauges[name] = gauge\n            return self._gauges[name]",
        "rewrite": "Here is the revised code:\n\n```\ndef gauge(self, name):\n    with self._lock:\n        if name not in self._gauges:\n            if self._registry._ignore_patterns and any(pattern.match(name) for pattern in self._registry._ignore_patterns):\n                self._gauges[name] = noop_metric\n            else:\n                self._gauges[name] = Gauge(name)\n        return self._gauges[name]\n```"
    },
    {
        "original": "def disconnect(service_instance):\n    \"\"\"\n    Function that disconnects from the vCenter server or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)",
        "rewrite": "Here is the revised code:\n\n```\ndef disconnect(service_instance):\n    log.trace('Disconnecting')\n    try:\n        service_instance.RetrieveContent().unbind()\n    except (vim.fault.NoPermission, vim.fault.VimFault, vmodl.RuntimeFault) as exc:\n        log.exception(exc)\n        if isinstance(exc, vim.fault.NoPermission):\n            raise salt.exceptions.VMwareApiError(f'Not enough permissions. Required privilege: {exc.privilegeId}')\n        else:\n"
    },
    {
        "original": "def h2i(self, pkt, seconds):\n        \"\"\"Convert the number of seconds since 1-Jan-70 UTC to the packed\n           representation.\"\"\"\n\n        if seconds is None:\n            seconds = 0\n\n        tmp_short = (seconds >> 32) & 0xFFFF\n        tmp_int = seconds & 0xFFFFFFFF\n\n        return struct.pack(\"!HI\", tmp_short, tmp_int)",
        "rewrite": "Here is the revised code:\n\n```\nimport struct\n\ndef h2i(self, pkt, seconds):\n    if seconds is None:\n        seconds = 0\n    tmp_short = (seconds // 2**16) & 0xFFFF\n    tmp_int = seconds & 0xFFFFFFFF\n    return struct.pack(\"!HI\", tmp_short, tmp_int)\n```"
    },
    {
        "original": "def notebook_start(self, **kwargs):\n        \"\"\"\n        Initialize a notebook, clearing its metadata, and save it.\n\n        When starting a notebook, this initializes and clears the metadata for\n        the notebook and its cells, and saves the notebook to the given\n        output path.\n\n        Called by Engine when execution begins.\n        \"\"\"\n        self.set_timer()\n\n        self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n        self.nb.metadata.papermill['end_time'] = None\n        self.nb.metadata.papermill['duration'] = None\n        self.nb.metadata.papermill['exception'] = None\n\n        for cell in self.nb.cells:\n            # Reset the cell execution counts.\n            if cell.get(\"execution_count\") is not None:\n                cell.execution_count = None\n\n            # Clear out the papermill metadata for each cell.\n            cell.metadata.papermill = dict(\n                exception=None,\n                start_time=None,\n                end_time=None,\n                duration=None,\n                status=self.PENDING,  # pending, running, completed\n            )\n            if cell.get(\"outputs\") is not None:\n                cell.outputs = []\n\n        self.save()",
        "rewrite": "Here is your revised code:\n\n```\ndef notebook_start(self):\n    self.set_timer()\n    self.nb.metadata\u7eb8permill.update({\n       'start_time':self.start_time.isoformat(),\n       'end_time':None,\n       'duration':None,\n       'exception':None\n    })\n    \n    for cell in self(nb.cells):\n         if 'execution_count' in cell:\n              del(cell['execution_count'])\n             \n         if '_uuid' not in \tcell:\n              continue \n         else"
    },
    {
        "original": "def dispatch_request(self, req):\n        \"\"\"\n        Dispatch a request object.\n        \"\"\"\n        log.debug(\"Dispatching request: {}\".format(str(req)))\n\n        # make sure it's valid\n        res = None\n        try:\n            req.validate()\n        except MissingFieldError as e:\n            res = APIMissingFieldErrorResponse(str(e))\n\n        # dispatch the request\n        if not res:\n            try:\n                res = req.dispatch()\n            except Exception as e:\n                msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n                log.exception(msg)\n                res = APIGenericErrorResponse(msg)\n\n        log.debug(\"Response: {}\".format(str(res)))\n\n        return res",
        "rewrite": "Here is the revised code:\n\n```\ndef dispatch_request(self, req):\n    log.debug(f\"Dispatching request: {req}\")\n    \n    try:\n        req.validate()\n    except MissingFieldError as e:\n       return APIMissingFieldErrorResponse(str(e))\n\n    try:\n       return req.dispatch()\n    except Exception as e:\n       msg = f\"Exception raised while dispatching request: {e!r}\"\n       log.exception(msg)\n       return APIGeneric ErrorResponse(msg)\n```"
    },
    {
        "original": "def _get_ngrams_with_counter(segment, max_order):\n  \"\"\"Extracts all n-grams up to a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n  ngram_counts = collections.Counter()\n  for order in xrange(1, max_order + 1):\n    for i in xrange(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts",
        "rewrite": "Here is the revised code:\n\n```\nfrom collections import Counter\n\ndef _get_ngrams_with_counter(segment, max_order):\n    ngram_counts = Counter()\n    for order in range(1, max_order + 1):\n        for i in range(len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return nigram_counts\n```"
    },
    {
        "original": "def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c",
        "rewrite": "Here's a revised version of the code:\n\n```python\ndef replace_species(self, species_mapping):\n    species_mapping = {get_el_sp(k): v for k, v in species_MAPPING.items()}\n    sp_to_replace = set(species_MAPPING.keys())\n    sp_in_structure = set(self.composition.keys())\n\n    if not sp_to_replace.intersection(sp_in_structure) == sp_to_replace:\n        warnings.warn(\"Some species to be substituted are not present in structure.\")\n\n    for site in self._sites"
    },
    {
        "original": "def show_item_dict(self, item):\n        \"\"\"Returns a json-able dict for show\"\"\"\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, string_types)):\n                v = str(v)\n            d[col] = v\n        return d",
        "rewrite": "```\ndef show_item_dict(self, item):\n    d = {}\n    for col in self.show_columns:\n        v = getattr(item, col)\n        if not isinstance(v, (int, float)) and not issubclass(type(v), string_types):\n            v = str(v)\n        d[col] = v\n    return d\n```"
    },
    {
        "original": "def FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]",
        "rewrite": "Here is the revised code:\n\n```\ndef filter_range(self, start_Time=None, end_Time=None):\n    if start_Time:\n        start_Time = self._NormalizeTime(start_Time)\n    else:\n        start_Time = float('-inf')\n    \n    if end_Time:\n        end_Time = self._NormalizeTime(end_Time)\n    else:\n        end_Time = float('inf')\n    \n    self.data = [p for p in self.data if start_Time <= p[1] < end_Time]\n"
    },
    {
        "original": "def get_connection(self, command_name, *keys, **options):\n        \"\"\"\n        Get a connection, blocking for ``self.timeout`` until a connection\n        is available from the pool.\n\n        If the connection returned is ``None`` then creates a new connection.\n        Because we use a last-in first-out queue, the existing connections\n        (having been returned to the pool after the initial ``None`` values\n        were added) will be returned before ``None`` values. This means we only\n        create new connections when we need to, i.e.: the actual number of\n        connections will only increase in response to demand.\n        \"\"\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n\n        # Try and get a connection from the pool. If one isn't available within\n        # self.timeout then raise a ``ConnectionError``.\n        connection = None\n        try:\n            connection = self.pool.get(block=True, timeout=self.timeout)\n        except Empty:\n            # Note that this is not caught by the redis client and will be\n            # raised unless handled by application code. If you want never to\n            raise ConnectionError(\"No connection available.\")\n\n        # If the ``connection`` is actually ``None`` then that's a cue to make\n        # a new connection to add to the pool.\n        if connection is None:\n            connection = self.make_connection()\n\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection",
        "rewrite": "Here's an improved version of your method:\n\n```Python\ndef get_connection(self, command_name, keys, options):\n    self._checkpid()\n    try:\n      return self.pool.get(block=True, timeout=self.timeout)\n    except Empty:\n      raise ConnectionError(\"Noconnectionavailable.\")\n    except Exception as e:\n      self.release(None)\n      raise e\n\n    conn = None if conn == 0 else conn \n    if conn is None: \n       conn = self.make_connection() \n"
    },
    {
        "original": "def check_error(res, error_enum):\n  \"\"\"Raise if the result has an error, otherwise return the result.\"\"\"\n  if res.HasField(\"error\"):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = error_enum.Name(res.error)\n    details = getattr(res, \"error_details\", \"<none>\")\n    raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n  return res",
        "rewrite": "Here is the revised code:\n\n```\ndef check_error(res, error_enum):\n  if hasattr(res, 'error') and res.HasField('error'):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = get_error_name(error_enum)(res.error)\n    details = getattr(res, 'error_details', '<none>')\n    raise RequestError(f\"{enum_name}.{error_name}: '{details}'\", res)\n  return res\n\ndef get_error_name(enum):\n  return lambda x"
    },
    {
        "original": "def initialize_plot(self, ranges=None):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = self.layout.get(pos, None)\n            subplot = self.subplots.get(pos, None)\n            ax = self.subaxes.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if None in [view, pos, subplot]:\n                ax.set_axis_off()\n                continue\n            subplot.initialize_plot(ranges=ranges)\n\n        self.adjust_positions()\n        self.drawn = True",
        "rewrite": "Here is a revised version of your code:\n\n```\ndef initialize_plot(self, ranges=None):\n    for pos in self.view_positions:\n        view = self.layout.get(pos)\n        subplot = self.subplots.get(pos)\n        ax = self.subaxes.get(pos)\n        \n        if not all([view, pos, subplot]):\n            ax.set_axis_off()\n            continue\n        \n        subplot.initialize_plot(ranges=ranges)\n\n    self.adjust_positions()\n    self.drawn = True\n```"
    },
    {
        "original": "def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]",
        "rewrite": "Here is the revised code:\n\n```\ndef split(sql, encoding=None):\n    stack = engine.FilterStack()\n    return [str(stmt).strip() for stmt in stack.run(sql, encoding)]\n```"
    },
    {
        "original": "def page_str(self):\n        \"\"\"\n        The RestructuredText documentation page for the enumeration. This is\n        the only API member for the class.\n        \"\"\"\n        tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n        components = (\n            self._ms_name, self._page_title, self._intro_text,\n            self._member_defs\n        )\n        return tmpl % components",
        "rewrite": "Here is the revised code:\n\n```\ndef page_str(self):\n    tmpl = '.. _{}:\\n\\n{}\\n\\n{}\\n\\n----\\n\\n{}'\n    components = (self._ms_name, self._page_title, self._intro_text, self._member_defs)\n    return tmpl.format(*components)\n```"
    },
    {
        "original": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this Attachment.\n\n        :returns: This Attachment, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        attachment = {}\n        if self.file_content is not None:\n            attachment[\"content\"] = self.file_content.get()\n\n        if self.file_type is not None:\n            attachment[\"type\"] = self.file_type.get()\n\n        if self.file_name is not None:\n            attachment[\"filename\"] = self.file_name.get()\n\n        if self.disposition is not None:\n            attachment[\"disposition\"] = self.disposition.get()\n\n        if self.content_id is not None:\n            attachment[\"content_id\"] = self.content_id.get()\n        return attachment",
        "rewrite": "```\ndef get(self):\n    attachment = {}\n    for attr, key in [(self.file_content, \"content\"), (self.file_type, \"type\"), (self.file_name, \"filename\"), (self.disposition, \"disposition\"), (self.content_id, \"content_id\")]:\n        if attr is not None:\n            attachment[key] = attr.get()\n    return attachment\n```"
    },
    {
        "original": "def load_ner_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"ner{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  try:\n    return pickle.load(fh)\n  except UnicodeDecodeError:\n    fh.seek(0)\n    return pickle.load(fh, encoding='latin1')",
        "rewrite": "Here is the revised code:\n\n```\ndef load_ner_model(lang=\"en\", version=\"2\"):\n  src_dir = f\"ner{version}\"\n  p = locate_resource(src_dir, lang)\n  with _open(p) as fh:\n    try:\n      return pickle.load(fh)\n    except UnicodeDecodeError:\n      fh.seek(0)\n      return pickle.load(fh, encoding='latin1')\n```"
    },
    {
        "original": "def _reset_state_mode(self, state, mode):\n        \"\"\"\n        Reset the state mode to the given mode, and apply the custom state options specified with this analysis.\n\n        :param state:    The state to work with.\n        :param str mode: The state mode.\n        :return:         None\n        \"\"\"\n\n        state.set_mode(mode)\n        state.options |= self._state_add_options\n        state.options = state.options.difference(self._state_remove_options)",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef _reset_state_mode(self, state, mode):\n    if not isinstance(mode, str):\n        raise TypeError(\"Mode must be a string\")\n    \n    if not isinstance(state, State):  # Assuming State is a class\n        raise TypeError(\"State must be an instance of State\")\n\n    if not hasattr(state, 'set_mode') or not callable(getattr(state, 'set_mode')):\n        raise AttributeError(\"State object must have a set"
    },
    {
        "original": "def normalize_words(self, ord=2, inplace=False):\n    \"\"\"Normalize embeddings matrix row-wise.\n\n    Args:\n      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n    \"\"\"\n    if ord == 2:\n      ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n      self.vectors = vectors.T\n      return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)",
        "rewrite": "Here is the revised code:\n\n```\ndef normalize_words(self, ord=2, inplace=False):\n    if ord == 2:\n        ord = None\n    vectors = (self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)).T\n    if inplace:\n        self.vectors = vectors\n        return self\n    return Embedding(vectors=vectors, vocabulary=self.vocabulary)\n```"
    },
    {
        "original": "def DEFINE_integer(self, name, default, help, constant=False):\n    \"\"\"A helper for defining integer options.\"\"\"\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)",
        "rewrite": "Here is the revised code:\n\n```\ndef define_integer(self, name: str, default: int, help: str, constant: bool = False):\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant\n    )\n```"
    },
    {
        "original": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != f.data.RUNNING,\n        timeout=timeout)\n    if f.data.state != f.data.TERMINATED:\n      raise errors.FlowFailedError(\n          \"Flow %s (%s) failed: %s\" %\n          (self.flow_id, self.client_id, f.data.context.current_state))\n    return f",
        "rewrite": "Here is the revised code:\n\n```\ndef WaitUntilDone(self, timeout=None):\n    if timeout is None:\n        timeout = 3600  # default to 1 hour\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != self.RUNNING,\n        timeout=timeout)\n    if f.data.state != self.TERMINATED:\n        raise errors.FlowFailedError(f\"Flow {self.flow_id} ({self.client_id}) failed:"
    },
    {
        "original": "def date_this_month(self, before_today=True, after_today=False):\n        \"\"\"\n        Gets a Date object for the current month.\n\n        :param before_today: include days in current month before today\n        :param after_today: include days in current month after today\n        :param tzinfo: timezone, instance of datetime.tzinfo subclass\n        :example DateTime('2012-04-04 11:02:02')\n        :return DateTime\n        \"\"\"\n        today = date.today()\n        this_month_start = today.replace(day=1)\n\n        next_month_start = this_month_start + \\\n            relativedelta.relativedelta(months=1)\n        if before_today and after_today:\n            return self.date_between_dates(this_month_start, next_month_start)\n        elif not before_today and after_today:\n            return self.date_between_dates(today, next_month_start)\n        elif not after_today and before_today:\n            return self.date_between_dates(this_month_start, today)\n        else:\n            return today",
        "rewrite": "Here is the revised code:\n\n```\ndef date_this_month(self, before\u4eca\u5929=True, after\u4eca\u5929=False):\n    today = date.today()\n    this_month_start = today.replace(day=1)\n    next\u6708start_date = this\u6708start_date + relativedelta.relativedelta(months=1)\n\n    if \u0642\u0628\u0644\u4eca\u5929and \u0628\u0639\u062f\u4eca\u5929:\n    \treturn self.date_between_dates(this\u6708start_date,next\u6708start_date )\n    elif Complexitynot\u06f0before\u4eca\u5929and \u0628\u0639\u062f\u062aody :\n    \treturn self"
    },
    {
        "original": "def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \"\"\" Update tree feature weights using decision path method.\n    \"\"\"\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff",
        "rewrite": "Here is the revised code:\n\n```\ndef _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \n    tree_value = np.squeeze(clf.tree_.valueAxis=1 if clf.tree_.value.shape[1] == 1 else 2)\n    \n    tree_feature = clf.tree_.feature\n    \n    _, indices = clf.decision_path(X).nonzero()\n    \n    norm_func = lambda x: x / x.sum() if isinstance(clf, DecisionTreeClassifier)"
    },
    {
        "original": "def get_team_push_restrictions(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/branches/:branch/protection/restrictions/teams <https://developer.github.com/v3/repos/branches>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Team.Team,\n            self._requester,\n            self.protection_url + \"/restrictions/teams\",\n            None\n        )",
        "rewrite": "Here is the revised code:\n\n```\ndef get_team_push_restrictions(self):\n    return github.PaginatedList(\n        Team,\n        self._requester,\n        f\"{self.protection_url}/restrictions/teams\",\n    )\n```"
    },
    {
        "original": "def url(self, schemes=None):\n        \"\"\"\n        :param schemes: a list of strings to use as schemes, one will chosen randomly.\n        If None, it will generate http and https urls.\n        Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n        :returns: a random url string.\n        \"\"\"\n        if schemes is None:\n            schemes = ['http', 'https']\n\n        pattern = '{}://{}'.format(\n            self.random_element(schemes) if schemes else \"\",\n            self.random_element(self.url_formats),\n        )\n\n        return self.generator.parse(pattern)",
        "rewrite": "Here is the revised code:\n\n```\ndef url(self, schemes=None):\n    if schemes is None:\n        schemes = ['http', 'https']\n    pattern = '{}://{}'.format(\n        self.random_element(schemes) if schemes else \"\",\n        self.random_element(self.url_formats),\n    )\n    return self.generator.parse(pattern)\n```"
    },
    {
        "original": "def get_data():\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame\n\n\t\tI.e.,\n\t\t>>> convention_df.iloc[0]\n\t\tcategory                                                    plot\n\t\tfilename                 subjectivity_html/obj/2002/Abandon.html\n\t\ttext           A senior at an elite college (Katie Holmes), a...\n\t\tmovie_name                                               abandon\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdata_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n\t\texcept:\n\t\t\turl = ROTTEN_TOMATOES_DATA_URL\n\t\t\tdata_stream = urlopen(url).read()\n\t\treturn pd.read_csv(io.BytesIO(bz2.decompress(data_stream)))",
        "rewrite": "Here is the revised code:\n\n```\nimport bz2\nimport io\nimport pkgutil\nimport urllib.request as urlopen\nimport pandas as pd\n\nROTTEN_TOMATOES_DATA_URL = 'https://nlp.stanford.edu/~ rhetedata/noName/result.csv.bz2'\n\ndef get_data():\n    try:\n        data_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n    except:\n        url = ROTTEN"
    },
    {
        "original": "def add_reward_function(self):\n        \"\"\"\n        add reward function tag to pomdpx model\n\n        Return\n        ---------------\n        string containing the xml for reward function tag\n        \"\"\"\n        reward_function = self.model['reward_function']\n        for condition in reward_function:\n            condprob = etree.SubElement(self.reward_function, 'Func')\n            self.add_conditions(condition, condprob)\n        return self.__str__(self.reward_function)[:-1]",
        "rewrite": "```\ndef add_reward_function(self):\n    self.reward_function = etree.Element('RewardFunction')\n    for condition in self.model['reward_function']:\n        condprob = etree.SubElement(self.reward_function, 'Func')\n        self.add_conditions(condition, condprob)\n    return etree.tostring(self.reward_function).decode()\n```"
    },
    {
        "original": "def get_all_values(self):\n        \"\"\"Returns a list of lists containing all cells' values as strings.\n\n        .. note::\n\n            Empty trailing rows and columns will not be included.\n        \"\"\"\n\n        data = self.spreadsheet.values_get(self.title)\n\n        try:\n            return fill_gaps(data['values'])\n        except KeyError:\n            return []",
        "rewrite": "```\ndef get_all_values(self):\n    data = self.spreadsheet.values_get(self.title)\n    return fill_gaps(data.get('values', []))\n```"
    },
    {
        "original": "def modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n        \"\"\"\n        \u4fee\u6539\u5c0f\u7a0b\u5e8f\u670d\u52a1\u5668\u6388\u6743\u57df\u540d\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1489138143_WPbOO\n\n        :param action: \u589e\u5220\u6539\u67e5\u7684\u64cd\u4f5c\u7c7b\u578b\uff0c\u4ec5\u652f\u6301 'add', 'delete', 'set', 'get'\n        :param request_domain: request \u5408\u6cd5\u57df\u540d\n        :param wsrequest_domain: socket \u5408\u6cd5\u57df\u540d\n        :param upload_domain: upload file \u5408\u6cd5\u57df\u540d\n        :param download_domain: download file \u5408\u6cd5\u57df\u540d\n        \"\"\"\n        return self._post(\n            'wxa/modify_domain',\n            data={\n                'action': action,\n                'requestdomain': request_domain,\n                'wsrequestdomain': wsrequest_domain,\n                'uploaddomain': upload_domain,\n                'downloaddomain': download_domain,\n            }\n        )",
        "rewrite": "Here is the revised code with some improvements:\n\n```Python\ndef modify_DOMAIN(self, action: str, *, \n                   request_domains: tuple = (), \n                   ws_request_domains: tuple = (), \n                   upload_domains: tuple = (), \n                   download_domains: tuple = ()):\n    return self._post(\n        'wxa/modify_DOMAIN',\n        data={\n            'action': action,\n            'requestdomain': list(request_domains),\n            'wsrequestdomain': list(ws_request_domains),\n           "
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "rewrite": "Here is the revised code:\n\n```\ndef list_children(self, urn, limit=None, age=NEWEST_TIME):\n    _, children_urns = self.multi_list_children([urn], limit=limit, age=age)[0]\n    return children_urns\n```"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "rewrite": "Here is the revised code:\n\n```\ndef saveFile(self):\n    filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n        self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n    if filepath:\n        if not filepath.endswith(\".ag\"):\n            filepath += \".ag\"\n\n        self.showStatus(f\"Saving {filepath}...\")\n        self.saveSession(filepath)\n        self.showStatus(f\"Saved Session to {filepath}!\")\n```"
    },
    {
        "original": "def Copy(self, field_number=None):\n    \"\"\"Returns descriptor copy, optionally changing field number.\"\"\"\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n      new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)",
        "rewrite": "Here is the revised code:\n\n```\ndef Copy(self, field_number=None):\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n        new_args[\"field_number\"] = field_number\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args\n    )\n```"
    }
]