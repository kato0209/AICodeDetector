[
    {
        "original": "def add_run(self, text=None, style=None):\n        \"\"\"\n        Append a run to this paragraph containing *text* and having character\n        style identified by style ID *style*. *text* can contain tab\n        (``\\\\t``) characters, which are converted to the appropriate XML form\n        for a tab. *text* can also include newline (``\\\\n``) or carriage\n        return (``\\\\r``) characters, each of which is converted to a line\n        break.\n        \"\"\"\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run",
        "rewrite": "Revise the code with your best effort: \"def add_run(self, text=None, style=None):\n        \"\"\"\n        Append a run to this paragraph containing *text* and having character\n        style identified by style ID *style*. *text* can contain tab\n        (``\\\\t``) characters, which are converted to the appropriate XML form\n        for a tab. *text* can also include newline (``\\\\n``) or carriage\n        return (``\\\\r``) characters, each of which is converted to a line\n        break.\n        \"\"\"\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run\" . No need to explain. Just write code:\n\n\\begin{code}\ndef add_run(self, text=None, style=None):\n    \"\"\"\n    Append a run to this paragraph containing *text* and having character\n    style identified by style ID *style*. *text* can contain tab\n    (``\\\\t``) characters, which are converted to the appropriate XML form\n    for a tab. *text* can also include newline (``\\\\n``) or carriage\n    return (``\\\\r``) characters, each of which is converted to a line\n    break.\n    \"\"\"\n    r = self._"
    },
    {
        "original": "def _read_execute_info(path, parents):\n  \"\"\"Read the ExecuteInfo.txt file and return the base directory.\"\"\"\n  path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n  if os.path.exists(path):\n    with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n      for line in f:\n        parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n        if len(parts) == 2 and parts[0] == \"executable\":\n          exec_path = parts[1].replace(\"\\\\\", \"/\")  # For windows compatibility.\n          for _ in range(parents):\n            exec_path = os.path.dirname(exec_path)\n          return exec_path",
        "rewrite": "Revise the code with your best effort: \"def _read_execute_info(path, parents):\n  \"\"\"Read the ExecuteInfo.txt file and return the base directory.\"\"\"\n  path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n  if os.path.exists(path):\n    with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n      for line in f:\n        parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n        if len(parts) == 2 and parts[0] == \"executable\":\n          exec_path = parts[1].replace(\"\\\\\", \"/\")  # For windows compatibility.\n          for _ in range(parents):\n            exec_path = os.path.dirname(exec_path)\n          return exec_path\" . No need to explain. Just write code:\n\ndef _read_execute_info(path, parents):\n    \"\"\"Read the ExecuteInfo.txt file and return the base directory.\"\"\"\n    path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n    if os.path.exists(path):\n        with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n            for line in f:\n                parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n                if len"
    },
    {
        "original": "def _modify_eni_properties(eni_id, properties=None, vm_=None):\n    \"\"\"\n    Change properties of the interface\n    with id eni_id to the values in properties dict\n    \"\"\"\n    if not isinstance(properties, dict):\n        raise SaltCloudException(\n            'ENI properties must be a dictionary'\n        )\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute',\n              'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params[k] = v\n\n    result = aws.query(params,\n                       return_root=True,\n                       location=get_location(vm_),\n                       provider=get_provider(),\n                       opts=__opts__,\n                       sigver='4')\n\n    if isinstance(result, dict) and result.get('error'):\n        raise SaltCloudException(\n            'Could not change interface <{0}> attributes <\\'{1}\\'>'.format(\n                eni_id, properties\n            )\n        )\n    else:\n        return result",
        "rewrite": "Revise the code with your best effort: \"def _modify_eni_properties(eni_id, properties=None, vm_=None):\n    \"\"\"\n    Change properties of the interface\n    with id eni_id to the values in properties dict\n    \"\"\"\n    if not isinstance(properties, dict):\n        raise SaltCloudException(\n            'ENI properties must be a dictionary'\n        )\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute',\n              'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params[k] = v\n\n    result = aws.query(params,\n                       return_root=True,\n                       location=get_location(vm_),\n                       provider=get_provider(),\n                       opts=__opts__,\n                       sigver='4')\n\n    if isinstance(result, dict) and result.get('error'):\n        raise SaltCloudException(\n            'Could not change interface <{0}> attributes <\\'{1}\\'>'.format(\n                eni_id, properties\n            )\n        )\n    else:\n        return result\" . No need to explain. Just write code:\n\ndef _modify_eni_properties(eni_id, properties=None, vm_=None):\n    \"\"\"\n    Change properties of the interface\n    with id eni_id to the values in properties dict\n    \"\"\"\n    if not isinstance(properties, dict):\n        raise SaltCloudException(\n            'ENI properties must be a dictionary'\n        )\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute',\n              'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params["
    },
    {
        "original": "def data(self):\n        \"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs",
        "rewrite": "Revise the code with your best effort: \"def data(self):\n        \"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def Start(self):\n    \"\"\"This uploads the rules to the foreman and, thus, starts the hunt.\"\"\"\n    # We are already running.\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n      return\n\n    # Determine when this hunt will expire.\n    self.context.duration = self.runner_args.expiry_time\n\n    # When the next client can be scheduled. Implements gradual client\n    # recruitment rate according to the client_rate.\n    self.context.next_client_due = rdfvalue.RDFDatetime.Now()\n\n    self._CreateAuditEvent(\"HUNT_STARTED\")\n\n    # Start the hunt.\n    self.hunt_obj.Set(self.hunt_obj.Schema.STATE(\"STARTED\"))\n    self.hunt_obj.Flush()\n\n    if self.runner_args.add_foreman_rules:\n      self._AddForemanRule()",
        "rewrite": "Revise the code with your best effort: \"def Start(self):\n    \"\"\"This uploads the rules to the foreman and, thus, starts the hunt.\"\"\"\n    # We are already running.\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n      return\n\n    # Determine when this hunt will expire.\n    self.context.duration = self.runner_args.expiry_time\n\n    # When the next client can be scheduled. Implements gradual client\n    # recruitment rate according to the client_rate.\n    self.context.next_client_due = rdfvalue.RDFDatetime.Now()\n\n    self._CreateAuditEvent(\"HUNT_STARTED\")\n\n    # Start the hunt.\n    self.hunt_obj.Set(self.hunt_obj.Schema.STATE(\"STARTED\"))\n    self.hunt_obj.Flush()\n\n    if self.runner_args.add_foreman_rules:\n      self._AddForemanRule()\" . No need to explain. Just write code:\n\ndef Start(self):\n    \"\"\"This uploads the rules to the foreman and, thus, starts the hunt.\"\"\"\n    # We are already running.\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n      return\n\n    # Determine when this hunt will expire.\n    self.context.duration = self.runner_args.expiry_time\n\n    # When the next client can be scheduled. Implements gradual client\n    # recruitment rate according to the"
    },
    {
        "original": "def _partition_spec(self, shape, partition_info):\n    \"\"\"Build magic (and sparsely documented) shapes_and_slices spec string.\"\"\"\n    if partition_info is None:\n      return ''  # Empty string indicates a non-partitioned tensor.\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec",
        "rewrite": "Revise the code with your best effort: \"def _partition_spec(self, shape, partition_info):\n    \"\"\"Build magic (and sparsely documented) shapes_and_slices spec string.\"\"\"\n    if partition_info is None:\n      return ''  # Empty string indicates a non-partitioned tensor.\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec\" . No need to explain. Just write code:\n\ndef _partition_spec(self, shape, partition_info):\n    if partition_info is None:\n        return ''\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def connection_id_to_endpoint(self, connection_id):\n        \"\"\"\n        Get stored public key for a connection.\n        \"\"\"\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.uri\n            except KeyError:\n                return None",
        "rewrite": "Revise the code with your best effort: \"def connection_id_to_endpoint(self, connection_id):\n        \"\"\"\n        Get stored public key for a connection.\n        \"\"\"\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.uri\n            except KeyError:\n                return None\" . No need to explain. Just write code:\n\n\ndef connection_id_to_endpoint(self, connection_id):\n    \"\"\"\n    Get stored public key for a connection.\n    \"\"\"\n    with self._connections_lock:\n        try:\n            connection_info = self._connections[connection_id]\n            return connection_info.uri\n        except KeyError:\n            return None\n\n\ndef connection_id_to_endpoint(self, connection_id):\n    \"\"\"\n    Get stored public key for a connection.\n    \"\"\"\n    with self._connections_lock:\n        try"
    },
    {
        "original": "def _ValidateAFF4Type(aff4_type):\n  \"\"\"Validates an AFF4 type.\"\"\"\n  if aff4_type is None:\n    return\n\n  if not isinstance(aff4_type, type):\n    raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n  if not issubclass(aff4_type, AFF4Object):\n    raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" %\n                    aff4_type)",
        "rewrite": "Revise the code with your best effort: \"def _ValidateAFF4Type(aff4_type):\n  \"\"\"Validates an AFF4 type.\"\"\"\n  if aff4_type is None:\n    return\n\n  if not isinstance(aff4_type, type):\n    raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n  if not issubclass(aff4_type, AFF4Object):\n    raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" %\n                    aff4_type)\" . No need to explain. Just write code:\n\ndef _ValidateAFF4Type(aff4_type):\n  \"\"\"Validates an AFF4 type.\"\"\"\n  if aff4_type is None:\n    return\n\n  if not isinstance(aff4_type, type):\n    raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n  if not issubclass(aff4_type, AFF4Object):\n    raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" %\n                    aff4_type)\n\n"
    },
    {
        "original": "def KernelVersion():\n  \"\"\"Gets the kernel version as string, eg. \"5.1.2600\".\n\n  Returns:\n    The kernel version, or \"unknown\" in the case of failure.\n  \"\"\"\n  rtl_osversioninfoexw = RtlOSVersionInfoExw()\n  try:\n    RtlGetVersion(rtl_osversioninfoexw)\n  except OSError:\n    return \"unknown\"\n\n  return \"%d.%d.%d\" % (rtl_osversioninfoexw.dwMajorVersion,\n                       rtl_osversioninfoexw.dwMinorVersion,\n                       rtl_osversioninfoexw.dwBuildNumber)",
        "rewrite": "Revise the code with your best effort: \"def KernelVersion():\n  \"\"\"Gets the kernel version as string, eg. \"5.1.2600\".\n\n  Returns:\n    The kernel version, or \"unknown\" in the case of failure.\n  \"\"\"\n  rtl_osversioninfoexw = RtlOSVersionInfoExw()\n  try:\n    RtlGetVersion(rtl_osversioninfoexw)\n  except OSError:\n    return \"unknown\"\n\n  return \"%d.%d.%d\" % (rtl_osversioninfoexw.dwMajorVersion,\n                       rtl_osversioninfoexw.dwMinorVersion,\n                       rtl_osversioninfoexw.dwBuildNumber)\" . No need to explain. Just write code:\n\ndef KernelVersion():\n  \"\"\"Gets the kernel version as string, eg. \"5.1.2600\".\n\n  Returns:\n    The kernel version, or \"unknown\" in the case of failure.\n  \"\"\"\n  rtl_osversioninfoexw = RtlOSVersionInfoExw()\n  try:\n    RtlGetVersion(rtl_osversioninfoexw)\n  except OSError:\n    return \"unknown\"\n\n  return \"%d.%d.%d\" % (rtl_osversioninfoexw"
    },
    {
        "original": "def guid_to_squid(guid):\n    \"\"\"\n    Converts a GUID   to a compressed guid (SQUID)\n\n    Each Guid has 5 parts separated by '-'. For the first three each one will be\n    totally reversed, and for the remaining two each one will be reversed by\n    every other character. Then the final compressed Guid will be constructed by\n    concatenating all the reversed parts without '-'.\n\n    .. Example::\n\n        Input:                  2BE0FA87-5B36-43CF-95C8-C68D6673FB94\n        Reversed:               78AF0EB2-63B5-FC34-598C-6CD86637BF49\n        Final Compressed Guid:  78AF0EB263B5FC34598C6CD86637BF49\n\n    Args:\n\n        guid (str): A valid GUID\n\n    Returns:\n        str: A valid compressed GUID (SQUID)\n    \"\"\"\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w\\w)(\\w\\w)-(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match is not None:\n        for index in range(1, 12):\n            squid += guid_match.group(index)[::-1]\n    return squid",
        "rewrite": "Revise the code with your best effort: \"def guid_to_squid(guid):\n    \"\"\"\n    Converts a GUID   to a compressed guid (SQUID)\n\n    Each Guid has 5 parts separated by '-'. For the first three each one will be\n    totally reversed, and for the remaining two each one will be reversed by\n    every other character. Then the final compressed Guid will be constructed by\n    concatenating all the reversed parts without '-'.\n\n    .. Example::\n\n        Input:                  2BE0FA87-5B36-43CF-95C8-C68D6673FB94\n        Reversed:               78AF0EB2-63B5-FC34-598C-6CD86637BF49\n        Final Compressed Guid:  78AF0EB263B5FC34598C6CD86637BF49\n\n    Args:\n\n        guid (str): A valid GUID\n\n    Returns:\n        str: A valid compressed GUID (SQUID)\n    \"\"\"\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w\\w)(\\w\\w)-(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match is not None:\n        for index in range(1, 12):\n            squid += guid_match.group(index)[::-1]\n    return squid\" . No need to explain. Just write code:\n\ndef guid_to_squid(guid):\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w\\w)(\\w\\w)-(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match is not None:\n        for index in range(1, "
    },
    {
        "original": "def commit_config(self, message=\"\"):\n        \"\"\"Commit configuration.\"\"\"\n        commit_args = {\"comment\": message} if message else {}\n        self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n        if not self.lock_disable and not self.session_config_lock:\n            self._unlock()",
        "rewrite": "Revise the code with your best effort: \"def commit_config(self, message=\"\"):\n        \"\"\"Commit configuration.\"\"\"\n        commit_args = {\"comment\": message} if message else {}\n        self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n        if not self.lock_disable and not self.session_config_lock:\n            self._unlock()\" . No need to explain. Just write code:\n\ndef commit_config(self, message=\"\"):\n    \"\"\"Commit configuration.\"\"\"\n    commit_args = {\"comment\": message} if message else {}\n    self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n    if not self.lock_disable and not self.session_config_lock:\n        self._unlock()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def fetch_metric(self, cursor, results, tags):\n        \"\"\"\n        Because we need to query the metrics by matching pairs, we can't query\n        all of them together without having to perform some matching based on\n        the name afterwards so instead we query instance by instance.\n        We cache the list of instance so that we don't have to look it up every time\n        \"\"\"\n        if self.sql_name not in results:\n            self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n            return\n\n        tags = tags + self.tags\n\n        results_list = results[self.sql_name]\n        done_instances = []\n        for ndx, row in enumerate(results_list):\n            ctype = row[0]\n            cval = row[1]\n            inst = row[2]\n            object_name = row[3]\n\n            if inst in done_instances:\n                continue\n\n            if (self.instance != ALL_INSTANCES and inst != self.instance) or (\n                self.object_name and object_name != self.object_name\n            ):\n                done_instances.append(inst)\n                continue\n\n            # find the next row which has the same instance\n            cval2 = None\n            ctype2 = None\n            for second_row in results_list[: ndx + 1]:\n                if inst == second_row[2]:\n                    cval2 = second_row[1]\n                    ctype2 = second_row[0]\n\n            if cval2 is None:\n                self.log.warning(\"Couldn't find second value for {}\".format(self.sql_name))\n                continue\n            done_instances.append(inst)\n            if ctype < ctype2:\n                value = cval\n                base = cval2\n            else:\n                value = cval2\n                base = cval\n\n            metric_tags = list(tags)\n            if self.instance == ALL_INSTANCES:\n                metric_tags.append('{}:{}'.format(self.tag_by, inst.strip()))\n            self.report_fraction(value, base, metric_tags)",
        "rewrite": "Revise the code with your best effort: \"def fetch_metric(self, cursor, results, tags):\n        \"\"\"\n        Because we need to query the metrics by matching pairs, we can't query\n        all of them together without having to perform some matching based on\n        the name afterwards so instead we query instance by instance.\n        We cache the list of instance so that we don't have to look it up every time\n        \"\"\"\n        if self.sql_name not in results:\n            self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n            return\n\n        tags = tags + self.tags\n\n        results_list = results[self.sql_name]\n        done_instances = []\n        for ndx, row in enumerate(results_list):\n            ctype = row[0]\n            cval = row[1]\n            inst = row[2]\n            object_name = row[3]\n\n            if inst in done_instances:\n                continue\n\n            if (self.instance != ALL_INSTANCES and inst != self.instance) or (\n                self.object_name and object_name != self.object_name\n            ):\n                done_instances.append(inst)\n                continue\n\n            # find the next row which has the same instance\n            cval2 = None\n            ctype2 = None\n            for second_row in results_list[: ndx + 1]:\n                if inst == second_row[2]:\n                    cval2 = second_row[1]\n                    ctype2 = second_row[0]\n\n            if cval2 is None:\n                self.log.warning(\"Couldn't find second value for {}\".format(self.sql_name))\n                continue\n            done_instances.append(inst)\n            if ctype < ctype2:\n                value = cval\n                base = cval2\n            else:\n                value = cval2\n                base = cval\n\n            metric_tags = list(tags)\n            if self.instance == ALL_INSTANCES:\n                metric_tags.append('{}:{}'.format(self.tag_by, inst.strip()))\n            self.report_fraction(value, base, metric_tags)\" . No need to explain. Just write code:\n\ndef fetch_metric(self, cursor, results, tags):\n    \"\"\"\n    Because we need to query the metrics by matching pairs, we can't query\n    all of them together without having to perform some matching based on\n    the name afterwards so instead we query instance by instance.\n    We cache the list of instance so that we don't have to look it up every time\n    \"\"\"\n    if self.sql_name not in results:\n        self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n        return\n"
    },
    {
        "original": "def apply(key, value):\n    \"\"\"\n    Set a single key\n\n    .. note::\n\n        This will strip comments from your config file\n    \"\"\"\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data, default_flow_style=False)",
        "rewrite": "Revise the code with your best effort: \"def apply(key, value):\n    \"\"\"\n    Set a single key\n\n    .. note::\n\n        This will strip comments from your config file\n    \"\"\"\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data, default_flow_style=False)\" . No need to explain. Just write code:\n\ndef apply(key, value):\n    \"\"\"\n    Set a single key\n\n    .. note::\n\n        This will strip comments from your config file\n    \"\"\"\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data,"
    },
    {
        "original": "def refresh_access_token(self, refresh_token):\n        \"\"\"\u5237\u65b0 access token\n\n        :param refresh_token: OAuth2 refresh token\n        :return: JSON \u6570\u636e\u5305\n        \"\"\"\n        res = self._get(\n            'sns/oauth2/refresh_token',\n            params={\n                'appid': self.app_id,\n                'grant_type': 'refresh_token',\n                'refresh_token': refresh_token\n            }\n        )\n        self.access_token = res['access_token']\n        self.open_id = res['openid']\n        self.refresh_token = res['refresh_token']\n        self.expires_in = res['expires_in']\n        return res",
        "rewrite": "Revise the code with your best effort: \"def refresh_access_token(self, refresh_token):\n        \"\"\"\u5237\u65b0 access token\n\n        :param refresh_token: OAuth2 refresh token\n        :return: JSON \u6570\u636e\u5305\n        \"\"\"\n        res = self._get(\n            'sns/oauth2/refresh_token',\n            params={\n                'appid': self.app_id,\n                'grant_type': 'refresh_token',\n                'refresh_token': refresh_token\n            }\n        )\n        self.access_token = res['access_token']\n        self.open_id = res['openid']\n        self.refresh_token = res['refresh_token']\n        self.expires_in = res['expires_in']\n        return res\" . No need to explain. Just write code:\n\n\ndef refresh_access_token(self, refresh_token):\n    \"\"\"\u5237\u65b0 access token\n\n    :param refresh_token: OAuth2 refresh token\n    :return: JSON \u6570\u636e\u5305\n    \"\"\"\n    res = self._get(\n        'sns/oauth2/refresh_token',\n        params={\n            'appid': self.app_id,\n            'grant_type': 'refresh_token',\n            'refresh_token': refresh_token\n        }\n    )\n    self.access_token = res['"
    },
    {
        "original": "def is_connection_to_a_vcenter(service_instance):\n    \"\"\"\n    Function that returns True if the connection is made to a vCenter Server and\n    False if the connection is made to an ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('api_type = %s', api_type)\n    if api_type == 'VirtualCenter':\n        return True\n    elif api_type == 'HostAgent':\n        return False\n    else:\n        raise salt.exceptions.VMwareApiError(\n            'Unexpected api type \\'{0}\\' . Supported types: '\n            '\\'VirtualCenter/HostAgent\\''.format(api_type))",
        "rewrite": "Revise the code with your best effort: \"def is_connection_to_a_vcenter(service_instance):\n    \"\"\"\n    Function that returns True if the connection is made to a vCenter Server and\n    False if the connection is made to an ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('api_type = %s', api_type)\n    if api_type == 'VirtualCenter':\n        return True\n    elif api_type == 'HostAgent':\n        return False\n    else:\n        raise salt.exceptions.VMwareApiError(\n            'Unexpected api type \\'{0}\\' . Supported types: '\n            '\\'VirtualCenter/HostAgent\\''.format(api_type))\" . No need to explain. Just write code:\n\ndef is_connection_to_a_vcenter(service_instance):\n    \"\"\"\n    Function that returns True if the connection is made to a vCenter Server and\n    False if the connection is made to an ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApi"
    },
    {
        "original": "def add_column(self, name, *, index=0, values=None):\n        \"\"\"\n        Adds a column to the table\n        :param str name: the name of the column\n        :param int index: the index at which the column should be added. Defaults to 0.\n        :param list values: a two dimension array of values to add to the column\n        \"\"\"\n        if name is None:\n            return None\n\n        params = {\n            'name': name,\n            'index': index\n        }\n        if values is not None:\n            params['values'] = values\n\n        url = self.build_url(self._endpoints.get('add_column'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})",
        "rewrite": "Revise the code with your best effort: \"def add_column(self, name, *, index=0, values=None):\n        \"\"\"\n        Adds a column to the table\n        :param str name: the name of the column\n        :param int index: the index at which the column should be added. Defaults to 0.\n        :param list values: a two dimension array of values to add to the column\n        \"\"\"\n        if name is None:\n            return None\n\n        params = {\n            'name': name,\n            'index': index\n        }\n        if values is not None:\n            params['values'] = values\n\n        url = self.build_url(self._endpoints.get('add_column'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})\" . No need to explain. Just write code:\n\ndef add_column(self, name, *, index=0, values=None):\n    \"\"\"\n    Adds a column to the table\n    :param str name: the name of the column\n    :param int index: the index at which the column should be added. Defaults to 0.\n    :param list values: a two dimension array of values to add to the column\n    \"\"\"\n    if name is None:\n        return None\n\n    params = {\n        'name': name,\n        'index': index\n    }\n    if values is not"
    },
    {
        "original": "def __get_stat_display(self, stats, layer):\n        \"\"\"Return a dict of dict with all the stats display.\n        stats: Global stats dict\n        layer: ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n        :returns: dict of dict\n            * key: plugin name\n            * value: dict returned by the get_stats_display Plugin method\n        \"\"\"\n        ret = {}\n\n        for p in stats.getPluginsList(enable=False):\n            if p == 'quicklook' or p == 'processlist':\n                # processlist is done later\n                # because we need to know how many processes could be displayed\n                continue\n\n            # Compute the plugin max size\n            plugin_max_width = None\n            if p in self._left_sidebar:\n                plugin_max_width = max(self._left_sidebar_min_width,\n                                       self.screen.getmaxyx()[1] - 105)\n                plugin_max_width = min(self._left_sidebar_max_width,\n                                       plugin_max_width)\n\n            # Get the view\n            ret[p] = stats.get_plugin(p).get_stats_display(args=self.args,\n                                                           max_width=plugin_max_width)\n\n        return ret",
        "rewrite": "Revise the code with your best effort: \"def __get_stat_display(self, stats, layer):\n        \"\"\"Return a dict of dict with all the stats display.\n        stats: Global stats dict\n        layer: ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n        :returns: dict of dict\n            * key: plugin name\n            * value: dict returned by the get_stats_display Plugin method\n        \"\"\"\n        ret = {}\n\n        for p in stats.getPluginsList(enable=False):\n            if p == 'quicklook' or p == 'processlist':\n                # processlist is done later\n                # because we need to know how many processes could be displayed\n                continue\n\n            # Compute the plugin max size\n            plugin_max_width = None\n            if p in self._left_sidebar:\n                plugin_max_width = max(self._left_sidebar_min_width,\n                                       self.screen.getmaxyx()[1] - 105)\n                plugin_max_width = min(self._left_sidebar_max_width,\n                                       plugin_max_width)\n\n            # Get the view\n            ret[p] = stats.get_plugin(p).get_stats_display(args=self.args,\n                                                           max_width=plugin_max_width)\n\n        return ret\" . No need to explain. Just write code:\n\ndef __get_stat_display(self, stats, layer):\n    \"\"\"Return a dict of dict with all the stats display.\n    stats: Global stats dict\n    layer: ~ cs_status\n        \"None\": standalone or server mode\n        \"Connected\": Client is connected to a Glances server\n        \"SNMP\": Client is connected to a SNMP server\n        \"Disconnected\": Client is disconnected from the server\n\n    :returns: dict of dict\n        * key: plugin name\n        * value: dict returned by the get_stats_display Plugin"
    },
    {
        "original": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)",
        "rewrite": "Revise the code with your best effort: \"def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)\" . No need to explain. Just write code:\n\ndef delete(self, id, **kwargs):\n    \"\"\"Delete an object on the server.\n\n    Args:\n        id: ID of the object to delete\n        **kwargs: Extra options to send to the server (e.g. sudo)\n\n    Raises:\n        GitlabAuthenticationError: If authentication is not correct\n        GitlabDeleteError: If the server cannot perform the request\n    \"\"\"\n    if id is None:\n        path = self.path\n    else:\n        if not isinstance(id, int):\n            id = id"
    },
    {
        "original": "def _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call",
        "rewrite": "Revise the code with your best effort: \"def _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call\" . No need to explain. Just write code:\n\ndef _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n   "
    },
    {
        "original": "def _build_next_request(self, verb, prior_request, prior_response):\n        \"\"\"Builds pagination-aware request object.\n\n        More details:\n          https://developers.google.com/api-client-library/python/guide/pagination\n\n        Args:\n            verb (str): Request verb (ex. insert, update, delete).\n            prior_request (httplib2.HttpRequest): Request that may trigger\n                paging.\n            prior_response (dict): Potentially partial response.\n\n        Returns:\n            httplib2.HttpRequest: HttpRequest or None. None is returned when\n                there is nothing more to fetch - request completed.\n        \"\"\"\n        method = getattr(self._component, verb + '_next')\n        return method(prior_request, prior_response)",
        "rewrite": "Revise the code with your best effort: \"def _build_next_request(self, verb, prior_request, prior_response):\n        \"\"\"Builds pagination-aware request object.\n\n        More details:\n          https://developers.google.com/api-client-library/python/guide/pagination\n\n        Args:\n            verb (str): Request verb (ex. insert, update, delete).\n            prior_request (httplib2.HttpRequest): Request that may trigger\n                paging.\n            prior_response (dict): Potentially partial response.\n\n        Returns:\n            httplib2.HttpRequest: HttpRequest or None. None is returned when\n                there is nothing more to fetch - request completed.\n        \"\"\"\n        method = getattr(self._component, verb + '_next')\n        return method(prior_request, prior_response)\" . No need to explain. Just write code:\n\ndef _build_next_request(self, verb, prior_request, prior_response):\n    method = getattr(self._component, verb + '_next')\n    return method(prior_request, prior_response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def _gpinv(probs, kappa, sigma):\n    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)  # pylint: disable=invalid-unary-operand-type\n        else:\n            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n        x *= sigma\n    else:\n        if np.abs(kappa) < np.finfo(float).eps:\n            x[ok] = -np.log1p(-probs[ok])  # pylint: disable=unsupported-assignment-operation, E1130\n        else:\n            x[ok] = (  # pylint: disable=unsupported-assignment-operation\n                np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n            )\n        x *= sigma\n        x[probs == 0] = 0\n        if kappa >= 0:\n            x[probs == 1] = np.inf  # pylint: disable=unsupported-assignment-operation\n        else:\n            x[probs == 1] = -sigma / kappa  # pylint: disable=unsupported-assignment-operation\n\n    return x",
        "rewrite": "Revise the code with your best effort: \"def _gpinv(probs, kappa, sigma):\n    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)  # pylint: disable=invalid-unary-operand-type\n        else:\n            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n        x *= sigma\n    else:\n        if np.abs(kappa) < np.finfo(float).eps:\n            x[ok] = -np.log1p(-probs[ok])  # pylint: disable=unsupported-assignment-operation, E1130\n        else:\n            x[ok] = (  # pylint: disable=unsupported-assignment-operation\n                np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n            )\n        x *= sigma\n        x[probs == 0] = 0\n        if kappa >= 0:\n            x[probs == 1] = np.inf  # pylint: disable=unsupported-assignment-operation\n        else:\n            x[probs == 1] = -sigma / kappa  # pylint: disable=unsupported-assignment-operation\n\n    return x\" . No need to explain. Just write code:\n\ndef _gpinv(probs, kappa, sigma):\n    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)  # pylint:"
    },
    {
        "original": "def _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    # Process kwargs which are split over both create and host_config\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    # All kwargs should have been consumed by this point, so raise\n    # error if any are left\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    # Fill in any kwargs which need processing by create_host_config first\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        # sort to make consistent for tests\n        create_kwargs['ports'] = [tuple(p.split('/', 1))\n                                  for p in sorted(port_bindings.keys())]\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [\n                v.get('bind') for v in volumes.values()\n            ]\n        else:\n            create_kwargs['volumes'] = [\n                _host_volume_from_bind(v) for v in volumes\n            ]\n    return create_kwargs",
        "rewrite": "Revise the code with your best effort: \"def _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    # Process kwargs which are split over both create and host_config\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    # All kwargs should have been consumed by this point, so raise\n    # error if any are left\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    # Fill in any kwargs which need processing by create_host_config first\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        # sort to make consistent for tests\n        create_kwargs['ports'] = [tuple(p.split('/', 1))\n                                  for p in sorted(port_bindings.keys())]\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [\n                v.get('bind') for v in volumes.values()\n            ]\n        else:\n            create_kwargs['volumes'] = [\n                _host_volume_from_bind(v) for v in volumes\n            ]\n    return create_kwargs\" . No need to explain. Just write code:\n\ndef _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:"
    },
    {
        "original": "def _CollectArtifact(self, artifact, apply_parsers):\n    \"\"\"Returns an `CollectedArtifact` rdf object for the requested artifact.\"\"\"\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n      parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n      parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources,\n                                                   parser_factory):\n      for response in source_result_list:\n        action_result = rdf_artifacts.ClientActionResult()\n        action_result.type = response.__class__.__name__\n        action_result.value = response\n        artifact_result.action_results.append(action_result)\n        self.UpdateKnowledgeBase(response, artifact.provides)\n\n    return artifact_result",
        "rewrite": "Revise the code with your best effort: \"def _CollectArtifact(self, artifact, apply_parsers):\n    \"\"\"Returns an `CollectedArtifact` rdf object for the requested artifact.\"\"\"\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n      parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n      parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources,\n                                                   parser_factory):\n      for response in source_result_list:\n        action_result = rdf_artifacts.ClientActionResult()\n        action_result.type = response.__class__.__name__\n        action_result.value = response\n        artifact_result.action_results.append(action_result)\n        self.UpdateKnowledgeBase(response, artifact.provides)\n\n    return artifact_result\" . No need to explain. Just write code:\n\ndef _CollectArtifact(self, artifact, apply_parsers):\n    \"\"\"Returns an `CollectedArtifact` rdf object for the requested artifact.\"\"\"\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n      parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n      parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources,\n                                                   parser"
    },
    {
        "original": "def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):\n        \"\"\"\n        Moments match of the marginal approximation in EP algorithm\n\n        :param i: number of observation (int)\n        :param tau_i: precision of the cavity distribution (float)\n        :param v_i: mean/variance of the cavity distribution (float)\n        \"\"\"\n        sigma2_hat = 1./(1./self.variance + tau_i)\n        mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n        sum_var = self.variance + 1./tau_i\n        Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5*(data_i - v_i/tau_i)**2./sum_var)\n        return Z_hat, mu_hat, sigma2_hat",
        "rewrite": "Revise the code with your best effort: \"def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):\n        \"\"\"\n        Moments match of the marginal approximation in EP algorithm\n\n        :param i: number of observation (int)\n        :param tau_i: precision of the cavity distribution (float)\n        :param v_i: mean/variance of the cavity distribution (float)\n        \"\"\"\n        sigma2_hat = 1./(1./self.variance + tau_i)\n        mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n        sum_var = self.variance + 1./tau_i\n        Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5*(data_i - v_i/tau_i)**2./sum_var)\n        return Z_hat, mu_hat, sigma2_hat\" . No need to explain. Just write code:\n\n        def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):\n            sigma2_hat = 1./(1./self.variance + tau_i)\n            mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n            sum_var = self.variance + 1./tau_i\n            Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5"
    },
    {
        "original": "def validate_ports_string(ports):\n        \"\"\" Validate that provided string has proper port numbers:\n            1. port number < 65535\n            2. range start < range end\n        \"\"\"\n        pattern = re.compile('^\\\\d+(-\\\\d+)?(,\\\\d+(-\\\\d+)?)*$')\n        if pattern.match(ports) is None:\n            return False\n\n        ranges = PortsRangeHelper._get_string_port_ranges(ports)\n        for r in ranges:\n            if r.start > r.end or r.start > 65535 or r.end > 65535:\n                return False\n        return True",
        "rewrite": "Revise the code with your best effort: \"def validate_ports_string(ports):\n        \"\"\" Validate that provided string has proper port numbers:\n            1. port number < 65535\n            2. range start < range end\n        \"\"\"\n        pattern = re.compile('^\\\\d+(-\\\\d+)?(,\\\\d+(-\\\\d+)?)*$')\n        if pattern.match(ports) is None:\n            return False\n\n        ranges = PortsRangeHelper._get_string_port_ranges(ports)\n        for r in ranges:\n            if r.start > r.end or r.start > 65535 or r.end > 65535:\n                return False\n        return True\" . No need to explain. Just write code:\n\ndef validate_ports_string(ports):\n    pattern = re.compile('^\\\\d+(-\\\\d+)?(,\\\\d+(-\\\\d+)?)*$')\n    if pattern.match(ports) is None:\n        return False\n\n    ranges = PortsRangeHelper._get_string_port_ranges(ports)\n    for r in ranges:\n        if r.start > r.end or r.start > 65535 or r.end > 65535:\n            return False\n    return True\n\n"
    },
    {
        "original": "def strongest_match(cls, overlay, mode, backend=None):\n        \"\"\"\n        Returns the single strongest matching compositor operation\n        given an overlay. If no matches are found, None is returned.\n\n        The best match is defined as the compositor operation with the\n        highest match value as returned by the match_level method.\n        \"\"\"\n        match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                          if op.mode == mode and (not op.backends or backend in op.backends)]\n        matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n        if matches == []: return None\n        else:             return sorted(matches)[0]",
        "rewrite": "Revise the code with your best effort: \"def strongest_match(cls, overlay, mode, backend=None):\n        \"\"\"\n        Returns the single strongest matching compositor operation\n        given an overlay. If no matches are found, None is returned.\n\n        The best match is defined as the compositor operation with the\n        highest match value as returned by the match_level method.\n        \"\"\"\n        match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                          if op.mode == mode and (not op.backends or backend in op.backends)]\n        matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n        if matches == []: return None\n        else:             return sorted(matches)[0]\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict",
        "rewrite": "Revise the code with your best effort: \"def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict\" . No need to explain. Just write code:\n\ndef _to_dict(self):\n    \"\"\"Return a json dictionary representing this model.\"\"\"\n    _dict = {}\n    if hasattr(self, 'label') and self.label is not None:\n        _dict['label'] = self.label\n    if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n        _dict['provenance_ids'] = self.provenance_ids\n    return _dict\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)",
        "rewrite": "Revise the code with your best effort: \"def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)\" . No need to explain. Just write code:\n\ndef _post_master_init(self, master):\n    \"\"\"\n    Function to finish init after connecting to a master\n\n    This is primarily loading modules, pillars, etc. (since they need\n    to know which master they connected to)\n\n    If this function is changed, please check Minion._post_master_init\n    to see if those changes need to be propagated.\n\n    ProxyMinions need a significantly different post master setup,\n    which is why the differences are not factored out into separate helper\n    functions.\n    \"\"\"\n    mp"
    },
    {
        "original": "def get_occupation(self, atom_index, orbital):\n        \"\"\"\n        Returns the occupation for a particular orbital of a particular atom.\n\n        Args:\n            atom_num (int): Index of atom in the PROCAR. It should be noted\n                that VASP uses 1-based indexing for atoms, but this is\n                converted to 0-based indexing in this parser to be\n                consistent with representation of structures in pymatgen.\n            orbital (str): An orbital. If it is a single character, e.g., s,\n                p, d or f, the sum of all s-type, p-type, d-type or f-type\n                orbitals occupations are returned respectively. If it is a\n                specific orbital, e.g., px, dxy, etc., only the occupation\n                of that orbital is returned.\n\n        Returns:\n            Sum occupation of orbital of atom.\n        \"\"\"\n\n        orbital_index = self.orbitals.index(orbital)\n        return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None])\n                for spin, d in self.data.items()}",
        "rewrite": "Revise the code with your best effort: \"def get_occupation(self, atom_index, orbital):\n        \"\"\"\n        Returns the occupation for a particular orbital of a particular atom.\n\n        Args:\n            atom_num (int): Index of atom in the PROCAR. It should be noted\n                that VASP uses 1-based indexing for atoms, but this is\n                converted to 0-based indexing in this parser to be\n                consistent with representation of structures in pymatgen.\n            orbital (str): An orbital. If it is a single character, e.g., s,\n                p, d or f, the sum of all s-type, p-type, d-type or f-type\n                orbitals occupations are returned respectively. If it is a\n                specific orbital, e.g., px, dxy, etc., only the occupation\n                of that orbital is returned.\n\n        Returns:\n            Sum occupation of orbital of atom.\n        \"\"\"\n\n        orbital_index = self.orbitals.index(orbital)\n        return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None])\n                for spin, d in self.data.items()}\" . No need to explain. Just write code:\n\ndef get_occupation(self, atom_index, orbital):\n    \"\"\"\n    Returns the occupation for a particular orbital of a particular atom.\n\n    Args:\n        atom_num (int): Index of atom in the PROCAR. It should be noted\n            that VASP uses 1-based indexing for atoms, but this is\n            converted to 0-based indexing in this parser to be\n            consistent with representation of structures in pymatgen.\n        orbital (str): An orbital. If it is a single character, e.g.,"
    },
    {
        "original": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False",
        "rewrite": "Revise the code with your best effort: \"def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False\" . No need to explain. Just write code:\n\ndef _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n    from ..exceptions import RequestedRangeNotSatisfiable\n\n    if accept_ranges is None:\n        return False\n    self.headers[\"Accept-Ranges\"] = accept_ranges\n    if not self._is_range_request_processable(environ) or complete_length is None:\n        return False\n    parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n    if parsed_range"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n            _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict",
        "rewrite": "Revise the code with your best effort: \"def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n            _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict\" . No need to explain. Just write code:\n\ndef _to_dict(self):\n    \"\"\"Return a json dictionary representing this model.\"\"\"\n    _dict = {}\n    if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n        _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n    if hasattr(self, 'pagination') and self.pagination is not None:\n        _dict['pagination'] = self.pagination._to_dict()\n    return _dict\n\n\n\n\n\n\n"
    },
    {
        "original": "def on_consumer_cancelled(self, method_frame):\n        \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n        receiving messages.\n\n        :param pika.frame.Method method_frame: The Basic.Cancel frame\n        \"\"\"\n        _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n        if self._channel:\n            self._channel.close()",
        "rewrite": "Revise the code with your best effort: \"def on_consumer_cancelled(self, method_frame):\n        \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n        receiving messages.\n\n        :param pika.frame.Method method_frame: The Basic.Cancel frame\n        \"\"\"\n        _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n        if self._channel:\n            self._channel.close()\" . No need to explain. Just write code:\n\ndef on_consumer_cancelled(self, method_frame):\n    \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n    receiving messages.\n\n    :param pika.frame.Method method_frame: The Basic.Cancel frame\n    \"\"\"\n    _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n    if self._channel:\n        self._channel.close()\n\n    # Here we set a new attribute on the connection object. When\n    # Rabbit"
    },
    {
        "original": "def record(self, timestamp, *args, **kwargs):\n        \"\"\" add custom data to data store \"\"\"\n        if self.output_file is None:\n            return\n\n        data = {'datetime': timestamp}\n\n        # append all data\n        if len(args) == 1:\n            if isinstance(args[0], dict):\n                data.update(dict(args[0]))\n            elif isinstance(args[0], pd.DataFrame):\n                data.update(args[0][-1:].to_dict(orient='records')[0])\n\n        # add kwargs\n        if kwargs:\n            data.update(dict(kwargs))\n\n        data['datetime'] = timestamp\n        # self.rows.append(pd.DataFrame(data=data, index=[timestamp]))\n\n        new_data = {}\n        if \"symbol\" not in data.keys():\n            new_data = dict(data)\n        else:\n            sym = data[\"symbol\"]\n            new_data[\"symbol\"] = data[\"symbol\"]\n            for key in data.keys():\n                if key not in ['datetime', 'symbol_group', 'asset_class']:\n                    new_data[sym + '_' + str(key).upper()] = data[key]\n\n        new_data['datetime'] = timestamp\n\n        # append to rows\n        self.rows.append(pd.DataFrame(data=new_data, index=[timestamp]))\n\n        # create dataframe\n        recorded = pd.concat(self.rows, sort=True)\n\n        if \"symbol\" not in recorded.columns:\n            return\n\n\n        # group by symbol\n        recorded['datetime'] = recorded.index\n        data = recorded.groupby(['symbol', 'datetime'], as_index=False).sum()\n        data.set_index('datetime', inplace=True)\n\n        symbols = data['symbol'].unique().tolist()\n        data.drop(columns=['symbol'], inplace=True)\n\n\n        # cleanup:\n\n        # remove symbols\n        recorded.drop(['symbol'] + [sym + '_SYMBOL' for sym in symbols],\n                      axis=1, inplace=True)\n\n        # remove non-option data if not working with options\n        for sym in symbols:\n            try:\n                opt_cols = recorded.columns[\n                    recorded.columns.str.startswith(sym + '_OPT_')].tolist()\n                if len(opt_cols) == len(recorded[opt_cols].isnull().all()):\n                    recorded.drop(opt_cols, axis=1, inplace=True)\n            except Exception as e:\n                pass\n\n        # group df\n        recorded = recorded.groupby(recorded['datetime']).first()\n\n        # shift position\n        for sym in symbols:\n            recorded[sym + '_POSITION'] = recorded[sym + '_POSITION'\n                                                   ].shift(1).fillna(0)\n\n        # make this public\n        self.recorded = recorded.copy()\n\n        # cleanup columns names before saving...\n        recorded.columns = [col.replace('_FUT_', '_').replace(\n                            '_OPT_OPT_', '_OPT_') for col in recorded.columns]\n\n        # save\n        if \".csv\" in self.output_file:\n            recorded.to_csv(self.output_file)\n        elif \".h5\" in self.output_file:\n            recorded.to_hdf(self.output_file, 0)\n        elif (\".pickle\" in self.output_file) | (\".pkl\" in self.output_file):\n            recorded.to_pickle(self.output_file)\n\n        chmod(self.output_file)",
        "rewrite": "Revise the code with your best effort: \"def record(self, timestamp, *args, **kwargs):\n        \"\"\" add custom data to data store \"\"\"\n        if self.output_file is None:\n            return\n\n        data = {'datetime': timestamp}\n\n        # append all data\n        if len(args) == 1:\n            if isinstance(args[0], dict):\n                data.update(dict(args[0]))\n            elif isinstance(args[0], pd.DataFrame):\n                data.update(args[0][-1:].to_dict(orient='records')[0])\n\n        # add kwargs\n        if kwargs:\n            data.update(dict(kwargs))\n\n        data['datetime'] = timestamp\n        # self.rows.append(pd.DataFrame(data=data, index=[timestamp]))\n\n        new_data = {}\n        if \"symbol\" not in data.keys():\n            new_data = dict(data)\n        else:\n            sym = data[\"symbol\"]\n            new_data[\"symbol\"] = data[\"symbol\"]\n            for key in data.keys():\n                if key not in ['datetime', 'symbol_group', 'asset_class']:\n                    new_data[sym + '_' + str(key).upper()] = data[key]\n\n        new_data['datetime'] = timestamp\n\n        # append to rows\n        self.rows.append(pd.DataFrame(data=new_data, index=[timestamp]))\n\n        # create dataframe\n        recorded = pd.concat(self.rows, sort=True)\n\n        if \"symbol\" not in recorded.columns:\n            return\n\n\n        # group by symbol\n        recorded['datetime'] = recorded.index\n        data = recorded.groupby(['symbol', 'datetime'], as_index=False).sum()\n        data.set_index('datetime', inplace=True)\n\n        symbols = data['symbol'].unique().tolist()\n        data.drop(columns=['symbol'], inplace=True)\n\n\n        # cleanup:\n\n        # remove symbols\n        recorded.drop(['symbol'] + [sym + '_SYMBOL' for sym in symbols],\n                      axis=1, inplace=True)\n\n        # remove non-option data if not working with options\n        for sym in symbols:\n            try:\n                opt_cols = recorded.columns[\n                    recorded.columns.str.startswith(sym + '_OPT_')].tolist()\n                if len(opt_cols) == len(recorded[opt_cols].isnull().all()):\n                    recorded.drop(opt_cols, axis=1, inplace=True)\n            except Exception as e:\n                pass\n\n        # group df\n        recorded = recorded.groupby(recorded['datetime']).first()\n\n        # shift position\n        for sym in symbols:\n            recorded[sym + '_POSITION'] = recorded[sym + '_POSITION'\n                                                   ].shift(1).fillna(0)\n\n        # make this public\n        self.recorded = recorded.copy()\n\n        # cleanup columns names before saving...\n        recorded.columns = [col.replace('_FUT_', '_').replace(\n                            '_OPT_OPT_', '_OPT_') for col in recorded.columns]\n\n        # save\n        if \".csv\" in self.output_file:\n            recorded.to_csv(self.output_file)\n        elif \".h5\" in self.output_file:\n            recorded.to_hdf(self.output_file, 0)\n        elif (\".pickle\" in self.output_file) | (\".pkl\" in self.output_file):\n            recorded.to_pickle(self.output_file)\n\n        chmod(self.output_file)\" . No need to explain. Just write code:\n\ndef record(self, timestamp, *args, **kwargs):\n    \"\"\" add custom data to data store \"\"\"\n    if self.output_file is None:\n        return\n\n    data = {'datetime': timestamp}\n\n    # append all data\n    if len(args) == 1:\n        if isinstance(args[0], dict):\n            data.update(dict(args[0]))\n        elif isinstance(args[0], pd.DataFrame):\n            data.update(args[0][-1:].to_dict(orient='records')["
    },
    {
        "original": "def dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n        \"\"\"\n        derivative of logpdf wrt link_f param\n        .. math::\n\n        :param link_f: latent variables link(f)\n        :type link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: likelihood evaluated for this point\n        :rtype: float\n        \"\"\"\n        assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        val = np.log(y) - link_f\n        val_scaled = val/np.sqrt(self.variance)\n        val_scaled2 = val/self.variance\n        uncensored = (1-c)*(val_scaled2)\n        a = (1- stats.norm.cdf(val_scaled))\n        # llg(z) = 1. / (1 - norm_cdf(r / sqrt(s2))). * (1 / sqrt(2 * pi * s2). * exp(-1 / (2. * s2). * r. ^ 2));\n        censored = c*( 1./a) * (np.exp(-1.* val**2 /(2*self.variance)) / np.sqrt(2*np.pi*self.variance))\n        # censored = c * (1. / (1 - stats.norm.cdf(val_scaled))) * (stats.norm.pdf(val_scaled))\n        gradient = uncensored + censored\n        return gradient",
        "rewrite": "Revise the code with your best effort: \"def dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n        \"\"\"\n        derivative of logpdf wrt link_f param\n        .. math::\n\n        :param link_f: latent variables link(f)\n        :type link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: likelihood evaluated for this point\n        :rtype: float\n        \"\"\"\n        assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        val = np.log(y) - link_f\n        val_scaled = val/np.sqrt(self.variance)\n        val_scaled2 = val/self.variance\n        uncensored = (1-c)*(val_scaled2)\n        a = (1- stats.norm.cdf(val_scaled))\n        # llg(z) = 1. / (1 - norm_cdf(r / sqrt(s2))). * (1 / sqrt(2 * pi * s2). * exp(-1 / (2. * s2). * r. ^ 2));\n        censored = c*( 1./a) * (np.exp(-1.* val**2 /(2*self.variance)) / np.sqrt(2*np.pi*self.variance))\n        # censored = c * (1. / (1 - stats.norm.cdf(val_scaled))) * (stats.norm.pdf(val_scaled))\n        gradient = uncensored + censored\n        return gradient\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def _compare_by_version(path1, path2):\n    \"\"\"Returns the current/latest learned path.\n\n    Checks if given paths are from same source/peer and then compares their\n    version number to determine which path is received later. If paths are from\n    different source/peer return None.\n    \"\"\"\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None",
        "rewrite": "Revise the code with your best effort: \"def _compare_by_version(path1, path2):\n    \"\"\"Returns the current/latest learned path.\n\n    Checks if given paths are from same source/peer and then compares their\n    version number to determine which path is received later. If paths are from\n    different source/peer return None.\n    \"\"\"\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None\" . No need to explain. Just write code:\n\ndef _compare_by_version(path1, path2):\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def from_parts(cls, parts):\n        \"\"\"\n        Return content types XML mapping each part in *parts* to the\n        appropriate content type and suitable for storage as\n        ``[Content_Types].xml`` in an OPC package.\n        \"\"\"\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti",
        "rewrite": "Revise the code with your best effort: \"def from_parts(cls, parts):\n        \"\"\"\n        Return content types XML mapping each part in *parts* to the\n        appropriate content type and suitable for storage as\n        ``[Content_Types].xml`` in an OPC package.\n        \"\"\"\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti\" . No need to explain. Just write code:\n\n    def from_parts(cls, parts):\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def ConfigureDatastore(config):\n  \"\"\"Guides the user through configuration of the datastore.\"\"\"\n  print(\"\\n\\n-=GRR Datastore=-\\n\"\n        \"For GRR to work each GRR server has to be able to communicate with\\n\"\n        \"the datastore. To do this we need to configure a datastore.\\n\")\n\n  existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n  if not existing_datastore or existing_datastore == \"FakeDataStore\":\n    ConfigureMySQLDatastore(config)\n    return\n\n  print(\"Found existing settings:\\n  Datastore: %s\" % existing_datastore)\n  if existing_datastore == \"SqliteDataStore\":\n    set_up_mysql = RetryBoolQuestion(\n        \"The SQLite datastore is no longer supported. Would you like to\\n\"\n        \"set up a MySQL datastore? Answering 'no' will abort config \"\n        \"initialization.\", True)\n    if set_up_mysql:\n      print(\"\\nPlease note that no data will be migrated from SQLite to \"\n            \"MySQL.\\n\")\n      ConfigureMySQLDatastore(config)\n    else:\n      raise ConfigInitError()\n  elif existing_datastore == \"MySQLAdvancedDataStore\":\n    print(\"  MySQL Host: %s\\n  MySQL Port: %s\\n  MySQL Database: %s\\n\"\n          \"  MySQL Username: %s\\n\" %\n          (grr_config.CONFIG.Get(\"Mysql.host\"),\n           grr_config.CONFIG.Get(\"Mysql.port\"),\n           grr_config.CONFIG.Get(\"Mysql.database_name\"),\n           grr_config.CONFIG.Get(\"Mysql.database_username\")))\n    if grr_config.CONFIG.Get(\"Mysql.client_key_path\"):\n      print(\"  MySQL client key file: %s\\n\"\n            \"  MySQL client cert file: %s\\n\"\n            \"  MySQL ca cert file: %s\\n\" %\n            (grr_config.CONFIG.Get(\"Mysql.client_key_path\"),\n             grr_config.CONFIG.Get(\"Mysql.client_cert_path\"),\n             grr_config.CONFIG.Get(\"Mysql.ca_cert_path\")))\n\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureMySQLDatastore(config)",
        "rewrite": "Revise the code with your best effort: \"def ConfigureDatastore(config):\n  \"\"\"Guides the user through configuration of the datastore.\"\"\"\n  print(\"\\n\\n-=GRR Datastore=-\\n\"\n        \"For GRR to work each GRR server has to be able to communicate with\\n\"\n        \"the datastore. To do this we need to configure a datastore.\\n\")\n\n  existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n  if not existing_datastore or existing_datastore == \"FakeDataStore\":\n    ConfigureMySQLDatastore(config)\n    return\n\n  print(\"Found existing settings:\\n  Datastore: %s\" % existing_datastore)\n  if existing_datastore == \"SqliteDataStore\":\n    set_up_mysql = RetryBoolQuestion(\n        \"The SQLite datastore is no longer supported. Would you like to\\n\"\n        \"set up a MySQL datastore? Answering 'no' will abort config \"\n        \"initialization.\", True)\n    if set_up_mysql:\n      print(\"\\nPlease note that no data will be migrated from SQLite to \"\n            \"MySQL.\\n\")\n      ConfigureMySQLDatastore(config)\n    else:\n      raise ConfigInitError()\n  elif existing_datastore == \"MySQLAdvancedDataStore\":\n    print(\"  MySQL Host: %s\\n  MySQL Port: %s\\n  MySQL Database: %s\\n\"\n          \"  MySQL Username: %s\\n\" %\n          (grr_config.CONFIG.Get(\"Mysql.host\"),\n           grr_config.CONFIG.Get(\"Mysql.port\"),\n           grr_config.CONFIG.Get(\"Mysql.database_name\"),\n           grr_config.CONFIG.Get(\"Mysql.database_username\")))\n    if grr_config.CONFIG.Get(\"Mysql.client_key_path\"):\n      print(\"  MySQL client key file: %s\\n\"\n            \"  MySQL client cert file: %s\\n\"\n            \"  MySQL ca cert file: %s\\n\" %\n            (grr_config.CONFIG.Get(\"Mysql.client_key_path\"),\n             grr_config.CONFIG.Get(\"Mysql.client_cert_path\"),\n             grr_config.CONFIG.Get(\"Mysql.ca_cert_path\")))\n\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureMySQLDatastore(config)\" . No need to explain. Just write code:\n\ndef ConfigureDatastore(config):\n  \"\"\"Guides the user through configuration of the datastore.\"\"\"\n  print(\"\\n\\n-=GRR Datastore=-\\n\"\n        \"For GRR to work each GRR server has to be able to communicate with\\n\"\n        \"the datastore. To do this we need to configure a datastore.\\n\")\n\n  existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n  if not existing_datastore or existing_datast"
    },
    {
        "original": "def _get_century_code(year):\n        \"\"\"Returns the century code for a given year\"\"\"\n        if 2000 <= year < 3000:\n            separator = 'A'\n        elif 1900 <= year < 2000:\n            separator = '-'\n        elif 1800 <= year < 1900:\n            separator = '+'\n        else:\n            raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n        return separator",
        "rewrite": "Revise the code with your best effort: \"def _get_century_code(year):\n        \"\"\"Returns the century code for a given year\"\"\"\n        if 2000 <= year < 3000:\n            separator = 'A'\n        elif 1900 <= year < 2000:\n            separator = '-'\n        elif 1800 <= year < 1900:\n            separator = '+'\n        else:\n            raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n        return separator\" . No need to explain. Just write code:\n\ndef _get_century_code(year):\n    if 2000 <= year < 3000:\n        return 'A'\n    elif 1900 <= year < 2000:\n        return '-'\n    elif 1800 <= year < 1900:\n        return '+'\n    else:\n        raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def bulk_build(jail, pkg_file, keep=False):\n    \"\"\"\n    Run bulk build on poudriere server.\n\n    Return number of pkg builds, failures, and errors, on error dump to CLI\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -N buildbox_group poudriere.bulk_build 90amd64 /root/pkg_list\n\n    \"\"\"\n    # make sure `pkg file` and jail is on file system\n    if not os.path.isfile(pkg_file):\n        return 'Could not find file {0} on filesystem'.format(pkg_file)\n    if not is_jail(jail):\n        return 'Could not find jail {0}'.format(jail)\n\n    # Generate command\n    if keep:\n        cmd = 'poudriere bulk -k -f {0} -j {1}'.format(pkg_file, jail)\n    else:\n        cmd = 'poudriere bulk -f {0} -j {1}'.format(pkg_file, jail)\n\n    # Bulk build this can take some time, depending on pkg_file ... hours\n    res = __salt__['cmd.run'](cmd)\n    lines = res.splitlines()\n    for line in lines:\n        if \"packages built\" in line:\n            return line\n    return ('There may have been an issue building packages dumping output: '\n            '{0}').format(res)",
        "rewrite": "Revise the code with your best effort: \"def bulk_build(jail, pkg_file, keep=False):\n    \"\"\"\n    Run bulk build on poudriere server.\n\n    Return number of pkg builds, failures, and errors, on error dump to CLI\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -N buildbox_group poudriere.bulk_build 90amd64 /root/pkg_list\n\n    \"\"\"\n    # make sure `pkg file` and jail is on file system\n    if not os.path.isfile(pkg_file):\n        return 'Could not find file {0} on filesystem'.format(pkg_file)\n    if not is_jail(jail):\n        return 'Could not find jail {0}'.format(jail)\n\n    # Generate command\n    if keep:\n        cmd = 'poudriere bulk -k -f {0} -j {1}'.format(pkg_file, jail)\n    else:\n        cmd = 'poudriere bulk -f {0} -j {1}'.format(pkg_file, jail)\n\n    # Bulk build this can take some time, depending on pkg_file ... hours\n    res = __salt__['cmd.run'](cmd)\n    lines = res.splitlines()\n    for line in lines:\n        if \"packages built\" in line:\n            return line\n    return ('There may have been an issue building packages dumping output: '\n            '{0}').format(res)\" . No need to explain. Just write code:\n\ndef bulk_build(jail, pkg_file, keep=False):\n    \"\"\"\n    Run bulk build on poudriere server.\n\n    Return number of pkg builds, failures, and errors, on error dump to CLI\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -N buildbox_group poudriere.bulk_build 90amd64 /root/pkg_list\n\n    \"\"\"\n    # make sure `pkg file` and jail is on file system\n    if not os.path.is"
    },
    {
        "original": "def copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error(\n            'The copy_snapshot function must be called with -f or --function.'\n        )\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A source_snapshot_id must be specified to copy a snapshot.')\n        return False\n\n    if 'description' not in kwargs:\n        kwargs['description'] = ''\n\n    params = {'Action': 'CopySnapshot'}\n\n    if 'source_region' in kwargs:\n        params['SourceRegion'] = kwargs['source_region']\n\n    if 'source_snapshot_id' in kwargs:\n        params['SourceSnapshotId'] = kwargs['source_snapshot_id']\n\n    if 'description' in kwargs:\n        params['Description'] = kwargs['description']\n\n    log.debug(params)\n\n    data = aws.query(params,\n                     return_url=True,\n                     location=get_location(),\n                     provider=get_provider(),\n                     opts=__opts__,\n                     sigver='4')\n    return data",
        "rewrite": "Revise the code with your best effort: \"def copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error(\n            'The copy_snapshot function must be called with -f or --function.'\n        )\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A source_snapshot_id must be specified to copy a snapshot.')\n        return False\n\n    if 'description' not in kwargs:\n        kwargs['description'] = ''\n\n    params = {'Action': 'CopySnapshot'}\n\n    if 'source_region' in kwargs:\n        params['SourceRegion'] = kwargs['source_region']\n\n    if 'source_snapshot_id' in kwargs:\n        params['SourceSnapshotId'] = kwargs['source_snapshot_id']\n\n    if 'description' in kwargs:\n        params['Description'] = kwargs['description']\n\n    log.debug(params)\n\n    data = aws.query(params,\n                     return_url=True,\n                     location=get_location(),\n                     provider=get_provider(),\n                     opts=__opts__,\n                     sigver='4')\n    return data\" . No need to explain. Just write code:\n\ndef copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error(\n            'The copy_snapshot function must be called with -f or --function.'\n        )\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A"
    },
    {
        "original": "def _left_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the left in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n      return r\n    elif not self._upper:\n      return 0\n    elif self._include_diagonal:\n      return r\n    else:\n      return r + 1",
        "rewrite": "Revise the code with your best effort: \"def _left_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the left in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n      return r\n    elif not self._upper:\n      return 0\n    elif self._include_diagonal:\n      return r\n    else:\n      return r + 1\" . No need to explain. Just write code:\n\n\\begin{code}\ndef _left_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the left in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n        return r\n    elif not self._upper:\n        return 0\n    elif self._include_diagonal:\n        return r\n    else:\n        return r + 1\n\\end{code}\n\nComment: What is the question?\n\nComment: I think the question is \"Can you revise the code"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "rewrite": "Revise the code with your best effort: \"def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def InferUserAndSubjectFromUrn(self):\n    \"\"\"Infers user name and subject urn from self.urn.\"\"\"\n    _, hunts_str, hunt_id, user, _ = self.urn.Split(5)\n\n    if hunts_str != \"hunts\":\n      raise access_control.UnauthorizedAccess(\n          \"Approval object has invalid urn %s.\" % self.urn,\n          requested_access=self.token.requested_access)\n\n    return (user, aff4.ROOT_URN.Add(\"hunts\").Add(hunt_id))",
        "rewrite": "Revise the code with your best effort: \"def InferUserAndSubjectFromUrn(self):\n    \"\"\"Infers user name and subject urn from self.urn.\"\"\"\n    _, hunts_str, hunt_id, user, _ = self.urn.Split(5)\n\n    if hunts_str != \"hunts\":\n      raise access_control.UnauthorizedAccess(\n          \"Approval object has invalid urn %s.\" % self.urn,\n          requested_access=self.token.requested_access)\n\n    return (user, aff4.ROOT_URN.Add(\"hunts\").Add(hunt_id))\" . No need to explain. Just write code:\n\ndef InferUserAndSubjectFromUrn(self):\n    \"\"\"Infers user name and subject urn from self.urn.\"\"\"\n    _, hunts_str, hunt_id, user, _ = self.urn.Split(5)\n\n    if hunts_str != \"hunts\":\n      raise access_control.UnauthorizedAccess(\n          \"Approval object has invalid urn %s.\" % self.urn,\n          requested_access=self.token.requested_access)\n\n    return (user, aff4."
    },
    {
        "original": "def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json",
        "rewrite": "Revise the code with your best effort: \"def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json\" . No need to explain. Just write code:\n\ndef _api_config_item(self, item):\n    \"\"\"Glances API RESTful implementation.\n\n    Return the JSON representation of the Glances configuration item\n    HTTP/200 if OK\n    HTTP/400 if item is not found\n    HTTP/404 if others error\n    \"\"\"\n    response.content_type = 'application/json; charset=utf-8'\n\n    config_dict = self.config.as_dict()\n    if item not in config_dict:\n        abort(400, \"Unknown configuration item"
    },
    {
        "original": "async def get_response(self, message=None, *, timeout=None):\n        \"\"\"\n        Returns a coroutine that will resolve once a response arrives.\n\n        Args:\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\n                The message (or the message ID) for which a response\n                is expected. By default this is the last sent message.\n\n            timeout (`int` | `float`, optional):\n                If present, this `timeout` (in seconds) will override the\n                per-action timeout defined for the conversation.\n        \"\"\"\n        return await self._get_message(\n            message, self._response_indices, self._pending_responses, timeout,\n            lambda x, y: True\n        )",
        "rewrite": "Revise the code with your best effort: \"async def get_response(self, message=None, *, timeout=None):\n        \"\"\"\n        Returns a coroutine that will resolve once a response arrives.\n\n        Args:\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\n                The message (or the message ID) for which a response\n                is expected. By default this is the last sent message.\n\n            timeout (`int` | `float`, optional):\n                If present, this `timeout` (in seconds) will override the\n                per-action timeout defined for the conversation.\n        \"\"\"\n        return await self._get_message(\n            message, self._response_indices, self._pending_responses, timeout,\n            lambda x, y: True\n        )\" . No need to explain. Just write code:\n\n        async def get_response(self, message=None, timeout=None):\n            return await self._get_message(\n                message, self._response_indices, self._pending_responses, timeout,\n                lambda x, y: True\n            )\n\n        \"\"\"\n\n        async def get_response(self, message=None, timeout=None):\n            return await self._get_message(\n                message, self._response_indices, self._pending_responses, timeout,\n                lambda x, y: True\n            )\n\n"
    },
    {
        "original": "def _extract_field_with_regex(self, field):\n        \"\"\" extract field from response content with regex.\n            requests.Response body could be json or html text.\n\n        Args:\n            field (str): regex string that matched r\".*\\(.*\\).*\"\n\n        Returns:\n            str: matched content.\n\n        Raises:\n            exceptions.ExtractFailure: If no content matched with regex.\n\n        Examples:\n            >>> # self.text: \"LB123abcRB789\"\n            >>> filed = \"LB[\\d]*(.*)RB[\\d]*\"\n            >>> _extract_field_with_regex(field)\n            abc\n\n        \"\"\"\n        matched = re.search(field, self.text)\n        if not matched:\n            err_msg = u\"Failed to extract data with regex! => {}\\n\".format(field)\n            err_msg += u\"response body: {}\\n\".format(self.text)\n            logger.log_error(err_msg)\n            raise exceptions.ExtractFailure(err_msg)\n\n        return matched.group(1)",
        "rewrite": "Revise the code with your best effort: \"def _extract_field_with_regex(self, field):\n        \"\"\" extract field from response content with regex.\n            requests.Response body could be json or html text.\n\n        Args:\n            field (str): regex string that matched r\".*\\(.*\\).*\"\n\n        Returns:\n            str: matched content.\n\n        Raises:\n            exceptions.ExtractFailure: If no content matched with regex.\n\n        Examples:\n            >>> # self.text: \"LB123abcRB789\"\n            >>> filed = \"LB[\\d]*(.*)RB[\\d]*\"\n            >>> _extract_field_with_regex(field)\n            abc\n\n        \"\"\"\n        matched = re.search(field, self.text)\n        if not matched:\n            err_msg = u\"Failed to extract data with regex! => {}\\n\".format(field)\n            err_msg += u\"response body: {}\\n\".format(self.text)\n            logger.log_error(err_msg)\n            raise exceptions.ExtractFailure(err_msg)\n\n        return matched.group(1)\" . No need to explain. Just write code:\n\ndef _extract_field_with_regex(self, field):\n    \"\"\" extract field from response content with regex.\n        requests.Response body could be json or html text.\n\n    Args:\n        field (str): regex string that matched r\".*\\(.*\\).*\"\n\n    Returns:\n        str: matched content.\n\n    Raises:\n        exceptions.ExtractFailure: If no content matched with regex.\n\n    Examples:\n        >>> # self.text: \"LB123abcRB789\"\n        >>> filed"
    },
    {
        "original": "def get_accumulator_dir(cachedir):\n    \"\"\"\n    Return the directory that accumulator data is stored in, creating it if it\n    doesn't exist.\n    \"\"\"\n    fn_ = os.path.join(cachedir, 'accumulator')\n    if not os.path.isdir(fn_):\n        # accumulator_dir is not present, create it\n        os.makedirs(fn_)\n    return fn_",
        "rewrite": "Revise the code with your best effort: \"def get_accumulator_dir(cachedir):\n    \"\"\"\n    Return the directory that accumulator data is stored in, creating it if it\n    doesn't exist.\n    \"\"\"\n    fn_ = os.path.join(cachedir, 'accumulator')\n    if not os.path.isdir(fn_):\n        # accumulator_dir is not present, create it\n        os.makedirs(fn_)\n    return fn_\" . No need to explain. Just write code:\n\ndef get_accumulator_dir(cachedir):\n    \"\"\"\n    Return the directory that accumulator data is stored in, creating it if it\n    doesn't exist.\n    \"\"\"\n    fn_ = os.path.join(cachedir, 'accumulator')\n    if not os.path.isdir(fn_):\n        # accumulator_dir is not present, create it\n        os.makedirs(fn_)\n    return fn_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "rewrite": "Revise the code with your best effort: \"def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs\" . No need to explain. Just write code:\n\ndef differing_functions_with_consts(self):\n    \"\"\"\n    :return: A list of function matches that appear to differ including just by constants\n    \"\"\"\n    different_funcs = []\n    for (func_a, func_b) in self.function_matches:\n        if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n            different_funcs.append((func_a, func_b))\n    return different_funcs\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt",
        "rewrite": "Revise the code with your best effort: \"def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt\" . No need to explain. Just write code:\n\ndef single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('"
    },
    {
        "original": "def __get_cpu(self):\n        \"\"\"Update and/or return the CPU using the psutil library.\"\"\"\n        # Never update more than 1 time per cached_time\n        if self.timer_cpu.finished():\n            self.cpu_percent = psutil.cpu_percent(interval=0.0)\n            # Reset timer for cache\n            self.timer_cpu = Timer(self.cached_time)\n        return self.cpu_percent",
        "rewrite": "Revise the code with your best effort: \"def __get_cpu(self):\n        \"\"\"Update and/or return the CPU using the psutil library.\"\"\"\n        # Never update more than 1 time per cached_time\n        if self.timer_cpu.finished():\n            self.cpu_percent = psutil.cpu_percent(interval=0.0)\n            # Reset timer for cache\n            self.timer_cpu = Timer(self.cached_time)\n        return self.cpu_percent\" . No need to explain. Just write code:\n\n\\begin{code}\ndef __get_cpu(self):\n    \"\"\"Update and/or return the CPU using the psutil library.\"\"\"\n    # Never update more than 1 time per cached_time\n    if self.timer_cpu.finished():\n        self.cpu_percent = psutil.cpu_percent(interval=0.0)\n        # Reset timer for cache\n        self.timer_cpu = Timer(self.cached_time)\n    return self.cpu_percent\n\\end{code}\n\nComment: I'm not sure"
    },
    {
        "original": "def text_of(relpath):\n    \"\"\"\n    Return string containing the contents of the file at *relpath* relative to\n    this file.\n    \"\"\"\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text",
        "rewrite": "Revise the code with your best effort: \"def text_of(relpath):\n    \"\"\"\n    Return string containing the contents of the file at *relpath* relative to\n    this file.\n    \"\"\"\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text\" . No need to explain. Just write code:\n\ndef text_of(relpath):\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text\n\n\ndef text_of(relpath):\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open("
    },
    {
        "original": "def relativize(self, absolute_address, target_region_id=None):\n        \"\"\"\n        Convert an absolute address to the memory offset in a memory region.\n\n        Note that if an address belongs to heap region is passed in to a stack region map, it will be converted to an\n        offset included in the closest stack frame, and vice versa for passing a stack address to a heap region.\n        Therefore you should only pass in address that belongs to the same category (stack or non-stack) of this region\n        map.\n\n        :param absolute_address:    An absolute memory address\n        :return:                    A tuple of the closest region ID, the relative offset, and the related function\n                                    address.\n        \"\"\"\n\n        if target_region_id is None:\n            if self.is_stack:\n                # Get the base address of the stack frame it belongs to\n                base_address = next(self._address_to_region_id.irange(minimum=absolute_address, reverse=False))\n\n            else:\n                try:\n                    base_address = next(self._address_to_region_id.irange(maximum=absolute_address, reverse=True))\n\n                except StopIteration:\n                    # Not found. It belongs to the global region then.\n                    return 'global', absolute_address, None\n\n            descriptor = self._address_to_region_id[base_address]\n\n        else:\n            if target_region_id == 'global':\n                # Just return the absolute address\n                return 'global', absolute_address, None\n\n            if target_region_id not in self._region_id_to_address:\n                raise SimRegionMapError('Trying to relativize to a non-existent region \"%s\"' % target_region_id)\n\n            descriptor = self._region_id_to_address[target_region_id]\n            base_address = descriptor.base_address\n\n        return descriptor.region_id, absolute_address - base_address, descriptor.related_function_address",
        "rewrite": "Revise the code with your best effort: \"def relativize(self, absolute_address, target_region_id=None):\n        \"\"\"\n        Convert an absolute address to the memory offset in a memory region.\n\n        Note that if an address belongs to heap region is passed in to a stack region map, it will be converted to an\n        offset included in the closest stack frame, and vice versa for passing a stack address to a heap region.\n        Therefore you should only pass in address that belongs to the same category (stack or non-stack) of this region\n        map.\n\n        :param absolute_address:    An absolute memory address\n        :return:                    A tuple of the closest region ID, the relative offset, and the related function\n                                    address.\n        \"\"\"\n\n        if target_region_id is None:\n            if self.is_stack:\n                # Get the base address of the stack frame it belongs to\n                base_address = next(self._address_to_region_id.irange(minimum=absolute_address, reverse=False))\n\n            else:\n                try:\n                    base_address = next(self._address_to_region_id.irange(maximum=absolute_address, reverse=True))\n\n                except StopIteration:\n                    # Not found. It belongs to the global region then.\n                    return 'global', absolute_address, None\n\n            descriptor = self._address_to_region_id[base_address]\n\n        else:\n            if target_region_id == 'global':\n                # Just return the absolute address\n                return 'global', absolute_address, None\n\n            if target_region_id not in self._region_id_to_address:\n                raise SimRegionMapError('Trying to relativize to a non-existent region \"%s\"' % target_region_id)\n\n            descriptor = self._region_id_to_address[target_region_id]\n            base_address = descriptor.base_address\n\n        return descriptor.region_id, absolute_address - base_address, descriptor.related_function_address\" . No need to explain. Just write code:\n\n        def relativize(self, absolute_address, target_region_id=None):\n            \"\"\"\n            Convert an absolute address to the memory offset in a memory region.\n\n            Note that if an address belongs to heap region is passed in to a stack region map, it will be converted to an\n            offset included in the closest stack frame, and vice versa for passing a stack address to a heap region.\n            Therefore you should only pass in address that belongs to the same category (stack or non-stack) of this region\n            map.\n\n            :param absolute_address:"
    },
    {
        "original": "def _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel, logits for per-class\n        predictions.\n\n    Raises:\n      IncompatibleShapeError: If images is not rank 4.\n      ValueError: If model_size is not one of 'basic' or 'large'.\n    \"\"\"\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n      raise base.IncompatibleShapeError(\n          \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n      if self._model_size == self.BASIC:\n        self._initializers[self.WEIGHTS] = identity_kernel_initializer\n      elif self._model_size == self.LARGE:\n        self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(\n            num_classes)\n      else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n      self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n      self._conv_modules = [\n          self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    elif self._model_size == self.LARGE:\n      self._conv_modules = [\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    else:\n      raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)",
        "rewrite": "Revise the code with your best effort: \"def _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel, logits for per-class\n        predictions.\n\n    Raises:\n      IncompatibleShapeError: If images is not rank 4.\n      ValueError: If model_size is not one of 'basic' or 'large'.\n    \"\"\"\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n      raise base.IncompatibleShapeError(\n          \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n      if self._model_size == self.BASIC:\n        self._initializers[self.WEIGHTS] = identity_kernel_initializer\n      elif self._model_size == self.LARGE:\n        self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(\n            num_classes)\n      else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n      self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n      self._conv_modules = [\n          self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    elif self._model_size == self.LARGE:\n      self._conv_modules = [\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    else:\n      raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)\" . No need to explain. Just write code:\n\ndef _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel"
    },
    {
        "original": "def load(self):\n        \"\"\"\n        call this function after the file exists to populate properties\n        \"\"\"\n        # If we are given a string, open it up else assume it's something we\n        # can call read on.\n        if isinstance(self.specfile, str):\n            f = open(self.specfile, 'r')\n        else:\n            f = self.specfile\n\n        for line in f:\n            if self.v_regex.match(line):\n                self._pkg_version = self.v_regex.match(line).group(1)\n            if self.n_regex.match(line):\n                self._pkg_name = self.n_regex.match(line).group(1)\n        f.close()\n        self._loaded = True",
        "rewrite": "Revise the code with your best effort: \"def load(self):\n        \"\"\"\n        call this function after the file exists to populate properties\n        \"\"\"\n        # If we are given a string, open it up else assume it's something we\n        # can call read on.\n        if isinstance(self.specfile, str):\n            f = open(self.specfile, 'r')\n        else:\n            f = self.specfile\n\n        for line in f:\n            if self.v_regex.match(line):\n                self._pkg_version = self.v_regex.match(line).group(1)\n            if self.n_regex.match(line):\n                self._pkg_name = self.n_regex.match(line).group(1)\n        f.close()\n        self._loaded = True\" . No need to explain. Just write code:\n\ndef load(self):\n    \"\"\"\n    call this function after the file exists to populate properties\n    \"\"\"\n    # If we are given a string, open it up else assume it's something we\n    # can call read on.\n    if isinstance(self.specfile, str):\n        f = open(self.specfile, 'r')\n    else:\n        f = self.specfile\n\n    for line in f:\n        if self.v_regex.match(line):\n            self._pkg_version = self.v_regex.match"
    },
    {
        "original": "def order_by_line_nos(objs, line_nos):\n    \"\"\"Orders the set of `objs` by `line_nos`\n    \"\"\"\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]",
        "rewrite": "Revise the code with your best effort: \"def order_by_line_nos(objs, line_nos):\n    \"\"\"Orders the set of `objs` by `line_nos`\n    \"\"\"\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]\" . No need to explain. Just write code:\n\ndef order_by_line_nos(objs, line_nos):\n    return sorted(objs, key=lambda obj: line_nos.index(obj))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def get_function_subgraph(self, start, max_call_depth=None):\n        \"\"\"\n        Get a sub-graph of a certain function.\n\n        :param start: The function start. Currently it should be an integer.\n        :param max_call_depth: Call depth limit. None indicates no limit.\n        :return: A CFG instance which is a sub-graph of self.graph\n        \"\"\"\n\n        # FIXME: syscalls are not supported\n        # FIXME: start should also take a CFGNode instance\n\n        start_node = self.get_any_node(start)\n\n        node_wrapper = (start_node, 0)\n        stack = [node_wrapper]\n        traversed_nodes = {start_node}\n        subgraph_nodes = set([start_node])\n\n        while stack:\n            nw = stack.pop()\n            n, call_depth = nw[0], nw[1]\n\n            # Get successors\n            edges = self.graph.out_edges(n, data=True)\n\n            for _, dst, data in edges:\n                if dst not in traversed_nodes:\n                    # We see a new node!\n                    traversed_nodes.add(dst)\n\n                    if data['jumpkind'] == 'Ijk_Call':\n                        if max_call_depth is None or (max_call_depth is not None and call_depth < max_call_depth):\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth + 1)\n                            stack.append(new_nw)\n                    elif data['jumpkind'] == 'Ijk_Ret':\n                        if call_depth > 0:\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth - 1)\n                            stack.append(new_nw)\n                    else:\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth)\n                        stack.append(new_nw)\n\n       #subgraph = networkx.subgraph(self.graph, subgraph_nodes)\n        subgraph = self.graph.subgraph(subgraph_nodes).copy()\n\n        # Make it a CFG instance\n        subcfg = self.copy()\n        subcfg._graph = subgraph\n        subcfg._starts = (start,)\n\n        return subcfg",
        "rewrite": "Revise the code with your best effort: \"def get_function_subgraph(self, start, max_call_depth=None):\n        \"\"\"\n        Get a sub-graph of a certain function.\n\n        :param start: The function start. Currently it should be an integer.\n        :param max_call_depth: Call depth limit. None indicates no limit.\n        :return: A CFG instance which is a sub-graph of self.graph\n        \"\"\"\n\n        # FIXME: syscalls are not supported\n        # FIXME: start should also take a CFGNode instance\n\n        start_node = self.get_any_node(start)\n\n        node_wrapper = (start_node, 0)\n        stack = [node_wrapper]\n        traversed_nodes = {start_node}\n        subgraph_nodes = set([start_node])\n\n        while stack:\n            nw = stack.pop()\n            n, call_depth = nw[0], nw[1]\n\n            # Get successors\n            edges = self.graph.out_edges(n, data=True)\n\n            for _, dst, data in edges:\n                if dst not in traversed_nodes:\n                    # We see a new node!\n                    traversed_nodes.add(dst)\n\n                    if data['jumpkind'] == 'Ijk_Call':\n                        if max_call_depth is None or (max_call_depth is not None and call_depth < max_call_depth):\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth + 1)\n                            stack.append(new_nw)\n                    elif data['jumpkind'] == 'Ijk_Ret':\n                        if call_depth > 0:\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth - 1)\n                            stack.append(new_nw)\n                    else:\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth)\n                        stack.append(new_nw)\n\n       #subgraph = networkx.subgraph(self.graph, subgraph_nodes)\n        subgraph = self.graph.subgraph(subgraph_nodes).copy()\n\n        # Make it a CFG instance\n        subcfg = self.copy()\n        subcfg._graph = subgraph\n        subcfg._starts = (start,)\n\n        return subcfg\" . No need to explain. Just write code:\n\n```\n\n```\ndef get_function_subgraph(self, start, max_call_depth=None):\n    \"\"\"\n    Get a sub-graph of a certain function.\n\n    :param start: The function start. Currently it should be an integer.\n    :param max_call_depth: Call depth limit. None indicates no limit.\n    :return: A CFG instance which is a sub-graph of self.graph\n    \"\"\"\n\n    start_node = self.get_any_node(start)\n\n    node_wrapper = (start_"
    },
    {
        "original": "def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        endpoint = instance.get('prometheus_url')\n        if endpoint is None:\n            raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n\n        metrics = [DEFAULT_METRICS, GO_METRICS]\n        metrics.extend(instance.get('metrics', []))\n\n        instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\n\n        return instance",
        "rewrite": "Revise the code with your best effort: \"def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        endpoint = instance.get('prometheus_url')\n        if endpoint is None:\n            raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n\n        metrics = [DEFAULT_METRICS, GO_METRICS]\n        metrics.extend(instance.get('metrics', []))\n\n        instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\n\n        return instance\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n        \"\"\"Omits a scheduled operation from the schedule, if present.\n\n        Args:\n            scheduled_operation: The operation to try to remove.\n\n        Returns:\n            True if the operation was present and is now removed, False if it\n            was already not present.\n        \"\"\"\n        try:\n            self.scheduled_operations.remove(scheduled_operation)\n            return True\n        except ValueError:\n            return False",
        "rewrite": "Revise the code with your best effort: \"def exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n        \"\"\"Omits a scheduled operation from the schedule, if present.\n\n        Args:\n            scheduled_operation: The operation to try to remove.\n\n        Returns:\n            True if the operation was present and is now removed, False if it\n            was already not present.\n        \"\"\"\n        try:\n            self.scheduled_operations.remove(scheduled_operation)\n            return True\n        except ValueError:\n            return False\" . No need to explain. Just write code:\n\ndef exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n    \"\"\"Omits a scheduled operation from the schedule, if present.\n\n    Args:\n        scheduled_operation: The operation to try to remove.\n\n    Returns:\n        True if the operation was present and is now removed, False if it\n        was already not present.\n    \"\"\"\n    try:\n        self.scheduled_operations.remove(scheduled_operation)\n        return True\n    except ValueError:\n        return False\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def server_show_libcloud(self, uuid):\n        \"\"\"\n        Make output look like libcloud output for consistency\n        \"\"\"\n        server_info = self.server_show(uuid)\n        server = next(six.itervalues(server_info))\n        server_name = next(six.iterkeys(server_info))\n        if not hasattr(self, 'password'):\n            self.password = None\n        ret = NovaServer(server_name, server, self.password)\n\n        return ret",
        "rewrite": "Revise the code with your best effort: \"def server_show_libcloud(self, uuid):\n        \"\"\"\n        Make output look like libcloud output for consistency\n        \"\"\"\n        server_info = self.server_show(uuid)\n        server = next(six.itervalues(server_info))\n        server_name = next(six.iterkeys(server_info))\n        if not hasattr(self, 'password'):\n            self.password = None\n        ret = NovaServer(server_name, server, self.password)\n\n        return ret\" . No need to explain. Just write code:\n\ndef server_show_libcloud(self, uuid):\n    \"\"\"\n    Make output look like libcloud output for consistency\n    \"\"\"\n    server_info = self.server_show(uuid)\n    server = next(six.itervalues(server_info))\n    server_name = next(six.iterkeys(server_info))\n    if not hasattr(self, 'password'):\n        self.password = None\n    ret = NovaServer(server_name, server, self.password)\n\n    return ret\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def extract(self, topic: str, parseNumbers=True) -> list:\n        \"\"\"\n        Extract items of given topic and return as list of objects.\n\n        The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n        Order, etc.\n        \"\"\"\n        cls = type(topic, (DynamicObject,), {})\n        results = [cls(**node.attrib) for node in self.root.iter(topic)]\n        if parseNumbers:\n            for obj in results:\n                d = obj.__dict__\n                for k, v in d.items():\n                    with suppress(ValueError):\n                        d[k] = float(v)\n                        d[k] = int(v)\n        return results",
        "rewrite": "Revise the code with your best effort: \"def extract(self, topic: str, parseNumbers=True) -> list:\n        \"\"\"\n        Extract items of given topic and return as list of objects.\n\n        The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n        Order, etc.\n        \"\"\"\n        cls = type(topic, (DynamicObject,), {})\n        results = [cls(**node.attrib) for node in self.root.iter(topic)]\n        if parseNumbers:\n            for obj in results:\n                d = obj.__dict__\n                for k, v in d.items():\n                    with suppress(ValueError):\n                        d[k] = float(v)\n                        d[k] = int(v)\n        return results\" . No need to explain. Just write code:\n\n        def extract(self, topic: str, parseNumbers=True) -> list:\n            \"\"\"\n            Extract items of given topic and return as list of objects.\n\n            The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n            Order, etc.\n            \"\"\"\n            cls = type(topic, (DynamicObject,), {})\n            results = [cls(**node.attrib) for node in self.root.iter(topic)]\n            if parseNumbers:\n                for obj in results:\n                    d = obj"
    },
    {
        "original": "def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv",
        "rewrite": "Revise the code with your best effort: \"def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv\" . No need to explain. Just write code:\n\ndef pin_auth(self, request):\n    \"\"\"Authenticates with the pin.\"\"\"\n    exhausted = False\n    auth = False\n    trust = self.check_pin_trust(request.environ)\n\n    # If the trust return value is `None` it means that the cookie is\n    # set but the stored pin hash value is bad.  This means that the\n    # pin was changed.  In this case we count a bad auth and unset the\n    # cookie.  This way it becomes harder to guess the cookie name\n    # instead of"
    },
    {
        "original": "def set_lim(min, max, name):\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\n\n    Parameters\n    ----------\n    name: hashable\n        Any variable that can be used as a key for a dictionary\n\n    Raises\n    ------\n    KeyError\n        When no context figure is associated with the provided key.\n\n    \"\"\"\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale",
        "rewrite": "Revise the code with your best effort: \"def set_lim(min, max, name):\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\n\n    Parameters\n    ----------\n    name: hashable\n        Any variable that can be used as a key for a dictionary\n\n    Raises\n    ------\n    KeyError\n        When no context figure is associated with the provided key.\n\n    \"\"\"\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale\" . No need to explain. Just write code:\n\ndef set_lim(min, max, name):\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\n\n    Parameters\n    ----------\n    name: hashable\n        Any variable that can be used as a key for a dictionary\n\n    Raises\n    ------\n    KeyError\n        When no context figure is associated with the provided key.\n\n    \"\"\"\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale"
    },
    {
        "original": "def ParseMultiple(self, stats, unused_file_obj, unused_kb):\n    \"\"\"Identify the init scripts and the start/stop scripts at each runlevel.\n\n    Evaluate all the stat entries collected from the system.\n    If the path name matches a runlevel spec, and if the filename matches a\n    sysv init symlink process the link as a service.\n\n    Args:\n      stats: An iterator of StatEntry rdfs.\n      unused_file_obj: An iterator of file contents. Not needed as the parser\n        only evaluates link attributes.\n      unused_kb: Unused KnowledgeBase rdf.\n\n    Yields:\n      rdf_anomaly.Anomaly if the startup link seems wierd.\n      rdf_client.LinuxServiceInformation for each detected service.\n    \"\"\"\n    services = {}\n    for stat_entry in stats:\n      path = stat_entry.pathspec.path\n      runlevel = self.runlevel_re.match(os.path.dirname(path))\n      runscript = self.runscript_re.match(os.path.basename(path))\n      if runlevel and runscript:\n        svc = runscript.groupdict()\n        service = services.setdefault(\n            svc[\"name\"],\n            rdf_client.LinuxServiceInformation(\n                name=svc[\"name\"], start_mode=\"INIT\"))\n        runlvl = GetRunlevelsNonLSB(runlevel.group(1))\n        if svc[\"action\"] == \"S\" and runlvl:\n          service.start_on.append(runlvl.pop())\n          service.starts = True\n        elif runlvl:\n          service.stop_on.append(runlvl.pop())\n        if not stat.S_ISLNK(int(stat_entry.st_mode)):\n          yield rdf_anomaly.Anomaly(\n              type=\"PARSER_ANOMALY\",\n              finding=[path],\n              explanation=\"Startup script is not a symlink.\")\n    for svc in itervalues(services):\n      yield svc",
        "rewrite": "Revise the code with your best effort: \"def ParseMultiple(self, stats, unused_file_obj, unused_kb):\n    \"\"\"Identify the init scripts and the start/stop scripts at each runlevel.\n\n    Evaluate all the stat entries collected from the system.\n    If the path name matches a runlevel spec, and if the filename matches a\n    sysv init symlink process the link as a service.\n\n    Args:\n      stats: An iterator of StatEntry rdfs.\n      unused_file_obj: An iterator of file contents. Not needed as the parser\n        only evaluates link attributes.\n      unused_kb: Unused KnowledgeBase rdf.\n\n    Yields:\n      rdf_anomaly.Anomaly if the startup link seems wierd.\n      rdf_client.LinuxServiceInformation for each detected service.\n    \"\"\"\n    services = {}\n    for stat_entry in stats:\n      path = stat_entry.pathspec.path\n      runlevel = self.runlevel_re.match(os.path.dirname(path))\n      runscript = self.runscript_re.match(os.path.basename(path))\n      if runlevel and runscript:\n        svc = runscript.groupdict()\n        service = services.setdefault(\n            svc[\"name\"],\n            rdf_client.LinuxServiceInformation(\n                name=svc[\"name\"], start_mode=\"INIT\"))\n        runlvl = GetRunlevelsNonLSB(runlevel.group(1))\n        if svc[\"action\"] == \"S\" and runlvl:\n          service.start_on.append(runlvl.pop())\n          service.starts = True\n        elif runlvl:\n          service.stop_on.append(runlvl.pop())\n        if not stat.S_ISLNK(int(stat_entry.st_mode)):\n          yield rdf_anomaly.Anomaly(\n              type=\"PARSER_ANOMALY\",\n              finding=[path],\n              explanation=\"Startup script is not a symlink.\")\n    for svc in itervalues(services):\n      yield svc\" . No need to explain. Just write code:\n\n```python\ndef ParseMultiple(self, stats, unused_file_obj, unused_kb):\n  \"\"\"Identify the init scripts and the start/stop scripts at each runlevel.\n\n  Evaluate all the stat entries collected from the system.\n  If the path name matches a runlevel spec, and if the filename matches a\n  sysv init symlink process the link as a service.\n\n  Args:\n    stats: An iterator of StatEntry rdfs.\n    unused_file_obj: An iterator of file contents. Not needed"
    },
    {
        "original": "def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs",
        "rewrite": "Revise the code with your best effort: \"def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs\" . No need to explain. Just write code:\n\ndef get_ext_outputs(self):\n    \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n    all_outputs = []\n    ext_outputs = []\n\n    paths = {self.bdist_dir: ''}\n    for base, dirs, files in sorted_walk(self.bdist_dir):\n        for filename in files:\n            if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                all_outputs.append(paths["
    },
    {
        "original": "def base64_bytes(x):\n    \"\"\"Turn base64 into bytes\"\"\"\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(bytes_encode(x))",
        "rewrite": "Revise the code with your best effort: \"def base64_bytes(x):\n    \"\"\"Turn base64 into bytes\"\"\"\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(bytes_encode(x))\" . No need to explain. Just write code:\n\ndef base64_bytes(x):\n    return base64.decodebytes(bytes_encode(x))\n\ndef bytes_encode(x):\n    return x.encode('utf-8')\n\ndef base64_encode(x):\n    return base64.b64encode(bytes_encode(x))\n\ndef base64_decode(x):\n    return base64.b64decode(bytes_encode(x))\n\ndef base64_bytes(x):\n    return base64.decodebytes(bytes_"
    },
    {
        "original": "def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node",
        "rewrite": "Revise the code with your best effort: \"def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node\" . No need to explain. Just write code:\n\ndef fold(self, node):\n    \"\"\"Do constant folding.\"\"\"\n    node = self.generic_visit(node)\n    try:\n        return nodes.Const.from_untrusted(node.as_const(),\n                                          lineno=node.lineno,\n                                          environment=self.environment)\n    except nodes.Impossible:\n        return node\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def from_node(index, data, modify_index=None):\n        \"\"\"\n        >>> ClusterConfig.from_node(1, '{') is None\n        False\n        \"\"\"\n\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)",
        "rewrite": "Revise the code with your best effort: \"def from_node(index, data, modify_index=None):\n        \"\"\"\n        >>> ClusterConfig.from_node(1, '{') is None\n        False\n        \"\"\"\n\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)\" . No need to explain. Just write code:\n\n        def from_node(index, data, modify_index=None):\n            try:\n                data = json.loads(data)\n            except (TypeError, ValueError):\n                data = None\n                modify_index = 0\n            if not isinstance(data, dict):\n                data = {}\n            return ClusterConfig(index, data, index if modify_index is None else modify_index)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "rewrite": "Revise the code with your best effort: \"def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )\" . No need to explain. Just write code:\n\ndef save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def Validate(self):\n    \"\"\"GlobExpression is valid.\"\"\"\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n      raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)",
        "rewrite": "Revise the code with your best effort: \"def Validate(self):\n    \"\"\"GlobExpression is valid.\"\"\"\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n      raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)\" . No need to explain. Just write code:\n\ndef Validate(self):\n    \"\"\"GlobExpression is valid.\"\"\"\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n      raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)\n\nComment: @user3367903: I've added a version that uses `re.findall()` to find the `**` in the string.\n\nComment: @user3367903: I've added a version that uses `re"
    },
    {
        "original": "def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        with self._lock:\n            if name not in self._gauges:\n                if self._registry._ignore_patterns and any(\n                    pattern.match(name) for pattern in self._registry._ignore_patterns\n                ):\n                    gauge = noop_metric\n                else:\n                    gauge = Gauge(name)\n                self._gauges[name] = gauge\n            return self._gauges[name]",
        "rewrite": "Revise the code with your best effort: \"def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        with self._lock:\n            if name not in self._gauges:\n                if self._registry._ignore_patterns and any(\n                    pattern.match(name) for pattern in self._registry._ignore_patterns\n                ):\n                    gauge = noop_metric\n                else:\n                    gauge = Gauge(name)\n                self._gauges[name] = gauge\n            return self._gauges[name]\" . No need to explain. Just write code:\n\ndef gauge(self, name):\n    with self._lock:\n        if name not in self._gauges:\n            if self._registry._ignore_patterns and any(\n                pattern.match(name) for pattern in self._registry._ignore_patterns\n            ):\n                gauge = noop_metric\n            else:\n                gauge = Gauge(name)\n            self._gauges[name] = gauge\n        return self._gauges[name]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def disconnect(service_instance):\n    \"\"\"\n    Function that disconnects from the vCenter server or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)",
        "rewrite": "Revise the code with your best effort: \"def disconnect(service_instance):\n    \"\"\"\n    Function that disconnects from the vCenter server or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\" . No need to explain. Just write code:\n\ndef disconnect(service_instance):\n    \"\"\"\n    Function that disconnects from the vCenter server or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}"
    },
    {
        "original": "def h2i(self, pkt, seconds):\n        \"\"\"Convert the number of seconds since 1-Jan-70 UTC to the packed\n           representation.\"\"\"\n\n        if seconds is None:\n            seconds = 0\n\n        tmp_short = (seconds >> 32) & 0xFFFF\n        tmp_int = seconds & 0xFFFFFFFF\n\n        return struct.pack(\"!HI\", tmp_short, tmp_int)",
        "rewrite": "Revise the code with your best effort: \"def h2i(self, pkt, seconds):\n        \"\"\"Convert the number of seconds since 1-Jan-70 UTC to the packed\n           representation.\"\"\"\n\n        if seconds is None:\n            seconds = 0\n\n        tmp_short = (seconds >> 32) & 0xFFFF\n        tmp_int = seconds & 0xFFFFFFFF\n\n        return struct.pack(\"!HI\", tmp_short, tmp_int)\" . No need to explain. Just write code:\n\ndef h2i(self, pkt, seconds):\n    \"\"\"Convert the number of seconds since 1-Jan-70 UTC to the packed\n       representation.\"\"\"\n\n    if seconds is None:\n        seconds = 0\n\n    return struct.pack(\"!HI\", (seconds >> 32) & 0xFFFF, seconds & 0xFFFFFFFF)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def notebook_start(self, **kwargs):\n        \"\"\"\n        Initialize a notebook, clearing its metadata, and save it.\n\n        When starting a notebook, this initializes and clears the metadata for\n        the notebook and its cells, and saves the notebook to the given\n        output path.\n\n        Called by Engine when execution begins.\n        \"\"\"\n        self.set_timer()\n\n        self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n        self.nb.metadata.papermill['end_time'] = None\n        self.nb.metadata.papermill['duration'] = None\n        self.nb.metadata.papermill['exception'] = None\n\n        for cell in self.nb.cells:\n            # Reset the cell execution counts.\n            if cell.get(\"execution_count\") is not None:\n                cell.execution_count = None\n\n            # Clear out the papermill metadata for each cell.\n            cell.metadata.papermill = dict(\n                exception=None,\n                start_time=None,\n                end_time=None,\n                duration=None,\n                status=self.PENDING,  # pending, running, completed\n            )\n            if cell.get(\"outputs\") is not None:\n                cell.outputs = []\n\n        self.save()",
        "rewrite": "Revise the code with your best effort: \"def notebook_start(self, **kwargs):\n        \"\"\"\n        Initialize a notebook, clearing its metadata, and save it.\n\n        When starting a notebook, this initializes and clears the metadata for\n        the notebook and its cells, and saves the notebook to the given\n        output path.\n\n        Called by Engine when execution begins.\n        \"\"\"\n        self.set_timer()\n\n        self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n        self.nb.metadata.papermill['end_time'] = None\n        self.nb.metadata.papermill['duration'] = None\n        self.nb.metadata.papermill['exception'] = None\n\n        for cell in self.nb.cells:\n            # Reset the cell execution counts.\n            if cell.get(\"execution_count\") is not None:\n                cell.execution_count = None\n\n            # Clear out the papermill metadata for each cell.\n            cell.metadata.papermill = dict(\n                exception=None,\n                start_time=None,\n                end_time=None,\n                duration=None,\n                status=self.PENDING,  # pending, running, completed\n            )\n            if cell.get(\"outputs\") is not None:\n                cell.outputs = []\n\n        self.save()\" . No need to explain. Just write code:\n\ndef notebook_start(self, **kwargs):\n    \"\"\"\n    Initialize a notebook, clearing its metadata, and save it.\n\n    When starting a notebook, this initializes and clears the metadata for\n    the notebook and its cells, and saves the notebook to the given\n    output path.\n\n    Called by Engine when execution begins.\n    \"\"\"\n    self.set_timer()\n\n    self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n    self.nb."
    },
    {
        "original": "def dispatch_request(self, req):\n        \"\"\"\n        Dispatch a request object.\n        \"\"\"\n        log.debug(\"Dispatching request: {}\".format(str(req)))\n\n        # make sure it's valid\n        res = None\n        try:\n            req.validate()\n        except MissingFieldError as e:\n            res = APIMissingFieldErrorResponse(str(e))\n\n        # dispatch the request\n        if not res:\n            try:\n                res = req.dispatch()\n            except Exception as e:\n                msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n                log.exception(msg)\n                res = APIGenericErrorResponse(msg)\n\n        log.debug(\"Response: {}\".format(str(res)))\n\n        return res",
        "rewrite": "Revise the code with your best effort: \"def dispatch_request(self, req):\n        \"\"\"\n        Dispatch a request object.\n        \"\"\"\n        log.debug(\"Dispatching request: {}\".format(str(req)))\n\n        # make sure it's valid\n        res = None\n        try:\n            req.validate()\n        except MissingFieldError as e:\n            res = APIMissingFieldErrorResponse(str(e))\n\n        # dispatch the request\n        if not res:\n            try:\n                res = req.dispatch()\n            except Exception as e:\n                msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n                log.exception(msg)\n                res = APIGenericErrorResponse(msg)\n\n        log.debug(\"Response: {}\".format(str(res)))\n\n        return res\" . No need to explain. Just write code:\n\ndef dispatch_request(self, req):\n    \"\"\"\n    Dispatch a request object.\n    \"\"\"\n    log.debug(\"Dispatching request: {}\".format(str(req)))\n\n    # make sure it's valid\n    res = None\n    try:\n        req.validate()\n    except MissingFieldError as e:\n        res = APIMissingFieldErrorResponse(str(e))\n\n    # dispatch the request\n    if not res:\n        try:\n            res = req.dispatch()\n        except Exception as e:\n            msg"
    },
    {
        "original": "def _get_ngrams_with_counter(segment, max_order):\n  \"\"\"Extracts all n-grams up to a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n  ngram_counts = collections.Counter()\n  for order in xrange(1, max_order + 1):\n    for i in xrange(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts",
        "rewrite": "Revise the code with your best effort: \"def _get_ngrams_with_counter(segment, max_order):\n  \"\"\"Extracts all n-grams up to a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n  ngram_counts = collections.Counter()\n  for order in xrange(1, max_order + 1):\n    for i in xrange(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts\" . No need to explain. Just write code:\n\ndef _get_ngrams_with_counter(segment, max_order):\n  ngram_counts = collections.Counter()\n  for order in range(1, max_order + 1):\n    for i in range(len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c",
        "rewrite": "Revise the code with your best effort: \"def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c\" . No need to explain. Just write code:\n\ndef replace_species(self, species_mapping):\n    \"\"\"\n    Swap species.\n\n    Args:\n        species_mapping (dict): dict of species to swap. Species can be\n            elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n            a Li for Na substitution. The second species can be a\n            sp_and_occu dict. For example, a site with 0.5 Si that is\n            passed the mapping {Element('Si): {Element('Ge'):0.75,\n            Element('C"
    },
    {
        "original": "def show_item_dict(self, item):\n        \"\"\"Returns a json-able dict for show\"\"\"\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, string_types)):\n                v = str(v)\n            d[col] = v\n        return d",
        "rewrite": "Revise the code with your best effort: \"def show_item_dict(self, item):\n        \"\"\"Returns a json-able dict for show\"\"\"\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, string_types)):\n                v = str(v)\n            d[col] = v\n        return d\" . No need to explain. Just write code:\n\ndef show_item_dict(self, item):\n    \"\"\"Returns a json-able dict for show\"\"\"\n    d = {}\n    for col in self.show_columns:\n        v = getattr(item, col)\n        if not isinstance(v, (int, float, string_types)):\n            v = str(v)\n        d[col] = v\n    return d\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]",
        "rewrite": "Revise the code with your best effort: \"def FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]\" . No need to explain. Just write code:\n\ndef FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_"
    },
    {
        "original": "def get_connection(self, command_name, *keys, **options):\n        \"\"\"\n        Get a connection, blocking for ``self.timeout`` until a connection\n        is available from the pool.\n\n        If the connection returned is ``None`` then creates a new connection.\n        Because we use a last-in first-out queue, the existing connections\n        (having been returned to the pool after the initial ``None`` values\n        were added) will be returned before ``None`` values. This means we only\n        create new connections when we need to, i.e.: the actual number of\n        connections will only increase in response to demand.\n        \"\"\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n\n        # Try and get a connection from the pool. If one isn't available within\n        # self.timeout then raise a ``ConnectionError``.\n        connection = None\n        try:\n            connection = self.pool.get(block=True, timeout=self.timeout)\n        except Empty:\n            # Note that this is not caught by the redis client and will be\n            # raised unless handled by application code. If you want never to\n            raise ConnectionError(\"No connection available.\")\n\n        # If the ``connection`` is actually ``None`` then that's a cue to make\n        # a new connection to add to the pool.\n        if connection is None:\n            connection = self.make_connection()\n\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection",
        "rewrite": "Revise the code with your best effort: \"def get_connection(self, command_name, *keys, **options):\n        \"\"\"\n        Get a connection, blocking for ``self.timeout`` until a connection\n        is available from the pool.\n\n        If the connection returned is ``None`` then creates a new connection.\n        Because we use a last-in first-out queue, the existing connections\n        (having been returned to the pool after the initial ``None`` values\n        were added) will be returned before ``None`` values. This means we only\n        create new connections when we need to, i.e.: the actual number of\n        connections will only increase in response to demand.\n        \"\"\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n\n        # Try and get a connection from the pool. If one isn't available within\n        # self.timeout then raise a ``ConnectionError``.\n        connection = None\n        try:\n            connection = self.pool.get(block=True, timeout=self.timeout)\n        except Empty:\n            # Note that this is not caught by the redis client and will be\n            # raised unless handled by application code. If you want never to\n            raise ConnectionError(\"No connection available.\")\n\n        # If the ``connection`` is actually ``None`` then that's a cue to make\n        # a new connection to add to the pool.\n        if connection is None:\n            connection = self.make_connection()\n\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection\" . No need to explain. Just write code:\n\ndef get_connection(self, command_name, *keys, **options):\n    \"\"\"\n    Get a connection, blocking for ``self.timeout`` until a connection\n    is available from the pool.\n\n    If the connection returned is ``None`` then creates a new connection.\n    Because we use a last-in first-out queue, the existing connections\n    (having been returned to the pool after the initial ``None`` values\n    were added) will be returned before ``None`` values. This means we only\n    create new connections when we need to, i.e"
    },
    {
        "original": "def check_error(res, error_enum):\n  \"\"\"Raise if the result has an error, otherwise return the result.\"\"\"\n  if res.HasField(\"error\"):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = error_enum.Name(res.error)\n    details = getattr(res, \"error_details\", \"<none>\")\n    raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n  return res",
        "rewrite": "Revise the code with your best effort: \"def check_error(res, error_enum):\n  \"\"\"Raise if the result has an error, otherwise return the result.\"\"\"\n  if res.HasField(\"error\"):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = error_enum.Name(res.error)\n    details = getattr(res, \"error_details\", \"<none>\")\n    raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n  return res\" . No need to explain. Just write code:\n\ndef check_error(res, error_enum):\n    if res.HasField(\"error\"):\n        raise RequestError(error_enum.Name(res.error), res)\n    return res\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def initialize_plot(self, ranges=None):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = self.layout.get(pos, None)\n            subplot = self.subplots.get(pos, None)\n            ax = self.subaxes.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if None in [view, pos, subplot]:\n                ax.set_axis_off()\n                continue\n            subplot.initialize_plot(ranges=ranges)\n\n        self.adjust_positions()\n        self.drawn = True",
        "rewrite": "Revise the code with your best effort: \"def initialize_plot(self, ranges=None):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = self.layout.get(pos, None)\n            subplot = self.subplots.get(pos, None)\n            ax = self.subaxes.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if None in [view, pos, subplot]:\n                ax.set_axis_off()\n                continue\n            subplot.initialize_plot(ranges=ranges)\n\n        self.adjust_positions()\n        self.drawn = True\" . No need to explain. Just write code:\n\ndef initialize_plot(self, ranges=None):\n    \"\"\"\n    Plot all the views contained in the AdjointLayout Object using axes\n    appropriate to the layout configuration. All the axes are\n    supplied by LayoutPlot - the purpose of the call is to\n    invoke subplots with correct options and styles and hide any\n    empty axes as necessary.\n    \"\"\"\n    for pos in self.view_positions:\n        # Pos will be one of 'main', 'top' or 'right' or None\n        view = self.layout.get(pos, None)\n       "
    },
    {
        "original": "def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]",
        "rewrite": "Revise the code with your best effort: \"def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]\" . No need to explain. Just write code:\n\ndef split(sql, encoding=None):\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def page_str(self):\n        \"\"\"\n        The RestructuredText documentation page for the enumeration. This is\n        the only API member for the class.\n        \"\"\"\n        tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n        components = (\n            self._ms_name, self._page_title, self._intro_text,\n            self._member_defs\n        )\n        return tmpl % components",
        "rewrite": "Revise the code with your best effort: \"def page_str(self):\n        \"\"\"\n        The RestructuredText documentation page for the enumeration. This is\n        the only API member for the class.\n        \"\"\"\n        tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n        components = (\n            self._ms_name, self._page_title, self._intro_text,\n            self._member_defs\n        )\n        return tmpl % components\" . No need to explain. Just write code:\n\n        def page_str(self):\n            \"\"\"\n            The RestructuredText documentation page for the enumeration. This is\n            the only API member for the class.\n            \"\"\"\n            tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n            components = (\n                self._ms_name, self._page_title, self._intro_text,\n                self._member_defs\n            )\n            return tmpl % components\n\n        def page_str(self"
    },
    {
        "original": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this Attachment.\n\n        :returns: This Attachment, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        attachment = {}\n        if self.file_content is not None:\n            attachment[\"content\"] = self.file_content.get()\n\n        if self.file_type is not None:\n            attachment[\"type\"] = self.file_type.get()\n\n        if self.file_name is not None:\n            attachment[\"filename\"] = self.file_name.get()\n\n        if self.disposition is not None:\n            attachment[\"disposition\"] = self.disposition.get()\n\n        if self.content_id is not None:\n            attachment[\"content_id\"] = self.content_id.get()\n        return attachment",
        "rewrite": "Revise the code with your best effort: \"def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this Attachment.\n\n        :returns: This Attachment, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        attachment = {}\n        if self.file_content is not None:\n            attachment[\"content\"] = self.file_content.get()\n\n        if self.file_type is not None:\n            attachment[\"type\"] = self.file_type.get()\n\n        if self.file_name is not None:\n            attachment[\"filename\"] = self.file_name.get()\n\n        if self.disposition is not None:\n            attachment[\"disposition\"] = self.disposition.get()\n\n        if self.content_id is not None:\n            attachment[\"content_id\"] = self.content_id.get()\n        return attachment\" . No need to explain. Just write code:\n\ndef get(self):\n    \"\"\"\n    Get a JSON-ready representation of this Attachment.\n\n    :returns: This Attachment, ready for use in a request body.\n    :rtype: dict\n    \"\"\"\n    attachment = {}\n    if self.file_content is not None:\n        attachment[\"content\"] = self.file_content.get()\n\n    if self.file_type is not None:\n        attachment[\"type\"] = self.file_type.get()\n\n    if self.file_name is not None:\n        attachment[\""
    },
    {
        "original": "def load_ner_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"ner{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  try:\n    return pickle.load(fh)\n  except UnicodeDecodeError:\n    fh.seek(0)\n    return pickle.load(fh, encoding='latin1')",
        "rewrite": "Revise the code with your best effort: \"def load_ner_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"ner{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  try:\n    return pickle.load(fh)\n  except UnicodeDecodeError:\n    fh.seek(0)\n    return pickle.load(fh, encoding='latin1')\" . No need to explain. Just write code:\n\ndef load_ner_model(lang=\"en\", version=\"2\"):\n    \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n    Args:\n        lang (string): language code.\n        version (string): version of the parameters to be used.\n    \"\"\"\n    src_dir = \"ner{}\".format(version)\n    p = locate_resource(src_dir, lang)\n    fh = _open(p)\n    try:\n        return pickle.load(fh)\n    except UnicodeDe"
    },
    {
        "original": "def _reset_state_mode(self, state, mode):\n        \"\"\"\n        Reset the state mode to the given mode, and apply the custom state options specified with this analysis.\n\n        :param state:    The state to work with.\n        :param str mode: The state mode.\n        :return:         None\n        \"\"\"\n\n        state.set_mode(mode)\n        state.options |= self._state_add_options\n        state.options = state.options.difference(self._state_remove_options)",
        "rewrite": "Revise the code with your best effort: \"def _reset_state_mode(self, state, mode):\n        \"\"\"\n        Reset the state mode to the given mode, and apply the custom state options specified with this analysis.\n\n        :param state:    The state to work with.\n        :param str mode: The state mode.\n        :return:         None\n        \"\"\"\n\n        state.set_mode(mode)\n        state.options |= self._state_add_options\n        state.options = state.options.difference(self._state_remove_options)\" . No need to explain. Just write code:\n\ndef _reset_state_mode(self, state, mode):\n    state.set_mode(mode)\n    state.options |= self._state_add_options\n    state.options = state.options.difference(self._state_remove_options)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def normalize_words(self, ord=2, inplace=False):\n    \"\"\"Normalize embeddings matrix row-wise.\n\n    Args:\n      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n    \"\"\"\n    if ord == 2:\n      ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n      self.vectors = vectors.T\n      return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)",
        "rewrite": "Revise the code with your best effort: \"def normalize_words(self, ord=2, inplace=False):\n    \"\"\"Normalize embeddings matrix row-wise.\n\n    Args:\n      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n    \"\"\"\n    if ord == 2:\n      ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n      self.vectors = vectors.T\n      return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)\" . No need to explain. Just write code:\n\ndef normalize_words(self, ord=2, inplace=False):\n    \"\"\"Normalize embeddings matrix row-wise.\n\n    Args:\n      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n    \"\"\"\n    if ord == 2:\n        ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n        self.vectors"
    },
    {
        "original": "def DEFINE_integer(self, name, default, help, constant=False):\n    \"\"\"A helper for defining integer options.\"\"\"\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)",
        "rewrite": "Revise the code with your best effort: \"def DEFINE_integer(self, name, default, help, constant=False):\n    \"\"\"A helper for defining integer options.\"\"\"\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)\" . No need to explain. Just write code:\n\ndef DEFINE_integer(self, name, default, help, constant=False):\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)\n\ndef DEFINE_integer(self, name, default, help, constant=False):\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)\n\ndef DEFINE_integer(self, name, default, help, constant=False"
    },
    {
        "original": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != f.data.RUNNING,\n        timeout=timeout)\n    if f.data.state != f.data.TERMINATED:\n      raise errors.FlowFailedError(\n          \"Flow %s (%s) failed: %s\" %\n          (self.flow_id, self.client_id, f.data.context.current_state))\n    return f",
        "rewrite": "Revise the code with your best effort: \"def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != f.data.RUNNING,\n        timeout=timeout)\n    if f.data.state != f.data.TERMINATED:\n      raise errors.FlowFailedError(\n          \"Flow %s (%s) failed: %s\" %\n          (self.flow_id, self.client_id, f.data.context.current_state))\n    return f\" . No need to explain. Just write code:\n\ndef WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f"
    },
    {
        "original": "def date_this_month(self, before_today=True, after_today=False):\n        \"\"\"\n        Gets a Date object for the current month.\n\n        :param before_today: include days in current month before today\n        :param after_today: include days in current month after today\n        :param tzinfo: timezone, instance of datetime.tzinfo subclass\n        :example DateTime('2012-04-04 11:02:02')\n        :return DateTime\n        \"\"\"\n        today = date.today()\n        this_month_start = today.replace(day=1)\n\n        next_month_start = this_month_start + \\\n            relativedelta.relativedelta(months=1)\n        if before_today and after_today:\n            return self.date_between_dates(this_month_start, next_month_start)\n        elif not before_today and after_today:\n            return self.date_between_dates(today, next_month_start)\n        elif not after_today and before_today:\n            return self.date_between_dates(this_month_start, today)\n        else:\n            return today",
        "rewrite": "Revise the code with your best effort: \"def date_this_month(self, before_today=True, after_today=False):\n        \"\"\"\n        Gets a Date object for the current month.\n\n        :param before_today: include days in current month before today\n        :param after_today: include days in current month after today\n        :param tzinfo: timezone, instance of datetime.tzinfo subclass\n        :example DateTime('2012-04-04 11:02:02')\n        :return DateTime\n        \"\"\"\n        today = date.today()\n        this_month_start = today.replace(day=1)\n\n        next_month_start = this_month_start + \\\n            relativedelta.relativedelta(months=1)\n        if before_today and after_today:\n            return self.date_between_dates(this_month_start, next_month_start)\n        elif not before_today and after_today:\n            return self.date_between_dates(today, next_month_start)\n        elif not after_today and before_today:\n            return self.date_between_dates(this_month_start, today)\n        else:\n            return today\" . No need to explain. Just write code:\n\n\ndef date_this_month(self, before_today=True, after_today=False):\n    \"\"\"\n    Gets a Date object for the current month.\n\n    :param before_today: include days in current month before today\n    :param after_today: include days in current month after today\n    :param tzinfo: timezone, instance of datetime.tzinfo subclass\n    :example DateTime('2012-04-04 11:02:02')\n    :return DateTime\n    \"\"\"\n    today = date.today"
    },
    {
        "original": "def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \"\"\" Update tree feature weights using decision path method.\n    \"\"\"\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff",
        "rewrite": "Revise the code with your best effort: \"def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \"\"\" Update tree feature weights using decision path method.\n    \"\"\"\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff\" . No need to explain. Just write code:\n\ndef _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf"
    },
    {
        "original": "def get_team_push_restrictions(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/branches/:branch/protection/restrictions/teams <https://developer.github.com/v3/repos/branches>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Team.Team,\n            self._requester,\n            self.protection_url + \"/restrictions/teams\",\n            None\n        )",
        "rewrite": "Revise the code with your best effort: \"def get_team_push_restrictions(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/branches/:branch/protection/restrictions/teams <https://developer.github.com/v3/repos/branches>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Team.Team,\n            self._requester,\n            self.protection_url + \"/restrictions/teams\",\n            None\n        )\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def url(self, schemes=None):\n        \"\"\"\n        :param schemes: a list of strings to use as schemes, one will chosen randomly.\n        If None, it will generate http and https urls.\n        Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n        :returns: a random url string.\n        \"\"\"\n        if schemes is None:\n            schemes = ['http', 'https']\n\n        pattern = '{}://{}'.format(\n            self.random_element(schemes) if schemes else \"\",\n            self.random_element(self.url_formats),\n        )\n\n        return self.generator.parse(pattern)",
        "rewrite": "Revise the code with your best effort: \"def url(self, schemes=None):\n        \"\"\"\n        :param schemes: a list of strings to use as schemes, one will chosen randomly.\n        If None, it will generate http and https urls.\n        Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n        :returns: a random url string.\n        \"\"\"\n        if schemes is None:\n            schemes = ['http', 'https']\n\n        pattern = '{}://{}'.format(\n            self.random_element(schemes) if schemes else \"\",\n            self.random_element(self.url_formats),\n        )\n\n        return self.generator.parse(pattern)\" . No need to explain. Just write code:\n\ndef url(self, schemes=None):\n    \"\"\"\n    :param schemes: a list of strings to use as schemes, one will chosen randomly.\n    If None, it will generate http and https urls.\n    Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n    :returns: a random url string.\n    \"\"\"\n    if schemes is None:\n        schemes = ['http', 'https']\n\n    pattern = '{}://{}'.format(\n        self.random_element(schemes) if schemes else \"\",\n"
    },
    {
        "original": "def get_data():\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame\n\n\t\tI.e.,\n\t\t>>> convention_df.iloc[0]\n\t\tcategory                                                    plot\n\t\tfilename                 subjectivity_html/obj/2002/Abandon.html\n\t\ttext           A senior at an elite college (Katie Holmes), a...\n\t\tmovie_name                                               abandon\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdata_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n\t\texcept:\n\t\t\turl = ROTTEN_TOMATOES_DATA_URL\n\t\t\tdata_stream = urlopen(url).read()\n\t\treturn pd.read_csv(io.BytesIO(bz2.decompress(data_stream)))",
        "rewrite": "Revise the code with your best effort: \"def get_data():\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame\n\n\t\tI.e.,\n\t\t>>> convention_df.iloc[0]\n\t\tcategory                                                    plot\n\t\tfilename                 subjectivity_html/obj/2002/Abandon.html\n\t\ttext           A senior at an elite college (Katie Holmes), a...\n\t\tmovie_name                                               abandon\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdata_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n\t\texcept:\n\t\t\turl = ROTTEN_TOMATOES_DATA_URL\n\t\t\tdata_stream = urlopen(url).read()\n\t\treturn pd.read_csv(io.BytesIO(bz2.decompress(data_stream)))\" . No need to explain. Just write code:\n\n\nimport bz2\nimport io\nimport pandas as pd\nimport pkgutil\n\n\ndef get_data():\n    \"\"\"\n    Returns\n    -------\n    pd.DataFrame\n\n    I.e.,\n    >>> convention_df.iloc[0]\n    category                                                    plot\n    filename                 subjectivity_html/obj/2002/Abandon.html\n    text           A senior at an elite college (Katie Holmes), a...\n    movie_name                                               abandon\n    \"\"\"\n    try:\n        data"
    },
    {
        "original": "def add_reward_function(self):\n        \"\"\"\n        add reward function tag to pomdpx model\n\n        Return\n        ---------------\n        string containing the xml for reward function tag\n        \"\"\"\n        reward_function = self.model['reward_function']\n        for condition in reward_function:\n            condprob = etree.SubElement(self.reward_function, 'Func')\n            self.add_conditions(condition, condprob)\n        return self.__str__(self.reward_function)[:-1]",
        "rewrite": "Revise the code with your best effort: \"def add_reward_function(self):\n        \"\"\"\n        add reward function tag to pomdpx model\n\n        Return\n        ---------------\n        string containing the xml for reward function tag\n        \"\"\"\n        reward_function = self.model['reward_function']\n        for condition in reward_function:\n            condprob = etree.SubElement(self.reward_function, 'Func')\n            self.add_conditions(condition, condprob)\n        return self.__str__(self.reward_function)[:-1]\" . No need to explain. Just write code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
        "original": "def get_all_values(self):\n        \"\"\"Returns a list of lists containing all cells' values as strings.\n\n        .. note::\n\n            Empty trailing rows and columns will not be included.\n        \"\"\"\n\n        data = self.spreadsheet.values_get(self.title)\n\n        try:\n            return fill_gaps(data['values'])\n        except KeyError:\n            return []",
        "rewrite": "Revise the code with your best effort: \"def get_all_values(self):\n        \"\"\"Returns a list of lists containing all cells' values as strings.\n\n        .. note::\n\n            Empty trailing rows and columns will not be included.\n        \"\"\"\n\n        data = self.spreadsheet.values_get(self.title)\n\n        try:\n            return fill_gaps(data['values'])\n        except KeyError:\n            return []\" . No need to explain. Just write code:\n\n\\begin{code}\ndef get_all_values(self):\n    \"\"\"Returns a list of lists containing all cells' values as strings.\n\n    .. note::\n\n        Empty trailing rows and columns will not be included.\n    \"\"\"\n\n    data = self.spreadsheet.values_get(self.title)\n\n    try:\n        return fill_gaps(data['values'])\n    except KeyError:\n        return []\n\\end{code}"
    },
    {
        "original": "def modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n        \"\"\"\n        \u4fee\u6539\u5c0f\u7a0b\u5e8f\u670d\u52a1\u5668\u6388\u6743\u57df\u540d\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1489138143_WPbOO\n\n        :param action: \u589e\u5220\u6539\u67e5\u7684\u64cd\u4f5c\u7c7b\u578b\uff0c\u4ec5\u652f\u6301 'add', 'delete', 'set', 'get'\n        :param request_domain: request \u5408\u6cd5\u57df\u540d\n        :param wsrequest_domain: socket \u5408\u6cd5\u57df\u540d\n        :param upload_domain: upload file \u5408\u6cd5\u57df\u540d\n        :param download_domain: download file \u5408\u6cd5\u57df\u540d\n        \"\"\"\n        return self._post(\n            'wxa/modify_domain',\n            data={\n                'action': action,\n                'requestdomain': request_domain,\n                'wsrequestdomain': wsrequest_domain,\n                'uploaddomain': upload_domain,\n                'downloaddomain': download_domain,\n            }\n        )",
        "rewrite": "Revise the code with your best effort: \"def modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n        \"\"\"\n        \u4fee\u6539\u5c0f\u7a0b\u5e8f\u670d\u52a1\u5668\u6388\u6743\u57df\u540d\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1489138143_WPbOO\n\n        :param action: \u589e\u5220\u6539\u67e5\u7684\u64cd\u4f5c\u7c7b\u578b\uff0c\u4ec5\u652f\u6301 'add', 'delete', 'set', 'get'\n        :param request_domain: request \u5408\u6cd5\u57df\u540d\n        :param wsrequest_domain: socket \u5408\u6cd5\u57df\u540d\n        :param upload_domain: upload file \u5408\u6cd5\u57df\u540d\n        :param download_domain: download file \u5408\u6cd5\u57df\u540d\n        \"\"\"\n        return self._post(\n            'wxa/modify_domain',\n            data={\n                'action': action,\n                'requestdomain': request_domain,\n                'wsrequestdomain': wsrequest_domain,\n                'uploaddomain': upload_domain,\n                'downloaddomain': download_domain,\n            }\n        )\" . No need to explain. Just write code:\n\ndef modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n    \"\"\"\n    \u4fee\u6539\u5c0f\u7a0b\u5e8f\u670d\u52a1\u5668\u6388\u6743\u57df\u540d\n    \u8be6\u60c5\u8bf7\u53c2\u8003\n    https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1489138143_WPbOO\n\n    :param action: \u589e\u5220\u6539\u67e5\u7684\u64cd"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "rewrite": "Revise the code with your best effort: \"def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns\" . No need to explain. Just write code:\n\ndef ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiList"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "rewrite": "Revise the code with your best effort: \"def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))\" . No need to explain. Just write code:\n\ndef saveFile(self):\n    \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n    filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n        self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n    if filepath:\n        # Ensure .ag as file ending\n        if not filepath.endswith(\".ag\"):\n            filepath = \"{}.ag\".format(filepath)\n\n        self.showStatus(\"Saving %s...\" % str(filepath))"
    },
    {
        "original": "def Copy(self, field_number=None):\n    \"\"\"Returns descriptor copy, optionally changing field number.\"\"\"\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n      new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)",
        "rewrite": "Revise the code with your best effort: \"def Copy(self, field_number=None):\n    \"\"\"Returns descriptor copy, optionally changing field number.\"\"\"\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n      new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)\" . No need to explain. Just write code:\n\ndef Copy(self, field_number=None):\n    \"\"\"Returns descriptor copy, optionally changing field number.\"\"\"\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n        new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    }
]