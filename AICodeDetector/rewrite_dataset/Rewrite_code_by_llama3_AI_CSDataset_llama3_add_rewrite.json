[
    {
        "original": "\nfrom kafka import KafkaConsumer\n\ndef beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {}\n    for p in partitions:\n        topic_partition = p.topic, p.partition\n        try:\n            offset = consumer.beginning_offset(topic_partition)\n            offsets[p] = offset\n        except Exception as e:\n            if isinstance(e, UnsupportedVersionError) or isinstance(e, KafkaTimeoutError):\n                raise e\n    return offsets\n",
        "rewrite": "```python\nfrom kafka import KafkaConsumer, UnsupportedVersionError, KafkaTimeoutError\n\ndef get_beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {p: None for p in partitions}\n    \n    try:\n        topic_partitions = [(p.topic, p.partition) for p in partitions]\n        beginning_offsets = consumer.beginning_offsets(topic_partitions)\n        \n        for (topic_partition, offset_response) in zip(topic_partitions, beginning_offsets):\n            topicPartitionObjectListoffset_value=offset_response.offset\n            offsetMap={partition_object:topicPartitionObjectListoffset_value \n                       for (partition_object JuniUsualTopic,_"
    },
    {
        "original": "\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0] if type == 'new' else value[1] for key, value in diff_dict.items() if len(value) == 2}\n",
        "rewrite": "```python\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0 if type == 'new' else -1] for key, value in diff_dict.items() if value}\n```"
    },
    {
        "original": "\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    \"\"\"\n    Resolve the field within the given state.\n    \"\"\"\n    obj = state[obj_alloc_id]\n    if not hasattr(obj, field_class_name):\n        raise AttributeError(f\"Object {obj_alloc_id} does not have class {field_class_name}\")\n    field_cls = getattr(obj, field_class_name)\n    if not hasattr(field_cls, field_name):\n        raise AttributeError(f\"",
        "rewrite": "```python\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name):\n    obj = state.get(obj_alloc_id)\n    if obj and hasattr(obj, field_class_name):\n        field_cls = getattr(obj, field_class_name)\n        if isinstance(field_cls, type) and hasattr(field_cls, 'get'):\n            return getattr(field_cls()._inst.__dict__[field_type], field_name)\n        elif isinstance(field_cls(), object) and hasattr(field_cls(), '_inst'):\n            return field_cls()._inst.__dict__.get('field_type', {}).get('field_name')\n```\n\n \nHowever there are some un"
    },
    {
        "original": "\ndef pd_coords(self, comp):\n    n_elements = len(comp)\n    coords = [0] * (n_elements - 1)\n    sum_comp = sum(comp)\n    \n    for i in range(n_elements - 1):\n        coords[i] = comp[i] / sum_comp\n    \n    return coords\n",
        "rewrite": "```python\ndef pd_coords(self, comp):\n    total = sum(comp)\n    return [i / total for i in comp[:-1]]\n```"
    },
    {
        "original": "\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    i = 0\n    while i < len(base_path) and i < len(target_path) and base_path[i] == target_path[i]:\n        i += 1\n    rel_ref = '../' * (len(base_path) - i) + '/'.join(target_path[i:])\n    return rel_ref if rel_ref else '.'\n",
        "rewrite": "```python\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    \n    for i, (a, b) in enumerate(zip(base_path, target_path)):\n        if a != b:\n            break\n    \n    common_prefix_length = i\n    \n    rel_ref = '../' * (len(base_path) - common_prefix_length - 1) + '/'.join(target_path[common_prefix_length:])\n    \n    return '.' if rel_ref == '../' else rel_ref\n```"
    },
    {
        "original": "\ndef is_enhanced_rr_cap_valid(self):\n    open_messages_sent = self.open_messages_sent\n    open_messages_received = self.open_messages_received\n\n    if open_messages_sent and open_messages_received:\n        return 'enhanced-route-refresh-capability' in open_messages_sent and 'enhanced-route-refresh-capability' in open_messages_received\n    else:\n        return False\n",
        "rewrite": "```python\ndef is_enhanced_rr_cap_valid(self):\n    return (self.__dict__.get('open_messages_sent') and \n            self.__dict__.get('open_messages_received') and \n            self.open_messages_sent.get('enhanced-route-refresh-capability') and \n            self.open_messages_received.get('enhanced-route-refresh-capability'))\n```"
    },
    {
        "original": "\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = element\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags = {k: v for k, v in element.items() if k != 'nodes' and k !=",
        "rewrite": "```python\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = {'id': element['id'], 'lat': element['lat'], 'lon': element ['lon']}\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags ={k: v for k, v in.element.items() if k not in ['nodes',]}\n                ways_refs =[ref for ref in  ("
    },
    {
        "original": "\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    # Get all possible permutations of mapping between species of two structures\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n\n    # Initialize minimum difference in electronegativity and best mapping\n    min_diff = float('inf')\n    best_mapping = None\n\n    # Iterate over",
        "rewrite": "```python\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n    min_diff = float('inf')\n    best_mapping = None\n    \n    for perm in perms:\n        diff = sum(abs(MoleculeClass.get_el_sp(species_1) - MoleculeClass.get_el_sp(species_2)) \n                   for species_1, species_2 in zip(struct1.species, [struct2.species[i] for i in perm"
    },
    {
        "original": "\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    pass\n\nclass HPackIndexedHdr(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithIncrIndexing(HPackHeaders):\n    pass\n\n\ndef _convert_a_header_to_a_h2_header(\n    hdr_name: str,\n    hdr_value: str,\n    is_sensitive: Callable[[str, str], bool",
        "rewrite": "```python\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    \"\"\"Base class for HPACK headers\"\"\"\n    pass\n\n\nclass HPackIndexedHdr(HPackHeaders):\n    \"\"\"Index-based header in the dynamic table.\"\"\"\n    def __init__(self, index: int):\n        self.index = index\n\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    \"\"\"Literal header field with incremental indexing disabled.\"\"\"\n    def __init__(self, name: str, value: str):\n        self.name = name\n        self.value = value\n\n\nclass HPackLitHdrWithIncrIndexing(HPack"
    },
    {
        "original": "\nclass BigchainTransactionCreator:\n    def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        from bigchaindb.common.transaction import Transaction\n        \n        # Initialize an empty transaction object \n        transaction = Transaction()\n\n        # Set transaction type to 'CREATE'\n        transaction.operation = 'CREATE'\n\n        # Add signers to transaction inputs \n        for signer in tx_signers:\n            transaction.add_input(signer)\n\n        # Add recipients and amounts to transaction outputs \n       ",
        "rewrite": "```python\nfrom bigchaindb.common.transaction import Transaction\nfrom typing import Dict, Any\n\nclass BigchainTransactionCreator:\n    @classmethod\n    def create(\n        cls,\n        tx_signers: list,\n        recipients: Dict[str, int],\n        metadata: Dict[str, Any] = None,\n        asset: Dict[str, Any] = None\n    ) -> Transaction:\n        \n        transaction = Transaction(operation='CREATE')\n        \n        for signer in tx_signers:\n            transaction.add_input(signer)\n        \n        for recipient, amount in recipients.items():\n            transaction.add_output(recipient, amount)\n        \n        if metadata:\n           "
    },
    {
        "original": "\nfrom datetime import datetime\nimport pytz\n\ndef utc_dt_to_local_dt(dtm):\n    utc_dt = datetime.strptime(dtm, '%Y-%m-%d %H:%M:%S')\n    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)\n    local_dt = utc_dt.astimezone()\n    return local_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
        "rewrite": "```python\nfrom datetime import datetime\nimport pytz\n\ndef utc_to_local(utc_dtm, tz_str):\n    \"\"\"Converts UTC time to local time.\"\"\"\n    utc_dtm = datetime.strptime(utc_dtm, '%Y-%m-%d %H:%M:%S')\n    utc_dtm = pytz.utc.localize(utc_dtm)\n    tz_obj = pytz.timezone(tz_str)\n    local_dt = utc_dtm.astimezone(tz_obj)\n    return local_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n# Usage:\nprint(utc_to"
    },
    {
        "original": "\ndef _getScriptSettingsFrom IniFile(policy_info):\n    # Open and read the file\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    # Initialize variables\n    scripts = {}\n    current_script = None\n\n    # Parse lines\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            # Section header found, extract script name\n            start_br",
        "rewrite": "```python\ndef _getScriptSettingsFromFile(policy_info):\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    scripts = {}\n    current_script = None\n\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            start_bracket_index = 0\n            end_bracket_index = 0\n            for i, char in enumerate(line):\n                if char == '[':\n                    start_bracket_index += 1\n                elif char == ']':\n                    end_bracket_index += 1\n                \n            script_name"
    },
    {
        "original": "\ndef _get_triplet_scores(self, triangles_list):\n    scores = {}\n    for triangle in triangles_list:\n        triangle_set = frozenset(triangle)\n        # assuming the score calculation is a placeholder, replace with actual logic\n        score = len(triangle) * 2.0\n        scores[triangle_set] = score\n    return scores\n",
        "rewrite": "```python\ndef _get_triplet_scores(self, triangles_list):\n    return {frozenset(triangle): float(len(triangle) * 2) for triangle in triangles_list}\n```"
    },
    {
        "original": "\nasync def vcx_messages_update_status(msg_json: str):\n    import json\n    import asyncio\n    from aiohttp import ClientSession\n\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n\n        for message in messages:\n            url = f'https://example.com/update_message_status/{connection_id}'\n            data = {'message_id': message['uid'], 'status",
        "rewrite": "```python\nimport json\nfrom aiohttp import ClientSession\n\nasync def vcx_messages_update_status(msg_json: str):\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n        \n        url_template = f'https://example.com/update_message_status/{connection_id}'\n        \n        async def update_message_status(message):\n            data = {'message_id': message['uid'], 'status': 'updated'}\n            async with session.post(url_template, json=data) as response:\n                return await response.json()\n        \n        # Run"
    },
    {
        "original": "\nimport socket\nimport time\n\nclass IsoTPSocket:\n    def __init__(self):\n        self.receive_buffer = b''\n\n    def recv_with_timeout(self, timeout=1):\n        start_time = time.time()\n        while True:\n            if self.receive_buffer:\n                return self.receive_buffer.pop(0)\n            elif time.time() - start_time > timeout:\n                return None\n",
        "rewrite": "```python\nimport socket\nimport select\nfrom queue import Queue\n\nclass IsoTPSocket(socket.socket):\n    def __init__(self, *connection):\n        super().__init__(socket.AF_INET, socket.SOCK_STREAM)\n        self.connect(connection)\n        self.receive_buffer = Queue()\n\n    def recv_with_timeout(self, timeout=1):\n        readable, _, _ = select.select([self], [], [], timeout)\n        if readable:\n            data = self.recv(4096)\n            for byte in data:\n                self.receive_buffer.put(byte)\n            return bytes(data)\n        else:\n            return None\n```\n\nHowever the code could look more"
    },
    {
        "original": "\nimport math\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, (p-1) * (p**(e-1)))\n    return lambda_val\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a, b)\n",
        "rewrite": "```python\nimport math\n\ndef euler_totient(p, e):\n    return int(math.pow(p - 1, e) * (p - 1))\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a,b)\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, euler_totient(p,e))\n    return lambda_val\n```"
    },
    {
        "original": "\nimport requests\n\ndef absent(name, profile=\"splunk\"):\n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    response = requests.delete(url, auth=(\"username\", \"password\"))\n    if response.status_code == 200:\n        return True\n    else:\n        return False\n",
        "rewrite": "```python\nimport requests\n\ndef delete_job(\n    name: str, \n    profile: str = \"splunk\", \n    username: str = None, \n    password: str = None\n) -> bool:\n    \"\"\"Deletes a Splunk job\"\"\"\n    \n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    \n    if not (username and password):\n        raise ValueError(\"Username and/or password cannot be empty or None.\")\n        \n    response = requests.delete(url, auth=(username, password))\n    \n    return response.status_code == 200\n```"
    },
    {
        "original": "\ndef GetNotificationsForAllShards(self, queue):\n    notifications = []\n    for shard in self.GetShardNames(queue):\n        notifications.extend(self.GetNotificationsForShard(queue, shard))\n    return notifications\n",
        "rewrite": "```python\ndef get_notifications_for_all_shards(self, queue):\n    return [n \n            for shard in self.get_shard_names(queue) \n            for n in self.get_notifications_for_shard(queue, shard)]\n```"
    },
    {
        "original": "\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n\n    def differing_functions_with_consts(self):\n        matches = []\n        for i in range(len(self.functions)):\n            for j in range(i + 1, len(self.functions)):\n                x = symbols('x')\n                eq1 = Eq(eval(self.functions[i]), 0)\n                eq2 = Eq(eval(self.functions[j]), 0)\n               ",
        "rewrite": "```python\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n        self.x = symbols('x')\n\n    def differing_functions_with_consts(self):\n        matches = []\n        for i in range(len(self.functions)):\n            for j in range(i + 1, len(self.functions)):\n                f1_expr = eval(self.functions[i])\n                f2_expr = eval(self.functions[j])\n                \n                if str(f1_expr) != str(f2_expr):  \n                    eq1 = Eq(f1_expr, 0)\n                    eq2 = Eq(f"
    },
    {
        "original": "\nclass EOPatchSaver:\n    def save(self, eopatch, use_tmp=True):\n        if use_tmp:\n            # Save to temporary file\n            with open(\"temp.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n        else:\n            # Save to intended location\n            with open(\"eopatch.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n",
        "rewrite": "```python\nimport os\nimport pickle\n\nclass EOPatchSaver:\n    def __init__(self, tmp_dir: str, final_dir: str):\n        self.tmp_dir = tmp_dir\n        self.final_dir = final_dir\n\n    def save(self, eopatch: object, filename: str, use_tmp: bool = True) -> None:\n        filepath = os.path.join(self.tmp_dir if use_tmp else self.final_dir, f\"{filename}.eopatch\")\n        with open(filepath, \"wb\") as f:\n            pickle.dump(eopatch, f)\n```"
    },
    {
        "original": "\nclass StateTrie:\n    def __init__(self):\n        self.trie = {}\n\n    def _addAttr(self, txn, isCommitted=False) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.tr",
        "rewrite": "```python\nclass StateTrie:\n    def __init__(self):\n        self.trie: dict[str, int] = {}\n\n    def add_attribute(self, txn: tuple[str, str, str, bytes]) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.trie[key] = hash(data)\n```\n\nNote that the input parameters were typed explicitly according"
    },
    {
        "original": "\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ET\n\ndef _find_packages(root):\n    for subdir, dirs, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                zf = zipfile.ZipFile(os.path.join(subdir, file))\n                pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                yield (pkg_xml, zf, os.path.relpath(subdir, root))\n",
        "rewrite": "```python\nimport os\nimport zipfile\nfrom xml.etree import ElementTree as ET\n\ndef find_packages(root: str) -> iter:\n    for subdir, _, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                zip_path = os.path.join(subdir, file)\n                with zipfile.ZipFile(zip_path) as zf:\n                    pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                    yield pkg_xml, zf, os.path.relpath(subdir, root)\n```"
    },
    {
        "original": "\nimport bluetooth\n\ndef srbt1(bt_address, pkts, *_args={}, **_kargs={}):\n    sock = bluetoothAMP.get_socket()\n    sock.connect((bt_address, 1))\n    sock.send(pkts)\n    data = sock.recv(1024)\n    return data\n",
        "rewrite": "```python\nimport bluetooth\n\ndef srbt1(bt_address: str, packets: bytes, **kwargs) -> bytes:\n    devices = bluetooth.discover_devices(lookup_names=True)\n    if not devices:\n        return b\"\"\n    \n    device = devices[0]\n    port = device.services[0][2] if device.services else None\n    if port is None:\n        return b\"\"\n\n    try:\n        sock = bluetooth.BluetoothSocket(bluetooth.RFCOMM)\n        sock.connect((bt_address, port))\n        sock.send(packets)\n        data = sock.recv(1024)\n        return data\n   "
    },
    {
        "original": "\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def GetIPAddresses(self):\n        ip_array = []\n        for interface in netifaces.interfaces():\n            if interface == 'lo':  # ignore loopback interface\n                continue\n            lst = netifaces.ifaddresses(interface)\n            teste = {}\n            for item in lst:\n                if(item == 2): # AF_INET \n                    for thing in lst[item]:\n                        teste={\n                            \"iname\":interface,\n                           ",
        "rewrite": "```python\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def get_ip_addresses(self):\n        ip_array = []\n        for interface in [i for i in netifaces.interfaces() if i != 'lo']:\n            addresses = netifaces.ifaddresses(interface)\n            ip_address_families = [(socket.AF_INET, 'ipv4'), (socket.AF_INET6, 'ipv6')]\n            for family, addr_type in zip(filter(lambda x: x in addresses, ip_address_families), ['ipv4', 'ipv6']):\n                for addr_info in addresses[family]:\n                    ip_array.append({\n                        'iname"
    },
    {
        "original": "\ndef operate(self, point):\n    # Assuming operate function is defined elsewhere\n    pass\n\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    \"\"\"\n    Checks if two points are symmetrically related.\n\n    Args:\n        point_a (3x1 array): First point.\n        point_b (3x1 array): Second point.\n        tol (float): Absolute tolerance for checking distance.\n\n    Returns:\n        True if self.operate(point",
        "rewrite": "```python\nfrom abc import ABC, abstractmethod\n\nclass SymmetryMixin(ABC):\n    @abstractmethod\n    def operate(self, point):\n        pass\n\n    def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        return False\n```\n\nor using NotImplementedError method \n\n```python\nclass SymmetryMixin:\n    def operate(self, point):\n        raise NotImplementedError(\"Subclass must implement this method\")\n\n    def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        raise NotImplementedError(\"Subclass must implement this method\")\n```"
    },
    {
        "original": "\ndef validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    parts = ip.split(\".\")\n    if len(parts) != 4:\n        return False\n    for part in parts:\n        if not part.isdigit():\n            return False\n        i = int(part)\n        if i < 0 or i > 255:\n            return False\n    return True\n",
        "rewrite": "```python\nimport ipaddress\n\ndef validate_rpc_host(ip):\n    try:\n        ipaddress.IPv4Address(ip)\n        return True\n    except ValueError:\n        return False\n```"
    },
    {
        "original": "\nimport subprocess\nimport logging\n\ndef find_available_interfaces():\n    \"\"\"\n    Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n    \n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-",
        "rewrite": "```python\nimport subprocess\n\ndef find_available_can_interfaces():\n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-8\").splitlines()\n        interfaces = [line.split(\": \")[1].split()[0] for line in lines if \"can\" in line.lower() and \":\" in line]\n        return interfaces\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return []\n```"
    },
    {
        "original": "\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {}\n\n    def save_session(self, sid, session, namespace=None):\n        if namespace is None:\n            namespace = self.namespace\n        if namespace not in self.sessions:\n            self.sessions[namespace] = {}\n        self.sessions[namespace][sid] = session\n",
        "rewrite": "```python\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {} if namespace is None else {namespace: {}}\n\n    def save_session(self, sid, session):\n        if self.namespace not in self.sessions:\n            self.sessions[self.namespace] = {}\n        self.sessions[self.namespace][sid] = session\n```"
    },
    {
        "original": "\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    public_keys = []\n    if self.v3_signature_block:\n        for cert in self.v3_signature_block.certs:\n            pub_key = cert.public_key()\n            der_pub_key = pub_key.public_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n            )\n            public_keys.append(der",
        "rewrite": "```python\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    if not self.v3_signature_block:\n        return []\n    \n    return [\n        cert.public_key().public_bytes(\n            encoding=serialization.Encoding.DER, \n            format=serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        for cert in self.v3_signature_block.certs\n    ]\n```"
    },
    {
        "original": "\ndef chemical_symbols(atom_species, symbol_length):\n    symbols = []\n    ascii_offset = 97  # ASCII value of 'a'\n    for i in range(atom_species):\n        symbol = \"\"\n        for j in range(symbol_length):\n            symbol += chr(ascii_offset + ((i + j) % 26))\n        symbols.append(symbol)\n    return symbols\n",
        "rewrite": "```python\ndef chemical_symbols(n, length):\n    return [''.join(chr(97 + (i + j) % 26) for j in range(length)) for i in range(n)]\n```"
    },
    {
        "original": "\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    if stream is None:\n        return yaml.safe_dump_all(documents, default_flow_style=True, **kwds)\n    else:\n        yaml.safe_dump_all(documents, stream=stream, default_flow_style=True, **kwds)\n",
        "rewrite": "```python\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    yaml.safe_dump_all(\n        documents,\n        stream=stream or sys.stdout,\n        default_flow_style=False,\n        **kwds\n    )\n```"
    },
    {
        "original": "\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit,\n                        param_resolver,\n                        qubit_order,\n                        initial_state: Union[int, np.ndarray]) -> Iterator:\n    # Check if initial_state is an integer \n    if isinstance(initial_state, int):\n      # Set initial_state to computational basis corresponding to this integer \n      pass\n  \n    # Check if initial_state is a numpy array  \n    elif isinstance",
        "rewrite": "```python\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit, \n                        param_resolver, \n                        qubit_order: list, \n                        initial_state: Union[int, np.ndarray]) -> Iterator:\n    if isinstance(initial_state, int):\n        num_qubits = len(qubit_order)\n        state_vector = np.zeros(2 ** num_qubits)\n        state_vector[initial_state] = 1\n    \n    elif isinstance(initial_state, np.ndarray):\n        state_vector = initial_state\n    \n    else:\n        raise ValueError(\"Invalid initial state type\")\n"
    },
    {
        "original": "\ndef predictive_variance(self, mu, variance, predictive_mean=None, Y_metadata=None):\n    if predictive_mean is None:\n      # If no predictive mean is provided assume it's 0\n      predictive_mean = 0  \n    expectation_squared = (mu - predictive_mean) ** 2 \n    variance_squared = variance ** 2 \n    return expectation_squared + variance_squared\n",
        "rewrite": "```python\ndef predictive_variance(self, mu, variance, predictive_mean=0, Y_metadata=None):\n    return (mu - predictive_mean) ** 2 + variance\n```"
    },
    {
        "original": "\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id):\n        if id in self.configs:\n            del self.configs[id]\n            return True\n        else:\n            raise docker.errors.NotFound(\"No config with that ID exists\")\n",
        "rewrite": "```python\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id: str) -> bool:\n        try:\n            del self.configs[id]\n            return True\n        except KeyError:\n            raise docker.errors.NotFound(\"No config with that ID exists\")\n```"
    },
    {
        "original": "\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    \n    \nimport pyVmomi\n    \ndef get_service_instance():\n",
        "rewrite": "```python\nimport pyVmomi\n\ndef get_service_instance():\n    return pyVmomi.SmartConnect(\"https://localhost/sdk\")\n\ndef get_mo_by_moid(si=None, obj_type=\"\", obj_moid=\"\"):\n    if not si:\n        si = get_service_instance()\n        \n    mo_ref = si.content.searchIndex.FindByInventoryPath(obj_type, obj_moid)\n    \n    if not mo_ref:\n        raise Exception(f\"Object {obj_type} with MOID {obj_moid} not found\")\n        \n    return mo_ref\n```"
    },
    {
        "original": "\ndef ConfigureUrls(config, external_hostname = None):\n    if external_hostname is None:\n        external_hostname = input(\"Enter the external hostname: \")\n    config[\"AdminUI.url\"] = f\"http://{external_hostname}:8000\"\n    config[\"Client.frontend_url\"] = f\"http://{external_hostname}:8080\"\n    config[\"ClientPoll.url\"] = f\"http://{external_hostname}:8081\"\n    return config\n",
        "rewrite": "```python\ndef configure_urls(config, external_hostname=None):\n    external_hostname = external_hostname or input(\"Enter the external hostname: \")\n    base_url = f\"http://{external_hostname}\"\n    config.update({\n        \"AdminUI.url\": f\"{base_url}:8000\",\n        \"Client.frontend_url\": f\"{base_url}:8080\",\n        \"ClientPoll.url\": f\"{base_url}:8081\"\n    })\n    return config\n```"
    },
    {
        "original": "\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def Parse(self, cmd, args, stdout, stderr, return_val, time_taken):\n        self.knowledge_base[cmd] = {\n            'args': args,\n            'stdout': stdout,\n            'stderr': stderr,\n            'return_val': return_val,\n            'time_taken': time_taken\n        }\n",
        "rewrite": "```python\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def parse(self, command: str, *args: list[str], stdout: str, stderr: str, return_code: int, execution_time: float) -> None:\n        self.knowledge_base[command] = {\n            'args': args,\n            'stdout': stdout,\n            'stderr': stderr,\n            'return_code': return_code,\n            'execution_time': execution_time\n        }\n\ndef copy_args(args: list[str]) -> tuple[str]:\n    return tuple(args)\n```\n\nYou would call `parse` function"
    },
    {
        "original": "\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as f:\n        content = f.read()\n        # parse ASF content into skeleton structure\n        # TO DO: implement parsing logic\n        pass\n",
        "rewrite": "```python\nimport xml.etree.ElementTree as ET\n\ndef load_skeleton(self, file_name):\n    tree = ET.parse(file_name)\n    root = tree.getroot()\n    self.skeleton = {}\n\n    for child in root:\n        if child.tag == 'Vertex':\n            self.skeleton[child.attrib['id']] = {\n                'x': float(child.attrib['x']),\n                'y': float(child.attrib['y']),\n                'z': float(child.attrib['z'])\n            }\n        elif child.tag == 'Segment':\n            self.skeleton[child.attrib['id']] = {\n                'type': child.attrib['type']\n           "
    },
    {
        "original": "\ndef _ruby_installed(ret, ruby, user=None):\n    if user is not None:\n        cmd = f\"su {user} -c 'ruby -v | grep {ruby}'\"\n    else:\n        cmd = f\"ruby -v | grep {ruby}\"\n    ret[\"installed\"] = True if os.system(cmd) == 0 else False\n",
        "rewrite": "```python\nimport subprocess\n\ndef _ruby_installed(ret, ruby, user=None):\n    if user:\n        cmd = [\"su\", \"-\", user, \"-c\", f\"ruby -v | grep {ruby}\"]\n        subprocess.run(cmd, check=True)\n    else:\n        cmd = [\"ruby\", \"-v\"]\n        out = subprocess.check_output(cmd)\n        ret[\"installed\"] = b\"{} {}\".format(ruby, ruby).decode() in out.decode()\n```"
    },
    {
        "original": "\nclass Structure:\n    # assuming Structure class is defined somewhere\n    pass\n\nclass Element:\n    # assuming Element class is defined somewhere\n    pass\n\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        A dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    \n    projection = {}\n    \n    # iterate over",
        "rewrite": "```python\nfrom dataclasses import defaultdict\nfrom enum import Enum\n\nclass Spin(Enum):\n    up = 1\n    down = 2\n\nclass ProjectionGetter:\n    def get_projection_on_elements(self, structure):\n        projection = defaultdict(lambda: defaultdict(dict))\n        \n        for spin in [Spin.up, Spin.down]:\n            for k_index in range(structure.n_kpoints):\n                element_projections = {}\n                for element in structure.elements_at_kpoint(k_index):\n                    element_projections[element] = self.calculate_projection(spin, k_index, None, element)\n                projection[spin][k_index] = element_projections\n        \n"
    },
    {
        "original": "\nclass VectorArgs:\n    def __init__(self, *args):\n        self.args = args\n\n    def vector_args(self):\n        lanes = self.args.split(',')\n        lane_pairs = [lane.split('..') for lane in lanes]\n        lane_pairs.sort(key=lambda x: int(x[1]), reverse=True)\n        return [(int(pair[0]), int(pair[1])) for pair in lane_pairs]\n\n# Example usage:\nvector_args_instance = VectorArgs('0..10,",
        "rewrite": "```python\nclass VectorArgs:\n    def __init__(self, args_string):\n        self.args_string = args_string\n\n    def parse_vector_args(self):\n        lanes = (lane.strip() for lane in self.args_string.replace('\"', '').replace('(', '').replace(')', '').split(','))\n        return sorted(((int(pair[0]), int(pair[1])) for lane in lanes if '..' in lane if len(lane) > 5 and (pair := tuple(lane.split('..'))))), key=lambda x: x[1], reverse=True)\n\n\nvector_args_instance = VectorArgs('(0..10), (3.."
    },
    {
        "original": "\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key, value in list(from_kwargs.items()):\n        if key.startswith(keyword + '_'):\n            to_kwargs[key.replace(keyword + '_', '')] = value\n            if clean_origin:\n                del from_kwargs[key]\n    return to_kwargs\n",
        "rewrite": "```python\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key in list(from_kwargs.keys()):\n        if key.startswith(f\"{keyword}_\"):\n            new_key = key[len(keyword) + 1:]\n            to_kwargs[new_key] = from_kwargs.pop(key) if clean_origin else from_kwargs[key]\n    return to_kwargs\n```"
    },
    {
        "original": "\nclass ActionExecutor:\n    def __init__(self):\n        self.actions = {}\n\n    def register_action(self, action_name, func):\n        self.actions[action_name] = func\n\n    def _RunAction(self, rule, client_id):\n        count = 0\n        for action in rule['actions']:\n            if action['name'] in self.actions:\n                self.actions[action['name']](client_id)\n                count += 1\n        return count\n",
        "rewrite": "```python\nclass ActionExecutor:\n    def __init__(self):\n        self.actions = {}\n\n    def register_action(self, action_name: str, func):\n        self.actions[action_name] = func\n\n    def run_actions(self, rule: dict, client_id) -> int:\n        count = sum(\n            action['name'] in self.actions and \n            (self.actions[action['name']](client_id), True)[1] \n            for action in rule.get('actions', []) if 'name' in action\n        )\n        return count\n```\n\nor \n\n```python\nclass ActionExecutor:\n    def __init__("
    },
    {
        "original": "\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def df(self):\n        try:\n            info = self.client.info()\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': len(self.client.volumes.list()),\n                'Networks': len(self.client.networks.list())\n            }\n        except docker.errors.APIError as e:\n            raise e\n",
        "rewrite": "```python\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def get_df_info(self):\n        try:\n            info = self.client.info()\n            volumes_count = len(self.client.volumes.list())\n            networks_count = len(self.client.networks.listatisch$\")\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': volumes_count,\n                'Networks': networks_count\n            }\n        except docker.errors.APIError as e:\n            raise \n```"
    },
    {
        "original": "\ndef file_extension(category=None):\n    extensions = {\n        'audio': ['mp3'],\n        'image': ['jpg', 'jpeg', 'png', 'gif'],\n        'office': ['docx', 'pdf', 'pptx'],\n        'text': ['txt', 'doc'],\n        'video': ['mp4']\n    }\n    if category:\n        return extensions.get(category.lower(), [])\n    else:\n        return []\n",
        "rewrite": "```python\ndef file_extensions(category=None):\n    extensions = {\n        \"audio\": [\"mp3\", \"wav\"],\n        \"image\": [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"tiff\"],\n        \"office\": [\"docx\", \"docm\", \".pdf\"], \n        # fixed typo Splash & added dot before pdf extension\n        # Consolidated text categories\n        'document': ['txt', 'doc'],\n        'video': ['mp4', 'avi', 'mkv']\n    }\n    return extensions.get(category.lower(), []) if category else []\n```"
    },
    {
        "original": "\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    import numpy as np\n    \n    # Calculate air mass\n    am = pressure / (101325 * np.exp(-0.000121 * surface_tilt))\n    \n    # Calculate relative air mass\n    mam = am /",
        "rewrite": "```python\nimport numpy as np\n\ndef _gti_dirint_gte_90(\n    poa_global, \n    aoi, \n    solar_zenith, \n    solar_azimuth,\n    surface_tilt, \n    times, \n    kt_prime,\n    pressure=101325.0,\n    temp_dew=None,\n):\n    \nam = pressure / (101325.0 * np.exp(-0.000121 * surface_tilt))\nmam = am / 1.00271 - 0.014 * (1 - 0.25) ** 1.261\n```\n\nPlease note that"
    },
    {
        "original": "\nclass RDFURN:\n    def __init__(self, value):\n        self.value = value\n\nclass Client:\n    def ListChildren(self, urn, limit=None, age=\"NEWEST_TIME\"):\n        # assuming we have a function get_children that returns all children\n        all_children = self.get_children(urn)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children = all_children\n        elif age == \"NEWEST_TIME\":\n            filtered_children = [all_children[0]]",
        "rewrite": "```python\nfrom typing import List, Optional\n\nclass RDFURN:\n    def __init__(self, value: str):\n        self.value = value\n\nclass Client:\n    def __init__(self):\n        pass\n    \n    def get_children(self, urn: str) -> List[object]:\n        # implement this method to return all children\n        pass\n\n    def list_children(self, urn: RDFURN, limit: Optional[int] = None, age: str = \"NEWEST_TIME\") -> List[object]:\n        all_children = self.get_children(urn.value)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children"
    },
    {
        "original": "\ndef _convert_validators_to_mapping(validators):\n    validator_mapping = {}\n    for validator in validators:\n        key = (str(validator[\"check\"]), validator[\"comparator\"])\n        validator_mapping[key] = validator\n    return validator_mapping\n",
        "rewrite": "```python\ndef convert_validators_to_mapping(validators):\n    return {(f\"{v['check']}_{v['comparator']}\"): v for v in validators}\n```"
    },
    {
        "original": "\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n    if users is None:\n        user_path = knowledge_base.interpolate_string(path, **path_args)\n        return user_path\n    else:\n        result = []\n        for user in users:\n            user_path_args = path_args.copy()\n            user_path_args['user'] = user\n            user_path = knowledge_base.interpolate_string(path",
        "rewrite": "```python\ndef interpolate_path(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n\n    if users is None:\n        return knowledge_base.interpolate_string(path, path_args)\n    else:\n        result = [\n            knowledge_base.interpolate_string(\n                path,\n                {**path_args or {}, 'user': user}\n            ) \n            for user in users\n        ]\n        return result\n```"
    },
    {
        "original": "\nimport numpy as np\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \n     from scipy.stats import gaussian_kde\n    \n     x = np.array(x)\n     if xmin is None:\n         xmin = np.min(x)\n     if xmax is None:\n         xmax = np.max(x)\n         \n     kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method='silverman')\n     xi = np.linspace(xmin,xmax",
        "rewrite": "```python\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\ndef _fast_kde(x, cumulative=False, bw='silverman', xmin=None, xmax=None):\n    x = np.array(x)\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n    kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method=bw)\n    xi = np.linspace(xmin, xmax, 4000)  # Increased resolution for smoother output\n    if cumulative:\n        yi = kde.integrate_box_1d(0.001*x"
    },
    {
        "original": "\ndef md_options_to_metadata(options):\n    metadata = {}\n    language = None\n    for option in options:\n        if option.startswith('language:'):\n            language = option.split('language:')[1].strip()\n        elif '=' in option:\n            key, value = option.split('=', 1)\n            metadata[key.strip()] = value.strip()\n    return language, metadata\n",
        "rewrite": "```python\ndef md_options_to_metadata(options):\n    metadata = {'language': None}\n    for option in options:\n        if '=' in option:\n            key, value = map(str.strip, option.split('=', 1))\n            metadata[key] = value\n        elif option.startswith('language:'):\n            metadata['language'] = option[9:].strip()\n    return metadata\n```"
    },
    {
        "original": "\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\nclass Trace:\n    def __init__(self):\n        self.frames = []\n\n    def add_frame(self,",
        "rewrite": "```python\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\n    def __repr__(self) -> str:\n        return f\"TraceFrame(caller='{self.caller}', callee='{self.callee}', kind={self.kind})\"\n\n\nclass Trace:\n    def __init__(self):\n        self.frames"
    },
    {
        "original": "\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n        batch_Y = self.Y[start:end]\n        \n        self.index += self.batch_size",
        "rewrite": "```python\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n\t\tbatch_Y=\tself.Y[start:end]\n\t\t\n\t\tself.index=(self.index+self.batch_size)%len(self.X)\n\t\t\n\t\treturn batch_X,batch_Y\n\n# usage example:\n# gen=BatchGenerator(x"
    },
    {
        "original": "\ndef createSummary(self, log):\n    \"\"\"\n    Create nice summary logs.\n\n    @param log: log to create summary off of.\n    \"\"\"\n    result = {}\n    for entry in log:\n        key = entry['type']\n        if key not in result:\n            result[key] = 1\n        else:\n            result[key] += 1\n    return result\n",
        "rewrite": "```python\ndef create_summary(self, log):\n    return {entry['type']: i for i, entry['type'] in enumerate(dict.fromkeys([entry['type'] for entry in log]))}\n```"
    },
    {
        "original": "\nimport requests\n\nclass GitRepository:\n    def __init__(self):\n        self.url = \"https://api.bitbucket.org/2.0\"\n    \n    def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        auth_token = (\"your_username\", \"your_password\")\n        \n\t\theaders = {\n\t\t\t\"Content-Type\": \"application/json\"\n\t\t}\n\t\t\n\t\tdata = {\n\t\t\t\"name\": name,\n\t\t\t\"start\": {\n\t\t\t",
        "rewrite": "```python\nimport requests\n\nclass GitRepository:\n    def __init__(self, username, password):\n        self.url = \"https://api.bitbucket.org/2.0\"\n        self.auth_token = (username, password)\n\n    def create_branch(self, project_key, repository, name, start_point):\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n\n        data = {\n            \"name\": name,\n            \"startPoint\": {\n                \"type\": \"commit\",\n                \"hash\": start_point\n            }\n        }\n\n        response = requests.post(\n            f"
    },
    {
        "original": "\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(list(old_OPTS.keys()) + list(new_OPTS.keys()))\n    \n    for key in all_keys:\n        merged[key] = {}\n        \n        if key in old_OPTS:\n            merged[key].update(old_OPTS[key])\n        \n        if key in new_OPTS:\n            merged[key].update(newOpts[key])\n    \n    return merged\n\n# Test case\nold.opts = {'a':{'x':'old','",
        "rewrite": "```python\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(old_opts.keys()) | set(new_opts.keys())\n    \n    for key in all_keys:\n        merged[key] = {}\n        merged[key].update({k: v for d in (old_opts.get(key), new_opts.get(key)) if d for k, v in d.items()})\n    \n    return {k: dict(v) for k, v in merged.items() if v}\n\nold_opts = {'a':{'x':'old'}, 'b':{'y':'old2'}}\nnew_opts = {'a':{'x':'new'}, '"
    },
    {
        "original": "\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def get(self, file_path, ref, **kwargs):\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        \n        if 'sudo' in kwargs",
        "rewrite": "```python\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url: str, token: str):\n        self.url = url.rstrip('/')\n        self.token = token\n\n    def get(self, file_path: str, ref: str, **kwargs) -> requests.Response:\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        \n        if 'sudo' in kwargs:\n            params['sudo'] = kwargs['sudo']\n            del kwargs['"
    },
    {
        "original": "\ndef clean_recipe_build(self, args):\n    import os\n    import shutil\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    if os.path.exists(recipe_build_dir):\n        shutil.rmtree(recipe_build_dir)\n    print(f\"Deleted build files for recipe {args.recipe_name}\")\n",
        "rewrite": "```python\nimport os\nimport shutil\n\ndef clean_recipe_build(self, args):\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    try:\n        shutil.rmtree(recipe_build_dir)\n        print(f\"Deleted build files for recipe {args.recipe_name}\")\n    except FileNotFoundError:\n        pass\n```"
    },
    {
        "original": "\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    bias_shape = [input_shape[i] if i in bias_dims else 1 for i in range(len(input_shape))]\n    return tuple(bias_shape)\n",
        "rewrite": "```python\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    return tuple(input_shape[i] if i in bias_dims else 1 for i in range(len(input_shape)))\n```"
    },
    {
        "original": "\ndef read_metadata(text, ext):\n    if ext == 'txt':\n        lines = text.split('\\n')\n        metadata = {}\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                metadata[key.strip()] = value.strip()\n        return metadata\n    else:\n        return {}\n",
        "rewrite": "```python\ndef read_metadata(text, ext):\n    if ext.lower() == 'txt':\n        metadata = {}\n        for line in text.split('\\n'):\n            if ':' in line:\n                k, v = [x.strip() for x in line.split(':', 1)]\n                metadata[k] = v\n    return metadata or {}\n```"
    },
    {
        "original": "\ndef _is_process_filtered(self, process, key=None):\n    \"\"\"\n    Return True if the process[key] should be filtered according to the current filter\n    \"\"\"\n    # Assuming self.filter is set elsewhere in your class\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n\n    if key is None:\n        return any(str(d) in str(self.filter) for d in process.values())\n    else:\n        return str(process.get(key)) in str",
        "rewrite": "```python\ndef _is_process_filtered(self, process, key=None):\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n    \n    filter_str = str(self.filter)\n    \n    if key is None:\n        return any(str(value) in filter_str for value in process.values())\n    else:\n        return str(process.get(key)) in filter_str\n```"
    },
    {
        "original": "\ndef modulation_type(self, value: int):\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    if 0 <= value <= 3:\n        return modulation_types[value]\n    else:\n        return None\n",
        "rewrite": "```python\ndef modulation_type(self, value: int) -> str | None:\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    try:\n        return modulation_types[value]\n    except IndexError:\n        return None\n```"
    },
    {
        "original": "\ndef load(self, fname):\n    with open(fname, 'r') as f:\n        self.signatures = [line.strip() for line in f.readlines()]\n",
        "rewrite": "```python\ndef load(self, fname):\n    with open(fname, 'r', encoding='utf-8', errors='ignore') as f:\n        self.signatures = [line.strip() for line in f if line.strip()]\n```"
    },
    {
        "original": "\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid, room, namespace=None):\n        if namespace is None:\n            namespace = self.__class__.__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n",
        "rewrite": "```python\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid: str, room: str, namespace: str = None) -> None:\n        namespace = namespace or type(self).__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n```"
    },
    {
        "original": "\ndef _get_args(self, **):\n    return {key: value for key, value in kwargs.items() if hasattr(self.__class__, key)}\n",
        "rewrite": "```python\ndef _get_args(self, **kwargs):\n    return {key: kwargs[key] for key in dir(self.__class__) if hasattr(self.__class__, key) and key in kwargs}\n```"
    },
    {
        "original": "\nfrom pyogram import Client\nfrom pyogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self, caption: str, parse_mode: str = \"\", reply_markup: Union[InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id, message_id=self.message_id,c",
        "rewrite": "```python\nfrom pyrogram import Client\nfrom pyrogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self,\n                      caption: str,\n                      parse_mode: str = \"\",\n                      reply_markup: Union[InlineKeyboardMarkup,\n                                          ReplyKeyboardMarkup,\n                                          ReplyKeyboardRemove,\n                                          ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id,\n                                       message_id=self.message_id,\n                                       caption=caption,\n                                       parse_mode=parse_mode,\n                                       reply_markup=reply_markup)\n```"
    },
    {
        "original": "\nimport numpy as np\nfrom scipy.linalg import kron\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    # Get the size of sub-matrices\n    n = int(np.sqrt(mat4x4.shape[0]))\n\n    # Reshape mat4x4 into 3D array (n*n matrices)\n    mats = mat4x4.reshape(n, n, n, n)\n\n    # Flip kron order by swapping last two axes",
        "rewrite": "```python\nimport numpy as np\n\ndef flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    n = int(np.sqrt(mat4x4.shape[0]))\n    return mat4x4.reshape(n, n, n, n).transpose(0, 1, 3, 2).reshape(-1)\n```"
    },
    {
        "original": "\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\" Hyper-Band Parameters: \")\n    print(\"     - Try {} possible sets of hyperparameters\".format(len(hyperband_schedule)))\n    if describe_hyperband:\n        print(\"     - With an average of {} iterations per trial\".format(sum([len(trial) for trial in hyperband_schedule]) / len(hyperband_schedule)))\n    print(\"\")\n    \n    max_r = max(max(trial)",
        "rewrite": "```python\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\"Hyper-Band Parameters:\")\n    print(f\"     - Try {len(hyperband_schedule)} possible sets of hyperparameters\")\n    if describe_hyperband:\n        avg_iterations_per_trial = sum(len(trial) for trial in hyperband_schedule) / len(hyperband_schedule)\n        print(f\"     - With an average of {avg_iterations_per_trial:.2f} iterations per trial\")\n        \n    max_r = max(len(trial) for trial in hyperband_schedule)\n```"
    },
    {
        "original": "\nclass Matrix:\n    def __init__(self, a, b, c, d, e, f):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        self.e = e\n        self.f = f\n\n    def shorthand(self):\n        return (self.a, self.b, self.c, self.d, self.e, self.f)\n",
        "rewrite": "```python\nfrom typing import Any\n\nclass Matrix:\n    def __init__(self, *args: Any):\n        self.elements = list(args)\n\n    def to_list(self) -> list[Any]:\n        return self.elements\n```"
    },
    {
        "original": "\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited:\n            if dfs",
        "rewrite": "```python\nfrom typing import Dict\n\ndef is_cyclic(graph: Dict[int, list]) -> bool:\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node: int) -> bool:\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited and dfs(node):\n            return True\n    return False\n```"
    },
    {
        "original": "\nfrom email import policy\nfrom email.parser import BytesParser\n\ndef get_header_items(self):\n    \"\"\"Get an iterable list of key/value pairs representing headers.\"\"\"\n    parser = BytesParser(policy=policy.default)\n    msg = parser.parsestr(self.request_text)\n    return [(k, v) for k, v in msg.items()]\n",
        "rewrite": "```python\nfrom email import policy, message_from_string\n\nclass EmailParser:\n    def __init__(self, request_text):\n        self.request_text = request_text\n\n    def get_header_items(self):\n        msg = message_from_string(self.request_text, policy=policy.default)\n        return [(k, v) for k, v in msg.items()]\n```"
    },
    {
        "original": "\nclass Unit:\n    def __init__(self, is_scv, is_building):\n        self.is_scv = is_scv\n        self.is_building = is_building\n\n    def is_constructing_scv(self) -> bool:\n        return self.is_scv and self.is_building\n",
        "rewrite": "```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Unit:\n    \"\"\"Represents a unit in the game.\"\"\"\n    is_scv: bool\n    is_building: bool\n\n    @property\n    def constructing_scv(self) -> bool:\n        return self.is_scv and self.is_building\n```"
    },
    {
        "original": "\ndef set_form_field_order(form, field_order):\n    if field_order is None:\n        return\n    new_fields = []\n    for field_name in field_order:\n        if hasattr(form, field_name):\n            new_fields.append((field_name, getattr(form, field_name)))\n    for field_name, field in form.fields.items():\n        if field_name not in field_order:\n            new_fields.append((field_name, field))\n    form.fields = OrderedDict(new_fields)\n",
        "rewrite": "```python\nfrom collections import OrderedDict\n\ndef set_form_field_order(form, field_order):\n    if field_order is None:\n        return\n    known_fields = [field_name for field_name in field_order if hasattr(form, field_name)]\n    new_fields = [(field_name, getattr(form, field_name)) for field_name in known_fields]\n    remaining_fields = {field_name: field for field_name, field in form.fields.items() if field_name not in known_fields}\n    new_fields.extend(remaining_fields.items())\n    form.fields = OrderedDict(new_fields)\n```"
    },
    {
        "original": "\nimport whatsapp\n\ndef init_client(client_id):\n    \"\"\"Initialse a driver for client and store for future reference\n    \n    @param client_id: ID of client user\n    @return whatsappapi object\n    \"\"\"\n    whatsapp_api = whatsapp.Client()\n    whatsapp_api.authenticate(client_id)\n    return whatsapp_api\n",
        "rewrite": "```python\nimport whatsapp\n\ndef initialize_whatsapp_client(client_id: str) -> whatsapp.Client:\n    client = whatsapp.Client()\n    try:\n        client.authenticate(client_id)\n        return client\n    except Exception as e:\n        raise ValueError(\"Failed to authenticate Whatsapp client\") from e\n```"
    },
    {
        "original": "\nclass Cell:\n    def __init__(self, row_idx, col_idx):\n        self.row_idx = row_idx\n        self.col_idx = col_idx\n\nclass Table:\n    def __init__(self, rows, cols):\n        self.rows = rows\n        self.cols = cols\n        self.cells = [[Cell(i, j) for j in range(cols)] for i in range(rows)]\n\n    def cell(self, row_idx, col_idx):\n        return self.cells[row_idx][",
        "rewrite": "```python\nclass Cell:\n    def __init__(self, row: int, col: int):\n        self.row_idx = row\n        self.col_idx = col\n\nclass Table:\n    def __init__(self, rows: int, cols: int):\n        if not isinstance(rows, int) or not isinstance(cols, int):\n            raise TypeError(\"Rows and columns must be integers.\")\n        if rows <= 0 or cols <= 0:\n            raise ValueError(\"Rows and columns must be positive.\")\n\n        self.rows = rows\n        self.cols = cols\n        self.cells = [[Cell(i, j) for j in range"
    },
    {
        "original": "\nclass CardSystem:\n    def __init__(self):\n        self.cards = {}\n\n    def get(self, card_id):\n        return self.cards.get(card_id)\n",
        "rewrite": "```python\nfrom typing import Dict\n\nclass CardSystem:\n    def __init__(self):\n        self.cards: Dict[str, dict] = {}\n\n    def get_card(self, card_id: str) -> dict | None:\n        return self.cards.get(card_id)\n```"
    },
    {
        "original": "\nclass API:\n    def __init__(self):\n        self.not_found_handlers = {}\n\n    def set_not_found_handler(self, handler, version=None):\n        if version is None:\n            self.not_found_handlers['default'] = handler\n        else:\n            self.not_found_handlers[version] = handler\n",
        "rewrite": "```python\nclass API:\n    def __init__(self):\n        self.not_found_handlers = {}\n\n    def set_not_found_handler(self, handler, version=None):\n        self.not_found_handlers.setdefault('default', None) \n        self.not_found_handlers[version or 'default'] = handler\n```"
    },
    {
        "original": "\nfrom typing import Callable, List\nfrom cirq import GateOperation, Qid\n\ndef measure_each(*qubits: Qid, key_func: Callable[[Qid], str] = str) -> List[GateOperation]:\n    return [GateOperation('measure', qubit) for qubit in qubits]\n",
        "rewrite": "```python\nfrom typing import Callable, List\nfrom cirq import GateOperation, Qid, MeasurementGate\n\ndef measure_each(\n    *qubits: Qid,\n    key_func: Callable[[Qid], str] = str\n) -> List[GateOperation]:\n    return [\n        GateOperation(MeasurementGate(key_func(qubit)), [qubit])\n        for qubit in qubits\n    ]\n```"
    },
    {
        "original": "\nclass RESTObjectList:\n    def __init__(self, items):\n        self.items = items\n\nclass GitlabAuthenticationError(Exception):\n    pass\n\nclass GitlabListError(Exception):\n    pass\n\nclass Participants:\n    def __init__(self, server):\n        self.server = server\n\n    def participants(self, **kwargs):\n        all_items = self.server.get_all_items()\n        if 'all' in kwargs and kwargs['all']:\n            return RESTObjectList(all_items)\n        elif",
        "rewrite": "```python\nfrom abc import ABC, abstractmethod\n\nclass RESTObjectList:\n    def __init__(self, items):\n        self.items = items\n\n    def __getitem__(self, index):\n        return self.items[index]\n\n    def __len__(self):\n        return len(self.items)\n\nclass GitlabException(Exception):\n    pass\n\nclass GitlabAuthenticationError(GitlabException):\n    pass\n\nclass GitlabListError(GitlabException):\n    pass\n\nclass Server(ABC):\n    @abstractmethod\n    def get_all_items(self, **kwargs) -> list:\n        raise NotImplementedError\n\n\nclass Participants:\n    def __init__("
    },
    {
        "original": "\nimport re\nimport portage\n\ndef revdep_rebuild(lib=None):\n    \"\"\"\n    Fix up broken reverse dependencies\n\n    lib\n        Search for reverse dependencies for a particular library rather\n        than every library on the system. It can be a full path to a\n        library or basic regular expression.\n    \"\"\"\n    if lib:\n        # If lib is provided, search for reverse dependencies for that particular library\n        revdeps = portage.dep.get_revdeps(lib, mydb",
        "rewrite": "```python\nimport re\nimport portage\n\ndef revdep_rebuild(lib=None):\n    if lib and not re.match(r'^=.+$', lib):\n        lib = f'={lib}'\n    return portage.dep.get_revdeps(lib, portage.db[portage.root][\"vartree\"].dbapi)\n```"
    },
    {
        "original": "\ndef _canonicalize_name(prefix, qvm_type, noisy):\n    if noisy:\n        return f\"{prefix}_{qvm_type}_noisy\"\n    else:\n        return f\"{prefix}_{qvm_type}\"\n",
        "rewrite": "```python\ndef canonicalize_name(prefix: str, qvm_type: str, noisy: bool) -> str:\n    return f\"{prefix}_{qvm_type}{'_noisy' if noisy else ''}\"\n```"
    },
    {
        "original": "\ndef _line(self, text, indent=0):\n    width = 80  # assuming the width is 80 characters\n    words = text.split()\n    line = ' ' * indent\n    for word in words:\n        if len(line) + len(word) + 1 > width:\n            yield line\n            line = ' ' * indent + word\n        else:\n            line += ' ' + word\n    yield line\n",
        "rewrite": "```python\ndef _line(self, text: str, indent: int = 0) -> str:\n    width = 80\n    words = text.split()\n    line = ' ' * indent\n\n    for word in words:\n        if len(line) + len(word) + 1 > width:\n            yield line.lstrip()\n            line = ' ' * indent + word\n        else:\n            line += f' {word}'\n\n    yield line.lstrip()\n```"
    },
    {
        "original": "\nfrom ibm_watson import DetailedResponse\n\nclass FeedbackService:\n    def get_feedback(self, feedback_id, model=None, **kwargs):\n        # Implement the logic to list a specified feedback entry\n        # For demonstration purposes, assume the feedback entry is stored in a dictionary\n        feedback_entries = {\n            \"feedback1\": {\"id\": \"feedback1\", \"content\": \"This is feedback 1\"},\n            \"feedback2\": {\"id\": \"feedback2\", \"content",
        "rewrite": "```python\nfrom ibm_watson import DetailedResponse\n\nclass FeedbackService:\n    def __init__(self):\n        self.feedback_entries = {\n            \"feedback1\": {\"id\": \"feedback1\", \"content\": \"This is feedback 1\"},\n            \"feedback2\": {\"id\": \"feedback2\", \"content\": \"This is feedback 2\"}\n        }\n\n    def get_feedback(self, feedback_id, model=None, **kwargs) -> DetailedResponse | None:\n        return self._get_feedback_entry(feedback_id)\n\n    def _get_feedback_entry(self, feedback_id: str) -> DetailedResponse | None:\n       "
    },
    {
        "original": "\ndef probably_identical(self, f, g):\n    \"\"\"\n    :param f: function 1\n    :param g: function 2\n    :returns: Whether or not these two functions are identical.\n    \"\"\"\n    import random\n    for _ in range(100):  # arbitrary number of tests\n        x = random.random()  # generate a random input\n        if f(x) != g(x):\n            return False\n    return True\n",
        "rewrite": "```python\ndef probably_identical(self, f, g, num_tests=100):\n    import random\n    tolerance = 1e-9\n    for _ in range(num_tests):\n        x = random.uniform(-1000, 1000)\n        if abs(f(x) - g(x)) > tolerance:\n            return False\n    return True\n```"
    },
    {
        "original": "\ndef logpdf_link(self, inv_link_f, y, Y_metadata=None):\n    return np.sum(y * np.log(inv_link_f) + (1 - y) * np.log(1 - inv_link_f))\n",
        "rewrite": "```python\ndef logpdf_link(self, inv_link_f, y, Y_metadata=None):\n    return np.sum(y * np.log(inv_link_f) + (1 - y) * np.log(1 - inv_link_f))\n```\nOr, considering log1p as in the original code would be more numerically stable:\n\n```python\ndef logpdf_link(self, inv_link_f, y, Y_metadata=None):\n    return np.sum(y * np.log(inv_link_f) + (1 - y) * np.log1p(-inv_lin\u0131k_f))\n```\nBut most likely errorfree corrected line should be:\n \n"
    },
    {
        "original": "\nimport yaml\nimport os\n\ndef __load_compose_definitions(path, definition):\n    compose_result = None\n    loaded_definition = None\n    err = None\n\n    if os.path.exists(path):\n        try:\n            with open(path, 'r') as file:\n                compose_result = yaml.safe_load(file)\n            loaded_definition = definition\n        except Exception as e:\n            err = str(e)\n    else:\n        err = \"File not found\"\n\n    return compose_result, loaded",
        "rewrite": "```python\nimport yaml\nimport os\n\ndef load_compose_definitions(path, definition):\n    if not os.path.isfile(path):\n        return None, \"File not found\"\n    \n    try:\n        with open(path, 'r') as file:\n            return yaml.safe_load(file), definition\n    except yaml.YAMLError as e:\n        return None, str(e)\n    except Exception as e:\n        return None, f\"An error occurred: {str(e)}\"\n```"
    },
    {
        "original": "\nfrom datetime import datetime\n\ndef date_or_None(obj):\n    try:\n        return datetime.strptime(obj, '%Y-%m-%d').date()\n    except ValueError:\n        return None\n",
        "rewrite": "```python\nfrom datetime import datetime\n\ndef parse_date(obj: str) -> datetime.date:\n    try:\n        return datetime.strptime(obj, '%Y-%m-%d').date()\n    except ValueError:\n        return None\n```"
    },
    {
        "original": "\nimport json\nfrom typing import Optional\n\nasync def build_get_cred_def_request(submitter_did: Optional[str], id_: str) -> str:\n    request = {\n        \"operation\": {\n            \"type\": \"105\",\n            \"dest\": id_\n        }\n    }\n    if submitter_did:\n        request[\"operation\"][\"sender_did\"] = submitter_did\n    return json.dumps(request)\n",
        "rewrite": "```\nimport json\nfrom typing import Optional\n\nasync def build_get_cred_def_request(submitter_did: Optional[str], cred_def_id: str) -> str:\n    request = {\n        \"operation\": {\n            \"type\": \"105\",\n            \"dest\": cred_def_id,\n            **({\"sender_did\": submitter_did} if submitter_did else {})\n        }\n    }\n    return json.dumps(request, separators=(',', ':'))\n```"
    },
    {
        "original": "\ndef _get_restartcheck_result(errors):\n    # TO DO: implement the logic to get the restartcheck result\n    # For now, let's assume the result is always True\n    result = True\n    # Append some dummy error to the errors list\n    errors.append(\"Dummy error\")\n    return result\n",
        "rewrite": "```python\ndef _get_restartcheck_result(errors):\n    try:\n        result = some_condition()\n    except Exception as e:\n        errors.append(str(e))\n        return False\n    else:\n        errors.append(\"No errors found\")\n    return result\n```"
    },
    {
        "original": "\ndef check_enable_mode(self, check_string=\"\"):\n    return \"enable\" in check_string.lower()\n",
        "rewrite": "def check_enable_mode(self, check_string: str = \"\") -> bool:\n    return \"enable\" in check_string.lower()"
    },
    {
        "original": "\ndef _check_rot_sym(self, axis):\n    # TO DO: implement the logic to determine the rotational symmetry about the supplied axis\n    pass\n",
        "rewrite": "```python\nimport math\n\ndef _check_rot_sym(self, axis):\n    vertices = self.get_vertices()\n    edges = self.get_edges()\n    faces = self.get_faces()\n\n    def rotate_point(point, axis, angle):\n        a, b, c = point\n        u, v, w = axis\n        x, y, z = a - u, b - v, c - w\n        r = math.sqrt(x**2 + y**2 + z**2)\n        theta = math.atan2(y, x)\n        phi = math.acos(z/r) if r != 0 else 0\n\n"
    },
    {
        "original": "\ndef _get_values(self, lst, list_columns):\n    return [{col: getattr(item, col) for col in list_columns} for item in lst]\n",
        "rewrite": "```python\ndef _get_values(self, lst, list_columns):\n    return [dict((col, item.__getattribute__(col)) for col in list_columns) for item in lst]\n```"
    },
    {
        "original": "\nimport requests\n\ndef post_message(message, chat_id=None, token=None):\n    if not chat_id or not token:\n        raise ValueError(\"Both chat_id and token are required\")\n    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n    data = {\"chat_id\": chat_id, \"text\": message}\n    response = requests.post(url, json=data)\n    return response.status_code == 200\n",
        "rewrite": "```python\nimport requests\nfrom typing import Optional\n\ndef post_message(message: str, chat_id: Optional[str] = None, token: Optional[str] = None) -> bool:\n    if not all([chat_id, token]):\n        raise ValueError(\"Both chat_id and token are required\")\n    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n    data = {\"chat_id\": chat_id, \"text\": message}\n    try:\n        response = requests.post(url, json=data)\n        return response.ok\n    except requests.exceptions.RequestException as e:\n        raise Exception(f\"Failed to send message:"
    },
    {
        "original": "\nclass Report:\n    def __init__(self, text):\n        self.text = text\n\n    def topics(self):\n        \"\"\"\n        Get the set of topics that can be extracted from this report.\n        \"\"\"\n        topics = set()\n        for word in self.text.split():\n            if word.startswith('#'):\n                topics.add(word[1:])\n        return topics\n",
        "rewrite": "```python\nimport re\n\nclass Report:\n    def __init__(self, text):\n        self._text = text\n\n    @property\n    def topics(self):\n        return set(re.findall(r'#(\\w+)', self._text))\n```"
    },
    {
        "original": "\nimport os\n\ndef get_data_home(path=None):\n    if path is None:\n        path = os.environ.get('YELLOWBRICK_DATA')\n    if path is None:\n        path = os.path.join(os.path.dirname(__file__), 'data')\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n    return os.path.abspath(path)\n",
        "rewrite": "```python\nimport os\n\ndef get_data_home(path=None):\n    path = path or os.getenv('YELLOWBRICK_DATA', \n                            os.path.join(os.path.dirname(__file__), 'data'))\n    return os.path.abspath(os.path.expanduser(path))\n```"
    },
    {
        "original": "\ndef get_monolayer(self, molecular_weight, langmuir_surface_area, langmuir_cross_sectional_area):\n    return (molecular_weight / langmuir_surface_area) * langmuir_cross_sectional_area\n",
        "rewrite": "```python\ndef get_monolayer(\n    self, \n    molecular_weight: float, \n    langmuir_surface_area: float, \n    langmuir_cross_sectional_area: float\n) -> float:\n    if langmuir_surface_area <= 0 or langmuir_cross_sectional_area <= 0:\n        raise ValueError(\"Surface areas must be positive\")\n\n    return (molecular_weight / langmuir_surface_area) * langmuir_cross_sectional_area\n```"
    },
    {
        "original": "\nimport shapefile\nimport re\n\ndef string_match(sf, regex, field=2):\n    matched_shapes = []\n    for shape in sf.shapeRecords():\n        if re.search(regex, shape.record[field-1]):\n            matched_shapes.append((shape.shape, shape.record))\n    return matched_shapes\n",
        "rewrite": "```python\nimport shapefile\nimport re\n\ndef string_match(sf, regex, field=2):\n    return [\n        (shape.shape, shape.record) \n        for shape in sf.shapeRecords() \n        if re.search(regex, str(shape.record[field-1]))\n    ]\n```"
    },
    {
        "original": "\nclass EventService:\n    def __init__(self):\n        self.subscribers = {}\n\n    def add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n        if not all(self._is_valid_filter(filter) for filter in subscriptions):\n            raise InvalidFilterError(\"One of the filters in the subscriptions is invalid.\")\n        self.subscribers[connection_id] = {\"subscriptions\": subscriptions, \"last_known_block_id\": last_known_block_id}\n\n    def _is_valid_filter(self",
        "rewrite": "```python\nfrom abc import ABC, abstractmethod\n\nclass EventService:\n    def __init__(self):\n        self.subscribers = {}\n\n    def add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n        if not all(self._validate_filter(filter) for filter in subscriptions):\n            raise InvalidFilterError(\"One of the filters in the subscriptions is invalid.\")\n        self.subscribers[connection_id] = {\"subscriptions\": subscriptions, \"last_known_block_id\": last_known_block_id}\n\n    def _validate_filter(self, filter):\n        try:\n            return FilterValidator.validate(filter)\n        except Exception as e:\n            raise"
    },
    {
        "original": "\ndef get_summed_cohp_by_label_and_orbital_list(self, label_list, orbital_list, divisor=1):\n    summed_cohp = 0\n    for label, orbital in zip(label_list, orbital_list):\n        cohp = self.get_cohp_by_label_and_orbital(label, orbital)\n        summed_cohp += cohp\n    summed_cohp /= divisor\n    return COHP(summed_cohp)\n",
        "rewrite": "```python\ndef get_summed_cohp_by_label_and_orbital_list(self, labels, orbitals, divisor=1):\n    return COHP(sum(c.get_cohp_by_label_and_orbital(label, orbital) for c in self.data for label, orbital in zip(labels, orbitals)) / divisor)\n```\nI changed the variable names and also I assumed that there was a mistake because 'c' variable is referenced but not defined anywhere in this function."
    },
    {
        "original": "\nimport qrcode\nfrom PIL import Image\n\nclass Client:\n    def get_qr(self, filename=None):\n        qr = qrcode.QRCode(\n            version=1,\n            error_correction=qrcode.constants.ERROR_CORRECT_L,\n            box_size=10,\n            border=4,\n        )\n        qr.add_data(\"https://example.com\")  # Replace with your data\n        qr.make(fit=True)\n\n        img = qr.make_image(fill_color=\"black\", back_color=\"",
        "rewrite": "```python\nimport qrcode\nfrom PIL import Image\n\nclass QRCodeGenerator:\n    def __init__(self, data: str):\n        self.data = data\n\n    def generate_qr(self, filename: str = None) -> None:\n        qr = qrcode.QRCode(\n            version=1,\n            error_correction=qrcode.constants.ERROR_CORRECT_L,\n            box_size=10,\n            border=4,\n        )\n        qr.add_data(self.data)\n        qr.make(fit=True)\n\n        img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n\n        if filename:\n            img.save(filename)\n           "
    },
    {
        "original": "\nimport requests\n\ndef edit_label(owner, repo, name, color, description=github.GithubObject.NotSet):\n    url = f\"https://api.github.com/repos/{owner}/{repo}/labels/{name}\"\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    data = {\"name\": name, \"color\": color, \"description\": description}\n    response = requests.patch(url, headers=headers, json=data)\n    response.raise_for_status()\n",
        "rewrite": "```python\nimport requests\n\ndef edit_label(owner: str, repo: str, name: str, color: str, description: str = \"\"):\n    url = f\"https://api.github.com/repos/{owner}/{repo}/labels/{name}\"\n    headers = {\"Accept\": \"application/vnd.github.v3+json\", \"User-Agent\": \"My App\"}\n    data = {\"name\": name, \"color\": color, \"description\": description}\n    try:\n        response = requests.patch(url, headers=headers, json=data)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n       "
    },
    {
        "original": "\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef switch_to_frame(driver, frame, timeout=settings.SMALL_TIMEOUT):\n    iframe = WebDriverWait(driver, timeout).until(\n        EC.frame_to_be_available_and_switch_to_it((By.XPATH, f\"//iframe[@name='{frame}']\"))\n    )\n    return iframe\n",
        "rewrite": "```python\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef switch_to_frame(driver, frame_name, timeout=10):\n    return WebDriverWait(driver, timeout).until(\n        EC.frame_to_be_available_and_switch_to_it((By.XPATH, f\"//iframe[@name='{frame_name}']\"))\n    )\n```"
    },
    {
        "original": "\ndef process_eni_metrics(\n    stream_eni, myips, stream,\n    start, end, period, sample_size,\n    resolver, sink_uri):\n    pass  # Please provide the problem description, input, and output specifications.\n",
        "rewrite": "```python\nimport boto3\nfrom datetime import datetime, timedelta\n\ndef process_eni_metrics(\n    stream_eni, \n    my_ips, \n    stream, \n    start_time, \n    end_time, \n    period_in_seconds,\n    sample_size,\n    dns_resolver,\n    output_uri):\n    \n    cloudwatch = boto3.client('cloudwatch')\n    \n    eni_metrics = cloudwatch.get_metric_statistics(\n        Namespace='AWS/NetworkInterfaces',\n        MetricName='BytesIn',\n        Dimensions=[{'Name': 'NetworkInterfaceId', 'Value': stream_eni}],\n        StartTime=start_time,\n        EndTime=end"
    },
    {
        "original": "\ndef read_existing_paths(bt_table):\n    existing_paths = []\n    for row in bt_table:\n        if row[1] is not None:\n            existing_paths.append(row[0])\n    return existing_paths\n",
        "rewrite": "```python\ndef read_existing_paths(bt_table):\n    return [row[0] for row in bt_table if row[1] is not None and isinstance(row, (list, tuple)) and len(row) > 1]\n```"
    },
    {
        "original": "\nimport pandas as pd\n\nclass SearchEngine:\n    def __init__(self, data):\n        self.data = data\n\n    def search(self, ngram):\n        result = self.data[self.data['texts'].str.contains(ngram)]\n        return result[['texts', 'categories']]\n",
        "rewrite": "```python\nimport pandas as pd\n\nclass SearchEngine:\n    def __init__(self, data: pd.DataFrame):\n        if not isinstance(data, pd.DataFrame):\n            raise TypeError(\"Data must be a pandas DataFrame\")\n        \n        required_columns = ['texts', 'categories']\n        if not all(column in data.columns for column in required_columns):\n            raise ValueError(\"DataFrame must contain 'texts' and 'categories' columns\")\n        \n        self.data = data\n\n    def search(self, ngram: str) -> pd.DataFrame:\n        return (self.data\n                .assign(matches=self.data['texts'].str.contains(ngram))\n                ."
    },
    {
        "original": "\nclass Register:\n    def __init__(self):\n        self.namespaces = {}\n\n    def delete(self, name):\n        if name in self.namespaces:\n            del self.namespaces[name]\n",
        "rewrite": "```python\nclass Register:\n    def __init__(self):\n        self.namespaces: dict[str, any] = {}\n\n    def delete(self, name: str) -> None:\n        del self.namespaces[name]\n```"
    },
    {
        "original": "\ndef getfield(self, pkt, s):\n    if pkt.msglen == 0:\n        if pkt.version == 'TLS 1.3':\n            return s\n        else:\n            return ''\n    else:\n        return s[:pkt.msglen]\n",
        "rewrite": "```python\ndef getfield(self, pkt, s):\n    return s if not pkt.msglen and pkt.version == 'TLS 1.3' else s[:pkt.msglen]\n```"
    },
    {
        "original": "\nregistry = {}\n\ndef archive(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the Archive class. Registers the decorated class\n    as the Archive known type.\n    \"\"\"\n    registry[class_obj.__name__] = class_obj\n    return class_obj\n",
        "rewrite": "```python\nregistry = {}\n\ndef archive(cls: type) -> type:\n    registry[cls.__name__] = cls\n    return cls\n```"
    },
    {
        "original": "\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import scan\n\ndef form_query(query_type, query):\n    es = Elasticsearch()\n    if query_type == \"multi_match\":\n        body = {\n            \"query\": {\n                \"multi_match\": {\n                    \"query\": query,\n                    \"fields\": [\"title\", \"description\"]\n                }\n            }\n        }\n    else:\n        raise ValueError(\"Invalid query type\")\n    return body\n",
        "rewrite": "```python\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import scan\n\nclass ElasticsearchClient:\n    def __init__(self):\n        self.es = Elasticsearch()\n        self.query_types = {\n            \"multi_match\": {\n                \"query\": {\n                    \"multi_match\": {\n                        \"query\": \"\",\n                        \"fields\": [\"title\", \"description\"]\n                    }\n                }\n            }\n        }\n\n    def form_query(self, query_type, query):\n        if query_type not in self.query_types:\n            return {\"error\": \"Invalid query type\"}\n        \n        self.query_types[query_type][\"query\"][\"multi_match\"][\"query\"] = query"
    },
    {
        "original": "\nclass ControlDependenceGraph:\n    def __init__(self, graph):\n        self.graph = graph\n\n    def get_dependants(self, run):\n        dependants = []\n        for node in self.graph:\n            if run in self.graph[node]:\n                dependants.append(node)\n        return dependants\n",
        "rewrite": "```python\nclass ControlDependenceGraph:\n    def __init__(self, graph: dict):\n        self._graph = graph\n\n    def get_dependants(self, run_id: str) -> list:\n        return [node for node in self._graph if run_id in self._graph[node]]\n```"
    },
    {
        "original": "\ndef _ExtractClientIdFromPath(entry, event):\n    import re\n    pattern = r\"/clients/(\\w+)/\"\n    match = re.search(pattern, entry.request.path)\n    if match:\n        return match.group(1)\n    else:\n        return None\n",
        "rewrite": "```python\nimport re\n\ndef extract_client_id_from_path(entry, event):\n    pattern = r\"/clients/([^/]+)/?\"\n    match = re.search(pattern, entry.request.path)\n    return match.group(1) if match else None\n```"
    },
    {
        "original": "\nclass OverrideElement:\n    def __init__(self):\n        self.override_elements = []\n\n    def add_override(self, partname, content_type):\n        self.override_elements.append({\"partname\": partname, \"content_type\": content_type})\n\n    def __str__(self):\n        result = \"\"\n        for override in self.override_elements:\n            result += f\"<Override partname='{override['partname']}' content_type='{override['content_type']}'/>\\n\"\n        return result\n",
        "rewrite": "```python\nclass OverrideElement:\n    def __init__(self) -> None:\n        self.override_elements: list[dict] = []\n\n    def add_override(self, partname: str, content_type: str) -> None:\n        self.override_elements.append({\"partname\": partname, \"content_type\": content_type})\n\n    def __str__(self) -> str:\n        return \"\\n\".join(f\"<Override partname='{override['partname']}' content_type='{override['content_type']}'/>\" for override in self.override_elements)\n```"
    },
    {
        "original": "\ndef _post_master_init(self, master):\n    pass\n",
        "rewrite": "```python\nfrom abc import ABC, abstractmethod\n\nclass MasterInitializer(ABC):\n    @abstractmethod\n    def post_master_init(self, master):\n        pass\n```"
    },
    {
        "original": "\ndef Kdiag(self, X, target):\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = np.exp(-0.5 * (X[i] - X[j])**2 / target)\n    return np.diag(K)\n",
        "rewrite": "```python\ndef Kdiag(self, X, target):\n    dy = X[:, None] - X[None, :]\n    return np.exp(-0.5 * (dy * dy) / target).diagonal()\n```"
    },
    {
        "original": "\ndef make_deprecated_class(oldname, NewClass):\n    class DeprecatedClass(NewClass):\n        def __init__(self, *args, **kwargs):\n            raise NotImplementedError(f\"'{oldname}' is deprecated. Use '{NewClass.__name__}' instead.\")\n    return DeprecatedClass\n",
        "rewrite": "```python\nimport warnings\n\ndef make_deprecated_class(oldname, NewClass):\n    class DeprecatedClass(NewClass):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            message = f\"'{oldname}' is deprecated. Use '{NewClass.__name__}' instead.\"\n            warnings.warn(message, DeprecationWarning)\n\n    return DeprecatedClass\n```"
    },
    {
        "original": "\ndef estimate_row_means(X, observed, column_means, column_scales):\n    row_means = []\n    for i in range(X.shape[0]):\n        numerator = 0\n        denominator = 0\n        for j in range(X.shape[1]):\n            if observed[i, j]:\n                numerator += (X[i, j] - column_means[j]) / column_scales[j]\n                denominator += 1 / column_scales[j]\n        row_means.append(numerator / denominator",
        "rewrite": "```python\nimport numpy as np\n\ndef estimate_row_means(X, observed, column_means, column_scales):\n    numerator = np.sum(((X - column_means) / column_scales) * observed[:, None], axis=1)\n    denominator = (observed.sum(axis=1) / column_scales)\n    return numerator / denominator\n```"
    },
    {
        "original": "\ndef ReadHuntOutputPluginLogEntries(self, hunt_id, output_plugin_id, offset, count, with_type=None, cursor=None):\n    # TO DO: implement the logic to read hunt output plugin log entries\n    pass\n",
        "rewrite": "```python\ndef ReadHuntOutputPluginLogEntries(self, hunt_id, output_plugin_id, offset, count, with_type=None, cursor=None):\n    query = \"\"\"\n        SELECT * FROM hunt_output_plugin_log_entries\n        WHERE hunt_id = %s AND output_plugin_id = %s\n    \"\"\"\n    params = (hunt_id, output_plugin_id)\n\n    if with_type:\n        query += \" AND type = %s\"\n        params += (with_type,)\n    \n    if cursor:\n        query += \" . \"\n    \n    query += \" OFFSET %s LIMIT %s\"\n    params += (offset, count)\n```\n"
    },
    {
        "original": "\nfrom scipy.sparse import lil_matrix\n\nclass CondensationDigraph:\n    def _condensation_lil(self):\n        # Assuming you have a method to get the condensation digraph\n        condensation_digraph = self.get_condensation_digraph()\n        \n        # Create a lil matrix with the same shape as the condensation digraph\n        lil_matrix_representation = lil_matrix(condensation_digraph.shape)\n        \n        # Populate the lil matrix with the edges of the condensation digraph\n",
        "rewrite": "```python\nfrom scipy.sparse import lil_matrix\n\nclass CondensationDigraph:\n    def _condensation_lil(self):\n        condensation_digraph = self.get_condensation_digraph()\n        lil_matrix_representation = lil_matrix(condensation_digraph.shape)\n        \n        for i, j in zip(*condensation_digraph.nonzero()):\n            lil_matrix_representation[i, j] = 1\n```"
    },
    {
        "original": "\ndef build_modules(is_training, vocab_size):\n    if is_training:\n        # training mode\n        embedding_module = tf.keras.layers.Embedding(vocab_size, 128)\n        rnn_module = tf.keras.layers.LSTM(128)\n        output_module = tf.keras.layers.Dense(vocab_size)\n    else:\n        # inference mode\n        embedding_module = tf.keras.layers.Embedding(vocab_size, 128)\n        rnn_module = tf.keras.layers.LSTM(128, return_sequences=True",
        "rewrite": "```python\nimport tensorflow as tf\n\ndef build_modules(is_training: bool, vocab_size: int) -> tuple:\n    embedding_module = tf.keras.layers.Embedding(vocab_size, 128)\n    rnn_module = tf.keras.layers.LSTM(128, return_sequences=True if is_training else False)\n    output_module = tf.keras.layers.Dense(vocab_size)\n\n    return embedding_module, rnn_module, output_module\n```"
    },
    {
        "original": "\nimport requests\n\nclass GitHub:\n    def __init__(self, owner, repo, ref):\n        self.owner = owner\n        self.repo = repo\n        self.ref = ref\n\n    def get_statuses(self):\n        url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/statuses/{self.ref}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json()\n",
        "rewrite": "```python\nimport requests\nfrom urllib.parse import quote\n\nclass GitHub:\n    def __init__(self, owner: str, repo: str, ref: str):\n        self._owner = quote(owner)\n        self._repo = quote(repo)\n        self._ref = quote(ref)\n\n    def get_statuses(self) -> dict:\n        url = f\"https://api.github.com/repos/{self._owner}/{self._repo}/statuses/{self._ref}\"\n        response = requests.get(url, headers={'Accept': 'application/vnd.github.v3+json'})\n        response.raise_for_status()\n        return response.json()\n\nclass GitHubClient:\n   "
    },
    {
        "original": "\nimport salt.client\n\ndef get_vm_ip(name=None, session=None, call=None):\n    local = salt.client.LocalClient()\n    if name:\n        vm_ip = local.cmd(name, 'xen_guest_tools.get_ip')\n        return vm_ip[name]\n    else:\n        return \"VM name is required\"\n",
        "rewrite": "```python\nimport salt.client\n\ndef get_vm_ip(name, session=None, call=None):\n    if not name:\n        return \"VM name is required\"\n\n    local = salt.client.LocalClient()\n    vm_ip = local.cmd(name, 'xen_guest_tools.get_ip')\n    \n    return vm_ip.get(name)\n```"
    },
    {
        "original": "\nimport xarray as xr\n\ndef as_dataset(obj):\n    if isinstance(obj, xr.Dataset):\n        return obj\n    elif isinstance(obj, xr.DataArray):\n        return obj.to_dataset(dim='variable')\n    elif isinstance(obj, dict):\n        return xr.Dataset(obj)\n    else:\n        raise ValueError(\"Object cannot be converted to a Dataset\")\n",
        "rewrite": "```python\nimport xarray as xr\n\ndef as_dataset(obj):\n    if isinstance(obj, xr.Dataset):\n        return obj\n    elif isinstance(obj, xr.DataArray):\n        return obj.to_dataset(dim='variable')\n    elif isinstance(obj, (dict, object)):\n        return xr.Dataset({k: as_dataset(v) for k, v in vars(obj).items()}) if hasattr(obj, '__dict__') else \n               {k: as_dataset(xr.DataArray(v)) for k,v in obj.items()} if all(isinstance(v,(xr.DataArray,xr.Dataset))) else \n               raise ValueError('Invalid keys or values must"
    },
    {
        "original": "\ndef str2float(text):\n    text = text.replace(\"(\", \"\").replace(\")\", \"\")\n    return float(text)\n",
        "rewrite": "```python\ndef str2float(text):\n    allowed_chars = set('0123456789.+-')\n    return float(''.join(c for c in text if c in allowed_chars))\n```"
    },
    {
        "original": "\nimport subprocess\n\ndef installed(name, channel=None):\n    cmd = [\"snap\", \"install\", name]\n    if channel:\n        cmd.extend([\"--channel\", channel])\n    subprocess.check_call(cmd)\n",
        "rewrite": "```python\nimport subprocess\n\ndef install_snap_package(name: str, channel: str = None) -> None:\n    command = [\"snap\", \"install\", name]\n    if channel:\n        command.extend([\"--channel\", channel])\n    subprocess.run(command, check=True)\n```"
    },
    {
        "original": "\nclass MyClass:\n    def __init__(self):\n        self.sender = None\n\n    async def get_sender(self):\n        if self.sender is None:\n            self.sender = await self._fetch_sender_from_api()\n        return self.sender\n\n    async def _fetch_sender_from_api(self):\n        # implement API call to fetch sender\n        pass\n",
        "rewrite": "```python\nclass MyClass:\n    def __init__(self):\n        self._sender = None\n\n    @property\n    def sender(self):\n        return self._get_sender()\n\n    async def _get_sender(self):\n        if self._sender is None:\n            self._sender = await self._fetch_sender_from_api()\n        return self._sender\n\n    async def _fetch_sender_from_api(self):\n        # implement API call to fetch sender\n        pass\n```"
    },
    {
        "original": "\nclass GKKPWork:\n    def from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True, manager=None):\n        # Initialize GKKPWork object\n        gkkp_work = cls()\n\n        # Get valence bands from PhononWfkqWork\n        valence_bands = phononwfkq_work.valence_bands\n\n        # Set nscf",
        "rewrite": "```python\nclass GKKPWork:\n    def __init__(self, valence_bands=None, nscf_vars=None, remove_wfkq=True, with_ddk=True):\n        self.valence_bands = valence_bands\n        self.nscf_vars = nscf_vars or {}\n        self.remove_wfkq = remove_wfkq\n        self.with_ddk = with_ddk\n\n    @classmethod\n    def from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True):\n        return cls"
    },
    {
        "original": "\ndef get_function_name(s):\n    start = s.find('(')\n    end = s.rfind(')')\n    func_name = s[:start].split()[-1]\n    return func_name\n",
        "rewrite": "```python\ndef get_function_name(s):\n    start = s.find('(')\n    if start != -1:\n        return s[:start].strip().rsplit(' ', 1)[-1]\n    else:\n        raise ValueError(\"Input string is not a function call\")\n```"
    },
    {
        "original": "\nclass APIAudit:\n    def __init__(self, db):\n        self.db = db\n\n    def ReadAPIAuditEntries(self, username=None, router_method_names=None, min_timestamp=None, max_timestamp=None):\n        query = \"SELECT * FROM audit_entries\"\n        conditions = []\n        \n        if username:\n            conditions.append(\"username = '{}'\".format(username))\n        if router_method_names:\n            conditions.append(\"router_method_name IN ({})\".format(','.join([\"'{}'",
        "rewrite": "```python\nclass APIAudit:\n    def __init__(self, db):\n        self.db = db\n\n    def read_api_audit_entries(\n        self,\n        username=None,\n        router_method_names=None,\n        min_timestamp=None,\n        max_timestamp=None\n    ):\n        query = \"SELECT * FROM audit_entries\"\n        conditions = []\n        \n        params = ()\n        \n        if username:\n            conditions.append(\"username = %s\")\n            params += (username,)\n            \n        if router_method_names:\n            placeholders = ', '.join(['%s'] * len(router_method_names))\n            conditions.append(f\"router_method_name IN ({placeholders})"
    },
    {
        "original": "\nclass SofMarker:\n    def __init__(self, stream, marker_code, offset):\n        self.stream = stream\n        self.marker_code = marker_code\n        self.offset = offset\n\n    @classmethod\n    def from_stream(cls, stream, marker_code, offset):\n        return cls(stream, marker_code, offset)\n",
        "rewrite": "```python\nclass SofMarker:\n    def __init__(self, stream, marker_code, offset):\n        self.stream = stream\n        self.marker_code = marker_code\n        self.offset = offset\n\n    @classmethod\n    def create_from_stream(cls, stream, marker_code, offset):\n        return cls(stream=stream, marker_code=marker_code, offset=offset)\n```"
    },
    {
        "original": "\ndef normalize(rendered):\n    \"\"\"Return the input string without non-functional spaces or newlines.\"\"\"\n    return ' '.join(rendered.replace('\\n', ' ').split())\n",
        "rewrite": "```python\ndef normalize(rendered):\n    result = []\n    prev_non_space = False\n    for e in rendered:\n        if e != ' ':\n            result.append(e)\n            prev_non_space = True\n        elif prev_non_space:\n            result.append(e)\n            prev_non_space = False\n    return ''.join(result)\n```"
    },
    {
        "original": "\nimport os\nimport hashlib\n\ndef hash_and_stat_file(self, path, saltenv='base'):\n    if path.startswith('salt://'):\n        path = path.replace('salt://', '')\n    else:\n        path = '/' + path\n\n    try:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.md5()\n            while chunk := f.read(8192):\n                file_hash.update(chunk)\n            file_hash = file_hash.hexdigest()\n    except",
        "rewrite": "```python\nimport os\nimport hashlib\n\ndef hash_and_stat_file(self, path, saltenv='base'):\n    if not isinstance(path, str) or not isinstance(saltenv, str):\n        raise ValueError(\"Both path and Salt environment must be strings\")\n\n    if not path.startswith('salt://'):\n        path = '/' + path\n    else:\n        path = path.replace('salt://', '')\n\n    try:\n        with open(os.path.abspath(os.path.expanduser(path)), 'rb') as f:\n            file_hash = hashlib.md5()\n            while chunk := f.read(8192):\n                file_hash.update(chunk)\n"
    },
    {
        "original": "\nclass AnsibleRunner:\n    def __init__(self, private_data_dir):\n        self.private_data_dir = private_data_dir\n        self.env_metadata = {}\n\n    def prepare_env(self):\n        for file in os.listdir(self.private_data_dir):\n            if file.endswith('.meta'):\n                with open(os.path.join(self.private_data_dir, file), 'r') as f:\n                    metadata = yaml.safe_load(f)\n                    self.env_metadata.update(metadata)\n",
        "rewrite": "```python\nimport os\nimport yaml\n\nclass AnsibleRunner:\n    def __init__(self, private_data_dir: str):\n        self._private_data_dir = private_data_dir\n        self._env_metadata = {}\n\n    @property\n    def env_metadata(self) -> dict:\n        return self._env_metadata\n\n    def prepare_env(self) -> None:\n        for filename in os.listdir(self._private_data_dir):\n            if filename.endswith('.meta'):\n                metadata_file_path = os.path.join(self._private_data_dir, filename)\n                try:\n                    with open(metadata_file_path, 'r') as file_stream:\n                        metadata"
    },
    {
        "original": "\nclass BitSetter:\n    def __init__(self):\n        self.bits = {}\n\n    def setbit(self, name, offset, value):\n        if name not in self.bits:\n            self.bits[name] = 0\n        prev_value = (self.bits[name] >> offset) & 1\n        if value:\n            self.bits[name] |= 1 << offset\n        else:\n            self.bits[name] &= ~(1 << offset)\n        return prev_value\n",
        "rewrite": "```python\nclass BitSetter:\n    def __init__(self):\n        self.bits = {}\n\n    def setbit(self, name: str, offset: int, value: bool) -> bool:\n        if name not in self.bits:\n            self.bits[name] = 0\n        prev_value = (self.bits[name] >> offset) & 1 == 1\n        self.bits[name] = (self.bits[name] & ~(1 << offset)) | ((value & 1) << offset)\n        return prev_value != value\n```"
    },
    {
        "original": "\ndef _checkValueItemParent(policy_element, policy_name, policy_key, policy_valueName, xpath_object, policy_file_data, check_deleted=False, test_item=True):\n    if test_item:\n        if policy_element.tag == '{http://www.microsoft.com/GroupPolicy/Settings}enabledValue':\n            return policy_valueName in policy_file_data\n        elif policy_element.tag == '{http://www.microsoft.com/GroupPolicy/Settings}disabledValue':\n            return policy_valueName not in",
        "rewrite": "```python\ndef _check_value_item_parent(\n    policy_element, \n    policy_name, \n    policy_key, \n    policy_value_name, \n    xpath_object, \n    policy_file_data, \n    check_deleted=False,\n):\n\"\"\" Revised function for better readability \"\"\"\n\nif test_item is not defined:\n   test_item = True\n\nmatch_type = {\n            \"{http://www.microsoft.com/GroupPolicy/Settings}enabledValue\": \"in\",\n            \"{http://www.microsoft.com/GroupPolicy/Settings}disabledValue\": \"not in\",\n            }\n\nif match_type.get(policy_element.tag):\n        return eval(f\"{repr(policy"
    },
    {
        "original": "\ndef extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n    if summed_spin_channels:\n        icoop_values = self.icoop_values['up'] + self.icoop_values['down']\n        icoop_extremum = max(icoop_values)\n        ichop_values = self.ichop_values['up'] + self.ichop_values['down']\n        ichop_extremum = min(ichop_values)\n        return max",
        "rewrite": "```python\ndef extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n    if summed_spin_channels:\n        icoop_values = self.icoop_values['up'] + self.icoop_values['down']\n        icoop_extremum = max(icoop_values)\n        ichop_extremum = min(self.ichop_values['up'] + self.ichop_values['down'])\n        return max(icoop_extremum, -ichop_extremum)\n```"
    },
    {
        "original": "\ndef secgroup_list(self):\n    \"\"\"\n    List security groups\n    \"\"\"\n    # Assuming you have a list of security groups\n    security_groups = [\"sg-12345678\", \"sg-23456789\", \"sg-34567890\"]\n    return security_groups\n",
        "rewrite": "```python\ndef get_security_groups(self):\n    security_groups = [\n        \"sg-12345678\",\n        \"sg-23456789\",\n        \"sg-34567890\"\n    ]\n    return security_groups.copy()\n```"
    },
    {
        "original": "\ndef detect_model_num(string):\n    return int(''.join(filter(str.isdigit, string)))\n",
        "rewrite": "```python\ndef detect_model_num(string):\n    return int(''.join(filter(str.isdigit, string)))\n```"
    },
    {
        "original": "\nfrom PyQt5.QtWidgets import QColorDialog\n\nclass ColorPicker:\n    def onColorPicker(self):\n        color_dialog = QColorDialog()\n        color_dialog.exec_()\n",
        "rewrite": "```python\nfrom PyQt5.QtWidgets import QApplication, QColorDialog\nimport sys\n\nclass ColorPicker:\n    def __init__(self):\n        self.app = QApplication(sys.argv)\n        self.color_dialog = QColorDialog()\n        self.color_dialog.show()\n\nif __name__ == \"__main__\":\n    picker = ColorPicker()\n    sys.exit(picker.app.exec_())\n```"
    },
    {
        "original": "\n\nimport subprocess\n\ndef check_table(table=None, family='ipv4'):\n    if table is None:\n        raise ValueError(\"Table name is required\")\n\n    command = f'nft -{family} list table {table}'\n    \n    try:\n        subprocess.check_output(command.split())\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n",
        "rewrite": "```python\nimport subprocess\n\ndef check_table(table: str, family: str = 'ipv4') -> bool:\n    if not table:\n        raise ValueError(\"Table name is required\")\n\n    command = ['nft', f'-{family}', 'list', 'table', table]\n\n    try:\n        subprocess.check_output(command, stderr=subprocess.DEVNULL)\n        return True\n    except subprocess.CalledProcessError as e:\n        if e.returncode != 0 and e.returncode != 255: # Added this line\n            raise \n        return False\n```"
    },
    {
        "original": "\ndef mean(name, num, minimum=0, maximum=0, ref=None):\n    if ref is None:\n        ref = []\n    ref.append(num)\n    if len(ref) > num:\n        ref.pop(0)\n    return sum(ref) / len(ref)\n",
        "rewrite": "```python\nfrom collections import deque\n\ndef mean(name, num, minimum=0, maximum=0):\n    ref = getattr(mean, name, None)\n    if ref is None:\n        setattr(mean, name, deque(maxlen=num))\n        ref = getattr(mean, name)\n    ref.append(num)\n    return sum(ref) / len(ref) if ref else 0\n```"
    },
    {
        "original": "\ndef strxor(s1, s2):\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must be of same length\")\n    return \"\".join(chr(ord(a) ^ ord(b)) for a, b in zip(s1, s2))\n",
        "rewrite": "```python\ndef str_xor(s1, s2):\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must be of the same length\")\n    return bytes(x ^ y for x, y in zip(map(ord, s1), map(ord, s2))).decode(encoding='utf-8', errors='replace')\n```"
    },
    {
        "original": "\nimport numpy as np\n\ndef get_ir_reciprocal_mesh(mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n    mesh = np.array(mesh)\n    is_shift = np.array(is_shift)\n    \n    # Generate all kpoints\n    kpoints = np.array(np.meshgrid(*[np.arange(i) for i in mesh])).T.reshape(-1, 3)\n    kpoints = kpoints / mesh\n    \n    # Apply shift",
        "rewrite": "```python\nimport numpy as np\n\ndef get_ir_reciprocal_mesh(mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n    mesh = np.array(mesh)\n    is_shift = np.array(is_shift)\n\n    kpoints = np.moveaxis(np.mgrid[:mesh[0], :mesh[1], :mesh[2]], 0, -1).reshape(-1, 3)\n    kpoints = (kpoints + is_shift) / mesh\n    return kpoints\n```"
    },
    {
        "original": "\nimport json\n\ndef json_pretty_dump(obj, filename):\n    with open(filename, 'w') as f:\n        json.dump(obj, f, indent=4)\n",
        "rewrite": "```python\nimport json\n\ndef json_pretty_dump(obj, filename):\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(obj, f, indent=4, ensure_ascii=False)\n\ndef main():\n    data = {\n        'name': 'John Doe',\n        'age': 30,\n        'city': 'New York'\n    }\n\n    filename = \"test.json\"\n    json_pretty_dump(data, filename)\n\nif __name__ == \"__main__\":\n    main()\n```\nAn alternative way to do this using `json.dumps` instead of `json.dump` and writing the"
    },
    {
        "original": "\nimport subprocess\n\ndef _hdparm(args, failhard=True):\n    try:\n        output = subprocess.check_output(['hdparm'] + args)\n        return output.decode('utf-8')\n    except subprocess.CalledProcessError as e:\n        if failhard:\n            raise\n        else:\n            return str(e)\n",
        "rewrite": "```python\nimport subprocess\n\ndef hdparm(*, failhard: bool = True, args: list[str]) -> str:\n    try:\n        result = subprocess.run(['hdparm'] + args, capture_output=True, check=failhard)\n        return result.stdout.decode('utf-8')\n    except subprocess.CalledProcessError as e:\n        return str(e)\n```"
    },
    {
        "original": "\nclass HInfinityFilter:\n    def __init__(self):\n        self.measurements = []\n\n    def update(self, z):\n        if z is not None:\n            self.measurements.append(z)\n",
        "rewrite": "```python\nclass HInfinityFilter:\n    def __init__(self):\n        self._measurements = []\n\n    @property\n    def measurements(self):\n        return self._measurements\n\n    def update(self, measurement):\n        if measurement is not None:\n            self._measurements.append(measurement)\n```"
    },
    {
        "original": "\ndef exhaust_stream(f):\n    def wrapper(*args, **kwargs):\n        result = f(*args, **kwargs)\n        if hasattr(result, '__iter__'):\n            list(result)\n        return result\n    return wrapper\n",
        "rewrite": "```python\ndef exhaust_stream(f):\n    def wrapper(*args, **kwargs):\n        result = f(*args, **kwargs)\n        if hasattr(result, '__iter__') and not isinstance(result, (str, bytes)):\n            try:\n                list(result)\n            except TypeError:\n                pass\n        return result\n    return wrapper\n```"
    },
    {
        "original": "\ndef _cs_path_exists(fspath):\n    return fspath.exists()\n",
        "rewrite": "```python\nfrom pathlib import Path\n\ndef cs_path_exists(path: str) -> bool:\n    return Path(path).is_file() or Path(path).is_dir()\n```"
    },
    {
        "original": "\nclass ProgrammingAssistant:\n    def addfield(self, pkt, s, val):\n        if,val not None:\n            setattr(pkt,s,val)\n",
        "rewrite": "```python\nclass ProgrammingAssistant:\n    def add_field(self, pkt, field_name, value):\n        if value is not None:\n            setattr(pkt, field_name, value)\n```"
    },
    {
        "original": "\nimport os\n\ndef _file_path(self, dirname, filename):\n    full_path = os.path.join(dirname, filename)\n    dir_path = os.path.dirname(full_path)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    return full_path\n",
        "rewrite": "```python\nimport os\n\ndef _file_path(self, dirname, filename):\n    full_path = os.path.join(dirname, filename)\n    dir_path = os.path.dirname(full_path)\n    os.makedirs(dir_path, exist_ok=True)\n    return full_path\n```\n \ncould be rewritten as \n\n```python\nimport os\n\ndef _file_path(self, dirname, filename):\n    full_path = os.path.join(dirname, filename)\n    if not os.path.exists(os.path.dirname(full_path)):\n      try:\n        \"Validate\":os.makedirs(os.path.dirname(full_path))\n      except Exception as e:\n         print(str(e))   \n   return"
    },
    {
        "original": "\ndef _get_stats_column_names(cls, stats):\n    return tuple(f\"{stat}_{col}\" for stat in stats for col in [\"mean\", \"std\", \"min\", \"max\"])\n",
        "rewrite": "```python\ndef _get_stats_column_names(cls, stats):\n    columns = (\"mean\", \"std\", \"min\", \"max\")\n    return tuple(f\"{stat}_{col}\" for stat in stats for col in columns)\n```"
    },
    {
        "original": "\nclass TransformedStructure:\n    def __init__(self):\n        self.structures = []\n\n    def add_structure(self, structure):\n        self.structures.append(structure)\n\n    def get_structures(self):\n        return self.structures.copy()\n",
        "rewrite": "```python\nfrom typing import List, Any\n\nclass TransformedStructure:\n    def __init__(self) -> None:\n        self._structures: List[Any] = []\n\n    def add_structure(self, structure: Any) -> None:\n        if structure not in self._structures:\n            self._structures.append(structure)\n\n    def get_structures(self) -> List[Any]:\n        return self._structures.copy()\n```"
    },
    {
        "original": "\nimport math \nfrom scipy.stats import norm \n\ndef norm_cdf(x_range, mu, var=1, std=None):\n  if std is None:\n      std = math.sqrt(var)\n  \n  lower_bound = (x_range[0] - mu) / std \n  upper_bound = (x_range[1] - mu) / std \n  \n  lower_prob = norm.cdf(lower_bound) \n  upper_prob = norm.cdf(upper_bound)\n  \n  return upper_prob",
        "rewrite": "```python\nfrom scipy.stats import norm\n\ndef norm_cdf(x_range, mu, sigma=1):\n    z_lower = (x_range[0] - mu) / sigma\n    z_upper = (x_range[1] - mu) / sigma\n    return norm.cdf(z_upper) - norm.cdf(z_lower)\n```"
    },
    {
        "original": "\nimport subprocess\nimport json\n\ndef get_current_versions():\n    result = subprocess.run(['brew', 'list', '--versions'], stdout=subprocess.PIPE)\n    return dict(line.split(' ') for line in result.stdout.decode('utf-8').strip().split('\\n'))\n\ndef get_latest_versions():\n    result = subprocess.run(['brew', 'outdated'], stdout=subprocess.PIPE)\n    return dict(line.split(' ') for line in result.stdout.decode('utf-8').strip().split('\\n'))\n\n",
        "rewrite": "```python\nimport subprocess\n\ndef get_current_versions():\n    result = subprocess.check_output(['brew', 'list', '--versions']).decode('utf-8').strip().split('\\n')\n    return {package: version for package, version in (line.split() if line else () for line in result)}\n\ndef get_latest_versions():\n    result = subprocess.check_output(['brew', 'outdated']).decode('utf-8').strip().split('\\n')\n    return {package: version for package, version in (line.split()[:2] if line else () for line in result)}\n```"
    },
    {
        "original": "\ndef prev_moment_operating_on(self, qubits, end_moment_index=None, max_distance=None):\n    if max_distance is not None and max_distance < 0:\n        raise ValueError(\"max_distance cannot be negative\")\n    \n    if end_moment_index is None:\n        end_moment_index = len(self.moments)\n    \n    for i in range(end_moment_index - 1, -1, -1):\n        moment = self.moments[i]\n       ",
        "rewrite": "```python\ndef prev_moment_operating_on(self, qubits, end_moment_index=None, max_distance=None):\n    if max_distance is not None and max_distance < 0:\n        raise ValueError(\"max_distance cannot be negative\")\n        \n    end_moment_index = end_moment_index or len(self.moments)\n    \n    for i in range(end_moment_index - 1, -1, -1):\n        moment = self.moments[i]\n```"
    },
    {
        "original": "\ndef _fix_ctx(m2_ctx, issuer=None):\n    if issuer is not None:\n        m2_ctx.set_app_data(issuer)\n    else:\n        m2_ctx.set_app_data(None)\n",
        "rewrite": "```python\ndef prepare_context(m2_ctx, issuer: object = None) -> None:\n    m2_ctx.set_app_data(issuer)\n```"
    },
    {
        "original": "\nfrom azure.storage.blob import BlobServiceClient\n\ndef get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):\n    if storage_account and storage_key:\n        conn_str = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_key};BlobEndpoint=https://{storage_account}.blob.core.windows.net/\"\n        return BlobServiceClient.from_connection_string(conn_str, **conn_kwargs)\n    else:\n        return None\n",
        "rewrite": "```python\nfrom azure.storage.blob import BlobServiceClient\n\ndef get_storage_conn(storage_account: str, storage_key: str, **kwargs) -> BlobServiceClient | None:\n    if storage_account and storage_key:\n        conn_str = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_key};BlobEndpoint=https://{storage_account}.blob.core.windows.net/\"\n        return BlobServiceClient.from_connection_string(conn_str, **kwargs)\n    return None\n```"
    },
    {
        "original": "\nimport requests\nimport json\n\ndef create_profile(hostname, username, password, profile_type, name, **kwargs):\n    auth = (username, password)\n    url = f\"https://{hostname}/mgmt/tm/{profile_type}\"\n    payload = {\"name\": name}\n    for key, value in kwargs.items():\n        payload[key] = value\n    response = requests.post(url, auth=auth, json=payload)\n    if response.status_code == 200:\n        return response",
        "rewrite": "```python\nimport requests\n\ndef create_profile(hostname: str, username: str, password: str, profile_type: str, name: str, **kwargs) -> dict:\n    auth = (username, password)\n    url = f\"https://{hostname}/mgmt/tm/{profile_type}\"\n    payload = {\"name\": name, **kwargs}\n    try:\n        response = requests.post(url, auth=auth, json=payload)\n        response.raise_for_status()\n        return response.json()\n    except requests.RequestException as e:\n        raise Exception(f\"Failed to create profile. Error details - {str(e)}\")\n```"
    },
    {
        "original": "\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef create_policy(name, policy_name, policy_type, policy_data,\n                  region=None,\n                  key=None,\n                  keyid=None,\n                  profile=None):\n    elb = boto3.client('elb', region_name=region)\n    \n    try:\n        response = elb.create_load_balancer_policy(\n            LoadBalancerName=name,\n            PolicyName=policy_name,\n            PolicyTypeName=policy_type,\n",
        "rewrite": "```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef create_policy(name, policy_name, policy_type, policy_data, region=None, key=None, keyid=None, profile=None):\n    session = boto3.Session(profile_name=profile)\n    elb = session.client('elb', aws_access_key_id=keyid or key if not profile else None,\n                          aws_secret_access_key=key if not profile else None,\n                          region_name=region)\n\n    try:\n        response = elb.create_load_balancer_policy(\n            LoadBalancerName=name,\n            PolicyName=policy_name,\n            PolicyTypeName=policy"
    },
    {
        "original": "\ndef _finalize_axis(self, key, **kwargs):\n    \"\"\"\n    Extends the ElementPlot _finalize_axis method to set appropriate\n    labels, and axes options for 3D Plots.\n    \"\"\"\n    if self.dim == 3:\n        axis = self.handles['axis']\n        axis.set_xlabel(kwargs.get('xlabel', 'X Axis'))\n        axis.set_ylabel(kwargs.get('ylabel', 'Y Axis'))\n        axis.set_zlabel(kwargs.get('zlabel', 'Z",
        "rewrite": "```python\ndef _finalize_axis(self, key, **kwargs):\n    if self.dim == 3:\n        xlabel = kwargs.get('xlabel', 'X Axis')\n        ylabel = kwargs.get('ylabel', 'Y Axis')\n        zlabel = kwargs.get('zlabel', 'Z Axis')\n\n        axis = self.handles['axis']\n        axis.set_xlabel(xlabel)\n        axis.set_ylabel(ylabel)\n        axis.set_zlabel(zlabel)\n```"
    },
    {
        "original": "\nfrom calendar import monthrange\nfrom datetime import datetime\n\ndef _calendar_month_middles(year):\n    middles = []\n    for month in range(1, 13):\n        _, num_days = monthrange(year, month)\n        middles.append((datetime(year, month, (num_days + 1) // 2)).strftime(\"%Y-%m-%d\"))\n    return middles\n",
        "rewrite": "```python\nfrom calendar import monthrange\nfrom datetime import datetime\n\ndef calendar_month_middles(year):\n    return [datetime(year, month, (num_days + 1) // 2).strftime(\"%Y-%m-%d\") \n            for month in range(1, 13) \n            for (_, num_days) in [monthrange(year, month)]]\n```"
    },
    {
        "original": "\nclass Formula:\n    def __init__(self, formula):\n        self.formula = formula\n\n    def reduced_formula(self):\n        elements = {}\n        i = 0\n        while i < len(self.formula):\n            if self.formula[i].isupper():\n                element = self.formula[i]\n                i += 1\n                if i < len(self.formula) and self.formula[i].isdigit():\n                    j = i\n                    while j < len(self.formula",
        "rewrite": "```python\nclass Formula:\n    def __init__(self, formula):\n        self.formula = formula\n\n    def reduced_formula(self):\n        elements = {}\n        i = 0\n        while i < len(self.formula):\n            if self.formula[i].isupper():\n                element = self.formula[i]\n                if len(self.formula) > i + 1 and self.formula[i + 1].islower():\n                    j = i + 2\n                    while j < len(self.formula) and self.formula[j].isdigit():\n                        j += 1\n                    element += self.formula[i+"
    },
    {
        "original": "\ndef find_cell_end(self, lines):\n    in_cell = False\n    end_idx = 0\n    start_idx = 0\n    for i, line in enumerate(lines):\n        if '' in line and not in_cell:\n            start_idx = i\n            in_cell = True\n        elif '' in line and in_cell:\n            end_idx = i\n            break\n    return end_idx + 1, start_idx if start_idx != 0 else",
        "rewrite": "```python\ndef find_cell_end(self, lines):\n    start_idx = None\n    for i, line in enumerate(lines):\n        if '' in line:\n            if start_idx is None:\n                start_idx = i\n            else:\n                return i + 1, start_idx \n    return None \n```"
    },
    {
        "original": "\ndef _filter_cache(self, dmap, kdims):\n    filtered_cache = {}\n    for key, value in dmap.cache.items():\n        if all(k in key and key[k] == v for k, v in kdims.items()):\n            filtered_cache[key] = value\n    return filtered_cache\n",
        "rewrite": "```python\ndef _filter_cache(self, dmap, kdims):\n    return {\n        key: value \n        for key, value in dmap.cache.items() \n        if all(value.get(k) == v for k, v in kdims.items())\n    }\n```"
    },
    {
        "original": "\ndef parse_dict_string(string, key, default):\n    result = {}\n    items = string.split(',')\n    for item in items:\n        k, v = item.split('=')\n        result[k] = v\n    return result.get(key, default)\n",
        "rewrite": "```python\ndef parse_dict_string(string, key, default=None):\n    return dict(item.split('=', 1) for item in string.replace(',', '&').split('&')).get(key, default)\n```"
    },
    {
        "original": "\nimport numpy as np\n\nclass DimensionValues:\n    def __init__(self, array):\n        self.array = array\n\n    def dimension_values(self, dimension, expanded=True, flat=True):\n        values = self.array.take(indices=0, axis=dimension)\n        if expanded:\n            values = np.expand_dims(values, axis=dimension)\n        if flat:\n            values = values.flatten()\n        return values\n",
        "rewrite": "```python\nimport numpy as np\n\nclass DimensionValues:\n    def __init__(self, array):\n        self.array = np.asarray(array)\n\n    def dimension_values(self, dimension):\n        values = self.array.take(indices=0, axis=dimension)\n        return values if dimension == 0 else np.atleast_2d(values.flatten()).T\n```"
    },
    {
        "original": "\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n    for validator in validators:\n        check = validator[\"check\"]\n        if not isinstance(check, str):\n            check = str(check)\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n    return validators_mapping\n",
        "rewrite": "```python\ndef convert_validators_to_mapping(validators):\n    return {(str(v[\"check\"]), v[\"comparator\"]): v for v in validators}\n```"
    },
    {
        "original": "\nimport re\nfrom rdfvalue import LogTarget\n\ndef ParseAction(action):\n    facility_severity, rest = action.split(' ', 1)\n    facility, severity = facility_severity.split('.')\n    \n    type_def, destination_template = rest.split(';', 1)\n    \n    if type_def == '@@':\n        protocol = 'TCP'\n    elif type_def == '@':\n        protocol = 'UDP'\n    elif type_def == '|':\n        protocol = 'Named Pipe'\n    elif",
        "rewrite": "```python\nimport re\nfrom rdfvalue import LogTarget\n\ndef parse_action(action):\n    facility_severity, rest = action.split(' ', 1)\n    facility, severity = facility_severity.split('.')\n    type_def, destination_template = rest.rsplit(';', 1)\n\n    protocol_mapping = {\n        '@@': 'TCP',\n        '@': 'UDP',\n        '|': 'Named Pipe'\n    }\n\n    protocol = protocol_mapping.get(type_def)\n\n    return {\n        'facility': facility,\n        'severity': severity,\n        'type_definition': type_def,\n        'protocol': protocol,\n        'destination_template': destination"
    },
    {
        "original": "\nclass MDP:\n    def __init__(self, states, actions, transition_model, reward_fn):\n        self.states = states\n        self.actions = actions\n        self.transition_model = transition_model\n        self.reward_fn = reward_fn\n\n    def modified_policy_iteration(self, v_init=None, epsilon=1e-8, max_iter=1000, k=20):\n        if v_init is None:\n            v_init = {s: 0 for s in self.states",
        "rewrite": "```python\nclass MDP:\n    def __init__(self, states, actions, transition_model, reward_fn):\n        self.states = states\n        self.actions = actions\n        self.transition_model = transition_model\n        self.reward_fn = reward_fn\n\n    def modified_policy_iteration(self, v_init=None, epsilon=1e-8, max_iter=1000, k=20):\n        if v_init is None:\n            v_init = {s: 0.0 for s in self.states}\n\n        policy = {s: next(iter(self.actions)) for s in self.states}\n        values = v_init.copy()\n\n"
    },
    {
        "original": "\ndef parse_env_file(env_file):\n    env_dict = {}\n    with open(env_file, 'r') as f:\n        for line in f:\n            key_value = line.strip().split('=')\n            if len(key_value) == 2:\n                key, value = key_value\n                env_dict[key] = value\n    return env_dict\n",
        "rewrite": "```python\ndef parse_env_file(env_file):\n    env_dict = {}\n    with open(env_file, 'r') as f:\n        for line in map(str.strip, filter(None, f)):\n            if '=' in line:\n                key, _, value = line.partition('=')\n                env_dict[key.strip()] = value.strip()\n    return env_dict\n```"
    },
    {
        "original": "\nimport math\n\ndef declination_spencer71(dayofyear):\n    \"\"\"\n    Solar declination from Duffie & Beckman [1] and attributed to\n    Spencer (1971) and Iqbal (1983).\n\n    .. warning::\n        Return units are radians, not degrees.\n\n    Parameters\n    ----------\n    dayofyear : numeric\n\n    Returns\n    -------\n    declination (radians) : numeric\n        Angular position of the sun at solar noon relative to",
        "rewrite": "```python\nimport math\n\ndef declination_spencer71(day_of_year):\n    return 0.4093 * math.sin(2 * math.pi / 365 * (day_of_year - 81))\n```"
    },
    {
        "original": "\nclass EOPatch:\n    def __init__(self, data):\n        self.data = data\n\ndef save(self, eopatch, use_tmp=True):\n    if use_tmp:\n        with open(\"temp_file.txt\", \"w\") as f:\n            f.write(str(eopatch.data))\n    else:\n        with open(\"final_file.txt\", \"w\") as f:\n            f.write(str(eopatch.data))\n",
        "rewrite": "```python\nimport os\nimport tempfile\n\nclass EOPatch:\n    def __init__(self, data):\n        self.data = data\n\n    def save(self, filename=\"final_file.txt\", overwrite=False):\n        if not overwrite and os.path.exists(filename):\n            raise FileExistsError(\"File already exists. Set overwrite to True to replace.\")\n\n        with open(filename, \"w\") as f:\n            f.write(str(self.data))\n\n    @property\n    def temp_file(self):\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as tmp:\n            tmp.write(str(self.data))\n            return tmp.name\n\n# Usage"
    },
    {
        "original": "\nfrom github import Github\n\nclass GitHubRepository:\n    def __init__(self, repo_owner, repo_name):\n        self.repo_owner = repo_owner\n        self.repo_name = repo_name\n        self.github = Github()\n    \n    def add_to_collaborators(self, collaborator, permission=\"pull\"):\n        if permission not in [\"pull\", \"push\", \"admin\"]:\n            raise ValueError(\"Invalid permission\")\n        \n        repo = self.github.get_repo(f\"{self.repo_owner}/{self.repo",
        "rewrite": "```python\nfrom github import Github, GithubException\n\nclass GitHubRepository:\n    def __init__(self, repo_owner: str, repo_name: str, access_token: str):\n        self.repo_owner = repo_owner\n        self.repo_name = repo_name\n        self.github = Github(access_token)\n\n    def add_to_collaborators(self, collaborator: str, permission: str = \"pull\") -> None:\n        valid_permissions = [\"pull\", \"push\", \"admin\"]\n        if permission not in valid_permissions:\n            raise ValueError(f\"Invalid permission. Must be one of {', '.join(valid_permissions)}\")\n        \n       "
    },
    {
        "original": "\ndef read_and_write(reader, writer, chunk_size):\n    chunk = reader.read(chunk_size)\n    if not chunk:\n        return True\n    writer.write(chunk)\n    return None\n",
        "rewrite": "```python\ndef read_and_write(reader, writer, chunk_size):\n    while chunk := reader.read(chunk_size):\n        writer.write(chunk)\n```"
    },
    {
        "original": "\nimport paramiko\nimport hashlib\n\ndef file_transfer(\n    ssh_conn,\n    source_file,\n    dest_file,\n    file_system=None,\n    direction=\"put\",\n    disable_md5=False,\n    inline_transfer=False,\n    overwrite_file=False\n):\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    \n    if direction == \"put\":\n        sftp = ssh_conn.open_sftp()\n        sftp.put(source_file, dest",
        "rewrite": "```python\nimport paramiko\nimport hashlib\n\ndef file_transfer(\n    ssh_conn: paramiko.SSHClient, \n    source_file: str, \n    dest_file: str, \n    file_system: str = None, \n    direction: str = \"put\", \n    disable_md5: bool = False, \n    inline_transfer: bool = False, \n    overwrite_file: bool = False\n):\n    \n    if direction == \"put\":\n        with ssh_conn.open_sftp() as sftp:\n            sftp.put(source_file, dest_file)\n            \n            if not disable_md5:\n                with open(source_file"
    },
    {
        "original": "\ndef modify_order(self, orderId: int, quantity: int):\n    # some implementation here \n\nclass TradingPlatform:\n    def __init__(self):\n        self.orderId = 0\n        self.quantity = 0\n\n    def move_stoploss(self, stoploss: float):\n        self.orderId = 1  # assume we have an orderId of 1\n        self.quantity = 10  # assume we have a quantity of 10\n        self.modify_order(self.order",
        "rewrite": "```python\nclass TradingPlatform:\n    def __init__(self):\n        self.orders = {}\n\n    def add_order(self, order_id: int, quantity: int):\n        self.orders[order_id] = {\"quantity\": quantity}\n\n    def modify_order(self, order_id: int, quantity: int):\n        if order_id in self.orders:\n            self.orders[order_id][\"quantity\"] = quantity\n\n    def move_stoploss(self, stoploss_price: float):\n        order_data = {\n            \"order_id\": 1,\n            \"quantity\": 10,\n            \"stoploss_price\": stoploss_price\n        }\n        \n"
    },
    {
        "original": "\nimport github\nfrom datetime import datetime\n\nclass GithubApi:\n    def __init__(self, token):\n        self.git = github.Github(token)\n\n    def get_comments(self, owner, repo, number, since=None):\n        if since is None:\n            comments = self.git.get_repo(repo).get_issue(number).get_comments()\n        else:\n            comments = self.git.get_repo(repo).get_issue(number).get_comments(since=since)\n        \n        return comments\n",
        "rewrite": "```python\nfrom github import Github, InvalidInputException\nfrom datetime import datetime, timedelta\n\nclass GithubApi:\n    def __init__(self, token):\n        self.git = Github(token)\n\n    def get_comments(self, owner: str, repo: str, number: int):\n        try:\n            repository = self.git.get_repo(f\"{owner}/{repo}\")\n            issue = repository.get_issue(number)\n            since_date = datetime.now() - timedelta(days=30)\n            return issue.get_comments(since=since_date)\n        except InvalidInputException as e:\n            print(f\"Invalid input: {e}\")\n        except Exception as e"
    },
    {
        "original": "\nimport os\nimport fnmatch\n\nclass FileProcessor:\n    def __init__(self, directory):\n        self.directory = directory\n\n    def list_filepaths(self, wildcard=None):\n        filepaths = [os.path.join(self.directory, f) for f in os.listdir(self.directory)]\n        \n        if wildcard:\n            patterns = [p.strip() for p in wildcard.split(\"|\")]\n            filtered_filepaths = []\n            for filepath in filepaths:\n                for pattern in patterns:\n                    if fn",
        "rewrite": "```python\nimport os\nimport fnmatch\n\nclass FileProcessor:\n    def __init__(self, directory):\n        self.directory = directory\n        self._verify_directory()\n\n    def _verify_directory(self):\n        if not os.path.isdir(self.directory):\n            raise ValueError(\"Invalid directory path\")\n\n    def list_filepaths(self, wildcard=None):\n        filepaths = [os.path.join(self.directory, f) for f in os.listdir(self.directory)]\n\n        if wildcard:\n            patterns = [p.strip() for p in wildcard.split(\"|\")]\n            filepaths = [\n                fp for fp in filepaths \n                if os.path.isfile(fp)"
    },
    {
        "original": "\nimport github\n\nclass GithubRepository:\n    def __init__(self, owner, repo):\n        self.owner = owner\n        self.repo = repo\n        self.g = github.Github()\n        self.repository = self.g.get_repo(f\"{owner}/{repo}\")\n\n    def merge(self, base, head, commit_message=github.GithubObject.NotSet):\n        return self.repository.merge(base, head, commit_message)\n",
        "rewrite": "```python\nimport github\n\nclass GithubRepository:\n    def __init__(self, owner: str, repo: str, token: str):\n        self.owner = owner\n        self.repo = repo\n        self.g = github.Github(token)\n        try:\n            self.repository = self.g.get_repo(f\"{owner}/{repo}\")\n        except github.UnknownObjectException as e:\n            raise ValueError(f\"Repository '{owner}/{repo}' not found\") from e\n\n    def merge(self, base: str, head: str, commit_message: str = \"\") -> github.PullRequestMerge:\n        return self.repository.merge(base, head, commit"
    },
    {
        "original": "\nfrom pymatgen import Structure\nfrom pymatgen.symmetry.kpoints import KpointPath, KpointException\n\ndef automatic_gamma_density(structure: Structure, kppa: float) -> dict:\n    \"\"\"\n    Returns an automatic Kpoint object based on a structure and a kpoint density.\n    Uses Gamma centered meshes always. For GW.\n\n    Args:\n        structure (Structure): Input structure\n        kppa (float): Grid density\n\n    Returns:\n        dict:",
        "rewrite": "```python\nfrom pymatgen import Structure\nfrom pymatgen.symmetry.kpoints import KpointPath, KpointException\n\ndef automatic_gamma_density(structure: Structure, kppa: float) -> dict:\n    try:\n        kpoints, _ = KpointPath.automatic_linemode(\n            structure=structure, \n            npoints=int(kppa), \n            reciprocal_density=1)\n        return {'kpoints': kpoints}\n    except KpointException as e:\n        return {'error': str(e)}\n```"
    },
    {
        "original": "\nimport gitlab\n\nclass GitLab:\n    def __init__(self, url, private_token):\n        self.gl = gitlab.Gitlab(url, private_token=private_token)\n\n    def unprotect(self, project_id, branch_name, **kwargs):\n        project = self.gl.projects.get(project_id)\n        branch = project.branches.get(branch_name)\n        try:\n            branch.unprotect()\n        except gitlab.exceptions.GitlabAuthenticationError as e:\n            raise GitlabAuthentication",
        "rewrite": "```python\nimport gitlab\n\nclass GitLab:\n    def __init__(self, url, private_token):\n        self.gl = gitlab.Gitlab(url, private_token=private_token)\n\n    def unprotect_branch(self, project_id: int, branch_name: str) -> None:\n        try:\n            project = self.gl.projects.get(project_id)\n            branch = project.branches.get(branch_name)\n            if not branch.protected:\n                raise Exception(f\"Branch '{branch_name}' is not protected.\")\n            branch.unprotect()\n        except gitlab.exceptions.GitlabAuthenticationError as e:\n            raise Exception(f\"Git"
    },
    {
        "original": "\nfrom werkzeug.datastructures import RequestCacheControl\n\ndef parse_cache_control_header(value, on_update=None, cls=None):\n    if cls is None:\n        cls = RequestCacheControl\n    cache_control = cls()\n    for directive in value.split(','):\n        directive = directive.strip()\n        if '=' in directive:\n            key, value = [x.strip() for x in directive.split('=', 1)]\n            cache_control.set(key, value)\n        else:\n            cache_control.set(d",
        "rewrite": "```python\nfrom werkzeug.datastructures import RequestCacheControl\n\ndef parse_cache_control_header(\n    value: str, \n    on_update=None, \n    cls=RequestCacheControl\n):\n    cache_control = cls()\n    \n    for directive in map(str.strip, value.split(',')):\n        if '=' in directive:\n            key, value = map(str.strip, directive.split('=', 1))\n            cache_control.set(key, value)\n        else:\n            cache_control.set(directive)\n```"
    },
    {
        "original": "\nclass ScenarioBuilder:\n    def _get_matrix(self):\n        return {\n            \"scenario_1\": {\"subcommand\": [\"action-1\", \"action-2\"]},\n            \"scenario_2\": {\"subcommand\": [\"action-1\"]}\n        }\n",
        "rewrite": "```python\nfrom enum import Enum\nfrom typing import Dict, List\n\nclass CommandType(Enum):\n    ACTION = 1\n\nclass Command:\n    def __init__(self, name: str, command_type: CommandType):\n        self.name = name\n        self.command_type = command_type\n\nclass Scenario:\n    def __init__(self, name: str, commands: List[Command]):\n        self.name = name\n        self.commands = commands\n\nclass ScenarioBuilder:\n    def _get_matrix(self) -> Dict[str, Scenario]:\n        action_1 = Command(\"action-1\", CommandType.ACTION)\n        action_2 = Command(\""
    },
    {
        "original": "\nimport socket\n\ndef set_tcp_md5sig(s, addr, key):\n    \"\"\"\n    Enable TCP-MD5 on the given socket.\n\n    :param s: Socket\n    :param addr: Associated address.  On some platforms, this has no effect.\n    :param key: Key.  On some platforms, this has no effect.\n    \"\"\"\n    \n    TCP_MD5SIG = 14\n    tcp_md5sig_struct = struct.pack('4s', key.encode())",
        "rewrite": "```python\nimport socket\nimport struct\n\nTCP_MD5SIG = 14\n\ndef set_tcp_md5sig(sock, addr, key):\n    if not isinstance(key, bytes):\n        if not isinstance(key, str):\n            raise TypeError(\"Key must be str or bytes\")\n        key = key.encode()\n    sock.setsockopt(socket.SOL_TCP, TCP_MD5SIG, struct.pack('16s', key.ljust(16, b'\\0')))\n```"
    },
    {
        "original": "\ndef _jobs():\n    # Add your implementation here\n    pass\n",
        "rewrite": "```python\ndef _jobs():\n    return [\n        {\"id\": 1, \"title\": \"Software Engineer\", \"description\": \"Develop software applications\"},\n        {\"id\": 2, \"title\": \"Data Scientist\", \"description\": \"Analyze and interpret complex data\"},\n        {\"id\": 3, \"title\": \"DevOps Engineer\", \"description\": \"Ensure smooth operation of systems\"}\n    ]\n```"
    },
    {
        "original": "\nimport base64\n\ndef hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    auth_str = f\"{user}:{passwd}\"\n    b64_str = base64.b64encode(auth_str.encode()).decode()\n    header_str = f\"Basic {b64_str}\"\n    \n    return header_str\n",
        "rewrite": "```python\nimport base64\nfrom urllib.parse import quote\n\ndef hidden_basic_auth(user: str = \"user\", password: str = \"password\") -> str:\n    auth_str = f\"{quote(user)}:{quote(password)}\"\n    return f\"Basic {base64.b64encode(auth_str.encode()).decode()}\"\n```"
    },
    {
        "original": "\nimport numpy as np\nfrom typing import Tuple\n\ndef get_noisy_gate(gate_name: str, params: Tuple[float]) -> Tuple[np.ndarray, str]:\n    \"\"\"\n    Look up the numerical gate representation and a proposed 'noisy' name.\n\n    Parameters:\n        gate_name (str): The Quil gate name\n        params (Tuple[float]): The gate parameters.\n\n    Returns:\n        A tuple (matrix, noisy_name) with the representation of the ideal gate matrix \n       ",
        "rewrite": "```python\nimport numpy as np\nfrom typing import Tuple\n\ndef get_noisy_gate(gate_name: str, *params) -> Tuple[np.ndarray, str]:\n    gates = {\n        'RX': (lambda theta: np.array([[np.cos(theta/2), -1j*np.sin(theta/2)], \n                                       [-1j*np.sin(theta/2), np.cos(theta/2)]]), \n               lambda theta: f\"Noisy RX({theta})\"),\n        'RY': (lambda theta: np.array([[np.cos(theta/2), -1*np.sin(theta/2)], \n                                       [np.sin(theta/"
    },
    {
        "original": "\ndef creep_data(data_set='creep_rupture'):\n    if data_set == 'creep_rupture':\n        # implement Brun and Yoshida's metal creep rupture data\n        pass \n    else:\n        raise ValueError(\"Invalid data set\")\n",
        "rewrite": "```python\nfrom enum import Enum\n\nclass DataSet(Enum):\n    CREEP_RUPTURE = 'creep_rupture'\n\ndef creep_data(data_set: str = DataSet.CREEP_RUPTURE.value):\n    try:\n        data_set_enum =DataSet[data_set]\n    except KeyError:\n        raise ValueError(\"Invalid data set\")\n\n    if data_set_enum == DataSet.CREEP_RUPTURE:\n        # implement Brun and Yoshida's metal creep rupture data\n        pass \n```"
    },
    {
        "original": "\n\nimport salt.client\n\ndef create_baseline(tag=\"baseline\", config='root'):\n    local = salt.client.LocalClient()\n    \n    if not tag:\n        tag = \"baseline\"\n        \n    result = local.cmd('snapper.create', [tag, config])\n    \n    return result[0]\n\n",
        "rewrite": "```python\nimport salt.client\n\ndef create_baseline(tag: str = \"baseline\", config: str = 'root') -> dict:\n    with salt.client.LocalClient() as local:\n        return local.cmd('snapper.create', [tag or \"baseline\", config])[0]\n```"
    },
    {
        "original": "\nclass Dispatcher:\n    def __init__(self):\n        self.send_last_message_functions = {}\n\n    def remove_send_last_message(self, connection):\n        if connection in self.send_last_message_functions:\n            del self.send_last_message_functions[connection]\n",
        "rewrite": "```python\nclass Dispatcher:\n    def __init__(self):\n        self.send_last_message_functions = {}\n\n    def deregister_connection(self, connection):\n        try:\n            del self.send_last_message_functions[connection]\n        except KeyError:\n            pass\n```"
    },
    {
        "original": "\n\ndef set_syslog_server(server=None, type=\"primary\"):\n    if not isinstance(server, str):\n        raise ValueError(\"Server must be a string\")\n    if type not in [\"primary\", \"secondary\"]:\n        raise ValueError(\"Type must be either 'primary' or 'secondary'\")\n    \n    # implement your logic here to set syslog server on host\n    # this example just prints out what would happen\n    if type == \"primary\":\n        print(f\"Setting primary syslog",
        "rewrite": "```python\ndef set_syslog_server(server: str, type: str = \"primary\") -> None:\n    if not isinstance(server, str):\n        raise ValueError(\"Server must be a string\")\n    if type not in [\"primary\", \"secondary\"]:\n        raise ValueError(\"Type must be either 'primary' or 'secondary'\")\n\n    syslog_types = {\n        \"primary\": f\"Setting primary syslog server to {server}\",\n        \"secondary\": f\"Setting secondary syslog server to {server}\"\n    }\n    \n    print(syslog_types[type])\n```"
    },
    {
        "original": "\ndef check_config_mode(self, check_string=\"#)\", pattern=\"#\"):\n    return pattern in check_string\n",
        "rewrite": "```python\ndef has_pattern(self, string: str = \"\", pattern: str = \"#\") -> bool:\n    return pattern in (string or \"\")\n```"
    },
    {
        "original": "\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef from_EV(E, V):\n    alpha = (E ** 2) / V\n    beta = E / V\n    return gamma(alpha, scale=1/beta)\n",
        "rewrite": "```python\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef from_EV(E, V):\n    alpha = E**2 / V\n    beta = E / V\n    return gamma(alpha, scale=1/beta)\n```"
    },
    {
        "original": "\nclass GitlabDeleteError(Exception):\n    pass\n\nclass GitlabAuthenticationError(Exception):\n    pass\n\nclass Server:\n    def __init__(self):\n        self.objects = {}\n\n    def delete(self, id, **kwargs):\n        if not self.authenticate(**kwargs):\n            raise GitlabAuthenticationError\n        if id not in self.objects:\n            raise GitlabDeleteError\n        del self.objects[id]\n\n    def authenticate(self, **kwargs):\n        # implement your authentication logic here\n       ",
        "rewrite": "```python\nclass GitlabDeleteError(Exception):\n    \"\"\"Raised when trying to delete an object that does not exist.\"\"\"\n    \n    def __init__(self, message):\n        self.message = message\n        super().__init__(message)\n\n\nclass GitlabAuthenticationError(Exception):\n    \"\"\"Raised when authentication with the server fails.\"\"\"\n\n\nclass Server:\n    def __init__(self):\n        self.objects = {}\n        self.authenticated = False\n\n    def delete(self, id: int, **kwargs) -> None:\n        if not self.authenticate(**kwargs):\n            raise GitlabAuthenticationError(\"Authentication failed\")\n        \n        try:\n            del self.objects"
    },
    {
        "original": "\nclass ISA:\n    def __init__(self):\n        self.data = {}\n\n    def add_gate(self, num_qubits, gate_type, **_gate_info):\n        if str(num_qubits) + 'Q' not in self.data:\n            self.data[str(num_qubits) + 'Q'] = {}\n        \n        if \"_\" in _gate_info:\n            qubit_str = _gate_info\n        else:\n            qubit_str = _gate_info\n        \n        if 'dead' in",
        "rewrite": "```python\nclass ISA:\n    def __init__(self):\n        self.data = {}\n\n    def add_gate(self, num_qubits, gate_type):\n        qubit_key = f\"{num_qubits}Q\"\n        \n        if qubit_key not in self.data:\n            self.data[qubit_key] = {}\n        \n        self.data[qubit_key][gate_type] = None\n\ndef main():\n    isa = ISA()\n    isa.add_gate(2, \"CNOT\")\n    \nif __name__ == \"__main__\":\n    main()\n```"
    },
    {
        "original": "\nclass ServiceScaler:\n    def __init__(self):\n        self.containers = []\n\n    def scale(self, replicas):\n        if replicas < len(self.containers):\n            del self.containers[replicas:]\n        elif replicas > len(self.containers):\n            self.containers.extend([None]*(replicas - len(self.containers)))\n        \n        return True\n",
        "rewrite": "```python\nclass ServiceScaler:\n    def __init__(self):\n        self.containers = []\n\n    def scale(self, replicas):\n        diff = replicas - len(self.containers)\n        self.containers += [None] * max(diff, 0)\n        del self.containers[replicas:]\n\n        return True\n```"
    },
    {
        "original": "\ndef register_switch_address(addr, interval=None):\n    \"\"\"\n    Registers a new address to initiate connection to switch.\n\n    Registers a new IP address and port pair of switch to let\n    ryu.controller(controller.OpenFlowController)to trytoinitiate \n   connectiontoswitch.\n\n   :paramaddr: Atupleof(host,port)pair ofswitch.\n   :paraminterval: Intervalin secondstotrytoconnecttoswitch\n  \"\"\"\nimport time\n\ndef register_switch_address(addr,",
        "rewrite": "```python\nimport time\n\ndef register_switch_address(addr: str, interval: int = 5) -> None:\n    pass\n```"
    },
    {
        "original": "Please provide the programming challenge in Markdown format, and I'll be happy to help you with it!",
        "rewrite": "```python\ndef get_programming_challenge() -> str:\n    \"\"\"Retrieves a programming challenge from the user in Markdown format.\"\"\"\n    return input(\"Please provide the programming challenge in Markdown format: \")\n\ndef print_acknowledgement():\n    \"\"\"Prints an acknowledgement message to the user.\"\"\"\n    print(\"I'll be happy to help you with it!\")\n\ndef main() -> None:\n    challenge = get_programming_challenge()\n    print_acknowledgement()\n\nif __name__ == \"__main__\":\n    main()\n```"
    },
    {
        "original": "\ndef build_lines_data(self, code_obj):\n    pass\n",
        "rewrite": "```python\ndef build_lines_data(self, code_obj):\n    lines_data = {\n        line_num: {'line': line, 'issues': []}\n        for line_num, line in enumerate(code_obj.code.splitlines(), start=1)\n    }\n    return lines_data\n```"
    },
    {
        "original": "\nimport hashlib\nfrom ecdsa import VerifyingKey, SigningKey\nfrom base58 import b58encode_check\n\ndef key_pair_from_ed25519_key(hex_private_key):\n    # Convert hex private key to bytes\n    priv\tkey_bytes = bytes.fromhex(hex_private_key)\n    \n    # Create an Ed25519 signing key from the private key bytes\n    signing_key = SigningKey.from_secret_exponent(priv_key_bytes, hashlib.sha256)\n    \n    # Get verifykey",
        "rewrite": "```python\nimport hashlib\nfrom ecdsa import VerifyingKey, SigningKey\nfrom base58 import b58encode_check\n\ndef key_pair_from_ed25519_key(hex_private_key):\n    priv_key_bytes = bytes.fromhex(hex_private_key)\n    signing_key = SigningKey.from_secret_exponent(priv_key_bytes, curve=ecdsa.SECP256k1, hashfunc=hashlib.sha256)\n    verifying_key = signing_key.get_verifying_key()\n    return signing_key, verifying_key\n```"
    },
    {
        "original": "\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\nclass BandstructurePlotter:\n    def __init__(self):\n        self.fig, self.ax = plt.subplots()\n\n    def get_plot(self, ylim=None, units=\"thz\"):\n        if ylim:\n            self.ax.set_ylim(ylim)\n        \n        if units == \"thz\":\n            self.ax.set_ylabel(\"Frequency (THz)\")\n        elif units == \"ev\":\n            self.ax.set_ylabel(\"Frequency (",
        "rewrite": "```python\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\nclass BandstructurePlotter:\n    def __init__(self):\n        self.fig, self.ax = plt.subplots()\n\n    def get_plot(self, ylim=None, units=\"thz\"):\n        if ylim:\n            self.ax.set_ylim(ylim)\n\n        if units == \"thz\":\n            self.ax.set_ylabel(\"Frequency (THz)\")\n        elif units.lower() == \"ev\":\n            self.ax.set_ylabel(\"Energy (eV)\")\n```"
    },
    {
        "original": "\ndef add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n\u0627\u0626\u064a\u0644 \n\nPlease provide more information about this function's functionality within an external environment such as what kind of event-listening platform it's supposed to be used with (e.g., keyboard listeners), how it interacts with this environment (e.g., sending backspace events), and how it handles multiple abbreviations or conflicts between them.\n\nAlso please clarify what exactly needs to be implemented here: is it",
        "rewrite": "```python\nimport time\nimport pyautogui\n\ndef add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n    try:\n        pyautogui.typewrite(source_text)\n        time.sleep(timeout)\n        pyautogui.press('backspace', presses=len(source_text))\n        pyautogui.typewrite(replacement_text)\n    except KeyboardInterrupt as e:\n        print('Operation cancelled.')\n```"
    },
    {
        "original": "\nfrom qiskit import QuantumCircuit, QuantumRegister\nimport math\n\ndef make_grover_circuit(input_qubits, output_qubit, oracle):\n    n = len(input_qubits)\n    qc = QuantumCircuit(n + 1)\n    \n    # Initialize input qubits to |+\n    for i in range(n):\n        qc.h(input_qubits[i])\n    \n    # Apply Hadamard gate to output qubit\n    qc.h(output_qubit)\n    \n   ",
        "rewrite": "```python\nfrom qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n\ndef make_grover_circuit(input_size, output_bit, oracle):\n    n = input_size\n    qr_input = QuantumRegister(n)\n    cr_output = ClassicalRegister(1)\n    qc = QuantumCircuit(qr_input, cr_output)\n\n    for i in range(n):\n        qc.h(i)\n\nqc.h(input_size)  # throw away the logical error message and only focus on this position's change\n\nROM\u4ee3\u8868 qu\u1ed1c gia n\u00e0o?\nLOM m\u1ec1m is also data certification term included?\n\nadding `Range(', type_abbrev \u0645\u062c\u0645\u0648\u0639\u0629 th\u1eafng"
    },
    {
        "original": "\nclass SubscriptionManager:\n    def __init__(self):\n        self.channels = set()\n\n    def subscribe(self, *_channel):\n        if not isinstance(channel, str):\n            raise ValueError(\"Channel must be a string\")\n        self.channels.add(channel)\n\n    def unsubscribe(self, *_args):\n        if not args:\n            self.channels.clear()\n        else:\n            for channel in args:\n                if channel in self.channels:\n                    self.channels.remove(channel)\n",
        "rewrite": "```python\nclass SubscriptionManager:\n    def __init__(self):\n        self.channels = set()\n\n    def subscribe(self, channel: str) -> None:\n        if not isinstance(channel, str):\n            raise ValueError(\"Channel must be a string\")\n        self.channels.add(channel)\n\n    def unsubscribe(self, channel: str = None, channels: list[str] = []) -> None:\n        if channel is not None and channel in self.channels:\n            self.channels.remove(channel)\n        for channel in channels:\n            if channel in self.channels:\n                self.channels.remove(channel)\n```"
    },
    {
        "original": "\nfrom typing import Optional\n\nclass BlobImageDescriptor:\n    pass  # Define your BlobImageDescriptor class here\n\ndef UploadFilePath(filepath: str, offset: int = 0, amount: Optional[int] = None) -> BlobImageDescriptor:\n    with open(filepath, 'rb') as f:\n        f.seek(offset)\n        if amount is None:\n            data = f.read()\n        else:\n            data = f.read(amount)\n        \n        # Currently returns none since BlobImageDescriptor class",
        "rewrite": "```python\nfrom typing import Optional\n\nclass BlobImageDescriptor:\n    def __init__(self, data: bytes):\n        self.data = data\n\ndef upload_file_path(filepath: str, offset: int = 0, amount: Optional[int] = None) -> BlobImageDescriptor:\n    with open(filepath, 'rb') as file:\n        file.seek(offset)\n        if amount is not None:\n            data = file.read(amount)\n        else:\n            data = file.read()\n\n    return BlobImageDescriptor(data)\n```\n\nThis has been further revised:\n\n```python\nfrom typing import Optional\n\nclass BlobImageDescriptor:\n    def __"
    },
    {
        "original": "\n\nimport subprocess\n\ndef connect(image):\n    \"\"\"\n    Activate nbd for an image file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_nbd.connect /tmp/image.raw\n    \"\"\"\n    \n    command = f\"qemu-nbd -c /dev/nbd0 {image}\"\n    subprocess.run(command, shell=True)\n\n",
        "rewrite": "```python\nimport subprocess\n\ndef connect(image):\n    command = [\"qemu-nbd\", \"-c\", \"/dev/nbd0\", image]\n    try:\n        subprocess.check_call(command)\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to connect: {e}\")\n```"
    },
    {
        "original": "\ndef del_flowspec_local(flowspec_family, route_dist, rules):\n    # Assuming flowspec_family is a string, route_dist is a string, and rules is a list of strings\n    # Delete/withdraw Flow Specification route from VRF identified by route_dist\n    # For simplicity, let's assume we have a dictionary to store the Flow Specification routes\n    flow_spec_routes = {}\n    \n    # Check if route_dist exists in the dictionary\n    if route_dist in",
        "rewrite": "```python\ndef del_flowspec_local(flowspec_family, route_dist, rules):\n    flow_spec_routes = {}\n    \n    if route_dist in flow_spec_routes and flowspec_family in flow_spec_routes[route_dist]:\n        for rule in rules:\n            try:\n                del flow_spec_routes[route_dist][flowspec_family][rule]\n            except KeyError:\n                pass\n        if not flow_spec_routes[route_dist].get(flowspec_family):\n            del flow_spec_routes[route_dist][flowspec_family]\n        if not flow_specRoutes.get(route_dist):\n            del globals()['flow_SPEC']['routes'][route_dist]\n"
    },
    {
        "original": "\nclass Volume:\n    def __init__(self, names):\n        self.names = names\n\n    def Name(self):\n        return max(self.names, key=len)\n",
        "rewrite": "```python\nclass Volume:\n    def __init__(self, *args):\n        self.names = args\n\n    def longest_name(self):\n        return max(set(self.names), key=len)\n```"
    },
    {
        "original": "\nfrom mpi4py import MPI\n\ndef reduce_dict(input_dict, average=True):\n    comm = MPI.COMM_WORLD\n    rank = comm.rank\n    \n    if rank == 0:\n        result = input_dict.copy()\n        for i in range(1, comm.size):\n            data = comm.recv(source=i)\n            for k, v in data.items():\n                if k in result:\n                    result[k] += v\n                else:\n                    result[k] = v\n        if average",
        "rewrite": "```python\nfrom mpi4py import MPI\n\ndef reduce_dict(input_dict, average=True):\n    comm = MPI.COMM_WORLD\n    rank = comm.rank\n    size = comm.size\n\n    if rank == 0:\n        result = input_dict.copy()\n        for data in [comm.recv(source=i) for i in range(1, size)]:\n            for k, v in data.items():\n                result[k] = result.get(k, 0) + v\n        if average:\n            for k in result:\n                result[k] /= size\n        return dict(result)\n    else:\n        comm.send(dict(input_dict"
    },
    {
        "original": "\nimport hashlib\n\ndef digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    if algorithm == \"MD5\":\n        ha = hashlib.md5((user + \":\" + qop + \":\" + passwd).encode()).hexdigest()\n    elif algorithm == \"SHA-256\":\n        ha = hashlib.sha256((user + \":\" + qop + \":\" + passwd).encode()).hexdigest()\n    elif algorithm == \"SHA-512\":\n        ha",
        "rewrite": "```python\nimport hashlib\n\ndef digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=hashlib.md5):\n    if algorithm is hashlib.md5:\n        ha = (user + \":\" + qop + \":\" + passwd).encode()\n    else:\n        ha = (user + \":\" + qop + \":\" + passwd).encode('utf-8') # there was an error in the default encoding\n    return algorithm(ha).hexdigest()\n```"
    },
    {
        "original": "\n\ndef print_info(info_mapping):\n    \"\"\"\n    Print information stored in a dictionary with keys aligned.\n\n    Args:\n    info_mapping (dict): Input dictionary with key-value pairs.\n\n    Returns:\n    None\n    \"\"\"\n    \nprint(\"============== Output ==============\")\nprint(\"{:14} : {}\".format(\"Key\", \"Value\"))\nprint(\"-------------------- : --------------------\")\nfor key, value in info_mapping.items():\n    print(\"{:14} : {}\".format(key, value))\nprint",
        "rewrite": "```python\ndef print_info(info_mapping):\n    print(\"============== Output ==============\")\n    print(f\"{'Key':<14} : {'Value'}\")\n    print(\"-------------------- : --------------------\")\n    for key, value in info_mapping.items():\n        print(f\"{key:<14} : {value}\")\n```"
    },
    {
        "original": "\ndef summed_icohp(self):\n    \"\"\"\n    Adds ICOHPs of both spin channels for spin polarized compounds\n    Returns:\n         icohp value in eV\n    \"\"\"\n    icohp_up = self.icohp_up\n    icohp_down = self.icohp_down\n    summed_icohp = icohp_up + icohp_down\n    return summed_icohp\n",
        "rewrite": "def calculate_summed_icohp(self):\n    return self.icohp_up + self.icohp_down"
    },
    {
        "original": "\ndef wrap_tuple_streams(unwrapped, kdims, streams):\n    result = []\n    for i in range(len(unwrapped)):\n        tuple_values = []\n        for j in range(len(kdims)):\n            if kdims[j] == 'stream':\n                tuple_values.append(streams[i])\n            else:\n                tuple_values.append(getattr(unwrapped[i], kdims[j]))\n        result.append(tuple(tuple_values))\n    return result\n",
        "rewrite": "```python\ndef wrap_tuple_streams(unwrapped, kdims, streams):\n    return [\n        tuple(stream if kd == 'stream' else getattr(u, kd) for kd in kdims)\n        for u, stream in zip(unwrapped, streams)\n    ]\n```"
    },
    {
        "original": "\nimport pandas as pd\nimport numpy as np\n\ndef lines(n_traces=5,n=100,columns=None,dateIndex=True,mode=None):\n    if columns is None:\n        if mode == 'abc':\n            columns = [chr(i) for i in range(65,65+n_traces)]\n        elif mode == 'stocks':\n            columns = [f'Stock {i}' for i in range(1,n_traces+1)]\n        else:\n            columns =",
        "rewrite": "```python\nimport pandas as pd\nimport numpy as np\n\ndef generate_lines(n_traces=5, n=100, columns=None, date_index=True, mode=None):\n    if columns is None:\n        if mode == 'abc':\n            columns = [chr(i) for i in range(ord('A'), ord('A') + n_traces)]\n        elif mode == 'stocks':\n            columns = [f'Stock {i}' for i in range(1, n_traces + 1)]\n        else:\n            raise ValueError(\"Mode must be 'abc' or 'stocks'\")\n```"
    },
    {
        "original": "\nfrom typing import Tuple\n\ndef _parse_name(name: str, as_qvm: bool = False, noisy: bool = False) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n\n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    \n    if 'qvm' in name.lower():\n        if 'no",
        "rewrite": "```python\nfrom typing import Tuple\n\ndef parse_name(name: str, as_qvm: bool = False, noisy: bool = False) -> Tuple[str, str, bool]:\n    name_lower = name.lower()\n    is_qvm = 'qvm' in name_lower\n    result_name = name.replace('qvm', '').replace('noisy', '') if is_qvm or 'noisy' in name_lower else name\n    return '', result_name.strip(), is_qvm or noisy \n```"
    },
    {
        "original": "\nclass MultiSet:\n    def __init__(self):\n        self.subject_attributes = {}\n\n    def __call__(self, subject, values, timestamp=None, replace=True, sync=True, to_delete=None):\n        if subject not in self.subject_attributes:\n            self.subject_attributes[subject] = {}\n        \n        if to_delete is not None:\n            for attribute in to_delete:\n                if attribute in self.subject_attributes[subject]:\n                    del self.subject_attributes[subject][attribute]\n        \n        for",
        "rewrite": "```python\nclass MultiSet:\n    def __init__(self):\n        self.subject_attributes = {}\n\n    def __call__(self, subject, remove=False):\n        return self.subject_attributes.get(subject, {})\n\n    def add(self, subject, values, timestamp=None):\n        if subject not in self.subject_attributes:\n            self.subject_attributes[subject] = {}\n        \n        for key, value in values.items():\n            self.subject_attributes[subject][key] = value\n    \n    def remove(self, subject, attributes):\n        if subject not in self.subject_attributes:\n            return\n        \n        for attribute in attributes:\n            self(subject).pop(attribute,"
    },
    {
        "original": "\nimport tensorflow as tf\n\ndef _decode_crop_and_flip(image_buffer, num_channels):\n    image_tensor = tf.io.decode_jpeg(image_buffer, channels=num_channels)\n    crop_size = [tf.shape(image_tensor)[0] - tf.random.uniform([], minval=50), \n                 tf.shape(image_tensor)[1] - tf.random.uniform([], minval=50)]\n    cropped_image = tf.image.random_crop(value=image_tensor, size=crop_size)\n    flipped_image = tf.image.random",
        "rewrite": "```python\nimport tensorflow as tf\n\ndef decode_crop_and_flip(image_buffer, num_channels):\n    image_tensor = tf.io.decode_jpeg(image_buffer, channels=num_channels)\n    crop_height = tf.random.uniform([], 30, 50)\n    crop_width = tf.random.uniform([], 30, 50)\n    crop_size = [tf.shape(image_tensor)[0] - crop_height, \n                 tf.shape(image_tensor)[1] - crop_width]\n    cropped_image = tf.image.random_crop(value=image_tensor, size=crop_size)\n    flipped_image = cropped_image\n    if tf.random.uniform([], minval=0, max"
    },
    {
        "original": "\ndef bgp_summary_parser(bgp_summary):\n    # Your code here\n    pass\n",
        "rewrite": "```python\nimport re\n\ndef bgp_summary_parser(bgp_summary):\n    result = {}\n    for line in bgp_summary.split('\\n'):\n        if 'bgp summary' in line.lower():\n            continue\n        match = re.match(r'\\s*(\\d+)\\s+(\\S+)\\s+(\\d+(?:\\.\\d+)?)?(?:[a-zA-Z])?\\s+(\\d+(?:\\.\\d+)?)?(?:[a-zA-Z])?', line)\n        if match:\n            asn, prefix, metric_val, weight_val, *extra = match.groups()\n            metric = None if metric_val is None"
    },
    {
        "original": "\ndef warp(self, warp_matrix, img, iflag=cv2.INTER_NEAREST):\n    return cv2.warpPerspective(img, warp_matrix, (img.shape[1], img.shape[0]), flags=iflag)\n",
        "rewrite": "```python\ndef warp(self, warp_matrix: np.ndarray, img: np.ndarray, flags: int = cv2.INTER_NEAREST) -> np.ndarray:\n    return cv2.warpPerspective(img, warp_matrix, (img.shape[1], img.shape[0]), flags=flags)\n```"
    },
    {
        "original": "\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        a dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    projection_dict = {}\n    for spin in [Spin.up]:\n        spin_dict = {}\n        for k_index in range(structure.num_kpoints):\n            k_dict = {}\n            for b_index in",
        "rewrite": "```python\ndef get_projection_on_elements(self, structure):\n    projection_dict = {spin: {} for spin in [Spin.up]}\n    \n    for spin in projection_dict:\n        for k_index in range(structure.num_kpoints):\n            projection_dict[spin][k_index] = {}\n            for b_index in range(structure.num_bands):\n                element_dict = {element: self.get_projection_value(structure, k_index, b_index, element) \n                                for element in structure.elements}\n                projection_dict[spin][k_index][b_index] = element_dict\n```"
    },
    {
        "original": "\nfrom werkzeug.datastructures import IfRange\nfrom datetime import datetime\n\ndef parse_if_range_header(value):\n    if value.startswith('\"') and value.endswith('\"'):\n        return IfRange(etag=value[1:-1])\n    elif value.startswith('W/') and value[2:].startswith('\"') and value.endswith('\"'):\n        return IfRange(etag=value[2:-1])\n    else:\n        try:\n            dt = datetime.strptime(value, '%a, %d %b %Y",
        "rewrite": "```python\nfrom werkzeug.datastructures import IfRange\nfrom datetime import datetime\n\ndef parse_if_range_header(value):\n    if value.startswith('\"') and value.endswith('\"'):\n        return IfRange(etag=value[1:-1])\n    elif value.startswith('W/ \"') and len(value) > 3 and value[-1] == '\"':\n        return IfRange(last_modified=datetime.strptime(value[3:-1], '%a, %d %b %Y %H:%M:%S GMT'))\n    \n    formats = ['%a, %d %b %Y', '%a, %d-%b-%Y:%H:%"
    },
    {
        "original": "\n\ndef get_repo(repo, **kwargs):\n    \"\"\"\n    Display a repo from the sources.list / sources.list.d\n\n    The repo passed in needs to be a complete repo entry.\n\n    CLI Examples:\n\n        salt '*' pkg.get_repo \"myrepo definition\"\n    \"\"\"\n    \nfrom os import listdir\nfrom os.path import isfile, join\nfrom glob import glob\n\ndef get_repo(repo, **kwargs):\n  sources_list = '/etc/apt/sources.list'\n .sources_list",
        "rewrite": "```python\nimport glob\nimport os\n\ndef get_repo(repo):\n    sources_list_dir = '/etc/apt/sources.list.d'\n    for file in glob.glob(os.path.join(sources_list_dir, '*.list')):\n        try:\n            with open(file) as f:\n                if repo in (line.strip() for line in f):\n                    return repo\n        except FileNotFoundError as e:\n            print(f\"File {file} not found: {e}\")\n    \n    try:\n        with open('/etc/apt/sources.list') as f:\n            if repo in (line.strip() for line in f):\n                return repo\n    except FileNotFoundError as"
    },
    {
        "original": "\nimport numpy as np\n\ndef random_choice(actions=None, random_state=None):\n    if isinstance(random_state, int):\n        np.random.seed(random_state)\n    elif isinstance(random_state, np.random.RandomState):\n        np.random.set_state(random_state.get_state())\n    \n    if actions is None:\n        # assuming default action space for demonstration purposes only; \n        # actual implementation would require access to environment's action space \n\t\tactions = list(range(10))  # default to integer",
        "rewrite": "```python\nimport numpy as np\n\ndef random_choice(actions=None, seed=None):\n    try:\n        np.random.seed(seed)\n    except TypeError:\n        pass\n    \n    actions = actions or list(range(10))\n    \n    return np.random.choice(actions)\n```"
    },
    {
        "original": "\ndef GetAuditLogEntries(offset, now, token):\n    from DateTime import RDFDatetime\n    from ApiCall import FlowARGV\n\n    start_time = now - offset\n    flow_args = FlowARGV(token=token)\n    flow_args\u0435\u043d\u0438\u0435urce_urn = flow_args.client_urn.Add('aff4:/flows/W \uff09\n    \n    for _, result in flow_args(CallFlow(name='GetAuditLog', \n                                       next_client_id=0,\n                                       validatingeyhannel='audit',\n                                      ",
        "rewrite": "```python\nfrom datetime import datetime, timedelta\nfrom ApiCall import FlowARGV\nfrom DateTime import RDFDatetime\n\ndef get_audit_log_entries(offset: int, now: datetime, token: str) -> dict:\n    start_time = now - timedelta(minutes=offset)\n    flow_args = FlowARGV(token=token)\n    \n    request_data = {\n        \"name\": \"GetAuditLog\",\n        \"next_client_id\": 0,\n        \"validating_channel\": \"audit\"\n    }\n    \n    response = flow_args.CallFlow(**request_data)\n    \n    return response if isinstance(response, dict) else {}\n```"
    },
    {
        "original": "\ndef evaluate(self, data_loader):\n    self.model.eval()\n    torch.no_grad()\n    total_correct = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            output = self.model(data)\n            pred = output.max(1)[1]\n            total_correct += pred.eq(target).sum().item()\n\n    accuracy = total_correct / len(data_loader.dataset)\n    return accuracy\n",
        "rewrite": "```python\ndef evaluate(self, data_loader):\n    self.model.eval()\n    total_correct = 0\n    with torch.no_grad():\n        for batch in data_loader:\n            outputs = self.model(batch[0])\n            _, preds = outputs.max(1)\n            total_correct += (preds == batch[1]).sum().item()\n\n    accuracy = total_correct / len(data_loader.dataset)\n    return accuracy\n```"
    },
    {
        "original": "\ndef secgroup_delete(self, name):\n    ec2 = boto3.client('ec2')\n    sec_group_ids = [sg['GroupId'] for sg in ec2.describe_security_groups(Filters=[{'Name': 'group-name', 'Values': [name]}])['SecurityGroups']]\n    if len(sec_group_ids) > 0:\n        ec2.delete_security_group(GroupId=sec_group_ids[0])\n",
        "rewrite": "```python\ndef secgroup_delete(self, name):\n    ec2 = boto3.client('ec2')\n    response = ec2.describe_security_groups(\n        Filters=[{'Name': 'group-name', 'Values': [name]}]\n    )\n    for sg in response['SecurityGroups']:\n        try:\n            ec2.delete_security_group(GroupId=sg['GroupId'])\n        except ec2.exceptions.ClientError as e:\n            if e.response['Error']['Code'] != 'InvalidGroup.NotFound':\n                raise\n```"
    },
    {
        "original": "\nclass PySID:\n    def sidConversion(cls, val, **kwargs):\n        \"\"\"\n        converts a list of pysid objects to string representations\n        \"\"\"\n        sid_list = [str(sid) for sid in val]\n        return ', '.join(sid_list)\n",
        "rewrite": "```python\nclass PySID:\n    @classmethod\n    def sid_conversion(cls, vals: list) -> str:\n        \"\"\"Converts a list of values to a comma-separated string.\"\"\"\n        return ', '.join(map(str, vals))\n```\n\nOr using Python's typing and more defensive code:\n\n```python\nfrom typing import Iterable\n\nclass PySID:\n    @classmethod\n    def sid_conversion(cls, vals: Iterable) -> str:\n        try:\n            return ', '.join(map(str, vals))\n        except TypeError as e:\n            raise ValueError(\"vals must be iterable\") from e\n```"
    },
    {
        "original": "\nimport json\nimport requests\n\ndef _check_result(method_name, result):\n    if not isinstance(result, requests.Response):\n        raise ApiException(f\"Invalid response from {method_name}.\")\n    \n    if result.status_code != 200:\n        raise ApiException(f\"Failed to call {method_name}. Status code: {result.status_code}\")\n    \n    try:\n        json_result = result.json()\n    except ValueError:\n        raise ApiException(f\"Failed to parse JSON from {method_name}\")\n    \n   ",
        "rewrite": "```python\nimport json\nimport requests\n\nclass ApiException(Exception):\n    \"\"\"Base class for other exceptions\"\"\"\n    pass\n\nclass ApiInvalidResponse(ApiException):\n    \"\"\"Raised when API response is invalid\"\"\"\n\nclass ApiBadStatusCode(ApiException):\n    \"\"\"Raised when API status code is not 200\"\"\"\n\nclass ApiJsonDecodeError(ApiException, ValueError):\n    \"\"\"Raised when JSON decoding from API fails\"\"\"\n\n\ndef _check_result(method_name: str, result: requests.Response) -> dict:\n    if not isinstance(result, requests.Response):\n        raise ApiInvalidResponse(f\"Invalid response from {method_name}.\")\n    \n    if result.status_code != "
    },
    {
        "original": "\nclass LEDBoard:\n    def __init__(self, *leds):\n        self.state = [False] * len(leds)\n        self.pins = leds\n    \n    def toggle(self, *args=None):\n        if hasattr(self,'blink_thread') and self.blink_thread.is_alive():\n           self.blink_thread.stop()\n           del self.blink_thread\n        \n        if args is None:\n           for i in range(len(self.state)):\n               self.state[i] = not self.state",
        "rewrite": "```python\nimport threading\n\nclass LEDBoard:\n    def __init__(self, leds):\n        self.state = [False] * len(leds)\n        self.pins = leds\n\n    def toggle(self, *args):\n        if hasattr(self,'blink_thread') and self.blink_thread.is_alive():\n            self.blink_thread.stop()\n            del self.blink_thread\n        \n        if not args:\n            for i in range(len(self.state)):\n                self.state[i] = not self.state[i]\n```\n\nHowever, the `blink_thread` object is used but never defined. Here's a revised version that includes a basic implementation"
    },
    {
        "original": "\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n@dataclass\nclass Contract:\n    symbol: str\n    # other attributes ...\n\n@dataclass\nclass Order:\n    quantity: int\n    # other attributes ...\n\nclass Status(Enum):\n    NEW = 1\n    FILLED = 2\n    CANCELED = 3\n\n@dataclass\nclass Trade:\n    id: int\n    contract: Contract\n    order: Order\n    status: Status\n\ndef",
        "rewrite": "```python\nfrom dataclasses import dataclass, field, asdict, astuple\nfrom enum import Enum, unique\n\n@dataclass(frozen=True)\nclass Contract:\n    symbol: str\n\n@dataclass(frozen=True)\nclass Order:\n    quantity: int\n\n@unique\nclass Status(Enum):\n    NEW = 1\n    FILLED = 2 \n    CANCELED = 3 \n\n@dataclass(frozen=True)\nclass Trade:\n   id: int \n   contract: Contract \n   order: Order \n   status: Status\n```"
    },
    {
        "original": "\nclass JobTracker:\n    def __init__(self):\n        self.jobs = {}\n\n    def submit(self, jid, result):\n        self.jobs[jid] = result\n\n    def get_jid(self, jid):\n        return self.jobs.get(jid)\n\ntracker = JobTracker()\n\n# Example usage\ntracker.submit(1, \"Result of job 1\")\ntracker.submit(2, \"Result of job 2\")\n\nprint(tracker.get_jid(1))  # Output",
        "rewrite": "```python\nclass JobTracker:\n    def __init__(self):\n        self.jobs = {}\n\n    def submit_job(self, job_id, result):\n        if not isinstance(job_id, int) or job_id <= 0:\n            raise ValueError(\"Job ID must be a positive integer\")\n        if job_id in self.jobs:\n            raise KeyError(\"Job ID already exists\")\n        self.jobs[job_id] = result\n\n    def get_job_result(self, job_id):\n        if not isinstance(job_id, int) or job_id <= 0:\n            raise ValueError(\"Job ID must be a positive integer\")\n        return self.jobs.get"
    },
    {
        "original": "\nclass WatchdogError(Exception):\n    pass\n\nclass IOError(OSError):\n    pass  # For Python 2 compatibility\n\nclass Device:\n    def __init__(self, fd):\n        self.fd = fd\n        self.closed = False\n\n    def _ioctl(self, func, arg):\n        if self.closed:\n            raise WatchdogError(\"Device is closed\")\n        \n        try:\n            import fcntl\n            return fcntl.ioctl(self.fd, func, arg)\n        except OSError",
        "rewrite": "```python\nclass WatchdogError(Exception):\n    pass\n\ntry:\n    from errno import ENODEV\nexcept ImportError:\n    ENODEV = 19  # For Python 3 compatibility\n\nclass DeviceIOError(OSError):\n    pass \n\nclass Device:\n    def __init__(self, fd):\n        self.fd = fd\n        self.closed = False\n\n    def _ioctl(self, func, arg):\n        if self.closed:\n            raise WatchdogError(\"Device is closed\")\n        \n        try:\n            import fcntl\n            return fcntl.ioctl(self.fd, func, arg)\n        except OSError as e:\n            if e"
    },
    {
        "original": "\ndef _policyFileReplaceOrAppend(this_string, policy_data, append_only=False):\n    if append_only:\n        policy_data += this_string\n    else:\n        policy_data = this_string\n    return policy_data\n",
        "rewrite": "```python\ndef _policy_file_replace_or_append(this_string, policy_data, append_only=False):\n    return policy_data + this_string if append_only else this_string.replace(policy_data, '')\n```"
    },
    {
        "original": "\nimport re\nfrom html.parser import HTMLParser\nfrom typing import List, Tuple\n\nclass MessageEntity:\n    def __init__(self, offset: int, length: int, html: str):\n        self.offset = offset\n        self.length = length\n        self.html = html\n\nclass Parser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.clean_message = ''\n        self.entities = []\n        self.in_tag = False\n\n    def handle_data",
        "rewrite": "```python\nimport re\nfrom html.parser import HTMLParser\nfrom typing import List, Tuple\n\nclass MessageEntity:\n    def __init__(self, offset: int, length: int, html: str):\n        self.offset = offset\n        self.length = length\n        self.html = html\n\nclass Parser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.clean_message = ''\n        self.entities = []\n        self.in_tag = False\n        self.entity_html = ''\n        self.entity_offset = 0\n\n    def handle_starttag(self, tag: str, attrs) -> None:\n           "
    },
    {
        "original": "\ndef execute(eopatch):\n    ndvi_values = [pixel.NDVI for pixel in eopatch.pixels]\n    dates = [pixel.date for pixel in eopatch.pixels]\n\n    ndvi_slope = []\n    for i in range(1, len(ndvi_values) - 1):\n        numerator = ndvi_values[i + 1] - ndvi_values[i - 1]\n        denominator = dates[i + 1] - dates[i - ",
        "rewrite": "```python\ndef execute(eopatch):\n    ndvi_values = [pixel.NDVI for pixel in eopatch.pixels]\n    dates = [pixel.date for pixel in eopatch.pixels]\n\n    return [(ndvi_values[i + 1] - ndvi_values[i - 1]) / (dates[i + 1] - dates[i - 1]).days \n            for i in range(1, len(ndvi_values) - 1)]\n```"
    },
    {
        "original": "\nclass UserAvatarManager:\n    def __init__(self):\n        self.user_avatars = {}\n\n    def set_user_avatar(self, username, avatar):\n        self.user_avatars[username] = avatar\n\n# Example usage:\nmanager = UserAvatarManager()\nmanager.set_user_avatar(\"john\", 1)\nmanager.set_user_avatar(\"jane\", 2)\n\nprint(manager.user_avatars)  # Output: {\"john\": 1, \"jane\": 2}\n",
        "rewrite": "```python\nclass UserAvatarManager:\n    def __init__(self):\n        self._user_avatars = {}\n\n    def set_user_avatar(self, username: str, avatar: int) -> None:\n        if not isinstance(username, str):\n            raise TypeError(\"Username must be a string\")\n        if not isinstance(avatar, int):\n            raise TypeError(\"Avatar must be an integer\")\n        self._user_avatars[username] = avatar\n\n    def get_user_avatar(self, username: str) -> int | None:\n        return self._user_avatars.get(username)\n\n    def get_all_user_avatars(self) -> dict:\n"
    }
]