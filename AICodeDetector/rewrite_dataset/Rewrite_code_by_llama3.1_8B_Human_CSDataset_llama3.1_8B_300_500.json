[
    {
        "original": "def getPluginsList(self, enable=True):\n        \"\"\"Return the plugins list.\n\n        if enable is True, only return the active plugins (default)\n        if enable is False, return all the plugins\n\n        Return: list of plugin name\n        \"\"\"\n        if enable:\n            return [p for p in self._plugins if self._plugins[p].is_enable()]\n        else:\n            return [p for p in self._plugins]",
        "rewrite": "```python\ndef get_plugins_list(self, enable=True):\n    return [p for p in self._plugins if (enable and self._plugins[p].is_enable()) or not enable]\n```"
    },
    {
        "original": "def get_nb_strings(self):\n        \"\"\"\n        Return the total number of strings in all Analysis objects\n        \"\"\"\n        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb",
        "rewrite": "```python\ndef get_nb_strings(self):\n    nb = 0\n    seen = set()\n    for digest, dx in self.analyzed_vms.items():\n        if dx not in seen:\n            seen.add(dx)\n            nb += len(dx.get_strings_analysis())\n    return nb\n```"
    },
    {
        "original": "def endian_swap_words(source):\n    \"\"\" Endian-swap each word in 'source' bitstring \"\"\"\n    assert len(source) % 4 == 0\n    words = \"I\" * (len(source) // 4)\n    return struct.pack(\"<\" + words, *struct.unpack(\">\" + words, source))",
        "rewrite": "```python\nimport struct\n\ndef endian_swap_words(source):\n    assert len(source) % 4 == 0\n    words = ''.join('i' for _ in range(len(source) // 4))\n    return struct.pack(\"<\" + words, *struct.unpack(\">\" + words, source.encode()))\n```"
    },
    {
        "original": "def GetValueRepresentation(cls, value,\n                             version=sorted(_SERVICE_MAP.keys())[-1]):\n    \"\"\"Converts a single python value to its PQL representation.\n\n    Args:\n      value: A python value.\n      version: A string identifying the Ad Manager version the value object\n          is compatible with. This defaults to what is currently the latest\n          version. This will be updated in future releases to point to what is\n          then the latest version.\n\n    Returns:\n      The value formatted for PQL statements which are compatible with a\n      particular API version.\n    \"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      return {'value': value, 'xsi_type': 'TextValue'}\n    elif isinstance(value, bool):\n      return {'value': value, 'xsi_type': 'BooleanValue'}\n    elif isinstance(value, numbers.Number):\n      return {'value': value, 'xsi_type': 'NumberValue'}\n    # It's important that datetime is checked for before date\n    # because isinstance(datetime.datetime.now(), datetime.date) is True\n    elif isinstance(value, datetime.datetime):\n      if value.tzinfo is None:\n        raise googleads.errors.GoogleAdsValueError(\n            'Datetime %s is not timezone aware.' % value\n        )\n\n      return {\n          'xsi_type': 'DateTimeValue',\n          'value': {\n              'date': {\n                  'year': value.year,\n                  'month': value.month,\n                  'day': value.day,\n              },\n              'hour': value.hour,\n              'minute': value.minute,\n              'second': value.second,\n              'timeZoneId' if version >= 'v201811' else 'timeZoneID':\n                  value.tzinfo.zone,\n          }\n      }\n    elif isinstance(value, datetime.date):\n      return {\n          'xsi_type': 'DateValue',\n          'value': {\n              'year': value.year,\n              'month': value.month,\n              'day': value.day,\n          }\n      }\n    elif isinstance(value, list):\n      if value and not all(isinstance(x, type(value[0])) for x in value):\n        raise googleads.errors.GoogleAdsValueError('Cannot pass more than one '\n                                                   'type in a set.')\n\n      return {\n          'xsi_type': 'SetValue',\n          'values': [cls.GetValueRepresentation(v, version) for v in value]\n      }\n    else:\n      raise googleads.errors.GoogleAdsValueError(\n          'Can\\'t represent unknown type: %s.' % type(value))",
        "rewrite": "```python\nfrom dataclasses import asdict\nimport googleads\nimport datetime\nfrom typing import Union, List\n\ndef get_value_representation(cls, value: Union[bool, str, int, float,\n                                               datetime.datetime,\n                                               datetime.date,\n                                               List]) -> dict:\n    if isinstance(value, str):\n        return {'value': value, 'xsi_type': 'TextValue'}\n    elif isinstance(value, bool):\n        return {'value': value, 'xsi_type': 'BooleanValue'}\n    elif isinstance(value, (int, float)):\n        return {'value': value.as_integer_ratio()[0], 'xsi_type"
    },
    {
        "original": "def CheckTemplates(self, base_dir, version):\n    \"\"\"Verify we have at least one template that matches maj.minor version.\"\"\"\n    major_minor = \".\".join(version.split(\".\")[0:2])\n    templates = glob.glob(\n        os.path.join(base_dir, \"templates/*%s*.zip\" % major_minor))\n    required_templates = set(\n        [x.replace(\"maj.minor\", major_minor) for x in self.REQUIRED_TEMPLATES])\n\n    # Client templates have an extra version digit, e.g. 3.1.0.0\n    templates_present = set([\n        re.sub(r\"_%s[^_]+_\" % major_minor, \"_%s_\" % major_minor,\n               os.path.basename(x)) for x in templates\n    ])\n\n    difference = required_templates - templates_present\n    if difference:\n      raise RuntimeError(\"Missing templates %s\" % difference)",
        "rewrite": "```python\ndef CheckTemplates(self, base_dir, version):\n    \"\"\"Verify we have at least one template that matches maj.minor version.\"\"\"\n    major_minor = \".\".join(version.split(\".\")[0:2])\n    templates = [x for x in glob.glob(\n        os.path.join(base_dir, \"templates/*%s*.zip\" % major_minor))]\n\n    required_templates = set(left.replace(\"maj.minor\", major_minor)\n                            for left in self.REQUIRED_TEMPLATES)\n\n    # Client templates have an extra version digit, e.g. 3.1.0.0\n    templates_present = set"
    },
    {
        "original": "def _get_cluster_sizes(self):\n        \"\"\"\n        Returns the marker size (in points, e.g. area of the circle) based on\n        the scores, using the prop_to_size scaling mechanism.\n        \"\"\"\n        # NOTE: log and power are hardcoded, should we allow the user to specify?\n        return prop_to_size(\n            self.scores_, mi=self.min_size, ma=self.max_size, log=False, power=0.5\n        )",
        "rewrite": "```python\ndef _get_cluster_sizes(self):\n    return prop_to_size(\n        self.scores_, \n        min_size=self.min_size, \n        max_size=self.max_size, \n        log=False, \n        power=0.5\n    )\n```"
    },
    {
        "original": "def _build_likelihood(self):\n        \"\"\"\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood. For a derivation of the terms in here, see the associated\n        SGPR notebook.\n        \"\"\"\n\n        num_inducing = len(self.feature)\n        num_data = tf.cast(tf.shape(self.Y)[0], settings.float_type)\n        output_dim = tf.cast(tf.shape(self.Y)[1], settings.float_type)\n\n        err = self.Y - self.mean_function(self.X)\n        Kdiag = self.kern.Kdiag(self.X)\n        Kuf = features.Kuf(self.feature, self.kern, self.X)\n        Kuu = features.Kuu(self.feature, self.kern, jitter=settings.numerics.jitter_level)\n        L = tf.cholesky(Kuu)\n        sigma = tf.sqrt(self.likelihood.variance)\n\n        # Compute intermediate matrices\n        A = tf.matrix_triangular_solve(L, Kuf, lower=True) / sigma\n        AAT = tf.matmul(A, A, transpose_b=True)\n        B = AAT + tf.eye(num_inducing, dtype=settings.float_type)\n        LB = tf.cholesky(B)\n        Aerr = tf.matmul(A, err)\n        c = tf.matrix_triangular_solve(LB, Aerr, lower=True) / sigma\n\n        # compute log marginal bound\n        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)\n        bound += tf.negative(output_dim) * tf.reduce_sum(tf.log(tf.matrix_diag_part(LB)))\n        bound -= 0.5 * num_data * output_dim * tf.log(self.likelihood.variance)\n        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance\n        bound += 0.5 * tf.reduce_sum(tf.square(c))\n        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance\n        bound += 0.5 * output_dim * tf.reduce_sum(tf.matrix_diag_part(AAT))\n\n        return bound",
        "rewrite": "```python\ndef _build_likelihood(self):\n    num_inducing = len(self.feature)\n    num_data = tf.cast(tf.shape(self.Y)[0], settings.float_type)\n    output_dim = tf.cast(tf.shape(self.Y)[1], settings.float_type)\n\n    err = self.Y - self.mean_function(self.X)\n    Kdiag = self.kern.Kdiag(self.X)\n    Kuf = features.Kuf(self.feature, self.kern, self.X)\n    Kuu = features.Kuu(self.feature, self.kern, jitter=settings.numerics.jitter_level)\n    L = tf.cholesky(Kuu)\n\n   "
    },
    {
        "original": "def updateRouterStatus(self):\n        \"\"\"force update to router as if there is child id request\"\"\"\n        print '%s call updateRouterStatus' % self.port\n        cmd = 'state'\n        while True:\n            state = self.__sendCommand(cmd)[0]\n            if state == 'detached':\n                continue\n            elif state == 'child':\n                break\n            else:\n                return False\n\n        cmd = 'state router'\n        return self.__sendCommand(cmd)[0] == 'Done'",
        "rewrite": "```python\ndef update_router_status(self) -> bool:\n    print(f\"{self.port} call update_router_status\")\n    while True:\n        state = self.__send_command('state')\n        if state == 'detached':\n            continue\n        elif state == 'child':\n            break\n        else:\n            return False\n\n    return self.__send_command('state router')[0] == 'Done'\n```"
    },
    {
        "original": "def put_group_policy(group_name, policy_name, policy_json, region=None, key=None,\n                     keyid=None, profile=None):\n    \"\"\"\n    Adds or updates the specified policy document for the specified group.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.put_group_policy mygroup policyname policyrules\n    \"\"\"\n    group = get_group(group_name, region=region, key=key, keyid=keyid,\n                      profile=profile)\n    if not group:\n        log.error('Group %s does not exist', group_name)\n        return False\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        if not isinstance(policy_json, six.string_types):\n            policy_json = salt.utils.json.dumps(policy_json)\n        created = conn.put_group_policy(group_name, policy_name,\n                                        policy_json)\n        if created:\n            log.info('Created policy for IAM group %s.', group_name)\n            return True\n        log.error('Could not create policy for IAM group %s', group_name)\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        log.error('Failed to create policy for IAM group %s', group_name)\n    return False",
        "rewrite": "```python\nimport salt.utils.json\nimport salt.log\n\ndef put_group_policy(group_name, policy_name, policy_json, region=None, key=None,\n                     keyid=None, profile=None):\n    group = get_group(group_name, region=region, key=key, keyid=keyid,\n                      profile=profile)\n    if not group:\n        salt.log.error('Group %s does not exist', group_name)\n        return False\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        if not isinstance(policy_json, str):\n            policy_json = salt.utils"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'language') and self.language is not None:\n            _dict['language'] = self.language\n        if hasattr(self, 'analyzed_text') and self.analyzed_text is not None:\n            _dict['analyzed_text'] = self.analyzed_text\n        if hasattr(self, 'retrieved_url') and self.retrieved_url is not None:\n            _dict['retrieved_url'] = self.retrieved_url\n        if hasattr(self, 'usage') and self.usage is not None:\n            _dict['usage'] = self.usage._to_dict()\n        if hasattr(self, 'concepts') and self.concepts is not None:\n            _dict['concepts'] = [x._to_dict() for x in self.concepts]\n        if hasattr(self, 'entities') and self.entities is not None:\n            _dict['entities'] = [x._to_dict() for x in self.entities]\n        if hasattr(self, 'keywords') and self.keywords is not None:\n            _dict['keywords'] = [x._to_dict() for x in self.keywords]\n        if hasattr(self, 'categories') and self.categories is not None:\n            _dict['categories'] = [x._to_dict() for x in self.categories]\n        if hasattr(self, 'emotion') and self.emotion is not None:\n            _dict['emotion'] = self.emotion._to_dict()\n        if hasattr(self, 'metadata') and self.metadata is not None:\n            _dict['metadata'] = self.metadata._to_dict()\n        if hasattr(self, 'relations') and self.relations is not None:\n            _dict['relations'] = [x._to_dict() for x in self.relations]\n        if hasattr(self, 'semantic_roles') and self.semantic_roles is not None:\n            _dict['semantic_roles'] = [\n                x._to_dict() for x in self.semantic_roles\n            ]\n        if hasattr(self, 'sentiment') and self.sentiment is not None:\n            _dict['sentiment'] = self.sentiment._to_dict()\n        if hasattr(self, 'syntax') and self.syntax is not None:\n            _dict['syntax'] = self.syntax._to_dict()\n        return _dict",
        "rewrite": "```python\ndef _to_dict(self):\n    _dict = {}\n    for attr in dir(self):\n        if not attr.startswith('_') and not callable(getattr(self, attr)):\n            value = getattr(self, attr)\n            if isinstance(value, list) and all(hasattr(x, '_to_dict') for x in value):\n                _dict[attr] = [x._to_dict() for x in value]\n            elif hasattr(value, '_to_dict'):\n                _dict[attr] = value._to_dict()\n            else:\n                _dict[attr] = value\n    return _dict\n```"
    },
    {
        "original": "def _pack(cls, tensors):\n    \"\"\"Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\"\"\"\n    if not tensors:\n      return None\n    elif len(tensors) == 1:\n      return array_ops.reshape(tensors[0], [-1])\n    else:\n      flattened = [array_ops.reshape(tensor, [-1]) for tensor in tensors]\n      return array_ops.concat(flattened, 0)",
        "rewrite": "```python\ndef _pack(cls, tensors):\n    if not tensors:\n        return None\n    elif len(tensors) == 1:\n        return tf.reshape(tensors[0], [-1])\n    else:\n        flattened = [tf.reshape(tensor, [-1]) for tensor in tensors]\n        return tf.concat(flattened, axis=0)\n```"
    },
    {
        "original": "def arg_locs(self, is_fp=None, sizes=None):\n        \"\"\"\n        Pass this a list of whether each parameter is floating-point or not, and get back a list of\n        SimFunctionArguments. Optionally, pass a list of argument sizes (in bytes) as well.\n\n        If you've customized this CC, this will sanity-check the provided locations with the given list.\n        \"\"\"\n        session = self.arg_session\n        if self.func_ty is None:\n            # No function prototype is provided. `is_fp` must be provided.\n            if is_fp is None:\n                raise ValueError('\"is_fp\" must be provided when no function prototype is available.')\n        else:\n            # let's rely on the func_ty for the number of arguments and whether each argument is FP or not\n            is_fp = [ True if isinstance(arg, (SimTypeFloat, SimTypeDouble)) else False for arg in self.func_ty.args ]\n\n        if sizes is None: sizes = [self.arch.bytes] * len(is_fp)\n        return [session.next_arg(ifp, size=sz) for ifp, sz in zip(is_fp, sizes)]",
        "rewrite": "```python\ndef arg_locs(self, is_fp=None, sizes=None):\n    session = self.arg_session\n    if self.func_ty is None:\n        if is_fp is None:\n            raise ValueError('\"is_fp\" must be provided when no function prototype is available.')\n        else:\n            # let's rely on the func_ty for the number of arguments and whether each argument is FP or not\n            num_args = len(is_fp)\n    else:\n        num_args = len(self.func_ty.args)\n        ifp_list = [True if isinstance(arg, (SimTypeFloat, SimTypeDouble)) else False for arg in self.func"
    },
    {
        "original": "def build(format='qcow2', path='/tmp/'):\n    \"\"\"\n    Build an image from a current system description.\n    The image is a system image can be output in bootable ISO or QCOW2 formats.\n\n    Node uses the image building library Kiwi to perform the actual build.\n\n    Parameters:\n\n    * **format**: Specifies output format: \"qcow2\" or \"iso. Default: `qcow2`.\n    * **path**: Specifies output path where to store built image. Default: `/tmp`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion inspector.build\n        salt myminion inspector.build format=iso path=/opt/builds/\n    \"\"\"\n    try:\n        _(\"collector\").Inspector(cachedir=__opts__['cachedir'],\n                                 piddir=os.path.dirname(__opts__['pidfile']),\n                                 pidfilename='').reuse_snapshot().build(format=format, path=path)\n    except InspectorKiwiProcessorException as ex:\n        raise CommandExecutionError(ex)\n    except Exception as ex:\n        log.error(_get_error_message(ex))\n        raise Exception(ex)",
        "rewrite": "```python\ndef build(format='qcow2', path='/tmp'):\n    try:\n        from salt_formula.collector_inspector import Inspector\n        from salt.exceptions import CommandExecutionError\n\n        collector = Inspector(cachedir=__opts__['cachedir'],\n                                piddir=os.path.dirname(__opts__['pidfile']),\n                                pidfilename='')\n        collector.reuse_snapshot()\n        collector.build(format=format, path=path)\n    except Exception as ex:\n        log.error(str(ex))\n        raise Exception(str(ex))\n```"
    },
    {
        "original": "def get_or_add_image(self, image_descriptor):\n        \"\"\"Return (rId, image) pair for image identified by *image_descriptor*.\n\n        *rId* is the str key (often like \"rId7\") for the relationship between this story\n        part and the image part, reused if already present, newly created if not.\n        *image* is an |Image| instance providing access to the properties of the image,\n        such as dimensions and image type.\n        \"\"\"\n        image_part = self._package.get_or_add_image_part(image_descriptor)\n        rId = self.relate_to(image_part, RT.IMAGE)\n        return rId, image_part.image",
        "rewrite": "```python\ndef get_or_add_image(self, image_descriptor):\n    return self._package.get_or_add_image_part(image_descriptor), self.relate_to(self._package.get_or_add_image_part(image_descriptor), RT.IMAGE)\n```"
    },
    {
        "original": "def set_tags(name=None,\n             tags=None,\n             call=None,\n             location=None,\n             instance_id=None,\n             resource_id=None,\n             kwargs=None):  # pylint: disable=W0613\n    \"\"\"\n    Set tags for a resource. Normally a VM name or instance_id is passed in,\n    but a resource_id may be passed instead. If both are passed in, the\n    instance_id will be used.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -a set_tags mymachine tag1=somestuff tag2='Other stuff'\n        salt-cloud -a set_tags resource_id=vol-3267ab32 tag=somestuff\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n\n    if location is None:\n        location = get_location()\n\n    if instance_id is None:\n        if 'resource_id' in kwargs:\n            resource_id = kwargs['resource_id']\n            del kwargs['resource_id']\n\n        if 'instance_id' in kwargs:\n            instance_id = kwargs['instance_id']\n            del kwargs['instance_id']\n\n        if resource_id is None:\n            if instance_id is None:\n                instance_id = _get_node(name=name, instance_id=None, location=location)['instanceId']\n        else:\n            instance_id = resource_id\n\n    # This second check is a safety, in case the above still failed to produce\n    # a usable ID\n    if instance_id is None:\n        return {\n            'Error': 'A valid instance_id or resource_id was not specified.'\n        }\n\n    params = {'Action': 'CreateTags',\n              'ResourceId.1': instance_id}\n\n    log.debug('Tags to set for %s: %s', name, tags)\n\n    if kwargs and not tags:\n        tags = kwargs\n\n    for idx, (tag_k, tag_v) in enumerate(six.iteritems(tags)):\n        params['Tag.{0}.Key'.format(idx)] = tag_k\n        params['Tag.{0}.Value'.format(idx)] = tag_v\n\n    attempts = 0\n    while attempts < aws.AWS_MAX_RETRIES:\n        aws.query(params,\n                           setname='tagSet',\n                           location=location,\n                           provider=get_provider(),\n                           opts=__opts__,\n                           sigver='4')\n\n        settags = get_tags(\n            instance_id=instance_id, call='action', location=location\n        )\n\n        log.debug('Setting the tags returned: %s', settags)\n\n        failed_to_set_tags = False\n        for tag in settags:\n            if tag['key'] not in tags:\n                # We were not setting this tag\n                continue\n\n            if tag.get('value') is None and tags.get(tag['key']) == '':\n                # This is a correctly set tag with no value\n                continue\n\n            if six.text_type(tags.get(tag['key'])) != six.text_type(tag['value']):\n                # Not set to the proper value!?\n                log.debug(\n                    'Setting the tag %s returned %s instead of %s',\n                    tag['key'], tags.get(tag['key']), tag['value']\n                )\n                failed_to_set_tags = True\n                break\n\n        if failed_to_set_tags:\n            log.warning('Failed to set tags. Remaining attempts %s', attempts)\n            attempts += 1\n            aws.sleep_exponential_backoff(attempts)\n            continue\n\n        return settags\n\n    raise SaltCloudSystemExit(\n        'Failed to set tags on {0}!'.format(name)\n    )",
        "rewrite": "```python\ndef set_tags(name=None, tags=None, call=None, location=None, instance_id=None,\n             resource_id=None, kwargs=None):\n    \"\"\"\n    Set tags for a resource.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -a set_tags mymachine tag1=somestuff tag2='Other stuff'\n        salt-cloud -a set_tags resource_id=vol-3267ab32 tag=somestuff\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n\n    if location is None:\n        location = get_location()\n\n    if instance_id is None:\n        if 'resource"
    },
    {
        "original": "def add_pool_member(hostname, username, password, name, member):\n    \"\"\"\n    A function to connect to a bigip device and add a new member to an existing pool.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    name\n        The name of the pool to modify\n    member\n        The member to add to the pool\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': member\n        }\n        )\n\n    #is this pool member currently configured?\n    existing_pool = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n    if existing_pool['code'] == 200:\n\n        # for some reason iControl REST doesn't support listing a single pool member.\n        # the response from GET for listing a member will return 200 even if it doesn't exists.\n        # because of this we have to do some rather \"unnecessary\" searching within a pool.\n\n        #what are the current members?\n        current_members = existing_pool['content']['membersReference']['items']\n\n        #loop through them\n        exists = False\n        for current_member in current_members:\n            if current_member['name'] == member['name']:\n                exists = True\n                break\n\n        if exists:\n            ret['result'] = True\n            ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = {}\n        else:\n            new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)\n\n            if new_member['code'] == 200:\n                ret['result'] = True\n                ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])\n                ret['changes']['old'] = {}\n\n                #look up the member again...\n                pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n                if pool_listing['code'] != 200:\n                    ret = _load_result(new_member, ret)\n                    return ret\n\n                members = pool_listing['content']['membersReference']['items']\n                #loop through them\n                for current_member in members:\n                    if current_member['name'] == member['name']:\n                        added_member = current_member\n                        break\n\n                ret['changes']['new'] = added_member\n\n            # member wasn't added\n            else:\n                ret = _load_result(new_member, ret)\n\n    #pool does not exists\n    elif existing_pool['code'] == 404:\n        ret['comment'] = 'A pool with this name was not found.'\n    else:\n        ret = _load_result(existing_pool, ret)\n\n    return ret",
        "rewrite": "```python\ndef add_pool_member(hostname, username, password, name, member):\n    \"\"\"\n    A function to connect to a bigip device and add a new member to an existing pool.\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': [member]\n        })\n\n    existing_pool = __salt__['bigip.list_pool"
    },
    {
        "original": "def Refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\n\n    Raises:\n      google.auth.exceptions.RefreshError: If the refresh fails.\n    \"\"\"\n    with requests.Session() as session:\n      session.proxies = self.proxy_config.proxies\n      session.verify = not self.proxy_config.disable_certificate_validation\n      session.cert = self.proxy_config.cafile\n\n      self.creds.refresh(\n          google.auth.transport.requests.Request(session=session))",
        "rewrite": "```python\nimport requests\n\ndef refresh_token(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\"\"\"\n    \n    self.creds.refresh(\n        google.auth.transport.requests.Request()\n    )\n```"
    },
    {
        "original": "def ParseFromUnicode(self, value):\n    \"\"\"Parse a string into a client URN.\n\n    Convert case so that all URNs are of the form C.[0-9a-f].\n\n    Args:\n      value: string value to parse\n    \"\"\"\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n      raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = match.group(\"clientid\")\n    clientid_correctcase = \"\".join((clientid[0].upper(), clientid[1:].lower()))\n\n    self._string_urn = self._string_urn.replace(clientid, clientid_correctcase,\n                                                1)",
        "rewrite": "```python\ndef ParseFromUnicode(self, value):\n    precondition.AssertType(value, Text)\n    value = value.strip()\n    super(ClientURN, self).ParseFromUnicode(value)\n    \n    match = self.CLIENT_ID_RE.match(self._string_urn) if hasattr(self, '_string_urn') else None\n    if match if match else not hasattr(self, 'CLIENT_ID_RE'):\n        raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = getattr(match or {}, \"clientid\", None) or ''\n    clientid_correctcase = \"\".join((clientid[0].upper"
    },
    {
        "original": "def close(self):\n        \"\"\"Close method.\n\n        Closes the underlying comm.\n        When the comm is closed, all of the widget views are automatically\n        removed from the front-end.\"\"\"\n        if self.comm is not None:\n            Widget.widgets.pop(self.model_id, None)\n            self.comm.close()\n            self.comm = None\n            self._ipython_display_ = None",
        "rewrite": "```python\ndef close(self):\n    if self.comm is not None:\n        Widget.widgets.pop(self.model_id, None)\n        self.comm.close()\n        self.comm = None\n        self._ipython_display_ = None\n```"
    },
    {
        "original": "def get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    exec_dir = os.path.dirname(os.path.realpath(__file__))\n    xml_path = os.path.join(exec_dir, 'data', 'FacebookLocales.xml')\n\n    fb_locales = _build_locale_table(xml_path)\n\n    def default_locale(request):\n        ",
        "rewrite": "\n\n```python\nimport os\nfrom django.utils import timezone\nfrom . import _build_locale_table\n\ndef get_default_locale_callable():\n    \"\"\"\n    Wrapper function to build a default locale mapping when needed.\n    \"\"\"\n    exec_dir = os.path.dirname(os.path.realpath(__file__))\n    xml_path = os.path.join(exec_dir, 'data', 'FacebookLocales.xml')\n    \n    def default_locale(request):\n        return _build_locale_table(xml_path).get(str(request.LANGUAGE_CODE))\n\ndef _build_locale_table(xml_path):\n    # Add functionality to build and return the locale table here\n    pass  #"
    },
    {
        "original": "def clitable_to_dict(cli_table):\n    \"\"\"Converts TextFSM cli_table object to list of dictionaries.\"\"\"\n    objs = []\n    for row in cli_table:\n        temp_dict = {}\n        for index, element in enumerate(row):\n            temp_dict[cli_table.header[index].lower()] = element\n        objs.append(temp_dict)\n    return objs",
        "rewrite": "```python\ndef cli_table_to_dict(cli_table):\n    \"\"\"Converts TextFSM cli_table object to list of dictionaries.\"\"\"\n    return [{cli_table.header[index].lower(): element} for row in cli_table for index, element in enumerate(row)]\n```"
    },
    {
        "original": "def execute_query(**kwargs):\n    \"\"\"\n      Executes a query against the connected db using pymapd\n      https://pymapd.readthedocs.io/en/latest/usage.html#querying\n\n      Kwargs:\n        query_name(str): Name of query\n        query_mapdql(str): Query to run\n        iteration(int): Iteration number\n\n      Returns:\n        query_execution(dict):::\n          result_count(int): Number of results returned\n          execution_time(float): Time (in ms) that pymapd reports\n                                 backend spent on query.\n          connect_time(float): Time (in ms) for overhead of query, calculated\n                               by subtracting backend execution time\n                               from time spent on the execution function.\n          results_iter_time(float): Time (in ms) it took to for\n                                    pymapd.fetchone() to iterate through all\n                                    of the results.\n          total_time(float): Time (in ms) from adding all above times.\n        False(bool): The query failed. Exception should be logged.\n    \"\"\"\n    start_time = timeit.default_timer()\n    try:\n        # Run the query\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(\n            \"Completed iteration \"\n            + str(kwargs[\"iteration\"])\n            + \" of query \"\n            + kwargs[\"query_name\"]\n        )\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error):\n        logging.exception(\n            \"Error running query \"\n            + kwargs[\"query_name\"]\n            + \" during iteration \"\n            + str(kwargs[\"iteration\"])\n        )\n        return False\n\n    # Calculate times\n    query_elapsed_time = (timeit.default_timer() - start_time) * 1000\n    execution_time = query_result._result.execution_time_ms\n    connect_time = round((query_elapsed_time - execution_time), 1)\n\n    # Iterate through each result from the query\n    logging.debug(\n        \"Counting results from query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n    )\n    result_count = 0\n    start_time = timeit.default_timer()\n    while query_result.fetchone():\n        result_count += 1\n    results_iter_time = round(\n        ((timeit.default_timer() - start_time) * 1000), 1\n    )\n\n    query_execution = {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": execution_time + connect_time + results_iter_time,\n    }\n    logging.debug(\n        \"Execution results for query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n        + \": \"\n        + str(query_execution)\n    )\n    return query_execution",
        "rewrite": "```python\nimport timeit\nimport logging\n\ndef execute_query(**kwargs):\n    start_time = timeit.default_timer()\n    try:\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(f\"Completed iteration {kwargs['iteration']} of query {kwargs['query_name']}\")\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error) as e:\n        logging.exception(f\"Error running query {kwargs['query_name']} during iteration {kwargs['iteration']}: {e}\")\n        return False\n\n    query_elapsed_time = (timeit.default_timer() - start_time) *"
    },
    {
        "original": "def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        assert isinstance(label, (github.Label.Label, str, unicode)), label\n        if isinstance(label, github.Label.Label):\n            label = label._identity\n        else:\n            label = urllib.quote(label)\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.issue_url + \"/labels/\" + label\n        )",
        "rewrite": "```python\ndef remove_from_labels(self, label):\n    assert isinstance(label, (github.Label.Label, str)), label\n    if isinstance(label, github.Label.Label):\n        label = label._identity\n    else:\n        label = urllib.parse.quote(label)\n    self._requester.requestJsonAndCheck(\n        \"DELETE\",\n        self.issue_url + \"/labels/\" + label\n    )\n```"
    },
    {
        "original": "def add_candidate_peer_endpoints(self, peer_endpoints):\n        \"\"\"Adds candidate endpoints to the list of endpoints to\n        attempt to peer with.\n\n        Args:\n            peer_endpoints ([str]): A list of public uri's which the\n                validator can attempt to peer with.\n        \"\"\"\n        with self._lock:\n            for endpoint in peer_endpoints:\n                if endpoint not in self._candidate_peer_endpoints:\n                    self._candidate_peer_endpoints.append(endpoint)",
        "rewrite": "```python\ndef add_candidate_peer_endpoints(self, peer_endpoints):\n    with self._lock:\n        self._candidate_peer_endpoints.extend(peer_endpoints)\n        self._candidate_peer_endpoints = list(set(self._candidate_peer_endpoints))\n```"
    },
    {
        "original": "def storeSenderKey(self, senderKeyName, senderKeyRecord):\n        \"\"\"\n        :type senderKeyName: SenderKeName\n        :type senderKeyRecord: SenderKeyRecord\n        \"\"\"\n        q = \"INSERT INTO sender_keys (group_id, sender_id, record) VALUES(?,?, ?)\"\n        cursor = self.dbConn.cursor()\n        serialized = senderKeyRecord.serialize()\n        if sys.version_info < (2,7):\n            serialized = buffer(serialized)\n        try:\n            cursor.execute(q, (senderKeyName.getGroupId(), senderKeyName.getSender().getName(), serialized))\n            self.dbConn.commit()\n        except sqlite3.IntegrityError as e:\n            q = \"UPDATE sender_keys set record = ? WHERE group_id = ? and sender_id = ?\"\n            cursor = self.dbConn.cursor()\n            cursor.execute(q, (serialized, senderKeyName.getGroupId(), senderKeyName.getSender().getName()))\n            self.dbConn.commit()",
        "rewrite": "```python\ndef storeSenderKey(self, senderKeyName, senderKeyRecord):\n    q = \"INSERT INTO sender_keys (group_id, sender_id, record) VALUES(?,?, ?)\"\n    cursor = self.dbConn.cursor()\n    serialized = senderKeyRecord.serialize()\n    \n    try:\n        cursor.execute(q, (senderKeyName.getGroupId(), senderKeyName.getSender().getName(), serialized))\n        self.dbConn.commit()\n    except sqlite3.IntegrityError:\n        q = \"UPDATE sender_keys set record = ? WHERE group_id = ? and sender_id = ?\"\n        cursor.execute(q, (serialized, senderKeyName.getGroupId(), sender"
    },
    {
        "original": "def detect_cadvisor(kubelet_url, cadvisor_port):\n        \"\"\"\n        Tries to connect to the cadvisor endpoint, with given params\n        :return: url if OK, raises exception if NOK\n        \"\"\"\n        if cadvisor_port == 0:\n            raise ValueError(\"cAdvisor port set to 0 in configuration\")\n        kubelet_hostname = urlparse(kubelet_url).hostname\n        if not kubelet_hostname:\n            raise ValueError(\"kubelet hostname empty\")\n        url = \"http://{}:{}{}\".format(kubelet_hostname, cadvisor_port, LEGACY_CADVISOR_METRICS_PATH)\n\n        # Test the endpoint is present\n        r = requests.head(url, timeout=1)\n        r.raise_for_status()\n\n        return url",
        "rewrite": "```python\nimport requests\nfrom urllib.parse import urlparse\n\ndef detect_cadvisor(kubelet_url, cadvisor_port):\n    if cadvisor_port == 0:\n        raise ValueError(\"cAdvisor port set to 0 in configuration\")\n    kubelet_hostname = urlparse(kubelet_url).hostname\n    if not kubelet_hostname:\n        raise ValueError(\"kubelet hostname empty\")\n    url = f\"http://{kubelet_hostname}:{cadvisor_port}{LEGACY_CADVISOR_METRICS_PATH}\"\n\n    try:\n        requests.head(url, timeout=1)\n        return url\n    except requests.RequestException as e:\n        raise Exception"
    },
    {
        "original": "def _SignedBinaryIDFromURN(binary_urn\n                          ):\n  \"\"\"Converts an AFF4 URN for a signed binary to a SignedBinaryID.\"\"\"\n  if binary_urn.RelativeName(GetAFF4PythonHackRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK,\n        path=binary_urn.RelativeName(GetAFF4PythonHackRoot()))\n  elif binary_urn.RelativeName(GetAFF4ExecutablesRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE,\n        path=binary_urn.RelativeName(GetAFF4ExecutablesRoot()))\n  else:\n    raise ValueError(\"Unable to determine type of signed binary: %s.\" %\n                     binary_urn)",
        "rewrite": "```python\ndef _signed_binary_id_from_urn(binary_urn):\n    if binary_urn.RelativeName(GetAFF4PythonHackRoot()):\n        return rdf_objects.SignedBinaryID(\n            binary_type=rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK,\n            path=binary_urn.RelativeName(GetAFF4PythonHackRoot()))\n    elif binary_urn.RelativeName(GetAFF4ExecutablesRoot()):\n        return rdf_objects.SignedBinaryID(\n            binary_type=rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE,\n            path=binary_urn.RelativeName(GetAFF4ExecutablesRoot()))\n"
    },
    {
        "original": "def _validate_disallowed(self, disallowed, field, value):\n        \"\"\" Readonly but with a custom error.\n\n        The rule's arguments are validated against this schema:\n        {'type': 'boolean'}\n        \"\"\"\n        if disallowed:\n            msg = 'disallowed user provided config option'\n            self._error(field, msg)",
        "rewrite": "```python\ndef _validate_disallowed(self, disallowed, field, value):\n    if disallowed and value is not None:\n        self._error(field, 'disallowed user provided config option')\n```"
    },
    {
        "original": "def make_type_consistent(s1, s2):\n    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n        return s1, s2\n\n    else:\n        return unicode(s1), unicode(s2)",
        "rewrite": "```python\ndef make_type_consistent(s1, s2):\n    return unicode(s1), unicode(s2)\n```"
    },
    {
        "original": "def _mod_repo_in_file(repo, repostr, filepath):\n    \"\"\"\n    Replace a repo entry in filepath with repostr\n    \"\"\"\n    with salt.utils.files.fopen(filepath) as fhandle:\n        output = []\n        for line in fhandle:\n            cols = salt.utils.args.shlex_split(\n                salt.utils.stringutils.to_unicode(line).strip()\n            )\n            if repo not in cols:\n                output.append(line)\n            else:\n                output.append(salt.utils.stringutils.to_str(repostr + '\\n'))\n    with salt.utils.files.fopen(filepath, 'w') as fhandle:\n        fhandle.writelines(output)",
        "rewrite": "```python\ndef _mod_repo_in_file(repo, repostr, filepath):\n    with salt.utils.files.fopen(filepath) as fhandle:\n        output = [line for line in fhandle if repo not in line]\n    with salt.utils.files.fopen(filepath, 'w') as fhandle:\n        cols = salt.utils.args.shlex_split(salt.utils.stringutils.to_unicode(line).strip()) for line in output]\n        new_line = '\\n'.join(cols)\n        fhandle.write(new_line)\n```\n\nOr:\n\n```python\ndef _mod_repo_in_file(repo, repostr, filepath):\n    with salt.utils.files"
    },
    {
        "original": "def bitphase_flip_operators(p):\n    \"\"\"\n    Return the bitphase flip kraus operators\n    \"\"\"\n    k0 = np.sqrt(1 - p) * I\n    k1 = np.sqrt(p) * Y\n    return k0, k1",
        "rewrite": "```python\nimport numpy as np\n\ndef bitphase_flip_operators(p):\n    I = np.array([[1, 0], [0, 1]])\n    Y = np.array([[0, -1j], [1j, 0]])\n    k0 = np.sqrt(1 - p) * I\n    k1 = np.sqrt(p) * Y\n    return k0, k1\n```"
    },
    {
        "original": "def _hook(self, hook_name, doc_uri=None, **kwargs):\n        \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n        doc = self.workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=self.workspace, document=doc, **kwargs)",
        "rewrite": "```python\ndef _hook(self, hook_name, doc_uri=None, **kwargs):\n    doc = self.workspace.get_document(doc_uri) if doc_uri else None\n    hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n    return [handler(config=self.config, workspace=self.workspace, document=doc, **kwargs) for handler in hook_handlers]\n```"
    },
    {
        "original": "def _get_col_items(mapping):\n    \"\"\"Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items",
        "rewrite": "```python\ndef _get_col_items(mapping):\n    from .variable import IndexVariable\n\n    col_items = []\n    for key, value in mapping.items():\n        col_items.append(key)\n        var = getattr(value, 'variable', value)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items.extend(level_names)\n    return col_items\n```"
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            ifrealname = i['interface_name'].split(':')[0]\n            # Convert rate in bps ( to be able to compare to interface speed)\n            bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n            bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n            # Decorate the bitrate with the configuration file thresolds\n            alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n            alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')\n            # If nothing is define in the configuration file...\n            # ... then use the interface speed (not available on all systems)\n            if alert_rx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_rx = self.get_alert(current=bps_rx,\n                                          maximum=i['speed'],\n                                          header='rx')\n            if alert_tx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_tx = self.get_alert(current=bps_tx,\n                                          maximum=i['speed'],\n                                          header='tx')\n            # then decorates\n            self.views[i[self.get_key()]]['rx']['decoration'] = alert_rx\n            self.views[i[self.get_key()]]['tx']['decoration'] = alert_tx",
        "rewrite": "```python\ndef update_views(self):\n    \"\"\"Update stats views.\"\"\"\n    super(Plugin, self).update_views()\n\n    for i in self.stats:\n        ifrealname = i['interface_name'].split(':')[0]\n        bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n        bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n        \n        alert_rx = self.get_alert(bps_rx, header=f\"{ifrealname}_rx\")\n        alert_tx = self.get_alert(bps_tx, header=f\"{ifrealname}_tx\")\n\n"
    },
    {
        "original": "def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        \"\"\"\n        label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label",
        "rewrite": "```python\nimport scipy.io\nimport numpy as np\n\ndef load_label(self, idx):\n    label_400 = scipy.io.loadmat(f'{self.context_dir}/trainval/{idx}')['LabelMap']\n    label = np.zeros_like(label_400, dtype=np.uint8)\n    for idx, l in enumerate(self.labels_59):\n        idx_400 = self.labels_400.index(l) + 1\n        label[label_400 == idx_400] = idx + 1\n    return np.expand_dims(label, axis=0)\n```"
    },
    {
        "original": "def read_channel(self):\n        \"\"\"Generic handler that will read all the data from an SSH or telnet channel.\"\"\"\n        output = \"\"\n        self._lock_netmiko_session()\n        try:\n            output = self._read_channel()\n        finally:\n            # Always unlock the SSH channel, even on exception.\n            self._unlock_netmiko_session()\n        return output",
        "rewrite": "```python\ndef read_channel(self):\n    output = \"\"\n    self._lock_netmiko_session()\n    try:\n        output = self._read_channel()\n    except Exception as e:\n        # Log the exception if needed\n        pass\n    finally:\n        self._unlock_netmiko_session()\n    return output\n```"
    },
    {
        "original": "def config_mode(self, config_command=\"config term\", pattern=\"\"):\n        \"\"\"\n        Enter into configuration mode on remote device.\n\n        Cisco IOS devices abbreviate the prompt at 20 chars in config mode\n        \"\"\"\n        if not pattern:\n            pattern = re.escape(self.base_prompt[:16])\n        return super(CiscoBaseConnection, self).config_mode(\n            config_command=config_command, pattern=pattern\n        )",
        "rewrite": "```python\ndef config_mode(self, config_command=\"config term\", pattern=\"\"):\n    if not pattern:\n        pattern = re.escape(self.base_prompt[:20])\n    return super(CiscoBaseConnection, self).config_mode(\n        config_command=config_command, pattern=pattern\n    )\n```"
    },
    {
        "original": "def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n        @returns: list of builder names to build on\n        \"\"\"\n\n        # self.builderNames is the configured list of builders\n        # available for try.  If the user supplies a list of builders,\n        # it must be restricted to the configured list.  If not, build\n        # on all of the configured builders.\n        if builderNames:\n            for b in builderNames:\n                if b not in self.builderNames:\n                    log.msg(\"%s got with builder %s\" % (self, b))\n                    log.msg(\" but that wasn't in our list: %s\"\n                            % (self.builderNames,))\n                    return []\n        else:\n            builderNames = self.builderNames\n        return builderNames",
        "rewrite": "```python\ndef filterBuilderList(self, builderNames):\n    return (self.builderNames if not builderNames \n            else [b for b in builderNames if b in self.builderNames])\n```\n\nOr more defensively: \n\n```python\ndef filterBuilderList(self, builderNames):\n    self.builderNames = self._get\\ConfiguredBuilders()  # assume this method exists.\n    if not isinstance(builderNames, list) or not all(isinstance(b, str) for b in (builderNames or [])):\n        log.msg(\"Error: invalid type passed to filterBuilderList\")\n        return []\n    \n    return (self.builderNames if"
    },
    {
        "original": "def fix_e262(self, result):\n        \"\"\"Fix spacing after comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column']\n\n        code = target[:offset].rstrip(' \\t#')\n        comment = target[offset:].lstrip(' \\t#')\n\n        fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed",
        "rewrite": "```python\ndef fix_e262(self, result):\n    \"\"\"Fix spacing after comment hash.\"\"\"\n    target = self.source[result['line'] - 1]\n    offset = result['column']\n\n    code = target[:offset].rstrip('#')\n    comment_text = target[offset:].lstrip('#')\n\n    fixed_line = f\"{code}{'  # ' if comment_text else ''}{comment_text or '\\n'}\"\n\n    self.source[result['line'] - 1] = fixed_line\n```"
    },
    {
        "original": "def annToRLE(self, ann):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        t = self.imgs[ann['image_id']]\n        h, w = t['height'], t['width']\n        segm = ann['segmentation']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm['counts']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle",
        "rewrite": "```python\ndef annToRLE(self, ann):\n    t = self.imgs[ann['image_id']]\n    h, w = t['height'], t['width']\n    segm = ann['segmentation']\n    \n    if isinstance(segm, list):\n        rles = maskUtils.frPyObjects(segm, h, w)\n        rle = maskUtils.merge(rles)\n    elif isinstance(segm.get('counts'), list):\n        rle = maskUtils.frPyObjects(segm, h, w)\n    else:\n        rle = segm\n    \n    return rle\n```"
    },
    {
        "original": "def cartesian_product(arrays, flat=True, copy=False):\n    \"\"\"\n    Efficient cartesian product of a list of 1D arrays returning the\n    expanded array views for each dimensions. By default arrays are\n    flattened, which may be controlled with the flat flag. The array\n    views can be turned into regular arrays with the copy flag.\n    \"\"\"\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(arr.flatten() if copy else arr.flat for arr in arrays)\n    return tuple(arr.copy() if copy else arr for arr in arrays)",
        "rewrite": "```python\nimport numpy as np\n\ndef cartesian_product(arrays, flat=True, copy=False):\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(np.atleast_1d(arr).flatten() if copy else arr.flat for arr in arrays)\n    return tuple(np.asarray(arr).copy() if copy else arr for arr in arrays)\n```"
    },
    {
        "original": "def share_with_invite(self, recipients, require_sign_in=True,\n                          send_email=True, message=None, share_type='view'):\n        \"\"\" Sends an invitation to access or edit this DriveItem\n\n        :param recipients: a string or Contact or a list of the former\n         representing recipients of this invitation\n        :type recipients: list[str] or list[Contact] or str or Contact\n        :param bool require_sign_in: if True the recipients\n         invited will need to log in to view the contents\n        :param bool send_email: if True an email will be send to the recipients\n        :param str message: the body text of the message emailed\n        :param str share_type: 'view': will allow to read the contents.\n         'edit' will allow to modify the contents\n        :return: link to share\n        :rtype: DriveItemPermission\n        \"\"\"\n        if not self.object_id:\n            return None\n\n        to = []\n        if recipients is None:\n            raise ValueError('Provide a valid to parameter')\n        elif isinstance(recipients, (list, tuple)):\n            for x in recipients:\n                if isinstance(x, str):\n                    to.append({'email': x})\n                elif isinstance(x, Contact):\n                    to.append({'email': x.main_email})\n                else:\n                    raise ValueError(\n                        'All the recipients must be either strings or Contacts')\n        elif isinstance(recipients, str):\n            to.append({'email': recipients})\n        elif isinstance(recipients, Contact):\n            to.append({'email': recipients.main_email})\n        else:\n            raise ValueError(\n                'All the recipients must be either strings or Contacts')\n\n        url = self.build_url(\n            self._endpoints.get('share_invite').format(id=self.object_id))\n\n        data = {\n            'recipients': to,\n            self._cc('requireSignIn'): require_sign_in,\n            self._cc('sendInvitation'): send_email,\n        }\n        if share_type in {'view', 'read'}:\n            data['roles'] = ['read']\n        elif share_type == {'edit', 'write'}:\n            data['roles'] = ['write']\n        else:\n            raise ValueError(\n                '\"{}\" is not a valid share_type'.format(share_type))\n        if send_email and message:\n            data['message'] = message\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return DriveItemPermission(parent=self, **{self._cloud_data_key: data})",
        "rewrite": "```python\ndef share_with_invite(self, recipients, require_sign_in=True,\n                      send_email=True, message=None, share_type='view'):\n    if not self.object_id:\n        return None\n\n    if recipients is None:\n        raise ValueError('Provide a valid to parameter')\n    \n    to = []\n    for x in (recipients if isinstance(recipients, (list, tuple)) else [recipients]):\n        if isinstance(x, (str)):\n            to.append({'email': x})\n        elif isinstance(x, Contact):\n            to.append({'email': x.main_email})\n        \n    url = self.build_url(\n        self._end"
    },
    {
        "original": "def vm2json(vm):\n    \"\"\"\n    Get a JSON representation of a DEX file\n\n    :param vm: :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`\n    :return:\n    \"\"\"\n    d = {\"name\": \"root\", \"children\": []}\n\n    for _class in vm.get_classes():\n        c_class = {\"name\": _class.get_name(), \"children\": []}\n\n        for method in _class.get_methods():\n            c_method = {\"name\": method.get_name(), \"children\": []}\n\n            c_class[\"children\"].append(c_method)\n\n        d[\"children\"].append(c_class)\n\n    return json.dumps(d)",
        "rewrite": "```python\nimport json\n\ndef vm2json(vm):\n    d = {\"name\": \"root\", \"children\": []}\n\n    for class_ in vm.get_classes():\n        child_class = {\n            \"name\": class_.get_name(),\n            \"children\": [\n                {\n                    \"name\": method.get_name(),\n                    # Add additional information about the method if needed\n                } for method in class_.get_methods()\n            ]\n        }\n        d[\"children\"].append(child_class)\n\n    return json.dumps(d, indent=4)\n```"
    },
    {
        "original": "def validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    \"\"\"\n    Ensure that the field's name does not shadow an existing attribute of the model.\n    \"\"\"\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )",
        "rewrite": "```python\nfrom typing import List, Type\n\ndef validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )\n```"
    },
    {
        "original": "def automatic_gamma_density(structure, kppa):\n        \"\"\"\n        Returns an automatic Kpoint object based on a structure and a kpoint\n        density. Uses Gamma centered meshes always. For GW.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n\n        Args:\n            structure:\n                Input structure\n            kppa:\n                Grid density\n        \"\"\"\n\n        latt = structure.lattice\n        lengths = latt.abc\n        ngrid = kppa / structure.num_sites\n\n        mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n        num_div = [int(round(mult / l)) for l in lengths]\n\n        # ensure that numDiv[i] > 0\n        num_div = [i if i > 0 else 1 for i in num_div]\n\n        # VASP documentation recommends to use even grids for n <= 8 and odd\n        # grids for n > 8.\n        num_div = [i + i % 2 if i <= 8 else i - i % 2 + 1 for i in num_div]\n\n        style = Kpoints.supported_modes.Gamma\n\n        comment = \"pymatgen 4.7.6+ generated KPOINTS with grid density = \" + \\\n                  \"{} / atom\".format(kppa)\n        num_kpts = 0\n        return Kpoints(comment, num_kpts, style, [num_div], [0, 0, 0])",
        "rewrite": "```python\ndef automatic_gamma_density(structure, kppa):\n    \"\"\"\n    Returns an automatic Kpoint object based on a structure and a kpoint density.\n    Uses Gamma centered meshes always. For GW.\n\n    Args:\n        structure:\n            Input structure\n        kppa:\n            Grid density\n    \"\"\"\n\n    latt = structure.lattice.abc  # Get lattice lengths directly\n    ngrid = kppa / structure.num_sites\n\n    mult = (ngrid * len(latt[0]) * len(latt[1]) * len(latt[2])) ** (1 / 3)\n    \n   "
    },
    {
        "original": "def read_cs_g0_contribution(self):\n        \"\"\"\n            Parse the  G0 contribution of NMR chemical shielding.\n\n            Returns:\n            G0 contribution matrix as list of list.\n        \"\"\"\n        header_pattern = r'^\\s+G\\=0 CONTRIBUTION TO CHEMICAL SHIFT \\(field along BDIR\\)\\s+$\\n' \\\n                         r'^\\s+-{50,}$\\n' \\\n                         r'^\\s+BDIR\\s+X\\s+Y\\s+Z\\s*$\\n' \\\n                         r'^\\s+-{50,}\\s*$\\n'\n        row_pattern = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 3)\n        footer_pattern = r'\\s+-{50,}\\s*$'\n        self.read_table_pattern(header_pattern, row_pattern, footer_pattern, postprocess=float,\n                                last_one_only=True, attribute_name=\"cs_g0_contribution\")",
        "rewrite": "```python\ndef read_cs_g0_contribution(self):\n    \"\"\"\n    Parse the G0 contribution of NMR chemical shielding.\n\n    Returns:\n        G0 contribution matrix as list of list.\n    \"\"\"\n    header_pattern = r'^\\s+G=0 CONTRIBUTION TO CHEMICAL SHIFT \\(field along BDIR\\)\\s+\\n' \\\n                    r'^\\s+-{50,}\\n' \\\n                    r'^\\s+BDIR\\s+X\\s+Y\\s+Z\\s*\\n' \\\n                    r'^\\s+-{50,}\\s*\\n'\n    row_pattern = r'(?:"
    },
    {
        "original": "def _split_arrs(array_2d, slices):\n    \"\"\"\n    Equivalent to numpy.split(array_2d, slices),\n    but avoids fancy indexing\n    \"\"\"\n    if len(array_2d) == 0:\n        return np.empty(0, dtype=np.object)\n\n    rtn = np.empty(len(slices) + 1, dtype=np.object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn",
        "rewrite": "```python\nimport numpy as np\n\ndef _split_arrs(array_2d, slices):\n    rtn = [array_2d[:]]\n    start = 0\n    for s in slices:\n        rtn.append(array_2d[start:s])\n        start = s\n    rtn.append(array_2d[start:])\n    return np.array(rtn, dtype=object)\n```"
    },
    {
        "original": "def load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n        # TODO: support JSON\n\n    try:\n        enc = detect_by_bom(swag_path)\n        with codecs.open(swag_path, encoding=enc) as yaml_file:\n            return yaml_file.read()\n    except IOError:\n        # not in the same dir, add dirname\n        swag_path = os.path.join(\n            root_path or os.path.dirname(__file__), swag_path\n        )\n        try:\n            enc = detect_by_bom(swag_path)\n            with codecs.open(swag_path, encoding=enc) as yaml_file:\n                return yaml_file.read()\n        except IOError:  # pragma: no cover\n            # if package dir\n            # see https://github.com/rochacbruno/flasgger/pull/104\n            # Still not able to reproduce this case\n            # test are in examples/package_example\n            # need more detail on how to reproduce IOError here\n            swag_path = swag_path.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n            path = swag_path.replace(\n                (root_path or os.path.dirname(__file__)), ''\n            ).split(os.sep)[1:]\n            site_package = imp.find_module(path[0])[1]\n            swag_path = os.path.join(site_package, os.sep.join(path[1:]))\n            with open(swag_path) as yaml_file:\n                return yaml_file.read()",
        "rewrite": "```python\nimport os\nimport codecs\nimport yaml\nimport imp\n\ndef load_from_file(swag_path, swag_type='yml', root_path=None):\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n\n    try:\n        with codecs.open(swag_path, encoding='utf-8') as yaml_file:\n            return yaml.safe_load(yaml_file)\n    except IOError:\n        swag_path = os.path.join(root_path or os.path.dirname(__file__), swag_path)\n        try:\n            with codecs.open(swag_path, encoding='"
    },
    {
        "original": "def render(template, dest, **kwargs):\n    \"\"\"Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    \"\"\"\n\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    f = open(dest, 'wb')\n    f.write(text.encode('utf-8'))\n    f.close()",
        "rewrite": "```python\ndef render(template, dest, **kwargs):\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    with open(dest, 'wb') as f:\n        f.write(text.encode('utf-8'))\n```"
    },
    {
        "original": "def imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n        \"\"\"\n        Show the image stored in X on the canvas.\n        \n        The origin of the image show is (0,0), such that X[0,0] gets plotted at [0,0] of the image!\n        \n        the kwargs are plotting library specific kwargs!\n        \"\"\"\n        raise NotImplementedError(\"Implement all plot functions in AbstractPlottingLibrary in order to use your own plotting library\")",
        "rewrite": "```python\ndef imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"X must be a numpy array\")\n    \n    if extent is None:\n        extent = [0.0] * 4\n    \n    if label is None:\n        label = \"\"\n    \n    if vmin is None or vmax is None:\n        # Use the full range of the data by default\n        vmin = X.min()\n        vmax = X.max()\n    \n    # Add any additional checks or handling for kwargs here\n    # For example:\n"
    },
    {
        "original": "def _probvec(r, out):\n    \"\"\"\n    Fill `out` with randomly sampled probability vectors as rows.\n\n    To be complied as a ufunc by guvectorize of Numba. The inputs must\n    have the same shape except the last axis; the length of the last\n    axis of `r` must be that of `out` minus 1, i.e., if out.shape[-1] is\n    k, then r.shape[-1] must be k-1.\n\n    Parameters\n    ----------\n    r : ndarray(float)\n        Array containing random values in [0, 1).\n\n    out : ndarray(float)\n        Output array.\n\n    \"\"\"\n    n = r.shape[0]\n    r.sort()\n    out[0] = r[0]\n    for i in range(1, n):\n        out[i] = r[i] - r[i-1]\n    out[n] = 1 - r[n-1]",
        "rewrite": "```python\ndef _probvec(r, out):\n    n = r.shape[0]\n    r.sort()\n    out.fill(0.0)\n    out[1:n] =rica(view=[r[i] - r[i-1] for i in range(1, n)])\n    out[n] = 1 - r[n-1]\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'text') and self.text is not None:\n            _dict['text'] = self.text\n        if hasattr(self, 'tokens') and self.tokens is not None:\n            _dict['tokens'] = self.tokens\n        if hasattr(self, 'readings') and self.readings is not None:\n            _dict['readings'] = self.readings\n        if hasattr(self, 'part_of_speech') and self.part_of_speech is not None:\n            _dict['part_of_speech'] = self.part_of_speech\n        return _dict",
        "rewrite": "```python\ndef _to_dict(self):\n        return {\n            key: getattr(self, key) \n            for key in ['text', 'tokens', 'readings', 'part_of_speech'] \n            if hasattr(self, key) and getattr(self, key) is not None\n        }\n```"
    },
    {
        "original": "def bbox_to_poly(north, south, east, west):\n    \"\"\"\n    Convenience function to parse bbox -> poly\n    \"\"\"\n    \n    return Polygon([(west, south), (east, south), (east, north), (west, north)])",
        "rewrite": "```python\ndef bbox_to_poly(north, south, east, west):\n    return Polygon([(west, south), (east, south), (east, north), (west, north)])\n```"
    },
    {
        "original": "def validate(config):\n    \"\"\"validate config file\"\"\"\n    with open(config) as fh:\n        content = fh.read()\n\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        log.error(\"config file: %s is not valid yaml\", config)\n        raise\n\n    try:\n        jsonschema.validate(data, CONFIG_SCHEMA)\n    except Exception:\n        log.error(\"config file: %s is not valid\", config)\n        raise\n\n    log.info(\"config file valid, accounts:%d\", len(data['accounts']))\n    return data",
        "rewrite": "```python\nimport yaml\nimport jsonschema\nimport logging as log\n\ndef validate(config):\n    try:\n        with open(config) as fh:\n            content = fh.read()\n    except FileNotFoundError:\n        log.error(\"config file: %s not found\", config)\n        raise\n\n    try:\n        data = yaml.safe_load(content)\n    except yaml.YAMLError as e:\n        log.error(\"config file: %s is not valid yaml: %s\", config, str(e))\n        raise\n\n    try:\n        jsonschema.validate(instance=data, schema=CONFIG_SCHEMA)\n    except json.JSONDecodeError as e:\n        log"
    },
    {
        "original": "def set_last_col_idx(self, last_col_idx):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tparam last_col_idx : int\n\t\t\tnumber of columns\n\t\t\"\"\"\n\t\tassert last_col_idx >= self._max_col\n\t\tself._max_col = last_col_idx\n\t\treturn self",
        "rewrite": "def set_last_col_idx(self, last_col_idx):\n    assert last_col_idx >= self._max_col\n    self._max_col = last_col_idx\n    return self"
    },
    {
        "original": "def _apply_mask(self):\n    \"\"\"Applies the passed-in mask to the convolution matrix.\n\n    Returns:\n      w: A copy of the convolution matrix that has had the mask applied.\n\n    Raises:\n      base.IncompatibleShapeError: If the mask shape has more dimensions than\n          the weight matrix.\n      base.IncompatibleShapeError: If the mask and the weight matrix don't\n          match on shape.\n    \"\"\"\n    w = self._w\n    w_shape = w.get_shape()\n    mask_shape = self._mask.get_shape()\n\n    if mask_shape.ndims > w_shape.ndims:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Max shape: {}\".format(\n              mask_shape.ndims, len(self._data_format)\n          )\n      )\n    if mask_shape != w_shape[:mask_shape.ndims]:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Weight shape: {}\".format(\n              mask_shape, w_shape\n          )\n      )\n    # TF broadcasting is a bit fragile.\n    # Expand the shape of self._mask by one dim at a time to the right\n    # until the rank matches `weight_shape`.\n    while self._mask.get_shape().ndims < w_shape.ndims:\n      self._mask = tf.expand_dims(self._mask, -1)\n\n    # tf.Variable & tf.ResourceVariable don't support *=.\n    w = w * self._mask  # pylint: disable=g-no-augmented-assignment\n\n    return w",
        "rewrite": "```python\ndef _apply_mask(self):\n    \"\"\"Applies the passed-in mask to the convolution matrix.\"\"\"\n\n    w = self._w\n    w_shape = w.get_shape()\n    mask_shape = self._mask.get_shape()\n\n    if mask_shape.ndims > w_shape.ndims:\n        raise base.IncompatibleShapeError(\n            \"Invalid mask shape: {}. Max shape: {}\".format(\n                mask_shape.ndims, len(self._data_format)\n            )\n        )\n    \n    if mask_shape != w_shape[:mask_shape.ndims]:\n        raise base.IncompatibleShapeError(\n            \"Invalid mask shape: {}. Weight shape:"
    },
    {
        "original": "def get_array_indices(self):\n        \"\"\"Returns an iterator of index token lists\"\"\"\n\n        for token in self.tokens:\n            if isinstance(token, SquareBrackets):\n                # Use [1:-1] index to discard the square brackets\n                yield token.tokens[1:-1]",
        "rewrite": "```python\ndef get_array_indices(self):\n    for token in self.tokens:\n        if isinstance(token, SquareBrackets):\n            yield token.tokens[1:-1]\n```"
    },
    {
        "original": "def set_embeddings(self, embeddings):\n        \"\"\"\n        Specifies fixed set of embeddings\n        :param embeddings: array-like, sparse or dense, shape should be (embedding size, # terms)\n        :return: EmbeddingsResolver\n        \"\"\"\n        if self.embeddings_ is not None:\n            raise Exception(\"You have already set embeddings by running set_embeddings or set_embeddings_model.\")\n        assert embeddings.shape[1] == self.corpus_.get_num_terms()\n        self.embeddings_ = embeddings.T\n        self.vocab_ = self.corpus_.get_terms()\n        return self",
        "rewrite": "```python\ndef set_embeddings(self, embeddings):\n    if self.embeddings_ is not None:\n        raise ValueError(\"Embeddings have already been set.\")\n    assert embeddings.shape[1] == self.corpus_.get_num_terms(), \"Embedding shape mismatch\"\n    self.embeddings_ = embeddings.T\n    self.vocab_ = list(self.corpus_.get_terms())\n    return self\n```"
    },
    {
        "original": "def parse_degrees(cls, degrees, arcminutes, arcseconds, direction=None):\n        \"\"\"\n        Parse degrees minutes seconds including direction (N, S, E, W)\n        \"\"\"\n        degrees = float(degrees)\n        negative = degrees < 0\n        arcminutes = float(arcminutes)\n        arcseconds = float(arcseconds)\n\n        if arcminutes or arcseconds:\n            more = units.degrees(arcminutes=arcminutes, arcseconds=arcseconds)\n            if negative:\n                degrees -= more\n            else:\n                degrees += more\n\n        if direction in [None, 'N', 'E']:\n            return degrees\n        elif direction in ['S', 'W']:\n            return -degrees\n        else:\n            raise ValueError(\"Invalid direction! Should be one of [NSEW].\")",
        "rewrite": "```python\ndef parse_degrees(cls, degrees, arcminutes, arcseconds, direction=None):\n    degrees = float(degrees)\n    negative = degrees < 0\n    arcminutes = float(arcminutes)\n    arcseconds = float(arcseconds)\n\n    if arcminutes or arcseconds:\n        more = degrees + (float(arcminutes) / 60) + (float(arcseconds) / 3600)\n        if negative:\n            degrees -= more\n        else:\n            degrees += more\n\n    def convert_direction():\n        if direction in ['N', 'E', None]:\n            return True\n        elif direction in ['S"
    },
    {
        "original": "def _execute_with_retries(conn, function, **kwargs):\n    \"\"\"\n    Retry if we're rate limited by AWS or blocked by another call.\n    Give up and return error message if resource not found or argument is invalid.\n\n    conn\n        The connection established by the calling method via _get_conn()\n\n    function\n        The function to call on conn. i.e. create_stream\n\n    **kwargs\n        Any kwargs required by the above function, with their keywords\n        i.e. StreamName=stream_name\n\n    Returns:\n        The result dict with the HTTP response and JSON data if applicable\n        as 'result', or an error as 'error'\n\n    CLI example::\n\n        salt myminion boto_kinesis._execute_with_retries existing_conn function_name function_kwargs\n\n    \"\"\"\n    r = {}\n    max_attempts = 18\n    max_retry_delay = 10\n    for attempt in range(max_attempts):\n        log.info(\"attempt: %s function: %s\", attempt, function)\n        try:\n            fn = getattr(conn, function)\n            r['result'] = fn(**kwargs)\n            return r\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response['Error']['Code']\n            if \"LimitExceededException\" in error_code or \"ResourceInUseException\" in error_code:\n                # could be rate limited by AWS or another command is blocking,\n                # retry with exponential backoff\n                log.debug(\"Retrying due to AWS exception\", exc_info=True)\n                time.sleep(_jittered_backoff(attempt, max_retry_delay))\n            else:\n                # ResourceNotFoundException or InvalidArgumentException\n                r['error'] = e.response['Error']\n                log.error(r['error'])\n                r['result'] = None\n                return r\n\n    r['error'] = \"Tried to execute function {0} {1} times, but was unable\".format(function, max_attempts)\n    log.error(r['error'])\n    return r",
        "rewrite": "```python\nimport logging\nimport time\n\ndef _execute_with_retries(conn, function, **kwargs):\n    max_attempts = 18\n    max_retry_delay = 10\n\n    for attempt in range(max_attempts):\n        try:\n            log.info(\"attempt: %s function: %s\", attempt, function)\n            fn = getattr(conn, function)\n            result = fn(**kwargs)\n            return {'result': result}\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response['Error']['Code']\n            if \"LimitExceededException\" in error_code or \"ResourceInUseException\" in"
    },
    {
        "original": "def calculate_recommendations(output_filename, model_name=\"als\"):\n    \"\"\" Generates artist recommendations for each user in the dataset \"\"\"\n    # train the model based off input params\n    artists, users, plays = get_lastfm()\n\n    # create a model from the input data\n    model = get_model(model_name)\n\n    # if we're training an ALS based model, weight input for last.fm\n    # by bm25\n    if issubclass(model.__class__, AlternatingLeastSquares):\n        # lets weight these models by bm25weight.\n        logging.debug(\"weighting matrix by bm25_weight\")\n        plays = bm25_weight(plays, K1=100, B=0.8)\n\n        # also disable building approximate recommend index\n        model.approximate_similar_items = False\n\n    # this is actually disturbingly expensive:\n    plays = plays.tocsr()\n\n    logging.debug(\"training model %s\", model_name)\n    start = time.time()\n    model.fit(plays)\n    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n\n    # generate recommendations for each user and write out to a file\n    start = time.time()\n    user_plays = plays.T.tocsr()\n    with tqdm.tqdm(total=len(users)) as progress:\n        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n            for userid, username in enumerate(users):\n                for artistid, score in model.recommend(userid, user_plays):\n                    o.write(\"%s\\t%s\\t%s\\n\" % (username, artists[artistid], score))\n                progress.update(1)\n    logging.debug(\"generated recommendations in %0.2fs\",  time.time() - start)",
        "rewrite": "```python\nimport time\nfrom tqdm import tqdm\nimport logging\n\ndef calculate_recommendations(output_filename, model_name=\"als\"):\n    \"\"\" Generates artist recommendations for each user in the dataset \"\"\"\n    artists, users, plays = get_lastfm()\n    model = get_model(model_name)\n\n    if issubclass(model.__class__, AlternatingLeastSquares):\n        logging.debug(\"weighting matrix by bm25_weight\")\n        plays = bm25_weight(plays, K1=100, B=0.8)\n        model.approximate_similar_items = False\n\n    plays_tocsr_start_time = time.time()\n    plays_c"
    },
    {
        "original": "def fit_transform(self, input, **fit_kwargs):\n        \"\"\"Execute fit and transform in sequence.\"\"\"\n        self.fit(input, **fit_kwargs)\n        X = self.transform(input)\n        return X",
        "rewrite": "```python\ndef fit_transform(self, input, **fit_kwargs):\n    self.fit(input, **fit_kwargs)\n    return self.transform(input)\n```"
    },
    {
        "original": "def patch_python_logging_handlers():\n    \"\"\"\n    Patch the python logging handlers with out mixed-in classes\n    \"\"\"\n    logging.StreamHandler = StreamHandler\n    logging.FileHandler = FileHandler\n    logging.handlers.SysLogHandler = SysLogHandler\n    logging.handlers.WatchedFileHandler = WatchedFileHandler\n    logging.handlers.RotatingFileHandler = RotatingFileHandler\n    if sys.version_info >= (3, 2):\n        logging.handlers.QueueHandler = QueueHandler",
        "rewrite": "```python\nimport sys\nfrom logging import StreamHandler, FileHandler, handlers\nimport logging\n\ndef patch_python_logging_handlers():\n    logging.StreamHandler = StreamHandler\n    logging.FileHandler = FileHandler\n    \n    class SysLogWrapper(handlers.SysLogHandler):\n        pass\n    \n    handlers.SysLogHandler = SysLogWrapper\n    \n    class WatchedFileWrapper(handlers.WatchedFileHandler):\n        pass\n    \n    handlers.WatchedFileHandler = WatchedFileWrapper\n    \n    class RotatingFileWrapper(handlers.RotatingFileHandler):\n        pass\n    \n    handlers.RotatingFileHandler = RotatingFileWrapper\n\n    if sys.version"
    },
    {
        "original": "def get_review_requests(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/pulls/:number/requested_reviewers <https://developer.github.com/v3/pulls/review_requests/>`_\n        :rtype: tuple of :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser` and of :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return (\n            github.PaginatedList.PaginatedList(\n                github.NamedUser.NamedUser,\n                self._requester,\n                self.url + \"/requested_reviewers\",\n                None,\n                list_item='users'\n            ),\n            github.PaginatedList.PaginatedList(\n                github.Team.Team,\n                self._requester,\n                self.url + \"/requested_reviewers\",\n                None,\n                list_item='teams'\n            )\n        )",
        "rewrite": "```python\ndef get_review_requests(self):\n    return (\n        github.PaginatedList.PaginatedList(\n            github.NamedUser.NamedUser,\n            self._requester,\n            self.url + \"/requested_reviewers\",\n            None,\n            list_item='users'\n        ),\n        github.PaginatedList.PaginatedList(\n            github.Team.Team,\n            self._requester,\n            self.url + \"/requested_reviewers\",\n            None,\n            list_item='teams'\n        )\n    )\n```"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"Transforms the object to a Python dictionary.\n\n            Note:\n                If an Input hasn't been signed yet, this method returns a\n                dictionary representation.\n\n            Returns:\n                dict: The Input as an alternative serialization format.\n        \"\"\"\n        try:\n            fulfillment = self.fulfillment.serialize_uri()\n        except (TypeError, AttributeError, ASN1EncodeError, ASN1DecodeError):\n            fulfillment = _fulfillment_to_details(self.fulfillment)\n\n        try:\n            # NOTE: `self.fulfills` can be `None` and that's fine\n            fulfills = self.fulfills.to_dict()\n        except AttributeError:\n            fulfills = None\n\n        input_ = {\n            'owners_before': self.owners_before,\n            'fulfills': fulfills,\n            'fulfillment': fulfillment,\n        }\n        return input_",
        "rewrite": "```python\ndef to_dict(self):\n    try:\n        fulfillment = self.fulfillment.serialize_uri()\n    except (TypeError, AttributeError, ASN1EncodeError, ASN1DecodeError):\n        fulfillment = _fulfillment_to_details(self.fulfillment)\n\n    try:\n        fulfills = self.fulfills.to_dict() if self.fulfills else None\n    except AttributeError:\n        fulfills = None\n\n    return {\n        'owners_before': self.owners_before,\n        'fulfills': fulfills,\n        'fulfillment': fulfillment,\n    }\n```"
    },
    {
        "original": "def clone(self, callable=None, **overrides):\n        \"\"\"Clones the Callable optionally with new settings\n\n        Args:\n            callable: New callable function to wrap\n            **overrides: Parameter overrides to apply\n\n        Returns:\n            Cloned Callable object\n        \"\"\"\n        old = {k: v for k, v in self.get_param_values()\n               if k not in ['callable', 'name']}\n        params = dict(old, **overrides)\n        callable = self.callable if callable is None else callable\n        return self.__class__(callable, **params)",
        "rewrite": "```python\ndef clone(self, callable=None, **overrides):\n    old = {k: v for k, v in self.get_param_values() if k not in ['callable', 'name']}\n    params = dict(old, **overrides)\n    return self.__class__(callable or self.callable, **params)\n```"
    },
    {
        "original": "def user_list(**connection_args):\n    \"\"\"\n    Return a list of users on a MySQL server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mysql.user_list\n    \"\"\"\n    dbc = _connect(**connection_args)\n    if dbc is None:\n        return []\n    cur = dbc.cursor(MySQLdb.cursors.DictCursor)\n    try:\n        qry = 'SELECT User,Host FROM mysql.user'\n        _execute(cur, qry)\n    except MySQLdb.OperationalError as exc:\n        err = 'MySQL Error {0}: {1}'.format(*exc.args)\n        __context__['mysql.error'] = err\n        log.error(err)\n        return []\n    results = cur.fetchall()\n    log.debug(results)\n    return results",
        "rewrite": "```python\ndef user_list(**connection_args):\n    dbc = _connect(**connection_args)\n    if not dbc:\n        return []\n    cur = dbc.cursor(MySQLdb.cursors.DictCursor)\n    try:\n        cur.execute('SELECT User,Host FROM mysql.user')\n        results = cur.fetchall()\n        log.debug(results)\n        return results\n    except MySQLdb.OperationalError as exc:\n        err = f'MySQL Error {exc.args[0]}: {exc.args[1]}'\n        __context__['mysql.error'] = err\n        log.error(err)\n        return []\n```"
    },
    {
        "original": "def reboot(search, one=True, force=False):\n    \"\"\"\n    Reboot one or more vms\n\n    search : string\n        filter vms, see the execution module.\n    one : boolean\n        reboot only one vm\n    force : boolean\n        force reboot, faster but no graceful shutdown\n\n    .. note::\n        If the search parameter does not contain an equal (=) symbol it will be\n        assumed it will be tried as uuid, hostname, and alias.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.reboot 91244bba-1146-e4ec-c07e-e825e0223aa9\n        salt-run vmadm.reboot search='alias=marije'\n        salt-run vmadm.reboot search='type=KVM' one=False\n    \"\"\"\n    return _action('reboot', search, one, force)",
        "rewrite": "```python\ndef reboot(search, one=True, force=False):\n    \"\"\"Reboot one or more vms\"\"\"\n    return _action('reboot', search, one, force)\n\ndef _action(action, search, one=False, force=False):\n    # This is where the actual logic of the function goes\n    pass\n\nclass VM:\n    def match(self, search_value):\n        return True  # This might need to be implemented based on the actual logic\n\nclass VmadmModule:\n    def __init__(self):\n        self.vms = []  # This should be populated with VM objects\n\ndef _get_vms"
    },
    {
        "original": "def InitFromAff4Object(self, aff4_obj, aff4_cls, attr_blacklist):\n    \"\"\"Initializes the current instance from an Aff4Object.\n\n    Iterates over all attributes of the Aff4Object defined by a given class\n    and adds a representation of them to the current instance.\n\n    Args:\n      aff4_obj: An Aff4Object to take the attributes from.\n      aff4_cls: A class in the inheritance hierarchy of the Aff4Object defining\n        which attributes to take.\n      attr_blacklist: A list of already added attributes as to not add\n        attributes multiple times.\n\n    Returns:\n      A reference to the current instance.\n    \"\"\"\n    self.name = str(aff4_cls.__name__)\n    self.attributes = []\n\n    schema = aff4_cls.SchemaCls\n    for name, attribute in sorted(iteritems(schema.__dict__)):\n      if not isinstance(attribute, aff4.Attribute):\n        continue\n\n      if name in attr_blacklist:\n        continue\n\n      attr_repr = ApiAff4ObjectAttribute()\n      attr_repr.name = name\n      attr_repr.description = attribute.description\n      attr_repr.values = []\n\n      values = list(aff4_obj.GetValuesForAttribute(attribute))\n      for value in values:\n        # This value is really a LazyDecoder() instance. We need to get at the\n        # real data here.\n        # TODO(user): Change GetValuesForAttribute to resolve\n        # lazy decoders and directly yield the rdf value.\n        if hasattr(value, \"ToRDFValue\"):\n          value = value.ToRDFValue()\n\n        value_repr = ApiAff4ObjectAttributeValue()\n        value_repr.Set(\"type\", compatibility.GetName(value.__class__))\n        value_repr.Set(\"age\", value.age)\n        value_repr.value = value\n        attr_repr.values.append(value_repr)\n\n      if attr_repr.values:\n        self.attributes.append(attr_repr)\n\n    return self",
        "rewrite": "```python\ndef init_from_aff4_object(self, aff4_obj, aff4_cls, attr_blacklist):\n    self.name = str(aff4_cls.__name__)\n    self.attributes = []\n\n    schema = aff4_cls.SchemaCls\n\n    for name, attribute in sorted(schema.__dict__.items()):\n        if not isinstance(attribute, aff4.Attribute):\n            continue\n\n        if name in attr_blacklist:\n            continue\n\n        attr_repr = ApiAff4ObjectAttribute()\n        attr_repr.name = name\n        attr_repr.description = attribute.description\n        attr_repr.values = []\n\n        values = list(aff4_obj.GetValuesFor"
    },
    {
        "original": "def setPartationId(self, partationId):\n        \"\"\"set Thread Network Partition ID\n\n        Args:\n            partitionId: partition id to be set by leader\n\n        Returns:\n            True: successful to set the Partition ID\n            False: fail to set the Partition ID\n        \"\"\"\n        print '%s call setPartationId' % self.port\n        print partationId\n\n        cmd = WPANCTL_CMD + 'setprop Network:PartitionId %s' %(str(hex(partationId)).rstrip('L'))\n        print cmd\n        return self.__sendCommand(cmd)[0] != 'Fail'",
        "rewrite": "```python\ndef set_partition_id(self, partition_id):\n    \"\"\"\n    Set Thread Network Partition ID\n\n    Args:\n        partition_id: ID to be set by the leader\n\n    Returns:\n        bool: Whether the set operation was successful\n    \"\"\"\n    print(f\"{self.port} call set_partition_id\")\n    print(partition_id)\n\n    command = f\"{WPANCTL_CMD}setprop Network:PartitionId {hex(int(partition_id))}\"\n    print(command)\n    \n    return self.__send_command(command)[0] != 'Fail'\n```\n\nNote that I have also assumed `__send_command` method is renamed"
    },
    {
        "original": "def get_values_json(self, lst, list_columns):\n        \"\"\"\n            Converts list of objects from query to JSON\n        \"\"\"\n        result = []\n        for item in self.get_values(lst, list_columns):\n            for key, value in list(item.items()):\n                if isinstance(value, datetime.datetime) or isinstance(\n                    value, datetime.date\n                ):\n                    value = value.isoformat()\n                    item[key] = value\n                if isinstance(value, list):\n                    item[key] = [str(v) for v in value]\n            result.append(item)\n        return result",
        "rewrite": "```python\ndef get_values_json(self, lst, list_columns):\n    result = []\n    for item in self.get_values(lst, list_columns):\n        item = {key: str(value) if isinstance(value, (datetime.datetime, datetime.date)) else value \n                for key, value in item.items()}\n        if isinstance(item['value'], list):\n            item['value'] = [str(v) for v in item['value']]\n        result.append(item)\n    return result\n```"
    },
    {
        "original": "def _gen_get_more_command(cursor_id, coll, batch_size, max_await_time_ms):\n    \"\"\"Generate a getMore command document.\"\"\"\n    cmd = SON([('getMore', cursor_id),\n               ('collection', coll)])\n    if batch_size:\n        cmd['batchSize'] = batch_size\n    if max_await_time_ms is not None:\n        cmd['maxTimeMS'] = max_await_time_ms\n    return cmd",
        "rewrite": "```python\ndef _gen_get_more_command(cursor_id, collection, batch_size=None, max_await_time_ms=None):\n    cmd = {\"getMore\": cursor_id, \"collection\": collection}\n    if batch_size is not None:\n        cmd[\"batchSize\"] = batch_size\n    if max_await_time_ms is not None:\n        cmd[\"maxTimeMS\"] = max_await_time_ms\n    return cmd\n```"
    },
    {
        "original": "def set_baudrate(self, channel, BTR, baudarate):\n        \"\"\"\n        This function is used to configure the baud rate of specific CAN channel of a device.\n\n        :param int channel:\n            CAN channel, which is to be configured (:data:`Channel.CHANNEL_CH0` or :data:`Channel.CHANNEL_CH1`).\n        :param int BTR:\n            Baud rate register BTR0 as high byte, baud rate register BTR1 as low byte (see enum :class:`Baudrate`).\n        :param int baudarate: Baud rate register for all systec USB-CANmoduls (see enum :class:`BaudrateEx`>).\n        \"\"\"\n        UcanSetBaudrateEx(self._handle, channel, BTR >> 8, BTR, baudarate)",
        "rewrite": "```python\ndef set_baudrate(self, channel, baud_rate_register_high_byte, baud_rate_register_low_byte):\n    \"\"\"\n    This function is used to configure the baud rate of specific CAN channel of a device.\n    \n    :param int channel:\n        CAN channel, which is to be configured (CHANNEL_CH0 or CHANNEL_CH1)\n    :param int baud_rate_register_high_byte:\n        Baud rate register BTR0 as high byte \n    :param int baud_rate_register_low_byte:\n        Baud rate register BTR1 as low byte\n    \"\"\"\n    \ndef set_baudrate_ex(self, channel, baudar"
    },
    {
        "original": "def _get_day_of_month(other, day_option):\n    \"\"\"Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    \"\"\"\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)",
        "rewrite": "```python\ndef _get_day_of_month(other, day_option):\n    days_in_month = _days_in_month(other)\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        return days_in_month\n    elif day_option is None:\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'configurations') and self.configurations is not None:\n            _dict['configurations'] = [\n                x._to_dict() for x in self.configurations\n            ]\n        return _dict",
        "rewrite": "```python\ndef _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'configurations') and self.configurations is not None:\n        _dict['configurations'] = [x._to_dict() for x in self.configurations]\n    return _dict\n```"
    },
    {
        "original": "def cli(self, method):\n        \"\"\"Registers a method on an Object as a CLI route\"\"\"\n        routes = getattr(method, '_hug_cli_routes', [])\n        routes.append(self.route)\n        method._hug_cli_routes = routes\n        return method",
        "rewrite": "```python\ndef cli(self, method):\n    routes = getattr(method, '_hug_cli_routes', [])\n    routes.append(self.route)\n    setattr(method, '_hug_cli_routes', routes)\n    return method\n```"
    },
    {
        "original": "def _run_and_measure(self, quil_program, qubits, trials, random_seed) -> np.ndarray:\n        \"\"\"\n        Run a Forest ``run_and_measure`` job.\n\n        Users should use :py:func:`WavefunctionSimulator.run_and_measure` instead of calling\n        this directly.\n        \"\"\"\n        payload = run_and_measure_payload(quil_program, qubits, trials, random_seed)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n        return np.asarray(response.json())",
        "rewrite": "```python\ndef _run_and_measure(self, quil_program, qubits, trials, random_seed):\n    payload = run_and_measure_payload(quil_program, qubits, trials, random_seed)\n    response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n    return np.array(response.json())\n```"
    },
    {
        "original": "def get_status(self):\n        \"\"\"\n        :calls: `GET /user/migrations/:migration_id`_\n        :rtype: str\n        \"\"\"\n        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url,\n            headers={\n                \"Accept\": Consts.mediaTypeMigrationPreview\n            }\n        )\n        self._useAttributes(data)\n        return self.state",
        "rewrite": "```python\ndef get_status(self):\n    headers, data = self._requester.requestJsonAndCheck(\n        \"GET\",\n        self.url,\n        headers={\"Accept\": Consts.mediaTypeMigrationPreview}\n    )\n    self._useAttributes(data)\n    return data.get(\"state\")\n```"
    },
    {
        "original": "def serialize_close(code: int, reason: str) -> bytes:\n    \"\"\"\n    Serialize the data for a close frame.\n\n    This is the reverse of :func:`parse_close`.\n\n    \"\"\"\n    check_close(code)\n    return struct.pack(\"!H\", code) + reason.encode(\"utf-8\")",
        "rewrite": "```python\nimport struct\n\ndef serialize_close(code: int, reason: str) -> bytes:\n    check_close(code)\n    return struct.pack(\"!H\", code) + reason.encode(\"utf-8\")\n```\n\nOr if `check_close` is defined elsewhere you could call it explicitly like so:\n\n```python\nimport inspect \n\ndef serialize_close(code: int, reason: str) -> bytes:\n    if 'check_close' in [name for name in globals()]+[name for name in dir(__builtins__) if not name.startswith(\"__\") and inspect.isfunction(getattr(__builtins__, name))]:\n        getattr(__"
    },
    {
        "original": "def configure_host_cache(host_ref, datastore_ref, swap_size_MiB,\n                         host_cache_manager=None):\n    \"\"\"\n    Configures the host cahe of the specified host\n\n    host_ref\n        The vim.HostSystem object representing the host that contains the\n        requested disks.\n\n    datastore_ref\n        The vim.Datastore opject representing the datastore the host cache will\n        be configured on.\n\n    swap_size_MiB\n        The size in Mibibytes of the swap.\n\n    host_cache_manager\n        The vim.HostCacheConfigurationManager object representing the cache\n        configuration manager on the specified host. Default is None. If None,\n        it will be retrieved in the method\n    \"\"\"\n    hostname = get_managed_object_name(host_ref)\n    if not host_cache_manager:\n        props = get_properties_of_managed_object(\n            host_ref, ['configManager.cacheConfigurationManager'])\n        if not props.get('configManager.cacheConfigurationManager'):\n            raise salt.exceptions.VMwareObjectRetrievalError(\n                'Host \\'{0}\\' has no host cache'.format(hostname))\n        host_cache_manager = props['configManager.cacheConfigurationManager']\n    log.trace('Configuring the host cache on host \\'%s\\', datastore \\'%s\\', '\n              'swap size=%s MiB', hostname, datastore_ref.name, swap_size_MiB)\n\n    spec = vim.HostCacheConfigurationSpec(\n        datastore=datastore_ref,\n        swapSize=swap_size_MiB)\n    log.trace('host_cache_spec=%s', spec)\n    try:\n        task = host_cache_manager.ConfigureHostCache_Task(spec)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    wait_for_task(task, hostname, 'HostCacheConfigurationTask')\n    log.trace('Configured host cache on host \\'%s\\'', hostname)\n    return True",
        "rewrite": "```python\ndef configure_host_cache(host_ref, datastore_ref, swap_size_MiB, host_cache_manager=None):\n    hostname = get_managed_object_name(host_ref)\n    if not host_cache_manager:\n        props = get_properties_of_managed_object(host_ref, ['configManager.cacheConfigurationManager'])\n        if not props.get('configManager.cacheConfigurationManager'):\n            raise salt.exceptions.VMwareObjectRetrievalError(f\"Host '{hostname}' has no host cache\")\n        host_cache_manager = props['configManager.cacheConfigurationManager']\n    log.trace(\"Configuring the host cache on host '%s', datastore '%s', swap size=%"
    },
    {
        "original": "def tearDown(self):\n        \"\"\"Clean up after each case.\n\n        Stop harness service, close browser and close DUT.\n        \"\"\"\n        if self.__class__ is HarnessCase:\n            return\n\n        logger.info('Tearing down')\n        self._destroy_harness()\n        self._destroy_browser()\n        self._destroy_dut()\n        self._destroy_rf_shield()",
        "rewrite": "```python\ndef tearDown(self):\n    logger.info('Tearing down')\n    if self.__class__ is HarnessCase:\n        return\n    \n    self._destroy_harness()\n    self._destroy_browser()\n    self._destroy_dut()\n    self._destroy_rf_shield()\n```"
    },
    {
        "original": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)",
        "rewrite": "```python\ndef add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n    \"\"\"Register the subscriber for the given event subscriptions.\n\n    Raises:\n        InvalidFilterError\n            One of the filters in the subscriptions is invalid.\n    \"\"\"\n    with self._subscribers_cv:\n        self._subscribers[connection_id] = EventSubscriber(\n            connection_id, subscriptions, last_known_block_id)\n\n    LOGGER.debug('Added Subscriber %s for %s', connection_id, subscriptions)\n```"
    },
    {
        "original": "def assert_coordinate_consistent(obj, coords):\n    \"\"\" Maeke sure the dimension coordinate of obj is\n    consistent with coords.\n\n    obj: DataArray or Dataset\n    coords: Dict-like of variables\n    \"\"\"\n    for k in obj.dims:\n        # make sure there are no conflict in dimension coordinates\n        if k in coords and k in obj.coords:\n            if not coords[k].equals(obj[k].variable):\n                raise IndexError(\n                    'dimension coordinate {!r} conflicts between '\n                    'indexed and indexing objects:\\n{}\\nvs.\\n{}'\n                    .format(k, obj[k], coords[k]))",
        "rewrite": "```python\ndef assert_coordinate_consistent(obj, coords):\n    \"\"\"Ensure the dimension coordinate of obj is consistent with coords.\"\"\"\n    for k in obj.dims:\n        if k in coords and k in obj.coords:\n            if not (coords[k].equals(obj.coords[k]) or \n                    (coords[k].size == 1 and coords[k].equals(obj.coords[k][0]))):\n                raise IndexError(\n                    f'dimension coordinate {k!r} conflicts between '\n                    f'indexed and indexing objects:\\n{obj.coords[k]}\\nvs.\\n{coords[k]}')\n```"
    },
    {
        "original": "def reply_chat_action(\n        self,\n        action: Union[ChatAction, str],\n        progress: int = 0\n    ) -> \"Message\":\n        \"\"\"Bound method *reply_chat_action* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_chat_action(\n                chat_id=message.chat.id,\n                action=\"typing\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_chat_action(\"typing\")\n\n        Args:\n            action (:obj:`ChatAction <pyrogram.ChatAction>` | ``str``):\n                Type of action to broadcast.\n                Choose one from the :class:`ChatAction <pyrogram.ChatAction>` enumeration,\n                depending on what the user is about to receive.\n                You can also provide a string (e.g. \"typing\", \"upload_photo\", \"record_audio\", ...).\n\n            progress (``int``, *optional*):\n                Progress of the upload process.\n                Currently useless because official clients don't seem to be handling this.\n\n        Returns:\n            On success, True is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n            ``ValueError`` if the provided string is not a valid ChatAction.\n        \"\"\"\n        return self._client.send_chat_action(\n            chat_id=self.chat.id,\n            action=action,\n            progress=progress\n        )",
        "rewrite": "```python\nfrom pyrogram import ChatAction\nfrom pyrogram.types import Message\nfrom typing import Union\n\ndef reply_chat_action(\n    self,\n    action: Union[ChatAction, str],\n    progress: int = 0\n) -> Message:\n    try:\n        message = self._client.send_chat_action(\n            chat_id=self.chat.id,\n            action=action.value if isinstance(action, ChatAction) else action,\n            progress=progress\n        )\n        return message\n    except Exception as e:\n        # Handle the exception when sending chat action fails.\n        pass  # Replace with actual error handling code.\n\n```"
    },
    {
        "original": "def write(self, path):\n        \"\"\"Create a new object with the desired output schema and write it.\"\"\"\n        meta = {\n            'generated_at': self.generated_at,\n            'elapsed_time': self.elapsed_time,\n        }\n        sources = {}\n        for result in self.results:\n            unique_id = result.node.unique_id\n            if result.error is not None:\n                result_dict = {\n                    'error': result.error,\n                    'state': 'runtime error'\n                }\n            else:\n                result_dict = {\n                    'max_loaded_at': result.max_loaded_at,\n                    'snapshotted_at': result.snapshotted_at,\n                    'max_loaded_at_time_ago_in_s': result.age,\n                    'state': result.status,\n                    'criteria': result.node.freshness,\n                }\n            sources[unique_id] = result_dict\n        output = FreshnessRunOutput(meta=meta, sources=sources)\n        output.write(path)",
        "rewrite": "```python\ndef write(self, path):\n    meta = {\n        'generated_at': self.generated_at,\n        'elapsed_time': self.elapsed_time,\n    }\n    output = FreshnessRunOutput(meta=meta)\n    \n    for result in self.results:\n        unique_id = result.node.unique_id\n        if result.error is not None:\n            output.sources[unique_id] = {\n                'error': result.error,\n                'state': 'runtime error'\n            }\n        else:\n            output.sources[unique_id] = {\n                'max_loaded_at': result.max_loaded_at,\n                'snapshotted_at': result.snapsh"
    },
    {
        "original": "def FromMany(cls, samples):\n    \"\"\"Constructs a single sample that best represents a list of samples.\n\n    Args:\n      samples: An iterable collection of `CpuSample` instances.\n\n    Returns:\n      A `CpuSample` instance representing `samples`.\n\n    Raises:\n      ValueError: If `samples` is empty.\n    \"\"\"\n    if not samples:\n      raise ValueError(\"Empty `samples` argument\")\n\n    # It only makes sense to average the CPU percentage. For all other values\n    # we simply take the biggest of them.\n    cpu_percent = sum(sample.cpu_percent for sample in samples) / len(samples)\n\n    return CpuSample(\n        timestamp=max(sample.timestamp for sample in samples),\n        cpu_percent=cpu_percent,\n        user_cpu_time=max(sample.user_cpu_time for sample in samples),\n        system_cpu_time=max(sample.system_cpu_time for sample in samples))",
        "rewrite": "```python\ndef from_many(cls, samples):\n    if not samples:\n        raise ValueError(\"Empty `samples` argument\")\n\n    cpu_percent = sum(sample.cpu_percent for sample in samples) / len(samples)\n\n    return cls(\n        timestamp=max(sample.timestamp for sample in samples),\n        cpu_percent=cpu_percent,\n        user_cpu_time=max(sample.user_cpu_time for sample in samples),\n        system_cpu_time=max(sample.system_cpu_time for sample in samples))\n```"
    },
    {
        "original": "def load_api_specific_resource_module(resource_name, api=None):\n    \"\"\"\n    Load the module from the JSON files and return a dict, which might be empty\n    if the resource could not be loaded.\n\n    If no api version is given, the default one from the CONF dict is used.\n\n    :param resource_name: Name of the resource to load\n    :param api: API version\n    :return: dict\n    \"\"\"\n    loader = dict(aosp_permissions=load_permissions,\n                  api_permission_mappings=load_permission_mappings)\n\n    if resource_name not in loader:\n        raise InvalidResourceError(\"Invalid Resource '{}', not in [{}]\".format(resource_name, \", \".join(loader.keys())))\n\n    if not api:\n        api = CONF[\"DEFAULT_API\"]\n\n    ret = loader[resource_name](api)\n\n    if ret == {}:\n        # No API mapping found, return default\n        log.warning(\"API mapping for API level {} was not found! \"\n                    \"Returning default, which is API level {}\".format(api, CONF['DEFAULT_API']))\n        ret = loader[resource_name](CONF['DEFAULT_API'])\n\n    return ret",
        "rewrite": "```python\ndef load_api_specific_resource_module(resource_name, api=None):\n    \"\"\"\n    Load the module from the JSON files and return a dict.\n    If no api version is given, the default one from the CONF dict is used.\n    \n    :param resource_name: Name of the resource to load\n    :param api: API version\n    :return: dict\n    \"\"\"\n    \n    loader = {\n        \"aosp_permissions\": load_permissions,\n        \"api_permission_mappings\": load_permission_mappings\n    }\n    \n    if resource_name not in loader:\n        raise ValueError(f\"Invalid Resource '{resource_name}', not in {',"
    },
    {
        "original": "async def run_forever(self, relax=0.1, offset=None, timeout=20, allowed_updates=None):\n        \"\"\"\n        Process new updates in infinity loop\n\n        :param relax: float\n        :param offset: int\n        :param timeout: int\n        :param allowed_updates: bool\n        \"\"\"\n        while 1:\n            try:\n                result = await self._bot.getUpdates(offset=offset,\n                                                    timeout=timeout,\n                                                    allowed_updates=allowed_updates)\n\n                # Once passed, this parameter is no longer needed.\n                allowed_updates = None\n\n                # No sort. Trust server to give messages in correct order.\n                for update in result:\n                    self._update_handler(update)\n                    offset = update['update_id'] + 1\n\n            except CancelledError:\n                break\n            except exception.BadHTTPResponse as e:\n                traceback.print_exc()\n\n                # Servers probably down. Wait longer.\n                if e.status == 502:\n                    await asyncio.sleep(30)\n            except:\n                traceback.print_exc()\n                await asyncio.sleep(relax)\n            else:\n                await asyncio.sleep(relax)",
        "rewrite": "```python\nimport logging\nfrom aiohttp import ClientError\n\nasync def run_forever(self, relax: float = 0.1, offset: int = None, timeout: int = 20,\n                      allowed_updates: bool | list[str] | None = None) -> None:\n    while True:\n        try:\n            result = await self._bot.getUpdates(offset=offset, timeout=timeout,\n                                                   allowed_updates=allowed_updates)\n            # Once passed, this parameter is no longer needed.\n            allowed_updates = None\n\n            # No sort. Trust server to give messages in correct order.\n            for update in result['"
    },
    {
        "original": "def huffman_conv2bitstring(cls, s):\n        # type: (str) -> Tuple[int, int]\n        \"\"\" huffman_conv2bitstring converts a string into its bitstring\n        representation. It returns a tuple: the bitstring and its bitlength.\n        This function DOES NOT compress/decompress the string!\n\n        @param str s: the bytestring to convert.\n        @return (int, int): the bitstring of s, and its bitlength.\n        @raise AssertionError\n        \"\"\"\n        i = 0\n        ibl = len(s) * 8\n        for c in s:\n            i = (i << 8) + orb(c)\n\n        ret = i, ibl\n        assert(ret[0] >= 0)\n        assert(ret[1] >= 0)\n        return ret",
        "rewrite": "```python\ndef huffman_conv2bitstring(s: str) -> tuple[int, int]:\n    i = 0\n    ibl = len(s) * 8\n    for c in s:\n        i = (i << 8) + ord(c)\n\n    ret = i, ibl\n    assert ret[0] >= 0\n    assert ret[1] >= 0\n    return ret\n```"
    },
    {
        "original": "def get_analysis(self, current_class):\n        \"\"\"\n        Returns the :class:`~androguard.core.analysis.analysis.Analysis` object\n        which contains the `current_class`.\n\n        :param current_class: The class to search for\n        :type current_class: androguard.core.bytecodes.dvm.ClassDefItem\n        :rtype: androguard.core.analysis.analysis.Analysis\n        \"\"\"\n        for digest in self.analyzed_vms:\n            dx = self.analyzed_vms[digest]\n            if dx.is_class_present(current_class.get_name()):\n                return dx\n        return None",
        "rewrite": "```python\ndef get_analysis(self, current_class):\n    return next((dx for digest, dx in self.analyzed_vms.items() if dx.is_class_present(current_class.get_name())), None)\n```"
    },
    {
        "original": "def apply_transformation(self, structure):\n        \"\"\"\n        Returns most primitive cell for structure.\n\n        Args:\n            structure: A structure\n\n        Returns:\n            The same structure in a conventional standard setting\n        \"\"\"\n        sga = SpacegroupAnalyzer(structure, symprec=self.symprec,\n                                 angle_tolerance=self.angle_tolerance)\n        return sga.get_conventional_standard_structure(international_monoclinic=self.international_monoclinic)",
        "rewrite": "def apply_transformation(self, structure):\n    sga = SpacegroupAnalyzer(structure, symprec=self.symprec,\n                            angle_tolerance=self.angle_tolerance)\n    return sga.get_conventional_standard_structure(international_monoclinic=True)"
    },
    {
        "original": "def get_object(self, cont, obj, local_file=None, return_bin=False):\n        \"\"\"\n        Retrieve a file from Swift\n        \"\"\"\n        try:\n            if local_file is None and return_bin is False:\n                return False\n\n            headers, body = self.conn.get_object(cont, obj, resp_chunk_size=65536)\n\n            if return_bin is True:\n                fp = sys.stdout\n            else:\n                dirpath = dirname(local_file)\n                if dirpath and not isdir(dirpath):\n                    mkdirs(dirpath)\n                fp = salt.utils.files.fopen(local_file, 'wb')  # pylint: disable=resource-leakage\n\n            read_length = 0\n            for chunk in body:\n                read_length += len(chunk)\n                fp.write(chunk)\n            fp.close()\n            return True\n\n        # ClientException\n        # file/dir exceptions\n        except Exception as exc:\n            log.error('There was an error::')\n            if hasattr(exc, 'code') and hasattr(exc, 'msg'):\n                log.error('    Code: %s: %s', exc.code, exc.msg)\n            log.error('    Content: \\n%s', getattr(exc, 'read', lambda: six.text_type(exc))())\n            return False",
        "rewrite": "```python\nimport os\nimport sys\nimport logging\n\ndef get_object(self, container, object_name, local_file=None, return_binary=False):\n    try:\n        if not local_file and not return_binary:\n            return False\n\n        headers, body = self.conn.get_object(container, object_name)\n\n        if return_binary:\n            fp = sys.stdout.buffer  # Use buffer mode for binary output\n        else:\n            dirpath = os.path.dirname(local_file)\n            if dirpath and not os.path.isdir(dirpath):\n                os.makedirs(dirpath)\n            fp = open(local_file, 'wb')\n\n        read_length = 0"
    },
    {
        "original": "def new(ext, content_type):\n        \"\"\"\n        Return a new ``<Default>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        xml = '<Default xmlns=\"%s\"/>' % nsmap['ct']\n        default = parse_xml(xml)\n        default.set('Extension', ext)\n        default.set('ContentType', content_type)\n        return default",
        "rewrite": "```python\ndef new(ext, content_type):\n    xml = f'<Default xmlns=\"{nsmap[\"ct\"]}\"/>'\n    default = parse_xml(xml)\n    default.set('Extension', ext)\n    default.set('ContentType', content_type)\n    return default\n```"
    },
    {
        "original": "def _set_conf_sockets():\n    \"\"\"Populate the conf.L2Socket and conf.L3Socket\n    according to the various use_* parameters\n    \"\"\"\n    if conf.use_bpf and not BSD:\n        Interceptor.set_from_hook(conf, \"use_bpf\", False)\n        raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n    if conf.use_winpcapy and not WINDOWS:\n        Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n        raise ScapyInvalidPlatformException(\"Windows only !\")\n    # we are already in an Interceptor hook, use Interceptor.set_from_hook\n    if conf.use_pcap or conf.use_dnet or conf.use_winpcapy:\n        try:\n            from scapy.arch.pcapdnet import L2pcapListenSocket, L2pcapSocket, \\\n                L3pcapSocket\n        except ImportError:\n            warning(\"No pcap provider available ! pcap won't be used\")\n            Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n            Interceptor.set_from_hook(conf, \"use_pcap\", False)\n        else:\n            conf.L3socket = L3pcapSocket\n            conf.L3socket6 = functools.partial(L3pcapSocket, filter=\"ip6\")\n            conf.L2socket = L2pcapSocket\n            conf.L2listen = L2pcapListenSocket\n            return\n    if conf.use_bpf:\n        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\\n            L2bpfSocket, L3bpfSocket\n        conf.L3socket = L3bpfSocket\n        conf.L3socket6 = functools.partial(L3bpfSocket, filter=\"ip6\")\n        conf.L2socket = L2bpfSocket\n        conf.L2listen = L2bpfListenSocket\n        return\n    if LINUX:\n        from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket\n        conf.L3socket = L3PacketSocket\n        conf.L3socket6 = functools.partial(L3PacketSocket, filter=\"ip6\")\n        conf.L2socket = L2Socket\n        conf.L2listen = L2ListenSocket\n        return\n    if WINDOWS:\n        from scapy.arch.windows import _NotAvailableSocket\n        from scapy.arch.windows.native import L3WinSocket, L3WinSocket6\n        conf.L3socket = L3WinSocket\n        conf.L3socket6 = L3WinSocket6\n        conf.L2socket = _NotAvailableSocket\n        conf.L2listen = _NotAvailableSocket\n        return\n    from scapy.supersocket import L3RawSocket\n    from scapy.layers.inet6 import L3RawSocket6\n    conf.L3socket = L3RawSocket\n    conf.L3socket6 = L3RawSocket6",
        "rewrite": "```python\ndef _set_conf_sockets():\n    \"\"\"Populate the conf.L2Socket and conf.L3Socket\n    according to the various use_* parameters\n    \"\"\"\n    \n    if conf.use_bpf and not BSD:\n        Interceptor.set_from_hook(conf, \"use_bpf\", False)\n        raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n        \n    if conf.use_winpcapy and not WINDOWS:\n        Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n        raise ScapyInvalidPlatformException(\"Windows only !\")\n        \n    if (conf.use"
    },
    {
        "original": "def get_location_posts(self, location: str) -> Iterator[Post]:\n        \"\"\"Get Posts which are listed by Instagram for a given Location.\n\n        :return:  Iterator over Posts of a location's posts\n\n        .. versionadded:: 4.2\n        \"\"\"\n        has_next_page = True\n        end_cursor = None\n        while has_next_page:\n            if end_cursor:\n                params = {'__a': 1, 'max_id': end_cursor}\n            else:\n                params = {'__a': 1}\n            location_data = self.context.get_json('explore/locations/{0}/'.format(location),\n                                                  params)['graphql']['location']['edge_location_to_media']\n            yield from (Post(self.context, edge['node']) for edge in location_data['edges'])\n            has_next_page = location_data['page_info']['has_next_page']\n            end_cursor = location_data['page_info']['end_cursor']",
        "rewrite": "```python\ndef get_location_posts(self, location: str) -> Iterator[Post]:\n    has_next_page = True\n    end_cursor = None\n    while has_next_page:\n        if end_cursor:\n            params = {'__a': 1, 'max_id': end_cursor}\n        else:\n            params = {'__a': 1}\n        location_data = self.context.get_json(f'explore/locations/{location}/', params)['graphql']['location']['edge_location_to_media']\n        \n        yield from (Post(self.context, edge['node']) for edge in location_data['edges'])\n        \n        if 'end_cursor"
    },
    {
        "original": "def add_peer_parser(subparsers, parent_parser):\n    \"\"\"Adds argument parser for the peer command\n\n        Args:\n            subparsers: Add parsers to this subparser object\n            parent_parser: The parent argparse.ArgumentParser object\n    \"\"\"\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)",
        "rewrite": "```python\ndef add_peer_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)\n```"
    },
    {
        "original": "def _parse_launch_error(data):\n        \"\"\"\n        Parses a LAUNCH_ERROR message and returns a LaunchFailure object.\n\n        :type data: dict\n        :rtype: LaunchFailure\n        \"\"\"\n        return LaunchFailure(\n            data.get(ERROR_REASON, None),\n            data.get(APP_ID),\n            data.get(REQUEST_ID),\n        )",
        "rewrite": "```python\ndef _parse_launch_error(data):\n    return LaunchFailure(\n        data.get('error_reason', None),\n        data.get('app_id'),\n        data.get('request_id'),\n    )\n```"
    },
    {
        "original": "def validate_timeout_or_zero(option, value):\n    \"\"\"Validates a timeout specified in milliseconds returning\n    a value in floating point seconds for the case where None is an error\n    and 0 is valid. Setting the timeout to nothing in the URI string is a\n    config error.\n    \"\"\"\n    if value is None:\n        raise ConfigurationError(\"%s cannot be None\" % (option, ))\n    if value == 0 or value == \"0\":\n        return 0\n    return validate_positive_float(option, value) / 1000.0",
        "rewrite": "```python\ndef validate_timeout_or_zero(option, value):\n    if isinstance(value, str) and not value:\n        raise ConfigurationError(\"%s cannot be an empty string\" % option)\n    if value is None:\n        raise ConfigurationError(\"%s cannot be None\" % option)\n    if (value == 0 or str(value) == \"0\") and not isinstance(value, bool):\n        return type(value)(0)\n    try:\n        return validate_positive_float(option, float(value)) / 1000.0\n    except ValueError:\n        raise ConfigurationError(\"Invalid timeout '%s' as %r for option '%s'\""
    },
    {
        "original": "def list_storage_services(conn=None, call=None):\n    \"\"\"\n    List VMs on this Azure account, with full information\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            ('The list_storage_services function must be called '\n             'with -f or --function.')\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    ret = {}\n    accounts = conn.list_storage_accounts()\n    for service in accounts.storage_services:\n        ret[service.service_name] = {\n            'capabilities': service.capabilities,\n            'service_name': service.service_name,\n            'storage_service_properties': service.storage_service_properties,\n            'extended_properties': service.extended_properties,\n            'storage_service_keys': service.storage_service_keys,\n            'url': service.url,\n        }\n    return ret",
        "rewrite": "```python\ndef list_storage_services(conn=None, call='function'):\n    if call != 'function':\n        raise SystemExit(\n            ('The list_storage_services function must be called '\n             'with -f or --function.')\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    ret = {}\n    accounts = conn.list_storage_accounts()\n    for service in accounts.storage_services or []:\n        ret[service.service_name] = {\n            'capabilities': service.capabilities,\n            'service_name': service.service_name,\n            'storage_service_properties': service.storage_service_properties,\n            'extended_properties': service.extended_properties,\n           "
    },
    {
        "original": "def list_nodes_full(kwargs=None, call=None):\n    \"\"\"\n    All information available about all nodes should be returned in this function.\n    The fields in the list_nodes() function should also be returned,\n    even if they would not normally be provided by the cloud provider.\n\n    This is because some functions both within Salt and 3rd party will break if an expected field is not present.\n    This function is normally called with the -F option:\n\n\n    .. code-block:: bash\n\n        salt-cloud -F\n\n\n    @param kwargs:\n    @type kwargs:\n    @param call:\n    @type call:\n    @return:\n    @rtype:\n    \"\"\"\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called '\n            'with -f or --function.'\n        )\n\n    machines = {}\n\n    # TODO ask for the correct attributes e.g state and private_ips\n    for machine in vb_list_machines():\n        name = machine.get(\"name\")\n        if name:\n            machines[name] = treat_machine_dict(machine)\n            del machine[\"name\"]\n\n    return machines",
        "rewrite": "```python\ndef list_nodes_full(kwargs=None, call=None):\n    \"\"\"\n    All information available about all nodes should be returned in this function.\n    The fields in the list_nodes() function should also be returned,\n    even if they would not normally be provided by the cloud provider.\n\n    This is because some functions both within Salt and 3rd party will break if an expected field is not present.\n    This function is normally called with the -F option:\n\n\n    .. code-block:: bash\n\n        salt-cloud -F\n\n\n    @param kwargs:\n    @type kwargs:\n    @param call:\n    @type call:\n    @"
    },
    {
        "original": "async def send_code_request(self, phone, *, force_sms=False):\n        \"\"\"\n        Sends a code request to the specified phone number.\n\n        Args:\n            phone (`str` | `int`):\n                The phone to which the code will be sent.\n\n            force_sms (`bool`, optional):\n                Whether to force sending as SMS.\n\n        Returns:\n            An instance of :tl:`SentCode`.\n        \"\"\"\n        phone = utils.parse_phone(phone) or self._phone\n        phone_hash = self._phone_code_hash.get(phone)\n\n        if not phone_hash:\n            try:\n                result = await self(functions.auth.SendCodeRequest(\n                    phone, self.api_id, self.api_hash, types.CodeSettings()))\n            except errors.AuthRestartError:\n                return self.send_code_request(phone, force_sms=force_sms)\n\n            self._tos = result.terms_of_service\n            self._phone_code_hash[phone] = phone_hash = result.phone_code_hash\n        else:\n            force_sms = True\n\n        self._phone = phone\n\n        if force_sms:\n            result = await self(\n                functions.auth.ResendCodeRequest(phone, phone_hash))\n\n            self._phone_code_hash[phone] = result.phone_code_hash\n\n        return result",
        "rewrite": "```python\nasync def send_code_request(self, phone, *, force_sms=False):\n    phone = utils.parse_phone(phone) or self._phone\n    phone_hash = self._phone_code_hash.get(phone)\n    \n    if not phone_hash:\n        try:\n            result = await self(functions.auth.SendCodeRequest(\n                phone, self.api_id, self.api_hash, types.CodeSettings()))\n        except errors.AuthRestartError:\n            return await self.send_code_request(phone, force_sms=force_sms)\n        \n        self._tos = result.terms_of_service\n        self._phone_code_HASH[phone] = result.phone_code_hash\n    \n"
    },
    {
        "original": "def _set_flow_entry(self, datapath, actions, in_port, dst, src=None):\n        \"\"\"set a flow entry.\"\"\"\n        set_flow = self._set_flow_func.get(datapath.ofproto.OFP_VERSION)\n        assert set_flow\n        set_flow(datapath, actions, in_port, dst, src)",
        "rewrite": "```python\ndef _set_flow_entry(self, datapath: object, actions: list, in_port: int, dst: str, src=None) -> None:\n    set_flow = self._set_flow_func.get(datapath.ofproto.OFP_VERSION)\n    assert set_flow is not None\n    set_flow(datapath, actions, in_port, dst, src)\n```"
    },
    {
        "original": "async def start(self):\n        \"\"\"\n        Start discarding media.\n        \"\"\"\n        for track, task in self.__tracks.items():\n            if task is None:\n                self.__tracks[track] = asyncio.ensure_future(blackhole_consume(track))",
        "rewrite": "async def start(self):\n    for track, task in self.__tracks.items():\n        if task is None:\n            self.__tracks[track] = asyncio.create_task(blackhole_consume(track))"
    },
    {
        "original": "def set_xlabels(self, label=None, **kwargs):\n        \"\"\"Label the x axis on the bottom row of the grid.\"\"\"\n        if label is None:\n            label = label_from_attrs(self.data[self._x_var])\n        for ax in self._bottom_axes:\n            ax.set_xlabel(label, **kwargs)\n        return self",
        "rewrite": "def set_xlabels(self, label=None, **kwargs):\n    if label is None:\n        label = self.data[self._x_var].name\n    for ax in self._bottom_axes:\n        ax.set_xlabel(label, **kwargs)\n    return self"
    },
    {
        "original": "def blockvisit(self, nodes, frame):\n        \"\"\"Visit a list of nodes as block in a frame.  If the current frame\n        is no buffer a dummy ``if 0: yield None`` is written automatically\n        unless the force_generator parameter is set to False.\n        \"\"\"\n        if frame.buffer is None:\n            self.writeline('if 0: yield None')\n        else:\n            self.writeline('pass')\n        try:\n            for node in nodes:\n                self.visit(node, frame)\n        except CompilerExit:\n            pass",
        "rewrite": "```python\ndef blockvisit(self, nodes, frame):\n    if not frame.buffer:\n        self.writeline('if 0: yield None')\n    else:\n        self.writeline('pass')\n    try:\n        for node in nodes:\n            self.visit(node, frame)\n    except CompilerExit:\n        pass\n```"
    },
    {
        "original": "def update(self, rotate_worker_token=False, rotate_manager_token=False,\n               **kwargs):\n        \"\"\"\n        Update the swarm's configuration.\n\n        It takes the same arguments as :py:meth:`init`, except\n        ``advertise_addr``, ``listen_addr``, and ``force_new_cluster``. In\n        addition, it takes these arguments:\n\n        Args:\n            rotate_worker_token (bool): Rotate the worker join token. Default:\n                ``False``.\n            rotate_manager_token (bool): Rotate the manager join token.\n                Default: ``False``.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        \"\"\"\n        # this seems to have to be set\n        if kwargs.get('node_cert_expiry') is None:\n            kwargs['node_cert_expiry'] = 7776000000000000\n\n        return self.client.api.update_swarm(\n            version=self.version,\n            swarm_spec=self.client.api.create_swarm_spec(**kwargs),\n            rotate_worker_token=rotate_worker_token,\n            rotate_manager_token=rotate_manager_token\n        )",
        "rewrite": "```python\ndef update(self, rotate_worker_token=False, rotate_manager_token=False, **kwargs):\n    \"\"\"\n    Update the swarm's configuration.\n\n    It takes the same arguments as :py:meth:`init`, except\n    ``advertise_addr``, ``listen_addr``, and ``force_new_cluster``. In\n    addition, it takes these arguments:\n\n    Args:\n        rotate_worker_token (bool): Rotate the worker join token. Default: False.\n        rotate_manager_token (bool): Rotate the manager join token. Default: False.\n\n    Raises:\n        docker.errors.APIError: If the server returns an error.\n    \"\"\"\n    \n"
    },
    {
        "original": "def image_to_osd(image,\n                 lang='osd',\n                 config='',\n                 nice=0,\n                 output_type=Output.STRING):\n    \"\"\"\n    Returns string containing the orientation and script detection (OSD)\n    \"\"\"\n    config = '{}-psm 0 {}'.format(\n        '' if get_tesseract_version() < '3.05' else '-',\n        config.strip()\n    ).strip()\n    args = [image, 'osd', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()",
        "rewrite": "```python\ndef image_to_osd(image, lang='osd', config='', nice=0, output_type=Output.STRING):\n    config = '{}-psm 0 {}'.format(\n        '' if get_tesseract_version() < '3.05' else '-', \n        config.strip()\n    ).strip()\n    args = [image, 'osd', lang, config, str(nice)]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run"
    },
    {
        "original": "def show(self):\n        \"\"\"\n        Prints the content of this method to stdout.\n\n        This will print the method signature and the decompiled code.\n        \"\"\"\n        args, ret = self.method.get_descriptor()[1:].split(\")\")\n        if self.code:\n            # We patch the descriptor here and add the registers, if code is available\n            args = args.split(\" \")\n\n            reg_len = self.code.get_registers_size()\n            nb_args = len(args)\n\n            start_reg = reg_len - nb_args\n            args = [\"{} v{}\".format(a, start_reg + i) for i, a in enumerate(args)]\n\n        print(\"METHOD {} {} {} ({}){}\".format(\n              self.method.get_class_name(),\n              self.method.get_access_flags_string(),\n              self.method.get_name(),\n              \", \".join(args), ret))\n        bytecode.PrettyShow(self, self.basic_blocks.gets(), self.method.notes)",
        "rewrite": "```python\ndef show(self):\n    args, ret = self.method.get_descriptor()[1:].split(\")\")\n    if self.code:\n        args = args.split(\" \")\n        reg_len = self.code.get_registers_size()\n        nb_args = len(args)\n        start_reg = reg_len - nb_args\n        args = [f\"{a} v{start_reg + i}\" for i, a in enumerate(args)]\n    print(f\"METHOD {self.method.get_class_name()} {self.method.get_access_flags_string()} {self.method.get_name()} ({', '.join(args)}){ret}\")\n    bytecode.PrettyShow(self, self.basic"
    },
    {
        "original": "def BuildDefaultValue(self, value_cls):\n    \"\"\"Renders default value of a given class.\n\n    Args:\n      value_cls: Default value of this class will be rendered. This class has to\n        be (or to be a subclass of) a self.value_class (i.e. a class that this\n        renderer is capable of rendering).\n\n    Returns:\n      An initialized default value.\n\n    Raises:\n      DefaultValueError: if something goes wrong.\n    \"\"\"\n    try:\n      return value_cls()\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(e)\n      raise DefaultValueError(\n          \"Can't create default for value %s: %s\" % (value_cls.__name__, e))",
        "rewrite": "```python\nfrom your_module import DefaultValueError\n\ndef BuildDefaultValue(self, value_cls):\n    try:\n        return value_cls()\n    except Exception as e:\n        raise DefaultValueError(\n            f\"Failed to create default for value {value_cls.__name__}: {e}\")\n```"
    },
    {
        "original": "def add_picture(self, image_path_or_stream, width=None, height=None):\n        \"\"\"\n        Return a new picture shape added in its own paragraph at the end of\n        the document. The picture contains the image at\n        *image_path_or_stream*, scaled based on *width* and *height*. If\n        neither width nor height is specified, the picture appears at its\n        native size. If only one is specified, it is used to compute\n        a scaling factor that is then applied to the unspecified dimension,\n        preserving the aspect ratio of the image. The native size of the\n        picture is calculated using the dots-per-inch (dpi) value specified\n        in the image file, defaulting to 72 dpi if no value is specified, as\n        is often the case.\n        \"\"\"\n        run = self.add_paragraph().add_run()\n        return run.add_picture(image_path_or_stream, width, height)",
        "rewrite": "```python\ndef add_picture(self, image_path_or_stream, width=None, height=None):\n    run = self.add_paragraph().add_run()\n    return run.add_picture(image_path_or_stream, width=width, height=height)\n```"
    },
    {
        "original": "async def process_request(self, path, headers):\n        \"\"\"\n        This hook is called to determine if the websocket should return\n        an HTTP response and close.\n\n        Our behavior here is to start the ASGI application, and then wait\n        for either `accept` or `close` in order to determine if we should\n        close the connection.\n        \"\"\"\n        path_portion, _, query_string = path.partition(\"?\")\n\n        websockets.handshake.check_request(headers)\n\n        subprotocols = []\n        for header in headers.get_all(\"Sec-WebSocket-Protocol\"):\n            subprotocols.extend([token.strip() for token in header.split(\",\")])\n\n        asgi_headers = [\n            (name.encode(\"ascii\"), value.encode(\"ascii\"))\n            for name, value in headers.raw_items()\n        ]\n\n        self.scope = {\n            \"type\": \"websocket\",\n            \"scheme\": self.scheme,\n            \"server\": self.server,\n            \"client\": self.client,\n            \"root_path\": self.root_path,\n            \"path\": unquote(path_portion),\n            \"query_string\": query_string.encode(\"ascii\"),\n            \"headers\": asgi_headers,\n            \"subprotocols\": subprotocols,\n        }\n        task = self.loop.create_task(self.run_asgi())\n        task.add_done_callback(self.on_task_complete)\n        self.tasks.add(task)\n        await self.handshake_started_event.wait()\n        return self.initial_response",
        "rewrite": "```python\nasync def process_request(self, path, headers):\n    path_portion, _, query_string = path.partition(\"?\")\n    websockets.handshake.check_request(headers)\n    subprotocols = [token.strip() for token in (header.split(\",\") if header else []) for header in headers.get_all(\"Sec-WebSocket-Protocol\")]\n    asgi_headers = [(name.encode(\"ascii\"), value.encode(\"ascii\")) for name, value in headers.raw_items()]\n    \n    self.scope = {\n        \"type\": \"websocket\",\n        \"scheme\": self.scheme,\n        \"server\": self.server,\n        \"client\": self.client,\n       "
    },
    {
        "original": "def queue_instances(instances):\n    \"\"\"\n    Queue a set of instances to be provisioned later. Expects a list.\n\n    Currently this only queries node data, and then places it in the cloud\n    cache (if configured). If the salt-cloud-reactor is being used, these\n    instances will be automatically provisioned using that.\n\n    For more information about the salt-cloud-reactor, see:\n\n    https://github.com/saltstack-formulas/salt-cloud-reactor\n    \"\"\"\n    for instance_id in instances:\n        node = _get_node(instance_id=instance_id)\n        __utils__['cloud.cache_node'](node, __active_provider_name__, __opts__)",
        "rewrite": "```python\ndef queue_instances(instances):\n    for instance_id in instances:\n        node = _get_node(instance_id=instance_id)\n        __utils__['cloud.cache_node'](node, __active_provider_name__, __opts__)\n```\n\nNote: This code snippet assumes that the functions `_get_node`, `__utils__`, `__active_provider_name__`, and `__opts__` are already defined elsewhere in the codebase. This code simply takes a list of instance IDs, queries each node using `_get_node`, and caches each node in the cloud cache using `cloud.cache_node`."
    },
    {
        "original": "def _validate_depedencies(batches):\n    \"\"\"Validates the transaction dependencies for the transactions contained\n    within the sequence of batches. Given that all the batches are expected to\n    to be executed for the genesis blocks, it is assumed that any dependent\n    transaction will proceed the depending transaction.\n    \"\"\"\n    transaction_ids = set()\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n\n            if txn_header.dependencies:\n                unsatisfied_deps = [\n                    id for id in txn_header.dependencies\n                    if id not in transaction_ids\n                ]\n                if unsatisfied_deps:\n                    raise CliException(\n                        'Unsatisfied dependency in given transactions:'\n                        ' {}'.format(unsatisfied_deps))\n\n            transaction_ids.add(txn.header_signature)",
        "rewrite": "```python\ndef _validate_dependencies(batches):\n    transaction_ids = set()\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n\n            if txn_header.dependencies:\n                unsatisfied_deps = [\n                    id for id in txn_header.dependencies\n                    if id not in transaction_ids\n                ]\n                if unsatisfied_deps:\n                    dependent_txns = [txn for txn in batches[-1].transactions \n                                        if any(dep_id == str(txn.header_signature) \n                                               for dep_id in unsatisfied_deps)]\n                    raise CliException(\n                        'Un"
    },
    {
        "original": "def validate(self, body, signature):\n        \"\"\"Check signature.\n\n        https://devdocs.line.me/en/#webhook-authentication\n\n        :param str body: Request body (as text)\n        :param str signature: X-Line-Signature value (as text)\n        :rtype: bool\n        :return: result\n        \"\"\"\n        gen_signature = hmac.new(\n            self.channel_secret,\n            body.encode('utf-8'),\n            hashlib.sha256\n        ).digest()\n\n        return compare_digest(\n                signature.encode('utf-8'), base64.b64encode(gen_signature)\n        )",
        "rewrite": "```python\nimport hmac\nimport hashlib\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n\ndef validate(self, body, signature):\n    gen_signature = hmac.new(\n        self.channel_secret.encode('utf-8'),\n        body.encode('utf-8'),\n        hashes.SHA256()\n    ).digest()\n\n    return compare_digest(\n            signature.encode('utf-8'), \n            base64.b64encode(gen_signature)\n    )\n```"
    },
    {
        "original": "def file_local_list(self, saltenv='base'):\n        \"\"\"\n        List files in the local minion files and localfiles caches\n        \"\"\"\n        filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n        localfilesdest = os.path.join(self.opts['cachedir'], 'localfiles')\n\n        fdest = self._file_local_list(filesdest)\n        ldest = self._file_local_list(localfilesdest)\n        return sorted(fdest.union(ldest))",
        "rewrite": "```python\ndef file_local_list(self, saltenv='base'):\n    filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n    localfilesdest = os.path.join(self.opts['cachedir'], 'localfiles')\n    \n    fdest = self._file_local_list(filesdest)\n    ldest = self._file_local_list(localfilesdest)\n    \n    return sorted(set(fdest).union(ldest))\n```"
    },
    {
        "original": "def CreateDataset(self):\n    \"\"\"Create a dataset.\"\"\"\n    body = {\n        \"datasetReference\": {\n            \"datasetId\": self.dataset_id,\n            \"description\": \"Data exported from GRR\",\n            \"friendlyName\": \"GRRExportData\",\n            \"projectId\": self.project_id\n        }\n    }\n    result = self.service.datasets().insert(\n        projectId=self.project_id, body=body).execute()\n    self.datasets[self.dataset_id] = result\n    return result",
        "rewrite": "```python\ndef create_dataset(self):\n    \"\"\"Create a dataset.\"\"\"\n    body = {\n        \"datasetReference\": {\n            \"datasetId\": self.dataset_id,\n            \"description\": \"Data exported from GRR\",\n            \"friendlyName\": \"GRRExportData\",\n            \"projectId\": self.project_id\n        }\n    }\n    response = self.service.datasets().insert(\n        projectId=self.project_id, body=body).execute()\n    if response:\n        self.datasets[self.dataset_id] = response.get('name')\n    return response\n```"
    },
    {
        "original": "def get_pr_review_status(pr: PullRequestDetails) -> Any:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/pulls/reviews/#list-reviews-on-a-pull-request\n    \"\"\"\n    url = (\"https://api.github.com/repos/{}/{}/pulls/{}/reviews\"\n           \"?access_token={}\".format(pr.repo.organization,\n                                     pr.repo.name,\n                                     pr.pull_id,\n                                     pr.repo.access_token))\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise RuntimeError(\n            'Get review failed. Code: {}. Content: {}.'.format(\n                response.status_code, response.content))\n\n    return json.JSONDecoder().decode(response.content.decode())",
        "rewrite": "```python\nimport requests\nimport json\n\ndef get_pr_review_status(pr: 'PullRequestDetails') -> list:\n    url = f\"https://api.github.com/repos/{pr.repo.organization}/{pr.repo.name}/pulls/{pr.pull_id}/reviews\"\n    headers = {\"Authorization\": f\"Bearer {pr.repo.access_token}\"}\n    response = requests.get(url, headers=headers)\n\n    if response.status_code != 200:\n        raise RuntimeError(f'Get review failed. Code: {response.status_code}')\n\n    return json.loads(response.content)\n```\n\nNote: I've assumed that `PullRequestDetails` is a class with attributes"
    },
    {
        "original": "def add_edge_lengths(G):\n    \"\"\"\n    Add length (meters) attribute to each edge by great circle distance between\n    nodes u and v.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    G : networkx multidigraph\n    \"\"\"\n\n    start_time = time.time()\n\n    # first load all the edges' origin and destination coordinates as a\n    # dataframe indexed by u, v, key\n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u', 'v', 'k']] = df_coords[['u', 'v', 'k']].astype(np.int64)\n    df_coords = df_coords.set_index(['u', 'v', 'k'])\n\n    # then calculate the great circle distance with the vectorized function\n    gc_distances = great_circle_vec(lat1=df_coords['u_y'],\n                                    lng1=df_coords['u_x'],\n                                    lat2=df_coords['v_y'],\n                                    lng2=df_coords['v_x'])\n\n    # fill nulls with zeros and round to the millimeter\n    gc_distances = gc_distances.fillna(value=0).round(3)\n    nx.set_edge_attributes(G, name='length', values=gc_distances.to_dict())\n\n    log('Added edge lengths to graph in {:,.2f} seconds'.format(time.time()-start_time))\n    return G",
        "rewrite": "```python\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance\n\ndef add_edge_lengths(G):\n    start_time = time.time()\n    \n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] \n                        for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u"
    },
    {
        "original": "def ProcessHuntFlowDone(flow_obj, status_msg=None):\n  \"\"\"Notifis hunt about a given hunt-induced flow completion.\"\"\"\n\n  if not hunt.IsLegacyHunt(flow_obj.parent_hunt_id):\n    hunt_obj = hunt.StopHuntIfCPUOrNetworkLimitsExceeded(\n        flow_obj.parent_hunt_id)\n    hunt.CompleteHuntIfExpirationTimeReached(hunt_obj)\n    return\n\n  hunt_urn = rdfvalue.RDFURN(\"hunts\").Add(flow_obj.parent_hunt_id)\n  client_urn = rdf_client.ClientURN(flow_obj.client_id)\n\n  # Update the counter metrics separately from collections to minimize\n  # contention.\n  with aff4.FACTORY.Open(hunt_urn, mode=\"rw\") as fd:\n    # Legacy AFF4 code expects token to be set.\n    fd.token = access_control.ACLToken(username=fd.creator)\n\n    if flow_obj.num_replies_sent:\n      fd.context.clients_with_results_count += 1\n\n    fd.context.completed_clients_count += 1\n    fd.context.results_count += flow_obj.num_replies_sent\n\n    fd.GetRunner().SaveResourceUsage(flow_obj.client_id, status_msg)\n\n  with aff4.FACTORY.Open(hunt_urn, mode=\"rw\") as fd:\n    # Legacy AFF4 code expects token to be set.\n    fd.token = access_control.ACLToken(username=fd.creator)\n\n    fd.RegisterCompletedClient(client_urn)\n    if flow_obj.num_replies_sent:\n      fd.RegisterClientWithResults(client_urn)\n\n    fd.StopHuntIfAverageLimitsExceeded()",
        "rewrite": "```python\ndef process_hunt_flow_done(flow_obj, status_msg=None):\n    if not hunt.is_legacy_hunt(flow_obj.parent_hunt_id):\n        hunt_obj = hunt.stop_hunt_if_cpu_or_network_limits_exceeded(\n            flow_obj.parent_hunt_id)\n        hunt.complete_hunt_if_expiration_time_reached(hunt_obj)\n        return\n\n    hunt_urn = rdfvalue.RDFURN(\"hunts\").Add(flow_obj.parent_hunt_id)\n    client_urn = rdf_client.ClientURN(flow_obj.client_id)\n\n    with aff4.FACTORY.Open(hunt_urn, mode=\"rw\") as fd"
    },
    {
        "original": "def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n        try:\n            data = requests.get(\"https://pypi.python.org/pypi/jira/json\", timeout=2.001).json()\n\n            released_version = data['info']['version']\n            if parse_version(released_version) > parse_version(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n        except requests.RequestException:\n            pass\n        except Exception as e:\n            logging.warning(e)",
        "rewrite": "```python\nimport requests\nfrom packaging import version\n\ndef _check_update_(self):\n    try:\n        response = requests.get(\"https://pypi.org/pypi/jira/json\", timeout=2.001)\n        if response.status_code == 200:\n            data = response.json()\n            released_version = data['info']['version']\n            if version.parse(released_version) > version.parse(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n   "
    },
    {
        "original": "def reifyWidget(self, parent, item):\r\n        \"\"\" Convert a JSON description of a widget into a WxObject \"\"\"\r\n        from gooey.gui.components import widgets\r\n        widgetClass = getattr(widgets, item['type'])\r\n        return widgetClass(parent, item)",
        "rewrite": "```python\ndef reify_widget(self, parent, item):\n    from gooey.gui.components import widgets\n    widget_class = getattr(widgets, item['type'])\n    return widget_class(parent, item)\n```"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        Json-serializable dict representation.\n        \"\"\"\n        d = MSONable.as_dict(self)\n        d[\"translation_vector\"] = self.translation_vector.tolist()\n        return d",
        "rewrite": "```python\ndef as_dict(self):\n    d = MSONable.as_dict(self)\n    d[\"translation_vector\"] = self.translation_vector.tolist()\n    return d\n```"
    },
    {
        "original": "def library_line(self, file_name):\n        \"\"\"\n        Specifies GULP library file to read species and potential parameters.\n        If using library don't specify species and potential\n        in the input file and vice versa. Make sure the elements of\n        structure are in the library file.\n\n        Args:\n            file_name: Name of GULP library file\n\n        Returns:\n            GULP input string specifying library option\n        \"\"\"\n        gulplib_set = lambda: 'GULP_LIB' in os.environ.keys()\n        readable = lambda f: os.path.isfile(f) and os.access(f, os.R_OK)\n\n        #dirpath, fname = os.path.split(file_name)\n        #if dirpath:  # Full path specified\n        #    if readable(file_name):\n        #        gin = 'library ' + file_name\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #else:\n        #    fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n        #    if readable(fpath):\n        #        gin = 'library ' + fpath\n        #    elif gulplib_set():\n        #        fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n        #        if readable(fpath):\n        #            gin = 'library ' + file_name\n        #        else:\n        #            raise GulpError('GULP Library not found')\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #gin += \"\\n\"\n        #return gin\n\n        gin = \"\"\n        dirpath, fname = os.path.split(file_name)\n        if dirpath and readable(file_name):  # Full path specified\n            gin = 'library ' + file_name\n        else:\n            fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n            if readable(fpath):\n                gin = 'library ' + fpath\n            elif gulplib_set():         # Check the GULP_LIB path\n                fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n                if readable(fpath):\n                    gin = 'library ' + file_name\n        if gin:\n            return gin + \"\\n\"\n        else:\n            raise GulpError('GULP Library not found')",
        "rewrite": "```python\nimport os\nfrom your_module import GulpError  # Assuming GulpError is a custom exception\n\ndef library_line(self, file_name):\n    gulplib_set = lambda: 'GULP_LIB' in os.environ.keys()\n    readable = lambda f: os.path.isfile(f) and os.access(f, os.R_OK)\n\n    gin = \"\"\n    dirpath, fname = os.path.split(file_name)\n    \n    if dirpath and readable(file_name):  # Full path specified\n        gin = 'library ' + file_name\n    else:\n        fpath = os.path.join(os.getcwd(), file_name)"
    },
    {
        "original": "def IsCloud(self, request, bios_version, services):\n    \"\"\"Test to see if we're on a cloud machine.\"\"\"\n    if request.bios_version_regex and bios_version:\n      if re.match(request.bios_version_regex, bios_version):\n        return True\n    if request.service_name_regex and services:\n      if re.search(request.service_name_regex, services):\n        return True\n    return False",
        "rewrite": "```python\nimport re\nfrom typing import Optional\n\ndef is_cloud(\n    self, \n    request: object, \n    bios_version: Optional[str], \n    services: Optional[str]\n) -> bool:\n    if bios_version and re.match(request.bios_version_regex, bios_version):\n        return True\n\n    if services and re.search(request.service_name_regex, services):\n        return True\n\n    return False\n```"
    },
    {
        "original": "def get_local_hypervisor(self):\n        \"\"\"\n        Returns the hypervisor running on this host, and assumes a 1-1 between host and hypervisor\n        \"\"\"\n        # Look up hypervisors available filtered by my hostname\n        host = self.get_my_hostname()\n        hyp = self.get_all_hypervisor_ids(filter_by_host=host)\n        if hyp:\n            return hyp[0]",
        "rewrite": "```python\ndef get_local_hypervisor(self):\n    host = self.get_my_hostname()\n    return next((h for h in self.get_all_hypervisor_ids(filter_by_host=host)), None)\n```"
    },
    {
        "original": "def get(self, file_path, ref, **kwargs):\n        \"\"\"Retrieve a single file.\n\n        Args:\n            file_path (str): Path of the file to retrieve\n            ref (str): Name of the branch, tag or commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            object: The generated RESTObject\n        \"\"\"\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)",
        "rewrite": "```python\ndef get(self, file_path, ref, **kwargs):\n    file_path = urllib.parse.quote(file_path)\n    return GetMixin.get(self, file_path, ref=ref, **kwargs)\n```"
    },
    {
        "original": "def from_values_indices(cls, values, indices, populate=False,\n                            structure=None, voigt_rank=None,\n                            vsym=True, verbose=False):\n        \"\"\"\n        Creates a tensor from values and indices, with options\n        for populating the remainder of the tensor.\n\n        Args:\n            values (floats): numbers to place at indices\n            indices (array-likes): indices to place values at\n            populate (bool): whether to populate the tensor\n            structure (Structure): structure to base population\n                or fit_to_structure on\n            voigt_rank (int): full tensor rank to indicate the\n                shape of the resulting tensor.  This is necessary\n                if one provides a set of indices more minimal than\n                the shape of the tensor they want, e.g.\n                Tensor.from_values_indices((0, 0), 100)\n            vsym (bool): whether to voigt symmetrize during the\n                optimization procedure\n            verbose (bool): whether to populate verbosely\n        \"\"\"\n        # auto-detect voigt notation\n        # TODO: refactor rank inheritance to make this easier\n        indices = np.array(indices)\n        if voigt_rank:\n            shape = ([3]*(voigt_rank % 2) + [6]*(voigt_rank // 2))\n        else:\n            shape = np.ceil(np.max(indices+1, axis=0) / 3.) * 3\n        base = np.zeros(shape.astype(int))\n        for v, idx in zip(values, indices):\n            base[tuple(idx)] = v\n        if 6 in shape:\n            obj = cls.from_voigt(base)\n        else:\n            obj = cls(base)\n        if populate:\n            assert structure, \"Populate option must include structure input\"\n            obj = obj.populate(structure, vsym=vsym, verbose=verbose)\n        elif structure:\n            obj = obj.fit_to_structure(structure)\n        return obj",
        "rewrite": "```python\nimport numpy as np\n\ndef from_values_indices(cls, values, indices, populate=False,\n                         structure=None, voigt_rank=None,\n                         vsym=True, verbose=False):\n    \"\"\"\n    Creates a tensor from values and indices, with options\n    for populating the remainder of the tensor.\n\n    Args:\n        values (floats): numbers to place at indices\n        indices (array-likes): indices to place values at\n        populate (bool): whether to populate the tensor\n        structure (Structure): structure to base population\n            or fit_to_structure on\n        voigt_rank (int): full tensor rank"
    },
    {
        "original": "def _raise_redirect_exceptions(response):\n    \"\"\"Return the new url or None if there are no redirects.\n\n    Raise exceptions if appropriate.\n\n    \"\"\"\n    if response.status_code not in [301, 302, 307]:\n        return None\n    new_url = urljoin(response.url, response.headers['location'])\n    if 'reddits/search' in new_url:  # Handle non-existent subreddit\n        subreddit = new_url.rsplit('=', 1)[1]\n        raise InvalidSubreddit('`{0}` is not a valid subreddit'\n                               .format(subreddit))\n    elif not RE_REDIRECT.search(response.url):\n        raise RedirectException(response.url, new_url)\n    return new_url",
        "rewrite": "```python\nimport re\nfrom urllib.parse import urljoin\n\ndef _raise_redirect_exceptions(response):\n    if response.status_code not in [301, 302, 307]:\n        return None\n\n    regex = re.compile(RE_REDIRECT)\n    new_url = urljoin(response.url, response.headers['location'])\n    if 'reddits/search' in new_url:\n        subreddit = new_url.rsplit('=', 1)[1]\n        raise InvalidSubreddit(f\"`{subreddit}` is not a valid subreddit\")\n\n    elif not regex.search(new_url):\n        raise RedirectException(response.url, new_url)\n\n    return new_url\n``"
    },
    {
        "original": "def _future_done_callback(self, request, result):\n        \"\"\"\n        :param request (bytes):the serialized request\n        :param result (FutureResult):\n        \"\"\"\n        self._in_process_transactions_count.dec()\n        req = processor_pb2.TpProcessRequest()\n        req.ParseFromString(request)\n        response = processor_pb2.TpProcessResponse()\n        response.ParseFromString(result.content)\n\n        processor_type = ProcessorType(\n            req.header.family_name,\n            req.header.family_version)\n\n        self._processor_manager[processor_type].get_processor(\n            result.connection_id).dec_occupancy()\n        self._processor_manager.notify()\n\n        self._get_tp_process_response_counter(\n            response.Status.Name(response.status)).inc()\n\n        if result.connection_id in self._open_futures and \\\n                req.signature in self._open_futures[result.connection_id]:\n            del self._open_futures[result.connection_id][req.signature]\n\n        if response.status == processor_pb2.TpProcessResponse.OK:\n            state_sets, state_deletes, events, data = \\\n                self._context_manager.get_execution_results(req.context_id)\n\n            state_changes = [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    value=value,\n                    type=transaction_receipt_pb2.StateChange.SET)\n                for addr, value in state_sets.items()\n            ] + [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    type=transaction_receipt_pb2.StateChange.DELETE)\n                for addr in state_deletes\n            ]\n\n            self._scheduler.set_transaction_execution_result(\n                txn_signature=req.signature,\n                is_valid=True,\n                context_id=req.context_id,\n                state_changes=state_changes,\n                events=events,\n                data=data)\n\n        elif response.status == processor_pb2.TpProcessResponse.INTERNAL_ERROR:\n            LOGGER.error(\n                \"Transaction processor internal error: %s \"\n                \"(transaction: %s, name: %s, version: %s)\",\n                response.message,\n                req.signature,\n                req.header.family_name,\n                req.header.family_version)\n\n            # Make sure that the transaction wasn't unscheduled in the interim\n            if self._scheduler.is_transaction_in_schedule(req.signature):\n                self._execute(\n                    processor_type=processor_type,\n                    content=request,\n                    signature=req.signature)\n\n        else:\n            self._context_manager.delete_contexts(\n                context_id_list=[req.context_id])\n\n            self._fail_transaction(\n                txn_signature=req.signature,\n                context_id=req.context_id,\n                error_message=response.message,\n                error_data=response.extended_data)",
        "rewrite": "```python\ndef _future_done_callback(self, request: bytes, result):\n    self._in_process_transactions_count.dec()\n    req = processor_pb2.TpProcessRequest()\n    req.ParseFromString(request)\n    response = processor_pb2.TpProcessResponse()\n    response.ParseFromString(result.content)\n\n    processor_type = ProcessorType(\n        req.header.family_name,\n        req.header.family_version)\n\n    self._processor_manager[processor_type].get_processor(\n        result.connection_id).dec_occupancy()\n    self._processor_manager.notify()\n\n    self._get_tp_process_response_counter(\n        response.Status.Name(response.status)).inc()\n\n    if"
    },
    {
        "original": "def validation_error_inform_error(err, data, schema):\n    \"\"\"\n    Custom validation error handler which produces 404 Bad Request\n    response in case validation fails and returns the error\n    \"\"\"\n    abort(Response(\n        json.dumps({'error': str(err), 'data': data, 'schema': schema}),\n        status=HTTPStatus.BAD_REQUEST))",
        "rewrite": "```python\nfrom fastapi import Response, HTTPException\n\ndef validation_error_inform_error(err, data, schema):\n    return HTTPException(\n        status_code=HTTPException.BAD_REQUEST,\n        detail=\"Validation errors\",\n        headers={\"Content-Type\": \"application/json\"},\n        background=True,\n    ).dict(exclude_defaults=True)\n```"
    },
    {
        "original": "def IterateAllClientSnapshots(self, min_last_ping=None, batch_size=50000):\n    \"\"\"Iterates over all available clients and yields client snapshot objects.\n\n    Args:\n      min_last_ping: If provided, only snapshots for clients with last-ping\n        timestamps newer than (or equal to) the given value will be returned.\n      batch_size: Always reads <batch_size> snapshots at a time.\n\n    Yields:\n      An rdfvalues.objects.ClientSnapshot object for each client in the db.\n    \"\"\"\n    all_client_ids = self.ReadAllClientIDs(min_last_ping=min_last_ping)\n\n    for batch in collection.Batch(all_client_ids, batch_size):\n      res = self.MultiReadClientSnapshot(batch)\n      for snapshot in itervalues(res):\n        if snapshot:\n          yield snapshot",
        "rewrite": "```python\ndef IterateAllClientSnapshots(self, min_last_ping=None, batch_size=50000):\n    \"\"\"Iterates over all available clients and yields client snapshot objects.\"\"\"\n    all_client_ids = self.ReadAllClientIDs(min_last_ping=min_last_ping)\n\n    for batch in itertools.islice(itertools.batch(all_client_ids, batch_size), 0, None):\n        res = self.MultiReadClientSnapshot(batch)\n        for snapshot in res.values():\n            if snapshot is not None:\n                yield snapshot\n```"
    },
    {
        "original": "def ClosureTable(model_class, foreign_key=None, referencing_class=None,\n                 referencing_key=None):\n    \"\"\"Model factory for the transitive closure extension.\"\"\"\n    if referencing_class is None:\n        referencing_class = model_class\n\n    if foreign_key is None:\n        for field_obj in model_class._meta.refs:\n            if field_obj.rel_model is model_class:\n                foreign_key = field_obj\n                break\n        else:\n            raise ValueError('Unable to find self-referential foreign key.')\n\n    source_key = model_class._meta.primary_key\n    if referencing_key is None:\n        referencing_key = source_key\n\n    class BaseClosureTable(VirtualModel):\n        depth = VirtualField(IntegerField)\n        id = VirtualField(IntegerField)\n        idcolumn = VirtualField(TextField)\n        parentcolumn = VirtualField(TextField)\n        root = VirtualField(IntegerField)\n        tablename = VirtualField(TextField)\n\n        class Meta:\n            extension_module = 'transitive_closure'\n\n        @classmethod\n        def descendants(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.id))\n                     .where(cls.root == node)\n                     .objects())\n            if depth is not None:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def ancestors(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.root))\n                     .where(cls.id == node)\n                     .objects())\n            if depth:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def siblings(cls, node, include_node=False):\n            if referencing_class is model_class:\n                # self-join\n                fk_value = node.__data__.get(foreign_key.name)\n                query = model_class.select().where(foreign_key == fk_value)\n            else:\n                # siblings as given in reference_class\n                siblings = (referencing_class\n                            .select(referencing_key)\n                            .join(cls, on=(foreign_key == cls.root))\n                            .where((cls.id == node) & (cls.depth == 1)))\n\n                # the according models\n                query = (model_class\n                         .select()\n                         .where(source_key << siblings)\n                         .objects())\n\n            if not include_node:\n                query = query.where(source_key != node)\n\n            return query\n\n    class Meta:\n        database = referencing_class._meta.database\n        options = {\n            'tablename': referencing_class._meta.table_name,\n            'idcolumn': referencing_key.column_name,\n            'parentcolumn': foreign_key.column_name}\n        primary_key = False\n\n    name = '%sClosure' % model_class.__name__\n    return type(name, (BaseClosureTable,), {'Meta': Meta})",
        "rewrite": "```python\ndef ClosureTable(model_class, foreign_key=None, referencing_class=None,\n                 referencing_key=None):\n    \"\"\"Model factory for the transitive closure extension.\"\"\"\n    if referencing_class is None:\n        referencing_class = model_class\n\n    if foreign_key is None:\n        for field_obj in model_class._meta.refs:\n            if field_obj.rel_model is model_class:\n                foreign_key = field_obj\n                break\n        else:\n            raise ValueError('Unable to find self-referential foreign key.')\n\n    source_key = model_class._meta.primary_key\n    if referencing_key is None:\n        referencing_key = source_key\n\n   "
    },
    {
        "original": "def CheckHashes(self, hashes):\n    \"\"\"Check hashes against the filestore.\n\n    Blobs use the hash in the schema:\n    aff4:/files/hash/generic/sha256/[sha256hash]\n\n    Args:\n      hashes: A list of Hash objects to check.\n\n    Yields:\n      Tuples of (RDFURN, hash object) that exist in the store.\n    \"\"\"\n    hash_map = {}\n    for hsh in hashes:\n      if hsh.HasField(\"sha256\"):\n        # The canonical name of the file is where we store the file hash.\n        hash_map[aff4.ROOT_URN.Add(\"files/hash/generic/sha256\").Add(\n            str(hsh.sha256))] = hsh\n\n    for metadata in aff4.FACTORY.Stat(list(hash_map)):\n      yield metadata[\"urn\"], hash_map[metadata[\"urn\"]]",
        "rewrite": "```python\ndef check_hashes(self, hashes):\n    \"\"\"Check hashes against the filestore.\n\n    Args:\n        hashes: A list of Hash objects to check.\n\n    Yields:\n        Tuples of (RDFURN, hash object) that exist in the store.\n    \"\"\"\n    hash_map = {str(h.sha256): h for h in hashes if hasattr(h, \"sha256\")}\n    \n    for metadata in aff4.FACTORY.Stat(list(hash_map.keys())):\n        yield metadata[\"urn\"], hash_map.get(metadata[\"urn\"])\n```"
    },
    {
        "original": "def check_pseudo_package(path):\n    \"\"\"\n    Verifies that a fake subpackage path for assets (notebooks, svgs,\n    pngs etc) both exists and is populated with files.\n    \"\"\"\n    if not os.path.isdir(path):\n        raise Exception(\"Please make sure pseudo-package %s exists.\" % path)\n    else:\n        assets = os.listdir(path)\n        if len(assets) == 0:\n            raise Exception(\"Please make sure pseudo-package %s is populated.\" % path)",
        "rewrite": "```python\nimport os\n\ndef check_pseudo_package(path):\n    if not os.path.isdir(path):\n        raise ValueError(f\"Pseudo-package {path} does not exist.\")\n    assets = os.listdir(path)\n    if not assets:\n        raise ValueError(f\"Pseudo-package {path} is empty.\")\n```\n\nAlternatively, you could use `pathlib` for a more modern and Pythonic way of handling paths:\n\n```python\nimport pathlib\n\ndef check_pseudo_package(path: str) -> None:\n    path = pathlib.Path(path)\n    if not path.is_dir():\n        raise ValueError(f\"Pseudo-package {path} does not exist"
    },
    {
        "original": "def setup_exchange(self, exchange_name):\n        \"\"\"Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n        command. When it is complete, the on_exchange_declareok method will\n        be invoked by pika.\n\n        :param str|unicode exchange_name: The name of the exchange to declare\n        \"\"\"\n        _logger.debug('Declaring exchange %s', exchange_name)\n        self._channel.exchange_declare(self.on_exchange_declareok,\n                                       exchange_name,\n                                       self.EXCHANGE_TYPE,\n                                       durable=True,\n                                       passive=True)",
        "rewrite": "```python\ndef setup_exchange(self, exchange_name):\n    _logger.debug('Declaring exchange %s', exchange_name)\n    self._channel.exchange_declare(exchange=exchange_name,\n                                 type=self.EXCHANGE_TYPE,\n                                 durable=True,\n                                 passive=True,\n                                 callback=self.on_exchange_declareok)\n```"
    },
    {
        "original": "def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node",
        "rewrite": "```python\ndef fold(self, node):\n    node = self.generic_visit(node)\n    try:\n        return nodes.Const.from_untrusted(node.as_const(), \n                                           lineno=node.lineno, \n                                           environment=self.environment)\n    except nodes.Impossible:\n        return node\n```"
    },
    {
        "original": "def pipeline_delete(id, hosts=None, profile=None):\n    \"\"\"\n    .. versionadded:: 2017.7.0\n\n    Delete Ingest pipeline. Available since Elasticsearch 5.0.\n\n    id\n        Pipeline id\n\n    CLI example::\n\n        salt myminion elasticsearch.pipeline_delete mypipeline\n    \"\"\"\n    es = _get_instance(hosts, profile)\n\n    try:\n        ret = es.ingest.delete_pipeline(id=id)\n        return ret.get('acknowledged', False)\n    except elasticsearch.NotFoundError:\n        return True\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot delete pipeline {0}, server returned code {1} with message {2}\".format(id, e.status_code, e.error))\n    except AttributeError:\n        raise CommandExecutionError(\"Method is applicable only for Elasticsearch 5.0+\")",
        "rewrite": "```python\ndef pipeline_delete(id, hosts=None, profile=None):\n    \"\"\"\n    Delete Ingest pipeline.\n\n    Args:\n        id (str): Pipeline id\n        hosts (list): List of Elasticsearch nodes. Defaults to None.\n        profile (str or dict): Selection profile or Selection Profile dictionary. Defaults to None.\n\n    Returns:\n        bool: Acknowledged status by Elasticsearch server\n\n    Raises:\n        CommandExecutionError: When deletion operation fails due to server-side error.\n    \"\"\"\n\n    es = _get_instance(hosts, profile)\n\n    try:\n        ret = es.ingest.delete_pipeline(body={\"name\": id})"
    },
    {
        "original": "def lattice_2_lmpbox(lattice, origin=(0, 0, 0)):\n    \"\"\"\n    Converts a lattice object to LammpsBox, and calculates the symmetry\n    operation used.\n\n    Args:\n        lattice (Lattice): Input lattice.\n        origin: A (3,) array/list of floats setting lower bounds of\n            simulation box. Default to (0, 0, 0).\n\n    Returns:\n        LammpsBox, SymmOp\n\n    \"\"\"\n    a, b, c = lattice.abc\n    xlo, ylo, zlo = origin\n    xhi = a + xlo\n    m = lattice.matrix\n    xy = np.dot(m[1], m[0] / a)\n    yhi = np.sqrt(b ** 2 - xy ** 2) + ylo\n    xz = np.dot(m[2], m[0] / a)\n    yz = (np.dot(m[1], m[2]) - xy * xz) / (yhi - ylo)\n    zhi = np.sqrt(c ** 2 - xz ** 2 - yz ** 2) + zlo\n    tilt = None if lattice.is_orthogonal else [xy, xz, yz]\n    rot_matrix = np.linalg.solve([[xhi - xlo, 0, 0],\n                                  [xy, yhi - ylo, 0],\n                                  [xz, yz, zhi - zlo]], m)\n    bounds = [[xlo, xhi], [ylo, yhi], [zlo, zhi]]\n    symmop = SymmOp.from_rotation_and_translation(rot_matrix, origin)\n    return LammpsBox(bounds, tilt), symmop",
        "rewrite": "```python\nimport numpy as np\n\ndef lattice_2_lmpbox(lattice, origin=(0, 0, 0)):\n    a, b, c = lattice.abc\n    xlo, ylo, zlo = origin\n    xhi = a + xlo\n    m = lattice.matrix.astype(np.float64)\n    \n    xy = np.dot(m[1], m[0] / a)\n    yhi = np.sqrt(b**2 - xy**2) + ylo if b != 0 else float('inf')\n    \n    xz = np.dot(m[2], m[0"
    },
    {
        "original": "def xgroup_setid(self, name, groupname, id):\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n        \"\"\"\n        return self.execute_command('XGROUP SETID', name, groupname, id)",
        "rewrite": "```python\ndef xgroup_setid(self, name, groupname, id):\n    return self.execute_command('XGROUP', 'SETID', name, groupname, id)\n```"
    },
    {
        "original": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False",
        "rewrite": "```python\ndef _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n    from ..exceptions import RequestedRangeNotSatisfiable\n\n    if accept_ranges is None:\n        return False\n    self.headers[\"Accept-Ranges\"] = accept_ranges\n\n    if not self._is_range_request_processable(environ) or complete_length is None:\n        return False\n\n    parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n    if parsed_range is None:\n        raise RequestedRangeNotSatisfiable(complete_length)\n\n    range_tuple = parsed_range.range_for_length(complete_length)\n    content"
    },
    {
        "original": "def gw_run(self):\n        \"\"\"\n        Performs FIESTA (gw) run\n        \"\"\"\n\n        if self.folder != os.getcwd():\n            init_folder = os.getcwd()\n            os.chdir(self.folder)\n\n        with zopen(self.log_file, 'w') as fout:\n            subprocess.call([\"mpirun\", \"-n\", str(self.mpi_procs), \"fiesta\",\n                             str(self.grid[0]), str(self.grid[1]),\n                             str(self.grid[2])], stdout=fout)\n\n        if self.folder != os.getcwd():\n            os.chdir(init_folder)",
        "rewrite": "```python\ndef gw_run(self):\n    \"\"\"\n    Performs FIESTA (gw) run\n    \"\"\"\n\n    if self.folder != os.getcwd():\n        init_folder = os.getcwd()\n        try:\n            os.chdir(self.folder)\n            with zopen(self.log_file, 'w') as fout:\n                subprocess.check_call([\"mpirun\", \"-n\", str(self.mpi_procs), \"fiesta\",\n                                      str(self.grid[0]), str(self.grid[1]), \n                                      str(self.grid[2])], stdout=fout)\n        finally:\n            if self.folder != os.getcwd() and os.path.exists(init_folder):\n                os"
    },
    {
        "original": "def _clip_gradient_op(dtype):\n  \"\"\"Create an op that clips gradients using a Defun.\n\n  The tensorflow Defun decorator creates an op and tensorflow caches these op\n  automatically according to `func_name`. Using a Defun decorator twice with the\n  same `func_name` does not create a new op, instead the cached op is used.\n\n  This method produces a new op the first time it is called with a given `dtype`\n  argument, and then uses the cached op each time it is called after that with\n  the same `dtype`. The min and max clip values are given as arguments for the\n  forward pass method so that they can be used in the backwards pass.\n\n  Args:\n    dtype: the dtype of the net whose gradient is being clipped.\n\n  Returns:\n    The op that clips gradients.\n  \"\"\"\n\n  def clip_gradient_backward(op, grad):\n    clip_value_min = op.inputs[1]\n    clip_value_max = op.inputs[2]\n    clipped_grad = tf.clip_by_value(grad, clip_value_min, clip_value_max)\n    return clipped_grad, None, None\n\n  def clip_gradient_forward(x, clip_value_min, clip_value_max):\n    del clip_value_min  # Unused.\n    del clip_value_max  # Unused.\n    return x\n\n  func_name = \"ClipGradient_{}\".format(dtype.name)\n  return function.Defun(\n      dtype, dtype, dtype,\n      python_grad_func=clip_gradient_backward,\n      func_name=func_name)(clip_gradient_forward)",
        "rewrite": "```python\nimport tensorflow as tf\n\ndef _clip_gradient_op(dtype):\n  def clip_gradient_backward(op, grad):\n    clip_value_min = op.inputs[1]\n    clip_value_max = op.inputs[2]\n    clipped_grad = tf.clip_by_value(grad, clip_value_min, clip_value_max)\n    return clipped_grad, None, None\n\n  def clip_gradient_forward(x, some_unused_param1, some_unused_param2):\n    return x\n\n  func_name = f\"ClipGradient_{dtype.name}\"\n  return tf.function(\n      function=tf.keras-backend-agnostic juegos cloned python functions would replace Clip gradient forward here).\n     "
    },
    {
        "original": "def deframesig(frames, siglen, frame_len, frame_step, winfunc=lambda x: numpy.ones((x,))):\n    \"\"\"Does overlap-add procedure to undo the action of framesig.\n\n    :param frames: the array of frames.\n    :param siglen: the length of the desired signal, use 0 if unknown. Output will be truncated to siglen samples.\n    :param frame_len: length of each frame measured in samples.\n    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n    :returns: a 1-D signal.\n    \"\"\"\n    frame_len = round_half_up(frame_len)\n    frame_step = round_half_up(frame_step)\n    numframes = numpy.shape(frames)[0]\n    assert numpy.shape(frames)[1] == frame_len, '\"frames\" matrix is wrong size, 2nd dim is not equal to frame_len'\n\n    indices = numpy.tile(numpy.arange(0, frame_len), (numframes, 1)) + numpy.tile(\n        numpy.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n    indices = numpy.array(indices, dtype=numpy.int32)\n    padlen = (numframes - 1) * frame_step + frame_len\n\n    if siglen <= 0: siglen = padlen\n\n    rec_signal = numpy.zeros((padlen,))\n    window_correction = numpy.zeros((padlen,))\n    win = winfunc(frame_len)\n\n    for i in range(0, numframes):\n        window_correction[indices[i, :]] = window_correction[\n                                               indices[i, :]] + win + 1e-15  # add a little bit so it is never zero\n        rec_signal[indices[i, :]] = rec_signal[indices[i, :]] + frames[i, :]\n\n    rec_signal = rec_signal / window_correction\n    return rec_signal[0:siglen]",
        "rewrite": "```python\nimport numpy as np\n\ndef deframe_sig(frames, siglen, frame_len, frame_step, winfunc=lambda x: np.ones((x,))):\n    \"\"\"\n    Does overlap-add procedure to undo the action of framesig.\n\n    :param frames: the array of frames.\n    :param siglen: the length of the desired signal.\n    :param frame_len: length of each frame measured in samples.\n    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n    :param winfunc: the analysis window to apply to each frame. By default no window"
    },
    {
        "original": "def MakeStatResponse(self, tsk_file, tsk_attribute=None, append_name=None):\n    \"\"\"Given a TSK info object make a StatEntry.\n\n    Note that tsk uses two things to uniquely identify a data stream - the inode\n    object given in tsk_file and the attribute object which may correspond to an\n    ADS of this file for filesystems which support ADS. We store both of these\n    in the stat response.\n\n    Args:\n      tsk_file: A TSK File object for the specified inode.\n      tsk_attribute: A TSK Attribute object for the ADS. If None we use the main\n        stream.\n      append_name: If specified we append this name to the last element of the\n        pathspec.\n\n    Returns:\n      A StatEntry which can be used to re-open this exact VFS node.\n    \"\"\"\n    precondition.AssertOptionalType(append_name, Text)\n\n    info = tsk_file.info\n    response = rdf_client_fs.StatEntry()\n    meta = info.meta\n    if meta:\n      response.st_ino = meta.addr\n      for attribute in [\n          \"mode\", \"nlink\", \"uid\", \"gid\", \"size\", \"atime\", \"mtime\", \"ctime\",\n          \"crtime\"\n      ]:\n        try:\n          value = int(getattr(meta, attribute))\n          if value < 0:\n            value &= 0xFFFFFFFF\n\n          setattr(response, \"st_%s\" % attribute, value)\n        except AttributeError:\n          pass\n\n    name = info.name\n    child_pathspec = self.pathspec.Copy()\n\n    if append_name is not None:\n      # Append the name to the most inner pathspec\n      child_pathspec.last.path = utils.JoinPath(child_pathspec.last.path,\n                                                append_name)\n\n    child_pathspec.last.inode = meta.addr\n    if tsk_attribute is not None:\n      child_pathspec.last.ntfs_type = int(tsk_attribute.info.type)\n      child_pathspec.last.ntfs_id = int(tsk_attribute.info.id)\n      child_pathspec.last.stream_name = tsk_attribute.info.name\n\n      # Update the size with the attribute size.\n      response.st_size = tsk_attribute.info.size\n\n      default = rdf_paths.PathSpec.tsk_fs_attr_type.TSK_FS_ATTR_TYPE_DEFAULT\n      last = child_pathspec.last\n      if last.ntfs_type != default or last.ntfs_id:\n        # This is an ads and should be treated as a file.\n        # Clear all file type bits.\n        response.st_mode &= ~self.stat_type_mask\n        response.st_mode |= stat.S_IFREG\n\n    else:\n      child_pathspec.last.ntfs_type = None\n      child_pathspec.last.ntfs_id = None\n      child_pathspec.last.stream_name = None\n\n    if name:\n      # Encode the type onto the st_mode response\n      response.st_mode |= self.FILE_TYPE_LOOKUP.get(int(name.type), 0)\n\n    if meta:\n      # What if the types are different? What to do here?\n      response.st_mode |= self.META_TYPE_LOOKUP.get(int(meta.type), 0)\n\n    # Write the pathspec on the response.\n    response.pathspec = child_pathspec\n    return response",
        "rewrite": "```python\ndef MakeStatResponse(self, tsk_file, tsk_attribute=None, append_name=None):\n    \"\"\"Given a TSK info object make a StatEntry.\"\"\"\n    precondition.AssertOptionalType(append_name, Text)\n\n    info = tsk_file.info\n    response = rdf_client_fs.StatEntry()\n    meta = info.meta\n\n    if meta:\n        response.st_ino = meta.addr\n        for attribute in [\n            \"mode\", \"nlink\", \"uid\", \"gid\", \"size\", \"atime\", \"mtime\",\n            \"ctime\", \"crtime\"\n        ]:\n            try:\n                value ="
    },
    {
        "original": "def get_values(self):\n        \"\"\"\n        Returns the cpd\n\n        Examples\n        --------\n        >>> from pgmpy.factors.discrete import TabularCPD\n        >>> cpd = TabularCPD('grade', 3, [[0.1, 0.1],\n        ...                               [0.1, 0.1],\n        ...                               [0.8, 0.8]],\n        ...                  evidence='evi1', evidence_card=2)\n        >>> cpd.get_values()\n        array([[ 0.1,  0.1],\n               [ 0.1,  0.1],\n               [ 0.8,  0.8]])\n        \"\"\"\n        if self.variable in self.variables:\n            return self.values.reshape(self.cardinality[0], np.prod(self.cardinality[1:]))\n        else:\n            return self.values.reshape(1, np.prod(self.cardinality))",
        "rewrite": "```python\nimport numpy as np\n\ndef get_values(self):\n    index = np.arange(len(self.cardinality))\n    return self.values.reshape(*self.cardinality)\n```"
    },
    {
        "original": "def join_chat(\n        self,\n        chat_id: str\n    ):\n        \"\"\"Use this method to join a group chat or channel.\n\n        Args:\n            chat_id (``str``):\n                Unique identifier for the target chat in form of a *t.me/joinchat/* link or username of the target\n                channel/supergroup (in the format @username).\n\n        Returns:\n            On success, a :obj:`Chat <pyrogram.Chat>` object is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        match = self.INVITE_LINK_RE.match(chat_id)\n\n        if match:\n            chat = self.send(\n                functions.messages.ImportChatInvite(\n                    hash=match.group(1)\n                )\n            )\n            if isinstance(chat.chats[0], types.Chat):\n                return pyrogram.Chat._parse_chat_chat(self, chat.chats[0])\n            elif isinstance(chat.chats[0], types.Channel):\n                return pyrogram.Chat._parse_channel_chat(self, chat.chats[0])\n        else:\n            resolved_peer = self.send(\n                functions.contacts.ResolveUsername(\n                    username=chat_id.lower().strip(\"@\")\n                )\n            )\n\n            channel = types.InputPeerChannel(\n                channel_id=resolved_peer.chats[0].id,\n                access_hash=resolved_peer.chats[0].access_hash\n            )\n\n            chat = self.send(\n                functions.channels.JoinChannel(\n                    channel=channel\n                )\n            )\n\n            return pyrogram.Chat._parse_channel_chat(self, chat.chats[0])",
        "rewrite": "```python\ndef join_chat(\n    self,\n    chat_id: str\n) -> pyrogram.Chat:\n    match = self.INVITE_LINK_RE.match(chat_id)\n\n    if match:\n        chat = self.send(\n            functions.messages.ImportChatInvite(\n                hash=match.group(1)\n            )\n        )\n        return pyrogram.Chat._parse_chat_chat(self, chat.chats[0]) if isinstance(chat.chats[0], types.Chat) else pyrogram.Chat._parse_channel_chat(self, chat.chats[0])\n\n    resolved_peer = self.send(\n        functions.contacts.ResolveUsername(\n            username=chat_id.lower()."
    },
    {
        "original": "def renderer_doc(*args):\n    \"\"\"\n    Return the docstrings for all renderers. Optionally, specify a renderer or a\n    function to narrow the selection.\n\n    The strings are aggregated into a single document on the master for easy\n    reading.\n\n    Multiple renderers can be specified.\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc\n        salt '*' sys.renderer_doc cheetah\n        salt '*' sys.renderer_doc jinja json\n\n    Renderer names can be specified as globs.\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc 'c*' 'j*'\n\n    \"\"\"\n    renderers_ = salt.loader.render(__opts__, [])\n    docs = {}\n    if not args:\n        for func in six.iterkeys(renderers_):\n            docs[func] = renderers_[func].__doc__\n        return _strip_rst(docs)\n\n    for module in args:\n        if '*' in module or '.' in module:\n            for func in fnmatch.filter(renderers_, module):\n                docs[func] = renderers_[func].__doc__\n        else:\n            moduledot = module + '.'\n            for func in six.iterkeys(renderers_):\n                if func.startswith(moduledot):\n                    docs[func] = renderers_[func].__doc__\n    return _strip_rst(docs)",
        "rewrite": "```python\nimport salt.loader as loader\nimport fnmatch\nimport six\n\ndef renderer_doc(*args):\n    renderers_ = loader.render(__opts__, [])\n    docs = {}\n\n    if not args:\n        for func in six.iterkeys(renderers_):\n            docs[func] = renderers_[func].__doc__\n        return _strip_rst(docs)\n\n    for module in args:\n        if '*' in module or '.' in module:\n            for func in fnmatch.filter(renderers_, module):\n                docs[func] = renderers_[func].__doc__\n        else:\n            moduledot = f\"{module"
    },
    {
        "original": "def commit(self):\n        \"\"\"\n        Commit this context's offsets:\n\n         -  If the high-water mark has moved, commit up to and position the\n            consumer at the high-water mark.\n         -  Otherwise, reset to the consumer to the initial offsets.\n        \"\"\"\n        if self.high_water_mark:\n            self.logger.info(\"Committing offsets: %s\", self.high_water_mark)\n            self.commit_partition_offsets(self.high_water_mark)\n            self.update_consumer_offsets(self.high_water_mark)\n        else:\n            self.update_consumer_offsets(self.initial_offsets)",
        "rewrite": "```python\ndef commit(self):\n    if self.high_water_mark:\n        self.logger.info(\"Committing offsets: %s\", self.high_water_mark)\n        self.commit_partition_offsets(self.high_water_mark)\n        self.update_consumer_offsets(self.high_water_mark)\n    else:\n        self.update_consumer_offsets(self.initial_offsets)\n```"
    },
    {
        "original": "def parametric_mean_function(max_iters=100, optimize=True, plot=True):\n    \"\"\"\n    A linear mean function with parameters that we'll learn alongside the kernel\n    \"\"\"\n    #create  simple mean function\n    mf = GPy.core.Mapping(1,1)\n    mf.f = np.sin\n\n    X = np.linspace(0,10,50).reshape(-1,1)\n    Y = np.sin(X) + 0.5*np.cos(3*X) + 0.1*np.random.randn(*X.shape) + 3*X\n\n    mf = GPy.mappings.Linear(1,1)\n\n    k =GPy.kern.RBF(1)\n    lik = GPy.likelihoods.Gaussian()\n    m = GPy.core.GP(X, Y, kernel=k, likelihood=lik, mean_function=mf)\n    if optimize:\n        m.optimize(max_iters=max_iters)\n    if plot:\n        m.plot()\n    return m",
        "rewrite": "```python\ndef parametric_mean_function(max_iters=100, optimize=True, plot=True):\n    X = np.linspace(0,10,50).reshape(-1,1)\n    Y = np.sin(X) + 0.5*np.cos(3*X) + 0.1*np.random.randn(*X.shape) + 3*X\n    \n    mf = GPy.mappings.Linear(1,1)\n    \n    k = GPy.kern.RBF(1)\n    lik = GPy.likelihoods.Gaussian()\n    \n    m = GPy.core.GP(X, Y - mfemean_object(), kernel=k"
    },
    {
        "original": "def get_network_adapter_type(adapter_type):\n    \"\"\"\n    Return the network adapter type.\n\n    adpater_type\n        The adapter type from which to obtain the network adapter type.\n    \"\"\"\n    if adapter_type == 'vmxnet':\n        return vim.vm.device.VirtualVmxnet()\n    elif adapter_type == 'vmxnet2':\n        return vim.vm.device.VirtualVmxnet2()\n    elif adapter_type == 'vmxnet3':\n        return vim.vm.device.VirtualVmxnet3()\n    elif adapter_type == 'e1000':\n        return vim.vm.device.VirtualE1000()\n    elif adapter_type == 'e1000e':\n        return vim.vm.device.VirtualE1000e()\n\n    raise ValueError('An unknown network adapter object type name.')",
        "rewrite": "```python\ndef get_network_adapter_type(adapter_type):\n    network_adapters = {\n        'vmxnet': vim.vm.device.VirtualVmxnet,\n        'vmxnet2': vim.vm.device.VirtualVmxnet2,\n        'vmxnet3': vim.vm.device.VirtualVmxnet3,\n        'e1000': vim.vm.device.VirtualE1000,\n        'e1000e': vim.vm.device.VirtualE1000e\n    }\n    \n    return network_adapters.get(adapter_type, None)\n    \nif __name__ == '__main__':\n    from apicem import *  # Assuming this is"
    },
    {
        "original": "def writef(notebook, nb_file, fmt=None):\n    \"\"\"Write a notebook to the file with given name\"\"\"\n    if nb_file == '-':\n        write(notebook, sys.stdout, fmt)\n        return\n\n    _, ext = os.path.splitext(nb_file)\n    fmt = copy(fmt or {})\n    fmt = long_form_one_format(fmt, update={'extension': ext})\n\n    create_prefix_dir(nb_file, fmt)\n\n    with io.open(nb_file, 'w', encoding='utf-8') as stream:\n        write(notebook, stream, fmt)",
        "rewrite": "```python\nimport os\nimport sys\nfrom copy import deepcopy as copy\nfrom nbformat import write, long_form_one_format, create_prefix_dir\n\ndef writef(notebook, nb_file, fmt=None):\n    if nb_file == '-':\n        write(notebook, sys.stdout)\n        return\n\n    _, ext = os.path.splitext(nb_file)\n    fmt = copy(fmt or {})\n    fmt = long_form_one_format(fmt, update={'extension': ext})\n\n    create_prefix_dir(nb_file, fmt)\n\n    with open(nb_file, 'w', encoding='utf-8') as stream:\n        write(notebook, stream)\n```"
    },
    {
        "original": "def pickle_dump(self):\n        \"\"\"\n        Save the status of the object in pickle format.\n        Returns 0 if success\n        \"\"\"\n        if self.has_chrooted:\n            warnings.warn(\"Cannot pickle_dump since we have chrooted from %s\" % self.has_chrooted)\n            return -1\n\n        #if self.in_spectator_mode:\n        #    warnings.warn(\"Cannot pickle_dump since flow is in_spectator_mode\")\n        #    return -2\n\n        protocol = self.pickle_protocol\n\n        # Atomic transaction with FileLock.\n        with FileLock(self.pickle_file):\n            with AtomicFile(self.pickle_file, mode=\"wb\") as fh:\n                pmg_pickle_dump(self, fh, protocol=protocol)\n\n        return 0",
        "rewrite": "```python\ndef pickle_dump(self):\n    if self.has_chrooted:\n        return -1\n\n    #if self.in_spectator_mode:\n    #    return -2\n\n    protocol = self.pickle_protocol\n\n    with FileLock(self.pickle_file):\n        with AtomicFile(self.pickle_file, mode=\"wb\") as fh:\n            pmg_pickle_dump(self, fh, protocol=protocol)\n\n    return 0\n```"
    },
    {
        "original": "def _MakeTimestamp(self, start=None, end=None):\n    \"\"\"Create a timestamp using a start and end time.\n\n    Args:\n      start: Start timestamp.\n      end: End timestamp.\n\n    Returns:\n      A tuple (start, end) of converted timestamps or None for all time.\n    \"\"\"\n    mysql_unsigned_bigint_max = 18446744073709551615\n    ts_start = int(start or 0)\n    if end is None:\n      ts_end = mysql_unsigned_bigint_max\n    else:\n      ts_end = int(end)\n    if ts_start == 0 and ts_end == mysql_unsigned_bigint_max:\n      return None\n    else:\n      return (ts_start, ts_end)",
        "rewrite": "```python\ndef _make_timestamp(self, start=None, end=None):\n    mysql_unsigned_bigint_max = 18446744073709551615\n    ts_start = int(start or 0)\n    if end is None:\n        ts_end = mysql_unsigned_bigint_max\n    else:\n        ts_end = int(end)\n    if (ts_start == 0 and ts_end == mysql_unsigned_bigint_max) or (ts_start > mysql_unsigned_bigint_max or ts_end > mysql_unsigned_bigint_max):\n        return None\n    else:\n        return (ts_start, ts_end)\n```"
    },
    {
        "original": "def loopUntil(\n            self, condition=None, timeout: float = 0) -> Iterator[object]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout in seconds.\n        The yielded value is that of the condition or False when timed out.\n\n        Args:\n            condition: Predicate function that is tested after every network\n            update.\n            timeout: Maximum time in seconds to wait.\n                If 0 then no timeout is used.\n        \"\"\"\n        endTime = time.time() + timeout\n        while True:\n            test = condition and condition()\n            if test:\n                yield test\n                return\n            elif timeout and time.time() > endTime:\n                yield False\n                return\n            else:\n                yield test\n            self.waitOnUpdate(endTime - time.time() if timeout else 0)",
        "rewrite": "```python\nfrom datetime import timedelta\nimport time\n\nclass WaitableIterator:\n    def loopUntil(\n            self, condition: callable = None, timeout: timedelta = timedelta(0)) -> tuple[iterator]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout.\n        The yielded value is that of the condition or False when timed out.\n\n        Args:\n            condition: Predicate function that is tested after every network update.\n            timeout: Maximum time to wait.\n                If 0 then no timeout is used.\n        \"\"\"\n        \n        if not isinstance(timeout, timedelta):\n            raise TypeError(\"Timeout must be a timedelta object\")\n        \n"
    },
    {
        "original": "def retrieve_equities(self, sids):\n        \"\"\"\n        Retrieve Equity objects for a list of sids.\n\n        Users generally shouldn't need to this method (instead, they should\n        prefer the more general/friendly `retrieve_assets`), but it has a\n        documented interface and tests because it's used upstream.\n\n        Parameters\n        ----------\n        sids : iterable[string]\n\n        Returns\n        -------\n        equities : dict[str -> Equity]\n\n        Raises\n        ------\n        EquitiesNotFound\n            When any requested asset isn't found.\n        \"\"\"\n        cache = self._asset_cache\n\n        try:\n            return {\n                k: cache[k]\n                for k in sids\n            }\n        except KeyError:\n            raise EquitiesNotFound(sids=sids)",
        "rewrite": "```python\ndef retrieve_equities(self, sids):\n    cache = self._asset_cache\n\n    try:\n        return {\n            k: v\n            for k, v in cache.items()\n            if k in sids\n        }\n    except KeyError as e:\n        raise EquitiesNotFound(sids=e.args[0])\n```"
    },
    {
        "original": "def _calc_taud(w, aod700, p):\n    \"\"\"Calculate the taud coefficient.\"\"\"\n\n    # isscalar tests needed to ensure that the arrays will have the\n    # right shape in the tds calculation.\n    # there's probably a better way to do this.\n\n    if np.isscalar(w) and np.isscalar(aod700):\n        w = np.array([w])\n        aod700 = np.array([aod700])\n    elif np.isscalar(w):\n        w = np.full_like(aod700, w)\n    elif np.isscalar(aod700):\n        aod700 = np.full_like(w, aod700)\n\n    # set up nan-tolerant masks\n    aod700_lt_0p05 = np.full_like(aod700, False, dtype='bool')\n    np.less(aod700, 0.05, where=~np.isnan(aod700), out=aod700_lt_0p05)\n    aod700_mask = np.array([aod700_lt_0p05, ~aod700_lt_0p05], dtype=np.int)\n\n    # create tuples of coefficients for\n    # aod700 < 0.05, aod700 >= 0.05\n    td4 = 86*w - 13800, -0.21*w + 11.6\n    td3 = -3.11*w + 79.4, 0.27*w - 20.7\n    td2 = -0.23*w + 74.8, -0.134*w + 15.5\n    td1 = 0.092*w - 8.86, 0.0554*w - 5.71\n    td0 = 0.0042*w + 3.12, 0.0057*w + 2.94\n    tdp = -0.83*(1+aod700)**(-17.2), -0.71*(1+aod700)**(-15.0)\n\n    tds = (np.array([td0, td1, td2, td3, td4, tdp]) * aod700_mask).sum(axis=1)\n\n    p0 = 101325.\n    taud = (tds[4]*aod700**4 + tds[3]*aod700**3 + tds[2]*aod700**2 +\n            tds[1]*aod700 + tds[0] + tds[5]*np.log(p/p0))\n\n    # be polite about matching the output type to the input type(s)\n    if len(taud) == 1:\n        taud = taud[0]\n\n    return taud",
        "rewrite": "```python\nimport numpy as np\n\ndef _calc_taud(w, aod700, p):\n    if np.isscalar(w) and np.isscalar(aod700):\n        w = np.array([w])\n        aod700 = np.array([aod700])\n    elif np.isscalar(w):\n        w = np.full_like(aod700, w)\n    elif np.isscalar(aod700):\n        aod700 = np.full_like(w, aod700)\n\n    mask_lt_0p05 = ~np.isnan(aod700) & (aod700 < 0.05)\n"
    },
    {
        "original": "def cleanup(temp_name):\n    \"\"\" Tries to remove temp files by filename wildcard path. \"\"\"\n    for filename in iglob(temp_name + '*' if temp_name else temp_name):\n        try:\n            os.remove(filename)\n        except OSError:\n            pass",
        "rewrite": "```python\nimport glob\n\ndef cleanup(temp_name: str):\n    try:\n        glob.glob(temp_name + '*').remove(stubs=\".DS_Store\")\n        for filename in glob.glob(temp_name + '*'):\n            os.remove(filename)\n    except OSError:\n        pass\n```\n\n OR \n\n```python\nimport glob\nimport os\n\ndef cleanup(*, temp_name: str = \"\"):\n    for filename in glob.iglob(os.path.join(temp_name, \"*\")):\n        try:\n            os.remove(filename)\n        except OSError as e:\n            print(f\"Error removing file {filename}: {e}\")\n```"
    },
    {
        "original": "def _acl_changes(name, id=None, type=None, rules=None, consul_url=None, token=None):\n    \"\"\"\n       return True if the acl need to be update, False if it doesn't need to be update\n    \"\"\"\n    info = __salt__['consul.acl_info'](id=id, token=token, consul_url=consul_url)\n\n    if info['res'] and info['data'][0]['Name'] != name:\n        return True\n    elif info['res'] and info['data'][0]['Rules'] != rules:\n        return True\n    elif info['res'] and info['data'][0]['Type'] != type:\n        return True\n    else:\n        return False",
        "rewrite": "```python\ndef _acl_changes(name, id=None, type=None, rules=None, consul_url=None, token=None):\n    info = __salt__['consul.acl_info'](id=id, token=token, consul_url=consul_url)\n    \n    return (info['res'] and \n            (info['data'][0]['Name'] != name or \n             info['data'][0]['Rules'] != rules or \n             info['data'][0]['Type'] != type))\n```"
    },
    {
        "original": "def StreamFile(self, filedesc, offset=0, amount=None):\n    \"\"\"Streams chunks of a given file starting at given offset.\n\n    Args:\n      filedesc: A `file` object to stream.\n      offset: An integer offset at which the file stream should start on.\n      amount: An upper bound on number of bytes to read.\n\n    Returns:\n      Generator over `Chunk` instances.\n    \"\"\"\n    reader = FileReader(filedesc, offset=offset)\n    return self.Stream(reader, amount=amount)",
        "rewrite": "```python\ndef StreamFile(self, filedesc, offset=0, amount=None):\n    return self._ReadFromStream(FileReader(filedesc=filedesc, offset=offset), amount=amount)\n```\n\nAssuming that `FileReader` has an `__iter__` method and the class has a `_ReadFromStream` method."
    },
    {
        "original": "def index(self, column):  # pylint: disable=C6409\n        \"\"\"Fetches the column number (0 indexed).\n\n    Args:\n      column: A string, column to fetch the index of.\n\n    Returns:\n      An int, the row index number.\n\n    Raises:\n      ValueError: The specified column was not found.\n    \"\"\"\n        for i, key in enumerate(self._keys):\n            if key == column:\n                return i\n        raise ValueError('Column \"%s\" not found.' % column)",
        "rewrite": "```python\ndef index(self, column):\n    for i, key in enumerate(self._keys):\n        if key == column:\n            return i\n    raise ValueError(f\"Column '{column}' not found.\")\n```"
    },
    {
        "original": "def pointer_gate(num_qubits, U):\n    \"\"\"\n    Make a pointer gate on `num_qubits`. The one-qubit gate U will act on the\n    qubit addressed by the pointer qubits interpreted as an unsigned binary\n    integer.\n\n    There are P = floor(lg(num_qubits)) pointer qubits, and qubits numbered\n\n        N - 1\n        N - 2\n        ...\n        N - P\n\n    are those reserved to represent the pointer. The first N - P qubits\n    are the qubits which the one-qubit gate U can act on.\n    \"\"\"\n    ptr_bits = int(floor(np.log2(num_qubits)))\n    data_bits = num_qubits - ptr_bits\n    ptr_state = 0\n    assert ptr_bits > 0\n\n    program = pq.Program()\n\n    program.defgate(\"CU\", controlled(ptr_bits, U))\n\n    for _, target_qubit, changed in gray(ptr_bits):\n        if changed is None:\n            for ptr_qubit in range(num_qubits - ptr_bits, num_qubits):\n                program.inst(X(ptr_qubit))\n                ptr_state ^= 1 << (ptr_qubit - data_bits)\n        else:\n            program.inst(X(data_bits + changed))\n            ptr_state ^= 1 << changed\n\n        if target_qubit < data_bits:\n            control_qubits = tuple(data_bits + i for i in range(ptr_bits))\n            program.inst((\"CU\",) + control_qubits + (target_qubit,))\n\n    fixup(program, data_bits, ptr_bits, ptr_state)\n    return program",
        "rewrite": "```python\nimport numpy as np\nfrom qiskit import QuantumCircuit, QuantumProgram\n\ndef pointer_gate(num_qubits, U):\n    ptr_bits = int(np.floor(np.log2(num_qubits)))\n    data_bits = num_qubits - ptr_bits\n    assert ptr_bits > 0\n\n    program = QuantumProgram()\n\n    def gate_U(control_qubits, target_qubit):\n        return (U,) + control_qubits + (target_qubit,)\n\n    for _, target_qubit, changed in gray(ptr_bits):\n        if changed is None:\n            for ptr_qubit in range(num_qubits - ptr_bits"
    },
    {
        "original": "def posterior_predictive_to_xarray(self):\n        \"\"\"Convert posterior_predictive samples to xarray.\"\"\"\n        posterior_predictive = self.posterior_predictive\n        columns = self.posterior[0].columns\n        if (\n            isinstance(posterior_predictive, (tuple, list))\n            and posterior_predictive[0].endswith(\".csv\")\n        ) or (isinstance(posterior_predictive, str) and posterior_predictive.endswith(\".csv\")):\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            chain_data = []\n            for path in posterior_predictive:\n                parsed_output = _read_output(path)\n                for sample, *_ in parsed_output:\n                    chain_data.append(sample)\n            data = _unpack_dataframes(chain_data)\n        else:\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            posterior_predictive_cols = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in posterior_predictive)\n            ]\n            data = _unpack_dataframes([item[posterior_predictive_cols] for item in self.posterior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)",
        "rewrite": "```python\ndef posterior_predictive_to_xarray(self):\n    \"\"\"Convert posterior_predictive samples to xarray.\"\"\"\n    if isinstance(self.posterior_predictive, str) and self.posterior_predictive.endswith(\".csv\"):\n        self.posterior_predictive = [self.posterior_predictive]\n    if isinstance(self.posterior_predictive, (tuple, list)) and any(path.endswith(\".csv\") for path in self.posterior_predictive):\n        chain_data = []\n        for path in self.posterior_predictive:\n            parsed_output = _read_output(path)\n            for sample, *_ in parsed_output:\n                chain_data.append(sample)\n       "
    },
    {
        "original": "def delete(name, timeout=90):\n    \"\"\"\n    Delete the named service\n\n    Args:\n\n        name (str): The name of the service to delete\n\n        timeout (int):\n            The time in seconds to wait for the service to be deleted before\n            returning. This is necessary because a service must be stopped\n            before it can be deleted. Default is 90 seconds\n\n            .. versionadded:: 2017.7.9,2018.3.4\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``. Also returns ``True``\n            if the service is not present\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.delete <service name>\n    \"\"\"\n    handle_scm = win32service.OpenSCManager(\n        None, None, win32service.SC_MANAGER_CONNECT)\n\n    try:\n        handle_svc = win32service.OpenService(\n            handle_scm, name, win32service.SERVICE_ALL_ACCESS)\n    except pywintypes.error as exc:\n        win32service.CloseServiceHandle(handle_scm)\n        if exc.winerror != 1060:\n            raise CommandExecutionError(\n                'Failed to open {0}. {1}'.format(name, exc.strerror))\n        log.debug('Service \"%s\" is not present', name)\n        return True\n\n    try:\n        win32service.DeleteService(handle_svc)\n    except pywintypes.error as exc:\n        raise CommandExecutionError(\n            'Failed to delete {0}. {1}'.format(name, exc.strerror))\n    finally:\n        log.debug('Cleaning up')\n        win32service.CloseServiceHandle(handle_scm)\n        win32service.CloseServiceHandle(handle_svc)\n\n    end_time = time.time() + int(timeout)\n    while name in get_all() and time.time() < end_time:\n        time.sleep(1)\n\n    return name not in get_all()",
        "rewrite": "```python\nimport win32service\nimport pywintypes\nimport time\n\ndef delete(name, timeout=90):\n    handle_scm = win32service.OpenSCManager(None, None, win32service.SC_MANAGER_CONNECT)\n    try:\n        handle_svc = win32service.OpenService(handle_scm, name, win32service.SERVICE_ALL_ACCESS)\n    except pywintypes.error as exc:\n        if exc.winerror != 1060:\n            raise CommandExecutionError(f'Failed to open {name}. {exc.strerror}')\n        log.debug('Service \"%s\" is not present', name)\n        return True"
    },
    {
        "original": "def remove_nodes(self, indices):\n        \"\"\"\n        A wrapper for Molecule.remove_sites().\n\n        :param indices: list of indices in the current Molecule (and graph) to\n            be removed.\n        :return:\n        \"\"\"\n\n        self.molecule.remove_sites(indices)\n        self.graph.remove_nodes_from(indices)\n\n        mapping = {}\n        for correct, current in enumerate(sorted(self.graph.nodes)):\n            mapping[current] = correct\n\n        nx.relabel_nodes(self.graph, mapping, copy=False)\n        self.set_node_attributes()",
        "rewrite": "```python\ndef remove_nodes(self, indices):\n    self.molecule.remove_sites(indices)\n    self.graph.remove_nodes_from(indices)\n\n    mapping = {node: idx for idx, node in enumerate(sorted(self.graph.nodes))}\n\n    nx.relabel_nodes(self.graph, mapping, copy=False)\n    self.set_node_attributes()\n```"
    },
    {
        "original": "def ParseYAMLAuthorizationsList(yaml_data):\n    \"\"\"Parses YAML data into a list of APIAuthorization objects.\"\"\"\n    try:\n      raw_list = yaml.ParseMany(yaml_data)\n    except (ValueError, pyyaml.YAMLError) as e:\n      raise InvalidAPIAuthorization(\"Invalid YAML: %s\" % e)\n\n    result = []\n    for auth_src in raw_list:\n      auth = APIAuthorization()\n      auth.router_cls = _GetRouterClass(auth_src[\"router\"])\n      auth.users = auth_src.get(\"users\", [])\n      auth.groups = auth_src.get(\"groups\", [])\n      auth.router_params = auth_src.get(\"router_params\", {})\n\n      result.append(auth)\n\n    return result",
        "rewrite": "```python\ndef parse_yaml_authorizations_list(yaml_data):\n    try:\n        raw_list = yaml.safe_load_all(yaml_data)\n    except (ValueError, yaml.YAMLError) as e:\n        raise InvalidAPIAuthorization(\"Invalid YAML: %s\" % e)\n\n    result = []\n    for auth_src in raw_list:\n        if not isinstance(auth_src, dict):\n            raise InvalidAPIAuthorization(\"Invalid YAML: expected a dictionary\")\n        \n        auth = APIAuthorization()\n        auth.router_cls = _get_router_class(auth_src.get(\"router\"))\n        auth.users = auth_src.get(\"users\", [])\n        auth.groups ="
    },
    {
        "original": "def _parse_canonical_int32(doc):\n    \"\"\"Decode a JSON int32 to python int.\"\"\"\n    i_str = doc['$numberInt']\n    if len(doc) != 1:\n        raise TypeError('Bad $numberInt, extra field(s): %s' % (doc,))\n    if not isinstance(i_str, string_type):\n        raise TypeError('$numberInt must be string: %s' % (doc,))\n    return int(i_str)",
        "rewrite": "```python\ndef _parse_canonical_int32(doc):\n    i_str = doc['$numberInt']\n    if len(doc) > 1:\n        raise ValueError('Bad $numberInt, extra field(s): {}'.format(dot.tojson(doc)))\n    if not isinstance(i_str, str):\n        raise ValueError('$numberInt must be string: {}'.format(dot.tojson(doc)))\n    return int(i_str)\n```"
    },
    {
        "original": "def keep_path(self, path):\n        \"\"\"\n        Given a path, returns True if the path should be kept, False if it should be cut.\n        \"\"\"\n        if len(path.addr_trace) < 2:\n            return True\n\n        return self.should_take_exit(path.addr_trace[-2], path.addr_trace[-1])",
        "rewrite": "```python\ndef keep_path(self, path):\n    return len(path.addr_trace) >= 2 and self.should_take_exit(path.addr_trace[-2], path.addr_trace[-1])\n```"
    },
    {
        "original": "def get(key, default=''):\n    \"\"\"\n    .. versionadded: 0.14.0\n\n    Attempt to retrieve the named value from opts, pillar, grains of the master\n    config, if the named value is not available return the passed default.\n    The default return is an empty string.\n\n    The value can also represent a value in a nested dict using a \":\" delimiter\n    for the dict. This means that if a dict looks like this::\n\n        {'pkg': {'apache': 'httpd'}}\n\n    To retrieve the value associated with the apache key in the pkg dict this\n    key can be passed::\n\n        pkg:apache\n\n    This routine traverses these data stores in this order:\n\n    - Local minion config (opts)\n    - Minion's grains\n    - Minion's pillar\n    - Master config\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' config.get pkg:apache\n    \"\"\"\n    ret = salt.utils.data.traverse_dict_and_list(__opts__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__grains__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__pillar__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__pillar__.get('master', {}), key, '_|-')\n    if ret != '_|-':\n        return ret\n    return default",
        "rewrite": "```python\nimport salt.utils.data\n\ndef get(key, default=''):\n    def traverse(data):\n        return salt.utils.data.traverse_dict_and_list(data, key, '_|-') if data else default\n\n    ret = traverse(__opts__)\n    if ret != '_|-':\n        return ret\n    ret = traverse(__grains__)\n    if ret != '_|-':\n        return ret\n    ret = traverse(__pillar__)\n    if isinstance(ret, dict):\n        del __pillar__['_return']  # Clear retained cache\n    elif isinstance(ret, (str, int)):\n        return ret\n    \n    master_config = __pillar__.get"
    },
    {
        "original": "def get_followers(self, first_user_id=None):\n        \"\"\"\n        \u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n        \u8be6\u60c5\u8bf7\u53c2\u8003 http://mp.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n\n        :param first_user_id: \u53ef\u9009\u3002\u7b2c\u4e00\u4e2a\u62c9\u53d6\u7684OPENID\uff0c\u4e0d\u586b\u9ed8\u8ba4\u4ece\u5934\u5f00\u59cb\u62c9\u53d6\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        params = {\"access_token\": self.token}\n        if first_user_id:\n            params[\"next_openid\"] = first_user_id\n        return self.get(\n            \"https://api.weixin.qq.com/cgi-bin/user/get\", params=params\n        )",
        "rewrite": "```python\ndef get_followers(self, first_user_id=None):\n    params = {\"access_token\": self.token}\n    if first_user_id:\n        params[\"next_openid\"] = first_user_id\n    return self.get(\"https://api.weixin.qq.com/cgi-bin/user/get\", params=params)\n```"
    },
    {
        "original": "def get_metric_value_by_labels(messages, _metric, _m, metric_suffix):\n        \"\"\"\n        :param messages: dictionary as metric_name: {labels: {}, value: 10}\n        :param _metric: dictionary as {labels: {le: '0.001', 'custom': 'value'}}\n        :param _m: str as metric name\n        :param metric_suffix: str must be in (count or sum)\n        :return: value of the metric_name matched by the labels\n        \"\"\"\n        metric_name = '{}_{}'.format(_m, metric_suffix)\n        expected_labels = set(\n            [(k, v) for k, v in iteritems(_metric[\"labels\"]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]\n        )\n        for elt in messages[metric_name]:\n            current_labels = set(\n                [(k, v) for k, v in iteritems(elt[\"labels\"]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]\n            )\n            # As we have two hashable objects we can compare them without any side effects\n            if current_labels == expected_labels:\n                return float(elt[\"value\"])\n\n        raise AttributeError(\"cannot find expected labels for metric %s with suffix %s\" % (metric_name, metric_suffix))",
        "rewrite": "```python\ndef get_metric_value_by_labels(messages, metric, m, metric_suffix):\n    metric_name = f\"{m}_{metric_suffix}\"\n    expected_labels = set(\n        (k, v) for k, v in metric[\"labels\"].items() if k not in PrometheusScraperMixin.UNWANTED_LABELS\n    )\n    for elt in messages[metric_name]:\n        current_labels = set(\n            (k, v) for k, v in elt[\"labels\"].items() if k not in PrometheusScraperMixin.UNWANTED_LABELS\n        )\n        if current_labels == expected_labels:\n            return float("
    },
    {
        "original": "def next(self):\n        \"\"\"Advance the cursor.\"\"\"\n        if len(self.__data) or self._refresh():\n            coll = self.__collection\n            return coll.database._fix_outgoing(self.__data.popleft(), coll)\n        else:\n            raise StopIteration",
        "rewrite": "```python\ndef next(self):\n    if self.__data or self._refresh():\n        coll = self.__collection\n        return coll.database._fix_outgoing(self.__data.popleft(), coll)\n    else:\n        raise StopIteration\n```"
    },
    {
        "original": "def get_outputs_filtered(self, owner, spent=None):\n        \"\"\"Get a list of output links filtered on some criteria\n\n        Args:\n            owner (str): base58 encoded public_key.\n            spent (bool): If ``True`` return only the spent outputs. If\n                          ``False`` return only unspent outputs. If spent is\n                          not specified (``None``) return all outputs.\n\n        Returns:\n            :obj:`list` of TransactionLink: list of ``txid`` s and ``output`` s\n            pointing to another transaction's condition\n        \"\"\"\n        outputs = self.fastquery.get_outputs_by_public_key(owner)\n        if spent is None:\n            return outputs\n        elif spent is True:\n            return self.fastquery.filter_unspent_outputs(outputs)\n        elif spent is False:\n            return self.fastquery.filter_spent_outputs(outputs)",
        "rewrite": "```python\ndef get_outputs_filtered(self, owner, spent=None):\n    outputs = self.fastquery.get_outputs_by_public_key(owner)\n    if spent is None:\n        return outputs\n    elif spent:\n        return self.fastquery.filter_unspent_outputs(outputs)\n    else:\n        return self.fastquery.filter_spent_outputs(outputs)\n```"
    },
    {
        "original": "def owns_endpoint(self, endpoint):\n        \"\"\"Tests if an endpoint name (not path) belongs to this Api.  Takes\n        in to account the Blueprint name part of the endpoint name.\n\n        :param endpoint: The name of the endpoint being checked\n        :return: bool\n        \"\"\"\n\n        if self.blueprint:\n            if endpoint.startswith(self.blueprint.name):\n                endpoint = endpoint.split(self.blueprint.name + '.', 1)[-1]\n            else:\n                return False\n        return endpoint in self.endpoints",
        "rewrite": "```python\ndef owns_endpoint(self, endpoint):\n    if self.blueprint:\n        if not endpoint.startswith(self.blueprint.name):\n            return False\n        endpoint = '.'.join(endpoint.split(self.blueprint.name + '.')[-1:])\n    return endpoint in self.endpoints\n```"
    },
    {
        "original": "def get_jobs(state='all'):\n    \"\"\"\n    List all jobs on the device.\n\n    state\n        The state of the jobs to display. Valid options are all, pending, or processed. Pending jobs are jobs\n        that are currently in a running or waiting state. Processed jobs are jobs that have completed\n        execution.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' panos.get_jobs\n        salt '*' panos.get_jobs state=pending\n\n    \"\"\"\n    if state.lower() == 'all':\n        query = {'type': 'op', 'cmd': '<show><jobs><all></all></jobs></show>'}\n    elif state.lower() == 'pending':\n        query = {'type': 'op', 'cmd': '<show><jobs><pending></pending></jobs></show>'}\n    elif state.lower() == 'processed':\n        query = {'type': 'op', 'cmd': '<show><jobs><processed></processed></jobs></show>'}\n    else:\n        raise CommandExecutionError(\"The state parameter must be all, pending, or processed.\")\n\n    return __proxy__['panos.call'](query)",
        "rewrite": "```python\ndef get_jobs(state='all'):\n    if state not in ['all', 'pending', 'processed']:\n        __context__['event_sender'].fire('cmd_error', detail=f\"The state parameter must be all, pending, or processed.\")\n        return {'error': 'Invalid state'}\n\n    query = {'type': 'op', 'cmd': f'<show><jobs><{state.lower()}></{state.lower()}></jobs></show>'}\n    return __proxy__['panos.call'](query)\n```"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"Return a list of IP addresses.\"\"\"\n    results = []\n    for address in self.addresses:\n      human_readable_address = address.human_readable_address\n      if human_readable_address is not None:\n        results.append(human_readable_address)\n\n    return results",
        "rewrite": "def get_ip_addresses(self):\n    return [address.human_readable_address for address in self.addresses if address.human_readable_address is not None]"
    },
    {
        "original": "def ikev2scan(ip, **kwargs):\n    \"\"\"Send a IKEv2 SA to an IP and wait for answers.\"\"\"\n    return sr(IP(dst=ip) / UDP() / IKEv2(init_SPI=RandString(8),\n                                         exch_type=34) / IKEv2_payload_SA(prop=IKEv2_payload_Proposal()), **kwargs)",
        "rewrite": "```python\nfrom scapy.all import IP, UDP, IKEv2, IKEv2_payload_SA, IKEv2_payload_Proposal, RandString\n\ndef ikev2scan(ip):\n    return sr(IP(dst=ip) / UDP() / IKEv2(init_SPI=RandString(8), exch_type=34) / \n               IKEv2_payload_SA(prop=IKEv2_payload_Proposal()), timeout=None)\n```"
    },
    {
        "original": "def list_versions(self, symbol=None, snapshot=None, latest_only=False):\n        \"\"\"\n        Return a list of versions filtered by the passed in parameters.\n\n        Parameters\n        ----------\n        symbol : `str`\n            Symbol to return versions for.  If None returns versions across all\n            symbols in the library.\n        snapshot : `str`\n            Return the versions contained in the named snapshot\n        latest_only : `bool`\n            Only include the latest version for a specific symbol\n\n        Returns\n        -------\n        List of dictionaries describing the discovered versions in the library\n        \"\"\"\n        if symbol is None:\n            symbols = self.list_symbols(snapshot=snapshot)\n        else:\n            symbols = [symbol]\n\n        query = {}\n\n        if snapshot is not None:\n            try:\n                query['parent'] = self._snapshots.find_one({'name': snapshot})['_id']\n            except TypeError:\n                raise NoDataFoundException('No snapshot %s in library %s' % (snapshot, self._arctic_lib.get_name()))\n\n        versions = []\n        snapshots = {ss.get('_id'): ss.get('name') for ss in self._snapshots.find()}\n        for symbol in symbols:\n            query['symbol'] = symbol\n            seen_symbols = set()\n            for version in self._versions.find(query, projection=['symbol', 'version', 'parent', 'metadata.deleted'], sort=[('version', -1)]):\n                if latest_only and version['symbol'] in seen_symbols:\n                    continue\n                seen_symbols.add(version['symbol'])\n                meta = version.get('metadata')\n                versions.append({'symbol': version['symbol'], 'version': version['version'],\n                                 'deleted': meta.get('deleted', False) if meta else False,\n                                 # We return offset-aware datetimes in Local Time.\n                                 'date': ms_to_datetime(datetime_to_ms(version['_id'].generation_time)),\n                                 'snapshots': [snapshots[s] for s in version.get('parent', []) if s in snapshots]})\n        return versions",
        "rewrite": "```python\ndef list_versions(self, symbol=None, snapshot=None, latest_only=False):\n    if symbol is None:\n        symbols = self.list_symbols(snapshot=snapshot)\n    else:\n        symbols = [symbol]\n\n    query = {}\n    if snapshot is not None:\n        try:\n            query['parent'] = self._snapshots.find_one({'name': snapshot})['_id']\n        except TypeError as e:\n            raise NoDataFoundException(f'No snapshot {snapshot} in library {self._arctic_lib.get_name()}') from e\n\n    versions = []\n    snapshots_dict = {ss['_id']: ss['name']"
    },
    {
        "original": "def get_function_policy(self, function_name):\n        # type: (str) -> Dict[str, Any]\n        \"\"\"Return the function policy for a lambda function.\n\n        This function will extract the policy string as a json document\n        and return the json.loads(...) version of the policy.\n\n        \"\"\"\n        client = self._client('lambda')\n        try:\n            policy = client.get_policy(FunctionName=function_name)\n            return json.loads(policy['Policy'])\n        except client.exceptions.ResourceNotFoundException:\n            return {'Statement': []}",
        "rewrite": "```python\ndef get_function_policy(self, function_name):\n    client = self._client('lambda')\n    try:\n        policy = client.get_policy(FunctionName=function_name)\n        return json.loads(policy['Policy'])\n    except client.exceptions.ResourceNotFoundException:\n        return {'Statement': []}\n```"
    },
    {
        "original": "def get_load(jid):\n    \"\"\"\n    Return the load data that marks a specified jid\n    \"\"\"\n    jid = _escape_jid(jid)\n    conn = _get_conn()\n    if conn is None:\n        return None\n    cur = conn.cursor()\n    sql = ",
        "rewrite": "```python\ndef get_load(jid):\n    jid = _escape_jid(jid)\n    conn = _get_conn()\n    if conn is None:\n        return None\n    cur = conn.cursor()\n    sql = \"SELECT * FROM load WHERE jid = %s\"\n    try:\n        cur.execute(sql, (jid,))\n        result = cur.fetchone()\n        if result:\n            return dict(zip([desc[0] for desc in cur.description], result))\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n```\n\nThis revised code includes"
    },
    {
        "original": "def best(cls):\n        \"\"\"\n        Select the best ScriptWriter for this environment.\n        \"\"\"\n        if sys.platform == 'win32' or (os.name == 'java' and os._name == 'nt'):\n            return WindowsScriptWriter.best()\n        else:\n            return cls",
        "rewrite": "```python\ndef best(cls):\n    if sys.platform in ['win32', 'cygwin'] or (os.name == 'java' and os.environ['OS'] == 'Windows_NT'):\n        return WindowsScriptWriter.best()\n    else:\n        return cls\n```"
    },
    {
        "original": "def xgroup_delconsumer(self, name, groupname, consumername):\n        \"\"\"\n        Remove a specific consumer from a consumer group.\n        Returns the number of pending messages that the consumer had before it\n        was deleted.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to delete\n        \"\"\"\n        return self.execute_command('XGROUP DELCONSUMER', name, groupname,\n                                    consumername)",
        "rewrite": "```python\ndef xgroup_delconsumer(self, name, groupname, consumername):\n    return self.execute_command('XGROUP DELCONSUMER', name, groupname, consumername)\n```"
    },
    {
        "original": "def pre_periodic_filtered(values, block, repeat_after):\n    \"\"\"\n    Blocks the first *block* items from *values*, repeating the block after\n    every *repeat_after* items, if *repeat_after* is non-zero. For example, to\n    discard the first 50 values read from an ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import pre_periodic_filtered\n\n        adc = MCP3008(channel=0)\n\n        for value in pre_periodic_filtered(adc, 50, 0):\n            print(value)\n\n    Or to only display every even item read from an ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import pre_periodic_filtered\n\n        adc = MCP3008(channel=0)\n\n        for value in pre_periodic_filtered(adc, 1, 1):\n            print(value)\n    \"\"\"\n    values = _normalize(values)\n    if block < 1:\n        raise ValueError(\"block must be 1 or larger\")\n    if repeat_after < 0:\n        raise ValueError(\"repeat_after must be 0 or larger\")\n    it = iter(values)\n    try:\n        if repeat_after == 0:\n            for _ in range(block):\n                next(it)\n            while True:\n                yield next(it)\n        else:\n            while True:\n                for _ in range(block):\n                    next(it)\n                for _ in range(repeat_after):\n                    yield next(it)\n    except StopIteration:\n        pass",
        "rewrite": "```python\ndef pre_periodic_filtered(values, block, repeat_after):\n    values = _normalize(values)\n    if block < 1:\n        raise ValueError(\"block must be 1 or larger\")\n    if repeat_after < 0:\n        raise ValueError(\"repeat_after must be 0 or larger\")\n    \n    it = iter(values)\n    \n    while True:\n        for _ in range(block):\n            next(it)\n        \n        if repeat_after == 0:\n            break\n        \n        for _ in range(repeat_after):\n            yield next(it)\n\ndef _normalize(values):\n    return iter(values) if not isinstance(values, (list"
    },
    {
        "original": "def skip_format_url(format_, url):\n    \"\"\"\n    Checks whether a give format/url should be skipped and not downloaded.\n\n    @param format_: Filename format (extension).\n    @type format_: str (e.g. html, txt, zip, pdf)\n\n    @param url: URL.\n    @type url: str\n\n    @return: True if format/url should be skipped, False otherwise.\n    @rtype bool\n    \"\"\"\n    # Do not download empty formats\n    if format_ == '':\n        return True\n\n    # Do not download email addresses\n    if ('mailto:' in url) and ('@' in url):\n        return True\n\n    # Is this localhost?\n    parsed = urlparse(url)\n    if parsed.hostname == 'localhost':\n        return True\n\n    # These are trusted manually added formats, do not skip them\n    if RE_VALID_FORMATS.match(format_):\n        return False\n\n    # Simple formats only contain letters, numbers, \"_\" and \"-\"\n    # If this a non simple format?\n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n\n    # Is this a link to the site root?\n    if parsed.path in ('', '/'):\n        return True\n\n    # Do not skip\n    return False",
        "rewrite": "```python\nimport re\nfrom urllib.parse import urlparse\n\ndef skip_format_url(format_, url, RE_VALID_FORMATS, RE_NON_SIMPLE_FORMAT):\n    if format_ == '':\n        return True\n    \n    if 'mailto:' in url and '@' in url:\n        return True\n    \n    parsed = urlparse(url)\n    \n    if parsed.hostname == 'localhost':\n        return True\n    \n    if RE_VALID_FORMATS.match(format_):\n        return False\n    \n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n    \n    if parsed.path in ('', '/'):\n        return True\n    \n    return False\n```"
    },
    {
        "original": "def get_domain_url(url):\n    \"\"\"\n    Use this to convert a url like this:\n    https://blog.xkcd.com/2014/07/22/what-if-book-tour/\n    Into this:\n    https://blog.xkcd.com\n    \"\"\"\n    if \"http://\" not in url and \"https://\" not in url:\n        return url\n    url_header = url.split('://')[0]\n    simple_url = url.split('://')[1]\n    base_url = simple_url.split('/')[0]\n    domain_url = url_header + '://' + base_url\n    return domain_url",
        "rewrite": "```python\ndef get_domain_url(url):\n    if not (url.startswith(\"http://\") or url.startswith(\"https://\")):\n        return url\n    header, simple_url = url.split('://', 1)\n    base_url = simple_url.split('/')[0]\n    return f\"{header}://{base_url}\"\n```"
    },
    {
        "original": "def dlogpdf_link_dr(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the log-likelihood function at y given f, w.r.t shape parameter\n\n        .. math::\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: derivative of likelihood evaluated at points f w.r.t variance parameter\n        :rtype: float\n        \"\"\"\n        # c = Y_metadata['censored']\n        # c = np.zeros((y.shape[0],))\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        link_f = inv_link_f #FIXME: Change names consistently...\n        y_link_f = y/link_f\n        log_y_link_f = np.log(y) - np.log(link_f)\n        y_link_f_r = y_link_f**self.r\n\n        #In terms of link_f\n        censored = c*(-y_link_f_r*log_y_link_f/(1 + y_link_f_r))\n        uncensored = (1-c)*(1./self.r + np.log(y) - np.log(link_f) - (2*y_link_f_r*log_y_link_f) / (1 + y_link_f_r))\n\n        dlogpdf_dr = censored + uncensored\n        return dlogpdf_dr",
        "rewrite": "```python\ndef dlogpdf_link_dr(self, inv_link_f, y, Y_metadata=None):\n    c = np.zeros_like(y)\n    if Y_metadata is not None and 'censored' in Y_metadata.keys():\n        c = Y_metadata['censored']\n\n    link_f = inv_link_f\n    y_link_f = y / link_f\n    log_y_link_f = np.log(y) - np.log(link_f)\n    y_link_f_r = y_link_f ** self.r\n\n    censored_term1 = -y_link_f_r * log_y_link_f / (1 + y_link_f_r)\n   "
    },
    {
        "original": "def get_p_vals(self, X):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tX : np.array\n\t\t\tArray of word counts, shape (N, 2) where N is the vocab size.  X[:,0] is the\n\t\t\tpositive class, while X[:,1] is the negative class. None by default\n\n\t\tReturns\n\t\t-------\n\t\tnp.array of p-values\n\n\t\t\"\"\"\n\t\tz_scores = self.get_scores(X[:, 0], X[:, 1])\n\t\treturn norm.cdf(z_scores)",
        "rewrite": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef get_p_vals(self, X):\n    z_scores = self.get_scores(X[:, 0], X[:, 1])\n    return norm.sf(z_scores)  # Using survival function to get p-values\n```"
    },
    {
        "original": "def settings_view_for_block(block_wrapper, settings_view_factory):\n        \"\"\"\n        Returns the settings view for an arbitrary block.\n\n        Args:\n            block_wrapper (BlockWrapper): The block for which a settings\n                view is to be returned\n            settings_view_factory (SettingsViewFactory): The settings\n                view factory used to create the SettingsView object\n\n        Returns:\n            SettingsView object associated with the block\n        \"\"\"\n        state_root_hash = \\\n            block_wrapper.state_root_hash \\\n            if block_wrapper is not None else None\n\n        return settings_view_factory.create_settings_view(state_root_hash)",
        "rewrite": "```python\ndef settings_view_for_block(block_wrapper: 'BlockWrapper', settings_view_factory: 'SettingsViewFactory') -> 'SettingsView':\n    state_root_hash = block_wrapper.state_root_hash if block_wrapper else None\n    return settings_view_factory.create_settings_view(state_root_hash)\n```"
    },
    {
        "original": "def authenticate(self, *, scopes, **kwargs):\n        \"\"\" Performs the oauth authentication flow resulting in a stored token\n        It uses the credentials passed on instantiation\n\n        :param list[str] scopes: list of protocol user scopes to be converted\n         by the protocol or scope helpers\n        :param kwargs: other configurations to be passed to the\n         Connection instance\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        kwargs.setdefault('token_backend', self.con.token_backend)\n        return oauth_authentication_flow(*self.con.auth, scopes=scopes,\n                                         protocol=self.protocol, **kwargs)",
        "rewrite": "```python\ndef authenticate(self, *, scopes, **kwargs):\n    kwargs.setdefault('token_backend', self.con.token_backend)\n    return oauth_authentication_flow(*self.con.auth, scopes=scopes,\n                                   protocol=self.protocol, **kwargs)\n```"
    },
    {
        "original": "def __convertRlocToRouterId(self, xRloc16):\n        \"\"\"mapping Rloc16 to router id\n\n        Args:\n            xRloc16: hex rloc16 short address\n\n        Returns:\n            actual router id allocated by leader\n        \"\"\"\n        routerList = []\n        routerList = self.__sendCommand('router list')[0].split()\n        print routerList\n        print xRloc16\n\n        for index in routerList:\n            router = []\n            cmd = 'router %s' % index\n            router = self.__sendCommand(cmd)\n\n            for line in router:\n                if 'Done' in line:\n                    break\n                elif 'Router ID' in line:\n                    routerid = line.split()[2]\n                elif 'Rloc' in line:\n                    rloc16 = line.split()[1]\n                else:\n                    pass\n\n            # process input rloc16\n            if isinstance(xRloc16, str):\n                rloc16 = '0x' + rloc16\n                if rloc16 == xRloc16:\n                    return routerid\n            elif isinstance(xRloc16, int):\n                if int(rloc16, 16) == xRloc16:\n                    return routerid\n            else:\n                pass\n\n        return None",
        "rewrite": "```python\ndef __convertRlocToRouterId(self, xRloc16):\n    routerList = self.__sendCommand('router list')[0].split()\n    for index in routerList:\n        cmd = 'router %s' % index\n        router = self.__sendCommand(cmd)\n        for line in router:\n            if 'Done' in line:\n                break\n            elif 'Router ID' in line:\n                routerid = line.split()[2]\n            elif 'Rloc' in line:\n                rloc16 = line.split()[1]\n        # process input rloc16\n        if isinstance(xR"
    },
    {
        "original": "def _get_connect_kwargs(self, host, port, user, password, database_url):\n        \"\"\"\n        Get the params to pass to psycopg2.connect() based on passed-in vals\n        from yaml settings file\n        \"\"\"\n        if database_url:\n            return {'dsn': database_url}\n\n        if not host:\n            raise CheckException(\"Please specify a PgBouncer host to connect to.\")\n\n        if not user:\n            raise CheckException(\"Please specify a user to connect to PgBouncer as.\")\n\n        if host in ('localhost', '127.0.0.1') and password == '':\n            # Use ident method\n            return {'dsn': \"user={} dbname={}\".format(user, self.DB_NAME)}\n\n        if port:\n            return {'host': host, 'user': user, 'password': password, 'database': self.DB_NAME, 'port': port}\n\n        return {'host': host, 'user': user, 'password': password, 'database': self.DB_NAME}",
        "rewrite": "```python\ndef _get_connect_kwargs(self, host, port, user, password, database_url):\n    if database_url:\n        return {'dsn': database_url}\n\n    if not host:\n        raise ValueError(\"Please specify a PgBouncer host to connect to.\")\n\n    if not user:\n        raise ValueError(\"Please specify a user to connect to PgBouncer as.\")\n\n    if host in ('localhost', '127.0.0.1') and not password:\n        # Use ident method\n        return {'dsn': f\"user={user} dbname={self.DB_NAME}\"}\n\n    if port:\n        return {'host': host"
    },
    {
        "original": "def set_salt_view():\n    \"\"\"\n    Helper function that sets the salt design\n    document. Uses get_valid_salt_views and some hardcoded values.\n    \"\"\"\n\n    options = _get_options(ret=None)\n\n    # Create the new object that we will shove in as the design doc.\n    new_doc = {}\n    new_doc['views'] = get_valid_salt_views()\n    new_doc['language'] = \"javascript\"\n\n    # Make the request to update the design doc.\n    _response = _request(\"PUT\",\n                         options['url'] + options['db'] + \"/_design/salt\",\n                         \"application/json\", salt.utils.json.dumps(new_doc))\n    if 'error' in _response:\n        log.warning('Unable to set the salt design document: %s', _response['error'])\n        return False\n    return True",
        "rewrite": "```python\ndef set_salt_view():\n    options = _get_options(ret=None)\n    new_doc = {\n        'views': get_valid_salt_views(),\n        'language': \"javascript\"\n    }\n    response = _request(\"PUT\", options['url'] + options['db'] + \"/_design/salt\", \"application/json\", salt.utils.json.dumps(new_doc))\n    if 'error' in response:\n        log.warning('Unable to set the salt design document: %s', response['error'])\n        return False\n    return True\n```"
    },
    {
        "original": "def while_not_sync_standby(self, func):\n        \"\"\"Runs specified action while trying to make sure that the node is not assigned synchronous standby status.\n\n        Tags us as not allowed to be a sync standby as we are going to go away, if we currently are wait for\n        leader to notice and pick an alternative one or if the leader changes or goes away we are also free.\n\n        If the connection to DCS fails we run the action anyway, as this is only a hint.\n\n        There is a small race window where this function runs between a master picking us the sync standby and\n        publishing it to the DCS. As the window is rather tiny consequences are holding up commits for one cycle\n        period we don't worry about it here.\"\"\"\n\n        if not self.is_synchronous_mode() or self.patroni.nosync:\n            return func()\n\n        with self._member_state_lock:\n            self._disable_sync += 1\n        try:\n            if self.touch_member():\n                # Master should notice the updated value during the next cycle. We will wait double that, if master\n                # hasn't noticed the value by then not disabling sync replication is not likely to matter.\n                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):\n                    try:\n                        if not self.is_sync_standby(self.dcs.get_cluster()):\n                            break\n                    except DCSError:\n                        logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\n                        break\n                    logger.info(\"Waiting for master to release us from synchronous standby\")\n            else:\n                logger.warning(\"Updating member state failed, skipping synchronous standby disable\")\n\n            return func()\n        finally:\n            with self._member_state_lock:\n                self._disable_sync -= 1",
        "rewrite": "```python\ndef while_not_sync_standby(self, func):\n    if not self.is_synchronous_mode() or self.patroni.nosync:\n        return func()\n\n    with self._member_state_lock:\n        self._disable_sync += 1\n    try:\n        if self.touch_member():\n            for _ in range(int(self.dcs.loop_wait * 2 / 2) + 1):\n                try:\n                    if not self.is_sync_standby(self.dcs.get_cluster()):\n                        break\n                except DCSError:\n                    logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\n                    break\n                logger"
    },
    {
        "original": "def _cleanup():\n    \"\"\"\n    Make sure that any terminal processes still running when __del__ was called\n    to the waited and cleaned up.\n    \"\"\"\n    for inst in _ACTIVE[:]:\n        res = inst.isalive()\n        if res is not True:\n            try:\n                _ACTIVE.remove(inst)\n            except ValueError:\n                # This can happen if two threads create a new Terminal instance\n                # It's harmless that it was already removed, so ignore.\n                pass",
        "rewrite": "```python\ndef _cleanup():\n    for inst in _ACTIVE[:]:\n        if not inst.isalive():\n            try:\n                _ACTIVE.remove(inst)\n            except ValueError:\n                pass\n```"
    },
    {
        "original": "def _write_recordio(f, data):\n    \"\"\"Writes a single data point as a RecordIO record to the given file.\"\"\"\n    length = len(data)\n    f.write(struct.pack('I', _kmagic))\n    f.write(struct.pack('I', length))\n    pad = (((length + 3) >> 2) << 2) - length\n    f.write(data)\n    f.write(padding[pad])",
        "rewrite": "```python\nimport struct\n\ndef _write_recordio(f, data):\n    length = len(data)\n    f.write(struct.pack('I', 0x52434552))  # _kmagic\n    f.write(struct.pack('I', length))\n    pad = (((length + 3) >> 2) << 2) - length\n    f.write(data)\n    f.write(b'\\x00' * pad)\n```"
    },
    {
        "original": "def make_api_method(func):\n    \"\"\"\n    Provides a single entry point for modifying all API methods.\n    For now this is limited to allowing the client object to be modified\n    with an `extra_params` keyword arg to each method, that is then used\n    as the params for each web service request.\n\n    Please note that this is an unsupported feature for advanced use only.\n    It's also currently incompatibile with multiple threads, see GH #160.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        args[0]._extra_params = kwargs.pop(\"extra_params\", None)\n        result = func(*args, **kwargs)\n        try:\n            del args[0]._extra_params\n        except AttributeError:\n            pass\n        return result\n    return wrapper",
        "rewrite": "```python\nimport functools\n\ndef make_api_method(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if 'extra_params' in kwargs:\n            args[0]._extra_params = kwargs.pop(\"extra_params\")\n        result = func(*args, **kwargs)\n        try:\n            del args[0]._extra_params\n        except AttributeError:\n            pass\n        return result\n    return wrapper\n```"
    },
    {
        "original": "def gwcalctyp(self):\n        \"\"\"Returns the value of the gwcalctyp input variable.\"\"\"\n        dig0 = str(self._SIGMA_TYPES[self.type])\n        dig1 = str(self._SC_MODES[self.sc_mode])\n        return dig1.strip() + dig0.strip()",
        "rewrite": "```python\ndef gwcalctyp(self):\n    return str(self._SC_MODES[self.sc_mode]) + str(self._SIGMA_TYPES[self.type])\n```"
    },
    {
        "original": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    if self._data_format == DATA_FORMAT_NWC:\n      h_dim = 1\n      two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n      h_dim = 2\n      two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (1,) + self._rate\n\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     strides=two_dim_conv_stride,\n                                     rate=two_dim_conv_rate,\n                                     padding=self._conv_op_padding,\n                                     data_format=two_dim_conv_data_format)\n    outputs = tf.squeeze(outputs, [h_dim])\n    return outputs",
        "rewrite": "```python\ndef _apply_conv(self, inputs, w):\n    if self._data_format == DATA_FORMAT_NWC:\n        h_dim = 0\n        two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n        h_dim = 1\n        two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (self._rate,) + (1,) * (len"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a MetricResponse object from a json dictionary.\"\"\"\n        args = {}\n        if 'aggregations' in _dict:\n            args['aggregations'] = [\n                MetricAggregation._from_dict(x)\n                for x in (_dict.get('aggregations'))\n            ]\n        return cls(**args)",
        "rewrite": "```python\ndef _from_dict(cls, _dict):\n    args = {}\n    if 'aggregations' in _dict:\n        args['aggregations'] = [MetricAggregation.from_dict(x) for x in (_dict.get('aggregations', []))]\n    return cls(**args)\n```"
    },
    {
        "original": "def add_heading(self, text=\"\", level=1):\n        \"\"\"Return a heading paragraph newly added to the end of the document.\n\n        The heading paragraph will contain *text* and have its paragraph style\n        determined by *level*. If *level* is 0, the style is set to `Title`. If *level*\n        is 1 (or omitted), `Heading 1` is used. Otherwise the style is set to `Heading\n        {level}`. Raises |ValueError| if *level* is outside the range 0-9.\n        \"\"\"\n        if not 0 <= level <= 9:\n            raise ValueError(\"level must be in range 0-9, got %d\" % level)\n        style = \"Title\" if level == 0 else \"Heading %d\" % level\n        return self.add_paragraph(text, style)",
        "rewrite": "```python\ndef add_heading(self, text=\"\", level=1):\n    if not 0 <= level <= 9:\n        raise ValueError(\"level must be in range 0-9, got %d\" % level)\n    style = \"Title\" if level == 0 else f\"Heading {level}\"\n    return self.add_paragraph(text, style)\n```"
    },
    {
        "original": "def _RegisterFlowProcessingHandler(self, handler):\n    \"\"\"Registers a handler to receive flow processing messages.\"\"\"\n    self.flow_handler_stop = False\n    self.flow_handler_thread = threading.Thread(\n        name=\"flow_processing_handler\",\n        target=self._HandleFlowProcessingRequestLoop,\n        args=(handler,))\n    self.flow_handler_thread.daemon = True\n    self.flow_handler_thread.start()",
        "rewrite": "```python\nfrom typing import Callable\n\ndef _RegisterFlowProcessingHandler(self, handler: Callable) -> None:\n    self.flow_handler_stop = False\n    self.flow_handler_thread = threading.Thread(\n        name=\"flow_processing_handler\",\n        target=self._HandleFlowProcessingRequestLoop,\n        args=(handler,))\n    self.flow_handler_thread.daemon = True\n    self.flow_handler_thread.start()\n```\n\n```python\nfrom threading import Thread\n\nclass YourClass:\n    def __init__(self):\n        pass\n\n    def _RegisterFlowProcessingHandler(self, handler: Callable) -> None:\n        self.flow_handler_stop = False\n       "
    }
]
