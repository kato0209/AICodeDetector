[
    {
        "original": "def beginning_offsets(self, partitions):\n        \"\"\"Get the first offset for the given partitions.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The earliest available offsets for the\n            given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms.\n        \"\"\"\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets",
        "rewrite": "Here is the revised code:\n\n```\ndef beginning_offsets(self, partitions):\n    offsets = self._fetcher.beginning_offsets(partitions, self.config['request_timeout_ms'])\n    return {tp: offset for tp, offset in offsets.items() if offset is not None}\n```"
    },
    {
        "original": "def _get_values(cls, diff_dict, type='new'):\n        \"\"\"\n        Returns a dictionaries with the 'new' values in a diff dict.\n\n        type\n            Which values to return, 'new' or 'old'\n        \"\"\"\n        ret_dict = {}\n        for p in diff_dict.keys():\n            if type in diff_dict[p].keys():\n                ret_dict.update({p: diff_dict[p][type]})\n            else:\n                ret_dict.update(\n                    {p: cls._get_values(diff_dict[p], type=type)})\n        return ret_dict",
        "rewrite": "Here is the revised code:\n\n```\ndef _get_values(cls, diff_dict, type='new'):\n    ret_dict = {}\n    for p in diff_dict:\n        if type in diff_dict[p]:\n            ret_dict[p] = diff_dict[p][type]\n        else:\n            ret_dict[p] = cls._get_values(diff_dict[p], type=type)\n    return ret_dict\n```"
    },
    {
        "original": "def get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n        \"\"\"\n        Resolve the field within the given state.\n        \"\"\"\n        # resolve field\n        field_class = state.javavm_classloader.get_class(field_class_name)\n        field_id = resolve_field(state, field_class, field_name, field_type)\n        # return field ref\n        return cls.from_field_id(obj_alloc_id, field_id)",
        "rewrite": "Here is the revised code:\n\n```\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    field_class = state.javavm_classloader.get_class(field_class_name)\n    field_id = resolve_field(state, field_class, field_name, field_type)\n    return cls.from_field_id(obj_alloc_id, field_id)\n```"
    },
    {
        "original": "def pd_coords(self, comp):\n        \"\"\"\n        The phase diagram is generated in a reduced dimensional space\n        (n_elements - 1). This function returns the coordinates in that space.\n        These coordinates are compatible with the stored simplex objects.\n        \"\"\"\n        if set(comp.elements).difference(self.elements):\n            raise ValueError('{} has elements not in the phase diagram {}'\n                             ''.format(comp, self.elements))\n        return np.array(\n            [comp.get_atomic_fraction(el) for el in self.elements[1:]])",
        "rewrite": "Here is the revised code:\n\n```\ndef pd_coords(self, comp):\n    if set(comp.elements).difference(self.elements):\n        raise ValueError('{} has elements not in the phase diagram {}'.format(comp, self.elements))\n    return np.array([comp.get_atomic_fraction(el) for el in self.elements[1:]])\n```"
    },
    {
        "original": "def relative_ref(self, baseURI):\n        \"\"\"\n        Return string containing relative reference to package item from\n        *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n        return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n        \"\"\"\n        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath",
        "rewrite": "Here is the revised code:\n\n```\nimport os\n\ndef relative_ref(self, baseURI):\n    if baseURI == '/':\n        relpath = self[1:]\n    else:\n        relpath = os.path.relpath(self, baseURI)\n    return relpath\n```"
    },
    {
        "original": "def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks is enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with\n        peer is capable of enhanced route refresh capability.\n        \"\"\"\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled",
        "rewrite": "Here is the revised code:\n\n```\ndef is_enhanced_rr_cap_valid(self):\n    if not self.recv_open_msg:\n        raise ValueError('Did not yet receive peers open message.')\n\n    local_caps = self.sent_open_msg.opt_param\n    peer_caps = self.recv_open_msg.opt_param\n\n    local_cap = any(cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH for cap in local_caps)\n    peer_cap = any(cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH for cap in peer_caps)\n\n    return local_cap and peer_cap\n```"
    },
    {
        "original": "def parse_osm_nodes_paths(osm_data):\n    \"\"\"\n    Construct dicts of nodes and paths with key=osmid and value=dict of\n    attributes.\n\n    Parameters\n    ----------\n    osm_data : dict\n        JSON response from from the Overpass API\n\n    Returns\n    -------\n    nodes, paths : tuple\n    \"\"\"\n\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            key = element['id']\n            nodes[key] = get_node(element)\n        elif element['type'] == 'way': #osm calls network paths 'ways'\n            key = element['id']\n            paths[key] = get_path(element)\n\n    return nodes, paths",
        "rewrite": "Here is the revised code:\n\n```\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    ways = {}\n    \n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            nodes[element['id']] = get_node(element)\n        elif element['type'] == 'way':\n            ways[element['id']] = get_path(element)\n\n    return nodes, ways\n```"
    },
    {
        "original": "def get_best_electronegativity_anonymous_mapping(self, struct1, struct2):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar. If multiple substitutions are within tolerance\n        this will return the one which minimizes the difference in\n        electronegativity between the matches species.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            min_mapping (Dict): Mapping of struct1 species to struct2 species\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        use_rms=True, break_on_match=True)\n\n        if matches:\n            min_X_diff = np.inf\n            for m in matches:\n                X_diff = 0\n                for k, v in m[0].items():\n                    X_diff += struct1.composition[k] * (k.X - v.X) ** 2\n                if X_diff < min_X_diff:\n                    min_X_diff = X_diff\n                    best = m[0]\n            return best",
        "rewrite": "Here is the revised code:\n\n```\ndef get_best_electronegativity_anonymous_mapping(self, struct1: Structure) -> Dict[AnySpecies_ELEM_SPECIES.AbstractSpecies : AnySpecies_ELEM_SPECIES.AbstractSpecies], \n               -> Dict[AnySpecies_ELEM_SPECIES.AbstractSpecies : AnySpecies_ELEM_SPECIES.Abstract Species]:\n    mappings_to_evaluate = []\n    mappings_to_evaluate.extend(self._anonymous_match(struct := self._process_species([struct])[0], \n                                                      sstruct := self._process_species([struct])[0], \n                                                      *fu*, \n                                                      *(self.supercells[struct.species]))\n    if not mappings_to_e"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "rewrite": "Here's the revised version of your code with better coding practices:\n\n```\nfrom typing import Tuple\n\nclass YourClassName:\n    def _convert_a_header_to_a_h2_header(self, \n                                         header-name: str, \n                                         header-value: str) -> Tuple[HPAckHeaders,int]:\n        \ndef __init__(self):\n    pass\n        \n            \n avril=self._ Aviv(initialize\toptimize_parameter=True)\n hName Henry_optimized=self._ optimize-header_length_and_packetify(header-name)$objPHPExceli\u015fti seemed  \nba\u0637\u0641\u0627\u0644optimized=self ._optimize -header-length-and-pac k etify(header-value)\n\n\nif(phiz == find-self-get-idx"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "rewrite": "Here is a revised version of the code:\n\n```\ndef create(cls, tx_signers: list[str], recipients: list[tuple], metadata: dict = None, asset: dict = None) -> 'Transaction':\n    inputs_outputs = cls.validate_create(tx_signers, recipients, asset if asset else {}, metadata if metadata else {})\n    return cls(cls.CREATEniciationType.CREATE.value, {'data': asset if asset else {}}, *inputs_outputs[0], inputs_outputs[1], metadata if metadata else {})\n```"
    },
    {
        "original": "def utc_dt_to_local_dt(dtm):\n    \"\"\"Convert a UTC datetime to datetime in local timezone\"\"\"\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo is not None and dtm.tzinfo != utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(mktz())",
        "rewrite": "Here is the revised code:\n\n```\ndef utc_dt_to_local_dt(dtm):\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo and dtm.tzinfo != utc_zone:\n        raise ValueError(\"Expected tz-naive or UTC, not %r\" % dtm.tzinfo)\n    \n    if not dtm.tzinfo:\n        dtm = dtm.replace(tzinfo=utc_zone)\n        \n    return(dt.etLocalzone())\n```"
    },
    {
        "original": "def _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n                log.debug('Have deserialized data %s', _existingData)\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecutionError(error)\n            if 'Section' in policy_info['ScriptIni'] and policy_info['ScriptIni']['Section'].lower() in [z.lower() for z in _existingData.keys()]:\n                if 'SettingName' in policy_info['ScriptIni']:\n                    log.debug('Need to look for %s', policy_info['ScriptIni']['SettingName'])\n                    if policy_info['ScriptIni']['SettingName'].lower() in [z.lower() for z in _existingData[policy_info['ScriptIni']['Section']].keys()]:\n                        return _existingData[policy_info['ScriptIni']['Section']][policy_info['ScriptIni']['SettingName'].lower()]\n                    else:\n                        return None\n                else:\n                    return _existingData[policy_info['ScriptIni']['Section']]\n            else:\n                return None\n\n    return None",
        "rewrite": "Here is the revised code:\n\n```\ndef get_script_settings_from_file(policy_info):\n    if not os.path.isfile(policy-info_dict[\"script_ini\"][\"spath\"]):\n        return None\n    \n    with salt.utils.files.fopen(polic-info_dict[\"script_ini\"][\"spath\"], \"rb\") as file_handle_read_binary:\n        existing_data = file_handle_read_binary.read()\n        \n        try:\n            existing_data = deserialize(existing_data.decode(\"utf-16-le\").lstrip(\"\\ufeff\"))\n            \n            section_name_lower = polic-info_dict[\"script_ini\"].get(\"section\", \"\").lower()\n            \n            for section, settings in existing_data.items():\n"
    },
    {
        "original": "def _get_triplet_scores(self, triangles_list):\n        \"\"\"\n        Returns the score of each of the triplets found in the current model\n\n        Parameters\n        ---------\n        triangles_list: list\n                        The list of variables forming the triangles to be updated. It is of the form of\n                        [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]\n\n        Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'var_5', 'var_4', 'var_7'}): 10.23}\n        \"\"\"\n        triplet_scores = {}\n        for triplet in triangles_list:\n\n            # Find the intersection sets of the current triplet\n            triplet_intersections = [intersect for intersect in it.combinations(triplet, 2)]\n\n            # Independent maximization\n            ind_max = sum([np.amax(self.objective[frozenset(intersect)].values) for intersect in triplet_intersections])\n\n            # Joint maximization\n            joint_max = self.objective[frozenset(triplet_intersections[0])]\n            for intersect in triplet_intersections[1:]:\n                joint_max += self.objective[frozenset(intersect)]\n            joint_max = np.amax(joint_max.values)\n            # score = Independent maximization solution - Joint maximization solution\n            score = ind_max - joint_max\n            triplet_scores[frozenset(triplet)] = score\n\n        return triplet_scores",
        "rewrite": "Here is a revised version of your code:\n\n```Python\nimport numpy as np\nfrom itertools import combinations\n\nclass TripletScorer:\n    def __init__(self):\n        self.objective = {}  # Initialize an empty dictionary to store objective values\n\n    def _get_triplet_scores(self, triangles_list):\n        triplet_scores = {}\n        for triangle in triangles_list:\n            intersections = [frozenset(pair) for pair in combinations(triangle, 2)]\n            \n            ind_max_sum = sum(np.max(self.objective.get(intersection, {}).values) for intersection in intersections)\n            \n            joint_sum_dict = {}\n\u3000\u3000\u3000\u3000\u3000"
    },
    {
        "original": "async def vcx_messages_update_status(msg_json: str):\n    \"\"\"\n    Update the status of messages from the specified connection\n    :param msg_json:\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,\n                           c_msg_json,\n                           vcx_messages_update_status.cb)\n\n    logger.debug(\"vcx_messages_update_status completed\")\n    return result",
        "rewrite": "Here is the revised code:\n\n```\nasync def vcx_messages_update_status(msg_json: str):\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status', c_status, c_msg_json, vcx_messages"
    },
    {
        "original": "def recv_with_timeout(self, timeout=1):\n        \"\"\"Receive a complete ISOTP message, blocking until a message is\n        received or the specified timeout is reached.\n        If timeout is 0, then this function doesn't block and returns the\n        first frame in the receive buffer or None if there isn't any.\"\"\"\n        msg = self.ins.recv(timeout)\n        t = time.time()\n        if msg is None:\n            raise Scapy_Exception(\"Timeout\")\n        return self.basecls, msg, t",
        "rewrite": "Here is the revised code:\n\n```\ndef recv_with_timeout(self, timeout=1):\n    msg = self.ins.recv(timeout) if timeout != 0 else self.ins.recv(0)\n    t = time.time()\n    if msg is None:\n        raise Scapy_Exception(\"Timeout\")\n    return self.basecls, msg, t\n```"
    },
    {
        "original": "def carmichael_of_factorized( f_list ):\n  \"\"\"Return the Carmichael function of a number that is\n  represented as a list of (prime,exponent) pairs.\n  \"\"\"\n\n  if len( f_list ) < 1: return 1\n\n  result = carmichael_of_ppower( f_list[0] )\n  for i in range( 1, len( f_list ) ):\n    result = lcm( result, carmichael_of_ppower( f_list[i] ) )\n\n  return result",
        "rewrite": "Here is the revised code:\n\n```\ndef carmichael_of_factorized(f_list):\n    result = 1\n    for prime, exponent in f_list:\n        result = lcm(result, carmichael_of_ppower((prime, exponent)))\n    return result\n```"
    },
    {
        "original": "def absent(name, profile=\"splunk\"):\n    \"\"\"\n    Ensure a search is absent\n\n    .. code-block:: yaml\n\n        API Error Search:\n          splunk_search.absent\n\n    The following parameters are required:\n\n    name\n        This is the name of the search in splunk\n    \"\"\"\n    ret = {\n        'name': name,\n        'changes': {},\n        'result': True,\n        'comment': '{0} is absent.'.format(name)\n    }\n\n    target = __salt__['splunk_search.get'](name, profile=profile)\n    if target:\n        if __opts__['test']:\n            ret = {}\n            ret[\"name\"] = name\n            ret['comment'] = \"Would delete {0}\".format(name)\n            ret['result'] = None\n            return ret\n\n        result = __salt__['splunk_search.delete'](name, profile=profile)\n        if result:\n            ret['comment'] = '{0} was deleted'.format(name)\n        else:\n            ret['comment'] = 'Failed to delete {0}'.format(name)\n            ret['result'] = False\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef absent(name, profile=\"splunk\"):\n    ret = {\n        \"name\": name,\n        \"changes\": {},\n        \"result\": True,\n        \"comment\": \"{} is absent.\".format(name)\n    }\n    \n    target = __salt__[\"splunk_search.get\"](name, profile=profile)\n    \n    if target:\n        if __opts__[\"test\"]:\n            ret[\"result\"] = None\n            ret[\"comment\"] = \"Would delete {}\".format(name)\n            return ret\n        \n        result = __salt__[\"splunk_search.delete\"](name, profile=profile)\n        \n        if result:\n"
    },
    {
        "original": "def GetNotificationsForAllShards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\n\n    Used by worker_test_lib.MockWorker to cover all shards with a single worker.\n\n    Args:\n      queue: usually rdfvalue.RDFURN(\"aff4:/W\")\n\n    Returns:\n      List of rdf_flows.GrrNotification objects\n    \"\"\"\n    notifications_by_session_id = {}\n    for queue_shard in self.GetAllNotificationShards(queue):\n      self._GetUnsortedNotifications(\n          queue_shard, notifications_by_session_id=notifications_by_session_id)\n\n    return notifications_by_session_id.values()",
        "rewrite": "Here is the revised code:\n\n```\ndef GetNotificationsForAllShards(self, queue):\n    notifications_by_session_id = {}\n    for queue_shard in self.GetAllNotificationShards(queue):\n        self._GetUnsortedNotifications(queue_shard, notifications_by_session_id)\n    return list(notifications_by_session_id.values())\n```"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "rewrite": "def differing_functions_with_consts(self):\n    different_funcs = [(func_a, func_b) for func_a, func_b in self.function_matches if not self.functions_probably_identical(func_a, func_b, check_consts=True)]\n    return different_funcs"
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "rewrite": "Here is the revised code:\n\n```\ndef save(self, eopatch, use_tmp=True):\n    filename = self.tmp_filename if use_tmp else self.final_filename\n\n    if self.feature_name is None:\n        data = eopatch[self.feature_type]\n        if self.feature_type.has_dict():\n            data = data.get_dict()\n        if self.feature_type is FeatureType.BBOX:\n            data = tuple(data) + (int(data.crs.value),)\n    else:\n        data = eopatch[self.feature_type][self.feature_name]\n\n    file_dir = os.path.dirname(filename)\n    os.makedirs(file_dir, exist_ok=True"
    },
    {
        "original": "def _addAttr(self, txn, isCommitted=False) -> None:\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        assert get_type(txn) == ATTRIB\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.attributeStore.set(hashed_value, value)",
        "rewrite": "Here is the revised code:\n\n```python\ndef _addAttr(self, txn: bytes) -> None:\n    assert get_type(txn) == ATTRIB\n    attr_type, path: str, value: bytes, hashed_value: str, value_bytes: bytes = domain.prepare_attr_for_state(txn)\n    self.state.set(path.encode(), value_bytes)\n    if attr_type != HASH:\n        self.attributeStore.set(hashed_value.encode(), value)\n```"
    },
    {
        "original": "def _find_packages(root):\n  \"\"\"\n  Helper for ``build_index()``: Yield a list of tuples\n  ``(pkg_xml, zf, subdir)``, where:\n    - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n    package\n    - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n    - ``subdir`` is the subdirectory (relative to ``root``) where\n    the package was found (e.g. 'corpora' or 'grammars').\n  \"\"\"\n  # Find all packages.\n  packages = []\n  for dirname, subdirs, files in os.walk(root):\n    relpath = '/'.join(_path_from(root, dirname))\n    for filename in files:\n      if filename.endswith('.xml'):\n        xmlfilename = os.path.join(dirname, filename)\n        zipfilename = xmlfilename[:-4]+'.zip'\n        try: zf = zipfile.ZipFile(zipfilename)\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (zipfilename, e))\n        try: pkg_xml = ElementTree.parse(xmlfilename).getroot()\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (xmlfilename, e))\n\n        # Check that the UID matches the filename\n        uid = os.path.split(xmlfilename[:-4])[1]\n        if pkg_xml.get('id') != uid:\n          raise ValueError('package identifier mismatch (%s '\n                   'vs %s)' % (pkg_xml.get('id'), uid))\n\n        # Check that the zipfile expands to a subdir whose\n        # name matches the uid.\n        if sum( (name!=uid and not name.startswith(uid+'/'))\n            for name in zf.namelist() ):\n          raise ValueError('Zipfile %s.zip does not expand to a '\n                   'single subdirectory %s/' % (uid, uid))\n\n        yield pkg_xml, zf, relpath\n    # Don't recurse into svn subdirectories:\n    try: subdirs.remove('.svn')\n    except ValueError: pass",
        "rewrite": "Here is the revised code:\n\n```\ndef _find_packages(root):\n    for dirname, subdirs, files in os.walk(root):\n        relpath = '/'.join(_path_from(root, dirname))\n        for filename in files:\n            if filename.endswith('.xml'):\n                xmlfilename = os.path.join(dirname, filename)\n                zipfilename = xmlfilename[:-4]+'.zip'\n                try:\n                    zf = zipfile.ZipFile(zipfilename)\n                except Exception as e:\n                    raise ValueError(f\"Error reading file {zipfilename}!\\n{e}\")\n                \n                try:\n                    pkg_xml = ElementTree.parse(xmlfilename).getroot"
    },
    {
        "original": "def srbt1(bt_address, pkts, *args, **kargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kargs)\n    if len(a) > 0:\n        return a[0][1]",
        "rewrite": "Here is the revised code:\n\n```\ndef srbt1(bt_address, pkts, *args, **kargs):\n    result = srbt(bt_address, pkts, *_args,** kargs)\n    return result[0][1] if result else None\n```"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"IP addresses from all interfaces.\"\"\"\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n      for address in interface.addresses:\n        if address.human_readable_address not in filtered_ips:\n          result.append(Text(address.human_readable_address))\n    return sorted(result)",
        "rewrite": "```\ndef get_ip_addresses(self):\n    filtered_ips = {\"127.0.0.1\", \"::1\", \"fe80::1\"}\n    return sorted([str(address.human_readable_address) for interface in self.interfaces for address in interface.addresses if address.human_readable_address not in filtered_ips])\n```"
    },
    {
        "original": "def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        \"\"\"\n        Checks if two points are symmetrically related.\n\n        Args:\n            point_a (3x1 array): First point.\n            point_b (3x1 array): Second point.\n            tol (float): Absolute tolerance for checking distance.\n\n        Returns:\n            True if self.operate(point_a) == point_b or vice versa.\n        \"\"\"\n        if np.allclose(self.operate(point_a), point_b, atol=tol):\n            return True\n        if np.allclose(self.operate(point_b), point_a, atol=tol):\n            return True\n        return False",
        "rewrite": "Here is the revised code:\n\n```\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    return np.allclose(self.operate(point_a), point_b, atol=tol) or np.allclose(self.operate(point_b), point_a, atol=tol)\n```"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "rewrite": "```\ndef validate_rpc_host(ip):\n    if not (is_valid_ipv4(ip) or is_valid_ipv6(ip)):\n        raise ApplicationException(f'Invalid RPC ip address: {ip}')\n    return ip\n```"
    },
    {
        "original": "def find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        # it might be good to add \"type vcan\", but that might (?) exclude physical can devices\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True)\n\n    except Exception as e: # subprocess.CalledProcessError was too specific\n        log.error(\"failed to fetch opened can devices: %s\", e)\n        return []\n\n    else:\n        #log.debug(\"find_available_interfaces(): output=\\n%s\", output)\n        # output contains some lines like \"1: vcan42: <NOARP,UP,LOWER_UP> ...\"\n        # extract the \"vcan42\" of each line\n        interface_names = [line.split(\": \", 3)[1] for line in output.splitlines()]\n        log.debug(\"find_available_interfaces(): detected: %s\", interface_names)\n        return filter(_PATTERN_CAN_INTERFACE.match, interface_names)",
        "rewrite": "Here is the revised code:\n\n```\nimport subprocess\nimport logging as log\n\n_PATTERN_CAN_INTERFACE = re.compile(r'(?=vcan[0-9]+)')\n\ndef find_available_interfaces():\n    try:\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True).decode('utf-8')\n        \n    except Exception as e:\n        log.error(\"Failed to fetch opened CAN devices: %s\" % e)\n        \n    else:\n            interface_names = [line.split(\": \", 2)[1] for line in output.splitlines"
    },
    {
        "original": "def save_session(self, sid, session, namespace=None):\n        \"\"\"Store the user session for a client.\n\n        The only difference with the :func:`socketio.Server.save_session`\n        method is that when the ``namespace`` argument is not given the\n        namespace associated with the class is used.\n        \"\"\"\n        return self.server.save_session(\n            sid, session, namespace=namespace or self.namespace)",
        "rewrite": "Here's a revised version of the code:\n\n```\ndef save_session(self, sid, session, namespace=None):\n    return self.server.save_session(sid, session, namespace=namespace or self.namespace)\n```"
    },
    {
        "original": "def get_public_keys_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 public keys from the v3 signature block\n        \"\"\"\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        public_keys = []\n\n        for signer in self._v3_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys",
        "rewrite": "Here is the revised code:\n\n```\ndef get_public_keys_der_v3(self):\n    if not self._v3_signing_data:\n        self.parse_v3_signing_block()\n    return [signer.public_key for signer in self._v3_signing_data]\n```"
    },
    {
        "original": "def chemical_symbols(self):\n        \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n        charr = self.read_value(\"chemical_symbols\")\n        symbols = []\n        for v in charr:\n            s = \"\".join(c.decode(\"utf-8\") for c in v)\n            symbols.append(s.strip())\n\n        return symbols",
        "rewrite": "```\ndef chemical_symbols(self):\n    charr = self.read_value(\"chemical_symbols\")\n    return [\"\".join(c.decode(\"utf-8\") for c in v).strip() for v in charr]\n```"
    },
    {
        "original": "def safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)",
        "rewrite": "```\ndef safe_dump_all(documents, stream=None, **kwds):\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)\n```"
    },
    {
        "original": "def _simulator_iterator(self, circuit: circuits.Circuit,\n                            param_resolver: study.ParamResolver,\n                            qubit_order: ops.QubitOrderOrList,\n                            initial_state: Union[int, np.ndarray]) -> Iterator:\n        \"\"\"See definition in `cirq.SimulatesIntermediateState`.\n\n        If the initial state is an int, the state is set to the computational\n        basis state corresponding to this state. Otherwise  if the initial\n        state is a np.ndarray it is the full initial state, either a pure state\n        or the full density matrix.  If it is the pure state it must be the\n        correct size, be normalized (an L2 norm of 1), and be safely castable\n        to an appropriate dtype for the simulator.  If it is a mixed state\n        it must be correctly sized and positive semidefinite with trace one.\n        \"\"\"\n        param_resolver = param_resolver or study.ParamResolver({})\n        resolved_circuit = protocols.resolve_parameters(circuit, param_resolver)\n        actual_initial_state = 0 if initial_state is None else initial_state\n        return self._base_iterator(resolved_circuit,\n                                   qubit_order,\n                                   actual_initial_state)",
        "rewrite": "Here's a revised version of your function:\n\n```\ndef _simulator_iterator(self, \n                        circuit: 'cirq.Circuit', \n                        param_resolver: 'study.ParamResolver' = None, \n                        qubit_order: 'ops.QubitOrderOrList', \n                        initial_state: Union[int, np.ndarray] = None) -> Iterator:\n    if not isinstance(initial_state, (type(None), int, np.ndarray)):\n            raise ValueError('initial_state must be None or an integer or numpy array')\n    if not isinstance(param_resolver, (type(None), study.ParamResolver)):\n            raise ValueError('param_resolver must be None"
    },
    {
        "original": "def predictive_variance(self, mu,variance, predictive_mean=None, Y_metadata=None):\n        \"\"\"\n        Approximation to the predictive variance: V(Y_star)\n\n        The following variance decomposition is used:\n        V(Y_star) = E( V(Y_star|f_star)**2 ) + V( E(Y_star|f_star) )**2\n\n        :param mu: mean of posterior\n        :param sigma: standard deviation of posterior\n        :predictive_mean: output's predictive mean, if None _predictive_mean function will be called.\n\n        \"\"\"\n        #sigma2 = sigma**2\n        normalizer = np.sqrt(2*np.pi*variance)\n\n        fmin_v = -np.inf\n        fmin_m = np.inf\n        fmin = -np.inf\n        fmax = np.inf\n\n        from ..util.misc import safe_exp\n        # E( V(Y_star|f_star) )\n        def int_var(f,m,v):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = safe_exp(exponent)\n            #If p is zero then conditional_variance will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_variance(f)*p\n        scaled_exp_variance = [quad(int_var, fmin_v, fmax,args=(mj,s2j))[0] for mj,s2j in zip(mu,variance)]\n        exp_var = np.array(scaled_exp_variance)[:,None] / normalizer\n\n        #V( E(Y_star|f_star) ) =  E( E(Y_star|f_star)**2 ) - E( E(Y_star|f_star) )**2\n\n        #E( E(Y_star|f_star) )**2\n        if predictive_mean is None:\n            predictive_mean = self.predictive_mean(mu,variance)\n        predictive_mean_sq = predictive_mean**2\n\n        #E( E(Y_star|f_star)**2 )\n        def int_pred_mean_sq(f,m,v,predictive_mean_sq):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = np.exp(exponent)\n            #If p is zero then conditional_mean**2 will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_mean(f)**2*p\n\n        scaled_exp_exp2 = [quad(int_pred_mean_sq, fmin_m, fmax,args=(mj,s2j,pm2j))[0] for mj,s2j,pm2j in zip(mu,variance,predictive_mean_sq)]\n        exp_exp2 = np.array(scaled_exp_exp2)[:,None] / normalizer\n\n        var_exp = exp_exp2 - predictive_mean_sq\n\n        # V(Y_star) = E[ V(Y_star|f_star) ] + V[ E(Y_star|f_star) ]\n        # V(Y_star) = E[ V(Y_star|f_star) ] + E(Y_star**2|f_star) - E[Y_star|f_star]**2\n        return exp_var + var_exp",
        "rewrite": "Here is the revised code:\n\n```\ndef predictive_variance(self, mu, variance, predictive mean=None):\n    normalizer=np.sqrt(3*np.pi*variance)\n\n    def int_var(f,m,v):\n       exponent=-(0.5/v)*np.square(f-m)\n       p=np.exp(exponent)\n       return self.conditional_variance(f)*p if p > 1e-10 else 0.\n\n    scaled-exp-var=[quad(int-var,-np.inf,np.inf,args=(mj,sjj))[O] for mj.sjj in zip(mu.variance)]\n    exp-var=np.array(scaled-exp-var)[: none]/normal"
    },
    {
        "original": "def remove_config(self, id):\n        \"\"\"\n            Remove a config\n\n            Args:\n                id (string): Full ID of the config to remove\n\n            Returns (boolean): True if successful\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no config with that ID exists\n        \"\"\"\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True",
        "rewrite": "Here is the revised code:\n\n```\ndef remove_config(self, id: str) -> bool:\n    url = f\"{self._url()}/configs/{id}\"\n    res = self._delete(url)\n    self._raise_for_status(res)\n    return True\n```"
    },
    {
        "original": "def get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    for item in container.view:\n        if item._moId == obj_moid:\n            return item\n    return None",
        "rewrite": "Here is the revised code:\n\n```\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    inventory = si.RetrieveContent().rootFolder.getChildEntity()\n    container = si.RetrieveContent().viewManager.CreateContainerView(inventory, [obj_type], True)\n    for view in container.view:\n        if view._moId == str(obj_moid):\n            return view\n```"
    },
    {
        "original": "def ConfigureUrls(config, external_hostname = None):\n  \"\"\"Guides the user through configuration of various URLs used by GRR.\"\"\"\n  print(\"\\n\\n-=GRR URLs=-\\n\"\n        \"For GRR to work each client has to be able to communicate with the\\n\"\n        \"server. To do this we normally need a public dns name or IP address\\n\"\n        \"to communicate with. In the standard configuration this will be used\\n\"\n        \"to host both the client facing server and the admin user interface.\\n\")\n\n  existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\", default=None)\n  existing_frontend_urns = grr_config.CONFIG.Get(\"Client.server_urls\")\n  if not existing_frontend_urns:\n    # Port from older deprecated setting Client.control_urls.\n    existing_control_urns = grr_config.CONFIG.Get(\n        \"Client.control_urls\", default=None)\n    if existing_control_urns is not None:\n      existing_frontend_urns = []\n      for existing_control_urn in existing_control_urns:\n        if not existing_control_urn.endswith(\"control\"):\n          raise RuntimeError(\"Invalid existing control URL: %s\" %\n                             existing_control_urn)\n\n        existing_frontend_urns.append(\n            existing_control_urn.rsplit(\"/\", 1)[0] + \"/\")\n\n      config.Set(\"Client.server_urls\", existing_frontend_urns)\n      config.Set(\"Client.control_urls\", [\"deprecated use Client.server_urls\"])\n\n  if not existing_frontend_urns or not existing_ui_urn:\n    ConfigureHostnames(config, external_hostname=external_hostname)\n  else:\n    print(\"Found existing settings:\\n  AdminUI URL: %s\\n  \"\n          \"Frontend URL(s): %s\\n\" % (existing_ui_urn, existing_frontend_urns))\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureHostnames(config, external_hostname=external_hostname)",
        "rewrite": "Here is the revised code:\n\n```\ndef ConfigureUrls(config, external_hostname=None):\n    print(\"\\n\\n-=GRR URLs=-\\n\"\n          \"For GRR to work each client has to be able to communicate with the\\n\"\n          \"server. To do this we normally need a public dns name or IP address\\n\"\n          \"to communicate with. In the standard configuration this will be used\\n\"\n          \"to host both the client facing server and the admin user interface.\\n\")\n\n    existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\")\n    existing_frontend_urns = grr_config.CONFIG"
    },
    {
        "original": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n            knowledge_base):\n    \"\"\"Parse the sysctl output.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused.\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    # The KeyValueParser generates an ordered dict by default. The sysctl vals\n    # aren't ordering dependent, but there's no need to un-order it.\n    for k, v in iteritems(self.lexer.ParseToOrderedDict(stdout)):\n      key = k.replace(\".\", \"_\")\n      if len(v) == 1:\n        v = v[0]\n      result[key] = v\n    return [result]",
        "rewrite": "Here is the revised code:\n\n```\ndef Parse(self, cmd, args, stdout, stderr, return_val, time_taken, knowledge_base):\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    for k, v in self.lexer.ParseToOrderedDict(stdout).items():\n        key = k.replace(\".\", \"_\")\n        result[key] = v[0] if len(v) == 1 else v\n    return [result]\n```"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "rewrite": "Here is the revised code:\n\n```\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as fid:\n        self.read_skel(fid)\n    self.name = file_name\n```"
    },
    {
        "original": "def _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \"\"\"\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists'\n            ret['default'] = default == ruby\n            break\n\n    return ret",
        "rewrite": "Here is the revised code:\n\n```\ndef _ruby_installed(ret, ruby, user=None):\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret.update({'result': True, 'comment': 'Requested ruby exists', 'default': default == ruby})\n            return ret\n    ret.update({'result': False, 'comment': f'Ruby {ruby} is not installed'})\n    return ret\n```"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "rewrite": "```\ndef get_projection_on_elements(self, structure):\n    dico = {spin: [[[defaultdict(float) for _ in range(self.nkpoints)] \n                   for _ in range(self.nbands)] \n                  for spin in self.data]\n    for iat, species_name in enumerate.Structure.species):\n        name = species_name.symbol\n        for spin, data_spin in self.data.items():\n            for k, b  itemrange(product(range(self.nkpoints), range(self.nbands))):\n                proj_sum = np.sum(data_spin[k, b, iat])\n                dico[spin][k][b][name]"
    },
    {
        "original": "def vector_args(self, args):\n        \"\"\"\n         Yields each of the individual lane pairs from the arguments, in\n         order from most significan to least significant\n        \"\"\"\n        for i in reversed(range(self._vector_count)):\n            pieces = []\n            for vec in args:\n                pieces.append(vec[(i+1) * self._vector_size - 1 : i * self._vector_size])\n            yield pieces",
        "rewrite": "Here's a revised version of the function:\n\n```\ndef vector_args(self, args):\n    for i in range(self._vector_count-1, -1, -1):\n        yield [vec[i * self._vector_size:(i+1) * self._vector_size] for vec in args]\n```"
    },
    {
        "original": "def kwargs_from_keyword(from_kwargs,to_kwargs,keyword,clean_origin=True):\n\t\"\"\"\n\tLooks for keys of the format keyword_value. \n\tAnd return a dictionary with {keyword:value} format\n\n\tParameters:\n\t-----------\n\t\tfrom_kwargs : dict\n\t\t\tOriginal dictionary\n\t\tto_kwargs : dict\n\t\t\tDictionary where the items will be appended\n\t\tkeyword : string\n\t\t\tKeyword to look for in the orginal dictionary\n\t\tclean_origin : bool\n\t\t\tIf True then the k,v pairs from the original \n\t\t\tdictionary are deleted\n\t\"\"\"\n\tfor k in list(from_kwargs.keys()):\n\t\tif '{0}_'.format(keyword) in k:\n\t\t\tto_kwargs[k.replace('{0}_'.format(keyword),'')]=from_kwargs[k]\n\t\t\tif clean_origin:\n\t\t\t\tdel from_kwargs[k]\n\treturn to_kwargs",
        "rewrite": "Here is the revised code:\n\n```\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key in list(from_kwargs.keys()):\n        if '{}_'.format(keyword) in key:\n            new_key = key.replace('{}_' .format(keyword), '')\n            to_kwargs[new_key] = from_kwargs[key]\n            if clean_origin:\n                del from.kwargs[key]\n    return to(kwargs)\n```"
    },
    {
        "original": "def _RunAction(self, rule, client_id):\n    \"\"\"Run all the actions specified in the rule.\n\n    Args:\n      rule: Rule which actions are to be executed.\n      client_id: Id of a client where rule's actions are to be executed.\n\n    Returns:\n      Number of actions started.\n    \"\"\"\n    actions_count = 0\n\n    try:\n      if self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n        logging.info(\n            \"Foreman: ignoring hunt %s on client %s: was started \"\n            \"here before\", client_id, rule.hunt_id)\n      else:\n        logging.info(\"Foreman: Starting hunt %s on client %s.\", rule.hunt_id,\n                     client_id)\n\n        # hunt_name is only used for legacy hunts.\n        if rule.hunt_name:\n          flow_cls = registry.AFF4FlowRegistry.FlowClassByName(rule.hunt_name)\n          hunt_urn = rdfvalue.RDFURN(\"aff4:/hunts/%s\" % rule.hunt_id)\n          flow_cls.StartClients(hunt_urn, [client_id])\n        else:\n          hunt.StartHuntFlowOnClient(client_id, rule.hunt_id)\n\n        actions_count += 1\n\n    # There could be all kinds of errors we don't know about when starting the\n    # hunt so we catch everything here.\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(\"Failure running foreman action on client %s: %s\",\n                        rule.hunt_id, e)\n\n    return actions_count",
        "rewrite": "Here is a revised version of the code:\n\n```Python\ndef _RunAction(self, rule, client_id):\n    try:\n        if not self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n            logging.info(f\"Foreman: Starting hunt {rule.hunt_id} on client {client.id}.\")\n            if rule=hgain\":\n                flow_cls = registry.AFF4FlowRegistry.FlowClassByName(rule.flow_name)\n                flow_cls.StartClients(f\"f rdf{rule_h aanURN(urn=f\"hunts/{rulehuntflowclsId}\"), [clientid])\n            elsee):\n                hun"
    },
    {
        "original": "def df(self):\n        \"\"\"\n        Get data usage information.\n\n        Returns:\n            (dict): A dictionary representing different resource categories\n            and their respective data usage.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/system/df')\n        return self._result(self._get(url), True)",
        "rewrite": "Here is the revised code:\n\n```\ndef df(self):\n    url = f\"{self._url}/system/df\"\n    return self._result(self._get(url), True)\n```"
    },
    {
        "original": "def file_extension(self, category=None):\n        \"\"\"\n        :param category: audio|image|office|text|video\n        \"\"\"\n        category = category if category else self.random_element(\n            list(self.file_extensions.keys()))\n        return self.random_element(self.file_extensions[category])",
        "rewrite": "```\ndef file_extension(self, category=None):\n    if not category:\n        category = self.random_element(list(self.file_extensions.keys()))\n    return self.random_element(self.file_extensions[category])\n```"
    },
    {
        "original": "def _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    \"\"\"\n    GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.\n\n    See gti_dirint signature for parameter details.\n    \"\"\"\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370, 'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)\n    disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)\n\n    dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,\n                                          False, temp_dew)\n\n    dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)\n    cos_surface_tilt = tools.cosd(surface_tilt)\n\n    # isotropic sky plus ground diffuse\n    dhi_gte_90 = (\n        (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /\n        (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))\n\n    ghi_gte_90 = dni_gte_90_proj + dhi_gte_90\n\n    return ghi_gte_90, dni_gte_90, dhi_gte_90",
        "rewrite": "Here is the revised code:\n\n```\ndef _gti_dirint_gte_90(poa_global: float or np.ndarray,\n                        aoi: float or np.ndarray,\n                        solar_zenth: float or np.ndarray,\n                        solar_azimuth: float or np.ndarray,\n                        surface_tilt: float or np.ndarray,\n                        times: pd.DatetimeIndex or str or list of str,\n                        kt_prime: float or int) -> tuple:\n    \n        pressure=101325., \n        temp_dew=None\n        albedo=.25):\n\n    \n     kt_prime_GTE090=_gti_dirint_GTE090_kt"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "rewrite": "Here is the revised code:\n\n```\ndef ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    _, children_urns = self.MultiListChildren([urn], limit=limit, age=age)[0]\n    return [RDFURN(u) for u in children_urns]\n```"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "rewrite": "Here is the revised code:\n\n```\nimport json\nimport collections\n\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n    \n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n        \n        key = (str(check), validator[\"comparator\"])\n        \n        if not isinstance(key[0], str):\n            raise ValueError(\"The 'key[0]' must be a string\")\n        \n        if len(key[0]) > 255:\n            raise ValueError(\"The length of '"
    },
    {
        "original": "def InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \"\"\"Take a string as a path on a client and interpolate with client data.\n\n  Args:\n    path: A single string/unicode to be interpolated.\n    knowledge_base: An rdf_client.KnowledgeBase object.\n    users: A list of string usernames, or None.\n    path_args: A dict of additional args to use in interpolation. These take\n      precedence over any system provided variables.\n    depth: A counter for recursion depth.\n\n  Returns:\n    A single string if users is None, otherwise a list of strings.\n  \"\"\"\n\n  sys_formatters = {\n      # TODO(user): Collect this during discovery from the registry.\n      # HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\\n      # Value: SystemRoot\n      \"systemroot\": \"c:\\\\Windows\"\n  }\n\n  # Override any system formatters with path_args.\n  if path_args:\n    sys_formatters.update(path_args)\n\n  if users:\n    results = []\n    for user in users:\n      # Extract and interpolate user specific formatters.\n      user = GetUserInfo(knowledge_base, user)\n      if user:\n        formatters = dict((x.name, y) for x, y in user.ListSetFields())\n        formatters.update(sys_formatters)\n        try:\n          results.append(path.format(**formatters))\n        except KeyError:\n          pass  # We may be missing values for some users.\n    return results\n  else:\n    try:\n      path = path.format(**sys_formatters)\n    except KeyError:\n      logging.warning(\"Failed path interpolation on %s\", path)\n      return \"\"\n    if \"{\" in path and depth < 10:\n      path = InterpolatePath(\n          path,\n          knowledge_base=knowledge_base,\n          users=users,\n          path_args=path_args,\n          depth=depth + 1)\n    return path",
        "rewrite": "Here is the revised code:\n\n```\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \n  sys_formaters = {\"systemroot\": \"c:\\\\Windows\"}\n  \n  if path_args: \n     sys_formaters.update(path_args)\n      \n  \nif users:. \n   resultss = []    \n   for user in userrss:.   \n       uaser_infooer == GetUserInfo(knowledge_basee , userr) \n       iff uaser_infooer :   \n         formatteresss =(dict(((xx.namee,y )for xx,y inn uaser_infooer.List"
    },
    {
        "original": "def _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \"\"\"Fast Fourier transform-based Gaussian kernel density estimate (KDE).\n\n    The code was adapted from https://github.com/mfouesneau/faststats\n\n    Parameters\n    ----------\n    x : Numpy array or list\n    cumulative : bool\n        If true, estimate the cdf instead of the pdf\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    xmin : float\n        Manually set lower limit.\n    xmax : float\n        Manually set upper limit.\n\n    Returns\n    -------\n    density: A gridded 1D KDE of the input points (x)\n    xmin: minimum value of x\n    xmax: maximum value of x\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax",
        "rewrite": "Here is a revised version of your function:\n\n```\ndef _fast_kde(x,cumulative=False,bw=4.5,xmin=None,xmax=None):\n\tx=np.asarray(x,dtype=float);x=x[np.isfinite(x)]\n\tif not x.size:return [np.nan],np.nan,np.nan\n\t\n\tlen_x=len(x);n_points=200 if xor_(xmin,xmax)is None else500\n\t\n\txmin=xmin or(np.min(x));xmax=xmax or(np.max(x))\n\tasserteminlteoreticallyeq_(x).all(),assertxmax>= maximumvaluesinarray_(x).all()\n\t\n\tlog_len_ln"
    },
    {
        "original": "def md_options_to_metadata(options):\n    \"\"\"Parse markdown options and return language and metadata\"\"\"\n    metadata = parse_md_code_options(options)\n\n    if metadata:\n        language = metadata[0][0]\n        for lang in _JUPYTER_LANGUAGES + ['julia', 'scheme', 'c++']:\n            if language.lower() == lang.lower():\n                return lang, dict(metadata[1:])\n\n    return None, dict(metadata)",
        "rewrite": "Here is the revised code:\n\n```\ndef md_options_to_metadata(options):\n    metadata = parse_md_code_options(options)\n    if metadata:\n        language = metadata[0][0].lower()\n        for lang in _JUPYTER_LANGUAGES + ['julia', 'scheme', 'c++']:\n            if language == lang.lower():\n                return lang, dict(metadata[1:])\n    return None, dict(metadata)\n```"
    },
    {
        "original": "def frames(\n        self,\n        *,\n        callers: Optional[Union[str, List[str]]] = None,\n        callees: Optional[Union[str, List[str]]] = None,\n        kind: Optional[TraceKind] = None,\n        limit: Optional[int] = 10,\n    ):\n        \"\"\"Display trace frames independent of the current issue.\n\n        Parameters (all optional):\n            callers: str or list[str]            filter traces by this caller name\n            callees: str or list[str]            filter traces by this callee name\n            kind: precondition|postcondition    the type of trace frames to show\n            limit: int (default: 10)            how many trace frames to display\n                                                (specify limit=None for all)\n\n        Sample usage:\n            frames callers=\"module.function\", kind=postcondition\n\n        String filters support LIKE wildcards (%, _) from SQL:\n            % matches anything (like .* in regex)\n            _ matches 1 character (like . in regex)\n        \"\"\"\n        with self.db.make_session() as session:\n            query = (\n                session.query(\n                    TraceFrame.id,\n                    CallerText.contents.label(\"caller\"),\n                    TraceFrame.caller_port,\n                    CalleeText.contents.label(\"callee\"),\n                    TraceFrame.callee_port,\n                )\n                .filter(TraceFrame.run_id == self.current_run_id)\n                .join(CallerText, CallerText.id == TraceFrame.caller_id)\n                .join(CalleeText, CalleeText.id == TraceFrame.callee_id)\n            )\n\n            if callers is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callers, query, CallerText.contents, \"callers\"\n                )\n\n            if callees is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callees, query, CalleeText.contents, \"callees\"\n                )\n\n            if kind is not None:\n                if kind not in {TraceKind.PRECONDITION, TraceKind.POSTCONDITION}:\n                    raise UserError(\n                        \"Try 'frames kind=postcondition'\"\n                        \" or 'frames kind=precondition'.\"\n                    )\n                query = query.filter(TraceFrame.kind == kind)\n\n            if limit is not None and not isinstance(limit, int):\n                raise UserError(\"'limit' should be an int or None.\")\n\n            trace_frames = query.group_by(TraceFrame.id).order_by(\n                CallerText.contents, CalleeText.contents\n            )\n\n            total_trace_frames = trace_frames.count()\n            limit = limit or total_trace_frames\n\n            self._output_trace_frames(\n                self._group_trace_frames(trace_frames, limit), limit, total_trace_frames\n            )",
        "rewrite": "Here is the revised code:\n\n```\ndef frames(\n    self,\n    *,\n    callers: Optional[Union[str, List[str]]] = None,\n    callees: Optional[Union[str, List[str]]] = None,\n    kind: Optional[TraceKind] = None,\n    limit: Optional[int] = 10,\n):\n    with self.db.make_session() as session:\n        query = session.query(\n            TraceFrame.id,\n            CallerText.contents.label(\"caller\"),\n            TraceFrame.caller_port,\n            CalleeText.contents.label(\"callee\"),\n            TraceFrame.callee_port,\n        ).filter(TraceFrame.run"
    },
    {
        "original": "def new_batch(self):\n        \"\"\"\n        Return a new batch of X and Y by taking a chunk of data from the complete X and Y\n        \"\"\"\n        i = next(self.slicer)\n        return self.X_all[i], self.Y_all[i]",
        "rewrite": "Here is the revised code:\n\n```\ndef new_batch(self):\n    i = next(self.slicer)\n    return self.X_all[i:i+self.batch_size], self.Y_all[i:i+self.batch_size]\n```"
    },
    {
        "original": "def createSummary(self, log):\n        \"\"\"\n        Create nice summary logs.\n\n        @param log: log to create summary off of.\n        \"\"\"\n        warnings = self.obs.warnings\n        errors = self.obs.errors\n\n        if warnings:\n            self.addCompleteLog('%d Warnings' % len(warnings), \"\\n\".join(warnings))\n            self.warnCount = len(warnings)\n        if errors:\n            self.addCompleteLog('%d Errors' % len(errors), \"\\n\".join(errors))\n            self.errCount = len(errors)",
        "rewrite": "```\ndef createSummary(self, log):\n    warnings, errors = self.obs.warnings, self.obs.errors\n    if warnings:\n        warning_msg = '%d Warnings\\n%s' % (len(warnings), '\\n'.join(warnings))\n        self.addCompleteLog(warning_msg)\n        self.warnCount = len(warnings)\n    if errors:\n        error_msg = '%d Errors\\n%s' % (len(errors), '\\n'.join(errors))\n        self.addCompleteLog(error_msg)\n        self.errCount = len(errors)"
    },
    {
        "original": "def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        \"\"\"Creates a branch using the information provided in the request.\n\n        The authenticated user must have REPO_WRITE permission for the context repository to call this resource.\n\n        :param project_key: The project matching the projectKey supplied in the resource path as shown in URL.\n        :type project_key: str\n        :param repository: Name of repository where branch is created (i.e. \"my_repo\").\n        :type repository: str\n        :param name: Name of branch to create (i.e. \"my_branch\").\n        :type name: str\n        :param start_point: Name of branch to branch from.\n        :type start_point: str\n        :param message: Branch message.\n        :type message: str\n        :return:\n            200 - application/json (repository)\n            401 - application/json (errors)\n            404 - application/json (errors)\n        :rtype: requests.Response\n        \"\"\"\n\n        url = 'rest/api/1.0/projects/{projectKey}/repos/{repository}/branches'.format(projectKey=project_key,\n                                                                                      repository=repository)\n        data = {\n            \"name\": name,\n            \"startPoint\": start_point,\n            \"message\": message\n        }\n        return self.post(url, data=data)",
        "rewrite": "```\ndef create_branch(self, project_key, repository, name, start_point, message=\"\"):\n    url = f\"rest/api/1.0/projects/{project_key}/repos/{repository}/branches\"\n    data = {\"name\": name, \"startPoint\": start_point}\n    if message:\n       data[\"message\"] = message\n    return self.post(url, json=data)\n```"
    },
    {
        "original": "def merge_option_dicts(old_opts, new_opts):\n    \"\"\"\n    Update the old_opts option dictionary with the options defined in\n    new_opts. Instead of a shallow update as would be performed by calling\n    old_opts.update(new_opts), this updates the dictionaries of all option\n    types separately.\n\n    Given two dictionaries\n        old_opts = {'a': {'x': 'old', 'y': 'old'}}\n    and\n        new_opts = {'a': {'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    this returns a dictionary\n        {'a': {'x': 'old', 'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    \"\"\"\n    merged = dict(old_opts)\n\n    for option_type, options in new_opts.items():\n        if option_type not in merged:\n            merged[option_type] = {}\n\n        merged[option_type].update(options)\n\n    return merged",
        "rewrite": "```\ndef merge_option_dicts(old_OPTS, new_OPTS):\n   merged = {** for k,v in old_OPTS.items()}\n   for option_type, options in new_OPTS.items():\n       if option_type not in merged:\n           merged[option_type] = {}\n       for k,v in options.items():\n           merged[option_type][k] = v\n   return merged"
    },
    {
        "original": "def get(self, file_path, ref, **kwargs):\n        \"\"\"Retrieve a single file.\n\n        Args:\n            file_path (str): Path of the file to retrieve\n            ref (str): Name of the branch, tag or commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            object: The generated RESTObject\n        \"\"\"\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)",
        "rewrite": "```\ndef get(self, file_path: str, ref: str, **: dict) -> object:\n    try:\n        sanitized_file_path = urllib.parse.quote(file_path)\n        return GetMixin.get(self, sanitized_file_path, ref=ref)\n    except exceptions.GitlabAuthenticationError as e:\n        raise exceptions.GitlabAuthenticationError(e)\n    except exceptions.GitlabGetError as e:\n        raise exceptions.GitlabGetError(e)\n```"
    },
    {
        "original": "def clean_recipe_build(self, args):\n        \"\"\"Deletes the build files of the given recipe.\n\n        This is intended for debug purposes. You may experience\n        strange behaviour or problems with some recipes if their\n        build has made unexpected state changes. If this happens, run\n        clean_builds, or attempt to clean other recipes until things\n        work again.\n        \"\"\"\n        recipe = Recipe.get_recipe(args.recipe, self.ctx)\n        info('Cleaning build for {} recipe.'.format(recipe.name))\n        recipe.clean_build()\n        if not args.no_clean_dists:\n            self.clean_dists(args)",
        "rewrite": "Here is the revised code:\n\n```\ndef clean_recipe_build(self, args):\n    recipe = Recipe.get_recipe(args.recipe, self.ctx)\n    info(f'Cleaning build for {recipe.name} recipe.')\n    recipe.clean_build()\n    if not args.no_clean_dists:\n        self.clean_dists(args)\n```"
    },
    {
        "original": "def calculate_bias_shape(input_shape, bias_dims):\n  \"\"\"Calculate `bias_shape` based on the `input_shape` and `bias_dims`.\n\n  Args:\n    input_shape: Shape of the input being passed into the module. The leading\n        dimension is the minibatch size.\n    bias_dims: The dimensions that bias should be applied over. The remaining\n        dimensions will get broadcasted over.\n\n  Returns:\n    bias_shape: Tuple corresponding to the shape of bias Variable to create.\n\n  Raises:\n    ValueError: If the user attempts to add bias over the minibatch dimension,\n        e.g. `bias_dims=[0]`.\n  \"\"\"\n  input_rank = len(input_shape)\n  # If None, default is to use all dimensions.\n  if bias_dims is None:\n    return input_shape[1:]\n  # If empty list, use a scalar bias.\n  elif not bias_dims:\n    return ()\n  # Otherwise, calculate bias_shape from bias_dims.\n  else:\n    bias_shape = [1] * input_rank\n    # Populate bias dimensions.\n    for dim in bias_dims:\n      dim %= input_rank\n      if dim == 0:\n        raise ValueError(\"Cannot apply bias across the minibatch dimension.\")\n      bias_shape[dim] = input_shape[dim]\n    # Strip leading unit dimensions.\n    start = input_rank\n    for dim in xrange(1, input_rank):\n      if bias_shape[dim] != 1:\n        start = dim\n        break\n    return tuple(bias_shape[start:])",
        "rewrite": "Here's a revised version of your function:\n\n```\ndef calculate_bias Shape(input shape, bia dims=None):\n   if bia dims is None :\n     return ingput shape[1:]\n   elif not bia dims :\n     return ()\n   else :\n     bia shape =[1]*len(input shape)\n     for i,dim in enumerate(bia dims) :\n       dim %= len(inputshape)\n       if dim ==0 : \n         raise ValueError(\"Cannot apply bia across th minibatch dimension.\")\n       bia shape[dim]=inputshape[dim]\n     start=len (inputshape )\n     for i in range (1 , len"
    },
    {
        "original": "def read_metadata(text, ext):\n    \"\"\"Return the header metadata\"\"\"\n    ext = '.' + ext.split('.')[-1]\n    lines = text.splitlines()\n\n    if ext in ['.md', '.Rmd']:\n        comment = ''\n    else:\n        comment = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment', '#')\n\n    metadata, _, _, _ = header_to_metadata_and_cell(lines, comment, ext)\n    if ext in ['.r', '.R'] and not metadata:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, \"#'\", ext)\n\n    return metadata",
        "rewrite": "Here is the revised code:\n\n```\ndef read_metadata(text, ext):\n    ext = '.' + ext.split('.')[-1]\n    lines = text.splitlines()\n    \n    comment = _SCRIPT_EXTENSIONS.get(ext[1:], {}).get('comment', '#') if ext not in ['.md', '.Rmd'] else ''\n    \n    metadata, *_, _, _ = header_to_metadata_and_cell(lines, comment, ext)\n    \n    if not metadata and ext in ['.r', '.R']:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, \"#'\",.ext)\n        \n    return metadata\n```"
    },
    {
        "original": "def _is_process_filtered(self, process, key=None):\n        \"\"\"Return True if the process[key] should be filtered according to the current filter\"\"\"\n        if key is None:\n            key = self.filter_key\n        try:\n            # If the item process[key] is a list, convert it to a string\n            # in order to match it with the current regular expression\n            if isinstance(process[key], list):\n                value = ' '.join(process[key])\n            else:\n                value = process[key]\n        except KeyError:\n            # If the key did not exist\n            return False\n        try:\n            return self._filter_re.match(value) is None\n        except (AttributeError, TypeError):\n            # AttributeError\n            # Filter processes crashs with a bad regular expression pattern (issue #665)\n            # TypeError\n            # Filter processes crashs if value is None (issue #1105)\n            return False",
        "rewrite": "```\ndef _is_process_filtered(self, process, key=None):\n    if key is None:\n        key = self.filter_key\n    try:\n        value = ' '.join(process[key]) if isinstance(process[key], list) else process[key]\n    except KeyError:\n        return False\n    try:\n        return self._filter_re.match(value) is None\n    except (AttributeError, TypeError):\n        return False"
    },
    {
        "original": "def modulation_type(self, value: int):\n        \"\"\"\n        0 - \"ASK\", 1 - \"FSK\", 2 - \"PSK\", 3 - \"APSK (QAM)\"\n\n        :param value:\n        :return:\n        \"\"\"\n        if self.__modulation_type != value:\n            self.__modulation_type = value\n            self._qad = None\n\n            self.modulation_type_changed.emit(self.__modulation_type)\n            if not self.block_protocol_update:\n                self.protocol_needs_update.emit()",
        "rewrite": "Here is the revised code:\n\n```\ndef modulation_type(self, value: int):\n    if self.__modulation_type != value:\n        self.__modulation_type = value\n        self._qad = None\n        self.modulation_type_changed.emit(value)\n        if not self.block_protocol_update:\n            self.protocol_needs_update.emit()\n```"
    },
    {
        "original": "def load(self, fname):\n        \"\"\"\n        Load signatures from a file.\n\n        @fname - Path to signature file.\n\n        Returns None.\n        \"\"\"\n        # Magic files must be ASCII, else encoding issues can arise.\n        fp = open(fname, \"r\")\n        lines = fp.readlines()\n        self.parse(lines)\n        fp.close()",
        "rewrite": "Here is the revised code:\n\n```\ndef load(self, fname):\n    with open(fname, \"r\", encoding=\"ascii\") as fp:\n        lines = fp.readlines()\n    self.parse(lines)\n```"
    },
    {
        "original": "def enter_room(self, sid, room, namespace=None):\n        \"\"\"Enter a room.\n\n        The only difference with the :func:`socketio.Server.enter_room` method\n        is that when the ``namespace`` argument is not given the namespace\n        associated with the class is used.\n        \"\"\"\n        return self.server.enter_room(sid, room,\n                                      namespace=namespace or self.namespace)",
        "rewrite": "```\ndef enter_room(self, sid, room, namespace=None):\n    return self.server.enter_room(sid, room, namespace=namespace or self.namespace)"
    },
    {
        "original": "def _get_args(self, kwargs):\n        \"\"\"\n        Discard all keywords which aren't function-specific from the kwargs.\n\n        :param kwargs:\n        :return:\n        \"\"\"\n        _args = list()\n        _kwargs = salt.utils.args.clean_kwargs(**kwargs)\n\n        return _args, _kwargs",
        "rewrite": "Here is the revised code:\n\n```\ndef get_filtered_args(self, **):\n    _args = []\n    return [], salt.utils.args.clean_kwargs(kwargs)\n```"
    },
    {
        "original": "def edit_caption(\n        self,\n        caption: str,\n        parse_mode: str = \"\",\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *edit_caption* of :obj:`Message <pyrogram.Message>`\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.edit_message_caption(\n                chat_id=message.chat.id,\n                message_id=message.message_id,\n                caption=\"hello\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.edit_caption(\"hello\")\n\n        Args:\n            caption (``str``):\n                New caption of the message.\n\n            parse_mode (``str``, *optional*):\n                Use :obj:`MARKDOWN <pyrogram.ParseMode.MARKDOWN>` or :obj:`HTML <pyrogram.ParseMode.HTML>`\n                if you want Telegram apps to show bold, italic, fixed-width text or inline URLs in your message.\n                Defaults to Markdown.\n\n            reply_markup (:obj:`InlineKeyboardMarkup`, *optional*):\n                An InlineKeyboardMarkup object.\n\n        Returns:\n            On success, the edited :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        return self._client.edit_message_caption(\n            chat_id=self.chat.id,\n            message_id=self.message_id,\n            caption=caption,\n            parse_mode=parse_mode,\n            reply_markup=reply_markup\n        )",
        "rewrite": "Here is the revised code:\n\n```python\nfrom pyrogram import Message, Client\nfrom pyrogram.types import InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass Message(Message):\n    def edit_caption(self, \n                     caption: str, \n                     parse_mode: str = \"\", \n                     reply_markup: Union[InlineKeyboardMarkup, ReplyKeyboardMarkup, Reply KeyboardRemove, ForceReply] = None) -> 'Message':\n        \n       return self._client.edit_message_caption(\n           chat_id=self.chat.id,\n           message_id=self.message_id,\n           caption=caption,\n           parse_mode="
    },
    {
        "original": "def _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n        \"\"\"Given M = sum(kron(a_i, b_i)), returns M' = sum(kron(b_i, a_i)).\"\"\"\n        result = np.array([[0] * 4] * 4, dtype=np.complex128)\n        order = [0, 2, 1, 3]\n        for i in range(4):\n            for j in range(4):\n                result[order[i], order[j]] = mat4x4[i, j]\n        return result",
        "rewrite": "Here is the revised code:\n\n```\nimport numpy as np\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    result = np.zeros((4, 4), dtype=np.complex128)\n    order = [0, 2, 1, 3]\n    for i in range(4):\n        for j in range(4):\n            result[order[i], order[j]] = mat4x4[i, j]\n    return result\n```"
    },
    {
        "original": "def pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        \"\"\"\n        Prints scheduler for user to read.\n        \"\"\"\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            # Print a message indicating what the below schedule means\n            print(\n                \"Table consists of tuples of \"\n                \"(num configs, num_resources_per_config) \"\n                \"which specify how many configs to run and \"\n                \"for how many epochs. \"\n            )\n            print(\n                \"Each bracket starts with a list of random \"\n                \"configurations which is successively halved \"\n                \"according the schedule.\"\n            )\n            print(\n                \"See the Hyperband paper \"\n                \"(https://arxiv.org/pdf/1603.06560.pdf) for more details.\"\n            )\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")",
        "rewrite": "Here is the revised code:\n\n```\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\"=========================================\")\n    print(\"|           Hyperband Schedule          |\")\n    print(\"=========================================\")\n    if describe_hyperband:\n        print(\"Table consists of tuples of (num configs, num_resources_per_config) which specify how many configs to run and for how many epochs.\")\n        print(\"Each bracket starts with a list of random configurations which is successively halved according the schedule.\")\n        print(\"See the Hyperband paper (https://arxiv.org/pdf/1603.06560.pdf)"
    },
    {
        "original": "def shorthand(self):\n        \"\"\"Return the 6-tuple (a,b,c,d,e,f) that describes this matrix\"\"\"\n        return (self.a, self.b, self.c, self.d, self.e, self.f)",
        "rewrite": "```\ndef shorthand(self):\n    return (self.a, self.b, self.c, self.d, self.e, self.f)\n```"
    },
    {
        "original": "def is_cyclic(graph):\n    \"\"\"\n    Return True if the directed graph g has a cycle. The directed graph\n    should be represented as a dictionary mapping of edges for each node.\n    \"\"\"\n    path = set()\n\n    def visit(vertex):\n        path.add(vertex)\n        for neighbour in graph.get(vertex, ()):\n            if neighbour in path or visit(neighbour):\n                return True\n        path.remove(vertex)\n        return False\n\n    return any(visit(v) for v in graph)",
        "rewrite": "Here is the revised code:\n\n```\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def is_cyclic_util(v):\n        if v in rec_stack:\n            return True\n        if v in visited:\n            return False\n        visited.add(v)\n        rec_stack.add(v)\n        for neighbour in graph.get(v, ()):\n            if is_cyclic_util(neighbour):\n                return True\n        rec_stack.remove(v)\n        return False\n\n    return any(is_cyclic_util(v) for v in graph)"
    },
    {
        "original": "def get_header_items(self):\n        \"\"\"\n        Get an iterable list of key/value pairs representing headers.\n\n        This function provides Python 2/3 compatibility as related to the\n        parsing of request headers. Python 2.7 is not compliant with\n        RFC 3875 Section 4.1.18 which requires multiple values for headers\n        to be provided. This function will return a matching list regardless\n        of Python version. It can be removed once Python 2.7 support\n        is dropped.\n\n        :return: List of tuples containing header hey/value pairs\n        \"\"\"\n        if PY2:\n            # For Python 2, process the headers manually according to\n            # W3C RFC 2616 Section 4.2.\n            items = []\n            for header in self.headers.headers:\n                # Remove \"\\n\\r\" from the header and split on \":\" to get\n                # the field name and value.\n                key, value = header[0:-2].split(\":\", 1)\n                # Add the key and the value once stripped of leading\n                # white space. The specification allows for stripping\n                # trailing white space but the Python 3 code does not\n                # strip trailing white space. Therefore, trailing space\n                # will be left as is to match the Python 3 behavior.\n                items.append((key, value.lstrip()))\n        else:\n            items = self.headers.items()\n\n        return items",
        "rewrite": "```\ndef get_header_items(self):\n    if PY2:\n        items = []\n        for header in self.headers.headers:\n            key, value = header[0:-2].split(\":\", 1)\n            items.append((key, value.lstrip()))\n    else:\n         items = self.headers.items()\n    return items"
    }
]