[
    {
        "original": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the gs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n       \"\"\"\n        _l.debug(\"Synchronizing gs segment register\")\n        state.regs.gs = self._read_gs_register_x64(concrete_target)",
        "rewrite": "```python\ndef initialize_segment_register_x64(self, state, concrete_target):\n    _l.debug(\"Synchronizing gs segment register\")\n    state.regs.gs = self._read_fs_register_x64(concrete_target)\n```"
    },
    {
        "original": "def load_csv_file(csv_file):\n    \"\"\" load csv file and check file content format\n\n    Args:\n        csv_file (str): csv file path, csv file content is like below:\n\n    Returns:\n        list: list of parameters, each parameter is in dict format\n\n    Examples:\n        >>> cat csv_file\n        username,password\n        test1,111111\n        test2,222222\n        test3,333333\n\n        >>> load_csv_file(csv_file)\n        [\n            {'username': 'test1', 'password': '111111'},\n            {'username': 'test2', 'password': '222222'},\n            {'username': 'test3', 'password': '333333'}\n        ]\n\n    \"\"\"\n    if not os.path.isabs(csv_file):\n        project_working_directory = tests_def_mapping[\"PWD\"] or os.getcwd()\n        # make compatible with Windows/Linux\n        csv_file = os.path.join(project_working_directory, *csv_file.split(\"/\"))\n\n    if not os.path.isfile(csv_file):\n        # file path not exist\n        raise exceptions.CSVNotFound(csv_file)\n\n    csv_content_list = []\n\n    with io.open(csv_file, encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            csv_content_list.append(row)\n\n    return csv_content_list",
        "rewrite": "```python\nimport os\nimport csv\nfrom io import StringIO\nfrom typing import List, Dict\n\nclass CSVNotFound(Exception):\n    pass\n\ndef load_csv_file(csv_file: str) -> List[Dict]:\n    if not os.path.isabs(csv_file):\n        project_working_directory = os.environ.get('PWD') or os.getcwd()\n        csv_file = os.path.join(project_working_directory, *csv_file.split(os.sep))\n\n    if not os.path.isfile(csv_file):\n        raise CSVNotFound(csv_file)\n\n    csv_content_list = []\n\n    with open(csv_file, encoding='utf-8') as csvfile:\n        reader = csv.Dict"
    },
    {
        "original": "async def get_proxies(self):\n        \"\"\"Receive proxies from the provider and return them.\n\n        :return: :attr:`.proxies`\n        \"\"\"\n        log.debug('Try to get proxies from %s' % self.domain)\n\n        async with aiohttp.ClientSession(\n            headers=get_headers(), cookies=self._cookies, loop=self._loop\n        ) as self._session:\n            await self._pipe()\n\n        log.debug(\n            '%d proxies received from %s: %s'\n            % (len(self.proxies), self.domain, self.proxies)\n        )\n        return self.proxies",
        "rewrite": "```python\nasync def get_proxies(self):\n    \"\"\"Receive proxies from the provider and return them.\"\"\"\n    log.debug(f'Try to get proxies from {self.domain}')\n\n    async with aiohttp.ClientSession(\n        headers=get_headers(), cookies=self._cookies, loop=self._loop\n    ) as self._session:\n        await self._pipe()\n\n    log.debug(\n        f'{len(self.proxies)} proxies received from {self.domain}: {self.proxies}'\n    )\n    return self.proxies\n```"
    },
    {
        "original": "def synthesize(vers, opts):\n    \"\"\" Synthesize a capability enabled version response\n        This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    failed = False  # noqa: F841 T25377293 Grandfathered in\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                \"client required capability `\"\n                + name\n                + \"` is not supported by this server\"\n            )\n    return vers",
        "rewrite": "```python\ndef synthesize(vers, opts):\n    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                f\"client required capability `{name}` is not supported by this server\"\n            )\n    return vers\n```"
    },
    {
        "original": "def _receive_message(self):\n        \"\"\"\n        Internal coroutine for receiving messages\n        \"\"\"\n        while True:\n            try:\n                if self._socket.getsockopt(zmq.TYPE) == zmq.ROUTER:\n                    zmq_identity, msg_bytes = \\\n                        yield from self._socket.recv_multipart()\n                    if msg_bytes == b'':\n                        # send ACK for connection probes\n                        LOGGER.debug(\"ROUTER PROBE FROM %s\", zmq_identity)\n                        self._socket.send_multipart(\n                            [bytes(zmq_identity), msg_bytes])\n                    else:\n                        self._received_from_identity(zmq_identity)\n                        self._dispatcher_queue.put_nowait(\n                            (zmq_identity, msg_bytes))\n                else:\n                    msg_bytes = yield from self._socket.recv()\n                    self._last_message_time = time.time()\n                    self._dispatcher_queue.put_nowait((None, msg_bytes))\n                self._get_queue_size_gauge(self.connection).set_value(\n                    self._dispatcher_queue.qsize())\n\n            except CancelledError:  # pylint: disable=try-except-raise\n                # The concurrent.futures.CancelledError is caught by asyncio\n                # when the Task associated with the coroutine is cancelled.\n                # The raise is required to stop this component.\n                raise\n            except Exception as e:  # pylint: disable=broad-except\n                LOGGER.exception(\"Received a message on address %s that \"\n                                 \"caused an error: %s\", self._address, e)",
        "rewrite": "```python\nimport asyncio\nimport zmq\nimport logging\nimport time\n\nclass MessageReceiver:\n    def __init__(self, socket, dispatcher_queue, connection, address):\n        self._socket = socket\n        self._dispatcher_queue = dispatcher_queue\n        self._connection = connection\n        self._address = address\n        self._last_message_time = time.time()\n        self._get_queue_size_gauge = None\n\n    async def _receive_message(self):\n        while True:\n            try:\n                if self._socket.getsockopt(zmq.TYPE) == zmq.ROUTER:\n                    zmq_identity, msg_bytes = await self._"
    },
    {
        "original": "def _always_running_service(name):\n    \"\"\"\n    Check if the service should always be running based on the KeepAlive Key\n    in the service plist.\n\n    :param str name: Service label, file name, or full path\n\n    :return: True if the KeepAlive key is set to True, False if set to False or\n        not set in the plist at all.\n\n    :rtype: bool\n\n    .. versionadded:: 2019.2.0\n    \"\"\"\n\n    # get all the info from the launchctl service\n    service_info = show(name)\n\n    # get the value for the KeepAlive key in service plist\n    try:\n        keep_alive = service_info['plist']['KeepAlive']\n    except KeyError:\n        return False\n\n    # check if KeepAlive is True and not just set.\n\n    if isinstance(keep_alive, dict):\n        # check for pathstate\n        for _file, value in six.iteritems(keep_alive.get('PathState', {})):\n            if value is True and os.path.exists(_file):\n                return True\n            elif value is False and not os.path.exists(_file):\n                return True\n\n    if keep_alive is True:\n        return True\n\n    return False",
        "rewrite": "```python\nimport os\nimport six\n\ndef _always_running_service(name):\n    service_info = show(name)\n    try:\n        keep_alive = service_info['plist']['KeepAlive']\n    except KeyError:\n        return False\n\n    if isinstance(keep_alive, dict):\n        for _file, value in six.iteritems(keep_alive.get('PathState', {})):\n            if value and os.path.exists(_file):\n                return True\n            elif not value and not os.path.exists(_file):\n                return True\n\n    return keep_alive\n```"
    },
    {
        "original": "def _flush_content(self):\n        \"\"\"\n        Flush content to the archive\n        :return:\n        \"\"\"\n        if self.__current_section is not None:\n            buff = BytesIO()\n            buff._dirty = False\n            for action_return in self.__current_section:\n                for title, ret_data in action_return.items():\n                    if isinstance(ret_data, file):\n                        self.out.put(ret_data.name, indent=4)\n                        self.__arch.add(ret_data.name, arcname=ret_data.name)\n                    else:\n                        buff.write(salt.utils.stringutils.to_bytes(title + '\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(('-' * len(title)) + '\\n\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(ret_data))\n                        buff.write(salt.utils.stringutils.to_bytes('\\n\\n\\n'))\n                        buff._dirty = True\n            if buff._dirty:\n                buff.seek(0)\n                tar_info = tarfile.TarInfo(name=\"{}/{}\".format(self.__default_root, self.__current_section_name))\n                if not hasattr(buff, 'getbuffer'):  # Py2's BytesIO is older\n                    buff.getbuffer = buff.getvalue\n                tar_info.size = len(buff.getbuffer())\n                self.__arch.addfile(tarinfo=tar_info, fileobj=buff)",
        "rewrite": "```python\ndef _flush_content(self):\n    if self.__current_section is not None:\n        buff = BytesIO()\n        for action_return in self.__current_section:\n            for title, ret_data in action_return.items():\n                if isinstance(ret_data, file):\n                    self.out.put(ret_data.name, indent=4)\n                    self.__arch.add(ret_data.name, arcname=ret_data.name)\n                else:\n                    buff.write(salt.utils.stringutils.to_bytes(title + '\\n'))\n                    buff.write(salt.utils.stringutils.to_bytes(('-' * len(title)) + '\\n\\n'))\n                    buff.write(salt.utils"
    },
    {
        "original": "def fit_linear(X, y):\n    \"\"\"\n    Uses OLS to fit the regression.\n    \"\"\"\n    model = linear_model.LinearRegression()\n    model.fit(X, y)\n    return model",
        "rewrite": "```python\nfrom sklearn.linear_model import LinearRegression\n\ndef fit_linear(X, y):\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n```"
    },
    {
        "original": "def ensure_coordinator_ready(self):\n        \"\"\"Block until the coordinator for this group is known\n        (and we have an active connection -- java client uses unsent queue).\n        \"\"\"\n        with self._client._lock, self._lock:\n            while self.coordinator_unknown():\n\n                # Prior to 0.8.2 there was no group coordinator\n                # so we will just pick a node at random and treat\n                # it as the \"coordinator\"\n                if self.config['api_version'] < (0, 8, 2):\n                    self.coordinator_id = self._client.least_loaded_node()\n                    if self.coordinator_id is not None:\n                        self._client.maybe_connect(self.coordinator_id)\n                    continue\n\n                future = self.lookup_coordinator()\n                self._client.poll(future=future)\n\n                if future.failed():\n                    if future.retriable():\n                        if getattr(future.exception, 'invalid_metadata', False):\n                            log.debug('Requesting metadata for group coordinator request: %s', future.exception)\n                            metadata_update = self._client.cluster.request_update()\n                            self._client.poll(future=metadata_update)\n                        else:\n                            time.sleep(self.config['retry_backoff_ms'] / 1000)\n                    else:\n                        raise future.exception",
        "rewrite": "```python\ndef ensure_coordinator_ready(self):\n    \"\"\"Block until the coordinator for this group is known\n    (and we have an active connection -- java client uses unsent queue).\n    \"\"\"\n    with self._client._lock, self._lock:\n        while self.coordinator_unknown():\n            if self.config['api_version'] < (0, 8, 2):\n                self.coordinator_id = self._client.least_loaded_node()\n                if self.coordinator_id is not None:\n                    self._client.maybe_connect(self.coordinator_id)\n                continue\n\n            future = self.lookup_coordinator()\n            self._client"
    },
    {
        "original": "def get_dim_indexers(data_obj, indexers):\n    \"\"\"Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \"\"\"\n    invalid = [k for k in indexers\n               if k not in data_obj.dims and k not in data_obj._level_coords]\n    if invalid:\n        raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n                         % invalid)\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        dim, = data_obj[key].dims\n        if key != dim:\n            # assume here multi-index level indexer\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key] = label\n\n    for dim, level_labels in level_indexers.items():\n        if dim_indexers.get(dim, False):\n            raise ValueError(\"cannot combine multi-index level indexers \"\n                             \"with an indexer for dimension %s\" % dim)\n        dim_indexers[dim] = level_labels\n\n    return dim_indexers",
        "rewrite": "```python\nfrom collections import defaultdict\n\ndef get_dim_indexers(data_obj, indexers):\n    invalid = [k for k in indexers if k not in data_obj.dims and k not in data_obj._level_coords]\n    if invalid:\n        raise ValueError(\"dimensions or multi-index levels %r do not exist\" % invalid)\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        dim, = data_obj[key].dims\n        if key != dim:\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key"
    },
    {
        "original": "def best_case(self, matrix, m_list, indices_left):\n        \"\"\"\n        Computes a best case given a matrix and manipulation list.\n\n        Args:\n            matrix: the current matrix (with some permutations already\n                performed)\n            m_list: [(multiplication fraction, number_of_indices, indices,\n                species)] describing the manipulation\n            indices: Set of indices which haven't had a permutation\n                performed on them.\n        \"\"\"\n        m_indices = []\n        fraction_list = []\n        for m in m_list:\n            m_indices.extend(m[2])\n            fraction_list.extend([m[0]] * m[1])\n\n        indices = list(indices_left.intersection(m_indices))\n\n        interaction_matrix = matrix[indices, :][:, indices]\n\n        fractions = np.zeros(len(interaction_matrix)) + 1\n        fractions[:len(fraction_list)] = fraction_list\n        fractions = np.sort(fractions)\n\n        # Sum associated with each index (disregarding interactions between\n        # indices)\n        sums = 2 * np.sum(matrix[indices], axis=1)\n        sums = np.sort(sums)\n\n        # Interaction corrections. Can be reduced to (1-x)(1-y) for x,y in\n        # fractions each element in a column gets multiplied by (1-x), and then\n        # the sum of the columns gets multiplied by (1-y) since fractions are\n        # less than 1, there is no effect of one choice on the other\n        step1 = np.sort(interaction_matrix) * (1 - fractions)\n        step2 = np.sort(np.sum(step1, axis=1))\n        step3 = step2 * (1 - fractions)\n        interaction_correction = np.sum(step3)\n\n        if self._algo == self.ALGO_TIME_LIMIT:\n            elapsed_time = datetime.utcnow() - self._start_time\n            speedup_parameter = elapsed_time.total_seconds() / 1800\n            avg_int = np.sum(interaction_matrix, axis=None)\n            avg_frac = np.average(np.outer(1 - fractions, 1 - fractions))\n            average_correction = avg_int * avg_frac\n\n            interaction_correction = average_correction * speedup_parameter \\\n                + interaction_correction * (1 - speedup_parameter)\n\n        best_case = np.sum(matrix) + np.inner(sums[::-1], fractions - 1) \\\n            + interaction_correction\n\n        return best_case",
        "rewrite": "```python\nimport numpy as np\nfrom datetime import datetime\n\ndef best_case(self, matrix, m_list, indices_left):\n    m_indices = []\n    fraction_list = []\n    for m in m_list:\n        m_indices.extend(m[2])\n        fraction_list.extend([m[0]] * m[1])\n\n    indices = list(set(indices_left.intersection(m_indices)))\n\n    interaction_matrix = matrix[np.array(indices), :][:, np.array(indices)]\n\n    fractions = np.zeros(len(interaction_matrix)) + 1\n    fractions[:len(fraction_list)] = fraction_list\n    fractions = np.sort(fractions)\n\n   "
    },
    {
        "original": "def tempo_account_delete_account_by_id(self, account_id):\n        \"\"\"\n        Delete an Account by id. Caller must have the Manage Account Permission for the Account.\n        The Account can not be deleted if it has an AccountLinkBean.\n        :param account_id: the id of the Account to be deleted.\n        :return:\n        \"\"\"\n        url = 'rest/tempo-accounts/1/account/{id}/'.format(id=account_id)\n        return self.delete(url)",
        "rewrite": "```python\ndef tempo_account_delete_account_by_id(self, account_id):\n    url = f'rest/tempo-accounts/1/account/{account_id}/'\n    return self.delete(url)\n```"
    },
    {
        "original": "def _encode_long(name, value, dummy0, dummy1):\n    \"\"\"Encode a python long (python 2.x)\"\"\"\n    try:\n        return b\"\\x12\" + name + _PACK_LONG(value)\n    except struct.error:\n        raise OverflowError(\"BSON can only handle up to 8-byte ints\")",
        "rewrite": "```python\nimport struct\n\ndef _encode_long(name, value, dummy0, dummy1):\n    try:\n        return b\"\\x12\" + name + struct.pack(\">q\", value)\n    except struct.error:\n        raise OverflowError(\"BSON can only handle up to 8-byte ints\")\n```"
    },
    {
        "original": "def read_can_msg(self, channel, count):\n        \"\"\"\n        Reads one or more CAN-messages from the buffer of the specified CAN channel.\n\n        :param int channel:\n            CAN channel to read from (:data:`Channel.CHANNEL_CH0`, :data:`Channel.CHANNEL_CH1`,\n            :data:`Channel.CHANNEL_ANY`).\n        :param int count: The number of CAN messages to be received.\n        :return: Tuple with list of CAN message/s received and the CAN channel where the read CAN messages came from.\n        :rtype: tuple(list(CanMsg), int)\n        \"\"\"\n        c_channel = BYTE(channel)\n        c_can_msg = (CanMsg * count)()\n        c_count = DWORD(count)\n        UcanReadCanMsgEx(self._handle, byref(c_channel), c_can_msg, byref(c_count))\n        return c_can_msg[:c_count.value], c_channel.value",
        "rewrite": "```python\ndef read_can_msg(self, channel, count):\n    c_channel = BYTE(channel)\n    c_can_msg = (CanMsg * count)()\n    c_count = DWORD(count)\n    UcanReadCanMsgEx(self._handle, byref(c_channel), c_can_msg, byref(c_count))\n    return [msg for msg in c_can_msg[:c_count.value]], c_channel.value\n```"
    },
    {
        "original": "def WriteFlowRequests(self, requests):\n    \"\"\"Writes a list of flow requests to the database.\"\"\"\n    flow_processing_requests = []\n\n    for request in requests:\n      if (request.client_id, request.flow_id) not in self.flows:\n        raise db.AtLeastOneUnknownFlowError([(request.client_id,\n                                              request.flow_id)])\n\n    for request in requests:\n      key = (request.client_id, request.flow_id)\n      request_dict = self.flow_requests.setdefault(key, {})\n      request_dict[request.request_id] = request.Copy()\n      request_dict[request.request_id].timestamp = rdfvalue.RDFDatetime.Now()\n\n      if request.needs_processing:\n        flow = self.flows[(request.client_id, request.flow_id)]\n        if flow.next_request_to_process == request.request_id:\n          flow_processing_requests.append(\n              rdf_flows.FlowProcessingRequest(\n                  client_id=request.client_id,\n                  flow_id=request.flow_id,\n                  delivery_time=request.start_time))\n\n    if flow_processing_requests:\n      self.WriteFlowProcessingRequests(flow_processing_requests)",
        "rewrite": "```python\ndef write_flow_requests(self, requests):\n    flow_processing_requests = []\n\n    for request in requests:\n        if (request.client_id, request.flow_id) not in self.flows:\n            raise ValueError(f\"Unknown flow for client {request.client_id} and flow {request.flow_id}\")\n\n    for request in requests:\n        key = (request.client_id, request.flow_id)\n        request_dict = self.flow_requests.setdefault(key, {})\n        request_dict[request.request_id] = request.copy()\n        request_dict[request.request_id].timestamp = rdfvalue.RDFDatetime.Now()\n\n        if request.needs_processing:\n            flow"
    },
    {
        "original": "def _create_gitlab_prometheus_instance(self, instance, init_config):\n        \"\"\"\n        Set up the gitlab instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        # Mapping from Prometheus metrics names to Datadog ones\n        # For now it's a 1:1 mapping\n        allowed_metrics = init_config.get('allowed_metrics')\n        if allowed_metrics is None:\n            raise CheckException(\"At least one metric must be whitelisted in `allowed_metrics`.\")\n\n        gitlab_instance = deepcopy(instance)\n        # gitlab uses 'prometheus_endpoint' and not 'prometheus_url', so we have to rename the key\n        gitlab_instance['prometheus_url'] = instance.get('prometheus_endpoint')\n\n        gitlab_instance.update(\n            {\n                'namespace': 'gitlab',\n                'metrics': allowed_metrics,\n                # Defaults that were set when gitlab was based on PrometheusCheck\n                'send_monotonic_counter': instance.get('send_monotonic_counter', False),\n                'health_service_check': instance.get('health_service_check', False),\n            }\n        )\n\n        return gitlab_instance",
        "rewrite": "```python\ndef _create_gitlab_prometheus_instance(self, instance, init_config):\n    allowed_metrics = init_config.get('allowed_metrics')\n    if not allowed_metrics:\n        raise CheckException(\"At least one metric must be whitelisted in `allowed_metrics`.\")\n\n    gitlab_instance = deepcopy(instance)\n    gitlab_instance['prometheus_url'] = instance.get('prometheus_endpoint', instance.get('prometheus_url'))\n\n    gitlab_instance.update({\n        'namespace': 'gitlab',\n        'metrics': allowed_metrics,\n        'send_monotonic_counter': instance.get('send_monotonic_counter', False),\n       "
    },
    {
        "original": "def setup_voronoi_list(self, indices, voronoi_cutoff):\n        \"\"\"\n        Set up of the voronoi list of neighbours by calling qhull\n        :param indices: indices of the sites for which the Voronoi is needed\n        :param voronoi_cutoff: Voronoi cutoff for the search of neighbours\n        :raise RuntimeError: If an infinite vertex is found in the voronoi construction\n        \"\"\"\n        self.voronoi_list2 = [None] * len(self.structure)\n        self.voronoi_list_coords = [None] * len(self.structure)\n        logging.info('Getting all neighbors in structure')\n        struct_neighbors = self.structure.get_all_neighbors(voronoi_cutoff, include_index=True)\n        t1 = time.clock()\n        logging.info('Setting up Voronoi list :')\n\n        for jj, isite in enumerate(indices):\n            logging.info('  - Voronoi analysis for site #{:d} ({:d}/{:d})'.format(isite, jj+1, len(indices)))\n            site = self.structure[isite]\n            neighbors1 = [(site, 0.0, isite)]\n            neighbors1.extend(struct_neighbors[isite])\n            distances = [i[1] for i in sorted(neighbors1, key=lambda s: s[1])]\n            neighbors = [i[0] for i in sorted(neighbors1, key=lambda s: s[1])]\n            qvoronoi_input = [s.coords for s in neighbors]\n            voro = Voronoi(points=qvoronoi_input, qhull_options=\"o Fv\")\n            all_vertices = voro.vertices\n\n            results2 = []\n            maxangle = 0.0\n            mindist = 10000.0\n            for iridge, ridge_points in enumerate(voro.ridge_points):\n                if 0 in ridge_points:\n                    ridge_vertices_indices = voro.ridge_vertices[iridge]\n                    if -1 in ridge_vertices_indices:\n                        raise RuntimeError(\"This structure is pathological,\"\n                                           \" infinite vertex in the voronoi \"\n                                           \"construction\")\n\n                    ridge_point2 = max(ridge_points)\n                    facets = [all_vertices[i] for i in ridge_vertices_indices]\n                    sa = my_solid_angle(site.coords, facets)\n                    maxangle = max([sa, maxangle])\n\n                    mindist = min([mindist, distances[ridge_point2]])\n                    for iii, sss in enumerate(self.structure):\n                        if neighbors[ridge_point2].is_periodic_image(sss):\n                            myindex = iii\n                            break\n                    results2.append({'site': neighbors[ridge_point2],\n                                     'angle': sa,\n                                     'distance': distances[ridge_point2],\n                                     'index': myindex})\n            for dd in results2:\n                dd['normalized_angle'] = dd['angle'] / maxangle\n                dd['normalized_distance'] = dd['distance'] / mindist\n            self.voronoi_list2[isite] = results2\n            self.voronoi_list_coords[isite] = np.array([dd['site'].coords for dd in results2])\n        t2 = time.clock()\n        logging.info('Voronoi list set up in {:.2f} seconds'.format(t2-t1))",
        "rewrite": "```python\nimport logging\nimport time\nimport numpy as np\n\ndef setup_voronoi_list(self, indices, voronoi_cutoff):\n    \"\"\"\n    Set up of the voronoi list of neighbours by calling qhull\n    :param indices: indices of the sites for which the Voronoi is needed\n    :param voronoi_cutoff: Voronoi cutoff for the search of neighbours\n    :raise RuntimeError: If an infinite vertex is found in the voronoi construction\n    \"\"\"\n    self.voronoi_list2 = [None] * len(self.structure)\n    self.voronoi_list_coords ="
    },
    {
        "original": "def validate_config(self, config):\n        \"\"\"We only validate the config if passed.\n\n        Also we use the ExperimentSpecification to check if this config was\n        intended as an experiment.\n        \"\"\"\n        # config is optional\n        if not config:\n            return config\n\n        spec = validate_experiment_spec_config(config)\n\n        if spec.is_experiment:\n            # Resume normal creation\n            return config\n\n        # Raise an error to tell the user to use experiment creation instead\n        raise ValidationError('Current experiment creation could not be performed.\\n'\n                              'The reason is that the specification sent correspond '\n                              'to a `{}`.\\n'.format(spec.kind))",
        "rewrite": "```python\ndef validate_config(self, config):\n    \"\"\"Validate the config if passed and check if it's intended as an experiment.\"\"\"\n    if not config:\n        return config\n\n    spec = validate_experiment_spec_config(config)\n\n    if spec.is_experiment:\n        return config\n\n    raise ValidationError(\n        f\"Current experiment creation could not be performed. \"\n        f\"The reason is that the specification sent corresponds to a `{spec.kind}`.\"\n    )\n```"
    },
    {
        "original": "def stash_state():\n    \"\"\"\n    Builds a list of all currently pressed scan codes, releases them and returns\n    the list. Pairs well with `restore_state` and `restore_modifiers`.\n    \"\"\"\n    # TODO: stash caps lock / numlock /scrollock state.\n    with _pressed_events_lock:\n        state = sorted(_pressed_events)\n    for scan_code in state:\n        _os_keyboard.release(scan_code)\n    return state",
        "rewrite": "```python\ndef stash_state():\n    \"\"\"\n    Builds a list of all currently pressed scan codes, releases them and returns\n    the list. Pairs well with `restore_state` and `restore_modifiers`.\n    \"\"\"\n    with _pressed_events_lock:\n        state = sorted(_pressed_events)\n    for scan_code in state:\n        _os_keyboard.release(scan_code)\n    return state.copy()\n```"
    },
    {
        "original": "def _marginalize_factor(self, nodes, factor):\n        \"\"\"\n        Marginalizing the factor selectively for a set of variables.\n\n        Parameters:\n        ----------\n        nodes: list, array-like\n            A container of nodes (list, dict, set, etc.).\n\n        factor: factor\n            factor which is to be marginalized.\n        \"\"\"\n        marginalizing_nodes = list(set(factor.scope()).difference(nodes))\n        return factor.marginalize(marginalizing_nodes, inplace=False)",
        "rewrite": "```python\ndef _marginalize_factor(self, nodes, factor):\n    marginalizing_nodes = set(factor.scope()).difference(nodes)\n    return factor.marginalize(marginalizing_nodes, inplace=False)\n```"
    },
    {
        "original": "def broadcast_info(team_id, date=datetime.now()):\n    \"\"\"Return BroadcastInfo object that containts information\n    about the television and radio broadcasts for the team_id\n    and year\"\"\"\n    data = mlbgame.info.broadcast_info(team_id, date)\n    return [mlbgame.info.BroadcastInfo(x) for x in data]",
        "rewrite": "```python\nfrom datetime import datetime\n\ndef broadcast_info(team_id, date=datetime.now()):\n    return [mlbgame.info.BroadcastInfo(x) for x in mlbgame.info.broadcast_info(team_id, date)]\n```"
    },
    {
        "original": "def current_size(self):\n        \"\"\"The size of the current line minus the indentation.\"\"\"\n        size = 0\n        for item in reversed(self._lines):\n            size += item.size\n            if isinstance(item, self._LineBreak):\n                break\n\n        return size",
        "rewrite": "```python\ndef current_size(self):\n    size = 0\n    for item in reversed(self._lines):\n        size += item.size\n        if isinstance(item, self._LineBreak):\n            break\n    return size\n```"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "rewrite": "```python\ndef execute(self):\n    return self.client.execute_command(*self.command)\n```"
    },
    {
        "original": "def _send_group_coordinator_request(self):\n        \"\"\"Discover the current coordinator for the group.\n\n        Returns:\n            Future: resolves to the node id of the coordinator\n        \"\"\"\n        node_id = self._client.least_loaded_node()\n        if node_id is None:\n            return Future().failure(Errors.NoBrokersAvailable())\n\n        elif not self._client.ready(node_id, metadata_priority=False):\n            e = Errors.NodeNotReadyError(node_id)\n            return Future().failure(e)\n\n        log.debug(\"Sending group coordinator request for group %s to broker %s\",\n                  self.group_id, node_id)\n        request = GroupCoordinatorRequest[0](self.group_id)\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_group_coordinator_response, future)\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future",
        "rewrite": "```python\ndef _send_group_coordinator_request(self):\n    node_id = self._client.least_loaded_node()\n    if node_id is None:\n        return Future().failure(Errors.NoBrokersAvailable())\n\n    elif not self._client.ready(node_id, metadata_priority=False):\n        return Future().failure(Errors.NodeNotReadyError(node_id))\n\n    log.debug(\"Sending group coordinator request for group %s to broker %s\",\n              self.group_id, node_id)\n    request = GroupCoordinatorRequest[0](self.group_id)\n    future = Future()\n    _f = self._client.send(node_id, request)\n   "
    },
    {
        "original": "def sort(expr, field = None, keytype=None, ascending=True):\n    \"\"\"\n    Sorts the vector.\n    If the field parameter is provided then the sort\n    operators on a vector of structs where the sort key\n    is the field of the struct.\n\n    Args:\n      expr (WeldObject)\n      field (Int)\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    expr_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        expr_var = expr.obj_id\n        weld_obj.dependencies[expr_var] = expr\n\n    if field is not None:\n        key_str = \"x.$%s\" % field\n    else:\n        key_str = \"x\"\n\n    if not ascending:\n        # The type is not necessarily f64.\n        key_str = key_str + \"* %s(-1)\" % keytype\n\n    weld_template = ",
        "rewrite": "```python\ndef sort(expr, field=None, key_type=None, ascending=True):\n    \"\"\"\n    Sorts the vector.\n\n    Args:\n        expr (WeldObject): The vector to be sorted.\n        field (int, optional): The field of the struct to be used as the sort key. Defaults to None.\n        key_type (str, optional): The type of the sort key. Defaults to None.\n        ascending (bool, optional): Whether to sort in ascending order. Defaults to True.\n\n    Returns:\n        WeldObject:"
    },
    {
        "original": "def get_redis(**kwargs):\n    \"\"\"Returns a redis client instance.\n\n    Parameters\n    ----------\n    redis_cls : class, optional\n        Defaults to ``redis.StrictRedis``.\n    url : str, optional\n        If given, ``redis_cls.from_url`` is used to instantiate the class.\n    **kwargs\n        Extra parameters to be passed to the ``redis_cls`` class.\n\n    Returns\n    -------\n    server\n        Redis client instance.\n\n    \"\"\"\n    redis_cls = kwargs.pop('redis_cls', defaults.REDIS_CLS)\n    url = kwargs.pop('url', None)\n    if url:\n        return redis_cls.from_url(url, **kwargs)\n    else:\n        return redis_cls(**kwargs)",
        "rewrite": "```python\nfrom typing import Any\n\ndef get_redis(**kwargs: Any) -> Any:\n    redis_cls = kwargs.pop('redis_cls', 'redis.StrictRedis')\n    url = kwargs.pop('url', None)\n    if url:\n        return getattr(redis_cls, 'from_url')(url, **kwargs)\n    else:\n        return redis_cls(**kwargs)\n```"
    },
    {
        "original": "def barrier():\n    \"\"\"\n    Works as a temporary distributed barrier, currently pytorch\n    doesn't implement barrier for NCCL backend.\n    Calls all_reduce on dummy tensor and synchronizes with GPU.\n    \"\"\"\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        torch.distributed.all_reduce(torch.cuda.FloatTensor(1))\n        torch.cuda.synchronize()",
        "rewrite": "```python\nimport torch\nimport torch.distributed as dist\n\ndef barrier():\n    if dist.is_available() and dist.is_initialized():\n        dist.all_reduce(torch.cuda.FloatTensor(1))\n        torch.cuda.synchronize()\n```"
    },
    {
        "original": "def get_excitation_spectrum(self, width=0.1, npoints=2000):\n        \"\"\"\n        Generate an excitation spectra from the singlet roots of TDDFT\n        calculations.\n\n        Args:\n            width (float): Width for Gaussian smearing.\n            npoints (int): Number of energy points. More points => smoother\n                curve.\n\n        Returns:\n            (ExcitationSpectrum) which can be plotted using\n                pymatgen.vis.plotters.SpectrumPlotter.\n        \"\"\"\n        roots = self.parse_tddft()\n        data = roots[\"singlet\"]\n        en = np.array([d[\"energy\"] for d in data])\n        osc = np.array([d[\"osc_strength\"] for d in data])\n\n        epad = 20.0 * width\n        emin = en[0] - epad\n        emax = en[-1] + epad\n        de = (emax - emin) / npoints\n\n        # Use width of at least two grid points\n        if width < 2 * de:\n            width = 2 * de\n\n        energies = [emin + ie * de for ie in range(npoints)]\n\n        cutoff = 20.0 * width\n        gamma = 0.5 * width\n        gamma_sqrd = gamma * gamma\n\n        de = (energies[-1] - energies[0]) / (len(energies) - 1)\n        prefac = gamma / np.pi * de\n\n        x = []\n        y = []\n        for energy in energies:\n            xx0 = energy - en\n            stot = osc / (xx0 * xx0 + gamma_sqrd)\n            t = np.sum(stot[np.abs(xx0) <= cutoff])\n            x.append(energy)\n            y.append(t * prefac)\n        return ExcitationSpectrum(x, y)",
        "rewrite": "```python\nimport numpy as np\n\ndef get_excitation_spectrum(self, width=0.1, npoints=2000):\n    roots = self.parse_tddft()\n    data = roots[\"singlet\"]\n    en = np.array([d[\"energy\"] for d in data])\n    osc = np.array([d[\"osc_strength\"] for d in data])\n\n    epad = 20.0 * width\n    emin = en[0] - epad\n    emax = en[-1] + epad\n    de = (emax - emin) / npoints\n\n    if width < 2 *"
    },
    {
        "original": "def GetFeeds(client):\n  \"\"\"Returns a list of all enabled Feeds.\n\n  Args:\n    client: an AdWordsClient instance.\n\n  Returns:\n    A list containing all enabled Feeds.\n  \"\"\"\n  feed_service = client.GetService('FeedService', 'v201809')\n\n  feeds = []\n  more_pages = True\n\n  selector = {\n      'fields': ['Id', 'Name', 'Attributes'],\n      'predicates': [\n          {\n              'field': 'Origin',\n              'operator': 'EQUALS',\n              'values': ['USER']\n          },\n          {\n              'field': 'FeedStatus',\n              'operator': 'EQUALS',\n              'values': ['ENABLED']\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = feed_service.get(selector)\n\n    if 'entries' in page:\n      feeds.extend(page['entries'])\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return feeds",
        "rewrite": "```python\ndef get_feeds(client):\n    feed_service = client.GetService('FeedService', 'v201809')\n\n    feeds = []\n    more_pages = True\n    page_size = 100  # Define PAGE_SIZE\n\n    selector = {\n        'fields': ['Id', 'Name', 'Attributes'],\n        'predicates': [\n            {\n                'field': 'Origin',\n                'operator': 'EQUALS',\n                'values': ['USER']\n            },\n            {\n                'field': 'FeedStatus',\n                'operator': 'EQUALS',\n                'values': ['ENABLED']\n            }\n        ],\n        'paging"
    },
    {
        "original": "def DetermineRunner(bbdir):\n    \"\"\"Checks if the given directory is a buildbot worker or a master and\n    returns the appropriate run function.\"\"\"\n    try:\n        import buildbot_worker.scripts.runner\n        tacfile = os.path.join(bbdir, 'buildbot.tac')\n\n        if os.path.exists(tacfile):\n            with open(tacfile, 'r') as f:\n                contents = f.read()\n                if 'import Worker' in contents:\n                    return buildbot_worker.scripts.runner.run\n\n    except ImportError:\n        # Use the default\n        pass\n\n    import buildbot.scripts.runner\n    return buildbot.scripts.runner.run",
        "rewrite": "```python\nimport os\nimport importlib.util\n\ndef determine_runner(bbdir):\n    try:\n        spec = importlib.util.find_spec('buildbot_worker.scripts.runner')\n        if spec is not None:\n            tacfile = os.path.join(bbdir, 'buildbot.tac')\n            if os.path.exists(tacfile):\n                with open(tacfile, 'r') as f:\n                    contents = f.read()\n                    if 'import Worker' in contents:\n                        return buildbot_worker.scripts.runner.run\n    except Exception:\n        pass\n\n    import buildbot.scripts.runner\n    return buildbot.scripts.runner"
    },
    {
        "original": "def _binary_sample(image, label, n_samples_per_label, label_count):\n        \"\"\" Sample `nsamples_per_label` points from the binary mask corresponding to `label`\n\n        Randomly sample `nsamples_per_label` point form the binary mask corresponding to `label`. Sampling with\n        replacement is used if the required `nsamples_per_label` is larger than the available `label_count`\n\n        :param image: Input 2D raster label image\n        :type image: uint8 numpy array\n        :param label: Scalar value of label to consider\n        :type label: uint8\n        :param n_samples_per_label: Number of points to sample form the binary mask\n        :type n_samples_per_label: uint32\n        :param label_count: Number of points available for `label`\n        :type label_count: uint32\n        :return: Sampled label value, row index of samples, col index of samples\n        \"\"\"\n        h_idx, w_idx = np.where(image == label)\n\n        rand_idx = np.random.choice(h_idx.size, size=n_samples_per_label, replace=label_count < n_samples_per_label)\n\n        return h_idx[rand_idx], w_idx[rand_idx]",
        "rewrite": "```python\nimport numpy as np\n\ndef _binary_sample(image, label, n_samples_per_label, label_count):\n    h_idx, w_idx = np.where(image == label)\n    rand_idx = np.random.choice(h_idx.size, size=n_samples_per_label, replace=True)\n    return h_idx[rand_idx], w_idx[rand_idx]\n```"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a AnalysisResults object from a json dictionary.\"\"\"\n        args = {}\n        if 'language' in _dict:\n            args['language'] = _dict.get('language')\n        if 'analyzed_text' in _dict:\n            args['analyzed_text'] = _dict.get('analyzed_text')\n        if 'retrieved_url' in _dict:\n            args['retrieved_url'] = _dict.get('retrieved_url')\n        if 'usage' in _dict:\n            args['usage'] = AnalysisResultsUsage._from_dict(_dict.get('usage'))\n        if 'concepts' in _dict:\n            args['concepts'] = [\n                ConceptsResult._from_dict(x) for x in (_dict.get('concepts'))\n            ]\n        if 'entities' in _dict:\n            args['entities'] = [\n                EntitiesResult._from_dict(x) for x in (_dict.get('entities'))\n            ]\n        if 'keywords' in _dict:\n            args['keywords'] = [\n                KeywordsResult._from_dict(x) for x in (_dict.get('keywords'))\n            ]\n        if 'categories' in _dict:\n            args['categories'] = [\n                CategoriesResult._from_dict(x)\n                for x in (_dict.get('categories'))\n            ]\n        if 'emotion' in _dict:\n            args['emotion'] = EmotionResult._from_dict(_dict.get('emotion'))\n        if 'metadata' in _dict:\n            args['metadata'] = AnalysisResultsMetadata._from_dict(\n                _dict.get('metadata'))\n        if 'relations' in _dict:\n            args['relations'] = [\n                RelationsResult._from_dict(x) for x in (_dict.get('relations'))\n            ]\n        if 'semantic_roles' in _dict:\n            args['semantic_roles'] = [\n                SemanticRolesResult._from_dict(x)\n                for x in (_dict.get('semantic_roles'))\n            ]\n        if 'sentiment' in _dict:\n            args['sentiment'] = SentimentResult._from_dict(\n                _dict.get('sentiment'))\n        if 'syntax' in _dict:\n            args['syntax'] = SyntaxResult._from_dict(_dict.get('syntax'))\n        return cls(**args)",
        "rewrite": "```python\ndef _from_dict(cls, _dict):\n    args = {}\n    for key, value in _dict.items():\n        if key == 'usage':\n            args['usage'] = AnalysisResultsUsage._from_dict(value)\n        elif key == 'concepts':\n            args['concepts'] = [ConceptsResult._from_dict(x) for x in value]\n        elif key == 'entities':\n            args['entities'] = [EntitiesResult._from_dict(x) for x in value]\n        elif key == 'keywords':\n            args['keywords'] = [KeywordsResult._from_dict(x) for x in value]\n"
    },
    {
        "original": "def edit(\n        self,\n        text: str,\n        parse_mode: str = \"\",\n        disable_web_page_preview: bool = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *edit* of :obj:`Message <pyrogram.Message>`\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.edit_message_text(\n                chat_id=message.chat.id,\n                message_id=message.message_id,\n                text=\"hello\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.edit(\"hello\")\n\n        Args:\n            text (``str``):\n                New text of the message.\n\n            parse_mode (``str``, *optional*):\n                Use :obj:`MARKDOWN <pyrogram.ParseMode.MARKDOWN>` or :obj:`HTML <pyrogram.ParseMode.HTML>`\n                if you want Telegram apps to show bold, italic, fixed-width text or inline URLs in your message.\n                Defaults to Markdown.\n\n            disable_web_page_preview (``bool``, *optional*):\n                Disables link previews for links in this message.\n\n            reply_markup (:obj:`InlineKeyboardMarkup`, *optional*):\n                An InlineKeyboardMarkup object.\n\n        Returns:\n            On success, the edited :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        return self._client.edit_message_text(\n            chat_id=self.chat.id,\n            message_id=self.message_id,\n            text=text,\n            parse_mode=parse_mode,\n            disable_web_page_preview=disable_web_page_preview,\n            reply_markup=reply_markup\n        )",
        "rewrite": "```python\ndef edit(\n    self,\n    text: str,\n    parse_mode: str = \"Markdown\",\n    disable_web_page_preview: bool = None,\n    reply_markup: Union[\n        \"pyrogram.InlineKeyboardMarkup\",\n        \"pyrogram.ReplyKeyboardMarkup\",\n        \"pyrogram.ReplyKeyboardRemove\",\n        \"pyrogram.ForceReply\n    ] = None\n) -> \"Message\":\n    return self._client.edit_message_text(\n        chat_id=self.chat.id,\n        message_id=self.message_id,\n        text=text,\n        parse_mode=parse_mode,\n        disable_web_page_preview=disable_web_page_preview"
    },
    {
        "original": "def _resize_with_dtype(arr, dtype):\n    \"\"\"\n    This function will transform arr into an array with the same type as dtype. It will do this by\n    filling new columns with zeros (or NaNs, if it is a float column). Also, columns that are not\n    in the new dtype will be dropped.\n    \"\"\"\n    structured_arrays = dtype.names is not None and arr.dtype.names is not None\n    old_columns = arr.dtype.names or []\n    new_columns = dtype.names or []\n\n    # In numpy 1.9 the ndarray.astype method used to handle changes in number of fields. The code below\n    # should replicate the same behaviour the old astype used to have.\n    #\n    # One may be tempted to use np.lib.recfunctions.stack_arrays to implement both this step and the\n    # concatenate that follows but it 2x slower and it requires providing your own default values (instead\n    # of np.zeros).\n    #\n    # Numpy 1.14 supports doing new_arr[old_columns] = arr[old_columns], which is faster than the code below\n    # (in benchmarks it seems to be even slightly faster than using the old astype). However, that is not\n    # supported by numpy 1.9.2.\n    if structured_arrays and (old_columns != new_columns):\n        old_columns = set(old_columns)\n        new_columns = set(new_columns)\n\n        new_arr = np.zeros(arr.shape, dtype)\n        for c in old_columns & new_columns:\n            new_arr[c] = arr[c]\n\n        # missing float columns should default to nan rather than zero\n        _is_float_type = lambda _dtype: _dtype.type in (np.float32, np.float64)\n        _is_void_float_type = lambda _dtype: _dtype.type == np.void and _is_float_type(_dtype.subdtype[0])\n        _is_float_or_void_float_type = lambda _dtype: _is_float_type(_dtype) or _is_void_float_type(_dtype)\n        _is_float = lambda column: _is_float_or_void_float_type(dtype.fields[column][0])\n        for new_column in filter(_is_float, new_columns - old_columns):\n            new_arr[new_column] = np.nan\n\n        return new_arr.astype(dtype)\n    else:\n        return arr.astype(dtype)",
        "rewrite": "```python\nimport numpy as np\n\ndef _resize_with_dtype(arr, dtype):\n    structured_arrays = dtype.names is not None and arr.dtype.names is not None\n    old_columns = arr.dtype.names or []\n    new_columns = dtype.names or []\n\n    if structured_arrays and (old_columns != new_columns):\n        old_columns = set(old_columns)\n        new_columns = set(new_columns)\n\n        new_arr = np.zeros(arr.shape, dtype)\n        for c in old_columns & new_columns:\n            new_arr[c] = arr[c]\n\n        for new_column in filter(lambda column: dtype.fields[column][0].type in (np"
    },
    {
        "original": "def count_special_chars(self, char, prev_char):\n        \"\"\"Update parenthesis counters\"\"\"\n        if char == '(':\n            self.parenthesis_count += 1\n        elif char == ')':\n            self.parenthesis_count -= 1\n            if self.parenthesis_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing parentheses'.format(self.line))\n        elif char == '{':\n            self.curly_bracket_count += 1\n        elif char == '}':\n            self.curly_bracket_count -= 1\n            if self.curly_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing curly brackets'.format(self.line))\n        elif char == '[':\n            self.square_bracket_count += 1\n        elif char == ']':\n            self.square_bracket_count -= 1\n            if self.square_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing square brackets'.format(self.line))\n        elif char == \"'\" and prev_char != '\\\\':\n            self.in_single_quote = not self.in_single_quote\n        elif char == '\"' and prev_char != '\\\\':\n            self.in_double_quote = not self.in_double_quote",
        "rewrite": "```python\ndef count_special_chars(self, char, prev_char):\n    \"\"\"Update parenthesis counters\"\"\"\n    if char == '(':\n        self.parenthesis_count += 1\n    elif char == ')':\n        self.parenthesis_count -= 1\n        if self.parenthesis_count < 0:\n            raise RMarkdownOptionParsingError(\n                f'Option line \"{self.line}\" has too many closing parentheses')\n    elif char == '{':\n        self.curly_bracket_count += 1\n    elif char == '}':\n        self.curly_bracket_count -= 1\n        if self.curly_bracket_count"
    },
    {
        "original": "def get_pullrequest(self, project, repository, pull_request_id):\n        \"\"\"\n        Retrieve a pull request.\n        The authenticated user must have REPO_READ permission\n        for the repository that this pull request targets to call this resource.\n        :param project:\n        :param repository:\n        :param pull_request_id: the ID of the pull request within the repository\n        :return:\n        \"\"\"\n        url = 'rest/api/1.0/projects/{project}/repos/{repository}/pull-requests/{pullRequestId}'.format(project=project,\n                                                                                                        repository=repository,\n                                                                                                        pullRequestId=pull_request_id)\n        return self.get(url)",
        "rewrite": "```python\ndef get_pull_request(self, project, repository, pull_request_id):\n    url = f'rest/api/1.0/projects/{project}/repos/{repository}/pull-requests/{pull_request_id}'\n    return self.get(url)\n```"
    },
    {
        "original": "def _get_slave_status(self, db, above_560, nonblocking):\n        \"\"\"\n        Retrieve the slaves' statuses using:\n        1. The `performance_schema.threads` table. Non-blocking, requires version > 5.6.0\n        2. The `information_schema.processlist` table. Blocking\n        \"\"\"\n        try:\n            with closing(db.cursor()) as cursor:\n                if above_560 and nonblocking:\n                    # Query `performance_schema.threads` instead of `\n                    # information_schema.processlist` to avoid mutex impact on performance.\n                    cursor.execute(\"SELECT THREAD_ID, NAME FROM performance_schema.threads WHERE NAME LIKE '%worker'\")\n                else:\n                    cursor.execute(\"SELECT * FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND LIKE '%Binlog dump%'\")\n                slave_results = cursor.fetchall()\n                slaves = 0\n                for _ in slave_results:\n                    slaves += 1\n\n                return {'Slaves_connected': slaves}\n\n        except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:\n            self.warning(\"Privileges error accessing the process tables (must grant PROCESS): %s\" % str(e))\n            return {}",
        "rewrite": "```python\ndef _get_slave_status(self, db, above_560, nonblocking):\n    try:\n        with closing(db.cursor()) as cursor:\n            if above_560 and nonblocking:\n                cursor.execute(\"SELECT THREAD_ID, NAME FROM performance_schema.threads WHERE NAME LIKE '%worker'\")\n            else:\n                cursor.execute(\"SELECT * FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND LIKE '%Binlog dump%'\")\n            slave_results = cursor.fetchall()\n            slaves = len(slave_results)\n\n            return {'Slaves_connected': slaves}\n\n    except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:\n"
    },
    {
        "original": "def _parse_user_flags():\n    \"\"\"\n    Parses user-flags file and loads it to register user defined options.\n    \"\"\"\n    try:\n        idx = list(sys.argv).index('--user-flags')\n        user_flags_file = sys.argv[idx + 1]\n    except (ValueError, IndexError):\n        user_flags_file = ''\n\n    if user_flags_file and os.path.isfile(user_flags_file):\n        from ryu.utils import _import_module_file\n        _import_module_file(user_flags_file)",
        "rewrite": "```python\nimport sys\nimport os\n\ndef _parse_user_flags():\n    try:\n        idx = next(i for i, arg in enumerate(sys.argv) if arg == '--user-flags')\n        user_flags_file = sys.argv[idx + 1]\n    except (ValueError, IndexError):\n        user_flags_file = ''\n\n    if user_flags_file and os.path.isfile(user_flags_file):\n        from ryu.utils import _import_module_file\n        _import_module_file(user_flags_file)\n```"
    },
    {
        "original": "def update_schemas(self, schemas):\n        \"\"\"Add multiple schemas to the set of known schemas (case-insensitive)\n\n        :param Iterable[str] schemas: An iterable of the schema names to add.\n        \"\"\"\n        self.schemas.update((_lower(d), _lower(s)) for (d, s) in schemas)",
        "rewrite": "```python\ndef update_schemas(self, schemas):\n    self.schemas.update((d.lower(), s.lower()) for d, s in schemas)\n```"
    },
    {
        "original": "def least_loaded_node(self):\n        \"\"\"Choose the node with fewest outstanding requests, with fallbacks.\n\n        This method will prefer a node with an existing connection and no\n        in-flight-requests. If no such node is found, a node will be chosen\n        randomly from disconnected nodes that are not \"blacked out\" (i.e.,\n        are not subject to a reconnect backoff). If no node metadata has been\n        obtained, will return a bootstrap node (subject to exponential backoff).\n\n        Returns:\n            node_id or None if no suitable node was found\n        \"\"\"\n        nodes = [broker.nodeId for broker in self.cluster.brokers()]\n        random.shuffle(nodes)\n\n        inflight = float('inf')\n        found = None\n        for node_id in nodes:\n            conn = self._conns.get(node_id)\n            connected = conn is not None and conn.connected()\n            blacked_out = conn is not None and conn.blacked_out()\n            curr_inflight = len(conn.in_flight_requests) if conn is not None else 0\n            if connected and curr_inflight == 0:\n                # if we find an established connection\n                # with no in-flight requests, we can stop right away\n                return node_id\n            elif not blacked_out and curr_inflight < inflight:\n                # otherwise if this is the best we have found so far, record that\n                inflight = curr_inflight\n                found = node_id\n\n        if found is not None:\n            return found\n\n        return None",
        "rewrite": "```python\ndef least_loaded_node(self):\n    nodes = [broker.nodeId for broker in self.cluster.brokers()]\n    random.shuffle(nodes)\n\n    inflight = float('inf')\n    found = None\n    for node_id in nodes:\n        conn = self._conns.get(node_id)\n        connected = conn is not None and conn.connected()\n        blacked_out = conn is not None and conn.blacked_out()\n        curr_inflight = len(conn.in_flight_requests) if conn is not None else 0\n        if connected and curr_inflight == 0:\n            return node_id\n        elif not blacked"
    },
    {
        "original": "def _retry_on_connection_error(func: Callable) -> Callable:\n    \"\"\"Decorator to retry the function max_connection_attemps number of times.\n\n    Herewith-decorated functions need an ``_attempt`` keyword argument.\n\n    This is to decorate functions that do network requests that may fail. Note that\n    :meth:`.get_json`, :meth:`.get_iphone_json`, :meth:`.graphql_query` and :meth:`.graphql_node_list` already have\n    their own logic for retrying, hence functions that only use these for network access must not be decorated with this\n    decorator.\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        try:\n            return func(instaloader, *args, **kwargs)\n        except (urllib3.exceptions.HTTPError, requests.exceptions.RequestException, ConnectionException) as err:\n            error_string = \"{}({}): {}\".format(func.__name__, ', '.join([repr(arg) for arg in args]), err)\n            if (kwargs.get('_attempt') or 1) == instaloader.context.max_connection_attempts:\n                raise ConnectionException(error_string) from None\n            instaloader.context.error(error_string + \" [retrying; skip with ^C]\", repeat_at_end=False)\n            try:\n                if kwargs.get('_attempt'):\n                    kwargs['_attempt'] += 1\n                else:\n                    kwargs['_attempt'] = 2\n                instaloader.context.do_sleep()\n                return call(instaloader, *args, **kwargs)\n            except KeyboardInterrupt:\n                instaloader.context.error(\"[skipped by user]\", repeat_at_end=False)\n                raise ConnectionException(error_string) from None\n    return call",
        "rewrite": "```python\nfrom functools import wraps\nfrom typing import Callable\nfrom urllib3.exceptions import HTTPError\nfrom requests.exceptions import RequestException\nfrom contextlib import suppress\n\ndef _retry_on_connection_error(func: Callable) -> Callable:\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        attempt = kwargs.get('_attempt') or 1\n        if attempt > instaloader.context.max_connection_attempts:\n            raise ConnectionException(f\"{func.__name__}({', '.join([repr(arg) for arg in args])}): {kwargs.get('error')}\")\n        \n        with suppress(Keyboard"
    },
    {
        "original": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Concatenates values on all nodes with requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        dimension = self.get_dimension(dimension, strict=True).name\n        all_dims = self.traverse(lambda x: [d.name for d in x.dimensions()])\n        if dimension in chain.from_iterable(all_dims):\n            values = [el.dimension_values(dimension) for el in self\n                      if dimension in el.dimensions(label=True)]\n            vals = np.concatenate(values)\n            return vals if expanded else util.unique_array(vals)\n        else:\n            return super(ViewableTree, self).dimension_values(\n                dimension, expanded, flat)",
        "rewrite": "```python\ndef dimension_values(self, dimension, expanded=True, flat=True):\n    dimension = self.get_dimension(dimension, strict=True).name\n    all_dims = self.traverse(lambda x: [d.name for d in x.dimensions()])\n    if dimension in chain.from_iterable(all_dims):\n        values = [el.dimension_values(dimension) for el in self\n                  if dimension in el.dimensions(label=True)]\n        vals = np.concatenate(values)\n        return vals if expanded else np.unique(vals)\n    else:\n        return super(ViewableTree, self).dimension_values(dimension, expanded, flat)\n```"
    },
    {
        "original": "def get_stat_name(self, header=\"\"):\n        \"\"\"\"Return the stat name with an optional header\"\"\"\n        ret = self.plugin_name\n        if header != \"\":\n            ret += '_' + header\n        return ret",
        "rewrite": "```python\ndef get_stat_name(self, header=\"\"):\n    return f\"{self.plugin_name}_{header}\" if header else self.plugin_name\n```"
    },
    {
        "original": "def compute_density(start, end, length, time_unit='us'):\n    \"\"\"\n    Computes a grid density given the edges and number of samples.\n    Handles datetime grids correctly by computing timedeltas and\n    computing a density for the given time_unit.\n    \"\"\"\n    if isinstance(start, int): start = float(start)\n    if isinstance(end, int): end = float(end)\n    diff = end-start\n    if isinstance(diff, timedelta_types):\n        if isinstance(diff, np.timedelta64):\n            diff = np.timedelta64(diff, time_unit).tolist()\n        tscale = 1./np.timedelta64(1, time_unit).tolist().total_seconds()\n        return (length/(diff.total_seconds()*tscale))\n    else:\n        return length/diff",
        "rewrite": "```python\nimport numpy as np\nfrom datetime import timedelta\n\ndef compute_density(start, end, length, time_unit='us'):\n    if isinstance(start, int): start = float(start)\n    if isinstance(end, int): end = float(end)\n    diff = end - start\n    if isinstance(diff, timedelta):\n        tscale = 1 / (diff.total_seconds() / float(getattr(np.timedelta64(1), time_unit)))\n        return length / (diff.total_seconds() * tscale)\n    else:\n        return length / diff\n```"
    },
    {
        "original": "def CreateAndStartHunt(flow_name, flow_args, creator, **kwargs):\n  \"\"\"Creates and starts a new hunt.\"\"\"\n\n  # This interface takes a time when the hunt expires. However, the legacy hunt\n  # starting interface took an rdfvalue.Duration object which was then added to\n  # the current time to get the expiry. This check exists to make sure we don't\n  # confuse the two.\n  if \"duration\" in kwargs:\n    precondition.AssertType(kwargs[\"duration\"], rdfvalue.Duration)\n\n  hunt_args = rdf_hunt_objects.HuntArguments(\n      hunt_type=rdf_hunt_objects.HuntArguments.HuntType.STANDARD,\n      standard=rdf_hunt_objects.HuntArgumentsStandard(\n          flow_name=flow_name, flow_args=flow_args))\n\n  hunt_obj = rdf_hunt_objects.Hunt(\n      creator=creator,\n      args=hunt_args,\n      create_time=rdfvalue.RDFDatetime.Now(),\n      **kwargs)\n\n  CreateHunt(hunt_obj)\n  StartHunt(hunt_obj.hunt_id)\n\n  return hunt_obj.hunt_id",
        "rewrite": "```python\ndef create_and_start_hunt(flow_name, flow_args, creator, **kwargs):\n    if \"duration\" in kwargs:\n        precondition.assert_type(kwargs[\"duration\"], rdfvalue.Duration)\n\n    hunt_args = rdf_hunt_objects.HuntArguments(\n        hunt_type=rdf_hunt_objects.HuntArguments.HuntType.STANDARD,\n        standard=rdf_hunt_objects.HuntArgumentsStandard(\n            flow_name=flow_name, flow_args=flow_args))\n\n    hunt_obj = rdf_hunt_objects.Hunt(\n        creator=creator,\n        args=hunt_args,\n        create_time=rdfvalue.RDFDatetime.Now(),\n       "
    },
    {
        "original": "def get_lattice_quanta(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n        \"\"\"\n        Returns the dipole / polarization quanta along a, b, and c for\n        all structures.\n        \"\"\"\n        lattices = [s.lattice for s in self.structures]\n        volumes = np.array([s.lattice.volume for s in self.structures])\n\n        L = len(self.structures)\n\n        e_to_muC = -1.6021766e-13\n        cm2_to_A2 = 1e16\n        units = 1.0 / np.array(volumes)\n        units *= e_to_muC * cm2_to_A2\n\n        # convert polarizations and lattice lengths prior to adjustment\n        if convert_to_muC_per_cm2 and not all_in_polar:\n            # adjust lattices\n            for i in range(L):\n                lattice = lattices[i]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[i], a)\n        elif convert_to_muC_per_cm2 and all_in_polar:\n            for i in range(L):\n                lattice = lattices[-1]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[-1], a)\n\n        quanta = np.array(\n            [np.array(l.lengths_and_angles[0]) for l in lattices])\n\n        return quanta",
        "rewrite": "```python\ndef get_lattice_quanta(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n    lattices = [s.lattice for s in self.structures]\n    volumes = np.array([s.lattice.volume for s in self.structures])\n\n    e_to_muC = -1.6021766e-13\n    cm2_to_A2 = 1e16\n    units = 1.0 / np.array(volumes)\n    units *= e_to_muC * cm2_to_A2\n\n    L = len(self.structures)\n\n    if convert_to_muC"
    },
    {
        "original": "def delete_account(self, account):\n        \"\"\"\n        \u5220\u9664\u5ba2\u670d\u8d26\u53f7\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/1/70a29afed17f56d537c833f89be979c9.html\n\n        :param account: \u5b8c\u6574\u5ba2\u670d\u8d26\u53f7\uff0c\u683c\u5f0f\u4e3a\uff1a\u8d26\u53f7\u524d\u7f00@\u516c\u4f17\u53f7\u5fae\u4fe1\u53f7\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        params_data = [\n            'access_token={0}'.format(quote(self.access_token)),\n            'kf_account={0}'.format(quote(to_binary(account), safe=b'/@')),\n        ]\n        params = '&'.join(params_data)\n        return self._get(\n            'https://api.weixin.qq.com/customservice/kfaccount/del',\n            params=params\n        )",
        "rewrite": "```python\ndef delete_account(self, account):\n    params_data = {\n        'access_token': self.access_token,\n        'kf_account': to_binary(account, safe=b'/@')\n    }\n    return self._get('https://api.weixin.qq.com/customservice/kfaccount/del', params=params_data)\n```"
    },
    {
        "original": "def find_sources(self, var_def, simplified_graph=True):\n        \"\"\"\n        Find all sources to the specified variable definition.\n\n        :param ProgramVariable var_def: The variable definition.\n        :param bool simplified_graph: True if we want to search in the simplified graph, False otherwise.\n        :return: A collection of all sources to the specified variable definition.\n        :rtype: list\n        \"\"\"\n\n        if simplified_graph:\n            graph = self.simplified_data_graph\n        else:\n            graph = self.data_graph\n\n        if var_def not in graph:\n            return []\n\n        sources = []\n        defs = [ var_def ]\n        traversed = set()\n\n        while defs:\n            definition = defs.pop()\n            in_edges = graph.in_edges(definition, data=True)\n            for src, _, data in in_edges:\n                if 'type' in data and data['type'] == 'kill':\n                    continue\n                if isinstance(src.variable, SimTemporaryVariable):\n                    if src not in traversed:\n                        defs.append(src)\n                        traversed.add(src)\n                else:\n                    if src not in sources:\n                        sources.append(src)\n\n        return sources",
        "rewrite": "```python\ndef find_sources(self, var_def, simplified_graph=True):\n    if simplified_graph:\n        graph = self.simplified_data_graph\n    else:\n        graph = self.data_graph\n\n    if var_def not in graph:\n        return []\n\n    sources = []\n    defs = [var_def]\n    traversed = set()\n\n    while defs:\n        definition = defs.pop()\n        in_edges = graph.in_edges(definition, data=True)\n        for src, _, data in in_edges:\n            if 'type' in data and data['type'] == 'kill':\n                continue\n            if isinstance(src.variable, SimTemporaryVariable"
    },
    {
        "original": "def resolve(self, key):\n        \"\"\"Looks up a variable like `__getitem__` or `get` but returns an\n        :class:`Undefined` object with the name of the name looked up.\n        \"\"\"\n        if key in self.vars:\n            return self.vars[key]\n        if key in self.parent:\n            return self.parent[key]\n        return self.environment.undefined(name=key)",
        "rewrite": "```python\ndef resolve(self, key):\n    return self.vars.get(key, self.parent.get(key, self.environment.undefined(name=key)))\n```"
    },
    {
        "original": "def inspect(self, nids=None, wslice=None, **kwargs):\n        \"\"\"\n        Inspect the tasks (SCF iterations, Structural relaxation ...) and\n        produces matplotlib plots.\n\n        Args:\n            nids: List of node identifiers.\n            wslice: Slice object used to select works.\n            kwargs: keyword arguments passed to `task.inspect` method.\n\n        .. note::\n\n            nids and wslice are mutually exclusive.\n            If nids and wslice are both None, all tasks in self are inspected.\n\n        Returns:\n            List of `matplotlib` figures.\n        \"\"\"\n        figs = []\n        for task in self.select_tasks(nids=nids, wslice=wslice):\n            if hasattr(task, \"inspect\"):\n                fig = task.inspect(**kwargs)\n                if fig is None:\n                    cprint(\"Cannot inspect Task %s\" % task, color=\"blue\")\n                else:\n                    figs.append(fig)\n            else:\n                cprint(\"Task %s does not provide an inspect method\" % task, color=\"blue\")\n\n        return figs",
        "rewrite": "```python\ndef inspect(self, nids=None, wslice=None, **kwargs):\n    \"\"\"\n    Inspect the tasks (SCF iterations, Structural relaxation ...) and\n    produces matplotlib plots.\n\n    Args:\n        nids: List of node identifiers.\n        wslice: Slice object used to select works.\n        kwargs: keyword arguments passed to `task.inspect` method.\n\n    Returns:\n        List of `matplotlib` figures.\n    \"\"\"\n    figs = []\n    for task in self.select_tasks(nids=nids, wslice=wslice):\n        if hasattr(task, \"inspect\"):\n            fig = task.inspect(**kwargs)\n"
    },
    {
        "original": "def _compute_rtfilter_map(self):\n        \"\"\"Returns neighbor's RT filter (permit/allow filter based on RT).\n\n        Walks RT filter tree and computes current RT filters for each peer that\n        have advertised RT NLRIs.\n        Returns:\n            dict of peer, and `set` of rts that a particular neighbor is\n            interested in.\n        \"\"\"\n        rtfilter_map = {}\n\n        def get_neigh_filter(neigh):\n            neigh_filter = rtfilter_map.get(neigh)\n            # Lazy creation of neighbor RT filter\n            if neigh_filter is None:\n                neigh_filter = set()\n                rtfilter_map[neigh] = neigh_filter\n            return neigh_filter\n\n        # Check if we have to use all paths or just best path\n        if self._common_config.max_path_ext_rtfilter_all:\n            # We have to look at all paths for a RtDest\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                known_path_list = rtcdest.known_path_list\n                for path in known_path_list:\n                    neigh = path.source\n\n                    # We ignore NC\n                    if neigh is None:\n                        continue\n\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n        else:\n            # We iterate over all destination of the RTC table and for iBGP\n            # peers we use all known paths' RTs for RT filter and for eBGP\n            # peers we only consider best-paths' RTs for RT filter\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                path = rtcdest.best_path\n                # If this destination does not have any path, we continue\n                if not path:\n                    continue\n\n                neigh = path.source\n                # Consider only eBGP peers and ignore NC\n                if neigh and neigh.is_ebgp_peer():\n                    # For eBGP peers we use only best-path to learn RT filter\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n                else:\n                    # For iBGP peers we use all known paths to learn RT filter\n                    known_path_list = rtcdest.known_path_list\n                    for path in known_path_list:\n                        neigh = path.source\n                        # We ignore NC, and eBGP peers\n                        if neigh and not neigh.is_ebgp_peer():\n                            neigh_filter = get_neigh_filter(neigh)\n                            neigh_filter.add(path.nlri.route_target)\n\n        return rtfilter_map",
        "rewrite": "```python\ndef _compute_rtfilter_map(self):\n    \"\"\"Returns neighbor's RT filter (permit/allow filter based on RT).\"\"\"\n    rtfilter_map = {}\n\n    def get_neigh_filter(neigh):\n        return rtfilter_map.setdefault(neigh, set())\n\n    if self._common_config.max_path_ext_rtfilter_all:\n        for rtcdest in self._table_manager.get_rtc_table().values():\n            known_path_list = rtcdest.known_path_list\n            for path in known_path_list:\n                neigh = path.source\n                if neigh is not None:\n                    get_neigh_filter(neigh).add(path.nlri"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "rewrite": "```python\ndef create(cls, tx_signers, recipients, metadata=None, asset=None):\n    inputs, outputs = cls.validate_create(tx_signers, recipients, asset, metadata)\n    return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)\n```"
    },
    {
        "original": "def custom_build_class_rule(self, opname, i, token, tokens, customize):\n        \"\"\"\n        # Should the first rule be somehow folded into the 2nd one?\n        build_class ::= LOAD_BUILD_CLASS mkfunc\n                        LOAD_CLASSNAME {expr}^n-1 CALL_FUNCTION_n\n                        LOAD_CONST CALL_FUNCTION_n\n        build_class ::= LOAD_BUILD_CLASS mkfunc\n                        expr\n                        call\n                        CALL_FUNCTION_3\n         \"\"\"\n        # FIXME: I bet this can be simplified\n        # look for next MAKE_FUNCTION\n        for i in range(i+1, len(tokens)):\n            if tokens[i].kind.startswith('MAKE_FUNCTION'):\n                break\n            elif tokens[i].kind.startswith('MAKE_CLOSURE'):\n                break\n            pass\n        assert i < len(tokens), \"build_class needs to find MAKE_FUNCTION or MAKE_CLOSURE\"\n        assert tokens[i+1].kind == 'LOAD_CONST', \\\n          \"build_class expecting CONST after MAKE_FUNCTION/MAKE_CLOSURE\"\n        call_fn_tok = None\n        for i in range(i, len(tokens)):\n            if tokens[i].kind.startswith('CALL_FUNCTION'):\n                call_fn_tok = tokens[i]\n                break\n        if not call_fn_tok:\n            raise RuntimeError(\"build_class custom rule for %s needs to find CALL_FUNCTION\"\n                               % opname)\n\n\n        # customize build_class rule\n        # FIXME: What's the deal with the two rules? Different Python versions?\n        # Different situations? Note that the above rule is based on the CALL_FUNCTION\n        # token found, while this one doesn't.\n        if self.version < 3.6:\n            call_function = self.call_fn_name(call_fn_tok)\n            args_pos, args_kw = self.get_pos_kw(call_fn_tok)\n            rule = (\"build_class ::= LOAD_BUILD_CLASS mkfunc %s\"\n                        \"%s\" % (('expr ' * (args_pos - 1) + ('kwarg ' * args_kw)),\n                                    call_function))\n        else:\n            # 3.6+ handling\n            call_function = call_fn_tok.kind\n            if call_function.startswith(\"CALL_FUNCTION_KW\"):\n                self.addRule(\"classdef ::= build_class_kw store\", nop_func)\n                rule = (\"build_class_kw ::= LOAD_BUILD_CLASS mkfunc %sLOAD_CONST %s\"\n                        % ('expr ' * (call_fn_tok.attr - 1), call_function))\n            else:\n                call_function = self.call_fn_name(call_fn_tok)\n                rule = (\"build_class ::= LOAD_BUILD_CLASS mkfunc %s%s\"\n                         % ('expr ' * (call_fn_tok.attr - 1), call_function))\n        self.addRule(rule, nop_func)\n        return",
        "rewrite": "```python\ndef custom_build_class_rule(self, opname, i, token, tokens, customize):\n    \"\"\"\n    Custom rule for build_class operation.\n    \"\"\"\n    # Find the next MAKE_FUNCTION or MAKE_CLOSURE\n    for j in range(i + 1, len(tokens)):\n        if tokens[j].kind.startswith('MAKE_FUNCTION') or tokens[j].kind.startswith('MAKE_CLOSURE'):\n            break\n    else:\n        raise AssertionError(\"build_class needs to find MAKE_FUNCTION or MAKE_CLOSURE\")\n\n    # Check if the next token is LOAD_CONST\n    if tokens[j + 1].kind != 'LOAD"
    },
    {
        "original": "def create(self, name):\n        \"\"\"\n        \u521b\u5efa\u6807\u7b7e\n\n        :param name: \u6807\u7b7e\u540d\uff0830\u4e2a\u5b57\u7b26\u4ee5\u5185\uff09\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n        \"\"\"\n        name = to_text(name)\n        return self._post(\n            'tags/create',\n            data={'tag': {'name': name}},\n            result_processor=lambda x: x['tag']\n        )",
        "rewrite": "```python\ndef create(self, name):\n    name = to_text(name)\n    return self._post('tags/create', data={'tag': {'name': name}}, result_processor=lambda x: x['tag'])\n```"
    },
    {
        "original": "async def packets_from_tshark(self, packet_callback, packet_count=None, close_tshark=True):\n        \"\"\"\n        A coroutine which creates a tshark process, runs the given callback on each packet that is received from it and\n        closes the process when it is done.\n\n        Do not use interactively. Can be used in order to insert packets into your own eventloop.\n        \"\"\"\n        tshark_process = await self._get_tshark_process(packet_count=packet_count)\n        try:\n            await self._go_through_packets_from_fd(tshark_process.stdout, packet_callback, packet_count=packet_count)\n        except StopCapture:\n            pass\n        finally:\n            if close_tshark:\n                await self._close_async()",
        "rewrite": "```python\nasync def packets_from_tshark(self, packet_callback, packet_count=None, close_tshark=True):\n    tshark_process = await self._get_tshark_process(packet_count=packet_count)\n    try:\n        await self._go_through_packets_from_fd(tshark_process.stdout, packet_callback, packet_count=packet_count)\n    except StopCapture:\n        pass\n    finally:\n        if close_tshark:\n            await self._close_async(tshark_process)\n```"
    },
    {
        "original": "def publish_alias(self, func_data, alias):\n        \"\"\"Create or update an alias for the given function.\n        \"\"\"\n        if not alias:\n            return func_data['FunctionArn']\n        func_name = func_data['FunctionName']\n        func_version = func_data['Version']\n\n        exists = resource_exists(\n            self.client.get_alias, FunctionName=func_name, Name=alias)\n\n        if not exists:\n            log.debug(\"Publishing custodian lambda alias %s\", alias)\n            alias_result = self.client.create_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        else:\n            if (exists['FunctionVersion'] == func_version and\n                    exists['Name'] == alias):\n                return exists['AliasArn']\n            log.debug('Updating custodian lambda alias %s', alias)\n            alias_result = self.client.update_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        return alias_result['AliasArn']",
        "rewrite": "```python\ndef publish_alias(self, func_data, alias):\n    if not alias:\n        return func_data['FunctionArn']\n    func_name = func_data['FunctionName']\n    func_version = func_data['Version']\n\n    exists = resource_exists(\n        self.client.get_alias, FunctionName=func_name, Name=alias)\n\n    if not exists:\n        log.debug(\"Publishing custodian lambda alias %s\", alias)\n        alias_result = self.client.create_alias(\n            FunctionName=func_name,\n            Name=alias,\n            FunctionVersion=func_version)\n    else:\n        if exists['FunctionVersion'] == func_version"
    },
    {
        "original": "def _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    count = 0\n    log.debug('Waiting for asynchronous operation to complete')\n    result = conn.get_operation_status(request_id)\n    while result.status == 'InProgress':\n        count = count + 1\n        if count > 120:\n            raise ValueError('Timed out waiting for asynchronous operation to complete.')\n        time.sleep(5)\n        result = conn.get_operation_status(request_id)\n\n    if result.status != 'Succeeded':\n        raise AzureException('Operation failed. {message} ({code})'\n                                .format(message=result.error.message,\n                                        code=result.error.code))",
        "rewrite": "```python\ndef _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    count = 0\n    log.debug('Waiting for asynchronous operation to complete')\n    \n    while True:\n        result = conn.get_operation_status(request_id)\n        if result.status == 'Succeeded':\n            return result\n        elif result.status == 'InProgress':\n            count += 1\n            if count > 120:\n                raise ValueError('Timed out waiting for asynchronous operation to complete.')\n            time.sleep(5)\n        else:\n            raise AzureException(f'Operation failed. {result.error.message} ({"
    },
    {
        "original": "def _FlushAllRows(self, db_connection, table_name):\n    \"\"\"Copies rows from the given db into the output file then deletes them.\"\"\"\n    for sql in db_connection.iterdump():\n      if (sql.startswith(\"CREATE TABLE\") or\n          sql.startswith(\"BEGIN TRANSACTION\") or sql.startswith(\"COMMIT\")):\n        # These statements only need to be written once.\n        continue\n      # The archive generator expects strings (not Unicode objects returned by\n      # the pysqlite library).\n      yield self.archive_generator.WriteFileChunk((sql + \"\\n\").encode(\"utf-8\"))\n    with db_connection:\n      db_connection.cursor().execute(\"DELETE FROM \\\"%s\\\";\" % table_name)",
        "rewrite": "```python\ndef _FlushAllRows(self, db_connection, table_name):\n    for sql in db_connection.iterdump():\n        if sql.startswith((\"CREATE TABLE\", \"BEGIN TRANSACTION\", \"COMMIT\")):\n            continue\n        yield self.archive_generator.WriteFileChunk((sql + \"\\n\").encode(\"utf-8\"))\n    with db_connection:\n        db_connection.cursor().execute(f\"DELETE FROM \\\"{table_name}\\\"\")\n```"
    },
    {
        "original": "def _create_empty_run(\n        self, status=RunStatus.FINISHED, status_description=None\n    ) -> Run:\n        \"\"\"setting boilerplate when creating a Run object\"\"\"\n        run = Run(\n            job_id=self.summary[\"job_id\"],\n            issue_instances=[],\n            date=datetime.datetime.now(),\n            status=status,\n            status_description=status_description,\n            repository=self.summary[\"repository\"],\n            branch=self.summary[\"branch\"],\n            commit_hash=self.summary[\"commit_hash\"],\n            kind=self.summary[\"run_kind\"],\n        )\n        return run",
        "rewrite": "```python\ndef _create_empty_run(\n    self, status: RunStatus = RunStatus.FINISHED, status_description: str = None\n) -> Run:\n    run = Run(\n        job_id=self.summary[\"job_id\"],\n        issue_instances=[],\n        date=datetime.datetime.now(),\n        status=status,\n        status_description=status_description,\n        repository=self.summary[\"repository\"],\n        branch=self.summary[\"branch\"],\n        commit_hash=self.summary[\"commit_hash\"],\n        kind=self.summary[\"run_kind\"]\n    )\n    return run\n```"
    },
    {
        "original": "def cell_complete(self, cell, cell_index=None, **kwargs):\n        \"\"\"\n        Finalize metadata for a cell and save notebook.\n\n        Optionally called by engines during execution to finalize the\n        metadata for a cell and save the notebook to the output path.\n        \"\"\"\n        end_time = self.now()\n\n        if self.log_output:\n            ceel_num = cell_index + 1 if cell_index is not None else ''\n            logger.info('Ending Cell {:-<43}'.format(ceel_num))\n            # Ensure our last cell messages are not buffered by python\n            sys.stdout.flush()\n            sys.stderr.flush()\n\n        cell.metadata.papermill['end_time'] = end_time.isoformat()\n        if cell.metadata.papermill.get('start_time'):\n            start_time = dateutil.parser.parse(cell.metadata.papermill['start_time'])\n            cell.metadata.papermill['duration'] = (end_time - start_time).total_seconds()\n        if cell.metadata.papermill['status'] != self.FAILED:\n            cell.metadata.papermill['status'] = self.COMPLETED\n\n        self.save()\n        if self.pbar:\n            self.pbar.update(1)",
        "rewrite": "```python\ndef cell_complete(self, cell, cell_index=None, **kwargs):\n    end_time = self.now()\n\n    if self.log_output:\n        ceel_num = cell_index + 1 if cell_index is not None else ''\n        logger.info(f'Ending Cell {ceel_num:<43}')\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n    cell.metadata.papermill['end_time'] = end_time.isoformat()\n    if 'start_time' in cell.metadata.papermill:\n        start_time = dateutil.parser.parse(cell.metadata.papermill['start_time'])\n        cell.metadata.papermill['duration'] = (end_time -"
    },
    {
        "original": "def from_ops(*operations: ops.OP_TREE,\n                 strategy: InsertStrategy = InsertStrategy.EARLIEST,\n                 device: devices.Device = devices.UnconstrainedDevice\n                 ) -> 'Circuit':\n        \"\"\"Creates an empty circuit and appends the given operations.\n\n        Args:\n            operations: The operations to append to the new circuit.\n            strategy: How to append the operations.\n            device: Hardware that the circuit should be able to run on.\n\n        Returns:\n            The constructed circuit containing the operations.\n        \"\"\"\n        result = Circuit(device=device)\n        result.append(operations, strategy)\n        return result",
        "rewrite": "```python\ndef from_ops(*operations: ops.OP_TREE,\n             strategy: InsertStrategy = InsertStrategy.EARLIEST,\n             device: devices.Device = devices.UnconstrainedDevice) -> 'Circuit':\n    return Circuit(device=device).append(operations, strategy)\n```"
    },
    {
        "original": "def get_extended_surface_mesh(self, repeat=(5, 5, 1)):\n        \"\"\"\n        Gets an extended surface mesh for to use for adsorption\n        site finding by constructing supercell of surface sites\n\n        Args:\n            repeat (3-tuple): repeat for getting extended surface mesh\n        \"\"\"\n        surf_str = Structure.from_sites(self.surface_sites)\n        surf_str.make_supercell(repeat)\n        return surf_str",
        "rewrite": "```python\ndef get_extended_surface_mesh(self, repeat: tuple = (5, 5, 1)) -> Structure:\n    surf_str = Structure.from_sites(self.surface_sites)\n    surf_str.make_supercell(repeat)\n    return surf_str\n```"
    },
    {
        "original": "def find_1den_files(self):\n        \"\"\"\n        Abinit adds the idir-ipert index at the end of the 1DEN file and this breaks the extension\n        e.g. out_DEN1. This method scans the files in the directories and returns a list of namedtuple\n        Each named tuple gives the `path` of the 1DEN file and the `pertcase` index.\n        \"\"\"\n        regex = re.compile(r\"out_DEN(\\d+)(\\.nc)?$\")\n        den_paths = [f for f in self.list_filepaths() if regex.match(os.path.basename(f))]\n        if not den_paths: return None\n\n        # Build list of (pertcase, path) tuples.\n        pertfile_list = []\n        for path in den_paths:\n            name = os.path.basename(path)\n            match = regex.match(name)\n            pertcase, ncext = match.groups()\n            pertfile_list.append((int(pertcase), path))\n\n        # DSU sort.\n        pertfile_list = sorted(pertfile_list, key=lambda t: t[0])\n        return [dict2namedtuple(pertcase=item[0], path=item[1]) for item in pertfile_list]",
        "rewrite": "```python\nimport re\nimport os\nfrom collections import namedtuple\n\ndef dict2namedtuple(**kwargs):\n    return namedtuple('NamedTuple', kwargs.keys())(*kwargs.values())\n\ndef find_1den_files(self):\n    regex = re.compile(r\"out_DEN(\\d+)(\\.nc)?$\")\n    den_paths = [f for f in self.list_filepaths() if regex.match(os.path.basename(f))]\n    if not den_paths: return None\n\n    pertfile_list = []\n    for path in den_paths:\n        name = os.path.basename(path)\n        match = regex.match(name)\n        pertcase, _ = match.groups"
    },
    {
        "original": "def set_workdir(self, workdir, chroot=False):\n        \"\"\"Set the working directory of the task.\"\"\"\n        super().set_workdir(workdir, chroot=chroot)\n        # Small hack: the log file of optics is actually the main output file.\n        self.output_file = self.log_file",
        "rewrite": "```python\ndef set_workdir(self, workdir, chroot=False):\n    \"\"\"Set the working directory of the task.\"\"\"\n    super().set_workdir(workdir, chroot=chroot)\n    self.output_file = self.log_file\n```"
    },
    {
        "original": "def long_form_multiple_formats(jupytext_formats, metadata=None):\n    \"\"\"Convert a concise encoding of jupytext.formats to a list of formats, encoded as dictionaries\"\"\"\n    if not jupytext_formats:\n        return []\n\n    if not isinstance(jupytext_formats, list):\n        jupytext_formats = [fmt for fmt in jupytext_formats.split(',') if fmt]\n\n    jupytext_formats = [long_form_one_format(fmt, metadata) for fmt in jupytext_formats]\n\n    return jupytext_formats",
        "rewrite": "```python\ndef long_form_multiple_formats(jupytext_formats, metadata=None):\n    if not jupytext_formats:\n        return []\n\n    if not isinstance(jupytext_formats, list):\n        jupytext_formats = [fmt.strip() for fmt in jupytext_formats.split(',') if fmt]\n\n    return [long_form_one_format(fmt, metadata) for fmt in jupytext_formats]\n```"
    },
    {
        "original": "async def prepare_decrypter(client, cdn_client, cdn_redirect):\n        \"\"\"\n        Prepares a new CDN decrypter.\n\n        :param client: a TelegramClient connected to the main servers.\n        :param cdn_client: a new client connected to the CDN.\n        :param cdn_redirect: the redirect file object that caused this call.\n        :return: (CdnDecrypter, first chunk file data)\n        \"\"\"\n        cdn_aes = AESModeCTR(\n            key=cdn_redirect.encryption_key,\n            # 12 first bytes of the IV..4 bytes of the offset (0, big endian)\n            iv=cdn_redirect.encryption_iv[:12] + bytes(4)\n        )\n\n        # We assume that cdn_redirect.cdn_file_hashes are ordered by offset,\n        # and that there will be enough of these to retrieve the whole file.\n        decrypter = CdnDecrypter(\n            cdn_client, cdn_redirect.file_token,\n            cdn_aes, cdn_redirect.cdn_file_hashes\n        )\n\n        cdn_file = await cdn_client(GetCdnFileRequest(\n            file_token=cdn_redirect.file_token,\n            offset=cdn_redirect.cdn_file_hashes[0].offset,\n            limit=cdn_redirect.cdn_file_hashes[0].limit\n        ))\n        if isinstance(cdn_file, CdnFileReuploadNeeded):\n            # We need to use the original client here\n            await client(ReuploadCdnFileRequest(\n                file_token=cdn_redirect.file_token,\n                request_token=cdn_file.request_token\n            ))\n\n            # We want to always return a valid upload.CdnFile\n            cdn_file = decrypter.get_file()\n        else:\n            cdn_file.bytes = decrypter.cdn_aes.encrypt(cdn_file.bytes)\n            cdn_hash = decrypter.cdn_file_hashes.pop(0)\n            decrypter.check(cdn_file.bytes, cdn_hash)\n\n        return decrypter, cdn_file",
        "rewrite": "```python\nasync def prepare_decrypter(client, cdn_client, cdn_redirect):\n    cdn_aes = AESModeCTR(\n        key=cdn_redirect.encryption_key,\n        iv=cdn_redirect.encryption_iv[:12] + bytes(4)\n    )\n\n    decrypter = CdnDecrypter(\n        cdn_client, cdn_redirect.file_token,\n        cdn_aes, cdn_redirect.cdn_file_hashes\n    )\n\n    cdn_file = await cdn_client(GetCdnFileRequest(\n        file_token=cdn_redirect.file_token,\n        offset=cdn_redirect.cdn_file_hashes[0].offset,\n       "
    },
    {
        "original": "def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        assert isinstance(label, (github.Label.Label, str, unicode)), label\n        if isinstance(label, github.Label.Label):\n            label = label._identity\n        else:\n            label = urllib.quote(label)\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.issue_url + \"/labels/\" + label\n        )",
        "rewrite": "```python\ndef remove_from_labels(self, label):\n    \"\"\"\n    :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n    :param label: :class:`github.Label.Label` or string\n    :rtype: None\n    \"\"\"\n    assert isinstance(label, (github.Label.Label, str)), label\n    if isinstance(label, github.Label.Label):\n        label = label._identity\n    else:\n        label = urllib.parse.quote(label)\n    self._requester.requestJsonAndCheck(\n        \"DELETE\",\n        self.issue_url +"
    },
    {
        "original": "def remove_terms_by_indices(self, idx_to_delete_list):\n        \"\"\"\n        Parameters\n        ----------\n        idx_to_delete_list, list\n\n        Returns\n        -------\n        TermDocMatrix\n        \"\"\"\n        new_X, new_term_idx_store = self._get_X_after_delete_terms(idx_to_delete_list)\n        return self._make_new_term_doc_matrix(new_X, self._mX, self._y, new_term_idx_store, self._category_idx_store,\n                                              self._metadata_idx_store, self._y == self._y)",
        "rewrite": "```python\ndef remove_terms_by_indices(self, idx_to_delete_list):\n    \"\"\"\n    Parameters\n    ----------\n    idx_to_delete_list : list\n\n    Returns\n    -------\n    TermDocMatrix\n    \"\"\"\n    new_X, new_term_idx_store = self._get_X_after_delete_terms(idx_to_delete_list)\n    \n    return self._make_new_term_doc_matrix(new_X, self._mX, self._y, new_term_idx_store,\n                                          self._category_idx_store, \n                                          self._metadata_idx_store,\n                                          (self._y == self._y).values)\n```"
    },
    {
        "original": "def confusion_matrix(\n    gold, pred, null_pred=False, null_gold=False, normalize=False, pretty_print=True\n):\n    \"\"\"A shortcut method for building a confusion matrix all at once.\n\n    Args:\n        gold: an array-like of gold labels (ints)\n        pred: an array-like of predictions (ints)\n        null_pred: If True, include the row corresponding to null predictions\n        null_gold: If True, include the col corresponding to null gold labels\n        normalize: if True, divide counts by the total number of items\n        pretty_print: if True, pretty-print the matrix before returning\n    \"\"\"\n    conf = ConfusionMatrix(null_pred=null_pred, null_gold=null_gold)\n    gold = arraylike_to_numpy(gold)\n    pred = arraylike_to_numpy(pred)\n    conf.add(gold, pred)\n    mat = conf.compile()\n\n    if normalize:\n        mat = mat / len(gold)\n\n    if pretty_print:\n        conf.display(normalize=normalize)\n\n    return mat",
        "rewrite": "```python\nimport numpy as np\n\nclass ConfusionMatrix:\n    def __init__(self, null_pred=False, null_gold=False):\n        self.null_pred = null_pred\n        self.null_gold = null_gold\n        self.matrix = np.zeros((2 + int(null_gold), 2 + int(null_pred)))\n\n    def add(self, gold, pred):\n        for i in range(len(gold)):\n            if gold[i] == -1 and self.null_gold:\n                self.matrix[0][0] += 1\n            elif pred[i] == -1 and self.null_pred:\n                self.matrix[0][0]"
    },
    {
        "original": "def nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    verbose : boolean\n        print additional information about the node\n        e.g. platform version, hvm capable, ...\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    ret = {} if verbose else []\n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    ## get list of nodes\n    try:\n        for cn in client.cmd_iter('G@virtual:physical and G@os:smartos',\n                                  'grains.items', tgt_type='compound'):\n            if not cn:\n                continue\n            node = next(six.iterkeys(cn))\n            if not isinstance(cn[node], dict) or \\\n                    'ret' not in cn[node] or \\\n                    not isinstance(cn[node]['ret'], dict):\n                continue\n            if verbose:\n                ret[node] = {}\n                ret[node]['version'] = {}\n                ret[node]['version']['platform'] = cn[node]['ret']['osrelease']\n                if 'computenode_sdc_version' in cn[node]['ret']:\n                    ret[node]['version']['sdc'] = cn[node]['ret']['computenode_sdc_version']\n                ret[node]['vms'] = {}\n                if 'computenode_vm_capable' in cn[node]['ret'] and \\\n                        cn[node]['ret']['computenode_vm_capable'] and \\\n                        'computenode_vm_hw_virt' in cn[node]['ret']:\n                    ret[node]['vms']['hw_cap'] = cn[node]['ret']['computenode_vm_hw_virt']\n                else:\n                    ret[node]['vms']['hw_cap'] = False\n                if 'computenode_vms_running' in cn[node]['ret']:\n                    ret[node]['vms']['running'] = cn[node]['ret']['computenode_vms_running']\n            else:\n                ret.append(node)\n    except SaltClientError as client_error:\n        return \"{0}\".format(client_error)\n\n    if not verbose:\n        ret.sort()\n    return ret",
        "rewrite": "```python\ndef nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    ret = {}\n    if verbose:\n        ret = {'nodes': {}}\n    \n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    \n        \n_async(msg_id=\"vmadm.nodes\",\n            fun=\"grains.items\",\n            tgt_type=\"compound\",\n            _timeout=None,\n            kwarg={'virtual:physical': True, 'os:smartos': True})\n    \n\noutilon_texts_dict"
    },
    {
        "original": "def type(self):\n        \"\"\"\n        Read-only. A member of :ref:`MsoColorType`, one of RGB, THEME, or\n        AUTO, corresponding to the way this color is defined. Its value is\n        |None| if no color is applied at this level, which causes the\n        effective color to be inherited from the style hierarchy.\n        \"\"\"\n        color = self._color\n        if color is None:\n            return None\n        if color.themeColor is not None:\n            return MSO_COLOR_TYPE.THEME\n        if color.val == ST_HexColorAuto.AUTO:\n            return MSO_COLOR_TYPE.AUTO\n        return MSO_COLOR_TYPE.RGB",
        "rewrite": "```python\ndef type(self):\n    color = self._color\n    if color is None:\n        return None\n    elif color.themeColor is not None:\n        return MSO_COLOR_TYPE.THEME\n    elif color.val == ST_HexColorAuto.AUTO:\n        return MSO_COLOR_TYPE.AUTO\n    else:\n        return MSO_COLOR_TYPE.RGB\n```"
    },
    {
        "original": "def _simplify_function(self):\n        \"\"\"\n        Simplify the entire function.\n\n        :return:    None\n        \"\"\"\n\n        # Computing reaching definitions\n        rd = self.project.analyses.ReachingDefinitions(func=self.function, func_graph=self.graph, observe_all=True)\n\n        simp = self.project.analyses.AILSimplifier(self.function, func_graph=self.graph, reaching_definitions=rd)\n\n        for key in list(self._blocks.keys()):\n            old_block = self._blocks[key]\n            if old_block in simp.blocks:\n                self._blocks[key] = simp.blocks[old_block]\n\n        self._update_graph()",
        "rewrite": "```python\ndef _simplify_function(self):\n    rd = self.project.analyses.ReachingDefinitions(func=self.function, func_graph=self.graph, observe_all=True)\n    simp = self.project.analyses.AILSimplifier(self.function, func_graph=self.graph, reaching_definitions=rd)\n\n    for key in list(self._blocks.keys()):\n        old_block = self._blocks[key]\n        if old_block in simp.blocks:\n            self._blocks[key] = simp.blocks[old_block]\n\n    self._update_graph()\n```"
    },
    {
        "original": "def execute_paged_query(self, verb, verb_arguments):\n        \"\"\"Executes query (ex. list) via a dedicated http object.\n\n        Args:\n            verb (str): Method to execute on the component (ex. get, list).\n            verb_arguments (dict): key-value pairs to be passed to _BuildRequest.\n\n        Yields:\n            dict: Service Response.\n\n        Raises:\n            PaginationNotSupportedError: When an API does not support paging.\n        \"\"\"\n        if not self.supports_pagination(verb=verb):\n            raise PaginationNotSupported('{} does not support pagination')\n\n        request = self._build_request(verb, verb_arguments)\n\n        number_of_pages_processed = 0\n        while request is not None:\n            response = self._execute(request)\n            number_of_pages_processed += 1\n            log.debug('Executing paged request #%s', number_of_pages_processed)\n            request = self._build_next_request(verb, request, response)\n            yield response",
        "rewrite": "```python\ndef execute_paged_query(self, verb, verb_arguments):\n    \"\"\"Executes query (ex. list) via a dedicated http object.\n\n    Args:\n        verb (str): Method to execute on the component (ex. get, list).\n        verb_arguments (dict): key-value pairs to be passed to _BuildRequest.\n\n    Yields:\n        dict: Service Response.\n\n    Raises:\n        PaginationNotSupportedError: When an API does not support paging.\n    \"\"\"\n    if not self.supports_pagination(verb=verb):\n        raise PaginationNotSupported(f\"{verb} does not support pagination\")\n\n    request = self"
    },
    {
        "original": "def get_all_nn_info(self, structure):\n        \"\"\"Get a listing of all neighbors for all sites in a structure\n\n        Args:\n            structure (Structure): Input structure\n        Return:\n            List of NN site information for each site in the structure. Each\n                entry has the same format as `get_nn_info`\n        \"\"\"\n\n        return [self.get_nn_info(structure, n) for n in range(len(structure))]",
        "rewrite": "```python\ndef get_all_nn_info(self, structure):\n    return [self.get_nn_info(structure, n) for n in range(len(structure))]\n```"
    },
    {
        "original": "def _read(self):\n    \"\"\"Actually read the response and parse it, returning a Response.\"\"\"\n    with sw(\"read_response\"):\n      with catch_websocket_connection_errors():\n        response_str = self._sock.recv()\n    if not response_str:\n      raise ProtocolError(\"Got an empty response from SC2.\")\n    response = sc_pb.Response()\n    with sw(\"parse_response\"):\n      response.ParseFromString(response_str)\n    return response",
        "rewrite": "```python\ndef _read(self):\n    with sw(\"read_response\"):\n        with catch_websocket_connection_errors():\n            response_str = self._sock.recv()\n    if not response_str:\n        raise ProtocolError(\"Got an empty response from SC2.\")\n    response = sc_pb.Response()\n    with sw(\"parse_response\"):\n        response.ParseFromString(response_str)\n    return response\n```"
    },
    {
        "original": "def _variable_on_cpu(name, shape, initializer):\n  \"\"\"Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var",
        "rewrite": "```python\ndef _variable_on_cpu(name, shape, initializer):\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name=name, shape=shape, initializer=initializer, dtype=dtype)\n  return var\n```"
    },
    {
        "original": "def get_users_in_organization(self, organization_id, start=0, limit=50):\n        \"\"\"\n        Get all the users of a specified organization\n\n        :param organization_id: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return: Users list in organization\n        \"\"\"\n        url = 'rest/servicedeskapi/organization/{}/user'.format(organization_id)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params)",
        "rewrite": "```python\ndef get_users_in_organization(self, organization_id, start: int = 0, limit: int = 50):\n    url = f'rest/servicedeskapi/organization/{organization_id}/user'\n    params = {'start': start, 'limit': limit}\n    return self.get(url, headers=self.experimental_headers, params=params)\n```"
    },
    {
        "original": "def before_request():\n    \"\"\"This runs before every API request. The function take cares of creating\n    driver object is not already created. Also it checks for few prerequisits\n    parameters and set global variables for other functions to use\n    \n    Required paramters for an API hit are:\n    auth-key: key string to identify valid request\n    client_id: to identify for which client the request is to be run\n    \"\"\"\n    global logger\n    \n    if not request.url_rule:\n        abort(404)\n\n    if logger == None:\n        create_logger()\n    logger.info(\"API call \" + request.method + \" \" + request.url)\n\n    auth_key = request.headers.get('auth-key')\n    g.client_id = request.headers.get('client_id')\n    rule_parent = request.url_rule.rule.split('/')[1]\n    \n    if API_KEY and auth_key != API_KEY:\n        abort(401, 'you must send valid auth-key')\n        raise Exception()\n\n    if not g.client_id and rule_parent != 'admin':\n        abort(400, 'client ID is mandatory')\n\n    acquire_semaphore(g.client_id)\n\n    # Create a driver object if not exist for client requests.\n    if rule_parent != 'admin':\n        if g.client_id not in drivers:\n            drivers[g.client_id] = init_client(g.client_id)\n        \n        g.driver = drivers[g.client_id]\n        g.driver_status = WhatsAPIDriverStatus.Unknown\n        \n        if g.driver is not None:\n            g.driver_status = g.driver.get_status()\n        \n        # If driver status is unkown, means driver has closed somehow, reopen it\n        if (g.driver_status != WhatsAPIDriverStatus.NotLoggedIn\n            and g.driver_status != WhatsAPIDriverStatus.LoggedIn):\n            drivers[g.client_id] = init_client(g.client_id)\n            g.driver_status = g.driver.get_status()\n        \n        init_timer(g.client_id)",
        "rewrite": "```python\nimport logging\n\nlogger = None\n\ndef create_logger():\n    global logger\n    logger = logging.getLogger('api_logger')\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handler = logging.FileHandler('api.log')\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\ndef before_request():\n    global logger\n    \n    if not request.url_rule:\n        abort(404)\n\n    if not hasattr(g, 'client_id'):\n        g.client_id = None\n\n    if not hasattr(g, 'driver'):\n        g.driver_status = Whats"
    },
    {
        "original": "def widget_from_single_value(o):\n        \"\"\"Make widgets from single values, which can be used as parameter defaults.\"\"\"\n        if isinstance(o, string_types):\n            return Text(value=unicode_type(o))\n        elif isinstance(o, bool):\n            return Checkbox(value=o)\n        elif isinstance(o, Integral):\n            min, max, value = _get_min_max_value(None, None, o)\n            return IntSlider(value=o, min=min, max=max)\n        elif isinstance(o, Real):\n            min, max, value = _get_min_max_value(None, None, o)\n            return FloatSlider(value=o, min=min, max=max)\n        else:\n            return None",
        "rewrite": "```python\ndef widget_from_single_value(obj: typing.Union[str, bool, int, float]) -> typing.Union[Text, Checkbox, IntSlider, FloatSlider]:\n    if isinstance(obj, str):\n        return Text(value=obj)\n    elif isinstance(obj, bool):\n        return Checkbox(value=obj)\n    elif isinstance(obj, int):\n        min_val = max_val = obj\n        value = obj\n        return IntSlider(value=value, min=min_val or -1000000 if not min_val else 0,\n                         max=max_val or 1000000 if not max_val else None)\n    elif isinstance(obj, float"
    },
    {
        "original": "def _domain_event_watchdog_cb(conn, domain, action, opaque):\n    \"\"\"\n    Domain watchdog events handler\n    \"\"\"\n    _salt_send_domain_event(opaque, conn, domain, opaque['event'], {\n        'action': _get_libvirt_enum_string('VIR_DOMAIN_EVENT_WATCHDOG_', action)\n    })",
        "rewrite": "```python\ndef _domain_event_watchdog_cb(conn, domain, action, opaque):\n    _salt_send_domain_event(opaque, conn, domain,\n                           opaque['event'], {\n                               'action': VIR_DOMAIN_EVENT_WATCHDOG__get_libvirt_enum_string(action)\n                           })\n```"
    },
    {
        "original": "def del_variables(self, variables):\n        \"\"\"\n        Deletes variables from the NoisyOrModel.\n\n        Parameters\n        ----------\n        variables: list, tuple, dict (array like)\n            list of variables to be deleted.\n\n        Examples\n        --------\n        >>> from pgmpy.models import NoisyOrModel\n        >>> model = NoisyOrModel(['x1', 'x2', 'x3'], [2, 3, 2], [[0.6, 0.4],\n        ...                                                      [0.2, 0.4, 0.7],\n        ...                                                      [0.1, 0. 4]])\n        >>> model.del_variables(['x1'])\n        \"\"\"\n        variables = [variables] if isinstance(variables, six.string_types) else set(variables)\n        indices = [index for index, variable in enumerate(self.variables) if variable in variables]\n        self.variables = np.delete(self.variables, indices, 0)\n        self.cardinality = np.delete(self.cardinality, indices, 0)\n        self.inhibitor_probability = [prob_array for index, prob_array in enumerate(self.inhibitor_probability)\n                                      if index not in indices]",
        "rewrite": "```python\ndef del_variables(self, variables):\n    \"\"\"\n    Deletes variables from the NoisyOrModel.\n\n    Parameters\n    ----------\n    variables: list, tuple, dict (array like)\n        list of variables to be deleted.\n\n    Examples\n    --------\n    >>> from pgmpy.models import NoisyOrModel\n    >>> model = NoisyOrModel(['x1', 'x2', 'x3'], [2, 3, 2], [[0.6, 0.4],\n                                                                 [0.2, 0.4, 0.7],\n                                                                 [0.1, 0"
    },
    {
        "original": "def set_auth_field(self, user_field, biz_field):\n        \"\"\"\n        \u8bbe\u7f6e\u6388\u6743\u9875\u5b57\u6bb5\u4fe1\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1497082828_r1cI2\n\n        :param user_field: \u6388\u6743\u9875\u4e2a\u4eba\u53d1\u7968\u5b57\u6bb5\n        :type user_field: dict\n        :param biz_field: \u6388\u6743\u9875\u5355\u4f4d\u53d1\u7968\u5b57\u6bb5\n        :type biz_field: dict\n        \"\"\"\n        return self._post(\n            'setbizattr',\n            params={\n                'action': 'set_auth_field',\n            },\n            data={\n                'auth_field': {\n                    'user_field': user_field,\n                    'biz_field': biz_field,\n                },\n            },\n        )",
        "rewrite": "```python\ndef set_auth_field(self, user_field, biz_field):\n    \"\"\"\n    \u8bbe\u7f6e\u6388\u6743\u9875\u5b57\u6bb5\u4fe1\u606f\n    \u8be6\u60c5\u8bf7\u53c2\u8003 https://mp.weixin.qq.com/wiki?id=mp1497082828_r1cI2\n\n    :param user_field: \u6388\u6743\u9875\u4e2a\u4eba\u53d1\u7968\u5b57\u6bb5\n    :type user_field: dict\n    :param biz_field: \u6388\u6743\u9875\u5355\u4f4d\u53d1\u7968\u5b57\u6bb5\n    :type biz_field: dict\n    \"\"\"\n    return self._post(\n        'setbizattr',\n        params={\n            'action': 'set_auth_field'\n        },\n        data={\n            'auth':"
    },
    {
        "original": "def next(self):\n        \"\"\"Fetch a set of items with IDs less than current set.\"\"\"\n        if self.limit and self.limit == self.num_tweets:\n            raise StopIteration\n\n        if self.index >= len(self.results) - 1:\n            data = self.method(max_id=self.max_id, parser=RawParser(), *self.args, **self.kargs)\n\n            if hasattr(self.method, '__self__'):\n                old_parser = self.method.__self__.parser\n                # Hack for models which expect ModelParser to be set\n                self.method.__self__.parser = ModelParser()\n\n            # This is a special invocation that returns the underlying\n            # APIMethod class\n            model = ModelParser().parse(self.method(create=True), data)\n            if hasattr(self.method, '__self__'):\n                self.method.__self__.parser = old_parser\n                result = self.method.__self__.parser.parse(self.method(create=True), data)\n            else:\n                result = model\n\n            if len(self.results) != 0:\n                self.index += 1\n            self.results.append(result)\n            self.model_results.append(model)\n        else:\n            self.index += 1\n            result = self.results[self.index]\n            model = self.model_results[self.index]\n\n        if len(result) == 0:\n            raise StopIteration\n        # TODO: Make this not dependant on the parser making max_id and\n        # since_id available\n        self.max_id = model.max_id\n        self.num_tweets += 1\n        return result",
        "rewrite": "```python\ndef next(self):\n    if self.limit and self.limit == self.num_tweets:\n        raise StopIteration\n\n    if self.index < len(self.results):\n        if hasattr(self.method, '__self__'):\n            old_parser = self.method.__self__.parser\n            # Hack for models which expect ModelParser to be set\n            self.method.__self__.parser = ModelParser()\n\n        data = RawParser().parse(self.method(create=True), { 'max_id': 1 + (len(self.results) - 1 - self.index) })\n        \n        model = ModelParser().parse(self.method(create=True), data)\n        \n       "
    },
    {
        "original": "def upload_file(self, file_name, file_path):\n        \"\"\" Upload a given file from the file_path to the bucket\n            with the new name/path file_name. \"\"\"\n        upload_key = Key(bucket=self.bucket, name=file_name)\n        content_type = \"text/plain\"\n        if file_name.endswith(\".html\"):\n            content_type = \"text/html\"\n        elif file_name.endswith(\".jpg\"):\n            content_type = \"image/jpeg\"\n        elif file_name.endswith(\".png\"):\n            content_type = \"image/png\"\n        upload_key.set_contents_from_filename(\n            file_path,\n            headers={\"Content-Type\": content_type})\n        upload_key.url = \\\n            upload_key.generate_url(expires_in=3600).split(\"?\")[0]\n        try:\n            upload_key.make_public()\n        except Exception:\n            pass",
        "rewrite": "```python\ndef upload_file(self, file_name, file_path):\n    upload_key = Key(bucket=self.bucket, name=file_name)\n    content_type_map = {\n        \".html\": \"text/html\",\n        \".jpg\": \"image/jpeg\",\n        \".png\": \"image/png\"\n    }\n    \n    content_type = \"text/plain\"\n    for extension, mime_type in content_type_map.items():\n        if file_name.endswith(extension):\n            content_type = mime_type\n            break\n    \n    upload_key.set_contents_from_filename(\n        file_path,\n        headers={\"Content-Type\": content_type})\n    \n    try:\n        upload_key.make_public()\n"
    },
    {
        "original": "def delete_cookies():\n    \"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Redirect to cookie list\n    \"\"\"\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.delete_cookie(key=key)\n\n    return r",
        "rewrite": "```python\nfrom flask import request, redirect, url_for\n\ndef delete_cookies():\n    \"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\"\"\"\n    \n    cookies = dict(request.args.items())\n    r = redirect(url_for(\"view_cookies\"))\n    \n    for key, value in cookies.items():\n        r.set_cookie(key=key, expires=0)\n        \n    return r\n```\n\nNote: I replaced `r.delete_cookie(key=key)` with `r.set_cookie(key=key, expires=0)` because Flask's `redirect` object does not have a `delete_cookie` method. Instead, you can use the"
    },
    {
        "original": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        # autoscaleconnection.get_all_groups()\n        limits['Auto Scaling groups'] = AwsLimit(\n            'Auto Scaling groups',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n        # autoscaleconnection.get_all_launch_configurations()\n        limits['Launch configurations'] = AwsLimit(\n            'Launch configurations',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::LaunchConfiguration',\n        )\n        self.limits = limits\n        return limits",
        "rewrite": "```python\ndef get_limits(self):\n    if not hasattr(self, 'limits') or self.limits == {}:\n        limits = {}\n        limits['Auto Scaling groups'] = AwsLimit(\n            'Auto Scaling groups',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n        limits['Launch configurations'] = AwsLimit(\n            'Launch configurations',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::LaunchConfiguration'\n        )\n    else:\n"
    },
    {
        "original": "def apply_to_structure(self, structure):\n        \"\"\"\n        Apply the deformation gradient to a structure.\n\n        Args:\n            structure (Structure object): the structure object to\n                be modified by the deformation\n        \"\"\"\n        def_struct = structure.copy()\n        old_latt = def_struct.lattice.matrix\n        new_latt = np.transpose(np.dot(self, np.transpose(old_latt)))\n        def_struct.lattice = Lattice(new_latt)\n        return def_struct",
        "rewrite": "```python\ndef apply_to_structure(self, structure):\n    def_struct = structure.copy()\n    old_latt = def_struct.lattice.matrix\n    new_latt = np.dot(self, old_latt)\n    new_latt = np.transpose(new_latt)\n    def_struct.lattice = Lattice(new_latt)\n    return def_struct\n```"
    },
    {
        "original": "def main():\n    \"\"\"Rewrite Thrift-generated Python clients to handle recursive structs. For\n    more details see: https://issues.apache.org/jira/browse/THRIFT-2642.\n\n    Requires package `RedBaron`, available via pip:\n    $ pip install redbaron\n\n    To use:\n\n    $ thrift -gen py mapd.thrift\n    $ mv gen-py/mapd/ttypes.py gen-py/mapd/ttypes-backup.py\n    $ python fix_recursive_structs.py gen-py/mapd/ttypes-backup.py gen-py/mapd/ttypes.py\n\n    \"\"\"\n    in_file = open(sys.argv[1], 'r')\n    out_file = open(sys.argv[2], 'w')\n\n    red_ast = RedBaron(in_file.read())\n\n    thrift_specs = [ts.parent for ts in red_ast.find_all(\n        'name', 'thrift_spec') if ts.parent.type == 'assignment' and ts.parent.parent.name in ['TDatumVal', 'TColumnData']]\n\n    nodes = []\n    for ts in thrift_specs:\n        node = ts.copy()\n        node.target = ts.parent.name + '.' + str(node.target)\n        nodes.append(node)\n        ts.value = 'None'\n\n    red_ast.extend(nodes)\n    out_file.write(red_ast.dumps())",
        "rewrite": "```python\nimport sys\nimport redbaron\n\ndef main():\n    if len(sys.argv) != 3:\n        print(\"Usage: python fix_recursive_structs.py in_file out_file\")\n        return\n\n    with open(sys.argv[1], 'r') as in_file, open(sys.argv[2], 'w') as out_file:\n        red_ast = redbaron.RedBaron(in_file.read())\n\n        thrift_specs = [ts.parent for ts in red_ast.find_all('Assign', requires=['Name']) \n                       if ts.value.asstring() == 'thrift_spec']\n\n        nodes = []\n        for spec in"
    },
    {
        "original": "def rest_api_exists(self, rest_api_id):\n        # type: (str) -> bool\n        \"\"\"Check if an an API Gateway REST API exists.\"\"\"\n        client = self._client('apigateway')\n        try:\n            client.get_rest_api(restApiId=rest_api_id)\n            return True\n        except client.exceptions.NotFoundException:\n            return False",
        "rewrite": "```python\ndef rest_api_exists(self, rest_api_id: str) -> bool:\n    client = self._client('apigateway')\n    try:\n        client.get_rest_api(restApiId=rest_api_id)\n        return True\n    except client.exceptions.NotFoundException as e:\n        if isinstance(e, client.exceptions.NotFoundException):\n            return False\n```"
    },
    {
        "original": "def _handle_result(self, result):\n        \"\"\"Mark the result as completed, insert the `CompiledResultNode` into\n        the manifest, and mark any descendants (potentially with a 'cause' if\n        the result was an ephemeral model) as skipped.\n        \"\"\"\n        is_ephemeral = result.node.is_ephemeral_model\n        if not is_ephemeral:\n            self.node_results.append(result)\n\n        node = CompileResultNode(**result.node)\n        node_id = node.unique_id\n        self.manifest.nodes[node_id] = node\n\n        if result.error is not None:\n            if is_ephemeral:\n                cause = result\n            else:\n                cause = None\n            self._mark_dependent_errors(node_id, result, cause)",
        "rewrite": "```python\ndef _handle_result(self, result):\n    is_ephemeral = result.node.is_ephemeral_model\n    if not is_ephemeral:\n        self.node_results.append(result)\n\n    node = CompileResultNode(**result.node)\n    node_id = node.unique_id\n    self.manifest.nodes[node_id] = node\n\n    error_cause = None\n    if result.error is not None:\n        if is_ephemeral:\n            error_cause = result\n\n    self._mark_dependent_errors(node_id, result, error_cause)\n```"
    },
    {
        "original": "def canonicalize_gates(gates: LogicalGates\n        ) -> Dict[frozenset, LogicalGates]:\n        \"\"\"Canonicalizes a set of gates by the qubits they act on.\n\n        Takes a set of gates specified by ordered sequences of logical\n        indices, and groups those that act on the same qubits regardless of\n        order.\"\"\"\n        canonicalized_gates = defaultdict(dict\n            ) # type: DefaultDict[frozenset, LogicalGates]\n        for indices, gate in gates.items():\n            indices = tuple(indices)\n            canonicalized_gates[frozenset(indices)][indices] = gate\n        return {canonical_indices: dict(list(gates.items()))\n                for canonical_indices, gates in canonicalized_gates.items()}",
        "rewrite": "```\ndef canonicalize_gates(gates: dict) -> dict:\n    canonicalized_gates = defaultdict(dict)\n    for indices, gate in gates.items():\n       canonicalized_gates[frozenset(indices)].setdefault(*indices, gate)\n    return {frozenset(indices): gates for indices, gates in canonicalized_gates.items()}\n```"
    },
    {
        "original": "def set_all_variables(self, delu_dict, delu_default):\n        \"\"\"\n        Sets all chemical potential values and returns a dictionary where\n            the key is a sympy Symbol and the value is a float (chempot).\n\n        Args:\n            entry (SlabEntry): Computed structure entry of the slab\n            delu_dict (Dict): Dictionary of the chemical potentials to be set as\n                constant. Note the key should be a sympy Symbol object of the\n                format: Symbol(\"delu_el\") where el is the name of the element.\n            delu_default (float): Default value for all unset chemical potentials\n\n        Returns:\n            Dictionary of set chemical potential values\n        \"\"\"\n\n        # Set up the variables\n        all_delu_dict = {}\n        for du in self.list_of_chempots:\n            if delu_dict and du in delu_dict.keys():\n                all_delu_dict[du] = delu_dict[du]\n            elif du == 1:\n                all_delu_dict[du] = du\n            else:\n                all_delu_dict[du] = delu_default\n\n        return all_delu_dict",
        "rewrite": "```python\ndef set_all_variables(self, delu_dict, delu_default):\n    \"\"\"\n    Sets all chemical potential values and returns a dictionary where\n        the key is a sympy Symbol and the value is a float (chempot).\n\n    Args:\n        delu_dict (Dict): Dictionary of the chemical potentials to be set as\n            constant. Note the key should be a sympy Symbol object of the\n            format: Symbol(\"delu_el\") where el is the name of the element.\n        delu_default (float): Default value for all unset chemical potentials\n\n    Returns:\n        Dictionary of set chemical potential values"
    },
    {
        "original": "def assign_license(service_instance, license_key, license_name,\n                   entity_ref=None, entity_name=None,\n                   license_assignment_manager=None):\n    \"\"\"\n    Assigns a license to an entity.\n\n    service_instance\n        The Service Instance Object from which to obrain the licenses.\n\n    license_key\n        The key of the license to add.\n\n    license_name\n        The description of the license to add.\n\n    entity_ref\n        VMware entity to assign the license to.\n        If None, the entity is the vCenter itself.\n        Default is None.\n\n    entity_name\n        Entity name used in logging.\n        Default is None.\n\n    license_assignment_manager\n        The LicenseAssignmentManager object of the service instance.\n        If not provided it will be retrieved\n        Default is None.\n    \"\"\"\n    if not license_assignment_manager:\n        license_assignment_manager = \\\n                get_license_assignment_manager(service_instance)\n    entity_id = None\n\n    if not entity_ref:\n        # vcenter\n        try:\n            entity_id = service_instance.content.about.instanceUuid\n        except vim.fault.NoPermission as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(\n                'Not enough permissions. Required privilege: '\n                '{0}'.format(exc.privilegeId))\n        except vim.fault.VimFault as exc:\n            raise salt.exceptions.VMwareApiError(exc.msg)\n        except vmodl.RuntimeFault as exc:\n            raise salt.exceptions.VMwareRuntimeError(exc.msg)\n        if not entity_name:\n            entity_name = 'vCenter'\n    else:\n        # e.g. vsan cluster or host\n        entity_id = entity_ref._moId\n\n    log.trace('Assigning license to \\'%s\\'', entity_name)\n    try:\n        vmware_license = license_assignment_manager.UpdateAssignedLicense(\n            entity_id,\n            license_key,\n            license_name)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    return vmware_license",
        "rewrite": "```python\ndef assign_license(service_instance, license_key, license_name,\n                   entity_ref=None, entity_name=None,\n                   license_assignment_manager=None):\n    \"\"\"\n    Assigns a license to an entity.\n    \"\"\"\n    if not license_assignment_manager:\n        from salt.utils.vmware import get_license_assignment_manager\n        license_assignment_manager = get_license_assignment_manager(service_instance)\n\n    if not entity_ref:\n        try:\n            entity_id = service_instance.content.about.instanceUuid\n            if not entity_name:\n                entity_name = 'vCenter'\n        except vim.fault.NoPermission as exc:\n            raise salt.exceptions.VMwareApiError(\n"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "rewrite": "```python\ndef _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):\n    idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n    if idx is not None:\n        return HPackIndexedHdr(index=idx), len(self[idx])\n\n    optimize_hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n    optimize_hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n    header_is_sensitive = is_sensitive(optimize_hdr_name.getfieldval('data').origin(),\n                                         optimize_hdr_value.getfieldval('data').origin())\n\n   "
    },
    {
        "original": "def get_args(cls, dist, header=None):\n        \"\"\"\n        Yield write_script() argument tuples for a distribution's\n        console_scripts and gui_scripts entry points.\n        \"\"\"\n        if header is None:\n            header = cls.get_header()\n        spec = str(dist.as_requirement())\n        for type_ in 'console', 'gui':\n            group = type_ + '_scripts'\n            for name, ep in dist.get_entry_map(group).items():\n                cls._ensure_safe_name(name)\n                script_text = cls.template % locals()\n                args = cls._get_script_args(type_, name, header, script_text)\n                for res in args:\n                    yield res",
        "rewrite": "```python\nimport itertools\n\ndef get_args(cls, dist, header=None):\n    \"\"\"Yield write_script() argument tuples for a distribution's console and gui scripts.\"\"\"\n    if header is None:\n        header = cls.get_header()\n    spec = str(dist.as_requirement())\n    \n    # Generate all possible combinations of script types and names\n    script_types = ['console', 'gui']\n    script_names = list(dist.get_entry_map('scripts').keys())\n    \n    for type_, name in itertools.product(script_types, script_names):\n        cls._ensure_safe_name(name)\n        group = type_ + '_scripts'\n        \n        #"
    },
    {
        "original": "def get_login_info(self, auth_code, provider_access_token=None):\n        \"\"\"\n        \u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        :param provider_access_token: \u670d\u52a1\u63d0\u4f9b\u5546\u7684 accesstoken\n        :param auth_code: OAuth 2.0 \u6388\u6743\u4f01\u4e1a\u53f7\u7ba1\u7406\u5458\u767b\u5f55\u4ea7\u751f\u7684 code\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self._post(\n            'service/get_login_info',\n            params={\n                'provider_access_token': provider_access_token,\n            },\n            data={\n                'auth_code': auth_code,\n            }\n        )",
        "rewrite": "```python\ndef get_login_info(self, auth_code, provider_access_token=None):\n    return self._post(\n        'service/get_login_info',\n        params={'provider_access_token': provider_access_token},\n        data={'auth_code': auth_code}\n    )\n```"
    },
    {
        "original": "def p_matrix(p):\n    \"\"\"matrix : LBRACKET RBRACKET\n              | LBRACKET concat_list RBRACKET\n              | LBRACKET concat_list SEMI RBRACKET\n              | LBRACKET expr_list RBRACKET\n              | LBRACKET expr_list SEMI RBRACKET\n    \"\"\"\n    if len(p) == 3:\n        p[0] = node.matrix()\n    else:\n        p[0] = node.matrix(p[2])",
        "rewrite": "```python\ndef p_matrix(p):\n    if len(p) == 3:\n        p[0] = node.matrix()\n    elif len(p) == 4 and p[2] == ';':\n        p[0] = node.matrix()\n    elif len(p) >= 4:\n        if isinstance(p[2], list):\n            p[0] = node.matrix(concat_list=p[2])\n        else:\n            raise ValueError(\"Invalid matrix syntax\")\n```"
    },
    {
        "original": "def check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    CLI Example::\n\n        salt '*' nftables.check_table nat\n    \"\"\"\n    ret = {'comment': '',\n           'result': False}\n\n    if not table:\n        ret['comment'] = 'Table needs to be specified'\n        return ret\n\n    nft_family = _NFTABLES_FAMILIES[family]\n    cmd = '{0} list tables {1}' . format(_nftables_cmd(), nft_family)\n    out = __salt__['cmd.run'](cmd, python_shell=False).find('table {0} {1}'.format(nft_family, table))\n\n    if out == -1:\n        ret['comment'] = 'Table {0} in family {1} does not exist'.\\\n                         format(table, family)\n    else:\n        ret['comment'] = 'Table {0} in family {1} exists'.\\\n                         format(table, family)\n        ret['result'] = True\n    return ret",
        "rewrite": "```python\ndef check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    CLI Example::\n\n        salt '*' nftables.check_table nat\n    \"\"\"\n    if not table:\n        return {'comment': 'Table needs to be specified', 'result': False}\n\n    nft_family = _NFTABLES_FAMILIES.get(family)\n    if not nft_family:\n        return {'comment': 'Unknown NFT family', 'result': False}\n\n    cmd = f'{_nftables_cmd()} list tables {nft_family}'\n    \n    try:\n        out ="
    },
    {
        "original": "def split(self, sequence):\n    \"\"\" Split into subsequences according to `sequence`.\"\"\"\n\n    major_idx = sequence.idx\n    idx2 = 0\n    for start, end in zip(major_idx[:-1], major_idx[1:]):\n      idx1 = self.idx.index(start, idx2)\n      idx2 = self.idx.index(end, idx2)\n      seq = Sequence(self.text[start:end])\n      seq.idx = [x-start for x in self.idx[idx1:idx2]]\n      yield seq",
        "rewrite": "```python\ndef split(self, sequence):\n    major_idx = [x for i, x in enumerate(sequence.idx) if i == 0 or x != sequence.idx[i-1]]\n    idx2 = 0\n    for start, end in zip(major_idx[:-1], major_idx[1:]):\n        idx1 = self.idx.index(start, idx2)\n        seq = Sequence(self.text[start:end])\n        seq.idx = [x-start for x in self.idx[idx1+idx2:idx2=self.idx.index(end, idx2)+1]]\n        yield seq\n```"
    },
    {
        "original": "def extra(name: str, desc: str) -> Callable:\n    \"\"\"\n    Decorator for slave channel's \"additional features\" interface.\n\n    Args:\n        name (str): A human readable name for the function.\n        desc (str): A short description and usage of it. Use\n            ``{function_name}`` in place of the function name\n            in the description.\n\n    Returns:\n        The decorated method.\n    \"\"\"\n\n    def attr_dec(f):\n        f.__setattr__(\"extra_fn\", True)\n        f.__setattr__(\"name\", name)\n        f.__setattr__(\"desc\", desc)\n        return f\n\n    return attr_dec",
        "rewrite": "```python\nfrom typing import Callable\n\ndef extra(name: str, desc: str) -> Callable:\n    def attr_dec(f) -> Callable:\n        f.__setattr__(\"extra_fn\", True)\n        f.__setattr__(\"name\", name)\n        f.__setattr__(\"desc\", desc)\n        return f\n    return attr_dec\n\n# Or with property decorators for better behavior:\n\nclass ExtraFunction(type):\n    @property\n    def extra_fn(cls):\n        return cls._extra_fn\n\n    @property\n    def name(cls):\n        return cls._name\n\n    @property\n    def desc(cls):\n        return cls._desc\n\n"
    },
    {
        "original": "def get_base_path() -> Path:\n    \"\"\"\n    Get the base data path for EFB. This can be defined by the\n    environment variable ``EFB_DATA_PATH``.\n    \n    If ``EFB_DATA_PATH`` is not defined, this gives\n    ``~/.ehforwarderbot``.\n    \n    This method creates the queried path if not existing.\n    \n    Returns:\n        The base path.\n    \"\"\"\n    env_data_path = os.environ.get(\"EFB_DATA_PATH\", None)\n    if env_data_path:\n        base_path = Path(env_data_path).resolve()\n    else:\n        base_path = Path.home() / \".ehforwarderbot\"\n    if not base_path.exists():\n        base_path.mkdir(parents=True)\n    return base_path",
        "rewrite": "```python\nimport os\nfrom pathlib import Path\n\ndef get_base_path() -> Path:\n    env_data_path = os.environ.get(\"EFB_DATA_PATH\", None)\n    base_path = Path(env_data_path) if env_data_path else Path.home() / \".ehforwarderbot\"\n    base_path = base_path.resolve()\n    if not base_path.exists():\n        base_path.mkdir(parents=True)\n    return base_path\n```"
    },
    {
        "original": "def score(self, X, y):\n        \"\"\"\n        Draws a confusion matrix based on the test data supplied by comparing\n        predictions on instances X with the true values specified by the\n        target vector y.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features\n\n        y : ndarray or Series of length n\n            An array or series of target or class values\n\n        Returns\n        -------\n\n        score_ : float\n            Global accuracy score\n        \"\"\"\n        # Create predictions from X (will raise not fitted error)\n        y_pred = self.predict(X)\n\n        # Encode the target with the supplied label encoder\n        if self.label_encoder:\n            try :\n                y = self.label_encoder.inverse_transform(y)\n                y_pred = self.label_encoder.inverse_transform(y_pred)\n            except AttributeError:\n                # if a mapping is passed to class apply it here.\n                y = np.array([self.label_encoder[x] for x in y])\n                y_pred = np.array([self.label_encoder[x] for x in y_pred])\n\n        # Compute the confusion matrix and class counts\n        self.confusion_matrix_ = confusion_matrix_metric(\n            y, y_pred, labels=self.classes_, sample_weight=self.sample_weight\n        )\n        self.class_counts_ = self.class_counts(y)\n\n        # Make array of only the classes actually being used.\n        # Needed because sklearn confusion_matrix only returns counts for\n        # selected classes but percent should be calculated on all classes\n        selected_class_counts = []\n        for c in self.classes_:\n            try:\n                selected_class_counts.append(self.class_counts_[c])\n            except KeyError:\n                selected_class_counts.append(0)\n        self.class_counts_ = np.array(selected_class_counts)\n\n        self.draw()\n\n        # Retrieve and store the score attribute from the sklearn classifier\n        self.score_ = self.estimator.score(X, y)\n\n        return self.score_",
        "rewrite": "```python\nimport numpy as np\n\ndef score(self, X, y):\n    # Create predictions from X (will raise not fitted error)\n    y_pred = self.predict(X)\n\n    # Encode the target with the supplied label encoder\n    if self.label_encoder:\n        try:\n            y_enc = self.label_encoder.transform(y)\n            y_pred_enc = self.label_encoder.transform(y_pred)\n        except AttributeError:\n            # if a mapping is passed to class apply it here.\n            y_enc = np.array([self.label_encoder[x] for x in y])\n            y_pred_enc = np.array([self.label_encoder[x] for x"
    },
    {
        "original": "def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        \"\"\"\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        \"\"\"\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset['annotations']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset['annotations']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n        else:\n            ids = [ann['id'] for ann in anns]\n        return ids",
        "rewrite": "```python\ndef getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n    imgIds = [imgIds] if not _isArrayLike(imgIds) else imgIds\n    catIds = [catIds] if not _isArrayLike(catIds) else catIds\n\n    if len(imgIds) == len(catIds) == len(areaRng) == 0:\n        anns = self.dataset['annotations']\n    elif not len(imgId for imgId in imgIDs if imgId in self.imgToAnns):\n        anns = self.dataset['annotations']\n    else:\n"
    },
    {
        "original": "def get_services(profile='pagerduty', subdomain=None, api_key=None):\n    \"\"\"\n    List services belonging to this account\n\n    CLI Example:\n\n        salt myminion pagerduty.get_services\n    \"\"\"\n\n    return _list_items(\n        'services',\n        'id',\n        profile=profile,\n        subdomain=subdomain,\n        api_key=api_key,\n    )",
        "rewrite": "```python\ndef get_services(profile='pagerduty', subdomain=None, api_key=None):\n    return _list_items(\n        'services',\n        'id',\n        profile=profile,\n        subdomain=subdomain,\n        api_key=api_key,\n    )\n```"
    },
    {
        "original": "def dumps(self, fd, **kwargs):\n        \"\"\"\n        Returns the concrete content for a file descriptor.\n\n        BACKWARD COMPATIBILITY: if you ask for file descriptors 0 1 or 2, it will return the data from stdin, stdout,\n        or stderr as a flat string.\n\n        :param fd:  A file descriptor.\n        :return:    The concrete content.\n        :rtype:     str\n        \"\"\"\n        if 0 <= fd <= 2:\n            data = [self.stdin, self.stdout, self.stderr][fd].concretize(**kwargs)\n            if type(data) is list:\n                data = b''.join(data)\n            return data\n        return self.get_fd(fd).concretize(**kwargs)",
        "rewrite": "```python\ndef dumps(self, fd, **kwargs):\n    if 0 <= fd <= 2:\n        data = [self.stdin, self.stdout, self.stderr][fd].concretize(**kwargs)\n        if isinstance(data, list):\n            data = b''.join(data)\n    else:\n        data = self.get_fd(fd).concretize(**kwargs)\n    return data.encode('bytes') if isinstance(data, str) else data\n```"
    },
    {
        "original": "def get_carrier_concentration(self):\n        \"\"\"\n        gives the carrier concentration (in cm^-3)\n\n        Returns\n            a dictionary {temp:[]} with an array of carrier concentration\n            (in cm^-3) at each temperature\n            The array relates to each step of electron chemical potential\n        \"\"\"\n\n        return {temp: [1e24 * i / self.vol for i in self._carrier_conc[temp]]\n                for temp in self._carrier_conc}",
        "rewrite": "```python\ndef get_carrier_concentration(self):\n    return {temp: [1e24 * i / self.vol for i in self._carrier_conc[temp]] for temp in self._carrier_conc}\n```"
    },
    {
        "original": "def _stack_values_to_string(self, stack_values):\n        \"\"\"\n        Convert each stack value to a string\n\n        :param stack_values: A list of values\n        :return: The converted string\n        \"\"\"\n\n        strings = [ ]\n        for stack_value in stack_values:\n            if self.solver.symbolic(stack_value):\n                concretized_value = \"SYMBOLIC - %s\" % repr(stack_value)\n            else:\n                if len(self.solver.eval_upto(stack_value, 2)) == 2:\n                    concretized_value = repr(stack_value)\n                else:\n                    concretized_value = repr(stack_value)\n            strings.append(concretized_value)\n\n        return \" .. \".join(strings)",
        "rewrite": "```python\ndef _stack_values_to_string(self, stack_values):\n    strings = [repr(stack_value)\n               if not self.solver.symbolic(stack_value)\n               else f\"Srng {repr(stack_value)}\"\n               for stack_value in stack_values]\n    return \" .. \".join(strings)\n```"
    },
    {
        "original": "def create_information_tear_sheet(factor_data,\n                                  group_neutral=False,\n                                  by_group=False):\n    \"\"\"\n    Creates a tear sheet for information analysis of a factor.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_neutral : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    ic = perf.factor_information_coefficient(factor_data, group_neutral)\n\n    plotting.plot_information_table(ic)\n\n    columns_wide = 2\n    fr_cols = len(ic.columns)\n    rows_when_wide = (((fr_cols - 1) // columns_wide) + 1)\n    vertical_sections = fr_cols + 3 * rows_when_wide + 2 * fr_cols\n    gf = GridFigure(rows=vertical_sections, cols=columns_wide)\n\n    ax_ic_ts = [gf.next_row() for _ in range(fr_cols)]\n    plotting.plot_ic_ts(ic, ax=ax_ic_ts)\n\n    ax_ic_hqq = [gf.next_cell() for _ in range(fr_cols * 2)]\n    plotting.plot_ic_hist(ic, ax=ax_ic_hqq[::2])\n    plotting.plot_ic_qq(ic, ax=ax_ic_hqq[1::2])\n\n    if not by_group:\n\n        mean_monthly_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=False,\n                                              by_time=\"M\")\n        ax_monthly_ic_heatmap = [gf.next_cell() for x in range(fr_cols)]\n        plotting.plot_monthly_ic_heatmap(mean_monthly_ic,\n                                         ax=ax_monthly_ic_heatmap)\n\n    if by_group:\n        mean_group_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=True)\n\n        plotting.plot_ic_by_group(mean_group_ic, ax=gf.next_row())\n\n    plt.show()\n    gf.close()",
        "rewrite": "```python\ndef create_information_tear_sheet(factor_data,\n                                 group_neutral=False,\n                                 by_group=False):\n    ic = perf.factor_information_coefficient(factor_data, group_neutral)\n\n    plotting.plot_information_table(ic)\n\n    columns_wide = 2\n    fr_cols = len(ic.columns)\n    rows_when_wide = ((fr_cols - 1) // columns_wide) + 1\n\n    gf = GridFigure(rows=3*(fr_cols * 2) + fr_cols + columns_wide*2, cols=columns_wide)\n    \n    ax_ic_ts = [gf.next_row() for _ in"
    },
    {
        "original": "def _build(self):\n    \"\"\"Returns a tuple containing observation and target one-hot tensors.\"\"\"\n    q = tf.FIFOQueue(\n        self._queue_capacity, [self._dtype, self._dtype],\n        shapes=[[self._num_steps, self._batch_size, self._vocab_size]]*2)\n    obs, target = tf.py_func(self._get_batch, [], [tf.int32, tf.int32])\n    obs = self._one_hot(obs)\n    target = self._one_hot(target)\n    enqueue_op = q.enqueue([obs, target])\n    obs, target = q.dequeue()\n    tf.train.add_queue_runner(tf.train.QueueRunner(q, [enqueue_op]))\n    return SequenceDataOpsNoMask(obs, target)",
        "rewrite": "```python\ndef _build(self):\n    q = tf.FIFOQueue(\n        capacity=self._queue_capacity, dtypes=[self._dtype, self._dtype],\n        shapes=[[self.num_steps, self.batch_size, self.vocab_size]]*2)\n    obs, target = tf.py_func(self._get_batch)\n    obs = self._one_hot(obs)\n    target = self._one_hot(target)\n    enqueue_op = q.enqueue([obs, target])\n    _, obs, target = tf.train.queue_runner.add_queue_runner(\n        tf.train.QueueRunner(q, [enqueue_op]))\n    return SequenceDataOpsNoMask(obs[:,"
    },
    {
        "original": "def dyndns_add(nameserver, name, rdata, type=\"A\", ttl=10):\n    \"\"\"Send a DNS add message to a nameserver for \"name\" to have a new \"rdata\"\ndyndns_add(nameserver, name, rdata, type=\"A\", ttl=10) -> result code (0=ok)\n\nexample: dyndns_add(\"ns1.toto.com\", \"dyn.toto.com\", \"127.0.0.1\")\nRFC2136\n\"\"\"\n    zone = name[name.find(\".\") + 1:]\n    r = sr1(IP(dst=nameserver) / UDP() / DNS(opcode=5,\n                                             qd=[DNSQR(qname=zone, qtype=\"SOA\")],  # noqa: E501\n                                             ns=[DNSRR(rrname=name, type=\"A\",\n                                                       ttl=ttl, rdata=rdata)]),\n            verbose=0, timeout=5)\n    if r and r.haslayer(DNS):\n        return r.getlayer(DNS).rcode\n    else:\n        return -1",
        "rewrite": "```python\nimport scapy.all as scapy\n\ndef dyndns_add(nameserver, name, rdata, type=\"A\", ttl=10):\n    zone = name[name.rfind(\".\") + 1:]\n    r = scapy.sr1(scapy.IP(dst=nameserver) / \n                 scapy.UDP(dport=53) / \n                 scapy.DNS(opcode=5,\n                            qd=[scapy.DNSQR(qname=zone, qtype=\"SOA\")],\n                            an=[scapy.DNSRR(rrname=name,\n                                             type=\"A\",\n                                             ttl=ttl,\n                                             r"
    },
    {
        "original": "def string_asset(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the StringAsset class. Registers the decorated class\n    as the StringAsset known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _string_asset_resource_type\n    _string_asset_resource_type = class_obj\n    return class_obj",
        "rewrite": "```python\ndef string_asset(class_obj: type) -> type:\n    if not isinstance(class_obj, type):\n        raise TypeError(\"class_obj is not a Class\")\n    global _string_asset_resource_type\n    _string_asset_resource_type = class_obj\n    return class_obj\n```"
    },
    {
        "original": "def group(self, id, expand=None):\n        \"\"\"Get a group Resource from the server.\n\n        :param id: ID of the group to get\n        :param id: str\n        :param expand: Extra information to fetch inside each resource\n        :type expand: Optional[Any]\n\n        :rtype: User\n        \"\"\"\n        group = Group(self._options, self._session)\n        params = {}\n        if expand is not None:\n            params['expand'] = expand\n        group.find(id, params=params)\n        return group",
        "rewrite": "```python\ndef group(self, id: str, expand: Any = None) -> User:\n    group = Group(self._options, self._session)\n    params = {}\n    if expand is not None:\n        params['expand'] = expand\n    return group.find(id, params=params)\n```"
    },
    {
        "original": "def _prepare_for_training(self, job_name=None):\n        \"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\n\n        Args:\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        super(Framework, self)._prepare_for_training(job_name=job_name)\n\n        # validate source dir will raise a ValueError if there is something wrong with the\n        # source directory. We are intentionally not handling it because this is a critical error.\n        if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n            validate_source_dir(self.entry_point, self.source_dir)\n\n        # if we are in local mode with local_code=True. We want the container to just\n        # mount the source dir instead of uploading to S3.\n        local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n        if self.sagemaker_session.local_mode and local_code:\n            # if there is no source dir, use the directory containing the entry point.\n            if self.source_dir is None:\n                self.source_dir = os.path.dirname(self.entry_point)\n            self.entry_point = os.path.basename(self.entry_point)\n\n            code_dir = 'file://' + self.source_dir\n            script = self.entry_point\n        else:\n            self.uploaded_code = self._stage_user_code_in_s3()\n            code_dir = self.uploaded_code.s3_prefix\n            script = self.uploaded_code.script_name\n\n        # Modify hyperparameters in-place to point to the right code directory and script URIs\n        self._hyperparameters[DIR_PARAM_NAME] = code_dir\n        self._hyperparameters[SCRIPT_PARAM_NAME] = script\n        self._hyperparameters[CLOUDWATCH_METRICS_PARAM_NAME] = self.enable_cloudwatch_metrics\n        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level\n        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name\n        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name",
        "rewrite": "```python\ndef _prepare_for_training(self, job_name=None):\n    super(Framework, self)._prepare_for_training(job_name=job_name)\n\n    if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n        try:\n            validate_source_dir(self.entry_point, self.source_dir)\n        except ValueError as e:\n            raise AssertionError(f\"Invalid source directory: {e}\")\n\n    local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n    if self.sagemaker_session.local_mode and local_code:\n        if not self.source_dir:\n            try:\n                import os\n               "
    },
    {
        "original": "def nice_output(self):\n        \"\"\"Return a string for printing\"\"\"\n        dates = [\n            str_format('Opening Day {0}: {1}.',\n                       [self.year, date_format(self.first_date_seas)]),\n            str_format('Last day of the 1st half: {0}.',\n                       [date_format(self.last_date_1sth)]),\n            str_format('{0} All Star Game: {1}.',\n                       [self.year, date_format(self.all_star_date)]),\n            str_format('First day of the 2nd half: {}.',\n                       [date_format(self.first_date_2ndh)]),\n            str_format('Last day of the {0} season: {1}.',\n                       [self.year, date_format(self.last_date_seas)]),\n            str_format('{0} Playoffs start: {1}.',\n                       [self.year, date_format(self.playoffs_start_date)]),\n            str_format('{0} Playoffs end: {1}.',\n                       [self.year, date_format(self.playoffs_end_date)])\n        ]\n        return '\\n'.join(dates)",
        "rewrite": "```python\ndef nice_output(self):\n    dates = [\n        f'Opening Day {self.year}: {self.first_date_seas}.',\n        f'Last day of the 1st half: {self.last_date_1sth}.',\n        f'{self.year} All Star Game: {self.all_star_date}.',\n        f'First day of the 2nd half: {self.first_date_2ndh}.',\n        f'Last day of the {self.year} season: {self.last_date_seas}.',\n        f'{year} Playoffs start: {playoffs_start_data}.format({year"
    },
    {
        "original": "def find(self, selector, collation=None):\n        \"\"\"Specify selection criteria for bulk operations.\n\n        :Parameters:\n          - `selector` (dict): the selection criteria for update\n            and remove operations.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only\n            supported on MongoDB 3.4 and above.\n\n        :Returns:\n          - A :class:`BulkWriteOperation` instance, used to add\n            update and remove operations to this bulk operation.\n\n        .. versionchanged:: 3.4\n           Added the `collation` option.\n\n        \"\"\"\n        validate_is_mapping(\"selector\", selector)\n        return BulkWriteOperation(selector, self.__bulk, collation)",
        "rewrite": "```python\ndef find(self, selector, collation=None):\n    validate_is_mapping(\"selector\", selector)\n    if collation is not None and self.server_version < (3, 4):\n        raise ValueError(\"Collation is only supported on MongoDB 3.4 and above.\")\n    return BulkWriteOperation(selector, self.__bulk, collation)\n```"
    },
    {
        "original": "def finalize(self, **kwargs):\n        \"\"\"\n        The finalize method executes any subclass-specific axes\n        finalization steps. The user calls poof & poof calls finalize.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        \"\"\"\n        # Set the title\n        self.set_title(\n            'Frequency Distribution of Top {} tokens'.format(self.N)\n        )\n\n        # Create the vocab, count, and hapaxes labels\n        infolabel = \"vocab: {:,}\\nwords: {:,}\\nhapax: {:,}\".format(\n            self.vocab_, self.words_, self.hapaxes_\n        )\n\n        self.ax.text(0.68, 0.97, infolabel, transform=self.ax.transAxes,\n                     fontsize=9, verticalalignment='top',\n                     bbox={'boxstyle':'round', 'facecolor':'white', 'alpha':.8})\n\n        # Set the legend and the grid\n        self.ax.legend(loc='upper right', frameon=True)",
        "rewrite": "```python\ndef finalize(self, **kwargs):\n    self.set_title('Frequency Distribution of Top {} tokens'.format(self.N))\n    infolabel = \"vocab: {:,}\\nwords: {:,}\\nhapax: {:,}\".format(\n        self.vocab_, self.words_, self.hapaxes_\n    )\n    self.ax.text(0.68, 0.97, infolabel,\n                transform=self.ax.transAxes,\n                fontsize=9,\n                verticalalignment='top',\n                bbox={'boxstyle': 'round', 'facecolor': 'white', 'alpha': .8})\n    if hasattr(self"
    },
    {
        "original": "def post_dump(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after serializing an object. The method\n    receives the serialized object and returns the processed object.\n\n    By default, receives a single object at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before serializing) will be passed as\n    an additional argument to the method.\n    \"\"\"\n    return set_hook(fn, (POST_DUMP, pass_many), pass_original=pass_original)",
        "rewrite": "```python\ndef post_dump(fn=None, pass_many=False, pass_original=False):\n    return set_hook(fn, (POST_DUMP, pass_many), **{'pass_original': pass_original})\n```"
    },
    {
        "original": "def fwd_chunk(self):\n        \"\"\"\n        Returns the chunk following this chunk in the list of free chunks. If this chunk is not free, then it resides in\n        no such list and this method raises an error.\n\n        :returns: If possible, the forward chunk; otherwise, raises an error\n        \"\"\"\n        if self.is_free():\n            base = self.state.memory.load(self.base + 2 * self._chunk_size_t_size, self._chunk_size_t_size)\n            return PTChunk(base, self.state)\n        else:\n            raise SimHeapError(\"Attempted to access the forward chunk of an allocated chunk\")",
        "rewrite": "```python\ndef fwd_chunk(self):\n    if self.is_free():\n        base = self.state.memory.load(self.base + 2 * self._chunk_size_t_size, self._chunk_size_t_size)\n        return PTChunk(base, self.state)\n    else:\n        raise SimHeapError(f\"Attempted to access the forward chunk of an allocated chunk ({self.base})\")\n```"
    },
    {
        "original": "def _validate(dns_proto, dns_servers, ip_proto, ip_addrs, gateway):\n    \"\"\"\n    Ensure that the configuration passed is formatted correctly and contains\n    valid IP addresses, etc.\n    \"\"\"\n    errors = []\n    # Validate DNS configuration\n    if dns_proto == 'dhcp':\n        if dns_servers is not None:\n            errors.append(\n                'The dns_servers param cannot be set if unless dns_proto is '\n                'set to \\'static\\''\n            )\n    else:\n        if str(dns_servers).lower() in ['none', '[]']:\n            pass\n        elif not isinstance(dns_servers, list):\n            errors.append(\n                'The dns_servers param must be formatted as a list'\n            )\n        else:\n            bad_ips = [x for x in dns_servers\n                       if not salt.utils.validate.net.ipv4_addr(x)]\n            if bad_ips:\n                errors.append('Invalid DNS server IPs: {0}'\n                              .format(', '.join(bad_ips)))\n\n    # Validate IP configuration\n    if ip_proto == 'dhcp':\n        if ip_addrs is not None:\n            errors.append(\n                'The ip_addrs param cannot be set if unless ip_proto is set '\n                'to \\'static\\''\n            )\n        if gateway is not None:\n            errors.append(\n                'A gateway IP cannot be set if unless ip_proto is set to '\n                '\\'static\\''\n            )\n    else:\n        if not ip_addrs:\n            errors.append(\n                'The ip_addrs param is required to set static IPs'\n            )\n        elif not isinstance(ip_addrs, list):\n            errors.append(\n                'The ip_addrs param must be formatted as a list'\n            )\n        else:\n            bad_ips = [x for x in ip_addrs\n                       if not salt.utils.validate.net.ipv4_addr(x)]\n            if bad_ips:\n                errors.append('The following static IPs are invalid: '\n                              '{0}'.format(', '.join(bad_ips)))\n\n            # Validate default gateway\n            if gateway is not None:\n                if not salt.utils.validate.net.ipv4_addr(gateway):\n                    errors.append('Gateway IP {0} is invalid'.format(gateway))\n\n    return errors",
        "rewrite": "```python\ndef _validate(dns_proto, dns_servers, ip_proto, ip_addrs, gateway):\n    errors = []\n    \n    if dns_proto == 'dhcp':\n        if dns_servers is not None:\n            errors.append('The dns_servers param cannot be set if unless dns_proto is set to \\'static\\'')\n    else:\n        try:\n            if str(dns_servers).lower() in ['none', '[]']:\n                pass\n            elif not isinstance(dns_servers, list):\n                errors.append('The dns_servers param must be formatted as a list')\n            else:\n                bad_ips = [x for x in dns"
    },
    {
        "original": "def create_and_link_vrf_table(self, vrf_conf):\n        \"\"\"Factory method to create VRF table for given `vrf_conf`.\n\n        Adds mapping to this table with appropriate scope. Also, adds mapping\n        for import RT of this VRF to created table to facilitate\n        importing/installing of paths from global tables.\n        Returns created table.\n        \"\"\"\n        route_family = vrf_conf.route_family\n\n        if route_family == VRF_RF_IPV4:\n            vrf_table = Vrf4Table\n        elif route_family == VRF_RF_IPV6:\n            vrf_table = Vrf6Table\n        elif route_family == VRF_RF_L2_EVPN:\n            vrf_table = VrfEvpnTable\n        elif route_family == VRF_RF_IPV4_FLOWSPEC:\n            vrf_table = Vrf4FlowSpecTable\n        elif route_family == VRF_RF_IPV6_FLOWSPEC:\n            vrf_table = Vrf6FlowSpecTable\n        elif route_family == VRF_RF_L2VPN_FLOWSPEC:\n            vrf_table = L2vpnFlowSpecTable\n        else:\n            raise ValueError('Unsupported route family for VRF: %s' %\n                             route_family)\n\n        vrf_table = vrf_table(vrf_conf, self._core_service, self._signal_bus)\n        table_id = (vrf_conf.route_dist, route_family)\n        self._tables[table_id] = vrf_table\n\n        assert vrf_table is not None\n        LOG.debug('Added new VrfTable with route_dist:%s and route_family:%s',\n                  vrf_conf.route_dist, route_family)\n\n        import_rts = vrf_conf.import_rts\n        # If VRF is configured with import RT, we put this table\n        # in a list corresponding to this RT for easy access.\n        if import_rts:\n            self._link_vrf_table(vrf_table, import_rts)\n\n        return vrf_table",
        "rewrite": "```python\ndef create_and_link_vrf_table(self, vrf_conf):\n    \"\"\"Factory method to create VRF table for given `vrf_conf`.\n\n    Adds mapping to this table with appropriate scope. Also, adds mapping\n    for import RT of this VRF to created table to facilitate\n    importing/installing of paths from global tables.\n    \n    Returns created table.\n    \"\"\"\n    \n    route_family = vrf_conf.route_family\n    \n    if route_family == VRF_RF_IPV4:\n        vrf_table_class = Vrf4Table\n    elif route_family == VRF_RF_IPV6:\n        vrf_table_class"
    },
    {
        "original": "def seek_to_end(self, *partitions):\n        \"\"\"Seek to the most recent available offset for partitions.\n\n        Arguments:\n            *partitions: Optionally provide specific TopicPartitions, otherwise\n                default to all assigned partitions.\n\n        Raises:\n            AssertionError: If any partition is not currently assigned, or if\n                no partitions are assigned.\n        \"\"\"\n        if not all([isinstance(p, TopicPartition) for p in partitions]):\n            raise TypeError('partitions must be TopicPartition namedtuples')\n        if not partitions:\n            partitions = self._subscription.assigned_partitions()\n            assert partitions, 'No partitions are currently assigned'\n        else:\n            for p in partitions:\n                assert p in self._subscription.assigned_partitions(), 'Unassigned partition'\n\n        for tp in partitions:\n            log.debug(\"Seeking to end of partition %s\", tp)\n            self._subscription.need_offset_reset(tp, OffsetResetStrategy.LATEST)",
        "rewrite": "```python\ndef seek_to_end(self, *partitions):\n    if not partitions:\n        partitions = self._subscription.assigned_partitions()\n        if not partitions:\n            raise AssertionError('No partitions are currently assigned')\n    else:\n        for p in partitions:\n            assert p in self._subscription.assigned_partitions(), 'Unassigned partition'\n\n    for tp in set(partitions):  # Using set to avoid duplicate log messages\n        self._subscription.need_offset_reset(tp, OffsetResetStrategy.LATEST)\n        log.debug(\"Seeking to end of partition %s\", tp)\n```"
    },
    {
        "original": "def check_quota(self):\n        \"\"\"\n        Check whether the user is within quota.  Should be called before\n        every write.  Will raise() if the library has exceeded its allotted\n        quota.\n        \"\"\"\n        # Don't check on every write, that would be slow\n        if self.quota_countdown > 0:\n            self.quota_countdown -= 1\n            return\n\n        # Re-cache the quota after the countdown\n        self.quota = self.get_library_metadata(ArcticLibraryBinding.QUOTA)\n        if self.quota is None or self.quota == 0:\n            self.quota = 0\n            return\n\n        # Figure out whether the user has exceeded their quota\n        library = self.arctic[self.get_name()]\n        stats = library.stats()\n\n        def to_gigabytes(bytes_):\n            return bytes_ / 1024. / 1024. / 1024.\n\n        # Have we exceeded our quota?\n        size = stats['totals']['size']\n        count = stats['totals']['count']\n        if size >= self.quota:\n            raise QuotaExceededException(\"Mongo Quota Exceeded: %s %.3f / %.0f GB used\" % (\n                '.'.join([self.database_name, self.library]),\n                to_gigabytes(size),\n                to_gigabytes(self.quota)))\n\n        # Quota not exceeded, print an informational message and return\n        try:\n            avg_size = size // count if count > 1 else 100 * 1024\n            remaining = self.quota - size\n            remaining_count = remaining / avg_size\n            if remaining_count < 100 or float(remaining) / self.quota < 0.1:\n                logger.warning(\"Mongo Quota: %s %.3f / %.0f GB used\" % (\n                    '.'.join([self.database_name, self.library]),\n                    to_gigabytes(size),\n                    to_gigabytes(self.quota)))\n            else:\n                logger.info(\"Mongo Quota: %s %.3f / %.0f GB used\" % (\n                    '.'.join([self.database_name, self.library]),\n                    to_gigabytes(size),\n                    to_gigabytes(self.quota)))\n\n            # Set-up a timer to prevent us for checking for a few writes.\n            # This will check every average half-life\n            self.quota_countdown = int(max(remaining_count // 2, 1))\n        except Exception as e:\n            logger.warning(\"Encountered an exception while calculating quota statistics: %s\" % str(e))",
        "rewrite": "```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass QuotaExceededException(Exception):\n    \"\"\" Raised when the quota for a library exceeds\"\"\" \n\ndef to_gigabytes(size: int) -> float:\n    \"\"\" Convert bytes to gigabytes\"\"\"\n    return size / 1024. / 1024. / 1024.\n\ndef check_quota(self):\n    \"\"\"\n    Check whether the user is within quota. \n    Should be called before every write.\n    Will raise QuotaExceededException if the library has exceeded its allotted quota.\n    \n    Raises:\n        QuotaExceededException : When the user has"
    },
    {
        "original": "def repr_size(n_bytes):\n    \"\"\"\n    >>> repr_size(1000)\n    '1000 Bytes'\n    >>> repr_size(8257332324597)\n    '7.5 TiB'\n    \"\"\"\n    if n_bytes < 1024:\n        return '{0} Bytes'.format(n_bytes)\n    i = -1\n    while n_bytes > 1023:\n        n_bytes /= 1024.0\n        i += 1\n    return '{0} {1}iB'.format(round(n_bytes, 1), si_prefixes[i])",
        "rewrite": "```python\ndef repr_size(n_bytes):\n    si_prefixes = ['Bytes', 'KiB', 'MiB', 'GiB', 'TiB']\n    if n_bytes < 1024:\n        return f'{n_bytes} {si_prefixes[0]}'\n    i = 0\n    while n_bytes >= 1024 and i < len(si_prefixes) - 1:\n        n_bytes /= 1024.0\n        i += 1\n    return f'{round(n_bytes, 1)} {si_prefixes[i]}'\n```"
    },
    {
        "original": "def get(self, key, default='', stringify=True):\n    \"\"\"Returns dictionary values or default.\n\n    Args:\n      key: string. Dictionary key to look up.\n      default: string. Return this value if key not found.\n      stringify: bool. Force all return values to string for compatibility\n                 reasons.\n    Returns:\n      python-wrapped CF object or default if not found.\n    \"\"\"\n    obj = self.__getitem__(key)\n    if obj is None:\n      obj = default\n    elif stringify:\n      obj = str(obj)\n    return obj",
        "rewrite": "```python\ndef get(self, key: str, default: str = '', stringify: bool = True) -> object:\n    return self.__getitem__(key) if self.__getitem__(key) is not None else (\n        default if stringify else None\n    ) or (str(self.__getitem__(key)) if stringify else self.__getitem__(key))\n```"
    },
    {
        "original": "def getImageDescriptor(self, im, xy=None):\n        \"\"\" getImageDescriptor(im, xy=None)\n\n        Used for the local color table properties per image.\n        Otherwise global color table applies to all frames irrespective of\n        whether additional colors comes in play that require a redefined\n        palette. Still a maximum of 256 color per frame, obviously.\n\n        Written by Ant1 on 2010-08-22\n        Modified by Alex Robinson in Janurari 2011 to implement subrectangles.\n\n        \"\"\"\n\n        # Defaule use full image and place at upper left\n        if xy is None:\n            xy = (0, 0)\n\n        # Image separator,\n        bb = '\\x2C'\n\n        # Image position and size\n        bb += intToBin(xy[0])  # Left position\n        bb += intToBin(xy[1])  # Top position\n        bb += intToBin(im.size[0])  # image width\n        bb += intToBin(im.size[1])  # image height\n\n        # packed field: local color table flag1, interlace0, sorted table0,\n        # reserved00, lct size111=7=2^(7+1)=256.\n\n        bb += '\\x87'\n\n        # LZW minimum size code now comes later, begining of [image data] blocks\n        return bb",
        "rewrite": "```python\ndef get_image_descriptor(self, im, xy=None):\n    if xy is None:\n        xy = (0, 0)\n\n    image_separator = '\\x2C'\n\n    bb = image_separator\n    bb += int_to_bin(xy[0])\n    bb += int_to_bin(xy[1])\n    width_int = im.size[0]\n    bb += int_to_bin(width_int)\n    \n    height_int = im.size[1]\n    bb += int_to_bin(height_int)\n    \n     # packed field: local color table flag1, interlace0, sorted table0,\n     # reserved00, lct"
    },
    {
        "original": "def CheckClientApprovalRequest(approval_request):\n  \"\"\"Checks if a client approval request is granted.\"\"\"\n\n  _CheckExpired(approval_request)\n  _CheckHasEnoughGrants(approval_request)\n\n  if not client_approval_auth.CLIENT_APPROVAL_AUTH_MGR.IsActive():\n    return True\n\n  token = access_control.ACLToken(username=approval_request.requestor_username)\n  approvers = set(g.grantor_username for g in approval_request.grants)\n\n  labels = sorted(\n      data_store.REL_DB.ReadClientLabels(approval_request.subject_id),\n      key=lambda l: l.name)\n  for label in labels:\n    client_approval_auth.CLIENT_APPROVAL_AUTH_MGR.CheckApproversForLabel(\n        token, rdfvalue.RDFURN(approval_request.subject_id),\n        approval_request.requestor_username, approvers, label.name)\n\n  return True",
        "rewrite": "```python\ndef check_client_approval_request(approval_request):\n    _check_expired(approval_request)\n    _check_has_enough_grants(approval_request)\n\n    if not client_approval_auth.ClientApprovalAuthManager().is_active():\n        return True\n\n    token = access_control.AclToken(username=approval_request.requestor_username)\n    approvers = set(g.grantor_username for g in approval_request.grants)\n\n    labels = sorted(\n        data_store.RelDb().read_client_labels(approval_request.subject_id),\n        key=lambda l: l.name)\n    \n    for label in labels:\n        client_approval_auth.ClientApproval"
    },
    {
        "original": "def AdjustDescriptor(self, fields):\n    \"\"\"Payload-aware metadata processor.\"\"\"\n\n    for f in fields:\n      if f.name == \"args_rdf_name\":\n        f.name = \"payload_type\"\n\n      if f.name == \"args\":\n        f.name = \"payload\"\n\n    return fields",
        "rewrite": "```python\ndef adjust_descriptor(self, fields):\n    for field in fields:\n        if field.name == \"args_rdf_name\":\n            field.name = \"payload_type\"\n        elif field.name == \"args\":\n            field.name = \"payload\"\n    return fields\n```"
    },
    {
        "original": "def on_change(self, server_description):\n        \"\"\"Process a new ServerDescription after an ismaster call completes.\"\"\"\n        # We do no I/O holding the lock.\n        with self._lock:\n            # Any monitored server was definitely in the topology description\n            # once. Check if it's still in the description or if some state-\n            # change removed it. E.g., we got a host list from the primary\n            # that didn't include this server.\n            if self._description.has_server(server_description.address):\n                td_old = self._description\n                if self._publish_server:\n                    old_server_description = td_old._server_descriptions[\n                        server_description.address]\n                    self._events.put((\n                        self._listeners.publish_server_description_changed,\n                        (old_server_description, server_description,\n                         server_description.address, self._topology_id)))\n\n                self._description = updated_topology_description(\n                    self._description, server_description)\n\n                self._update_servers()\n\n                if self._publish_tp:\n                    self._events.put((\n                        self._listeners.publish_topology_description_changed,\n                        (td_old, self._description, self._topology_id)))\n\n                # Wake waiters in select_servers().\n                self._condition.notify_all()",
        "rewrite": "```python\ndef on_change(self, server_description):\n    with self._lock:\n        if self._description.has_server(server_description.address):\n            td_old = self._description\n            if self._publish_server:\n                old_server_description = td_old._server_descriptions[\n                    server_description.address]\n                event_data = (old_server_description, \n                             server_description,\n                             server_description.address,\n                             self._topology_id)\n            else:\n                event_data = None\n\n            new_servers = updated_topology_DESCRIPTION(\n                td_old, \n                [s for s in td_old.get('servers')]) # Not clear what this function does"
    },
    {
        "original": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered",
        "rewrite": "```python\ndef _filter_cache(self, dmap, kdims):\n    filtered = [\n        (k, v)\n        for k, v in dmap.data.items()\n        if not any(v not in kd.values for kd, dv in zip(kdims, k) if kd.values)\n    ]\n    return filtered\n```"
    },
    {
        "original": "def _add_item(self, item, indent_amt):\n        \"\"\"Add an item to the line.\n\n        Reflow the line to get the best formatting after the item is\n        inserted. The bracket depth indicates if the item is being\n        inserted inside of a container or not.\n\n        \"\"\"\n        if self._prev_item and self._prev_item.is_string and item.is_string:\n            # Place consecutive string literals on separate lines.\n            self._lines.append(self._LineBreak())\n            self._lines.append(self._Indent(indent_amt))\n\n        item_text = unicode(item)\n        if self._lines and self._bracket_depth:\n            # Adding the item into a container.\n            self._prevent_default_initializer_splitting(item, indent_amt)\n\n            if item_text in '.,)]}':\n                self._split_after_delimiter(item, indent_amt)\n\n        elif self._lines and not self.line_empty():\n            # Adding the item outside of a container.\n            if self.fits_on_current_line(len(item_text)):\n                self._enforce_space(item)\n\n            else:\n                # Line break for the new item.\n                self._lines.append(self._LineBreak())\n                self._lines.append(self._Indent(indent_amt))\n\n        self._lines.append(item)\n        self._prev_item, self._prev_prev_item = item, self._prev_item\n\n        if item_text in '([{':\n            self._bracket_depth += 1\n\n        elif item_text in '}])':\n            self._bracket_depth -= 1\n            assert self._bracket_depth >= 0",
        "rewrite": "```python\ndef _add_item(self, item, indent_amt):\n    \"\"\"Add an item to the line.\n\n    Reflow the line to get the best formatting after the item is\n    inserted. The bracket depth indicates if the item is being\n    inserted inside of a container or not.\n    \"\"\"\n    \n    if self._prev_item and self._prev_item.is_string and item.is_string:\n        # Place consecutive string literals on separate lines.\n        self._lines.append(self._LineBreak())\n        self._lines.append(self._Indent(indent_amt))\n            \n    item_text = str(item)  # Renamed from unicode() for"
    },
    {
        "original": "def create_ipsecpolicy(name, profile=None, **kwargs):\n    \"\"\"\n    Creates a new IPsecPolicy\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutron.create_ipsecpolicy ipsecpolicy-name\n                transform_protocol=esp auth_algorithm=sha1\n                encapsulation_mode=tunnel encryption_algorithm=aes-128\n\n    :param name: Name of the IPSec policy\n    :param transform_protocol: Transform protocol in lowercase,\n            default: esp (Optional)\n    :param auth_algorithm: Authentication algorithm in lowercase,\n            default: sha1 (Optional)\n    :param encapsulation_mode: Encapsulation mode in lowercase,\n            default: tunnel (Optional)\n    :param encryption_algorithm: Encryption algorithm in lowercase,\n            default:aes-128 (Optional)\n    :param pfs: Prefect Forward Security in lowercase,\n            default: group5 (Optional)\n    :param units: IPSec lifetime attribute. default: seconds (Optional)\n    :param value: IPSec lifetime attribute. default: 3600 (Optional)\n    :param profile: Profile to build on (Optional)\n    :return: Created IPSec policy information\n    \"\"\"\n    conn = _auth(profile)\n    return conn.create_ipsecpolicy(name, **kwargs)",
        "rewrite": "```python\nfrom salt.client import LocalClient\n\ndef create_ipsecpolicy(name, transform_protocol='esp', auth_algorithm='sha1',\n                        encapsulation_mode='tunnel', encryption_algorithm='aes-128',\n                        pfs='group5', units='seconds', value=3600, profile=None):\n    _auth(profile)\n    cli = LocalClient()\n    return cli.cmd('neutron.create_ipsecpolicy',\n                  ipsecpolicy_name=name,\n                  transform_protocol=transform_protocol,\n                  auth_algorithm=auth_algorithm,\n                  encapsulation_mode=encapsulation_mode,\n                  encryptionAlgorithm=encryption_algorithm,\n                  pfs"
    },
    {
        "original": "def IsRunning(self):\n    \"\"\"Returns True if there's a currently running iteration of this job.\"\"\"\n    current_urn = self.Get(self.Schema.CURRENT_FLOW_URN)\n    if not current_urn:\n      return False\n\n    try:\n      current_flow = aff4.FACTORY.Open(\n          urn=current_urn, aff4_type=flow.GRRFlow, token=self.token, mode=\"r\")\n    except aff4.InstantiationError:\n      # This isn't a flow, something went really wrong, clear it out.\n      logging.error(\"Unable to open cron job run: %s\", current_urn)\n      self.DeleteAttribute(self.Schema.CURRENT_FLOW_URN)\n      self.Flush()\n      return False\n\n    return current_flow.GetRunner().IsRunning()",
        "rewrite": "```python\ndef IsRunning(self):\n    current_urn = self.Get(self.Schema.CURRENT_FLOW_URN)\n    if not current_urn:\n        return False\n\n    try:\n        flow = aff4.FACTORY.Open(\n            urn=current_urn, aff4_type=flow.GRRFlow, token=self.token, mode=\"r\")\n        return flow.GetRunner().IsRunning()\n    except aff4.InstantiationError as e:\n        logging.error(\"Unable to open cron job run: %s\", current_urn)\n        self.DeleteAttribute(self.Schema.CURRENT_FLOW_URN)\n        self.Flush()\n```"
    },
    {
        "original": "def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json",
        "rewrite": "```python\ndef _api_config_item(self, item):\n    response.content_type = 'application/json; charset=utf-8'\n\n    config_dict = self.config.as_dict()\n    \n    if item not in config_dict:\n        raise web.HTTPError(400, \"Unknown configuration item %s\" % item)\n\n    try:\n        args_json = json.dumps(config_dict[item])\n    except Exception as e:\n        raise web.HTTPError(404, \"Cannot get config item (%s)\" % str(e))\n\n    return args_json\n```"
    },
    {
        "original": "def from_file(cls, filepath):\n        \"\"\"Build a :class:`Product` instance from a filepath.\"\"\"\n        # Find the abinit extension.\n        for i in range(len(filepath)):\n            if filepath[i:] in abi_extensions():\n                ext = filepath[i:]\n                break\n        else:\n            raise ValueError(\"Cannot detect abinit extension in %s\" % filepath)\n\n        return cls(ext, filepath)",
        "rewrite": "```python\ndef from_file(cls, filepath):\n    for i in range(len(filepath), 0, -1):\n        if filepath.endswith(abi_extensions()[i:]):\n            ext = filepath[i:]\n            break\n    else:\n        raise ValueError(\"Cannot detect abinit extension in %s\" % filepath)\n    return cls(ext,(filepath))\n```"
    },
    {
        "original": "def protocols(self):\n        \"\"\"\n        :rtype: dict[int, list of ProtocolAnalyzer]\n        \"\"\"\n        result = {}\n        for i, group in enumerate(self.rootItem.children):\n            result[i] = [child.protocol for child in group.children]\n\n        return result",
        "rewrite": "```python\ndef protocols(self) -> dict[int, list[str]]:\n    return {i: [child.protocol for child in group.children] for i, group in enumerate(self.rootItem.children)}\n```"
    },
    {
        "original": "def process_element(self, element, key, **params):\n        \"\"\"\n        The process_element method allows a single element to be\n        operated on given an externally supplied key.\n        \"\"\"\n        self.p = param.ParamOverrides(self, params)\n        return self._apply(element, key)",
        "rewrite": "```python\ndef process_element(self, element, key, **params):\n    self.p = param.ParamOverrides(self, params)\n    return self._apply(element, key)\n```"
    },
    {
        "original": "def remove_bond(self, idx1, idx2):\n        \"\"\"\n        Remove a bond from an openbabel molecule\n\n        Args:\n            idx1: The atom index of one of the atoms participating the in bond\n            idx2: The atom index of the other atom participating in the bond \n        \"\"\"\n        for obbond in ob.OBMolBondIter(self._obmol):\n            if (obbond.GetBeginAtomIdx() == idx1 and obbond.GetEndAtomIdx() == idx2) or (obbond.GetBeginAtomIdx() == idx2 and obbond.GetEndAtomIdx() == idx1):\n                self._obmol.DeleteBond(obbond)",
        "rewrite": "```python\ndef remove_bond(self, idx1, idx2):\n    for obbond in ob.OBMolBondIter(self._obmol):\n        if (obbond.GetBeginAtomIdx() == idx1 and obbond.GetEndAtomIdx() == idx2) or \\\n           (obbond.GetBeginAtomIdx() == idx2 and obbond.GetEndAtomIdx() == idx1):\n            self._obmol.DeleteBond(obbond)\n            break\n```"
    },
    {
        "original": "def expand_specializations(session, class_names):\n    \"\"\"\n    Checks whether any given name is not a class but a specialization.\n\n    If it's a specialization, expand the list of class names with the child\n    class names.\n    \"\"\"\n    result = []\n    for class_name in class_names:\n        specialization = SpecializationV1.create(session, class_name)\n        if specialization is None:\n            result.append(class_name)\n        else:\n            result.extend(specialization.children)\n            logging.info('Expanded specialization \"%s\" into the following'\n                         ' classes: %s',\n                         class_name, ' '.join(specialization.children))\n\n    return result",
        "rewrite": "```python\ndef expand_specializations(session, class_names):\n    result = {class_name: [] for class_name in class_names}\n    for class_name in class_names:\n        specialization = SpecializationV1.create(session, class_name)\n        if specialization is None:\n            result[class_name] = [class_name]\n        else:\n            result[class_name] = specialization.children\n\n    expanded_classes = []\n    for _, children in result.items():\n        if len(children) > 1:\n            expanded_classes.extend(children)\n            logging.info('Expanded specialization \"%s\" into the following classes: %s', list(result.keys())[list(result.values"
    },
    {
        "original": "def find_region_end(self, lines):\n        \"\"\"Find the end of the region started with start and end markers\"\"\"\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            # New code region\n            # Simple code pattern in LightScripts must be preceded with a blank line\n            if self.start_code_re.match(line) or (\n                    self.simple_start_code_re and self.simple_start_code_re.match(line) and\n                    (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n\n                if self.explicit_end_marker_required:\n                    # Metadata here was conditioned on finding an explicit end marker\n                    # before the next start marker. So we dismiss it.\n                    self.metadata = None\n                    self.language = None\n\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if not self.ignore_end_marker and self.end_code_re:\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n            elif _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False",
        "rewrite": "```python\ndef find_region_end(self, lines):\n    if self.metadata and 'cell_type' in self.metadata:\n        self.cell_type = self.metadata.pop('cell_type')\n    else:\n        self.cell_type = 'code'\n\n    parser = StringParser(self.language or self.default_language)\n    \n    for i, line in enumerate(lines):\n        if i == 0 and ('metadata' in vars(self) and vars(self)['metadata'] is not None):\n            continue\n\n        \n        parser.read_line(line)\n        \n        if (\n                (self.start_code_re.match(line) \n                 or (self.simple_start_code_re and self.simple"
    },
    {
        "original": "def _WritePartial(self, data):\n    \"\"\"Writes at most one chunk of data.\"\"\"\n\n    chunk = self.offset // self.chunksize\n    chunk_offset = self.offset % self.chunksize\n    data = utils.SmartStr(data)\n\n    available_to_write = min(len(data), self.chunksize - chunk_offset)\n\n    fd = self._GetChunkForWriting(chunk)\n    fd.seek(chunk_offset)\n\n    fd.write(data[:available_to_write])\n    self.offset += available_to_write\n\n    return data[available_to_write:]",
        "rewrite": "```python\ndef _WritePartial(self, data):\n    chunk = self.offset // self.chunksize\n    chunk_offset = self.offset % self.chunksize\n    data = utils.SmartStr(data)\n    \n    available_to_write = min(len(data), self.chunksize - chunk_offset)\n    \n    with self._GetChunkForWriting(chunk) as fd:\n        fd.seek(chunk_offset)\n        fd.write(data[:available_to_write])\n        \n        remaining_data = data[available_to_write:]\n        \n        if remaining_data:\n            # If there's still remaining data, update the offset and return it.\n            new_chunk_index = (chunk +"
    },
    {
        "original": "def require_fresh_games(self, number_fresh):\n        \"\"\"Require a given number of fresh games to be played.\n\n        Args:\n          number_fresh:  integer, number of new fresh games needed\n\n        Increments the cell `table_state=metadata:wait_for_game_number`\n        by the given number of games.  This will cause\n        `self.wait_for_fresh_games()` to block until the game\n        counter has reached this number.\n        \"\"\"\n        latest = self.latest_game_number\n        table_state = self.bt_table.row(TABLE_STATE)\n        table_state.set_cell(METADATA, WAIT_CELL, int(latest + number_fresh))\n        table_state.commit()\n        print(\"== Setting wait cell to \", int(latest + number_fresh), flush=True)",
        "rewrite": "```python\ndef require_fresh_games(self, number_fresh):\n    self.latest_game_number = int(\n        self.bt_table.get_cell(METADATA, WAIT_CELL).value\n    )\n    table_state = self.bt_table.row(TABLE_STATE)\n    table_state.set_cell(METADATA, WAIT_CELL, int(self.latest_game_number + number_fresh))\n    table_state.commit()\n    print(\"== Setting wait cell to \", int(self.latest_game_number + number_fresh), flush=True)\n```"
    },
    {
        "original": "def create_organization(self, name):\n        \"\"\"\n        To create an organization Jira administrator global permission or agent permission is required\n        depending on the settings\n\n        :param name: str\n        :return: Organization data\n        \"\"\"\n        log.warning('Creating organization...')\n        url = 'rest/servicedeskapi/organization'\n        data = {'name': name}\n\n        return self.post(url, headers=self.experimental_headers, data=data)",
        "rewrite": "```python\ndef create_organization(self, name: str) -> dict:\n    log.warning('Creating organization...')\n    url = 'rest/servicedeskapi/organization'\n    data = {'name': name}\n    \n    return self.post(url, headers=self.experimental_headers, data=data)\n```\n\nor if you prefer to tuple/ in docstrings over comments:\n\n```python\ndef create_organization(self, name: str) -> dict:\n    log.warning('Creating organization...')\n    \n    response = self.post(\n        url='rest/servicedeskapi/organization',\n        headers=self.experimental_headers,\n        data={'name': name}\n   "
    },
    {
        "original": "def new_datetime(d):\n    \"\"\"\n    Generate a safe datetime from a datetime.date or datetime.datetime object.\n    \"\"\"\n    kw = [d.year, d.month, d.day]\n    if isinstance(d, real_datetime):\n        kw.extend([d.hour, d.minute, d.second, d.microsecond, d.tzinfo])\n    return datetime(*kw)",
        "rewrite": "```python\nfrom datetime import datetime, date, time\n\ndef new_datetime(d):\n    if isinstance(d, (date, datetime)):\n        kw = [d.year, d.month, d.day]\n        if isinstance(d, datetime):\n            kw.extend([d.hour, d.minute, d.second])\n            if hasattr(d, 'microsecond'):\n                kw.append(d.microsecond)\n            if hasattr(d.tzinfo):\n                kw.append(hasattr(d.tzinfo.__class__, 'utcoffset') and d.tzinfo.utcoffset() or None)\n        return datetime(*kw)\n    else:\n        raise TypeError(\"Input must"
    },
    {
        "original": "def new_feed(self, name: str, layer_shape: tuple):\n        \"\"\"\n        Creates a feed layer. This is usually the first layer in the network.\n        :param name: name of the layer\n        :return:\n        \"\"\"\n\n        feed_data = tf.placeholder(tf.float32, layer_shape, 'input')\n        self.__network.add_layer(name, layer_output=feed_data)",
        "rewrite": "```python\ndef new_feed(self, name: str, layer_shape: tuple):\n    feed_data = tf.placeholder(tf.float32, shape=layer_shape, name=f'input_{name}')\n    self.__network.add_layer(name, layer_output=feed_data)\n```"
    },
    {
        "original": "def token(request, response, verify_user, context=None, **kwargs):\n    \"\"\"Token verification\n\n    Checks for the Authorization header and verifies using the verify_user function\n    \"\"\"\n    token = request.get_header('Authorization')\n    if token:\n        try:\n            verified_token = verify_user(token)\n        except TypeError:\n            verified_token = verify_user(token, context)\n        if verified_token:\n            return verified_token\n        else:\n            return False\n    return None",
        "rewrite": "```python\ndef token(request, response, verify_user, context=None, **kwargs):\n    \"\"\"Token verification\n\n    Checks for the Authorization header and verifies using the verify_user function\n    \"\"\"\n    token = request.get_header('Authorization')\n    \n    if token:\n        try:\n            return verify_user(token)\n        except TypeError:\n            verified_token = verify_user(token, context)\n            return verified_token\n        else:\n            return False\n    \n    return None\n```"
    },
    {
        "original": "def ParseHeader(table):\n  \"\"\"Parses header of osquery output.\n\n  Args:\n    table: A table in a \"parsed JSON\" representation.\n\n  Returns:\n    A parsed `rdf_osquery.OsqueryHeader` instance.\n  \"\"\"\n  precondition.AssertIterableType(table, dict)\n\n  prototype = None  # type: List[Text]\n\n  for row in table:\n    columns = list(iterkeys(row))\n    if prototype is None:\n      prototype = columns\n    elif prototype != columns:\n      message = \"Expected columns '{expected}', got '{actual}' for table {json}\"\n      message = message.format(expected=prototype, actual=columns, json=table)\n      raise ValueError(message)\n\n  result = rdf_osquery.OsqueryHeader()\n  for name in prototype or []:\n    result.columns.append(rdf_osquery.OsqueryColumn(name=name))\n  return result",
        "rewrite": "```python\nfrom typing import Iterable\nfrom rdf_node import rdf_osquery\n\ndef parse_header(table: Iterable[dict]) -> rdf_osquery.OsqueryHeader:\n    precondition.assert_iterable_type(table, dict)\n\n    columns = set()\n    for row in table:\n        if not columns:\n            columns = set(iter(row))\n        elif set(iter(row)) != columns:\n            raise ValueError(f\"Expected columns '{columns}', got '{iter(row)}' for table {table}\")\n\n    result = rdf_osquery.OsqueryHeader()\n    for name in sorted(columns):  # Added sorting to maintain column order\n        result.columns.append"
    },
    {
        "original": "def send_fetches(self):\n        \"\"\"Send FetchRequests for all assigned partitions that do not already have\n        an in-flight fetch or pending fetch data.\n\n        Returns:\n            List of Futures: each future resolves to a FetchResponse\n        \"\"\"\n        futures = []\n        for node_id, request in six.iteritems(self._create_fetch_requests()):\n            if self._client.ready(node_id):\n                log.debug(\"Sending FetchRequest to node %s\", node_id)\n                future = self._client.send(node_id, request)\n                future.add_callback(self._handle_fetch_response, request, time.time())\n                future.add_errback(log.error, 'Fetch to node %s failed: %s', node_id)\n                futures.append(future)\n        self._fetch_futures.extend(futures)\n        self._clean_done_fetch_futures()\n        return futures",
        "rewrite": "```python\ndef send_fetches(self):\n    futures = []\n    for node_id, request in self._create_fetch_requests().items():\n        if self._client.ready(node_id):\n            log.debug(\"Sending FetchRequest to node %s\", node_id)\n            future = self._client.send(node_id, request)\n            future.addCallback(self._handle_fetch_response, request, time.time())\n            future.addErrback(log.error, 'Fetch to node %s failed: %s', node_id)\n            futures.append(future)\n    self._fetch_futures.extend(futures)\n    self._clean_done_fetch_futures()\n    return"
    },
    {
        "original": "def lock(self, source_node):\n        \"\"\"Lock the task, source is the :class:`Node` that applies the lock.\"\"\"\n        if self.status != self.S_INIT:\n            raise ValueError(\"Trying to lock a task with status %s\" % self.status)\n\n        self._status = self.S_LOCKED\n        self.history.info(\"Locked by node %s\", source_node)",
        "rewrite": "```python\ndef lock(self, source_node):\n    if self.status != self.S_INIT:\n        raise ValueError(f\"Trying to lock a task with status {self.status}\")\n\n    self._status = self.S_LOCKED\n    self.history.info(f\"Locked by node {source_node}\")\n```"
    },
    {
        "original": "def get_dp(app, dpid):\n    \"\"\"\n    :type dpid: datapath id\n    :param dpid:\n    :rtype: ryu.controller.controller.Datapath\n    :returns: datapath corresponding to dpid\n    \"\"\"\n    switches = topo_api.get_switch(app, dpid)\n    if not switches:\n        return None\n    assert len(switches) == 1\n    return switches[0].dp",
        "rewrite": "```python\ndef get_dp(app, dpid):\n    \"\"\"Returns the datapath corresponding to the given dpid\"\"\"\n    switches = topo_api.get_switch(app, dpid)\n    return switches[0].dp if switches else None\n```"
    },
    {
        "original": "def setup_ordered_indices_local_geometry(self, coordination):\n        \"\"\"\n        Sets up ordered indices for the local geometry, for testing purposes\n        :param coordination: coordination of the local geometry\n        \"\"\"\n        self.icentral_site = 0\n        self.indices = list(range(1, coordination + 1))",
        "rewrite": "```python\ndef setup_ordered_indices_local_geometry(self, coordination):\n    self.icentral_site = 0\n    self.indices = list(range(1, coordination + 1))\n    self.indices.insert(0, 0)\n```"
    },
    {
        "original": "def fetchExternalUpdates(self):\r\n        \"\"\"\r\n        !Experimental!\r\n        Calls out to the client code requesting seed values to use in the UI\r\n        !Experimental!\r\n        \"\"\"\r\n        seeds = seeder.fetchDynamicProperties(\r\n            self.buildSpec['target'],\r\n            self.buildSpec['encoding']\r\n        )\r\n        for config in self.configs:\r\n            config.seedUI(seeds)",
        "rewrite": "```python\ndef fetch_external_updates(self):\n    seeds = self.seeder.fetch_dynamic_properties(\n        self.build_spec['target'],\n        self.build_spec['encoding']\n    )\n    for config in self.configs:\n        config.seed_ui(seeds)\n```\n\nNote: I've followed PEP 8 conventions for Python code, which includes using lowercase and underscore notation for method names."
    },
    {
        "original": "def get_root_path(obj):\n    \"\"\"\n    Get file path for object and returns its dirname\n    \"\"\"\n    try:\n        filename = os.path.abspath(obj.__globals__['__file__'])\n    except (KeyError, AttributeError):\n        if getattr(obj, '__wrapped__', None):\n            # decorator package has been used in view\n            return get_root_path(obj.__wrapped__)\n        filename = inspect.getfile(obj)\n    return os.path.dirname(filename)",
        "rewrite": "```python\nimport os\nimport inspect\n\ndef get_root_path(obj):\n    try:\n        filename = os.path.abspath(obj.__globals__['__file__'])\n    except (KeyError, AttributeError):\n        filename = inspect.getfile(obj) or getattr(obj, '__code__', None).co_filename\n    return os.path.dirname(filename)\n```"
    },
    {
        "original": "def reset_default(verbose=False):\n    \"\"\"Remove custom.css and custom fonts\"\"\"\n    paths = [jupyter_custom, jupyter_nbext]\n\n    for fpath in paths:\n        custom = '{0}{1}{2}.css'.format(fpath, os.sep, 'custom')\n        try:\n            os.remove(custom)\n        except Exception:\n            pass\n    try:\n        delete_font_files()\n    except Exception:\n        check_directories()\n        delete_font_files()\n\n    copyfile(defaultCSS, jupyter_customcss)\n    copyfile(defaultJS, jupyter_customjs)\n\n    if os.path.exists(theme_name_file):\n        os.remove(theme_name_file)\n\n    if verbose:\n        print(\"Reset css and font defaults in:\\n{} &\\n{}\".format(*paths))",
        "rewrite": "```python\ndef reset_default(verbose=False):\n    paths = [jupyter_custom, jupyter_nbext]\n    \n    for fpath in paths:\n        custom_path = f'{fpath}{os.sep}custom.css'\n        os.remove(custom_path)\n    \n    try:\n        delete_font_files()\n    except Exception as e:\n        check_directories()\n        delete_font_files()\n    \n    copyfile(defaultCSS, jupyter_customcss)\n    copyfile(defaultJS, jupyter_customjs)\n\n    if os.path.exists(theme_name_file):\n        os.remove(theme_name_file)\n\n    if verbose:\n       print(f\"Reset css and font defaults"
    },
    {
        "original": "def set_last_col_idx(self, last_col_idx):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tparam last_col_idx : int\n\t\t\tnumber of columns\n\t\t\"\"\"\n\t\tassert last_col_idx >= self._max_col\n\t\tself._max_col = last_col_idx\n\t\treturn self",
        "rewrite": "```python\ndef set_last_col_idx(self, last_col_idx: int) -> 'IndexManager':\n    assert last_col_idx > self._max_col\n    self._max_col = last_col_idx\n    return self\n```"
    },
    {
        "original": "def ListDescendentPathInfos(self,\n                              client_id,\n                              path_type,\n                              components,\n                              timestamp=None,\n                              max_depth=None,\n                              cursor=None):\n    \"\"\"Lists path info records that correspond to descendants of given path.\"\"\"\n    path_infos = []\n\n    query = \"\"\n\n    path = mysql_utils.ComponentsToPath(components)\n    values = {\n        \"client_id\": db_utils.ClientIDToInt(client_id),\n        \"path_type\": int(path_type),\n        \"path\": db_utils.EscapeWildcards(path),\n    }\n\n    query += ",
        "rewrite": "```python\ndef list_descendent_path_infos(self, client_id, path_type, components, timestamp=None, max_depth=None, cursor=None):\n    \"\"\"Lists path info records that correspond to descendants of given path.\"\"\"\n    path = mysql_utils.ComponentsToPath(components)\n    db_values = {\n        \"client_id\": db_utils.ClientIDToInt(client_id),\n        \"path_type\": int(path_type),\n        \"path\": db_utils.EscapeWildcards(path),\n    }\n    \n    query = \"\"\"\n        SELECT p.path_info_record\n        FROM PathInfoRecords pir\n            JOIN Paths p ON pir.path_id = p.path_id AND pir.client_id"
    },
    {
        "original": "def ReadPathInfoHistory(self, client_id, path_type, components):\n    \"\"\"Reads a collection of hash and stat entry for given path.\n\n    Args:\n      client_id: An identifier string for a client.\n      path_type: A type of a path to retrieve path history for.\n      components: A tuple of path components corresponding to path to retrieve\n        information for.\n\n    Returns:\n      A list of `rdf_objects.PathInfo` ordered by timestamp in ascending order.\n    \"\"\"\n    histories = self.ReadPathInfosHistories(client_id, path_type, [components])\n    return histories[components]",
        "rewrite": "```python\ndef ReadPathInfoHistory(self, client_id, path_type, components):\n    histories = self.ReadPathInfosHistories(client_id, path_type, [components])\n    return histories[0][components]\n```"
    },
    {
        "original": "def metrics(self, name):\n        \"\"\"\n        Return the metrics received under the given name\n        \"\"\"\n        return [\n            MetricStub(\n                ensure_unicode(stub.name),\n                stub.type,\n                stub.value,\n                normalize_tags(stub.tags),\n                ensure_unicode(stub.hostname),\n            )\n            for stub in self._metrics.get(to_string(name), [])\n        ]",
        "rewrite": "```python\ndef metrics(self, name):\n    return [\n        MetricStub(stub.name, stub.type, stub.value, normalize_tags(stub.tags), stub.hostname)\n        for stub in self._metrics.get(to_string(name), [])\n    ]\n```"
    },
    {
        "original": "def _find_penultimate_layer(model, layer_idx, penultimate_layer_idx):\n    \"\"\"Searches for the nearest penultimate `Conv` or `Pooling` layer.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_idx: The layer index within `model.layers`.\n        penultimate_layer_idx: The pre-layer to `layer_idx`. If set to None, the nearest penultimate\n            `Conv` or `Pooling` layer is used.\n\n    Returns:\n        The penultimate layer.\n    \"\"\"\n    if penultimate_layer_idx is None:\n        for idx, layer in utils.reverse_enumerate(model.layers[:layer_idx - 1]):\n            if isinstance(layer, Wrapper):\n                layer = layer.layer\n            if isinstance(layer, (_Conv, _Pooling1D, _Pooling2D, _Pooling3D)):\n                penultimate_layer_idx = idx\n                break\n\n    if penultimate_layer_idx is None:\n        raise ValueError('Unable to determine penultimate `Conv` or `Pooling` '\n                         'layer for layer_idx: {}'.format(layer_idx))\n\n    # Handle negative indexing otherwise the next check can fail.\n    if layer_idx < 0:\n        layer_idx = len(model.layers) + layer_idx\n    if penultimate_layer_idx > layer_idx:\n        raise ValueError('`penultimate_layer_idx` needs to be before `layer_idx`')\n\n    return model.layers[penultimate_layer_idx]",
        "rewrite": "```python\ndef _find_penultimate_layer(model, layer_idx, penultimate_layer_idx=None):\n    if penultimate_layer_idx is None:\n        for idx in range(len(model.layers) - 1, -1, -1):\n            layer = model.layers[idx]\n            if isinstance(layer, Wrapper):\n                layer = layer.layer\n            if isinstance(layer, (_Conv,__Pooling)):\n                penultimate_layer_idx = idx\n                break\n\n    if penultimate_layer_idx is None:\n        raise ValueError('Unable to determine penultimate `Conv` or `Pooling` '\n                         'layer for provided layers')\n\n    # Handle negative indexing otherwise"
    },
    {
        "original": "def get_scores_and_p_values(self, tdm, category):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\ttdm: TermDocMatrix\n\t\tcategory: str, category name\n\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame(['coef', 'p-val'])\n\t\t\"\"\"\n\t\tX = tdm._X\n\t\ty = self._make_response_variable_1_or_negative_1(category, tdm)\n\t\tpX = X / X.sum(axis=1)\n\t\tansX = self._anscombe_transform(pX.copy())\n\t\tB, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var\\\n\t\t\t= lsqr(A=ansX, b=y, calc_var=True)",
        "rewrite": "```python\ndef get_scores_and_p_values(self, tdm: pd.DataFrame, category: str) -> pd.DataFrame:\n    X = tdm.copy().astype(float)\n    y = self._make_response_variable_1_or_negative_1(category, tdm)\n    \n    pX = (X / X.sum(axis=1)).clip(lower=1e-12, upper=None)  # Avoid division by zero\n    ansX = self._anscombe_transform(pX.copy())\n    \n    B, istop, itn, r1norm, r2norm, anorm, acond,\\\n        arnorm , x"
    },
    {
        "original": "def format_stats(stats):\n    \"\"\"Given a dictionary following this layout:\n\n        {\n            'encoded:label': 'Encoded',\n            'encoded:value': 'Yes',\n            'encoded:description': 'Indicates if the column is encoded',\n            'encoded:include': True,\n\n            'size:label': 'Size',\n            'size:value': 128,\n            'size:description': 'Size of the table in MB',\n            'size:include': True,\n        }\n\n    format_stats will convert the dict into this structure:\n\n        {\n            'encoded': {\n                'id': 'encoded',\n                'label': 'Encoded',\n                'value': 'Yes',\n                'description': 'Indicates if the column is encoded',\n                'include': True\n            },\n            'size': {\n                'id': 'size',\n                'label': 'Size',\n                'value': 128,\n                'description': 'Size of the table in MB',\n                'include': True\n            }\n        }\n    \"\"\"\n    stats_collector = {}\n    for stat_key, stat_value in stats.items():\n        stat_id, stat_field = stat_key.split(\":\")\n\n        stats_collector.setdefault(stat_id, {\"id\": stat_id})\n        stats_collector[stat_id][stat_field] = stat_value\n\n    # strip out all the stats we don't want\n    stats_collector = {\n        stat_id: stats\n        for stat_id, stats in stats_collector.items()\n        if stats.get('include', False)\n    }\n\n    # we always have a 'has_stats' field, it's never included\n    has_stats = {\n        'id': 'has_stats',\n        'label': 'Has Stats?',\n        'value': len(stats_collector) > 0,\n        'description': 'Indicates whether there are statistics for this table',\n        'include': False,\n    }\n    stats_collector['has_stats'] = has_stats\n    return stats_collector",
        "rewrite": "```python\ndef format_stats(stats):\n    stats_collector = {}\n    for stat_key, stat_value in stats.items():\n        stat_id, _ = stat_key.split(\":\", 1)\n        if stat_id not in stats_collector:\n            stats_collector[stat_id] = {\"id\": stat_id}\n        if \"include\" not in stats_collector[stat_id] or (stats_collector[stat_id].get(\"include\") and isinstance(stat_value, bool)):\n            stats_collector[stat_id][list(stats.items())[len(statscollector)][0]] = stat_value\n\n    # Strip out all the stats we"
    },
    {
        "original": "def WriteFromFD(self, src_fd, arcname=None, compress_type=None, st=None):\n    \"\"\"Write a zip member from a file like object.\n\n    Args:\n      src_fd: A file like object, must support seek(), tell(), read().\n      arcname: The name in the archive this should take.\n      compress_type: Compression type (zipfile.ZIP_DEFLATED, or ZIP_STORED)\n      st: An optional stat object to be used for setting headers.\n\n    Raises:\n      ArchiveAlreadyClosedError: If the zip if already closed.\n\n    Yields:\n      Chunks of binary data.\n    \"\"\"\n    yield self.WriteFileHeader(\n        arcname=arcname, compress_type=compress_type, st=st)\n    while 1:\n      buf = src_fd.read(1024 * 1024)\n      if not buf:\n        break\n\n      yield self.WriteFileChunk(buf)\n\n    yield self.WriteFileFooter()",
        "rewrite": "```python\ndef WriteFromFD(self, src_fd, arcname=None, compress_type=None, st=None):\n    \"\"\"Write a zip member from a file like object\"\"\"\n    \n    if self.closed:\n        raise ArchiveAlreadyClosedError(\"Archive is already closed\")\n    \n    yield self.WriteFileHeader(arcname=arcname, compress_type=compress_type, st=st)\n    \n    while True:\n        buf = src_fd.read(1024 * 1024)\n        if not buf:\n            break\n        \n        yield self.WriteFileChunk(buf)\n\n    yield self.WriteFileFooter()\n```"
    },
    {
        "original": "def local(reload, port):\n    \"\"\"run local app server, assumes into the account\n    \"\"\"\n    import logging\n    from bottle import run\n    from app import controller, app\n    from c7n.resources import load_resources\n    load_resources()\n    print(\"Loaded resources definitions\")\n    logging.basicConfig(level=logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.WARNING)\n    if controller.db.provision():\n        print(\"Table Created\")\n    run(app, reloader=reload, port=port)",
        "rewrite": "```python\nimport logging\nfrom bottle import run\nfrom app import controller, app\nfrom c7n.resources import load_resources\n\ndef local(reload, port):\n    load_resources()\n    print(\"Loaded resources definitions\")\n    logging.basicConfig(level=logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.WARNING)\n    \n    try:\n        if controller.db.provision():\n            print(\"Table Created\")\n        run(app, reloader=reload, port=port)\n    except KeyboardInterrupt:\n        print(\"\\nStopping server...\")\n```"
    },
    {
        "original": "def evict(cls, urls):\n        \"\"\"Remove items from cache matching URLs.\n\n        Return the number of items removed.\n\n        \"\"\"\n        if isinstance(urls, text_type):\n            urls = [urls]\n        urls = set(normalize_url(url) for url in urls)\n        retval = 0\n        with cls.ca_lock:\n            for key in list(cls.cache):\n                if key[0] in urls:\n                    retval += 1\n                    del cls.cache[key]\n                    del cls.timeouts[key]\n        return retval",
        "rewrite": "```python\ndef evict(cls, urls):\n    if isinstance(urls, str):\n        urls = [urls]\n    urls = set(normalize_url(url) for url in urls)\n    with cls.ca_lock:\n        for key in list(cls.cache.keys()):\n            if key[0] in urls:\n                del cls.cache[key]\n                del cls.timeouts[key]\n    return len(urls)\n```"
    },
    {
        "original": "def add_custom_service_account(self, account, nickname, password):\n        \"\"\"\n        \u6dfb\u52a0\u5ba2\u670d\u5e10\u53f7\u3002\n\n        :param account: \u5ba2\u670d\u8d26\u53f7\u7684\u7528\u6237\u540d\n        :param nickname: \u5ba2\u670d\u8d26\u53f7\u7684\u6635\u79f0\n        :param password: \u5ba2\u670d\u8d26\u53f7\u7684\u5bc6\u7801\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/customservice/kfaccount/add\",\n            data={\n                \"kf_account\": account,\n                \"nickname\": nickname,\n                \"password\": password\n            }\n        )",
        "rewrite": "```python\ndef add_custom_service_account(self, account, nickname, password):\n    return self.post(\n        url=\"https://api.weixin.qq.com/customservice/kfaccount/add\",\n        json={\n            \"kf_account\": account,\n            \"nickname\": nickname,\n            \"password\": password\n        }\n    )\n```"
    },
    {
        "original": "def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns\n        None.\n\n        :return: None\n        \"\"\"\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n\n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)",
        "rewrite": "```python\ndef bake(self):\n    options = self.options\n    default_exclude_list = options.pop('default_exclude')\n    options_exclude_list = options.pop('exclude')\n    excludes = default_exclude_list + [x for x in optionsExcludeList if x not in defaultExcludeList]\n    \n    exclude_args = (f\"--exclude={exclude}\" for exclude in excludes)\n    x_args = tuple((f\"-x\" if i == 0 else f\", -x\") + (y,) for i, y in enumerate(options.get(\"x\", [])))\n        \n    self._ansible_lint_command = sh.ansible_lint.bake"
    },
    {
        "original": "def _dict_to_bson(doc, check_keys, opts, top_level=True):\n    \"\"\"Encode a document to BSON.\"\"\"\n    if _raw_document_class(doc):\n        return doc.raw\n    try:\n        elements = []\n        if top_level and \"_id\" in doc:\n            elements.append(_name_value_to_bson(b\"_id\\x00\", doc[\"_id\"],\n                                                check_keys, opts))\n        for (key, value) in iteritems(doc):\n            if not top_level or key != \"_id\":\n                elements.append(_element_to_bson(key, value,\n                                                 check_keys, opts))\n    except AttributeError:\n        raise TypeError(\"encoder expected a mapping type but got: %r\" % (doc,))\n\n    encoded = b\"\".join(elements)\n    return _PACK_INT(len(encoded) + 5) + encoded + b\"\\x00\"",
        "rewrite": "```python\ndef _dict_to_bson(doc, check_keys, opts, top_level=True):\n    \"\"\"\n    Encode a document to BSON.\n    \"\"\"\n    \n    if _raw_document_class(doc):\n        return doc.raw\n    \n    elements = []\n    \n    if top_level and \"_id\" in doc:\n        elements.append(_name_value_to_bson(b\"_id\\x00\", doc[\"_id\"], check_keys, opts))\n        \n    for key, value in iteritems(doc):\n        if not top_level or key != \"_id\":\n            elements.append(_element_to_bson(key, value, check_keys, opts))\n    \n    try:\n"
    },
    {
        "original": "def _fix_reindent(self, result):\n        \"\"\"Fix a badly indented line.\n\n        This is done by adding or removing from its initial indent only.\n\n        \"\"\"\n        num_indent_spaces = int(result['info'].split()[1])\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        self.source[line_index] = ' ' * num_indent_spaces + target.lstrip()",
        "rewrite": "```python\ndef _fix_reindent(self, result):\n    num_indent_spaces = int(result['info'].split()[1])\n    line_index = result['line'] - 1\n    target = self.source[line_index]\n    leading_space_count = len(target) - len(target.lstrip())\n    added_spaces = max(0, num_indent_spaces - leading_space_count)\n    modified_line_start = ' ' * added_spaces + target.lstrip()\n    self.source[line_index] = modified_line_start\n```"
    },
    {
        "original": "def load_cli_config(args):\n    \"\"\"Modifies ARGS in-place to have the attributes defined in the CLI\n    config file if it doesn't already have them. Certain default\n    values are given if they are not in ARGS or the config file.\n    \"\"\"\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key in args and getattr(args, key) is not None:\n                pass\n            else:\n                setattr(args, key, val)",
        "rewrite": "```python\ndef load_cli_config(args):\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key not in args or getattr(args, key) is None:\n                setattr(args, key, val)\n```"
    },
    {
        "original": "def _construct(self):\n        \"\"\"\n        Construct a control dependence graph.\n\n        This implementation is based on figure 6 of paper An Efficient Method of Computing Static Single Assignment\n        Form by Ron Cytron, etc.\n        \"\"\"\n\n        self._acyclic_cfg = self._cfg.copy()\n        # TODO: Cycle-removing is not needed - confirm it later\n        # The CFG we use should be acyclic!\n        #self._acyclic_cfg.remove_cycles()\n\n        # Pre-process the acyclic CFG\n        self._pre_process_cfg()\n\n        # Construct post-dominator tree\n        self._pd_construct()\n\n        self._graph = networkx.DiGraph()\n\n        # Construct the reversed dominance frontier mapping\n        rdf = compute_dominance_frontier(self._normalized_cfg, self._post_dom)\n\n        for y in self._cfg.graph.nodes():\n            if y not in rdf:\n                continue\n            for x in rdf[y]:\n                self._graph.add_edge(x, y)",
        "rewrite": "```python\ndef _construct(self):\n    self._acyclic_cfg = self._cfg.copy()\n    #self._acyclic_cfg.remove_cycles()  # Assuming self._cfg is already acyclic\n\n    self._pre_process_cfg()\n\n    self._pd_construct()\n\n    self._graph = networkx.DiGraph()\n\n    rdf = compute_dominance_frontier(self._normalized_cfg, self._post_dom)\n\n    for x in rdf:\n        for node in rdf[x]:\n            if node in networkx.DiGraph.from_dict(rdf).out_edges(x):\n                continue\n            if node in rf:  # Assuming rf"
    },
    {
        "original": "def pad_tensor(tensor, length, padding_index=DEFAULT_PADDING_INDEX):\n    \"\"\" Pad a ``tensor`` to ``length`` with ``padding_index``.\n\n    Args:\n        tensor (torch.Tensor [n, ...]): Tensor to pad.\n        length (int): Pad the ``tensor`` up to ``length``.\n        padding_index (int, optional): Index to pad tensor with.\n\n    Returns\n        (torch.Tensor [length, ...]) Padded Tensor.\n    \"\"\"\n    n_padding = length - tensor.shape[0]\n    assert n_padding >= 0\n    if n_padding == 0:\n        return tensor\n    padding = tensor.new(n_padding, *tensor.shape[1:]).fill_(padding_index)\n    return torch.cat((tensor, padding), dim=0)",
        "rewrite": "```python\ndef pad_tensor(tensor, length, padding_index=0):\n    n_padding = max(0, length - tensor.shape[0])\n    if n_padding == 0:\n        return tensor\n    padding = tensor.new(n_padding, *tensor.shape[1:]).fill_(padding_index)\n    return torch.cat((tensor, padding), dim=0)\n```"
    },
    {
        "original": "def _malloc(self, sim_size):\n        \"\"\"\n        Handler for any libc `malloc` SimProcedure call. If the heap has faithful support for `malloc`, it ought to be\n        implemented in a `malloc` function (as opposed to the `_malloc` function).\n\n        :param sim_size: the amount of memory (in bytes) to be allocated\n        \"\"\"\n        raise NotImplementedError(\"%s not implemented for %s\" % (self._malloc.__func__.__name__,\n                                                                 self.__class__.__name__))",
        "rewrite": "```python\ndef _malloc(self, sim_size):\n    raise NotImplementedError(\"_malloc not implemented for {0}\".format(type(self).__name__))\n```"
    },
    {
        "original": "def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "rewrite": "```python\ndef save_config(\n    self, \n    cmd=\"copy running-config startup-config\", \n    confirm=True, \n    confirm_response=\"y\"\n):\n    return super(ExtremeNosSSH, self).save_config(cmd=cmd, confirm=confirm, confirm_response=confirm_response)\n```"
    },
    {
        "original": "def predict_wishart_embedding(self, Xnew, kern=None, mean=True, covariance=True):\n        \"\"\"\n        Predict the wishart embedding G of the GP. This is the density of the\n        input of the GP defined by the probabilistic function mapping f.\n        G = J_mean.T*J_mean + output_dim*J_cov.\n\n        :param array-like Xnew: The points at which to evaluate the magnification.\n        :param :py:class:`~GPy.kern.Kern` kern: The kernel to use for the magnification.\n\n        Supplying only a part of the learning kernel gives insights into the density\n        of the specific kernel part of the input function. E.g. one can see how dense the\n        linear part of a kernel is compared to the non-linear part etc.\n        \"\"\"\n        if kern is None:\n            kern = self.kern\n\n        mu_jac, var_jac = self.predict_jacobian(Xnew, kern, full_cov=False)\n        mumuT = np.einsum('iqd,ipd->iqp', mu_jac, mu_jac)\n        Sigma = np.zeros(mumuT.shape)\n        if var_jac.ndim == 4: # Missing data\n            Sigma = var_jac.sum(-1)\n        else:\n            Sigma = self.output_dim*var_jac\n\n        G = 0.\n        if mean:\n            G += mumuT\n        if covariance:\n            G += Sigma\n        return G",
        "rewrite": "```python\ndef predict_wishart_embedding(self, Xnew, kern=None, mean=True, covariance=True):\n    if kern is None:\n        kern = self.kern\n\n    mu_jac, var_jac = self.predict_jacobian(Xnew, kern, full_cov=False)\n    mumuT = np.einsum('iqd,ipd->iqp', mu_jac.T @ mu_jac)\n    Sigma = np.zeros(mumuT.shape)\n    \n    if var_jac.ndim == 4: \n        Sigma += var_jac.sum((-1,-2))\n        \n    G = mumuT.copy"
    },
    {
        "original": "def check_model(self):\n        \"\"\"\n        Check the model for various errors. This method checks for the following\n        errors.\n\n        * Checks if factors are defined for all the cliques or not.\n        * Check for running intersection property is not done explicitly over\n          here as it done in the add_edges method.\n        * Checks if cardinality information for all the variables is availble or not. If\n          not it raises an error.\n        * Check if cardinality of random variable remains same across all the\n          factors.\n\n        Returns\n        -------\n        check: boolean\n            True if all the checks are passed\n        \"\"\"\n        for clique in self.nodes():\n            factors = filter(lambda x: set(x.scope()) == set(clique), self.factors)\n            if not any(factors):\n                raise ValueError('Factors for all the cliques or clusters not defined.')\n\n        cardinalities = self.get_cardinality()\n        if len(set((x for clique in self.nodes() for x in clique))) != len(cardinalities):\n            raise ValueError('Factors for all the variables not defined.')\n\n        for factor in self.factors:\n            for variable, cardinality in zip(factor.scope(), factor.cardinality):\n                if (cardinalities[variable] != cardinality):\n                    raise ValueError(\n                        'Cardinality of variable {var} not matching among factors'.format(var=variable))\n\n        return True",
        "rewrite": "```python\ndef check_model(self):\n    for clique in self.nodes():\n        factors = [f for f in self.factors if set(f.scope()) == set(clique)]\n        if not factors:\n            raise ValueError('Factors for all the cliques or clusters not defined.')\n\n    cardinalities = self.get_cardinality()\n    valid_variables = tuple(var for clique in self.nodes() for var in clique)\n    if len(valid_variables) != len(cardinalities):\n        raise ValueError('Factors for all the variables not defined.')\n\n    unique_var = None\n    updated_cardinalities1 = {}\n    updated_cardinalities2={}\n    \n"
    },
    {
        "original": "def get_default_database(self):\n        \"\"\"DEPRECATED - Get the database named in the MongoDB connection URI.\n\n        >>> uri = 'mongodb://host/my_database'\n        >>> client = MongoClient(uri)\n        >>> db = client.get_default_database()\n        >>> assert db.name == 'my_database'\n        >>> db = client.get_database()\n        >>> assert db.name == 'my_database'\n\n        Useful in scripts where you want to choose which database to use\n        based only on the URI in a configuration file.\n\n        .. versionchanged:: 3.5\n           Deprecated, use :meth:`get_database` instead.\n        \"\"\"\n        warnings.warn(\"get_default_database is deprecated. Use get_database \"\n                      \"instead.\", DeprecationWarning, stacklevel=2)\n        if self.__default_database_name is None:\n            raise ConfigurationError('No default database defined')\n\n        return self[self.__default_database_name]",
        "rewrite": "```python\ndef get_default_database(self):\n    warnings.warn(\"get_default_database is deprecated. Use get_database \"\n                 \"instead.\", DeprecationWarning, stacklevel=2)\n    if not self.__default_database_name:\n        raise ConfigurationError('No default database defined')\n    return getattr(self, self.__default_database_name)\n```"
    },
    {
        "original": "def service_set_tag(path, service_name, tag):\n    \"\"\"\n    Change the tag of a docker-compose service\n    This does not pull or up the service\n    This wil re-write your yaml file. Comments will be lost. Indentation is set to 2 spaces\n\n    path\n        Path where the docker-compose file is stored on the server\n    service_name\n        Name of the service to remove\n    tag\n        Name of the tag (often used as version) that the service image should have\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion dockercompose.service_create /path/where/docker-compose/stored service_name tag\n    \"\"\"\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return err\n    services = compose_result['compose_content']['services']\n    if service_name not in services:\n        return __standardize_result(False,\n                                    'Service {0} did not exists'.format(service_name),\n                                    None, None)\n    if 'image' not in services[service_name]:\n        return __standardize_result(False,\n                                    'Service {0} did not contain the variable \"image\"'.format(service_name),\n                                    None, None)\n    image = services[service_name]['image'].split(':')[0]\n    services[service_name]['image'] = '{0}:{1}'.format(image, tag)\n    return __dump_compose_file(path, compose_result,\n                               'Service {0} is set to tag \"{1}\"'.format(service_name, tag),\n                               already_existed=True)",
        "rewrite": "```python\ndef service_set_tag(path, service_name, tag):\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return err\n    services = compose_result['compose_content']['services']\n    if service_name not in services:\n        return False, 'Service {0} did not exist'.format(service_name)\n    if 'image' not in services[service_name]:\n        return False, 'Service {0} did not contain the variable \"image\"'.format(service_name)\n    original_image = f\"{services[service_name]['image'].split(':')[0]}:{services[service_name"
    },
    {
        "original": "def enqueue(self, destination):\n        \"\"\"Enqueues given destination for processing.\n\n        Given instance should be a valid destination.\n        \"\"\"\n        if not destination:\n            raise BgpProcessorError('Invalid destination %s.' % destination)\n\n        dest_queue = self._dest_queue\n        # RtDest are queued in a separate queue\n        if destination.route_family == RF_RTC_UC:\n            dest_queue = self._rtdest_queue\n\n        # We do not add given destination to the queue for processing if\n        # it is already on the queue.\n        if not dest_queue.is_on_list(destination):\n            dest_queue.append(destination)\n\n        # Wake-up processing thread if sleeping.\n        self.dest_que_evt.set()",
        "rewrite": "```python\ndef enqueue(self, destination):\n    if not destination:\n        raise BgpProcessorError('Invalid destination')\n\n    dest_queue = self._dest_queue\n    if destination.route_family == RF_RTC_UC:\n        dest_queue = self._rtdest_queue\n\n    if not dest_queue.is_on_list(destination):\n        dest_queue.append(destination)\n        self.dest_que_evt.set()\n```"
    },
    {
        "original": "def parse_tags(self, tags):\n        \"\"\"Parse tags into a dict.\n\n        input tags: a comma separated list of 'key:value' pairs.\n            Example: foo:bar,spam:eggs\n        output dtags: a dict of tags.\n            Example: {'foo': 'bar', 'spam': 'eggs'}\n        \"\"\"\n        dtags = {}\n        if tags:\n            try:\n                dtags = dict([x.split(':') for x in tags.split(',')])\n            except ValueError:\n                # one of the 'key:value' pairs was missing\n                logger.info('Invalid tags passed: %s', tags)\n                dtags = {}\n\n        return dtags",
        "rewrite": "```python\ndef parse_tags(self, tags):\n    dtags = {}\n    if tags:\n        try:\n            dtags = dict([x.split(':') for x in tags.split(',') if ':' in x])\n        except ValueError as e:\n            logger.info('Invalid tags passed: %s', e)\n    return dtags\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'customer_id') and self.customer_id is not None:\n            _dict['customer_id'] = self.customer_id\n        if hasattr(self, 'document_type') and self.document_type is not None:\n            _dict['document_type'] = self.document_type\n        if hasattr(self, 'natural_language_query'\n                  ) and self.natural_language_query is not None:\n            _dict['natural_language_query'] = self.natural_language_query\n        if hasattr(self,\n                   'document_results') and self.document_results is not None:\n            _dict['document_results'] = self.document_results._to_dict()\n        if hasattr(self,\n                   'created_timestamp') and self.created_timestamp is not None:\n            _dict['created_timestamp'] = datetime_to_string(\n                self.created_timestamp)\n        if hasattr(self,\n                   'client_timestamp') and self.client_timestamp is not None:\n            _dict['client_timestamp'] = datetime_to_string(\n                self.client_timestamp)\n        if hasattr(self, 'query_id') and self.query_id is not None:\n            _dict['query_id'] = self.query_id\n        if hasattr(self, 'session_token') and self.session_token is not None:\n            _dict['session_token'] = self.session_token\n        if hasattr(self, 'collection_id') and self.collection_id is not None:\n            _dict['collection_id'] = self.collection_id\n        if hasattr(self, 'display_rank') and self.display_rank is not None:\n            _dict['display_rank'] = self.display_rank\n        if hasattr(self, 'document_id') and self.document_id is not None:\n            _dict['document_id'] = self.document_id\n        if hasattr(self, 'event_type') and self.event_type is not None:\n            _dict['event_type'] = self.event_type\n        if hasattr(self, 'result_type') and self.result_type is not None:\n            _dict['result_type'] = self.result_type\n        return _dict",
        "rewrite": "```python\ndef _to_dict(self):\n    _dict = {\n        'environment_id': getattr(self, 'environment_id', None),\n        'customer_id': getattr(self, 'customer_id', None),\n        'document_type': getattr(self, 'document_type', None),\n        'natural_language_query': getattr(self, \n                                            'natural_language_query',\n                                            None),\n        'document_results': self.document_results._to_dict(),\n        # Custom attributes:\n        # Newton SJO custom fields go ahead and add those manually\n    }\n\n    for key in [\n            ('created_timestamp', lambda x: datetime_to_string(x) if"
    },
    {
        "original": "def list_role_policies(role_name, region=None, key=None, keyid=None,\n                       profile=None):\n    \"\"\"\n    Get a list of policy names from a role.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.list_role_policies myirole\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        response = conn.list_role_policies(role_name)\n        _list = response.list_role_policies_response.list_role_policies_result\n        return _list.policy_names\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return []",
        "rewrite": "```python\ndef list_role_policies(role_name, region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Get a list of policy names from a role.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.list_role_policies myirole\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        response = conn.list_role_policies(RoleName=role_name).get('ResponseMetadata').get('HTTPStatusCode')\n        assert response == 200\n        return response['ListRole"
    },
    {
        "original": "def build(self):\n        \"\"\"\n        Create the current layer\n\n        :return: string of the packet with the payload\n        \"\"\"\n        p = self.do_build()\n        p += self.build_padding()\n        p = self.build_done(p)\n        return p",
        "rewrite": "```python\ndef build(self) -> str:\n    packet = self.do_build()\n    packet += self.build_padding()\n    return self.build_done(packet)\n```"
    },
    {
        "original": "def stop_recording(self):\n        \"\"\"Stop recording from the audio source.\"\"\"\n        self._stop_recording.set()\n        with self._source_lock:\n            self._source.stop()\n        self._recording = False",
        "rewrite": "def stop_recording(self):\n    self._stop_recording.set()\n    with self._source_lock:\n        self._source.stop()\n    self.recording = False"
    },
    {
        "original": "def remove_label_from_pr(repo: GithubRepository,\n                         pull_id: int,\n                         label: str) -> bool:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/issues/labels/#remove-a-label-from-an-issue\n    \"\"\"\n    url = (\"https://api.github.com/repos/{}/{}/issues/{}/labels/{}\"\n           \"?access_token={}\".format(repo.organization,\n                                     repo.name,\n                                     pull_id,\n                                     label,\n                                     repo.access_token))\n    response = requests.delete(url)\n\n    if response.status_code == 404:\n        payload = json.JSONDecoder().decode(response.content.decode())\n        if payload['message'] == 'Label does not exist':\n            return False\n\n    if response.status_code == 200:\n        # Removed the label.\n        return True\n\n    raise RuntimeError(\n        'Label remove failed. Code: {}. Content: {}.'.format(\n            response.status_code, response.content))",
        "rewrite": "```python\nimport requests\nimport json\n\ndef remove_label_from_pr(repo: object, pull_id: int, label: str) -> bool:\n    url = f\"https://api.github.com/repos/{repo.organization}/{repo.name}/pulls/{pull_id}/labels/{label}\"\n    headers = {\"Authorization\": f\"Bearer {repo.access_token}\", \"Content-Type\": \"application/json\"}\n\n    response = requests.delete(url, headers=headers)\n\n    if response.status_code == 422:\n        return False\n\n    if response.status_code == 204:\n        return True\n\n    raise RuntimeError(f\"Label remove failed. Code: {response"
    },
    {
        "original": "def get_labels(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/labels <http://developer.github.com/v3/issues/labels>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Label.Label`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Label.Label,\n            self._requester,\n            self.issue_url + \"/labels\",\n            None\n        )",
        "rewrite": "```python\ndef get_labels(self):\n    path = self.issue_url + \"/labels\"\n    return self._requester.request_by_path(\n        'GET',\n        path,\n        None\n    ).from\u0456\u0434\u043e\u043c_label()\n```"
    },
    {
        "original": "def enable_branching_model(self, project, repository):\n        \"\"\"\n        Enable branching model by setting it with default configuration\n        :param project:\n        :param repository:\n        :return:\n        \"\"\"\n        default_model_data = {'development': {'refId': None, 'useDefault': True},\n                              'types': [{'displayName': 'Bugfix',\n                                         'enabled': True,\n                                         'id': 'BUGFIX',\n                                         'prefix': 'bugfix/'},\n                                        {'displayName': 'Feature',\n                                         'enabled': True,\n                                         'id': 'FEATURE',\n                                         'prefix': 'feature/'},\n                                        {'displayName': 'Hotfix',\n                                         'enabled': True,\n                                         'id': 'HOTFIX',\n                                         'prefix': 'hotfix/'},\n                                        {'displayName': 'Release',\n                                         'enabled': True,\n                                         'id': 'RELEASE',\n                                         'prefix': 'release/'}]}\n        return self.set_branching_model(project,\n                                        repository,\n                                        default_model_data)",
        "rewrite": "```python\ndef enable_branching_model(self, project, repository):\n    default_model_data = {\n        'development': {'refId': None, 'useDefault': True},\n        'types': [\n            {'displayName': 'Bugfix', 'enabled': True, 'id': 'BUGFIX', 'prefix': 'bugfix/'},\n            {'displayName': 'Feature', \t'enabled': True, \t'id'   :   \"FEATURE\", >/n \t\t'prefix':\"feature/\" },\n            {'displayName':'Hotfix', \t\t\t'enabled':'True'||\"true\",Slashreme \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'id':'H"
    },
    {
        "original": "def _compute_state_estimate(self):\n        \"\"\"\n        Computes the IMM's mixed state estimate from each filter using\n        the the mode probability self.mu to weight the estimates.\n        \"\"\"\n        self.x.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            self.x += f.x * mu\n\n        self.P.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            y = f.x - self.x\n            self.P += mu * (outer(y, y) + f.P)",
        "rewrite": "```python\ndef _compute_state_estimate(self):\n    self.x.fill(0)\n    for f, mu in zip(self.filters, self.mu):\n        self.x += f.x * mu\n\n    self.P.fill(0)\n    for f, mu in zip(self.filters, self.mu):\n        y = (f.x - self.x) \n        P_y = outer(y, y) + np.eye(y.size)\n        if isinstance(f.P, float):  # To avoid float + array broadcasting\n            P_y *= f.P\n        elif isinstance(f.P, np.ndarray):  # Matrix multiplication operator when possible\n            P"
    },
    {
        "original": "def GetOutputPluginStates(output_plugins, source=None, token=None):\n  \"\"\"Initializes state for a list of output plugins.\"\"\"\n  output_plugins_states = []\n  for plugin_descriptor in output_plugins:\n    plugin_class = plugin_descriptor.GetPluginClass()\n    try:\n      _, plugin_state = plugin_class.CreatePluginAndDefaultState(\n          source_urn=source, args=plugin_descriptor.plugin_args, token=token)\n    except Exception as e:  # pylint: disable=broad-except\n      raise ValueError(\"Plugin %s failed to initialize (%s)\" %\n                       (plugin_class, e))\n\n    # TODO(amoser): Those do not need to be inside the state, they\n    # could be part of the plugin descriptor.\n    plugin_state[\"logs\"] = []\n    plugin_state[\"errors\"] = []\n\n    output_plugins_states.append(\n        rdf_flow_runner.OutputPluginState(\n            plugin_state=plugin_state, plugin_descriptor=plugin_descriptor))\n\n  return output_plugins_states",
        "rewrite": "```python\ndef get_output_plugin_states(output_plugins, source=None, token=None):\n    return [\n        rdf_flow_runner.OutputPluginState(\n            plugin_state={\n                **plugin_descriptor.GetPluginClass().CreatePluginAndDefaultState(\n                    source_urn=source, args=plugin_descriptor.plugin_args, token=token)[1],\n                \"logs\": [],\n                \"errors\": []\n            },\n            plugin_descriptor=plugin_descriptor)\n        for plugin_descriptor in output_plugins\n    ]\n```"
    },
    {
        "original": "def get_cluster(dc_ref, cluster):\n    \"\"\"\n    Returns a cluster in a datacenter.\n\n    dc_ref\n        The datacenter reference\n\n    cluster\n        The cluster to be retrieved\n    \"\"\"\n    dc_name = get_managed_object_name(dc_ref)\n    log.trace('Retrieving cluster \\'%s\\' from datacenter \\'%s\\'',\n              cluster, dc_name)\n    si = get_service_instance_from_managed_object(dc_ref, name=dc_name)\n    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(\n        path='hostFolder',\n        skip=True,\n        type=vim.Datacenter,\n        selectSet=[vmodl.query.PropertyCollector.TraversalSpec(\n            path='childEntity',\n            skip=False,\n            type=vim.Folder)])\n    items = [i['object'] for i in\n             get_mors_with_properties(si,\n                                      vim.ClusterComputeResource,\n                                      container_ref=dc_ref,\n                                      property_list=['name'],\n                                      traversal_spec=traversal_spec)\n             if i['name'] == cluster]\n    if not items:\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'Cluster \\'{0}\\' was not found in datacenter '\n            '\\'{1}\\''. format(cluster, dc_name))\n    return items[0]",
        "rewrite": "```python\ndef get_cluster(dc_ref, cluster):\n    dc_name = get_managed_object_name(dc_ref)\n    log.trace('Retrieving cluster \\'%s\\' from datacenter \\'%s\\'', cluster, dc_name)\n    si = get_service_instance_from_managed_object(dc_ref, name=dc_name)\n    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(\n        path='hostFolder',\n        skip=True,\n        type=vim.Datacenter,\n        selectSet=[vmodl.query.PropertyCollector.TraversalSpec(\n            path='childEntity',\n            skip=False,\n            type=vim.Folder)])\n    items ="
    },
    {
        "original": "def unlock_repo(self, repo_name):\n        \"\"\"\n        :calls: `DELETE /user/migrations/:migration_id/repos/:repo_name/lock`_\n        :param repo_name: str\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo_name, (str, unicode)), repo_name\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.url + \"/repos/\" + repo_name + \"/lock\",\n            headers={\n                \"Accept\": Consts.mediaTypeMigrationPreview\n            }\n        )",
        "rewrite": "```python\ndef unlock_repo(self, repo_name):\n    assert isinstance(repo_name, (str, str)), repo_name\n    self._requester.requestJsonAndCheck(\n        \"DELETE\",\n        f\"{self.url}/repos/{repo_name}/lock\",\n        headers={\n            \"Accept\": Consts.mediaTypeMigrationPreview\n        }\n    )\n```\n\nI made the following changes:\n- Removed the unnecessary `data` variable since it's not used.\n- Used an f-string to format the URL string for better readability and efficiency.\n- Removed the mention of `unicode` type since it's a deprecated type in Python 3."
    },
    {
        "original": "def normalize(score, alpha=15):\n    \"\"\"\n    Normalize the score to be between -1 and 1 using an alpha that\n    approximates the max expected value\n    \"\"\"\n    norm_score = score / math.sqrt((score * score) + alpha)\n    if norm_score < -1.0:\n        return -1.0\n    elif norm_score > 1.0:\n        return 1.0\n    else:\n        return norm_score",
        "rewrite": "```python\nimport math\n\ndef normalize(score, alpha=15):\n    return min(max((score / math.sqrt(score ** 2 + alpha)), -1), 1)\n```"
    },
    {
        "original": "def remove_attribute_listener(self, attr_name, *args, **kwargs):\n        \"\"\"\n        Remove a paremeter listener that was previously added using :py:func:`add_attribute_listener`.\n\n        For example to remove the ``thr_min_callback()`` callback function:\n\n        .. code:: python\n\n            vehicle.parameters.remove_attribute_listener('thr_min', thr_min_callback)\n\n        See :ref:`vehicle_state_observing_parameters` for more information.\n\n        :param String attr_name: The parameter name that is to have an observer removed (or '*' to remove an 'all attribute' observer).\n        :param args: The callback function to remove.\n\n        \"\"\"\n        attr_name = attr_name.upper()\n        return super(Parameters, self).remove_attribute_listener(attr_name, *args, **kwargs)",
        "rewrite": "```python\ndef remove_attribute_listener(self, attr_name, *args, **kwargs):\n    attr_name = attr_name.upper()\n    return super().remove_attribute_listener(attr_name, *args, **kwargs)\n```"
    },
    {
        "original": "def _graph_add_edge(self, cfg_node, src_node, src_jumpkind, src_ins_addr, src_stmt_idx):\n        \"\"\"\n        Add edge between nodes, or add node if entry point\n\n        :param CFGNode cfg_node: node which is jumped to\n        :param CFGNode src_node: node which is jumped from none if entry point\n        :param str src_jumpkind: what type of jump the edge takes\n        :param int or str src_stmt_idx: source statements ID\n        :return: None\n        \"\"\"\n\n        if src_node is None:\n            self.graph.add_node(cfg_node)\n        else:\n            self.graph.add_edge(src_node, cfg_node, jumpkind=src_jumpkind, ins_addr=src_ins_addr,\n                                stmt_idx=src_stmt_idx)",
        "rewrite": "```python\ndef _graph_add_edge(self, cfg_node, src_node, src_jumpkind, src_ins_addr, src_stmt_idx):\n    if not isinstance(src_stmt_idx, int) and not isinstance(src_stmt_idx, str):\n        raise ValueError(\"src_stmt_idx must be either an integer or a string\")\n\n    if not isinstance(cfg_node, self._cfgnode_type):\n        raise ValueError(\"cfg_node must be of type {}\".format(self._cfgnode_type))\n\n    if src_node is None:\n        self.graph.add_node(cfg_node)\n    else:\n        self.graph.add_edge(src_node, cfg_node,\n                             jumpkind=src"
    },
    {
        "original": "def _pre_index_check(handler, host=None, core_name=None):\n    \"\"\"\n    PRIVATE METHOD - MASTER CALL\n    Does a pre-check to make sure that all the options are set and that\n    we can talk to solr before trying to send a command to solr. This\n    Command should only be issued to masters.\n\n    handler : str\n        The import handler to check the state of\n    host : str (None):\n        The solr host to query. __opts__['host'] is default\n    core_name (None):\n        The name of the solr core if using cores. Leave this blank if you are\n        not using cores or if you want to check all cores.\n        REQUIRED if you are using cores.\n\n    Return:  dict<str,obj>::\n\n        {'success':boolean, 'data':dict, 'errors':list, 'warnings':list}\n    \"\"\"\n    # make sure that it's a master minion\n    if _get_none_or_value(host) is None and not _is_master():\n        err = [\n            'solr.pre_indexing_check can only be called by \"master\" minions']\n        return _get_return_dict(False, err)\n    # solr can run out of memory quickly if the dih is processing multiple\n    # handlers at the same time, so if it's a multicore setup require a\n    # core_name param.\n    if _get_none_or_value(core_name) is None and _check_for_cores():\n        errors = ['solr.full_import is not safe to multiple handlers at once']\n        return _get_return_dict(False, errors=errors)\n    # check to make sure that we're not already indexing\n    resp = import_status(handler, host, core_name)\n    if resp['success']:\n        status = resp['data']['status']\n        if status == 'busy':\n            warn = ['An indexing process is already running.']\n            return _get_return_dict(True, warnings=warn)\n        if status != 'idle':\n            errors = ['Unknown status: \"{0}\"'.format(status)]\n            return _get_return_dict(False, data=resp['data'], errors=errors)\n    else:\n        errors = ['Status check failed. Response details: {0}'.format(resp)]\n        return _get_return_dict(False, data=resp['data'], errors=errors)\n\n    return resp",
        "rewrite": "```python\ndef _pre_index_check(handler, host=None, core_name=None):\n    \"\"\"\n    PRIVATE METHOD - MASTER CALL\n    Does a pre-check to make sure that all the options are set and that \n    we can talk to solr before trying to send a command to solr. This \n    Command should only be issued to masters.\n    \n    handler : str\n        The import handler to check the state of\n    host : str (None):\n        The solr host to query. __opts__['host'] is default\n    core_name (None):\n        The name of the solr core if using cores"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a AcousticModels object from a json dictionary.\"\"\"\n        args = {}\n        if 'customizations' in _dict:\n            args['customizations'] = [\n                AcousticModel._from_dict(x)\n                for x in (_dict.get('customizations'))\n            ]\n        else:\n            raise ValueError(\n                'Required property \\'customizations\\' not present in AcousticModels JSON'\n            )\n        return cls(**args)",
        "rewrite": "```python\ndef _from_dict(cls, _dict):\n    args = {}\n    if 'customizations' in _dict:\n        args['customizations'] = [\n            AcousticModel._from_dict(x) for x in _dict.get('customizations', [])\n        ]\n    else:\n        raise ValueError(\n            'Required property \\'customizations\\' not present in AcousticModels JSON'\n        )\n    return cls(**args)\n```"
    },
    {
        "original": "def process_metric(self, message, **kwargs):\n        \"\"\"\n        Handle a prometheus metric message according to the following flow:\n            - search self.metrics_mapper for a prometheus.metric <--> datadog.metric mapping\n            - call check method with the same name as the metric\n            - log some info if none of the above worked\n\n        `send_histograms_buckets` is used to specify if yes or no you want to send\n        the buckets as tagged values when dealing with histograms.\n        \"\"\"\n\n        # If targeted metric, store labels\n        self.store_labels(message)\n\n        if message.name in self.ignore_metrics:\n            return  # Ignore the metric\n\n        # Filter metric to see if we can enrich with joined labels\n        self.join_labels(message)\n\n        send_histograms_buckets = kwargs.get('send_histograms_buckets', True)\n        send_monotonic_counter = kwargs.get('send_monotonic_counter', False)\n        custom_tags = kwargs.get('custom_tags')\n        ignore_unmapped = kwargs.get('ignore_unmapped', False)\n\n        try:\n            if not self._dry_run:\n                try:\n                    self._submit(\n                        self.metrics_mapper[message.name],\n                        message,\n                        send_histograms_buckets,\n                        send_monotonic_counter,\n                        custom_tags,\n                    )\n                except KeyError:\n                    if not ignore_unmapped:\n                        # call magic method (non-generic check)\n                        handler = getattr(self, message.name)  # Lookup will throw AttributeError if not found\n                        try:\n                            handler(message, **kwargs)\n                        except Exception as err:\n                            self.log.warning(\"Error handling metric: {} - error: {}\".format(message.name, err))\n                    else:\n                        # build the wildcard list if first pass\n                        if self._metrics_wildcards is None:\n                            self._metrics_wildcards = [x for x in self.metrics_mapper.keys() if '*' in x]\n                        # try matching wildcard (generic check)\n                        for wildcard in self._metrics_wildcards:\n                            if fnmatchcase(message.name, wildcard):\n                                self._submit(\n                                    message.name, message, send_histograms_buckets, send_monotonic_counter, custom_tags\n                                )\n\n        except AttributeError as err:\n            self.log.debug(\"Unable to handle metric: {} - error: {}\".format(message.name, err))",
        "rewrite": "```python\ndef process_metric(self, message, **kwargs):\n    \"\"\"\n    Handle a prometheus metric message according to the following flow:\n        - search self.metrics_mapper for a prometheus.metric <--> datadog.metric mapping\n        - call check method with the same name as the metric\n        - log some info if none of the above worked\n\n    `send_histograms_buckets` is used to specify if yes or no you want to send\n    the buckets as tagged values when dealing with histograms.\n    \"\"\"\n\n    # If targeted metric, store labels\n    self.store_labels(message)\n\n    if message.name in self.ignore_metrics"
    },
    {
        "original": "async def _handle_future_salts(self, message):\n        \"\"\"\n        Handles future salt results, which don't come inside a\n        ``rpc_result`` but are still sent through a request:\n\n            future_salts#ae500895 req_msg_id:long now:int\n            salts:vector<future_salt> = FutureSalts;\n        \"\"\"\n        # TODO save these salts and automatically adjust to the\n        # correct one whenever the salt in use expires.\n        self._log.debug('Handling future salts for message %d', message.msg_id)\n        state = self._pending_state.pop(message.msg_id, None)\n        if state:\n            state.future.set_result(message.obj)",
        "rewrite": "```python\nasync def _handle_future_salts(self, message):\n    self._log.debug('Handling future salts for message %d', message.msg_id)\n    state = self._pending_state.pop(message.msg_id)\n    if state:\n        state.future.set_result(message.obj)\n```"
    },
    {
        "original": "def _get_day_of_month(other, day_option):\n    \"\"\"Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    \"\"\"\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)",
        "rewrite": "```python\ndef _get_day_of_month(other, day_option):\n    if not isinstance(day_option, str):\n        raise ValueError(\"day_option must be a string\")\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif not other:\n        raise ValueError(\"Date cannot be empty\")\n    else:\n        raise ValueError(f\"Invalid value for `day_option`: '{day_option}'\")\n```"
    },
    {
        "original": "def nie(self):\n        \"\"\"\n        https://es.wikipedia.org/wiki/N%C3%BAmero_de_identidad_de_extranjero\n        :return: a random Spanish NIE\n        \"\"\"\n\n        first_chr = random.randrange(0, 3)\n        doi_body = str(random.randrange(0, 10000000)).zfill(7)\n        control = self._calculate_control_doi(str(first_chr) + doi_body)\n        return \"XYZ\"[first_chr] + doi_body + control",
        "rewrite": "```python\nimport random\n\ndef _calculate_control_doi(nie_body: str) -> str:\n    weights = [49, 47, 43, 39, 35, 31, 27]\n    control = sum(int(digit) * weight for digit, weight in zip(map(str, nie_body), weights)) % 24\n    return str((10 - control) % 10)\n\ndef generate_nie(self) -> str:\n    first_chr = random.choices('XYZ', k=1)[0]\n    doi_body = str(random.randrange(0, 10000000)).zfill(7"
    },
    {
        "original": "def string(s):\n    \"\"\"\n    Convert a string to a escaped ASCII representation including quotation marks\n    :param s: a string\n    :return: ASCII escaped string\n    \"\"\"\n    ret = ['\"']\n    for c in s:\n        if ' ' <= c < '\\x7f':\n            if c == \"'\" or c == '\"' or c == '\\\\':\n                ret.append('\\\\')\n            ret.append(c)\n            continue\n        elif c <= '\\x7f':\n            if c in ('\\r', '\\n', '\\t'):\n                # unicode-escape produces bytes\n                ret.append(c.encode('unicode-escape').decode(\"ascii\"))\n                continue\n        i = ord(c)\n        ret.append('\\\\u')\n        ret.append('%x' % (i >> 12))\n        ret.append('%x' % ((i >> 8) & 0x0f))\n        ret.append('%x' % ((i >> 4) & 0x0f))\n        ret.append('%x' % (i & 0x0f))\n    ret.append('\"')\n    return ''.join(ret)",
        "rewrite": "```python\ndef string(s):\n    ret = ['\"']\n    for c in s:\n        if 32 <= ord(c) < 128: # ' ' <= c < '\\x7f'\n            if c in (\"'\", '\"', '\\\\'):\n                ret.append('\\\\')\n            ret.append(c)\n        elif ord(c) <= 127 and c not in ('\\r', '\\n', '\\t'):\n            i = ord(c)\n            ret.append('\\\\u')\n            ret.append('%04x' % (i >> 12))\n            ret.append('%04x' % ((i >> 8) & 0x0"
    },
    {
        "original": "def _jq_format(code):\n    \"\"\"\n    DEPRECATED - Use re.escape() instead, which performs the intended action.\n    Use before throwing raw code such as 'div[tab=\"advanced\"]' into jQuery.\n    Selectors with quotes inside of quotes would otherwise break jQuery.\n    If you just want to escape quotes, there's escape_quotes_if_needed().\n    This is similar to \"json.dumps(value)\", but with one less layer of quotes.\n    \"\"\"\n    code = code.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n    code = code.replace('\\\"', '\\\\\\\"').replace('\\'', '\\\\\\'')\n    code = code.replace('\\v', '\\\\v').replace('\\a', '\\\\a').replace('\\f', '\\\\f')\n    code = code.replace('\\b', '\\\\b').replace(r'\\u', '\\\\u').replace('\\r', '\\\\r')\n    return code",
        "rewrite": "```python\ndef _jq_format(code):\n    code = code.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n    code = code.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n    return str(code)\n```"
    },
    {
        "original": "def add_header_part(self):\n        \"\"\"Return (header_part, rId) pair for newly-created header part.\"\"\"\n        header_part = HeaderPart.new(self.package)\n        rId = self.relate_to(header_part, RT.HEADER)\n        return header_part, rId",
        "rewrite": "```python\ndef add_header_part(self):\n    header_part = HeaderPart.new(self.package)\n    rId = self.relate_to(header_part, 'hdrlock')\n    return header_part, rId\n```"
    },
    {
        "original": "def file_local_list(self, saltenv='base'):\n        \"\"\"\n        List files in the local minion files and localfiles caches\n        \"\"\"\n        filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n        localfilesdest = os.path.join(self.opts['cachedir'], 'localfiles')\n\n        fdest = self._file_local_list(filesdest)\n        ldest = self._file_local_list(localfilesdest)\n        return sorted(fdest.union(ldest))",
        "rewrite": "```python\ndef file_local_list(self, saltenv='base'):\n    files_dest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n    local_files_dest = os.path.join(self.opts['cachedir'], 'localfiles')\n\n    local_files = self._file_local_list(files_dest)\n    files_and_local_files = self._file_local_list(local_files_dest)\n\n    return sorted(set(local_files) | set(files_and_local_files))\n```"
    },
    {
        "original": "def distribute_aars(self, arch):\n        \"\"\"Process existing .aar bundles and copy to current dist dir.\"\"\"\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)",
        "rewrite": "```python\ndef distribute_aars(self, arch):\n    info('Unpacking aars')\n    for aar_file in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n        self._unpack_aar(aar_file, arch)\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'document') and self.document is not None:\n            _dict['document'] = self.document\n        if hasattr(self, 'targets') and self.targets is not None:\n            _dict['targets'] = self.targets\n        return _dict",
        "rewrite": "```python\ndef to_dict(self):\n    _dict = {}\n    for attr in dir(self):\n        value = getattr(self, attr)\n        if isinstance(value, (list, dict)) and value is not None:\n            _dict[attr] = value\n    return _dict.copy()\n```"
    },
    {
        "original": "def _get_sliced_variables(var_list):\n  \"\"\"Separates the sliced (partitioned) and unsliced variables in var_list.\n\n  Args:\n    var_list: a list of variables.\n\n  Returns:\n    A list of unsliced variables in var_list, and a dict mapping names to parts\n    for the sliced variables in var_list.\n  \"\"\"\n  unsliced_variables = []\n  sliced_variables = collections.defaultdict(lambda: [])\n  for var in var_list:\n    if var._save_slice_info:\n      sliced_variables[var._save_slice_info.full_name].append(var)\n    else:\n      unsliced_variables.append(var)\n  return unsliced_variables, sliced_variables",
        "rewrite": "```python\nfrom collections import defaultdict\n\ndef _get_sliced_variables(var_list):\n  unsliced_variables = []\n  sliced_variables = defaultdict(list)\n  for var in var_list:\n    if hasattr(var, '_save_slice_info'):\n      name = var._save_slice_info.full_name\n      sliced_variables[name].append(var)\n    else:\n      unsliced_variables.append(var)\n  return unsliced_variables, dict(sliced_variables.items())\n```"
    },
    {
        "original": "def AckFlowProcessingRequests(self, requests, cursor=None):\n    \"\"\"Deletes a list of flow processing requests from the database.\"\"\"\n    if not requests:\n      return\n\n    query = \"DELETE FROM flow_processing_requests WHERE \"\n\n    conditions = []\n    args = []\n    for r in requests:\n      conditions.append(\n          \"(client_id=%s AND flow_id=%s AND timestamp=FROM_UNIXTIME(%s))\")\n      args.append(db_utils.ClientIDToInt(r.client_id))\n      args.append(db_utils.FlowIDToInt(r.flow_id))\n      args.append(mysql_utils.RDFDatetimeToTimestamp(r.timestamp))\n\n    query += \" OR \".join(conditions)\n    cursor.execute(query, args)",
        "rewrite": "```python\ndef AckFlowProcessingRequests(self, requests, cursor=None):\n    if not requests:\n        return\n\n    conditions = []\n    args = []\n    for r in requests:\n        conditions.append(\n            \"(client_id=%s AND flow_id=%s AND timestamp=FROM_UNIXTIME(%s))\")\n        args.extend([db_utils.ClientIDToInt(r.client_id),\n                    db_utils.FlowIDToInt(r.flow_id),\n                    mysql_utils.RDFDatetimeToTimestamp(r.timestamp)])\n\n    query = \"DELETE FROM flow_processing_requests WHERE \" + \" OR \".join(conditions)\n    cursor.execute(query, args)\n```"
    },
    {
        "original": "def seconds_to_hms(input_seconds):\n    \"\"\"Convert seconds to human-readable time.\"\"\"\n    minutes, seconds = divmod(input_seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n\n    hours = int(hours)\n    minutes = int(minutes)\n    seconds = str(int(seconds)).zfill(2)\n\n    return hours, minutes, seconds",
        "rewrite": "```python\ndef seconds_to_hms(input_seconds):\n    hours, remainder = divmod(input_seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n\n    return int(hours), str(int(minutes)).zfill(2), str(int(seconds)).zfill(2)\n```"
    },
    {
        "original": "def _compute_H(self, t, index, t2, index2, update_derivatives=False, stationary=False):\n        \"\"\"Helper function for computing part of the ode1 covariance function.\n\n        :param t: first time input.\n        :type t: array\n        :param index: Indices of first output.\n        :type index: array of int\n        :param t2: second time input.\n        :type t2: array\n        :param index2: Indices of second output.\n        :type index2: array of int\n        :param update_derivatives: whether to update derivatives (default is False)\n        :return h : result of this subcomponent of the kernel for the given values.\n        :rtype: ndarray\n\"\"\"\n\n        if stationary:\n            raise NotImplementedError, \"Error, stationary version of this covariance not yet implemented.\"\n        # Vector of decays and delays associated with each output.\n        Decay = self.decay[index]\n        Decay2 = self.decay[index2]\n        t_mat = t[:, None]\n        t2_mat = t2[None, :]\n        if self.delay is not None:\n            Delay = self.delay[index]\n            Delay2 = self.delay[index2]\n            t_mat-=Delay[:, None]\n            t2_mat-=Delay2[None, :]\n\n        diff_t = (t_mat - t2_mat)\n        inv_sigma_diff_t = 1./self.sigma*diff_t\n        half_sigma_decay_i = 0.5*self.sigma*Decay[:, None]\n\n        ln_part_1, sign1 = ln_diff_erfs(half_sigma_decay_i + t2_mat/self.sigma, \n                                        half_sigma_decay_i - inv_sigma_diff_t,\n                                        return_sign=True)\n        ln_part_2, sign2 = ln_diff_erfs(half_sigma_decay_i,\n                                        half_sigma_decay_i - t_mat/self.sigma,\n                                        return_sign=True)\n\n        h = sign1*np.exp(half_sigma_decay_i\n                         *half_sigma_decay_i\n                         -Decay[:, None]*diff_t+ln_part_1\n                         -np.log(Decay[:, None] + Decay2[None, :]))\n        h -= sign2*np.exp(half_sigma_decay_i*half_sigma_decay_i\n                          -Decay[:, None]*t_mat-Decay2[None, :]*t2_mat+ln_part_2\n                          -np.log(Decay[:, None] + Decay2[None, :]))\n\n        if update_derivatives:\n            sigma2 = self.sigma*self.sigma\n            # Update ith decay gradient\n\n            dh_ddecay = ((0.5*Decay[:, None]*sigma2*(Decay[:, None] + Decay2[None, :])-1)*h\n                         + (-diff_t*sign1*np.exp(\n                half_sigma_decay_i*half_sigma_decay_i-Decay[:, None]*diff_t+ln_part_1\n                )\n                            +t_mat*sign2*np.exp(\n                half_sigma_decay_i*half_sigma_decay_i-Decay[:, None]*t_mat\n                - Decay2*t2_mat+ln_part_2))\n                         +self.sigma/np.sqrt(np.pi)*(\n                -np.exp(\n                -diff_t*diff_t/sigma2\n                )+np.exp(\n                -t2_mat*t2_mat/sigma2-Decay[:, None]*t_mat\n                )+np.exp(\n                -t_mat*t_mat/sigma2-Decay2[None, :]*t2_mat\n                )-np.exp(\n                -(Decay[:, None]*t_mat + Decay2[None, :]*t2_mat)\n                )\n                ))\n            self._dh_ddecay = (dh_ddecay/(Decay[:, None]+Decay2[None, :])).real\n            \n            # Update jth decay gradient\n            dh_ddecay2 = (t2_mat*sign2\n                         *np.exp(\n                half_sigma_decay_i*half_sigma_decay_i\n                -(Decay[:, None]*t_mat + Decay2[None, :]*t2_mat)\n                +ln_part_2\n                )\n                         -h)\n            self._dh_ddecay2 = (dh_ddecay/(Decay[:, None] + Decay2[None, :])).real\n            \n            # Update sigma gradient\n            self._dh_dsigma = (half_sigma_decay_i*Decay[:, None]*h\n                               + 2/(np.sqrt(np.pi)\n                                    *(Decay[:, None]+Decay2[None, :]))\n                               *((-diff_t/sigma2-Decay[:, None]/2)\n                                 *np.exp(-diff_t*diff_t/sigma2)\n                                 + (-t2_mat/sigma2+Decay[:, None]/2)\n                                 *np.exp(-t2_mat*t2_mat/sigma2-Decay[:, None]*t_mat) \n                                 - (-t_mat/sigma2-Decay[:, None]/2) \n                                 *np.exp(-t_mat*t_mat/sigma2-Decay2[None, :]*t2_mat) \n                                 - Decay[:, None]/2\n                                 *np.exp(-(Decay[:, None]*t_mat+Decay2[None, :]*t2_mat))))\n                \n        return h",
        "rewrite": "```python\ndef _compute_H(self, t, index, t2, index2, update_derivatives=False, stationary=False):\n    if stationary:\n        raise NotImplementedError\n\n    Decay = self.decay[index]\n    Decay2 = self.decay[index2]\n    t_mat = np.broadcast_to(t[:, None], (len(t), len(t2)))\n    t2_mat = np.broadcast_to(t2[None, :], (len(t), len(t2)))\n\n    if self.delay is not None:\n        Delay = self.delay[index]\n        Delay2 = self.delay[index2]\n        t_mat -= np.broadcast_to(Delay[:,"
    },
    {
        "original": "def change(governor, freq=None):\n    \"\"\"\n    change function\n    \"\"\"\n    cpu_number = 0\n\n    while True:\n        try:\n            subprocess.check_output([\n                \"sudo\", \"bash\", \"-c\",\n                \"echo {governor} > {CPU_PREFIX}cpu{cpu_number}/cpufreq/scaling_governor\"\n                .format(governor=governor,\n                        CPU_PREFIX=CPU_PREFIX,\n                        cpu_number=cpu_number)],\n                                    stderr=subprocess.STDOUT)\n        except:\n            break\n\n        if freq:\n            subprocess.check_output([\n                \"sudo\", \"bash\", \"-c\",\n                \"echo {freq} > {CPU_PREFIX}cpu{cpu_number}/cpufreq/scaling_setspeed\"\n                .format(freq=freq,\n                        CPU_PREFIX=CPU_PREFIX,\n                        cpu_number=cpu_number)],\n                                    stderr=subprocess.STDOUT)\n\n        cpu_number += 1",
        "rewrite": "```python\nimport subprocess\n\ndef change(governor, freq=None):\n    while True:\n        try:\n            cpu_number = 0\n            for i in range(int(subprocess.check_output([\"lscpu\"]).decode().split('\\n')[6].split(':')[1])):\n                subprocess.check_call([\"sudo\", \"bash\", \"-c\",\n                    f\"echo {governor} > /sys/devices/system/cpu/cpu{i}/cpufreq/scaling_governor\"])\n                if freq:\n                    subprocess.check_call([\"sudo\", \"bash\", \"-c\",\n                        f\"echo {freq} > /sys/devices/system/cpu/cpu{i}/"
    },
    {
        "original": "def stopped(name, kill=False, path=None):\n    \"\"\"\n    Ensure that a container is stopped\n\n    .. note::\n\n        This state does not enforce the existence of the named container, it\n        just stops the container if it running or frozen. To ensure that the\n        named container exists, use :mod:`lxc.present\n        <salt.states.lxc.present>`, or use the :mod:`lxc.absent\n        <salt.states.lxc.absent>` state to ensure that the container does not\n        exist.\n\n    name\n        The name of the container\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    kill : False\n        Do not wait for the container to stop, kill all tasks in the container.\n        Older LXC versions will stop containers like this irrespective of this\n        argument.\n\n        .. versionadded:: 2015.5.0\n\n    .. code-block:: yaml\n\n        web01:\n          lxc.stopped\n    \"\"\"\n    ret = {'name': name,\n           'result': True,\n           'comment': 'Container \\'{0}\\' is already stopped'.format(name),\n           'changes': {}}\n\n    state = {'old': __salt__['lxc.state'](name, path=path)}\n    if state['old'] is None:\n        ret['result'] = False\n        ret['comment'] = 'Container \\'{0}\\' does not exist'.format(name)\n        return ret\n    elif state['old'] == 'stopped':\n        return ret\n\n    if kill:\n        action = ('force-stop', 'force-stopped')\n    else:\n        action = ('stop', 'stopped')\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = ('Container \\'{0}\\' would be {1}'\n                          .format(name, action[1]))\n        return ret\n\n    try:\n        result = __salt__['lxc.stop'](name, kill=kill, path=path)\n    except (CommandExecutionError, SaltInvocationError) as exc:\n        ret['result'] = False\n        ret['comment'] = exc.strerror\n        state['new'] = __salt__['lxc.state'](name, path=path)\n    else:\n        state['new'] = result['state']['new']\n        if state['new'] != 'stopped':\n            ret['result'] = False\n            ret['comment'] = ('Unable to {0} container \\'{1}\\''\n                              .format(action[0], name))\n        else:\n            ret['comment'] = ('Container \\'{0}\\' was successfully {1}'\n                              .format(name, action[1]))\n\n    if state['old'] != state['new']:\n        ret['changes']['state'] = state\n    return ret",
        "rewrite": "```python\ndef stopped(name, kill=False, path=\"/var/lib/lxc\"):\n    \"\"\"\n    Ensure that a container is stopped\n    \"\"\"\n    ret = {'name': name,\n           'result': True,\n           'comment': f'Container \\'{name}\\' is already stopped',\n           'changes': {}}\n\n    state = {'old': __salt__['lxc.state'](name, path=path)}\n    \n    if state['old'] is None:\n        ret['result'] = False\n        ret['comment'] = f'Container \\'{name}\\' does not exist'\n        return ret\n    \n    elif state['"
    },
    {
        "original": "def Parse(self, rdf_data):\n    \"\"\"Process rdf data through filters. Test if results match expectations.\n\n    Processing of rdf data is staged by a filter handler, which manages the\n    processing of host data. The output of the filters are compared against\n    expected results.\n\n    Args:\n      rdf_data: An list containing 0 or more rdf values.\n\n    Returns:\n      An anomaly if data didn't match expectations.\n\n    Raises:\n      ProcessingError: If rdf_data is not a handled type.\n\n    \"\"\"\n    if not isinstance(rdf_data, (list, set)):\n      raise ProcessingError(\"Bad host data format: %s\" % type(rdf_data))\n    if self.baseline:\n      comparison = self.baseliner.Parse(rdf_data)\n    else:\n      comparison = rdf_data\n    found = self.handler.Parse(comparison)\n    results = self.hint.Render(found)\n    return self.matcher.Detect(comparison, results)",
        "rewrite": "```python\ndef Parse(self, rdf_data):\n    if not isinstance(rdf_data, (list, set)):\n        raise ProcessingError(\"Bad host data format: %s\" % type(rdf_data))\n    \n    comparison = self.baseline and self.baseliner.Parse(rdf_data) or rdf_data\n    \n    found = self.handler.Parse(comparison)\n    results = self.hint.Render(found)\n    \n    return self.matcher.Detect(comparison, results)\n```"
    },
    {
        "original": "def _unpickle_method(func_name, obj, cls):\n  \"\"\"Unpickle methods properly, including class methods.\"\"\"\n\n  if obj is None:\n    return cls.__dict__[func_name].__get__(obj, cls)\n  for cls in cls.__mro__:\n    try:\n      func = cls.__dict__[func_name]\n    except KeyError:\n      pass\n    else:\n      break\n  return func.__get__(obj, cls)",
        "rewrite": "```python\ndef _unpickle_method(func_name, obj, cls):\n  if obj is None:\n    return getattr(cls, func_name).__get__(obj, cls)\n  for parent_cls in reversed(cls.__mro__):\n    func = getattr(parent_cls, func_name, None)\n    if func is not None:\n      break\n  return func.__get__(obj) or lambda: None\n```"
    },
    {
        "original": "def _validate_resource_path(path):\n        \"\"\"\n        Validate the resource paths according to the docs.\n        https://setuptools.readthedocs.io/en/latest/pkg_resources.html#basic-resource-access\n\n        >>> warned = getfixture('recwarn')\n        >>> warnings.simplefilter('always')\n        >>> vrp = NullProvider._validate_resource_path\n        >>> vrp('foo/bar.txt')\n        >>> bool(warned)\n        False\n        >>> vrp('../foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('/foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> vrp('foo/../../bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('foo/f../bar.txt')\n        >>> bool(warned)\n        False\n\n        Windows path separators are straight-up disallowed.\n        >>> vrp(r'\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        >>> vrp(r'C:\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        Blank values are allowed\n\n        >>> vrp('')\n        >>> bool(warned)\n        False\n\n        Non-string values are not.\n\n        >>> vrp(None)\n        Traceback (most recent call last):\n        ...\n        AttributeError: ...\n        \"\"\"\n        invalid = (\n            os.path.pardir in path.split(posixpath.sep) or\n            posixpath.isabs(path) or\n            ntpath.isabs(path)\n        )\n        if not invalid:\n            return\n\n        msg = \"Use of .. or absolute path in a resource path is not allowed.\"\n\n        # Aggressively disallow Windows absolute paths\n        if ntpath.isabs(path) and not posixpath.isabs(path):\n            raise ValueError(msg)\n\n        # for compatibility, warn; in future\n        # raise ValueError(msg)\n        warnings.warn(\n            msg[:-1] + \" and will raise exceptions in a future release.\",\n            DeprecationWarning,\n            stacklevel=4,\n        )",
        "rewrite": "```python\nimport os\nimport posixpath\nimport ntpath\nimport warnings\n\ndef _validate_resource_path(path):\n    if not isinstance(path, str):\n        raise AttributeError(\"Resource path must be a string.\")\n\n    invalid = (\n        posixpath.pardir in path.split(posixpath.sep) or\n        posixpath.isabs(path) or\n        ntpath.isabs(path)\n    )\n    \n    if not invalid:\n        return\n\n    msg = \"Use of .. or absolute path in a resource path is not allowed.\"\n\n    # Aggressively disallow Windows absolute paths and other absolute paths for compatibility.\n   "
    },
    {
        "original": "def get_initials(pinyin, strict):\n    \"\"\"\u83b7\u53d6\u5355\u4e2a\u62fc\u97f3\u4e2d\u7684\u58f0\u6bcd.\n\n    :param pinyin: \u5355\u4e2a\u62fc\u97f3\n    :type pinyin: unicode\n    :param strict: \u662f\u5426\u4e25\u683c\u9075\u7167\u300a\u6c49\u8bed\u62fc\u97f3\u65b9\u6848\u300b\u6765\u5904\u7406\u58f0\u6bcd\u548c\u97f5\u6bcd\n    :return: \u58f0\u6bcd\n    :rtype: unicode\n    \"\"\"\n    if strict:\n        _initials = _INITIALS\n    else:\n        _initials = _INITIALS_NOT_STRICT\n\n    for i in _initials:\n        if pinyin.startswith(i):\n            return i\n    return ''",
        "rewrite": "```python\ndef get_initials(pinyin, strict):\n    _INITIALS = ['b', 'p', 'm', 'f', 'd', 't', 'n', \n                 'l', 'g', '', '', '', \n                 '', '', '',\n                 '', '',\n                 '')\n    _INITIALS_NOT_STRICT = ['b1','b2','p1','p2','m1','m2',\n                            'f1','f2',\n                            \"d\", \"t\",\n                            \"n\", \"l\",\n                            \"\", \"\",\n                            \"\", \"\",\n                            \"\"]\n    \n    if strict:\n        initials_map ="
    },
    {
        "original": "def open_phdos(self):\n        \"\"\"Open PHDOS file produced by Anaddb and returns :class:`PhdosFile` object.\"\"\"\n        from abipy.dfpt.phonons import PhdosFile\n        phdos_path = os.path.join(self.workdir, \"run.abo_PHDOS.nc\")\n        if not phdos_path:\n            if self.status == self.S_OK:\n                logger.critical(\"%s reached S_OK but didn't produce a PHBST file in %s\" % (self, self.outdir))\n            return None\n\n        try:\n            return PhdosFile(phdos_path)\n        except Exception as exc:\n            logger.critical(\"Exception while reading GSR file at %s:\\n%s\" % (phdos_path, str(exc)))\n            return None",
        "rewrite": "```python\ndef open_phdos(self) -> PhdosFile:\n    import os\n    from abipy.dfpt.phonons import PhdosFile\n\n    phdos_path = os.path.join(self.workdir, \"run.abo_PHDOS.nc\")\n    \n    if not phdos_path and self.status == self.S_OK:\n        logger.critical(\"%s reached S_OK but didn't produce a PHBST file in %s\" % (self, self.outdir))\n        return None\n\n    try:\n        return PhdosFile(phdos_path)\n    except Exception as exc:\n        logger.critical(f\"Exception while reading"
    },
    {
        "original": "def createDataChannel(self, label, maxPacketLifeTime=None, maxRetransmits=None,\n                          ordered=True, protocol='', negotiated=False, id=None):\n        \"\"\"\n        Create a data channel with the given label.\n\n        :rtype: :class:`RTCDataChannel`\n        \"\"\"\n        if maxPacketLifeTime is not None and maxRetransmits is not None:\n            raise ValueError('Cannot specify both maxPacketLifeTime and maxRetransmits')\n\n        if not self.__sctp:\n            self.__createSctpTransport()\n\n        parameters = RTCDataChannelParameters(\n            id=id,\n            label=label,\n            maxPacketLifeTime=maxPacketLifeTime,\n            maxRetransmits=maxRetransmits,\n            negotiated=negotiated,\n            ordered=ordered,\n            protocol=protocol)\n        return RTCDataChannel(self.__sctp, parameters)",
        "rewrite": "```python\ndef createDataChannel(self, label, maxPacketLifeTime: int = None, maxRetransmits: int = None,\n                      ordered: bool = True, protocol: str = '', negotiated: bool = False, id: str = None):\n    if maxPacketLifeTime is not None and (maxRetransmit is not None or self.__maxRetransmit is not None):\n        raise ValueError('Cannot specify both maxPacketLifeTime and maxRetransmits')\n\n    if not self.__sctp:\n        self.__createSctpTransport()\n\n    parameters = RTCDataChannelParameters(\n        id=id or f"
    },
    {
        "original": "def delete_model(self, model_name):\n        \"\"\"Delete an Amazon SageMaker Model.\n\n        Args:\n            model_name (str): Name of the Amazon SageMaker model to delete.\n\n        \"\"\"\n        LOGGER.info('Deleting model with name: {}'.format(model_name))\n        self.sagemaker_client.delete_model(ModelName=model_name)",
        "rewrite": "```python\ndef delete_model(self, model_name: str) -> None:\n    LOGGER.info(f'Deleting model with name: {model_name}')\n    self.sagemaker_client.delete_model(ModelName=model_name)\n```"
    },
    {
        "original": "def play_move(self, c):\n        \"\"\"Notable side effects:\n          - finalizes the probability distribution according to\n          this roots visit counts into the class' running tally, `searches_pi`\n          - Makes the node associated with this move the root, for future\n            `inject_noise` calls.\n        \"\"\"\n        if not self.two_player_mode:\n            self.searches_pi.append(self.root.children_as_pi(\n                self.root.position.n < self.temp_threshold))\n        self.comments.append(self.root.describe())\n        try:\n            self.root = self.root.maybe_add_child(coords.to_flat(c))\n        except go.IllegalMove:\n            dbg(\"Illegal move\")\n            if not self.two_player_mode:\n                self.searches_pi.pop()\n            self.comments.pop()\n            raise\n\n        self.position = self.root.position  # for showboard\n        del self.root.parent.children\n        return True",
        "rewrite": "```python\ndef play_move(self, c):\n    if not self.two_player_mode:\n        self.searches_pi.append(self.root.children_as_pi(\n            self.root.position.n < self.temp_threshold))\n    self.comments.append(self.root.describe())\n    \n    try:\n        new_root = self.root.maybe_add_child(coords.to_flat(c))\n        if new_root is None:\n            dbg(\"Illegal move\")\n            if not self.two_player_mode:\n                del self.searches_pi[-1]\n            del self.comments[-1]\n            raise go.IllegalMove\n        else:\n            return True\n    except go.IllegalMove as e:\n       "
    },
    {
        "original": "def upload_permanent_video(self, title, introduction, video):\n        \"\"\"\n        \u4e0a\u4f20\u6c38\u4e45\u89c6\u9891\u3002\n\n        :param title: \u89c6\u9891\u7d20\u6750\u7684\u6807\u9898\n        :param introduction: \u89c6\u9891\u7d20\u6750\u7684\u63cf\u8ff0\n        :param video: \u8981\u4e0a\u4f20\u7684\u89c6\u9891\uff0c\u4e00\u4e2a File-object\n        :return: requests \u7684 Response \u5b9e\u4f8b\n        \"\"\"\n        return requests.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/add_material\",\n            params={\n                \"access_token\": self.token,\n                \"type\": \"video\"\n            },\n            data={\n                \"description\": _json.dumps(\n                    {\n                        \"title\": title,\n                        \"introduction\": introduction\n                    },\n                    ensure_ascii=False\n                ).encode(\"utf-8\")\n            },\n            files={\"media\": video}\n        )",
        "rewrite": "```python\ndef upload_permanent_video(self, title, introduction, video):\n    response = requests.post(\n        url=\"https://api.weixin.qq.com/cgi-bin/material/add_material\",\n        params={\n            \"access_token\": self.token,\n            \"type\": \"video\"\n        },\n        data={\n            \"description\": json.dumps(\n                {\"title\": title, \"introduction\": introduction},\n                ensure_ascii \u0441\u0435\u0431\u0435 s\u00f6z decode=True) \n            # changed .encode(), decode=True in dumped_json instead  \n        },\n        files={\"media\": video}\n    )\n    return response\n```"
    },
    {
        "original": "def next_k_array(a):\n    \"\"\"\n    Given an array `a` of k distinct nonnegative integers, sorted in\n    ascending order, return the next k-array in the lexicographic\n    ordering of the descending sequences of the elements [1]_. `a` is\n    modified in place.\n\n    Parameters\n    ----------\n    a : ndarray(int, ndim=1)\n        Array of length k.\n\n    Returns\n    -------\n    a : ndarray(int, ndim=1)\n        View of `a`.\n\n    Examples\n    --------\n    Enumerate all the subsets with k elements of the set {0, ..., n-1}.\n\n    >>> n, k = 4, 2\n    >>> a = np.arange(k)\n    >>> while a[-1] < n:\n    ...     print(a)\n    ...     a = next_k_array(a)\n    ...\n    [0 1]\n    [0 2]\n    [1 2]\n    [0 3]\n    [1 3]\n    [2 3]\n\n    References\n    ----------\n    .. [1] `Combinatorial number system\n       <https://en.wikipedia.org/wiki/Combinatorial_number_system>`_,\n       Wikipedia.\n\n    \"\"\"\n    # Logic taken from Algotirhm T in D. Knuth, The Art of Computer\n    # Programming, Section 7.2.1.3 \"Generating All Combinations\".\n    k = len(a)\n    if k == 1 or a[0] + 1 < a[1]:\n        a[0] += 1\n        return a\n\n    a[0] = 0\n    i = 1\n    x = a[i] + 1\n\n    while i < k-1 and x == a[i+1]:\n        i += 1\n        a[i-1] = i - 1\n        x = a[i] + 1\n    a[i] = x\n\n    return a",
        "rewrite": "```python\ndef next_k_array(a):\n    k = len(a)\n    if k == 1 or a[0] + 1 < a[1]:\n        a[0] += 1\n        return a\n\n    for i in range(1, k):\n        if i >= len(a) - 2:\n            val_to_increment = minus_inds[i-2]\n            x = minus_inds[val_to_increment] + 1\n            break\n\n        elif x > a[i+1]:\n            smaller_one_right_initially_ind = i - 2 \n            while not (x > a[i+1])"
    },
    {
        "original": "def init_app(self, app):\n        \"\"\"Initialize this class with the given :class:`flask.Flask`\n        application or :class:`flask.Blueprint` object.\n\n        :param app: the Flask application or blueprint object\n        :type app: flask.Flask\n        :type app: flask.Blueprint\n\n        Examples::\n\n            api = Api()\n            api.add_resource(...)\n            api.init_app(app)\n\n        \"\"\"\n        # If app is a blueprint, defer the initialization\n        try:\n            app.record(self._deferred_blueprint_init)\n        # Flask.Blueprint has a 'record' attribute, Flask.Api does not\n        except AttributeError:\n            self._init_app(app)\n        else:\n            self.blueprint = app",
        "rewrite": "```python\ndef init_app(self, app):\n    try:\n        app.record(self._deferred_blueprint_init)\n    except AttributeError:\n        self._init_app(app)\n    else:\n        self.blueprint = app\n```"
    },
    {
        "original": "def rotate_view(self, axis_ind=0, angle=0):\n        \"\"\"\n        Rotate the camera view.\n\n        Args:\n            axis_ind: Index of axis to rotate. Defaults to 0, i.e., a-axis.\n            angle: Angle to rotate by. Defaults to 0.\n        \"\"\"\n        camera = self.ren.GetActiveCamera()\n        if axis_ind == 0:\n            camera.Roll(angle)\n        elif axis_ind == 1:\n            camera.Azimuth(angle)\n        else:\n            camera.Pitch(angle)\n        self.ren_win.Render()",
        "rewrite": "```python\ndef rotate_view(self, axis_ind=0, angle=0.0):\n    \"\"\"\n    Rotate the camera view.\n    \"\"\"\n    camera = self.ren.GetActiveCamera()\n    \n    if axis_ind in (1, 2):  # Pitch if axis_ind is not 0 or 1 then it is z-axis\n        camera.Roll(angle)\n    \n    elif axis_ind == 1:     \n        camera.Azimuth(angle)\n        \n    else:\n        camera.Pitch(angle)\n    \n    self.ren_win.Render()   \n```"
    },
    {
        "original": "def get_item_key(self, item):\n        \"\"\"Return the value of the item 'key'.\"\"\"\n        try:\n            ret = item[item['key']]\n        except KeyError:\n            logger.error(\"No 'key' available in {}\".format(item))\n        if isinstance(ret, list):\n            return ret[0]\n        else:\n            return ret",
        "rewrite": "```python\ndef get_item_key(self, item):\n    try:\n        ret = item['key']\n    except KeyError:\n        logger.error(\"No 'key' available in {}\".format(item))\n        return None\n    if isinstance(ret, list):\n        return ret[0]\n    else:\n        return ret\n```"
    },
    {
        "original": "def build_vocab(self, *args, **kwargs):\n        \"\"\"Construct the Vocab object for this field from one or more datasets.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for this field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        \"\"\"\n        counter = Counter()\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in data:\n                if not self.sequential:\n                    x = [x]\n                try:\n                    counter.update(x)\n                except TypeError:\n                    counter.update(chain.from_iterable(x))\n        specials = list(OrderedDict.fromkeys(\n            tok for tok in [self.unk_token, self.pad_token, self.init_token,\n                            self.eos_token] + kwargs.pop('specials', [])\n            if tok is not None))\n        self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)",
        "rewrite": "```python\nfrom collections import Counter, OrderedDict\nfrom itertools import chain\n\ndef build_vocab(self, *args, **kwargs):\n    counter = Counter()\n    sources = []\n    \n    for arg in args:\n        if isinstance(arg, Dataset):\n            sources += [getattr(arg, name) for name, field in arg.fields.items() if field is self]\n        else:\n            sources.append(arg)\n            \n    for data in sources:\n        for x in data:\n            if not self.sequential:\n                x = [x]\n            try:\n                counter.update(x)\n            except TypeError:\n                try:  # assume its a list or"
    },
    {
        "original": "def get_recipe_dir(self):\n        \"\"\"\n        Returns the local recipe directory or defaults to the core recipe\n        directory.\n        \"\"\"\n        if self.ctx.local_recipes is not None:\n            local_recipe_dir = join(self.ctx.local_recipes, self.name)\n            if exists(local_recipe_dir):\n                return local_recipe_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)",
        "rewrite": "```python\nfrom pathlib import Path\n\ndef get_recipe_dir(self):\n    local_recipe_dir = self.ctx.local_recipes / self.name if self.ctx.local_recipes else None\n    return Path(local_recipe_dir) if local_recipe_dir and Path(local_recipe_dir).exists() else (self.ctx.root_dir / 'recipes' / self.name)\n```"
    },
    {
        "original": "def config(check):\n    \"\"\"Validate default configuration files.\"\"\"\n    if check:\n        checks = [check]\n    else:\n        checks = sorted(get_valid_checks())\n\n    files_failed = {}\n    files_warned = {}\n    num_files = 0\n\n    echo_waiting('Validating default configuration files...')\n    for check in checks:\n        check_display_queue = []\n\n        config_files = get_config_files(check)\n        for config_file in config_files:\n            num_files += 1\n            file_display_queue = []\n            file_name = basepath(config_file)\n\n            try:\n                config_data = yaml.safe_load(read_file(config_file))\n            except Exception as e:\n                files_failed[config_file] = True\n\n                # We must convert to text here to free Exception object before it goes out of scope\n                error = str(e)\n\n                check_display_queue.append(lambda: echo_info('{}:'.format(file_name), indent=True))\n                check_display_queue.append(lambda: echo_failure('Invalid YAML -', indent=FILE_INDENT))\n                check_display_queue.append(lambda: echo_info(error, indent=FILE_INDENT * 2))\n                continue\n\n            # Verify there is an `instances` section\n            if 'instances' not in config_data:\n                files_failed[config_file] = True\n                file_display_queue.append(lambda: echo_failure('Missing `instances` section', indent=FILE_INDENT))\n\n            # Verify there is a default instance\n            else:\n                instances = config_data['instances']\n                if check not in IGNORE_DEFAULT_INSTANCE and not isinstance(instances, list):\n                    files_failed[config_file] = True\n                    file_display_queue.append(lambda: echo_failure('No default instance', indent=FILE_INDENT))\n\n            if file_display_queue:\n                check_display_queue.append(lambda: echo_info('{}:'.format(file_name), indent=True))\n                check_display_queue.extend(file_display_queue)\n\n        if check_display_queue:\n            echo_success('{}:'.format(check))\n            for display in check_display_queue:\n                display()\n\n    files_failed = len(files_failed)\n    files_warned = len(files_warned)\n    files_passed = num_files - (files_failed + files_warned)\n\n    if files_failed or files_warned:\n        click.echo()\n\n    if files_failed:\n        echo_failure('Files with errors: {}'.format(files_failed))\n\n    if files_warned:\n        echo_warning('Files with warnings: {}'.format(files_warned))\n\n    if files_passed:\n        if files_failed or files_warned:\n            echo_success('Files valid: {}'.format(files_passed))\n        else:\n            echo_success('All {} configuration files are valid!'.format(num_files))\n\n    if files_failed:\n        abort()",
        "rewrite": "```python\ndef config(check):\n    checks = sorted(get_valid_checks()) if not check else [check]\n\n    files_failed = {}\n    files_warned = {}\n    num_files_passed = 0\n\n    echo_waiting('Validating default configuration files...')\n    for check_name in checks:\n        check_display_queue = []\n\n        config_files = get_config_files(check_name)\n        for config_file in config_files:\n            try:\n                with open(config_file, 'r') as file:\n                    content = file.read()\n                yaml.safe_load(content)\n            except yaml.YAMLError as e:\n                files_failed[config_file] = True\n"
    },
    {
        "original": "def section(self, section):\n        \"\"\"The block sections of code to be used as substitutions\n\n        :rtype: Section, list(Section)\n        \"\"\"\n        if isinstance(section, list):\n            for h in section:\n                self.add_section(h)\n        else:\n            self.add_section(section)",
        "rewrite": "```python\ndef section(self, section):\n    \"\"\"The block sections of code to be used as substitutions\n\n    :rtype: Section, list(Section)\n    \"\"\"\n    if isinstance(section, list):\n        for item in section:\n            self.add_section(item)\n    else:\n        self.add_section(section)\n```"
    },
    {
        "original": "def from_dict(cls, d):\n        \"\"\"\n        As in :Class: `pymatgen.core.Molecule` except\n        restoring graphs using `from_dict_of_dicts`\n        from NetworkX to restore graph information.\n        \"\"\"\n        m = Molecule.from_dict(d['molecule'])\n        return cls(m, d['graphs'])",
        "rewrite": "```python\ndef from_dict(cls, d):\n    m = cls.from_dict(d['molecule'])\n    graphs = [\n        nx.from_dict_of_dicts(graph) if isinstance(graph, dict)\n        else graph for graph in d.get('graphs', [])\n    ]\n    return cls(m, graphs)\n```"
    },
    {
        "original": "def send_over_websocket(self, *, payload: dict):\n        \"\"\"Sends a message to Slack over the WebSocket connection.\n\n        Note:\n            The RTM API only supports posting simple messages formatted using\n            our default message formatting mode. It does not support\n            attachments or other message formatting modes. For this reason\n            we recommend users send messages via the Web API methods.\n            e.g. web_client.chat_postMessage()\n\n            If the message \"id\" is not specified in the payload, it'll be added.\n\n        Args:\n            payload (dict): The message to send over the wesocket.\n            e.g.\n            {\n                \"id\": 1,\n                \"type\": \"typing\",\n                \"channel\": \"C024BE91L\"\n            }\n\n        Raises:\n            SlackClientNotConnectedError: Websocket connection is closed.\n        \"\"\"\n        if self._websocket is None or self._event_loop is None:\n            raise client_err.SlackClientNotConnectedError(\n                \"Websocket connection is closed.\"\n            )\n        if \"id\" not in payload:\n            payload[\"id\"] = self._next_msg_id()\n        asyncio.ensure_future(\n            self._websocket.send_str(json.dumps(payload)), loop=self._event_loop\n        )",
        "rewrite": "```python\nasync def send_over_websocket(self, *, payload: dict):\n    if self._websocket is None or self._event_loop is None:\n        raise client_err.SlackClientNotConnectedError(\"Websocket connection is closed.\")\n    if \"id\" not in payload:\n        payload[\"id\"] = self._next_msg_id()\n    await self._websocket.send_str(json.dumps(payload))\n```\n\nNote that I've changed `self.send_over_websocket` to `async def send_over_websocket(self, *, payload: dict):`, as it seems more likely that this method should be awaited (since it involves sending data over"
    },
    {
        "original": "def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c",
        "rewrite": "```python\ndef replace_species(self, species_mapping):\n    \"\"\"\n    Swap species.\n\n    Args:\n        species_mapping (dict): dict of species to swap. Species can be\n            elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n            a Li for Na substitution. The second species can be a\n            sp_and_occu dict. For example, a site with 0.5 Si that is\n            passed the mapping {Element('Si'): {Element('Ge'):0.75,\n            Element('C'):0.25} } will have .375 Ge and .125 C.\n   "
    },
    {
        "original": "def get_variables(self):\n        \"\"\"\n        Returns list of variables of the network\n\n        Example\n        -------\n        >>> reader = PomdpXReader(\"pomdpx.xml\")\n        >>> reader.get_variables()\n        {'StateVar': [\n                        {'vnamePrev': 'rover_0',\n                         'vnameCurr': 'rover_1',\n                         'ValueEnum': ['s0', 's1', 's2'],\n                         'fullyObs': True},\n                        {'vnamePrev': 'rock_0',\n                         'vnameCurr': 'rock_1',\n                         'fullyObs': False,\n                         'ValueEnum': ['good', 'bad']}],\n                        'ObsVar': [{'vname': 'obs_sensor',\n                                    'ValueEnum': ['ogood', 'obad']}],\n                        'RewardVar': [{'vname': 'reward_rover'}],\n                        'ActionVar': [{'vname': 'action_rover',\n                                       'ValueEnum': ['amw', 'ame',\n                                                     'ac', 'as']}]\n                        }\n        \"\"\"\n        self.variables = defaultdict(list)\n        for variable in self.network.findall('Variable'):\n            _variables = defaultdict(list)\n            for var in variable.findall('StateVar'):\n                state_variables = defaultdict(list)\n                state_variables['vnamePrev'] = var.get('vnamePrev')\n                state_variables['vnameCurr'] = var.get('vnameCurr')\n                if var.get('fullyObs'):\n                    state_variables['fullyObs'] = True\n                else:\n                    state_variables['fullyObs'] = False\n                state_variables['ValueEnum'] = []\n                if var.find('NumValues') is not None:\n                    for i in range(0, int(var.find('NumValues').text)):\n                        state_variables['ValueEnum'].append('s' + str(i))\n                if var.find('ValueEnum') is not None:\n                    state_variables['ValueEnum'] = \\\n                        var.find('ValueEnum').text.split()\n                _variables['StateVar'].append(state_variables)\n\n            for var in variable.findall('ObsVar'):\n                obs_variables = defaultdict(list)\n                obs_variables['vname'] = var.get('vname')\n                obs_variables['ValueEnum'] = \\\n                    var.find('ValueEnum').text.split()\n                _variables['ObsVar'].append(obs_variables)\n\n            for var in variable.findall('ActionVar'):\n                action_variables = defaultdict(list)\n                action_variables['vname'] = var.get('vname')\n                action_variables['ValueEnum'] = \\\n                    var.find('ValueEnum').text.split()\n                _variables['ActionVar'].append(action_variables)\n\n            for var in variable.findall('RewardVar'):\n                reward_variables = defaultdict(list)\n                reward_variables['vname'] = var.get('vname')\n                _variables['RewardVar'].append(reward_variables)\n\n            self.variables.update(_variables)\n\n        return self.variables",
        "rewrite": "```python\ndef get_variables(self):\n    self.variables = defaultdict(list)\n    for variable in self.network.findall('Variable'):\n        _variables = defaultdict(list)\n        for var in variable.findall('StateVar'):\n            state_variables = {\n                'vnamePrev': var.get('vnamePrev'),\n                'vnameCurr': var.get('vnameCurr'),\n                'fullyObs': True if var.get('fullyObs') else False,\n                'ValueEnum': []\n            }\n            if var.find('NumValues') is not None:\n                state_variables['ValueEnum'] = ['s' + str(i) for i in range"
    },
    {
        "original": "def get_availability_zone(vm_):\n    \"\"\"\n    Return the availability zone to use\n    \"\"\"\n    avz = config.get_cloud_config_value(\n        'availability_zone', vm_, __opts__, search_global=False\n    )\n\n    if avz is None:\n        return None\n\n    zones = _list_availability_zones(vm_)\n\n    # Validate user-specified AZ\n    if avz not in zones:\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t valid in this region: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    # check specified AZ is available\n    elif zones[avz] != 'available':\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t currently available: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    return avz",
        "rewrite": "```python\ndef get_availability_zone(vm):\n    avz = config.get_cloud_config_value('availability_zone', vm, __opts__, search_global=False)\n    if avz is None:\n        return None\n    zones = _list_availability_zones(vm)\n    if avz not in zones:\n        raise SaltCloudException(f'The specified availability zone isn\\'t valid in this region: {avz}')\n    if zones[avz] != 'available':\n        raise SaltCloudException(f'The specified availability zone isn\\'t currently available: {avz}')\n    return avz\n```"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "rewrite": "```python\ndef saveFile(self):\n    filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n        self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n    if filepath:\n        if not filepath.endswith(\".ag\"):\n            filepath += \".ag\"\n\n        self.showStatus(f\"Saving {filepath}...\")\n        self.saveSession(filepath)\n        self.showStatus(f\"Saved Session to {filepath}\")\n```\n\nOr with f-string formatting is used for status message updates (this solution uses same behavior just following modern specification)\n\n```python\ndef saveFile(self):\n    filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n       "
    },
    {
        "original": "def findall_operations(self, predicate: Callable[[ops.Operation], bool]\n                           ) -> Iterable[Tuple[int, ops.Operation]]:\n        \"\"\"Find the locations of all operations that satisfy a given condition.\n\n        This returns an iterator of (index, operation) tuples where each\n        operation satisfies op_cond(operation) is truthy. The indices are\n        in order of the moments and then order of the ops within that moment.\n\n        Args:\n            predicate: A method that takes an Operation and returns a Truthy\n                value indicating the operation meets the find condition.\n\n        Returns:\n            An iterator (index, operation)'s that satisfy the op_condition.\n        \"\"\"\n        for index, moment in enumerate(self._moments):\n            for op in moment.operations:\n                if predicate(op):\n                    yield index, op",
        "rewrite": "```python\ndef findall_operations(self, predicate: Callable[[ops.Operation], bool]) -> Iterable[Tuple[int, ops.Operation]]:\n    for index, moment in enumerate(self._moments):\n        for op in moment.operations:\n            if predicate(op):\n                yield index << 32 | self._moments.index(moment), op\n```"
    }
]
