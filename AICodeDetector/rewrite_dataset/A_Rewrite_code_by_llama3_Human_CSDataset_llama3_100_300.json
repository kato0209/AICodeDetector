[
    {
        "original": "def is_constructing_scv(self) -> bool:\n        \"\"\" Checks if the unit is an SCV that is currently building. \"\"\"\n        return self.orders and self.orders[0].ability.id in {\n            AbilityId.TERRANBUILD_ARMORY,\n            AbilityId.TERRANBUILD_BARRACKS,\n            AbilityId.TERRANBUILD_BUNKER,\n            AbilityId.TERRANBUILD_COMMANDCENTER,\n            AbilityId.TERRANBUILD_ENGINEERINGBAY,\n            AbilityId.TERRANBUILD_FACTORY,\n            AbilityId.TERRANBUILD_FUSIONCORE,\n            AbilityId.TERRANBUILD_GHOSTACADEMY,\n            AbilityId.TERRANBUILD_MISSILETURRET,\n            AbilityId.TERRANBUILD_REFINERY,\n            AbilityId.TERRANBUILD_SENSORTOWER,\n            AbilityId.TERRANBUILD_STARPORT,\n            AbilityId.TERRANBUILD_SUPPLYDEPOT,\n        }",
        "rewrite": "\n\n\ndef is_constructing_scv(self) -> bool:\n    return self.orders and self.orders[0].ability.id in (\n        AbilityId.TERRANBUILD_ARMORY,\n        AbilityId.TERRANBUILD_BARRACKS,\n        AbilityId.TERRANBUILD_BUNKER,\n        AbilityId.TERRANBUILD_COMMANDCENTER,\n        AbilityId.TERRANBUILD_ENGINEERINGBAY,\n        AbilityId.TERRANBUILD_FACTORY,\n        AbilityId.TERRANBUILD_FUSIONCORE,\n        AbilityId.TERRANBUILD_GHOSTACADEMY,\n        AbilityId.TERRAN"
    },
    {
        "original": "def set_form_field_order(form, field_order):\n    \"\"\"\n    This function is a verbatim copy of django.forms.Form.order_fields() to\n    support field ordering below Django 1.9.\n\n    field_order is a list of field names specifying the order. Append fields\n    not included in the list in the default order for backward compatibility\n    with subclasses not overriding field_order. If field_order is None, keep\n    all fields in the order defined in the class. Ignore unknown fields in\n    field_order to allow disabling fields in form subclasses without\n    redefining ordering.\n    \"\"\"\n    if field_order is None:\n        return\n    fields = OrderedDict()\n    for key in field_order:\n        try:\n            fields[key] = form.fields.pop(key)\n        except KeyError:  # ignore unknown fields\n            pass\n    fields.update(form.fields)  # add remaining fields in original order\n    form.fields = fields",
        "rewrite": "\n\n\ndef set_form_field_order(form, field_order):\n    if field_order is None:\n        return\n    fields = OrderedDict()\n    for key in field_order:\n        fields[key] = form.fields.pop(key) if key in form.fields else None\n    fields.update(form.fields)\n    form.fields = fields\n"
    },
    {
        "original": "def _region_base(self, region):\n        \"\"\"\n        Get the base address of a memory region.\n\n        :param str region: ID of the memory region\n        :return: Address of the memory region\n        :rtype: int\n        \"\"\"\n\n        if region == 'global':\n            region_base_addr = 0\n        elif region.startswith('stack_'):\n            region_base_addr = self._stack_region_map.absolutize(region, 0)\n        else:\n            region_base_addr = self._generic_region_map.absolutize(region, 0)\n\n        return region_base_addr",
        "rewrite": "\n\n\ndef _region_base(self, region):\n    if region == 'global':\n        return 0\n    elif region.startswith('stack_'):\n        return self._stack_region_map.absolutize(region, 0)\n    else:\n        return self._generic_region_map.absolutize(region, 0)\n"
    },
    {
        "original": "def init_client(client_id):\n    \"\"\"Initialse a driver for client and store for future reference\n    \n    @param client_id: ID of client user\n    @return whebwhatsapi object\n    \"\"\"\n    if client_id not in drivers:\n        drivers[client_id] = init_driver(client_id)\n    return drivers[client_id]",
        "rewrite": "\n\n\ndef init_client(client_id):\n    if client_id not in drivers:\n        drivers[client_id] = init_driver(client_id)\n    return drivers[client_id]\n"
    },
    {
        "original": "def cell(self, row_idx, col_idx):\n        \"\"\"\n        Return |_Cell| instance correponding to table cell at *row_idx*,\n        *col_idx* intersection, where (0, 0) is the top, left-most cell.\n        \"\"\"\n        cell_idx = col_idx + (row_idx * self._column_count)\n        return self._cells[cell_idx]",
        "rewrite": "\n\n\ndef cell(self, row_idx, col_idx):\n    return self._cells[col_idx + row_idx * self._column_count]\n"
    },
    {
        "original": "def get(self, card_id):\n        \"\"\"\n        \u67e5\u8be2\u5361\u5238\u8be6\u60c5\n        \"\"\"\n        result = self._post(\n            'card/get',\n            data={\n                'card_id': card_id\n            },\n            result_processor=lambda x: x['card']\n        )\n        return result",
        "rewrite": "\n\n\ndef get(self, card_id):\n    return self._post('card/get', data={'card_id': card_id}, result_processor=lambda x: x['card'])\n"
    },
    {
        "original": "def set_not_found_handler(self, handler, version=None):\n        \"\"\"Sets the not_found handler for the specified version of the api\"\"\"\n        if not self.not_found_handlers:\n            self._not_found_handlers = {}\n\n        self.not_found_handlers[version] = handler",
        "rewrite": "\n\n\ndef set_not_found_handler(self, handler, version=None):\n    self.not_found_handlers = self.not_found_handlers or {}\n    self.not_found_handlers[version] = handler\n"
    },
    {
        "original": "def measure_each(*qubits: raw_types.Qid,\n                 key_func: Callable[[raw_types.Qid], str] = str\n                 ) -> List[gate_operation.GateOperation]:\n    \"\"\"Returns a list of operations individually measuring the given qubits.\n\n    The qubits are measured in the computational basis.\n\n    Args:\n        *qubits: The qubits to measure.\n        key_func: Determines the key of the measurements of each qubit. Takes\n            the qubit and returns the key for that qubit. Defaults to str.\n\n    Returns:\n        A list of operations individually measuring the given qubits.\n    \"\"\"\n    return [MeasurementGate(1, key_func(q)).on(q) for q in qubits]",
        "rewrite": "\n\n\nfrom typing import Callable, List\nfrom cirq import gate_operation, raw_types\n\ndef measure_each(*qubits: raw_types.Qid, \n                 key_func: Callable[[raw_types.Qid], str] = str\n                 ) -> List[gate_operation.GateOperation]:\n    return [gate_operation.MeasurementGate(1, key_func(q)).on(q) for q in qubits]\n"
    },
    {
        "original": "def participants(self, **kwargs):\n        \"\"\"List the participants.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of participants\n        \"\"\"\n\n        path = '%s/%s/participants' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)",
        "rewrite": "\n\n\ndef participants(self, all=False, per_page=None, page=None, as_list=True, **kwargs):\n    path = f\"{self.manager.path}/{self.get_id()}/participants\"\n    return self.manager.gitlab.http_get(path, all=all, per_page=per_page, page=page, as_list=as_list, **kwargs)\n"
    },
    {
        "original": "def revdep_rebuild(lib=None):\n    \"\"\"\n    Fix up broken reverse dependencies\n\n    lib\n        Search for reverse dependencies for a particular library rather\n        than every library on the system. It can be a full path to a\n        library or basic regular expression.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gentoolkit.revdep_rebuild\n    \"\"\"\n    cmd = 'revdep-rebuild -i --quiet --no-progress'\n    if lib is not None:\n        cmd += ' --library={0}'.format(lib)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0",
        "rewrite": "\n\n\ndef revdep_rebuild(lib=None):\n    cmd = ['revdep-rebuild', '-i', '--quiet', '--no-progress']\n    if lib is not None:\n        cmd.extend(['--library={}'.format(lib)])\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0\n"
    },
    {
        "original": "def _canonicalize_name(prefix, qvm_type, noisy):\n    \"\"\"Take the output of _parse_name to create a canonical name.\n    \"\"\"\n    if noisy:\n        noise_suffix = '-noisy'\n    else:\n        noise_suffix = ''\n\n    if qvm_type is None:\n        qvm_suffix = ''\n    elif qvm_type == 'qvm':\n        qvm_suffix = '-qvm'\n    elif qvm_type == 'pyqvm':\n        qvm_suffix = '-pyqvm'\n    else:\n        raise ValueError(f\"Unknown qvm_type {qvm_type}\")\n\n    name = f'{prefix}{noise_suffix}{qvm_suffix}'\n    return name",
        "rewrite": "\n\n\ndef _canonicalize_name(prefix, qvm_type, noisy):\n    noise_suffix = '-noisy' if noisy else ''\n    qvm_suffix = '' if qvm_type is None else f\"-{qvm_type}\" if qvm_type in ['qvm', 'pyqvm'] else raise ValueError(f\"Unknown qvm_type {qvm_type}\")\n    return f\"{prefix}{noise_suffix}{qvm_suffix}\"\n"
    },
    {
        "original": "def _line(self, text, indent=0):\n        \"\"\"Write 'text' word-wrapped at self.width characters.\"\"\"\n        leading_space = '  ' * indent\n        while len(leading_space) + len(text) > self.width:\n            # The text is too wide; wrap if possible.\n\n            # Find the rightmost space that would obey our width constraint and\n            # that's not an escaped space.\n            available_space = self.width - len(leading_space) - len(' $')\n            space = available_space\n            while True:\n                space = text.rfind(' ', 0, space)\n                if (space < 0 or\n                    self._count_dollars_before_index(text, space) % 2 == 0):\n                    break\n\n            if space < 0:\n                # No such space; just use the first unescaped space we can find.\n                space = available_space - 1\n                while True:\n                    space = text.find(' ', space + 1)\n                    if (space < 0 or\n                        self._count_dollars_before_index(text, space) % 2 == 0):\n                        break\n            if space < 0:\n                # Give up on breaking.\n                break\n\n            self.output.write(leading_space + text[0:space] + ' $\\n')\n            text = text[space+1:]\n\n            # Subsequent lines are continuations, so indent them.\n            leading_space = '  ' * (indent+2)\n\n        self.output.write(leading_space + text + '\\n')",
        "rewrite": "\n\n\ndef _line(self, text, indent=0):\n    leading_space = '  ' * indent\n    while len(leading_space) + len(text) > self.width:\n        available_space = self.width - len(leading_space) - len(' $')\n        space = text.rfind(' ', 0, available_space)\n        if space < 0:\n            space = available_space - 1\n            while True:\n                space = text.find(' ', space + 1)\n                if space < 0:\n                    break\n        if space < 0:\n            break\n       "
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            ifrealname = i['interface_name'].split(':')[0]\n            # Convert rate in bps ( to be able to compare to interface speed)\n            bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n            bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n            # Decorate the bitrate with the configuration file thresolds\n            alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n            alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')\n            # If nothing is define in the configuration file...\n            # ... then use the interface speed (not available on all systems)\n            if alert_rx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_rx = self.get_alert(current=bps_rx,\n                                          maximum=i['speed'],\n                                          header='rx')\n            if alert_tx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_tx = self.get_alert(current=bps_tx,\n                                          maximum=i['speed'],\n                                          header='tx')\n            # then decorates\n            self.views[i[self.get_key()]]['rx']['decoration'] = alert_rx\n            self.views[i[self.get_key()]]['tx']['decoration'] = alert_tx",
        "rewrite": "\n\n\ndef update_views(self):\n    super(Plugin, self).update_views()\n    for i in self.stats:\n        ifrealname = i['interface_name'].split(':')[0]\n        bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n        bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n        alert_rx = self.get_alert(bps_rx, header=f\"{ifrealname}_rx\")\n        alert_tx = self.get_alert(bps_tx, header=f\"{ifrealname}_tx\")\n        if"
    },
    {
        "original": "def get_feedback(self, feedback_id, model=None, **kwargs):\n        \"\"\"\n        List a specified feedback entry.\n\n        Lists a feedback entry with a specified `feedback_id`.\n\n        :param str feedback_id: A string that specifies the feedback entry to be included\n        in the output.\n        :param str model: The analysis model to be used by the service. For the **Element\n        classification** and **Compare two documents** methods, the default is\n        `contracts`. For the **Extract tables** method, the default is `tables`. These\n        defaults apply to the standalone methods as well as to the methods' use in\n        batch-processing requests.\n        :param dict headers: A `dict` containing the request headers\n        :return: A `DetailedResponse` containing the result, headers and HTTP status code.\n        :rtype: DetailedResponse\n        \"\"\"\n\n        if feedback_id is None:\n            raise ValueError('feedback_id must be provided')\n\n        headers = {}\n        if 'headers' in kwargs:\n            headers.update(kwargs.get('headers'))\n        sdk_headers = get_sdk_headers('compare-comply', 'V1', 'get_feedback')\n        headers.update(sdk_headers)\n\n        params = {'version': self.version, 'model': model}\n\n        url = '/v1/feedback/{0}'.format(*self._encode_path_vars(feedback_id))\n        response = self.request(\n            method='GET',\n            url=url,\n            headers=headers,\n            params=params,\n            accept_json=True)\n        return response",
        "rewrite": "\n\n\ndef get_feedback(self, feedback_id, model=None, **kwargs):\n    if feedback_id is None:\n        raise ValueError('feedback_id must be provided')\n\n    headers = kwargs.get('headers', {})\n    headers.update(get_sdk_headers('compare-comply', 'V1', 'get_feedback'))\n\n    params = {'version': self.version}\n    if model is not None:\n        params['model'] = model\n\n    url = '/v1/feedback/{0}'.format(*self._encode_path_vars(feedback_id))\n    response = self.request(\n        method='GET',\n        url=url,\n"
    },
    {
        "original": "def probably_identical(self):\n        \"\"\"\n        :returns: Whether or not these two functions are identical.\n        \"\"\"\n        if len(self._unmatched_blocks_from_a | self._unmatched_blocks_from_b) > 0:\n            return False\n        for (a, b) in self._block_matches:\n            if not self.blocks_probably_identical(a, b):\n                return False\n        return True",
        "rewrite": "\n\n\ndef probably_identical(self):\n    return not self._unmatched_blocks_from_a | self._unmatched_blocks_from_b and all(self.blocks_probably_identical(a, b) for a, b in self._block_matches)\n"
    },
    {
        "original": "def logpdf_link(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Log Likelihood function given inverse link of f.\n\n        .. math::\n            \\\\ln p(y_{i}|\\\\lambda(f_{i})) = y_{i}\\\\log\\\\lambda(f_{i}) + (1-y_{i})\\\\log (1-f_{i})\n\n        :param inv_link_f: latent variables inverse link of f.\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata must contain 'trials'\n        :returns: log likelihood evaluated at points inverse link of f.\n        :rtype: float\n        \"\"\"\n        N = Y_metadata['trials']\n        np.testing.assert_array_equal(N.shape, y.shape)\n\n        nchoosey = special.gammaln(N+1) - special.gammaln(y+1) - special.gammaln(N-y+1)\n        \n        Ny = N-y\n        t1 = np.zeros(y.shape)\n        t2 = np.zeros(y.shape)\n        t1[y>0] = y[y>0]*np.log(inv_link_f[y>0])\n        t2[Ny>0] = Ny[Ny>0]*np.log(1.-inv_link_f[Ny>0])\n        \n        return nchoosey + t1 + t2",
        "rewrite": "\n\n\ndef logpdf_link(self, inv_link_f, y, Y_metadata=None):\n    N = Y_metadata['trials']\n    np.testing.assert_array_equal(N.shape, y.shape)\n\n    nchoosey = special.gammaln(N+1) - special.gammaln(y+1) - special.gammaln(N-y+1)\n    \n    Ny = N-y\n    log_likelihood = np.where(y > 0, y * np.log(inv_link_f), 0) + np.where(Ny > 0, Ny * np.log(1. - inv_link_f), "
    },
    {
        "original": "def __load_compose_definitions(path, definition):\n    \"\"\"\n    Will load the compose file located at path\n    Then determines the format/contents of the sent definition\n\n    err or results are only set if there were any\n\n    :param path:\n    :param definition:\n    :return tuple(compose_result, loaded_definition, err):\n    \"\"\"\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return None, None, err\n    if isinstance(definition, dict):\n        return compose_result, definition, None\n    elif definition.strip().startswith('{'):\n        try:\n            loaded_definition = json.deserialize(definition)\n        except json.DeserializationError as jerr:\n            msg = 'Could not parse {0} {1}'.format(definition, jerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    else:\n        try:\n            loaded_definition = yaml.load(definition)\n        except yaml.YAMLError as yerr:\n            msg = 'Could not parse {0} {1}'.format(definition, yerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    return compose_result, loaded_definition, None",
        "rewrite": "\n\n\ndef __load_compose_definitions(path, definition):\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return None, None, err\n    if isinstance(definition, dict):\n        return compose_result, definition, None\n    try:\n        if definition.strip().startswith('{'):\n            loaded_definition = json.loads(definition)\n        else:\n            loaded_definition = yaml.safe_load(definition)\n        return compose_result, loaded_definition, None\n    except (json.JSONDecodeError, yaml.YAMLError) as err:\n        msg = f'Could not parse"
    },
    {
        "original": "def convert_date(obj):\n    \"\"\"Returns a DATE column as a date object:\n\n      >>> date_or_None('2007-02-26')\n      datetime.date(2007, 2, 26)\n\n    Illegal values are returned as None:\n\n      >>> date_or_None('2007-02-31') is None\n      True\n      >>> date_or_None('0000-00-00') is None\n      True\n\n    \"\"\"\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n    try:\n        return datetime.date(*[ int(x) for x in obj.split('-', 2) ])\n    except ValueError:\n        return obj",
        "rewrite": "\n\n\ndef convert_date(obj):\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n    try:\n        year, month, day = map(int, obj.split('-', 2))\n        return datetime.date(year, month, day)\n    except ValueError:\n        return None\n"
    },
    {
        "original": "async def build_get_cred_def_request(submitter_did: Optional[str],\n                                     id_: str) -> str:\n    \"\"\"\n   Builds a GET_CRED_DEF request. Request to get a credential definition (in particular, public key),\n   that Issuer creates for a particular Credential Schema.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param id_: Credential Definition Id in ledger.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_cred_def_request: >>> submitter_did: %r, id: %r\",\n                 submitter_did,\n                 id_)\n\n    if not hasattr(build_get_cred_def_request, \"cb\"):\n        logger.debug(\"build_get_cred_def_request: Creating callback\")\n        build_get_cred_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_id = c_char_p(id_.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_cred_def_request',\n                                 c_submitter_did,\n                                 c_id,\n                                 build_get_cred_def_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_cred_def_request: <<< res: %r\", res)\n    return res",
        "rewrite": "\n\n\nasync def build_get_cred_def_request(submitter_did: Optional[str], id_: str) -> str:\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_cred_def_request: >>> submitter_did: %r, id: %r\", submitter_did, id_)\n\n    cb = build_get_cred_def_request.cb if hasattr(build_get_cred_def_request, \"cb\") else create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n    build_get_cred_def_request.cb = cb\n\n    c_submitter_did = c"
    },
    {
        "original": "def _get_restartcheck_result(errors):\n    \"\"\"\n    Return restartcheck result and append errors (if any) to ``errors``\n    \"\"\"\n    rs_result = __salt__['restartcheck.restartcheck'](verbose=False)\n    if isinstance(rs_result, dict) and 'comment' in rs_result:\n        errors.append(rs_result['comment'])\n    return rs_result",
        "rewrite": "\n\n\ndef _get_restartcheck_result(errors):\n    rs_result = __salt__['restartcheck.restartcheck'](verbose=False)\n    if isinstance(rs_result, dict) and 'comment' in rs_result:\n        errors.extend([rs_result['comment']])\n    return rs_result\n"
    },
    {
        "original": "def check_enable_mode(self, check_string=\"\"):\n        \"\"\"Check if in enable mode. Return boolean.\n\n        :param check_string: Identification of privilege mode from device\n        :type check_string: str\n        \"\"\"\n        self.write_channel(self.RETURN)\n        output = self.read_until_prompt()\n        return check_string in output",
        "rewrite": "\n\n\ndef check_enable_mode(self, check_string: str = \"\") -> bool:\n    self.write_channel(self.RETURN)\n    output = self.read_until_prompt()\n    return check_string in output\n"
    },
    {
        "original": "def _check_rot_sym(self, axis):\n        \"\"\"\n        Determines the rotational symmetry about supplied axis.  Used only for\n        symmetric top molecules which has possible rotational symmetry\n        operations > 2.\n        \"\"\"\n        min_set = self._get_smallest_set_not_on_axis(axis)\n        max_sym = len(min_set)\n        for i in range(max_sym, 0, -1):\n            if max_sym % i != 0:\n                continue\n            op = SymmOp.from_axis_angle_and_translation(axis, 360 / i)\n            rotvalid = self.is_valid_op(op)\n            if rotvalid:\n                self.symmops.append(op)\n                self.rot_sym.append((axis, i))\n                return i\n        return 1",
        "rewrite": "\n\n\ndef _check_rot_sym(self, axis):\n    min_set = self._get_smallest_set_not_on_axis(axis)\n    max_sym = len(min_set)\n    for i in range(max_sym, 0, -1):\n        if max_sym % i != 0:\n            continue\n        op = SymmOp.from_axis_angle_and_translation(axis, 360 / i)\n        if self.is_valid_op(op):\n            self.symmops.append(op)\n            self.rot_sym.append((axis, i))\n            return i\n    return 1\n"
    },
    {
        "original": "def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst",
        "rewrite": "\n\n\ndef _get_values(self, lst, list_columns):\n    return [{col: self._get_attr_value(item, col) for col in list_columns} for item in lst]\n"
    },
    {
        "original": "def post_message(message, chat_id=None, token=None):\n    \"\"\"\n    Send a message to a Telegram chat.\n\n    :param message: The message to send to the Telegram chat.\n    :param chat_id: (optional) The Telegram chat id.\n    :param token:   (optional) The Telegram API token.\n    :return:        Boolean if message was sent successfully.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' telegram.post_message message=\"Hello Telegram!\"\n\n    \"\"\"\n    if not chat_id:\n        chat_id = _get_chat_id()\n\n    if not token:\n        token = _get_token()\n\n    if not message:\n        log.error('message is a required option.')\n\n    return _post_message(message=message, chat_id=chat_id, token=token)",
        "rewrite": "\n\n\ndef post_message(message, chat_id=None, token=None):\n    if not message:\n        log.error('message is a required option.')\n        return False\n\n    if not chat_id:\n        chat_id = _get_chat_id()\n\n    if not token:\n        token = _get_token()\n\n    return _post_message(message=message, chat_id=chat_id, token=token)\n"
    },
    {
        "original": "def topics(self):\n        \"\"\"\n        Get the set of topics that can be extracted from this report.\n        \"\"\"\n        return set(node.tag for node in self.root.iter() if node.attrib)",
        "rewrite": "\ndef topics(self):\n    return {node.tag for node in self.root.iter() if node.attrib}\n"
    },
    {
        "original": "def get_data_home(path=None):\n    \"\"\"\n    Return the path of the Yellowbrick data directory. This folder is used by\n    dataset loaders to avoid downloading data several times.\n\n    By default, this folder is colocated with the code in the install directory\n    so that data shipped with the package can be easily located. Alternatively\n    it can be set by the YELLOWBRICK_DATA environment variable, or\n    programmatically by giving a folder path. Note that the '~' symbol is\n    expanded to the user home directory, and environment variables are also\n    expanded when resolving the path.\n    \"\"\"\n    if path is None:\n        path = os.environ.get('YELLOWBRICK_DATA', FIXTURES)\n\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    return path",
        "rewrite": "\n\n\nimport os\n\nFIXTURES = os.path.join(os.path.dirname(__file__), 'fixtures')\n\ndef get_data_home(path=None):\n    if path is None:\n        path = os.environ.get('YELLOWBRICK_DATA', FIXTURES)\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n    os.makedirs(path, exist_ok=True)\n    return path\n"
    },
    {
        "original": "def get_monolayer(self):\n        \"\"\"\n        Returns the primitive unit surface area density of the\n            adsorbate.\n        \"\"\"\n\n        unit_a = self.get_unit_primitive_area\n        Nsurfs = self.Nsurfs_ads_in_slab\n        Nads = self.Nads_in_slab\n        return Nads / (unit_a * Nsurfs)",
        "rewrite": "\n\n\ndef get_monolayer(self):\n    unit_a = self.get_unit_primitive_area()\n    Nsurfs = self.Nsurfs_ads_in_slab\n    Nads = self.Nads_in_slab\n    return Nads / (unit_a * Nsurfs)\n"
    },
    {
        "original": "def string_match(sf,regex,field=2):\n    \"\"\"\n    Return the geometry and attributes of a shapefile whose fields match a regular expression given\n\n    :param sf: shapefile\n    :type sf: shapefile object\n    :regex: regular expression to match\n    :type regex: string\n    :field: field number to be matched with the regex\n    :type field: integer\n    \"\"\"\n    index = []\n    shape_records = []\n    for rec in enumerate(sf.shapeRecords()):\n        m = re.search(regex,rec[1].record[field])\n        if m is not None:\n            index.append(rec[0])\n            shape_records.append(rec[1])\n    return index,shape_records",
        "rewrite": "\n\n\nimport re\n\ndef string_match(sf, regex, field=2):\n    matches = [(i, rec) for i, rec in enumerate(sf.shapeRecords()) if re.search(regex, rec.record[field])]\n    return [i for i, _ in matches], [rec for _, rec in matches]\n"
    },
    {
        "original": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)",
        "rewrite": "\n\n\ndef add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n    with self._subscribers_cv:\n        self._subscribers[connection_id] = EventSubscriber(connection_id, subscriptions, last_known_block_id)\n    LOGGER.debug('Added Subscriber %s for %s', connection_id, subscriptions)\n"
    },
    {
        "original": "def get_summed_cohp_by_label_and_orbital_list(self, label_list, orbital_list, divisor=1):\n        \"\"\"\n        Returns a COHP object that includes a summed COHP divided by divisor\n\n        Args:\n            label_list: list of labels for the COHP that should be included in the summed cohp\n            orbital_list: list of orbitals for the COHPs that should be included in the summed cohp (same order as label_list)\n            divisor: float/int, the summed cohp will be divided by this divisor\n        Returns:\n            Returns a COHP object including a summed COHP\n        \"\"\"\n        # check if cohps are spinpolarized or not\n        first_cohpobject = self.get_orbital_resolved_cohp(label_list[0], orbital_list[0])\n        summed_cohp = first_cohpobject.cohp.copy()\n        summed_icohp = first_cohpobject.icohp.copy()\n        for ilabel, label in enumerate(label_list[1:], 1):\n            cohp_here = self.get_orbital_resolved_cohp(label, orbital_list[ilabel])\n            summed_cohp[Spin.up] = np.sum([summed_cohp[Spin.up], cohp_here.cohp.copy()[Spin.up]], axis=0)\n            if Spin.down in summed_cohp:\n                summed_cohp[Spin.down] = np.sum([summed_cohp[Spin.down], cohp_here.cohp.copy()[Spin.down]], axis=0)\n            summed_icohp[Spin.up] = np.sum([summed_icohp[Spin.up], cohp_here.icohp.copy()[Spin.up]], axis=0)\n            if Spin.down in summed_icohp:\n                summed_icohp[Spin.down] = np.sum([summed_icohp[Spin.down], cohp_here.icohp.copy()[Spin.down]], axis=0)\n\n        divided_cohp = {}\n        divided_icohp = {}\n        divided_cohp[Spin.up] = np.divide(summed_cohp[Spin.up], divisor)\n        divided_icohp[Spin.up] = np.divide(summed_icohp[Spin.up], divisor)\n        if Spin.down in summed_cohp:\n            divided_cohp[Spin.down] = np.divide(summed_cohp[Spin.down], divisor)\n            divided_icohp[Spin.down] = np.divide(summed_icohp[Spin.down], divisor)\n\n        return Cohp(efermi=first_cohpobject.efermi, energies=first_cohpobject.energies, cohp=divided_cohp,\n                    are_coops=first_cohpobject.are_coops,\n                    icohp=divided_icohp)",
        "rewrite": "\n\n\ndef get_summed_cohp_by_label_and_orbital_list(self, label_list, orbital_list, divisor=1):\n    first_cohpobject = self.get_orbital_resolved_cohp(label_list[0], orbital_list[0])\n    summed_cohp = {spin: cohp.copy() for spin, cohp in first_cohpobject.cohp.items()}\n    summed_icohp = {spin: icohp.copy() for spin, icohp in first_cohpobject.icohp.items()}\n\n    for label, orbital in"
    },
    {
        "original": "def get_qr(self, filename=None):\n        \"\"\"Get pairing QR code from client\"\"\"\n        if \"Click to reload QR code\" in self.driver.page_source:\n            self.reload_qr()\n        qr = self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])\n        if filename is None:\n            fd, fn_png = tempfile.mkstemp(prefix=self.username, suffix='.png')\n        else:\n            fd = os.open(filename, os.O_RDWR | os.O_CREAT)\n            fn_png = os.path.abspath(filename)\n        self.logger.debug(\"QRcode image saved at %s\" % fn_png)\n        qr.screenshot(fn_png)\n        os.close(fd)\n        return fn_png",
        "rewrite": "\n\n\ndef get_qr(self, filename=None):\n    if \"Click to reload QR code\" in self.driver.page_source:\n        self.reload_qr()\n    qr = self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])\n    if filename is None:\n        fd, fn_png = tempfile.mkstemp(prefix=self.username, suffix='.png')\n    else:\n        fd = os.open(filename, os.O_RDWR | os.O_CREAT)\n        fn_png = os.path.abspath(filename)\n    qr.screenshot(fn_png)\n    os.close(fd)\n    self.logger.debug(f\"QR code image"
    },
    {
        "original": "def edit(self, name, color, description=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PATCH /repos/:owner/:repo/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param name: string\n        :param color: string\n        :param description: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(name, (str, unicode)), name\n        assert isinstance(color, (str, unicode)), color\n        assert description is github.GithubObject.NotSet or isinstance(description, (str, unicode)), description\n        post_parameters = {\n            \"name\": name,\n            \"color\": color,\n        }\n        if description is not github.GithubObject.NotSet:\n            post_parameters[\"description\"] = description\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PATCH\",\n            self.url,\n            input=post_parameters,\n            headers={'Accept': Consts.mediaTypeLabelDescriptionSearchPreview}\n        )\n        self._useAttributes(data)",
        "rewrite": "\n\n\ndef edit(self, name: str, color: str, description: str = github.GithubObject.NotSet) -> None:\n    assert isinstance(name, str), name\n    assert isinstance(color, str), color\n    assert description is github.GithubObject.NotSet or isinstance(description, str), description\n    post_parameters = {\"name\": name, \"color\": color}\n    if description is not github.GithubObject.NotSet:\n        post_parameters[\"description\"] = description\n    headers, data = self._requester.requestJsonAndCheck(\n        \"PATCH\", self.url, input=post"
    },
    {
        "original": "def documentation(self, add_to=None):\n        \"\"\"Produces general documentation for the interface\"\"\"\n        doc = OrderedDict if add_to is None else add_to\n\n        usage = self.interface.spec.__doc__\n        if usage:\n            doc['usage'] = usage\n        if getattr(self, 'requires', None):\n            doc['requires'] = [getattr(requirement, '__doc__', requirement.__name__) for requirement in self.requires]\n        doc['outputs'] = OrderedDict()\n        doc['outputs']['format'] = self.outputs.__doc__\n        doc['outputs']['content_type'] = self.outputs.content_type\n        parameters = [param for param in self.parameters if not param in ('request', 'response', 'self')\n                                                        and not param in ('api_version', 'body')\n                                                        and not param.startswith('hug_')\n                                                        and not hasattr(param, 'directive')]\n        if parameters:\n            inputs = doc.setdefault('inputs', OrderedDict())\n            types = self.interface.spec.__annotations__\n            for argument in parameters:\n                kind = types.get(argument, text)\n                if getattr(kind, 'directive', None) is True:\n                    continue\n\n                input_definition = inputs.setdefault(argument, OrderedDict())\n                input_definition['type'] = kind if isinstance(kind, str) else kind.__doc__\n                default = self.defaults.get(argument, None)\n                if default is not None:\n                    input_definition['default'] = default\n\n        return doc",
        "rewrite": "\n\n\ndef documentation(self, add_to=None):\n    doc = OrderedDict() if add_to is None else add_to\n\n    usage = self.interface.spec.__doc__\n    if usage:\n        doc['usage'] = usage\n\n    if getattr(self, 'requires', None):\n        doc['requires'] = [getattr(requirement, '__doc__', requirement.__name__) for requirement in self.requires]\n\n    doc['outputs'] = OrderedDict()\n    doc['outputs']['format'] = self.outputs.__doc__\n    doc['outputs']['content_type'] = self.outputs.content_type\n\n    parameters = [param for param in"
    },
    {
        "original": "def switch_to_frame(driver, frame, timeout=settings.SMALL_TIMEOUT):\n    \"\"\"\n    Wait for an iframe to appear, and switch to it. This should be usable\n    as a drop-in replacement for driver.switch_to.frame().\n    @Params\n    driver - the webdriver object (required)\n    frame - the frame element, name, or index\n    timeout - the time to wait for the alert in seconds\n    \"\"\"\n\n    start_ms = time.time() * 1000.0\n    stop_ms = start_ms + (timeout * 1000.0)\n    for x in range(int(timeout * 10)):\n        try:\n            driver.switch_to.frame(frame)\n            return True\n        except NoSuchFrameException:\n            now_ms = time.time() * 1000.0\n            if now_ms >= stop_ms:\n                break\n            time.sleep(0.1)\n    raise Exception(\"Frame was not present after %s seconds!\" % timeout)",
        "rewrite": "\n\n\nfrom selenium.common.exceptions import NoSuchFrameException\n\ndef switch_to_frame(driver, frame, timeout):\n    start_ms = time.time() * 1000.0\n    stop_ms = start_ms + (timeout * 1000.0)\n    while time.time() * 1000.0 < stop_ms:\n        try:\n            driver.switch_to.frame(frame)\n            return True\n        except NoSuchFrameException:\n            time.sleep(0.1)\n    raise Exception(\"Frame was not present after %s seconds!\" % timeout)\n"
    },
    {
        "original": "def process_eni_metrics(\n        stream_eni, myips, stream,\n        start, end, period, sample_size,\n        resolver, sink_uri):\n    \"\"\"ENI flow stream processor that rollups, enhances,\n       and indexes the stream by time period.\"\"\"\n    stats = Counter()\n    period_counters = flow_stream_stats(myips, stream, period)\n    client = InfluxDBClient.from_dsn(sink_uri)\n    resource = resolver.resolve_resource(stream_eni)\n    points = []\n\n    for period in sorted(period_counters):\n        pc = period_counters[period]\n        pd = datetime.fromtimestamp(period)\n\n        for t in ('inbytes', 'outbytes'):\n            tpc = pc[t]\n            ips = [ip for ip, _ in tpc.most_common(sample_size)]\n            resolved = resolver.resolve(ips, pd - timedelta(900), pd + timedelta(900))\n            logical_counter = rollup_logical(tpc, resolved, ('app', 'env'))\n            for (app, env), v in logical_counter.items():\n                p = {}\n#                rinfo = resolved.get(ip, {})\n                p['fields'] = {'Bytes': v}\n                p['measurement'] = 'traffic_%s' % t\n                p['time'] = datetime.fromtimestamp(period)\n                p['tags'] = {\n                    'Kind': resource['type'],\n                    'AccountId': resource['account_id'],\n                    'App': resource['app'],\n                    'Env': resource['env'],\n                    'ForeignApp': app,\n                    'ForeignEnv': env}\n                points.append(p)\n\n        if len(points) > 2000:\n            client.write_points(points)\n            stats['Points'] += len(points)\n            points = []\n\n    client.write_points(points)\n    stats['Points'] += len(points)\n    log.info('periods:%d resource:%s points:%d',\n             len(period_counters), resource, stats['Points'])\n    return stats",
        "rewrite": "\n\n\ndef process_eni_metrics(\n    stream_eni, myips, stream,\n    start, end, period, sample_size,\n    resolver, sink_uri\n):\n    stats = Counter()\n    period_counters = flow_stream_stats(myips, stream, period)\n    client = InfluxDBClient.from_dsn(sink_uri)\n    resource = resolver.resolve_resource(stream_eni)\n    points = []\n\n    for period in sorted(period_counters):\n        pc = period_counters[period]\n        pd = datetime.fromtimestamp(period)\n\n        for t in ('inbytes', 'outbytes'):\n            tpc ="
    },
    {
        "original": "def read_existing_paths(bt_table):\n    \"\"\"Return the SGF filename for each existing eval record.\"\"\"\n    rows = bt_table.read_rows(\n        filter_=row_filters.ColumnRangeFilter(\n            METADATA, SGF_FILENAME, SGF_FILENAME))\n    names = (row.cell_value(METADATA, SGF_FILENAME).decode() for row in rows)\n    processed = [os.path.splitext(os.path.basename(r))[0] for r in names]\n    return processed",
        "rewrite": "\n\n\ndef read_existing_paths(bt_table):\n    rows = bt_table.read_rows(\n        filter_=row_filters.ColumnRangeFilter(\n            METADATA, SGF_FILENAME, SGF_FILENAME))\n    return [os.path.splitext(os.path.basename(row.cell_value(METADATA, SGF_FILENAME).decode()))[0] for row in rows]\n"
    },
    {
        "original": "def search(self, ngram):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tngram str or unicode, string to search for\n\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame, {'texts': <matching texts>, 'categories': <corresponding categories>}\n\n\t\t\"\"\"\n\t\tmask = self._document_index_mask(ngram)\n\t\treturn pd.DataFrame({\n\t\t\t'text': self.get_texts()[mask],\n\t\t\t'category': [self._category_idx_store.getval(idx)\n\t\t\t             for idx in self._y[mask]]\n\t\t})",
        "rewrite": "\n\n\ndef search(self, ngram):\n    mask = self._document_index_mask(ngram)\n    texts = self.get_texts()[mask]\n    categories = [self._category_idx_store.getval(idx) for idx in self._y[mask]]\n    return pd.DataFrame({'texts': texts, 'categories': categories})\n"
    },
    {
        "original": "def delete(name):\n    \"\"\"\n    Delete the namespace from the register\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        deletens:\n          reg.delete:\n            - name: myregister\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': True}\n    if name in __reg__:\n        del __reg__[name]\n    return ret",
        "rewrite": "\n\n\ndef delete(name):\n    ret = {'name': name, 'changes': {}, 'comment': '', 'result': True}\n    if name in __reg__:\n        del __reg__[name]\n    return ret\n"
    },
    {
        "original": "def getfield(self, pkt, s):\n        \"\"\"\n        We try to compute a length, usually from a msglen parsed earlier.\n        If this length is 0, we consider 'selection_present' (from RFC 5246)\n        to be False. This means that there should not be any length field.\n        However, with TLS 1.3, zero lengths are always explicit.\n        \"\"\"\n        ext = pkt.get_field(self.length_of)\n        tmp_len = ext.length_from(pkt)\n        if tmp_len is None or tmp_len <= 0:\n            v = pkt.tls_session.tls_version\n            if v is None or v < 0x0304:\n                return s, None\n        return super(_ExtensionsLenField, self).getfield(pkt, s)",
        "rewrite": "\n\n\ndef getfield(self, pkt, s):\n    ext = pkt.get_field(self.length_of)\n    tmp_len = ext.length_from(pkt)\n    if tmp_len is None or tmp_len <= 0:\n        if pkt.tls_session.tls_version is None or pkt.tls_session.tls_version < 0x0304:\n            return s, None\n    return super(_ExtensionsLenField, self).getfield(pkt, s)\n"
    },
    {
        "original": "def archive(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the Archive class. Registers the decorated class\n    as the Archive known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _archive_resource_type\n    _archive_resource_type = class_obj\n    return class_obj",
        "rewrite": "\n\n\ndef archive(class_obj: type) -> type:\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    archive._archive_resource_type = class_obj\n    return class_obj\narchive._archive_resource_type = None\n"
    },
    {
        "original": "def form_query(query_type, query):\n    \"\"\"\n    Returns a multi match query\n    \"\"\"\n    fields = [\n        field + \"^\" + str(SEARCH_BOOSTS[field]) if field in SEARCH_BOOSTS else field\n        for field in SEARCH_FIELDS\n    ]\n    return Q(\"multi_match\", fields=fields, query=query, type=query_type)",
        "rewrite": "\n\n\ndef form_query(query_type, query):\n    fields = [f\"{field}^{SEARCH_BOOSTS.get(field, 1)}\" for field in SEARCH_FIELDS]\n    return Q(\"multi_match\", fields=fields, query=query, type=query_type)\n"
    },
    {
        "original": "def get_dependants(self, run):\n        \"\"\"\n        Return a list of nodes that are control dependent on the given node in the control dependence graph\n        \"\"\"\n        if run in self._graph.nodes():\n            return list(self._graph.successors(run))\n        else:\n            return []",
        "rewrite": "\ndef get_dependants(self, run):\n    return list(self._graph.successors(run)) if run in self._graph.nodes() else []\n"
    },
    {
        "original": "def _ExtractClientIdFromPath(entry, event):\n  \"\"\"Extracts a Client ID from an APIAuditEntry's HTTP request path.\"\"\"\n  match = re.match(r\".*(C\\.[0-9a-fA-F]{16}).*\", entry.http_request_path)\n  if match:\n    event.client = match.group(1)",
        "rewrite": "\n\n\nimport re\n\ndef extract_client_id_from_path(entry, event):\n    pattern = r\"C\\.[0-9a-fA-F]{16}\"\n    match = re.search(pattern, entry.http_request_path)\n    if match:\n        event.client = match.group(0)\n"
    },
    {
        "original": "def add_override(self, partname, content_type):\n        \"\"\"\n        Add a child ``<Override>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        override = CT_Override.new(partname, content_type)\n        self.append(override)",
        "rewrite": "\n\n\ndef add_override(self, partname, content_type):\n    override = CT_Override.new(partname, content_type)\n    self.append(override)\n"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)",
        "rewrite": "\n\n\ndef _post_master_init(self, master):\n    mp_call = _metaproxy_call(self.opts, 'post_master_init')\n    return mp_call(self, master)\n"
    },
    {
        "original": "def Kdiag(self, X, target):\n        \"\"\"Compute the diagonal of the covariance matrix associated to X.\"\"\"\n        ly=1/self.lengthscaleY\n        lu=np.sqrt(3)/self.lengthscaleU\n        #ly=self.lengthscaleY\n        #lu=self.lengthscaleU\n        \n        k1 = (2*lu+ly)/(lu+ly)**2\n        k2 = (ly-2*lu + 2*lu-ly ) / (ly-lu)**2 \n        k3 = 1/(lu+ly) + (lu)/(lu+ly)**2 \n\n        np.add(self.varianceU*self.varianceY*(k1+k2+k3), target, target)",
        "rewrite": "\n\n\ndef Kdiag(self, X, target):\n    ly = 1 / self.lengthscaleY\n    lu = np.sqrt(3) / self.lengthscaleU\n\n    k1 = (2*lu + ly) / (lu + ly)**2\n    k2 = (ly - 2*lu) / (ly - lu)**2\n    k3 = 1 / (lu + ly) + lu / (lu + ly)**2\n\n    np.add(self.varianceU * self.varianceY * (k1 + k2 + k3), target,"
    },
    {
        "original": "def make_deprecated_class(oldname, NewClass):\n    \"\"\"\n    Returns a class that raises NotImplementedError on instantiation.\n    e.g.:\n    >>> Kern = make_deprecated_class(\"Kern\", Kernel)\n    \"\"\"\n    msg = (\"{module}.{} has been renamed to {module}.{}\"\n           .format(oldname, NewClass.__name__, module=NewClass.__module__))\n\n    class OldClass(NewClass):\n        def __new__(cls, *args, **kwargs):\n            raise NotImplementedError(msg)\n    OldClass.__doc__ = msg\n    OldClass.__qualname__ = OldClass.__name__ = oldname\n    return OldClass",
        "rewrite": "\n\n\ndef make_deprecated_class(oldname, NewClass):\n    msg = f\"{NewClass.__module__}.{oldname} has been renamed to {NewClass.__module__}.{NewClass.__name__}\"\n    class OldClass(NewClass):\n        def __new__(cls, *args, **kwargs):\n            raise NotImplementedError(msg)\n    OldClass.__doc__ = msg\n    OldClass.__qualname__ = OldClass.__name__ = oldname\n    return OldClass\n"
    },
    {
        "original": "def estimate_row_means(\n            self,\n            X,\n            observed,\n            column_means,\n            column_scales):\n        \"\"\"\n        row_center[i] =\n        sum{j in observed[i, :]}{\n            (1 / column_scale[j]) * (X[i, j] - column_center[j])\n        }\n        ------------------------------------------------------------\n        sum{j in observed[i, :]}{1 / column_scale[j]}\n        \"\"\"\n\n        n_rows, n_cols = X.shape\n\n        column_means = np.asarray(column_means)\n        if len(column_means) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_means.shape))\n        X = X - column_means.reshape((1, n_cols))\n        column_weights = 1.0 / column_scales\n        X *= column_weights.reshape((1, n_cols))\n        row_means = np.zeros(n_rows, dtype=X.dtype)\n        row_residual_sums = np.nansum(X, axis=1)\n        for i in range(n_rows):\n            row_mask = observed[i, :]\n            sum_weights = column_weights[row_mask].sum()\n            row_means[i] = row_residual_sums[i] / sum_weights\n        return row_means",
        "rewrite": "\n\n\ndef estimate_row_means(self, X, observed, column_means, column_scales):\n    n_rows, n_cols = X.shape\n    column_means = np.asarray(column_means)\n    if len(column_means) != n_cols:\n        raise ValueError(\"Expected length %d but got shape %s\" % (n_cols, column_means.shape))\n    X_centered = X - column_means.reshape((1, n_cols))\n    column_weights = 1.0 / column_scales\n    X_weighted = X_centered * column_weights.reshape((1, n_cols))\n    row_residual_s"
    },
    {
        "original": "def ReadHuntOutputPluginLogEntries(self,\n                                     hunt_id,\n                                     output_plugin_id,\n                                     offset,\n                                     count,\n                                     with_type=None,\n                                     cursor=None):\n    \"\"\"Reads hunt output plugin log entries.\"\"\"\n    query = (\"SELECT client_id, flow_id, log_entry_type, message, \"\n             \"UNIX_TIMESTAMP(timestamp) \"\n             \"FROM flow_output_plugin_log_entries \"\n             \"FORCE INDEX (flow_output_plugin_log_entries_by_hunt) \"\n             \"WHERE hunt_id = %s AND output_plugin_id = %s \")\n    args = [\n        db_utils.HuntIDToInt(hunt_id),\n        db_utils.OutputPluginIDToInt(output_plugin_id)\n    ]\n\n    if with_type is not None:\n      query += \"AND log_entry_type = %s \"\n      args.append(int(with_type))\n\n    query += \"ORDER BY log_id ASC LIMIT %s OFFSET %s\"\n    args.append(count)\n    args.append(offset)\n\n    cursor.execute(query, args)\n\n    ret = []\n    for (client_id_int, flow_id_int, log_entry_type, message,\n         timestamp) in cursor.fetchall():\n      ret.append(\n          rdf_flow_objects.FlowOutputPluginLogEntry(\n              hunt_id=hunt_id,\n              client_id=db_utils.IntToClientID(client_id_int),\n              flow_id=db_utils.IntToFlowID(flow_id_int),\n              output_plugin_id=output_plugin_id,\n              log_entry_type=log_entry_type,\n              message=message,\n              timestamp=mysql_utils.TimestampToRDFDatetime(timestamp)))\n\n    return ret",
        "rewrite": "\n\n\ndef ReadHuntOutputPluginLogEntries(self, hunt_id, output_plugin_id, offset, count, with_type=None, cursor=None):\n    query = \"\"\"\n        SELECT client_id, flow_id, log_entry_type, message, UNIX_TIMESTAMP(timestamp)\n        FROM flow_output_plugin_log_entries\n        FORCE INDEX (flow_output_plugin_log_entries_by_hunt)\n        WHERE hunt_id = %s AND output_plugin_id = %s\n    \"\"\"\n    args = [db_utils.HuntIDToInt(hunt_id), db_utils.OutputPluginIdToInt(output_plugin_id)]\n\n    if with_type is not None:\n"
    },
    {
        "original": "def _condensation_lil(self):\n        \"\"\"\n        Return the sparse matrix representation of the condensation digraph\n        in lil format.\n\n        \"\"\"\n        condensation_lil = sparse.lil_matrix(\n            (self.num_strongly_connected_components,\n             self.num_strongly_connected_components), dtype=bool\n        )\n\n        scc_proj = self.scc_proj\n        for node_from, node_to in _csr_matrix_indices(self.csgraph):\n            scc_from, scc_to = scc_proj[node_from], scc_proj[node_to]\n            if scc_from != scc_to:\n                condensation_lil[scc_from, scc_to] = True\n\n        return condensation_lil",
        "rewrite": "\n\n\ndef _condensation_lil(self):\n    condensation_lil = sparse.lil_matrix((self.num_strongly_connected_components, self.num_strongly_connected_components), dtype=bool)\n    scc_proj = self.scc_proj\n    for node_from, node_to in zip(self.csgraph.row, self.csgraph.col):\n        scc_from, scc_to = scc_proj[node_from], scc_proj[node_to]\n        if scc_from != scc_to:\n            condensation_lil[scc_from, scc_to] = True\n    return condensation_lil"
    },
    {
        "original": "def build_modules(is_training, vocab_size):\n  \"\"\"Construct the modules used in the graph.\"\"\"\n\n  # Construct the custom getter which implements Bayes by Backprop.\n  if is_training:\n    estimator_mode = tf.constant(bbb.EstimatorModes.sample)\n  else:\n    estimator_mode = tf.constant(bbb.EstimatorModes.mean)\n  lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n  non_lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=non_lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n\n  embed_layer = snt.Embed(\n      vocab_size=vocab_size,\n      embed_dim=FLAGS.embedding_size,\n      custom_getter=non_lstm_bbb_custom_getter,\n      name=\"input_embedding\")\n\n  cores = []\n  for i in range(FLAGS.n_layers):\n    cores.append(\n        snt.LSTM(FLAGS.hidden_size,\n                 custom_getter=lstm_bbb_custom_getter,\n                 forget_bias=0.0,\n                 name=\"lstm_layer_{}\".format(i)))\n\n  rnn_core = snt.DeepRNN(\n      cores,\n      skip_connections=False,\n      name=\"deep_lstm_core\")\n\n  # Do BBB on weights but not biases of output layer.\n  output_linear = snt.Linear(\n      vocab_size, custom_getter={\"w\": non_lstm_bbb_custom_getter})\n  return embed_layer, rnn_core, output_linear",
        "rewrite": "\n\n\ndef build_modules(is_training, vocab_size):\n  estimator_mode = tf.constant(bbb.EstimatorModes.sample if is_training else bbb.EstimatorModes.mean)\n\n  lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n\n  non_lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=non_lstm_posterior_builder"
    },
    {
        "original": "def run(self):\n        \"\"\"\n        Run the master service!\n        \"\"\"\n        self.utils = salt.loader.utils(self.opts, proxy=self.proxy)\n        if salt.utils.platform.is_windows():\n            # Calculate function references since they can't be pickled.\n            if self.opts['__role'] == 'master':\n                self.runners = salt.loader.runner(self.opts, utils=self.utils)\n            else:\n                self.runners = []\n            self.funcs = salt.loader.minion_mods(self.opts, utils=self.utils, proxy=self.proxy)\n\n        self.engine = salt.loader.engines(self.opts,\n                                          self.funcs,\n                                          self.runners,\n                                          self.utils,\n                                          proxy=self.proxy)\n        kwargs = self.config or {}\n        try:\n            self.engine[self.fun](**kwargs)\n        except Exception as exc:\n            log.critical(\n                'Engine \\'%s\\' could not be started!',\n                self.fun.split('.')[0], exc_info=True\n            )",
        "rewrite": "\n\n\ndef run(self):\n    self.utils = salt.loader.utils(self.opts, proxy=self.proxy)\n    if salt.utils.platform.is_windows():\n        if self.opts['__role'] == 'master':\n            self.runners = salt.loader.runner(self.opts, utils=self.utils)\n        else:\n            self.runners = []\n        self.funcs = salt.loader.minion_mods(self.opts, utils=self.utils, proxy=self.proxy)\n    self.engine = salt.loader.engines(self.opts, self.funcs, self.runners, self.utils, proxy=self.proxy)\n    kwargs = self.config or {}\n    try:\n        self.engine[self"
    },
    {
        "original": "def get_statuses(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/statuses/:ref <http://developer.github.com/v3/repos/statuses>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.CommitStatus.CommitStatus`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.CommitStatus.CommitStatus,\n            self._requester,\n            self._parentUrl(self._parentUrl(self.url)) + \"/statuses/\" + self.sha,\n            None\n        )",
        "rewrite": "\n\n\ndef get_statuses(self):\n    return github.PaginatedList(\n        github.CommitStatus,\n        self._requester,\n        f\"{self._parentUrl(self._parentUrl(self.url))}/statuses/{self.sha}\",\n        None\n    )\n"
    },
    {
        "original": "def get_vm_ip(name=None, session=None, call=None):\n    \"\"\"\n    Get the IP address of the VM\n\n    .. code-block:: bash\n\n        salt-cloud -a get_vm_ip xenvm01\n\n    .. note:: Requires xen guest tools to be installed in VM\n\n    \"\"\"\n    if call == 'function':\n        raise SaltCloudException(\n            'This function must be called with -a or --action.'\n        )\n    if session is None:\n        log.debug('New session being created')\n        session = _get_session()\n    vm = _get_vm(name, session=session)\n    ret = None\n    # -- try to get ip from vif\n    vifs = session.xenapi.VM.get_VIFs(vm)\n    if vifs is not None:\n        for vif in vifs:\n            if session.xenapi.VIF.get_ipv4_addresses(vif):\n                cidr = session.xenapi.VIF.get_ipv4_addresses(vif).pop()\n                ret, subnet = cidr.split('/')\n                log.debug(\n                    'VM vif returned for instance: %s ip: %s', name, ret)\n                return ret\n    # -- try to get ip from get tools metrics\n    vgm = session.xenapi.VM.get_guest_metrics(vm)\n    try:\n        net = session.xenapi.VM_guest_metrics.get_networks(vgm)\n        if \"0/ip\" in net.keys():\n            log.debug(\n                'VM guest metrics returned for instance: %s 0/ip: %s',\n                name, net[\"0/ip\"]\n            )\n            ret = net[\"0/ip\"]\n    # except Exception as ex:\n    except XenAPI.Failure:\n        log.info('Could not get vm metrics at this time')\n    return ret",
        "rewrite": "\n\n\ndef get_vm_ip(name=None, session=None, call=None):\n    if call == 'function':\n        raise SaltCloudException('This function must be called with -a or --action.')\n    if session is None:\n        log.debug('New session being created')\n        session = _get_session()\n    vm = _get_vm(name, session=session)\n    ret = None\n    vifs = session.xenapi.VM.get_VIFs(vm)\n    if vifs is not None:\n        for vif in vifs:\n            if session.xenapi.VIF.get_ipv4_addresses"
    },
    {
        "original": "def as_dataset(obj):\n    \"\"\"Cast the given object to a Dataset.\n\n    Handles Datasets, DataArrays and dictionaries of variables. A new Dataset\n    object is only created if the provided object is not already one.\n    \"\"\"\n    if hasattr(obj, 'to_dataset'):\n        obj = obj.to_dataset()\n    if not isinstance(obj, Dataset):\n        obj = Dataset(obj)\n    return obj",
        "rewrite": "\n\n\ndef as_dataset(obj):\n    return obj.to_dataset() if hasattr(obj, 'to_dataset') else Dataset(obj) if not isinstance(obj, Dataset) else obj\n"
    },
    {
        "original": "def str2float(text):\n    \"\"\"\n    Remove uncertainty brackets from strings and return the float.\n    \"\"\"\n\n    try:\n        # Note that the ending ) is sometimes missing. That is why the code has\n        # been modified to treat it as optional. Same logic applies to lists.\n        return float(re.sub(r\"\\(.+\\)*\", \"\", text))\n    except TypeError:\n        if isinstance(text, list) and len(text) == 1:\n            return float(re.sub(r\"\\(.+\\)*\", \"\", text[0]))\n    except ValueError as ex:\n        if text.strip() == \".\":\n            return 0\n        raise ex",
        "rewrite": "\n\n\nimport re\n\ndef str2float(text):\n    if isinstance(text, list) and len(text) == 1:\n        text = text[0]\n    text = re.sub(r\"\\(.+\\)*\", \"\", text)\n    if text.strip() == \".\":\n        return 0\n    return float(text)\n"
    },
    {
        "original": "def installed(name, channel=None):\n    \"\"\"\n    Ensure that the named snap package is installed\n\n    name\n        The snap package\n\n    channel\n        Optional. The channel to install the package from.\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'pchanges': {},\n           'result': None,\n           'comment': ''}\n\n    old = __salt__['snap.versions_installed'](name)\n    if not old:\n        if __opts__['test']:\n            ret['comment'] = 'Package \"{0}\" would have been installed'.format(name)\n            ret['pchanges']['new'] = name\n            ret['pchanges']['old'] = None\n            ret['result'] = None\n            return ret\n\n        install = __salt__['snap.install'](name, channel=channel)\n        if install['result']:\n            ret['comment'] = 'Package \"{0}\" was installed'.format(name)\n            ret['changes']['new'] = name\n            ret['changes']['old'] = None\n            ret['result'] = True\n            return ret\n\n        ret['comment'] = 'Package \"{0}\" failed to install'.format(name)\n        ret['comment'] += '\\noutput:\\n' + install['output']\n        ret['result'] = False\n        return ret\n\n    # Currently snap always returns only one line?\n    old_channel = old[0]['tracking']\n    if old_channel != channel and channel is not None:\n        if __opts__['test']:\n            ret['comment'] = 'Package \"{0}\" would have been switched to channel {1}'.format(name, channel)\n            ret['pchanges']['old_channel'] = old_channel\n            ret['pchanges']['new_channel'] = channel\n            ret['result'] = None\n            return ret\n\n        refresh = __salt__['snap.install'](name, channel=channel, refresh=True)\n        if refresh['result']:\n            ret['comment'] = 'Package \"{0}\" was switched to channel {1}'.format(name, channel)\n            ret['pchanges']['old_channel'] = old_channel\n            ret['pchanges']['new_channel'] = channel\n            ret['result'] = True\n            return ret\n\n        ret['comment'] = 'Failed to switch Package \"{0}\" to channel {1}'.format(name, channel)\n        ret['comment'] += '\\noutput:\\n' + install['output']\n        ret['result'] = False\n        return ret\n\n    ret['comment'] = 'Package \"{0}\" is already installed'.format(name)\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n\n    ret['result'] = True\n    return ret",
        "rewrite": "\n\n\ndef installed(name, channel=None):\n    ret = {'name': name, 'changes': {}, 'pchanges': {}, 'result': None, 'comment': ''}\n\n    old = __salt__['snap.versions_installed'](name)\n    if not old:\n        if __opts__['test']:\n            ret['comment'] = 'Package \"{0}\" would have been installed'.format(name)\n            ret['pchanges']['new'] = name\n            ret['pchanges']['old'] = None\n            ret['result'] = None\n        else:\n            install = __salt__['"
    },
    {
        "original": "async def get_sender(self):\n        \"\"\"\n        Returns `sender`, but will make an API call to find the\n        sender unless it's already cached.\n        \"\"\"\n        # ``sender.min`` is present both in :tl:`User` and :tl:`Channel`.\n        # It's a flag that will be set if only minimal information is\n        # available (such as display name, but username may be missing),\n        # in which case we want to force fetch the entire thing because\n        # the user explicitly called a method. If the user is okay with\n        # cached information, they may use the property instead.\n        if (self._sender is None or self._sender.min) \\\n                and await self.get_input_sender():\n            try:\n                self._sender =\\\n                    await self._client.get_entity(self._input_sender)\n            except ValueError:\n                await self._reload_message()\n        return self._sender",
        "rewrite": "\n\n\nasync def get_sender(self):\n    if self._sender is None or self._sender.min:\n        if await self.get_input_sender():\n            try:\n                self._sender = await self._client.get_entity(self._input_sender)\n            except ValueError:\n                await self._reload_message()\n    return self._sender\n"
    },
    {
        "original": "def from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True, manager=None):\n        \"\"\"\n        Construct a `GKKPWork` from a `PhononWfkqWork` object.\n        The WFQ are the ones used for PhononWfkqWork so in principle have only valence bands\n        \"\"\"\n        # Get list of qpoints from the the phonon tasks in this work\n        qpoints = []\n        qpoints_deps = []\n        for task in phononwfkq_work:\n            if isinstance(task,PhononTask):\n                # Store qpoints\n                qpt = task.input.get(\"qpt\", [0,0,0])\n                qpoints.append(qpt)\n                # Store dependencies\n                qpoints_deps.append(task.deps)\n\n        # Create file nodes\n        ddb_path  = phononwfkq_work.outdir.has_abiext(\"DDB\")\n        dvdb_path = phononwfkq_work.outdir.has_abiext(\"DVDB\")\n        ddb_file = FileNode(ddb_path)\n        dvdb_file = FileNode(dvdb_path)\n\n        # Get scf_task from first q-point\n        for dep in qpoints_deps[0]:\n            if isinstance(dep.node,ScfTask) and dep.exts[0] == 'WFK':\n                scf_task = dep.node\n\n        # Create new work\n        new = cls(manager=manager)\n        new.remove_wfkq = remove_wfkq\n        new.wfkq_tasks = []\n        new.wfk_task = []\n\n        # Add one eph task per qpoint\n        for qpt,qpoint_deps in zip(qpoints,qpoints_deps):\n            # Create eph task\n            eph_input = scf_task.input.new_with_vars(optdriver=7, prtphdos=0, eph_task=-2,\n                                                     ddb_ngqpt=[1,1,1], nqpt=1, qpt=qpt)\n            deps = {ddb_file: \"DDB\", dvdb_file: \"DVDB\" }\n            for dep in qpoint_deps:\n                deps[dep.node] = dep.exts[0]\n            # If no WFQ in deps link the WFK with WFQ extension\n            if 'WFQ' not in deps.values():\n                inv_deps = dict((v, k) for k, v in deps.items())\n                wfk_task = inv_deps['WFK']\n                wfk_path = wfk_task.outdir.has_abiext(\"WFK\")\n                # Check if netcdf\n                filename, extension = os.path.splitext(wfk_path)\n                infile = 'out_WFQ' + extension\n                wfq_path = os.path.join(os.path.dirname(wfk_path), infile)\n                if not os.path.isfile(wfq_path): os.symlink(wfk_path, wfq_path)\n                deps[FileNode(wfq_path)] = 'WFQ'\n            new.register_eph_task(eph_input, deps=deps)\n\n        return new",
        "rewrite": "\n\n\ndef from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True, manager=None):\n    qpoints = []\n    qpoints_deps = []\n    for task in phononwfkq_work:\n        if isinstance(task, PhononTask):\n            qpt = task.input.get(\"qpt\", [0,0,0])\n            qpoints.append(qpt)\n            qpoints_deps.append(task.deps)\n\n    ddb_path = phononwfkq_work.outdir.has_abiext(\""
    },
    {
        "original": "def get_function_name(s):\n    \"\"\"\n    Get the function name from a C-style function declaration string.\n\n    :param str s: A C-style function declaration string.\n    :return:      The function name.\n    :rtype:       str\n    \"\"\"\n\n    s = s.strip()\n    if s.startswith(\"__attribute__\"):\n        # Remove \"__attribute__ ((foobar))\"\n        if \"))\" not in s:\n            raise ValueError(\"__attribute__ is present, but I cannot find double-right parenthesis in the function \"\n                             \"declaration string.\")\n\n        s = s[s.index(\"))\") + 2 : ].strip()\n\n    if '(' not in s:\n        raise ValueError(\"Cannot find any left parenthesis in the function declaration string.\")\n\n    func_name = s[:s.index('(')].strip()\n\n    for i, ch in enumerate(reversed(func_name)):\n        if ch == ' ':\n            pos = len(func_name) - 1 - i\n            break\n    else:\n        raise ValueError('Cannot find any space in the function declaration string.')\n\n    func_name = func_name[pos + 1 : ]\n    return func_name",
        "rewrite": "\n\n\ndef get_function_name(s):\n    s = s.strip()\n    if s.startswith(\"__attribute__\"):\n        s = s[s.index(\"))\") + 2:].strip()\n    if '(' not in s:\n        raise ValueError(\"Cannot find any left parenthesis in the function declaration string.\")\n    func_name = s[:s.index('(')].strip()\n    pos = len(func_name) - 1 - func_name[::-1].index(' ')\n    return func_name[pos + 1:]\n"
    },
    {
        "original": "def ReadAPIAuditEntries(self,\n                          username=None,\n                          router_method_names=None,\n                          min_timestamp=None,\n                          max_timestamp=None):\n    \"\"\"Returns audit entries stored in the database.\"\"\"\n    results = []\n\n    for entry in self.api_audit_entries:\n      if username is not None and entry.username != username:\n        continue\n\n      if (router_method_names and\n          entry.router_method_name not in router_method_names):\n        continue\n\n      if min_timestamp is not None and entry.timestamp < min_timestamp:\n        continue\n\n      if max_timestamp is not None and entry.timestamp > max_timestamp:\n        continue\n\n      results.append(entry)\n\n    return sorted(results, key=lambda entry: entry.timestamp)",
        "rewrite": "\n\n\ndef ReadAPIAuditEntries(self, username=None, router_method_names=None, min_timestamp=None, max_timestamp=None):\n    results = [entry for entry in self.api_audit_entries \n               if (username is None or entry.username == username) \n               and (router_method_names is None or entry.router_method_name in router_method_names) \n               and (min_timestamp is None or entry.timestamp >= min_timestamp) \n               and (max_timestamp is None or entry.timestamp <= max_timestamp)]\n    return sorted(results, key=lambda entry: entry.timestamp)\n"
    },
    {
        "original": "def from_stream(cls, stream, marker_code, offset):\n        \"\"\"\n        Return an |_SofMarker| instance for the SOFn marker at *offset* in\n        stream.\n        \"\"\"\n        # field                 off  type   notes\n        # ------------------  ---  -----  ----------------------------\n        # segment length       0   short\n        # Data precision       2   byte\n        # Vertical lines       3   short  px_height\n        # Horizontal lines     5   short  px_width\n        # ------------------  ---  -----  ----------------------------\n        segment_length = stream.read_short(offset)\n        px_height = stream.read_short(offset, 3)\n        px_width = stream.read_short(offset, 5)\n        return cls(marker_code, offset, segment_length, px_width, px_height)",
        "rewrite": "\n\n\ndef from_stream(cls, stream, marker_code, offset):\n    segment_length = stream.read_short(offset)\n    data_precision = stream.read_byte(offset + 2)\n    px_height = stream.read_short(offset + 3)\n    px_width = stream.read_short(offset + 5)\n    return cls(marker_code, offset, segment_length, px_width, px_height)\n"
    },
    {
        "original": "def normalize(rendered):\n    \"\"\"Return the input string without non-functional spaces or newlines.\"\"\"\n    out = ''.join([line.strip()\n                   for line in rendered.splitlines()\n                   if line.strip()])\n    out = out.replace(', ', ',')\n    return out",
        "rewrite": "\ndef normalize(rendered):\n    return ', '.join(line.strip() for line in rendered.splitlines() if line.strip()).replace(', ,', ',')\n"
    },
    {
        "original": "def hash_and_stat_file(self, path, saltenv='base'):\n        \"\"\"\n        Return the hash of a file, to get the hash of a file in the pillar_roots\n        prepend the path with salt://<file on server> otherwise, prepend the\n        file with / for a local file.\n\n        Additionally, return the stat result of the file, or None if no stat\n        results were found.\n        \"\"\"\n        ret = {}\n        fnd = self.__get_file_path(path, saltenv)\n        if fnd is None:\n            return ret, None\n\n        try:\n            # Remote file path (self._find_file() invoked)\n            fnd_path = fnd['path']\n            fnd_stat = fnd.get('stat')\n        except TypeError:\n            # Local file path\n            fnd_path = fnd\n            try:\n                fnd_stat = list(os.stat(fnd_path))\n            except Exception:\n                fnd_stat = None\n\n        hash_type = self.opts.get('hash_type', 'md5')\n        ret['hsum'] = salt.utils.hashutils.get_hash(fnd_path, form=hash_type)\n        ret['hash_type'] = hash_type\n        return ret, fnd_stat",
        "rewrite": "\n\n\ndef hash_and_stat_file(self, path, saltenv='base'):\n    ret = {}\n    fnd = self.__get_file_path(path, saltenv)\n    if fnd is None:\n        return ret, None\n\n    if isinstance(fnd, dict):\n        fnd_path = fnd['path']\n        fnd_stat = fnd.get('stat')\n    else:\n        fnd_path = fnd\n        try:\n            fnd_stat = list(os.stat(fnd_path))\n        except Exception:\n            fnd_stat = None\n\n    hash_type = self.opts.get"
    },
    {
        "original": "def prepare_env(self):\n        \"\"\"\n        Manages reading environment metadata files under ``private_data_dir`` and merging/updating\n        with existing values so the :py:class:`ansible_runner.runner.Runner` object can read and use them easily\n        \"\"\"\n        try:\n            passwords = self.loader.load_file('env/passwords', Mapping)\n            self.expect_passwords = {\n                re.compile(pattern, re.M): password\n                for pattern, password in iteritems(passwords)\n            }\n        except ConfigurationError:\n            output.debug('Not loading passwords')\n            self.expect_passwords = dict()\n        self.expect_passwords[pexpect.TIMEOUT] = None\n        self.expect_passwords[pexpect.EOF] = None\n\n        try:\n            # seed env with existing shell env\n            self.env = os.environ.copy()\n            envvars = self.loader.load_file('env/envvars', Mapping)\n            if envvars:\n                self.env.update({k:six.text_type(v) for k, v in envvars.items()})\n            if self.envvars and isinstance(self.envvars, dict):\n                self.env.update({k:six.text_type(v) for k, v in self.envvars.items()})\n        except ConfigurationError:\n            output.debug(\"Not loading environment vars\")\n            # Still need to pass default environment to pexpect\n            self.env = os.environ.copy()\n\n        try:\n            self.settings = self.loader.load_file('env/settings', Mapping)\n        except ConfigurationError:\n            output.debug(\"Not loading settings\")\n            self.settings = dict()\n\n        try:\n            self.ssh_key_data = self.loader.load_file('env/ssh_key', string_types)\n        except ConfigurationError:\n            output.debug(\"Not loading ssh key\")\n            self.ssh_key_data = None\n\n        self.idle_timeout = self.settings.get('idle_timeout', None)\n        self.job_timeout = self.settings.get('job_timeout', None)\n        self.pexpect_timeout = self.settings.get('pexpect_timeout', 5)\n\n        self.process_isolation = self.settings.get('process_isolation', self.process_isolation)\n        self.process_isolation_executable = self.settings.get('process_isolation_executable', self.process_isolation_executable)\n        self.process_isolation_path = self.settings.get('process_isolation_path', self.process_isolation_path)\n        self.process_isolation_hide_paths = self.settings.get('process_isolation_hide_paths', self.process_isolation_hide_paths)\n        self.process_isolation_show_paths = self.settings.get('process_isolation_show_paths', self.process_isolation_show_paths)\n        self.process_isolation_ro_paths = self.settings.get('process_isolation_ro_paths', self.process_isolation_ro_paths)\n\n        self.pexpect_use_poll = self.settings.get('pexpect_use_poll', True)\n        self.suppress_ansible_output = self.settings.get('suppress_ansible_output', self.quiet)\n        self.directory_isolation_cleanup = bool(self.settings.get('directory_isolation_cleanup', True))\n\n        if 'AD_HOC_COMMAND_ID' in self.env or not os.path.exists(self.project_dir):\n            self.cwd = self.private_data_dir\n        else:\n            if self.directory_isolation_path is not None:\n                self.cwd = self.directory_isolation_path\n            else:\n                self.cwd = self.project_dir\n\n        if 'fact_cache' in self.settings:\n            if 'fact_cache_type' in self.settings:\n                if self.settings['fact_cache_type'] == 'jsonfile':\n                    self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])\n            else:\n                self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])",
        "rewrite": "\n\n\ndef prepare_env(self):\n    self.expect_passwords = {}\n    try:\n        passwords = self.loader.load_file('env/passwords', Mapping)\n        self.expect_passwords = {re.compile(pattern, re.M): password for pattern, password in iteritems(passwords)}\n    except ConfigurationError:\n        output.debug('Not loading passwords')\n    self.expect_passwords[pexpect.TIMEOUT] = None\n    self.expect_passwords[pexpect.EOF] = None\n\n    try:\n        self.env = os.environ.copy()\n        envvars = self.loader.load_file('env/envvars', Mapping)\n        if"
    },
    {
        "original": "def setbit(self, name, offset, value):\n        \"\"\"\n        Flag the ``offset`` in ``name`` as ``value``. Returns a boolean\n        indicating the previous value of ``offset``.\n        \"\"\"\n        value = value and 1 or 0\n        return self.execute_command('SETBIT', name, offset, value)",
        "rewrite": "\n\n\ndef setbit(self, name, offset, value):\n    return self.execute_command('SETBIT', name, offset, 1 if value else 0)\n"
    },
    {
        "original": "def _checkValueItemParent(policy_element, policy_name, policy_key,\n                          policy_valueName, xpath_object, policy_file_data,\n                          check_deleted=False, test_item=True):\n    \"\"\"\n    helper function to process the parent of a value item object\n    if test_item is True, it will determine if the policy is enabled/disabled\n    returns True if the value is configured in the registry.pol file, otherwise returns False\n\n    if test_item is False, the expected search string will be returned\n\n    value type parents:\n        boolean: https://msdn.microsoft.com/en-us/library/dn606009(v=vs.85).aspx\n        enabledValue: https://msdn.microsoft.com/en-us/library/dn606006(v=vs.85).aspx\n        disabledValue: https://msdn.microsoft.com/en-us/library/dn606001(v=vs.85).aspx\n\n    \"\"\"\n    for element in xpath_object(policy_element):\n        for value_item in element.getchildren():\n            search_string = _processValueItem(value_item,\n                                              policy_key,\n                                              policy_valueName,\n                                              policy_element,\n                                              element,\n                                              check_deleted=check_deleted)\n            if not test_item:\n                return search_string\n            if _regexSearchRegPolData(re.escape(search_string), policy_file_data):\n                log.debug('found the search string in the pol file, '\n                          '%s is configured', policy_name)\n                return True\n    return False",
        "rewrite": "\n\n\ndef _checkValueItemParent(policy_element, policy_name, policy_key, policy_valueName, xpath_object, policy_file_data, check_deleted=False, test_item=True):\n    for element in xpath_object(policy_element):\n        for value_item in element.getchildren():\n            search_string = _processValueItem(value_item, policy_key, policy_valueName, policy_element, element, check_deleted=check_deleted)\n            if not test_item:\n                return search_string\n            if _regexSearchRegPolData(re.escape(search_string), policy_file_data):\n                log.debug(f'found the search string in the"
    },
    {
        "original": "def RegisterCheck(cls, check, source=\"unknown\", overwrite_if_exists=False):\n    \"\"\"Adds a check to the registry, refresh the trigger to check map.\"\"\"\n    if not overwrite_if_exists and check.check_id in cls.checks:\n      raise DefinitionError(\n          \"Check named %s already exists and \"\n          \"overwrite_if_exists is set to False.\" % check.check_id)\n    check.loaded_from = source\n    cls.checks[check.check_id] = check\n    cls.triggers.Update(check.triggers, check)",
        "rewrite": "\n\n\ndef RegisterCheck(cls, check, source=\"unknown\", overwrite_if_exists=False):\n    if not overwrite_if_exists and check.check_id in cls.checks:\n        raise DefinitionError(f\"Check named {check.check_id} already exists and overwrite_if_exists is set to False.\")\n    check.loaded_from = source\n    cls.checks[check.check_id] = check\n    cls.triggers.Update(check.triggers, check)\n"
    },
    {
        "original": "def extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n        \"\"\"\n        get ICOHP/ICOOP of strongest bond\n        Args:\n            summed_spin_channels: Boolean to indicate whether the ICOHPs/ICOOPs of both spin channels should be summed\n\n            spin: if summed_spin_channels is equal to False, this spin indicates which spin channel should be returned\n        Returns:\n            lowest ICOHP/largest ICOOP value (i.e. ICOHP/ICOOP value of strongest bond)\n        \"\"\"\n        if not self._are_coops:\n            extremum = sys.float_info.max\n        else:\n            extremum = -sys.float_info.max\n\n        if not self._is_spin_polarized:\n            if spin == Spin.down:\n                warnings.warn(\"This spin channel does not exist. I am switching to Spin.up\")\n            spin = Spin.up\n\n        for value in self._icohplist.values():\n            if not value.is_spin_polarized or not summed_spin_channels:\n                if not self._are_coops:\n                    if value.icohpvalue(spin) < extremum:\n                        extremum = value.icohpvalue(spin)\n                        # print(extremum)\n                else:\n                    if value.icohpvalue(spin) > extremum:\n                        extremum = value.icohpvalue(spin)\n                        # print(extremum)\n            else:\n                if not self._are_coops:\n                    if value.summed_icohp < extremum:\n                        extremum = value.summed_icohp\n                        # print(extremum)\n                else:\n                    if value.summed_icohp > extremum:\n                        extremum = value.summed_icohp\n                        # print(extremum)\n        return extremum",
        "rewrite": "\n\n\ndef extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n    extremum = sys.float_info.max if not self._are_coops else -sys.float_info.max\n\n    if not self._is_spin_polarized:\n        spin = Spin.up\n\n    for value in self._icohplist.values():\n        if not value.is_spin_polarized or not summed_spin_channels:\n            ico = value.icohpvalue(spin)\n        else:\n            ico = value.summed_icohp\n\n        if not self._are_coops:\n           "
    },
    {
        "original": "def secgroup_list(self):\n        \"\"\"\n        List security groups\n        \"\"\"\n        nt_ks = self.compute_conn\n        ret = {}\n        for item in nt_ks.security_groups.list():\n            ret[item.name] = {\n                'name': item.name,\n                'description': item.description,\n                'id': item.id,\n                'tenant_id': item.tenant_id,\n                'rules': item.rules,\n            }\n        return ret",
        "rewrite": "\n\n\ndef secgroup_list(self):\n    nt_ks = self.compute_conn\n    return {item.name: {\n        'name': item.name,\n        'description': item.description,\n        'id': item.id,\n        'tenant_id': item.tenant_id,\n        'rules': item.rules,\n    } for item in nt_ks.security_groups.list()}\n"
    },
    {
        "original": "def get_following(self):\n        \"\"\"\n        :calls: `GET /users/:user/following <http://developer.github.com/v3/users/followers>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            NamedUser,\n            self._requester,\n            self.url + \"/following\",\n            None\n        )",
        "rewrite": "\n\n\ndef get_following(self):\n    return github.PaginatedList(\n        github.NamedUser,\n        self._requester,\n        f\"{self.url}/following\",\n        None\n    )\n"
    },
    {
        "original": "def detect_model_num(string):\n    \"\"\"Takes a string related to a model name and extract its model number.\n\n    For example:\n        '000000-bootstrap.index' => 0\n    \"\"\"\n    match = re.match(MODEL_NUM_REGEX, string)\n    if match:\n        return int(match.group())\n    return None",
        "rewrite": "\n\n\nimport re\n\nMODEL_NUM_REGEX = r'^(\\d+)'\n\ndef detect_model_num(string):\n    match = re.match(MODEL_NUM_REGEX, string)\n    return int(match.group(1)) if match else None\n"
    },
    {
        "original": "def onColorPicker(self):\n        \"\"\"\n        Show color-picker dialog to select color.\n\n        Qt will use the native dialog by default.\n\n        \"\"\"\n        dlg = QtGui.QColorDialog(QtGui.QColor(self._color), None)\n\n        # if self._color:\n        #    dlg.setCurrentColor(QtGui.QColor(self._color))\n\n        if dlg.exec_():\n            self.setColor(dlg.currentColor().name())",
        "rewrite": "\n\n\ndef onColorPicker(self):\n    dlg = QtGui.QColorDialog(QtGui.QColor(self._color))\n    dlg.setCurrentColor(QtGui.QColor(self._color))\n    if dlg.exec_():\n        self.setColor(dlg.currentColor().name())\n"
    },
    {
        "original": "def check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    CLI Example::\n\n        salt '*' nftables.check_table nat\n    \"\"\"\n    ret = {'comment': '',\n           'result': False}\n\n    if not table:\n        ret['comment'] = 'Table needs to be specified'\n        return ret\n\n    nft_family = _NFTABLES_FAMILIES[family]\n    cmd = '{0} list tables {1}' . format(_nftables_cmd(), nft_family)\n    out = __salt__['cmd.run'](cmd, python_shell=False).find('table {0} {1}'.format(nft_family, table))\n\n    if out == -1:\n        ret['comment'] = 'Table {0} in family {1} does not exist'.\\\n                         format(table, family)\n    else:\n        ret['comment'] = 'Table {0} in family {1} exists'.\\\n                         format(table, family)\n        ret['result'] = True\n    return ret",
        "rewrite": "\n\n\ndef check_table(table=None, family='ipv4'):\n    ret = {'comment': '', 'result': False}\n\n    if not table:\n        ret['comment'] = 'Table needs to be specified'\n        return ret\n\n    nft_family = _NFTABLES_FAMILIES[family]\n    cmd = f\"{_nftables_cmd()} list tables {nft_family}\"\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    if f\"table {table} {nft_family}\" not in out:\n        ret['comment'] = f\""
    },
    {
        "original": "def mean(name, num, minimum=0, maximum=0, ref=None):\n    \"\"\"\n    Calculates the mean of the ``num`` most recent values. Requires a list.\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        foo:\n          calc.mean:\n            - name: myregentry\n            - num: 5\n    \"\"\"\n    return calc(\n        name=name,\n        num=num,\n        oper='mean',\n        minimum=minimum,\n        maximum=maximum,\n        ref=ref\n    )",
        "rewrite": "\n\n\ndef mean(name, num, minimum=0, maximum=0, ref=None):\n    return calc(\n        name=name,\n        num=num,\n        oper='mean',\n        minimum=minimum,\n        maximum=maximum,\n        ref=ref\n    )\n"
    },
    {
        "original": "def strxor(s1, s2):\n    \"\"\"\n    Returns the binary XOR of the 2 provided strings s1 and s2. s1 and s2\n    must be of same length.\n    \"\"\"\n    return b\"\".join(map(lambda x, y: chb(orb(x) ^ orb(y)), s1, s2))",
        "rewrite": "\n\n\ndef orb(c):\n    return ord(c)\n\ndef chb(i):\n    return chr(i)\n\ndef strxor(s1, s2):\n    return bytes([orb(x) ^ orb(y) for x, y in zip(s1, s2)])\n"
    },
    {
        "original": "def get_ir_reciprocal_mesh(self, mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n        \"\"\"\n        k-point mesh of the Brillouin zone generated taken into account\n        symmetry.The method returns the irreducible kpoints of the mesh\n        and their weights\n\n        Args:\n            mesh (3x1 array): The number of kpoint for the mesh needed in\n                each direction\n            is_shift (3x1 array): Whether to shift the kpoint grid. (1, 1,\n            1) means all points are shifted by 0.5, 0.5, 0.5.\n\n        Returns:\n            A list of irreducible kpoints and their weights as a list of\n            tuples [(ir_kpoint, weight)], with ir_kpoint given\n            in fractional coordinates\n        \"\"\"\n        shift = np.array([1 if i else 0 for i in is_shift])\n        mapping, grid = spglib.get_ir_reciprocal_mesh(\n            np.array(mesh), self._cell, is_shift=shift, symprec=self._symprec)\n\n        results = []\n        for i, count in zip(*np.unique(mapping, return_counts=True)):\n            results.append(((grid[i] + shift * (0.5, 0.5, 0.5)) / mesh,\n                            count))\n        return results",
        "rewrite": "\n\n\ndef get_ir_reciprocal_mesh(self, mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n    shift = np.array([0.5 if i else 0 for i in is_shift])\n    mapping, grid = spglib.get_ir_reciprocal_mesh(\n        np.array(mesh), self._cell, is_shift=shift, symprec=self._symprec)\n\n    results = []\n    for i, count in zip(*np.unique(mapping, return_counts=True)):\n        results.append(((grid[i] + shift) / mesh,"
    },
    {
        "original": "def json_pretty_dump(obj, filename):\n    \"\"\"\n    Serialize obj as a JSON formatted stream to the given filename (\n    pretty printing version)\n    \"\"\"\n    with open(filename, \"wt\") as fh:\n        json.dump(obj, fh, indent=4, sort_keys=4)",
        "rewrite": "\ndef json_pretty_dump(obj, filename):\n    with open(filename, \"w\", encoding=\"utf-8\") as fh:\n        import json\n        json.dump(obj, fh, indent=4)\n"
    },
    {
        "original": "def _hdparm(args, failhard=True):\n    \"\"\"\n    Execute hdparm\n    Fail hard when required\n    return output when possible\n    \"\"\"\n    cmd = 'hdparm {0}'.format(args)\n    result = __salt__['cmd.run_all'](cmd)\n    if result['retcode'] != 0:\n        msg = '{0}: {1}'.format(cmd, result['stderr'])\n        if failhard:\n            raise CommandExecutionError(msg)\n        else:\n            log.warning(msg)\n\n    return result['stdout']",
        "rewrite": "\n\n\ndef _hdparm(args, failhard=True):\n    cmd = f'hdparm {args}'\n    result = __salt__['cmd.run_all'](cmd)\n    if result['retcode'] != 0:\n        msg = f'{cmd}: {result[\"stderr\"]}'\n        if failhard:\n            raise CommandExecutionError(msg)\n        log.warning(msg)\n    return result['stdout']\n"
    },
    {
        "original": "def update(self, z):\n        \"\"\"\n        Add a new measurement `z` to the H-Infinity filter. If `z` is None,\n        nothing is changed.\n\n        Parameters\n        ----------\n        z : ndarray\n            measurement for this update.\n        \"\"\"\n\n        if z is None:\n            return\n\n        # rename for readability and a tiny extra bit of speed\n        I = self._I\n        gamma = self.gamma\n        Q = self.Q\n        H = self.H\n        P = self.P\n        x = self.x\n        V_inv = self._V_inv\n        F = self.F\n        W = self.W\n\n        # common subexpression H.T * V^-1\n        HTVI = dot(H.T, V_inv)\n\n        L = linalg.inv(I - gamma * dot(Q, P) + dot(HTVI, H).dot(P))\n\n        # common subexpression P*L\n        PL = dot(P, L)\n\n        K = dot(F, PL).dot(HTVI)\n\n        self.y = z - dot(H, x)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the H-Infinity gain\n        self.x = self.x + dot(K, self.y)\n        self.P = dot(F, PL).dot(F.T) + W\n\n        # force P to be symmetric\n        self.P = (self.P + self.P.T) / 2\n\n        # pylint: disable=bare-except\n        try:\n            self.z = np.copy(z)\n        except:\n            self.z = copy.deepcopy(z)",
        "rewrite": "\n\n\ndef update(self, z):\n    if z is None:\n                return\n    \n    I         ,gamma      ,Q      ,H      ,P      ,x       ,V_inv   ,F      ,W       =(self._I,self.gamma,self.Q,self.H,self.P,self.x,self._V_inv,self.F,self.W)\n    \n    HTVI=dot(H.T,V_inv)\n    L=linalg.inv(I-gamma*dot(Q,P)+dot(dot(HTVI,H),P))\n    \n    PL=dot(P,L)\n    K=dot(dot(F,PL),HTVI)\n    \n"
    },
    {
        "original": "def exhaust_stream(f):\n    \"\"\"Helper decorator for methods that exhausts the stream on return.\"\"\"\n\n    def wrapper(self, stream, *args, **kwargs):\n        try:\n            return f(self, stream, *args, **kwargs)\n        finally:\n            exhaust = getattr(stream, \"exhaust\", None)\n            if exhaust is not None:\n                exhaust()\n            else:\n                while 1:\n                    chunk = stream.read(1024 * 64)\n                    if not chunk:\n                        break\n\n    return update_wrapper(wrapper, f)",
        "rewrite": "\n\n\ndef exhaust_stream(f):\n    def wrapper(self, stream, *args, **kwargs):\n        try:\n            return f(self, stream, *args, ** kwargs)\n        finally:\n            exhaust = getattr(stream, \"exhaust\", None)\n            if callable(exhaust):\n                exhaust()\n            else:\n                while True:\n                    chunk = stream.read(1024 * 64)\n                    if not chunk:\n                        break\n    return wrapper\n"
    },
    {
        "original": "def PushState(self, **_):\n    \"\"\"Push the current state on the state stack.\"\"\"\n    if self.verbose:\n      logging.debug(\"Storing state %r\", self.state)\n    self.state_stack.append(self.state)",
        "rewrite": "\n\n\ndef push_state(self, **_):\n    self.state_stack.append(self.state)\n    if self.verbose:\n        logging.debug(\"Storing state %r\", self.state)\n"
    },
    {
        "original": "def _execute_handling(self, *eopatches, **kwargs):\r\n        \"\"\" Handles measuring execution time and error propagation\r\n        \"\"\"\r\n        self.private_task_config.start_time = datetime.datetime.now()\r\n\r\n        caught_exception = None\r\n        try:\r\n            return_value = self.execute(*eopatches, **kwargs)\r\n        except BaseException as exception:\r\n            caught_exception = exception, sys.exc_info()[2]\r\n\r\n        if caught_exception is not None:  # Exception is not raised in except statement to prevent duplicated traceback\r\n            exception, traceback = caught_exception\r\n            raise type(exception)('During execution of task {}: {}'.format(self.__class__.__name__,\r\n                                                                           exception)).with_traceback(traceback)\r\n\r\n        self.private_task_config.end_time = datetime.datetime.now()\r\n        return return_value",
        "rewrite": "\n\n\ndef _execute_handling(self, *eopatches, **kwargs):\n    self.private_task_config.start_time = datetime.datetime.now()\n\n    try:\n        return_value = self.execute(*eopatches, **kwargs)\n    except BaseException as e:\n        raise type(e)('During execution of task {}: {}'.format(self.__class__.__name__, e)) from e\n\n    self.private_task_config.end_time = datetime.datetime.now()\n    return return_value\n"
    },
    {
        "original": "def _cells(self):\n        \"\"\"\n        A sequence of |_Cell| objects, one for each cell of the layout grid.\n        If the table contains a span, one or more |_Cell| object references\n        are repeated.\n        \"\"\"\n        col_count = self._column_count\n        cells = []\n        for tc in self._tbl.iter_tcs():\n            for grid_span_idx in range(tc.grid_span):\n                if tc.vMerge == ST_Merge.CONTINUE:\n                    cells.append(cells[-col_count])\n                elif grid_span_idx > 0:\n                    cells.append(cells[-1])\n                else:\n                    cells.append(_Cell(tc, self))\n        return cells",
        "rewrite": "\n\n\ndef _cells(self):\n    col_count = self._column_count\n    cells = []\n    for tc in self._tbl.iter_tcs():\n        for _ in range(tc.grid_span):\n            if tc.vMerge == ST_Merge.CONTINUE:\n                cells.append(cells[-col_count])\n            elif len(cells) > 0:\n                cells.append(cells[-1])\n            else:\n                cells.append(_Cell(tc, self))\n    return cells\n"
    },
    {
        "original": "def _cs_path_exists(fspath):\n        \"\"\"\n        Case-sensitive path existence check\n\n        >>> sdist_add_defaults._cs_path_exists(__file__)\n        True\n        >>> sdist_add_defaults._cs_path_exists(__file__.upper())\n        False\n        \"\"\"\n        if not os.path.exists(fspath):\n            return False\n        # make absolute so we always have a directory\n        abspath = os.path.abspath(fspath)\n        directory, filename = os.path.split(abspath)\n        return filename in os.listdir(directory)",
        "rewrite": "\n\n\nimport os\n\ndef _cs_path_exists(fspath):\n    if not os.path.exists(fspath):\n        return False\n    abspath = os.path.abspath(fspath)\n    directory, filename = os.path.split(abspath)\n    return filename == os.listdir(directory)[os.listdir(directory).index(filename)] if filename in os.listdir(directory) else False\n"
    },
    {
        "original": "def addfield(self, pkt, s, val):\n        \"\"\"Add an internal value to a string\"\"\"\n        if self.length_of(pkt) == 8:\n            return s + struct.pack(self.fmt[0] + \"B\", val)\n        if self.length_of(pkt) == 16:\n            return s + struct.pack(self.fmt[0] + \"H\", val)\n        if self.length_of(pkt) == 32:\n            return s + struct.pack(self.fmt[0] + \"2H\", val)  # TODO: fix!\n        if self.length_of(pkt) == 48:\n            return s + struct.pack(self.fmt[0] + \"3H\", val)  # TODO: fix!\n        elif self.length_of(pkt) == 64:\n            return s + struct.pack(self.fmt[0] + \"Q\", val)\n        elif self.length_of(pkt) == 128:\n            # TODO: FIX THE PACKING!!\n            return s + struct.pack(self.fmt[0] + \"16s\", raw(val))\n        else:\n            return s",
        "rewrite": "\n\n\ndef addfield(self, pkt, s, val):\n    fmt_map = {\n        8: \"B\",\n        16: \"H\",\n        32: \"I\",  # Assuming 4-byte integer\n        48: \"3I\",  # Assuming three 2-byte integers\n        64: \"Q\",\n        128: \"16s\"  # Assuming a string of length 16\n    }\n    if self.length_of(pkt) in fmt_map:\n        return s + struct.pack(self.fmt[0] + fmt_map[self.length_of(pkt)],"
    },
    {
        "original": "def _file_path(self, dirname, filename):\n        \"\"\"\n        Builds an absolute path and creates the directory and file if they don't already exist.\n\n        @dirname  - Directory path.\n        @filename - File name.\n\n        Returns a full path of 'dirname/filename'.\n        \"\"\"\n        if not os.path.exists(dirname):\n            try:\n                os.makedirs(dirname)\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        fpath = os.path.join(dirname, filename)\n\n        if not os.path.exists(fpath):\n            try:\n                open(fpath, \"w\").close()\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        return fpath",
        "rewrite": "\n\n\nimport os\n\ndef _file_path(self, dirname, filename):\n    if not os.path.exists(dirname):\n        try:\n            os.makedirs(dirname)\n        except KeyboardInterrupt as e:\n            raise\n    fpath = os.path.join(dirname, filename)\n    if not os.path.exists(fpath]:\n         with open(fpath, \"w\") as f: pass\n    return fpath\n"
    },
    {
        "original": "def _get_stats_column_names(cls):\n        \"\"\"Construct a tuple of the column names for stats. Each stat has 4\n        columns of data.\n        \"\"\"\n        columns = []\n        stats = ('num_bytes', 'num_rows', 'location', 'partitioning_type',\n                 'clustering_fields')\n        stat_components = ('label', 'value', 'description', 'include')\n        for stat_id in stats:\n            for stat_component in stat_components:\n                columns.append('stats:{}:{}'.format(stat_id, stat_component))\n        return tuple(columns)",
        "rewrite": "\n\n\ndef _get_stats_column_names(cls):\n    stats = ('num_bytes', 'num_rows', 'location', 'partitioning_type', 'clustering_fields')\n    stat_components = ('label', 'value', 'description', 'include')\n    return tuple(f'stats:{stat_id}:{stat_component}' for stat_id in stats for stat_component in stat_components)\n"
    },
    {
        "original": "def structures(self):\n        \"\"\"\n        Copy of all structures in the TransformedStructure. A\n        structure is stored after every single transformation.\n        \"\"\"\n        hstructs = [Structure.from_dict(s['input_structure'])\n                    for s in self.history if 'input_structure' in s]\n        return hstructs + [self.final_structure]",
        "rewrite": "\n\n\ndef structures(self):\n    return [Structure.from_dict(s['input_structure']) for s in self.history if 'input_structure' in s] + [self.final_structure]\n"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'level') and self.level is not None:\n            _dict['level'] = self.level\n        if hasattr(self, 'names') and self.names is not None:\n            _dict['names'] = self.names\n        return _dict",
        "rewrite": "\n\n\ndef _to_dict(self):\n    _dict = {}\n    for attr in ['level', 'names']:\n        if hasattr(self, attr) and getattr(self, attr) is not None:\n            _dict[attr] = getattr(self, attr)\n    return _dict\n"
    },
    {
        "original": "def norm_cdf(x_range, mu, var=1, std=None):\n    \"\"\"\n    Computes the probability that a Gaussian distribution lies\n    within a range of values.\n\n    Parameters\n    ----------\n\n    x_range : (float, float)\n        tuple of range to compute probability for\n\n    mu : float\n        mean of the Gaussian\n\n    var : float, optional\n        variance of the Gaussian. Ignored if `std` is provided\n\n    std : float, optional\n       standard deviation of the Gaussian. This overrides the `var` parameter\n\n    Returns\n    -------\n\n    probability : float\n        probability that Gaussian is within x_range. E.g. .1 means 10%.\n    \"\"\"\n\n    if std is None:\n        std = math.sqrt(var)\n    return abs(norm.cdf(x_range[0], loc=mu, scale=std) -\n               norm.cdf(x_range[1], loc=mu, scale=std))",
        "rewrite": "\n\n\nimport math\nfrom scipy.stats import norm\n\ndef norm_cdf(x_range, mu, var=1, std=None):\n    if std is None:\n        std = math.sqrt(var)\n    return abs(norm.cdf(x_range[1], loc=mu, scale=std) - \n               norm.cdf(x_range[0], loc=mu, scale=std))\n"
    },
    {
        "original": "def upgrade(refresh=True, **kwargs):\n    \"\"\"\n    Upgrade outdated, unpinned brews.\n\n    refresh\n        Fetch the newest version of Homebrew and all formulae from GitHub before installing.\n\n    Returns a dictionary containing the changes:\n\n    .. code-block:: python\n\n        {'<package>':  {'old': '<old-version>',\n                        'new': '<new-version>'}}\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.upgrade\n    \"\"\"\n    ret = {'changes': {},\n           'result': True,\n           'comment': '',\n           }\n\n    old = list_pkgs()\n\n    if salt.utils.data.is_true(refresh):\n        refresh_db()\n\n    result = _call_brew('upgrade', failhard=False)\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem encountered upgrading packages',\n            info={'changes': ret, 'result': result}\n        )\n\n    return ret",
        "rewrite": "\n\n\ndef upgrade(refresh=True, **kwargs):\n    ret = {'changes': {}, 'result': True, 'comment': ''}\n    old = list_pkgs()\n    if salt.utils.data.is_true(refresh):\n        refresh_db()\n    result = _call_brew('upgrade', failhard=False)\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n    if result['retcode'] != 0:\n        raise CommandExecutionError('Problem encountered upgrading packages', info={'changes': ret,"
    },
    {
        "original": "def prev_moment_operating_on(\n            self,\n            qubits: Sequence[ops.Qid],\n            end_moment_index: Optional[int] = None,\n            max_distance: Optional[int] = None) -> Optional[int]:\n        \"\"\"Finds the index of the next moment that touches the given qubits.\n\n        Args:\n            qubits: We're looking for operations affecting any of these qubits.\n            end_moment_index: The moment index just after the starting point of\n                the reverse search. Defaults to the length of the list of\n                moments.\n            max_distance: The number of moments (starting just before from the\n                end index and moving backward) to check. Defaults to no limit.\n\n        Returns:\n            None if there is no matching moment, otherwise the index of the\n            latest matching moment.\n\n        Raises:\n            ValueError: negative max_distance.\n        \"\"\"\n        if end_moment_index is None:\n            end_moment_index = len(self._moments)\n\n        if max_distance is None:\n            max_distance = len(self._moments)\n        elif max_distance < 0:\n            raise ValueError('Negative max_distance: {}'.format(max_distance))\n        else:\n            max_distance = min(end_moment_index, max_distance)\n\n        # Don't bother searching indices past the end of the list.\n        if end_moment_index > len(self._moments):\n            d = end_moment_index - len(self._moments)\n            end_moment_index -= d\n            max_distance -= d\n        if max_distance <= 0:\n            return None\n\n        return self._first_moment_operating_on(qubits,\n                                               (end_moment_index - k - 1\n                                                for k in range(max_distance)))",
        "rewrite": "\n\n\ndef prev_moment_operating_on(\n    self,\n    qubits: Sequence[ops.Qid],\n    end_moment_index: Optional[int] = None,\n    max_distance: Optional[int] = None\n) -> Optional[int]:\n    if end_moment_index is None:\n        end_moment_index = len(self._moments)\n    \n    if max_distance is None:\n        max_distance = len(self._moments)\n    elif max_distance < 0:\n        raise ValueError('Negative max_distance: {}'.format(max_distance))\n    else:\n        max_distance = min(end_moment_index"
    },
    {
        "original": "def _fix_ctx(m2_ctx, issuer=None):\n    \"\"\"\n    This is part of an ugly hack to fix an ancient bug in M2Crypto\n    https://bugzilla.osafoundation.org/show_bug.cgi?id=7530#c13\n    \"\"\"\n    ctx = _Ctx.from_address(int(m2_ctx))  # pylint: disable=no-member\n\n    ctx.flags = 0\n    ctx.subject_cert = None\n    ctx.subject_req = None\n    ctx.crl = None\n    if issuer is None:\n        ctx.issuer_cert = None\n    else:\n        ctx.issuer_cert = int(issuer.x509)",
        "rewrite": "\n\n\ndef _fix_ctx(m2_ctx, issuer=None):\n    ctx = _Ctx.from_address(int(m2_ctx))\n    ctx.flags = 0\n    ctx.subject_cert = None\n    ctx.subject_req = None\n    ctx.crl = None\n    ctx.issuer_cert = int(issuer.x509) if issuer else None\n"
    },
    {
        "original": "def get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Return a storage_conn object for the storage account\n    \"\"\"\n    if conn_kwargs is None:\n        conn_kwargs = {}\n\n    if not storage_account:\n        storage_account = config.get_cloud_config_value(\n            'storage_account',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_account', None)\n        )\n    if not storage_key:\n        storage_key = config.get_cloud_config_value(\n            'storage_key',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_key', None)\n        )\n    return azure.storage.BlobService(storage_account, storage_key)",
        "rewrite": "\n\n\ndef get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):\n    if conn_kwargs is None:\n        conn_kwargs = {}\n\n    if not storage_account:\n        storage_account = config.get_cloud_config_value(\n            'storage_account',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_account')\n        )\n\n    if not storage_key:\n        storage_key = config.get_cloud_config_value(\n            'storage_key',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_key')\n        )\n\n"
    },
    {
        "original": "def create_profile(hostname, username, password, profile_type, name, **kwargs):\n    r\"\"\"\n    A function to connect to a bigip device and create a profile.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    profile_type\n        The type of profile to create\n    name\n        The name of the profile to create\n    kwargs\n        [ arg=val ] ...\n\n        Consult F5 BIGIP user guide for specific options for each profile type.\n        Typically, tmsh arg names are used.\n\n    Special Characters ``|``, ``,`` and ``:`` must be escaped using ``\\`` when\n    used within strings.\n\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    #is this profile currently configured?\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    # if it exists\n    if existing['code'] == 200:\n\n        ret['result'] = True\n        ret['comment'] = 'A profile by this name currently exists.  No change made.'\n\n    # if it doesn't exist\n    elif existing['code'] == 404:\n\n        response = __salt__['bigip.create_profile'](hostname, username, password, profile_type, name, **kwargs)\n\n        if response['code'] == 200:\n            ret['result'] = True\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = response['content']\n            ret['comment'] = 'Profile was successfully created.'\n        else:\n            ret = _load_result(response, ret)\n\n    # else something else was returned\n    else:\n        ret = _load_result(existing, ret)\n\n    return ret",
        "rewrite": "\n\n\ndef create_profile(hostname, username, password, profile_type, name, **kwargs):\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    if existing['"
    },
    {
        "original": "def create_policy(name, policy_name, policy_type, policy, region=None,\n                  key=None, keyid=None, profile=None):\n    \"\"\"\n    Create an ELB policy.\n\n    .. versionadded:: 2016.3.0\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_elb.create_policy myelb mypolicy LBCookieStickinessPolicyType '{\"CookieExpirationPeriod\": 3600}'\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if not exists(name, region, key, keyid, profile):\n        return False\n    try:\n        success = conn.create_lb_policy(name, policy_name, policy_type, policy)\n        if success:\n            log.info('Created policy %s on ELB %s', policy_name, name)\n            return True\n        else:\n            log.error('Failed to create policy %s on ELB %s', policy_name, name)\n            return False\n    except boto.exception.BotoServerError as e:\n        log.error('Failed to create policy %s on ELB %s: %s',\n                  policy_name, name, e.message,\n                  exc_info_on_loglevel=logging.DEBUG)\n        return False",
        "rewrite": "\n\n\ndef create_policy(name, policy_name, policy_type, policy, region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    \n    if not exists(name, region=region, key=key, keyid=keyid, profile=profile):\n        return False\n    \n    try:\n        if conn.create_lb_policy(name, policy_name, policy_type, policy):\n            log.info(f'Created policy {policy_name} on ELB {name}')\n            return True\n        else:\n            log"
    },
    {
        "original": "def _finalize_axis(self, key, **kwargs):\n        \"\"\"\n        Extends the ElementPlot _finalize_axis method to set appropriate\n        labels, and axes options for 3D Plots.\n        \"\"\"\n        axis = self.handles['axis']\n        self.handles['fig'].set_frameon(False)\n        axis.grid(self.show_grid)\n        axis.view_init(elev=self.elevation, azim=self.azimuth)\n        axis.dist = self.distance\n\n        if self.xaxis is None:\n            axis.w_xaxis.line.set_lw(0.)\n            axis.w_xaxis.label.set_text('')\n        if self.yaxis is None:\n            axis.w_yaxis.line.set_lw(0.)\n            axis.w_yaxis.label.set_text('')\n        if self.zaxis is None:\n            axis.w_zaxis.line.set_lw(0.)\n            axis.w_zaxis.label.set_text('')\n        if self.disable_axes:\n            axis.set_axis_off()\n\n        if mpl_version <= '1.5.9':\n            axis.set_axis_bgcolor(self.bgcolor)\n        else:\n            axis.set_facecolor(self.bgcolor)\n        return super(Plot3D, self)._finalize_axis(key, **kwargs)",
        "rewrite": "\n\n\ndef _finalize_axis(self, key, **kwargs):\n    axis = self.handles['axis']\n    self.handles['fig'].set_frameon(False)\n    axis.grid(self.show_grid)\n    axis.view_init(elev=self.elevation, azim=self.azimuth)\n    axis.dist = self.distance\n\n    for attr, axis_obj in [('x', axis.w_xaxis), ('y', axis.w_yaxis), ('z', axis.w_zaxis)]:\n        if getattr(self, f\"{attr}axis\") is None:\n            setattr(axis_obj, \"line\").set_lw"
    },
    {
        "original": "def _calendar_month_middles(year):\n    \"\"\"List of middle day of each month, used by Linke turbidity lookup\"\"\"\n    # remove mdays[0] since January starts at mdays[1]\n    # make local copy of mdays since we need to change\n    # February for leap years\n    mdays = np.array(calendar.mdays[1:])\n    ydays = 365\n    # handle leap years\n    if calendar.isleap(year):\n        mdays[1] = mdays[1] + 1\n        ydays = 366\n    middles = np.concatenate(\n        [[-calendar.mdays[-1] / 2.0],  # Dec last year\n         np.cumsum(mdays) - np.array(mdays) / 2.,  # this year\n         [ydays + calendar.mdays[1] / 2.0]])  # Jan next year\n    return middles",
        "rewrite": "\n\n\nimport numpy as np\nimport calendar\n\ndef _calendar_month_middles(year):\n    mdays = np.array(calendar.mdays[1:])\n    ydays = 365\n    if calendar.isleap(year):\n        mdays[1] += 1\n        ydays = 366\n    middles = np.concatenate([\n        [-calendar.mdays[-1] / 2.0],\n        np.cumsum(mdays) - mdays / 2.,\n        [ydays + calendar.mdays[1] / 2.0]\n    ])\n    return middles\n"
    },
    {
        "original": "def reduced_formula(self):\n        \"\"\"\n        Returns a reduced formula string with appended charge.\n        \"\"\"\n        reduced_formula = super().reduced_formula\n        charge = self._charge / self.get_reduced_composition_and_factor()[1]\n        if charge > 0:\n            if abs(charge) == 1:\n                chg_str = \"[+]\"\n            else:\n                chg_str = \"[\" + formula_double_format(charge, False) + \"+]\"\n        elif charge < 0:\n            if abs(charge) == 1:\n                chg_str = \"[-]\"\n            else:\n                chg_str = \"[{}-]\".format(formula_double_format(abs(charge),\n                                                               False))\n        else:\n            chg_str = \"(aq)\"\n        return reduced_formula + chg_str",
        "rewrite": "\n\n\ndef reduced_formula(self):\n    reduced_formula = super().reduced_formula\n    charge = self._charge / self.get_reduced_composition_and_factor()[1]\n    if charge:\n        sign = '+' if charge > 0 else '-'\n        chg_str = \"[{}{}]\".format(sign if abs(charge) != 1 else '', formula_double_format(abs(charge), False))\n    else:\n        chg_str = \"(aq)\"\n    return reduced_formula + chg_str\n"
    },
    {
        "original": "def query(self, *args):\n        \"\"\" Send a query to the watchman service and return the response\n\n        This call will block until the response is returned.\n        If any unilateral responses are sent by the service in between\n        the request-response they will be buffered up in the client object\n        and NOT returned via this method.\n        \"\"\"\n\n        log(\"calling client.query\")\n        self._connect()\n        try:\n            self.sendConn.send(args)\n\n            res = self.receive()\n            while self.isUnilateralResponse(res):\n                res = self.receive()\n\n            return res\n        except EnvironmentError as ee:\n            # When we can depend on Python 3, we can use PEP 3134\n            # exception chaining here.\n            raise WatchmanEnvironmentError(\n                \"I/O error communicating with watchman daemon\",\n                ee.errno,\n                ee.strerror,\n                args,\n            )\n        except WatchmanError as ex:\n            ex.setCommand(args)\n            raise",
        "rewrite": "\n\n\ndef query(self, *args):\n    log(\"calling client.query\")\n    self._connect()\n    try:\n        self.sendConn.send(args)\n        res = self.receive()\n        while self.isUnilateralResponse(res):\n            res = self.receive()\n        return res\n    except EnvironmentError as ee:\n        raise WatchmanEnvironmentError(\n            \"I/O error communicating with watchman daemon\",\n            ee.errno,\n            ee.strerror,\n            args,\n        ) from ee\n    except WatchmanError as ex:\n        ex.setCommand(args)\n        raise\n"
    },
    {
        "original": "def find_cell_end(self, lines):\n        \"\"\"Return position of end of cell marker, and position of first line after cell\"\"\"\n        if self.metadata is None and not (self.cell_marker_end and self.end_code_re.match(lines[0])) \\\n                and paragraph_is_fully_commented(lines, self.comment, self.default_language):\n            self.cell_type = 'markdown'\n            for i, line in enumerate(lines):\n                if _BLANK_LINE.match(line):\n                    return i, i + 1, False\n            return len(lines), len(lines), False\n\n        if self.metadata is None:\n            self.end_code_re = None\n        elif not self.cell_marker_end:\n            end_of_cell = self.metadata.get('endofcell', '-')\n            self.end_code_re = re.compile('^' + self.comment + ' ' + end_of_cell + r'\\s*$')\n\n        return self.find_region_end(lines)",
        "rewrite": "\n\n\ndef find_cell_end(self, lines):\n    if self.metadata is None and not (self.cell_marker_end and self.end_code_re.match(lines[0])) \\\n            and paragraph_is_fully_commented(lines, self.comment, self.default_language):\n        self.cell_type = 'markdown'\n        for i, line in enumerate(lines):\n            if _BLANK_LINE.match(line):\n                return i, i + 1, False\n        return len(lines), len(lines), False\n\n    if self.metadata is None:\n        self.end_code_re = None\n    else:\n        end_of_cell = self"
    },
    {
        "original": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered",
        "rewrite": "\n\n\ndef _filter_cache(self, dmap, kdims):\n    return [(key, value) for key, value in dmap.data.items() \n            if not any(v not in kd.values for kd, v in zip(kdims, key))]\n"
    },
    {
        "original": "def _parse_dict_string(self, string, key, default):\n        \"\"\"Take from a more recent redis.py, parse_info\"\"\"\n        try:\n            for item in string.split(','):\n                k, v = item.rsplit('=', 1)\n                if k == key:\n                    try:\n                        return int(v)\n                    except ValueError:\n                        return v\n            return default\n        except Exception:\n            self.log.exception(\"Cannot parse dictionary string: %s\" % string)\n            return default",
        "rewrite": "\n\n\ndef _parse_dict_string(self, string, key, default):\n    try:\n        for item in (x.strip() for x in string.split(',')):\n            k, v = item.rsplit('=', 1)\n            if k == key:\n                try:\n                    return int(v)\n                except ValueError:\n                    return v\n        return default\n    except Exception as e:\n        self.log.exception(f\"Cannot parse dictionary string: {string}. Error: {str(e)}\")\n        return default\n"
    },
    {
        "original": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        index = self.get_dimension_index(dimension)\n        if index == 0:\n            return np.array([self.x])\n        elif index == 1:\n            return np.array([self.y])\n        else:\n            return super(Arrow, self).dimension_values(dimension)",
        "rewrite": "\n\n\ndef dimension_values(self, dimension, expanded=True, flat=True):\n    index = self.get_dimension_index(dimension)\n    if index == 0:\n        return np.array([self.x])\n    elif index == 1:\n        return np.array([self.y])\n    else:\n        return super().dimension_values(dimension)\n"
    },
    {
        "original": "def enable(self):\n        \"\"\"Return True|False if the AMP is enabled in the configuration file (enable=true|false).\"\"\"\n        ret = self.get('enable')\n        if ret is None:\n            return False\n        else:\n            return ret.lower().startswith('true')",
        "rewrite": "\ndef enable(self):\n    ret = self.get('enable')\n    return ret and ret.lower() == 'true'\n"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "rewrite": "\n\n\nimport json\nimport collections\n\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n    \n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n        \n        key = (str(check), validator[\"comparator\"])\n        \n        if key not in validators_mapping:\n           validators_mapping[key] = {'count': 0, 'validators': []}\n            \n        new_validator_info = {'expect': validator['expect']}\n        \n        if 'info' in"
    },
    {
        "original": "def ParseAction(self, action):\n    \"\"\"Extract log configuration data from rsyslog actions.\n\n    Actions have the format:\n      <facility>/<severity> <type_def><destination>;<template>\n      e.g. *.* @@loghost.example.com.:514;RSYSLOG_ForwardFormat\n\n    Actions are selected by a type definition. These include:\n      \"@@\": TCP syslog\n      \"@\": UDP syslog\n      \"|\": Named pipe\n      \"~\": Drop to /dev/null\n      \"^\": Shell script\n      \":om<string>:\": An output module\n      Or a file path.\n\n    Args:\n      action: The action string from rsyslog.\n\n    Returns:\n      a rdfvalue.LogTarget message.\n    \"\"\"\n    rslt = rdf_config_file.LogTarget()\n    for dst_str, dst_re in iteritems(self.destinations):\n      dst = dst_re.match(action)\n      if dst:\n        rslt.transport = dst_str\n        rslt.destination = dst.group(1)\n        break\n    return rslt",
        "rewrite": "\n\n\ndef ParseAction(self, action):\n    rslt = rdf_config_file.LogTarget()\n    for dst_str, dst_re in self.destinations.items():\n        match = dst_re.match(action)\n        if match:\n            rslt.transport = dst_str\n            rslt.destination = match.group(1)\n            return rslt\n"
    },
    {
        "original": "def modified_policy_iteration(self, v_init=None, epsilon=None,\n                                  max_iter=None, k=20):\n        \"\"\"\n        Solve the optimization problem by modified policy iteration. See\n        the `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n        if epsilon is None:\n            epsilon = self.epsilon\n\n        def span(z):\n            return z.max() - z.min()\n\n        def midrange(z):\n            return (z.min() + z.max()) / 2\n\n        v = np.empty(self.num_states)\n        if v_init is None:\n            v[:] = self.R[self.R > -np.inf].min() / (1 - self.beta)\n        else:\n            v[:] = v_init\n\n        u = np.empty(self.num_states)\n        sigma = np.empty(self.num_states, dtype=int)\n\n        try:\n            tol = epsilon * (1-self.beta) / self.beta\n        except ZeroDivisionError:  # Raised if beta = 0\n            tol = np.inf\n\n        for i in range(max_iter):\n            # Policy improvement\n            self.bellman_operator(v, Tv=u, sigma=sigma)\n            diff = u - v\n            if span(diff) < tol:\n                v[:] = u + midrange(diff) * self.beta / (1 - self.beta)\n                break\n            # Partial policy evaluation with k iterations\n            self.operator_iteration(T=self.T_sigma(sigma), v=u, max_iter=k)\n            v[:] = u\n\n        num_iter = i + 1\n\n        res = DPSolveResult(v=v,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='modified policy iteration',\n                            epsilon=epsilon,\n                            max_iter=max_iter,\n                            k=k)\n\n        return res",
        "rewrite": "\n\n\ndef modified_policy_iteration(self, v_init=None, epsilon=None, max_iter=None, k=20):\n    if self.beta == 1:\n       raise NotImplementedError(self._error_msg_no_discounting)\n\n    max_iter = max.Iter or=self.maxiter \n    epsilon or=self.epsilon \n\n    def span(z):\n       return z.max() - z.min()\n\n    def midrange(z):\n       return (z.min() + z.max()) / 2 \n\n    v=np.empty((self.num_states,))\n    if not isinstance(v_init,np.ndarray): \n         v[:] =(self.R[self.R>-np"
    },
    {
        "original": "def parse_env_file(env_file):\n    \"\"\"\n    Reads a line-separated environment file.\n    The format of each line should be \"key=value\".\n    \"\"\"\n    environment = {}\n\n    with open(env_file, 'r') as f:\n        for line in f:\n\n            if line[0] == '#':\n                continue\n\n            line = line.strip()\n            if not line:\n                continue\n\n            parse_line = line.split('=', 1)\n            if len(parse_line) == 2:\n                k, v = parse_line\n                environment[k] = v\n            else:\n                raise errors.DockerException(\n                    'Invalid line in environment file {0}:\\n{1}'.format(\n                        env_file, line))\n\n    return environment",
        "rewrite": "\ndef parse_env_file(env_file):\n    environment = {}\n    \n    with open(env_file, 'r') as f:\n        for line in f:\n            if not (line.startswith('#') or not line.strip()):\n                k, v =[line.split('=', 1)]\n                environment[k] = v\n    \n    return environment\n"
    },
    {
        "original": "def declination_spencer71(dayofyear):\n    \"\"\"\n    Solar declination from Duffie & Beckman [1] and attributed to\n    Spencer (1971) and Iqbal (1983).\n\n    .. warning::\n        Return units are radians, not degrees.\n\n    Parameters\n    ----------\n    dayofyear : numeric\n\n    Returns\n    -------\n    declination (radians) : numeric\n        Angular position of the sun at solar noon relative to the plane of the\n        equator, approximately between +/-23.45 (degrees).\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition\" pp. 13-14, J. Wiley and Sons, New York (2006)\n\n    [2] J. W. Spencer, \"Fourier series representation of the position of the\n    sun\" in Search 2 (5), p. 172 (1971)\n\n    [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n    Energy Applications\", p. 4 CRC Press (2013)\n\n    See Also\n    --------\n    declination_cooper69\n    \"\"\"\n    day_angle = _calculate_simple_day_angle(dayofyear)\n    return (\n        0.006918 -\n        0.399912 * np.cos(day_angle) + 0.070257 * np.sin(day_angle) -\n        0.006758 * np.cos(2. * day_angle) + 0.000907 * np.sin(2. * day_angle) -\n        0.002697 * np.cos(3. * day_angle) + 0.00148 * np.sin(3. * day_angle)\n    )",
        "rewrite": "Here is a revised version of the function:\n\n\nimport numpy as np\n\ndef _calculate_simple_day_angle(dayofyear):\n\treturn 2*np.pi*(dayofyear-81)/364\n\ndef declination_spencer71(dayofyear):\n\tday_angle = _calculate_simple_day_ANGLE/dayofyear)\n\treturn (\n\t\tnp.radians(\n\t\t\t23*np.pi/180 - \n\t\t\t(-0 busiestication912 - \n\t\t\t(19brains9528e-4)*np.clients(elevationsingrroryongearSpan centerY_endlewaterra(\u65e5Angle)) +  \n\t\t\u03b9\u03b1\u03c2ation(cerelative\u0639\u0644"
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "rewrite": "Here's a revised version of the code:\n\n\ndef save(self, eopatch: 'EOPatch', use_tmp: bool=True):\n    filename = self.tmp_filename if use_tmp else self.final_filename\n\n    if not hasattr(eopatch.data.get('features', {}), '__iter__'):\r\n      raise ValueError(f\"EOPATCH must contain iterable features. Got {eopatch}\")\n\n    try:\n      feature_data  = getattr(eopetch.data['features'], '__getitem__')(self.FeatureType.value)\n    except KeyError:\n      raise KeyError(f\"FeatureType '{self.FeatureType}' not found in EOPATCH"
    },
    {
        "original": "def add_to_collaborators(self, collaborator, permission=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PUT /repos/:owner/:repo/collaborators/:user <http://developer.github.com/v3/repos/collaborators>`_\n        :param collaborator: string or :class:`github.NamedUser.NamedUser`\n        :param permission: string 'pull', 'push' or 'admin'\n        :rtype: None\n        \"\"\"\n        assert isinstance(collaborator, github.NamedUser.NamedUser) or isinstance(collaborator, (str, unicode)), collaborator\n        assert permission is github.GithubObject.NotSet or isinstance(permission, (str, unicode)), permission\n\n        if isinstance(collaborator, github.NamedUser.NamedUser):\n            collaborator = collaborator._identity\n\n        if permission is not github.GithubObject.NotSet:\n            put_parameters = {'permission': permission}\n        else:\n            put_parameters = None\n\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/collaborators/\" + collaborator,\n            input=put_parameters\n        )\n        # return an invitation object if there's data returned by the API. If data is empty\n        # there's a pending invitation for the given user.\n        return github.Invitation.Invitation(self._requester, headers, data, completed=True) if \\\n            data is not None else None",
        "rewrite": "\n\n\ndef add_to_collaborators(self, collaborator, permission=github.GithubObject.NotSet):\n    if not isinstance(collaborator, (github.NamedUser NamedUser , str)) and not isinstance(collaborator,(str , unicode)):\n       raise TypeError(f\"Invalid type for collaborators {collaboration}\")\n    if not (permission is github.GithubObject.NotSet or isinstance(permission,(str , unicode))):\n       raise TypeError(f\"Invalid type for permssion {permission}\")\n\n    if isinstance(collaborator , github.NamedUser .NamedUser):\n       colaboratore = colaborattor ._identity"
    },
    {
        "original": "def read_and_write(self, reader, writer, chunk_size):\n        \"\"\"\n        Read ``chunk_size`` from ``reader``, writing result to ``writer``.\n\n        Returns ``None`` if successful, or ``True`` if the read was empty.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        data = reader.recv(chunk_size)\n        if len(data) == 0:\n            return True\n        writer.sendall(data)",
        "rewrite": "\ndef read_and_write(self, reader, writer, chunk_size):\n    data = reader.recv(chunk_size)\n    if not data:\n        return True\n    writer.sendall(data)\n"
    },
    {
        "original": "def get_results(self, **kwargs):\n        \"\"\"\n        Returns :class:`NodeResults` instance.\n        Subclasses should extend this method (if needed) by adding\n        specialized code that performs some kind of post-processing.\n        \"\"\"\n        # Check whether the process completed.\n        if self.returncode is None:\n            raise self.Error(\"return code is None, you should call wait, communicate or poll\")\n\n        if self.status is None or self.status < self.S_DONE:\n            raise self.Error(\"Task is not completed\")\n\n        return self.Results.from_node(self)",
        "rewrite": "\ndef get_results(self, **kwargs):\n    if self.returncode is None:\n        raise self.Error(\"return code is None, you should call wait, communicate or poll\")\n    if self.status is None or self.status < self.S_DONE:\n        raise self.Error(\"Task is not completed\")\n    return NodeResults.from_node(self)\n"
    },
    {
        "original": "def file_transfer(\n    ssh_conn,\n    source_file,\n    dest_file,\n    file_system=None,\n    direction=\"put\",\n    disable_md5=False,\n    inline_transfer=False,\n    overwrite_file=False,\n):\n    \"\"\"Use Secure Copy or Inline (IOS-only) to transfer files to/from network devices.\n\n    inline_transfer ONLY SUPPORTS TEXT FILES and will not support binary file transfers.\n\n    return {\n        'file_exists': boolean,\n        'file_transferred': boolean,\n        'file_verified': boolean,\n    }\n    \"\"\"\n    transferred_and_verified = {\n        \"file_exists\": True,\n        \"file_transferred\": True,\n        \"file_verified\": True,\n    }\n    transferred_and_notverified = {\n        \"file_exists\": True,\n        \"file_transferred\": True,\n        \"file_verified\": False,\n    }\n    nottransferred_but_verified = {\n        \"file_exists\": True,\n        \"file_transferred\": False,\n        \"file_verified\": True,\n    }\n\n    if \"cisco_ios\" in ssh_conn.device_type or \"cisco_xe\" in ssh_conn.device_type:\n        cisco_ios = True\n    else:\n        cisco_ios = False\n    if not cisco_ios and inline_transfer:\n        raise ValueError(\"Inline Transfer only supported for Cisco IOS/Cisco IOS-XE\")\n\n    scp_args = {\n        \"ssh_conn\": ssh_conn,\n        \"source_file\": source_file,\n        \"dest_file\": dest_file,\n        \"direction\": direction,\n    }\n    if file_system is not None:\n        scp_args[\"file_system\"] = file_system\n\n    TransferClass = InLineTransfer if inline_transfer else FileTransfer\n\n    with TransferClass(**scp_args) as scp_transfer:\n        if scp_transfer.check_file_exists():\n            if overwrite_file:\n                if not disable_md5:\n                    if scp_transfer.compare_md5():\n                        return nottransferred_but_verified\n                    else:\n                        # File exists, you can overwrite it, MD5 is wrong (transfer file)\n                        verifyspace_and_transferfile(scp_transfer)\n                        if scp_transfer.compare_md5():\n                            return transferred_and_verified\n                        else:\n                            raise ValueError(\n                                \"MD5 failure between source and destination files\"\n                            )\n                else:\n                    # File exists, you can overwrite it, but MD5 not allowed (transfer file)\n                    verifyspace_and_transferfile(scp_transfer)\n                    return transferred_and_notverified\n            else:\n                # File exists, but you can't overwrite it.\n                if not disable_md5:\n                    if scp_transfer.compare_md5():\n                        return nottransferred_but_verified\n                msg = \"File already exists and overwrite_file is disabled\"\n                raise ValueError(msg)\n        else:\n            verifyspace_and_transferfile(scp_transfer)\n            # File doesn't exist\n            if not disable_md5:\n                if scp_transfer.compare_md5():\n                    return transferred_and_verified\n                else:\n                    raise ValueError(\"MD5 failure between source and destination files\")\n            else:\n                return transferred_and_notverified",
        "rewrite": "\n\n\ndef file-transfer(ssh_conn, source-file, dest-file,.fs=None,direction=\"put\",disable-mds=False,intnl_trnsfr=False,vrwrtr_fl=False):\n  ret_val= {'fl_exsts':bool,'fl_trndfrd':bool,'fl_vrfld':bool}\n  trnsfrd_nd_vrfd= {'fl_exsts':True,'fl_trndfrd':True,'fl_vrfld'=True}\n  trnsfrd_nd_ntvrfd={'fl_exsts':True,'fl_trndfrd':True,'fl"
    },
    {
        "original": "def move_stoploss(self, stoploss):\n        \"\"\"Modify stop order.\n        Auto-discover **orderId** and **quantity** and invokes ``self.modify_order(...)``.\n\n        :Parameters:\n            stoploss : float\n                the new stoploss limit price\n\n        \"\"\"\n        stopOrder = self.get_active_order(order_type=\"STOP\")\n\n        if stopOrder is not None and \"orderId\" in stopOrder.keys():\n            self.modify_order(orderId=stopOrder['orderId'],\n                              quantity=stopOrder['quantity'], limit_price=stoploss)",
        "rewrite": "\n\n\ndef move_stoploss(self, stoploss):\n    stop_order = self.get_active_order(order_type=\"STOP\")\n    if stop_order and \"orderId\" in stop_order:\n        self.modify_order(\n            orderId=stop_order[\"orderId\"],\n            quantity=stop_order[\"quantity\"],\n            limit_price=stoploss\n        )\n"
    },
    {
        "original": "def get_comments(self, since=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/comments <http://developer.github.com/v3/issues/comments>`_\n        :param since: datetime.datetime format YYYY-MM-DDTHH:MM:SSZ\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.IssueComment.IssueComment`\n        \"\"\"\n        assert since is github.GithubObject.NotSet or isinstance(since, datetime.datetime), since\n        url_parameters = dict()\n        if since is not github.GithubObject.NotSet:\n            url_parameters[\"since\"] = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        return github.PaginatedList.PaginatedList(\n            github.IssueComment.IssueComment,\n            self._requester,\n            self.url + \"/comments\",\n            url_parameters\n        )",
        "rewrite": "\n\n\ndef get_comments(self, since=github.GithubObject.NotSet):\n    assert since is github.GithubObject.NotSet or isinstance(since, datetime.datetime), since\n    url_parameters = {}\n    if since is not github.GithubObject.NotSet:\n        url_parameters[\"since\"] = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    return github.PaginatedList.PaginatedList(\n        github.IssueComment.IssueComment,\n        self._requester,\n        f\"{self.url}/comments\",\n        url_parameters\n    )\n"
    },
    {
        "original": "def open(self):\n        \"\"\"Start. Multiple calls have no effect.\n\n        Not safe to call from multiple threads at once.\n        \"\"\"\n        with self._lock:\n            if self._thread_will_exit:\n                # If the background thread has read self._stopped as True\n                # there is a chance that it has not yet exited. The call to\n                # join should not block indefinitely because there is no\n                # other work done outside the while loop in self._run.\n                try:\n                    self._thread.join()\n                except ReferenceError:\n                    # Thread terminated.\n                    pass\n            self._thread_will_exit = False\n            self._stopped = False\n        started = False\n        try:\n            started = self._thread and self._thread.is_alive()\n        except ReferenceError:\n            # Thread terminated.\n            pass\n\n        if not started:\n            thread = threading.Thread(target=self._run, name=self._name)\n            thread.daemon = True\n            self._thread = weakref.proxy(thread)\n            _register_executor(self)\n            thread.start()",
        "rewrite": "\n\n\ndef open(self):\n    with self._lock:\n        if self._thread_will_exit:\n            try:\n                self._thread.join()\n            except ReferenceError:\n                pass\n        self._thread_will_exit = False\n        self._stopped = False\n\n    started = False\n    try:\n        started = self._thread and self._thread.is_alive()\n    except ReferenceError:\n        pass\n\n    if not started:\n        thread = threading.Thread(target=self._run, name=self._name)\n        thread.daemon = True\n        self._thread = weakref.proxy(thread)\n       "
    },
    {
        "original": "def list_filepaths(self, wildcard=None):\n        \"\"\"\n        Return the list of absolute filepaths in the directory.\n\n        Args:\n            wildcard: String of tokens separated by \"|\". Each token represents a pattern.\n                If wildcard is not None, we return only those files that match the given shell pattern (uses fnmatch).\n                Example:\n                  wildcard=\"*.nc|*.pdf\" selects only those files that end with .nc or .pdf\n        \"\"\"\n        # Select the files in the directory.\n        fnames = [f for f in os.listdir(self.path)]\n        filepaths = filter(os.path.isfile, [os.path.join(self.path, f) for f in fnames])\n\n        # Filter using the shell patterns.\n        if wildcard is not None:\n            filepaths = WildCard(wildcard).filter(filepaths)\n\n        return filepaths",
        "rewrite": "\n\n\nimport os\nimport fnmatch\n\nclass WildCard:\n    def __init__(self, pattern):\n        self.patterns = [p.strip() for p in pattern.split(\"|\")]\n\n    def filter(self, paths):\n       return [path for path in paths if any(fnmatch.fnmatch(path.lower(), p.lower()) for p in self.patterns)]\n\nclass DirManager:\n    def __init__(self, path):\n       self.path = path\n\n    def list_filepaths(self, wildcard=None):\n       fnames = [f for f in os.listdir(self.path)]\n       filepaths = filter(os.path"
    },
    {
        "original": "def _fulfillment_from_details(data, _depth=0):\n    \"\"\"Load a fulfillment for a signing spec dictionary\n\n    Args:\n        data: tx.output[].condition.details dictionary\n    \"\"\"\n    if _depth == 100:\n        raise ThresholdTooDeep()\n\n    if data['type'] == 'ed25519-sha-256':\n        public_key = base58.b58decode(data['public_key'])\n        return Ed25519Sha256(public_key=public_key)\n\n    if data['type'] == 'threshold-sha-256':\n        threshold = ThresholdSha256(data['threshold'])\n        for cond in data['subconditions']:\n            cond = _fulfillment_from_details(cond, _depth+1)\n            threshold.add_subfulfillment(cond)\n        return threshold\n\n    raise UnsupportedTypeError(data.get('type'))",
        "rewrite": "\n\n\ndef _fulfillment_from_details(data, _depth=0):\n    if _depth == 100:\n        raise ThresholdTooDeep()\n\n    fulfillment_map = {\n        'ed25519-sha-256': lambda: Ed25519Sha256(public_key=base58.b58decode(data['public_key'])),\n        'threshold-sha-256': lambda: threshold_sha256_from_data(data, _depth)\n    }\n\n    return fulfillment_map.get(data['type'], lambda: unsupported_type_error(data))( )\n\ndef threshold_sha256_from_data(data, _depth):\n    threshold = ThresholdSha256"
    },
    {
        "original": "def merge(self, base, head, commit_message=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `POST /repos/:owner/:repo/merges <http://developer.github.com/v3/repos/merging>`_\n        :param base: string\n        :param head: string\n        :param commit_message: string\n        :rtype: :class:`github.Commit.Commit`\n        \"\"\"\n        assert isinstance(base, (str, unicode)), base\n        assert isinstance(head, (str, unicode)), head\n        assert commit_message is github.GithubObject.NotSet or isinstance(commit_message, (str, unicode)), commit_message\n        post_parameters = {\n            \"base\": base,\n            \"head\": head,\n        }\n        if commit_message is not github.GithubObject.NotSet:\n            post_parameters[\"commit_message\"] = commit_message\n        headers, data = self._requester.requestJsonAndCheck(\n            \"POST\",\n            self.url + \"/merges\",\n            input=post_parameters\n        )\n        if data is None:\n            return None\n        else:\n            return github.Commit.Commit(self._requester, headers, data, completed=True)",
        "rewrite": "\n\n\ndef merge(self, base: str, head: str, commit_message=github.GithubObject.NotSet) -> 'Commit':\n    assert isinstance(base, str), base\n    assert isinstance(head, str), head\n    assert commit_message is github.GithubObject.NotSet or isinstance(commit_message , str), commit_MESSAGE\n    \n    post_parameters = {\"base\": base,\"head\": head}\n    \n    if commit_MESSAGE is not github.GithubObject.NotSet:\n    \tpost_parameters[\"commit_MESSAGE\"] = commit_MESSAGE\n    \n    headers ,data=self._requester.requestJsonAndCheck(\"POST\",self.url"
    },
    {
        "original": "def automatic_gamma_density(structure, kppa):\n        \"\"\"\n        Returns an automatic Kpoint object based on a structure and a kpoint\n        density. Uses Gamma centered meshes always. For GW.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n\n        Args:\n            structure:\n                Input structure\n            kppa:\n                Grid density\n        \"\"\"\n\n        latt = structure.lattice\n        lengths = latt.abc\n        ngrid = kppa / structure.num_sites\n\n        mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n        num_div = [int(round(mult / l)) for l in lengths]\n\n        # ensure that numDiv[i] > 0\n        num_div = [i if i > 0 else 1 for i in num_div]\n\n        # VASP documentation recommends to use even grids for n <= 8 and odd\n        # grids for n > 8.\n        num_div = [i + i % 2 if i <= 8 else i - i % 2 + 1 for i in num_div]\n\n        style = Kpoints.supported_modes.Gamma\n\n        comment = \"pymatgen 4.7.6+ generated KPOINTS with grid density = \" + \\\n                  \"{} / atom\".format(kppa)\n        num_kpts = 0\n        return Kpoints(comment, num_kpts, style, [num_div], [0, 0, 0])",
        "rewrite": "\n\n\ndef automatic_gamma_density(structure, kppa):\n    latt = structure.lattice\n    lengths = latt.abc\n    ngrid = kppa / structure.num_sites\n    \n    mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n    num_div = [max(1, int(round(mult / l))) for l in lengths]\n    \n    if any(i <= 8 for i in num_div):\n       even_mask= [(i%2 ==0) for i in num_div]\n      "
    },
    {
        "original": "def unprotect(self, **kwargs):\n        \"\"\"Unprotect the branch.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProtectError: If the branch could not be unprotected\n        \"\"\"\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/unprotect' % (self.manager.path, id)\n        self.manager.gitlab.http_put(path, **kwargs)\n        self._attrs['protected'] = False",
        "rewrite": "\n\n\ndef unprotect(self, **, *):\n    id = self.get_id().replace('/', '%2F')\n    path = f\"{self.manager.path}/{id}/unprotect\"\n    self.manager.gitlab.http_put(path, **)\n    self._attrs[\"protected\"] = False\n"
    },
    {
        "original": "def parse_cache_control_header(value, on_update=None, cls=None):\n    \"\"\"Parse a cache control header.  The RFC differs between response and\n    request cache control, this method does not.  It's your responsibility\n    to not use the wrong control statements.\n\n    .. versionadded:: 0.5\n       The `cls` was added.  If not specified an immutable\n       :class:`~werkzeug.datastructures.RequestCacheControl` is returned.\n\n    :param value: a cache control header to be parsed.\n    :param on_update: an optional callable that is called every time a value\n                      on the :class:`~werkzeug.datastructures.CacheControl`\n                      object is changed.\n    :param cls: the class for the returned object.  By default\n                :class:`~werkzeug.datastructures.RequestCacheControl` is used.\n    :return: a `cls` object.\n    \"\"\"\n    if cls is None:\n        cls = RequestCacheControl\n    if not value:\n        return cls(None, on_update)\n    return cls(parse_dict_header(value), on_update)",
        "rewrite": "Here's the revised code:\n\n\ndef parse_cache_control_header(value, on_update=None, cls=None):\n    if cls is None:\n        cls = RequestCacheControl\n    if not value:\n        return cls(None, on_update)\n    \n    parts = []\n    \n    for item in map(lambda x: x.strip(), value.split(',')):\n        kv = item.split('=', 1)\n        k = kv[0].strip()\n        v = kv[1].strip() if len(kv) > 1 else None\n        \n        parts.append((k.lower(), v))\n        \nENOQUOTE_slash_open_delimiter deselect\n    \nend_del"
    },
    {
        "original": "def buckets(bucket=None, account=None, matched=False, kdenied=False,\n            errors=False, dbpath=None, size=None, denied=False,\n            format=None, incomplete=False, oversize=False, region=(),\n            not_region=(), inventory=None, output=None, config=None, sort=None,\n            tagprefix=None, not_bucket=None):\n    \"\"\"Report on stats by bucket\"\"\"\n\n    d = db.db(dbpath)\n\n    if tagprefix and not config:\n        raise ValueError(\n            \"account tag value inclusion requires account config file\")\n\n    if config and tagprefix:\n        with open(config) as fh:\n            data = json.load(fh).get('accounts')\n            account_data = {}\n            for a in data:\n                for t in a['tags']:\n                    if t.startswith(tagprefix):\n                        account_data[a['name']] = t[len(tagprefix):]\n\n    buckets = []\n    for b in sorted(d.buckets(account),\n                    key=operator.attrgetter('bucket_id')):\n        if bucket and b.name not in bucket:\n            continue\n        if not_bucket and b.name in not_bucket:\n            continue\n        if matched and not b.matched:\n            continue\n        if kdenied and not b.keys_denied:\n            continue\n        if errors and not b.error_count:\n            continue\n        if size and b.size < size:\n            continue\n        if inventory and not b.using_inventory:\n            continue\n        if denied and not b.denied:\n            continue\n        if oversize and b.scanned <= b.size:\n            continue\n        if incomplete and b.percent_scanned >= incomplete:\n            continue\n        if region and b.region not in region:\n            continue\n        if not_region and b.region in not_region:\n            continue\n        if tagprefix:\n            setattr(b, tagprefix[:-1], account_data[b.account])\n        buckets.append(b)\n\n    if sort:\n        key = operator.attrgetter(sort)\n        buckets = list(reversed(sorted(buckets, key=key)))\n    formatter = format == 'csv' and format_csv or format_plain\n    keys = tagprefix and (tagprefix[:-1],) or ()\n    formatter(buckets, output, keys=keys)",
        "rewrite": "\n\n\ndef report_buckets(\n    bucket=None,\n    account=None,\n    matched=False,\n    kdenied=False,\n    errors=False,\n    dbpath='/var/db',\n    size=0,\n    denied=False,\n    format='plain',\n    incomplete=True,\n    oversize=True,\n   region=[],\n  \tnot_region=[],\n  \tinventory=True,\n  \toutput=sys.stdout,\n\tconfiggne_file='/etc/config.json',\n\tsort='-size'\n):\nudy,data' insistuig_uids flvlhe associates 's \u0648\u062ch`\"]\nbher kontrollel acmes her kvUr vit Cnttrol"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'name') and self.name is not None:\n            _dict['name'] = self.name\n        if hasattr(self, 'classifier_id') and self.classifier_id is not None:\n            _dict['classifier_id'] = self.classifier_id\n        if hasattr(self, 'classes') and self.classes is not None:\n            _dict['classes'] = [x._to_dict() for x in self.classes]\n        return _dict",
        "rewrite": "\ndef to_dict(self):\n    return {\n        'name': getattr(self, 'name', None),\n        'classifier_id': getattr(self, 'classifier_id', None),\n        **: [x.to_dict() for x in getattr(self, 'classes', []) if x]\n    }\n"
    },
    {
        "original": "def frame_msg(body, header=None, raw_body=False):  # pylint: disable=unused-argument\n    \"\"\"\n    Frame the given message with our wire protocol\n    \"\"\"\n    framed_msg = {}\n    if header is None:\n        header = {}\n\n    framed_msg['head'] = header\n    framed_msg['body'] = body\n    return salt.utils.msgpack.dumps(framed_msg)",
        "rewrite": "\ndef frame_msg(body, header=None, raw_body=False):\n    framed_msg = {'head': header or {}, 'body': body}\n    return salt.utils.msgpack.dumps(framed_msg)\n"
    },
    {
        "original": "def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a\n        dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })",
        "rewrite": "\n\n\ndef _get_matrix(self):\n    return {\n        s.name: {k: getattr(s, f\"{k}_sequence\") for k in (\n            \"check\", \"cleanup\", \"converge\", \"create\", \"dependency\",\n            \"destroy\", \"idempotence\", \"lint\", \"prepare\",\n            \"side_effect\", \"syntax\", \"test\", \"verify\"\n        )}\n        for s in self.all\n    }\n"
    },
    {
        "original": "def set_tcp_md5sig(s, addr, key):\n    \"\"\"Enable TCP-MD5 on the given socket.\n\n    :param s: Socket\n    :param addr: Associated address.  On some platforms, this has no effect.\n    :param key: Key.  On some platforms, this has no effect.\n    \"\"\"\n    impls = {\n        'FreeBSD': _set_tcp_md5sig_bsd,\n        'Linux': _set_tcp_md5sig_linux,\n        'NetBSD': _set_tcp_md5sig_bsd,\n    }\n    system = platform.system()\n    try:\n        impl = impls[system]\n    except KeyError:\n        raise NotImplementedError(\"TCP-MD5 unsupported on this platform\")\n    impl(s, addr, key)",
        "rewrite": "\n\n\ndef set_tcp_md5sig(s, addr, key):\n    impls = {\n        'FreeBSD': _set_tcp_md5sig_bsd,\n        'Linux': _set_tcp_md5sig_linux,\n        'NetBSD': _set_tcp_md5sig_bsd,\n        'Darwin': lambda s, a, k: None  # macOS doesn't support TCP-MD5\n    }\n    \n    system = platform.system()\n    \n    if system not in impls:\n        raise NotImplementedError(\"TCP-MD5 unsupported on this platform\")\n    \n    impls[system](s"
    },
    {
        "original": "def _jobs():\n    \"\"\"\n    Return the currently configured jobs.\n    \"\"\"\n    response = salt.utils.http.query(\n        \"{0}/scheduler/jobs\".format(_base_url()),\n        decode_type='json',\n        decode=True,\n    )\n    jobs = {}\n    for job in response['dict']:\n        jobs[job.pop('name')] = job\n    return jobs",
        "rewrite": "\n\n\ndef _jobs():\n    response = salt.utils.http.query(\n        \"{0}/scheduler/jobs\".format(_base_url()),\n        decode_type='json',\n        decode=True,\n    )\n    return {job.pop('name'): job for job in response['dict']}\n"
    },
    {
        "original": "def get_special_folder(self, name):\n        \"\"\" Returns the specified Special Folder\n\n        :return: a special Folder\n        :rtype: drive.Folder\n        \"\"\"\n\n        name = name if \\\n            isinstance(name, OneDriveWellKnowFolderNames) \\\n            else OneDriveWellKnowFolderNames(name.lower())\n        name = name.value\n\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('get_special').format(id=self.object_id,\n                                                          name=name))\n        else:\n            # we don't know the drive_id so go to the default\n            url = self.build_url(\n                self._endpoints.get('get_special_default').format(name=name))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self._classifier(data)(parent=self,\n                                      **{self._cloud_data_key: data})",
        "rewrite": "\n\n\ndef get_special_folder(self, name):\n    if not isinstance(name, OneDriveWellKnowFolderNames):\n        name = OneDriveWellKnowFolderNames(name.lower())\n    name = name.value\n    \n    url_template = self._endpoints.get('get_special') if self.object_id else self._endpoints.get('get_special_default')\n    url = self.build_url(url_template.format(id=self.object_id, name=name) if self.object_id else url_template.format(name=name))\n    \n    response = self.con.get(url)\n    if not response:\n        return None\n    \n    data = response.json()\n    \n"
    },
    {
        "original": "def hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      404:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(404)\n    return jsonify(authenticated=True, user=user)",
        "rewrite": "\n\n\nfrom flask import jsonify\n\ndef hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \n    if not check_basic_auth(user, passwd):\n        return {\"error\": \"Unauthorized\"}, 401\n    \n    return jsonify({\"authenticated\": True, \"user\": user}), 200\n"
    },
    {
        "original": "def get_noisy_gate(gate_name, params):\n    \"\"\"\n    Look up the numerical gate representation and a proposed 'noisy' name.\n\n    :param str gate_name: The Quil gate name\n    :param Tuple[float] params: The gate parameters.\n    :return: A tuple (matrix, noisy_name) with the representation of the ideal gate matrix\n        and a proposed name for the noisy version.\n    :rtype: Tuple[np.array, str]\n    \"\"\"\n    params = tuple(params)\n    if gate_name == \"I\":\n        assert params == ()\n        return np.eye(2), \"NOISY-I\"\n    if gate_name == \"RX\":\n        angle, = params\n        if np.isclose(angle, np.pi / 2, atol=ANGLE_TOLERANCE):\n            return (np.array([[1, -1j],\n                              [-1j, 1]]) / np.sqrt(2),\n                    \"NOISY-RX-PLUS-90\")\n        elif np.isclose(angle, -np.pi / 2, atol=ANGLE_TOLERANCE):\n            return (np.array([[1, 1j],\n                              [1j, 1]]) / np.sqrt(2),\n                    \"NOISY-RX-MINUS-90\")\n        elif np.isclose(angle, np.pi, atol=ANGLE_TOLERANCE):\n            return (np.array([[0, -1j],\n                              [-1j, 0]]),\n                    \"NOISY-RX-PLUS-180\")\n        elif np.isclose(angle, -np.pi, atol=ANGLE_TOLERANCE):\n            return (np.array([[0, 1j],\n                              [1j, 0]]),\n                    \"NOISY-RX-MINUS-180\")\n    elif gate_name == \"CZ\":\n        assert params == ()\n        return np.diag([1, 1, 1, -1]), \"NOISY-CZ\"\n    raise NoisyGateUndefined(\"Undefined gate and params: {}{}\\n\"\n                             \"Please restrict yourself to I, RX(+/-pi), RX(+/-pi/2), CZ\"\n                             .format(gate_name, params))",
        "rewrite": "\n\n\nimport numpy as np\nfrom typing import Tuple\n\nANGLE_TOLERANCE = 0.001\n\nclass NoisyGateUndefined(Exception):\n    pass\n\ndef get_noisy_gate(gate_name: str ,params: Tuple[float]) -> Tuple[np.ndarray,str]:\n    \n     if gate_name == \"I\":\n         assert len(params) == 0 \n         return np.eye(2), f\"NOISY-{gate_name}\"\n     elif gate_name == \"RX\":\n         angle = params[0]\n         if abs(angle - np.pi / 2) < ANGLE_TOLERANCE:\n"
    },
    {
        "original": "def load(self, config):\n        \"\"\"Load the server list from the configuration file.\"\"\"\n        server_list = []\n\n        if config is None:\n            logger.debug(\"No configuration file available. Cannot load server list.\")\n        elif not config.has_section(self._section):\n            logger.warning(\"No [%s] section in the configuration file. Cannot load server list.\" % self._section)\n        else:\n            logger.info(\"Start reading the [%s] section in the configuration file\" % self._section)\n            for i in range(1, 256):\n                new_server = {}\n                postfix = 'server_%s_' % str(i)\n                # Read the server name (mandatory)\n                for s in ['name', 'port', 'alias']:\n                    new_server[s] = config.get_value(self._section, '%s%s' % (postfix, s))\n                if new_server['name'] is not None:\n                    # Manage optionnal information\n                    if new_server['port'] is None:\n                        new_server['port'] = '61209'\n                    new_server['username'] = 'glances'\n                    # By default, try empty (aka no) password\n                    new_server['password'] = ''\n                    try:\n                        new_server['ip'] = gethostbyname(new_server['name'])\n                    except gaierror as e:\n                        logger.error(\"Cannot get IP address for server %s (%s)\" % (new_server['name'], e))\n                        continue\n                    new_server['key'] = new_server['name'] + ':' + new_server['port']\n\n                    # Default status is 'UNKNOWN'\n                    new_server['status'] = 'UNKNOWN'\n\n                    # Server type is 'STATIC'\n                    new_server['type'] = 'STATIC'\n\n                    # Add the server to the list\n                    logger.debug(\"Add server %s to the static list\" % new_server['name'])\n                    server_list.append(new_server)\n\n            # Server list loaded\n            logger.info(\"%s server(s) loaded from the configuration file\" % len(server_list))\n            logger.debug(\"Static server list: %s\" % server_list)\n\n        return server_list",
        "rewrite": "Here's a revised version of your code:\n\n\ndef load(self, config):\n    if not config or not hasattr(config, '_section'):\n        return []\n    \n    section_name = self._section\n\n    if section_name not in config.sections():\n        return []\n\n    servers_configured_count = 0\n\n    servers_info_collection= []\n\n    for i in range(1, 256):\n        \n        postfix= f'server_{i}_'\n\n        \nfor mandatory_keys_simple_chunked_iterationer001pxe:[\"in2ame\",\"por6t6\",\"aliasbus\"]:\n            \n           temp_val_holder=config.get_value(section_name,'%dp"
    },
    {
        "original": "def creep_data(data_set='creep_rupture'):\n    \"\"\"Brun and Yoshida's metal creep rupture data.\"\"\"\n    if not data_available(data_set):\n        download_data(data_set)\n        path = os.path.join(data_path, data_set)\n        tar_file = os.path.join(path, 'creeprupt.tar')\n        tar = tarfile.open(tar_file)\n        print('Extracting file.')\n        tar.extractall(path=path)\n        tar.close()\n    all_data = np.loadtxt(os.path.join(data_path, data_set, 'taka'))\n    y = all_data[:, 1:2].copy()\n    features = [0]\n    features.extend(range(2, 31))\n    X = all_data[:, features].copy()\n    return data_details_return({'X': X, 'y': y}, data_set)",
        "rewrite": "\n\n\nimport os\nimport numpy as np\nfrom tempfile import TemporaryDirectory\nfrom pathlib import Path\nfrom tarfile import TarFile\n\ndef creep_data(data_set='creep_rupture'):\n    if not data_available(data_set):\n        download_data(data_set)\n        \n    with TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir) / data_set\n        tar_file = path / 'creeprupt.tar'\n        \n        with TarFile(tar_file) as tf:\n            print('Extracting file.')\n            tf.extractall(path=path)\n            \n    all_data = np.loadtxt(("
    },
    {
        "original": "def exit_enable_mode(self, exit_command=\"exit\"):\n        \"\"\"Exit enable mode.\"\"\"\n        delay_factor = self.select_delay_factor(delay_factor=0)\n        output = \"\"\n        if self.check_enable_mode():\n            self.write_channel(self.normalize_cmd(exit_command))\n            time.sleep(0.3 * delay_factor)\n            self.set_base_prompt()\n            if self.check_enable_mode():\n                raise ValueError(\"Failed to exit enable mode.\")\n        return output",
        "rewrite": "\ndef exit_enable_mode(self, exit_command=\"exit\"):\n    delay_factor = self.select_delay_factor(delay_factor=0)\n    if self.check_enable_mode():\n        self.write_channel(self.normalize_cmd(exit_command))\n        time.sleep(0.3 * delay_factor)\n        self.set_base_prompt()\n    if self.check_enable_mode():\n        raise ValueError(\"Failed to exit enable mode.\")\n"
    },
    {
        "original": "def create_baseline(tag=\"baseline\", config='root'):\n    \"\"\"\n    Creates a snapshot marked as baseline\n\n    tag\n        Tag name for the baseline\n\n    config\n        Configuration name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' snapper.create_baseline\n        salt '*' snapper.create_baseline my_custom_baseline\n    \"\"\"\n    return __salt__['snapper.create_snapshot'](config=config,\n                                               snapshot_type='single',\n                                               description=\"baseline snapshot\",\n                                               cleanup_algorithm=\"number\",\n                                               userdata={\"baseline_tag\": tag})",
        "rewrite": "\ndef create_baseline(tag=\"baseline\", config='root'):\n    return __salt__['snapper.create_snapshot'](config, 'single', \"baseline snapshot\", \"number\", {\"baseline_tag\": tag})\n"
    },
    {
        "original": "def catch_game_end(func):\n  \"\"\"Decorator to handle 'Game has already ended' exceptions.\"\"\"\n  @functools.wraps(func)\n  def _catch_game_end(self, *args, **kwargs):\n    ",
        "rewrite": "\n\n\nimport functools\n\ndef catch_game_end(func):\n    @functools.wraps(func)\n    def _catch_game_end(self, *args, **kwargs):\n        try:\n            return func(self, *args, **kwargs)\n        except GameOverError:\n            pass\n    return _catch_game_end\n\n\nNote: I assumed that `GameOverError` is the exception type raised when the game has already ended. You may need to replace this with the actual exception type used in your code."
    },
    {
        "original": "def publish_alias(self, func_data, alias):\n        \"\"\"Create or update an alias for the given function.\n        \"\"\"\n        if not alias:\n            return func_data['FunctionArn']\n        func_name = func_data['FunctionName']\n        func_version = func_data['Version']\n\n        exists = resource_exists(\n            self.client.get_alias, FunctionName=func_name, Name=alias)\n\n        if not exists:\n            log.debug(\"Publishing custodian lambda alias %s\", alias)\n            alias_result = self.client.create_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        else:\n            if (exists['FunctionVersion'] == func_version and\n                    exists['Name'] == alias):\n                return exists['AliasArn']\n            log.debug('Updating custodian lambda alias %s', alias)\n            alias_result = self.client.update_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        return alias_result['AliasArn']",
        "rewrite": "\n\n\ndef publish_alias(self, func_data, alias):\n    if not alias:\n        return func_data['FunctionArn']\n    func_name = func_data['FunctionName']\n    func_version = func_data['Version']\n\n    exists = resource_exists(self.client.get_alias, FunctionName=func_name, Name=alias)\n\n    if not exists:\n        log.debug(f\"Publishing custodian lambda alias {alias}\")\n       (alias_result := {\n            'AliasArn': self.client.create_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)['AliasArn']\n        })\n"
    },
    {
        "original": "def remove_send_last_message(self, connection):\n        \"\"\"Removes a send_last_message function previously registered\n        with the Dispatcher.\n\n        Args:\n            connection (str): A locally unique identifier provided\n                by the receiver of messages.\n        \"\"\"\n        if connection in self._send_last_message:\n            del self._send_last_message[connection]\n            LOGGER.debug(\"Removed send_last_message function \"\n                         \"for connection %s\", connection)\n        else:\n            LOGGER.warning(\"Attempted to remove send_last_message \"\n                           \"function for connection %s, but no \"\n                           \"send_last_message function was registered\",\n                           connection)",
        "rewrite": "\n\n\ndef remove_send_last_message(self, connection):\n    if connection in self._send_last_message:\n        del self._send_last_message[connection]\n        LOGGER.debug(f\"Removed send_last_message function for connection {connection}\")\n    else:\n        LOGGER.warning(f\"Attempted to remove send_last_message function for connection {connection}, but no send_last_messages function was registered\")\n"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict",
        "rewrite": "\ndef to_dict(self):\n    return {\n        'label': self.label if hasattr(self, 'label') and self.label is not None else None,\n        'provenance_ids': self.provenance_ids if hasattr(self, 'provenance_ids') and self.provenance_ids is not None else None\n    }\n"
    },
    {
        "original": "def identical_blocks(self):\n        \"\"\"\n        :return A list of all block matches that appear to be identical\n        \"\"\"\n        identical_blocks = []\n        for (func_a, func_b) in self.function_matches:\n            identical_blocks.extend(self.get_function_diff(func_a, func_b).identical_blocks)\n        return identical_blocks",
        "rewrite": "\ndef identical_blocks(self):\n    return [block for func_a, func_b in self.function_matches \n            for block in self.get_function_diff(func_a, func_b).identical_blocks]\n"
    },
    {
        "original": "def Validate(self, type_names):\n    \"\"\"Filtered types need to be RDFValues.\"\"\"\n    errs = [n for n in self._RDFTypes(type_names) if not self._GetClass(n)]\n    if errs:\n      raise DefinitionError(\"Undefined RDF Types: %s\" % \",\".join(errs))",
        "rewrite": "\n\n\ndef validate(self, type_names):\n    errs = [n for n in self._RDFTypes(type_names) if not self._GetClass(n)]\n    if errs:\n        raise DefinitionError(\"Undefined RDF Types: \" + \", \".join(errs))\n"
    },
    {
        "original": "def set_syslog_server(server=None, type=\"primary\"):\n    \"\"\"\n    Set the SYSLOG server on the host.\n\n    Args:\n        server(str): The hostname or IP address of the SYSLOG server.\n\n        type(str): Specifies the type of SYSLOG server. This can either be primary (default) or secondary.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cimc.set_syslog_server foo.bar.com\n\n        salt '*' cimc.set_syslog_server foo.bar.com primary\n\n        salt '*' cimc.set_syslog_server foo.bar.com secondary\n\n    \"\"\"\n\n    if not server:\n        raise salt.exceptions.CommandExecutionError(\"The SYSLOG server must be specified.\")\n\n    if type == \"primary\":\n        dn = \"sys/svc-ext/syslog/client-primary\"\n        inconfig = ",
        "rewrite": "Here is the revised code with some improvements:\n\n\ndef set_syslog_server(server, type=\"primary\"):\n    if not server:\n        raise ValueError(\"The SYSLOG server must be specified.\")\n    \n    if type not in (\"primary\", \"secondary\"):\n        raise ValueError(\"Invalid syslog server type. It must be either 'primary' or 'secondary'.\")\n        \n    if type == \"primary\":\n        dn = \"sys/svc-ext/syslog/client-primary\"\n"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'customer_id') and self.customer_id is not None:\n            _dict['customer_id'] = self.customer_id\n        if hasattr(self, 'document_type') and self.document_type is not None:\n            _dict['document_type'] = self.document_type\n        if hasattr(self, 'natural_language_query'\n                  ) and self.natural_language_query is not None:\n            _dict['natural_language_query'] = self.natural_language_query\n        if hasattr(self,\n                   'document_results') and self.document_results is not None:\n            _dict['document_results'] = self.document_results._to_dict()\n        if hasattr(self,\n                   'created_timestamp') and self.created_timestamp is not None:\n            _dict['created_timestamp'] = datetime_to_string(\n                self.created_timestamp)\n        if hasattr(self,\n                   'client_timestamp') and self.client_timestamp is not None:\n            _dict['client_timestamp'] = datetime_to_string(\n                self.client_timestamp)\n        if hasattr(self, 'query_id') and self.query_id is not None:\n            _dict['query_id'] = self.query_id\n        if hasattr(self, 'session_token') and self.session_token is not None:\n            _dict['session_token'] = self.session_token\n        if hasattr(self, 'collection_id') and self.collection_id is not None:\n            _dict['collection_id'] = self.collection_id\n        if hasattr(self, 'display_rank') and self.display_rank is not None:\n            _dict['display_rank'] = self.display_rank\n        if hasattr(self, 'document_id') and self.document_id is not None:\n            _dict['document_id'] = self.document_id\n        if hasattr(self, 'event_type') and self.event_type is not None:\n            _dict['event_type'] = self.event_type\n        if hasattr(self, 'result_type') and self.result_type is not None:\n            _dict['result_type'] = self.result_type\n        return _dict",
        "rewrite": "Here's the revised code:\n\n\ndef to_dict(self):\n    return {\n       attr: getattr/selfbane() \n       for attr in (\n           \"environment-id\", \"customer-id\", \"document-type\", \n           \"natural-language-query\", \"query-id\", \"session-token\",\n           \"collection-id\", \"display-rank\". document-id\". event-type\",\n           result-type\"\n       ) \n       + ((\"created-timestamp\". client-timestamp\"), \n          lambda x: datetime-to-string(getattr/self.x)))\n       + ((\"document-results\".), lambda x: getattr/self.x)._to-dict())\n       )\n    }\n"
    },
    {
        "original": "def check_config_mode(self, check_string=\")#\", pattern=\"#\"):\n        \"\"\"Checks if the device is in configuration mode or not.\"\"\"\n        return super(CiscoNxosSSH, self).check_config_mode(\n            check_string=check_string, pattern=pattern\n        )",
        "rewrite": "\ndef check_config_mode(self, check_string=\"#\" , pattern=\"#\"):\n    return super().check_config_mode(check_string, pattern)\n"
    },
    {
        "original": "def from_EV(E, V):\n        \"\"\"\n        Creates an instance of a Gamma Prior  by specifying the Expected value(s)\n        and Variance(s) of the distribution.\n\n        :param E: expected value\n        :param V: variance\n        \"\"\"\n        a = np.square(E) / V\n        b = E / V\n        return Gamma(a, b)",
        "rewrite": "\n\n\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef from_EV(E, V):\n    a = np.square(E) / V\n    b = E / V\n    return gamma(a, scale=1/b)\n"
    },
    {
        "original": "def run_command(self, commands, timeout_sec=None, exception=None):\n        \"\"\"\n        Executes the given commands and sends OVSDB messages.\n\n        ``commands`` must be a list of\n        :py:mod:`ryu.lib.ovs.vsctl.VSCtlCommand`.\n\n        If ``timeout_sec`` is specified, raises exception after the given\n        timeout [sec]. Additionally, if ``exception`` is specified, this\n        function will wraps exception using the given exception class.\n\n        Retruns ``None`` but fills ``result`` attribute for each command\n        instance.\n        \"\"\"\n        if timeout_sec is None:\n            self._run_command(commands)\n        else:\n            with hub.Timeout(timeout_sec, exception):\n                self._run_command(commands)",
        "rewrite": "\n\n\ndef run_command(self, commands, timeout_sec=None, exception=RuntimeError):\n    if timeout_sec is None:\n       return self._run_command(commands)\n    else:\n       with hub.Timeout(timeout_sec, exception):\n           return self._run_command(commands)\n"
    },
    {
        "original": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)",
        "rewrite": "Here is a revised version of the code:\n\n\ndef delete(self, oid: int | str, **kwargs) -> None:\n    if oid is None:\n        path = self.path\n    else:\n        oid = str(oid).replace('/', '%2F')\n        path = f\"{self.path}/{oid}\"\n    self.gitlab.http_delete(path, ** kwargs)\n"
    },
    {
        "original": "def get_edges(self):\n        \"\"\"\n        Returns the edges of the network\n\n        Examples\n        --------\n        >>> reader = XMLBIF.XMLBIFReader(\"xmlbif_test.xml\")\n        >>> reader.get_edges()\n        [['family-out', 'light-on'],\n         ['family-out', 'dog-out'],\n         ['bowel-problem', 'dog-out'],\n         ['dog-out', 'hear-bark']]\n        \"\"\"\n        edge_list = [[value, key] for key in self.variable_parents\n                     for value in self.variable_parents[key]]\n        return edge_list",
        "rewrite": "\ndef get_edges(self):\n    return [[value, key] for key in self.variable_parents for value in self.variable_parents[key]]\n"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"\n        Create a JSON-serializable representation of the ISA.\n\n        The dictionary representation is of the form::\n\n            {\n                \"1Q\": {\n                    \"0\": {\n                        \"type\": \"Xhalves\"\n                    },\n                    \"1\": {\n                        \"type\": \"Xhalves\",\n                        \"dead\": True\n                    },\n                    ...\n                },\n                \"2Q\": {\n                    \"1-4\": {\n                        \"type\": \"CZ\"\n                    },\n                    \"1-5\": {\n                        \"type\": \"CZ\"\n                    },\n                    ...\n                },\n                ...\n            }\n\n        :return: A dictionary representation of self.\n        :rtype: Dict[str, Any]\n        \"\"\"\n\n        def _maybe_configure(o, t):\n            # type: (Union[Qubit,Edge], str) -> dict\n            ",
        "rewrite": "Here is the revised code with some improvements:\n\n\ndef to_dict(self) -> Dict[str, Any]:\n    \"\"\"\n    Create a JSON-serializable representation of the ISA.\n\n    The dictionary representation is of the form:\n    {\n        \"<gate_size>Q\": {\n            \"<qubit_label>\"/\"<edge_label>\"/\"...\":\n                {\"type\": \"<gate_type>\", ...}\n            ...\n        }\n    }\n\n    Returns:\n    A dictionary representation of self.\n    \"\"\"\n\ndef _maybe_configure(o: Union[Qubit, Edge], t: str) -> dict:\n    if isinstance(o, Qubit):\n        return {\""
    },
    {
        "original": "def _sort_text(definition):\n    \"\"\" Ensure builtins appear at the bottom.\n    Description is of format <type>: <module>.<item>\n    \"\"\"\n    if definition.name.startswith(\"_\"):\n        # It's a 'hidden' func, put it next last\n        return 'z' + definition.name\n    elif definition.scope == 'builtin':\n        return 'y' + definition.name\n\n    # Else put it at the front\n    return 'a' + definition.name",
        "rewrite": "\n\n\ndef _sort_text(definition):\n    if definition.name.startswith(\"_\"):\n        return 'z' + definition.name\n    elif definition.scope == 'builtin':\n        return 'y' + definition.name\n    else:\n        return '' + definition.name\n"
    },
    {
        "original": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)",
        "rewrite": "\n\n\ndef scale(self, replicas):\n    if 'Global' in self.attrs['Spec']['Mode'].keys():\n        raise InvalidArgument('Cannot scale a global container')\n    service_mode = ServiceMode('replicated', replicas)\n    return self.client.api.update_service(self.id, self.version, mode=service_mode, fetch_current_spec=True)"
    },
    {
        "original": "def register_switch_address(addr, interval=None):\n    \"\"\"\n    Registers a new address to initiate connection to switch.\n\n    Registers a new IP address and port pair of switch to let\n    ryu.controller.controller.OpenFlowController to try to initiate\n    connection to switch.\n\n    :param addr: A tuple of (host, port) pair of switch.\n    :param interval: Interval in seconds to try to connect to switch\n    \"\"\"\n    assert len(addr) == 2\n    assert ip.valid_ipv4(addr[0]) or ip.valid_ipv6(addr[0])\n    ofp_handler = app_manager.lookup_service_brick(ofp_event.NAME)\n    _TMP_ADDRESSES[addr] = interval\n\n    def _retry_loop():\n        # Delays registration if ofp_handler is not started yet\n        while True:\n            if ofp_handler.controller is not None:\n                for a, i in _TMP_ADDRESSES.items():\n                    ofp_handler.controller.spawn_client_loop(a, i)\n                    hub.sleep(1)\n                break\n            hub.sleep(1)\n\n    hub.spawn(_retry_loop)",
        "rewrite": "\n\n\ndef register_switch_address(addr, interval=None):\n    assert len(addr) == 2\n    assert ip.valid_ipv4(addr[0]) or ip.valid_ipv6(addr[0])\n    \n    _TMP_ADDRESSES[addr] = interval\n    \n    def _retry_loop():\n        while True:\n            if app_manager.lookup_service_brick(ofp_event.NAME).controller:\n                for a, i in list(_TMP_ADDRESSES.items()):\n                    app_manager.lookup_service_brick(ofp_event.NAME).controller.spawn_client_loop(a, i)\n                break\n            hub.sleep(1)\n    \n   "
    },
    {
        "original": "def neighbor_update(self, address, conf_type, conf_value):\n        \"\"\" This method changes the neighbor configuration.\n\n        ``address`` specifies the IP address of the peer.\n\n        ``conf_type`` specifies configuration type which you want to change.\n        Currently ryu.services.protocols.bgp.bgpspeaker.MULTI_EXIT_DISC\n        can be specified.\n\n        ``conf_value`` specifies value for the configuration type.\n        \"\"\"\n\n        assert conf_type == MULTI_EXIT_DISC or conf_type == CONNECT_MODE\n\n        func_name = 'neighbor.update'\n        attribute_param = {}\n        if conf_type == MULTI_EXIT_DISC:\n            attribute_param = {neighbors.MULTI_EXIT_DISC: conf_value}\n        elif conf_type == CONNECT_MODE:\n            attribute_param = {neighbors.CONNECT_MODE: conf_value}\n\n        param = {neighbors.IP_ADDRESS: address,\n                 neighbors.CHANGES: attribute_param}\n\n        call(func_name, **param)",
        "rewrite": "\n\n\ndef neighbor_update(self, address, conf_type, conf_value):\n    assert conf_type in (MULTI_EXIT_DISC, CONNECT_MODE)\n\n    param = {\n        neighbors.IP_ADDRESS: address,\n        neighbors.CHANGES: {\n            getattr(neighbors, conf_type.upper()): conf_value\n    }}\n\n    call('neighbor.update', param)\n"
    },
    {
        "original": "def save(self):\n        \"\"\" save the current state of the Kalman filter\"\"\"\n\n        kf = self._kf\n\n        # force all attributes to be computed. this is only necessary\n        # if the class uses properties that compute data only when\n        # accessed\n        for prop in self.properties:\n            self._DL[prop[0]].append(getattr(kf, prop[0]))\n\n        v = copy.deepcopy(kf.__dict__)\n\n        if self._skip_private:\n            for key in list(v.keys()):\n                if key.startswith('_'):\n                    print('deleting', key)\n                    del v[key]\n\n        if self._skip_callable:\n            for key in list(v.keys()):\n                if callable(v[key]):\n                    del v[key]\n\n        for ig in self._ignore:\n            if ig in v:\n                del v[ig]\n\n        for key in list(v.keys()):\n            self._DL[key].append(v[key])\n\n        self.__dict__.update(self._DL)\n        self._len += 1",
        "rewrite": "\n\n\ndef save(self):\n    kf = self._kf\n    \n    for prop, _ in self.properties:\n        getattr(kf, prop)\n    \n    v = copy.deepcopy(kf.__dict__)\n    \n    keys_to_delete = [key for key in v if (self._skip_private and key.startswith('_')) or (self._skip_callable and callable(v[key]))]\n    keys_to_delete.extend(self._ignore)\n    \n    for key in keys_to_delete:\n        del v[key]\n    \n    for key, value in v.items():\n         self._DL.setdefault(key, []).append(value)\n        \n    self"
    },
    {
        "original": "def build_lines_data(self, code_obj):\n        \"\"\"\n        Generate various line-related helper data.\n        \"\"\"\n\n        # Offset: lineno pairs, only for offsets which start line.\n        # Locally we use list for more convenient iteration using indices\n        if self.version > 1.4:\n            linestarts = list(self.opc.findlinestarts(code_obj))\n        else:\n            linestarts = [[0, 1]]\n        self.linestarts = dict(linestarts)\n\n        # 'List-map' which shows line number of current op and offset of\n        # first op on following line, given offset of op as index\n        lines = []\n        LineTuple = namedtuple('LineTuple', ['l_no', 'next'])\n\n        # Iterate through available linestarts, and fill\n        # the data for all code offsets encountered until\n        # last linestart offset\n        _, prev_line_no = linestarts[0]\n        offset = 0\n        for start_offset, line_no in linestarts[1:]:\n            while offset < start_offset:\n                lines.append(LineTuple(prev_line_no, start_offset))\n                offset += 1\n            prev_line_no = line_no\n\n        # Fill remaining offsets with reference to last line number\n        # and code length as start offset of following non-existing line\n        codelen = len(self.code)\n        while offset < codelen:\n            lines.append(LineTuple(prev_line_no, codelen))\n            offset += 1\n        return lines",
        "rewrite": "\ndef build_lines_data(self, code_obj):\n    if self.version > 1.4:\n       linestarts = list(self.opc.findlinestarts(code_obj))\n    else:\n       linestarts = [[0, 1]]\n    self.linestarts = dict(linestarts)\n\n    from collections import namedtuple\n\n    LineTuple = namedtuple('LineTuple', ['l_no', 'next'])\n    _, prev_line_no = linestarts[0]\n    lines, offset= [], 0\n\n    for start_offset, line_no in linestarts[1:]:\n       while offset < start_offset:\n          "
    },
    {
        "original": "def key_pair_from_ed25519_key(hex_private_key):\n    \"\"\"Generate base58 encode public-private key pair from a hex encoded private key\"\"\"\n    priv_key = crypto.Ed25519SigningKey(bytes.fromhex(hex_private_key)[:32], encoding='bytes')\n    public_key = priv_key.get_verifying_key()\n    return CryptoKeypair(private_key=priv_key.encode(encoding='base58').decode('utf-8'),\n                         public_key=public_key.encode(encoding='base58').decode('utf-8'))",
        "rewrite": "\n\n\ndef key_pair_from_ed25519_key(hex_private_key):\n    priv_Key = crypto.Ed25519SigningKey(bytes.fromhex(hex_private_KEY)[:32], encoding='bytes')\n    pub_Key = priv_Key.get_verifying_Key()\n    return CryptoKeypair(\n        private_Key=base64.b64encode(priv_Key.to_bytes()).decode('utf-8'),\n        public_Key=base64.b64encode(pub-Key.to_bytes()).decode('utf-8')\n    )\n"
    },
    {
        "original": "def get_plot(self, ylim=None, units=\"thz\"):\n        \"\"\"\n        Get a matplotlib object for the bandstructure plot.\n\n        Args:\n            ylim: Specify the y-axis (frequency) limits; by default None let\n                the code choose.\n            units: units for the frequencies. Accepted values thz, ev, mev, ha, cm-1, cm^-1.\n        \"\"\"\n\n        u = freq_units(units)\n\n        plt = pretty_plot(12, 8)\n\n        band_linewidth = 1\n\n        data = self.bs_plot_data()\n        for d in range(len(data['distances'])):\n            for i in range(self._nb_bands):\n                plt.plot(data['distances'][d],\n                         [data['frequency'][d][i][j] * u.factor\n                          for j in range(len(data['distances'][d]))], 'b-',\n                         linewidth=band_linewidth)\n\n        self._maketicks(plt)\n\n        # plot y=0 line\n        plt.axhline(0, linewidth=1, color='k')\n\n        # Main X and Y Labels\n        plt.xlabel(r'$\\mathrm{Wave\\ Vector}$', fontsize=30)\n        ylabel = r'$\\mathrm{{Frequencies\\ ({})}}$'.format(u.label)\n        plt.ylabel(ylabel, fontsize=30)\n\n        # X range (K)\n        # last distance point\n        x_max = data['distances'][-1][-1]\n        plt.xlim(0, x_max)\n\n        if ylim is not None:\n            plt.ylim(ylim)\n\n        plt.tight_layout()\n\n        return plt",
        "rewrite": "\n\n\ndef get_plot(self, ylim=None, units=\"thz\"):\n    u = freq_units(units)\n    fig, ax = subplots(figsize=(12, 8))\n    band_linewidth = 1\n\n    data = self.bs_plot_data()\n    for d in range(len(data['distances'])):\n       for i in range(self._nb_bands):\n           ax.plot(data['distances'][d], \n                   [data['frequency'][d][i][j] * u.factor \n                    for j in range(len(data['distances'][d]))], 'b-', \n                   linewidth"
    },
    {
        "original": "def add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n    \"\"\"\n    Registers a hotkey that replaces one typed text with another. For example\n\n        add_abbreviation('tm', u'\u2122')\n\n    Replaces every \"tm\" followed by a space with a \u2122 symbol (and no space). The\n    replacement is done by sending backspace events.\n\n    - `match_suffix` defines if endings of words should also be checked instead\n    of only whole words. E.g. if true, typing 'carpet'+space will trigger the\n    listener for 'pet'. Defaults to false, only whole words are checked.\n    - `timeout` is the maximum number of seconds between typed characters before\n    the current word is discarded. Defaults to 2 seconds.\n    \n    For more details see `add_word_listener`.\n    \"\"\"\n    replacement = '\\b'*(len(source_text)+1) + replacement_text\n    callback = lambda: write(replacement)\n    return add_word_listener(source_text, callback, match_suffix=match_suffix, timeout=timeout)",
        "rewrite": "\n\n\ndef add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n        replacement = '\\b' * (len(source_text) + 1) + replacement_text\n        callback = lambda: write(replacement)\n        return add_word_listener(source_text, callback, match_suffix=match_suffix, timeout=timeout)\n"
    },
    {
        "original": "def make_grover_circuit(input_qubits, output_qubit, oracle):\n    \"\"\"Find the value recognized by the oracle in sqrt(N) attempts.\"\"\"\n    # For 2 input qubits, that means using Grover operator only once.\n    c = cirq.Circuit()\n\n    # Initialize qubits.\n    c.append([\n        cirq.X(output_qubit),\n        cirq.H(output_qubit),\n        cirq.H.on_each(*input_qubits),\n    ])\n\n    # Query oracle.\n    c.append(oracle)\n\n    # Construct Grover operator.\n    c.append(cirq.H.on_each(*input_qubits))\n    c.append(cirq.X.on_each(*input_qubits))\n    c.append(cirq.H.on(input_qubits[1]))\n    c.append(cirq.CNOT(input_qubits[0], input_qubits[1]))\n    c.append(cirq.H.on(input_qubits[1]))\n    c.append(cirq.X.on_each(*input_qubits))\n    c.append(cirq.H.on_each(*input_qubits))\n\n    # Measure the result.\n    c.append(cirq.measure(*input_qubits, key='result'))\n\n    return c",
        "rewrite": "Here is a revised version of the function:\n\n\ndef make_grover_circuit(input-qbits, output-qbit, oracle):\n\tc = circuit.Circuit()\n\t\n\tc.append([circuit.op.X(output-qbit), circuit.op.H(output-qbit)] + [circuit.op.H qb for qb in input-qbits])\n\tc\u0642\u0641append(oracle)\n\t\n\tfor _ in range(int(len(input-qbits) ** 0.5)):\n\t\tc-append([circuit.op>H(qb) for qb in input-qbits])\n\t\tfor i in range(len(input-qbits) - 1):\n\t\t\tcontrol qed = input-q"
    },
    {
        "original": "def unsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command('UNSUBSCRIBE', *args)",
        "rewrite": "\n\n\ndef unsubscribe(self, *args):\n    if args:\n        args = list(args) if not isinstance(args[0], list) else args[0]\n        channels = self._normalize_keys(dict.fromkeys(args))\n    else:\n        channels = self.channels\n    self.pending_unsubscribe_channels.update(channels)\n    return self.execute_command('UNSUBSCRIBE', *)"
    },
    {
        "original": "def UploadFilePath(self, filepath, offset=0, amount=None):\n    \"\"\"Uploads chunks of a file on a given path to the transfer store flow.\n\n    Args:\n      filepath: A path to the file to upload.\n      offset: An integer offset at which the file upload should start on.\n      amount: An upper bound on number of bytes to stream. If it is `None` then\n        the whole file is uploaded.\n\n    Returns:\n      A `BlobImageDescriptor` object.\n    \"\"\"\n    return self._UploadChunkStream(\n        self._streamer.StreamFilePath(filepath, offset=offset, amount=amount))",
        "rewrite": "\n\n\ndef UploadFilePath(self, filepath: str, offset: int = 0, amount: int = None) -> 'BlobImageDescriptor':\n    return self._UploadChunkStream(self._streamer.StreamFilePath(filepath, offset=offset, amount=amount))\n"
    },
    {
        "original": "def connect(image):\n    \"\"\"\n    Activate nbd for an image file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_nbd.connect /tmp/image.raw\n    \"\"\"\n    if not os.path.isfile(image):\n        log.warning('Could not connect image: %s does not exist', image)\n        return ''\n\n    if salt.utils.path.which('sfdisk'):\n        fdisk = 'sfdisk -d'\n    else:\n        fdisk = 'fdisk -l'\n    __salt__['cmd.run']('modprobe nbd max_part=63')\n    for nbd in glob.glob('/dev/nbd?'):\n        if __salt__['cmd.retcode']('{0} {1}'.format(fdisk, nbd)):\n            while True:\n                # Sometimes nbd does not \"take hold\", loop until we can verify\n                __salt__['cmd.run'](\n                        'qemu-nbd -c {0} {1}'.format(nbd, image),\n                        python_shell=False,\n                        )\n                if not __salt__['cmd.retcode']('{0} {1}'.format(fdisk, nbd)):\n                    break\n            return nbd\n    log.warning('Could not connect image: %s', image)\n    return ''",
        "rewrite": "\n\ndef connect(image):\n    if not os.path.isfile(image):\n        log.warning(f'Could not connect image: {image} does not exist')\n        return ''\n\n    fdisk = 'sfdisk -d' if salt.utils.path.which('sfdisk') else 'fdisk -l'\n\n    __salt__['cmd.run']('modprobe nbd max_part=63')\n\n    for nbd in glob.glob('/dev/nb*d'):\n        if __salt__['cmd.retcode'](f'{fdisk} {nbd}'):\n            while True:\n                try:\n                    __"
    },
    {
        "original": "def del_flowspec_local(flowspec_family, route_dist, rules):\n    \"\"\"Deletes/withdraws Flow Specification route from VRF identified\n    by *route_dist*.\n    \"\"\"\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, route_dist=route_dist,\n            rules=rules, is_withdraw=True)\n\n        # Send success response.\n        return [{FLOWSPEC_FAMILY: flowspec_family,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 FLOWSPEC_RULES: rules}]\n\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)",
        "rewrite": "def delete_flowspec_local(flowspec_family, route_dist, rules):\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(flowspec_family, route_dist, rules, True)\n        return [{\"flwospecFamily\": flowspec_family, \"routeDistinguisher\": route_dist, \"flowspecRules\": rules}]\n    except BgpCoreError as e:\n        raise PrefixError(str(e))"
    },
    {
        "original": "def Name(self):\n    \"\"\"Return the best available name for this volume.\"\"\"\n    return (self.name or self.device_path or self.windowsvolume.drive_letter or\n            self.unixvolume.mount_point or None)",
        "rewrite": "\ndef name(self):\n    return self.name or self.device_path or getattr(self.windowsvolume, 'drive_letter', None) or getattr(self.unixvolume, 'mount_point', None)\n"
    },
    {
        "original": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict",
        "rewrite": "\n\n\ndef reduce_dict(input_dict, average=True):\n    world_size = get_world_size()\n    \n    if world_size < 2:\n        return input_dict\n    \n    with torch.no_grad():\n        names = list(sorted(input_dict.keys()))\n        \n        tensor_values = [input Dict[k] for k in names]\n        \n        tensor_values = torch.stack(tensor_values)\n        \n        dist.reduce(tensor_values, dst=0)\n        \n            \n                \n               \n            \n                \n               \n            \n            \n       if dist.get_rank() == 0 and average: \n           tensor_values /= world_size\n        \n            \n       reducedDict ={k:v.item()"
    },
    {
        "original": "def setex(self, name, time, value):\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n        return self.execute_command('SETEX', name, time, value)",
        "rewrite": "\n\n\ndef setex(self, name, time, value):\n    if isinstance(time, datetime.timedelta):\n        time = int(time.total_seconds())\n    return self.execute_command('SETEX', name, time.value)"
    },
    {
        "original": "def digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    return digest_auth(qop, user, passwd, algorithm, \"never\")",
        "rewrite": "\n\n\ndef digest_auth_nostale(\n    qop=None,\n    user=\"user\",\n    passwd=\"passwd\",\n    algorithm=\"MD5\"\n):\n    return digest_auth(qop, user, passwd, algorithm, \"never\")\n"
    },
    {
        "original": "def print_info(info_mapping):\n    \"\"\" print info in mapping.\n\n    Args:\n        info_mapping (dict): input(variables) or output mapping.\n\n    Examples:\n        >>> info_mapping = {\n                \"var_a\": \"hello\",\n                \"var_b\": \"world\"\n            }\n        >>> info_mapping = {\n                \"status_code\": 500\n            }\n        >>> print_info(info_mapping)\n        ==================== Output ====================\n        Key              :  Value\n        ---------------- :  ----------------------------\n        var_a            :  hello\n        var_b            :  world\n        ------------------------------------------------\n\n    \"\"\"\n    if not info_mapping:\n        return\n\n    content_format = \"{:<16} : {:<}\\n\"\n    content = \"\\n==================== Output ====================\\n\"\n    content += content_format.format(\"Variable\", \"Value\")\n    content += content_format.format(\"-\" * 16, \"-\" * 29)\n\n    for key, value in info_mapping.items():\n        if isinstance(value, (tuple, collections.deque)):\n            continue\n        elif isinstance(value, (dict, list)):\n            value = json.dumps(value)\n        elif value is None:\n            value = \"None\"\n\n        if is_py2:\n            if isinstance(key, unicode):\n                key = key.encode(\"utf-8\")\n            if isinstance(value, unicode):\n                value = value.encode(\"utf-8\")\n\n        content += content_format.format(key, value)\n\n    content += \"-\" * 48 + \"\\n\"\n    logger.log_info(content)",
        "rewrite": "\n\n\nimport json\nimport collections\n\ndef print_info(info_mapping):\n    if not info_mapping:\n         return\n\n     try:\n         from vip\u3057\u3044logger import logger\n     except ImportError:\n         def logger():\n             pass\n\n     is_py2 = False\n     try:\n         ''\n     except NameError as e:\n         is_py2 = True\n\n\ncontent_format = \"{:<16} : {:<}\\n\"\ncontent = \"\\n==================== Output ====================\\n\"\ncontent += content_format.format(\"Key\", \"Value\")\ncontent += content_format.format(\"-\" * 16, \"-\" * 29"
    },
    {
        "original": "def summed_icohp(self):\n        \"\"\"\n        Adds ICOHPs of both spin channels for spin polarized compounds\n        Returns:\n             icohp value in eV\n        \"\"\"\n        if self._is_spin_polarized:\n            sum_icohp = self._icohp[Spin.down] + self._icohp[Spin.up]\n        else:\n            sum_icohp = self._icohp[Spin.up]\n        return sum_icohp",
        "rewrite": "\ndef summed_icohp(self):\n    if self._is_spin_polarized:\n        return self._icohp[Spin.down] + self._icohp[Spin.up]\n    return self._icohp[Spin.up]\n"
    },
    {
        "original": "def wrap_tuple_streams(unwrapped, kdims, streams):\n    \"\"\"\n    Fills in tuple keys with dimensioned stream values as appropriate.\n    \"\"\"\n    param_groups = [(s.contents.keys(), s) for s in streams]\n    pairs = [(name,s)  for (group, s) in param_groups for name in group]\n    substituted = []\n    for pos,el in enumerate(wrap_tuple(unwrapped)):\n        if el is None and pos < len(kdims):\n            matches = [(name,s) for (name,s) in pairs if name==kdims[pos].name]\n            if len(matches) == 1:\n                (name, stream) = matches[0]\n                el = stream.contents[name]\n        substituted.append(el)\n    return tuple(substituted)",
        "rewrite": "Here's a revised version of the function:\n\n\ndef wrap_tuple_streams(unwrapped, kdims, streams):\n    param_groups = {(s.contents.keys(): s) for s in streams}\n    substituted = []\n    for pos, el in enumerate(wrapped := unwrapped):\n        if el is None and pos < len(kdims):\n            if kdims[pos].name in param_groups:\n                el = param_groups[kdims[pos].name].contents[kdims[pos].name]\n        substituted.append(el)\n    return tuple(substituted)\n"
    },
    {
        "original": "def lines(n_traces=5,n=100,columns=None,dateIndex=True,mode=None):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta scatter (lines) plot\n\n\tParameters:\n\t-----------\n\t\tn_traces : int\n\t\t\tNumber of traces \n\t\tn : int\n\t\t\tNumber of points for each trace\n\t\tcolumns : [str]\n\t\t\tList of column names\n\t\tdateIndex : bool\n\t\t\tIf True it will return a datetime index\n\t\t\tif False it will return a enumerated index\n\t\tmode : string\n\t\t\tFormat for each item\n\t\t\t\t'abc' for alphabet columns\n\t\t\t\t'stocks' for random stock names\n\t\"\"\"\t\n\tindex=pd.date_range('1/1/15',periods=n) if dateIndex else list(range(n))\n\tdf=pd.DataFrame(np.random.randn(n,n_traces),index=index,\n\t\tcolumns=getName(n_traces,columns=columns,mode=mode))\n\treturn df.cumsum()",
        "rewrite": "\n\n\nimport pandas as pd\nimport numpy as np\n\ndef getName(n, columns=None, mode=None):\n    if mode == 'abc':\n        return [chr(i) for i in range(ord('A'), ord('A') + n)]\n    elif mode == 'stocks':\n        stocks = ['AAPL', 'GOOGL', 'FB', 'MSFT', 'AMZN']\n        return np.random.choice(stocks, n)\n    else:\n        return [f\"col{i}\" for i in range(1, n+1)]\n\ndef lines(n_traces=5,n=100"
    },
    {
        "original": "def _parse_name(name: str, as_qvm: bool, noisy: bool) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n\n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    parts = name.split('-')\n    if len(parts) >= 2 and parts[-2] == 'noisy' and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n\n        if noisy is not None and (not noisy):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `noisy=False`\")\n\n        qvm_type = parts[-1]\n        noisy = True\n        prefix = '-'.join(parts[:-2])\n        return prefix, qvm_type, noisy\n\n    if len(parts) >= 1 and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n        qvm_type = parts[-1]\n        if noisy is None:\n            noisy = False\n        prefix = '-'.join(parts[:-1])\n        return prefix, qvm_type, noisy\n\n    if as_qvm is not None and as_qvm:\n        qvm_type = 'qvm'\n    else:\n        qvm_type = None\n\n    if noisy is None:\n        noisy = False\n\n    return name, qvm_type, noisy",
        "rewrite": "Here's the revised code:\n\n\nfrom typing import Tuple\n\ndef _parse_name(name: str, as_q vm: bool=None , nois y :bool=None ) -> Tuple[str ,str ,bool ] :\nparts= n ame .split ('-')\n\nif len( p arts ) >= 2 an d p arts [-2 ] == 'n ois y' an d p arts [-1 ] in ['qv m' ,'p yqv m']:\nif as_qv m i s no t No ne an d no t a s_qv m :\nraise Va lu e Er ror (\"T he pr o"
    },
    {
        "original": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               conf):\n    \"\"\"\n    Check neutron for all data\n    \"\"\"\n    comps = conf.split()\n\n    profile = None\n    if comps[0]:\n        profile = comps[0]\n\n    conn = _auth(profile)\n    ret = {}\n    networks = conn.list_networks()\n    for network in networks['networks']:\n        ret[network['name']] = network\n\n    if len(comps) < 2:\n        comps.append('networks')\n    return {comps[1]: ret}",
        "rewrite": "\n\n\ndef ext_pillar(minion_id, pillar, conf):\n    comps = conf.split()\n    \n    if not comps:\n        return {}\n    \n    profile = comps[0]\n    \n    conn = _auth(profile)\n    \n    if not conn:\n        return {}\n    \n    networks = conn.list_networks().get('networks', [])\n        \n    ret = {network['name']: network for network in networks}\n    \n     if len(comps) < 2:\n         comps.append('networks')\n        \n     return {comps[1]: ret}\n"
    },
    {
        "original": "def MultiSet(self,\n               subject,\n               values,\n               timestamp=None,\n               replace=True,\n               sync=True,\n               to_delete=None):\n    \"\"\"Set multiple attributes' values for this subject in one operation.\"\"\"\n    to_delete = set(to_delete or [])\n\n    # Prepare a bulk insert operation.\n    subject = utils.SmartUnicode(subject)\n    to_insert = []\n    to_replace = []\n    transaction = []\n\n    # Build a document for each unique timestamp.\n    for attribute, sequence in iteritems(values):\n      for value in sequence:\n        if isinstance(value, tuple):\n          value, entry_timestamp = value\n        else:\n          entry_timestamp = timestamp\n\n        if entry_timestamp is None:\n          entry_timestamp = timestamp\n\n        if entry_timestamp is not None:\n          entry_timestamp = int(entry_timestamp)\n        else:\n          entry_timestamp = time.time() * 1e6\n\n        attribute = utils.SmartUnicode(attribute)\n        data = self._Encode(value)\n\n        # Replacing means to delete all versions of the attribute first.\n        if replace or attribute in to_delete:\n          existing = self._CountExistingRows(subject, attribute)\n          if existing:\n            to_replace.append([subject, attribute, data, entry_timestamp])\n          else:\n            to_insert.append([subject, attribute, data, entry_timestamp])\n          if attribute in to_delete:\n            to_delete.remove(attribute)\n\n        else:\n          to_insert.append([subject, attribute, data, entry_timestamp])\n\n    if to_delete:\n      self.DeleteAttributes(subject, to_delete)\n\n    if sync:\n      if to_replace:\n        transaction.extend(self._BuildReplaces(to_replace))\n      if to_insert:\n        transaction.extend(self._BuildInserts(to_insert))\n      if transaction:\n        self._ExecuteTransaction(transaction)\n    else:\n      if to_replace:\n        with self.buffer_lock:\n          self.to_replace.extend(to_replace)\n      if to_insert:\n        with self.buffer_lock:\n          self.to_insert.extend(to_insert)",
        "rewrite": "\n\n\ndef MultiSet(self,\n             subject,\n             values,\n             timestamp=None,\n             replace=True,\n             sync=True,\n             to_delete=None):\n    \n    subject_idnex tho\u1ea1ide_keyeset\ud83d\ude0ato_setdefault((), []))\n\nblob_itemssubjecto_utils.SmartUnicodesubject\n    \n \nto_setsu812464822-set()\n    \nfor attriibutesequencesobgestedvaluues.items():\n   valuue\n        \n     tuplevalueentry_timestamop sider.uiidia3mpsltimestamp timestamppass\n    \n   elif none notifyryssesttimestamp s>();"
    },
    {
        "original": "def _decode_crop_and_flip(image_buffer, num_channels):\n  \"\"\"Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  \"\"\"\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n\n  min_object_covered=0.1\n  aspect_ratio_range=[0.75, 1.33]\n  area_range=[0.05, 1.0]\n  max_attempts=100\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MIN_OBJ_COV,\n                          value=min_object_covered)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_RATIO_RANGE,\n                          value=aspect_ratio_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_AREA_RANGE,\n                          value=area_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MAX_ATTEMPTS,\n                          value=max_attempts)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_CROP_USES_BBOXES, value=False)\n\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                     dtype=tf.float32, shape=[1, 1, 4])   #From the entire image\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=min_object_covered,\n      aspect_ratio_range=aspect_ratio_range,\n      area_range=area_range,\n      max_attempts=max_attempts,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RANDOM_FLIP)\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped",
        "rewrite": "Here is your revised code:\n\n\ndef _decode_crop_and_flip(image_buffer: bytes , num_channels: int) -> tf.Tensor:\n    min_object_covered = .1 \n    aspect_ratio_range=[ .75 ,1 .33 ]\n    area_range=[ .05 ,1 ]\n    max_attempts =100 \n\n    mlperlilogres_netprint (key(mlperlilogINPUTDISTORTEDCROPMIN OBJCOV) ,value(minobjectcovered))\n    mlperlilogres_netprint (key(mlperlilogINPUTDISTORTEDCROPRATIO RANGE),value(aspect ratiorange))\n    mlpelog"
    },
    {
        "original": "def bgp_summary_parser(bgp_summary):\n    \"\"\"Parse 'show bgp all summary vrf' output information from NX-OS devices.\"\"\"\n\n    bgp_summary_dict = {}\n    # Check for BGP summary information lines that have no data\n    if len(bgp_summary.strip().splitlines()) <= 1:\n        return {}\n\n    allowed_afi = [\"ipv4\", \"ipv6\", \"l2vpn\"]\n    vrf_regex = r\"^BGP summary information for VRF\\s+(?P<vrf>\\S+),\"\n    afi_regex = (\n        r\"^BGP summary information.*address family (?P<afi>\\S+ (?:Unicast|EVPN))\"\n    )\n    local_router_regex = (\n        r\"^BGP router identifier\\s+(?P<router_id>\\S+)\"\n        r\",\\s+local AS number\\s+(?P<local_as>\\S+)\"\n    )\n\n    for pattern in [vrf_regex, afi_regex, local_router_regex]:\n        match = re.search(pattern, bgp_summary, flags=re.M)\n        if match:\n            bgp_summary_dict.update(match.groupdict(1))\n\n    # Some post regex cleanup and validation\n    vrf = bgp_summary_dict[\"vrf\"]\n    if vrf.lower() == \"default\":\n        bgp_summary_dict[\"vrf\"] = \"global\"\n\n    afi = bgp_summary_dict[\"afi\"]\n    afi = afi.split()[0].lower()\n    if afi not in allowed_afi:\n        raise ValueError(\"AFI ({}) is invalid and not supported.\".format(afi))\n    bgp_summary_dict[\"afi\"] = afi\n\n    local_as = bgp_summary_dict[\"local_as\"]\n    local_as = helpers.as_number(local_as)\n\n    match = re.search(IPV4_ADDR_REGEX, bgp_summary_dict[\"router_id\"])\n    if not match:\n        raise ValueError(\n            \"BGP router_id ({}) is not valid\".format(bgp_summary_dict[\"router_id\"])\n        )\n\n    vrf = bgp_summary_dict[\"vrf\"]\n    bgp_return_dict = {vrf: {\"router_id\": bgp_summary_dict[\"router_id\"], \"peers\": {}}}\n\n    # Extract and process the tabular data\n    tabular_divider = r\"^Neighbor\\s+.*PfxRcd$\"\n    tabular_data = re.split(tabular_divider, bgp_summary, flags=re.M)\n    if len(tabular_data) != 2:\n        msg = \"Unexpected data processing BGP summary information:\\n\\n{}\".format(\n            bgp_summary\n        )\n        raise ValueError(msg)\n    tabular_data = tabular_data[1]\n    bgp_table = bgp_normalize_table_data(tabular_data)\n    for bgp_entry in bgp_table_parser(bgp_table):\n        bgp_return_dict[vrf][\"peers\"].update(bgp_entry)\n\n    bgp_new_dict = {}\n    for neighbor, bgp_data in bgp_return_dict[vrf][\"peers\"].items():\n        received_prefixes = bgp_data.pop(\"received_prefixes\")\n        bgp_data[\"address_family\"] = {}\n        prefixes_dict = {\n            \"sent_prefixes\": -1,\n            \"accepted_prefixes\": -1,\n            \"received_prefixes\": received_prefixes,\n        }\n        bgp_data[\"address_family\"][afi] = prefixes_dict\n        bgp_data[\"local_as\"] = local_as\n        # FIX, hard-coding\n        bgp_data[\"remote_id\"] = \"0.0.0.0\"\n        bgp_new_dict[neighbor] = bgp_data\n\n    bgp_return_dict[vrf][\"peers\"] = bgp_new_dict\n\n    return bgp_return_dict",
        "rewrite": "\n\n\nimport re\nfrom collections import defaultdict\n\ndef helpers():\n   ...\n\ndef.bg_normalize_table-data(table_string):\n   ...\n\ndef.bg-table-parser(table_list):\n   ...\n\ndef.bg-summary-parser(bg-summary):\n   result= defaultdict(dict)\n\n   if not.bg-summary.strip():\n       return result\n   \n   allowed_afia=['ipv4', 'ipv6', 'l2vpn'] \n   \n   patterns=[\n       (r'^BGP.summary-information-for-VRF\\s*(?P<vffjfS+)',),\n       (r'^BGPSummary-information.*address-family(?P<afijI_CHARSET]),"
    },
    {
        "original": "def warp(self, warp_matrix, img, iflag=cv2.INTER_NEAREST):\n        \"\"\" Function to warp input image given an estimated 2D linear transformation\n\n        :param warp_matrix: Linear 2x3 matrix to use to linearly warp the input images\n        :type warp_matrix: ndarray\n        :param img: Image to be warped with estimated transformation\n        :type img: ndarray\n        :param iflag: Interpolation flag, specified interpolation using during resampling of warped image\n        :type iflag: cv2.INTER_*\n        :return: Warped image using the linear matrix\n        \"\"\"\n\n        height, width = img.shape[:2]\n        warped_img = np.zeros_like(img, dtype=img.dtype)\n\n        # Check if image to warp is 2D or 3D. If 3D need to loop over channels\n        if (self.interpolation_type == InterpolationType.LINEAR) or img.ndim == 2:\n            warped_img = cv2.warpAffine(img.astype(np.float32), warp_matrix, (width, height),\n                                        flags=iflag).astype(img.dtype)\n\n        elif img.ndim == 3:\n            for idx in range(img.shape[-1]):\n                warped_img[..., idx] = cv2.warpAffine(img[..., idx].astype(np.float32), warp_matrix, (width, height),\n                                                      flags=iflag).astype(img.dtype)\n        else:\n            raise ValueError('Image has incorrect number of dimensions: {}'.format(img.ndim))\n\n        return warped_img",
        "rewrite": "\n\n\ndef warp(self, warp_matrix, img, iflag=cv2.INTER_NEAREST):\n    height, width = img.shape[:2]\n    dtype = np.result_type(iflag ,img.dtype)\n    warped_img = np.zeros((height,width) + img.shape[2:], dtype=dtype)\n\n    if self.interpolation_type == InterpolationType.LINEAR or img.ndim == 2:\n       warped_img[:] = cv2.warpAffine(img.astype(np.float32), \n                                       M=warp_matrix,\n                                       dsize=(width,height), \n                                       flags=iflag).astype(dtype)\n    elif"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "rewrite": "\n\n\ndef get_projection_on_elements(self, structure):\n    dico = {}\n    for spin in self.data.keys():\n        dico[spin] = [[defaultdict(float) for _ in range(self.nkpoints)] \n                       for _ in range(self.nbands)]\n\n    for iat in range(self.nions):\n        name = structure.species[iat].symbol\n        for spin, data_spin in self.data.items():\n            for k, b in itertools.product(range(self.nkpoints), \n                                          range(self.nbands)):\n                dico[spin][b][k][name] += np.sum"
    },
    {
        "original": "def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])",
        "rewrite": "Here's the revised code:\n\n\ndef parse_if_range_header(value):\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    return IfRange(unquote_etag(value)[0])\n"
    },
    {
        "original": "def get_repo(repo, **kwargs):\n    \"\"\"\n    Display a repo from the sources.list / sources.list.d\n\n    The repo passed in needs to be a complete repo entry.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.get_repo \"myrepo definition\"\n    \"\"\"\n    _check_apt()\n    ppa_auth = kwargs.get('ppa_auth', None)\n    # we have to be clever about this since the repo definition formats\n    # are a bit more \"loose\" than in some other distributions\n    if repo.startswith('ppa:') and __grains__['os'] in ('Ubuntu', 'Mint', 'neon'):\n        # This is a PPA definition meaning special handling is needed\n        # to derive the name.\n        dist = __grains__['lsb_distrib_codename']\n        owner_name, ppa_name = repo[4:].split('/')\n        if ppa_auth:\n            auth_info = '{0}@'.format(ppa_auth)\n            repo = LP_PVT_SRC_FORMAT.format(auth_info, owner_name,\n                                            ppa_name, dist)\n        else:\n            if HAS_SOFTWAREPROPERTIES:\n                try:\n                    if hasattr(softwareproperties.ppa, 'PPAShortcutHandler'):\n                        repo = softwareproperties.ppa.PPAShortcutHandler(\n                            repo).expand(dist)[0]\n                    else:\n                        repo = softwareproperties.ppa.expand_ppa_line(\n                            repo,\n                            dist)[0]\n                except NameError as name_error:\n                    raise CommandExecutionError(\n                        'Could not find ppa {0}: {1}'.format(repo, name_error)\n                    )\n            else:\n                repo = LP_SRC_FORMAT.format(owner_name, ppa_name, dist)\n\n    repos = list_repos()\n\n    if repos:\n        try:\n            repo_type, repo_architectures, repo_uri, repo_dist, repo_comps = _split_repo_str(repo)\n            if ppa_auth:\n                uri_match = re.search('(http[s]?://)(.+)', repo_uri)\n                if uri_match:\n                    if not uri_match.group(2).startswith(ppa_auth):\n                        repo_uri = '{0}{1}@{2}'.format(uri_match.group(1),\n                                                       ppa_auth,\n                                                       uri_match.group(2))\n        except SyntaxError:\n            raise CommandExecutionError(\n                'Error: repo \\'{0}\\' is not a well formatted definition'\n                .format(repo)\n            )\n\n        for source in six.itervalues(repos):\n            for sub in source:\n                if (sub['type'] == repo_type and\n                    # strip trailing '/' from repo_uri, it's valid in definition\n                    # but not valid when compared to persisted source\n                    sub['uri'].rstrip('/') == repo_uri.rstrip('/') and\n                        sub['dist'] == repo_dist):\n                    if not repo_comps:\n                        return sub\n                    for comp in repo_comps:\n                        if comp in sub.get('comps', []):\n                            return sub\n    return {}",
        "rewrite": "\n\n\ndef get_repo(repo, **kwargs):\n\t_check_apt()\n\tppa_auth = kwargs.get('ppa_auth')\n\tifisions ['os']in ('Ubuntui., Mint.l.neon'):\n\t\tif.repo.startswith \u043b\u0456\u043a\u0430\u0435rettP\u041f\u0410.: \n\t\t\tdist=.  grain s \ud55c ['lsb d str ib codename']\n\t\t\towner nam.e ,ppaname=repo[4:].spilt('/')\n\t\t\tifopa.auth :\n\t\t\t\tauth info='{\u041e@'.form at(paa.auth) \n\t\t\t\trepo=LP PVT SRC FORMAT.form at(auth info. own er-name"
    },
    {
        "original": "def random_choice(self, actions=None, random_state=None):\n        \"\"\"\n        Return a pure action chosen randomly from `actions`.\n\n        Parameters\n        ----------\n        actions : array_like(int), optional(default=None)\n            An array of integers representing pure actions.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        scalar(int)\n            If `actions` is given, returns an integer representing a\n            pure action chosen randomly from `actions`; if not, an\n            action is chosen randomly from the player's all actions.\n\n        \"\"\"\n        random_state = check_random_state(random_state)\n\n        if actions is not None:\n            n = len(actions)\n        else:\n            n = self.num_actions\n\n        if n == 1:\n            idx = 0\n        else:\n            idx = random_state.randint(n)\n\n        if actions is not None:\n            return actions[idx]\n        else:\n            return idx",
        "rewrite": "Here's a revised version of the function:\n\n\ndef random_choice(self, actions=None, random_state=None):\n    random_state = check_random_state(random_state)\n    n = len(actions) if actions is not None else self.num_actions\n    \n    return actions[random_state.choice(n)] if actions is not None else random_State.choice(n)\n"
    },
    {
        "original": "def ListClientsForKeywords(self, keywords, start_time=None, cursor=None):\n    \"\"\"Lists the clients associated with keywords.\"\"\"\n    keywords = set(keywords)\n    hash_to_kw = {mysql_utils.Hash(kw): kw for kw in keywords}\n    result = {kw: [] for kw in keywords}\n\n    query = ",
        "rewrite": "\n\n\ndef list_clients_for_keywords(self, keywords, start_time=None, cursor=None):\n    unique_keywords = set(keywords)\n    hash_to_kw_map = {mysql_utils.Hash(kw): kw for kw in unique_keywords}\n    result = {kw: [] for kw in unique_keywords}\n\n    query = (\"SELECT keyword_hash, client_id \"\n             \"FROM client_keyword \"\n             \"WHERE keyword_hash IN (%s)\" %\n             \",\".join([\"%s\"] * len(hash_to_kw_map)))\n\n    with self.db.cursor() as cur:\n        cur.execute(query, list(hash_to_kw_map.keys()))\n"
    },
    {
        "original": "def GetAuditLogEntries(offset, now, token):\n  \"\"\"Return all audit log entries between now-offset and now.\n\n  Args:\n    offset: rdfvalue.Duration how far back to look in time\n    now: rdfvalue.RDFDatetime for current time\n    token: GRR access token\n  Yields:\n    AuditEvents created during the time range\n  \"\"\"\n  start_time = now - offset - audit.AUDIT_ROLLOVER_TIME\n\n  for fd in audit.LegacyAuditLogsForTimespan(start_time, now, token):\n    for event in fd.GenerateItems():\n      if now - offset < event.timestamp < now:\n        yield event",
        "rewrite": "\ndef get_audit_log_entries(offset, now, token):\n    start_time = now - offset - audit.AUDIT_ROLLOVER_TIME\n    for fd in audit_LEGACY_AUDIT_LOGS_FOR_TIMESpan(start_time, now, token):\n        for event in fd.generate_items():\n            if start_time <= event.timestamp < (now - offset):\n                yield event"
    },
    {
        "original": "def evaluate(self, data_loader):\n        \"\"\"\n        Sets model in eval mode, disables gradients, preallocates memory and\n        runs validation on data provided by data_loader.\n\n        :param data_loader: data loader\n        \"\"\"\n        torch.set_grad_enabled(False)\n        self.model.eval()\n        torch.cuda.empty_cache()\n        self.preallocate(data_loader, training=False)\n        output = self.feed_data(data_loader, training=False)\n        self.model.zero_grad()\n        torch.cuda.empty_cache()\n        return output",
        "rewrite": "\n\n\ndef evaluate(self, data_loader):\n    torch.set_grad_enabled(False)\n    self.model.eval()\n    torch.cuda.empty_cache()\n    self.preallocate(data_loader, training=False)\n    output = self.feed_data(data_loader, training=False)\n    del output \n    self.model.train() \n"
    },
    {
        "original": "def secgroup_delete(self, name):\n        \"\"\"\n        Delete a security group\n        \"\"\"\n        nt_ks = self.compute_conn\n        for item in nt_ks.security_groups.list():\n            if item.name == name:\n                nt_ks.security_groups.delete(item.id)\n                return {name: 'Deleted security group: {0}'.format(name)}\n        return 'Security group not found: {0}'.format(name)",
        "rewrite": "\n\n\ndef secgroup_delete(self, name):\n    nt_ks = self.compute_conn\n    for item in nt_ks.security_groups.list():\n        if item.name == name:\n            nt_ks.security_groups.delete(item.id)\n            return {name: f'Deleted security group: {name}'}\n    return f'Security group not found: {name}'\n"
    },
    {
        "original": "def _sidConversion(cls, val, **kwargs):\n        \"\"\"\n        converts a list of pysid objects to string representations\n        \"\"\"\n        if isinstance(val, six.string_types):\n            val = val.split(',')\n        usernames = []\n        for _sid in val:\n            try:\n                userSid = win32security.LookupAccountSid('', _sid)\n                if userSid[1]:\n                    userSid = '{1}\\\\{0}'.format(userSid[0], userSid[1])\n                else:\n                    userSid = '{0}'.format(userSid[0])\n            # TODO: This needs to be more specific\n            except Exception:\n                userSid = win32security.ConvertSidToStringSid(_sid)\n                log.warning('Unable to convert SID \"%s\" to a friendly name.  The SID will be disaplayed instead of a user/group name.', userSid)\n            usernames.append(userSid)\n        return usernames",
        "rewrite": "\n\n\ndef _sid_conversion(val, **kwargs):\n    if isinstance(val, str):\n        val = val.split(',')\n    usernames = [_convert_sid(sid) for sid in val]\n    return usernames\n\n\ndef _convert_sid(sid):\n    try:\n        user_sid = win32security.LookupAccount Sid(None, sid)\n        return '{}\\\\{}'.format(user_sid[1], user_sid[0]) if user_sid[1] else '{}'.format(user_sid[0])\n    except Exception as e:\n        log.warning(f'Unable to convert SID \"{sid}\" to a friendly name. The SID will be"
    },
    {
        "original": "def _check_result(method_name, result):\n    \"\"\"\n    Checks whether `result` is a valid API response.\n    A result is considered invalid if:\n        - The server returned an HTTP response code other than 200\n        - The content of the result is invalid JSON.\n        - The method call was unsuccessful (The JSON 'ok' field equals False)\n\n    :raises ApiException: if one of the above listed cases is applicable\n    :param method_name: The name of the method called\n    :param result: The returned result of the method request\n    :return: The result parsed to a JSON dictionary.\n    \"\"\"\n    if result.status_code != 200:\n        msg = 'The server returned HTTP {0} {1}. Response body:\\n[{2}]' \\\n            .format(result.status_code, result.reason, result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    try:\n        result_json = result.json()\n    except:\n        msg = 'The server returned an invalid JSON response. Response body:\\n[{0}]' \\\n            .format(result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    if not result_json['ok']:\n        msg = 'Error code: {0} Description: {1}' \\\n            .format(result_json['error_code'], result_json['description'])\n        raise ApiException(msg, method_name, result)\n    return result_json",
        "rewrite": "\n\n\ndef _check_result(method_name, result):\n    if result.status_code != 200:\n        raise ApiException(f'The server returned HTTP {result.status_code} {result.reason}. Response body:\\n[{result.text.encode(\"utf8\")}]', method_name, result)\n\n    try:\n        result_json = result.json()\n    except ValueError:\n        raise ApiException(f'The server returned an invalid JSON response. Response body:\\n[{result.text.encode(\"utf8\")}]', method_name, result)\n\n    if not result_json.get('ok'):\n        raise ApiException(f'Error code: {result"
    },
    {
        "original": "def yield_figs(self, **kwargs):  # pragma: no cover\n        \"\"\"\n        This function *generates* a predefined list of matplotlib figures with minimal input from the user.\n        \"\"\"\n        yield self.plot_densities(title=\"PAW densities\", show=False)\n        yield self.plot_waves(title=\"PAW waves\", show=False)\n        yield self.plot_projectors(title=\"PAW projectors\", show=False)",
        "rewrite": "\n\n\ndef yield_figs(self, **kwargs):\n    figs = [\n        self.plot_densities(title=\"PAW densities\", show=False),\n        self.plot_waves(title=\"PAW waves\", show=False),\n        self.plot_projectors(title=\"PAW projectors\", show=False)\n    ]\n    for fig in figs:\n        yield fig\n"
    },
    {
        "original": "def dispatch_command(self, command, params=None):\n        \"\"\"Dispatch device commands to the appropriate handler.\"\"\"\n        try:\n            if command in self.handlers:\n                self.handlers[command](**params)\n            else:\n                logging.warning('Unsupported command: %s: %s',\n                                command, params)\n        except Exception as e:\n            logging.warning('Error during command execution',\n                            exc_info=sys.exc_info())\n            raise e",
        "rewrite": "\n\n\ndef dispatch_command(self, command, params=None):\n    try:\n        if command in self.handlers:\n            self.handlers[command](**params or {})\n        else:\n            logging.warning('Unsupported command: %s: %s', command, params)\n    except Exception as e:\n        logging.warning('Error during command execution', exc_info=sys.exc_info())\n        raise\n"
    },
    {
        "original": "def toggle(self, *args):\n        \"\"\"\n        If no arguments are specified, toggle the state of all LEDs. If\n        arguments are specified, they must be the indexes of the LEDs you wish\n        to toggle. For example::\n\n            from gpiozero import LEDBoard\n\n            leds = LEDBoard(2, 3, 4, 5)\n            leds.toggle(0)   # turn on the first LED (pin 2)\n            leds.toggle(-1)  # turn on the last LED (pin 5)\n            leds.toggle()    # turn the first and last LED off, and the\n                             # middle pair on\n\n        If :meth:`blink` is currently active, it will be stopped first.\n\n        :param int args:\n            The index(es) of the LED(s) to toggle. If no indexes are specified\n            toggle the state of all LEDs.\n        \"\"\"\n        self._stop_blink()\n        if args:\n            for index in args:\n                self[index].toggle()\n        else:\n            super(LEDBoard, self).toggle()",
        "rewrite": "\n\n\ndef toggle(self, *args):\n    self._stop_blink()\n    if args:\n        for index in args:\n            self[index].toggle()\n    else:\n        for led in self:\n            led.toggle()\n"
    },
    {
        "original": "def placeOrder(self, contract: Contract, order: Order) -> Trade:\n        \"\"\"\n        Place a new order or modify an existing order.\n        Returns a Trade that is kept live updated with\n        status changes, fills, etc.\n\n        Args:\n            contract: Contract to use for order.\n            order: The order to be placed.\n        \"\"\"\n        orderId = order.orderId or self.client.getReqId()\n        self.client.placeOrder(orderId, contract, order)\n        now = datetime.datetime.now(datetime.timezone.utc)\n        key = self.wrapper.orderKey(\n            self.wrapper.clientId, orderId, order.permId)\n        trade = self.wrapper.trades.get(key)\n        if trade:\n            # this is a modification of an existing order\n            assert trade.orderStatus.status not in OrderStatus.DoneStates\n            logEntry = TradeLogEntry(now, trade.orderStatus.status, 'Modify')\n            trade.log.append(logEntry)\n            self._logger.info(f'placeOrder: Modify order {trade}')\n            trade.modifyEvent.emit(trade)\n            self.orderModifyEvent.emit(trade)\n        else:\n            # this is a new order\n            order.clientId = self.wrapper.clientId\n            order.orderId = orderId\n            orderStatus = OrderStatus(status=OrderStatus.PendingSubmit)\n            logEntry = TradeLogEntry(now, orderStatus.status, '')\n            trade = Trade(\n                contract, order, orderStatus, [], [logEntry])\n            self.wrapper.trades[key] = trade\n            self._logger.info(f'placeOrder: New order {trade}')\n            self.newOrderEvent.emit(trade)\n        return trade",
        "rewrite": "\n\n\ndef placeOrder(self, contract: Contract, order: Order) -> Trade:\n    orderId = order.orderId or self.client.getReqId()\n    self.client.placeOrder(orderId, contract, order)\n    now = datetime.datetime.now(datetime.timezone.utc)\n    key = self.wrapper.orderKey(self.wrapper.clientId, orderId, order.permId)\n    trade = self.wrapper.trades.get(key)\n    if trade:\n        assert trade.orderStatus.status not in OrderStatus.DoneStates\n        logEntry = TradeLogEntry(now, trade.orderStatus.status, 'Modify')\n        trade.log.append(logEntry)\n"
    },
    {
        "original": "def get_jid(jid):\n    \"\"\"\n    Return the information returned when the specified job id was executed\n    \"\"\"\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    sql = ",
        "rewrite": "Here is a revised version of the code:\n\n\ndef get_jid(jid):\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM job_runs WHERE job_id=%s\", (jid,))\n    result = cur.fetchone()\n    cur.close()\n    conn.close()\n    return result\n"
    },
    {
        "original": "def _ioctl(self, func, arg):\n        \"\"\"Runs the specified ioctl on the underlying fd.\n\n        Raises WatchdogError if the device is closed.\n        Raises OSError or IOError (Python 2) when the ioctl fails.\"\"\"\n        if self._fd is None:\n            raise WatchdogError(\"Watchdog device is closed\")\n        if os.name != 'nt':\n            import fcntl\n            fcntl.ioctl(self._fd, func, arg, True)",
        "rewrite": "Here's a revised version of the function:\n\n\ndef _ioctl(self, func, arg):\n    if self._fd is None:\n        raise WatchdogError(\"Watchdog device is closed\")\n    if os.name != 'nt':\n        import fcntl\n        return fcntl.ioctl(self._fd, func, arg)\n    else:\n        import ctypes\n        return ctypes.windll.kernel32.IOCTL(self._fd, func)(arg)\n"
    },
    {
        "original": "def _policyFileReplaceOrAppend(this_string, policy_data, append_only=False):\n    \"\"\"\n    helper function to take a ADMX policy string for registry.pol file data and\n    update existing string or append the string to the data\n    \"\"\"\n    # we are going to clean off the special pre-fixes, so we get only the valuename\n    if not policy_data:\n        policy_data = b''\n    specialValueRegex = salt.utils.stringutils.to_bytes(r'(\\*\\*Del\\.|\\*\\*DelVals\\.){0,1}')\n    item_key = None\n    item_value_name = None\n    data_to_replace = None\n    if not append_only:\n        item_key = this_string.split(b'\\00;')[0].lstrip(b'[')\n        item_value_name = re.sub(specialValueRegex,\n                                 b'',\n                                 this_string.split(b'\\00;')[1],\n                                 flags=re.IGNORECASE)\n        log.debug('item value name is %s', item_value_name)\n        data_to_replace = _regexSearchKeyValueCombo(policy_data, item_key, item_value_name)\n    if data_to_replace:\n        log.debug('replacing %s with %s', data_to_replace, this_string)\n        policy_data = policy_data.replace(data_to_replace, this_string)\n    else:\n        log.debug('appending %s', this_string)\n        policy_data = b''.join([policy_data, this_string])\n\n    return policy_data",
        "rewrite": "Here's a revised version of the code:\n\n\ndef _policyFileReplaceOrAppend(this_string, policy_data, append_only=False):\n    \n     if not policy_data: \n         policy_data=b''\n     specialValueRegex=salt.utils.stringutils.to_bytes(r'(\\*\\*Del\\.|\\*\\*DelVals\\.){0,1}')\n     if not append_only:  \n         key,value=this_string.split(b'\\00;', 1)        \n         value=re.sub(specialValueRegex,b'',value).strip()   \n         try: \n              index=policy_data.index(key+b'\\00;')\n              end_index=policy_data"
    },
    {
        "original": "def parse(html):\n    \"\"\"\n    Parses the given HTML message and returns its stripped representation\n    plus a list of the MessageEntity's that were found.\n\n    :param message: the message with HTML to be parsed.\n    :return: a tuple consisting of (clean message, [message entities]).\n    \"\"\"\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(_add_surrogate(html))\n    text = helpers.strip_text(parser.text, parser.entities)\n    return _del_surrogate(text), parser.entities",
        "rewrite": "\n\n\ndef parse(html):\n    if not html:\n        return html, []\n    parser = HTMLToTelegramParser()\n    parser.feed(_add_surrogate(html))\n    text = helpers.strip_text(parser.text, parser.entities)\n    return _del_surrogate(text), parser.entities\n"
    },
    {
        "original": "def execute(self, eopatch):\n        \"\"\" Computation of NDVI slope using finite central differences\n\n        This implementation loops through every spatial location, considers the valid NDVI values and approximates their\n        first order derivative using central differences. The argument of min and max is added to the eopatch.\n\n        The NDVI slope at date t is comuted as $(NDVI_{t+1}-NDVI_{t-1})/(date_{t+1}-date_{t-1})$.\n\n        :param eopatch: Input eopatch\n        :return: eopatch with NDVI slope argmin/argmax features\n        \"\"\"\n        # pylint: disable=invalid-name\n        if self.mask_data:\n            valid_data_mask = eopatch.mask['VALID_DATA']\n        else:\n            valid_data_mask = eopatch.mask['IS_DATA']\n\n        ndvi = np.ma.array(eopatch.data[self.data_feature],\n                           dtype=np.float32,\n                           mask=~valid_data_mask.astype(np.bool))\n\n        all_dates = np.asarray([x.toordinal() for x in eopatch.timestamp])\n\n        if ndvi.ndim == 4:\n            h, w = ndvi.shape[1: 3]\n        else:\n            raise ValueError('{} feature has incorrect number of dimensions'.format(self.data_feature))\n\n        argmax_ndvi_slope, argmin_ndvi_slope = np.zeros((h, w, 1), dtype=np.uint8), np.zeros((h, w, 1), dtype=np.uint8)\n\n        for ih, iw in it.product(range(h), range(w)):\n\n            ndvi_curve = ndvi[:, ih, iw, :]\n            valid_idx = np.where(~ndvi.mask[:, ih, iw])[0]\n\n            ndvi_curve = ndvi_curve[valid_idx]\n            valid_dates = all_dates[valid_idx]\n\n            ndvi_slope = np.convolve(ndvi_curve.squeeze(), [1, 0, -1], 'valid') / np.convolve(valid_dates, [1, 0, -1],\n                                                                                              'valid')\n\n            # +1 to compensate for the 'valid' convolution which eliminates first and last\n            argmax_ndvi_slope[ih, iw] = valid_idx[np.argmax(ndvi_slope) + 1]\n            argmin_ndvi_slope[ih, iw] = valid_idx[np.argmin(ndvi_slope) + 1]\n\n            del ndvi_curve, valid_idx, valid_dates, ndvi_slope\n\n        eopatch.data_timeless[self.argmax_feature] = argmax_ndvi_slope\n        eopatch.data_timeless[self.argmin_feature] = argmin_ndvi_slope\n\n        return eopatch",
        "rewrite": "\n\n\ndef execute(self, eopatch):\n    if self.mask_data:\n        valid_data_mask = eopatch.mask['VALID_DATA']\n    else:\n        valid_data_mask = eopatch.mask['IS_DATA']\n\n    ndvi = np.ma.array(eopatch.data[self.data_feature], dtype=np.float32, mask=~valid_data_mask.astype(np.bool))\n\n    all_dates = np.asarray([x.toordinal() for x in eopatch.timestamp])\n\n    if ndvi.ndim == 4:\n        h, w = ndvi.shape[1: 3]\n    else:\n        raise ValueError"
    },
    {
        "original": "def set_user_avatar(self, username, avatar):\n        \"\"\"Set a user's avatar.\n\n        :param username: the user to set the avatar for\n        :param avatar: ID of the avatar to set\n        \"\"\"\n        self._set_avatar(\n            {'username': username}, self._get_url('user/avatar'), avatar)",
        "rewrite": "\n\n\ndef set_user_avatar(self, username: str, avatar: int):\n    self._set_avatar({'username': username}, self._get_url('user/avatar'), avatar)\n"
    }
]
