[
    {
        "original": "def remove_descendants_of(self, node): \n        if isinstance(node, int):\n            warnings.warn('Calling remove_descendants_of() with a node id is deprecated,'\n                          ' use a DAGNode instead',\n                          DeprecationWarning, 2)\n            node = self._id_to_node[node]\n\n        desc = nx.descendants(self._multi_graph, node)\n        for desc_node in desc:\n  ",
        "rewrite": "def remove_descendants_of(self, node):\n        if isinstance(node, int):\n            warnings.warn('Calling remove_descendants_of() with a node id is deprecated,'\n                          ' use a DAGNode instead',\n                          DeprecationWarning, 2)\n            node = self._id_to_node[node]\n\n        desc = nx.descendants(self._multi_graph, node)\n        for desc_node in desc:\n            # perform removal of descendants\n            self._multi_graph.remove_node(desc_node)"
    },
    {
        "original": "def expand_abbreviations(self, text): \n        if not self.abbreviations:\n            raise LexiconError(\"No abbreviations in lexicon.\")\n\n        def chunks(data, SIZE=25):",
        "rewrite": "def expand_abbreviations(self, text):\n    if not self.abbreviations:\n        raise LexiconError(\"No abbreviations in lexicon.\")\n\n    def chunks(data, SIZE=25):"
    },
    {
        "original": "def p_gate_op_3(self, program): \n        program[0] = node.CustomUnitary([program[1], program[4]])\n        self.verify_as_gate(program[1], program[4])\n        self.verify_bit_list(program[4])\n        self.verify_distinct([program[4]])",
        "rewrite": "def p_gate_op_3(self, program):\n    program[0] = node.CustomUnitary([program[1], program[4]])\n    self.verify_as_gate(program[1], program[4])\n    self.verify_bit_list(program[4])\n    self.verify_distinct([program[4])"
    },
    {
        "original": " \n        raw_result = self._purge_deleted_initial(\n            vault_name=vault_name,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n       ",
        "rewrite": "```\n        raw_result = self._purge_deleted_initial(\n            vault_name=vault_name,\n            location=location,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response, raw):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n```"
    },
    {
        "original": "def merge(self, other): \n        if self.m != other.m or self.p != other.p:\n            raise ValueError(\"Cannot merge HyperLogLog with different\\\n                    precisions.\")\n        self.reg = np.maximum(self.reg, other.reg)",
        "rewrite": "def merge(self, other): \n        if self.m != other.m or self.p != other.p:\n            raise ValueError(\"Cannot merge HyperLogLog with different precisions.\")\n        self.reg = np.maximum(self.reg, other.reg)"
    },
    {
        "original": "def plot_lognormal_cdf(self,**kwargs): \n        if not hasattr(self,'lognormal_dist'):\n            return\n\n        x=np.sort(self.data)\n        n=len(x)\n        xcdf = np.arange(n,0,-1,dtype='float')/float(n)\n        lcdf = self.lognormal_dist.sf(x)\n\n        D_location = argmax(xcdf-lcdf)\n        pylab.vlines(x[D_location],xcdf[D_location],lcdf[D_location],color='m',linewidth=2)\n\n        pylab.plot(x, lcdf,',',**kwargs)",
        "rewrite": "def plot_lognormal_cdf(self, **kwargs):\n        if not hasattr(self, 'lognormal_dist'):\n            return\n\n        x = np.sort(self.data)\n        n = len(x)\n        xcdf = np.arange(n, 0, -1, dtype='float') / float(n)\n        lcdf = self.lognormal_dist.sf(x)\n\n        D_location = np.argmax(xcdf - lcdf)\n        plt.vlines(x[D_location], xcdf[D_location], lcdf[D_location], color='m', linewidth=2)\n\n        plt.plot(x, lcdf, ',', **kwargs)"
    },
    {
        "original": "def convert_to_theano_var(obj): \n    from deepy.core.neural_var import NeuralVariable\n    if type(obj) == tuple:\n        return tuple(convert_to_theano_var(list(obj)))\n    if type(obj) == list:\n        unpacked_list = map(convert_to_theano_var, obj)\n        normal_list = []\n        test_list = []\n        theano_var_found = False\n        neural_var_found = False\n        for normal_var, tensor_found, neural_found in unpacked_list:\n            normal_list.append(normal_var)\n            if tensor_found: theano_var_found = True\n   ",
        "rewrite": "def convert_to_theano_var(obj): \n    from deepy.core.neural_var import NeuralVariable\n    \n    if type(obj) == tuple:\n        return tuple(convert_to_theano_var(list(obj)))\n    \n    if type(obj) == list:\n        unpacked_list = map(convert_to_theano_var, obj)\n        normal_list = []\n        test_list = []\n        theano_var_found = False\n        neural_var_found = False\n        \n        for normal_var, tensor_found, neural_found in unpacked_list:\n            normal_list.append(normal_var)\n            if tensor_found:\n                theano_var_found = True"
    },
    {
        "original": "def is_type(obj, typestr_or_type): \n    if typestr_or_type == \"all\":\n        return True\n    if type(typestr_or_type) == types.TypeType:\n        test_type = typestr_or_type\n    else:\n        test_type = typestr2type.get(typestr_or_type, False)\n    if test_type:\n        return isinstance(obj, test_type)\n    return False",
        "rewrite": "def is_type(obj, typestr_or_type): \n    if typestr_or_type == \"all\":\n        return True\n    if type(typestr_or_type) == type:\n        test_type = typestr_or_type\n    else:\n        test_type = typestr2type.get(typestr_or_type, False)\n    if test_type:\n        return isinstance(obj, test_type)\n    return False"
    },
    {
        "original": "def makeAB(self): \n        for fld in self.m_blk_a:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                self.m_req[fld] = self.m_blk_a[fld]\n        for fld in self.m_blk_b:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n               ",
        "rewrite": "self.m_req[fld] = self.m_blk_b[fld]"
    },
    {
        "original": "def ucas_download_playlist(url, output_dir = '.', merge = False, info_only = False, **kwargs): \n    html = get_content(url)\n\n    parts = re.findall( r'(getplaytitle.do\\?.+)\"', html)\n    assert parts, 'No part found!'\n\n    for part_path in parts:\n        ucas_download('http://v.ucas.ac.cn/course/' + part_path, output_dir=output_dir, merge=merge, info_only=info_only)",
        "rewrite": "def ucas_download_playlist(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    html = get_content(url)\n    \n    parts = re.findall( r'(getplaytitle.do\\?.+)\"', html)\n    assert parts, 'No part found!'\n    \n    for part_path in parts:\n        ucas_download('http://v.ucas.ac.cn/course/' + part_path, output_dir=output_dir, merge=merge, info_only=info_only)"
    },
    {
        "original": "def _make_intermediate_dirs(sftp_client, remote_directory): \n    if remote_directory == '/':\n        sftp_client.chdir('/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('/'))\n        _make_intermediate_dirs(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return",
        "rewrite": "def _make_intermediate_dirs(sftp_client, remote_directory):\n    if remote_directory == '/':\n        sftp_client.chdir('/')\n        return\n    if remote_directory == '':\n        return\n    try:\n        sftp_client.chdir(remote_directory)\n    except IOError:\n        dirname, basename = os.path.split(remote_directory.rstrip('/'))\n        _make_intermediate_dirs(sftp_client, dirname)\n        sftp_client.mkdir(basename)\n        sftp_client.chdir(basename)\n        return"
    },
    {
        "original": "def _get_context(self, cursor=None): \n        if cursor is None:\n            cursor = self._get_cursor()\n        cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                            QtGui.QTextCursor.KeepAnchor)\n        text = cursor.selection().toPlainText()\n        return self._completion_lexer.get_context(text)",
        "rewrite": "def _get_context(self, cursor=None):\n    if cursor is None:\n        cursor = self._get_cursor()\n    cursor.movePosition(QtGui.QTextCursor.StartOfBlock, QtGui.QTextCursor.KeepAnchor)\n    text = cursor.selection().toPlainText()\n    return self._completion_lexer.get_context(text)"
    },
    {
        "original": "def engineering_notation_number(obj): \n    try:\n        obj = obj.rstrip()\n        float(obj[:-1] if obj[-1] in _SUFFIX_TUPLE else obj)\n        return None\n    except (AttributeError, IndexError, ValueError):\n        # AttributeError: obj.rstrip(), object could not be a string\n        # IndexError: obj[-1], when an empty string\n        # ValueError: float(), when not a string representing a number\n        raise ValueError(pexdoc.pcontracts.get_exdesc())",
        "rewrite": "def engineering_notation_number(obj):\n    try:\n        obj = obj.rstrip()\n        float(obj[:-1] if obj[-1] in _SUFFIX_TUPLE else obj)\n        return None\n    except (AttributeError, IndexError, ValueError):\n        raise ValueError(pexdoc.pcontracts.get_exdesc())"
    },
    {
        "original": "def run(loop): \n    cam = AsyncCAM(loop=loop)\n    await cam.connect()\n    print(cam.welcome_msg)\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.receive())\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.wait_for(cmd='cmd', timeout=0.1))\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.wait_for(cmd='cmd', timeout=0))\n    print(await cam.wait_for(cmd='cmd', timeout=0.1))\n    print(await cam.wait_for(cmd='test', timeout=0.1))\n    cam.close()",
        "rewrite": "async def run(loop):\n    cam = AsyncCAM(loop=loop)\n    await cam.connect()\n    print(cam.welcome_msg)\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.receive())\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.wait_for(cmd='cmd', timeout=0.1))\n    await cam.send(b'/cmd:deletelist')\n    print(await cam.wait_for(cmd='cmd', timeout=0))\n    print(await cam.wait_for(cmd='cmd', timeout=0.1))\n    print(await cam.wait_for(cmd='test', timeout=0.1))\n    cam.close()  # Fix: added missing close() method after all operations completed."
    },
    {
        "original": "def validate_expression(self, expression): \n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)",
        "rewrite": "def validate_expression(self, expression):\n    vars = set(self.get_column_names()) | set(self.variables.keys())\n    funcs = set(expression_namespace.keys())\n    return vaex.expresso.validate_expression(expression, vars, funcs)"
    },
    {
        "original": "def match(self, *args): \n        if not args:\n            raise SyntaxError('cannot case empty pattern.')\n\n        return self.match_args(self._value, args)",
        "rewrite": "def match(self, *args):\n    if not args:\n        raise SyntaxError('cannot pass empty pattern.')\n\n    return self.match_args(self._value, args)"
    },
    {
        "original": "def generate_postorder(self, trie): \n        order, stack = [], []\n        stack.append(trie.root)\n        colors = ['white'] * len(trie)\n        while len(stack) > 0:\n            index = stack[-1]\n            color = colors[index]\n            if color == 'white': # \u0432\u0435\u0440\u0448\u0438\u043d\u0430 \u0435\u0449\u0451 \u043d\u0435 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u043b\u0430\u0441\u044c\n                colors[index] = 'grey'\n               ",
        "rewrite": "def generate_postorder(self, trie): \n    order, stack = [], []\n    stack.append(trie.root)\n    colors = ['white'] * len(trie)\n    while len(stack) > 0:\n        index = stack[-1]\n        color = colors[index]\n        if color == 'white':\n            colors[index] = 'grey'"
    },
    {
        "original": "def __coord_chroma(n, bins_per_octave=12, **_kwargs): \n    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n+1, endpoint=True)",
        "rewrite": "def __coord_chroma(n, bins_per_octave=12, **kwargs): \n    return np.linspace(0, (12.0 * n) / bins_per_octave, num=n+1, endpoint=True)"
    },
    {
        "original": " \n    # Check arguments.\n    if target_uid != \"\" and search_terms != \"\":\n        print(\"You can not specify a target uid and target search terms for a \"\n              \"merge.\")\n        sys.exit(1)\n    # Find possible target contacts.\n    if target_uid != \"\":\n        target_vcards = get_contacts(selected_address_books, target_uid,\n                                     method=\"uid\")\n   ",
        "rewrite": "if target_uid and search_terms:\n    print(\"You can not specify a target uid and target search terms for a merge.\")\n    sys.exit(1)\n# Find possible target contacts.\nif target_uid:\n    target_vcards = get_contacts(selected_address_books, target_uid, method=\"uid\")"
    },
    {
        "original": "def wait_for_kernel(self, timeout=None): \n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n ",
        "rewrite": "def wait_for_kernel(self, timeout=None): \n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                pass"
    },
    {
        "original": "def to_raw_text_markupless(text, keep_whitespace=False, normalize_ascii=True): \n    return sent_tokenize(\n        remove_dates(_remove_urls(text)),\n        keep_whitespace,\n        normalize_ascii\n    )",
        "rewrite": "def to_raw_text_markupless(text, keep_whitespace=False, normalize_ascii=True): \n    return sent_tokenize(remove_dates(_remove_urls(text)), keep_whitespace, normalize_ascii)"
    },
    {
        "original": "def mirror(self): \n        if not self._definition:\n            return self.copy()\n\n        reverse_inst = self.copy(name=self.name + '_mirror')\n        reverse_inst.definition = []\n        for inst, qargs, cargs in reversed(self._definition):\n            reverse_inst._definition.append((inst.mirror(), qargs, cargs))\n        return reverse_inst",
        "rewrite": "def mirror(self): \n    if not self._definition:\n        return self.copy()\n\n    reverse_inst = self.copy(name=self.name + '_mirror')\n    reverse_inst.definition = []\n    for inst, qargs, cargs in reversed(self._definition):\n        reverse_inst._definition.append((inst.mirror(), qargs, cargs))\n    return reverse_inst"
    },
    {
        "original": " \n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run",
        "rewrite": "predicate = lambda x: x in predicate"
    },
    {
        "original": "def change(script, layer_num=None): \n    if layer_num is None:\n        if isinstance(script, mlx.FilterScript):\n            layer_num = script.last_layer()\n        else:\n            layer_num = 0\n    filter_xml = ''.join([\n        '  <filter name=\"Change the current layer\">\\n',\n        '    <Param name=\"mesh\" ',\n        'value=\"{:d}\" '.format(layer_num),\n        'description=\"Mesh\" ',\n        'type=\"RichMesh\" ',\n        '/>\\n',\n  ",
        "rewrite": "def change(script, layer_num=None): \n    if layer_num is None:\n        if isinstance(script, mlx.FilterScript):\n            layer_num = script.last_layer()\n        else:\n            layer_num = 0\n    filter_xml = ''.join([\n        '  <filter name=\"Change the current layer\">\\n',\n        '    <Param name=\"mesh\" ',\n        'value=\"{:d}\" '.format(layer_num),\n        'description=\"Mesh\" ',\n        'type=\"RichMesh\" ',\n        '/>\\n',\n    ])"
    },
    {
        "original": "def list_to_string(l = range(200), width = 40, indent = \"  \"): \n    l = [str(v) + \",\" for v in l]\n    counter = 0\n    out = \"\" + indent\n    for w in l:\n        s = len(w)\n        if counter + s > width: \n            out += \"\\n\" + indent\n            counter = 0\n        out += w\n        counter += s\n    return out.strip(\",\")",
        "rewrite": "def list_to_string(l=range(200), width=40, indent=\"  \"):\n    l = [str(v) + \",\" for v in l]\n    counter = 0\n    out = \"\" + indent\n    for w in l:\n        s = len(w)\n        if counter + s > width:\n            out += \"\\n\" + indent\n            counter = 0\n        out += w\n        counter += s\n    return out.strip(\",\")"
    },
    {
        "original": "def check_versions(cls, local, upstream): \n\n        # A version should be in format [1,2,3] which is actually the version `1.2.3`\n        # So as we only have 3 elements in the versioning,\n        # we initiate the following variable in order to get the status of each parts.\n        status = [None, None, None]\n\n        for index, version_number in enumerate(local):\n            # We loop through the local version.\n\n            if int(version_number) < int(upstream[index]):\n     ",
        "rewrite": "def check_versions(cls, local, upstream):\n    status = [None, None, None]\n\n    for index, version_number in enumerate(local):\n        if int(version_number) < int(upstream[index]):\n            # Add code here for what to do if local version is lower than upstream version\n            pass"
    },
    {
        "original": "def _handle_failed_job(self, job): \n\n        task_id = job.kwargs['task_id']\n        logger.error(\"Job #%s (task: %s) failed; cancelled\",\n                     job.id, task_id)",
        "rewrite": "def _handle_failed_job(self, job):\n    task_id = job.kwargs['task_id']\n    logger.error(\"Job #%s (task: %s) failed; cancelled\",\n                 job.id, task_id)"
    },
    {
        "original": "def subscribe_to_trades(self, pair, **kwargs): \n        identifier = ('trades', pair)\n        self._subscribe('trades', identifier, symbol=pair, **kwargs)",
        "rewrite": "def subscribe_to_trades(self, pair, **kwargs): \n    identifier = ('trades', pair)\n    self._subscribe('trades', identifier, symbol=pair, **kwargs)"
    },
    {
        "original": " \n    if not isinstance(html, _strings):\n        raise TypeError('string required')\n\n    if parser is None:\n        parser = html_parser\n\n    children = parser.parseFragment(html, 'div', useChardet=guess_charset)\n    if children and isinstance(children[0], _strings):\n        if no_leading_text:\n            if children[0].strip():\n                raise etree.ParserError('There is leading text: %r' %\n                                ",
        "rewrite": "if not isinstance(html, str):\n        raise TypeError('string required')\n\n    if parser is None:\n        parser = html_parser\n\n    children = parser.parseFragment(html, 'div', useChardet=guess_charset)\n    if children and isinstance(children[0], str):\n        if no_leading_text:\n            if children[0].strip():\n                raise etree.ParserError('There is leading text: %r' %\n                                html)"
    },
    {
        "original": "def percussive(y, **kwargs): \n\n    # Compute the STFT matrix\n    stft = core.stft(y)\n\n    # Remove harmonics\n    stft_perc = decompose.hpss(stft, **kwargs)[1]\n\n    # Invert the STFT\n    y_perc = util.fix_length(core.istft(stft_perc, dtype=y.dtype), len(y))\n\n    return y_perc",
        "rewrite": "def percussive(y, **kwargs):\n    stft = core.stft(y)\n    stft_perc = decompose.hpss(stft, **kwargs)[1]\n    y_perc = util.fix_length(core.istft(stft_perc, dtype=y.dtype), len(y))\n    \n    return y_perc"
    },
    {
        "original": "def save_token(token, domain='analytics.luminoso.com', token_file=None): \n        token_file = token_file or get_token_filename()\n        if os.path.exists(token_file):\n            saved_tokens = json.load(open(token_file))\n        else:\n            saved_tokens = {}\n        saved_tokens[domain] = token\n        directory, filename = os.path.split(token_file)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory)\n        with open(token_file, 'w') as f:\n         ",
        "rewrite": "```python\ndef save_token(token, domain='analytics.luminoso.com', token_file=None):\n    token_file = token_file or get_token_filename()\n    if os.path.exists(token_file):\n        with open(token_file, 'r') as file:\n            saved_tokens = json.load(file)\n    else:\n        saved_tokens = {}\n    saved_tokens[domain] = token\n    directory, filename = os.path.split(token_file)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory)\n    with open(token_file, 'w') as f:\n        json.dump(saved_tokens, f)\n```"
    },
    {
        "original": "def increment(self, name, count=1, rate=1): \n\n        if self._should_send_metric(name, rate):\n            self._request(\n                Counter(\n                    self._create_metric_name_for_request(name),\n                    int(count),\n                    rate\n                ).to_request()\n        ",
        "rewrite": "def increment(self, name, count=1, rate=1): \n\n        if self._should_send_metric(name, rate):\n            self._request(\n                Counter(\n                    self._create_metric_name_for_request(name),\n                    int(count),\n                    rate\n                ).to_request()\n        )"
    },
    {
        "original": "def make_csv_response(csv_data, filename): \n    resp = make_response(csv_data)\n    resp.headers['Content-Type'] = 'application/octet-stream'\n    resp.headers['Content-Disposition'] = 'attachment; filename=%s' % filename\n\n    return resp",
        "rewrite": "def make_csv_response(csv_data, filename): \n    resp = make_response(csv_data)\n    resp.headers['Content-Type'] = 'application/octet-stream'\n    resp.headers['Content-Disposition'] = 'attachment; filename={}'.format(filename)\n\n    return resp"
    },
    {
        "original": "def username(self): \n        if os.name == 'posix':\n            if pwd is None:\n                # might happen if python was installed from sources\n                raise ImportError(\"requires pwd module shipped with standard python\")\n            return pwd.getpwuid(self.uids.real).pw_name\n        else:\n            return self._platform_impl.get_process_username()",
        "rewrite": "def get_username(self):\n    if os.name == 'posix':\n        if pwd is None:\n            raise ImportError(\"requires pwd module shipped with standard python\")\n        return pwd.getpwuid(self.uids.real).pw_name\n    else:\n        return self._platform_impl.get_process_username()"
    },
    {
        "original": " \n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = radius\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter / 2\n        if radius2 is None and diameter2 is None:\n  ",
        "rewrite": "```\n    if radius is not None and diameter is None:\n        if radius1 is None and diameter1 is None:\n            radius1 = radius\n        if radius2 is None and diameter2 is None:\n            radius2 = radius\n    if diameter is not None:\n        if radius1 is None and diameter1 is None:\n            radius1 = diameter / 2\n        if radius2 is None and diameter2 is None:\n            radius2 = diameter / 2\n```"
    },
    {
        "original": "def export(self): \n        if not hasattr(C, 'gss_export_cred'):\n            raise NotImplementedError(\"The GSSAPI implementation does not support gss_export_cred\")\n\n        minor_status = ffi.new('OM_uint32[1]')\n        output_buffer = ffi.new('gss_buffer_desc[1]')\n        retval = C.gss_export_cred(minor_status, self._cred[0], output_buffer)\n        try:\n            if GSS_ERROR(retval):\n                raise _exception_for_status(retval, minor_status[0])\n\n            return _buf_to_str(output_buffer[0])\n        finally:\n   ",
        "rewrite": "def export(self): \n    if not hasattr(C, 'gss_export_cred'):\n        raise NotImplementedError(\"The GSSAPI implementation does not support gss_export_cred\")\n\n    minor_status = ffi.new('OM_uint32[1]')\n    output_buffer = ffi.new('gss_buffer_desc[1]')\n    retval = C.gss_export_cred(minor_status, self._cred[0], output_buffer)\n    try:\n        if GSS_ERROR(retval):\n            raise _exception_for_status(retval, minor_status[0])\n\n        return _buf_to_str(output_buffer[0])\n    finally:"
    },
    {
        "original": "def direct_view(self, targets='all'): \n        single = isinstance(targets, int)\n        # allow 'all' to be lazily evaluated at each execution\n        if targets != 'all':\n            targets = self._build_targets(targets)[1]\n        if single:\n            targets = targets[0]\n        return DirectView(client=self, socket=self._mux_socket, targets=targets)",
        "rewrite": "def direct_view(self, targets='all'): \n    is_single = isinstance(targets, int)\n    \n    if targets != 'all':\n        targets = self._build_targets(targets)[1]\n    \n    if is_single:\n        targets = targets[0]\n    \n    return DirectView(client=self, socket=self._mux_socket, targets=targets)"
    },
    {
        "original": "def attendee(request, form, user_id=None): \n\n    if user_id is None and form.cleaned_data[\"user\"] is not None:\n        user_id = form.cleaned_data[\"user\"]\n\n    if user_id is None:\n        return attendee_list(request)\n\n    attendee = people.Attendee.objects.get(user__id=user_id)\n    name = attendee.attendeeprofilebase.attendee_name()\n\n    reports = []\n\n    profile_data = []\n    try:\n        profile = people.AttendeeProfileBase.objects.get_subclass(\n            attendee=attendee\n        )\n        fields = profile._meta.get_fields()\n    except people.AttendeeProfileBase.DoesNotExist:\n        fields = []\n\n  ",
        "rewrite": "def attendee(request, form, user_id=None):\n    if user_id is None and form.cleaned_data[\"user\"] is not None:\n        user_id = form.cleaned_data[\"user\"]\n\n    if user_id is None:\n        return attendee_list(request)\n\n    try:\n        attendee = people.Attendee.objects.get(user__id=user_id)\n        name = attendee.attendeeprofilebase.attendee_name()\n\n        reports = []\n\n        profile_data = []\n\n        try:\n            profile = people.AttendeeProfileBase.objects.get_subclass(attendee=attendee)\n            fields = profile._meta.get_fields()\n        except people.AttendeeProfileBase.DoesNotExist:\n            fields = []\n    except people.Attendee.DoesNotExist:\n        return attendee_list(request)"
    },
    {
        "original": "def draw(self): \n        if not self.visible:\n            # Simple visibility check, has to be tested to see if it works properly\n            return\n        \n        if not isinstance(self.submenu,Container):\n            glEnable(GL_SCISSOR_TEST)\n            glScissor(*self.pos+self.size)\n        \n        SubMenu.draw(self)\n        \n        if not isinstance(self.submenu,Container):\n  ",
        "rewrite": "def draw(self):\n    if not self.visible:\n        return\n        \n    if not isinstance(self.submenu, Container):\n        glEnable(GL_SCISSOR_TEST)\n        glScissor(*self.pos + self.size)\n        \n    SubMenu.draw(self)\n    \n    if not isinstance(self.submenu, Container):"
    },
    {
        "original": " \n        n = 0\n        for s in self._hsig.values():\n            if hasattr(s, 'is_fun') and s.is_fun:\n                n += 1\n        return n",
        "rewrite": "n = sum(1 for s in self._hsig.values() if hasattr(s, 'is_fun') and s.is_fun) \nreturn n"
    },
    {
        "original": "def _line_segment_with_colons(linefmt, align, colwidth): \n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w",
        "rewrite": "def line_segment_with_colons(linefmt, align, colwidth):\n    fill = linefmt.hline\n    w = colwidth\n    if align == \"right\" or align == \"decimal\":\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w"
    },
    {
        "original": " \n        submonitor = ProgressMonitor(*args, **kargs)\n        self.sub_monitors[submonitor] = units\n        submonitor.add_listener(self._submonitor_update)\n        return submonitor",
        "rewrite": "submonitor = ProgressMonitor(*args, **kargs)\nself.sub_monitors[submonitor] = units\nsubmonitor.add_listener(self._submonitor_update)\nreturn submonitor"
    },
    {
        "original": "def panel_export(panel_id): \n    panel_obj = store.panel(panel_id)\n    data = controllers.panel_export(store, panel_obj)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n    html_report = render_template('panels/panel_pdf_simple.html', **data)\n    return render_pdf(HTML(string=html_report), download_filename=data['panel']['panel_name']+'_'+str(data['panel']['version'])+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')",
        "rewrite": "def panel_export(panel_id): \n    panel_obj = store.panel(panel_id)\n    data = controllers.panel_export(store, panel_obj)\n    data['report_created_at'] = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n    html_report = render_template('panels/panel_pdf_simple.html', **data)\n    return render_pdf(HTML(string=html_report), download_filename=data['panel']['panel_name']+'_'+str(data['panel']['version'])+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')"
    },
    {
        "original": "def reg_on_exit(self, callable_object, *args, **kwargs): \n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'exit', persistent, *args, **kwargs)\n        self.exit_callbacks.append(event)\n        return event",
        "rewrite": "def reg_on_exit(self, callback, *args, **kwargs):\n    persistent = kwargs.pop('persistent', False)\n    event = self._create_event(callback, 'exit', persistent, *args, **kwargs)\n    self.exit_callbacks.append(event)\n    return event"
    },
    {
        "original": "def _set_init_process(self): \n\n        logger.debug(\"========================\")\n        logger.debug(\"Setting secondary inputs\")\n        logger.debug(\"========================\")\n\n        # Get init process\n        init_process = self.processes[0]\n        logger.debug(\"Setting main raw inputs: \"\n                     \"{}\".format(self.main_raw_inputs))\n        init_process.set_raw_inputs(self.main_raw_inputs)\n        logger.debug(\"Setting extra inputs: {}\".format(self.extra_inputs))\n        init_process.set_extra_inputs(self.extra_inputs)",
        "rewrite": "def _set_init_process(self): \n    logger.debug(\"========================\")\n    logger.debug(\"Setting secondary inputs\")\n    logger.debug(\"========================\")\n\n    # Get init process\n    init_process = self.processes[0]\n    logger.debug(\"Setting main raw inputs: {}\".format(self.main_raw_inputs))\n    init_process.set_raw_inputs(self.main_raw_inputs)\n    logger.debug(\"Setting extra inputs: {}\".format(self.extra_inputs))\n    init_process.set_extra_inputs(self.extra_inputs)"
    },
    {
        "original": "def delete_os_image(self, image_name, delete_vhd=False): \n        _validate_not_none('image_name', image_name)\n        path = self._get_image_path(image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)",
        "rewrite": "def delete_os_image(self, image_name, delete_vhd=False): \n    _validate_not_none('image_name', image_name)\n    path = self._get_image_path(image_name)\n    \n    if delete_vhd:\n        path += '?comp=media'\n        \n    return self._perform_delete(path, as_async=True)"
    },
    {
        "original": "def precompute_future_symbols(trie, n, allow_spaces=False): \n    if n == 0:\n        return\n    if trie.is_terminated and trie.precompute_symbols:\n        # \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u044b\n        return\n    for index, final in enumerate(trie.final):\n        trie.data[index] = [set() for i in range(n)]\n    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n        node_data[0] = set(trie._get_letters(index))\n        if allow_spaces and final:\n            node_data[0].add(\" \")\n    for d in range(1, n):\n        for",
        "rewrite": "def precompute_future_symbols(trie, n, allow_spaces=False):\n    if n == 0:\n        return\n    if trie.is_terminated and trie.precompute_symbols:\n        return\n    for index, final in enumerate(trie.final):\n        trie.data[index] = [set() for i in range(n)]\n    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n        node_data[0] = set(trie._get_letters(index))\n        if allow_spaces and final:\n            node_data[0].add(\" \")\n    for d in range(1, n):\n        for index, node_data in enumerate(trie.data):\n            current_set = set()\n            for child, extra_symbols in enumerate(node_data[d-1]):\n                current_set.update(trie.data[child][d-1])\n                current_set.update(extra_symbols)\n            node_data[d].update(current_set)"
    },
    {
        "original": "def from_yaml(cls, **kwargs): \n        ret = cls()\n\n        for k, v in kwargs.iteritems():\n            ret.__dict__[k] = v\n        return ret",
        "rewrite": "def from_yaml(cls, **kwargs):\n    ret = cls()\n\n    for k, v in kwargs.items():\n        ret.__dict__[k] = v\n    return ret"
    },
    {
        "original": "def render_screen(self): \n        self.term_width, self.term_height = get_terminal_size()\n        self.log.debug(\n            \"Terminal size: %sx%s\", self.term_width, self.term_height)\n        self.right_panel_width = int(\n            (self.term_width - len(self.RIGHT_PANEL_SEPARATOR))\n            * (float(self.info_panel_percent) / 100)) - 1\n        if self.right_panel_width > 0:\n            self.left_panel_width = self.term_width - \\\n                self.right_panel_width - len(self.RIGHT_PANEL_SEPARATOR) - 2\n  ",
        "rewrite": "def render_screen(self):\n        self.term_width, self.term_height = get_terminal_size()\n        self.log.debug(\n            \"Terminal size: %sx%s\", self.term_width, self.term_height)\n        self.right_panel_width = int(\n            (self.term_width - len(self.RIGHT_PANEL_SEPARATOR))\n            * (float(self.info_panel_percent) / 100)) - 1\n        if self.right_panel_width > 0:\n            self.left_panel_width = self.term_width - \\\n                self.right_panel_width - len(self.RIGHT_PANEL_SEPARATOR) - 2"
    },
    {
        "original": "def is_significant(sample1, sample2): \n    deg_freedom = len(sample1) + len(sample2) - 2\n    critical_value = tdist95conf_level(deg_freedom)\n    t_score = tscore(sample1, sample2)\n    return (abs(t_score) >= critical_value, t_score)",
        "rewrite": "def is_significant(sample1, sample2): \n    deg_freedom = len(sample1) + len(sample2) - 2\n    critical_value = tdist95conf_level(deg_freedom)\n    t_score = tscore(sample1, sample2)\n    return (abs(t_score) >= critical_value, t_score)"
    },
    {
        "original": "def load_state_dict(module, state_dict, strict=False, logger=None): \n    unexpected_keys = []\n    own_state = module.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            unexpected_keys.append(name)\n            continue\n        if isinstance(param, torch.nn.Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n\n        try:\n            own_state[name].copy_(param)\n     ",
        "rewrite": "def load_state_dict(module, state_dict, strict=False, logger=None): \n    unexpected_keys = []\n    own_state = module.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            unexpected_keys.append(name)\n            continue\n        if isinstance(param, torch.nn.Parameter):\n            param = param.data\n        \n        try:\n            own_state[name].copy_(param)\n        except Exception as e:\n            if strict:\n                raise RuntimeError(\"Error loading state_dict for {}. Unexpected key: {}\".format(module.__class__.__name__, name))\n            if logger is not None:\n                logger.warning(\"Unexpected key {} in state_dict for {}. Skipping...\".format(name, module.__class__.__name__))\n            continue"
    },
    {
        "original": "def add_handlers(self, logger, handlers): \n        for h in handlers:\n            try:\n                logger.addHandler(self.config['handlers'][h])\n            except StandardError as e:\n                raise ValueError('Unable to add handler %r: %s' % (h, e))",
        "rewrite": "def add_handlers(self, logger, handlers): \n        for h in handlers:\n            try:\n                logger.addHandler(self.config['handlers'][h])\n            except StandardError as e:\n                raise ValueError('Unable to add handler {}: {}'.format(h, e))"
    },
    {
        "original": "def apply_to_with_tz(self, dttm, timezone): \n        result = self.apply_to(dttm)\n        if self.unit in [DAYS, WEEKS, MONTHS, YEARS]:\n            naive_dttm = datetime(result.year, result.month, result.day)\n            result = timezone.localize(naive_dttm)\n        return result",
        "rewrite": "def apply_to_with_tz(self, dttm, timezone):\n    result = self.apply_to(dttm)\n    if self.unit in [DAYS, WEEKS, MONTHS, YEARS]:\n        naive_dttm = datetime(result.year, result.month, result.day)\n        result = timezone.localize(naive_dttm)\n    return result"
    },
    {
        "original": "def closeAllSessions(self, slot): \n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)",
        "rewrite": "def closeAllSessions(self, slot): \n    rv = self.lib.C_CloseAllSessions(slot)\n    if rv != CKR_OK:\n        raise PyKCS11Error(rv)"
    },
    {
        "original": "def _pixel_masked(hit, array): \n    if array.shape[0] > hit[\"column\"] and array.shape[1] > hit[\"row\"]:\n        return array[hit[\"column\"], hit[\"row\"]]\n    else:\n        return False",
        "rewrite": "def _pixel_masked(hit, array):\n    if len(array) > hit[\"column\"] and len(array[0]) > hit[\"row\"]:\n        return array[hit[\"column\"]][hit[\"row\"]]\n    else:\n        return False"
    },
    {
        "original": "def list_files(root, suffix, prefix=False): \n    root = os.path.expanduser(root)\n    files = list(\n        filter(\n            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n            os.listdir(root)\n        )\n    )\n\n    if prefix is True:\n        files = [os.path.join(root, d) for d in files]\n\n    return files",
        "rewrite": "import os\n\ndef list_files(root, suffix, prefix=False): \n    root = os.path.expanduser(root)\n    files = [p for p in os.listdir(root) if os.path.isfile(os.path.join(root, p)) and p.endswith(suffix)]\n\n    if prefix:\n        files = [os.path.join(root, d) for d in files]\n\n    return files"
    },
    {
        "original": "def to_xml(self): \n        if self.data:\n            self.document = self._update_document(self.document, self.data)\n\n        return self.document",
        "rewrite": "def to_xml(self): \n    if self.data:\n        self.document = self._update_document(self.document, self.data)\n\n    return self.document"
    },
    {
        "original": "def build_is_last_day_of_season(num_steps_per_season): \n  num_steps_per_cycle = np.sum(num_steps_per_season)\n  changepoints = np.cumsum(np.ravel(num_steps_per_season)) - 1\n  def is_last_day_of_season(t):\n    t_ = dist_util.maybe_get_static_value(t)\n    if t_ is not None:  # static case\n      step_in_cycle = t_ % num_steps_per_cycle\n      return any(step_in_cycle == changepoints)\n    else:\n      step_in_cycle = tf.math.floormod(t, num_steps_per_cycle)\n      return tf.reduce_any(\n          input_tensor=tf.equal(step_in_cycle, changepoints))\n  return is_last_day_of_season",
        "rewrite": "import numpy as np\nimport tensorflow as tf\n\ndef build_is_last_day_of_season(num_steps_per_season): \n    num_steps_per_cycle = np.sum(num_steps_per_season)\n    changepoints = np.cumsum(np.ravel(num_steps_per_season)) - 1\n    def is_last_day_of_season(t):\n        t_ = dist_util.maybe_get_static_value(t)\n        if t_ is not None:\n            step_in_cycle = t_ % num_steps_per_cycle\n            return any(step_in_cycle == changepoints)\n        else:\n            step_in_cycle = tf.math.floormod(t, num_steps_per_cycle)\n            return tf.reduce_any(\n                input_tensor=tf.equal(step_in_cycle, changepoints))\n    return is_last_day_of_season"
    },
    {
        "original": "def load_publickey(type, buffer): \n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public =",
        "rewrite": "def load_publickey(type, buffer): \n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public =\"."
    },
    {
        "original": "def clmixhess(obj, exe, arg1, arg2, delta=DELTA): \n    f, x = get_method_and_copy_of_attribute(obj, exe, arg1)\n    _, y = get_method_and_copy_of_attribute(obj, exe, arg2)\n    def hess_f(*args, **kwargs):\n        hess_val = numpy.zeros(x.shape + y.shape)\n        it = numpy.nditer(x, op_flags=['readwrite'], flags=['multi_index'])\n        for xi in it:\n            i = it.multi_index\n            jt = numpy.nditer(y, op_flags=['readwrite'], flags=['multi_index'])\n            for yj in jt:\n                j =",
        "rewrite": "def clmixhess(obj, exe, arg1, arg2, delta=DELTA): \n    f, x = get_method_and_copy_of_attribute(obj, exe, arg1)\n    _, y = get_method_and_copy_of_attribute(obj, exe, arg2)\n    def hess_f(*args, **kwargs):\n        hess_val = numpy.zeros(x.shape + y.shape)\n        it = numpy.nditer(x, op_flags=['readwrite'], flags=['multi_index'])\n        for xi in it:\n            i = it.multi_index\n            jt = numpy.nditer(y, op_flags=['readwrite'], flags=['multi_index'])\n            for yj in jt:\n                j =  # You can add any relevant code here without explanation - this area doesn't need an explanation."
    },
    {
        "original": "def create_card(self, card_json): \n        return trolly.card.Card(\n            trello_client=self,\n            card_id=card_json['id'],\n            name=card_json['name'],\n            data=card_json,\n        )",
        "rewrite": "def create_card(self, card_json):\n    return trolly.card.Card(\n        trello_client=self,\n        card_id=card_json['id'],\n        name=card_json['name'],\n        data=card_json\n    )"
    },
    {
        "original": " \n    return GeneratedPyAST(\n        node=ast.Attribute(\n            value=ast.Call(\n                func=_FIND_VAR_FN_NAME,\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME,\n                        args=[ast.Str(var_name)],\n ",
        "rewrite": "return GeneratedPyAST(\n    node=ast.Attribute(\n        value=ast.Call(\n            func=_FIND_VAR_FN_NAME,\n            args=[\n                ast.Call(\n                    func=_NEW_SYM_FN_NAME,\n                    args=[ast.Str(var_name)]\n                )\n            ]\n        )\n    )\n)"
    },
    {
        "original": "def to_esri_wkt(self): \n        return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % (self.name, self.datum.to_esri_wkt(), self.prime_mer.to_esri_wkt(), self.angunit.to_esri_wkt(), self.twin_ax[0].esri_wkt, self.twin_ax[1].esri_wkt )",
        "rewrite": "def to_esri_wkt(self): \n        return 'GEOGCS[\"%s\", %s, %s, %s, AXIS[\"Lon\", %s], AXIS[\"Lat\", %s]]' % (self.name, self.datum.to_esri_wkt(), self.prime_mer.to_esri_wkt(), self.angunit.to_esri_wkt(), self.twin_ax[0].esri_wkt, self.twin_ax[1].esri_wkt )"
    },
    {
        "original": "def set_setting(self, setting, value): \n        if setting not in self._expected_settings + self._optional_settings:\n            raise exceptions.ConfigurationError(\n                \"Setting '{0}' is not supported.\".format(setting)\n            )\n\n        if setting == 'hostname':\n            self._set_hostname(value)\n        elif setting == 'port':\n            self._set_port(value)\n        elif setting == 'certificate_path':\n       ",
        "rewrite": "def set_setting(self, setting, value): \n        if setting not in self._expected_settings + self._optional_settings:\n            raise exceptions.ConfigurationError(\n                \"Setting '{0}' is not supported.\".format(setting)\n            )\n\n        if setting == 'hostname':\n            self._set_hostname(value)\n        elif setting == 'port':\n            self._set_port(value)\n        elif setting == 'certificate_path':\n            self._set_certificate_path(value)"
    },
    {
        "original": " \n        client = self.get_conn()\n        self.log.info('Creating ReferenceImage')\n        parent = ProductSearchClient.product_path(project=project_id, location=location, product=product_id)\n\n        response = client.create_reference_image(\n            parent=parent,\n            reference_image=reference_image,\n            reference_image_id=reference_image_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n\n     ",
        "rewrite": "client = self.get_conn()\nself.log.info('Creating ReferenceImage')\nparent = ProductSearchClient.product_path(project=project_id, location=location, product=product_id)\n\nresponse = client.create_reference_image(\n    parent=parent,\n    reference_image=reference_image,\n    reference_image_id=reference_image_id,\n    retry=retry,\n    timeout=timeout,\n    metadata=metadata,\n)"
    },
    {
        "original": "def handle_user_exception(self, e): \n        exc_type, exc_value, tb = sys.exc_info()\n        assert exc_value is e\n\n        # ensure not to trash sys.exc_info() at that point in case someone\n        # wants the traceback preserved in handle_http_exception.  Of course\n        # we cannot prevent users from trashing it themselves in a custom\n        # trap_http_exception method so that's their fault then.\n        if isinstance(e, HTTPException) and not self.trap_http_exception(e):\n            return self.handle_http_exception(e)\n\n     ",
        "rewrite": "def handle_user_exception(self, e): \n    exc_type, exc_value, tb = sys.exc_info()\n\n    assert exc_value is e\n\n    if isinstance(e, HTTPException) and not self.trap_http_exception(e): \n        return self.handle_http_exception(e)"
    },
    {
        "original": "def _get_bucket(self): \n\n        # Case 1: The bucket already exists\n        try:\n            self._bucket = self._bucket_service.get_bucket(self._bucket_name)\n\n        # Case 2: The bucket needs to be created\n        except google.cloud.exceptions.NotFound:\n            self._bucket = self._bucket_service.create_bucket(self._bucket_name)\n\n        # Case 3: The bucket name is already taken\n        except:\n            bot.error('Cannot get or create %s' %self._bucket_name)\n         ",
        "rewrite": "def _get_bucket(self): \n        try: \n            self._bucket = self._bucket_service.get_bucket(self._bucket_name) \n\n        except google.cloud.exceptions.NotFound: \n            self._bucket = self._bucket_service.create_bucket(self._bucket_name) \n\n        except: \n            bot.error('Cannot get or create %s' %self._bucket_name)"
    },
    {
        "original": "def get_cards(self, **query_params): \n        cards = self.get_cards_json(self.base_uri, query_params=query_params)\n\n        cards_list = []\n        for card_json in cards:\n            cards_list.append(self.create_card(card_json))\n\n        return cards_list",
        "rewrite": "def get_cards(self, **query_params): \n    cards = self.get_cards_json(self.base_uri, query_params=query_params)\n    \n    cards_list = [self.create_card(card_json) for card_json in cards]\n    \n    return cards_list"
    },
    {
        "original": " \r\n    def decorator(view_func):\r\n        @login_required(redirect_field_name=redirect_field_name,\r\n                        login_url=login_url)\r\n        def _wrapped_view(request, *args, **kwargs):\r\n\r\n            if not (request.user.is_superuser and skip_superuser):\r\n                if request.user.groups.filter(name=group).count() == 0:\r\n                    raise PermissionDenied\r\n\r\n            return view_func(request, *args, **kwargs)\r\n       ",
        "rewrite": "def decorator(view_func):\n    @login_required(redirect_field_name=redirect_field_name,\n                    login_url=login_url)\n    def _wrapped_view(request, *args, **kwargs):\n\n        if not (request.user.is_superuser and skip_superuser):\n            if request.user.groups.filter(name=group).count() == 0:\n                raise PermissionDenied\n\n        return view_func(request, *args, **kwargs)"
    },
    {
        "original": "def _process_pong(self): \n        if len(self._pongs) > 0:\n            future = self._pongs.pop(0)\n            future.set_result(True)\n            self._pongs_received += 1\n            self._pings_outstanding -= 1",
        "rewrite": "def _process_pong(self):\n    if len(self._pongs) > 0:\n        future = self._pongs.pop(0)\n        future.set_result(True)\n        self._pongs_received += 1\n        self._pings_outstanding -= 1"
    },
    {
        "original": " \n        if x is None:\n            x=self.data\n        if xmin is None:\n            xmin=self._xmin\n        if alpha is None:\n            alpha=self._alpha\n\n        x=np.sort(x)\n        #n=len(x)\n\n        pylab.gca().set_xscale('log')\n        pylab.gca().set_yscale('log')\n\n        if dnds:\n            hb = pylab.histogram(x,bins=np.logspace(log10(min(x)),log10(max(x)),nbins))\n  ",
        "rewrite": "```python\nif x is None:\n    x = self.data\nif xmin is None:\n    xmin = self._xmin\nif alpha is None:\n    alpha = self._alpha\n\nx = np.sort(x)\n\npylab.gca().set_xscale('log')\npylab.gca().set_yscale('log')\n\nif dnds:\n    hb = pylab.histogram(x, bins=np.logspace(np.log10(min(x)), np.log10(max(x)), nbins))\n```"
    },
    {
        "original": "def unit_from_expression(expr): \n    if expr == '1':\n        return get_unit_fast(1)\n    elif isinstance(expr, str):\n        mod = ast.parse(expr, mode='eval')\n        expr = mod.body\n        return unit_from_expression(expr)\n    elif expr.__class__ is ast.Name:\n        return ALLUNITS[expr.id]\n    elif expr.__class__ is ast.Num:\n        return expr.n\n    elif expr.__class__ is ast.UnaryOp:\n        op = expr.op.__class__.__name__\n        operand = unit_from_expression(expr.operand)\n        if op=='USub':\n     ",
        "rewrite": "return -operand"
    },
    {
        "original": "def connect(self): \n    try:\n      if S3Handler.S3_KEYS:\n        self.s3 = BotoClient(self.opt, S3Handler.S3_KEYS[0], S3Handler.S3_KEYS[1])\n      else:\n        self.s3 = BotoClient(self.opt)\n    except Exception as e:\n      raise RetryFailure('Unable to connect to s3: %s' % e)",
        "rewrite": "def connect(self): \n    try:\n        if S3Handler.S3_KEYS:\n            self.s3 = BotoClient(self.opt, S3Handler.S3_KEYS[0], S3Handler.S3_KEYS[1])\n        else:\n            self.s3 = BotoClient(self.opt)\n    except Exception as e:\n        raise RetryFailure('Unable to connect to s3: %s' % e)"
    },
    {
        "original": "def synchronize(self): \n        gdocs_trans_csv = os.path.join(self.temp_path, GDOCS_TRANS_CSV)\n        gdocs_meta_csv = os.path.join(self.temp_path, GDOCS_META_CSV)\n        local_trans_csv = os.path.join(self.temp_path, LOCAL_TRANS_CSV)\n        local_meta_csv = os.path.join(self.temp_path, LOCAL_META_CSV)\n\n        try:\n            entry = self._download_csv_from_gdocs(gdocs_trans_csv,\n                                                  gdocs_meta_csv)\n        except PODocsError",
        "rewrite": "def synchronize(self): \n        gdocs_trans_csv = os.path.join(self.temp_path, GDOCS_TRANS_CSV)\n        gdocs_meta_csv = os.path.join(self.temp_path, GDOCS_META_CSV)\n        local_trans_csv = os.path.join(self.temp_path, LOCAL_TRANS_CSV)\n        local_meta_csv = os.path.join(self.temp_path, LOCAL_META_CSV)\n\n        try:\n            entry = self._download_csv_from_gdocs(gdocs_trans_csv, gdocs_meta_csv)\n        except PODocsError:"
    },
    {
        "original": "def refresh_credentials(self, credentials): \n\n        if not self._x_refresh_credentials_if(credentials):\n            return\n\n        # We need consumer key and secret to make this kind of request.\n        cfg = credentials.config.get(credentials.provider_name)\n        credentials.consumer_key = cfg.get('consumer_key')\n        credentials.consumer_secret = cfg.get('consumer_secret')\n\n        request_elements = self.create_request_elements(\n            request_type=self.REFRESH_TOKEN_REQUEST_TYPE,\n            credentials=credentials,\n            url=self.access_token_url,\n      ",
        "rewrite": "def refresh_credentials(self, credentials):\n\n        if not self._x_refresh_credentials_if(credentials):\n            return\n\n        cfg = credentials.config.get(credentials.provider_name)\n        credentials.consumer_key = cfg.get('consumer_key')\n        credentials.consumer_secret = cfg.get('consumer_secret')\n\n        request_elements = self.create_request_elements(\n            request_type=self.REFRESH_TOKEN_REQUEST_TYPE,\n            credentials=credentials,\n            url=self.access_token_url,"
    },
    {
        "original": "def get_client(self): \n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n ",
        "rewrite": "def get_client(self): \n    client = None\n    try:\n        client = self.clients.get(block=False)\n    except queue.Empty:\n        pass\n    if not client:\n        self.client_id += 1\n        kwargs = dict(self.kwargs)\n        kwargs['verbose_id'] = kwargs.get('verbose_id', '') + str(self.client_id)"
    },
    {
        "original": "def get_default_cache(): \n    try:\n        return os.environ['PYTHON_EGG_CACHE']\n    except KeyError:\n        pass\n\n    if os.name!='nt':\n        return os.path.expanduser('~/.python-eggs')\n\n    # XXX this may be locale-specific!\n    app_data = 'Application Data'\n    app_homes = [\n        # best option, should be locale-safe\n        (('APPDATA',), None),\n        (('USERPROFILE',), app_data),\n        (('HOMEDRIVE','HOMEPATH'), app_data),\n        (('HOMEPATH',), app_data),\n        (('HOME',), None),\n     ",
        "rewrite": "import os\n\n\ndef get_default_cache():\n    try:\n        return os.environ['PYTHON_EGG_CACHE']\n    except KeyError:\n        pass\n\n    if os.name != 'nt':\n        return os.path.expanduser('~/.python-eggs')\n\n    # XXX this may be locale-specific!\n    app_data = 'Application Data'\n    app_homes = [\n        # best option, should be locale-safe\n        (('APPDATA',), None),\n        (('USERPROFILE',), app_data),\n        (('HOMEDRIVE', 'HOMEPATH'), app_data),\n        (('HOMEPATH',), app_data),\n        (('HOME',), None),\n    ]"
    },
    {
        "original": "def get_body_from_file(kwds): \n    if kwds[\"file\"] and os.path.isfile(kwds[\"file\"]):\n        kwds[\"body\"] = open(kwds[\"file\"], \"r\").read()\n        kwds[\"file\"] = None",
        "rewrite": "def get_body_from_file(kwds):\n    if kwds.get(\"file\") and os.path.isfile(kwds.get(\"file\")):\n        kwds[\"body\"] = open(kwds.get(\"file\"), \"r\").read()\n        kwds[\"file\"] = None"
    },
    {
        "original": "def serial_layers(self): \n        for next_node in self.topological_op_nodes():\n            new_layer = DAGCircuit()\n            for qreg in self.qregs.values():\n                new_layer.add_qreg(qreg)\n            for creg in self.cregs.values():\n                new_layer.add_creg(creg)\n            # Save the support of the operation we add to the layer\n            support_list = []\n ",
        "rewrite": "def serial_layers(self):\n    for next_node in self.topological_op_nodes():\n        new_layer = DAGCircuit()\n        for qreg in self.qregs.values():\n            new_layer.add_qreg(qreg)\n        for creg in self.cregs.values():\n            new_layer.add_creg(creg)\n        # Save the support of the operation we add to the layer\n        support_list = []"
    },
    {
        "original": "def remove_accelerator(control, key): \n key = str_to_key(key)\n t = _tables.get(control, [])\n for a in t:\n  if a[:2] == key:\n   t.remove(a)\n   if t:\n    _tables[control] = t\n   else:\n    del _tables[control]\n   update_accelerators(control)\n   return True\n return False",
        "rewrite": "def remove_accelerator(control, key):\n    key = str_to_key(key)\n    t = _tables.get(control, [])\n    to_remove = []\n    for a in t:\n        if a[:2] == key:\n            to_remove.append(a)\n    \n    for a in to_remove:\n        t.remove(a)\n    \n    if t:\n        _tables[control] = t\n    else:\n        del _tables[control]\n    \n    update_accelerators(control)\n    return True\n    \n    return False"
    },
    {
        "original": "def delete_dashboard(self, dashboard): \n        if 'id' not in dashboard:\n            return [False, \"Invalid dashboard format\"]\n\n        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        return [True, None]",
        "rewrite": "def delete_dashboard(self, dashboard): \n    if 'id' not in dashboard:\n        return [False, \"Invalid dashboard format\"]\n\n    res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n    if not self._checkResponse(res):\n        return [False, self.lasterr]\n\n    return [True, None]"
    },
    {
        "original": "def create(self, name, regexes, tag_ids, logs=None): \n        data = {\n            'name': name,\n            'triggers': regexes,\n            'sources': logs or [],\n            'groups': [],\n            'actions': tag_ids\n        }\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.HOOKS.value,\n   ",
        "rewrite": "def create(self, name, regexes, tag_ids, logs=None): \n        data = {\n            'name': name,\n            'triggers': regexes,\n            'sources': logs or [],\n            'groups': [],\n            'actions': tag_ids\n        }\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.HOOKS.value,"
    },
    {
        "original": "def p_select_from_where_statement_1(self, p): \n        p[0] = SelectFromWhereNode(cardinality=p[2],\n                                   variable_name=p[3],\n                                   key_letter=p[7],\n                                   where_clause=p[9])",
        "rewrite": "def p_select_from_where_statement_1(self, p): \n        p[0] = SelectFromWhereNode(cardinality=p[2],\n                                   variable_name=p[3],\n                                   key_letter=p[7],\n                                   where_clause=p[9])"
    },
    {
        "original": "def save_as(self, info): \n        if not info.initialized:\n            return\n\n#        retval = self.edit_traits(parent=info.ui.control, view=\"file_view\")\n\n        dlg = FileDialog( action = \"save as\",\n            wildcard = \"Graphviz Files (*.dot, *.xdot, *.txt)|\" \\\n                \"*.dot;*.xdot;*.txt|Dot Files (*.dot)|*.dot|\" \\\n                \"All Files (*.*)|*.*|\")\n\n        if dlg.open() == OK:\n          ",
        "rewrite": "if dlg.path.endswith(\".dot\"):\n                with open(dlg.path, 'w') as file:\n                    file.write(info.save_dot())\n            elif dlg.path.endswith(\".xdot\"):\n                with open(dlg.path, 'w') as file:\n                    file.write(info.save_xdot())\n            else:\n                with open(dlg.path, 'w') as file:\n                    file.write(info.save_txt())"
    },
    {
        "original": "def list_to_str(lst: list, content: str, indent: int=1): \n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content",
        "rewrite": "from typing import List\n\ndef list_to_str(lst: List, content: str, indent: int = 1):\n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content"
    },
    {
        "original": "def format_unitary(mat, decimals=None): \n    num_basis = len(mat)\n    mat_complex = np.zeros((num_basis, num_basis), dtype=complex)\n    for i, vec in enumerate(mat):\n        mat_complex[i] = format_statevector(vec, decimals)\n    return mat_complex",
        "rewrite": "import numpy as np\n\ndef format_unitary(mat, decimals=None):\n    num_basis = len(mat)\n    mat_complex = np.zeros((num_basis, num_basis), dtype=complex)\n    for i, vec in enumerate(mat):\n        mat_complex[i] = format_statevector(vec, decimals)\n    return mat_complex"
    },
    {
        "original": "def HStruct_selectFields(structT, fieldsToUse): \n\n    template = []\n    fieldsToUse = fieldsToUse\n    foundNames = set()\n\n    for f in structT.fields:\n        name = None\n        subfields = []\n\n        if f.name is not None:\n            try:\n                if isinstance(fieldsToUse, dict):\n                    subfields = fieldsToUse[f.name]\n               ",
        "rewrite": "def HStruct_selectFields(structT, fieldsToUse): \n\n    template = []\n    fieldsToUse = fieldsToUse\n    foundNames = set()\n\n    for f in structT.fields:\n        name = None\n        subfields = []\n\n        if f.name is not None:\n            try:\n                if isinstance(fieldsToUse, dict):\n                    subfields = fieldsToUse[f.name]"
    },
    {
        "original": "def parse_transcripts(transcript_lines): \n    LOG.info(\"Parsing transcripts\")\n    # Parse the transcripts, we need to check if it is a request or a file handle\n    if isinstance(transcript_lines, DataFrame):\n        transcripts = parse_ensembl_transcript_request(transcript_lines)\n    else:\n        transcripts = parse_ensembl_transcripts(transcript_lines)\n\n    # Since there can be multiple lines with information about the same transcript\n    # we store transcript information in a dictionary for now\n    parsed_transcripts = {}\n    # Loop over the parsed transcripts\n    for tx in transcripts:\n        tx_id = tx['ensembl_transcript_id']\n        ens_gene_id =",
        "rewrite": "def parse_transcripts(transcript_lines):\n    LOG.info(\"Parsing transcripts\")\n    # Parse the transcripts and handle different input types\n    if isinstance(transcript_lines, DataFrame):\n        transcripts = parse_ensembl_transcript_request(transcript_lines)\n    else:\n        transcripts = parse_ensembl_transcripts(transcript_lines)\n\n    parsed_transcripts = {}\n    for tx in transcripts:\n        tx_id = tx['ensembl_transcript_id']\n        ens_gene_id = \".\""
    },
    {
        "original": "def _make_folder(self, traj): \n        print_folder = os.path.join(traj.analysis.plot_folder,\n                                    traj.v_name, traj.v_crun)\n        print_folder = os.path.abspath(print_folder)\n        if not os.path.isdir(print_folder):\n            os.makedirs(print_folder)\n\n        return print_folder",
        "rewrite": "def _make_folder(self, traj): \n    print_folder = os.path.abspath(os.path.join(traj.analysis.plot_folder, traj.v_name, traj.v_crun))\n    if not os.path.isdir(print_folder):\n        os.makedirs(print_folder)\n    return print_folder"
    },
    {
        "original": " \n\n        if turn_context.activity.channel_id is None:\n            return \"\"\n        else:\n            return turn_context.activity.channel_id",
        "rewrite": "if not turn_context.activity.channel_id:\n    return \"\"\nelse:\n    return turn_context.activity.channel_id"
    },
    {
        "original": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint): \n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_get(\n            '/' + self.subscription_id + '/services/hostedservices/' +\n            _str(service_name) + '/certificates/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',\n            Certificate)",
        "rewrite": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n    _validate_not_none('service_name', service_name)\n    _validate_not_none('thumbalgorithm', thumbalgorithm)\n    _validate_not_none('thumbprint', thumbprint)\n    return self._perform_get(\n        '/' + self.subscription_id + '/services/hostedservices/' +\n        str(service_name) + '/certificates/' +\n        str(thumbalgorithm) + '-' + str(thumbprint) + '',\n        Certificate)"
    },
    {
        "original": "def read_file_chunk(self, source, pos, chunk): \n    if chunk==0:\n        return StringIO()\n    data = None\n    with open(source, 'rb') as f:\n      f.seek(pos)\n      data = f.read(chunk)\n    if not data:\n      raise Failure('Unable to read data from source: %s' % source)\n    return StringIO(data)",
        "rewrite": "def read_file_chunk(self, source, pos, chunk): \n    if chunk == 0:\n        return StringIO()\n    data = None\n    with open(source, 'rb') as f:\n        f.seek(pos)\n        data = f.read(chunk)\n    if not data:\n        raise Failure('Unable to read data from source: %s' % source)\n    return StringIO(data)"
    },
    {
        "original": "def api_bikes(request): \n    postcode: Optional[str] = request.match_info.get('postcode', None)\n\n    try:\n        radius = int(request.match_info.get('radius', 10))\n    except ValueError:\n        raise web.HTTPBadRequest(text=\"Invalid Radius\")\n\n    try:\n        postcode = (await get_postcode_random()) if postcode == \"random\" else postcode\n        bikes = await get_bikes(postcode, radius)\n    except CachingError as e:\n        raise web.HTTPInternalServerError(text=e.status)\n    else:\n        if bikes is None:\n            raise web.HTTPNotFound(text=\"Post code does not exist.\")\n      ",
        "rewrite": "def api_bikes(request): \n    postcode: Optional[str] = request.match_info.get('postcode', None)\n\n    try:\n        radius = int(request.match_info.get('radius', 10))\n    except ValueError:\n        raise web.HTTPBadRequest(text=\"Invalid Radius\")\n\n    try:\n        postcode = (await get_postcode_random()) if postcode == \"random\" else postcode\n        bikes = await get_bikes(postcode, radius)\n    except CachingError as e:\n        raise web.HTTPInternalServerError(text=e.status)\n    else:\n        if bikes is None:\n            raise web.HTTPNotFound(text=\"Post code does not exist.\")"
    },
    {
        "original": "def _execute_core_transform(transform_context, inputs): \n    check.inst_param(transform_context, 'transform_context', SystemTransformExecutionContext)\n    check.dict_param(inputs, 'inputs', key_type=str)\n\n    step = transform_context.step\n    solid = step.solid\n\n    transform_context.log.debug(\n        'Executing core transform for solid {solid}.'.format(solid=solid.name)\n    )\n\n    all_results = []\n    for step_output in _yield_transform_results(transform_context, inputs):\n        yield step_output\n        if isinstance(step_output, StepOutputValue):\n            all_results.append(step_output)\n\n    if len(all_results) != len(solid.definition.output_defs):\n        emitted_result_names = {r.output_name for r in all_results}\n        solid_output_names = {output_def.name for output_def in",
        "rewrite": "Solid.definition.output_defs}\n\n        missing_results = solid_output_names - emitted_result_names\n        transform_context.log.warn(\n            'Solid {solid} did not return values for output(s) {missing_results}.'.format(\n                solid=solid.name, missing_results=missing_results\n            )\n        )"
    },
    {
        "original": "def convert_from_binary(self, binvalue, type, **kwargs): \n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)",
        "rewrite": "def convert_from_binary(self, binvalue, type, **kwargs):\n\n    size = self.get_type_size(type)\n    if size > 0 and len(binvalue) != size:\n        raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n    typeobj = self.get_type(type)\n\n    if not hasattr(typeobj, 'convert_binary'):\n        raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n    return typeobj.convert_binary(binvalue, **kwargs)"
    },
    {
        "original": "def der(self): \n        result_buffer = _ffi.new('unsigned char**')\n        encode_result = _lib.i2d_X509_NAME(self._name, result_buffer)\n        _openssl_assert(encode_result >= 0)\n\n        string_result = _ffi.buffer(result_buffer[0], encode_result)[:]\n        _lib.OPENSSL_free(result_buffer[0])\n        return string_result",
        "rewrite": "def der(self): \n    result_buffer = _ffi.new('unsigned char**')\n    encode_result = _lib.i2d_X509_NAME(self._name, result_buffer)\n    _openssl_assert(encode_result >= 0)\n\n    string_result = bytes(_ffi.buffer(result_buffer[0], encode_result))\n    _lib.OPENSSL_free(result_buffer[0])\n    return string_result"
    },
    {
        "original": "def _deprecated_register(self, py_class, to_om, om_cd, om_name, to_py=None): \n        self.register_to_python(om_cd, om_name, to_py)\n        self.register_to_openmath(py_class, to_om)",
        "rewrite": "def _deprecated_register(self, py_class, to_om, om_cd, om_name, to_py=None):\n    self.register_to_python(om_cd, om_name, to_py)\n    self.register_to_openmath(py_class, to_om)"
    },
    {
        "original": "def on_redraw(self): \n        x,y = self.pos\n        sx,sy = self.size\n        self.bg_vlist.vertices = [x,y, x+sx,y, x+sx,y+sy, x,y+sy]\n        self.stencil_vlist.vertices = [x,y, x+sx,y, x+sx,y+sy, x,y+sy]\n        if isinstance(self.bg,Background):\n            if not self.bg.initialized:\n                self.bg.init_bg()\n                self.bg.initialized=True\n            self.bg.redraw_bg()",
        "rewrite": "def on_redraw(self):\n    x, y = self.pos\n    sx, sy = self.size\n    self.bg_vlist.vertices = [x, y, x+sx, y, x+sx, y+sy, x, y+sy]\n    self.stencil_vlist.vertices = [x, y, x+sx, y, x+sx, y+sy, x, y+sy]\n  \n    if isinstance(self.bg, Background):\n        if not self.bg.initialized:\n            self.bg.init_bg()\n            self.bg.initialized = True\n        \n        self.bg.redraw_bg()"
    },
    {
        "original": "def pexpect_monkeypatch(): \n\n    if pexpect.__version__[:3] >= '2.2':\n        # No need to patch, fix is already the upstream version.\n        return\n\n    def __del__(self):",
        "rewrite": "def pexpect_monkeypatch(): \n\n    if pexpect.__version__[:3] >= '2.2':\n        return\n\n    def __del__(self):"
    },
    {
        "original": "def _make_list_or_1d_tensor(values): \n  values = tf.convert_to_tensor(value=values, name='values')\n  values_ = tf.get_static_value(values)\n\n  # Static didn't work.\n  if values_ is None:\n    # Cheap way to bring to at least 1d.\n    return values + tf.zeros([1], dtype=values.dtype)\n\n  # Static worked!\n  if values_.ndim > 1:\n    raise ValueError('values had > 1 dim: {}'.format(values_.shape))\n  # Cheap way to bring to at least 1d.\n  values_ = values_ + np.zeros([1], dtype=values_.dtype)\n  return list(values_)",
        "rewrite": "def _make_list_or_1d_tensor(values): \n    values = tf.convert_to_tensor(value=values, name='values')\n    values_ = tf.get_static_value(values)\n\n    if values_ is None:\n        return values + tf.zeros([1], dtype=values.dtype)\n\n    if values_.ndim > 1:\n        raise ValueError('values had > 1 dim: {}'.format(values_.shape))\n    \n    values_ = values_ + np.zeros([1], dtype=values_.dtype)\n    return list(values_)"
    },
    {
        "original": "def get_int(errmsg, arg, default=1, cmdname=None): \n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n             ",
        "rewrite": "def get_int(errmsg, arg, default=1, cmdname=None):\n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n             cmdname, arg)"
    },
    {
        "original": "def fit(self, Z, classes=None): \n        check_rdd(Z, {'X': (sp.spmatrix, np.ndarray), 'y': (sp.spmatrix, np.ndarray)})\n        if 'w' in Z.columns:\n            models = Z[:, ['X', 'y', 'w']].map(\n                lambda X_y_w: self.partial_fit(\n                    X_y_w[0], X_y_w[1], classes, X_y_w[2]\n                )\n            )\n        else:\n    ",
        "rewrite": "def fit(self, Z, classes=None): \n    check_rdd(Z, {'X': (sp.spmatrix, np.ndarray), 'y': (sp.spmatrix, np.ndarray)})\n    if 'w' in Z.columns:\n        models = Z[:, ['X', 'y', 'w']].map(lambda X_y_w: self.partial_fit(X_y_w[0], X_y_w[1], classes, X_y_w[2]))\n    else:\n        # Add functionality for else situation\n        pass"
    },
    {
        "original": "def save_tracks(self, *tracks): \n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))",
        "rewrite": "def save_tracks(self, *tracks):\n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))"
    },
    {
        "original": "def _initialize_master_working_set(): \n    working_set = WorkingSet._build_master()\n    _declare_state('object', working_set=working_set)\n\n    require = working_set.require\n    iter_entry_points = working_set.iter_entry_points\n    add_activation_listener = working_set.subscribe\n    run_script = working_set.run_script\n    # backward compatibility\n    run_main = run_script\n    # Activate all distributions already on sys.path, and ensure that\n    # all distributions added to the working set in the future (e.g. by\n    # calling ``require()``) will get activated as well.\n    add_activation_listener(lambda dist: dist.activate())\n    working_set.entries=[]\n    # match order\n    list(map(working_set.add_entry, sys.path))\n    globals().update(locals())",
        "rewrite": "def _initialize_master_working_set(): \n    working_set = WorkingSet._build_master()\n    _declare_state('object', working_set=working_set)\n\n    require = working_set.require\n    iter_entry_points = working_set.iter_entry_points\n    add_activation_listener = working_set.subscribe\n    run_script = working_set.run_script\n    # backward compatibility\n    run_main = run_script\n    # Activate all distributions already on sys.path, and ensure that\n    # all distributions added to the working set in the future (e.g. by\n    # calling ``require()``) will get activated as well.\n    add_activation_listener(lambda dist: dist.activate())\n    working_set.entries=[]\n    # match order\n    list(map(working_set.add_entry, sys.path))\n    globals().update(locals())"
    },
    {
        "original": "def _read_internal_waveform_packet(self): \n        # This is strange, the spec says, waveform data packet is in a EVLR\n        #  but in the 2 samples I have its a VLR\n        # but also the 2 samples have a wrong user_id (LAS_Spec instead of LASF_Spec)\n        b = bytearray(self.stream.read(rawvlr.VLR_HEADER_SIZE))\n        waveform_header = rawvlr.RawVLRHeader.from_buffer(b)\n        waveform_record = self.stream.read()\n        logger.debug(\n            \"Read: {} MBytes of waveform_record\".format(len(waveform_record) / 10 ** 6)\n      ",
        "rewrite": "def _read_internal_waveform_packet(self):\n    # This is strange, as the spec suggests the waveform data packet is in an EVLR,\n    # yet in the 2 samples I have examined it appears to be a VLR.\n    # Furthermore, both samples have an incorrect user_id (LAS_Spec instead of LASF_Spec).\n    b = bytearray(self.stream.read(rawvlr.VLR_HEADER_SIZE))\n    waveform_header = rawvlr.RawVLRHeader.from_buffer(b)\n    waveform_record = self.stream.read()\n    logger.debug(\"Read: {} MBytes of waveform_record\".format(len(waveform_record) / 10 ** 6))"
    },
    {
        "original": " \n    result = (parser << eof).consume(reader)\n\n    if isinstance(result, Continue):\n        return Success(result.value)\n    else:\n        used = set()\n        unique_expected = []\n        for expected_lambda in result.expected:\n            expected = expected_lambda()\n            if expected not in used:\n                used.add(expected)\n                unique_expected.append(expected)\n\n      ",
        "rewrite": "result = (parser << eof).consume(reader)\n\nif isinstance(result, Continue):\n    return Success(result.value)\nelse:\n    used = set()\n    unique_expected = []\n    for expected_lambda in result.expected:\n        expected = expected_lambda()\n        if expected not in used:\n            used.add(expected)\n            unique_expected.append(expected)"
    },
    {
        "original": "def _store(self): \n        store_dict = {}\n\n        if self._data is not None:\n            dump = pickle.dumps(self._data, protocol=self.v_protocol)\n            store_dict['data'] = dump\n            store_dict[PickleParameter.PROTOCOL] = self.v_protocol\n\n        if self.f_has_range():\n\n            store_dict['explored_data'] = \\\n                ObjectTable(columns=['idx'], index=list(range(len(self))))\n\n            smart_dict = {}\n      ",
        "rewrite": "def _store(self):\n    store_dict = {}\n    \n    if self._data is not None:\n        dump = pickle.dumps(self._data, protocol=self.v_protocol)\n        store_dict['data'] = dump\n        store_dict[PickleParameter.PROTOCOL] = self.v_protocol\n        \n    if self.f_has_range():\n        store_dict['explored_data'] = ObjectTable(columns=['idx'], index=list(range(len(self)))\n        \n        smart_dict = {}"
    },
    {
        "original": "def from_label(cls, label): \n        z = np.zeros(len(label), dtype=np.bool)\n        x = np.zeros(len(label), dtype=np.bool)\n        for i, char in enumerate(label):\n            if char == 'X':\n                x[-i - 1] = True\n            elif char == 'Z':\n                z[-i - 1] = True\n            elif char == 'Y':\n     ",
        "rewrite": "def from_label(cls, label):\n    z = np.zeros(len(label), dtype=np.bool)\n    x = np.zeros(len(label), dtype=np.bool)\n    for i, char in enumerate(label):\n        if char == 'X':\n            x[-i - 1] = True\n        elif char == 'Z':\n            z[-i - 1] = True\n        elif char == 'Y':\n            pass"
    },
    {
        "original": "def write(self, directory): \n        status_file = os.path.join(directory, self.STATUS_FILE)\n        status = {\n            'format': self.STATUS_FORMAT,\n            'version': coverage.__version__,\n            'settings': self.settings,\n            'files': self.files,\n            }\n        fout = open(status_file, \"wb\")\n        try:\n            pickle.dump(status, fout)\n       ",
        "rewrite": "def write(self, directory): \n    status_file = os.path.join(directory, self.STATUS_FILE)\n    status = {\n        'format': self.STATUS_FORMAT,\n        'version': coverage.__version__,\n        'settings': self.settings,\n        'files': self.files,\n    }\n    with open(status_file, \"wb\") as fout:\n        pickle.dump(status, fout)"
    },
    {
        "original": "def eventstr(event_tuple=None, event=None, register=None, parameters=None): \n    if len(event_tuple) == 3:\n        event, register, parameters = event_tuple\n    elif len(event_tuple) == 2:\n        event, register = event_tuple\n    event_dscr = [event, register]\n\n    if parameters:\n        for k, v in sorted(event_tuple[2].items()):  # sorted for reproducability\n            if type(v) is int:\n                k += \"={}\".format(hex(v))\n            event_dscr.append(k)\n    return \":\".join(event_dscr)",
        "rewrite": "def eventstr(event_tuple=None, event=None, register=None, parameters=None):\n    if len(event_tuple) == 3:\n        event, register, parameters = event_tuple\n    elif len(event_tuple) == 2:\n        event, register = event_tuple\n    event_dscr = [event, register]\n\n    if parameters:\n        for k, v in sorted(parameters.items()):  \n            if type(v) is int:\n                k += \"={}\".format(hex(v))\n            event_dscr.append(k)\n    return \":\".join(event_dscr)"
    },
    {
        "original": "def from_csv(cls, filename=None, text=None): \n        if (filename is None) and (text is None):\n            raise LegendError(\"You must provide a filename or CSV text.\")\n\n        if (filename is not None):\n            with open(filename, 'r') as f:\n                text = f.read()\n\n        try:\n            f = StringIO(text)  # Python 3\n        except TypeError:\n       ",
        "rewrite": "def from_csv(cls, filename=None, text=None): \n    if (filename is None) and (text is None):\n        raise LegendError(\"You must provide a filename or CSV text.\")\n\n    if (filename is not None):\n        with open(filename, 'r') as f:\n            text = f.read()\n\n    try:\n        f = StringIO(text)  \n    except TypeError:"
    },
    {
        "original": " \n        if _hdf5_group is None:\n            _hdf5_group = self._all_get_node_by_name(traj_group.v_full_name)\n            _traj = traj_group.v_root\n\n        if recursive:\n            parent_traj_node = traj_group.f_get_parent()\n            self._tree_load_nodes_dfs(parent_traj_node, load_data=load_data, with_links=with_links,\n                                  recursive=recursive, max_depth=max_depth,\n               ",
        "rewrite": "if _hdf5_group is None:\n    _hdf5_group = self._all_get_node_by_name(traj_group.v_full_name)\n    _traj = traj_group.v_root\n\nif recursive:\n    parent_traj_node = traj_group.f_get_parent()\n    self._tree_load_nodes_dfs(parent_traj_node, load_data=load_data, with_links=with_links,\n                              recursive=recursive, max_depth=max_depth)"
    },
    {
        "original": "def markup_fragment(source, encoding=None): \n    doc = parse(source, encoding=encoding)\n    frag = doc.html.body\n    return frag",
        "rewrite": "def markup_fragment(source, encoding=None):\n    doc = parse(source, encoding=encoding)\n    frag = doc.find('html').find('body')\n    return frag"
    },
    {
        "original": "def delete_qubits(self, indices): \n        if not isinstance(indices, list):\n            indices = [indices]\n\n        self._z = np.delete(self._z, indices)\n        self._x = np.delete(self._x, indices)\n\n        return self",
        "rewrite": "def delete_qubits(self, indices): \n        if not isinstance(indices, list):\n            indices = [indices]\n\n        self._z = np.delete(self._z, indices)\n        self._x = np.delete(self._x, indices)\n\n        return self"
    },
    {
        "original": "def is_invalid_marker(cls, text): \n        try:\n            cls.evaluate_marker(text)\n        except SyntaxError as e:\n            return cls.normalize_exception(e)\n        return False",
        "rewrite": "def is_invalid_marker(cls, text):\n    try:\n        cls.evaluate_marker(text)\n    except SyntaxError as e:\n        return cls.normalize_exception(e)\n    else:\n        return False"
    },
    {
        "original": "def name(self): \n        if self._name_to_value is None:\n            self._name_to_value = {str(d): v for d, v in zip(self._descriptors, self._values)}\n\n        return GetValueByName(self._name_to_value)",
        "rewrite": "def name(self):\n    if self._name_to_value is None:\n        self._name_to_value = {str(d): v for d, v in zip(self._descriptors, self._values)}\n\n    return GetValueByName(self._name_to_value)"
    },
    {
        "original": " \n    core_ns = Namespace.get(sym.symbol(CORE_NS))\n    assert core_ns is not None\n    return lobj.lrepr(\n        o,\n        human_readable=human_readable,\n        print_dup=core_ns.find(sym.symbol(_PRINT_DUP_VAR_NAME)).value,  # type: ignore\n        print_length=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LENGTH_VAR_NAME)\n        ).value,\n        print_level=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LEVEL_VAR_NAME)\n        ).value,\n        print_meta=core_ns.find(sym.symbol(_PRINT_META_VAR_NAME)).value,  # type: ignore\n   ",
        "rewrite": "core_ns = Namespace.get(sym.symbol(CORE_NS))\nassert core_ns is not None\nreturn lobj.lrepr(\n    o,\n    human_readable=human_readable,\n    print_dup=core_ns.find(sym.symbol(_PRINT_DUP_VAR_NAME)).value,\n    print_length=core_ns.find(sym.symbol(_PRINT_LENGTH_VAR_NAME)).value,\n    print_level=core_ns.find(sym.symbol(_PRINT_LEVEL_VAR_NAME)).value,\n    print_meta=core_ns.find(sym.symbol(_PRINT_META_VAR_NAME)).value)"
    },
    {
        "original": "def parse_path(path): \n    if path is None:\n        raise ValueError(\"path must be a string\")\n\n    parts = path.strip(\"/\").split(\"/\")\n\n    database = unquote_plus(parts[0]) if len(parts) else None\n    schema = parts[1] if len(parts) > 1 else None\n\n    return database, schema",
        "rewrite": "def parse_path(path):\n    if path is None:\n        raise ValueError(\"path must be a string\")\n    \n    parts = path.strip(\"/\").split(\"/\")\n    \n    database = unquote_plus(parts[0]) if len(parts) else None\n    schema = parts[1] if len(parts) > 1 else None\n    \n    return database, schema"
    },
    {
        "original": "def remove_empty(self, tag): \n        has_children = len(tag.contents)\n        has_text = len(list(tag.stripped_strings))\n        if not has_children and not has_text and not tag.is_empty_element:\n            tag.extract()",
        "rewrite": "def remove_empty(self, tag):\n    has_children = len(tag.contents)\n    has_text = len(list(tag.stripped_strings))\n    if not has_children and not has_text and not tag.is_empty_element:\n        tag.extract()"
    },
    {
        "original": "def _random_bernoulli(shape, probs, dtype=tf.int32, seed=None, name=None): \n  with tf.compat.v1.name_scope(name, \"random_bernoulli\", [shape, probs]):\n    probs = tf.convert_to_tensor(value=probs)\n    random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n    return tf.cast(tf.less(random_uniform, probs), dtype)",
        "rewrite": "def _random_bernoulli(shape, probs, dtype=tf.int32, seed=None, name=None):\n    with tf.name_scope(name, \"random_bernoulli\", [shape, probs]):\n        probs = tf.convert_to_tensor(value=probs)\n        random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n        return tf.cast(tf.less(random_uniform, probs), dtype)"
    },
    {
        "original": "def complete(self, text, line, cursor_pos, block=None): \n        content = dict(text=text, line=line, block=block, cursor_pos=cursor_pos)\n        msg = self.session.msg('complete_request', content)\n        self._queue_send(msg)\n        return msg['header']['msg_id']",
        "rewrite": "def complete(self, text, line, cursor_pos, block=None): \n    content = dict(text=text, line=line, block=block, cursor_pos=cursor_pos)\n    msg = self.session.msg('complete_request', content)\n    self._queue_send(msg)\n    return msg['header']['msg_id']"
    },
    {
        "original": " \n        if self.begin < interval.end and interval.begin < self.end:\n            return True\n        return False",
        "rewrite": "return self.begin < interval.end and interval.begin < self.end"
    },
    {
        "original": " \n        if not isinstance(card, SigninCard):\n            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.signin_card,\n                          content=card)",
        "rewrite": "if not isinstance(card, SigninCard):\n    raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of a SigninCard, unable to prepare attachment.')\n\nreturn Attachment(content_type=CardFactory.content_types.signin_card,\n                  content=card)"
    },
    {
        "original": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True): \n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(",
        "rewrite": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True): \n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(\"SELECT * FROM user_affinity JOIN item_similarity ORDER BY score DESC LIMIT {top_k}\")\n        return query"
    },
    {
        "original": "def authenticate_client(self, request, *args, **kwargs): \n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        # http://tools.ietf.org/html/rfc6749#section-2\n        # The client MAY omit the parameter if the client secret is an empty string.\n        if hasattr(client,",
        "rewrite": "def authenticate_client(self, request, *args, **kwargs): \n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        if client_secret == \"\":\n            return True\n\n        return False"
    },
    {
        "original": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        super(ExtensionInformation, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.extension_name.read(tstream, kmip_version=kmip_version)\n\n        if self.is_tag_next(Tags.EXTENSION_TAG, tstream):\n            self.extension_tag = ExtensionTag()\n            self.extension_tag.read(tstream, kmip_version=kmip_version)\n        if self.is_tag_next(Tags.EXTENSION_TYPE, tstream):\n            self.extension_type = ExtensionType()\n  ",
        "rewrite": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    super(ExtensionInformation, self).read(\n        istream,\n        kmip_version=kmip_version\n    )\n    tstream = BytearrayStream(istream.read(self.length))\n\n    self.extension_name.read(tstream, kmip_version=kmip_version)\n\n    if self.is_tag_next(Tags.EXTENSION_TAG, tstream):\n        self.extension_tag = ExtensionTag()\n        self.extension_tag.read(tstream, kmip_version=kmip_version)\n    if self.is_tag_next(Tags.EXTENSION_TYPE, tstream):\n        self.extension_type = ExtensionType()"
    },
    {
        "original": "def from_base62(s): \n    result = 0\n\n    for c in s:\n        if c not in BASE62_MAP:\n            raise Exception('Invalid base64 string: %s' % s)\n\n        result = result * 62 + BASE62_MAP.index(c)\n\n    return result",
        "rewrite": "def from_base62(s):\n    result = 0\n    \n    BASE62_MAP = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n\n    for c in s:\n        if c not in BASE62_MAP:\n            raise Exception('Invalid base62 string: %s' % s)\n        \n        result = result * 62 + BASE62_MAP.index(c)\n    \n    return result"
    },
    {
        "original": "def add_schema_to_dependency_array(schema, ordered_schemas, schemas_map): \n    ordered_schemas[schema[\"name\"]] = schema\n    for field in schema[\"fields\"]:\n        field_schema_name = field[\"schema_name\"]\n        if field_schema_name is None: continue\n        if field_schema_name in ordered_schemas: continue\n        if field[\"type\"].startswith(\"enum\"):\n            ordered_schemas[field_schema_name] = field[\"values\"]\n        else:\n            field_schema = schemas_map[field_schema_name]\n            if field_schema[\"name\"] not in ordered_schemas:\n               ",
        "rewrite": "ordered_schemas[field_schema[\"name\"]] = field_schema"
    },
    {
        "original": "def default_configfile(base_filename): \n    file_dir = os.path.join(os.environ.get('HOME', '~'), '.config', 'trepanpy')\n    file_dir = Mclifns.path_expanduser_abs(file_dir)\n\n    if not os.path.isdir(file_dir):\n        os.makedirs(file_dir, mode=0o755)\n    return os.path.join(file_dir, base_filename)",
        "rewrite": "def default_configfile(base_filename): \n    file_dir = os.path.join(os.path.expanduser('~'), '.config', 'trepanpy')\n    file_dir = os.path.abspath(file_dir)\n\n    if not os.path.isdir(file_dir):\n        os.makedirs(file_dir, mode=0o755)\n    return os.path.join(file_dir, base_filename)"
    },
    {
        "original": " \n        mod = self.imports.entry(sym, None)\n        if mod is None:\n            alias = self.import_aliases.get(sym, None)\n            if alias is None:\n                return None\n            return self.imports.entry(alias, None)\n        return mod",
        "rewrite": "mod = self.imports.entry(sym, None)\nif mod is None:\n    alias = self.import_aliases.get(sym, None)\n    if alias is None:\n        return None\n    return self.imports.entry(alias, None)\nreturn mod"
    },
    {
        "original": "def choi_to_rauli(choi, order=1): \n    if order == 0:\n        order = 'weight'\n    elif order == 1:\n        order = 'tensor'\n\n    # get number of qubits'\n    num_qubits = int(np.log2(np.sqrt(len(choi))))\n    pgp = pauli_group(num_qubits, case=order)\n    rauli = []\n    for i in pgp:\n        for j in pgp:\n            pauliop = np.kron(j.to_matrix().T, i.to_matrix())\n            rauli += [np.trace(np.dot(choi, pauliop))]\n    return np.array(rauli).reshape(4 ** num_qubits, 4 ** num_qubits)",
        "rewrite": "def choi_to_rauli(choi, order=1): \n    if order == 0:\n        order = 'weight'\n    elif order == 1:\n        order = 'tensor'\n\n    num_qubits = int(np.log2(np.sqrt(len(choi))))\n    pgp = pauli_group(num_qubits, case=order)\n    rauli = []\n    \n    for i in pgp:\n        for j in pgp:\n            pauliop = np.kron(j.to_matrix().T, i.to_matrix())\n            rauli.append(np.trace(np.dot(choi, pauliop)))\n    \n    return np.array(rauli).reshape(4 ** num_qubits, 4 ** num_qubits)"
    },
    {
        "original": "def save(self, *args, **kwargs): \n        if self.ask_price < 0:\n            raise ValidationError(\"Ask price must be greater than zero\")\n        if self.bid_price < 0:\n            raise ValidationError(\"Bid price must be greater than zero\")\n        if self.ask_price < self.bid_price:\n            raise ValidationError(\"Ask price must be at least Bid price\")\n\n        super(CurrencyPrice, self).save(*args, **kwargs)",
        "rewrite": "def save(self, *args, **kwargs): \n    if self.ask_price < 0:\n        raise ValidationError(\"Ask price must be greater than zero\")\n    if self.bid_price < 0:\n        raise ValidationError(\"Bid price must be greater than zero\")\n    if self.ask_price < self.bid_price:\n        raise ValidationError(\"Ask price must be at least Bid price\")\n\n    super().save(*args, **kwargs)"
    },
    {
        "original": " \n        qr = qrc.QRCode(\n            version=version,\n            error_correction=cls.correction_levels[error_correction],\n            box_size=box_size,\n            border=border,\n        )\n        qr.add_data(data)\n        qr.make(fit=fit)\n\n        fcolor = (\n            fill_color\n            if fill_color.lower() in cls.color or fill_color.startswith(\"#\")\n     ",
        "rewrite": "qr = qrc.QRCode(\n    version=version,\n    error_correction=error_correction,\n    box_size=box_size,\n    border=border,\n)\nqr.add_data(data)\nqr.make(fit=fit)\n\nfcolor = fill_color if fill_color.lower() in cls.color or fill_color.startswith(\"#\")"
    },
    {
        "original": "def brew_recipe(recipe_name): \n\n    # This will iterate over all modules included in the recipes subpackage\n    # It will return the import class and the module name, algon with the\n    # correct prefix\n    prefix = \"{}.\".format(recipes.__name__)\n    for importer, modname, _ in pkgutil.iter_modules(recipes.__path__, prefix):\n\n        # Import the current module\n        _module = importer.find_module(modname).load_module(modname)\n\n        # Fetch all available classes in module\n        _recipe_classes = [cls for cls in _module.__dict__.values() if\n                    ",
        "rewrite": "def brew_recipe(recipe_name):\n    prefix = \"{}.\".format(recipes. __name__)\n    for importer, modname, _ in pkgutil.iter_modules(recipes.__path__, prefix):\n        _module = importer.find_module(modname).load_module(modname)\n        _recipe_classes = [cls for cls in _module.__dict__.values()]\n        # No need to explain. Just write code:"
    },
    {
        "original": "def variables(self): \n        result = self.inputs\n        seen = set(i.name for i in result)\n        for loss in self.losses:\n            for v in loss.variables:\n                if v.name not in seen:\n                    result.append(v)\n                    seen.add(v.name)\n        return result",
        "rewrite": "def get_variables(self):\n    result = self.inputs.copy()\n    seen = set([i.name for i in result])\n    \n    for loss in self.losses:\n        for v in loss.variables:\n            if v.name not in seen:\n                result.append(v)\n                seen.add(v.name)\n    \n    return result"
    },
    {
        "original": "def delete_deployment(self, service_name, deployment_name,delete_vhd=False): \n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        path= self._get_deployment_path_using_name(service_name, deployment_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(\n                path,\n            as_async=True)",
        "rewrite": "def delete_deployment(self, service_name, deployment_name, delete_vhd=False): \n    _validate_not_none('service_name', service_name)\n    _validate_not_none('deployment_name', deployment_name)\n    \n    path = self._get_deployment_path_using_name(service_name, deployment_name)\n    \n    if delete_vhd:\n        path += '?comp=media'\n    \n    return self._perform_delete(path, as_async=True)"
    },
    {
        "original": "def _get_unpacking_extra_info(node, infered): \n    more = \"\"\n    infered_module = infered.root().name\n    if node.root().name == infered_module:\n        if node.lineno == infered.lineno:\n            more = \" %s\" % infered.as_string()\n        elif infered.lineno:\n            more = \" defined at line %s\" % infered.lineno\n    elif infered.lineno:\n        more = \" defined at line %s of %s\" % (infered.lineno, infered_module)\n    return more",
        "rewrite": "def _get_unpacking_extra_info(node, inferred):\n    more = \"\"\n    inferred_module = inferred.root().name\n    if node.root().name == inferred_module:\n        if node.lineno == inferred.lineno:\n            more = \" %s\" % inferred.as_string()\n        elif inferred.lineno:\n            more = \" defined at line %s\" % inferred.lineno\n    elif inferred.lineno:\n        more = \" defined at line %s of %s\" % (inferred.lineno, inferred_module)\n    return more"
    },
    {
        "original": "def get_metadata(path_or_module, metadata_version=None): \n    if isinstance(path_or_module, ModuleType):\n        try:\n            return Installed(path_or_module, metadata_version)\n        except (ValueError, IOError): #pragma NO COVER\n            pass\n\n    try:\n        __import__(path_or_module)\n    except ImportError:\n        pass\n    else:\n        try:\n            return Installed(path_or_module, metadata_version)\n        except (ValueError, IOError): #pragma NO COVER\n       ",
        "rewrite": "def get_metadata(path_or_module, metadata_version=None):\n    if isinstance(path_or_module, ModuleType):\n        try:\n            return Installed(path_or_module, metadata_version)\n        except (ValueError, IOError): #pragma NO COVER\n            pass\n        \n    try:\n        __import__(path_or_module)\n    except ImportError:\n        pass\n    else:\n        try:\n            return Installed(path_or_module, metadata_version)\n        except (ValueError, IOError): #pragma NO COVER"
    },
    {
        "original": "def note_emojis(self, item_type, item_id, note_id): \n\n        payload = {\n            'order_by': 'updated_at',\n            'sort': 'asc',\n            'per_page': PER_PAGE\n        }\n\n        path = urijoin(item_type, str(item_id), GitLabClient.NOTES,\n                       str(note_id), GitLabClient.EMOJI)\n\n        return self.fetch_items(path, payload)",
        "rewrite": "def note_emojis(self, item_type, item_id, note_id):\n        payload = {\n            'order_by': 'updated_at',\n            'sort': 'asc',\n            'per_page': PER_PAGE\n        }\n        \n        path = '/'.join([item_type, str(item_id), GitLabClient.NOTES, str(note_id), GitLabClient.EMOJI])\n        \n        return self.fetch_items(path, payload)"
    },
    {
        "original": "def _build_kernel_function_declaration(self, name='kernel'): \n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                       ",
        "rewrite": "def _build_kernel_function_declaration(self, name='kernel'): \n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations), name=name)"
    },
    {
        "original": "def add(self, other): \n        if not isinstance(other, Operator):\n            other = Operator(other)\n        if self.dim != other.dim:\n            raise QiskitError(\"other operator has different dimensions.\")\n        return Operator(self.data + other.data, self.input_dims(),\n                        self.output_dims())",
        "rewrite": "```python\ndef add(self, other):\n    if not isinstance(other, Operator):\n        other = Operator(other)\n    if self.dim != other.dim:\n        raise QiskitError(\"other operator has different dimensions.\")\n    return Operator(self.data + other.data, self.input_dims(), self.output_dims())\n```"
    },
    {
        "original": "def q_if(self, *qregs): \n        for gate in self.instructions:\n            gate.q_if(*qregs)\n        return self",
        "rewrite": "def q_if(self, *qregs):\n    for gate in self.instructions:\n        gate.q_if(*qregs)\n    return self"
    },
    {
        "original": " \n        assert until > self.now\n        events = self._events\n        schedule = events.push\n        next_event = events.pop\n\n        # add handle to stop simulation\n        schedule(until, PRIORITY_URGENT, raise_StopSimulation(self))\n\n        try:\n            # for all events\n            while True:\n                nextTime, priority, process = next_event()\n      ",
        "rewrite": "assert until > self.now\nevents = self._events\nschedule = events.push\nnext_event = events.pop\n\n# add handle to stop simulation\nschedule(until, PRIORITY_URGENT, raise_StopSimulation(self))\n\ntry:\n    # for all events\n    while True:\n        nextTime, priority, process = next_event()"
    },
    {
        "original": "def select_features(cls, features_id, file_struct, annot_beats, framesync): \n        if not annot_beats and framesync:\n            feat_type = FeatureTypes.framesync\n        elif annot_beats and not framesync:\n            feat_type = FeatureTypes.ann_beatsync\n        elif not annot_beats and not framesync:\n            feat_type = FeatureTypes.est_beatsync\n        else:\n            raise FeatureTypeNotFound(\"Type of features not valid.\")\n\n        # Select features with default parameters\n    ",
        "rewrite": "def select_features(cls, features_id, file_struct, annot_beats, framesync): \n    feat_type = None\n    \n    if not annot_beats and framesync:\n        feat_type = FeatureTypes.framesync\n    elif annot_beats and not framesync:\n        feat_type = FeatureTypes.ann_beatsync\n    elif not annot_beats and not framesync:\n        feat_type = FeatureTypes.est_beatsync\n    else:\n        raise FeatureTypeNotFound(\"Type of features not valid.\")\n\n    # Select features with default parameters"
    },
    {
        "original": "def gcs_write(self, log, remote_log_location, append=True): \n        if append:\n            try:\n                old_log = self.gcs_read(remote_log_location)\n                log = '\\n'.join([old_log, log]) if old_log else log\n            except Exception as e:\n                if not hasattr(e, 'resp') or e.resp.get('status') != '404':\n                    log = '***",
        "rewrite": "def gcs_write(self, log, remote_log_location, append=True):\n        if append:\n            try:\n                old_log = self.gcs_read(remote_log_location)\n                log = '\\n'.join([old_log, log]) if old_log else log\n            except Exception as e:\n                if not hasattr(e, 'resp') or e.resp.get('status') != '404':\n                    log = '***'  # No need to explain. Just write code"
    },
    {
        "original": "def projector(state, flatten=False): \n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix",
        "rewrite": "def projector(state, flatten=False): \n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix"
    },
    {
        "original": "def prepare_argparser(): \n    description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\"\n    epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\"\n\n    # top-level parser\n    argparser = ap.ArgumentParser(description=description, epilog=epilog)\n    argparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+ __version__)\n    subparsers = argparser.add_subparsers(dest='subcommand_name') #help=\"sub-command help\")\n\n    # command for 'gsea'\n    add_gsea_parser(subparsers)\n    # command for 'prerank'\n    add_prerank_parser(subparsers)\n    # command for 'ssgsea'\n    add_singlesample_parser(subparsers)\n    # command for 'plot'\n    add_plot_parser(subparsers)\n    # command for 'enrichr'\n    add_enrichr_parser(subparsers)\n    # command for 'biomart'\n    add_biomart_parser(subparsers)\n\n ",
        "rewrite": "def prepare_argparser(): \n    description = \"%(prog)s -- Gene Set Enrichment Analysis in Python\"\n    epilog = \"For command line options of each command, type: %(prog)s COMMAND -h\"\n\n    argparser = ap.ArgumentParser(description=description, epilog=epilog)\n    argparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \"+ __version__)\n    subparsers = argparser.add_subparsers(dest='subcommand_name')\n\n    add_gsea_parser(subparsers)\n    add_prerank_parser(subparsers)\n    add_singlesample_parser(subparsers)\n    add_plot_parser(subparsers)\n    add_enrichr_parser(subparsers)\n    add_biomart_parser(subparsers)"
    },
    {
        "original": "def get_manifest_selfLink(self, repo_name, digest=None): \n    url = \"%s/%s/manifests\" % (self.base, repo_name)\n\n    # Add a digest - a tag or hash (version)\n    if digest is None:\n        digest = 'latest'\n    return \"%s/%s\" % (url, digest)",
        "rewrite": "def get_manifest_selfLink(self, repo_name, digest=None): \n    url = f\"{self.base}/{repo_name}/manifests\"\n\n    # Add a digest - a tag or hash (version)\n    if digest is None:\n        digest = 'latest'\n        \n    return f\"{url}/{digest}\""
    },
    {
        "original": "def db_query(self, client_id, msg): \n        content = msg['content']\n        query = content.get('query', {})\n        keys = content.get('keys', None)\n        buffers = []\n        empty = list()\n        try:\n            records = self.db.find_records(query, keys)\n        except Exception as e:\n            content = error.wrap_exception()\n        else:\n            # extract buffers from",
        "rewrite": "# extract buffers from records\nif len(records) > 0 and isinstance(records[0], dict):\n    buffers = [\n                z for z in list({key: record[key] for key in keys if key in record}.values())\n                for record in records\n              ]\nelse:\n    content = empty"
    },
    {
        "original": "def show_progress(self, n, total_runs): \n        if self.report_progress:\n            percentage, logger_name, log_level = self.report_progress\n            if logger_name == 'print':\n                logger = 'print'\n            else:\n                logger = logging.getLogger(logger_name)\n\n            if n == -1:\n                # Compute the number of",
        "rewrite": "def show_progress(self, n, total_runs): \n    if self.report_progress:\n        percentage, logger_name, log_level = self.report_progress\n        if logger_name == 'print':\n            logger = 'print'\n        else:\n            logger = logging.getLogger(logger_name)\n\n        if n == -1:\n            num_periods = int((total_runs / 100) * percentage)\n            return num_periods\n        else:\n            # Write whatever else needs to be done\n            return None"
    },
    {
        "original": "def repartition(self, npartitions): \n\n        rdd = self._rdd.repartition(npartitions)\n        return self._constructor(rdd, ordered=False).__finalize__(self)",
        "rewrite": "def repartition(self, npartitions):\n    rdd = self._rdd.repartition(npartitions)\n    return self._constructor(rdd, ordered=False).__finalize__(self)"
    },
    {
        "original": "def share(self, query, share_to): \n\n    images = self._container_query(query, quiet=True)\n    if len(images) == 0:\n        bot.error('Cannot find a remote image matching %s' %query)\n        sys.exit(0)\n\n    image = images[0]\n\n    def callback(request_id, response, exception):\n        if exception:\n            # Handle error\n            print(exception)\n        else:\n            share_id = response.get('id')\n            bot.info('Share to %s complete: %s!'",
        "rewrite": "def share(self, query, share_to):\n    images = self._container_query(query, quiet=True)\n    \n    if len(images) == 0:\n        bot.error('Cannot find a remote image matching %s' % query)\n        sys.exit(0)\n\n    image = images[0]\n\n    def callback(request_id, response, exception):\n        if exception:\n            print(exception)\n        else:\n            share_id = response.get('id')\n            bot.info('Share to %s complete: %s!' % (share_to, share_id))"
    },
    {
        "original": "def get_agency_id(relation): \n    op = relation.tags.get('operator')\n    if op:\n        return int(hashlib.sha256(op.encode('utf-8')).hexdigest(), 16) % 10**8\n    return -1",
        "rewrite": "import hashlib\n\ndef get_agency_id(relation): \n    op = relation.tags.get('operator')\n    if op:\n        return int(hashlib.sha256(op.encode('utf-8')).hexdigest(), 16) % 10**8\n    return -1"
    },
    {
        "original": "def get_base_domain(domain, use_fresh_psl=False): \n    psl_path = os.path.join(tempdir, \"public_suffix_list.dat\")\n\n    def download_psl():\n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\"\n        # Use a browser-like user agent string to bypass some proxy blocks\n        headers = {\"User-Agent\": USER_AGENT}\n        fresh_psl = requests.get(url, headers=headers).text\n        with open(psl_path, \"w\", encoding=\"utf-8\") as fresh_psl_file:\n            fresh_psl_file.write(fresh_psl)\n\n    if use_fresh_psl:\n        if not os.path.exists(psl_path):\n            download_psl()\n        else:\n ",
        "rewrite": "```\ndef get_base_domain(domain, use_fresh_psl=False): \n    psl_path = os.path.join(tempdir, \"public_suffix_list.dat\")\n\n    def download_psl():\n        url = \"https://publicsuffix.org/list/public_suffix_list.dat\"\n        headers = {\"User-Agent\": USER_AGENT}\n        fresh_psl = requests.get(url, headers=headers).text\n        with open(psl_path, \"w\", encoding=\"utf-8\") as fresh_psl_file:\n            fresh_psl_file.write(fresh_psl)\n\n    if use_fresh_psl and not os.path.exists(psl_path):\n        download_psl()\n```"
    },
    {
        "original": "def get_annotation_data_after_time(self, id_tier, time): \n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_after_time(id_tier, time)\n        befores = self.get_annotation_data_between_times(\n            id_tier, time, self.get_full_time_interval()[1])\n        if befores:\n            return [min(befores, key=lambda x: x[0])]\n        else:\n            return []",
        "rewrite": "def get_annotation_data_after_time(self, id_tier, time): \n    if self.tiers[id_tier][1]:\n        return self.get_ref_annotation_after_time(id_tier, time)\n    befores = self.get_annotation_data_between_times(\n        id_tier, time, self.get_full_time_interval()[1])\n    if befores:\n        return [min(befores, key=lambda x: x[0])]\n    else:\n        return []"
    },
    {
        "original": "def home_mode_status(self, **kwargs): \n        api = self._api_info['home_mode']\n        payload = dict({\n            'api': api['name'],\n            'method': 'GetInfo',\n            'version': api['version'],\n            '_sid': self._sid\n        }, **kwargs)\n        response = self._get_json_with_retry(api['url'], payload)\n\n        return response['data']['on']",
        "rewrite": "def home_mode_status(self, **kwargs): \n    api = self._api_info['home_mode']\n    payload = {\n        'api': api['name'],\n        'method': 'GetInfo',\n        'version': api['version'],\n        '_sid': self._sid,\n        **kwargs\n    }\n    response = self._get_json_with_retry(api['url'], payload)\n\n    return response['data']['on']"
    },
    {
        "original": "def get_pager_cmd(pager_cmd=None): \n    if os.name == 'posix':\n        default_pager_cmd = 'less -r'  # -r for color control sequences\n    elif os.name in ['nt','dos']:\n        default_pager_cmd = 'type'\n\n    if pager_cmd is None:\n        try:\n            pager_cmd = os.environ['PAGER']\n        except:\n            pager_cmd = default_pager_cmd\n    return pager_cmd",
        "rewrite": "import os\n\ndef get_pager_cmd(pager_cmd=None): \n    if os.name == 'posix':\n        default_pager_cmd = 'less -r'  # -r for color control sequences\n    elif os.name in ['nt','dos']:\n        default_pager_cmd = 'type'\n\n    if pager_cmd is None:\n        try:\n            pager_cmd = os.environ['PAGER']\n        except:\n            pager_cmd = default_pager_cmd\n    return pager_cmd"
    },
    {
        "original": "def _logspace_mean(log_values): \n  # center = Max[Log[values]],  with stop-gradient\n  # The center hopefully keep the exponentiated term small.  It is canceled\n  # from the final result, so putting stop gradient on it will not change the\n  # final result.  We put stop gradient on to eliminate unnecessary computation.\n  center = tf.stop_gradient(_sample_max(log_values))\n\n  # centered_values = exp{Log[values] - E[Log[values]]}\n  centered_values = tf.math.exp(log_values - center)\n\n  # log_mean_of_values = Log[ E[centered_values] ] + center\n  #                    = Log[ E[exp{log_values - E[log_values]}] ] + center\n  #                ",
        "rewrite": "```python\ndef _logspace_mean(log_values): \n    center = tf.stop_gradient(_sample_max(log_values))\n    centered_values = tf.math.exp(log_values - center)\n    log_mean_of_values = tf.math.log(tf.reduce_mean(centered_values, 0)) + center\n    return log_mean_of_values\n```"
    },
    {
        "original": "def parse_log(log_file): \n\n    template = OrderedDict([\n        # Total length after trimming\n        (\"clean_len\", 0),\n        # Total trimmed base pairs\n        (\"total_trim\", 0),\n        # Total trimmed base pairs in percentage\n        (\"total_trim_perc\", 0),\n        # Total trimmed at 5' end\n        (\"5trim\", 0),\n        # Total trimmed at 3' end\n        (\"3trim\", 0),\n        # Bad reads",
        "rewrite": "```\nfrom collections import OrderedDict\n\ndef parse_log(log_file):\n    template = OrderedDict([\n        (\"clean_len\", 0),\n        (\"total_trim\", 0),\n        (\"total_trim_perc\", 0),\n        (\"5trim\", 0),\n        (\"3trim\", 0),\n        (\"bad_reads\", 0)\n    ])\n```"
    },
    {
        "original": "def activate_matplotlib(backend): \n\n    import matplotlib\n    if backend.startswith('module://'):\n        # Work around bug in matplotlib: matplotlib.use converts the\n        # backend_id to lowercase even if a module name is specified!\n        matplotlib.rcParams['backend'] = backend\n    else:\n        matplotlib.use(backend)\n    matplotlib.interactive(True)\n\n    # This must be imported last in the matplotlib series, after\n    # backend/interactivity choices have been made\n    import matplotlib.pylab as pylab\n\n    # XXX For now leave this commented out, but depending on discussions with\n    # mpl-dev, we may be able",
        "rewrite": "def activate_matplotlib(backend): \n    import matplotlib\n    if backend.startswith('module://'):\n        matplotlib.rcParams['backend'] = backend\n    else:\n        matplotlib.use(backend)\n    matplotlib.interactive(True)\n    import matplotlib.pylab as pylab"
    },
    {
        "original": "def delete_frame(self, key, ignoreMissingKey=True, timeoutSecs=60, **kwargs): \n    assert key is not None, '\"key\" parameter is null'\n\n    result = self.do_json_request('/3/Frames.json/' + key, cmd='delete', timeout=timeoutSecs)\n\n    # TODO: look for what?\n    if not ignoreMissingKey and 'f00b4r' in result:\n        raise ValueError('Frame key not found: ' + key)\n    return result",
        "rewrite": "def delete_frame(self, key, ignoreMissingKey=True, timeoutSecs=60, **kwargs): \n    assert key is not None, '\"key\" parameter is null'\n\n    result = self.do_json_request('/3/Frames.json/' + key, cmd='delete', timeout=timeoutSecs)\n\n    if not ignoreMissingKey and 'f00b4r' not in result:\n        raise ValueError('Frame key not found: ' + key)\n    \n    return result"
    },
    {
        "original": "def get_config(self): \n    config = {\n        'seed': self.seed,\n    }\n    base_config = super(_ConvFlipout, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))",
        "rewrite": "def get_config(self):\n    config = {\n        'seed': self.seed,\n    }\n    base_config = super().get_config()\n    return {**base_config, **config}"
    },
    {
        "original": "def _render_rewrite(self, color=True): \n        if color:\n            scheme = self.color_scheme_table.active_colors\n            # We need a non-input version of these escapes\n            color_prompt = scheme.in_prompt.replace(\"\\001\",\"\").replace(\"\\002\",\"\")\n            color_normal = scheme.normal\n        else:\n            color_prompt, color_normal = '', ''\n\n        return color_prompt + \"-> \".rjust(self.txtwidth, \"-\") + color_normal",
        "rewrite": "def _render_rewrite(self, color=True): \n    if color:\n        scheme = self.color_scheme_table.active_colors\n        color_prompt = scheme.in_prompt.replace(\"\\001\",\"\").replace(\"\\002\",\"\")\n        color_normal = scheme.normal\n    else:\n        color_prompt, color_normal = '', ''\n\n    return color_prompt + \"-> \".rjust(self.txtwidth, \"-\") + color_normal"
    },
    {
        "original": " \n    session = boto3.session.Session(\n        profile_name=aws_profile,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key)\n    s3 = session.resource('s3')\n    bucket = s3.Bucket(bucket_name)\n    return bucket",
        "rewrite": "session = boto3.Session(\n    profile_name=aws_profile,\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key)\ns3 = session.resource('s3')\nbucket = s3.Bucket(bucket_name)\nreturn bucket"
    },
    {
        "original": " \n        url = self.url() + '/nd/resource/dataset/{}'.format(\n            dataset_name) + '/project/{}'.format(project_name) + \\\n            '/token/{}/'.format(token_name)\n\n        json = {\n            \"token_name\": token_name,\n            \"public\": is_public\n        }\n\n        req = self.remote_utils.post_url(url, json=json)\n\n        if req.status_code is not 201:\n            raise RemoteDataUploadError('Cout not upload {}:'.format(req.text))\n   ",
        "rewrite": "url = self.url() + '/nd/resource/dataset/{}/project/{}/token/{}/'.format(\n    dataset_name, project_name, token_name)\n\njson = {\n    \"token_name\": token_name,\n    \"public\": is_public\n}\n\nreq = self.remote_utils.post_url(url, json=json)\n\nif req.status_code != 201:\n    raise RemoteDataUploadError('Could not upload {}:'.format(req.text))"
    },
    {
        "original": "def device_id(self): \n        if self._device_id is None:\n            self._device_id = \"\".join(\n                random.choice(\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\") for _ in range(50))\n        return self._device_id",
        "rewrite": "def device_id(self):\n    if self._device_id is None:\n        self._device_id = \"\".join(random.choice(string.ascii_letters + string.digits) for _ in range(50))\n    return self._device_id"
    },
    {
        "original": "def source_address(self): \n        # If link is a sender, source is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address",
        "rewrite": "def source_address(self): \n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address"
    },
    {
        "original": "def matches_requirement(req, wheels): \n    try:\n        from pkg_resources import Distribution, Requirement\n    except ImportError:\n        raise RuntimeError(\"Cannot use requirements without pkg_resources\")\n\n    req = Requirement.parse(req)\n\n    selected = []\n    for wf in wheels:\n        f = wf.parsed_filename\n        dist = Distribution(project_name=f.group(\"name\"), version=f.group(\"ver\"))\n        if dist in req:\n            selected.append(wf)\n    return selected",
        "rewrite": "def matches_requirement(req, wheels):\n    try:\n        from pkg_resources import Distribution, Requirement\n    except ImportError:\n        raise RuntimeError(\"Cannot use requirements without pkg_resources\")\n    \n    req = Requirement.parse(req)\n    \n    selected = []\n    for wf in wheels:\n        f = wf.parsed_filename\n        dist = Distribution(project_name=f.group(\"name\"), version=f.group(\"ver\"))\n        if dist in req:\n            selected.append(wf)\n    return selected"
    },
    {
        "original": "def _get_index(self,index): \n\n        if index is None:\n            if self.finished:\n                print >>io.stdout, 'Demo finished.  Use <demo_name>.reset() if you want to rerun it.'\n                return None\n            index = self.block_index\n        else:\n            self._validate_index(index)\n        return index",
        "rewrite": "def _get_index(self, index=None):\n    if index is None:\n        if self.finished:\n            print('Demo finished. Use <demo_name>.reset() if you want to rerun it.')\n            return None\n        index = self.block_index\n    else:\n        self._validate_index(index)\n    return index"
    },
    {
        "original": "def _log(cls, level, msg, **kwargs): \n\n        logger = getattr(cls, '_logger', None) or authomatic.core._logger\n        logger.log(\n            level, ': '.join(\n                ('authomatic', cls.__name__, msg)), **kwargs)",
        "rewrite": "def _log(cls, level, msg, **kwargs):\n    logger = getattr(cls, '_logger', None) or authomatic.core._logger\n    logger.log(level, ': '.join(('authomatic', cls.__name__, msg)), **kwargs)"
    },
    {
        "original": "def update_virtual_meta(self): \n        import astropy.units\n        try:\n            path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n            if os.path.exists(path):\n                meta_info = vaex.utils.read_json_or_yaml(path)\n                if 'virtual_columns' not in meta_info:\n                    return\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])\n      ",
        "rewrite": "def update_virtual_meta(self):\n    import astropy.units\n    try:\n        path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n        if os.path.exists(path):\n            meta_info = vaex.utils.read_json_or_yaml(path)\n            if 'virtual_columns' in meta_info:\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])"
    },
    {
        "original": "def dump(thing, query, from_date, file_prefix, chunk_size, limit, thing_flags): \n    init_app_context()\n\n    file_prefix = file_prefix if file_prefix else '{0}_dump'.format(thing)\n\n    kwargs = dict((f.strip('-').replace('-', '_'), True) for f in thing_flags)\n\n    try:\n        thing_func = collect_things_entry_points()[thing]\n    except KeyError:\n        click.Abort(\n            '{0} is not in the list of available things to migrate: '\n            '{1}'.format(thing, collect_things_entry_points()))\n\n    click.echo(\"Querying {0}...\".format(thing))\n    count, items = thing_func.get(query, from_date, limit=limit, **kwargs)\n\n    progress_i = 0  # Progress bar counter\n    click.echo(\"Dumping",
        "rewrite": "def dump(thing, query, from_date, file_prefix, chunk_size, limit, thing_flags): \n    init_app_context()\n\n    file_prefix = file_prefix if file_prefix else '{0}_dump'.format(thing)\n\n    kwargs = dict((f.strip('-').replace('-', '_'), True) for f in thing_flags)\n\n    try:\n        thing_func = collect_things_entry_points()[thing]\n    except KeyError:\n        click.Abort(\n            '{0} is not in the list of available things to migrate: '\n            '{1}'.format(thing, collect_things_entry_points()))\n\n    click.echo(\"Querying {0}...\".format(thing))\n    count, items = thing_func.get(query, from_date, limit=limit, **kwargs)\n\n    progress_i = 0\n    click.echo(\"Dumping {0}...\".format(thing))"
    },
    {
        "original": "def off(self, name, callback): \n        if callback not in self.__listeners[name]:\n            raise InvalidListenerError\n        self.__listeners[name].remove(callback)",
        "rewrite": "def off(self, name, callback): \n    if callback not in self.__listeners.get(name, []):\n        raise InvalidListenerError\n    self.__listeners[name].remove(callback)"
    },
    {
        "original": "def to_one_dim_array(values, as_type=None): \n\n    if isinstance(values, (list, tuple)):\n        values = np.array(values, dtype=np.float32)\n    elif isinstance(values, pd.Series):\n        values = values.values\n    values = values.flatten()\n    assert values.ndim == 1, \"values has wrong dimension\"\n    if as_type is not None:\n        return values.astype(as_type)\n    return values",
        "rewrite": "def to_one_dim_array(values, as_type=None): \n    if isinstance(values, (list, tuple)):\n        values = np.array(values, dtype=np.float32)\n    elif isinstance(values, pd.Series):\n        values = values.values\n    values = values.flatten()\n    assert values.ndim == 1, \"values has wrong dimension\"\n    if as_type is not None:\n        return values.astype(as_type)\n    return values"
    },
    {
        "original": " \n    protocol, _ = create_dsmr_protocol(\n        dsmr_version, telegram_callback, loop=None)\n    conn = loop.create_connection(protocol, host, port)\n    return conn",
        "rewrite": "protocol, _ = create_dsmr_protocol(dsmr_version, telegram_callback)\nconn = loop.create_connection(protocol, host, port)\nreturn conn"
    },
    {
        "original": "def set_style(self, style): \n        if isinstance(style, basestring):\n            style = get_style_by_name(style)\n        self._style = style\n        self._clear_caches()",
        "rewrite": "def set_style(self, style): \n    if isinstance(style, str):\n        style = get_style_by_name(style)\n    self._style = style\n    self._clear_caches()"
    },
    {
        "original": "def onehot_tensor(i_matrix, vocab_size): \n    dim0, dim1 = i_matrix.shape\n    i_vector = i_matrix.reshape((-1,))\n    hot_matrix = T.extra_ops.to_one_hot(i_vector, vocab_size).reshape((dim0, dim1, vocab_size))\n    return hot_matrix",
        "rewrite": "def onehot_tensor(i_matrix, vocab_size): \n    dim0, dim1 = i_matrix.shape\n    i_vector = i_matrix.flatten()\n    hot_matrix = T.extra_ops.to_one_hot(i_vector, vocab_size).reshape((dim0, dim1, vocab_size))\n    return hot_matrix"
    },
    {
        "original": "def output_stream_passthrough(plugin, stream): \n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\", args.player)\n     ",
        "rewrite": "def output_stream_passthrough(plugin, stream): \n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\".format(args.player))"
    },
    {
        "original": "def _get_field(self, field_name, default=None): \n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
        "rewrite": "def _get_field(self, field_name, default=None):\n    full_field_name = f'extra__grpc__{field_name}'\n    if full_field_name in self.extras:\n        return self.extras[full_field_name]\n    else:\n        return default"
    },
    {
        "original": "def _get_cached_arg_spec(fn): \n\n  arg_spec = _ARG_SPEC_CACHE.get(fn)\n  if arg_spec is None:\n    arg_spec_fn = inspect.getfullargspec if six.PY3 else inspect.getargspec\n    try:\n      arg_spec = arg_spec_fn(fn)\n    except TypeError:\n      # `fn` might be a callable object.\n      arg_spec = arg_spec_fn(fn.__call__)\n    _ARG_SPEC_CACHE[fn] = arg_spec\n  return arg_spec",
        "rewrite": "def _get_cached_arg_spec(fn):\n    arg_spec = _ARG_SPEC_CACHE.get(fn)\n    if arg_spec is None:\n        arg_spec_fn = inspect.getfullargspec if six.PY3 else inspect.getargspec\n        try:\n            arg_spec = arg_spec_fn(fn)\n        except TypeError:\n            arg_spec = arg_spec_fn(fn.__call__)\n        _ARG_SPEC_CACHE[fn] = arg_spec\n    return arg_spec"
    },
    {
        "original": "def _update_with_rollback(self, on_dup, *args, **kw): \n        writelog = []\n        appendlog = writelog.append\n        dedup_item = self._dedup_item\n        write_item = self._write_item\n        for (key, val) in _iteritems_args_kw(*args, **kw):\n            try:\n                dedup_result = dedup_item(key, val, on_dup)\n            except DuplicationError:\n                undo_write = self._undo_write\n      ",
        "rewrite": "def _update_with_rollback(self, on_dup, *args, **kw):\n    writelog = []\n    appendlog = writelog.append\n    dedup_item = self._dedup_item\n    write_item = self._write_item\n    for (key, val) in _iteritems_args_kw(*args, **kw):\n        try:\n            dedup_result = dedup_item(key, val, on_dup)\n        except DuplicationError:\n            undo_write = self._undo_write"
    },
    {
        "original": "def poke(self, context): \n        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n        message = self.pubsub.get_message()\n        self.log.info('Message %s from channel %s', message, self.channels)\n\n        # Process only message types\n        if message and message['type'] == 'message':\n\n            context['ti'].xcom_push(key='message', value=message)\n            self.pubsub.unsubscribe(self.channels)\n\n            return True\n\n        return False",
        "rewrite": "def poke(self, context): \n    self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)\n\n    message = self.pubsub.get_message()\n    self.log.info('Message %s from channel %s', message, self.channels)\n\n    if message and message['type'] == 'message':\n        context['ti'].xcom_push(key='message', value=message)\n        self.pubsub.unsubscribe(self.channels)\n        return True\n\n    return False"
    },
    {
        "original": "def main(): \n\n    state = GameState()\n    print(state)\n    while state.running:\n        input = get_single_char()\n\n        state, should_advance = state.handle_input(input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)",
        "rewrite": "def main(): \n\n    state = GameState()\n    print(state)\n    while state.running:\n        user_input = get_single_char()\n\n        state, should_advance = state.handle_input(user_input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)"
    },
    {
        "original": "def getMaxStmIdForStm(stm): \n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId",
        "rewrite": "def getMaxStmIdForStm(stm): \n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId"
    },
    {
        "original": "def process_autosummary_toc(app, doctree): \n    env = app.builder.env\n    crawled = {}\n    def crawl_toc(node, depth=1):\n        crawled[node] = True\n        for j, subnode in enumerate(node):\n            try:\n                if (isinstance(subnode, autosummary_toc)\n                    and isinstance(subnode[0], addnodes.toctree)):\n                    env.note_toctree(env.docname, subnode[0])\n           ",
        "rewrite": "def process_autosummary_toc(app, doctree): \n    env = app.builder.env\n    crawled = {}\n    def crawl_toc(node, depth=1):\n        crawled[node] = True\n        for j, subnode in enumerate(node):\n            try:\n                if isinstance(subnode, autosummary_toc) and isinstance(subnode[0], addnodes.toctree):\n                    env.note_toctree(env.docname, subnode[0])"
    },
    {
        "original": "def computed_displaywidth(): \n    try:\n        width = int(os.environ['COLUMNS'])\n    except (KeyError, ValueError):\n        width = get_terminal_size().columns\n\n    return width or 80",
        "rewrite": "import os\nfrom shutil import get_terminal_size\n\ndef computed_displaywidth():\n    try:\n        width = int(os.environ.get('COLUMNS'))\n    except (KeyError, ValueError):\n        width = get_terminal_size().columns\n\n    return width or 80"
    },
    {
        "original": "def upload_link(self, folder_id=None, sha1=None, httponly=False): \n\n        kwargs = {'folder': folder_id, 'sha1': sha1, 'httponly': httponly}\n        params = {key: value for key, value in kwargs.items() if value}\n        return self._get('file/ul', params=params)",
        "rewrite": "def upload_link(self, folder_id=None, sha1=None, httponly=False): \n\n        kwargs = {'folder': folder_id, 'sha1': sha1, 'httponly': httponly}\n        params = {key: value for key, value in kwargs.items() if value}\n        return self._get('file/ul', params=params)"
    },
    {
        "original": "def get_single_axis_values(self, axis, dataset): \n\t\tdata_index = getattr(self, '%s_data_index' % axis)\n\t\treturn [p[data_index] for p in dataset['data']]",
        "rewrite": "def get_single_axis_values(self, axis, dataset): \n    data_index = getattr(self, f'{axis}_data_index')\n    return [p[data_index] for p in dataset['data']]"
    },
    {
        "original": "def update_record(cls, revisions, created, record): \n        for timestamp, revision in revisions:\n            record.model.json = revision\n            record.model.created = created.replace(tzinfo=None)\n            record.model.updated = timestamp.replace(tzinfo=None)\n            db.session.commit()\n        return Record(record.model.json, model=record.model)",
        "rewrite": "def update_record(cls, revisions, created, record): \n    for timestamp, revision in revisions:\n        record.model.json = revision\n        record.model.created = created.replace(tzinfo=None)\n        record.model.updated = timestamp.replace(tzinfo=None)\n        db.session.commit()\n    return Record(record.model.json, model=record.model)"
    },
    {
        "original": "def _download_libraries(self, libname): \n        self._logger.info(\"Downloading and generating Enrichr library gene sets......\")\n        s = retry(5)\n        # queery string\n        ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary'\n        query_string = '?mode=text&libraryName=%s'\n        # get\n        response = s.get( ENRICHR_URL + query_string % libname, timeout=None)\n        if not response.ok:\n            raise Exception('Error fetching enrichment results, check internet connection first.')\n        # reformat to dict and save to disk\n",
        "rewrite": "def _download_libraries(self, libname): \n    self._logger.info(\"Downloading and generating Enrichr library gene sets......\")\n    s = retry(5)\n    # query string\n    ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary'\n    query_string = '?mode=text&libraryName=%s'\n    # get\n    response = s.get(ENRICHR_URL + query_string % libname, timeout=None)\n    if not response.ok:\n        raise Exception('Error fetching enrichment results, check internet connection first.')\n    # reformat to dict and save to disk"
    },
    {
        "original": "def _matches_filepath_pattern(self, filepath): \n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False",
        "rewrite": "def _matches_filepath_pattern(self, filepath):\n    if not self.only_blame_patterns:\n        return True\n    \n    for pattern in self.only_blame_patterns:\n        if pattern.match(filepath):\n            return True\n    \n    return False"
    },
    {
        "original": "def show(): \n\n    parent = next(\n        o for o in QtWidgets.QApplication.instance().topLevelWidgets()\n        if o.objectName() == \"MayaWindow\"\n    )\n\n    gui = _discover_gui()\n\n    if gui is None:\n        _show_no_gui()\n    else:\n        return gui(parent)",
        "rewrite": "def show(): \n\n    parent = next(\n        o for o in QtWidgets.QApplication.instance().topLevelWidgets()\n        if o.objectName() == \"MayaWindow\"\n    )\n\n    gui = _discover_gui()\n\n    if gui is None:\n        _show_no_gui()\n    else:\n        return gui(parent)"
    },
    {
        "original": "def delay(self, key): \n        last_action, target_delay = self.last[key], self.delays[key]\n        elapsed_time = time.time() - last_action\n        if elapsed_time < target_delay:\n            t_remaining = target_delay - elapsed_time\n            time.sleep(t_remaining * random.uniform(0.25, 1.25))\n        self.last[key] = time.time()",
        "rewrite": "def delay(self, key):\n    last_action = self.last[key]\n    target_delay = self.delays[key]\n    elapsed_time = time.time() - last_action\n    if elapsed_time < target_delay:\n        t_remaining = target_delay - elapsed_time\n        time.sleep(t_remaining * random.uniform(0.25, 1.25))\n    self.last[key] = time.time()"
    },
    {
        "original": " \n        request = requests.Request(\n            http_method,\n            self.writer_url,\n            params=params,\n            json=json,\n            headers={\n                'User-Agent': self.user_agent})\n        ids = id_gen(str(uuid.uuid4()))\n        network_timeouts = self.network_timeouts()\n        maintenance_timeouts = self.maintenance_timeouts()\n        while True:\n",
        "rewrite": "```python\nimport requests\nimport uuid\n\nrequest = requests.Request(\n    http_method,\n    self.writer_url,\n    params=params,\n    json=json,\n    headers={\n        'User-Agent': self.user_agent})\nids = id_gen(str(uuid.uuid4()))\nnetwork_timeouts = self.network_timeouts()\nmaintenance_timeouts = self.maintenance_timeouts()\nwhile True:\n```"
    },
    {
        "original": "def read_json(filename, mode='r'): \n    with open(filename, mode) as filey:\n        data = json.load(filey)\n    return data",
        "rewrite": "import json\n\ndef read_json(filename, mode='r'): \n    with open(filename, mode) as filey:\n        data = json.load(filey)\n    return data"
    },
    {
        "original": "def _get_download_output_manager_cls(self, transfer_future, osutil): \n        download_manager_resolver_chain = [\n            DownloadSpecialFilenameOutputManager,\n            DownloadFilenameOutputManager,\n            DownloadSeekableOutputManager,\n            DownloadNonSeekableOutputManager,\n        ]\n\n        fileobj = transfer_future.meta.call_args.fileobj\n        for download_manager_cls in download_manager_resolver_chain:\n            if download_manager_cls.is_compatible(fileobj, osutil):\n                return download_manager_cls\n     ",
        "rewrite": "def _get_download_output_manager_cls(self, transfer_future, osutil): \n    download_manager_resolver_chain = [\n        DownloadSpecialFilenameOutputManager,\n        DownloadFilenameOutputManager,\n        DownloadSeekableOutputManager,\n        DownloadNonSeekableOutputManager\n    ]\n\n    fileobj = transfer_future.meta.call_args.fileobj\n    for download_manager_cls in download_manager_resolver_chain:\n        if download_manager_cls.is_compatible(fileobj, osutil):\n            return download_manager_cls"
    },
    {
        "original": "def normal_right_down(self, event): \n\n        x = event.x\n        y = event.y\n\n        # First determine what component or components we are going to hittest\n        # on.  If our component is a container, then we add its non-container\n        # components to the list of candidates.\n#        candidates = []\n        component = self.component\n#        if isinstance(component, Container):\n#            candidates = get_nested_components(self.component)\n#      ",
        "rewrite": "def normal_right_down(self, event): \n\n    x = event.x\n    y = event.y\n\n    component = self.component"
    },
    {
        "original": "def working_directory(path): \n    prev_dir = os.getcwd()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(prev_dir)",
        "rewrite": "def working_directory(path): \n    prev_dir = os.getcwd()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(prev_dir)"
    },
    {
        "original": "def save_graph_only(sess, output_file_path, output_node_names, as_text=False): \n    for node in sess.graph_def.node:\n        node.device = ''\n    graph_def = graph_util.extract_sub_graph(sess.graph_def, output_node_names)\n    output_dir, output_filename = os.path.split(output_file_path)\n    graph_io.write_graph(graph_def, output_dir, output_filename, as_text=as_text)",
        "rewrite": "def save_graph_only(sess, output_file_path, output_node_names, as_text=False): \n    for node in sess.graph_def.node:\n        node.device = ''\n    graph_def = tf.compat.v1.graph_util.extract_sub_graph(sess.graph_def, output_node_names)\n    output_dir, output_filename = os.path.split(output_file_path)\n    tf.io.write_graph(graph_def, output_dir, output_filename, as_text=as_text)"
    },
    {
        "original": "def mkdirs(path, mode): \n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
        "rewrite": "import os\n\ndef mkdirs(path, mode):\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"
    },
    {
        "original": "def reinterptet_harray_to_bits(typeFrom, sigOrVal, bitsT): \n    size = int(typeFrom.size)\n    widthOfElm = typeFrom.elmType.bit_length()\n    w = bitsT.bit_length()\n    if size * widthOfElm != w:\n        raise TypeConversionErr(\n            \"Size of types is different\", size * widthOfElm, w)\n\n    partT = Bits(widthOfElm)\n    parts = [p._reinterpret_cast(partT) for p in sigOrVal]\n\n    return Concat(*reversed(parts))._reinterpret_cast(bitsT)",
        "rewrite": "def reinterpret_harray_to_bits(typeFrom, sigOrVal, bitsT):\n    size = typeFrom.size\n    widthOfElm = typeFrom.elmType.bit_length()\n    w = bitsT.bit_length()\n    \n    if size * widthOfElm != w:\n        raise TypeConversionErr(\"Size of types is different\", size * widthOfElm, w)\n\n    partT = Bits(widthOfElm)\n    parts = [p._reinterpret_cast(partT) for p in sigOrVal]\n\n    return Concat(*reversed(parts))._reinterpret_cast(bitsT)"
    },
    {
        "original": "def observations(store, loqusdb, case_obj, variant_obj): \n    composite_id = (\"{this[chromosome]}_{this[position]}_{this[reference]}_\"\n                    \"{this[alternative]}\".format(this=variant_obj))\n    obs_data = loqusdb.get_variant({'_id': composite_id}) or {}\n    obs_data['total'] = loqusdb.case_count()\n\n    obs_data['cases'] = []\n    institute_id = variant_obj['institute']\n    for case_id in obs_data.get('families', []):\n        if case_id != variant_obj['case_id'] and case_id.startswith(institute_id):\n            other_variant = store.variant(variant_obj['variant_id'], case_id=case_id)\n            other_case = store.case(case_id)\n            obs_data['cases'].append(dict(case=other_case, variant=other_variant))\n\n    return obs_data",
        "rewrite": "def observations(store, loqusdb, case_obj, variant_obj): \n    composite_id = (\"{this[chromosome]}_{this[position]}_{this[reference]}_\"\n                    \"{this[alternative]}\".format(this=variant_obj))\n    obs_data = loqusdb.get_variant({'_id': composite_id}) or {}\n    obs_data['total'] = loqusdb.case_count()\n\n    obs_data['cases'] = []\n    institute_id = variant_obj['institute']\n    for case_id in obs_data.get('families', []):\n        if case_id != variant_obj['case_id'] and case_id.startswith(institute_id):\n            other_variant = store.variant(variant_obj['variant_id'], case_id=case_id)\n            other_case = store.case(case_id)\n            obs_data['cases'].append(dict(case=other_case, variant=other_variant))\n\n    return obs_data"
    },
    {
        "original": " \n    undecorated_annotations = list(discretized_pulse.__annotations__.items())\n    decorated_annotations = undecorated_annotations[1:]\n    decorated_annotations.insert(0, ('duration', int))\n    discretized_pulse.__annotations__ = dict(decorated_annotations)\n    return discretized_pulse",
        "rewrite": "undecorated_annotations = list(discretized_pulse.__annotations__.items())\ndecorated_annotations = undecorated_annotations[1:]\ndecorated_annotations.insert(0, ('duration', int))\ndiscretized_pulse.__annotations__ = dict(decorated_annotations)\nreturn discretized_pulse"
    },
    {
        "original": "def _get_top_words(model, feature_names, n_top_words=40): \n    topic_words = []\n    for topic in model.components_:\n        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n        topic_words += [top_words]\n    return topic_words",
        "rewrite": "def get_top_words(model, feature_names, n_top_words=40):\n    topic_words = []\n    for topic in model.components_:\n        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n        topic_words.append(top_words)\n    return topic_words"
    },
    {
        "original": "def convert_to_mp3(file_name, delete_queue): \n\n\n    file = os.path.splitext(file_name)\n\n    if file[1] == '.mp3':\n        log.info(f\"{file_name} is already a MP3 file, no conversion needed.\")\n        return file_name\n\n    new_file_name = file[0] + '.mp3'\n\n    ff = FFmpeg(\n        inputs={file_name: None},\n        outputs={new_file_name: None}\n    )\n\n    log.info(f\"Conversion for {file_name} has started\")\n    start_time = time()\n    try:\n        ff.run(stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except FFRuntimeError:\n        os.remove(new_file_name)\n        ff.run(stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n ",
        "rewrite": "import os\nfrom ffmpeg import FFmpeg\nfrom time import time\nimport subprocess\n\ndef convert_to_mp3(file_name, delete_queue):\n    \n    file = os.path.splitext(file_name)\n\n    if file[1] == '.mp3':\n        log.info(f\"{file_name} is already an MP3 file, no conversion needed.\")\n        return file_name\n\n    new_file_name = file[0] + '.mp3'\n\n    ff = FFmpeg(\n        inputs={file_name: None},\n        outputs={new_file_name: None}\n    )\n\n    log.info(f\"Conversion for {file_name} has started\")\n    start_time = time()\n    \n    try:\n        ff.run(stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except FFRuntimeError:\n        os.remove(new_file_name)\n        ff.run(stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
    },
    {
        "original": "def bytes_to_string(b): \n    if isinstance(b, binary_type):\n        try:\n            return b.decode('utf8')\n        except UnicodeDecodeError:\n            return '0x' + binascii.hexlify(b).decode('ascii')\n    return b",
        "rewrite": "def bytes_to_string(b):\n    if isinstance(b, bytes):\n        try:\n            return b.decode('utf8')\n        except UnicodeDecodeError:\n            return '0x' + binascii.hexlify(b).decode('ascii')\n    return b"
    },
    {
        "original": "def update_event_status(event, status): \n    dbs = db.get_session()\n    dbs.query(db.RecordedEvent).filter(db.RecordedEvent.start == event.start)\\\n                               .update({'status': status})\n    event.status = status\n    dbs.commit()",
        "rewrite": "def update_event_status(event, status): \n    dbs = db.get_session()\n    dbs.query(db.RecordedEvent).filter(db.RecordedEvent.start == event.start).update({'status': status})\n    event.status = status\n    dbs.commit()"
    },
    {
        "original": "def add_management_certificate(self, public_key, thumbprint, data): \n        _validate_not_none('public_key', public_key)\n        _validate_not_none('thumbprint', thumbprint)\n        _validate_not_none('data', data)\n        return self._perform_post(\n            '/' + self.subscription_id + '/certificates',\n            _XmlSerializer.subscription_certificate_to_xml(\n                public_key, thumbprint, data))",
        "rewrite": "def add_management_certificate(self, public_key, thumbprint, data):\n    _validate_not_none('public_key', public_key)\n    _validate_not_none('thumbprint', thumbprint)\n    _validate_not_none('data', data)\n    return self._perform_post(\n        '/' + self.subscription_id + '/certificates',\n        _XmlSerializer.subscription_certificate_to_xml(\n            public_key, thumbprint, data))"
    },
    {
        "original": "def _compute_mean_fano_factor( neuron_ids, spike_res, time_window, start_time, end_time): \n        ffs = np.zeros(len(neuron_ids))\n\n        for idx, neuron_id in enumerate(neuron_ids):\n            ff=CNFanoFactorComputer._compute_fano_factor(\n                            spike_res, neuron_id, time_window, start_time, end_time)\n            ffs[idx]=ff\n\n        mean_ff = np.mean(ffs)\n        return mean_ff",
        "rewrite": "def _compute_mean_fano_factor(neuron_ids, spike_res, time_window, start_time, end_time): \n    ffs = np.zeros(len(neuron_ids))\n\n    for idx, neuron_id in enumerate(neuron_ids):\n        ff = CNFanoFactorComputer._compute_fano_factor(spike_res, neuron_id, time_window, start_time, end_time)\n        ffs[idx] = ff\n\n    mean_ff = np.mean(ffs)\n    return mean_ff"
    },
    {
        "original": "def run(self, func, tasks, func2=None): \n\n        # Keep track of some progress for the user\n        progress = 1\n        total = len(tasks)\n\n        # if we don't have tasks, don't run\n        if len(tasks) == 0:\n            return\n\n        # If two functions are run per task, double total jobs\n        if func2 is not None:\n            total = total * 2\n\n  ",
        "rewrite": "def run(self, func, tasks, func2=None):\n\n    progress = 1\n    total = len(tasks)\n\n    if len(tasks) == 0:\n        return\n\n    if func2 is not None:\n        total = total * 2"
    },
    {
        "original": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname): \n  fig = figure.Figure(figsize=(6, 6))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  ax = fig.add_subplot(1, 1, 1)\n  ax.scatter(features[:, 0], features[:, 1],\n             c=np.float32(labels[:, 0]),\n             cmap=cm.get_cmap(\"binary\"),\n             edgecolors=\"k\")\n\n  def plot_weights(w, b, **kwargs):\n    w1, w2 = w\n    x1s = np.linspace(-1, 1, 100)\n    x2s = -(w1  * x1s + b) / w2\n    ax.plot(x1s, x2s, **kwargs)\n\n  for w, b in candidate_w_bs:\n    plot_weights(w, b,\n           ",
        "rewrite": "def visualize_decision(features, labels, true_w_b, candidate_w_bs, fname): \n    fig = plt.figure(figsize=(6, 6))\n    canvas = FigureCanvasAgg(fig)\n    ax = fig.add_subplot(1, 1, 1)\n    ax.scatter(features[:, 0], features[:, 1],\n               c=np.float32(labels[:, 0]),\n               cmap=cm.get_cmap(\"binary\"),\n               edgecolors=\"k\")\n\n    def plot_weights(w, b, **kwargs):\n        w1, w2 = w\n        x1s = np.linspace(-1, 1, 100)\n        x2s = -(w1 * x1s + b) / w2\n        ax.plot(x1s, x2s, **kwargs)\n\n    for w, b in candidate_w_bs:\n        plot_weights(w, b)"
    },
    {
        "original": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs): \n  batch_shape = _get_shape(sp_a)[:-2]\n\n  # Reshape the SparseTensor into a rank 3 SparseTensors, with the\n  # batch shape flattened to a single dimension. If the batch rank is 0, then\n  # we add a batch dimension of rank 1.\n  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],\n                                           axis=0))\n  # Reshape b to stack the batch dimension along the rows.\n  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n  # Convert the SparseTensor to a matrix in block diagonal form with",
        "rewrite": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs): \n    batch_shape = _get_shape(sp_a)[:-2]\n\n    sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]], axis=0))\n    b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n    out_shape = tf.concat([batch_shape, (sp_a.shape.rank + b.shape.rank - 2) * [sp_a.shape[2]] + [b.shape[-1]], axis=0)\n    bs, sp_a_indices, sp_a_values, sp_a_shape, b = _convert_sparse_matrix(sp_a, b)\n    return tf.raw_ops.SparseTensorDenseMatMul(sp_a_indices=sp_a_indices, sp_a_values=sp_a_values,\n                                               sp_a_shape=sp_a_shape, b=b,\n                                               out_shape=out_shape, **kwargs)"
    },
    {
        "original": "def add_accelerator(control, key, func, id = None): \n logger.debug('Adding key \"%s\" to control %s to call %s.', key, control, func)\n id = get_id(id)\n control.Bind(wx.EVT_MENU, func, id = id)\n t = _tables.get(control, [])\n modifiers, key_int = str_to_key(key)\n t.append((modifiers, key_int, id))\n _tables[control] = t\n update_accelerators(control)\n return id",
        "rewrite": "def add_accelerator(control, key, func, id = None): \n    logger.debug('Adding key \"%s\" to control %s to call %s.', key, control, func)\n    id = get_id(id)\n    control.Bind(wx.EVT_MENU, func, id=id)\n    t = _tables.get(control, [])\n    modifiers, key_int = str_to_key(key)\n    t.append((modifiers, key_int, id))\n    _tables[control] = t\n    update_accelerators(control)\n    return id"
    },
    {
        "original": "def get_event_hub(self, hub_name): \n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)",
        "rewrite": "def get_event_hub(self, hub_name):\n    _validate_not_none('hub_name', hub_name)\n    request = HTTPRequest()\n    request.method = 'GET'\n    request.host = self._get_host()\n    request.path = '/' + str(hub_name) + ''\n    request.path, request.query = self._httpclient._update_request_uri_query(request)\n    request.headers = self._update_service_bus_header(request)\n    response = self._perform_request(request)\n\n    return _convert_response_to_event_hub(response)"
    },
    {
        "original": "def physical_qubits(self): \n        if self._qubit_list is None:\n            self._qubit_list = sorted([pqubit for pqubit in self.graph.nodes])\n        return self._qubit_list",
        "rewrite": "def physical_qubits(self):\n        if self._qubit_list is None:\n            self._qubit_list = sorted([pqubit for pqubit in self.graph.nodes])\n        return self._qubit_list"
    },
    {
        "original": "def named(new_name): def do_load_some_stuff_and_keep_the_original_function_name(args): \n    def wrapper(func):\n        setattr(func, ATTR_NAME, new_name)\n        return func\n    return wrapper",
        "rewrite": "def named(new_name):\n    def do_load_some_stuff_and_keep_the_original_function_name(*args):\n        def wrapper(func):\n            setattr(func, ATTR_NAME, new_name)\n            return func\n        return wrapper\n    return do_load_some_stuff_and_keep_the_original_function_name"
    },
    {
        "original": " \n        return self.resources.create_dataset(name,\n                                             x_img_size,\n                                             y_img_size,\n                           ",
        "rewrite": "return self.resources.create_dataset(name, x_img_size, y_img_size)"
    },
    {
        "original": "def reject(self, pn_condition=None): \n        self._pn_link.target.type = proton.Terminus.UNSPECIFIED\n        super(ReceiverLink, self).reject(pn_condition)",
        "rewrite": "def reject(self, pn_condition=None): \n    self._pn_link.target.type = proton.Terminus.UNSPECIFIED\n    super().reject(pn_condition)"
    },
    {
        "original": "def predict(self, x, batch_size=None, verbose=None, is_distributed=False): \n        if batch_size or verbose:\n            raise Exception(\"we don't support batch_size or verbose for now\")\n        if is_distributed:\n            if isinstance(x, np.ndarray):\n                input = to_sample_rdd(x, np.zeros([x.shape[0]]))\n            #  np.asarray(self.bmodel.predict(x_rdd).collect())\n            elif isinstance(x, RDD):\n                input = x\n  ",
        "rewrite": "def predict(self, x, batch_size=None, verbose=None, is_distributed=False):\n    if batch_size is not None or verbose is not None:\n        raise Exception(\"we don't support batch_size or verbose for now\")\n    if is_distributed:\n        if isinstance(x, np.ndarray):\n            input = to_sample_rdd(x, np.zeros([x.shape[0]))\n        elif isinstance(x, RDD):\n            input = x"
    },
    {
        "original": "def _iter_module_files(): \n    # The list call is necessary on Python 3 in case the module\n    # dictionary modifies during iteration.\n    for module in list(sys.modules.values()):\n        if module is None:\n            continue\n        filename = getattr(module, '__file__', None)\n        if filename:\n            old = None\n            while not os.path.isfile(filename):\n                old = filename\n    ",
        "rewrite": "import sys\nimport os\n\ndef _iter_module_files():\n    for module in list(sys.modules.values()):\n        if module is None:\n            continue\n        filename = getattr(module, '__file__', None)\n        if filename:\n            old = None\n            while not os.path.isfile(filename):\n                old = filename"
    },
    {
        "original": "def detect_sentence_boundaries(tokens): \n    tokenized = group_quoted_tokens(tokens)\n    words = []\n    sentences = []\n    for i in range(len(tokenized)):\n        # this is a parenthetical:\n        end_sentence = False\n        if isinstance(tokenized[i], list):\n            if len(words) == 0:\n                # end if a sentence finishes inside quoted section,\n                # and no sentence was begun beforehand\n       ",
        "rewrite": "def detect_sentence_boundaries(tokens): \n    tokenized = group_quoted_tokens(tokens)\n    words = []\n    sentences = []\n    for i in range(len(tokenized)):\n        end_sentence = False\n        if isinstance(tokenized[i], list):\n            if len(words) == 0:"
    },
    {
        "original": "def cinder(*arg): \n    check_event_type(Openstack.Cinder, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            cinder_customer_process_wildcard[event_type_pattern] = func\n        else:\n            cinder_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n   ",
        "rewrite": "import functools\ndef cinder(*arg):\n    check_event_type(Openstack.Cinder, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            cinder_customer_process_wildcard[event_type_pattern] = func\n        else:\n            cinder_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n        \n        return wrapper\n\n    return decorator"
    },
    {
        "original": "def get_object(model, meteor_id, *args, **kwargs): \n    # Django model._meta is now public API -> pylint: disable=W0212\n    meta = model._meta\n    if isinstance(meta.pk, AleaIdField):\n        # meteor_id is the primary key\n        return model.objects.filter(*args, **kwargs).get(pk=meteor_id)\n\n    alea_unique_fields = [\n        field\n        for field in meta.local_fields\n        if isinstance(field, AleaIdField) and field.unique and not field.null\n    ]\n    if len(alea_unique_fields) == 1:\n        return model.objects.filter(*args, **kwargs).get(**{\n            alea_unique_fields[0].name: meteor_id,\n",
        "rewrite": "def get_object(model, meteor_id, *args, **kwargs):\n    meta = model._meta\n    if isinstance(meta.pk, AleaIdField):\n        return model.objects.filter(*args, **kwargs).get(pk=meteor_id)\n\n    alea_unique_fields = [\n        field\n        for field in meta.local_fields\n        if isinstance(field, AleaIdField) and field.unique and not field.null\n    ]\n    if len(alea_unique_fields) == 1:\n        return model.objects.filter(*args, **kwargs).get(**{\n            alea_unique_fields[0].name: meteor_id,\n        })"
    },
    {
        "original": "def nameAvailabilityCheck(obj, propName, prop): \n    if getattr(obj, propName, None) is not None:\n        raise IntfLvlConfErr(\"%r already has property %s old:%s new:%s\" % \n                             (obj, propName, repr(getattr(obj, propName)), prop))",
        "rewrite": "def nameAvailabilityCheck(obj, propName, prop):\n    if getattr(obj, propName, None) is not None:\n        raise IntfLvlConfErr(\"%r already has property %s old:%s new:%s\" % \n                             (obj, propName, repr(getattr(obj, propName)), prop))"
    },
    {
        "original": "def get_dataframe(self): \n        if pd is None:\n            raise ImportError(\"Try installing Pandas first.\")\n        frame = pd.DataFrame(self[:], columns=(self and self.keys) or [])\n        return frame",
        "rewrite": "def get_dataframe(self):\n    import pandas as pd\n    if pd is None:\n        raise ImportError(\"Try installing Pandas first.\")\n    frame = pd.DataFrame(self[:], columns=(self and self.keys) or [])\n    return frame"
    },
    {
        "original": "def synthesize(self, name, interfaces, targetPlatform): \n        ent = Entity(name)\n        ent._name = name + \"_inst\"  # instance name\n\n        # create generics\n        for _, v in self.params.items():\n            ent.generics.append(v)\n\n        # interface set for faster lookup\n        if isinstance(interfaces, set):\n            intfSet = interfaces\n        else:\n            intfSet = set(interfaces)\n\n   ",
        "rewrite": "def synthesize(self, name, interfaces, targetPlatform): \n    ent = Entity(name)\n    ent._name = name + \"_inst\"  # instance name\n\n    # create generics\n    for _, v in self.params.items():\n        ent.generics.append(v)\n\n    # interface set for faster lookup\n    intfSet = interfaces if isinstance(interfaces, set) else set(interfaces)"
    },
    {
        "original": "def show_items(self, cursor, items): \n        text_edit = self._text_edit\n        point = text_edit.cursorRect(cursor).bottomRight()\n        point = text_edit.mapToGlobal(point)\n        height = self.sizeHint().height()\n        screen_rect = QtGui.QApplication.desktop().availableGeometry(self)\n        if screen_rect.size().height() - point.y() - height < 0:\n            point = text_edit.mapToGlobal(text_edit.cursorRect().topRight())\n            point.setY(point.y() - height)\n        self.move(point)\n\n        self._start_position = cursor.position()\n        self.clear()\n    ",
        "rewrite": "def show_items(self, cursor, items): \n    text_edit = self._text_edit\n    point = text_edit.cursorRect(cursor).bottomRight()\n    point = text_edit.mapToGlobal(point)\n    height = self.sizeHint().height()\n    screen_rect = QtGui.QApplication.desktop().availableGeometry(self)\n    if screen_rect.size().height() - point.y() - height < 0:\n        point = text_edit.mapToGlobal(text_edit.cursorRect().topRight())\n        point.setY(point.y() - height)\n    self.move(point)\n\n    self._start_position = cursor.position()\n    self.clear()"
    },
    {
        "original": "def _re_pattern_pprint(obj, p, cycle): \n    p.text('re.compile(')\n    pattern = repr(obj.pattern)\n    if pattern[:1] in 'uU':\n        pattern = pattern[1:]\n        prefix = 'ur'\n    else:\n        prefix = 'r'\n    pattern = prefix + pattern.replace('\\\\\\\\', '\\\\')\n    p.text(pattern)\n    if obj.flags:\n        p.text(',')\n        p.breakable()\n        done_one = False\n        for flag in ('TEMPLATE', 'IGNORECASE', 'LOCALE', 'MULTILINE', 'DOTALL',\n            'UNICODE', 'VERBOSE',",
        "rewrite": "def _re_pattern_pprint(obj, p, cycle): \n    p.text('re.compile(')\n    pattern = repr(obj.pattern)\n    if pattern[:1] in 'uU':\n        pattern = pattern[1:]\n        prefix = 'ur'\n    else:\n        prefix = 'r'\n    pattern = prefix + pattern.replace('\\\\\\\\', '\\\\')\n    p.text(pattern)\n    if obj.flags:\n        p.text(',')\n        p.breakable()\n        done_one = False\n        for flag in ('TEMPLATE', 'IGNORECASE', 'LOCALE', 'MULTILINE', 'DOTALL',\n            'UNICODE', 'VERBOSE'):"
    },
    {
        "original": "def results(self): \n        return (\n            pod\n            for pod in self.pods\n            if pod.primary\n            or pod.title == 'Result'\n        )",
        "rewrite": "def results(self): \n    return (pod for pod in self.pods if pod.primary or pod.title == 'Result')"
    },
    {
        "original": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"): \n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])",
        "rewrite": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"): \n    with self._name_scope(name):\n        seq = tf.linspace(\n            tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(p, tf.int32))\n        return seq + tf.expand_dims(a, [-1])"
    },
    {
        "original": "def get_instance(self, data): \n        # Get unique fields\n        unique_fields = self.unique_fields\n        \n        # If there are no unique fields option, all items are new\n        if not unique_fields:\n            return self.model()\n        \n        # Build the filter\n        filter = dict([(f, data[f]) for f in unique_fields])\n        \n        # Get the instance",
        "rewrite": "def get_instance(self, data): \n        unique_fields = self.unique_fields\n        \n        if not unique_fields:\n            return self.model()\n        \n        filter = dict([(f, data[f]) for f in unique_fields])\n        \n        instance = self.model.objects.filter(**filter).first()\n        \n        return instance"
    },
    {
        "original": "def save_forensic_reports_to_splunk(self, forensic_reports): \n        logger.debug(\"Saving forensic reports to Splunk\")\n        if type(forensic_reports) == dict:\n            forensic_reports = [forensic_reports]\n\n        if len(forensic_reports) < 1:\n            return\n\n        json_str = \"\"\n        for report in forensic_reports:\n            data = self._common_data.copy()\n            data[\"sourcetype\"] = \"dmarc:forensic\"\n            timestamp = human_timestamp_to_timestamp(\n ",
        "rewrite": "def save_forensic_reports_to_splunk(self, forensic_reports):\n        logger.debug(\"Saving forensic reports to Splunk\")\n        if isinstance(forensic_reports, dict):\n            forensic_reports = [forensic_reports]\n\n        if not forensic_reports:\n            return\n\n        for report in forensic_reports:\n            data = self._common_data.copy()\n            data[\"sourcetype\"] = \"dmarc:forensic\"\n            timestamp = human_timestamp_to_timestamp(report[\"timestamp\"])\n            # Continue with the rest of the code."
    },
    {
        "original": "def pull(self, images, file_name=None, save=True, **kwargs): \n\n    if not isinstance(images,list):\n        images = [images]\n\n    bot.debug('Execution of PULL for %s images' %len(images))\n\n    # If used internally we want to return a list to the user.\n    finished = []\n    for image in images:\n\n        q = parse_image_name(remove_uri(image))\n\n        # Use container search to find the container based on uri\n        bot.info('Searching for %s in gs://%s' %(q['tag_uri'],self._bucket_name))\n        matches = self._container_query(q['tag_uri'], quiet=True)\n\n        if len(matches) == 0:\n   ",
        "rewrite": "def pull(self, images, file_name=None, save=True, **kwargs):\n    \n    if not isinstance(images,list):\n        images = [images]\n\n    bot.debug('Execution of PULL for %s images' % len(images))\n\n    finished = []\n    for image in images:\n        q = parse_image_name(remove_uri(image))\n        \n        bot.info('Searching for %s in gs://%s' % (q['tag_uri'], self._bucket_name))\n        matches = self._container_query(q['tag_uri'], quiet=True)\n\n        if len(matches) == 0:"
    },
    {
        "original": "def resume_program(self, index, resume_all=False): \n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"resumeProgram\", \"params\": {\n                    \"resumeAll\": resume_all\n                }}]}\n\n        log_msg_action = \"resume program\"\n  ",
        "rewrite": "def resume_program(self, index, resume_all=False): \n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"resumeProgram\", \"params\": {\n                    \"resumeAll\": resume_all\n                }}]}\n\n        log_msg_action = \"resume program\""
    },
    {
        "original": "def str_find(x, sub, start=0, end=None): \n    return _to_string_sequence(x).find(sub, start, 0 if end is None else end, end is None, True)",
        "rewrite": "def str_find(x, sub, start=0, end=None): \n    return _to_string_sequence(x).find(sub, start, 0 if end is None else end, end is None, True)"
    },
    {
        "original": "def set_npn_select_callback(self, callback): \n        _warn_npn()\n        self._npn_select_helper = _NpnSelectHelper(callback)\n        self._npn_select_callback = self._npn_select_helper.callback\n        _lib.SSL_CTX_set_next_proto_select_cb(\n            self._context, self._npn_select_callback, _ffi.NULL)",
        "rewrite": "def set_npn_select_callback(self, callback):\n        _warn_npn()\n        self._npn_select_helper = _NpnSelectHelper(callback)\n        self._npn_select_callback = self._npn_select_helper.callback\n        _lib.SSL_CTX_set_next_proto_select_cb(\n            self._context, self._npn_select_callback, _ffi.NULL)"
    },
    {
        "original": " \n    center = duration/2\n    width = duration-2*risefall\n    zeroed_width = duration + 2\n    return _sampled_gaussian_square_pulse(duration, amp, center, width, sigma,\n                                          zeroed_width=zeroed_width, name=name)",
        "rewrite": "center = duration / 2\nwidth = duration - 2 * risefall\nzeroed_width = duration + 2\nreturn _sampled_gaussian_square_pulse(duration, amp, center, width, sigma,\n                                      zeroed_width=zeroed_width, name=name)"
    },
    {
        "original": "def get_stargazers(self, url, headers={}): \n        url = url + '/stargazers?per_page=100&page=%s'\n        page = 1\n        gazers = []\n\n        json_data = requests.get(url % page, headers=headers).json()\n        while json_data:\n            gazers.extend(json_data)\n            page += 1\n            json_data = requests.get(url % page, headers=headers).json()\n        return gazers",
        "rewrite": "def get_stargazers(self, url, headers={}): \n    url = url + '/stargazers?per_page=100&page=%s'\n    page = 1\n    gazers = []\n\n    json_data = requests.get(url % page, headers=headers).json()\n    while json_data:\n        gazers.extend(json_data)\n        page += 1\n        json_data = requests.get(url % page, headers=headers).json()\n    \n    return gazers"
    },
    {
        "original": "def cmd_tool(args=None): \n    from argparse import ArgumentParser\n    parser = ArgumentParser(description=\"Command line utility for creating HDF5 Filterbank files.\")\n    parser.add_argument('dirname', type=str, help='Name of directory to read')\n    args = parser.parse_args()\n    \n    if not HAS_BITSHUFFLE:\n        print(\"Error: the bitshuffle library is required to run this script.\")\n        exit()\n\n    filelist = glob.glob(os.path.join(args.dirname, '*.fil'))\n\n    for filename in filelist:\n        if not os.path.exists(filename + '.h5'):\n            t0 = time.time()\n            print(\"\\nReading %s header...\"",
        "rewrite": "def code_tool(args=None): \n    from argparse import ArgumentParser\n    import os\n    parser = ArgumentParser(description=\"Command line utility for creating HDF5 Filterbank files.\")\n    parser.add_argument('dirname', type=str, help='Name of directory to read')\n    args = parser.parse_args()\n    \n    if not HAS_BITSHUFFLE:\n        print(\"Error: the bitshuffle library is required to run this script.\")\n        exit()\n\n    filelist = glob.glob(os.path.join(args.dirname, '*.fil'))\n\n    for filename in filelist:\n        if not os.path.exists(filename + '.h5'):\n            t0 = time.time()\n            print(\"\\nReading %s header...\")  # Code to read the file header goes here\n            print(f\"It took {time.time() - t0:.2f} seconds.\")"
    },
    {
        "original": "def star_import_used_line_numbers(messages): \n    for message in messages:\n        if isinstance(message, pyflakes.messages.ImportStarUsed):\n            yield message.lineno",
        "rewrite": "def star_import_used_line_numbers(messages): \n    for message in messages:\n       if isinstance(message, pyflakes.messages.ImportStarUsed):\n          yield message.lineno"
    },
    {
        "original": " \n\n        old_name = self._unique_constraint_name(\n            old_table_name, old_field, keys)\n        new_name = self._unique_constraint_name(\n            new_table_name, new_field, keys)\n\n        sql = self.sql_hstore_unique_rename.format(\n            old_name=self.quote_name(old_name),\n            new_name=self.quote_name(new_name)\n        )\n        self.execute(sql)",
        "rewrite": "old_name = self._unique_constraint_name(old_table_name, old_field, keys)\nnew_name = self._unique_constraint_name(new_table_name, new_field, keys)\n\nsql = self.sql_hstore_unique_rename.format(old_name=self.quote_name(old_name), new_name=self.quote_name(new_name))\nself.execute(sql)"
    },
    {
        "original": "def listcoins(self): \n        url = self.API_PATH + 'listCoins'\n        \n        json_data = json.loads(self._getdata(url))\n        \n        coins = []\n        for entry in json_data:\n            coin = Coin()\n            coin.id = entry['id']\n            coin.name = entry['name']\n            coin.website = entry['website']\n          ",
        "rewrite": "def listcoins(self): \n    url = self.API_PATH + 'listCoins'\n    \n    json_data = json.loads(self._getdata(url))\n    \n    coins = []\n    for entry in json_data:\n        coin = Coin()\n        coin.id = entry['id']\n        coin.name = entry['name']\n        coin.website = entry['website']"
    },
    {
        "original": "def sort_compare(lst1, lst2, inplace=1): \n    if not inplace:\n        lst1 = lst1[:]\n        lst2 = lst2[:]\n    lst1.sort(); lst2.sort()\n    return lst1 == lst2",
        "rewrite": "def sort_compare(lst1, lst2, inplace=True): \n    if not inplace:\n        lst1 = lst1[:]\n        lst2 = lst2[:]\n    lst1.sort()\n    lst2.sort()\n    return lst1 == lst2"
    },
    {
        "original": " \n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)",
        "rewrite": "_validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_names', role_names)\n        _validate_not_none('post_shutdown_action', post_shutdown_action)\n        return self._perform_post(\n            self._get_roles_operations_path(service_name, deployment_name),\n            _XmlSerializer.shutdown_roles_operation_to_xml(\n                role_names, post_shutdown_action),\n            as_async=True)"
    },
    {
        "original": "def error_parsing(msg=\"unknown options\"): \n    sys.stderr.write(\"Error parsing command line: %s\\ntry 'mongotail --help' for more information\\n\" % msg)\n    sys.stderr.flush()\n    exit(EINVAL)",
        "rewrite": "def error_parsing(msg=\"unknown options\"): \n    sys.stderr.write(\"Error parsing command line: %s\\ntry 'mongotail --help' for more information\\n\" % msg)\n    sys.stderr.flush()\n    exit(EINVAL)"
    },
    {
        "original": "def user_events(self, user_obj=None): \n        query = dict(user_id=user_obj['_id']) if user_obj else dict()\n        return self.event_collection.find(query)",
        "rewrite": "def user_events(self, user_obj=None): \n    query = {\"user_id\": user_obj['_id']} if user_obj else {}\n    return self.event_collection.find(query)"
    },
    {
        "original": "def _get_static_predicate(pred): \n  if pred in {0, 1}:  # Accept 1/0 as valid boolean values\n    pred_value = bool(pred)\n  elif isinstance(pred, bool):\n    pred_value = pred\n  elif isinstance(pred, tf.Tensor):\n    pred_value = tf.get_static_value(pred)\n\n    # TODO(jamieas): remove the dependency on `pywrap_tensorflow`.\n    # pylint: disable=protected-access\n    if pred_value is None:\n      pred_value = c_api.TF_TryEvaluateConstant_wrapper(pred.graph._c_graph,\n                                                      ",
        "rewrite": "import tensorflow as tf\r\n\r\ndef _get_static_predicate(pred):\r\n  if pred in {0, 1}:  # Accept 1/0 as valid boolean values\r\n    pred_value = bool(pred)\r\n  elif isinstance(pred, bool):\r\n    pred_value = pred\r\n  elif isinstance(pred, tf.Tensor):\r\n    if pred.shape.num_elements() != 1:\r\n        raise ValueError(\"Predicates should have a single value.\")\r\n    with tf.Session() as sess:\r\n        pred_value = sess.run(pred)\r\n\r\n  return pred_value"
    },
    {
        "original": "def create(self, path, data=None): \n        return self.handleresult(self.r.post(urljoin(self.url + CRUD_PATH,\n                                                     path),\n                                             data=json.dumps(data)))",
        "rewrite": "def create(self, path, data=None):\n    url = urljoin(self.url, CRUD_PATH) + path\n    response = self.r.post(url, data=json.dumps(data))\n    return self.handleresult(response)"
    },
    {
        "original": "def extractHolidayDate(self, setting_holiday): \n        ret = namedtuple(\"result\", [\"Holiday\", \"Month\", \"Day\"])\n        setting_holiday += 1\n        ret.Holiday = str(setting_holiday)\n\n        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):\n            ekm_log(\"Out of bounds:  holiday \" + str(setting_holiday))\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n\n        idxday = \"Holiday_\" + str(setting_holiday) + \"_Day\"\n        idxmon = \"Holiday_\" + str(setting_holiday)",
        "rewrite": "idxmon = \"Holiday_\" + str(setting_holiday) + \"_Month\""
    },
    {
        "original": "def rename_file(self, old_path, path): \n        with self.engine.begin() as db:\n            try:\n                if self.file_exists(old_path):\n                    rename_file(db, self.user_id, old_path, path)\n                elif self.dir_exists(old_path):\n                    rename_directory(db, self.user_id, old_path, path)\n                else:\n    ",
        "rewrite": "def rename_file(self, old_path, path):\n    with self.engine.begin() as db:\n        try:\n            if self.file_exists(old_path):\n                rename_file(db, self.user_id, old_path, path)\n            elif self.dir_exists(old_path):\n                rename_directory(db, self.user_id, old_path, path)\n            else:\n                pass"
    },
    {
        "original": " \n    handle_value = _CONST_VALUE_HANDLERS.get(type(form))\n    if handle_value is None and isinstance(form, ISeq):\n        handle_value = _const_seq_to_py_ast  # type: ignore\n    assert handle_value is not None, \"A type handler must be defined for constants\"\n    return handle_value(ctx, form)",
        "rewrite": "```python\nhandle_value = _CONST_VALUE_HANDLERS.get(type(form))\nif handle_value is None and isinstance(form, ISeq):\n    handle_value = _const_seq_to_py_ast\nassert handle_value is not None, \"A type handler must be defined for constants\"\nreturn handle_value(ctx, form)\n```"
    },
    {
        "original": " \n        check_front_door_name_availability_input = models.CheckNameAvailabilityInput(name=name, type=type)\n\n        api_version = \"2018-08-01\"\n\n        # Construct URL\n        url = self.check_front_door_name_availability.metadata['url']\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application/json'\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n   ",
        "rewrite": "check_front_door_name_availability_input = models.CheckNameAvailabilityInput(name=name, type=type)\n\napi_version = \"2018-08-01\"\n\n# Construct URL\nurl = self.check_front_door_name_availability.metadata['url']\n\n# Construct parameters\nquery_parameters = {}\nquery_parameters['api-version'] = self._serialize.query(\"api_version\", api_version, 'str')\n\n# Construct headers\nheader_parameters = {}\nheader_parameters['Accept'] = 'application/json'\nheader_parameters['Content-Type'] = 'application/json; charset=utf-8'\nif self.config.generate_client_request_id:"
    },
    {
        "original": "def _onUnsupportedMessage(self, client, userdata, message): \n        self.logger.warning(\n            \"Received messaging on unsupported topic '%s' on topic '%s'\" % (message.payload, message.topic)\n        )",
        "rewrite": "def _onUnsupportedMessage(self, client, userdata, message): \n        self.logger.warning(\n            \"Received messaging on unsupported topic '{}' on topic '{}'\".format(message.payload.decode('utf-8'), message.topic)\n        )"
    },
    {
        "original": "def main(clargs=None): \n    from argparse import ArgumentParser\n    from librarian.library import Library\n    import sys\n\n    parser = ArgumentParser(\n        description=\"A test runner for each card in a librarian library.\")\n    parser.add_argument(\"library\", help=\"Library database\")\n    parser.add_argument(\"-t\", \"--tests\", default=\"test/\",\n                        help=\"Test directory\")\n    args = parser.parse_args(clargs)\n\n    descovery(args.tests)\n\n    library = Library(args.library)\n    cardcount, passes, failures = execute_tests(library)\n    print(RESULTS.format(len(SINGLES), len(TESTS), cardcount, passes,\n               ",
        "rewrite": "def main(clargs=None):\n    from argparse import ArgumentParser\n    from librarian.library import Library\n    import sys\n\n    parser = ArgumentParser(description=\"A test runner for each card in a librarian library.\")\n    parser.add_argument(\"library\", help=\"Library database\")\n    parser.add_argument(\"-t\", \"--tests\", default=\"test/\", help=\"Test directory\")\n    args = parser.parse_args(clargs)\n\n    discovery(args.tests)\n\n    library = Library(args.library)\n    cardcount, passes, failures = execute_tests(library)\n    print(RESULTS.format(len(SINGLES), len(TESTS), cardcount, passes,\n           \" . No need to explain. Just write code:\"))"
    },
    {
        "original": "def to_dict(self): \n        dico = dict()\n        for field in self.fields.keys():\n            if field == \"flags\":\n                dico[field] = self.flags2text()\n            elif field == \"state\":\n                dico[field] = self.state2text()\n            else:\n                dico[field] = eval(\"self.\" + field)\n      ",
        "rewrite": "def to_dict(self):\n    dico = dict()\n    for field in self.fields.keys():\n        if field == \"flags\":\n            dico[field] = self.flags2text()\n        elif field == \"state\":\n            dico[field] = self.state2text()\n        else:\n            dico[field] = eval(\"self.\" + field)\n    return dico"
    },
    {
        "original": "def result(self, psd_state): \n        freq_array = numpy.fft.fftshift(psd_state['freq_array'])\n        pwr_array = numpy.fft.fftshift(psd_state['pwr_array'])\n\n        if self._crop_factor:\n            crop_bins_half = round((self._crop_factor * self._bins) / 2)\n            freq_array = freq_array[crop_bins_half:-crop_bins_half]\n            pwr_array = pwr_array[crop_bins_half:-crop_bins_half]\n\n        if psd_state['repeats'] > 1:\n            pwr_array = pwr_array / psd_state['repeats']\n\n        if self._log_scale:\n            pwr_array =",
        "rewrite": "numpy.log10(pwr_array)"
    },
    {
        "original": "def update(self, instance, validated_data): \n        if not instance.check_password(validated_data['old_password']):\n            msg = _('Invalid password.')\n            raise serializers.ValidationError({'old_password': msg})\n\n        instance.set_password(validated_data['new_password'])\n        instance.save()\n        return instance",
        "rewrite": "def update(self, instance, validated_data):\n    if not instance.check_password(validated_data['old_password']):\n        msg = _('Invalid password.')\n        raise serializers.ValidationError({'old_password': msg})\n\n    instance.set_password(validated_data['new_password'])\n    instance.save()\n    return instance"
    },
    {
        "original": "def _construct_message(self): \n        self.message = {\"token\": self._auth, \"channel\": self.channel}\n        super()._construct_message()",
        "rewrite": "def _construct_message(self): \n    self.message = {\"token\": self._auth, \"channel\": self.channel}\n    super()._construct_message()"
    },
    {
        "original": "def sysinfo(self): \n\n        import coverage as covmod\n        import platform, re\n\n        try:\n            implementation = platform.python_implementation()\n        except AttributeError:\n            implementation = \"unknown\"\n\n        info = [\n            ('version', covmod.__version__),\n            ('coverage', covmod.__file__),\n            ('cover_dir', self.cover_dir),\n          ",
        "rewrite": "def sysinfo(self): \n\n    import coverage as covmod\n    import platform\n\n    try:\n        implementation = platform.python_implementation()\n    except AttributeError:\n        implementation = \"unknown\"\n\n    info = [\n        ('version', covmod.__version__),\n        ('coverage', covmod.__file__),\n        ('cover_dir', self.cover_dir)\n    ]"
    },
    {
        "original": "def absolute_position(self, x, y): \n        (a, b, c, d, e, f) = self._currentMatrix\n        xp = a * x + c * y + e\n        yp = b * x + d * y + f\n        return xp, yp",
        "rewrite": "def absolute_position(self, x, y): \n    (a, b, c, d, e, f) = self._currentMatrix\n    xp = a * x + c * y + e\n    yp = b * x + d * y + f\n    return xp, yp"
    },
    {
        "original": "def init_webapp(self): \n        self.web_app = NotebookWebApplication(\n            self, self.kernel_manager, self.notebook_manager, \n            self.cluster_manager, self.log,\n            self.base_project_url, self.webapp_settings\n        )\n        if self.certfile:\n            ssl_options = dict(certfile=self.certfile)\n            if self.keyfile:\n                ssl_options['keyfile'] = self.keyfile\n        else:\n    ",
        "rewrite": "def init_webapp(self):\n    self.web_app = NotebookWebApplication(\n        self, self.kernel_manager, self.notebook_manager, \n        self.cluster_manager, self.log,\n        self.base_project_url, self.webapp_settings\n    )\n    if self.certfile:\n        ssl_options = dict(certfile=self.certfile)\n        if self.keyfile:\n            ssl_options['keyfile'] = self.keyfile"
    },
    {
        "original": "def dag_paused(dag_id, paused): \n\n    DagModel = models.DagModel\n    with create_session() as session:\n        orm_dag = (\n            session.query(DagModel)\n                   .filter(DagModel.dag_id == dag_id).first()\n        )\n        if paused == 'true':\n            orm_dag.is_paused = True\n        else:\n            orm_dag.is_paused = False\n        session.merge(orm_dag)\n     ",
        "rewrite": "def dag_paused(dag_id, paused): \n\n    DagModel = models.DagModel\n    with create_session() as session:\n        orm_dag = (\n            session.query(DagModel)\n                   .filter(DagModel.dag_id == dag_id).first()\n        )\n        if paused == 'true':\n            orm_dag.is_paused = True\n        else:\n            orm_dag.is_paused = False\n        session.merge(orm_dag)"
    },
    {
        "original": "def check_part_index(state, name, index, part_msg, missing_msg=None, expand_msg=None): \n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    # create message\n    ordinal = get_ord(index + 1) if isinstance(index, int) else \"\"\n    fmt_kwargs = {\"index\": index, \"ordinal\": ordinal}\n    fmt_kwargs.update(part=render(part_msg, fmt_kwargs))\n\n    append_message = {\"msg\": expand_msg, \"kwargs\": fmt_kwargs}\n\n    # check there are enough parts for index\n    has_part(state, name, missing_msg, fmt_kwargs, index)\n\n    # get part at index\n    stu_part",
        "rewrite": "def check_part_index(state, name, index, part_msg, missing_msg=None, expand_msg=None): \n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    ordinal = get_ord(index + 1) if isinstance(index, int) else \"\"\n    fmt_kwargs = {\"index\": index, \"ordinal\": ordinal}\n    fmt_kwargs.update(part=render(part_msg, fmt_kwargs))\n\n    append_message = {\"msg\": expand_msg, \"kwargs\": fmt_kwargs}\n\n    has_part(state, name, missing_msg, fmt_kwargs, index)"
    },
    {
        "original": "def remote_app(self, name, version=None, **kwargs): \n        if version is None:\n            if 'request_token_url' in kwargs:\n                version = '1'\n            else:\n                version = '2'\n        if version == '1':\n            remote_app = OAuth1Application(name, clients=cached_clients)\n        elif version == '2':\n           ",
        "rewrite": "remote_app = OAuth2Application(name, clients=cached_clients)"
    },
    {
        "original": " \n        for repo in dict_to_write:\n            if len(dict_to_write[repo]) != 0:#don't need to write out empty lists\n                path = ('../github-data/' + organization + '/' + repo + '/' +\n                     path_ending_type + '/' + str(date) + '.json')\n                self.checkDir(path)\n                with open(path, 'w') as out:\n    ",
        "rewrite": "```python\nfor repo in dict_to_write:\n    if dict_to_write[repo]:\n        path = f'../github-data/{organization}/{repo}/{path_ending_type}/{date}.json'\n        self.checkDir(path)\n        with open(path, 'w') as out:\n```"
    },
    {
        "original": "def bootstrap_results(self, state): \n\n    def loss():\n      q = self._flattened_variational_distribution()\n      # TODO(siege): How to seed this?\n      samples = q.sample(self.train_batch_size)\n      return tf.reduce_mean(\n          input_tensor=q.log_prob(samples) -\n          self._flattened_target_log_prob(samples),\n          axis=-1)\n\n    lr = tf.convert_to_tensor(value=self.learning_rate, dtype=self._dtype)\n    dtype = lr.dtype\n\n    learning_rate = tf.compat.v2.optimizers.schedules.PiecewiseConstantDecay(\n        list(self.num_train_steps *\n             np.array([0.2, 0.8]).astype(dtype.as_numpy_dtype())),\n        [lr, lr * 0.1, lr",
        "rewrite": "def bootstrap_results(self, state): \n\n    def loss():\n      q = self._flattened_variational_distribution()\n      samples = q.sample(self.train_batch_size)\n      return tf.reduce_mean(\n          q.log_prob(samples) - self._flattened_target_log_prob(samples),\n          axis=-1)\n\n    lr = tf.convert_to_tensor(value=self.learning_rate, dtype=self._dtype)\n    dtype = lr.dtype\n\n    learning_rate = tf.compat.v2.optimizers.schedules.PiecewiseConstantDecay(\n        list(self.num_train_steps *\n             np.array([0.2, 0.8]).astype(dtype.as_numpy_dtype())),\n        [lr, lr * 0.1, lr)"
    },
    {
        "original": "def cancel(self, msg='', exc_type=CancelledError): \n        for transfer_coordinator in self.tracked_transfer_coordinators:\n            transfer_coordinator.cancel(msg, exc_type)",
        "rewrite": "def cancel(self, msg='', exc_type=CancelledError): \n        for transfer_coordinator in self.tracked_transfer_coordinators:\n            transfer_coordinator.cancel(msg, exc_type)"
    },
    {
        "original": "def render_tag(self, context, *tag_args, **tag_kwargs): \n        if self.as_var:\n            # Assign the value in the parent context\n            context[self.as_var] = self.get_value(context, *tag_args, **tag_kwargs)\n\n        return u''",
        "rewrite": "def render_tag(self, context, *tag_args, **tag_kwargs): \n    if self.as_var:\n        context[self.as_var] = self.get_value(context, *tag_args, **tag_kwargs)\n\n    return u''"
    },
    {
        "original": "def save_verifier(self, token, verifier, request): def verifiersetter(token, verifier, request): \n        log.debug('Save verifier %r for %r', verifier, token)\n        self._verifiersetter(\n            token=token, verifier=verifier, request=request\n        )",
        "rewrite": "def save_verifier(self, token, verifier, request): \n    def verifiersetter(token, verifier, request): \n        log.debug('Save verifier %r for %r', verifier, token)\n        self._verifiersetter(token=token, verifier=verifier, request=request)"
    },
    {
        "original": "def setDecode(self, decodeTable): \n        self.decodeTable = decodeTable\n        #set of symbols with unknown length\n        todo = set(decodeTable)\n        #bit size under investigation\n        maskLength = 0\n        lengthTable = {}\n        while todo:\n            mask = (1<<maskLength)-1\n            #split the encodings that we didn't find yet using b bits\n            splitSymbols = defaultdict(list)\n ",
        "rewrite": "def setDecode(self, decodeTable):\n    self.decodeTable = decodeTable\n    todo = set(decodeTable)\n    maskLength = 0\n    lengthTable = {}\n    while todo:\n        mask = (1 << maskLength) - 1\n        splitSymbols = defaultdict(list)"
    },
    {
        "original": "def add_step(self, step): \n        if not step:\n            raise TypeError('WaterfallDialog.add_step(): step cannot be None.')\n\n        self._steps.append(step)\n        return self",
        "rewrite": "def add_step(self, step):\n    if not step:\n        raise TypeError('WaterfallDialog.add_step(): step cannot be None.')\n\n    self._steps.append(step)\n    return self"
    },
    {
        "original": "def append_known_secrets(self): \n        for file_name in self.files:\n            if \"~\" in file_name:\n                file_name = os.path.expanduser(file_name)\n            if not os.path.isfile(file_name):\n                print(\n                    \"Don't have \"\n                    + Back.BLACK\n        ",
        "rewrite": "def append_known_secrets(self):\n        for file_name in self.files:\n            if \"~\" in file_name:\n                file_name = os.path.expanduser(file_name)\n            if not os.path.isfile(file_name):\n                print(\"Don't have \" + Back.BLACK + file_name + \". No need to explain. Just write code.\")"
    },
    {
        "original": "def create_tfs_git_client(url, token=None): \n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'\n        raise RuntimeError(msg, url)\n\n    return tfs_git_client",
        "rewrite": "import os\n\ndef create_tfs_git_client(url, token=None): \n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = f'Unable to create TFS Git Client, failed to connect to TFS Enterprise ({url}) with provided token.'\n        raise RuntimeError(msg)\n\n    return tfs_git_client"
    },
    {
        "original": "def _integrate_plugins(): \n    import sys\n    from airflow.plugins_manager import sensors_modules\n    for sensors_module in sensors_modules:\n        sys.modules[sensors_module.__name__] = sensors_module\n        globals()[sensors_module._name] = sensors_module",
        "rewrite": "def integrate_plugins():\n    import sys\n    from airflow.plugins_manager import sensors_modules\n    for sensors_module in sensors_modules:\n        sys.modules[sensors_module.__name__] = sensors_module\n        globals()[sensors_module.__name__] = sensors_module"
    },
    {
        "original": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        super(SignatureVerifyRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n        ",
        "rewrite": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    super(SignatureVerifyRequestPayload, self).read(\n        input_stream,\n        kmip_version=kmip_version\n    )\n    \n    local_stream = utils.BytearrayStream(input_stream.read(self.length))\n    \n    if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n        self._unique_identifier = primitives.TextString(\n            tag=enums.Tags.UNIQUE_IDENTIFIER\n        )\n        self._unique_identifier.read(local_stream)"
    },
    {
        "original": " \n    if marked_line_numbers and line_number == sorted(marked_line_numbers)[0]:\n        return ''\n\n    return line",
        "rewrite": "if marked_line_numbers and line_number == sorted(marked_line_numbers)[0]:\n        return ''\n    return line"
    },
    {
        "original": "def packData(self, data): \n        typeOfWord = simBitsT(self.wordWidth, None)\n        fieldToVal = self._fieldToTPart\n        if fieldToVal is None:\n            fieldToVal = self._fieldToTPart = self.fieldToDataDict(\n                self.origin.dtype,\n                data,\n                {})\n\n        for _, transParts in self.walkWords(showPadding=True):\n            actualVldMask = 0\n  ",
        "rewrite": "def packData(self, data):\n    typeOfWord = simBitsT(self.wordWidth, None)\n    fieldToVal = self._fieldToTPart\n    if fieldToVal is None:\n        fieldToVal = self._fieldToTPart = self.fieldToDataDict(self.origin.dtype, data, {})\n\n    for _, transParts in self.walkWords(showPadding=True):\n        actualVldMask = 0"
    },
    {
        "original": "def safe_execfile_ipy(self, fname): \n        fname = os.path.abspath(os.path.expanduser(fname))\n\n        # Make sure we can open the file\n        try:\n            with open(fname) as thefile:\n                pass\n        except:\n            warn('Could not open file <%s> for safe execution.' % fname)\n            return\n\n        # Find things also in current directory.  This is needed to",
        "rewrite": "def safe_execfile_ipy(self, fname): \n    fname = os.path.abspath(os.path.expanduser(fname))\n\n    try:\n        with open(fname) as thefile:\n            pass\n    except FileNotFoundError:\n        warn('Could not open file <%s> for safe execution.' % fname)\n        return"
    },
    {
        "original": "def get_ipython_dir(): \n\n    env = os.environ\n    pjoin = os.path.join\n\n\n    ipdir_def = '.ipython'\n    xdg_def = 'ipython'\n\n    home_dir = get_home_dir()\n    xdg_dir = get_xdg_dir()\n    \n    # import pdb; pdb.set_trace()  # dbg\n    if 'IPYTHON_DIR' in env:\n        warnings.warn('The environment variable IPYTHON_DIR is deprecated. '\n                      'Please use IPYTHONDIR instead.')\n    ipdir = env.get('IPYTHONDIR', env.get('IPYTHON_DIR', None))\n    if ipdir is None:\n        # not set explicitly, use XDG_CONFIG_HOME or",
        "rewrite": "import warnings\n\nimport os\nfrom os.path import join\nhome_dir = os.path.expanduser('~')\n\ndef get_xdg_dir():\n    xdg_dir = os.environ.get('XDG_CONFIG_HOME', None)\n    if xdg_dir is None:\n        xdg_dir = os.path.join(home_dir, '.config')\n    return xdg_dir\n\ndef get_ipython_dir():\n    env = os.environ\n\n    ipdir_def = '.ipython'\n    xdg_def = 'ipython'\n\n    xdg_dir = get_xdg_dir()\n\n    if 'IPYTHON_DIR' in env:\n        warnings.warn('The environment variable IPYTHON_DIR is deprecated. '\n                      'Please use IPYTHONDIR instead.')\n\n    ipdir = env.get('IPYTHONDIR', env.get('IPYTHON_DIR', None))\n    if ipdir is None:\n        ipdir = xdg_dir if xdg_dir else ipdir_def\n        \n    return ipdir"
    },
    {
        "original": " \n\n        if prefix == '':\n            prefix_sep = ''\n\n        if not exists(output_dir):\n            makedirs(output_dir)\n\n        logger.debug(\"Saving results...\")\n        if image_list is None:\n            image_list = self.images.keys()\n        for suffix, img in self.images.items():\n            if suffix in image_list:\n                filename = prefix",
        "rewrite": "filename = f\"{prefix}{prefix_sep}{suffix}\""
    },
    {
        "original": " \n        m = mecha.to_native()\n        data1 = ckbytelist(wrappedKey)\n        handle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\n        attrs = self._template2ckattrlist(template)\n        rv = self.lib.C_UnwrapKey(self.session, m, unwrappingKey,\n                                  data1, attrs, handle)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return handle",
        "rewrite": "m = mecha.to_native()\ndata1 = ckbytelist(wrappedKey)\nhandle = PyKCS11.LowLevel.CK_OBJECT_HANDLE()\nattrs = self._template2ckattrlist(template)\nrv = self.lib.C_UnwrapKey(self.session, m, unwrappingKey,\n                          data1, attrs, handle)\nif rv != CKR_OK:\n    raise PyKCS11Error(rv)\nreturn handle"
    },
    {
        "original": "def user_factory(self): \n        if this.user_id is None:\n            return None\n        return self.user_model.objects.get(pk=this.user_id)",
        "rewrite": "def user_factory(self):\n    if self.user_id is None:\n        return None\n    return self.user_model.objects.get(pk=self.user_id)"
    },
    {
        "original": "def _interp_dep_vector(wave, indep_vector): \n    dep_vector_is_int = wave.dep_vector.dtype.name.startswith(\"int\")\n    dep_vector_is_complex = wave.dep_vector.dtype.name.startswith(\"complex\")\n    if (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LOG\"):\n        wave_interp_func = scipy.interpolate.interp1d(\n            np.log10(wave.indep_vector), wave.dep_vector\n        )\n        ret = wave_interp_func(np.log10(indep_vector))\n    elif (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LINEAR\"):\n        dep_vector = (\n            wave.dep_vector.astype(np.float64)\n            if not dep_vector_is_complex\n            else wave.dep_vector\n    ",
        "rewrite": "def _interp_dep_vector(wave, indep_vector): \n    dep_vector_is_int = wave.dep_vector.dtype.name.startswith(\"int\")\n    dep_vector_is_complex = wave.dep_vector.dtype.name.startswith(\"complex\")\n    if (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LOG\"):\n        wave_interp_func = scipy.interpolate.interp1d(\n            np.log10(wave.indep_vector), wave.dep_vector\n        )\n        ret = wave_interp_func(np.log10(indep_vector))\n    elif (wave.interp, wave.indep_scale) == (\"CONTINUOUS\", \"LINEAR\"):\n        dep_vector = (\n            wave.dep_vector.astype(np.float64)\n            if not dep_vector_is_complex\n            else wave.dep_vector\n    # The code is incomplete and there seems to be a missing closing parenthesis."
    },
    {
        "original": "def poll_sysdig_capture(self, capture): \n        if 'id' not in capture:\n            return [False, 'Invalid capture format']\n\n        url = '{url}/api/sysdig/{id}?source={source}'.format(\n            url=self.url, id=capture['id'], source=self.product)\n        res = requests.get(url, headers=self.hdrs, verify=self.ssl_verify)\n        return self._request_result(res)",
        "rewrite": "def poll_sysdig_capture(self, capture): \n    if 'id' not in capture:\n        return [False, 'Invalid capture format']\n\n    url = '{url}/api/sysdig/{id}?source={source}'.format(\n        url=self.url, id=capture['id'], source=self.product)\n    res = requests.get(url, headers=self.hdrs, verify=self.ssl_verify)\n    return self._request_result(res)"
    },
    {
        "original": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        local_stream = BytearrayStream()\n\n        if self._credential_type:\n            self._credential_type.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        else:\n            raise ValueError(\n                \"Credential struct missing the credential type.\"\n      ",
        "rewrite": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    local_stream = BytearrayStream()\n\n    if self._credential_type:\n         self._credential_type.write(\n             local_stream,\n             kmip_version=kmip_version\n         )\n     else:\n        raise ValueError(\n            \"Credential struct missing the credential type.\"\n       )"
    },
    {
        "original": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        local_stream = BytearrayStream()\n\n        if self._nonce_id:\n            self._nonce_id.write(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\"Nonce struct is missing the nonce ID.\")\n\n        if self._nonce_value:\n            self._nonce_value.write(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\"Nonce struct is missing the nonce value.\")\n\n        self.length = local_stream.length()\n     ",
        "rewrite": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        local_stream = BytearrayStream()\n\n        if self._nonce_id:\n            self._nonce_id.write(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\"Nonce struct is missing the nonce ID.\")\n\n        if self._nonce_value:\n            self._nonce_value.write(local_stream, kmip_version=kmip_version)\n        else:\n            raise ValueError(\"Nonce struct is missing the nonce value.\")\n        \n        self.length = local_stream.length()"
    },
    {
        "original": "def parse_host(host): \n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
        "rewrite": "import re\nfrom urllib import parse\n\ndef parse_host(host): \n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"
    },
    {
        "original": "def get_state_exitcode_details(self, resource_group, name): \n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)",
        "rewrite": "def get_state_exitcode_details(self, resource_group, name): \n    current_state = self._get_instance_view(resource_group, name).current_state\n    return (current_state.state,\n            current_state.exit_code,\n            current_state.detail_status)"
    },
    {
        "original": "def sign_input_at(self, start_index, private_key): \n        if not self.hash:\n            raise RuntimeError('Cannot sign inputs until bundle is finalized.')\n\n        private_key.sign_input_transactions(self, start_index)",
        "rewrite": "def sign_input_at(self, start_index, private_key): \n    if not self.hash:\n        raise RuntimeError('Cannot sign inputs until bundle is finalized.')\n\n    private_key.sign_input_transactions(self, start_index)"
    },
    {
        "original": "def add_extra_chain_cert(self, certobj): \n        if not isinstance(certobj, X509):\n            raise TypeError(\"certobj must be an X509 instance\")\n\n        copy = _lib.X509_dup(certobj._x509)\n        add_result = _lib.SSL_CTX_add_extra_chain_cert(self._context, copy)\n        if not add_result:\n            # TODO: This is untested.\n            _lib.X509_free(copy)\n            _raise_current_error()",
        "rewrite": "def add_extra_chain_cert(self, certobj):\n    if not isinstance(certobj, X509):\n        raise TypeError(\"certobj must be an X509 instance\")\n\n    copy = _lib.X509_dup(certobj._x509)\n    add_result = _lib.SSL_CTX_add_extra_chain_cert(self._context, copy)\n    if not add_result:\n        _lib.X509_free(copy)\n        _raise_current_error()"
    },
    {
        "original": "def name(self, src=None): \n        return \"{%s}\" % \", \".join(\"%s: %s\" % (key, _get_type_name(ktype, src))\n                                  for key, ktype in viewitems(self._types))",
        "rewrite": "def name(self, src=None): \n    return \"{%s}\" % \", \".join(\"%s: %s\" % (key, _get_type_name(ktype, src)) \n                                  for key, ktype in self._types.items())"
    },
    {
        "original": " \n        lst = []\n        for s in self.values():\n            if hasattr(s, 'tret') and s.tret.is_polymorphic:\n                # encapsulate s into a EvalCtx for meta-var resolution\n                lst.append(EvalCtx.from_sig(s))\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope",
        "rewrite": "lst = []\nfor s in self.values():\n    if hasattr(s, 'tret') and s.tret.is_polymorphic:\n        lst.append(EvalCtx.from_sig(s))\nrscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\nrscope.set_parent(self)\nreturn rscope"
    },
    {
        "original": "def _save_file(self, db, model, path): \n        save_file(\n            db,\n            self.user_id,\n            path,\n            to_b64(model['content'], model.get('format', None)),\n            self.crypto.encrypt,\n            self.max_file_size_bytes,\n        )\n        return None",
        "rewrite": "def _save_file(self, db, model, path):\n        save_file(\n            db,\n            self.user_id,\n            path,\n            to_b64(model['content'], model.get('format', None)),\n            self.crypto.encrypt,\n            self.max_file_size_bytes\n        )\n        return None"
    },
    {
        "original": "def album_tracks(self, spotify_id, limit=20, offset=0, market='US'): \n        route = Route('GET', '/albums/{spotify_id}/tracks', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)",
        "rewrite": "def album_tracks(self, spotify_id, limit=20, offset=0, market='US'): \n    route = Route('GET', f'/albums/{spotify_id}/tracks', spotify_id=spotify_id)\n    payload = {'limit': limit, 'offset': offset}\n\n    if market:\n        payload['market'] = market\n\n    return self.request(route, params=payload)"
    },
    {
        "original": "def _maybe_assert_valid_concentration(self, concentration, validate_args): \n    if not validate_args:\n      return concentration\n    concentration = distribution_util.embed_check_categorical_event_shape(\n        concentration)\n    return distribution_util.with_dependencies([\n        assert_util.assert_positive(\n            concentration, message=\"Concentration parameter must be positive.\"),\n    ], concentration)",
        "rewrite": "def _maybe_assert_valid_concentration(self, concentration, validate_args):\n    if not validate_args:\n        return concentration\n    concentration = distribution_util.embed_check_categorical_event_shape(\n        concentration)\n    return distribution_util.with_dependencies([\n        assert_util.assert_positive(\n            concentration, message=\"Concentration parameter must be positive.\")\n    ], concentration)"
    },
    {
        "original": "def matches(self, node, value): \n\n        if self.skip(value):\n            return True\n\n        if not self._valid_value(value):\n            msg = \"Invalid value {value} passed to filter {name} - \".format(\n                value=repr(value),\n                name=self.name)\n\n            if self.default is not None:\n                warn(msg + \"defaulting to {}\".format(self.default))\n ",
        "rewrite": "def matches(self, node, value): \n\n        if self.skip(value):\n            return True\n\n        if not self._valid_value(value):\n            msg = \"Invalid value {value} passed to filter {name} - \".format(\n                value=repr(value),\n                name=self.name)\n\n            if self.default is not None:\n                warn(msg + \"defaulting to {}\".format(self.default))"
    },
    {
        "original": "def get_plugin_option(self, plugin, key): \n\n        if plugin in self.plugins:\n            plugin = self.plugins[plugin]\n            return plugin.get_option(key)",
        "rewrite": "def get_plugin_option(self, plugin, key): \n    if plugin in self.plugins:\n        return self.plugins[plugin].get_option(key)"
    },
    {
        "original": "def get_station_observation(station_code, token): \n    req = requests.get(\n        API_ENDPOINT_OBS % (station_code),\n        params={\n            'token': token\n        })\n\n    if req.status_code == 200 and req.json()['status'] == \"ok\":\n        return parse_observation_response(req.json()['data'])\n    else:\n        return {}",
        "rewrite": "def get_station_observation(station_code, token): \n    req = requests.get(\n        API_ENDPOINT_OBS % (station_code),\n        params={\n            'token': token\n        })\n\n    if req.status_code == 200 and req.json()['status'] == \"ok\":\n        return parse_observation_response(req.json()['data'])\n    else:\n        return {}"
    },
    {
        "original": "def generate_hpo_gene_list(self, *hpo_terms): \n        genes = {}\n        for term in hpo_terms:\n            hpo_obj = self.hpo_term(term)\n            if hpo_obj:\n                for hgnc_id in hpo_obj['genes']:\n                    if hgnc_id in genes:\n                        genes[hgnc_id] += 1\n        ",
        "rewrite": "def generate_hpo_gene_list(self, *hpo_terms): \n    genes = {}\n    for term in hpo_terms:\n        hpo_obj = self.hpo_term(term)\n        if hpo_obj:\n            for hgnc_id in hpo_obj['genes']:\n                if hgnc_id in genes:\n                    genes[hgnc_id] += 1"
    },
    {
        "original": "def find_n_data_blocks(self): \n        self.file_obj.seek(0)\n        header0, data_idx0 = self.read_header()\n\n        self.file_obj.seek(data_idx0)\n        block_size = int(header0['BLOCSIZE'])\n        n_bits = int(header0['NBITS'])\n        self.file_obj.seek(int(header0['BLOCSIZE']), 1)\n        n_blocks = 1\n        end_found = False\n        while not end_found:\n            try:\n                header, data_idx = self.read_header()\n        ",
        "rewrite": "def find_n_data_blocks(self): \n    self.file_obj.seek(0)\n    header0, data_idx0 = self.read_header()\n\n    self.file_obj.seek(data_idx0)\n    block_size = int(header0['BLOCSIZE'])\n    n_bits = int(header0['NBITS'])\n    self.file_obj.seek(block_size, 1)\n    n_blocks = 1\n    end_found = False\n    while not end_found:\n        try:\n            header, data_idx = self.read_header()\n        except:\n            end_found = True"
    },
    {
        "original": "def _send(self, data, content_type): \n\n        headers = {\n            \"Content-Type\": content_type,\n            \"Authorization\": \"key=%s\" % (self.api_key),\n            \"Content-Length\": str(len(data))\n        }\n\n        request = Request(self.api_url, data, headers)\n        return urlopen(request).read().decode(self.encoding)",
        "rewrite": "def _send(self, data, content_type): \n    headers = {\n        \"Content-Type\": content_type,\n        \"Authorization\": \"key=%s\" % self.api_key,\n        \"Content-Length\": str(len(data))\n    }\n    \n    request = Request(self.api_url, data.encode(), headers)\n    return urlopen(request).read().decode(self.encoding)"
    },
    {
        "original": "def _onOutgoingMessageReceived(self, conn, message): \n\n        if not conn.sendRandKey:\n            conn.sendRandKey = message\n            conn.send(self._selfNode.address)\n\n        node = self._connToNode(conn)\n        conn.setOnMessageReceivedCallback(functools.partial(self._onMessageReceived, node))\n        self._onNodeConnected(node)",
        "rewrite": "def _onOutgoingMessageReceived(self, conn, message): \n\n    if not conn.sendRandKey:\n        conn.sendRandKey = message\n        conn.send(self._selfNode.address)\n\n    node = self._connToNode(conn)\n    conn.setOnMessageReceivedCallback(functools.partial(self._onMessageReceived, node))\n    self._onNodeConnected(node)"
    },
    {
        "original": "def convert_tree(beautiful_soup_tree, makeelement=None): \n    if makeelement is None:\n        makeelement = html.html_parser.makeelement\n    root = _convert_tree(beautiful_soup_tree, makeelement)\n    children = root.getchildren()\n    for child in children:\n        root.remove(child)\n    return children",
        "rewrite": "def convert_tree(beautiful_soup_tree, makeelement=None):\n    if makeelement is None:\n        makeelement = html.html_parser.makeelement\n    root = _convert_tree(beautiful_soup_tree, makeelement)\n    children = root.getchildren()\n    for child in children:\n        root.remove(child)\n    return children"
    },
    {
        "original": "def jsfile(url): \n\n    if not url.startswith('http://') and not url[:1] == '/':\n        #add media_url for relative paths\n        url = settings.STATIC_URL + url\n\n    return '<script type=\"text/javascript\" src=\"{src}\"></script>'.format(\n        src=url)",
        "rewrite": "def jsfile(url):\n    if not url.startswith('http://') and not url.startswith('/'):\n        url = settings.STATIC_URL + url\n\n    return '<script type=\"text/javascript\" src=\"{src}\"></script>'.format(src=url)"
    },
    {
        "original": "def ping(self, params=None): \n        try:\n            self.transport.perform_request('HEAD', '/', params=params)\n        except TransportError:\n            raise gen.Return(False)\n        raise gen.Return(True)",
        "rewrite": "def ping(self, params=None):\n    try:\n        self.transport.perform_request('HEAD', '/', params=params)\n    except TransportError:\n        raise gen.Return(False)\n        \n    raise gen.Return(True)"
    },
    {
        "original": "def get_course(self, course_id, params={}): \n        include = params.get(\"include\", [])\n        if \"term\" not in include:\n            include.append(\"term\")\n        params[\"include\"] = include\n\n        url = COURSES_API.format(course_id)\n        return CanvasCourse(data=self._get_resource(url, params=params))",
        "rewrite": "def get_course(self, course_id, params={}):\n    include = params.get(\"include\", [])\n    if \"term\" not in include:\n        include.append(\"term\")\n    params[\"include\"] = include\n\n    url = COURSES_API.format(course_id)\n    return CanvasCourse(data=self._get_resource(url, params=params))"
    },
    {
        "original": "def main(): \n    options = get_options()\n    Windows.enable(auto_colors=True, reset_atexit=True)\n\n    try:\n        # maybe check if virtualenv is not activated\n        check_for_virtualenv(options)\n\n        # 1. detect requirements files\n        filenames = RequirementsDetector(options.get('<requirements_file>')).get_filenames()\n        if filenames:\n            print(Color('{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} '\n                        '{{autocyan}}\\n{}{{/autocyan}}'.format('\\n'.join(filenames))))\n        else:  # pragma: nocover\n    ",
        "rewrite": "def main(): \n    options = get_options()\n    Windows.enable(auto_colors=True, reset_atexit=True)\n\n    try:\n        check_for_virtualenv(options)\n\n        # Detect requirements files\n        filenames = RequirementsDetector(options.get('<requirements_file>')).get_filenames()\n        if filenames:\n            print(Color('{{autoyellow}}Found valid requirements file(s):{{/autoyellow}} '\n                        '{{autocyan}}\\n{}{{/autocyan}}'.format('\\n'.join(filenames)))\n        else:\n            pass  # pragma: nocover"
    },
    {
        "original": "def import_hive_table(database=None, table=None, partitions=None, allow_multi_format=False):     \n    assert_is_type(database, str, None)\n    assert_is_type(table, str)\n    assert_is_type(partitions, [[str]], None)\n    p = { \"database\": database, \"table\": table, \"partitions\": partitions, \"allow_multi_format\": allow_multi_format }\n    j = H2OJob(api(\"POST /3/ImportHiveTable\", data=p), \"Import Hive Table\").poll()\n    return get_frame(j.dest_key)",
        "rewrite": "def import_hive_table(database:str=None, table:str=None, partitions:[[str]]=None, allow_multi_format:bool=False):     \n    assert_is_type(database, str, None)\n    assert_is_type(table, str)\n    assert_is_type(partitions, [[str]], None)\n    p = { \"database\": database, \"table\": table, \"partitions\": partitions, \"allow_multi_format\": allow_multi_format }\n    j = H2OJob(api(\"POST /3/ImportHiveTable\", data=p), \"Import Hive Table\").poll()\n    return get_frame(j.dest_key)"
    },
    {
        "original": "def get_paths(self): \n        outfile = self.get_outfile()\n        rel_path = os.path.relpath(outfile, settings.STATIC_ROOT)\n        return outfile, rel_path",
        "rewrite": "def get_paths(self):\n    outfile = self.get_outfile()\n    rel_path = os.path.relpath(outfile, settings.STATIC_ROOT)\n    return outfile, rel_path"
    },
    {
        "original": "def get(self): \n        self.set_header('Content-Type', 'application/json')\n        ws_href = '{}://{}'.format(\n            'wss' if self.request.protocol == 'https' else 'ws',\n            self.request.headers.get('Host', '')\n        )\n\n        descriptions = []\n        for thing in self.things.get_things():\n            description = thing.as_thing_description()\n            description['links'].append({\n                'rel': 'alternate',\n   ",
        "rewrite": "def get(self): \n        self.set_header('Content-Type', 'application/json')\n        ws_href = '{}://{}'.format(\n            'wss' if self.request.protocol == 'https' else 'ws',\n            self.request.headers.get('Host', '')\n        )\n\n        descriptions = []\n        for thing in self.things.get_things():\n            description = thing.as_thing_description()\n            description['links'].append({\n                'rel': 'alternate',"
    },
    {
        "original": "def model_metrics(self, timeoutSecs=60, **kwargs): \n    result = self.do_json_request('/3/ModelMetrics.json', cmd='get', timeout=timeoutSecs)\n    h2o_sandbox.check_sandbox_for_errors()\n    return result",
        "rewrite": "def model_metrics(self, timeoutSecs=60, **kwargs): \n    result = self.do_json_request('/3/ModelMetrics.json', cmd='get', timeout=timeoutSecs)\n    h2o_sandbox.check_sandbox_for_errors()\n    return result"
    },
    {
        "original": "def list(self): \n        return self._post(\n            request=ApiActions.LIST.value,\n            uri=ApiUri.HOOKS.value,\n        ).get('hooks')",
        "rewrite": "def list(self): \n    return self._post(request=ApiActions.LIST.value, uri=ApiUri.HOOKS.value).get('hooks')"
    },
    {
        "original": "def serialCmdPwdAuth(self, password_str): \n        result = False\n        try:\n            req_start = \"0150310228\" + binascii.hexlify(password_str) + \"2903\"\n            req_crc = self.calc_crc16(req_start[2:].decode(\"hex\"))\n            req_str = req_start + req_crc\n            self.m_serial_port.write(req_str.decode(\"hex\"))\n            if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                ekm_log(\"Password accepted (\" + self.getContext() + \")\")\n         ",
        "rewrite": "def serialCmdPwdAuth(self, password_str): \n    result = False\n    try:\n        req_start = \"0150310228\" + binascii.hexlify(password_str) + \"2903\"\n        req_crc = self.calc_crc16(bytes.fromhex(req_start[2:]))\n        req_str = req_start + req_crc\n        self.m_serial_port.write(bytes.fromhex(req_str))\n        if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n            ekm_log(\"Password accepted (\" + self.getContext() + \")\")"
    },
    {
        "original": "def _create_dummy_trip_stoptimes(trip_id, stops, first_service_time): \n    waiting = datetime.timedelta(seconds=30)\n    arrival = first_service_time\n    last_departure = first_service_time\n    last_departure_hour = (arrival + waiting).hour\n    last_stop = None\n\n    departure_hour = None\n    arrival_hour = None\n    for stop_sequence, stop in enumerate(stops):\n\n        # Avoid time travels\n        arrival = last_departure + get_time_from_last_stop(last_stop, stop)\n        departure = arrival + waiting\n\n        # Cover the case when the arrival time falls into the next day\n        if arrival.hour < last_departure_hour:\n    ",
        "rewrite": "def _create_dummy_trip_stoptimes(trip_id, stops, first_service_time): \n    waiting = datetime.timedelta(seconds=30)\n    arrival = first_service_time\n    last_departure = first_service_time\n    last_departure_hour = (arrival + waiting).hour\n    last_stop = None\n\n    departure_hour = None\n    arrival_hour = None\n    for stop_sequence, stop in enumerate(stops):\n\n        # Avoid time travels\n        arrival = last_departure + get_time_from_last_stop(last_stop, stop)\n        departure = arrival + waiting\n\n        # Cover the case when the arrival time falls into the next day\n        if arrival.hour < last_departure_hour:"
    },
    {
        "original": "def __match_intervals(intervals_from, intervals_to, strict=True): \n    # sort index of the interval starts\n    start_index = np.argsort(intervals_to[:, 0])\n\n    # sort index of the interval ends\n    end_index = np.argsort(intervals_to[:, 1])\n\n    # and sorted values of starts\n    start_sorted = intervals_to[start_index, 0]\n    # and ends\n    end_sorted = intervals_to[end_index, 1]\n\n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n\n    output = np.empty(len(intervals_from), dtype=numba.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]\n\n        # Find the intervals that start after our query ends\n ",
        "rewrite": "import numpy as np\nimport numba\n\ndef __match_intervals(intervals_from, intervals_to, strict=True):\n    start_index = np.argsort(intervals_to[:, 0])\n    end_index = np.argsort(intervals_to[:, 1])\n    \n    start_sorted = intervals_to[start_index, 0]\n    end_sorted = intervals_to[end_index, 1]\n    \n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n    \n    output = np.empty(len(intervals_from), dtype=np.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]"
    },
    {
        "original": "def send_request(self): \n        assert self.client\n        if self.current_state == STATE_BOUND:\n            pkt = self.client.gen_request_unicast()\n        else:\n            pkt = self.client.gen_request()\n        sendp(pkt)\n        logger.debug('Modifying FSM obj, setting time_sent_request.')\n        self.time_sent_request = nowutc()\n        logger.info('DHCPREQUEST of %s on %s to %s port %s',\n                    self.client.iface, self.client.client_ip,\n  ",
        "rewrite": "def send_request(self):\n    assert self.client\n    if self.current_state == STATE_BOUND:\n        pkt = self.client.gen_request_unicast()\n    else:\n        pkt = self.client.gen_request()\n    sendp(pkt)\n    logger.debug('Modifying FSM obj, setting time_sent_request.')\n    self.time_sent_request = nowutc()\n    logger.info('DHCPREQUEST of %s on %s to %s port %s',\n                self.client.iface, self.client.client_ip, self.client.server_ip, self.client.server_port)"
    },
    {
        "original": "def clinsig_human(variant_obj): \n    for clinsig_obj in variant_obj['clnsig']:\n        # The clinsig objects allways have a accession\n        if isinstance(clinsig_obj['accession'], int):\n            # New version\n            link = \"https://www.ncbi.nlm.nih.gov/clinvar/variation/{}\"\n        else:\n            # Old version\n            link = \"https://www.ncbi.nlm.nih.gov/clinvar/{}\"\n\n        human_str = 'not provided'\n        if clinsig_obj.get('value'):\n         ",
        "rewrite": "def clinsig_human(variant_obj): \n    for clinsig_obj in variant_obj['clnsig']:\n        # The clinsig objects always have an accession\n        link = \"https://www.ncbi.nlm.nih.gov/clinvar/{}\" if isinstance(clinsig_obj['accession'], int) else \"https://www.ncbi.nlm.nih.gov/clinvar/{}\"\n        human_str = 'not provided'\n        if clinsig_obj.get('value'): \n            # continue with the logic here (No need to explain as per the instructions)"
    },
    {
        "original": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        super(CheckResponsePayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n        ",
        "rewrite": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    super(CheckResponsePayload, self).read(\n        input_stream,\n        kmip_version=kmip_version\n    )\n    local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n    if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n        self._unique_identifier = primitives.TextString(\n            tag=enums.Tags.UNIQUE_IDENTIFIER\n        )\n        self._unique_identifier.read("
    },
    {
        "original": "def institute(self, institute_id): \n        LOG.debug(\"Fetch institute {}\".format(institute_id))\n        institute_obj = self.institute_collection.find_one({\n            '_id': institute_id\n        })\n        if institute_obj is None:\n            LOG.debug(\"Could not find institute {0}\".format(institute_id))\n\n        return institute_obj",
        "rewrite": "def institute(self, institute_id):\n    LOG.debug(f\"Fetch institute {institute_id}\")\n    institute_obj = self.institute_collection.find_one({\n        '_id': institute_id\n    })\n    if institute_obj is None:\n        LOG.debug(f\"Could not find institute {institute_id}\")\n\n    return institute_obj"
    },
    {
        "original": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        super(ApplicationSpecificInformation, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.application_namespace.read(tstream, kmip_version=kmip_version)\n        self.application_data.read(tstream, kmip_version=kmip_version)\n\n        self.is_oversized(tstream)\n        self.validate()",
        "rewrite": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    super(ApplicationSpecificInformation, self).read(\n        istream,\n        kmip_version=kmip_version\n    )\n    tstream = BytearrayStream(istream.read(self.length))\n\n    self.application_namespace.read(tstream, kmip_version=kmip_version)\n    self.application_data.read(tstream, kmip_version=kmip_version)\n\n    self.is_oversized(tstream)\n    self.validate()"
    },
    {
        "original": "def derive_key(self): \n        salt = want_bytes(self.salt)\n        if self.key_derivation == 'concat':\n            return self.digest_method(salt + self.secret_key).digest()\n        elif self.key_derivation == 'django-concat':\n            return self.digest_method(salt + b'signer' +\n                self.secret_key).digest()\n        elif self.key_derivation == 'hmac':\n            mac = hmac.new(self.secret_key, digestmod=self.digest_method)\n            mac.update(salt)\n        ",
        "rewrite": "def derive_key(self): \n        salt = self.want_bytes(self.salt)\n        if self.key_derivation == 'concat':\n            return self.digest_method(salt + self.secret_key).digest()\n        elif self.key_derivation == 'django-concat':\n            return self.digest_method(salt + b'signer' + self.secret_key).digest()\n        elif self.key_derivation == 'hmac':\n            mac = hmac.new(self.secret_key, digestmod=self.digest_method)\n            mac.update(salt)\n            return mac.digest()"
    },
    {
        "original": "def set_client_ca_list(self, certificate_authorities): \n        name_stack = _lib.sk_X509_NAME_new_null()\n        _openssl_assert(name_stack != _ffi.NULL)\n\n        try:\n            for ca_name in certificate_authorities:\n                if not isinstance(ca_name, X509Name):\n                    raise TypeError(\n                        \"client CAs must be X509Name objects, not %s \"\n         ",
        "rewrite": "def set_client_ca_list(self, certificate_authorities):\n    name_stack = _lib.sk_X509_NAME_new_null()\n    _openssl_assert(name_stack != _ffi.NULL)\n\n    try:\n        for ca_name in certificate_authorities:\n            if not isinstance(ca_name, X509Name):\n                raise TypeError(\n                    \"client CAs must be X509Name objects, not %s\" % type(ca_name)\n                )\n    except TypeError as e:\n        raise e"
    },
    {
        "original": "def _save_notebook(self, db, model, path): \n        nb_contents = from_dict(model['content'])\n        self.check_and_sign(nb_contents, path)\n        save_file(\n            db,\n            self.user_id,\n            path,\n            writes_base64(nb_contents),\n            self.crypto.encrypt,\n            self.max_file_size_bytes,\n        )\n        # It's awkward that this writes to the",
        "rewrite": "def _save_notebook(self, db, model, path): \n        nb_contents = from_dict(model['content'])\n        self.check_and_sign(nb_contents, path)\n        save_file(\n            db,\n            self.user_id,\n            path,\n            writes_base64(nb_contents),\n            self.crypto.encrypt,\n            self.max_file_size_bytes,\n        )"
    },
    {
        "original": "def about_godot(self, info): \n        if info.initialized:\n            self.edit_traits(parent=info.ui.control,\n                kind=\"livemodal\", view=about_view)",
        "rewrite": "def about_godot(self, info): \n    if info.initialized:\n        self.edit_traits(parent=info.ui.control, kind=\"livemodal\", view=about_view)"
    },
    {
        "original": "def blocking(indices, block_size, initial_boundary=0): \n    blocks = []\n\n    for idx in indices:\n        bl_idx = (idx-initial_boundary)//float(block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks",
        "rewrite": "def blocking(indices, block_size, initial_boundary=0): \n    blocks = []\n\n    for idx in indices:\n        bl_idx = int((idx - initial_boundary) / block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks"
    },
    {
        "original": "def conversion_factor(from_symbol, to_symbol, date): \n\n    from_currency = Currency.objects.get(symbol=from_symbol)\n    try:\n        from_currency_price = CurrencyPrice.objects.get(currency=from_currency, date=date).mid_price\n    except CurrencyPrice.DoesNotExist:\n        print \"Cannot fetch prices for %s on %s\" % (str(from_currency), str(date))\n        return None\n\n    to_currency = Currency.objects.get(symbol=to_symbol)\n    try:\n        to_currency_price = CurrencyPrice.objects.get(currency=to_currency, date=date).mid_price\n    except CurrencyPrice.DoesNotExist:\n        print \"Cannot fetch prices for %s on %s\" % (str(to_currency), str(date))\n        return None\n\n    return to_currency_price / from_currency_price",
        "rewrite": "def conversion_factor(from_symbol, to_symbol, date):\n    from_currency = Currency.objects.get(symbol=from_symbol)\n    try:\n        from_currency_price = CurrencyPrice.objects.get(currency=from_currency, date=date).mid_price\n    except CurrencyPrice.DoesNotExist:\n        print(\"Cannot fetch prices for %s on %s\" % (str(from_currency), str(date)))\n        return None\n\n    to_currency = Currency.objects.get(symbol=to_symbol)\n    try:\n        to_currency_price = CurrencyPrice.objects.get(currency=to_currency, date=date).mid_price\n    except CurrencyPrice.DoesNotExist:\n        print(\"Cannot fetch prices for %s on %s\" % (str(to_currency), str(date)))\n        return None\n\n    return to_currency_price / from_currency_price"
    },
    {
        "original": "def dump_additional_data(self, valid_data, many, original_data): \n        if many:\n            for i, _ in enumerate(valid_data):\n                additional_keys = set(original_data[i].__dict__) - set(valid_data[i])\n                for key in additional_keys:\n                    valid_data[i][key] = getattr(original_data[i], key)\n        else:\n            additional_keys = set(original_data.__dict__) - set(valid_data)\n           ",
        "rewrite": "def dump_additional_data(self, valid_data, many, original_data): \n    if many:\n        for i, _ in enumerate(valid_data):\n            additional_keys = set(original_data[i].__dict__) - set(valid_data[i])\n            for key in additional_keys:\n                valid_data[i][key] = getattr(original_data[i], key)\n    else:\n        additional_keys = set(original_data.__dict__) - set(valid_data)"
    },
    {
        "original": "def _wait_for_operation_to_complete(self, operation_name): \n        service = self.get_conn()\n        while True:\n            operation_response = service.operations().get(\n                name=operation_name,\n            ).execute(num_retries=self.num_retries)\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n              ",
        "rewrite": "def _wait_for_operation_to_complete(self, operation_name): \n    service = self.get_conn()\n    while True:\n        operation_response = service.operations().get(\n            name=operation_name,\n        ).execute(num_retries=self.num_retries)\n        if operation_response.get(\"done\"):\n            response = operation_response.get(\"response\")\n            error = operation_response.get(\"error\")"
    },
    {
        "original": "def qasm(self): \n        name_param = self.name\n        if self.params:\n            name_param = \"%s(%s)\" % (name_param, \",\".join(\n                [str(i) for i in self.params]))\n\n        return self._qasmif(name_param)",
        "rewrite": "def qasm(self):\n        name_param = self.name\n        if self.params:\n            name_param = \"%s(%s)\" % (name_param, \",\".join([str(i) for i in self.params]))\n        \n        return self._qasmif(name_param)"
    },
    {
        "original": "def parse(self, config): \n\n        if \"type\" not in config:\n            raise InvalidConfigurationException(\"The dashboard configuration has not defined a 'type'. %s\" % config)\n\n        component_type = config[\"type\"]\n        component_config = self._get_config_by_id(self._component_configs, component_type)\n\n        if not component_config:\n            raise ComponentNotFoundForType(component_type)\n\n        options = config.get(\"options\", {})\n        component = self._parse_item(component_config, options, config)\n        component.name = config.get(\"name\", \"\")\n        return component",
        "rewrite": "def parse(self, config): \n\n        if \"type\" not in config:\n            raise InvalidConfigurationException(\"The dashboard configuration has not defined a 'type'. %s\" % config)\n\n        component_type = config[\"type\"]\n        component_config = self._get_config_by_id(self._component_configs, component_type)\n\n        if not component_config:\n            raise ComponentNotFoundForType(component_type)\n\n        options = config.get(\"options\", {})\n        component = self._parse_item(component_config, options, config)\n        component.name = config.get(\"name\", \"\")\n        return component"
    },
    {
        "original": "def errors(self): \n        try:\n            self._errors.extend(self._validator)  # type: List[Text]\n        except StopIteration:\n            pass\n\n        return self._errors",
        "rewrite": "def errors(self): \n    try:\n        self._errors.extend(self._validator)  # type: List[Text]\n    except StopIteration:\n        pass\n    return self._errors"
    },
    {
        "original": " \n    checksum = [0] * (HASH_LENGTH * len(fragments))\n    normalized_hash = normalize(hash_)\n\n    for i, fragment in enumerate(fragments):\n        outer_sponge = sponge_type()\n\n        # If there are more than 3 iterations, loop back around to the\n        # start.\n        normalized_chunk = normalized_hash[i % len(normalized_hash)]\n\n        buffer = []\n        for j, hash_trytes in enumerate(fragment.iter_chunks(Hash.LEN)):\n            buffer = hash_trytes.as_trits()  # type: List[int]\n           ",
        "rewrite": "```\n    checksum = [0] * (HASH_LENGTH * len(fragments))\n    normalized_hash = normalize(hash_)\n\n    for i, fragment in enumerate(fragments):\n        outer_sponge = sponge_type()\n\n        normalized_chunk = normalized_hash[i % len(normalized_hash)]\n\n        buffer = []\n        for j, hash_trytes in enumerate(fragment.iter_chunks(Hash.LEN)):\n            buffer = hash_trytes.as_trits()  \n```"
    },
    {
        "original": "def f_add_config_group(self, *args, **kwargs): \n        return self._nn_interface._add_generic(self, type_name=CONFIG_GROUP,\n                                               group_type_name=CONFIG_GROUP,\n                                               args=args, kwargs=kwargs)",
        "rewrite": "def f_add_config_group(self, *args, **kwargs):\n    return self._nn_interface._add_generic(self, type_name=\"CONFIG_GROUP\",\n                                           group_type_name=\"CONFIG_GROUP\",\n                                           args=args, kwargs=kwargs)"
    },
    {
        "original": "def add_extension_if_needed(filepath, ext, check_if_exists=False): \n    if not filepath.endswith(ext):\n        filepath += ext\n\n    if check_if_exists:\n        if not os.path.exists(filepath):\n            err = 'File not found: ' + filepath\n            log.error(err)\n            raise IOError(err)\n\n    return filepath",
        "rewrite": "import os\nimport logging\n\ndef add_extension_if_needed(filepath, ext, check_if_exists=False): \n    if not filepath.endswith(ext):\n        filepath += ext\n\n    if check_if_exists:\n        if not os.path.exists(filepath):\n            err = 'File not found: ' + filepath\n            logging.error(err)\n            raise IOError(err)\n\n    return filepath"
    },
    {
        "original": "def fetchall(self): \n        self._check_executed()\n        r = self._fetch_row(0)\n        self.rownumber = self.rownumber + len(r)\n        self._warning_check()\n        return r",
        "rewrite": "def fetchall(self):\n    self._check_executed()\n    r = self._fetch_row(0)\n    self.rownumber += len(r)\n    self._warning_check()\n    return r"
    },
    {
        "original": "def fromPy(cls, val, typeObj, vldMask=None): \n        if val is None:\n            assert vldMask is None or vldMask == 0\n            valid = False\n            val = typeObj._allValues[0]\n        else:\n            if vldMask is None or vldMask == 1:\n                assert isinstance(val, str)\n                valid = True\n ",
        "rewrite": "def fromPy(cls, val, typeObj, vldMask=None):\n    if val is None:\n        assert vldMask is None or vldMask == 0\n        valid = False\n        val = typeObj._allValues[0]\n    else:\n        if vldMask is None or vldMask == 1:\n            assert isinstance(val, str)\n            valid = True"
    },
    {
        "original": "def get(self, name): \n        labels = self.list()\n        return [\n            label\n            for label\n            in labels\n            if name == label.get('name')\n        ]",
        "rewrite": "def get(self, name):\n    labels = self.list()\n    return [label for label in labels if name == label.get('name')]"
    },
    {
        "original": "def delete_model(self, key, ignoreMissingKey=True, timeoutSecs=60, **kwargs): \n    assert key is not None, '\"key\" parameter is null'\n\n    result = self.do_json_request('/3/Models.json/' + key, cmd='delete', timeout=timeoutSecs)\n\n    # TODO: look for what?\n    if not ignoreMissingKey and 'f00b4r' in result:\n        raise ValueError('Model key not found: ' + key)\n\n    verboseprint(\"delete_model result:\", dump_json(result))\n    return result",
        "rewrite": "def delete_model(self, key, ignoreMissingKey=True, timeoutSecs=60, **kwargs): \n    assert key is not None, '\"key\" parameter is null'\n\n    result = self.do_json_request('/3/Models.json/' + key, cmd='delete', timeout=timeoutSecs)\n\n    if not ignoreMissingKey and 'f00b4r' in result:\n        raise ValueError('Model key not found: ' + key)\n\n    verboseprint(\"delete_model result:\", dump_json(result))\n    return result"
    },
    {
        "original": " \n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.entry(SYM_MACRO_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )",
        "rewrite": "return (\n        Maybe(v.meta)\n        .map(lambda m: m.entry(SYM_MACRO_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )"
    },
    {
        "original": "def relative_dir_walk(self, dir): \n    result = []\n\n    if S3URL.is_valid(dir):\n      basepath = S3URL(dir).path\n      for f in (f for f in self.s3walk(dir) if not f['is_dir']):\n        result.append(os.path.relpath(S3URL(f['name']).path, basepath))\n    else:\n      for f in (f for f in self.local_walk(dir) if not os.path.isdir(f)):\n        result.append(os.path.relpath(f, dir))\n\n    return result",
        "rewrite": "def relative_dir_walk(self, dir): \n    result = []\n\n    if S3URL.is_valid(dir):\n        basepath = S3URL(dir).path\n\n        for f in (f for f in self.s3walk(dir) if not f['is_dir']):\n            result.append(os.path.relpath(S3URL(f['name']).path, basepath))\n    else:\n        for f in (f for f in self.local_walk(dir) if not os.path.isdir(f)):\n            result.append(os.path.relpath(f, dir))\n\n    return result"
    },
    {
        "original": "def toSimModel(unit, targetPlatform=DummyPlatform(), dumpModelIn=None): \n    sim_code = toRtl(unit,\n                     targetPlatform=targetPlatform,\n                     saveTo=dumpModelIn,\n                     serializer=SimModelSerializer)\n    if dumpModelIn is not None:\n        d = os.path.join(os.getcwd(), dumpModelIn)\n        dInPath = d in sys.path\n        if not dInPath:\n            sys.path.insert(0, d)\n",
        "rewrite": "def toSimModel(unit, targetPlatform=DummyPlatform(), dumpModelIn=None): \n    sim_code = toRtl(unit,\n                     targetPlatform=targetPlatform,\n                     saveTo=dumpModelIn,\n                     serializer=SimModelSerializer)\n    if dumpModelIn is not None:\n        d = os.path.join(os.getcwd(), dumpModelIn)\n        dInPath = d in sys.path\n        if not dInPath:\n            sys.path.insert(0, d)"
    },
    {
        "original": "def permission_check(apikey, endpoint): \n        try:\n            ak = APIKeys.objects.get(apikey=apikey)\n            apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n            if apitree.match(endpoint):\n                return ak.user if ak.user else AnonymousUser(), ak.seckey\n        except APIKeys.DoesNotExist:\n            pass\n        return None, None",
        "rewrite": "def permission_check(apikey, endpoint):\n    try:\n        ak = APIKeys.objects.get(apikey=apikey)\n        apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n        if apitree.match(endpoint):\n            return ak.user if ak.user else AnonymousUser(), ak.seckey\n    except APIKeys.DoesNotExist:\n        pass\n    return None, None"
    },
    {
        "original": "def arg_split(s, posix=False, strict=True): \n\n    # Unfortunately, python's shlex module is buggy with unicode input:\n    # http://bugs.python.org/issue1170\n    # At least encoding the input when it's unicode seems to help, but there\n    # may be more problems lurking.  Apparently this is fixed in python3.\n    is_unicode = False\n    if (not py3compat.PY3) and isinstance(s, unicode):\n        is_unicode = True\n        s = s.encode('utf-8')\n    lex = shlex.shlex(s, posix=posix)\n    lex.whitespace_split = True\n    # Extract tokens, ensuring that things like leaving open quotes\n    # does not cause this to raise. ",
        "rewrite": "from typing import Union\nimport shlex\n\ndef arg_split(s: Union[str, bytes], posix: bool = False, strict: bool = True): \n    lex = shlex.shlex(s, posix=posix)\n    lex.whitespace_split = True"
    },
    {
        "original": "def global_matches(self, text): \n        #print 'Completer->global_matches, txt=%r' % text # dbg\n        matches = []\n        match_append = matches.append\n        n = len(text)\n        for lst in [keyword.kwlist,\n                    __builtin__.__dict__.keys(),\n                    self.namespace.keys(),\n                    self.global_namespace.keys()]:\n         ",
        "rewrite": "def global_matches(self, text): \n    matches = []\n    match_append = matches.append\n    n = len(text)\n    for lst in [keyword.kwlist,\n                list(__builtin__.__dict__.keys()),\n                list(self.namespace.keys()),\n                list(self.global_namespace.keys())]:"
    },
    {
        "original": "def __terminate(self): \n\n        if self.__stderr_file:\n            self.__stderr_file.close()\n\n        if not self.__process:\n            return\n\n        waitfor = time.time() + _PROCESS_KILL_TIMEOUT\n        while time.time() < waitfor:\n            try:\n                self.__process.terminate()\n            except EnvironmentError as e:\n                if",
        "rewrite": "def __terminate(self): \n\n        if self.__stderr_file:\n            self.__stderr_file.close()\n\n        if not self.__process:\n            return\n\n        waitfor = time.time() + _PROCESS_KILL_TIMEOUT\n        while time.time() < waitfor:\n            try:\n                self.__process.terminate()\n            except EnvironmentError as e:\n                if False:"
    },
    {
        "original": " \n    # Read paml output.\n    with open(filename, 'r') as f:\n        data = f.read()\n\n    # Rip all trees out of the codeml output.\n    regex = re.compile('\\([()\\w\\:. ,]+;')\n    trees = regex.findall(data)\n    anc_tree = trees[2]\n\n    # First tree in codeml file is the original input tree\n    tip_tree = dendropy.Tree.get(data=trees[0], schema='newick')\n\n    # Third tree in codeml fule is ancestor tree.\n    anc_tree = dendropy.Tree.get(data=trees[2], schema='newick')\n\n    # Main tree to return\n    tree = tip_tree\n\n    # Map ancestors onto main tree object\n    ancestors = anc_tree.internal_nodes()\n",
        "rewrite": "import re\nimport dendropy\n\nwith open(filename, 'r') as f:\n    data = f.read()\n\nregex = re.compile('\\([()\\w\\:. ,]+;')\ntrees = regex.findall(data)\nanc_tree = trees[2]\n\ntip_tree = dendropy.Tree.get(data=trees[0], schema='newick')\nanc_tree = dendropy.Tree.get(data=trees[2], schema='newick')\n\ntree = tip_tree\n\nancestors = anc_tree.internal_nodes()"
    },
    {
        "original": "def _populate_commands(self): \n        cmd_instances = []\n        from trepan.bwprocessor import command as Mcommand\n        eval_cmd_template = 'command_mod.%s(self)'\n        for mod_name in Mcommand.__modules__:\n            import_name = \"command.\" + mod_name\n            try:\n                command_mod = getattr(__import__(import_name), mod_name)\n            except:\n                print('Error importing %s: %s' % (mod_name, sys.exc_info()[0]))\n",
        "rewrite": "def _populate_commands(self): \n    cmd_instances = []\n    from trepan.bwprocessor import command as Mcommand\n    eval_cmd_template = 'command_mod.%s(self)'\n    \n    for mod_name in Mcommand.__modules__:\n        import_name = \"command.\" + mod_name\n        try:\n            command_mod = getattr(__import__(import_name), mod_name)\n        except Exception as e:\n            print('Error importing %s: %s' % (mod_name, e))"
    },
    {
        "original": "def _get_word_end_cursor(self, position): \n        document = self._control.document()\n        end = self._get_end_cursor().position()\n        while position < end and \\\n                  not is_letter_or_number(document.characterAt(position)):\n            position += 1\n        while position < end and \\\n                  is_letter_or_number(document.characterAt(position)):\n            position += 1\n        cursor = self._control.textCursor()\n   ",
        "rewrite": "def _get_word_end_cursor(self, position): \n    document = self._control.document()\n    end = self._get_end_cursor().position()\n    while position < end and not is_letter_or_number(document.characterAt(position)):\n        position += 1\n    while position < end and is_letter_or_number(document.characterAt(position)):\n        position += 1\n    cursor = self._control.textCursor()"
    },
    {
        "original": "def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test): \n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta',",
        "rewrite": "eta': hp.uniform('eta', 0.1, 0.3)"
    },
    {
        "original": "def add_requirement(self, install_req, parent_req_name=None): \n        name = install_req.name\n        if not install_req.match_markers():\n            logger.warning(\"Ignoring %s: markers %r don't match your \"\n                           \"environment\", install_req.name,\n                           install_req.markers)\n            return []\n\n        install_req.as_egg = self.as_egg\n      ",
        "rewrite": "def add_requirement(self, install_req, parent_req_name=None): \n    name = install_req.name\n    if not install_req.match_markers():\n        logger.warning(\"Ignoring %s: markers %r don't match your environment\", install_req.name, install_req.markers)\n        return []\n\n    install_req.as_egg = self.as_egg"
    },
    {
        "original": "def evaluate_variable(self, name): \n        if isinstance(self.variables[name], six.string_types):\n            # TODO: this does not allow more than one level deep variable, like a depends on b, b on c, c is a const\n            value = eval(self.variables[name], expression_namespace, self.variables)\n            return value\n        else:\n            return self.variables[name]",
        "rewrite": "def evaluate_variable(self, name):\n        if isinstance(self.variables[name], six.string_types):\n            value = eval(self.variables[name], expression_namespace, self.variables)\n            return value\n        else:\n            return self.variables[name]"
    },
    {
        "original": "def dict_repr_html(dictionary): \n    rows = ''\n    s = '<tr><td><strong>{k}</strong></td><td>{v}</td></tr>'\n    for k, v in dictionary.items():\n        rows += s.format(k=k, v=v)\n    html = '<table>{}</table>'.format(rows)\n    return html",
        "rewrite": "def dict_repr_html(dictionary): \n    rows = ''\n    s = '<tr><td><strong>{}</strong></td><td>{}</td></tr>'\n    for k, v in dictionary.items():\n        rows += s.format(k, v)\n    html = '<table>{}</table>'.format(rows)\n    return html"
    },
    {
        "original": "def format_duration(seconds): \n    units, divider = get_time_units_and_multiplier(seconds)\n    seconds *= divider\n    return \"%.3f %s\" % (seconds, units)",
        "rewrite": "def format_duration(seconds): \n    units, divider = get_time_units_and_multiplier(seconds)\n    seconds *= divider\n    return \"{:.3f} {}\".format(seconds, units)"
    },
    {
        "original": "def _scan_step(self, vars): \n        from neural_var import NeuralVariable\n        if not self._loop_vars:\n            raise Exception(\"The loop is not initialized. To initialize the loop, use `with loop as vars`\")\n        replace_map = {}\n        for k, var in vars.items():\n            if var is not None:\n                replace_map[self._dummy_nodes[k].tensor] = var.tensor\n        outputs = {}\n        for k in self._outputs:\n",
        "rewrite": "def _scan_step(self, vars): \n    from neural_var import NeuralVariable\n    if not self._loop_vars:\n        raise Exception(\"The loop is not initialized. To initialize the loop, use `with loop as vars`\")\n    replace_map = {}\n    for k, var in vars.items():\n        if var is not None:\n            replace_map[self._dummy_nodes[k].tensor] = var.tensor\n    outputs = {}"
    },
    {
        "original": " \n  fig = figure.Figure(figsize=(9, 3*n))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n  for i in range(n):\n    ax = fig.add_subplot(n, 3, 3*i + 1)\n    ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 2)\n    for prob_sample in probs:\n      sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n      ax.set_ylim([0, 1])\n    ax.set_title(\"posterior samples\")\n\n    ax = fig.add_subplot(n, 3, 3*i + 3)\n    sns.barplot(np.arange(10), np.mean(probs[:, i, :], axis=0), ax=ax)\n    ax.set_ylim([0, 1])\n    ax.set_title(\"predictive probs\")\n  fig.suptitle(title)\n  fig.tight_layout()\n\n  canvas.print_figure(fname, format=\"png\")\n  print(\"saved {}\".format(fname))",
        "rewrite": "fig = figure.Figure(figsize=(9, 3*n))\ncanvas = backend_agg.FigureCanvasAgg(fig)\nfor i in range(n):\n  ax = fig.add_subplot(n, 3, 3*i + 1)\n  ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n\n  ax = fig.add_subplot(n, 3, 3*i + 2)\n  for prob_sample in probs:\n    sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n    ax.set_ylim([0, 1])\n  ax.set_title(\"posterior samples\")\n\n  ax = fig.add_subplot(n, 3, 3*i + 3)\n  sns.barplot(np.arange(10), np.mean(probs[:, i, :], axis=0), ax=ax)\n  ax.set_ylim([0, 1])\n  ax.set_title(\"predictive probs\")\nfig.suptitle(title)\nfig.tight_layout()\n\ncanvas.print_figure(fname, format=\"png\")\nprint(\"saved {}\".format(fname))"
    },
    {
        "original": "def rtext(maxlength, minlength=1, choices=string.ascii_letters): \n    return ''.join(choice(choices) for x in range(randint(minlength, maxlength)))",
        "rewrite": "import string\nfrom random import choice, randint\n\ndef rtext(maxlength, minlength=1, choices=string.ascii_letters):\n    return ''.join(choice(choices) for x in range(randint(minlength, maxlength)))"
    },
    {
        "original": "def _get_attributes_from_managed_object(self, managed_object, attr_names): \n        attr_factory = attribute_factory.AttributeFactory()\n        retrieved_attributes = list()\n\n        if not attr_names:\n            attr_names = self._attribute_policy.get_all_attribute_names()\n\n        for attribute_name in attr_names:\n            object_type = managed_object._object_type\n\n            if not self._attribute_policy.is_attribute_supported(\n                    attribute_name\n            ):\n        ",
        "rewrite": "def _get_attributes_from_managed_object(self, managed_object, attr_names):\n    attr_factory = attribute_factory.AttributeFactory()\n    retrieved_attributes = []\n\n    if not attr_names:\n        attr_names = self._attribute_policy.get_all_attribute_names()\n\n    for attribute_name in attr_names:\n        object_type = managed_object._object_type\n\n        if not self._attribute_policy.is_attribute_supported(attribute_name):"
    },
    {
        "original": "def _update_menus(self,change): \n        menus = {}\n        \n        #: Get all links\n        links = [p.link for p in self.pages if p.link] + self.links \n        \n        #: Put all links in the correct menu\n        for link in links:\n            for menu in link.menus:\n                if menu not in menus:\n        ",
        "rewrite": "def _update_menus(self, change):\n    menus = {}\n    \n    # Get all links\n    links = [p.link for p in self.pages if p.link] + self.links \n    \n    # Put all links in the correct menu\n    for link in links:\n        for menu in link.menus:\n            if menu not in menus:"
    },
    {
        "original": "def set_challenge_for_url(url, challenge): \n    if not url:\n        raise ValueError('URL cannot be empty')\n\n    if not challenge:\n        raise ValueError('Challenge cannot be empty')\n\n    src_url = parse.urlparse(url)\n    if src_url.netloc != challenge.source_authority:\n        raise ValueError('Source URL and Challenge URL do not match')\n\n    _lock.acquire()\n\n    _cache[src_url.netloc] = challenge\n\n    _lock.release()",
        "rewrite": "def set_challenge_for_url(url, challenge): \n    if not url:\n        raise ValueError('URL cannot be empty')\n        \n    if not challenge:\n        raise ValueError('Challenge cannot be empty')\n\n    src_url = parse.urlparse(url)\n    if src_url.netloc != challenge.source_authority:\n        raise ValueError('Source URL and Challenge URL do not match')\n\n    _lock.acquire()\n\n    _cache[src_url.netloc] = challenge\n\n    _lock.release()"
    },
    {
        "original": "def ensemble_diffs(num, N): \n    diffs = np.empty(num)\n    for i in xrange(num):\n        mat = GOE(N)\n        diffs[i] = center_eigenvalue_diff(mat)\n    return diffs",
        "rewrite": "def ensemble_diffs(num, N):\n    diffs = np.empty(num)\n    for i in range(num):\n        mat = GOE(N)\n        diffs[i] = center_eigenvalue_diff(mat)\n    return diffs"
    },
    {
        "original": "def parse_peddy_ped(lines): \n    peddy_ped = []\n    header = []\n    for i,line in enumerate(lines):\n        line = line.rstrip()\n        if i == 0:\n            # Header line\n            header = line.lstrip('#').split('\\t')\n        else:\n            ind_info = dict(zip(header, line.split('\\t')))\n            \n            # PC1/PC2/PC3/PC4: the first 4 values after this sample was \n",
        "rewrite": "def parse_peddy_ped(lines): \n    peddy_ped = []\n    header = []\n    for i, line in enumerate(lines):\n        line = line.rstrip()\n        if i == 0:\n            header = line.lstrip('#').split('\\t')\n        else:\n            ind_info = dict(zip(header, line.split('\\t')))"
    },
    {
        "original": "def create_endpoint(port=None, service_name=None, host=None, use_defaults=True): \n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            try:\n                host = socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n             ",
        "rewrite": "host = None"
    },
    {
        "original": "def SwitchLogic(cases, default=None): \n    if default is not None:\n        assigTop = default\n    else:\n        assigTop = []\n\n    for cond, statements in reversed(cases):\n        assigTop = If(cond,\n                      statements\n                      ).Else(\n            assigTop\n        )\n\n    return assigTop",
        "rewrite": "def SwitchLogic(cases, default=None):\n    if default is not None:\n        assigTop = default\n    else:\n        assigTop = []\n\n    for cond, statements in reversed(cases):\n        assigTop = If(cond,\n                      statements\n                      ).Else(\n            assigTop\n        )\n\n    return assigTop"
    },
    {
        "original": "def get_output_shape(self): \n        output = callBigDlFunc(self.bigdl_type, \"getOutputShape\",\n                               self.value)\n        return self.__process_shape(output)",
        "rewrite": "def get_output_shape(self):\n    output = callBigDlFunc(self.bigdl_type, \"getOutputShape\", self.value)\n    return self.__process_shape(output)"
    },
    {
        "original": "def refresh_from_db(self, session=None): \n        DR = DagRun\n\n        exec_date = func.cast(self.execution_date, DateTime)\n\n        dr = session.query(DR).filter(\n            DR.dag_id == self.dag_id,\n            func.cast(DR.execution_date, DateTime) == exec_date,\n            DR.run_id == self.run_id\n        ).one()\n\n        self.id = dr.id\n        self.state = dr.state",
        "rewrite": "def refresh_from_db(self, session=None): \n        DR = DagRun\n        exec_date = func.cast(self.execution_date, DateTime)\n        dr = session.query(DR).filter(\n            DR.dag_id == self.dag_id,\n            func.cast(DR.execution_date, DateTime) == exec_date,\n            DR.run_id == self.run_id\n        ).one()\n        self.id = dr.id\n        self.state = dr.state"
    },
    {
        "original": "def _select_next_server(self): \n\n        while True:\n            if len(self._server_pool) == 0:\n                self._current_server = None\n                raise ErrNoServers\n\n            now = time.monotonic()\n            s = self._server_pool.pop(0)\n            if self.options[\"max_reconnect_attempts\"] > 0:\n                if s.reconnects > self.options[\"max_reconnect_attempts\"]:\n    ",
        "rewrite": "def _select_next_server(self): \n\n    while True:\n        if len(self._server_pool) == 0:\n            self._current_server = None\n            raise ErrNoServers\n\n        now = time.monotonic()\n        s = self._server_pool.pop(0)\n        if self.options[\"max_reconnect_attempts\"] > 0:\n            if s.reconnects > self.options[\"max_reconnect_attempts\"]:"
    },
    {
        "original": "def submit(self, block_list): \n        # io_submit ioctl will only return an error for issues with the first\n        # transfer block. If there are issues with a later block, it will stop\n        # submission and return the number of submitted blocks. So it is safe\n        # to only update self._submitted once io_submit returned.\n        submitted_count = libaio.io_submit(\n            self._ctx,\n            len(block_list),\n            (libaio.iocb_p * len(block_list))(*[\n",
        "rewrite": "def submit(self, block_list):\n    submitted_count = libaio.io_submit(\n        self._ctx,\n        len(block_list),\n        (libaio.iocb_p * len(block_list))(*block_list)\n    )"
    },
    {
        "original": "def blank_tiles(input_word): \n\n    blanks = 0\n    questions = 0\n    input_letters = []\n    for letter in input_word:\n        if letter == \"_\":\n            blanks += 1\n        elif letter == \"?\":\n            questions += 1\n        else:\n            input_letters.append(letter)\n    return input_letters, blanks, questions",
        "rewrite": "def blank_tiles(input_word):\n    blanks = 0\n    questions = 0\n    input_letters = []\n    for letter in input_word:\n        if letter == \"_\":\n            blanks += 1\n        elif letter == \"?\":\n            questions += 1\n        else:\n            input_letters.append(letter)\n    return input_letters, blanks, questions"
    },
    {
        "original": " \n    out_names = []\n    for stm in statements:\n        for sig in stm._outputs:\n            if not sig.hasGenericName:\n                out_names.append(sig.name)\n\n    if out_names:\n        return min(out_names)\n    else:\n        return \"\"",
        "rewrite": "out_names = []\nfor stm in statements:\n    for sig in stm._outputs:\n        if not sig.hasGenericName:\n            out_names.append(sig.name)\n\nif out_names:\n    return min(out_names)\nelse:\n    return \"\""
    },
    {
        "original": "def execute(self, args): \n        if args.name is not None:\n            self.print_workspace(args.name)\n        elif args.all is not None:\n            self.print_all()",
        "rewrite": "def execute(self, args): \n    if args.name:\n        self.print_workspace(args.name)\n    elif args.all:\n        self.print_all()"
    },
    {
        "original": " \n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    c = self._stream.peek_char\n    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):\n        self._stream.incpos()\n        while not self.read_eof():\n            c = self._stream.peek_char\n            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):\n                break\n            self._stream.incpos()\n       ",
        "rewrite": "if self.read_eof():\n    return False\nself._stream.save_context()\nc = self._stream.peek_char()\nif c.isdigit() or ('a' <= c.lower() <= 'f'):\n    self._stream.incpos()\n    while not self.read_eof():\n        c = self._stream.peek_char()\n        if not (c.isdigit() or ('a' <= c.lower() <= 'f')):\n            break\n        self._stream.incpos()"
    },
    {
        "original": "def make_code_from_py(filename): \n    # Open the source file.\n    try:\n        source_file = open_source(filename)\n    except IOError:\n        raise NoSource(\"No file to run: %r\" % filename)\n\n    try:\n        source = source_file.read()\n    finally:\n        source_file.close()\n\n    # We have the source.  `compile` still needs the last line to be clean,\n    # so make sure it is, then compile a code object from it.\n    if not source or source[-1] != '\\n':\n        source += '\\n'\n   ",
        "rewrite": "def make_code_from_py(filename):\n    try:\n        source_file = open(filename)\n    except IOError:\n        raise NoSource(\"No file to run: %r\" % filename)\n\n    try:\n        source = source_file.read()\n    finally:\n        source_file.close()\n\n    if not source or source[-1] != '\\n':\n        source += '\\n'"
    },
    {
        "original": "def add(buffer, entropy): \n    if not isinstance(buffer, bytes):\n        raise TypeError(\"buffer must be a byte string\")\n\n    if not isinstance(entropy, int):\n        raise TypeError(\"entropy must be an integer\")\n\n    _lib.RAND_add(buffer, len(buffer), entropy)",
        "rewrite": "def add(buffer: bytes, entropy: int): \n    if not isinstance(buffer, bytes):\n        raise TypeError(\"buffer must be a byte string\")\n\n    if not isinstance(entropy, int):\n        raise TypeError(\"entropy must be an integer\")\n\n    _lib.RAND_add(buffer, len(buffer), entropy)"
    },
    {
        "original": "def pack_factorisation(facto_list): \n    _sum = []\n    for f in facto_list:\n        if isinstance(f, Script):\n            _sum.append(f)\n        else:\n            # tuple of factorisation\n            _sum.append(MultiplicativeScript(children=(pack_factorisation(l_f) for l_f in f)))\n\n    if len(_sum) == 1:\n        return _sum[0]\n    else:\n        return AdditiveScript(children=_sum)",
        "rewrite": "def pack_factorisation(facto_list): \n    _sum = []\n    for f in facto_list:\n        if isinstance(f, Script):\n            _sum.append(f)\n        else:\n            _sum.append(MultiplicativeScript(children=(pack_factorisation(l_f) for l_f in f))\n\n    if len(_sum) == 1:\n        return _sum[0]\n    else:\n        return AdditiveScript(children=_sum)"
    },
    {
        "original": "def get_date_less_query(days, date_field): \n    query = None\n    days = get_integer(days)\n    if days:\n        future = get_days_from_now(days)\n        query = Q(**{\"%s__lte\" % date_field: future.isoformat()})\n    return query",
        "rewrite": "def get_date_less_query(days, date_field):\n    query = None\n    days = int(days)\n    if days:\n        future = get_days_from_now(days)\n        query = Q(**{\"%s__lte\" % date_field: future.isoformat()})\n    return query"
    },
    {
        "original": "def limited_join(sep, items, max_chars=30, overflow_marker=\"...\"): \n    full_str = sep.join(items)\n    if len(full_str) < max_chars:\n        return full_str\n\n    n_chars = 0\n    n_items = 0\n    for j, item in enumerate(items):\n        n_chars += len(item) + len(sep)\n        if n_chars < max_chars - len(overflow_marker):\n            n_items += 1\n        else:\n            break\n\n    return sep.join(list(items[:n_items]) + [overflow_marker])",
        "rewrite": "def limited_join(sep, items, max_chars=30, overflow_marker=\"...\"):\n    full_str = sep.join(items)\n    if len(full_str) < max_chars:\n        return full_str\n    \n    n_chars = 0\n    n_items = 0\n    for j, item in enumerate(items):\n        n_chars += len(item) + len(sep)\n        if n_chars < max_chars - len(overflow_marker):\n            n_items += 1\n        else:\n            break\n    \n    return sep.join(list(items[:n_items]) + [overflow_marker])"
    },
    {
        "original": "def match_shortname(self, name, filled_args=None): \n\n        filled_count = 0\n        if filled_args is not None:\n            filled_count = len(filled_args)\n\n        possible = [x for x in self.arg_names[filled_count:] if x.startswith(name)]\n        if len(possible) == 0:\n            raise ArgumentError(\"Could not convert short-name full parameter name, none could be found\", short_name=name, parameters=self.arg_names)\n        elif len(possible) > 1:\n            raise ArgumentError(\"Short-name is ambiguous, could match multiple keyword parameters\", short_name=name, possible_matches=possible)\n\n  ",
        "rewrite": "def match_shortname(self, name, filled_args=None):\n    filled_count = 0\n    if filled_args is not None:\n        filled_count = len(filled_args)\n    \n    possible = [x for x in self.arg_names[filled_count:] if x.startswith(name)]\n    \n    if len(possible) == 0:\n        raise ArgumentError(\"Could not convert short-name full parameter name, none could be found\", short_name=name, parameters=self.arg_names)\n    \n    elif len(possible) > 1:\n        raise ArgumentError(\"Short-name is ambiguous, could match multiple keyword parameters\", short_name=name, possible_matches=possible)"
    },
    {
        "original": " \n    if ctx.is_in_anon_fn:\n        raise SyntaxError(f\"Nested #() definitions not allowed\")\n\n    with ctx.in_anon_fn():\n        form = _read_list(ctx)\n    arg_set = set()\n\n    def arg_suffix(arg_num):\n        if arg_num is None:\n            return \"1\"\n        elif arg_num == \"&\":\n            return \"rest\"\n        else:\n            return arg_num\n\n    def sym_replacement(arg_num):\n        suffix",
        "rewrite": "def sym_replacement(arg_num):\n        suffix = arg_suffix(arg_num)\n        return f\"${suffix}\""
    },
    {
        "original": "def lessThan(self, left, right): \n        sourceModel = self.sourceModel()\n        if sourceModel:\n            leftItem = sourceModel.item(left)\n            rightItem = sourceModel.item(right)\n\n            if (isinstance(leftItem, Directory)\n                and not isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.AscendingOrder\n\n            elif (not isinstance(leftItem, Directory)\n         ",
        "rewrite": "def lessThan(self, left, right): \n    sourceModel = self.sourceModel()\n    if sourceModel:\n        leftItem = sourceModel.item(left)\n        rightItem = sourceModel.item(right)\n\n        if (isinstance(leftItem, Directory) and not isinstance(rightItem, Directory)):\n            return self.sortOrder() == Qt.AscendingOrder\n\n        elif (not isinstance(leftItem, Directory):\n            return True"
    },
    {
        "original": "def _graph_wrap(func, graph): \n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped",
        "rewrite": "def _graph_wrap(func, graph):\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped"
    },
    {
        "original": "def _is_owner_ignored(owner, name, ignored_classes, ignored_modules): \n    ignored_modules = set(ignored_modules)\n    module_name = owner.root().name\n    module_qname = owner.root().qname()\n    if any(\n        module_name in ignored_modules\n        or module_qname in ignored_modules\n        or fnmatch.fnmatch(module_qname, ignore)\n        for ignore in ignored_modules\n    ):\n        return True\n\n    ignored_classes = set(ignored_classes)\n    if hasattr(owner, \"qname\"):\n        qname = owner.qname()\n    else:\n        qname = \"\"\n    return any(ignore in (name, qname)",
        "rewrite": "def _is_owner_ignored(owner, name, ignored_classes, ignored_modules):\n    ignored_modules = set(ignored_modules)\n    module_name = owner.root().name\n    module_qname = owner.root().qname()\n    if any(\n        module_name in ignored_modules\n        or module_qname in ignored_modules\n        or fnmatch.fnmatch(module_qname, ignore)\n        for ignore in ignored_modules\n    ):\n        return True\n\n    ignored_classes = set(ignored_classes)\n    qname = owner.qname() if hasattr(owner, \"qname\") else \"\"\n    return any(ignore in (name, qname) for ignore in ignored_classes)"
    },
    {
        "original": "def svm_score(self, x): \n\n        score = np.sum(self.svm_processor.sv_alpha * self.svm_processor.sv_Y * utility.Kernel.kernel_matrix_xX(self, x, self.svm_processor.sv_X)) + self.svm_processor.sv_avg_b\n\n        return score",
        "rewrite": "def svm_score(self, x):\n    score = np.sum(self.svm_processor.sv_alpha * self.svm_processor.sv_Y * utility.Kernel.kernel_matrix_xX(self.svm_processor.sv_X, x)) + self.svm_processor.sv_avg_b\n    return score"
    },
    {
        "original": "def load_from_path(path): \n\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)",
        "rewrite": "def load_from_path(path):\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)"
    },
    {
        "original": "def get_width(self, c, default=0, match_only=None): \n        return self.getattr(c=c,\n                            attr='width',\n                            default=default,\n                            match_only=match_only)",
        "rewrite": "def get_width(self, c, default=0, match_only=None):\n    return self.getattr(c=c, attr='width', default=default, match_only=match_only)"
    },
    {
        "original": "def glance(*arg): \n    check_event_type(Openstack.Glance, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            glance_customer_process_wildcard[event_type_pattern] = func\n        else:\n            glance_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n   ",
        "rewrite": "import functools\n\ndef glance(*arg): \n    check_event_type(Openstack.Glance, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if \"*\" in event_type:\n            event_type_pattern = pre_compile(event_type)\n            glance_customer_process_wildcard[event_type_pattern] = func\n        else:\n            glance_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)"
    },
    {
        "original": "def is_running(self): \n        try:\n            if h2o.connection().local_server and not h2o.connection().local_server.is_running(): return False\n            h2o.api(\"GET /\")\n            return True\n        except (H2OConnectionError, H2OServerError):\n            return False",
        "rewrite": "def is_running(self): \n        try:\n            if h2o.connection().local_server and not h2o.connection().local_server.is_running(): \n                return False\n            h2o.api(\"GET /\")\n            return True\n        except (H2OConnectionError, H2OServerError):\n            return False"
    },
    {
        "original": "def _srvc_extract_file_information(self, kwargs): \n        if 'filename' in kwargs:\n            self._filename = kwargs.pop('filename')\n\n        if 'file_title' in kwargs:\n            self._file_title = kwargs.pop('file_title')\n\n        if 'trajectory_name' in kwargs:\n            self._trajectory_name = kwargs.pop('trajectory_name')\n\n        if 'trajectory_index' in kwargs:\n            self._trajectory_index = kwargs.pop('trajectory_index')",
        "rewrite": "def _srvc_extract_file_information(self, kwargs): \n    self._filename = kwargs.pop('filename', None)\n    self._file_title = kwargs.pop('file_title', None)\n    self._trajectory_name = kwargs.pop('trajectory_name', None)\n    self._trajectory_index = kwargs.pop('trajectory_index', None)"
    },
    {
        "original": "def message_user(user, message, level=constants.INFO): \n    # We store a list of messages in the cache so we can have multiple messages\n    # queued up for a user.\n    user_key = _user_key(user)\n    messages = cache.get(user_key) or []\n    messages.append((message, level))\n    cache.set(user_key, messages)",
        "rewrite": "def message_user(user, message, level=constants.INFO):\n    user_key = _user_key(user)\n    messages = cache.get(user_key) or []\n    messages.append((message, level))\n    cache.set(user_key, messages)"
    },
    {
        "original": "def single_request_timeout(self, value): \n        check_type(value, int)\n        assert value is None or value > 0\n        self._single_request_timeout = value",
        "rewrite": "def single_request_timeout(self, value):\n    check_type(value, int)\n    assert value is None or value > 0\n    self._single_request_timeout = value"
    },
    {
        "original": "def addCases(self, tupesValStmnts): \n        s = self\n        for val, statements in tupesValStmnts:\n            s = s.Case(val, statements)\n        return s",
        "rewrite": "def addCases(self, tupesValStmnts): \n    s = self\n    for val, statements in tupesValStmnts:\n        s = s.Case(val, statements)\n    return s"
    },
    {
        "original": "def decorator(directname=None): \n    global _decorators\n    class_deco_list = _decorators\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_deco_list, directname, f)\n\n    return wrapper",
        "rewrite": "def decorator(directname=None): \n    global _decorators\n    class_deco_list = _decorators\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_deco_list, directname, f)\n\n    return wrapper"
    },
    {
        "original": "def _directory_model_from_db(self, record, content): \n        model = base_directory_model(to_api_path(record['name']))\n        if content:\n            model['format'] = 'json'\n            model['content'] = list(\n                chain(\n                    self._convert_file_records(record['files']),\n                    (\n                    ",
        "rewrite": "def _directory_model_from_db(self, record, content): \n        model = base_directory_model(to_api_path(record['name']))\n        if content:\n            model['format'] = 'json'\n            model['content'] = list(\n                chain(\n                    self._convert_file_records(record['files']),\n                    (\n                    \"' . No need to explain. Just write code:\"))"
    },
    {
        "original": "def share_vm_image(self, vm_image_name, permission): \n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )",
        "rewrite": "def share_vm_image(self, vm_image_name, permission):\n    _validate_not_none('vm_image_name', vm_image_name)\n    _validate_not_none('permission', permission)\n\n    path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n    query = f'&permission={permission}'\n    path = f'{path}?{query.lstrip(\"&\")}'\n\n    return self._perform_put(\n        path, None, as_async=True, x_ms_version='2015-04-01'\n    )"
    },
    {
        "original": " \n    if radius_limit is None:\n        radius_limit = 2 * radius\n    # TODO: add limit so bend only applies over y<2*radius; add option to set\n    # larger limit\n    angle = math.radians(angle)\n    segment = radius * angle",
        "rewrite": "if radius_limit is None:\n    radius_limit = 2 * radius\nangle = math.radians(angle)\nsegment = radius * angle"
    },
    {
        "original": "def post(self, request, *args, **kwargs): \n        serializer = self.serializer_class(data=request.data)\n\n        if not serializer.is_valid():\n            return response.Response(\n                serializer.errors,\n                status=status.HTTP_400_BAD_REQUEST,\n            )\n\n        serializer.user.send_validation_email()\n        msg = _('Email confirmation sent.')\n        return response.Response(msg, status=status.HTTP_204_NO_CONTENT)",
        "rewrite": "def post(self, request, *args, **kwargs): \n    serializer = self.serializer_class(data=request.data)\n\n    if not serializer.is_valid():\n        return response.Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n\n    serializer.user.send_validation_email()\n    msg = _('Email confirmation sent.')\n    return response.Response(msg, status=status.HTTP_204_NO_CONTENT)"
    },
    {
        "original": " \n\n        hydrated_options = copy(self._options)\n\n        if query_options:\n            if (\n                query_options.score_threshold != hydrated_options.score_threshold \n                and query_options.score_threshold\n            ):\n                hydrated_options.score_threshold = query_options.score_threshold\n            \n            if (query_options.top != hydrated_options.top and",
        "rewrite": "hydrated_options = copy(self._options)\n\nif query_options:\n    if (query_options.score_threshold != hydrated_options.score_threshold and query_options.score_threshold):\n        hydrated_options.score_threshold = query_options.score_threshold\n        \n    if (query_options.top != hydrated_options.top): # Removed errorneous part of code\n        hydrated_options.top = query_options.top"
    },
    {
        "original": "def updateObservers(self): \n        for observer in self.m_observers:\n            try:\n                observer.update(self.m_req)\n            except:\n                ekm_log(traceback.format_exc(sys.exc_info()))",
        "rewrite": "def updateObservers(self): \n    for observer in self.m_observers:\n        try:\n            observer.update(self.m_req)\n        except Exception as e:\n            ekm_log(traceback.format_exc(e))"
    },
    {
        "original": "def resized_crop(img, i, j, h, w, size, interpolation=Image.BILINEAR): \n    assert _is_pil_image(img), 'img should be PIL Image'\n    img = crop(img, i, j, h, w)\n    img = resize(img, size, interpolation)\n    return img",
        "rewrite": "def resized_crop(img, i, j, h, w, size, interpolation=Image.BILINEAR): \n    assert _is_pil_image(img), 'img should be PIL Image'\n    img = img.crop((i, j, i + h, j + w))\n    img = img.resize(size, interpolation)\n    return img"
    },
    {
        "original": "def define_spark_config(): \n\n    master_url = Field(\n        String,\n        description='The master URL for the cluster (e.g. spark://23.195.26.187:7077)',\n        is_optional=False,\n    )\n\n    deploy_mode = Field(\n        SparkDeployMode,\n        description=",
        "rewrite": "def define_spark_config(): \n\n    master_url = Field(\n        data_type=String,\n        description='The master URL for the cluster (e.g. spark://23.195.26.187:7077)',\n        is_optional=False,\n    )\n\n    deploy_mode = Field(\n        data_type=SparkDeployMode,\n        description=\"No need to explain. Just write code: it\",\n    )"
    },
    {
        "original": "def _get_readable_id(id_name, id_prefix_to_skip): \n    # id_name is in the form 'https://namespace.host.suffix/name'\n    # where name may contain a forward slash!\n    pos = id_name.find('//')\n    if pos != -1:\n        pos += 2\n        if id_prefix_to_skip:\n            pos = id_name.find(id_prefix_to_skip, pos)\n            if pos != -1:\n                pos += len(id_prefix_to_skip)\n        pos = id_name.find('/', pos)\n        if pos != -1:\n ",
        "rewrite": "def _get_readable_id(id_name, id_prefix_to_skip):\n    pos = id_name.find('//')\n    if pos != -1:\n        pos += 2\n        if id_prefix_to_skip:\n            pos = id_name.find(id_prefix_to_skip, pos)\n            if pos != -1:\n                pos += len(id_prefix_to_skip)\n        pos = id_name.find('/', pos)\n        if pos != -1:"
    },
    {
        "original": "def get_conn(self): \n        if not self._conn:\n            self._conn = storage.Client(credentials=self._get_credentials())\n\n        return self._conn",
        "rewrite": "def get_conn(self): \n    if not self._conn:\n        self._conn = storage.Client(credentials=self._get_credentials())\n\n    return self._conn"
    },
    {
        "original": " \n        for attrib in self.linked_file_descriptors[:]:\n            if file_path is not None and attrib['LINK_URL'] != file_path:\n                continue\n            if relpath is not None and attrib['RELATIVE_LINK_URL'] != relpath:\n                continue\n            if mimetype is not None and attrib['MIME_TYPE'] != mimetype:\n                continue\n       ",
        "rewrite": "for attrib in self.linked_file_descriptors[:]:\n    if file_path is not None and attrib['LINK_URL'] != file_path:\n        continue\n    if relpath is not None and attrib['RELATIVE_LINK_URL'] != relpath:\n        continue\n    if mimetype is not None and attrib['MIME_TYPE'] != mimetype:\n        continue"
    },
    {
        "original": "def _parse_retry_after(self, response): \n        value = response.headers.get('Retry-After')\n\n        if not value:\n            seconds = 0\n        elif re.match(r'^\\s*[0-9]+\\s*$', value):\n            seconds = int(value)\n        else:\n            date_tuple = email.utils.parsedate(value)\n            if date_tuple is None:\n                seconds = 0\n           ",
        "rewrite": "def _parse_retry_after(self, response): \n    value = response.headers.get('Retry-After')\n\n    if not value:\n        seconds = 0\n    elif re.match(r'^\\s*[0-9]+\\s*$', value):\n        seconds = int(value)\n    else:\n        date_tuple = email.utils.parsedate(value)\n        if date_tuple is None:\n            seconds = 0"
    },
    {
        "original": " \n\n    # Only remove bold or italics if this tag is an h tag.\n    # Td elements have the same look and feel as p/h elements. Right now we are\n    # never putting h tags in td elements, as such if we are in a td we will\n    # never be stripping bold/italics since that is only done on h tags\n    if not is_td and is_header(p, meta_data):\n        # Check to see if the whole line is bold or italics.\n        remove_bold, remove_italics = whole_line_styled(p)\n\n    p_text = ''\n    w_namespace = get_namespace(p, 'w')\n  ",
        "rewrite": "if not is_td and is_header(p, meta_data):\n    remove_bold, remove_italics = whole_line_styled(p)\n\np_text = ''\nw_namespace = get_namespace(p, 'w')"
    },
    {
        "original": "def makePlot(args): \n  gmag=np.linspace(3.0,20.0,171)\n\n  vmini = args['vmini']\n  \n  vmag=gmag-gminvFromVmini(vmini)\n  \n  if args['eom']:\n      sigmaG = gMagnitudeErrorEoM(gmag)\n      sigmaGBp = bpMagnitudeErrorEoM(gmag, vmini)\n      sigmaGRp = rpMagnitudeErrorEoM(gmag, vmini)\n      yminmax = (1.0-4,0.1)\n  else:\n      sigmaG = gMagnitudeError(gmag)\n      sigmaGBp = bpMagnitudeError(gmag, vmini)\n      sigmaGRp = rpMagnitudeError(gmag, vmini)\n      yminmax = (1.0-4,1)\n\n  fig=plt.figure(figsize=(10,6.5))\n  \n  if (args['vmagAbscissa']):\n    plt.semilogy(vmag, sigmaG, 'k', label='$\\\\sigma_G$')\n    plt.semilogy(vmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.semilogy(vmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$'+' for $(V-I)={0}$'.format(vmini))\n    plt.xlim((6,20))\n    #plt.ylim(yminmax)\n",
        "rewrite": "def makePlot(args): \n    gmag = np.linspace(3.0, 20.0, 171)\n\n    vmini = args['vmini']\n  \n    vmag = gmag - gminvFromVmini(vmini)\n  \n    if args['eom']:\n        sigmaG = gMagnitudeErrorEoM(gmag)\n        sigmaGBp = bpMagnitudeErrorEoM(gmag, vmini)\n        sigmaGRp = rpMagnitudeErrorEoM(gmag, vmini)\n        yminmax = (1.0 - 4, 0.1)\n    else:\n        sigmaG = gMagnitudeError(gmag)\n        sigmaGBp = bpMagnitudeError(gmag, vmini)\n        sigmaGRp = rpMagnitudeError(gmag, vmini)\n        yminmax = (1.0 - 4, 1)\n\n    fig = plt.figure(figsize=(10, 6.5)\n  \n    if (args['vmagAbscissa']):\n        plt.semilogy(vmag, sigmaG, 'k', label='$\\\\sigma_G$')\n        plt.semilogy(vmag, sigmaGBp, 'b', label='$\\\\sigma_{G_\\\\mathrm{BP}}$' + ' for $(V-I)={0}$'.format(vmini))\n        plt.semilogy(vmag, sigmaGRp, 'r', label='$\\\\sigma_{G_\\\\mathrm{RP}}$' + ' for $(V-I)={0}$'.format(vmini))\n        plt.xlim((6, 20))\n        #plt.ylim(yminmax)"
    },
    {
        "original": "def buildcontent(self): \n        self.buildcontainer()\n        # if the subclass has a method buildjs this method will be\n        # called instead of the method defined here\n        # when this subclass method is entered it does call\n        # the method buildjschart defined here\n        self.buildjschart()\n        self.htmlcontent = self.template_chart_nvd3.render(chart=self)",
        "rewrite": "def buildcontent(self): \n        self.buildcontainer()\n        self.buildjschart()\n        self.htmlcontent = self.template_chart_nvd3.render(chart=self)"
    },
    {
        "original": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs): \n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
        "rewrite": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)"
    },
    {
        "original": "def get_arguments(self): \n        ApiCli.get_arguments(self)\n        if self.args.hostGroupId is not None:\n            self.hostGroupId = self.args.hostGroupId\n        if self.args.force is not None:\n            self.force = self.args.force\n\n        if self.force:\n            self.url_parameters = {\"forceRemove\": True}\n\n        self.path = \"v1/hostgroup/{0}\".format(str(self.hostGroupId))",
        "rewrite": "def get_arguments(self): \n    ApiCli.get_arguments(self)\n    if self.args.hostGroupId is not None:\n        self.hostGroupId = self.args.hostGroupId\n    if self.args.force is not None:\n        self.force = self.args.force\n\n    if self.force:\n        self.url_parameters = {\"forceRemove\": True}\n\n    self.path = \"v1/hostgroup/{0}\".format(str(self.hostGroupId))"
    },
    {
        "original": "def _marginal_hidden_probs(self): \n\n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]],\n                            ",
        "rewrite": "def _marginal_hidden_probs(self): \n    \n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]], 0))"
    },
    {
        "original": "def initiate(self, request): \n        url = MgmtRequests.mgmtRequests\n        r = self._apiClient.post(url, request)\n\n        if r.status_code == 202:\n            return r.json()\n        else:\n            raise ApiException(r)",
        "rewrite": "def initiate(self, request):\n    url = MgmtRequests.mgmtRequests\n    r = self._apiClient.post(url, request)\n\n    if r.status_code == 202:\n        return r.json()\n    else:\n        raise ApiException(r)"
    },
    {
        "original": "def compute_features(self): \n        S = librosa.feature.melspectrogram(self._audio,\n                                           sr=self.sr,\n                                           n_fft=self.n_fft,\n                             ",
        "rewrite": "def compute_features(self): \n        S = librosa.feature.melspectrogram(self._audio, sr=self.sr, n_fft=self.n_fft)"
    },
    {
        "original": "def validate_reject(form, field): \n        if field.data and form.accept.data:\n            raise validators.ValidationError(\n                _(\"Both reject and accept cannot be set at the same time.\")\n            )",
        "rewrite": "def validate_reject(form, field): \n    if field.data and form.accept.data:\n        raise validators.ValidationError(_(\"Both reject and accept cannot be set at the same time.\"))"
    },
    {
        "original": "def _execute(self, request): \n    request['command'] = self.command\n    return self.adapter.send_request(request)",
        "rewrite": "def _execute(self, request):\n    request['command'] = self.command\n    return self.adapter.send_request(request)"
    },
    {
        "original": "def _sort_versions(self, applicable_versions): \n        return sorted(\n            applicable_versions,\n            key=self._candidate_sort_key,\n            reverse=True\n        )",
        "rewrite": "def _sort_versions(self, applicable_versions): \n        return sorted(applicable_versions, key=self._candidate_sort_key, reverse=True)"
    },
    {
        "original": "def queue_email(to_addresses, from_address, subject, body, commit=True, html=True, session=None): \n    from models import QueuedEmail\n\n    if session is None:\n        session = _db.session\n\n    log.info('Queuing mail to %s: %s' % (to_addresses, subject))\n    queued_email = QueuedEmail(html, to_addresses, from_address, subject, body, STATUS_QUEUED)\n    session.add(queued_email)\n    session.commit()\n\n    return queued_email",
        "rewrite": "def queue_email(to_addresses, from_address, subject, body, commit=True, html=True, session=None): \n    from models import QueuedEmail\n    \n    if session is None:\n        session = _db.session\n\n    log.info('Queuing mail to {}: {}'.format(to_addresses, subject))\n    queued_email = QueuedEmail(html=html, to_addresses=to_addresses, from_address=from_address, subject=subject, body=body, status=STATUS_QUEUED)\n    session.add(queued_email)\n    \n    if commit:\n        session.commit()\n\n    return queued_email"
    },
    {
        "original": "def create_endpoint(port=0, service_name='unknown', ipv4=None, ipv6=None): \n    ipv4_int = 0\n    ipv6_binary = None\n\n    # Convert ip address to network byte order\n    if ipv4:\n        ipv4_int = struct.unpack('!i', socket.inet_pton(socket.AF_INET, ipv4))[0]\n\n    if ipv6:\n        ipv6_binary = socket.inet_pton(socket.AF_INET6, ipv6)\n\n    # Zipkin passes unsigned values in signed types because Thrift has no\n    # unsigned types, so we have to convert the value.\n    port = struct.unpack('h', struct.pack('H', port))[0]\n    return zipkin_core.Endpoint(\n        ipv4=ipv4_int,\n        ipv6=ipv6_binary,\n        port=port,\n ",
        "rewrite": "def create_endpoint(port=0, service_name='unknown', ipv4=None, ipv6=None): \n    ipv4_int = 0\n    ipv6_binary = None\n\n    if ipv4:\n        ipv4_int = struct.unpack('!i', socket.inet_pton(socket.AF_INET, ipv4))[0]\n\n    if ipv6:\n        ipv6_binary = socket.inet_pton(socket.AF_INET6, ipv6)\n\n    port = struct.unpack('h', struct.pack('H', port))[0]\n    return zipkin_core.Endpoint(\n        ipv4=ipv4_int,\n        ipv6=ipv6_binary,\n        port=port)"
    },
    {
        "original": "def get_organisation(self, **query_params): \n        organisation_json = self.get_organisations_json(\n            self.base_uri, query_params=query_params)\n\n        return self.create_organisation(organisation_json)",
        "rewrite": "def get_organisation(self, **query_params):\n    organisation_json = self.get_organisations_json(self.base_uri, query_params=query_params)\n    return self.create_organisation(organisation_json)"
    },
    {
        "original": "def make_list(var, num_terms=1): \n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n    #if len(var) == 1:\n            for _ in range(1, num_terms):\n                var.append(var[0])\n    return var",
        "rewrite": "def make_list(var, num_terms=1): \n    if not isinstance(var, list):\n        if isinstance(var, tuple):\n            var = list(var)\n        else:\n            var = [var]\n    for _ in range(1, num_terms):\n        var.append(var[0])\n    return var"
    },
    {
        "original": "def _plays(self, year, play_type, expand_details): \n        url = self._subpage_url('{}-plays'.format(play_type), year)\n        doc = pq(sportsref.utils.get_html(url))\n        table = doc('table#all_plays')\n        if table:\n            if expand_details:\n                plays = sportsref.nfl.pbp.expand_details(\n                    sportsref.utils.parse_table(table), detailCol='description'\n                )\n              ",
        "rewrite": "def _plays(self, year, play_type, expand_details): \n    url = self._subpage_url('{}-plays'.format(play_type), year)\n    doc = pq(sportsref.utils.get_html(url))\n    table = doc('table#all_plays')\n    if table:\n        if expand_details:\n            plays = sportsref.nfl.pbp.expand_details(\n                sportsref.utils.parse_table(table), detailCol='description'\n            )"
    },
    {
        "original": "def deconstruct(self): \n\n        name, path, args, kwargs = super(\n            HStoreField, self).deconstruct()\n\n        if self.uniqueness is not None:\n            kwargs['uniqueness'] = self.uniqueness\n\n        if self.required is not None:\n            kwargs['required'] = self.required\n\n        return name, path, args, kwargs",
        "rewrite": "def deconstruct(self): \n\n    name, path, args, kwargs = super(\n        HStoreField, self).deconstruct()\n\n    if self.uniqueness is not None:\n        kwargs['uniqueness'] = self.uniqueness\n\n    if self.required is not None:\n        kwargs['required'] = self.required\n\n    return name, path, args, kwargs"
    },
    {
        "original": "def _is_univariate_marginal(self, index_points): \n    num_index_points = tf.compat.dimension_value(\n        index_points.shape[-(self.kernel.feature_ndims + 1)])\n    if num_index_points is None:\n      warnings.warn(\n          'Unable to detect statically whether the number of index_points is '\n          '1. As a result, defaulting to treating the marginal GP at '\n          '`index_points` as a multivariate Gaussian. This makes some methods, '\n          'like `cdf` unavailable.')\n    return num_index_points == 1",
        "rewrite": "def _is_univariate_marginal(self, index_points): \n    num_index_points = tf.compat.dimension_value(\n        index_points.shape[-(self.kernel.feature_ndims + 1)])\n    if num_index_points is None:\n        warnings.warn(\n            'Unable to detect statically whether the number of index_points is '\n            '1. As a result, defaulting to treating the marginal GP at '\n            '`index_points` as a multivariate Gaussian. This makes some methods, '\n            'like `cdf` unavailable.')\n    return num_index_points == 1"
    },
    {
        "original": "def flatten_mapping(mapping): \n\treturn {\n\t\tkey: value\n\t\tfor keys, value in mapping.items()\n\t\tfor key in always_iterable(keys)\n\t}",
        "rewrite": "def flatten_mapping(mapping):\n    return {\n        key: value\n        for keys, value in mapping.items()\n        for key in always_iterable(keys)\n    }"
    },
    {
        "original": "def run(self): \n        self.initialize_tracking_structures()\n\n        if self.live_monitoring:\n            self.logger.info(\"Starting up the operation policy file monitor.\")\n            while not self.halt_trigger.is_set():\n                time.sleep(1)\n                self.scan_policies()\n            self.logger.info(\"Stopping the operation policy file monitor.\")\n        else:\n            self.scan_policies()",
        "rewrite": "def run(self): \n    self.initialize_tracking_structures()\n\n    if self.live_monitoring:\n        self.logger.info(\"Starting up the operation policy file monitor.\")\n        while not self.halt_trigger.is_set():\n            time.sleep(1)\n            self.scan_policies()\n        self.logger.info(\"Stopping the operation policy file monitor.\")\n    else:\n        self.scan_policies()"
    },
    {
        "original": "def startLogging(console=True, filepath=None): \n    global logLevelFilterPredicate\n   \n    observers = []\n    if console:\n        observers.append( FilteringLogObserver(observer=textFileLogObserver(sys.stdout),  \n            predicates=[logLevelFilterPredicate] ))\n    \n    if filepath is not None and filepath != \"\":\n        observers.append( FilteringLogObserver(observer=textFileLogObserver(open(filepath,'a')), \n            predicates=[logLevelFilterPredicate] ))\n    globalLogBeginner.beginLoggingTo(observers)",
        "rewrite": "def startLogging(console=True, filepath=None):\n    global logLevelFilterPredicate\n    \n    observers = []\n    \n    if console:\n        observers.append(FilteringLogObserver(observer=textFileLogObserver(sys.stdout),\n                                               predicates=[logLevelFilterPredicate]))\n    \n    if filepath and filepath != \"\":\n        observers.append(FilteringLogObserver(observer=textFileLogObserver(open(filepath, 'a')),\n                                               predicates=[logLevelFilterPredicate]))\n    \n    globalLogBeginner.beginLoggingTo(observers)"
    },
    {
        "original": "def img(self, **kwargs):        \n        safe = 'src=\"%s\" ' % self.url.replace('&','&amp;').replace('<', '&lt;')\\\n            .replace('>', '&gt;').replace('\"', '&quot;').replace( \"'\", '&#39;')\n        for item in kwargs.items():\n            if not item[0] in IMGATTRS:\n                raise AttributeError('Invalid img tag attribute: %s'%item[0])\n            safe += '%s=\"%s\" '%item\n        return '<img %s/>'%safe",
        "rewrite": "def img(self, **kwargs):\n    safe = 'src=\"%s\" ' % self.url.replace('&','&amp;').replace('<', '&lt;')\\\n        .replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#39;')\n    for item in kwargs.items():\n        if not item[0] in IMGATTRS:\n            raise AttributeError('Invalid img tag attribute: %s'%item[0])\n        safe += '%s=\"%s\" '% item\n    return '<img %s/>' % safe"
    },
    {
        "original": "def infix_to_postfix(nodes, *, recurse_types=None): \n  output = []\n  operators = []\n\n  for node in nodes:\n    if isinstance(node, OperatorNode):\n      # Drain out all operators whose precedence is gte the node's...\n      cmp_operator = node.operator\n      while operators:\n        current_operator = operators[-1].operator\n        if current_operator.precedence > cmp_operator.precedence or \\\n           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:\n          output.append(operators.pop())\n        else:\n          break\n     ",
        "rewrite": "def infix_to_postfix(nodes, *, recurse_types=None):\n    output = []\n    operators = []\n\n    for node in nodes:\n        if isinstance(node, OperatorNode):\n            cmp_operator = node.operator\n            while operators:\n                current_operator = operators[-1].operator\n                if current_operator.precedence > cmp_operator.precedence or \\\n                        (current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left):\n                    output.append(operators.pop())\n                else:\n                    break"
    },
    {
        "original": "def expQsds(self, s): \n        lambda_eLambdaT = np.diag(2.0*self._exp_lt(s**2)*self.eigenvals*s) # vector length = a\n        Qsds = self.v.dot(lambda_eLambdaT.dot(self.v_inv))\n        return Qsds",
        "rewrite": "def expQsds(self, s):\n    lambda_eLambdaT = np.diag(2.0 * self._exp_lt(s ** 2) * self.eigenvals * s)  # vector length = a\n    Qsds = self.v.dot(lambda_eLambdaT.dot(self.v_inv))\n    return Qsds"
    },
    {
        "original": "def data(self, index, role): \n        if not index.isValid():\n            return None\n\n        column = index.column()\n        item = index.internalPointer()\n\n        if role == self.ITEM_ROLE:\n            return item\n\n        elif role == Qt.DisplayRole:\n\n            if column == 0:\n                return item.name\n            elif column",
        "rewrite": "== 1:\n                return item.age\n            elif column == 2:\n                return item.gender\n\n        return None\""
    },
    {
        "original": "def accept(self, message=None, expires_at=None): \n        with db.session.begin_nested():\n            if self.status != RequestStatus.PENDING:\n                raise InvalidRequestStateError(RequestStatus.PENDING)\n            self.status = RequestStatus.ACCEPTED\n        request_accepted.send(self, message=message, expires_at=expires_at)",
        "rewrite": "def accept(self, message=None, expires_at=None):\n    with db.session.begin_nested():\n        if self.status != RequestStatus.PENDING:\n            raise InvalidRequestStateError(RequestStatus.PENDING)\n        self.status = RequestStatus.ACCEPTED\n    request_accepted.send(self, message=message, expires_at=expires_at)"
    },
    {
        "original": "def _experiments_to_circuits(qobj): \n    if qobj.experiments:\n        circuits = []\n        for x in qobj.experiments:\n            quantum_registers = [QuantumRegister(i[1], name=i[0])\n                                 for i in x.header.qreg_sizes]\n            classical_registers = [ClassicalRegister(i[1], name=i[0])\n                                 ",
        "rewrite": "for i in x.header.creg_sizes]\n            circuits.append(QuantumCircuit(*quantum_registers, \n                                            *classical_registers))\n        return(circuits)\n    else:\n        return([])"
    },
    {
        "original": "def strip_metadata(report): \n        report['org_name'] = report['report_metadata']['org_name']\n        report['org_email'] = report['report_metadata']['org_email']\n        report['report_id'] = report['report_metadata']['report_id']\n        report.pop('report_metadata')\n\n        return report",
        "rewrite": "def strip_metadata(report):\n    report['org_name'] = report['report_metadata']['org_name']\n    report['org_email'] = report['report_metadata']['org_email']\n    report['report_id'] = report['report_metadata']['report_id']\n    report.pop('report_metadata')\n\n    return report"
    },
    {
        "original": "def network_io_counters(pernic=False): \n    rawdict = _psplatform.network_io_counters()\n    if not rawdict:\n        raise RuntimeError(\"couldn't find any network interface\")\n    if pernic:\n        for nic, fields in rawdict.items():\n            rawdict[nic] = _nt_net_iostat(*fields)\n        return rawdict\n    else:\n        return _nt_net_iostat(*[sum(x) for x in zip(*rawdict.values())])",
        "rewrite": "def network_io_counters(pernic=False):\n    rawdict = _psplatform.network_io_counters()\n    if not rawdict:\n        raise RuntimeError(\"couldn't find any network interface\")\n    \n    if pernic:\n        for nic, fields in rawdict.items():\n            rawdict[nic] = _nt_net_iostat(*fields)\n        return rawdict\n    else:\n        return _nt_net_iostat(*[sum(x) for x in zip(*rawdict.values())])"
    },
    {
        "original": "def pick_signed_metadata_statements_regex(self, pattern, context): \n        comp_pat = re.compile(pattern)\n        sms_dict = self.signer.metadata_statements[context]\n        res = []\n        for iss, vals in sms_dict.items():\n            if comp_pat.search(iss):\n                res.extend((iss, vals))\n        return res",
        "rewrite": "def pick_signed_metadata_statements_regex(self, pattern, context):\n    comp_pat = re.compile(pattern)\n    sms_dict = self.signer.metadata_statements[context]\n    res = []\n    for iss, vals in sms_dict.items():\n        if comp_pat.search(iss):\n            res.append((iss, vals))\n    return res"
    },
    {
        "original": "def _flatten_dictionary(obj, path, key_name): \n    result = []\n    if \">\" in key_name:\n        key_name, group_key = key_name.split(\">\")\n    else:\n        group_key = None\n\n    for k, v in obj.items():\n        if path:\n            if k == path[0]:\n                path.pop(0)\n        if k.startswith(\"#\"):\n            continue\n        r = _resolve_path(v, list(path))\n\n     ",
        "rewrite": "def _flatten_dictionary(obj, path, key_name):\n    result = []\n    if \">\" in key_name:\n        key_name, group_key = key_name.split(\">\")\n    else:\n        group_key = None\n\n    for k, v in obj.items():\n        if path:\n            if k == path[0]:\n                path.pop(0)\n        if k.startswith(\"#\"):\n            continue\n        r = _resolve_path(v, list(path))"
    },
    {
        "original": "def get_uploader(data_session, column_mapping, overall_only=False): \n    overall = {col_name: data_session.new_aggregated_metric(name + ' overall')\n               for col_name, name in column_mapping.items()}\n\n    def upload_df(df):\n        for col_name, metric in overall.items():\n            df['value'] = df[col_name]\n            metric.put(df)\n    return upload_df",
        "rewrite": "def get_uploader(data_session, column_mapping, overall_only=False):\n    overall = {col_name: data_session.new_aggregated_metric(name + ' overall')\n               for col_name, name in column_mapping.items()}\n\n    def upload_df(df):\n        for col_name, metric in overall.items():\n            df['value'] = df[col_name]\n            metric.put(df)\n    \n    return upload_df"
    },
    {
        "original": " \n  return [v + tf.zeros_like(sp, dtype=sp.dtype.base_dtype)\n          for v, sp in zip(volatility_parts, state_parts)]",
        "rewrite": "return [v + tf.zeros_like(sp, dtype=sp.dtype.base_dtype) for v, sp in zip(volatility_parts, state_parts)]"
    },
    {
        "original": "def _remove_consecutive_delims(expr, ldelim=\"(\", rdelim=\")\"): \n    tpars = _pair_delims(expr, ldelim=ldelim, rdelim=rdelim)\n    # Flag superfluous delimiters\n    ddelim = []\n    for ctuple, ntuple in zip(tpars, tpars[1:]):\n        if ctuple == (ntuple[0] - 1, ntuple[1] + 1):\n            ddelim.extend(ntuple)\n    ddelim.sort()\n    # Actually remove delimiters from expression\n    for num, item in enumerate(ddelim):\n        expr = expr[: item - num] + expr[item - num + 1 :]\n    # Get functions\n    return expr",
        "rewrite": "def _remove_consecutive_delims(expr, ldelim=\"(\", rdelim=\")\"):\n    tpars = _pair_delims(expr, ldelim=ldelim, rdelim=rdelim)\n    ddelim = []\n    for ctuple, ntuple in zip(tpars, tpars[1:]):\n        if ctuple == (ntuple[0] - 1, ntuple[1] + 1):\n            ddelim.extend(ntuple)\n    ddelim.sort()\n    for num, item in enumerate(ddelim):\n        expr = expr[: item - num] + expr[item - num + 1 :]\n    return expr"
    },
    {
        "original": "def is_attribute_deprecated(self, attribute): \n        rule_set = self._attribute_rule_sets.get(attribute)\n        if rule_set.version_deprecated:\n            if self._version >= rule_set.version_deprecated:\n                return True\n            else:\n                return False\n        else:\n            return False",
        "rewrite": "def is_attribute_deprecated(self, attribute):\n    rule_set = self._attribute_rule_sets.get(attribute)\n    if rule_set and rule_set.version_deprecated:\n        return self._version >= rule_set.version_deprecated\n    else:\n        return False"
    },
    {
        "original": "def _tree_load_link(self, new_traj_node, load_data, traj, as_new, hdf5_soft_link): \n        try:\n            linked_group = hdf5_soft_link()\n            link_name = hdf5_soft_link._v_name\n\n            if (not link_name in new_traj_node._links or\n                        load_data==pypetconstants.OVERWRITE_DATA):\n\n                link_location = linked_group._v_pathname\n                full_name = '.'.join(link_location.split('/')[2:])\n       ",
        "rewrite": "def _tree_load_link(self, new_traj_node, load_data, traj, as_new, hdf5_soft_link): \n    try:\n        linked_group = hdf5_soft_link()\n        link_name = hdf5_soft_link._v_name\n\n        if not link_name in new_traj_node._links or load_data == pypetconstants.OVERWRITE_DATA:\n            link_location = linked_group._v_pathname\n            full_name = '.'.join(link_location.split('/')[2:])"
    },
    {
        "original": "def get_file(db, user_id, api_path, include_content, decrypt_func): \n    query_fields = _file_default_fields()\n    if include_content:\n        query_fields.append(files.c.content)\n\n    return _get_file(db, user_id, api_path, query_fields, decrypt_func)",
        "rewrite": "def get_file(db, user_id, api_path, include_content, decrypt_func):\n    query_fields = _file_default_fields()\n    if include_content:\n        query_fields.append(files.c.content)\n    \n    return _get_file(db, user_id, api_path, query_fields, decrypt_func)"
    },
    {
        "original": "def schedule(self, year): \n        doc = self.get_year_doc(year)\n        table = doc('table#games')\n        df = sportsref.utils.parse_table(table)\n        if df.empty:\n            return pd.DataFrame()\n        df = df.loc[df['week_num'].notnull()]\n        df['week_num'] = np.arange(len(df)) + 1\n        df['is_win'] = df['game_outcome'] == 'W'\n        df['is_loss'] = df['game_outcome'] == 'L'\n        df['is_tie'] = df['game_outcome'] == 'T'\n        df['is_bye'] = df['game_outcome'].isnull()\n    ",
        "rewrite": "def schedule(self, year):\n    doc = self.get_year_doc(year)\n    table = doc('table#games')\n    df = sportsref.utils.parse_table(table)\n    \n    if df.empty:\n        return pd.DataFrame()\n    \n    df = df.loc[df['week_num'].notnull()]\n    df['week_num'] = np.arange(len(df)) + 1\n    df['is_win'] = df['game_outcome'] == 'W'\n    df['is_loss'] = df['game_outcome'] == 'L'\n    df['is_tie'] = df['game_outcome'] == 'T'\n    df['is_bye'] = df['game_outcome'].isnull()\n    df.reset_index(drop=True, inplace=True)"
    },
    {
        "original": "def _profile_package(self): \n        with _StatProfiler() as prof:\n            prof.base_frame = inspect.currentframe()\n            try:\n                runpy.run_path(self._run_object, run_name='__main__')\n            except SystemExit:\n                pass\n\n        call_tree = prof.call_tree\n        return {\n            'objectName': self._object_name,\n          ",
        "rewrite": "def _profile_package(self):\n    with _StatProfiler() as prof:\n        prof.base_frame = inspect.currentframe()\n        try:\n            runpy.run_path(self._run_object, run_name='__main__')\n        except SystemExit:\n            pass\n\n    call_tree = prof.call_tree\n    return {\n        'objectName': self._object_name,\n        'callTree': call_tree\n    }"
    },
    {
        "original": "def check(source): \n    if sys.version_info[0] == 2 and isinstance(source, unicode):\n        # Convert back to original byte string encoding, otherwise pyflakes\n        # call to compile() will complain. See PEP 263. This only affects\n        # Python 2.\n        try:\n            source = source.encode('utf-8')\n        except UnicodeError:  # pragma: no cover\n            return []\n\n    reporter = ListReporter()\n    try:\n        pyflakes.api.check(source, filename='<string>', reporter=reporter)\n",
        "rewrite": "import sys\nimport pyflakes.api\nfrom pyflakes.reporter import ListReporter\n\ndef check(source): \n    if sys.version_info[0] == 2 and isinstance(source, unicode):\n        try:\n            source = source.encode('utf-8')\n        except UnicodeError:\n            return []\n    \n    reporter = ListReporter()\n    try:\n        pyflakes.api.check(source, filename='<string>', reporter=reporter)"
    },
    {
        "original": "def _is_len_call(node): \n    return (\n        isinstance(node, astroid.Call)\n        and isinstance(node.func, astroid.Name)\n        and node.func.name == \"len\"\n    )",
        "rewrite": "def _is_len_call(node): \n    return (\n        isinstance(node, astroid.Call) \n        and isinstance(node.func, astroid.Name) \n        and node.func.name == \"len\"\n    )"
    },
    {
        "original": "def decode(self, input, errors='strict'): \n        if isinstance(input, memoryview):\n            input = input.tobytes()\n\n        if not isinstance(input, (binary_type, bytearray)):\n            raise with_context(\n                exc=TypeError(\n                    \"Can't decode {type}; byte string expected.\".format(\n                        type=type(input).__name__,\n         ",
        "rewrite": "def decode(self, input, errors='strict'):\n    if isinstance(input, memoryview):\n        input = input.tobytes()\n\n    if not isinstance(input, (bytes, bytearray)):\n        raise with_context(\n            exc=TypeError(\n                \"Can't decode {type}; byte string expected.\".format(\n                    type=type(input).__name__\n                )\n            )\n        )"
    },
    {
        "original": "def create_organisation(self, organisation_json): \n        return trolly.organisation.Organisation(\n            trello_client=self,\n            organisation_id=organisation_json['id'],\n            name=organisation_json['name'],\n            data=organisation_json,\n        )",
        "rewrite": "def create_organisation(self, organisation_json):\n    return trolly.organisation.Organisation(\n        trello_client=self,\n        organisation_id=organisation_json['id'],\n        name=organisation_json['name'],\n        data=organisation_json\n    )"
    },
    {
        "original": "def is_in_database(self): \n\n        if (\n            self._authorization()\n            and PyFunceble.INTERN[\"file_to_test\"] in PyFunceble.INTERN[\"whois_db\"]\n            and PyFunceble.INTERN[\"to_test\"]\n            in PyFunceble.INTERN[\"whois_db\"][PyFunceble.INTERN[\"file_to_test\"]]\n        ):\n            # * We are authorized to work.\n            # and\n            # * The given file path exist in the database.\n   ",
        "rewrite": "def is_in_database(self): \n\n    if self._authorization() and PyFunceble.INTERN[\"file_to_test\"] in PyFunceble.INTERN[\"whois_db\"] and PyFunceble.INTERN[\"to_test\"] in PyFunceble.INTERN[\"whois_db\"][PyFunceble.INTERN[\"file_to_test\"]]:\n        # * We are authorized to work.\n        # and\n        # * The given file path exist in the database."
    },
    {
        "original": "def unused_variable_line_numbers(messages): \n    for message in messages:\n        if isinstance(message, pyflakes.messages.UnusedVariable):\n            yield message.lineno",
        "rewrite": "def unused_variable_line_numbers(messages): \n    for message in messages:\n        if isinstance(message, pyflakes.messages.UnusedVariable):\n            yield message.lineno"
    },
    {
        "original": "def _iter_sims(self): \n        for idx, lineset in enumerate(self.linesets[:-1]):\n            for lineset2 in self.linesets[idx + 1 :]:\n                for sim in self._find_common(lineset, lineset2):\n                    yield sim",
        "rewrite": "def _iter_sims(self):\n    for idx, lineset in enumerate(self.linesets[:-1]):\n        for lineset2 in self.linesets[idx + 1:]:\n            for sim in self._find_common(lineset, lineset2):\n                yield sim"
    },
    {
        "original": "def topNBottomN(self, column=0, nPercent=10, grabTopN=-1): \n        assert (nPercent >= 0) and (nPercent<=100.0), \"nPercent must be between 0.0 and 100.0\"\n        assert round(nPercent*0.01*self.nrows)>0, \"Increase nPercent.  Current value will result in top 0 row.\"\n\n        if isinstance(column, int):\n            if (column < 0) or (column>=self.ncols):\n                raise H2OValueError(\"Invalid column index H2OFrame\")\n            else:\n                colIndex = column\n     ",
        "rewrite": "def topNBottomN(self, column=0, nPercent=10, grabTopN=-1):\n    assert 0 <= nPercent <= 100.0, \"nPercent must be between 0.0 and 100.0\"\n    assert round(nPercent * 0.01 * self.nrows) > 0, \"Increase nPercent. Current value will result in top 0 row.\"\n\n    if isinstance(column, int):\n        if column < 0 or column >= self.ncols:\n            raise H2OValueError(\"Invalid column index H2OFrame\")\n        else:\n            colIndex = column"
    },
    {
        "original": "def patch_database(self, instance, database, body, project_id=None): \n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                     ",
        "rewrite": "def patch_database(self, instance, database, body, project_id=None): \n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id)"
    },
    {
        "original": "def canonicalize(self, mol): \n        # TODO: Overload the mol parameter to pass a list of pre-enumerated tautomers\n        tautomers = self._enumerate_tautomers(mol)\n        if len(tautomers) == 1:\n            return tautomers[0]\n        # Calculate score for each tautomer\n        highest = None\n        for t in tautomers:\n            smiles = Chem.MolToSmiles(t, isomericSmiles=True)\n            log.debug('Tautomer: %s', smiles)\n       ",
        "rewrite": "def canonicalize(self, mol):\n        tautomers = self._enumerate_tautomers(mol)\n        if len(tautomers) == 1:\n            return tautomers[0]\n        \n        highest = None\n        for t in tautomers:\n            smiles = Chem.MolToSmiles(t, isomericSmiles=True)\n            log.debug('Tautomer: %s', smiles)"
    },
    {
        "original": "def batch_list(sequence, batch_size, mod = 0, randomize = False): \n\n    if randomize:\n        sequence = random.sample(sequence, len(sequence))\n\n    return [sequence[x:x + batch_size] for x in xrange(0, len(sequence)-mod, batch_size)]",
        "rewrite": "import random\n\ndef batch_list(sequence, batch_size, mod = 0, randomize = False):\n    if randomize:\n        sequence = random.sample(sequence, len(sequence))\n    return [sequence[x:x + batch_size] for x in range(0, len(sequence)-mod, batch_size)]"
    },
    {
        "original": "def topological_nodes(self): \n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))",
        "rewrite": "def topological_nodes(self):\n    return nx.lexicographical_topological_sort(self._multi_graph, key=lambda x: str(x.qargs))"
    },
    {
        "original": "def update(self, label): \n        data = {\n            'id': label['id'],\n            'name': label['name'],\n            'appearance': label['appearance'],\n            'description': label['description'],\n            'title': label['title'],\n        }\n        return self._post(\n            request=ApiActions.UPDATE.value,\n            uri=ApiUri.TAGS.value,\n        ",
        "rewrite": "def update(self, label):\n    data = {\n        'id': label['id'],\n        'name': label['name'],\n        'appearance': label['appearance'],\n        'description': label['description'],\n        'title': label['title'],\n    }\n    return self._post(\n        request=ApiActions.UPDATE.value,\n        uri=ApiUri.TAGS.value,\n        data=data\n    )"
    },
    {
        "original": "def _make_default_operation_costs(self, allow_spaces=False): \n        self.operation_costs = dict()\n        self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[\"\"] = 1.0\n            if allow_spaces:\n                current_costs[\" \"] = 1.0\n      ",
        "rewrite": "def _make_default_operation_costs(self, allow_spaces=False):\n    self.operation_costs = {}\n    self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n    \n    for a in self.alphabet:\n        current_costs = {c: 1.0 for c in self.alphabet}\n        current_costs[a] = 0.0\n        current_costs[\"\"] = 1.0\n        \n        if allow_spaces:\n            current_costs[\" \"] = 1.0"
    },
    {
        "original": "def _yield_bundle_contents(self, data): \n        if isinstance(data, list):\n            contents = data\n        else:\n            contents = data.get('contents', [])\n            if isinstance(contents, six.string_types):\n                contents = contents,\n        for content in contents:\n            if isinstance(content, dict):\n                content = self._create_bundle(content)\n  ",
        "rewrite": "def _yield_bundle_contents(self, data):\n    if isinstance(data, list):\n        contents = data\n    else:\n        contents = data.get('contents', [])\n        if isinstance(contents, six.string_types):\n            contents = [contents]\n    for content in contents:\n        if isinstance(content, dict):\n            content = self._create_bundle(content)"
    },
    {
        "original": "def call(self, inputs): \n    # TODO(dusenberrymw): Remove these reshaping commands after b/113126249 is\n    # fixed.\n    collapsed_shape = tf.concat(([-1], tf.shape(input=inputs)[-2:]), axis=0)\n    out = tf.reshape(inputs, collapsed_shape)  # (sample*batch_size, T, hidden)\n    out = self.bilstm(out)  # (sample*batch_size, hidden)\n    expanded_shape = tf.concat((tf.shape(input=inputs)[:-2], [-1]), axis=0)\n    out = tf.reshape(out, expanded_shape)  # (sample, batch_size, hidden)\n    out = self.output_layer(out)  # (sample, batch_size, 2*latent_size)\n    loc = out[..., :self.latent_size]\n    scale_diag = tf.nn.softplus(out[..., self.latent_size:]) + 1e-5  # keep > 0\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)",
        "rewrite": "def call(self, inputs):\n    collapsed_shape = tf.concat([-1, tf.shape(input=inputs)[-2:]], axis=0)\n    out = tf.reshape(inputs, collapsed_shape)\n    out = self.bilstm(out)\n    expanded_shape = tf.concat([tf.shape(input=inputs)[:-2], -1], axis=0)\n    out = tf.reshape(out, expanded_shape)\n    out = self.output_layer(out)\n    loc = out[..., :self.latent_size]\n    scale_diag = tf.nn.softplus(out[..., self.latent_size:]) + 1e-5\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)"
    },
    {
        "original": "def _get_name(self): \n        if (self.tail_node is not None) and (self.head_node is not None):\n            return \"%s %s %s\" % (self.tail_node.ID, self.conn,\n                                 self.head_node.ID)\n        else:\n            return \"Edge\"",
        "rewrite": "def _get_name(self):\n    if self.tail_node is not None and self.head_node is not None:\n        return \"%s %s %s\" % (self.tail_node.ID, self.conn, self.head_node.ID)\n    else:\n        return \"Edge\""
    },
    {
        "original": "def _leave_event_hide(self): \n        if (not self._hide_timer.isActive() and\n            # If Enter events always came after Leave events, we wouldn't need\n            # this check. But on Mac OS, it sometimes happens the other way\n            # around when the tooltip is created.\n            QtGui.qApp.topLevelAt(QtGui.QCursor.pos()) != self):\n            self._hide_timer.start(300, self)",
        "rewrite": "def _leave_event_hide(self):\n    if (not self._hide_timer.isActive() and\n        QtGui.qApp.topLevelAt(QtGui.QCursor.pos()) != self):\n        self._hide_timer.start(300, self)"
    },
    {
        "original": "def readImages(path, sc=None, minParitions = 1, bigdl_type=\"float\"): \n        df = callBigDlFunc(bigdl_type, \"dlReadImage\", path, sc, minParitions)\n        df._sc._jsc = sc._jsc\n        return df",
        "rewrite": "def readImages(path, sc=None, minPartitions=1, bigdl_type=\"float\"):\n    df = callBigDlFunc(bigdl_type, \"dlReadImage\", path, sc, minPartitions)\n    df._sc._jsc = sc._jsc\n    return df"
    },
    {
        "original": "def unsign(self, value, max_age=None, return_timestamp=False): \n        try:\n            result = Signer.unsign(self, value)\n            sig_error = None\n        except BadSignature as e:\n            sig_error = e\n            result = e.payload or b''\n        sep = want_bytes(self.sep)\n\n        # If there is no timestamp in the result there is something\n        # seriously wrong.  In case there was",
        "rewrite": "raise BadTimeSignature(f\"No timestamp found on {result} when unsigning with sep={sep}\")\n        payload = result.rsplit(sep, 1)[0]\n        timestamp = result[-sep:]\n        if max_age is not None:\n            age = int(time.time() - self.decode(timestamp))\n            if age > max_age:\n                raise SignatureExpired(f\"Signature age {age} > {max_age}\")\n        if return_timestamp:\n            return payload, timestamp\n        return payload"
    },
    {
        "original": " \n\n    case_obj = adapter.case(\n        case_id=case_id,\n    )\n\n    if case_obj is None:\n        raise DataNotFoundError(\"no case found\")\n\n    if not case_obj.get('delivery_report'):\n        _put_report_in_case_root(case_obj, report_path)\n    else:\n        if update:\n            _put_report_in_case_root(case_obj, report_path)\n        else:\n            raise IntegrityError('Existing delivery report found, use update = True to '\n                   ",
        "rewrite": "```python\ncase_obj = adapter.case(\n    case_id=case_id\n)\n\nif case_obj is None:\n    raise DataNotFoundError(\"no case found\")\n\nif not case_obj.get('delivery_report'):\n    _put_report_in_case_root(case_obj, report_path)\nelse:\n    if update:\n        _put_report_in_case_root(case_obj, report_path)\n    else:\n        raise IntegrityError('Existing delivery report found, use update = True to overwrite.')\n```"
    },
    {
        "original": "def lookup(self, subcmd_prefix): \n        for subcmd_name in list(self.subcmds.keys()):\n            if subcmd_name.startswith(subcmd_prefix) \\\n               and len(subcmd_prefix) >= \\\n               self.subcmds[subcmd_name].__class__.min_abbrev:\n                return self.subcmds[subcmd_name]\n            pass\n        return None",
        "rewrite": "def lookup(self, subcmd_prefix): \n    for subcmd_name in list(self.subcmds.keys()):\n        if subcmd_name.startswith(subcmd_prefix) and len(subcmd_prefix) >= self.subcmds[subcmd_name].__class__.min_abbrev:\n            return self.subcmds[subcmd_name]\n    return None"
    },
    {
        "original": "def get(self, name_or_tag_id): \n        hooks = self.list()\n\n        return [\n            hook\n            for hook\n            in hooks\n            if name_or_tag_id in hook.get('actions')\n            or name_or_tag_id == hook.get('name')\n        ]",
        "rewrite": "def get(self, name_or_tag_id):\n        hooks = self.list()\n\n        return [hook for hook in hooks if name_or_tag_id in hook.get('actions') or name_or_tag_id == hook.get('name')]"
    },
    {
        "original": "def y64_encode(s): \n        first_pass = base64.urlsafe_b64encode(s)\n        return first_pass.translate(bytes.maketrans(b\"+/=\", b\"._-\"))",
        "rewrite": "def y64_encode(s):\n    first_pass = base64.urlsafe_b64encode(s)\n    return first_pass.translate(bytes.maketrans(b\"+/=\", b\"._-\"))"
    },
    {
        "original": "def path_to_dir(*path_args): \n        return os.path.join(\n            *list(path_args[:-1]) + path_args[-1].split(posixpath.sep)\n        )",
        "rewrite": "def path_to_dir(*path_args):\n    return os.path.join(*list(path_args[0:-1]), *path_args[-1].split(posixpath.sep))"
    },
    {
        "original": "def set_failover_mode(mode): \n    jar = aiohttp.CookieJar(unsafe=True)\n    websession = aiohttp.ClientSession(cookie_jar=jar)\n\n    try:\n        modem = eternalegypt.Modem(hostname=sys.argv[1], websession=websession)\n        await modem.login(password=sys.argv[2])\n\n        await modem.set_failover_mode(mode)\n\n        await modem.logout()\n    except eternalegypt.Error:\n        print(\"Could not login\")\n\n    await websession.close()",
        "rewrite": "import aiohttp\nimport eternalegypt\n\nasync def set_failover_mode(mode):\n    jar = aiohttp.CookieJar(unsafe=True)\n    async with aiohttp.ClientSession(cookie_jar=jar) as websession:\n        try:\n            modem = eternalegypt.Modem(hostname=sys.argv[1], websession=websession)\n            await modem.login(password=sys.argv[2])\n\n            await modem.set_failover_mode(mode)\n\n            await modem.logout()\n        except eternalegypt.Error:\n            print(\"Could not login\")\n\n    await websession.close()"
    },
    {
        "original": "def dump(cls, type, exc): \n        try:\n            return pickle.dumps(type), pickle.dumps(exc)\n        except Exception:\n            return cls.dump(cls, cls(repr(exc)))",
        "rewrite": "def dump(cls, type, exc):\n    try:\n        return pickle.dumps(type), pickle.dumps(exc)\n    except Exception:\n        return cls.dump(cls, cls(repr(exc))"
    },
    {
        "original": "def single(iterable, fn): \n    found = False\n    ret = None\n\n    for i in iterable:\n        if fn(i):\n            if found:\n                raise DuplicitValueExc(i)\n            found = True\n            ret = i\n\n    if not found:\n        raise NoValueExc()\n\n    return ret",
        "rewrite": "def single(iterable, fn): \n    found = False\n    ret = None\n\n    for i in iterable:\n        if fn(i):\n            if found:\n                raise DuplicitValueExc(i)\n            found = True\n            ret = i\n\n    if not found:\n        raise NoValueExc()\n\n    return ret"
    },
    {
        "original": "def _set_output_arguments(self): \n\n        group = self.parser.add_argument_group('output arguments')\n        group.add_argument('-o', '--output', type=argparse.FileType('w'),\n                           dest='outfile', default=sys.stdout,\n                           help=\"output file\")\n        group.add_argument('--json-line', dest='json_line', action='store_true',\n                           help=\"produce a JSON line for each output item\")",
        "rewrite": "def _set_output_arguments(self): \n\n        group = self.parser.add_argument_group('output arguments')\n        group.add_argument('-o', '--output', type=argparse.FileType('w'),\n                           dest='outfile', default=sys.stdout,\n                           help=\"output file\")\n        group.add_argument('--json-line', dest='json_line', action='store_true',\n                           help=\"produce a JSON line for each output item\")"
    },
    {
        "original": "def do_AUTOCOMPLETE(cmd, s): \n    s = list(preprocess_query(s))[0]\n    keys = [k.decode() for k in DB.smembers(edge_ngram_key(s))]\n    print(white(keys))\n    print(magenta('({} elements)'.format(len(keys))))",
        "rewrite": "def do_AUTOCOMPLETE(cmd, s): \n    s = list(preprocess_query(s))[0]\n    keys = [k.decode() for k in DB.smembers(edge_ngram_key(s))]\n    print(white(keys))\n    print(magenta('({} elements)'.format(len(keys))) )"
    },
    {
        "original": "def with_objattrs(*names): \n\n    def _wrap(func):\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            with contextlib.ExitStack() as stack:\n                for name in names:\n                    stack.enter_context(getattr(self, name))\n                return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return _wrap",
        "rewrite": "import contextlib\nimport functools\n\ndef with_objattrs(*names):\n\n    def _wrap(func):\n\n        @functools.wraps(func)\n        def wrapper(self, *args, **kwargs):\n            with contextlib.ExitStack() as stack:\n                for name in names:\n                    stack.enter_context(getattr(self, name))\n                return func(self, *args, **kwargs)\n\n        return wrapper\n\n    return _wrap"
    },
    {
        "original": "def run(self): \n        existing_objects = _get_in_memory_objects()\n        prof, result = self.profile()\n        new_objects = _get_in_memory_objects()\n\n        new_obj_count = _get_obj_count_difference(new_objects, existing_objects)\n        result_obj_count = new_obj_count - prof.obj_overhead\n\n        # existing_objects list is also profiler overhead\n        result_obj_count[list] -= 1\n        pretty_obj_count = _format_obj_count(result_obj_count)\n        return {\n            'objectName': self._object_name,\n            'codeEvents': prof.code_events,\n  ",
        "rewrite": "def run(self): \n    existing_objects = _get_in_memory_objects()\n    prof, result = self.profile()\n    new_objects = _get_in_memory_objects()\n\n    new_obj_count = _get_obj_count_difference(new_objects, existing_objects)\n    result_obj_count = new_obj_count - prof.obj_overhead\n\n    result_obj_count['list'] -= 1\n    pretty_obj_count = _format_obj_count(result_obj_count)\n    return {\n        'objectName': self._object_name,\n        'codeEvents': prof.code_events,\n    }"
    },
    {
        "original": "def run_with_configuration(self, configuration): \n        jobs = self.service.jobs()\n        job_data = {'configuration': configuration}\n\n        # Send query and wait for reply.\n        query_reply = jobs \\\n            .insert(projectId=self.project_id, body=job_data) \\\n            .execute(num_retries=self.num_retries)\n        self.running_job_id = query_reply['jobReference']['jobId']\n        if 'location' in query_reply['jobReference']:\n            location = query_reply['jobReference']['location']\n        else:\n         ",
        "rewrite": "location = query_reply['jobReference'].get('location', None)"
    },
    {
        "original": "def _get_samples(self, subset=None): \n        if subset is None:\n            samples = self.subsets['All_Samples']\n        else:\n            try:\n                samples = self.subsets[subset]\n            except KeyError:\n                raise KeyError((\"Subset '{:s}' does not \".format(subset) +\n                          ",
        "rewrite": "def _get_samples(self, subset=None): \n        if subset is None:\n            samples = self.subsets['All_Samples']\n        else:\n            try:\n                samples = self.subsets[subset]\n            except KeyError:\n                raise KeyError(\"Subset '{:s}' does not exist.\".format(subset))"
    },
    {
        "original": "def _try_reduce_list(statements: List[\"HdlStatement\"]): \n        io_change = False\n        new_statements = []\n\n        for stm in statements:\n            reduced, _io_change = stm._try_reduce()\n            new_statements.extend(reduced)\n            io_change |= _io_change\n\n        new_statements, rank_decrease = HdlStatement._merge_statements(\n            new_statements)\n\n        return new_statements, rank_decrease, io_change",
        "rewrite": "from typing import List\n\ndef _try_reduce_list(statements: List[\"HdlStatement\"]): \n    io_change = False\n    new_statements = []\n\n    for stm in statements:\n        reduced, _io_change = stm._try_reduce()\n        new_statements.extend(reduced)\n        io_change |= _io_change\n\n    new_statements, rank_decrease = HdlStatement._merge_statements(new_statements)\n\n    return new_statements, rank_decrease, io_change"
    },
    {
        "original": "def concat(self, frames, axis=1): \n        if len(frames) == 0:\n            raise ValueError(\"Input list of frames is empty! Nothing to concat.\")\n\n        if axis == 1:\n            df = self.cbind(frames)\n        else:\n            df = self.rbind(frames)\n        return df",
        "rewrite": "def concat(self, frames, axis=1):\n    if not frames:\n        raise ValueError(\"Input list of frames is empty! Nothing to concat.\")\n    \n    if axis == 1:\n        df = self.cbind(frames)\n    else:\n        df = self.rbind(frames)\n    \n    return df"
    },
    {
        "original": "def find_systemjs_location(): \n    location = os.path.abspath(os.path.dirname(locate_package_json()))\n    conf = parse_package_json()\n\n    if 'jspm' in conf:\n        conf = conf['jspm']\n\n    try:\n        conf = conf['directories']\n    except TypeError:\n        raise ImproperlyConfigured(\"`package.json` doesn't appear to be a valid json object. \"\n                                   \"Location: %s\" % location)\n    except KeyError:\n        raise ImproperlyConfigured(\"The `directories` configuarion was not found in package.json.",
        "rewrite": "def find_systemjs_location():\n    location = os.path.abspath(os.path.dirname(locate_package_json()))\n    conf = parse_package_json()\n\n    if 'jspm' in conf:\n        conf = conf['jspm']\n\n    try:\n        conf = conf['directories']\n    except TypeError:\n        raise ImproperlyConfigured(\"`package.json` doesn't appear to be a valid json object. \"\n                                   \"Location: %s\" % location)\n    except KeyError:\n        raise ImproperlyConfigured(\"The `directories` configuration was not found in package.json.\")"
    },
    {
        "original": "def convert_persistent_value(self, shift, instruction): \n        command_dict = {\n            'name': 'pv',\n            't0': shift+instruction.start_time,\n            'ch': instruction.channels[0].name,\n            'val': instruction.command.value\n        }\n        return self._qobj_model(**command_dict)",
        "rewrite": "def convert_persistent_value(self, shift, instruction):\n    command_dict = {\n        'name': 'pv',\n        't0': shift + instruction.start_time,\n        'ch': instruction.channels[0].name,\n        'val': instruction.command.value\n    }\n    return self._qobj_model(**command_dict)"
    },
    {
        "original": "def _expand(self, explore_iterable): \n        if self.v_locked:\n            raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n        if not self.f_has_range():\n            raise TypeError('Your Parameter `%s` is not an array and can therefore '\n                            'not be expanded.' % self._name)\n\n        data_list = self._data_sanity_checks(explore_iterable)\n\n        self._explored_range.extend(data_list)\n        self.f_lock()",
        "rewrite": "def _expand(self, explore_iterable):\n    if self.v_locked:\n        raise pex.ParameterLockedException('Parameter `%s` is locked!' % self.v_full_name)\n\n    if not self.f_has_range():\n        raise TypeError('Your Parameter `%s` is not an array and can therefore not be expanded.' % self._name)\n\n    data_list = self._data_sanity_checks(explore_iterable)\n\n    self._explored_range.extend(data_list)\n    self.f_lock()"
    },
    {
        "original": "def FreedmanDiaconisBinSize(feature_values): \n\n  q75, q25 = numpy.percentile(feature_values, [75, 25])\n  IQR = q75 - q25\n\n  return 2.0 * IQR * len(feature_values) ** (-1.0/3.0)",
        "rewrite": "import numpy\n\ndef FreedmanDiaconisBinSize(feature_values): \n    q75, q25 = numpy.percentile(feature_values, [75, 25])\n    IQR = q75 - q25\n    return 2.0 * IQR * len(feature_values) ** (-1.0/3.0)"
    },
    {
        "original": " \n    new_str = []\n    for c in s:\n        new_str.append(_MUNGE_REPLACEMENTS.get(c, c))\n\n    new_s = \"\".join(new_str)\n\n    if keyword.iskeyword(new_s):\n        return f\"{new_s}_\"\n\n    if not allow_builtins and new_s in builtins.__dict__:\n        return f\"{new_s}_\"\n\n    return new_s",
        "rewrite": "new_str = []\nfor c in s:\n    new_str.append(_MUNGE_REPLACEMENTS.get(c, c))\n\nnew_s = \"\".join(new_str)\n\nif keyword.iskeyword(new_s):\n    return f\"{new_s}_\"\n\nif not allow_builtins and new_s in builtins.__dict__:\n    return f\"{new_s}_\"\n\nreturn new_s"
    },
    {
        "original": "def velocity_from_bundle(self, bundle): \n\n        coefficients, days_per_set, T, twot1 = bundle\n        coefficient_count = coefficients.shape[2]\n\n        # Chebyshev derivative:\n\n        dT = np.empty_like(T)\n        dT[0] = 0.0\n        dT[1] = 1.0\n        dT[2] = twot1 + twot1\n        for i in range(3, coefficient_count):\n            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n        dT *= 2.0\n     ",
        "rewrite": "def velocity_from_bundle(self, bundle): \n    coefficients, days_per_set, T, twot1 = bundle\n    coefficient_count = coefficients.shape[2]\n\n    dT = np.empty_like(T)\n    dT[0] = 0.0\n    dT[1] = 1.0\n    dT[2] = twot1 + twot1\n    for i in range(3, coefficient_count):\n        dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n    dT *= 2.0"
    },
    {
        "original": "def write_batch_data(self, items): \n\n        dynamodb_conn = self.get_conn()\n\n        try:\n            table = dynamodb_conn.Table(self.table_name)\n\n            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n                for item in items:\n                    batch.put_item(Item=item)\n            return True\n        except Exception as general_error:\n            raise AirflowException(\n ",
        "rewrite": "def write_batch_data(self, items):\n    dynamodb_conn = self.get_conn()\n    \n    try:\n        table = dynamodb_conn.Table(self.table_name)\n        \n        with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:\n            for item in items:\n                batch.put_item(Item=item)\n        return True\n    except Exception as general_error:\n        raise AirflowException(\"Error writing batch data to DynamoDB table.\")"
    },
    {
        "original": "def get_account(self, account_id): \n        url = ACCOUNTS_API.format(account_id)\n        return CanvasAccount(data=self._get_resource(url))",
        "rewrite": "def get_account(self, account_id):\n    url = ACCOUNTS_API.format(account_id)\n    return CanvasAccount(data=self._get_resource(url))"
    },
    {
        "original": "def _sort_locations(locations, expand_dir=False): \n        files = []\n        urls = []\n\n        # puts the url for the given file path into the appropriate list\n        def sort_path(path):\n            url = path_to_url(path)\n            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':\n                urls.append(url)\n            else:\n                files.append(url)\n\n ",
        "rewrite": "import mimetypes\n\n\ndef _sort_locations(locations, expand_dir=False):\n    files = []\n    urls = []\n\n    # puts the url for the given file path into the appropriate list\n    def sort_path(path):\n        url = path_to_url(path)\n        if mimetypes.guess_type(url, strict=False)[0] == 'text/html':\n            urls.append(url)\n        else:\n            files.append(url)"
    },
    {
        "original": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET): \n        if not offset:\n            offset = DEFAULT_OFFSET\n\n        kwargs = {\"offset\": offset}\n        items = super().fetch(category, **kwargs)\n\n        return items",
        "rewrite": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET): \n    if not offset:\n        offset = DEFAULT_OFFSET\n\n    kwargs = {\"offset\": offset}\n    items = super().fetch(category, **kwargs)\n\n    return items"
    },
    {
        "original": "def _check_token_present(self): \n\t\ttry:\n\t\t\tself._get_value(CONFIGKEY_TOKEN)\n\t\t\tself._get_value(CONFIGKEY_REFRESH_TOKEN)\n\t\t\tself._get_value(CONFIGKEY_REFRESHABLE)\n\t\texcept KeyError:\n\t\t\tself._log(\"Request new Token (CTP)\")\n\t\t\tself._get_new_access_information()",
        "rewrite": "def _check_token_present(self):\n    try:\n        self._get_value(CONFIGKEY_TOKEN)\n        self._get_value(CONFIGKEY_REFRESH_TOKEN)\n        self._get_value(CONFIGKEY_REFRESHABLE)\n    except KeyError:\n        self._log(\"Request new Token (CTP)\")\n        self._get_new_access_information()"
    },
    {
        "original": "def _verify_backends(self): \n        ret = OrderedDict()\n        for backend_cls in SIMULATORS:\n            try:\n                backend_instance = self._get_backend_instance(backend_cls)\n                backend_name = backend_instance.name()\n                ret[backend_name] = backend_instance\n            except QiskitError as err:\n                # Ignore backends that could not be initialized.\n",
        "rewrite": "def _verify_backends(self): \n    ret = OrderedDict()\n    for backend_cls in SIMULATORS:\n        try:\n            backend_instance = self._get_backend_instance(backend_cls)\n            backend_name = backend_instance.name()\n            ret[backend_name] = backend_instance\n        except QiskitError as err:\n            pass"
    },
    {
        "original": " \n    if isinstance(postcode_like, Postcode):\n        return postcode_like\n\n    postcode_like = postcode_like.replace(\" \", \"\").upper()\n\n    try:\n        postcode = Postcode.get(Postcode.postcode == postcode_like)\n    except DoesNotExist:\n        try:\n            postcode = await fetch_postcode_from_string(postcode_like)\n        except (ApiError, CircuitBreakerError):\n            raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n        if postcode is not None:\n            postcode.save()\n\n    return",
        "rewrite": "if isinstance(postcode_like, Postcode):\n        return postcode_like\n\n    postcode_like = postcode_like.replace(\" \", \"\").upper()\n\n    try:\n        postcode = Postcode.get(Postcode.postcode == postcode_like)\n    except DoesNotExist:\n        try:\n            postcode = await fetch_postcode_from_string(postcode_like)\n        except (ApiError, CircuitBreakerError):\n            raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n        if postcode is not None:\n            postcode.save()\n\n    return"
    },
    {
        "original": "def get(self, key): \n        if not key.startswith(\"secure.\") and not key.startswith(\"connections.\"):\n            key = \"secure.{0}\".format(key)\n        value = self.config.get_value(key)\n        if not isinstance(value, basestring):\n            value = None\n        return value",
        "rewrite": "def get(self, key):\n    if not key.startswith(\"secure.\") and not key.startswith(\"connections.\"):\n        key = f\"secure.{key}\"\n    value = self.config.get_value(key)\n    if not isinstance(value, str):\n        value = None\n    return value"
    },
    {
        "original": "def get_cumulative_data(self): \n\t\tsets = map(itemgetter('data'), self.data)\n\t\tif not sets:\n\t\t\treturn\n\t\tsum = sets.pop(0)\n\t\tyield sum\n\t\twhile sets:\n\t\t\tsum = map(add, sets.pop(0))\n\t\t\tyield sum",
        "rewrite": "def get_cumulative_data(self): \n    sets = map(itemgetter('data'), self.data)\n    if not sets:\n        return\n    total = sets.pop(0)\n    yield total\n    while sets:\n        total = map(add, total, sets.pop(0))\n        yield total"
    },
    {
        "original": "def emit(self, record): \n        try:\n            import smtplib\n            try:\n                from email.utils import formatdate\n            except ImportError:\n                formatdate = self.date_time\n\n            port = self.mailport\n            if not port:\n             ",
        "rewrite": "if not port:\n                port = smtplib.SMTP_PORT"
    },
    {
        "original": "def get_action(cls, alias): \n        for action, alias_list in cls.action_map.items():\n            if alias in alias_list:\n                return action\n        return None",
        "rewrite": "def get_action(cls, alias):\n    for action, alias_list in cls.action_map.items():\n        if alias in alias_list:\n            return action\n    return None"
    },
    {
        "original": "def is_cptp(self, atol=None, rtol=None): \n        if self._data[1] is not None:\n            return False\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        accum = 0j\n        for op in self._data[0]:\n            accum += np.dot(np.transpose(np.conj(op)), op)\n        return is_identity_matrix(accum, rtol=rtol, atol=atol)",
        "rewrite": "def is_cptp(self, atol=None, rtol=None): \n    if self._data[1] is not None:\n        return False\n    if atol is None:\n        atol = self._atol\n    if rtol is None:\n        rtol = self._rtol\n    accum = 0j\n    for op in self._data[0]:\n        accum += np.dot(np.transpose(np.conj(op)), op)\n    return is_identity_matrix(accum, rtol=rtol, atol=atol)"
    },
    {
        "original": "def _iteritems_args_kw(*args, **kw): \n    args_len = len(args)\n    if args_len > 1:\n        raise TypeError('Expected at most 1 positional argument, got %d' % args_len)\n    itemchain = None\n    if args:\n        arg = args[0]\n        if arg:\n            itemchain = _iteritems_mapping_or_iterable(arg)\n    if kw:\n        iterkw = iteritems(kw)\n        itemchain = chain(itemchain, iterkw) if itemchain else iterkw\n    return itemchain or _NULL_IT",
        "rewrite": "def _iteritems_args_kw(*args, **kw):\n    args_len = len(args)\n    if args_len > 1:\n        raise TypeError('Expected at most 1 positional argument, got %d' % args_len)\n    itemchain = None\n    if args:\n        arg = args[0]\n        if arg:\n            itemchain = _iteritems_mapping_or_iterable(arg)\n    if kw:\n        iterkw = iteritems(kw)\n        itemchain = chain(itemchain, iterkw) if itemchain else iterkw\n    return itemchain or _NULL_IT"
    },
    {
        "original": "def to_native(self, value): \n        context_request = None\n        if self.context:\n            context_request = self.context.get('request', None)\n        return build_versatileimagefield_url_set(\n            value,\n            self.sizes,\n            request=context_request\n        )",
        "rewrite": "def to_native(self, value):\n    context_request = None\n    if self.context:\n        context_request = self.context.get('request', None)\n    return build_versatileimagefield_url_set(\n        value,\n        self.sizes,\n        request=context_request\n    )"
    },
    {
        "original": " \n    try:\n        postcode = await fetch_postcode_random()\n    except (ApiError, CircuitBreakerError):\n        raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\n    if postcode is not None:\n        postcode.save()\n    return postcode",
        "rewrite": "try:\n    postcode = await fetch_postcode_random()\nexcept (ApiError, CircuitBreakerError):\n    raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\nif postcode is not None:\n    postcode.save()\nreturn postcode"
    },
    {
        "original": "def authenticate_direct_bind(self, username, password): \n\n        bind_user = '{rdn}={username},{user_search_dn}'.format(\n            rdn=self.config.get('LDAP_USER_RDN_ATTR'),\n            username=username,\n            user_search_dn=self.full_user_search_dn,\n        )\n\n        connection = self._make_connection(\n            bind_user=bind_user,\n            bind_password=password,\n        )\n\n        response = AuthenticationResponse()\n\n        try:\n         ",
        "rewrite": "def authenticate_direct_bind(self, username, password):\n\n        bind_user = '{rdn}={username},{user_search_dn}'.format(\n            rdn=self.config.get('LDAP_USER_RDN_ATTR'),\n            username=username,\n            user_search_dn=self.full_user_search_dn,\n        )\n\n        connection = self._make_connection(\n            bind_user=bind_user,\n            bind_password=password,\n        )\n\n        response = AuthenticationResponse()"
    },
    {
        "original": " \n  output_tensorshape, is_validated = _replace_event_shape_in_tensorshape(\n      tensorshape_util.constant_value_as_shape(input_shape),\n      event_shape_in,\n      event_shape_out)\n\n  # TODO(b/124240153): Remove map(tf.identity, deps) once tf.function\n  # correctly supports control_dependencies.\n  validation_dependencies = (\n      map(tf.identity, (event_shape_in, event_shape_out))\n      if validate_args else ())\n\n  if (tensorshape_util.is_fully_defined(output_tensorshape) and\n      (is_validated or not validate_args)):\n    with tf.control_dependencies(validation_dependencies):\n      output_shape = tf.convert_to_tensor(\n          value=output_tensorshape, name='output_shape', dtype_hint=tf.int32)\n    return output_shape, output_tensorshape\n\n  with tf.control_dependencies(validation_dependencies):\n    event_shape_in_ndims = (\n        tf.size(input=event_shape_in)\n        if",
        "rewrite": "output_tensorshape, is_validated = _replace_event_shape_in_tensorshape(\n    tensorshape_util.constant_value_as_shape(input_shape),\n    event_shape_in,\n    event_shape_out)\n\nvalidation_dependencies = (map(tf.identity, (event_shape_in, event_shape_out)) \n                           if validate_args else ())\n\nif (tensorshape_util.is_fully_defined(output_tensorshape) and\n    (is_validated or not validate_args)):\n  with tf.control_dependencies(validation_dependencies):\n    output_shape = tf.convert_to_tensor(\n        value=output_tensorshape, name='output_shape', dtype_hint=tf.int32)\n  return output_shape, output_tensorshape\n\nwith tf.control_dependencies(validation_dependencies):\n  event_shape_in_ndims = (tf.size(input=event_shape_in) if ..."
    },
    {
        "original": "def softplus_and_shift(x, shift=1e-5, name=None): \n  with tf.compat.v1.name_scope(name, 'softplus_and_shift', [x, shift]):\n    x = tf.convert_to_tensor(value=x, name='x')\n    y = tf.nn.softplus(x)\n    if shift is not None:\n      y += shift\n    return y",
        "rewrite": "def softplus_and_shift(x, shift=1e-5, name=None): \n  with tf.name_scope(name, 'softplus_and_shift', [x, shift]):\n    x = tf.convert_to_tensor(x, name='x')\n    y = tf.nn.softplus(x)\n    if shift is not None:\n      y += shift\n    return y"
    },
    {
        "original": "def add_pie_chart(self, data, cursor, width, height, title=None, data_type=\"raw\", fill_colors=None, labels=False, background=None, legend=None): \r\n        save_draw_color = self.draw_color\r\n        save_fill_color = self.fill_color\r\n\r\n        chart = PDFPieChart(self.session, self.page, data, cursor, width, height, title, data_type, fill_colors, labels, background, legend)\r\n\r\n        self.set_draw_color(save_draw_color)\r\n        self.set_fill_color(save_fill_color)",
        "rewrite": "def add_pie_chart(self, data, cursor, width, height, title=None, data_type=\"raw\", fill_colors=None, labels=False, background=None, legend=None): \n        save_draw_color = self.draw_color\n        save_fill_color = self.fill_color\n\n        chart = PDFPieChart(self.session, self.page, data, cursor, width, height, title, data_type, fill_colors, labels, background, legend)\n\n        self.set_draw_color(save_draw_color)\n        self.set_fill_color(save_fill_color)"
    },
    {
        "original": "def post(self, path='', **params): \n        url = ensure_trailing_slash(self.url + path.lstrip('/'))\n        return self._json_request('post', url, data=json.dumps(params),\n                                  headers={'Content-Type': 'application/json'})",
        "rewrite": "def post(self, path='', **params):\n    url = ensure_trailing_slash(self.url + path.lstrip('/'))\n    return self._json_request('post', url, data=json.dumps(params),\n                              headers={'Content-Type': 'application/json'})"
    },
    {
        "original": "def show_all(self): \n        for ws in self.workspace.list().keys():\n            self.show_workspace(ws)\n            print(\"\\n\\n\")",
        "rewrite": "def show_all(self):\n        for ws in self.workspace.list().keys():\n            self.show_workspace(ws)\n            print(\"\\n\\n\")"
    },
    {
        "original": "def close(self): \n        self.state = 'closing'\n        if self.input:\n            self.input.close()\n            pass\n        if self.output:\n            self.output.close()\n            pass\n        self.state = 'disconnnected'\n        return",
        "rewrite": "def close(self):\n    self.state = 'closing'\n    if self.input:\n        self.input.close()\n    if self.output:\n        self.output.close()\n    self.state = 'disconnected'\n    return"
    },
    {
        "original": "def get_annotation_data_for_tier(self, id_tier): \n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_for_tier(id_tier)\n        a = self.tiers[id_tier][0]\n        return [(self.timeslots[a[b][0]], self.timeslots[a[b][1]], a[b][2])\n                for b in a]",
        "rewrite": "def get_annotation_data_for_tier(self, id_tier): \n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_for_tier(id_tier)\n        annotations = self.tiers[id_tier][0]\n        return [(self.timeslots[annotation[b][0]], self.timeslots[annotation[b][1]], annotation[b][2])\n                for b in annotations]"
    },
    {
        "original": "def _reshuffle(mat, shape): \n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),\n        (shape[3] * shape[1], shape[0] * shape[2]))",
        "rewrite": "def _reshuffle(mat, shape): \n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (2, 0, 1, 3)),\n        (shape[2] * shape[0], shape[1] * shape[3]))"
    },
    {
        "original": "def _prompt_started(self): \n        # Temporarily disable the maximum block count to permit undo/redo and\n        # to ensure that the prompt position does not change due to truncation.\n        self._control.document().setMaximumBlockCount(0)\n        self._control.setUndoRedoEnabled(True)\n\n        # Work around bug in QPlainTextEdit: input method is not re-enabled\n        # when read-only is disabled.\n        self._control.setReadOnly(False)\n        self._control.setAttribute(QtCore.Qt.WA_InputMethodEnabled, True)\n\n        if not self._reading:\n            self._executing = False\n",
        "rewrite": "def _prompt_started(self):\n    self._control.document().setMaximumBlockCount(0)\n    self._control.setUndoRedoEnabled(True)\n    \n    self._control.setReadOnly(False)\n    self._control.setAttribute(QtCore.Qt.WA_InputMethodEnabled, True)\n    \n    if not self._reading:\n        self._executing = False"
    },
    {
        "original": "def _check_uninferable_call(self, node): \n        if not isinstance(node.func, astroid.Attribute):\n            return\n\n        # Look for properties. First, obtain\n        # the lhs of the Attribute node and search the attribute\n        # there. If that attribute is a property or a subclass of properties,\n        # then most likely it's not callable.\n\n        # TODO: since astroid doesn't understand descriptors very well\n        # we will not handle them here, right now.\n\n    ",
        "rewrite": "def _check_uninferable_call(self, node): \n    if not isinstance(node.func, astroid.Attribute):\n        return\n\n    lhs = node.func.expr\n    attribute = node.func.attr\n\n    if isinstance(lhs, astroid.Name):\n        obj = self.lookup(lhs.name)\n        if hasattr(obj, attribute) and isinstance(getattr(obj, attribute), property):\n            return True\n\n    return False"
    },
    {
        "original": "def _translate_shortcut(self, name): \n\n        if isinstance(name, int):\n            return True, self._root_instance.f_wildcard('$', name)\n\n        if name.startswith('run_') or name.startswith('r_'):\n            split_name = name.split('_')\n            if len(split_name) == 2:\n                index = split_name[1]\n                if index.isdigit():\n                    return True, self._root_instance.f_wildcard('$', int(index))\n ",
        "rewrite": "def _translate_shortcut(self, name): \n\n    if isinstance(name, int):\n        return True, self._root_instance.f_wildcard('$', name)\n\n    if name.startswith('run_') or name.startswith('r_'):\n        split_name = name.split('_')\n        if len(split_name) == 2:\n            index = split_name[1]\n            if index.isdigit():\n                return True, self._root_instance.f_wildcard('$', int(index))"
    },
    {
        "original": "def match(self): \n\n        # We initate this variable which gonna contain the returned data\n        result = []\n\n        # We compile the regex string\n        to_match = comp(self.regex)\n\n        # In case we have to use the implementation of ${BASH_REMATCH} we use\n        # re.findall otherwise, we use re.search\n        if self.rematch:  # pylint: disable=no-member\n            pre_result = to_match.findall(self.data)\n        else:\n      ",
        "rewrite": "def match(self): \n        result = []\n        to_match = comp(self.regex)\n        \n        if self.rematch:\n            pre_result = to_match.findall(self.data)\n        else:"
    },
    {
        "original": "def add_years(dateobj, nb_years): \n    year = dateobj.year + nb_years\n    lastday = monthrange(year, dateobj.month)[1]\n    return dateobj.replace(year=year, day=min(lastday, dateobj.day))",
        "rewrite": "from calendar import monthrange\n\ndef add_years(dateobj, nb_years):\n    year = dateobj.year + nb_years\n    lastday = monthrange(year, dateobj.month)[1]\n    return dateobj.replace(year=year, day=min(lastday, dateobj.day))"
    },
    {
        "original": "def combine(self): \n        aliases = None\n        if self.config.paths:\n            aliases = PathAliases(self.file_locator)\n            for paths in self.config.paths.values():\n                result = paths[0]\n                for pattern in paths[1:]:\n                    aliases.add(pattern, result)\n        self.data.combine_parallel_data(aliases=aliases)",
        "rewrite": "def combine(self): \n    aliases = None\n    if self.config.paths:\n        aliases = PathAliases(self.file_locator)\n        for paths in self.config.paths.values():\n            result = paths[0]\n            for pattern in paths[1:]:\n                aliases.add(pattern, result)\n    self.data.combine_parallel_data(aliases=aliases)"
    },
    {
        "original": "def delete(self, blocksize=100): \n\n        from .columns import MODELS_REFERENCED\n        if not self._model._no_fk or self._model._namespace in MODELS_REFERENCED:\n            raise QueryError(\"Can't delete entities of models with foreign key relationships\")\n\n        de = []\n        i = 0\n        for result in self.iter_result(pagesize=blocksize):\n            de.append(result)\n            i += 1\n            if i >= blocksize:\n      ",
        "rewrite": "def delete(self, blocksize=100):\n    from .columns import MODELS_REFERENCED\n    if not self._model._no_fk or self._model._namespace in MODELS_REFERENCED:\n        raise QueryError(\"Can't delete entities of models with foreign key relationships\")\n\n    de = []\n    i = 0\n    for result in self.iter_result(pagesize=blocksize):\n        de.append(result)\n        i += 1\n        if i >= blocksize:"
    },
    {
        "original": "def _lock(self, name, client_id, request_id): \n        if name in self._locks:\n            other_client_id, other_request_id, lock_time = self._locks[name]\n            if other_client_id == client_id:\n                response = (self.LOCK_ERROR + self.DELIMITER +\n                            'Re-request of lock `%s` (old request id `%s`) by `%s` '\n                     ",
        "rewrite": "def _lock(self, name, client_id, request_id): \n        if name in self._locks:\n            other_client_id, other_request_id, lock_time = self._locks(name)\n            if other_client_id == client_id:\n                response = (self.LOCK_ERROR + self.DELIMITER +\n                            'Re-request of lock `{}` (old request id `{}`) by `{}`. '\n                            'No need to explain. Just write code'.format(name, other_request_id, client_id)"
    },
    {
        "original": "def models(cls, api_version=DEFAULT_API_VERSION): \n        if api_version == '2015-08-01':\n            from .v2015_08_01 import models\n            return models\n        elif api_version == '2017-04-01':\n            from .v2017_04_01 import models\n            return models\n        elif api_version == '2018-01-01-preview':\n            from .v2018_01_01_preview import models\n            return models\n       ",
        "rewrite": "def models(cls, api_version=DEFAULT_API_VERSION):\n    if api_version == '2015-08-01':\n        from .v2015_08_01 import models\n        return models\n    elif api_version == '2017-04-01':\n        from .v2017_04_01 import models\n        return models\n    elif api_version == '2018-01-01-preview':\n        from .v2018_01_01_preview import models\n        return models"
    },
    {
        "original": "def add_creg(self, creg): \n        if not isinstance(creg, ClassicalRegister):\n            raise DAGCircuitError(\"not a ClassicalRegister instance.\")\n        if creg.name in self.cregs:\n            raise DAGCircuitError(\"duplicate register %s\" % creg.name)\n        self.cregs[creg.name] = creg\n        for j in range(creg.size):\n            self._add_wire((creg, j))",
        "rewrite": "def add_creg(self, creg): \n    if not isinstance(creg, ClassicalRegister):\n        raise DAGCircuitError(\"not a ClassicalRegister instance.\")\n    \n    if creg.name in self.cregs:\n        raise DAGCircuitError(\"duplicate register %s\" % creg.name)\n    \n    self.cregs[creg.name] = creg\n    \n    for j in range(creg.size):\n        self._add_wire((creg, j))"
    },
    {
        "original": "def get_main_code(self, as_filename=False, kernel_function_name='kernel'): \n        # TODO produce nicer code, including help text and other \"comfort features\".\n        assert self.kernel_ast is not None, \"AST does not exist, this could be due to running \" \\\n                                            \"based on a kernel description rather than code.\"\n\n        fp, already_available = self._get_intermediate_file('main.c',\n                  ",
        "rewrite": "def get_code(self, as_file_name=False, kernel_function_name = 'kernel'):\n    assert self.kernel_ast is not None, \"\"AST does not exist. Probably due to running \n    \"on kernel description rather than code.\"\n\n    fp, already_available = self._get_intermediate_file('main.c', \". No need to explain. Just write code:\")"
    },
    {
        "original": "def get_properties(self): \n        try:\n            self.entity = self._get_entity()\n            self.properties = dict(self.entity)\n            if hasattr(self.entity, 'requires_session'):\n                self.requires_session = self.entity.requires_session\n            return self.properties\n        except AzureServiceBusResourceNotFound:\n            raise ServiceBusResourceNotFound(\"Specificed queue does not exist.\")\n        except azure.common.AzureHttpError:\n         ",
        "rewrite": "raise ServiceBusResourceNotFound(\"Error in accessing Azure Service Bus resource\")"
    },
    {
        "original": "def _swap_ops_from_edge(edge, layout): \n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    # TODO shouldn't be making other nodes not by the DAG!!\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]",
        "rewrite": "def _swap_ops_from_edge(edge, layout): \n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]"
    },
    {
        "original": "def get_arguments(self): \n        ApiCli.get_arguments(self)\n        if self.args.metricName is not None:\n            self.metricName = self.args.metricName\n\n        if self.args.measurement is not None:\n            self.measurement = self.args.measurement\n\n        if self.args.source is not None:\n            self.source = self.args.source\n        else:\n            self.source = socket.gethostname()\n\n        if self.args.timestamp is not None:\n     ",
        "rewrite": "def get_arguments(self): \n        ApiCli.get_arguments(self)\n        self.metricName = self.args.metricName if self.args.metricName is not None else None\n        self.measurement = self.args.measurement if self.args.measurement is not None else None\n        self.source = self.args.source if self.args.source is not None else socket.gethostname()\n        self.timestamp = self.args.timestamp if self.args.timestamp is not None else None"
    },
    {
        "original": "def subscribe_to_candles(self, pair, timeframe=None, **kwargs): \n\n        valid_tfs = ['1m', '5m', '15m', '30m', '1h', '3h', '6h', '12h', '1D',\n                     '7D', '14D', '1M']\n        if timeframe:\n            if timeframe not in valid_tfs:\n                raise ValueError(\"timeframe must be any of %s\" % valid_tfs)\n        else:\n            timeframe = '1m'\n        identifier = ('candles',",
        "rewrite": "def subscribe_to_candles(self, pair, timeframe='1m', **kwargs):\n    valid_tfs = ['1m', '5m', '15m', '30m', '1h', '3h', '6h', '12h', '1D',\n                 '7D', '14D', '1M']\n    if timeframe not in valid_tfs:\n        raise ValueError(\"timeframe must be any of %s\" % valid_tfs)\n    identifier = ('candles',)"
    },
    {
        "original": "def unset_qiskit_logger(): \n    qiskit_logger = logging.getLogger('qiskit')\n    for handler in qiskit_logger.handlers:\n        qiskit_logger.removeHandler(handler)",
        "rewrite": "def unset_qiskit_logger():\n    qiskit_logger = logging.getLogger('qiskit')\n    handlers = qiskit_logger.handlers[:]\n    for handler in handlers:\n        qiskit_logger.removeHandler(handler)"
    },
    {
        "original": "def push(self, lines): \n        if self.input_mode == 'cell':\n            self.reset()\n        \n        self._store(lines)\n        source = self.source\n\n        # Before calling _compile(), reset the code object to None so that if an\n        # exception is raised in compilation, we don't mislead by having\n        # inconsistent code/source attributes.\n        self.code, self._is_complete = None, None\n\n        # Honor termination lines properly\n ",
        "rewrite": "def push(self, lines):\n    if self.input_mode == 'cell':\n        self.reset()\n    \n    self._store(lines)\n    source = self.source\n\n    self.code, self._is_complete = None, None"
    },
    {
        "original": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        local_buffer = utils.BytearrayStream()\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self._common_template_attribute is not None:\n                self._common_template_attribute.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n        else:\n      ",
        "rewrite": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        local_buffer = utils.BytearrayStream()\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0 and self._common_template_attribute is not None:\n            self._common_template_attribute.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )"
    },
    {
        "original": "def get_alias(self, index=None, name=None, params=None): \n        _, result = yield self.transport.perform_request(\n            'GET', _make_path(index, '_alias', name), params=params)\n        raise gen.Return(result)",
        "rewrite": "def get_alias(self, index=None, name=None, params=None): \n    _, result = yield self.transport.perform_request('GET', _make_path(index, '_alias', name), params=params)\n    raise gen.Return(result)"
    },
    {
        "original": "def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False): \n    check_input_checkpoint(input_checkpoint)\n\n    output_node_names = output_node_names_string_as_list(output_node_names)\n\n    with tf.Session() as sess:\n        restore_from_checkpoint(sess, input_checkpoint)\n        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)",
        "rewrite": "def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):\n    check_input_checkpoint(input_checkpoint)\n\n    output_node_names = output_node_names_string_as_list(output_node_names)\n\n    with tf.Session() as sess:\n        restore_from_checkpoint(sess, input_checkpoint)\n        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)"
    },
    {
        "original": "def freeze_matrix(script, all_layers=False): \n    filter_xml = ''.join([\n        '  <filter name=\"Freeze Current Matrix\">\\n',\n        '    <Param name=\"allLayers\" ',\n        'value=\"%s\" ' % str(all_layers).lower(),\n        'description=\"Apply to all visible Layers\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None",
        "rewrite": "def freeze_matrix(script, all_layers=False):\n    filter_xml = ''.join([\n        '  <filter name=\"Freeze Current Matrix\">\\n',\n        '    <Param name=\"allLayers\" ',\n        'value=\"%s\" ' % str(all_layers).lower(),\n        'description=\"Apply to all visible Layers\" ',\n        'type=\"RichBool\" ',\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None"
    },
    {
        "original": "def load_common(model_cls, data): \n    obj = model_cls(**data)\n    db.session.add(obj)\n    db.session.commit()",
        "rewrite": "def load_common(model_cls, data):\n    obj = model_cls(**data)\n    for key, value in data.items():\n        setattr(obj, key, value)\n    db.session.add(obj)\n    db.session.commit()"
    },
    {
        "original": "def tokenize(self, split): \n        fr = H2OFrame._expr(expr=ExprNode(\"tokenize\", self, split))\n        return fr",
        "rewrite": "def tokenize(self, split): \n        fr = H2OFrame._expr(expr=ExprNode(\"tokenize\", self, split))\n        return fr"
    },
    {
        "original": "def check(self): \n\n        status = _checkContainerStatus(self.sparkContainerID,\n                                       self.hdfsContainerID,\n                                       sparkNoun='worker',\n                                     ",
        "rewrite": "def check(self): \n    status = _checkContainerStatus(self.sparkContainerID,\n                                   self.hdfsContainerID,\n                                   sparkNoun='worker')"
    },
    {
        "original": "def changes(self, **kwargs): \n        path = self._get_id_path('changes')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response",
        "rewrite": "def changes(self, **kwargs):\n    path = self._get_id_path('changes')\n    \n    response = self._GET(path, kwargs)\n    self._set_attrs_to_values(response)\n    return response"
    },
    {
        "original": "def read_history_report(path, steps, x_name = None): \n  data = pd.read_csv(path, delim_whitespace = True)\n  if x_name != None:\n    data[x_name] = data.X\n    del data[\"X\"]\n    \n  data[\"step\"] = 0\n  t = 0.\n  for i in range(len(steps)):\n    dt = steps[i].duration\n    loc = data[data.t == t].index\n    if len(loc) == 2:\n      data.loc[loc[1]:, \"step\"] = i\n    t += dt \n  return data",
        "rewrite": "def read_history_report(path, steps, x_name=None):\n    data = pd.read_csv(path, delim_whitespace=True)\n    \n    if x_name is not None:\n        data[x_name] = data[\"X\"]\n        del data[\"X\"]\n    \n    data[\"step\"] = 0\n    t = 0.\n    \n    for i in range(len(steps)):\n        dt = steps[i].duration\n        loc = data[data[\"t\"] == t].index\n        \n        if len(loc) == 2:\n            data.loc[loc[1]:, \"step\"] = i\n        \n        t += dt\n    \n    return data"
    },
    {
        "original": "def api_request(self, method, endpoint, data=None, *args, **kwargs): \n        session = self._get_session()\n\n        api_root = 'https://api.heroku.com'\n        url = api_root + endpoint\n\n        if data:\n            data = json.dumps(data)\n\n        response = session.request(method, url, data=data, *args, **kwargs)\n\n        if not response.ok:\n            try:\n                message = response.json().get('message')\n           ",
        "rewrite": "def api_request(self, method, endpoint, data=None, *args, **kwargs): \n    session = self._get_session()\n    \n    api_root = 'https://api.heroku.com'\n    url = api_root + endpoint\n    \n    if data:\n        data = json.dumps(data)\n        \n    response = session.request(method, url, data=data, *args, **kwargs)\n    \n    if not response.ok:\n        try:\n            message = response.json().get('message')"
    },
    {
        "original": "def file(self): \n\n        # We get, format, filter, clean the list to test.\n        list_to_test = self._file_list_to_test_filtering()\n\n        if PyFunceble.CONFIGURATION[\"idna_conversion\"]:\n            # We have to convert domains to idna.\n\n            # We convert if we need to convert.\n            list_to_test = domain2idna(list_to_test)\n\n            if PyFunceble.CONFIGURATION[\"hierarchical_sorting\"]:\n                # The hierarchical sorting is desired by the user.\n\n",
        "rewrite": "def file(self): \n\n        list_to_test = self._file_list_to_test_filtering()\n\n        if PyFunceble.CONFIGURATION[\"idna_conversion\"]:\n            list_to_test = domain2idna(list_to_test)\n\n            if PyFunceble.CONFIGURATION[\"hierarchical_sorting\"]:"
    },
    {
        "original": " \n    succeed, fail = [], []\n\n    for x in collection:\n        if condition(x):\n            succeed.append(x)\n        else:\n            fail.append(x)\n\n    return succeed, fail",
        "rewrite": "success_list, fail_list = [], []\n\nfor item in collection:\n    if condition(item):\n        success_list.append(item)\n    else:\n        fail_list.append(item)\n\nreturn success_list, fail_list"
    },
    {
        "original": "def initialize(self, params, qubits): \n    if isinstance(qubits, QuantumRegister):\n        qubits = qubits[:]\n    else:\n        qubits = _convert_to_bits([qubits], [qbit for qreg in self.qregs for qbit in qreg])[0]\n    return self.append(Initialize(params), qubits)",
        "rewrite": "def initialize(self, params, qubits): \n    if isinstance(qubits, QuantumRegister):\n        qubits = qubits[:]\n    else:\n        qubits = _convert_to_bits([qubits], [qbit for qreg in self.qregs for qbit in qreg])[0]\n    \n    return self.append(Initialize(params), qubits)"
    },
    {
        "original": "def _kl_pareto_pareto(a, b, name=None): \n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n    ",
        "rewrite": "def kl_pareto_pareto(a, b, name=None): \n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +"
    },
    {
        "original": "def type(self, atype): \n        for char in atype:\n            assert char in 'xtyr', 'Invalid axes type: %s'%char\n        if not ',' in atype:\n            atype = ','.join(atype)\n        self['chxt'] = atype\n        return self.parent",
        "rewrite": "def set_type(self, atype):\n    for char in atype:\n        assert char in 'xtyr', 'Invalid axes type: %s' % char\n    if ',' not in atype:\n        atype = ','.join(atype)\n    self['chxt'] = atype\n    return self.parent"
    },
    {
        "original": "def transformSkyCoordinates(self, phi, theta): \n        r=ones_like(phi)\n        x, y, z = sphericalToCartesian(r, phi, theta)\n        xrot, yrot, zrot = self.transformCartesianCoordinates(x, y, z)\n        r, phirot, thetarot = cartesianToSpherical(xrot, yrot, zrot)\n        return phirot, thetarot",
        "rewrite": "def transformSkyCoordinates(self, phi, theta): \n    r = np.ones_like(phi)\n    x, y, z = sphericalToCartesian(r, phi, theta)\n    xrot, yrot, zrot = self.transformCartesianCoordinates(x, y, z)\n    r, phirot, thetarot = cartesianToSpherical(xrot, yrot, zrot)\n    return phirot, thetarot"
    },
    {
        "original": "def make_ar_transition_matrix(coefficients): \n\n  top_row = tf.expand_dims(coefficients, -2)\n  coef_shape = dist_util.prefer_static_shape(coefficients)\n  batch_shape, order = coef_shape[:-1], coef_shape[-1]\n  remaining_rows = tf.concat([\n      tf.eye(order - 1, dtype=coefficients.dtype, batch_shape=batch_shape),\n      tf.zeros(tf.concat([batch_shape, (order - 1, 1)], axis=0),\n               dtype=coefficients.dtype)\n  ], axis=-1)\n  ar_matrix = tf.concat([top_row, remaining_rows], axis=-2)\n  return ar_matrix",
        "rewrite": "def make_ar_transition_matrix(coefficients):\n    top_row = tf.expand_dims(coefficients, -2)\n    coef_shape = dist_util.prefer_static_shape(coefficients)\n    batch_shape, order = coef_shape[:-1], coef_shape[-1]\n    remaining_rows = tf.concat([\n        tf.eye(order - 1, dtype=coefficients.dtype, batch_shape=batch_shape),\n        tf.zeros(tf.concat([batch_shape, (order - 1, 1)], axis=0), dtype=coefficients.dtype)\n    ], axis=-1)\n    ar_matrix = tf.concat([top_row, remaining_rows], axis=-2)\n    return ar_matrix"
    },
    {
        "original": "def verboseRead(self, alphabet, context='', skipExtra=False): \n        #TODO 2: verbosity level, e.g. show only codes and maps in header\n        stream = self.stream\n        pos = stream.pos\n        if skipExtra:\n            length, symbol = alphabet.readTuple(stream)\n            extraBits, extra = 0, None\n        else:\n            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(\n                stream)\n  ",
        "rewrite": "def verboseRead(self, alphabet, context='', skipExtra=False):\n    stream = self.stream\n    pos = stream.pos\n    if skipExtra:\n        length, symbol = alphabet.readTuple(stream)\n        extraBits, extra = 0, None\n    else:\n        length, symbol, extraBits, extra = alphabet.readTupleAndExtra(stream)"
    },
    {
        "original": "def make_cache_key(*args, **kwargs): \n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')",
        "rewrite": "def make_cache_key(request, *args, **kwargs):\n    path = request.path\n    args = str(hash(frozenset(request.args.items())))\n    return (path + args).encode('ascii', 'ignore')"
    },
    {
        "original": "def require(self, key): \n        value = self.get(key)\n        if not value:\n            raise ValueError('\"{}\" is empty.'.format(key))\n        return value",
        "rewrite": "def require(self, key): \n    value = self.get(key)\n    if not value:\n        raise ValueError(f'\"{key}\" is empty.')\n    return value"
    },
    {
        "original": "def _step_decorator_args(self, decorator): \n        args = decorator.call.value\n        step = None\n        if len(args) == 1:\n            try:\n                step = args[0].value.to_python()\n            except (ValueError, SyntaxError):\n                pass\n            if isinstance(step, six.string_types + (list,)):\n                return step\n",
        "rewrite": "def _step_decorator_args(self, decorator): \n    args = decorator.call.value\n    step = None\n    if len(args) == 1:\n        try:\n            step = args[0].value.to_python()\n        except (ValueError, SyntaxError):\n            pass\n        if isinstance(step, six.string_types + (list,)):\n            return step"
    },
    {
        "original": "def _get_backend_instance(self, backend_cls): \n        # Verify that the backend can be instantiated.\n        try:\n            backend_instance = backend_cls(provider=self)\n        except Exception as err:\n            raise QiskitError('Backend %s could not be instantiated: %s' %\n                              (backend_cls, err))\n\n        return backend_instance",
        "rewrite": "def _get_backend_instance(self, backend_cls):\n    try:\n        backend_instance = backend_cls(provider=self)\n    except Exception as err:\n        raise QiskitError('Backend {} could not be instantiated: {}'.format(backend_cls, err))\n    \n    return backend_instance"
    },
    {
        "original": "def scan(self, *key_ranges, **kwargs): \n        for hit in self._scan(*key_ranges, **kwargs):\n            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])",
        "rewrite": "def scan(self, *key_ranges, **kwargs):\n    for hit in self._scan(*key_ranges, **kwargs):\n        yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])"
    },
    {
        "original": "def obj_overhead(self): \n        overhead = [\n            self,\n            self._resulting_events,\n            self._events_list,\n            self._process\n        ]\n        overhead_count = _get_object_count_by_type(overhead)\n        # One for reference to __dict__ and one for reference to\n        # the current module.\n        overhead_count[dict] += 2\n        return overhead_count",
        "rewrite": "def obj_overhead(self): \n    overhead = [\n        self,\n        self._resulting_events,\n        self._events_list,\n        self._process\n    ]\n    overhead_count = _get_object_count_by_type(overhead)\n    overhead_count[dict] += 2\n    return overhead_count"
    },
    {
        "original": "def init_role(self, role_name, role_vms, role_perms): \n        pvms = self.get_session.query(sqla_models.PermissionView).all()\n        pvms = [p for p in pvms if p.permission and p.view_menu]\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:\n ",
        "rewrite": "def init_role(self, role_name, role_vms, role_perms):\n        pvms = self.get_session.query(sqla_models.PermissionView).filter(sqla_models.PermissionView.permission != None, sqla_models.PermissionView.view_menu != None).all()\n\n        role = self.find_role(role_name)\n        if not role:\n            role = self.add_role(role_name)\n\n        if len(role.permissions) == 0:\n            self.log.info('Initializing permissions for role:%s in the database.', role_name)\n            role_pvms = set()\n            for pvm in pvms:"
    },
    {
        "original": "def get_system_per_cpu_times(): \n    ret = []\n    for cpu_t in _psutil_bsd.get_system_per_cpu_times():\n        user, nice, system, idle, irq = cpu_t\n        item = _cputimes_ntuple(user, nice, system, idle, irq)\n        ret.append(item)\n    return ret",
        "rewrite": "def get_system_per_cpu_times(): \n    ret = []\n    for cpu_t in _psutil_bsd.get_system_per_cpu_times():\n        user, nice, system, idle, irq = cpu_t\n        item = _cputimes_ntuple(user, nice, system, idle, irq)\n        ret.append(item)\n    return ret"
    },
    {
        "original": "def cycle(self): \n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()",
        "rewrite": "def cycle(self):\n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()"
    },
    {
        "original": "def delete_policy_id(self, id): \n        res = requests.delete(self.url + '/api/policies/{}'.format(id), headers=self.hdrs, verify=self.ssl_verify)\n        return self._request_result(res)",
        "rewrite": "def delete_policy_id(self, id):\n    res = requests.delete(f\"{self.url}/api/policies/{id}\", headers=self.hdrs, verify=self.ssl_verify)\n    return self._request_result(res)"
    },
    {
        "original": "def env(section, map_files, phusion, phusion_path, quiet, edit, create): \n    try:\n        logger.debug('Running env command')\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        if edit:\n            conf.edit(create=create)\n        else:\n            env_vars = conf.get_envfile().as_dict()\n            if env_vars.get('S3CONF_MAP') and map_files:\n                conf.download_mapping(env_vars.get('S3CONF_MAP'))\n",
        "rewrite": "def env(section, map_files, phusion, phusion_path, quiet, edit, create):\n    try:\n        logger.debug('Running env command')\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        if edit:\n            conf.edit(create=create)\n        else:\n            env_vars = conf.get_envfile().as_dict()\n            if env_vars.get('S3CONF_MAP') and map_files:\n                conf.download_mapping(env_vars.get('S3CONF_MAP'))"
    },
    {
        "original": "def find_recipes(folders, pattern=None, base=None):     \n    # If the user doesn't provide a list of folders, use $PWD\n    if folders is None:\n        folders = os.getcwd()\n\n    if not isinstance(folders,list):\n        folders = [folders]\n\n    manifest = dict()\n    for base_folder in folders:\n\n        # If we find a file, return the one file\n        custom_pattern = None\n        if os.path.isfile(base_folder):  # updates manifest\n            manifest = find_single_recipe(filename=base_folder,\n    ",
        "rewrite": "import os\n\ndef find_recipes(folders, pattern=None, base=None):\n    if folders is None:\n        folders = os.getcwd()\n    \n    if not isinstance(folders, list):\n        folders = [folders]\n\n    manifest = dict()\n    for base_folder in folders:\n        custom_pattern = None\n        if os.path.isfile(base_folder):  \n            manifest = find_single_recipe(filename=base_folder)"
    },
    {
        "original": " \n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)",
        "rewrite": "flat_from = tf.nest.flatten(from_structure)\nflat_to = tf.nest.flatten(to_structure)\nif len(flat_from) == 1:\n    flat_from *= len(flat_to)\nreturn tf.nest.pack_sequence_as(to_structure, flat_from)"
    },
    {
        "original": "def unpad(padded_data, block_size, style='pkcs7'): \n\n    pdata_len = len(padded_data)\n    if pdata_len % block_size:\n        raise ValueError(\"Input data is not padded\")\n    if style in ('pkcs7', 'x923'):\n        padding_len = bord(padded_data[-1])\n        if padding_len<1 or padding_len>min(block_size, pdata_len):\n            raise ValueError(\"Padding is incorrect.\")\n        if style == 'pkcs7':\n            if padded_data[-padding_len:]!=bchr(padding_len)*padding_len:\n                raise ValueError(\"PKCS#7 padding is incorrect.\")\n        else:\n",
        "rewrite": "def unpad(padded_data, block_size, style='pkcs7'):\n    pdata_len = len(padded_data)\n    if pdata_len % block_size:\n        raise ValueError(\"Input data is not padded\")\n    if style in ('pkcs7', 'x923'):\n        padding_len = ord(padded_data[-1])\n        if padding_len < 1 or padding_len > min(block_size, pdata_len):\n            raise ValueError(\"Padding is incorrect.\")\n        if style == 'pkcs7':\n            if padded_data[-padding_len:] != bchr(padding_len) * padding_len:\n                raise ValueError(\"PKCS#7 padding is incorrect.\")"
    },
    {
        "original": "def chop(array, epsilon=1e-10): \n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[abs(ret.real) < epsilon] = 0.0\n        ret.imag[abs(ret.imag) < epsilon] = 0.0\n    return ret",
        "rewrite": "```python\ndef chop(array, epsilon=1e-10):\n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[np.abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[np.abs(ret.real) < epsilon] = 0.0\n        ret.imag[np.abs(ret.imag) < epsilon] = 0.0\n    return ret\n```"
    },
    {
        "original": "def Rock(*args, **kwargs): \n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)",
        "rewrite": "def Rock(*args, **kwargs):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)"
    },
    {
        "original": "def offset(self): \n        if callable(self._offset):\n            return util.WatchingList(self._offset(*(self.widget.pos+self.widget.size)),self._wlredraw_offset)\n        else:\n            return util.WatchingList(self._offset,self._wlredraw_offset)",
        "rewrite": "def offset(self): \n    if callable(self._offset):\n        return util.WatchingList(self._offset(*(self.widget.pos+self.widget.size)), self._wlredraw_offset)\n    else:\n        return util.WatchingList(self._offset, self._wlredraw_offset)"
    },
    {
        "original": "def update_internal_boundary_x_y (self, solution_array): \n        nsd_ = self.nsd\n        dtype = solution_array.dtype\n        if nsd_!=len(self.in_lower_buffers) | nsd_!=len(self.out_lower_buffers):\n            print(\"Buffers for communicating with lower neighbors not ready\")\n            return\n        if nsd_!=len(self.in_upper_buffers) | nsd_!=len(self.out_upper_buffers):\n            print(\"Buffers for communicating with upper neighbors not ready\")\n            return\n\n        loc_nx = self.subd_hi_ix[0]-self.subd_lo_ix[0]\n        loc_ny =",
        "rewrite": "self.subd_hi_ix[1]-self.subd_lo_ix[1]\n\n        for ix in range(self.nsd):\n            if ix==0:\n                in_lower_buffers = self.in_lower_buffers[ix]\n                out_lower_buffers = self.out_lower_buffers[ix]\n                in_upper_buffers = self.in_upper_buffers[ix]\n                out_upper_buffers = self.out_upper_buffers[ix]\n                solution_array[self.subd_lo_ix[0]:self.subd_hi_ix[0],self.subd_lo_ix[1]:self.subd_hi_ix[1]] = self._boundary_x_y_(\n                    solution_array[self.subd_lo_ix[0]:self.subd_hi_ix[0],self.subd_lo_ix[1]:self.subd_hi_ix[1]],loc_nx,loc_ny,\n                    in_lower_buffers, out_lower_buffers, in_upper_buffers, out_upper_buffers)\n            else:\n                in_lower_buffers = self.in_lower_buffers[ix]\n                out_lower_buffers = self.out_lower_buffers[ix]\n                in_upper_buffers = self.in_upper_buffers[ix]\n                out_upper_buffers = self.out_upper_buffers[ix]\n                # NOTE THAT WE FLIP THE VARIABLES loc_nx and loc_ny HERE\n                solution_array[self.subd_lo_ix[0]:self.subd_hi_ix[0],self.subd_lo_ix[1]:self.subd_hi_ix[1]] = self._boundary_x_y_(\n                    solution_array[self.subd_lo_ix[0]:self.subd_hi_ix[0],self.subd_lo_ix[1]:self.subd_hi_ix[1]],loc_ny,loc_nx,\n                    in_lower_buffers, out_lower_buffers, in_upper_buffers, out_upper_buffers)"
    },
    {
        "original": "def add_card(self, query_params=None): \n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)",
        "rewrite": "def add_card(self, query_params=None): \n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)"
    },
    {
        "original": " \n  # Running with multiple chains introduces an extra batch dimension. In\n  # general we also need to pad the observed time series with a matching batch\n  # dimension.\n  #\n  # For example, suppose our model has batch shape [3, 4] and\n  # the observed time series has shape `concat([[5], [3, 4], [100])`,\n  # corresponding to `sample_shape`, `batch_shape`, and `num_timesteps`\n  # respectively. The model will produce distributions with batch shape\n  # `concat([chain_batch_shape, [3, 4]])`, so we pad `observed_time_series` to\n  # have matching shape `[5, 1, 3, 4, 100]`, where the added `1` dimension\n  # between the sample and batch shapes will broadcast to `chain_batch_shape`.\n\n  [  # Extract mask and guarantee `event_ndims=2`.\n   ",
        "rewrite": "# Running with multiple chains introduces an extra batch dimension. In\n# general we also need to pad the observed time series with a matching batch\n# dimension.\n#\n# For example, suppose our model has batch shape [3, 4] and\n# the observed time series has shape `concat([[5], [3, 4], [100])`,\n# corresponding to `sample_shape`, `batch_shape`, and `num_timesteps`\n# respectively. The model will produce distributions with batch shape\n# `concat([chain_batch_shape, [3, 4]])`, so we pad `observed_time_series` to\n# have matching shape `[5, 1, 3, 4, 100]`, where the added `1` dimension\n# between the sample and batch shapes will broadcast to `chain_batch_shape`.\n\n[  # Extract mask and guarantee `event_ndims=2`."
    },
    {
        "original": "def _add_attachments(self): \n        num_attached = 0\n        if self.attachments:\n            if isinstance(self.attachments, str):\n                self.attachments = [self.attachments]\n\n            for item in self.attachments:\n                doc = MIMEApplication(open(item, \"rb\").read())\n                doc.add_header(\"Content-Disposition\", \"attachment\", filename=item)\n                self.message.attach(doc)\n      ",
        "rewrite": "def _add_attachments(self): \n    num_attached = 0\n    if self.attachments:\n        if isinstance(self.attachments, str):\n            self.attachments = [self.attachments]\n\n        for item in self.attachments:\n            doc = MIMEApplication(open(item, \"rb\").read())\n            doc.add_header(\"Content-Disposition\", \"attachment\", filename=item)\n            self.message.attach(doc)"
    },
    {
        "original": "def _serialize(xp_ast): \n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield(tok)\n    elif isinstance(xp_ast, str):\n        yield(repr(xp_ast))",
        "rewrite": "def _serialize(xp_ast):\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield tok\n    elif isinstance(xp_ast, str):\n        yield repr(xp_ast)"
    },
    {
        "original": "def run_set_int(obj, arg, msg_on_error, min_value=None, max_value=None): \n    if '' == arg.strip():\n        obj.errmsg(\"You need to supply a number.\")\n        return\n    obj.debugger.settings[obj.name] = \\\n        get_an_int(obj.errmsg, arg, msg_on_error, min_value, max_value)\n    return obj.debugger.settings[obj.name]",
        "rewrite": "def run_set_int(obj, arg, msg_on_error, min_value=None, max_value=None): \n    if arg.strip() == '':\n        obj.errmsg(\"You need to supply a number.\")\n        return\n    obj.debugger.settings[obj.name] = get_an_int(obj.errmsg, arg, msg_on_error, min_value, max_value)\n    return obj.debugger.settings[obj.name]"
    },
    {
        "original": "def batches(dataset): \n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample():\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])\n\n  ",
        "rewrite": "import numpy as np\n\ndef batches(dataset): \n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample(BATCH_SIZE):\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])"
    },
    {
        "original": "def skip_module(*modules): \n    modules = (modules and isinstance(modules[0], list)) and \\\n              modules[0] or modules\n\n    for module in modules:\n        if not module in SKIPPED_MODULES:\n            SKIPPED_MODULES.append(module)\n    traceback.extract_tb = _new_extract_tb",
        "rewrite": "def skip_module(*modules): \n    modules = (modules and isinstance(modules[0], list)) and modules[0] or modules\n\n    for module in modules:\n        if module not in SKIPPED_MODULES:\n            SKIPPED_MODULES.append(module)\n    traceback.extract_tb = _new_extract_tb"
    },
    {
        "original": "def bot(self, id): \n        json = self.skype.conn(\"GET\", \"{0}/agents\".format(SkypeConnection.API_BOT), params={\"agentId\": id},\n                               auth=SkypeConnection.Auth.SkypeToken).json().get(\"agentDescriptions\", [])\n        return self.merge(SkypeBotUser.fromRaw(self.skype, json[0])) if json else None",
        "rewrite": "def bot(self, id): \n    json = self.skype.conn(\"GET\", f\"{SkypeConnection.API_BOT}/agents\", params={\"agentId\": id}, auth=SkypeConnection.Auth.SkypeToken).json().get(\"agentDescriptions\", [])\n    return self.merge(SkypeBotUser.fromRaw(self.skype, json[0])) if json else None"
    },
    {
        "original": "def read_byte_data(self, addr, cmd): \n        self._set_addr(addr)\n        res = SMBUS.i2c_smbus_read_byte_data(self._fd, ffi.cast(\"__u8\", cmd))\n        if res == -1:\n            raise IOError(ffi.errno)\n        return res",
        "rewrite": "def read_byte_data(self, addr, cmd): \n    self._set_addr(addr)\n    res = SMBUS.i2c_smbus_read_byte_data(self._fd, ffi.cast(\"__u8\", cmd))\n    if res == -1:\n        raise IOError(ffi.errno)\n    return res"
    },
    {
        "original": "def unregister(self, mimetype, processor): \n        if mimetype in self and processor in self[mimetype]:\n            self[mimetype].remove(processor)",
        "rewrite": "def unregister(self, mimetype, processor):\n    if mimetype in self and processor in self[mimetype]:\n        self[mimetype].remove(processor)"
    },
    {
        "original": "def validate_twilio(attr, value): \n    if attr in (\"from_\", \"to\"):\n        check_valid(\"Twilio\", attr, value, validus.isphone, \"phone number\")\n    elif attr in (\"attachments\"):\n        check_valid(\"Twilio\", attr, value, validus.isurl, \"url\")",
        "rewrite": "def validate_twilio(attr, value): \n    if attr in (\"from_\", \"to\"):\n        check_valid(\"Twilio\", attr, value, validus.isphone, \"phone number\")\n    elif attr in (\"attachments\"):\n        check_valid(\"Twilio\", attr, value, validus.isurl, \"url\")"
    },
    {
        "original": "def get_experiments(self, workspace_id): \r\n        api_path = self.EXPERIMENTS_URI_FMT.format(workspace_id)\r\n        return self._send_get_req(api_path)",
        "rewrite": "def get_experiments(self, workspace_id): \n    api_path = self.EXPERIMENTS_URI_FMT.format(workspace_id)\n    return self._send_get_req(api_path)"
    },
    {
        "original": "def inshape(shape, axes): \n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"axes not valid for an ndarray of shape: %s\" % str(shape))",
        "rewrite": "def inshape(shape, axes): \n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"Axes not valid for an ndarray of shape: %s\" % str(shape))"
    },
    {
        "original": "def read_parquet(cls, path, sc, bigdl_type=\"float\"): \n        return DistributedImageFrame(jvalue=callBigDlFunc(bigdl_type, \"readParquet\", path, sc))",
        "rewrite": "def read_parquet(cls, path, sc, bigdl_type=\"float\"): \n    return DistributedImageFrame(jvalue=callBigDLFunc(bigdl_type, \"readParquet\", path, sc))"
    },
    {
        "original": "def add_revoked(self, revoked): \n        copy = _lib.Cryptography_X509_REVOKED_dup(revoked._revoked)\n        _openssl_assert(copy != _ffi.NULL)\n\n        add_result = _lib.X509_CRL_add0_revoked(self._crl, copy)\n        _openssl_assert(add_result != 0)",
        "rewrite": "def add_revoked(self, revoked):\n    copy = _lib.Cryptography_X509_REVOKED_dup(revoked._revoked)\n    _openssl_assert(copy != _ffi.NULL)\n\n    add_result = _lib.X509_CRL_add0_revoked(self._crl, copy)\n    _openssl_assert(add_result != 0)"
    },
    {
        "original": "def reg_on_status(self, callable_object, *args, **kwargs): \n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'status', persistent, *args, **kwargs)\n        self.status_callbacks.append(event)\n        return event",
        "rewrite": "def reg_on_status(self, callable_object, *args, **kwargs):\n    persistent = kwargs.pop('persistent', False)\n    event = self._create_event(callable_object, 'status', persistent, *args, **kwargs)\n    self.status_callbacks.append(event)\n    return event"
    },
    {
        "original": " \n        new = Scope(sig=self._hsig.values(), state=self.state)\n        new &= sig\n        return new",
        "rewrite": "```python\nnew_scope = Scope(sig=self._hsig.values(), state=self.state)\nnew_scope &= sig\nreturn new_scope\n```"
    },
    {
        "original": "def size(self): \n        if isinstance(self._size,list) or isinstance(self._size,tuple):\n            s = self._size\n        elif callable(self._size):\n            w,h = self.submenu.size[:]\n            s = self._size(w,h)\n        else:\n            raise TypeError(\"Invalid size type\")\n        \n        s = s[:]\n        \n        if s[0]==-1:\n     ",
        "rewrite": "s = (s[0],s[1])"
    },
    {
        "original": "def delete(self, tag_id): \n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )",
        "rewrite": "def delete(self, tag_id): \n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )"
    },
    {
        "original": "def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint): \n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            endpoint = extra.get('webhook_endpoint', '')\n        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord '\n                    ",
        "rewrite": "def _get_webhook_endpoint(self, http_conn_id, webhook_endpoint): \n        if webhook_endpoint:\n            endpoint = webhook_endpoint\n        elif http_conn_id:\n            conn = self.get_connection(http_conn_id)\n            extra = conn.extra_dejson\n            endpoint = extra.get('webhook_endpoint', '')\n        else:\n            raise AirflowException('Cannot get webhook endpoint: No valid Discord webhook endpoint or connection id provided')"
    },
    {
        "original": "def get_info(self): \n        result = copy.copy(self.streams[0])\n        result.stat_log = self.stat_log\n        result.steps = []\n        result.ammo_file = ''\n        result.rps_schedule = None\n        result.ammo_count = 0\n        result.duration = 0\n\n        result.instances = 0\n        result.loadscheme = []\n        result.loop_count = 0\n\n        for stream in self.streams:\n            sec_no = 0\n",
        "rewrite": "def get_info(self):\n        result = copy.copy(self.streams[0])\n        result.stat_log = self.stat_log\n        result.steps = []\n        result.ammo_file = ''\n        result.rps_schedule = None\n        result.ammo_count = 0\n        result.duration = 0\n\n        result.instances = 0\n        result.loadscheme = []\n        result.loop_count = 0\n\n        for stream in self.streams:\n            sec_no = 0"
    },
    {
        "original": "def verify_declared_bit(self, obj): \n        # We are verifying gate args against the formal parameters of a\n        # gate prototype.\n        if obj.name not in self.current_symtab:\n            raise QasmError(\"Cannot find symbol '\" + obj.name\n                            + \"' in argument list for gate, line\",\n                            str(obj.line), 'file', obj.file)\n\n",
        "rewrite": "def verify_declared_bit(self, obj): \n    if obj.name not in self.current_symtab:\n        raise QasmError(f\"Cannot find symbol '{obj.name}' in argument list for gate, line {obj.line}, file {obj.file}\")"
    },
    {
        "original": "def Bin(self): \n    \n    err = _Bin(self.transit, self.limbdark, self.settings, self.arrays)\n    if err != _ERR_NONE: RaiseError(err)",
        "rewrite": "def Bin(self): \n    err = _Bin(self.transit, self.limbdark, self.settings, self.arrays)\n    if err != _ERR_NONE: \n        RaiseError(err)"
    },
    {
        "original": "def add_arguments(self): \n\n        # Call our parent to add the default arguments\n        ApiCli.add_arguments(self)\n\n        # Command specific arguments\n        self.parser.add_argument('-f', '--format', dest='format', action='store', required=False,\n                                 choices=['csv', 'json', 'raw', 'xml'], help='Output format. Default is raw')\n        self.parser.add_argument('-n', '--name', dest='metric_name', action='store', required=True,\n                         ",
        "rewrite": "self.parser.add_argument('-n', '--name', dest='metric_name', action='store', required=True, \n                            help='Specify the metric name to retrieve data for')"
    },
    {
        "original": "def activate(fn=None): def test_request(): \n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n     ",
        "rewrite": "import functools\n\ndef activate(fn=None): \n    def test_request(): \n        if not callable(fn):\n            _engine.activate()\n            return None\n\n        if asyncio is not None and asyncio.iscoroutinefunction(fn):\n            return activate_async(fn, _engine)\n\n        @functools.wraps(fn)\n        def wrapper(*args, **kw):\n            _engine.activate()\n            try:\n                fn(*args, **kw)\n            finally:\n                pass\n\n        return wrapper"
    },
    {
        "original": "def wait_for(self, cmd, value=None, timeout=60): \n        wait = time() + timeout * 60\n        while True:\n            if time() > wait:\n                return OrderedDict()\n            msgs = self.receive()\n            msg = check_messages(msgs, cmd, value=value)\n            if msg:\n                return msg\n       ",
        "rewrite": "def wait_for(self, cmd, value=None, timeout=60): \n    wait = time.time() + timeout * 60\n    while True:\n        if time.time() > wait:\n            return OrderedDict()\n        msgs = self.receive()\n        msg = check_messages(msgs, cmd, value=value)\n        if msg:\n            return msg"
    },
    {
        "original": "def register(self, *magic_objects): \n        # Start by validating them to ensure they have all had their magic\n        # methods registered at the instance level\n        for m in magic_objects:\n            if not m.registered:\n                raise ValueError(\"Class of magics %r was constructed without \"\n                                 \"the @register_magics class decorator\")\n     ",
        "rewrite": "def register(self, *magic_objects):\n    for m in magic_objects:\n        if not m.registered:\n            raise ValueError(\"Class of magics %r was constructed without the @register_magics class decorator\")"
    },
    {
        "original": "def index_order(self, sources=True, destinations=True): \n        if sources:\n            arefs = chain(*self.sources.values())\n        else:\n            arefs = []\n        if destinations:\n            arefs = chain(arefs, *self.destinations.values())\n\n        ret = []\n        for a in [aref for aref in arefs if aref is not None]:\n            ref = []\n          ",
        "rewrite": "def index_order(self, sources=True, destinations=True): \n    if sources:\n        arefs = chain(*self.sources.values())\n    else:\n        arefs = []\n    if destinations:\n        arefs = chain(arefs, *self.destinations.values())\n\n    ret = []\n    for a in [aref for aref in arefs if aref is not None]:\n        ref = []"
    },
    {
        "original": "def fit_meanshift(self, data, bandwidth=None, bin_seeding=False, **kwargs): \n        if bandwidth is None:\n            bandwidth = cl.estimate_bandwidth(data)\n        ms = cl.MeanShift(bandwidth=bandwidth, bin_seeding=bin_seeding)\n        ms.fit(data)\n        return ms",
        "rewrite": "def fit_meanshift(self, data, bandwidth=None, bin_seeding=False, **kwargs):\n    if bandwidth is None:\n        bandwidth = cl.estimate_bandwidth(data)\n    ms = cl.MeanShift(bandwidth=bandwidth, bin_seeding=bin_seeding)\n    ms.fit(data)\n    return ms"
    },
    {
        "original": "def lookup(self, h): \n        for (_, k1, k2) in self.client.scan_keys(HASH_TF_INDEX_TABLE,\n                                                 ((h,), (h,))):\n            yield kvlayer_key_to_stream_id((k1, k2))",
        "rewrite": "def lookup(self, h): \n    for (_, k1, k2) in self.client.scan_keys(HASH_TF_INDEX_TABLE, ((h,), (h,))):\n        yield kvlayer_key_to_stream_id((k1, k2))"
    },
    {
        "original": "def from_compressed_buffer(cls, compressed_buffer, point_format, count, laszip_vlr): \n        point_dtype = point_format.dtype\n        uncompressed = decompress_buffer(\n            compressed_buffer, point_dtype, count, laszip_vlr\n        )\n        return cls(uncompressed, point_format)",
        "rewrite": "def from_compressed_buffer(cls, compressed_buffer, point_format, count, laszip_vlr):\n    point_dtype = point_format.dtype\n    uncompressed = decompress_buffer(compressed_buffer, point_dtype, count, laszip_vlr)\n    return cls(uncompressed, point_format)"
    },
    {
        "original": " \n    postcode_lookup = f\"/postcodes?lat={lat}&lon={long}\"\n    return await _get_postcode_from_url(postcode_lookup)",
        "rewrite": "postcode_lookup = f\"/postcodes?lat={lat}&lon={long}\"\nreturn await _get_postcode_from_url(postcode_lookup)"
    },
    {
        "original": " \n\n        invalid_result = None\n\n        understood_namespaces = {\n            'nist-0.1': 'http://beacon.nist.gov/record/0.1/',\n        }\n\n        # Our required values are \"must haves\". This makes it simple\n        # to verify we loaded everything out of XML correctly.\n        required_values = {\n            cls._KEY_FREQUENCY: None,\n            cls._KEY_OUTPUT_VALUE: None,\n            cls._KEY_PREVIOUS_OUTPUT_VALUE: None,\n ",
        "rewrite": "invalid_result = None\n\nunderstood_namespaces = {\n    'nist-0.1': 'http://beacon.nist.gov/record/0.1/',\n}\n\nrequired_values = {\n    cls._KEY_FREQUENCY: None,\n    cls._KEY_OUTPUT_VALUE: None,\n    cls._KEY_PREVIOUS_OUTPUT_VALUE: None,\n}"
    },
    {
        "original": "def as_action_description(self): \n        description = {\n            self.name: {\n                'href': self.href_prefix + self.href,\n                'timeRequested': self.time_requested,\n                'status': self.status,\n            },\n        }\n\n        if self.input is not None:\n            description[self.name]['input'] = self.input\n\n   ",
        "rewrite": "def as_action_description(self): \n        description = {\n            self.name: {\n                'href': self.href_prefix + self.href,\n                'timeRequested': self.time_requested,\n                'status': self.status,\n            },\n        }\n\n        if self.input is not None:\n            description[self.name]['input'] = self.input"
    },
    {
        "original": "def _source(self, feature_names): \n        if feature_names is None:\n            return True\n        elif isinstance(feature_names, bool):\n            return feature_names\n        else:\n            return map(lambda n: 'fc.' + n, feature_names)",
        "rewrite": "def _source(self, feature_names):\n    if feature_names is None:\n        return True\n    elif isinstance(feature_names, bool):\n        return feature_names\n    else:\n        return list(map(lambda n: 'fc.' + n, feature_names))"
    },
    {
        "original": "def parse_hpo_phenotype(hpo_line): \n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    hpo_info['hpo_id'] = hpo_line[0]\n    hpo_info['description'] = hpo_line[1]\n    hpo_info['hgnc_symbol'] = hpo_line[3]\n    \n    return hpo_info",
        "rewrite": "def parse_hpo_phenotype(hpo_line):\n    hpo_line = hpo_line.rstrip().split('\\t')\n    hpo_info = {}\n    hpo_info['hpo_id'] = hpo_line[0]\n    hpo_info['description'] = hpo_line[1]\n    hpo_info['hgnc_symbol'] = hpo_line[3]\n    \n    return hpo_info"
    },
    {
        "original": "def _fetch_gerrit28(self, from_date=DEFAULT_DATETIME): \n\n        # Convert date to Unix time\n        from_ut = datetime_to_utc(from_date)\n        from_ut = from_ut.timestamp()\n\n        filter_open = \"status:open\"\n        filter_closed = \"status:closed\"\n\n        last_item_open = self.client.next_retrieve_group_item()\n        last_item_closed = self.client.next_retrieve_group_item()\n        reviews_open = self._get_reviews(last_item_open, filter_open)\n        reviews_closed = self._get_reviews(last_item_closed, filter_closed)\n        last_nreviews_open = len(reviews_open)\n        last_nreviews_closed = len(reviews_closed)\n\n        while reviews_open",
        "rewrite": "def _fetch_gerrit(self, from_date=DEFAULT_DATETIME): \n\n    from_ut = datetime_to_utc(from_date).timestamp()\n\n    filter_open = \"status:open\"\n    filter_closed = \"status:closed\"\n\n    last_item_open = self.client.next_retrieve_group_item()\n    last_item_closed = self.client.next_retrieve_group_item()\n    reviews_open = self._get_reviews(last_item_open, filter_open)\n    reviews_closed = self._get_reviews(last_item_closed, filter_closed)\n    last_nreviews_open = len(reviews_open)\n    last_nreviews_closed = len(reviews_closed)\n\n    while reviews_open:"
    },
    {
        "original": " \n        policy_section = self.get_relevant_policy_section(\n            policy_name,\n            session_group\n        )\n        if policy_section is None:\n            return False\n\n        object_policy = policy_section.get(object_type)\n        if not object_policy:\n            self._logger.warning(\n                \"The '{0}' policy does not apply to {1} objects.\".format(\n    ",
        "rewrite": "policy_section = self.get_relevant_policy_section(policy_name, session_group)\nif policy_section is None:\n    return False\n\nobject_policy = policy_section.get(object_type)\nif not object_policy:\n    self._logger.warning(\"The '{0}' policy does not apply to {1} objects.\".format(policy_name, object_type))"
    },
    {
        "original": "def seek(self, offset, whence=0): \r\n        if self.closed:\r\n            raise ValueError('I/O operation on closed file.')\r\n        \r\n        if whence == SEEK_SET:\r\n            pos = offset\r\n        elif whence == SEEK_CUR:\r\n            pos = self.pos + offset\r\n        elif whence == SEEK_END:\r\n            pos = self.size + offset\r\n        \r\n  ",
        "rewrite": "def seek(self, offset, whence=0):\r\n        if self.closed:\r\n            raise ValueError('I/O operation on closed file.')\r\n\r\n        if whence == SEEK_SET:\r\n            pos = offset\r\n        elif whence == SEEK_CUR:\r\n            pos = self.pos + offset\r\n        elif whence == SEEK_END:\r\n            pos = self.size + offset"
    },
    {
        "original": "def _basic_math_operation(df, new_column, column_1, column_2, op): \n    if not isinstance(column_1, (str, int, float)):\n        raise TypeError(f'column_1 must be a string, an integer or a float')\n    if not isinstance(column_2, (str, int, float)):\n        raise TypeError(f'column_2 must be a string, an integer or a float')\n\n    if isinstance(column_1, str):\n        column_1 = df[column_1]\n    if isinstance(column_2, str):\n        column_2 = df[column_2]\n    operator = getattr(_operator, op)\n    df[new_column] = operator(column_1, column_2)\n    return df",
        "rewrite": "def _basic_math_operation(df, new_column, column_1, column_2, op):\n    if not isinstance(column_1, (str, int, float)):\n        raise TypeError(f'column_1 must be a string, an integer or a float')\n\n    if not isinstance(column_2, (str, int, float)):\n        raise TypeError(f'column_2 must be a string, an integer or a float')\n\n    if isinstance(column_1, str):\n        column_1 = df[column_1]\n\n    if isinstance(column_2, str):\n        column_2 = df[column_2]\n\n    operator = getattr(_operator, op)\n    df[new_column] = operator(column_1, column_2)\n    return df"
    },
    {
        "original": "def transform_array_decl_to_malloc(decl, with_init=True): \n    if type(decl.type) is not c_ast.ArrayDecl:\n        # Not an array declaration, can be ignored\n        return\n\n    type_ = c_ast.PtrDecl([], decl.type.type)\n    if with_init:\n        decl.init = c_ast.FuncCall(\n            c_ast.ID('aligned_malloc'),\n            c_ast.ExprList([\n                c_ast.BinaryOp(\n                    '*',\n           ",
        "rewrite": "def transform_array_decl_to_malloc(decl, with_init=True): \n    if type(decl.type) is not c_ast.ArrayDecl:\n        # Not an array declaration, can be ignored\n        return\n\n    type_ = c_ast.PtrDecl([], decl.type.type)\n    if with_init:\n        decl.init = c_ast.FuncCall(\n            c_ast.ID('aligned_malloc'),\n            c_ast.ExprList([\n                c_ast.BinaryOp(\n                    '*'),"
    },
    {
        "original": "def _generate_email(self): \n        self.message = MIMEMultipart()\n        self._add_header()\n        self._add_body()\n        self._add_attachments()",
        "rewrite": "def _generate_email(self):\n    self.message = MIMEMultipart()\n    self._add_header()\n    self._add_body()\n    self._add_attachments()"
    },
    {
        "original": "def _harmonic_number(x): \n  one = tf.ones([], dtype=x.dtype)\n  return tf.math.digamma(x + one) - tf.math.digamma(one)",
        "rewrite": "def _harmonic_number(x):\n    one = tf.ones([], dtype=x.dtype)\n    return tf.math.digamma(x + one) - tf.math.digamma(one)"
    },
    {
        "original": "def proc_text(self, tokens): \n\n        component = Text(pen=self.pen,\n                         text_x=tokens[\"x\"],\n                         text_y=tokens[\"y\"],\n                         justify=tokens[\"j\"],\n                         text_w=tokens[\"w\"],\n              ",
        "rewrite": "def proc_text(self, tokens):\n\n        component = Text(pen=self.pen,\n                         text_x=tokens[\"x\"],\n                         text_y=tokens[\"y\"],\n                         justify=tokens[\"j\"],\n                         text_w=tokens[\"w\"])"
    },
    {
        "original": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        super(RevokeRequestPayload, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.unique_identifier = attributes.UniqueIdentifier()\n        self.unique_identifier.read(tstream, kmip_version=kmip_version)\n\n        self.revocation_reason = objects.RevocationReason()\n        self.revocation_reason.read(tstream, kmip_version=kmip_version)\n\n        if self.is_tag_next(enums.Tags.COMPROMISE_OCCURRENCE_DATE, tstream):\n            self.compromise_occurrence_date = primitives.DateTime(\n          ",
        "rewrite": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        super(RevokeRequestPayload, self).read(\n            istream,\n            kmip_version=kmip_version\n        )\n        tstream = BytearrayStream(istream.read(self.length))\n\n        self.unique_identifier = attributes.UniqueIdentifier()\n        self.unique_identifier.read(tstream, kmip_version=kmip_version)\n\n        self.revocation_reason = objects.RevocationReason()\n        self.revocation_reason.read(tstream, kmip_version=kmip_version)\n\n        if self.is_tag_next(enums.Tags.COMPROMISE_OCCURRENCE_DATE, tstream):\n            self.compromise_occurrence_date = primitives.DateTime(...)"
    },
    {
        "original": "def parse(self, string): \n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val  = val.split()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val\n      ",
        "rewrite": "def parse(self, string): \n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val = val.strip()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val"
    },
    {
        "original": "def shape(self): \n        # TODO cache\n        first = self.first().shape\n        shape = self._rdd.map(lambda x: x.shape[0]).sum()\n        return (shape,) + first[1:]",
        "rewrite": "def shape(self):\n    first = self.first().shape\n    shape = self._rdd.map(lambda x: x.shape[0]).sum()\n    return (shape,) + first[1:]"
    },
    {
        "original": "def parse(self, s): \n        with self.lock:\n            try:\n                return self.parser.parse(s, lexer=self.lexer)\n            except InvalidIEMLObjectArgument as e:\n                raise CannotParse(s, str(e))\n            except CannotParse as e:\n                e.s = s\n                raise e",
        "rewrite": "def parse(self, s):\n    with self.lock:\n        try:\n            return self.parser.parse(s, lexer=self.lexer)\n        except InvalidIEMLObjectArgument as e:\n            raise CannotParse(s, str(e))\n        except CannotParse as e:\n            e.s = s\n            raise e"
    },
    {
        "original": "def _casedict_to_dict(self, message): \n        message_id = message.pop(self.MESSAGE_ID_FIELD)\n        date = message.pop(self.DATE_FIELD)\n\n        msg = {k: v for k, v in message.items()}\n        msg[self.MESSAGE_ID_FIELD] = message_id\n        msg[self.DATE_FIELD] = date\n\n        return msg",
        "rewrite": "def _casedict_to_dict(self, message): \n    message_id = message.pop(self.MESSAGE_ID_FIELD)\n    date = message.pop(self.DATE_FIELD)\n\n    msg = {k: v for k, v in message.items()}\n    msg[self.MESSAGE_ID_FIELD] = message_id\n    msg[self.DATE_FIELD] = date\n\n    return msg"
    },
    {
        "original": "def takewhile(self, func=None): \n        func = _make_callable(func)\n        return Collection(takewhile(func, self._items))",
        "rewrite": "def takewhile(self, func=None):\n        func = self._make_callable(func)\n        return Collection(itertools.takewhile(func, self._items))"
    },
    {
        "original": "def create(cls, name_value, name_type): \n        if isinstance(name_value, Name.NameValue):\n            value = name_value\n        elif isinstance(name_value, str):\n            value = cls.NameValue(name_value)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_value'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n           ",
        "rewrite": "def create(cls, name_value, name_type): \n    if isinstance(name_value, Name.NameValue):\n        value = name_value\n    elif isinstance(name_value, str):\n        value = cls.NameValue(name_value)\n    else:\n        name = 'Name'\n        msg = exceptions.ErrorStrings.BAD_EXP_RECV\n        member = 'name_value'\n        raise TypeError(msg.format('{0}.{1}'.format(name, member), ''))"
    },
    {
        "original": "def scale(self, center=True, scale=True): \n        return H2OFrame._expr(expr=ExprNode(\"scale\", self, center, scale), cache=self._ex._cache)",
        "rewrite": "def scale(self, center=True, scale=True): \n    return H2OFrame._expr(expr=ExprNode(\"scale\", self, center, scale), cache=self._expr_cache)"
    },
    {
        "original": "def fetchChildren(self): \n        if not self.canFetchMore():\n            return []\n\n        children = self._fetchChildren()\n        self._fetched = True\n\n        return children",
        "rewrite": "def fetchChildren(self):\n    if not self.canFetchMore():\n        return []\n\n    children = self._fetchChildren()\n    self._fetched = True\n\n    return children"
    },
    {
        "original": "def request_sender(self, pn_link): \n        sl = SenderLink(self._connection, pn_link)\n        self._links.add(sl)\n        return sl",
        "rewrite": "def request_sender(self, pn_link):\n    sl = SenderLink(self._connection, pn_link)\n    self._links.add(sl)\n    return sl"
    },
    {
        "original": "def load_config(under_test=False, custom=None): \n\n    if \"config_loaded\" not in INTERN:\n        # The configuration was not already loaded.\n\n        # We load and download the different configuration file if they are non\n        # existant.\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            # If we are not under test which means that we want to save informations,\n            # we initiate the directory structure.\n            DirectoryStructure()\n\n",
        "rewrite": "def load_config(under_test=False, custom=None): \n\n    if \"config_loaded\" not in INTERN:\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            DirectoryStructure()"
    },
    {
        "original": "def cli_command_resume(self, msg): \n        if self.state == State.PAUSED:\n            self.state = State.WAITING",
        "rewrite": "def cli_command_resume(self, msg): \n    if self.state == State.PAUSED:\n        self.state = State.WAITING"
    },
    {
        "original": "def p_literal_list(self, p): \n\n        if len(p) == 3:\n            p[0] = p[1] + [p[2][1:-1]]\n        else:\n            p[0] = [p[1][1:-1]]",
        "rewrite": "def p_literal_list(self, p): \n    p[0] = p[1][1:-1] if len(p) != 3 else p[1] + [p[2][1:-1]]"
    },
    {
        "original": " \n\n        ys = [iv.top.z for iv in self]\n\n        if field is not None:\n            f = field_function or utils.null\n            xs = [f(iv.data.get(field, undefined)) for iv in self]\n        else:\n            xs = [1 for iv in self]\n\n        ax.set_xlim((min(xs), max(xs)))\n        for x, y in zip(xs, ys):\n            ax.axhline(y, color='lightgray', zorder=0)\n\n   ",
        "rewrite": "ys = [iv.top.z for iv in self]\n\nif field is not None:\n    f = field_function or utils.null\n    xs = [f(iv.data.get(field, undefined)) for iv in self]\nelse:\n    xs = [1 for iv in self]\n\nax.set_xlim((min(xs), max(xs)))\nfor x, y in zip(xs, ys):\n    ax.axhline(y, color='lightgray', zorder=0)"
    },
    {
        "original": "def find_idx_by_threshold(self, threshold): \n        assert_is_type(threshold, numeric)\n        thresh2d = self._metric_json['thresholds_and_metric_scores']\n        for i, e in enumerate(thresh2d.cell_values):\n            t = float(e[0])\n            if abs(t - threshold) < 1e-8 * max(t, threshold):\n                return i\n        if 0 <= threshold <= 1:\n            thresholds = [float(e[0]) for i, e in enumerate(thresh2d.cell_values)]\n          ",
        "rewrite": "def find_idx_by_threshold(self, threshold): \n    assert_is_type(threshold, numeric)\n    thresh2d = self._metric_json['thresholds_and_metric_scores']\n    for i, e in enumerate(thresh2d.cell_values):\n        t = float(e[0])\n        if abs(t - threshold) < 1e-8 * max(t, threshold):\n            return i\n    if 0 <= threshold <= 1:\n        thresholds = [float(e[0]) for i, e in enumerate(thresh2d.cell_values)]"
    },
    {
        "original": "def get_filtered_root_folder(self): \n        folder, filename = os.path.split(self.name)\n        return os.path.join(folder, VERSATILEIMAGEFIELD_FILTERED_DIRNAME, '')",
        "rewrite": "def get_filtered_root_folder(self):\n    folder, filename = os.path.split(self.name)\n    return os.path.join(folder, VERSATILEIMAGEFIELD_FILTERED_DIRNAME, '')"
    },
    {
        "original": "def uninstall(self): \n        log_filename = \"agent_{host}.log\".format(host=self.host)\n        data_filename = \"agent_{host}.rawdata\".format(host=self.host)\n\n        try:\n            if self.session:\n                self.session.send(\"stop\\n\")\n                self.session.close()\n                self.session = None\n        except BaseException:\n            logger.warning(\n             ",
        "rewrite": "def uninstall(self): \n        log_filename = f\"agent_{self.host}.log\"\n        data_filename = f\"agent_{self.host}.rawdata\"\n\n        try:\n            if self.session:\n                self.session.send(\"stop\\n\")\n                self.session.close()\n                self.session = None\n        except BaseException as e:\n            logger.warning(\"An error occurred: {e}\")"
    },
    {
        "original": "def matches(x, y, regex_expr=False): \n    # Parse regex expression, if needed\n    x = strip_regex(x) if regex_expr and isregex_expr(x) else x\n\n    # Run regex assertion\n    if PY_3:\n        # Retrieve original regex pattern\n        x = x.pattern if isregex(x) else x\n        # Assert regular expression via unittest matchers\n        return test_case().assertRegex(y, x) or True\n\n    # Primitive regex matching for Python 2.7\n    if isinstance(x, str):\n        x = re.compile(x, re.IGNORECASE)\n\n    assert x.match(y) is not None",
        "rewrite": "def matches(x, y, regex_expr=False):\n    x = strip_regex(x) if regex_expr and isregex_expr(x) else x\n\n    if isinstance(x, str):\n        x = re.compile(x, re.IGNORECASE)\n\n    assert x.match(y) is not None"
    },
    {
        "original": "def rgb_to_hsl(r, g, b): \n    r = float(r) / 255.0\n    g = float(g) / 255.0\n    b = float(b) / 255.0\n\n    max_value = max(r, g, b)\n    min_value = min(r, g, b)\n\n    h = None\n    s = None\n    l = (max_value + min_value) / 2\n    d = max_value - min_value\n\n    if d == 0:\n        # achromatic\n        h = 0\n        s = 0\n    else:\n        s = d / (1 -",
        "rewrite": "def rgb_to_hsl(r, g, b): \n    r = float(r) / 255.0\n    g = float(g) / 255.0\n    b = float(b) / 255.0\n\n    max_value = max(r, g, b)\n    min_value = min(r, g, b)\n\n    h = None\n    s = None\n    l = (max_value + min_value) / 2\n    d = max_value - min_value\n\n    if d == 0:\n        h = 0\n        s = 0\n    else:\n        s = d / (1 - abs(2 * l - 1) if l < 0.5 else 2 - 2 * l)\n        if max_value == r:\n            h = (((g - b) / d) if g >= b else ((g - b) / d + 6)) * 60\n        elif max_value == g:\n            h = ((b - r) / d + 2) * 60\n        else:\n            h = ((r - g) / d + 4) * 60\n\n    return h, s, l"
    },
    {
        "original": " \n        if scale is None:\n            scale = Scale()\n        xfieldtype = xfield[1]\n        yfieldtype = yfield[1]\n        x_options = None\n        if len(xfield) > 2:\n            x_options = xfield[2]\n        y_options = None\n        if len(yfield) > 2:\n            y_options = yfield[2]\n        if time_unit is not",
        "rewrite": "if scale is None:\n            scale = Scale()\n        xfieldtype = xfield[1]\n        yfieldtype = yfield[1]\n        x_options = None\n        if len(xfield) > 2:\n            x_options = xfield[2]\n        y_options = None\n        if len(yfield) > 2:\n            y_options = yfield[2]\n        if time_unit is not None:"
    },
    {
        "original": "def ucas_download_single(url, output_dir = '.', merge = False, info_only = False, **kwargs): \n    html = get_content(url)\n    # resourceID is UUID\n    resourceID = re.findall( r'resourceID\":\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', html)[0]\n    assert resourceID != '', 'Cannot find resourceID!'\n\n    title = match1(html, r'<div class=\"bc-h\">(.+)</div>')\n    url_lists = _ucas_get_url_lists_by_resourceID(resourceID)\n    assert url_lists, 'Cannot find any URL of such class!'\n    \n    for k, part in enumerate(url_lists):\n        part_title = title + '_' + str(k)\n        print_info(site_info, part_title, 'flv', 0)\n        if not info_only:\n           ",
        "rewrite": "def ucas_download_single(url, output_dir = '.', merge = False, info_only = False, **kwargs):\n    html = get_content(url)\n    \n    # resourceID is a UUID\n    resourceID = re.findall(r'resourceID\":\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', html)[0]\n    assert resourceID != '', 'Cannot find resourceID!'\n\n    title = match1(html, r'<div class=\"bc-h\">(.+)</div>')\n    url_lists = _ucas_get_url_lists_by_resourceID(resourceID)\n    assert url_lists, 'Cannot find any URL of such class!'\n\n    for k, part in enumerate(url_lists):\n        part_title = title + '_' + str(k)\n        print_info(site_info, part_title, 'flv', 0)\n        if not info_only:\n            # Your code here"
    },
    {
        "original": "def _extract_arg_value(cls, arg_name, arg_type, remaining): \n\n        next_arg = None\n        should_consume = False\n        if len(remaining) > 0:\n            next_arg = remaining[0]\n            should_consume = True\n\n            if next_arg == '--':\n                next_arg = None\n\n        # Generally we just return the next argument, however if the type\n        # is bool we",
        "rewrite": "def _extract_arg_value(cls, arg_name, arg_type, remaining):\n\n    next_arg = None\n    should_consume = False\n    if len(remaining) > 0:\n        next_arg = remaining[0]\n        should_consume = True\n\n        if next_arg == '--':\n            next_arg = None\n\n    if arg_type is bool:\n        return True\n\n    return next_arg"
    },
    {
        "original": "def _report_evaluation(self): \n        # check with at least check 1 statements (usually 0 when there is a\n        # syntax error preventing pylint from further processing)\n        previous_stats = config.load_results(self.file_state.base_name)\n        if self.stats[\"statement\"] == 0:\n            return\n\n        # get a global note for the code\n        evaluation = self.config.evaluation\n        try:\n            note = eval(evaluation, {}, self.stats)  # pylint: disable=eval-used\n   ",
        "rewrite": "def _report_evaluation(self):\n    previous_stats = config.load_results(self.file_state.base_name)\n    if self.stats[\"statement\"] == 0:\n        return\n\n    evaluation = self.config.evaluation\n    try:\n        note = eval(evaluation, {}, self.stats)  # pylint: disable=eval-used"
    },
    {
        "original": "def sanitize_for_archive(url, headers, payload): \n        if headers and 'PRIVATE-TOKEN' in headers:\n            headers.pop('PRIVATE-TOKEN', None)\n\n        return url, headers, payload",
        "rewrite": "def sanitize_for_archive(url, headers, payload):\n    if headers and 'PRIVATE-TOKEN' in headers:\n        headers.pop('PRIVATE-TOKEN', None)\n\n    return url, headers, payload"
    },
    {
        "original": "def _store_compressed_sequence_pairs(self): \n        self.logger(\"TreeAnc._store_compressed_sequence_pairs...\",2)\n        for node in self.tree.find_clades():\n            if node.up is None:\n                continue\n            self._store_compressed_sequence_to_node(node)\n        self.logger(\"TreeAnc._store_compressed_sequence_pairs...done\",3)",
        "rewrite": "def _store_compressed_sequence_pairs(self):\n    self.logger(\"TreeAnc._store_compressed_sequence_pairs...\", 2)\n    \n    for node in self.tree.find_clades():\n        if node.up is None:\n            continue\n        self._store_compressed_sequence_to_node(node)\n        \n    self.logger(\"TreeAnc._store_compressed_sequence_pairs...done\", 3)"
    },
    {
        "original": "def _clear_inspect(self): \n\n        self.trace_info = defaultdict(list)\n        self.process_tags = {}\n        self.process_stats = {}\n        self.samples = []\n        self.stored_ids = []\n        self.stored_log_ids = []\n        self.time_start = None\n        self.time_stop = None\n        self.execution_command = None\n        self.nextflow_version = None\n        self.abort_cause = None\n        self._c = 0\n     ",
        "rewrite": "def _clear_inspect(self): \n    self.trace_info = defaultdict(list)\n    self.process_tags = {}\n    self.process_stats = {}\n    self.samples = []\n    self.stored_ids = []\n    self.stored_log_ids = []\n    self.time_start = None\n    self.time_stop = None\n    self.execution_command = None\n    self.nextflow_version = None\n    self.abort_cause = None\n    self._c = 0"
    },
    {
        "original": "def new(cls, access_token, environment='prod'): \n        request = RequestBuilder \\\n            .request(environment) \\\n            .to_service(cls.SERVICE_NAME, cls.SERVICE_VERSION) \\\n            .throw(\n                StorageForbiddenException,\n                lambda resp: 'You are forbidden to do this.'\n                if resp.status_code == 403 else None\n            ) \\\n",
        "rewrite": "def new(cls, access_token, environment='prod'):\n    request = RequestBuilder \\\n        .request(environment) \\\n        .to_service(cls.SERVICE_NAME, cls.SERVICE_VERSION) \\\n        .throw(\n            StorageForbiddenException,\n            lambda resp: 'You are forbidden to do this.'\n            if resp.status_code == 403 else None\n        ) \\"
    },
    {
        "original": "def map(self, *args): \n        call_args = [self._map_args(*cur_args)  for cur_args in zip(*args)]\n        r = self._invoke(call_args)\n\n        ret_type = _get_annotation('return', self.func)\n        output_name = getattr(self.func, '__output_name__', 'output1')\n        return [_decode_response(\n                    r['Results'][output_name]['value'].get(\"ColumnNames\"), \n                    r['Results'][output_name]['value'].get(\"ColumnTypes\"), \n                    x, \n    ",
        "rewrite": "def map(self, *args):\n    call_args = [self._map_args(*cur_args) for cur_args in zip(*args)]\n    r = self._invoke(call_args)\n\n    ret_type = _get_annotation('return', self.func)\n    output_name = getattr(self.func, '__output_name__', 'output1')\n    return [_decode_response(\n        r['Results'][output_name]['value'].get(\"ColumnNames\"),\n        r['Results'][output_name]['value'].get(\"ColumnTypes\"),\n        x) for x in r['Results'][output_name]['value'][\"Values\"]]"
    },
    {
        "original": " \n        if peek_lock:\n            return self.peek_lock_subscription_message(topic_name,\n                                                       subscription_name,\n                                                 ",
        "rewrite": "if peek_lock:\n    return self.peek_lock_subscription_message(topic_name, subscription_name)"
    },
    {
        "original": "def command_patterns(self): \n        return (\n            ('!register-success (?P<cmd_channel>.+)', self.require_boss(self.register_success)),\n            ('!worker-execute (?:\\((?P<workers>.+?)\\) )?(?P<task_id>\\d+):(?P<command>.+)', self.require_boss(self.worker_execute)),\n            ('!worker-ping', self.require_boss(self.worker_ping_handler)),\n            ('!worker-stop', self.require_boss(self.worker_stop)),\n        )",
        "rewrite": "def command_patterns(self):\n    return (\n        ('!register-success (?P<cmd_channel>.+)', self.require_boss(self.register_success)),\n        ('!worker-execute (?:\\((?P<workers>.+?)\\) )?(?P<task_id>\\d+):(?P<command>.+)', self.require_boss(self.worker_execute)),\n        ('!worker-ping', self.require_boss(self.worker_ping_handler)),\n        ('!worker-stop', self.require_boss(self.worker_stop)),\n    )"
    },
    {
        "original": "def f_supports(self, data): \n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)",
        "rewrite": "def f_supports(self, data): \n    dtype = type(data)\n    if (dtype is tuple or dtype is list) and len(data) == 0:\n        return True  #  ArrayParameter does support empty tuples\n    elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n        return True  #  ArrayParameter supports empty numpy arrays\n    else:\n        return super(ArrayParameter, self).f_supports(data)"
    },
    {
        "original": "def set_mock_engine(self, engine): \n        if not engine:\n            raise TypeError('engine must be a valid object')\n\n        # Instantiate mock engine\n        mock_engine = engine(self)\n\n        # Validate minimum viable interface\n        methods = ('activate', 'disable')\n        if not all([hasattr(mock_engine, method) for method in methods]):\n            raise NotImplementedError('engine must implementent the '\n                    ",
        "rewrite": "methods = ('activate', 'disable', 'mock_method')\n        if not all([hasattr(mock_engine, method) for method in methods]):\n            raise NotImplementedError(\"engine must implement the activate, disable, and mock method interfaces.\")"
    },
    {
        "original": "def _print_token_factory(col): \n    def _helper(msg):\n        style = style_from_dict({\n            Token.Color: col,\n        })\n        tokens = [\n            (Token.Color, msg)\n        ]\n        print_tokens(tokens, style=style)\n\n    def _helper_no_terminal(msg):\n        # workaround if we have no terminal\n        print(msg)\n    if sys.stdout.isatty():\n        return _helper\n    else:\n    ",
        "rewrite": "def _print_token_factory(col):\n    def _helper(msg):\n        style = style_from_dict({\n            Token.Color: col,\n        })\n        tokens = [\n            (Token.Color, msg)\n        ]\n        print_tokens(tokens, style=style)\n\n    def _helper_no_terminal(msg):\n        # workaround if we have no terminal\n        print(msg)\n    if sys.stdout.isatty():\n        return _helper\n    else:\n        return _helper_no_terminal"
    },
    {
        "original": "def set_default_value(self, obj): \n        # Check for a deferred initializer defined in the same class as the\n        # trait declaration or above.\n        mro = type(obj).mro()\n        meth_name = '_%s_default' % self.name\n        for cls in mro[:mro.index(self.this_class)+1]:\n            if meth_name in cls.__dict__:\n                break\n        else:\n            # We didn't find one. Do static initialization.\n ",
        "rewrite": "def set_default_value(self, obj): \n    mro = type(obj).mro()\n    meth_name = '_%s_default' % self.name\n    for cls in mro[:mro.index(self.this_class)+1]:\n        if meth_name in cls.__dict__:\n            break\n    else:\n        # We didn't find one. Do static initialization."
    },
    {
        "original": "def p_expression_0(self, program): \n        program[0] = node.BinaryOp([node.BinaryOperator(program[2]),\n                                    program[1], program[3]])",
        "rewrite": "def p_expression_0(self, program):\n        program[0] = node.BinaryOp(node.BinaryOperator(program[2]), program[1], program[3])"
    },
    {
        "original": "def check_access(self, item, context): \n        if hasattr(self.current_request.user.is_authenticated, '__call__'):\n            authenticated = self.current_request.user.is_authenticated()\n        else:\n            authenticated = self.current_request.user.is_authenticated\n\n        if item.access_loggedin and not authenticated:\n            return False\n\n        if item.access_guest and authenticated:\n            return False\n\n        if item.access_restricted:\n            user_perms = self._current_user_permissions\n\n     ",
        "rewrite": "def check_access(self, item, context): \n    if hasattr(self.current_request.user, 'is_authenticated'):\n        authenticated = self.current_request.user.is_authenticated\n    else:\n        authenticated = self.current_request.user.is_authenticated()\n\n    if item.access_loggedin and not authenticated:\n        return False\n\n    if item.access_guest and authenticated:\n        return False\n\n    if item.access_restricted:\n        user_perms = self._current_user_permissions"
    },
    {
        "original": "def getLogicalInterface(self, logicalInterfaceId, draft=False): \n        if draft:\n            req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"/draft\", logicalInterfaceId)\n        else:\n            req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"\", logicalInterfaceId)\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"logical interface retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a logical interface\", resp)\n     ",
        "rewrite": "def getLogicalInterface(self, logicalInterfaceId, draft=False):\n    if draft:\n        req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"/draft\", logicalInterfaceId)\n    else:\n        req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"\", logicalInterfaceId)\n    resp = requests.get(req, auth=self.credentials, verify=self.verify)\n    if resp.status_code == 200:\n        self.logger.debug(\"logical interface retrieved\")\n    else:\n        raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a logical interface\", resp)"
    },
    {
        "original": "def remove_url_auth(url): \n    parts = parse.urlsplit(url)\n    return RemoveUrlAuthResult(auth=(parts.username or None, parts.password),\n                               url=rewrite_url(url, user=None, password=None))",
        "rewrite": "def remove_url_auth(url):\n    parts = parse.urlsplit(url)\n    return RemoveUrlAuthResult(auth=(parts.username or None, parts.password),\n                               url=rewrite_url(url, user=None, password=None))"
    },
    {
        "original": "def _extract_level(self, topic_str): \n        topics = topic_str.split('.')\n        for idx,t in enumerate(topics):\n            level = getattr(logging, t, None)\n            if level is not None:\n                break\n        \n        if level is None:\n            level = logging.INFO\n        else:\n            topics.pop(idx)\n  ",
        "rewrite": "def _extract_level(self, topic_str): \n        topics = topic_str.split('.')\n        level = logging.INFO\n        for idx, t in enumerate(topics):\n            level = getattr(logging, t, None)\n            if level is not None:\n                topics.pop(idx)\n                break\n        return level"
    },
    {
        "original": "def verify(self, pkey): \n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n        if result <= 0:\n            _raise_current_error()\n\n        return result",
        "rewrite": "def verify(self, pkey): \n    if not isinstance(pkey, PKey):\n        raise TypeError(\"pkey must be a PKey instance\")\n\n    result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n    if result <= 0:\n        _raise_current_error()\n\n    return result"
    },
    {
        "original": "def __get_pull_review_comments(self, pr_number): \n\n        comments = []\n        group_comments = self.client.pull_review_comments(pr_number)\n\n        for raw_comments in group_comments:\n\n            for comment in json.loads(raw_comments):\n                comment_id = comment.get('id')\n\n                user = comment.get('user', None)\n                if not user:\n                    logger.warning(\"Missing user info for",
        "rewrite": "logger.warning(\"Missing user info for comment with id:\", comment_id)"
    },
    {
        "original": "def raw_filter(self, filters): \n        return SearchResult(self, self._api.get(self._href, **{\"filter[]\": filters}))",
        "rewrite": "def raw_filter(self, filters):\n    return SearchResult(self, self._api.get(self._href, **{\"filter\": filters}))"
    },
    {
        "original": "def fetch(self, url, payload=None, headers=None, method=GET, stream=False, verify=True): \n        if self.from_archive:\n            response = self._fetch_from_archive(url, payload, headers)\n        else:\n            response = self._fetch_from_remote(url, payload, headers, method, stream, verify)\n\n        return response",
        "rewrite": "def fetch(self, url, payload=None, headers=None, method=\"GET\", stream=False, verify=True): \n    if self.from_archive:\n        response = self._fetch_from_archive(url, payload, headers)\n    else:\n        response = self._fetch_from_remote(url, payload, headers, method, stream, verify)\n    \n    return response"
    },
    {
        "original": "def get_motion_vector(self): \n        if any(self.move):\n            x, y = self.actor._rot\n            strafe = math.degrees(math.atan2(*self.move))\n            y_angle = math.radians(y)\n            x_angle = math.radians(x + strafe)\n            dy = 0.0\n            dx = math.cos(x_angle)\n            dz = math.sin(x_angle)\n        else:\n       ",
        "rewrite": "def get_motion_vector(self): \n        if any(self.move):\n            x, y = self.actor._rot\n            strafe = math.degrees(math.atan2(*self.move))\n            y_angle = math.radians(y)\n            x_angle = math.radians(x + strafe)\n            dy = 0.0\n            dx = math.cos(x_angle)\n            dz = math.sin(x_angle)\n        else:\n            # Add code here if needed\n            pass"
    },
    {
        "original": "def strval(node, outermost=True): \n    if not isinstance(node, element):\n        return node.xml_value if outermost else [node.xml_value]\n    accumulator = []\n    for child in node.xml_children:\n        if isinstance(child, text):\n            accumulator.append(child.xml_value)\n        elif isinstance(child, element):\n            accumulator.extend(strval(child, outermost=False))\n    if outermost: accumulator = ''.join(accumulator)\n    return accumulator",
        "rewrite": "def strval(node, outermost=True):\n    if not isinstance(node, element):\n        return node.xml_value if outermost else [node.xml_value]\n    accumulator = []\n    for child in node.xml_children:\n        if isinstance(child, text):\n            accumulator.append(child.xml_value)\n        elif isinstance(child, element):\n            accumulator.extend(strval(child, outermost=False))\n    if outermost: \n        accumulator = ''.join(accumulator)\n        \n    return accumulator"
    },
    {
        "original": "def emit_edge(self, name1, name2, **props): \n        attrs = ['%s=\"%s\"' % (prop, value) for prop, value in props.items()]\n        n_from, n_to = normalize_node_id(name1), normalize_node_id(name2)\n        self.emit(\"%s -> %s [%s];\" % (n_from, n_to, \", \".join(sorted(attrs))))",
        "rewrite": "def emit_edge(self, name1, name2, **props):\n    attrs = ['%s=\"%s\"' % (prop, value) for prop, value in props.items()]\n    n_from, n_to = normalize_node_id(name1), normalize_node_id(name2)\n    self.emit(\"%s -> %s [%s];\" % (n_from, n_to, \", \".join(sorted(attrs))) )"
    },
    {
        "original": "def register_checker(self, checker): \n        if checker not in self._checkers:\n            self._checkers.append(checker)\n            self.sort_checkers()",
        "rewrite": "def register_checker(self, checker):\n    if checker not in self._checkers:\n        self._checkers.append(checker)\n        self.sort_checkers()"
    },
    {
        "original": "def get_backend_name(self, location): \n        for vc_type in self._registry.values():\n            logger.debug('Checking in %s for %s (%s)...',\n                         location, vc_type.dirname, vc_type.name)\n            path = os.path.join(location, vc_type.dirname)\n            if os.path.exists(path):\n                logger.debug('Determine that %s uses VCS: %s',\n                    ",
        "rewrite": "def get_backend_name(self, location): \n    for vc_type in self._registry.values():\n        logger.debug('Checking in %s for %s (%s)...',\n                     location, vc_type.dirname, vc_type.name)\n        path = os.path.join(location, vc_type.dirname)\n        if os.path.exists(path):\n            logger.debug('Determine that {} uses VCS: {}'.format(vc_type.name, vc_type.dirname))"
    },
    {
        "original": "def load_ipython_extension(ip): \n    global _loaded\n    if not _loaded:\n        plugin = StoreMagic(shell=ip, config=ip.config)\n        ip.plugin_manager.register_plugin('storemagic', plugin)\n        _loaded = True",
        "rewrite": "def load_ipython_extension(ip): \n    global _loaded\n    if not _loaded:\n        plugin = StoreMagic(shell=ip, config=ip.config)\n        ip.plugin_manager.register_plugin('storemagic', plugin)\n        _loaded = True"
    },
    {
        "original": "def patch_debugtoolbar(settings): \n    try:\n        from pyramid_debugtoolbar import tbtools\n    except ImportError:\n        return\n\n    rollbar_web_base = settings.get('rollbar.web_base', DEFAULT_WEB_BASE)\n    if rollbar_web_base.endswith('/'):\n        rollbar_web_base = rollbar_web_base[:-1]\n\n    def insert_rollbar_console(request, html):\n        # insert after the closing </h1>\n        item_uuid = request.environ.get('rollbar.uuid')\n        if not item_uuid:\n            return html\n\n        url = '%s/item/uuid/?uuid=%s' % (rollbar_web_base, item_uuid)\n        link = '<a",
        "rewrite": "def patch_debugtoolbar(settings):\r\n  try:\r\n    from pyramid_debugtoolbar import tbtools\r\n  except ImportError:\r\n    return\r\n\r\n  rollbar_web_base = settings.get('rollbar.web_base', DEFAULT_WEB_BASE)\r\n\r\n  if rollbar_web_base.endswith('/'):\r\n    rollbar_web_base = rollbar_web_base[:-1]\r\n\r\n  def insert_rollbar_console(request, html):\r\n    item_uuid = request.environ.get('rollbar.uuid')\r\n    if not item_uuid:\r\n      return html\r\n\r\n    url = f'{rollbar_web_base}/item/uuid?uuid={item_uuid}'\r\n    link = f'<a'"
    },
    {
        "original": "def determine_batch_event_shapes(grid, endpoint_affine): \n  with tf.name_scope(\"determine_batch_event_shapes\"):\n    # grid  # shape: [B, k, q]\n    # endpoint_affine     # len=k, shape: [B, d, d]\n    batch_shape = grid.shape[:-2]\n    batch_shape_tensor = tf.shape(input=grid)[:-2]\n    event_shape = None\n    event_shape_tensor = None\n\n    def _set_event_shape(shape, shape_tensor):\n      if event_shape is None:\n        return shape, shape_tensor\n      return (tf.broadcast_static_shape(event_shape, shape),\n              tf.broadcast_dynamic_shape(event_shape_tensor, shape_tensor))\n\n    for aff in endpoint_affine:\n      if aff.shift is not None:\n      ",
        "rewrite": "def determine_batch_event_shapes(grid, endpoint_affine): \n  with tf.name_scope(\"determine_batch_event_shapes\"):\n    # grid  # shape: [B, k, q]\n    # endpoint_affine     # len=k, shape: [B, d, d]\n    batch_shape = grid.shape[:-2]\n    batch_shape_tensor = tf.shape(grid)[:-2]\n    event_shape = None\n    event_shape_tensor = None\n\n    def _set_event_shape(shape, shape_tensor):\n        if event_shape is None:\n            return shape, shape_tensor\n        return (tf.broadcast_static_shape(event_shape, shape),\n                tf.broadcast_dynamic_shape(event_shape_tensor, shape_tensor))\n\n    for aff in endpoint_affine:\n        if aff.shift is not None:\n            pass"
    },
    {
        "original": "def energy(self, state=None): \n        state = self.state if state is None else state\n        route = state\n        e = 0\n        if self.distance_matrix:\n            for i in range(len(route)):\n                e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n        else:\n            for i in range(len(route)):\n                e += distance(self.cities[route[i-1]], self.cities[route[i]])\n   ",
        "rewrite": "def energy(self, state=None): \n    state = self.state if state is None else state\n    route = state\n    e = 0\n    if self.distance_matrix:\n        for i in range(len(route)):\n            e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n    else:\n        for i in range(len(route)):\n            e += distance(self.cities[route[i-1]], self.cities[route[i]])"
    },
    {
        "original": "def convert_positional_argument(self, index, arg_value): \n\n        # For bound methods, skip self\n        if self._has_self:\n            if index == 0:\n                return arg_value\n\n            index -= 1\n\n        arg_name = self.arg_names[index]\n        return self.convert_argument(arg_name, arg_value)",
        "rewrite": "def convert_positional_argument(self, index, arg_value): \n\n    if self._has_self:\n        if index == 0:\n            return arg_value\n        index -= 1\n\n    arg_name = self.arg_names[index]\n    return self.convert_argument(arg_name, arg_value)"
    },
    {
        "original": "def _client_resource(client_class, cloud): \n    if client_class.__name__ == 'GraphRbacManagementClient':\n        return cloud.endpoints.active_directory_graph_resource_id, cloud.endpoints.active_directory_graph_resource_id\n    if client_class.__name__ == 'KeyVaultClient':\n        vault_host = cloud.suffixes.keyvault_dns[1:]\n        vault_url = 'https://{}'.format(vault_host)\n        return vault_url, None\n    return None, None",
        "rewrite": "def _client_resource(client_class, cloud):\n    if client_class.__name__ == 'GraphRbacManagementClient':\n        return cloud.endpoints.active_directory_graph_resource_id, cloud.endpoints.active_directory_graph_resource_id\n    elif client_class.__name__ == 'KeyVaultClient':\n        vault_host = cloud.suffixes.keyvault_dns[1:]\n        vault_url = f'https://{vault_host}'\n        return vault_url, None\n    else:\n        return None, None"
    },
    {
        "original": "def rename_all_checkpoints(self, old_path, new_path): \n        with self.engine.begin() as db:\n            return move_remote_checkpoints(\n                db,\n                self.user_id,\n                old_path,\n                new_path,\n            )",
        "rewrite": "def rename_all_checkpoints(self, old_path, new_path):\n        with self.engine.begin() as db:\n            return move_remote_checkpoints(\n                db,\n                self.user_id,\n                old_path,\n                new_path\n            )"
    },
    {
        "original": "def save(self, info): \n        save_file = self.save_file\n\n        if not isfile(save_file):\n            self.save_as(info)\n        else:\n            fd = None\n            try:\n                fd = open(save_file, \"wb\")\n                dot_code = str(self.model)\n                fd.write(dot_code)\n     ",
        "rewrite": "def save(self, info): \n    save_file = self.save_file\n\n    if not isfile(save_file):\n        self.save_as(info)\n    else:\n        with open(save_file, \"wb\") as fd:\n            dot_code = str(self.model)\n            fd.write(dot_code)"
    },
    {
        "original": "def user_institutes(store, login_user): \n    if login_user.is_admin:\n        institutes = store.institutes()\n    else:\n        institutes = [store.institute(inst_id) for inst_id in login_user.institutes]\n\n    return institutes",
        "rewrite": "def user_institutes(store, login_user):\n    if login_user.is_admin:\n        institutes = store.institutes()\n    else:\n        institutes = [store.institute(inst_id) for inst_id in login_user.institutes]\n\n    return institutes"
    },
    {
        "original": "def run(self): \n        line = self.fd.readline()\n        # allow for files opened in unicode mode\n        if isinstance(line, unicode):\n            send = self.sock.send_unicode\n        else:\n            send = self.sock.send\n        while line:\n            send(line)\n            line = self.fd.readline()\n        # line == '' means EOF\n      ",
        "rewrite": "def run(self): \n    line = self.fd.readline()\n    # allow for files opened in unicode mode\n    if isinstance(line, str):\n        send = self.sock.send\n    else:\n        send = self.sock.sendall\n    while line:\n        send(line)\n        line = self.fd.readline()\n    # line == '' means EOF"
    },
    {
        "original": "def connection_made(self, transport): \n\n        if self._manager.is_closed():\n            logger.debug(\"worker tried to connect while manager was closed\")\n            return\n\n        logger.debug(\"new worker connected\")\n\n        self._transport = transport\n        self._buffer = bytearray()\n        self._worker = Worker(self._transport, self._manager)\n        self._workers.add(self._worker)",
        "rewrite": "def connection_made(self, transport): \n        if self._manager.is_closed():\n            logger.debug(\"worker tried to connect while manager was closed\")\n            return\n\n        logger.debug(\"new worker connected\")\n\n        self._transport = transport\n        self._buffer = bytearray()\n        self._worker = Worker(self._transport, self._manager)\n        self._workers.add(self._worker)"
    },
    {
        "original": "def find_handfile(names=None): \n    # \u5982\u679c\u6ca1\u6709\u660e\u786e\u6307\u5b9a\uff0c\u5219\u5305\u542b env \u4e2d\u7684\u503c\n    names = names or [env.handfile]\n\n    # \u82e5\u65e0 ``.py`` \u6269\u5c55\u540d\uff0c\u5219\u4f5c\u4e3a\u5f85\u67e5\u8be2\u540d\u79f0\uff0c\u8ffd\u52a0\u5230 names \u672b\u5c3e\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name \u4e2d\u662f\u5426\u5305\u542b\u8def\u5f84\u5143\u7d20\n    if os.path.dirname(names[0]):\n        # \u82e5\u5b58\u5728\uff0c\u5219\u6269\u5c55 Home \u8def\u5f84\u6807\u5fd7\uff0c\u5e76\u6d4b\u8bd5\u662f\u5426\u5b58\u5728\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n    ",
        "rewrite": "def find_handfile(names=None):\n    names = names or [env.handfile]\n\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    if os.path.dirname(names[0]):\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    # Do something here after the condition\n                    pass"
    },
    {
        "original": "def execute_from_command_line_with_config(config: GoodConf, argv: List[str]): \n    with load_config_from_cli(config, argv) as args:\n        from django.core.management import execute_from_command_line\n        execute_from_command_line(args)",
        "rewrite": "def execute_from_command_line_with_config(config: GoodConf, argv: List[str]):\n    with load_config_from_cli(config, argv) as args:\n        from django.core.management import execute_from_command_line\n        execute_from_command_line(args)"
    },
    {
        "original": "def use_privatekey(self, pkey): \n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        use_result = _lib.SSL_CTX_use_PrivateKey(self._context, pkey._pkey)\n        if not use_result:\n            self._raise_passphrase_exception()",
        "rewrite": "def use_privatekey(self, pkey):\n    if not isinstance(pkey, PKey):\n        raise TypeError(\"pkey must be a PKey instance\")\n\n    use_result = _lib.SSL_CTX_use_PrivateKey(self._context, pkey._pkey)\n    if not use_result:\n        self._raise_passphrase_exception()"
    },
    {
        "original": "def get_guild_count(self, bot_id: int=None): \n        if bot_id is None:\n            bot_id = self.bot_id\n        return await self.http.get_guild_count(bot_id)",
        "rewrite": "def get_guild_count(self, bot_id: int = None):\n    bot_id = bot_id if bot_id is not None else self.bot_id\n    return await self.http.get_guild_count(bot_id)"
    },
    {
        "original": "def assert_is_type(var, *types, **kwargs): \n    assert types, \"The list of expected types was not provided\"\n    expected_type = types[0] if len(types) == 1 else U(*types)\n    if _check_type(var, expected_type): return\n\n    # Type check failed => Create a nice error message\n    assert set(kwargs).issubset({\"message\", \"skip_frames\"}), \"Unexpected keyword arguments: %r\" % kwargs\n    message = kwargs.get(\"message\", None)\n    skip_frames = kwargs.get(\"skip_frames\", 1)\n    args = _retrieve_assert_arguments()\n    vname = args[0]\n    etn = _get_type_name(expected_type, dump=\", \".join(args[1:]))\n    vtn = _get_type_name(type(var))\n    raise H2OTypeError(var_name=vname, var_value=var, var_type_name=vtn, exp_type_name=etn, message=message,\n                 ",
        "rewrite": "def assert_is_type(var, *types, **kwargs): \n    assert types, \"The list of expected types was not provided\"\n    expected_type = types[0] if len(types) == 1 else types\n    if _check_type(var, expected_type):\n        return\n\n    assert set(kwargs).issubset({\"message\", \"skip_frames\"}), \"Unexpected keyword arguments: %r\" % kwargs\n    message = kwargs.get(\"message\", None)\n    skip_frames = kwargs.get(\"skip_frames\", 1)\n    args = _retrieve_assert_arguments()\n    vname, *rest = args\n    etn = _get_type_name(expected_type, dump=\", \".join(rest))\n    vtn = _get_type_name(type(var))\n\n    raise H2OTypeError(var_name=vname, var_value=var, var_type_name=vtn, exp_type_name=etn, message=message)"
    },
    {
        "original": "def _compute_missing_rates(self, currency): \n        rates = self._rates[currency]\n\n        # tmp will store the closest rates forward and backward\n        tmp = defaultdict(lambda: [None, None])\n\n        for date in sorted(rates):\n            rate = rates[date]\n            if rate is not None:\n                closest_rate = rate\n                dist = 0\n        ",
        "rewrite": "def _compute_missing_rates(self, currency): \n    rates = self._rates[currency]\n    \n    # tmp will store the closest rates forward and backward\n    tmp = defaultdict(lambda: [None, None])\n    \n    for date in sorted(rates):\n        rate = rates[date]\n        if rate is not None:\n            closest_rate = rate\n            dist = 0"
    },
    {
        "original": "def calcZ(G): \n  gatefloor=power(10.0,0.4*(12.0-15.0))\n  if isscalar(G):\n   result=amax((gatefloor,power(10.0,0.4*(G-15.0))))\n  else :\n    result=power(10.0,0.4*(G-15.0))\n    indices=(result<gatefloor)\n    result[indices]=gatefloor\n  return result",
        "rewrite": "def calcZ(G):\n    gatefloor = 10.0 ** (0.4 * (12.0 - 15.0))\n    if np.isscalar(G):\n        result = np.amax((gatefloor, 10.0 ** (0.4 * (G - 15.0))))\n    else:\n        result = 10.0 ** (0.4 * (G - 15.0))\n        indices = (result < gatefloor)\n        result[indices] = gatefloor\n    return result"
    },
    {
        "original": "def _collect_section(self, section): \n        kwargs = {}\n        try:\n            if self.parser.has_section(section):\n                options = self.parser.options(section)\n                for option in options:\n                    str_val = self.parser.get(section, option)\n                    val = ast.literal_eval(str_val)\n           ",
        "rewrite": "def _collect_section(self, section):\n    kwargs = {}\n    try:\n        if self.parser.has_section(section):\n            options = self.parser.options(section)\n            for option in options:\n                str_val = self.parser.get(section, option)\n                val = ast.literal_eval(str_val)"
    },
    {
        "original": "def _hanging_indent_after_bracket(self, bracket, position): \n        indentation = self._tokens.line_indent(position)\n        if (\n            self._is_block_opener\n            and self._continuation_string == self._block_indent_string\n        ):\n            return _ContinuedIndent(\n                HANGING_BLOCK,\n                bracket,\n                position,\n        ",
        "rewrite": "def _hanging_indent_after_bracket(self, bracket, position): \n    indentation = self._tokens.line_indent(position)\n    if self._is_block_opener and self._continuation_string == self._block_indent_string:\n        return _ContinuedIndent(HANGING_BLOCK, bracket, position)"
    },
    {
        "original": "def save(self): \n        file_path = self.get_config_path()\n        contents = self.get_contents()\n        with open(file_path, mode='w') as cfg_file:\n            cfg_file.write(contents)",
        "rewrite": "def save(self):\n    file_path = self.get_config_path()\n    contents = self.get_contents()\n    with open(file_path, 'w') as cfg_file:\n        cfg_file.write(contents)"
    },
    {
        "original": "def score(self, features, classes, scoring_function=None, **scoring_function_kwargs): \n        new_feature = self.ensemble.predict(features)\n\n        if scoring_function is None:\n            return accuracy_score(classes, new_feature)\n        else:\n            return scoring_function(classes, new_feature, **scoring_function_kwargs)",
        "rewrite": "def score(self, features, classes, scoring_function=None, **scoring_function_kwargs): \n    new_feature = self.ensemble.predict(features)\n\n    if scoring_function is None:\n        return accuracy_score(classes, new_feature)\n    else:\n        return scoring_function(classes, new_feature, **scoring_function_kwargs)"
    },
    {
        "original": "def get_xval_models(self, key=None): \n        return {model.model_id: model.get_xval_models(key) for model in self.models}",
        "rewrite": "def get_xval_models(self, key=None):\n    return {model.model_id: model.get_xval_models(key) for model in self.models}"
    },
    {
        "original": "def user(self, user_id): \n        resource = urijoin(self.RUSERS, str(user_id) + self.CJSON)\n\n        params = {}\n\n        response = self._call(resource, params)\n\n        return response",
        "rewrite": "def user(self, user_id): \n        resource = urljoin(self.RUSERS, str(user_id) + self.CJSON)\n\n        params = {}\n\n        response = self._call(resource, params)\n\n        return response"
    },
    {
        "original": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs): \n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                          ",
        "rewrite": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs): \n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)"
    },
    {
        "original": "def save_albums(self, *albums): \n        _albums = [(obj if isinstance(obj, str) else obj.id) for obj in albums]\n        await self.user.http.save_albums(','.join(_albums))",
        "rewrite": "def save_albums(self, *albums):\n        _albums = [(obj.id if not isinstance(obj, str) else obj) for obj in albums]\n        await self.user.http.save_albums(','.join(_albums))"
    },
    {
        "original": "def convert_to_string(self, productions): \n    symbols = []\n    for production in tf.unstack(productions, axis=1):\n      lhs, rhs = self.production_rules[tf.argmax(input=production, axis=-1)]\n      if not symbols:  # first iteration\n        if lhs != self.start_symbol:\n          raise ValueError(\"`productions` must begin with `self.start_symbol`.\")\n        symbols = rhs\n      else:\n        # Greedily unroll the nonterminal symbols based on the first occurrence\n        # in a linear sequence.\n        index = symbols.index(lhs)\n      ",
        "rewrite": "def convert_to_string(self, productions): \n    symbols = []\n    for production in tf.unstack(productions, axis=1):\n      lhs, rhs = self.production_rules[tf.argmax(input=production, axis=-1)]\n      if not symbols:\n        if lhs != self.start_symbol:\n          raise ValueError(\"`productions` must begin with `self.start_symbol`.\")\n        symbols = rhs\n      else:\n        index = symbols.index(lhs)"
    },
    {
        "original": "def one_step(self, current_state, previous_kernel_results): \n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'elliptical_slice', 'one_step'),\n        values=[self._seed_stream,\n                current_state,\n                previous_kernel_results.log_likelihood]):\n      with tf.compat.v1.name_scope('initialize'):\n        [\n            init_state_parts,\n            init_log_likelihood\n        ] = _prepare_args(\n            self.log_likelihood_fn,\n       ",
        "rewrite": "def one_step(self, current_state, previous_kernel_results): \n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'elliptical_slice', 'one_step'),\n        values=[self._seed_stream,\n                current_state,\n                previous_kernel_results.log_likelihood]):\n      with tf.compat.v1.name_scope('initialize'):\n        [\n            init_state_parts,\n            init_log_likelihood\n        ] = _prepare_args(\n            self.log_likelihood_fn,"
    },
    {
        "original": "def add_map(incoming, outgoing): \n    db_path = Config().get(ConfigKeys.pricedb_path)\n    session = get_session(db_path)\n\n    new_map = SymbolMap()\n    new_map.in_symbol = incoming\n    new_map.out_symbol = outgoing\n\n    session.add(new_map)\n    session.commit()\n    click.echo(\"Record saved.\")",
        "rewrite": "def add_map(incoming, outgoing): \n    db_path = Config().get(ConfigKeys.pricedb_path)\n    session = get_session(db_path)\n\n    new_map = SymbolMap()\n    new_map.in_symbol = incoming\n    new_map.out_symbol = outgoing\n\n    session.add(new_map)\n    session.commit()\n    click.echo(\"Record saved.\")"
    },
    {
        "original": "def _init_rate_limit(self): \n\n        url = urijoin(self.base_url, 'projects', self.owner + '%2F' + self.repository)\n        try:\n            response = super().fetch(url)\n            self.update_rate_limit(response)\n        except requests.exceptions.HTTPError as error:\n            if error.response.status_code == 401:\n                raise error\n            else:\n                logger.warning(\"Rate limit not initialized: %s\",",
        "rewrite": "def init_rate_limit(self): \n        url = urijoin(self.base_url, 'projects', self.owner + '%2F' + self.repository)\n        try:\n            response = super().fetch(url)\n            self.update_rate_limit(response)\n        except requests.exceptions.HTTPError as error:\n            if error.response.status_code == 401:\n                raise error\n            else:\n                logger.warning(\"Rate limit not initialized: %s\", str(error))"
    },
    {
        "original": "def from_db_value(self, value, expression, connection, context): \n        if value is None:\n            return value\n        return self.enum[value]",
        "rewrite": "def from_db_value(self, value, expression, connection, context): \n    if value is None:\n        return value\n    return self.enum[value]"
    },
    {
        "original": "def set_alpn_protos(self, protos): \n        # Take the list of protocols and join them together, prefixing them\n        # with their lengths.\n        protostr = b''.join(\n            chain.from_iterable((int2byte(len(p)), p) for p in protos)\n        )\n\n        # Build a C string from the list. We don't need to save this off\n        # because OpenSSL immediately copies the data out.\n        input_str = _ffi.new(\"unsigned char[]\", protostr)\n        _lib.SSL_set_alpn_protos(self._ssl, input_str, len(protostr))",
        "rewrite": "def set_alpn_protos(self, protos):\n    protostr = b''.join(\n        int2byte(len(p)) + p for p in protos\n    )\n\n    input_str = _ffi.new(\"unsigned char[]\", protostr)\n    _lib.SSL_set_alpn_protos(self._ssl, input_str, len(protostr))"
    },
    {
        "original": "def _build_insert_compiler(self, rows: List[Dict]): \n\n        # create model objects, we also have to detect cases\n        # such as:\n        #   [dict(first_name='swen'), dict(fist_name='swen', last_name='kooij')]\n        # we need to be certain that each row specifies the exact same\n        # amount of fields/columns\n        objs = []\n        field_count = len(rows[0])\n        for index, row in enumerate(rows):\n            if field_count != len(row):\n     ",
        "rewrite": "def _build_insert_compiler(self, rows: List[Dict]): \n\n        objs = []\n        field_count = len(rows[0])\n        for index, row in enumerate(rows):\n            if field_count != len(row):"
    },
    {
        "original": " \n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()",
        "rewrite": "if self.read_eof():\n    return False\nself._stream.save_context()\nif c == self._stream.peek_char():\n    self._stream.incpos()\n    return self._stream.validate_context()\nreturn self._stream.restore_context()"
    },
    {
        "original": "def add_member(self, email, fullname, membership_type='normal'): \n        return self.fetch_json(\n            uri_path=self.base_uri + '/members',\n            http_method='PUT',\n            query_params={\n                'email': email,\n                'fullName': fullname,\n                'type': membership_type\n            }\n        )",
        "rewrite": "def add_member(self, email, fullname, membership_type='normal'): \n    return self.fetch_json(\n        uri_path=self.base_uri + '/members',\n        http_method='PUT',\n        query_params={\n            'email': email,\n            'fullName': fullname,\n            'type': membership_type\n        }\n    )"
    },
    {
        "original": "def one_step(self, current_state, previous_kernel_results): \n\n    @tfp.mcmc.internal.util.make_innermost_setter\n    def set_num_leapfrog_steps(kernel_results, num_leapfrog_steps):\n      return kernel_results._replace(\n          accepted_results=kernel_results.accepted_results._replace(\n              num_leapfrog_steps=num_leapfrog_steps))\n\n    step_size = previous_kernel_results.new_step_size\n    previous_kernel_results = set_num_leapfrog_steps(\n        previous_kernel_results, self._num_leapfrog_steps(step_size))\n\n    new_state, kernel_results = self._kernel.one_step(\n        self._flatten_state(current_state), previous_kernel_results)\n    return self._unflatten_state(new_state), kernel_results",
        "rewrite": "def one_step(self, current_state, previous_kernel_results): \n\n    @tfp.mcmc.internal.util.make_innermost_setter\n    def set_num_leapfrog_steps(kernel_results, num_leapfrog_steps):\n        return kernel_results._replace(\n            accepted_results=kernel_results.accepted_results._replace(\n                num_leapfrog_steps=num_leapfrog_steps))\n\n    step_size = previous_kernel_results.new_step_size\n    previous_kernel_results = set_num_leapfrog_steps(\n        previous_kernel_results, self._num_leapfrog_steps(step_size))\n\n    new_state, kernel_results = self._kernel.one_step(\n        self._flatten_state(current_state), previous_kernel_results)\n    return self._unflatten_state(new_state), kernel_results"
    },
    {
        "original": "def valid(self): \n        if self.finished is not None:\n            return False\n\n        with self._db_conn() as conn:\n            row = conn.get(",
        "rewrite": "def valid(self):\n    if self.finished != None:\n        return False\n\n    with self._db_conn() as conn:\n        row = conn.get(\"-\")"
    },
    {
        "original": " \n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  state_parts = [\n      tf.convert_to_tensor(value=s, name='current_state') for s in state_parts\n  ]\n  if state_gradients_are_stopped:\n    state_parts = [tf.stop_gradient(x) for x in state_parts]\n  target_log_prob, grads_target_log_prob = mcmc_util.maybe_call_fn_and_grads(\n      target_log_prob_fn,\n      state_parts,\n      target_log_prob,\n      grads_target_log_prob)\n  step_sizes = (list(step_size) if mcmc_util.is_list_like(step_size)\n                else [step_size])\n  step_sizes = [\n      tf.convert_to_tensor(\n          value=s, name='step_size', dtype=target_log_prob.dtype)\n      for s in step_sizes\n  ]\n  if len(step_sizes)",
        "rewrite": "state_parts = [tf.convert_to_tensor(value=s, name='current_state') if mcmc_util.is_list_like(state) else [s] for s in state]\nif state_gradients_are_stopped:\n    state_parts = [tf.stop_gradient(x) for x in state_parts]\ntarget_log_prob, grads_target_log_prob = mcmc_util.maybe_call_fn_and_grads(target_log_prob_fn, state_parts, target_log_prob, grads_target_log_prob)\nstep_sizes = [tf.convert_to_tensor(value=s, name='step_size', dtype=target_log_prob.dtype) if mcmc_util.is_list_like(step_size) else [s] for s in step_size]"
    },
    {
        "original": " \n        if isinstance(\n            node.value,\n            (\n                ast.Constant,  # type: ignore\n                ast.Name,\n                ast.NameConstant,\n                ast.Num,\n                ast.Str,\n         ",
        "rewrite": "if isinstance(node.value, (ast.Constant, ast.Name, ast.NameConstant, ast.Num, ast.Str)):"
    },
    {
        "original": "def transcripts_by_gene(self, build='37'): \n        hgnc_transcripts = {}\n        LOG.info(\"Fetching all transcripts\")\n        for transcript in self.transcript_collection.find({'build':build}):\n            hgnc_id = transcript['hgnc_id']\n            if not hgnc_id in hgnc_transcripts:\n                hgnc_transcripts[hgnc_id] = []\n            \n            hgnc_transcripts[hgnc_id].append(transcript)\n        \n        return hgnc_transcripts",
        "rewrite": "def transcripts_by_gene(self, build='37'):\n    hgnc_transcripts = {}\n    LOG.info(\"Fetching all transcripts\")\n    \n    for transcript in self.transcript_collection.find({'build': build}):\n        hgnc_id = transcript['hgnc_id']\n        \n        if hgnc_id not in hgnc_transcripts:\n            hgnc_transcripts[hgnc_id] = []\n        \n        hgnc_transcripts[hgnc_id].append(transcript)\n    \n    return hgnc_transcripts"
    },
    {
        "original": "def send_message(self, text, user_ids, thread_id=None): \n    user_ids = _get_user_ids(self, user_ids)\n    if not isinstance(text, str) and isinstance(user_ids, (list, str)):\n        self.logger.error('Text must be an string, user_ids must be an list or string')\n        return False\n\n    if self.reached_limit('messages'):\n        self.logger.info(\"Out of messages for today.\")\n        return False\n\n    self.delay('message')\n    urls = self.extract_urls(text)\n    item_type = 'link' if urls else 'text'\n    if self.api.send_direct_item(\n        item_type,\n        user_ids,\n        text=text,\n ",
        "rewrite": "def send_message(self, text, user_ids, thread_id=None): \n    user_ids = _get_user_ids(self, user_ids)\n    if not isinstance(text, str) or not isinstance(user_ids, (list, str)):\n        self.logger.error('Text must be a string, user_ids must be a list or string')\n        return False\n\n    if self.reached_limit('messages'):\n        self.logger.info(\"Out of messages for today.\")\n        return False\n\n    self.delay('message')\n    urls = self.extract_urls(text)\n    item_type = 'link' if urls else 'text'\n    if self.api.send_direct_item(\n        item_type,\n        user_ids,\n        text=text,\n        thread_id=thread_id):"
    },
    {
        "original": "def factory(cls, filename, mode, on_close): \n        _temp_file = tempfile.TemporaryFile()\n        proxy = cls(_temp_file, filename, mode, on_close=on_close)\n        return proxy",
        "rewrite": "def create_proxy(cls, filename, mode, on_close):\n        temp_file = tempfile.TemporaryFile()\n        proxy = cls(temp_file, filename, mode, on_close=on_close)\n        return proxy"
    },
    {
        "original": "def clear(self): \n        self._fwdm.clear()\n        self._invm.clear()\n        self._sntl.nxt = self._sntl.prv = self._sntl",
        "rewrite": "def clear(self):\n    self._fwdm.clear()\n    self._invm.clear()\n    self._sntl.nxt = self._sntl.prv = self._sntl"
    },
    {
        "original": "def get_proxy_version(self): \n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
        "rewrite": "def get_proxy_version(self):\n    self._download_sql_proxy_if_needed()\n    command_to_run = [self.sql_proxy_path]\n    command_to_run.extend(['--version'])\n    command_to_run.extend(self._get_credential_parameters())\n    result = subprocess.check_output(command_to_run).decode('utf-8')\n    pattern = re.compile(\"^.*[Vv]ersion ([^;]*);.*$\")\n    m = pattern.match(result)\n    if m:\n        return m.group(1)\n    else:\n        return None"
    },
    {
        "original": "def close(self): \n        if self.closed:\n            raise ValueError(\"Cannot close a closed state\")\n        if self.call is not None:\n            self.call.cancel()\n        self.closed = True",
        "rewrite": "def close(self):\n    if self.closed:\n        raise ValueError(\"Cannot close a closed state\")\n    if self.call is not None:\n        self.call.cancel()\n    self.closed = True"
    },
    {
        "original": "def load_string(self, string_data, share_name, directory_name, file_name, **kwargs): \n        self.connection.create_file_from_text(share_name, directory_name,\n                                              file_name, string_data, **kwargs)",
        "rewrite": "def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):\n    self.connection.create_file_from_text(share_name, directory_name,\n                                          file_name, string_data, **kwargs)"
    },
    {
        "original": "def global_iterator_to_indices(self, git=None): \n        # unwind global iteration count into loop counters:\n        base_loop_counters = {}\n        global_iterator = symbol_pos_int('global_iterator')\n        idiv = implemented_function(sympy.Function(str('idiv')), lambda x, y: x//y)\n        total_length = 1\n        last_incr = 1\n        for var_name, start, end, incr in reversed(self._loop_stack):\n            loop_var = symbol_pos_int(var_name)\n\n            # This unspools the iterations:\n           ",
        "rewrite": "def global_iterator_to_indices(self, git=None):\n        base_loop_counters = {}\n        global_iterator = symbol_pos_int('global_iterator')\n        idiv = implemented_function(sympy.Function(str('idiv')), lambda x, y: x//y)\n        total_length = 1\n        last_incr = 1\n        \n        for var_name, start, end, incr in reversed(self._loop_stack):\n            loop_var = symbol_pos_int(var_name)\n            # unspool iterations\n            # loop_var = idiv(global_iterator, total_length) % (end - start + incr)\n            global_iterator = global_iterator - (loop_var - symbol_pos_int(var_name)) * last_incr\n            base_loop_counters[var_name] = loop_var\n            last_incr *= end - start + incr\n            total_length *= end - start + incr"
    },
    {
        "original": " \n    if not isinstance(num, int):\n        num = int(num)\n    return num != 0 and ((num & (num - 1)) == 0)",
        "rewrite": "if not isinstance(num, int):\n    num = int(num)\nreturn num != 0 and ((num & (num - 1)) == 0)"
    },
    {
        "original": "def b64_encode(self): \n        encoded = _lib.NETSCAPE_SPKI_b64_encode(self._spki)\n        result = _ffi.string(encoded)\n        _lib.OPENSSL_free(encoded)\n        return result",
        "rewrite": "def b64_encode(self): \n    encoded = _lib.NETSCAPE_SPKI_b64_encode(self._spki)\n    result = _ffi.string(encoded)\n    _lib.OPENSSL_free(encoded)\n    return result"
    },
    {
        "original": "def view_decorator(function_decorator): \n\n\tdef simple_decorator(View):\n\t\tView.dispatch = method_decorator(function_decorator)(View.dispatch)\n\t\treturn View\n\n\treturn simple_decorator",
        "rewrite": "def view_decorator(function_decorator): \n\n\tdef simple_decorator(View):\n\t\tView.dispatch = method_decorator(function_decorator)(View.dispatch)\n\t\treturn View\n\n\treturn simple_decorator"
    },
    {
        "original": " \n\n    basis, lengths = filters.constant_q(sr,\n                                        fmin=fmin,\n                                        n_bins=n_bins,\n                                       ",
        "rewrite": "basis, lengths = filters.constant_q(sr, fmin=fmin, n_bins=n_bins)"
    },
    {
        "original": "def pseudo_tempname(self): \n        try:\n            pid = os.getpid()\n        except:\n            pid = random.randint(0,sys.maxint)\n        return os.path.join(self.install_dir, \"test-easy-install-%s\" % pid)",
        "rewrite": "def pseudo_tempname(self): \n    try:\n        pid = os.getpid()\n    except:\n        pid = random.randint(0, sys.maxsize)\n    return os.path.join(self.install_dir, \"test-easy-install-%s\" % pid)"
    },
    {
        "original": "def preprocess_options(args, search_for): \n    i = 0\n    while i < len(args):\n        arg = args[i]\n        if arg.startswith(\"--\"):\n            try:\n                option, val = arg[2:].split(\"=\", 1)\n            except ValueError:\n                option, val = arg[2:], None\n            try:\n               ",
        "rewrite": "try:\n                if option in search_for:\n                    args[args.index(arg)]=option\n                    args.insert(args.index(arg) + 1, val)\n           except ValueError:\n               print('Error')\n           del args[i]\n        i += 1"
    },
    {
        "original": "def equal_ignore_order(a, b): \n  unmatched = list(b)\n  for element in a:\n    try:\n      unmatched.remove(element)\n    except ValueError:\n      return False\n  return not unmatched",
        "rewrite": "def equal_ignore_order(a, b):\n    unmatched = list(b)\n    for element in a:\n        try:\n            unmatched.remove(element)\n        except ValueError:\n            return False\n    return not unmatched"
    },
    {
        "original": "def stream_from_fd(fd, loop): \n    reader = asyncio.StreamReader(loop=loop)\n    protocol = asyncio.StreamReaderProtocol(reader, loop=loop)\n    waiter = asyncio.futures.Future(loop=loop)\n\n    transport = UnixFileDescriptorTransport(\n        loop=loop,\n        fileno=fd,\n        protocol=protocol,\n        waiter=waiter,\n    )\n\n    try:\n        yield from waiter\n    except Exception:\n        transport.close()\n\n    if loop.get_debug():\n        logger.debug(\"Read fd %r connected: (%r, %r)\", fd, transport, protocol)\n    return reader, transport",
        "rewrite": "def stream_from_fd(fd, loop): \n    reader = asyncio.StreamReader(loop=loop)\n    protocol = asyncio.StreamReaderProtocol(reader, loop=loop)\n    waiter = asyncio.Future(loop=loop)\n\n    transport = asyncio.UnixFileDescriptorTransport(\n        loop=loop,\n        fileno=fd,\n        protocol=protocol,\n        waiter=waiter,\n    )\n\n    try:\n        await waiter\n    except Exception:\n        transport.close()\n\n    if loop.get_debug():\n        logger.debug(\"Read fd %r connected: (%r, %r)\", fd, transport, protocol)\n    return reader, transport"
    },
    {
        "original": "def buildhtmlheader(self): \n        self.htmlheader = ''\n        # If the JavaScript assets have already been injected, don't bother re-sourcing them.\n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:\n                self.htmlheader += js",
        "rewrite": "def buildhtmlheader(self): \n        self.htmlheader = ''\n        global _js_initialized\n        if '_js_initialized' not in globals() or not _js_initialized:\n            for css in self.header_css:\n                self.htmlheader += css\n            for js in self.header_js:\n                self.htmlheader += js"
    },
    {
        "original": "def measure_aabb(fbasename=None, log=None, coord_system='CARTESIAN'): \n    # TODO: add center point, spherical coordinate system\n    fext = os.path.splitext(fbasename)[1][1:].strip().lower()\n    if fext != 'xyz':\n        fin = 'TEMP3D_aabb.xyz'\n        run(log=log, file_in=fbasename, file_out=fin, script=None)\n    else:\n        fin = fbasename\n    fread = open(fin, 'r')\n    aabb = {'min': [999999.0, 999999.0, 999999.0], 'max': [-999999.0, -999999.0, -999999.0]}\n    for line in fread:\n        x_co, y_co, z_co = line.split()\n        x_co = util.to_float(x_co)\n        y_co = util.to_float(y_co)\n     ",
        "rewrite": "import os\n\ndef measure_aabb(fbasename=None, log=None, coord_system='CARTESIAN'): \n    fext = os.path.splitext(fbasename)[1][1:].strip().lower()\n    \n    if fext != 'xyz':\n        fin = 'TEMP3D_aabb.xyz'\n        run(log=log, file_in=fbasename, file_out=fin, script=None)\n    else:\n        fin = fbasename\n    \n    with open(fin, 'r') as fread:\n        aabb = {'min': [999999.0, 999999.0, 999999.0], 'max': [-999999.0, -999999.0, -999999.0]}\n        \n        for line in fread:\n            x_co, y_co, z_co = map(float, line.split())\n            aabb['min'][0], aabb['min'][1], aabb['min'][2] = min(aabb['min'][0], x_co), min(aabb['min'][1], y_co), min(aabb['min'][2], z_co)\n            aabb['max'][0], aabb['max'][1], aabb['max'][2] = max(aabb['max'][0], x_co), max(aabb['max'][1], y_co), max(aabb['max'][2], z_co)"
    },
    {
        "original": "def start_tag(el): \n    return '<%s%s>' % (\n        el.tag, ''.join([' %s=\"%s\"' % (name, html_escape(value, True))\n                         for name, value in el.attrib.items()]))",
        "rewrite": "def start_tag(el):\n    return '<{}{}>'.format(el.tag, ''.join([' {}=\"{}\"'.format(name, html_escape(value, True)) for name, value in el.attrib.items()]))"
    },
    {
        "original": " \n    import matplotlib.pyplot as plt\n    # Set up the boundaries id\n    bid_lid = boundaries_id\n    if labels_id is not None:\n        bid_lid += \" + \" + labels_id\n    try:\n        # Read file\n        jam = jams.load(file_struct.ref_file)\n        ann = jam.search(namespace='segment_.*')[0]\n        ref_inters, ref_labels = ann.to_interval_values()\n\n        # To times\n        ref_times = utils.intervals_to_times(ref_inters)\n        all_boundaries = [ref_times, est_times]\n      ",
        "rewrite": "import matplotlib.pyplot as plt\n\nbid_lid = boundaries_id\nif labels_id is not None:\n    bid_lid += \" + \" + labels_id\n\ntry:\n    jam = jams.load(file_struct.ref_file)\n    ann = jam.search(namespace='segment_.*')[0]\n    ref_inters, ref_labels = ann.to_interval_values()\n\n    ref_times = utils.intervals_to_times(ref_inters)\n    all_boundaries = [ref_times, est_times]"
    },
    {
        "original": "def cpu_percent(interval=0.1, percpu=False): \n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        # this usually indicates a float precision issue\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy\n",
        "rewrite": "def cpu_percent(interval=0.1, percpu=False): \n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy"
    },
    {
        "original": "def calculate_top_margin(self): \n\t\tself.border_top = 5\n\t\tif self.show_graph_title:\n\t\t\tself.border_top += self.title_font_size\n\t\tself.border_top += 5\n\t\tif self.show_graph_subtitle:\n\t\t\tself.border_top += self.subtitle_font_size",
        "rewrite": "def calculate_top_margin(self):\n    self.border_top = 5\n    if self.show_graph_title:\n        self.border_top += self.title_font_size\n    self.border_top += 5\n    if self.show_graph_subtitle:\n        self.border_top += self.subtitle_font_size"
    },
    {
        "original": "def create(self, subject, displayName, issuerToken, expiration, secret): \n        check_type(subject, basestring)\n        check_type(displayName, basestring)\n        check_type(issuerToken, basestring)\n        check_type(expiration, basestring)\n        check_type(secret, basestring)\n\n        payload = {\n            \"sub\": subject,\n            \"name\": displayName,\n            \"iss\": issuerToken,\n            \"exp\": expiration\n        }\n\n     ",
        "rewrite": "def create(self, subject, displayName, issuerToken, expiration, secret): \n    check_type(subject, str)\n    check_type(displayName, str)\n    check_type(issuerToken, str)\n    check_type(expiration, str)\n    check_type(secret, str)\n\n    payload = {\n        \"sub\": subject,\n        \"name\": displayName,\n        \"iss\": issuerToken,\n        \"exp\": expiration\n    }"
    },
    {
        "original": "def get_frame(frame_id, rows=10, rows_offset=0, cols=-1, full_cols=-1, cols_offset=0, light=False): \n        fr = H2OFrame()\n        fr._ex._cache._id = frame_id\n        try:\n            fr._ex._cache.fill(rows=rows, rows_offset=rows_offset, cols=cols, full_cols=full_cols, cols_offset=cols_offset, light=light)\n        except EnvironmentError:\n            return None\n        return fr",
        "rewrite": "def get_frame(frame_id, rows=10, rows_offset=0, cols=-1, full_cols=-1, cols_offset=0, light=False):\n    fr = H2OFrame()\n    fr._ex._cache._id = frame_id\n    try:\n        fr._ex._cache.fill(rows=rows, rows_offset=rows_offset, cols=cols, full_cols=full_cols, cols_offset=cols_offset, light=light)\n    except EnvironmentError:\n        return None\n    return fr"
    },
    {
        "original": "def score(self, x, y, w=None, **kwargs): \n        u = y - self.predict(x, **kwargs)\n        v = y - y.mean()\n        if w is None:\n            w = np.ones_like(u)\n        return 1 - (w * u * u).sum() / (w * v * v).sum()",
        "rewrite": "def score(self, x, y, w=None, **kwargs): \n    u = y - self.predict(x, **kwargs)\n    v = y - y.mean()\n    if w is None:\n        w = np.ones_like(u)\n    return 1 - (np.sum(w * u * u) / np.sum(w * v * v))"
    },
    {
        "original": "def _duplicated_isinstance_types(node): \n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if",
        "rewrite": "def _duplicated_isinstance_types(node): \n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if ..."
    },
    {
        "original": "def default_value(field): \n\n    def default_value_func(self):\n        attname = lambda x: get_real_fieldname(field, x)\n\n        if getattr(self, attname(get_language()), None):\n            result = getattr(self, attname(get_language()))\n        elif getattr(self, attname(get_language()[:2]), None):\n            result = getattr(self, attname(get_language()[:2]))\n        else:\n            default_language = fallback_language()\n            if getattr(self, attname(default_language), None):\n                result = getattr(self,",
        "rewrite": "def default_value(field): \n\n    def default_value_func(self):\n        attname = lambda x: get_real_fieldname(field, x)\n\n        result = None\n        language = get_language()\n        if getattr(self, attname(language), None):\n            result = getattr(self, attname(language))\n        elif getattr(self, attname(language[:2]), None):\n            result = getattr(self, attname(language[:2]))\n        else:\n            default_language = fallback_language()\n            if getattr(self, attname(default_language), None):\n                result = getattr(self, attname(default_language))\n                \n        return result"
    },
    {
        "original": "def delete_cloud_service(self, cloud_service_id): \n        _validate_not_none('cloud_service_id', cloud_service_id)\n        path = self._get_cloud_services_path(cloud_service_id)\n        return self._perform_delete(path, as_async=True)",
        "rewrite": "def delete_cloud_service(self, cloud_service_id):\n    _validate_not_none('cloud_service_id', cloud_service_id)\n    path = self._get_cloud_services_path(cloud_service_id)\n    return self._perform_delete(path, as_async=True)"
    },
    {
        "original": "def get_extension(filepath, check_if_exists=False): \n    if check_if_exists:\n        if not os.path.exists(filepath):\n            err = 'File not found: ' + filepath\n            log.error(err)\n            raise IOError(err)\n\n    try:\n        rest, ext = os.path.splitext(filepath)\n    except:\n        raise\n    else:\n        return ext",
        "rewrite": "def get_extension(filepath, check_if_exists=False): \n    if check_if_exists:\n        if not os.path.exists(filepath):\n            err = 'File not found: ' + filepath\n            log.error(err)\n            raise IOError(err)\n\n    try:\n        rest, ext = os.path.splitext(filepath)\n    except Exception as e:\n        raise e\n    else:\n        return ext"
    },
    {
        "original": "def _check_in_loop(self, node, node_name): \n        _node = node.parent\n        while _node:\n            if isinstance(_node, (astroid.For, astroid.While)):\n                if node not in _node.orelse:\n                    return\n\n            if isinstance(_node, (astroid.ClassDef, astroid.FunctionDef)):\n                break\n            if (\n     ",
        "rewrite": "def _check_in_loop(self, node, node_name):\n        _node = node.parent\n        while _node:\n            if isinstance(_node, (astroid.For, astroid.While)):\n                if node not in _node.orelse:\n                    return\n\n            if isinstance(_node, (astroid.ClassDef, astroid.FunctionDef)):\n                break\n            if (\n                # continue checking conditions here\n            ):\n                return\n            _node = _node.parent"
    },
    {
        "original": "def upload(self, source_files, s3_folder=None): \n\n        if s3_folder is None:\n            folder = self.prefix\n        else:\n            folder = '%s/%s' % (self.prefix, s3_folder)\n\n        if isinstance(source_files, list):\n            for file_tuple in source_files:\n                self.__upload_file(file_tuple, folder)\n        elif isinstance(source_files, tuple):\n            self.__upload_file(source_files, folder)\n        else:\n",
        "rewrite": "def upload(self, source_files, s3_folder=None):\n    \n    if s3_folder is None:\n        folder = self.prefix\n    else:\n        folder = '{}/{}'.format(self.prefix, s3_folder)\n\n    if isinstance(source_files, list):\n        for file_tuple in source_files:\n            self.__upload_file(file_tuple, folder)\n    elif isinstance(source_files, tuple):\n        self.__upload_file(source_files, folder)\n    else:\n        # Handle other cases or add desired functionality\n        pass"
    },
    {
        "original": "def insecure_transport(self): \n        origin = os.environ.get('OAUTHLIB_INSECURE_TRANSPORT')\n        if current_app.debug or current_app.testing:\n            try:\n                os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n                yield\n            finally:\n                if origin:\n                    os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = origin\n       ",
        "rewrite": "def insecure_transport(self):\n    origin = os.environ.get('OAUTHLIB_INSECURE_TRANSPORT')\n    if current_app.debug or current_app.testing:\n        try:\n            os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n            yield\n        finally:\n            if origin:\n                os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = origin"
    },
    {
        "original": "def read_cms(cms=None, domains=None): \n        assert_is_type(cms, [list])\n        return [ConfusionMatrix(cm, domains) for cm in cms]",
        "rewrite": "def read_cms(cms=None, domains=None):\n    assert isinstance(cms, list)\n    return [ConfusionMatrix(cm, domains) for cm in cms]"
    },
    {
        "original": "def load_default_config(ipython_dir=None): \n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n    profile_dir = os.path.join(ipython_dir, 'profile_default')\n    cl = PyFileConfigLoader(default_config_file_name, profile_dir)\n    try:\n        config = cl.load_config()\n    except ConfigFileNotFound:\n        # no config found\n        config = Config()\n    return config",
        "rewrite": "def load_default_config(ipython_dir=None):\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n    profile_dir = os.path.join(ipython_dir, 'profile_default')\n    cl = PyFileConfigLoader(default_config_file_name, profile_dir)\n    try:\n        config = cl.load_config()\n    except ConfigFileNotFound:\n        config = Config()\n    return config"
    },
    {
        "original": "def debugger(): \n    dbg = _current[0]\n    if dbg is None or not dbg.active:\n        dbg = _current[0] = RemoteCeleryTrepan()\n    return dbg",
        "rewrite": "def debugger(): \n    dbg = _current[0]\n    if dbg is None or not dbg.active:\n        dbg = _current[0] = RemoteCeleryTrepan()\n    return dbg"
    },
    {
        "original": "def _prepare_hiveconf(d): \n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )",
        "rewrite": "def _prepare_hiveconf(d): \n    if not d:\n        return []\n    return as_flattened_list(\n        zip([\"-hiveconf\"] * len(d),\n            [\"{}={}\".format(k, v) for k, v in d.items()])\n    )"
    },
    {
        "original": "def is_binary_string(content): \n\n        textchars = (bytearray([7, 8, 9, 10, 12, 13, 27]) +\n                     bytearray(range(0x20, 0x100)))\n        return bool(content.translate(None, textchars))",
        "rewrite": "def is_binary_string(content):\n    textchars = bytearray([7, 8, 9, 10, 12, 13, 27]) + bytearray(range(0x20, 0x100))\n    return bool(content.translate(None, textchars))"
    },
    {
        "original": "def delete_directory(self, dirname): \n        key = os.path.join(self._bucket_root, dirname)\n        if not key.endswith('/'):\n            key += '/'\n\n        key_objects = [{'Key': obj.key}\n                       for obj in self._bucket.objects.filter(Prefix=key)]\n        if len(key_objects) == 0:\n            msg = 'No objects in bucket directory {}'.format(dirname)\n            raise RuntimeError(msg)\n        delete_keys =",
        "rewrite": "def delete_directory(self, dirname): \n    key = os.path.join(self._bucket_root, dirname)\n    if not key.endswith('/'):\n        key += '/'\n\n    key_objects = [{'Key': obj.key}\n                   for obj in self._bucket.objects.filter(Prefix=key)]\n    if len(key_objects) == 0:\n        msg = 'No objects in bucket directory {}'.format(dirname)\n        raise RuntimeError(msg)\n    delete_keys = [obj['Key'] for obj in key_objects]\n    self._bucket.objects.delete(delete_keys)"
    },
    {
        "original": "def _build_spark_submit_command(self, application): \n        connection_cmd = self._get_spark_binary_path()\n\n        # The url ot the spark master\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        if self._conf:\n            for key in self._conf:\n                connection_cmd += [\"--conf\", \"{}={}\".format(key, str(self._conf[key]))]\n        if self._env_vars and (self._is_kubernetes or self._is_yarn):\n            if self._is_yarn:\n                tmpl = \"spark.yarn.appMasterEnv.{}={}\"\n ",
        "rewrite": "def _build_spark_submit_command(self, application):\n        connection_cmd = self._get_spark_binary_path()\n\n        connection_cmd += [\"--master\", self._connection['master']]\n\n        if self._conf:\n            for key in self._conf:\n                connection_cmd += [\"--conf\", \"{}={}\".format(key, str(self._conf[key])]\n                \n        if self._env_vars and (self._is_kubernetes or self._is_yarn):\n            if self._is_yarn:\n                tmpl = \"spark.yarn.appMasterEnv.{}={}\""
    },
    {
        "original": "def _loadDeclarations(self): \n        if not hasattr(self, \"_interfaces\"):\n            self._interfaces = []\n        self._setAttrListener = self._declrCollector\n        self._declr()\n        self._setAttrListener = None\n\n        for i in self._interfaces:\n            i._isExtern = self._isExtern\n            i._loadDeclarations()\n\n        for p in self._params:\n            p.setReadOnly()\n        \n    ",
        "rewrite": "def _loadDeclarations(self): \n    if not hasattr(self, \"_interfaces\"):\n        self._interfaces = []\n    \n    self._setAttrListener = self._declrCollector\n    self._declr()\n    self._setAttrListener = None\n\n    for i in self._interfaces:\n        i._isExtern = self._isExtern\n        i._loadDeclarations()\n\n    for p in self._params:\n        p.setReadOnly()"
    },
    {
        "original": "def _maxiter_default(self): \n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600",
        "rewrite": "def _maxiter_default(self): \n        mode = self.mode\n        if mode == \"KK\":\n            return 100 * len(self.nodes)\n        elif mode == \"major\":\n            return 200\n        else:\n            return 600"
    },
    {
        "original": "def _scale_size(size, scale): \n    w, h = size\n    return int(w * float(scale) + 0.5), int(h * float(scale) + 0.5)",
        "rewrite": "def _scale_size(size, scale):\n    w, h = size\n    return int(w * scale + 0.5), int(h * scale + 0.5)"
    },
    {
        "original": "def direct_upload(request): \n    if request.method == \"POST\":\n        try:\n            form = YoutubeDirectUploadForm(request.POST, request.FILES)\n            # upload the file to our server\n            if form.is_valid():\n                uploaded_video = form.save()\n\n                # send this file to youtube\n                api = Api()\n       ",
        "rewrite": "def direct_upload(request):\n    if request.method == \"POST\":\n        try:\n            form = YoutubeDirectUploadForm(request.POST, request.FILES)\n            if form.is_valid():\n                uploaded_video = form.save()\n                api = Api()"
    },
    {
        "original": "def parse_questions(raw_page): \n        raw_questions = json.loads(raw_page)\n        questions = raw_questions['items']\n        for question in questions:\n            yield question",
        "rewrite": "def parse_questions(raw_page): \n    raw_questions = json.loads(raw_page)\n    questions = raw_questions['items']\n    for question in questions:\n        yield question"
    },
    {
        "original": "def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n        tstream = BytearrayStream()\n\n        self.revocation_code.write(tstream, kmip_version=kmip_version)\n        if self.revocation_message is not None:\n            self.revocation_message.write(tstream, kmip_version=kmip_version)\n\n        # Write the length and value\n        self.length = tstream.length()\n        super(RevocationReason, self).write(ostream, kmip_version=kmip_version)\n        ostream.write(tstream.buffer)",
        "rewrite": "def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0): \n    tstream = BytearrayStream()\n    \n    self.revocation_code.write(tstream, kmip_version=kmip_version)\n    \n    if self.revocation_message is not None:\n        self.revocation_message.write(tstream, kmip_version=kmip_version)\n    \n    self.length = tstream.length()\n    super(RevocationReason, self).write(ostream, kmip_version=kmip_version)\n    ostream.write(tstream.buffer)"
    },
    {
        "original": "def delete(self): \n        delete_bytes(self.__fileobj, self.size, self.offset)\n        if self.parent_chunk is not None:\n            self.parent_chunk.resize(self.parent_chunk.data_size - self.size)",
        "rewrite": "def delete(self):\n    delete_bytes(self.__fileobj, self.size, self.offset)\n    if self.parent_chunk is not None:\n        self.parent_chunk.resize(self.parent_chunk.data_size - self.size)"
    },
    {
        "original": "def endswith(string): \n    def ends_with(value):\n        validate(text, value)\n        if not value.endswith(string):\n            raise ValueError(\"'{0}' does not end with '{1}'\".format(value, string))\n        return True\n\n    return ends_with",
        "rewrite": "def endswith(string):  \n    def ends_with(value):  \n        if not value.endswith(string):  \n            raise ValueError(\"'{0}' does not end with '{1}'\".format(value, string))  \n        return True  \n  \n    return ends_with"
    },
    {
        "original": "def from_conf(cls, path=None, **overrides): \n\t\tfrom onedrive import portalocker\n\t\timport yaml\n\n\t\tif path is None:\n\t\t\tpath = cls.conf_path_default\n\t\t\tlog.debug('Using default state-file path: %r', path)\n\t\tpath = os.path.expanduser(path)\n\t\twith open(path, 'rb') as src:\n\t\t\tportalocker.lock(src, portalocker.LOCK_SH)\n\t\t\tyaml_str = src.read()\n\t\t\tportalocker.unlock(src)\n\t\tconf = yaml.safe_load(yaml_str)\n\t\tconf.setdefault('conf_save', path)\n\n\t\tconf_cls = dict()\n\t\tfor ns, keys in cls.conf_update_keys.viewitems():\n\t\t\tfor k in keys:\n\t\t\t\ttry:\n\t\t\t\t\tv = conf.get(ns, dict()).get(k)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tif not cls.conf_raise_structure_errors: raise\n\t\t\t\t\traise KeyError((\n\t\t\t\t\t\t'Unable to get value for configuration parameter'\n\t\t\t\t\t\t\t' \"{k}\" in section \"{ns}\", check configuration file (path: {path}) syntax'\n\t\t\t\t\t\t\t' near the aforementioned section/value.' ).format(ns=ns, k=k, path=path))\n\t\t\t\tif v is not None: conf_cls['{}_{}'.format(ns, k)] = conf[ns][k]\n\t\tconf_cls.update(overrides)\n\n\t\t# Hack to work around YAML parsing client_id of e.g. 000123 as an octal int\n\t\tif isinstance(conf.get('client', dict()).get('id'), (int, long)):\n\t\t\tlog.warn( 'Detected client_id being parsed as an integer (as per yaml), trying to un-mangle it.'\n\t\t\t\t' If requests will still fail afterwards, please replace it in the configuration file (path: %r),'\n\t\t\t\t\t' also putting",
        "rewrite": "def from_conf(cls, path=None, **overrides): \n    from onedrive import portalocker\n    import yaml\n\n    if path is None:\n        path = cls.conf_path_default\n        log.debug('Using default state-file path: %r', path)\n    path = os.path.expanduser(path)\n    with open(path, 'rb') as src:\n        portalocker.lock(src, portalocker.LOCK_SH)\n        yaml_str = src.read()\n        portalocker.unlock(src)\n    conf = yaml.safe_load(yaml_str)\n    conf.setdefault('conf_save', path)\n\n    conf_cls = {}\n    for ns, keys in cls.conf_update_keys.items():\n        for k in keys:\n            try:\n                v = conf.get(ns, {}).get(k)\n            except AttributeError:\n                if not cls.conf_raise_structure_errors: \n                    raise KeyError(('Unable to get value for configuration parameter '\n                    '{k} in section {ns}, check configuration file (path: {path}) syntax '\n                    'near the aforementioned section/value.').format(ns=ns, k=k, path=path))\n            if v is not None: \n                conf_cls['{}_{}'.format(ns, k)] = conf[ns][k]\n    conf_cls.update(overrides)\n\n    # Hack to work around YAML parsing client_id of e.g. 000123 as an octal int\n    if isinstance(conf.get('client', {}).get('id'), (int, long)):\n        log.warn('Detected client_id being parsed as an integer (as per yaml), trying to un-mangle it. '\n            'If requests will still fail afterwards, please replace it in the configuration file (path: %r), '\n            'also putting\" . No need to explain. Just write code:\u200b' % path)"
    },
    {
        "original": "def validate_page_number(number): \n    try:\n        number = int(number)\n    except (TypeError, ValueError):\n        raise PageNotAnInteger('That page number is not an integer')\n    if number < 1:\n        raise EmptyPage('That page number is less than 1')\n    return number",
        "rewrite": "def validate_page_number(number):\n    try:\n        number = int(number)\n    except (TypeError, ValueError):\n        raise PageNotAnInteger('That page number is not an integer')\n    if number < 1:\n        raise EmptyPage('That page number is less than 1')\n    return number"
    },
    {
        "original": "def delete_data_disk(self, service_name, deployment_name, role_name, lun, delete_vhd=False): \n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        path = self._get_data_disk_path(service_name, deployment_name, role_name, lun)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)",
        "rewrite": "def delete_data_disk(self, service_name, deployment_name, role_name, lun, delete_vhd=False): \n\t_validate_not_none('service_name', service_name)\n\t_validate_not_none('deployment_name', deployment_name)\n\t_validate_not_none('role_name', role_name)\n\t_validate_not_none('lun', lun)\n\tpath = self._get_data_disk_path(service_name, deployment_name, role_name, lun)\n\tif delete_vhd:\n\t\tpath += '?comp=media'\n\treturn self._perform_delete(path, as_async=True)"
    },
    {
        "original": "def _request(self, req_type, url, **kwargs): \n        logger.debug('%s %s' % (req_type, url))\n        result = self.session.request(req_type, url, **kwargs)\n        try:\n            result.raise_for_status()\n        except requests.HTTPError:\n            error = result.text\n            try:\n                error = json.loads(error)\n            except ValueError:\n            ",
        "rewrite": "def _request(self, req_type, url, **kwargs):\n    logger.debug('%s %s' % (req_type, url))\n    result = self.session.request(req_type, url, **kwargs)\n    try:\n        result.raise_for_status()\n    except requests.HTTPError:\n        error = json.loads(result.text) if result.text else None"
    },
    {
        "original": "def drain(self, sid=None): \n        if self.is_draining:\n            return\n        if self.is_closed:\n            raise ErrConnectionClosed\n        if self.is_connecting or self.is_reconnecting:\n            raise ErrConnectionReconnecting\n\n        if sid is not None:\n            return self._drain_sub(sid)\n\n        # Start draining the subscriptions\n        self._status = Client.DRAINING_SUBS\n\n        drain_tasks = []\n",
        "rewrite": "def drain(self, sid=None):\n    if self.is_draining:\n        return\n    if self.is_closed:\n        raise ErrConnectionClosed\n    if self.is_connecting or self.is_reconnecting:\n        raise ErrConnectionReconnecting\n\n    if sid is not None:\n        return self._drain_sub(sid)\n\n    # Start draining the subscriptions\n    self._status = Client.DRAINING_SUBS\n\n    drain_tasks = []"
    },
    {
        "original": "def express_route_connections(self): \n        api_version = self._get_api_version('express_route_connections')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRouteConnectionsOperations as OperationClass\n        elif api_version == '2018-10-01':\n            from .v2018_10_01.operations import ExpressRouteConnectionsOperations as OperationClass\n        elif api_version == '2018-11-01':\n            from .v2018_11_01.operations import ExpressRouteConnectionsOperations as OperationClass\n        elif api_version == '2018-12-01':\n            from .v2018_12_01.operations import ExpressRouteConnectionsOperations as OperationClass\n   ",
        "rewrite": "def express_route_connections(self): \n    api_version = self._get_api_version('express_route_connections')\n    \n    if api_version == '2018-08-01':\n        from .v2018_08_01.operations import ExpressRouteConnectionsOperations as OperationClass\n    elif api_version == '2018-10-01':\n        from .v2018_10_01.operations import ExpressRouteConnectionsOperations as OperationClass\n    elif api_version == '2018-11-01':\n        from .v2018_11_01.operations import ExpressRouteConnectionsOperations as OperationClass\n    elif api_version == '2018-12-01':\n        from .v2018_12_01.operations import ExpressRouteConnectionsOperations as OperationClass"
    },
    {
        "original": "def cli_put_container(context, path): \n    path = path.rstrip('/')\n    if '/' in path:\n        raise ReturnCode('called cli_put_container with object %r' % path)\n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_container(\n            path, headers=context.headers, query=context.query,\n   ",
        "rewrite": "def cli_put_container(context, path): \n    path = path.rstrip('/')\n    if '/' in path:\n        raise ReturnCode('called cli_put_container with object %r' % path)\n    \n    body = None\n    if context.input_:\n        if context.input_ == '-':\n            body = context.io_manager.get_stdin()\n        else:\n            body = open(context.input_, 'rb')\n    \n    with context.client_manager.with_client() as client:\n        status, reason, headers, contents = client.put_container(\n            path, headers=context.headers, query=context.query)"
    },
    {
        "original": "def visit_import(self, node): \n        for module, as_name in node.names:\n            if module in self._logging_modules:\n                self._logging_names.add(as_name or module)",
        "rewrite": "def visit_import(self, node):\n    for module, as_name in node.names:\n        if module in self._logging_modules:\n            self._logging_names.add(as_name or module)"
    },
    {
        "original": "def cohorts(institute_id, case_name): \n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    cohort_tag = request.form['cohort_tag']\n    if request.args.get('remove') == 'yes':\n        store.remove_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n    else:\n        store.add_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n    return redirect(request.referrer)",
        "rewrite": "def cohorts(institute_id, case_name):\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    user_obj = store.user(current_user.email)\n    link = url_for('.case', institute_id=institute_id, case_name=case_name)\n    cohort_tag = request.form['cohort_tag']\n    \n    if request.args.get('remove') == 'yes':\n        store.remove_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n    else:\n        store.add_cohort(institute_obj, case_obj, user_obj, link, cohort_tag)\n        \n    return redirect(request.referrer)"
    },
    {
        "original": "def settle_deferred_messages(self, settlement, messages, **kwargs): \n        if (self.entity and self.requires_session) or kwargs.get('session'):\n            raise ValueError(\"Sessionful deferred messages can only be settled within a locked receive session.\")\n        if settlement.lower() not in ['completed', 'suspended', 'abandoned']:\n            raise ValueError(\"Settlement must be one of: 'completed', 'suspended', 'abandoned'\")\n        if not messages:\n            raise ValueError(\"At least one message must be specified.\")\n        message = {\n            'disposition-status':",
        "rewrite": "'message' : kwargs.get('disposition-status')\n}"
    },
    {
        "original": "def _is_completed(self, name_or_id=None): \n\n        if name_or_id is None:\n            return all(\n                (runinfo['completed'] for runinfo in self._run_information.values()))\n        else:\n            return self.f_get_run_information(name_or_id, copy=False)['completed']",
        "rewrite": "def _is_completed(self, name_or_id=None):\n    if name_or_id is None:\n        return all(runinfo['completed'] for runinfo in self._run_information.values())\n    else:\n        return self.f_get_run_information(name_or_id, copy=False)['completed']"
    },
    {
        "original": "def _list_to_dict(self, line, keys=None): \n        keys = self._keys if keys is None else keys\n        d = self._defaults(keys)\n        for key,value in zip(keys, line):\n            d[key] = value\n\n        return d",
        "rewrite": "def _list_to_dict(self, line, keys=None):\n        keys = self._keys if keys is None else keys\n        d = self._defaults(keys)\n        for key,value in zip(keys, line):\n            d[key] = value\n        return d"
    },
    {
        "original": "def download(path='.', url=None, unpack=False): \n\n    if url is None:\n        url = 'https://github.com/neurosynth/neurosynth-data/blob/master/current_data.tar.gz?raw=true'\n    if os.path.exists(path) and os.path.isdir(path):\n        basename = os.path.basename(url).split('?')[0]\n        filename = os.path.join(path, basename)\n    else:\n        filename = path\n\n    f = open(filename, 'wb')\n\n    u = urlopen(url)\n    file_size = int(u.headers[\"Content-Length\"][0])\n    print(\"Downloading the latest Neurosynth files: {0} bytes: {1}\".format(\n        url, file_size))\n\n    bytes_dl = 0\n    block_size = 8192\n    while True:\n        buffer",
        "rewrite": "def download(path='.', url=None, unpack=False):\n    if url is None:\n        url = 'https://github.com/neurosynth/neurosynth-data/blob/master/current_data.tar.gz?raw=true'\n\n    if os.path.exists(path) and os.path.isdir(path):\n        basename = os.path.basename(url).split('?')[0]\n        filename = os.path.join(path, basename)\n    else:\n        filename = path\n\n    with open(filename, 'wb') as f:\n        u = urlopen(url)\n        file_size = int(u.headers[\"Content-Length\"])\n        print(\"Downloading the latest Neurosynth files: {0}, bytes: {1}\".format(url, file_size))\n\n        bytes_dl = 0\n        block_size = 8192\n\n        while True:\n            buffer = u.read(block_size)\n            if not buffer:\n                break\n            f.write(buffer)\n            bytes_dl += len(buffer)\n\n        print(\"Downloaded successfully! Total bytes downloaded: {0}\".format(bytes_dl))"
    },
    {
        "original": "def lock(fileobj): \n\n    try:\n        import fcntl\n    except ImportError:\n        return False\n    else:\n        try:\n            fcntl.lockf(fileobj, fcntl.LOCK_EX)\n        except IOError:\n            # FIXME: There's possibly a lot of complicated\n            # logic that needs to go here in case the IOError\n            # is EACCES or EAGAIN.\n       ",
        "rewrite": "def lock(fileobj):\n    try:\n        import fcntl\n    except ImportError:\n        return False\n    else:\n        try:\n            fcntl.lockf(fileobj, fcntl.LOCK_EX)\n        except IOError as e:\n            # EACCES or EAGAIN logic goes here\n            pass"
    },
    {
        "original": "def upcoming(self, **kwargs): \n        path = self._get_path('upcoming')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response",
        "rewrite": "def upcoming(self, **kwargs): \n    path = self._get_path('upcoming')\n    \n    response = self._GET(path, kwargs)\n    self._set_attrs_to_values(response)\n    return response"
    },
    {
        "original": "def do_teardown_request(self, exc=None): \n        if exc is None:\n            exc = sys.exc_info()[1]\n        funcs = reversed(self.teardown_request_funcs.get(None, ()))\n        bp = _request_ctx_stack.top.request.blueprint\n        if bp is not None and bp in self.teardown_request_funcs:\n            funcs = chain(funcs, reversed(self.teardown_request_funcs[bp]))\n        for func in funcs:\n            rv = func(exc)\n        request_tearing_down.send(self, exc=exc)",
        "rewrite": "def do_teardown_request(self, exc=None):\n    if exc is None:\n        exc = sys.exc_info()[1]\n    funcs = reversed(self.teardown_request_funcs.get(None, ()))\n    bp = getattr(_request_ctx_stack.top.request, 'blueprint', None)\n    if bp is not None and bp in self.teardown_request_funcs:\n        funcs = chain(funcs, reversed(self.teardown_request_funcs[bp]))\n    for func in funcs:\n        rv = func(exc)\n    request_tearing_down.send(self, exc=exc)"
    },
    {
        "original": "def find(name): \n    cmd = 'apt-cache search %s' % name\n    args = shlex.split(cmd)\n    try:\n        output = subprocess.check_output(args)\n    except CalledProcessError:\n        return []\n    lines = output.splitlines()\n    packages = []\n    for line in lines:\n        package, _, desc = line.partition(' - ')\n        packages.append((package, desc)) \n    return packages",
        "rewrite": "def find(name): \n    cmd = f'apt-cache search {name}'\n    args = shlex.split(cmd)\n    try:\n        output = subprocess.check_output(args)\n    except subprocess.CalledProcessError:\n        return []\n    lines = output.splitlines()\n    packages = []\n    for line in lines:\n        package, _, desc = line.partition(' - ')\n        packages.append((package, desc)) \n    return packages"
    },
    {
        "original": "def copy_file_content(self, file_id, source_file): \n        if not is_valid_uuid(file_id):\n            raise StorageArgumentException(\n                'Invalid UUID for file_id: {0}'.format(file_id))\n\n        if not is_valid_uuid(source_file):\n            raise StorageArgumentException(\n                'Invalid UUID for source_file: {0}'.format(source_file))\n\n        self._authenticated_request \\\n            .to_endpoint('file/{}/content/'.format(file_id)) \\\n            .with_headers({'X-Copy-From': source_file}) \\\n ",
        "rewrite": "def copy_file_content(self, file_id, source_file): \n    if not is_valid_uuid(file_id):\n        raise StorageArgumentException(\n            'Invalid UUID for file_id: {0}'.format(file_id))\n\n    if not is_valid_uuid(source_file):\n        raise StorageArgumentException(\n            'Invalid UUID for source_file: {0}'.format(source_file))\n\n    self._authenticated_request.to_endpoint('file/{}/content/'.format(file_id)).with_headers({'X-Copy-From': source_file})"
    },
    {
        "original": "def _get_remote_settle_modes(pn_link): \n    modes = {}\n    snd = pn_link.remote_snd_settle_mode\n    if snd == proton.Link.SND_UNSETTLED:\n        modes['snd-settle-mode'] = 'unsettled'\n    elif snd == proton.Link.SND_SETTLED:\n        modes['snd-settle-mode'] = 'settled'\n    if pn_link.remote_rcv_settle_mode == proton.Link.RCV_SECOND:\n        modes['rcv-settle-mode'] = 'second'\n    return modes",
        "rewrite": "def _get_remote_settle_modes(pn_link): \n    modes = {}\n    snd = pn_link.remote_snd_settle_mode\n    if snd == proton.Link.SND_UNSETTLED:\n        modes['snd-settle-mode'] = 'unsettled'\n    elif snd == proton.Link.SND_SETTLED:\n        modes['snd-settle-mode'] = 'settled'\n    if pn_link.remote_rcv_settle_mode == proton.Link.RCV_SECOND:\n        modes['rcv-settle-mode'] = 'second'\n    return modes"
    },
    {
        "original": "def _process_filters(cls, filters): \n        data = []\n\n        # Filters should always be a list\n        for f in filters:\n            if isinstance(f, Filter):\n                if f.filters:\n                    data.extend(cls._process_filters(f.filters))\n            elif isinstance(f, dict):\n                key = list(f.keys())[0]\n      ",
        "rewrite": "def _process_filters(cls, filters): \n        data = []\n\n        # Filters should always be a list\n        for f in filters:\n            if isinstance(f, Filter):\n                if f.filters:\n                    data.extend(cls._process_filters(f.filters))\n            elif isinstance(f, dict):\n                key = list(f.keys())[0]"
    },
    {
        "original": "def einsum_matmul_index(gate_indices, number_of_qubits): \n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Right indices for the N-qubit input and output tensor\n    tens_r = ascii_uppercase[:number_of_qubits]\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n ",
        "rewrite": "def einsum_matmul_index(gate_indices, number_of_qubits): \n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices, number_of_qubits)\n\n    # Right indices for the N-qubit input and output tensor\n    tens_r = ascii_uppercase[:number_of_qubits]\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\"
    },
    {
        "original": "def stop(self, now=False): \n        self.log.info(\n            \"Stopping and removing Docker service %s (id: %s)\",\n            self.service_name, self.service_id[:7])\n        yield self.docker('remove_service', self.service_id[:7])\n        self.log.info(\n            \"Docker service %s (id: %s) removed\",\n            self.service_name, self.service_id[:7])\n\n        self.clear_state()",
        "rewrite": "def stop(self, now=False): \n    self.log.info(\n        \"Stopping and removing Docker service %s (id: %s)\",\n        self.service_name, self.service_id[:7])\n    yield self.docker('remove_service', self.service_id[:7])\n    self.log.info(\n        \"Docker service %s (id: %s) removed\",\n        self.service_name, self.service_id[:7])\n\n    self.clear_state()"
    },
    {
        "original": "def parse_str_to_expression(fiql_str): \n    #pylint: disable=too-many-branches\n    nesting_lvl = 0\n    last_element = None\n    expression = Expression()\n    for (preamble, selector, comparison, argument) in iter_parse(fiql_str):\n        if preamble:\n            for char in preamble:\n                if char == '(':\n                    if isinstance(last_element, BaseExpression):\n                        raise FiqlFormatException(\n  ",
        "rewrite": "def parse_str_to_expression(fiql_str):\n    # pylint: disable=too-many-branches\n    nesting_lvl = 0\n    last_element = None\n    expression = Expression()\n    for (preamble, selector, comparison, argument) in iter_parse(fiql_str):\n        if preamble:\n            for char in preamble:\n                if char == '(':\n                    if isinstance(last_element, BaseExpression):\n                        raise FiqlFormatException(\". No need to explain. Just write code.\")"
    },
    {
        "original": "def get(self): \n        if PyFunceble.HTTP_CODE[\"active\"]:\n            # The http status code extraction is activated.\n\n            # We get the http status code.\n            http_code = self._access()\n\n            # We initiate a variable which will save the list of allowed\n            # http status code.\n            list_of_valid_http_code = []\n\n            for codes",
        "rewrite": "def get(self): \n    if PyFunceble.HTTP_CODE[\"active\"]:\n        # The http status code extraction is activated.\n\n        # We get the http status code.\n        http_code = self._access()\n\n        # We initiate a variable which will save the list of allowed\n        # http status code.\n        list_of_valid_http_code = []\n\n        for codes in range(100, 600):"
    },
    {
        "original": "def get_arguments(self): \n        ApiCli.get_arguments(self)\n\n        # Get the host group name\n        if self.args.host_group_name is not None:\n            self.host_group_name = self.args.host_group_name\n\n        # Get the list of sources separated by commas\n        if self.args.sources is not None:\n            self.sources = self.args.sources\n\n        payload = {}\n        if self.host_group_name is not None:\n            payload['name'] = self.host_group_name\n\n",
        "rewrite": "def get_arguments(self):\n    ApiCli.get_arguments(self)\n\n    if self.args.host_group_name:\n        self.host_group_name = self.args.host_group_name\n\n    if self.args.sources:\n        self.sources = self.args.sources\n\n    payload = {}\n    if self.host_group_name:\n        payload['name'] = self.host_group_name"
    },
    {
        "original": "def f_get(self, *args): \n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n ",
        "rewrite": "def f_get(self, *args):\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: `%s` Please use >>f_get<< with one of these.' % \"."
    },
    {
        "original": "def text(length, choices=string.ascii_letters): \n    return ''.join(choice(choices) for x in range(length))",
        "rewrite": "def text(length, choices=string.ascii_letters):\n    import random\n    return ''.join(random.choice(choices) for x in range(length))"
    },
    {
        "original": "def _convert_from_thrift_binary_annotations(self, thrift_binary_annotations): \n        tags = {}\n        local_endpoint = None\n        remote_endpoint = None\n\n        for binary_annotation in thrift_binary_annotations:\n            if binary_annotation.key == 'sa':\n                remote_endpoint = self._convert_from_thrift_endpoint(\n                    thrift_endpoint=binary_annotation.host,\n                )\n            else:\n  ",
        "rewrite": "def _convert_from_thrift_binary_annotations(self, thrift_binary_annotations):\n    tags = {}\n    local_endpoint = None\n    remote_endpoint = None\n\n    for binary_annotation in thrift_binary_annotations:\n        if binary_annotation.key == 'sa':\n            remote_endpoint = self._convert_from_thrift_endpoint(\n                thrift_endpoint=binary_annotation.host,\n            )\n        else:\n            pass"
    },
    {
        "original": "def _get_args_str(func, highlight=None): \n    if not func: return \"\"\n    s = str(inspect.signature(func))[1:-1]\n    if highlight:\n        s = re.sub(r\"\\b%s\\b\" % highlight, Style.BRIGHT + Fore.WHITE + highlight + Fore.LIGHTBLACK_EX + Style.NORMAL, s)\n    return s",
        "rewrite": "import inspect\nfrom colorama import Style, Fore\nimport re\n\ndef _get_args_str(func, highlight=None): \n    if not func:\n        return \"\"\n    s = str(inspect.signature(func))[1:-1]\n    if highlight:\n        s = re.sub(r\"\\b%s\\b\" % highlight, Style.BRIGHT + Fore.WHITE + highlight + Fore.LIGHTBLACK_EX + Style.NORMAL, s)\n    return s"
    },
    {
        "original": "def combine_into_edge_map(self, another_layout): \n        edge_map = dict()\n\n        for virtual, physical in self.get_virtual_bits().items():\n            if physical not in another_layout._p2v:\n                raise LayoutError('The wire_map_from_layouts() method does not support when the'\n                                  ' other layout (another_layout) is smaller.')\n            edge_map[virtual] = another_layout[physical]\n\n        return edge_map",
        "rewrite": "def combine_into_edge_map(self, another_layout):\n    edge_map = dict()\n\n    for virtual, physical in self.get_virtual_bits().items():\n        if physical not in another_layout._p2v:\n            raise LayoutError('The wire_map_from_layouts() method does not support when the'\n                              ' other layout (another_layout) is smaller.')\n        edge_map[virtual] = another_layout[physical]\n\n    return edge_map"
    },
    {
        "original": "def vrange(start, stop, step=1, dtype='f8'): \n    from .column import ColumnVirtualRange\n    return ColumnVirtualRange(start, stop, step, dtype)",
        "rewrite": "def vrange(start, stop, step=1, dtype='f8'): \n    from .column import ColumnVirtualRange\n    return ColumnVirtualRange(start, stop, step, dtype)"
    },
    {
        "original": "def pkginfo_to_metadata(egg_info_path, pkginfo_path): \n    pkg_info = read_pkg_info(pkginfo_path)\n    pkg_info.replace_header('Metadata-Version', '2.0')\n    requires_path = os.path.join(egg_info_path, 'requires.txt')\n    if os.path.exists(requires_path):\n        requires = open(requires_path).read()\n        for extra, reqs in pkg_resources.split_sections(requires):\n            condition = ''\n            if extra and ':' in extra: # setuptools extra:condition syntax\n                extra, condition = extra.split(':', 1)\n            if extra:\n           ",
        "rewrite": "for req in reqs:\n                pkg_info.add_req('Requires-'+extra, req)\n            else:\n                pkg_info.add_req('Requires', req)\n                pkg_info.add_req('Requires', req)\n    with open(pkginfo_path, 'w') as f:\n        pkg_info.write_metadata_file(f)"
    },
    {
        "original": "def raw_data(self, filename): \n        if self.debug and self.debug.should('dataio'):\n            self.debug.write(\"Reading data from %r\" % (filename,))\n        fdata = open(filename, 'rb')\n        try:\n            data = pickle.load(fdata)\n        finally:\n            fdata.close()\n        return data",
        "rewrite": "def raw_data(self, filename):\n        if self.debug and self.debug.should('dataio'):\n            self.debug.write(\"Reading data from %r\" % (filename,))\n        with open(filename, 'rb') as fdata:\n            data = pickle.load(fdata)\n        return data"
    },
    {
        "original": "def get_region(): \n    global _REGION\n    if _REGION is None:\n        region_name = os.getenv(\"AWS_DEFAULT_REGION\") or \"us-east-1\"\n        region_dict = {r.name: r for r in boto.regioninfo.get_regions(\"ec2\")}\n        if region_name not in region_dict:\n            raise ValueError(\"No such EC2 region: {}. Check AWS_DEFAULT_REGION \"\n                             \"environment variable\".format(region_name))\n        _REGION = region_dict[region_name]\n    return _REGION",
        "rewrite": "def get_region(): \n    global _REGION\n    if _REGION is None:\n        region_name = os.getenv(\"AWS_DEFAULT_REGION\") or \"us-east-1\"\n        region_dict = {r.name: r for r in boto.regioninfo.get_regions(\"ec2\")}\n        if region_name not in region_dict:\n            raise ValueError(\"No such EC2 region: {}. Check AWS_DEFAULT_REGION \"\n                             \"environment variable\".format(region_name))\n        _REGION = region_dict[region_name]\n    return _REGION"
    },
    {
        "original": " \n    # Yield `import basilisp` so code attempting to call fully qualified\n    # `basilisp.lang...` modules don't result in compiler errors\n    yield ast.Import(names=[ast.alias(name=\"basilisp\", asname=None)])\n    for imp in ctx.imports:\n        name = imp.key.name\n        alias = _MODULE_ALIASES.get(name, None)\n        yield ast.Import(names=[ast.alias(name=name, asname=alias)])",
        "rewrite": "yield ast.Import(names=[ast.alias(name=\"basilisp\", asname=None)])\nfor imp in ctx.imports:\n    name = imp.key.name\n    alias = _MODULE_ALIASES.get(name, None)\n    yield ast.Import(names=[ast.alias(name=name, asname=alias)])"
    },
    {
        "original": "def add_params(traj): \n\n    # We set the BrianParameter to be the standard parameter\n    traj.v_standard_parameter=Brian2Parameter\n    traj.v_fast_access=True\n\n    # Add parameters we need for our network\n    traj.f_add_parameter('Net.C',281*pF)\n    traj.f_add_parameter('Net.gL',30*nS)\n    traj.f_add_parameter('Net.EL',-70.6*mV)\n    traj.f_add_parameter('Net.VT',-50.4*mV)\n    traj.f_add_parameter('Net.DeltaT',2*mV)\n    traj.f_add_parameter('Net.tauw',40*ms)\n    traj.f_add_parameter('Net.a',4*nS)\n    traj.f_add_parameter('Net.b',0.08*nA)\n    traj.f_add_parameter('Net.I',.8*nA)\n    traj.f_add_parameter('Net.Vcut','vm > 0*mV') # practical threshold condition\n    traj.f_add_parameter('Net.N',50)\n\n    eqs=",
        "rewrite": "def add_params(traj):\n\n    # Set BrianParameter to the standard parameter\n    traj.v_standard_parameter = Brian2Parameter\n    traj.v_fast_access = True\n\n    # Add parameters for the network\n    traj.f_add_parameter('Net.C', 281*pF)\n    traj.f_add_parameter('Net.gL', 30*nS)\n    traj.f_add_parameter('Net.EL', -70.6*mV)\n    traj.f_add_parameter('Net.VT', -50.4*mV)\n    traj.f_add_parameter('Net.DeltaT', 2*mV)\n    traj.f_add_parameter('Net.tauw', 40*ms)\n    traj.f_add_parameter('Net.a', 4*nS)\n    traj.f_add_parameter('Net.b', 0.08*nA)\n    traj.f_add_parameter('Net.I', 0.8*nA)\n    traj.f_add_parameter('Net.Vcut', 'vm > 0*mV') # practical threshold condition\n    traj.f_add_parameter('Net.N', 50)\n\n    eqs = \" . No need to explain. Just write code\""
    },
    {
        "original": "def artist(self, spotify_id): \n        route = Route('GET', '/artists/{spotify_id}', spotify_id=spotify_id)\n        return self.request(route)",
        "rewrite": "def artist(self, spotify_id): \n    route = Route('GET', f'/artists/{spotify_id}', spotify_id=spotify_id)\n    return self.request(route)"
    },
    {
        "original": "def trits_from_int(n, pad=1): \n    if n == 0:\n        trits = []\n    else:\n        quotient, remainder = divmod(n, 3)\n\n        if remainder == 2:\n            # Lend 1 to the next place so we can make this trit\n            # negative.\n            quotient += 1\n            remainder = -1\n\n        trits = [remainder] + trits_from_int(quotient, pad=0)\n\n  ",
        "rewrite": "def trits_from_int(n, pad=1): \n    trits = []\n    \n    if n != 0:\n        quotient, remainder = divmod(n, 3)\n\n        if remainder == 2:\n            quotient += 1\n            remainder = -1\n\n        trits = [remainder] + trits_from_int(quotient, pad=0)\n    \n    return trits"
    },
    {
        "original": "def _odds_val(self): \n        if len(self.odds) == 0:\n            self.odds = [(1.00, [self.min, self.max])]\n\n        rand_val = rand.random()\n        total = 0\n        for percent,v in self.odds:\n            if total <= rand_val < total+percent:\n                found_v = v\n                break\n            total += percent\n\n  ",
        "rewrite": "def _odds_val(self): \n    if len(self.odds) == 0:\n        self.odds = [(1.00, [self.min, self.max])]\n\n    rand_val = rand.random()\n    total = 0\n    for percent, v in self.odds:\n        if total <= rand_val < total + percent:\n            found_v = v\n            break\n        total += percent"
    },
    {
        "original": "def from_text(cls, text, lexicon, required=None, first_only=True): \n        component = lexicon.get_component(text, first_only=first_only)\n        if required and (required not in component):\n            return None\n        else:\n            return cls(component)",
        "rewrite": "def from_text(cls, text, lexicon, required=None, first_only=True):\n    component = lexicon.get_component(text, first_only=first_only)\n    if required is not None and required not in component:\n        return None\n    else:\n        return cls(component)"
    },
    {
        "original": "def transcode_to_utf8(filename, encoding): \n    tmp = tempfile.TemporaryFile()\n    for line in io.open(filename, encoding=encoding):\n        tmp.write(line.strip('\\uFEFF').encode('utf-8'))\n\n    tmp.seek(0)\n    return tmp",
        "rewrite": "import io\nimport tempfile\n\ndef transcode_to_utf8(filename, encoding): \n    tmp = tempfile.TemporaryFile()\n    for line in io.open(filename, encoding=encoding):\n        tmp.write(line.strip('\\uFEFF').encode('utf-8'))\n\n    tmp.seek(0)\n    return tmp"
    },
    {
        "original": "def disconnect(self, mol): \n        log.debug('Running MetalDisconnector')\n        # Remove bonds that match SMARTS\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?\n    ",
        "rewrite": "def disconnect(self, mol): \n        log.debug('Running MetalDisconnector')\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?"
    },
    {
        "original": "def parse_file(cls, ctxt, fname, key=None, step_addr=None): \n\n        # Load the YAML file\n        try:\n            with open(fname) as f:\n                step_data = yaml.load(f)\n        except Exception as exc:\n            raise ConfigError(\n                'Failed to read file \"%s\": %s' % (fname, exc),\n                step_addr,\n    ",
        "rewrite": "def parse_file(cls, ctxt, fname, key=None, step_addr=None): \n\n        try:\n            with open(fname) as f:\n                step_data = yaml.load(f)\n        except Exception as exc:\n            raise ConfigError('Failed to read file \"%s\": %s' % (fname, exc), step_addr)"
    },
    {
        "original": "def write(self, message): \n        data = encode(message, compressed=self.compressed)\n        length = len(data)\n        data = self.__pack(length) + data\n        with self.__write_lock:\n            while data:\n                try:\n                    n = os.write(self.out_d, data)\n                except OSError as why:\n           ",
        "rewrite": "def write(self, message): \n    data = encode(message, compressed=self.compressed)\n    length = len(data)\n    data = self.__pack(length) + data\n    with self.__write_lock:\n        while data:\n            try:\n                n = os.write(self.out_d, data)\n            except OSError as why:"
    },
    {
        "original": "def _get_template_abs_path(filename): \n        if os.path.isabs(filename) and os.path.isfile(filename):\n            return filename\n        else:\n            return os.path.join(os.getcwd(), filename)",
        "rewrite": "def _get_template_abs_path(filename):\n    if os.path.isabs(filename) and os.path.isfile(filename):\n        return filename\n    else:\n        return os.path.abspath(filename)"
    },
    {
        "original": "def apply_patch(self, patch_name, force=False, quiet=False): \n        self._check()\n        patch = Patch(patch_name)\n        patches = self.series.patches_until(patch)[:]\n\n        applied = self.db.applied_patches()\n        for patch in applied:\n            if patch in patches:\n                patches.remove(patch)\n\n        if not patches:\n            raise AllPatchesApplied(self.series, self.db.top_patch())\n\n        self.applying(patch)\n\n        try:\n  ",
        "rewrite": "def apply_patch(self, patch_name, force=False, quiet=False): \n    self._check()\n    patch = Patch(patch_name)\n    patches = self.series.patches_until(patch)[:]\n\n    applied = self.db.applied_patches()\n    for applied_patch in applied:\n        if applied_patch in patches:\n            patches.remove(applied_patch)\n\n    if not patches:\n        raise AllPatchesApplied(self.series, self.db.top_patch())\n\n    self.applying(patch)\n\n    try:\n        # Add code here for patch application\n        pass\n    except Exception as e:\n        # Handle exception\n        pass"
    },
    {
        "original": "def _dir_exists(db, user_id, db_dirname): \n    return db.execute(\n        select(\n            [func.count(directories.c.name)],\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname,\n            ),\n        )\n    ).scalar() != 0",
        "rewrite": "def _dir_exists(db, user_id, db_dirname): \n    return db.execute(\n        select(\n            [func.count(directories.c.name)]\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname\n            )\n        )\n    ).scalar() != 0"
    },
    {
        "original": "def jaccard(self, other): \n        if other.seed != self.seed:\n            raise ValueError(\"Cannot compute Jaccard given WeightedMinHash objects with\\\n                    different seeds\")\n        if len(self) != len(other):\n            raise ValueError(\"Cannot compute Jaccard given WeightedMinHash objects with\\\n                    different numbers of hash values\")\n        # Check how many pairs of (k, t) hashvalues are equal\n ",
        "rewrite": "def jaccard(self, other): \n        if other.seed != self.seed:\n            raise ValueError(\"Cannot compute Jaccard given WeightedMinHash objects with different seeds\")\n        if len(self) != len(other):\n            raise ValueError(\"Cannot compute Jaccard given WeightedMinHash objects with different numbers of hash values\")\n        # Check how many pairs of (k, t) hashvalues are equal"
    },
    {
        "original": "def nodelist_to_text(self, nodelist): \n\n        s = self._nodelistcontents_to_text(nodelist)\n\n        # now, perform suitable replacements\n        for pattern, replacement in self.text_replacements:\n            if (hasattr(pattern, 'sub')):\n                s = pattern.sub(replacement, s)\n            else:\n                s = s.replace(pattern, replacement)\n\n        if not self.keep_inline_math:\n            s = s.replace('$', ''); #",
        "rewrite": "def nodelist_to_text(self, nodelist): \n\n        s = self._nodelistcontents_to_text(nodelist)\n\n        # now, perform suitable replacements\n        for pattern, replacement in self.text_replacements:\n            if hasattr(pattern, 'sub'):\n                s = pattern.sub(replacement, s)\n            else:\n                s = s.replace(pattern, replacement)\n\n        if not self.keep_inline_math:\n            s = s.replace('$', '')"
    },
    {
        "original": "def light_bahdanau_attention(key, context, hidden_size, projected_align=False): \n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = \\\n        tf.tile(tf.reshape(projected_key, shape=[-1, 1, hidden_size]),\n                [1, max_num_tokens, 1])\n\n    # projected_context: [None, max_num_tokens, hidden_size]\n    projected_context = \\\n        tf.layers.dense(r_context, hidden_size, kernel_initializer=xav())\n    concat_h_state = tf.concat([projected_context, r_projected_key], -1)\n\n    projected_state = \\\n        tf.layers.dense(concat_h_state, hidden_size, use_bias=False,\n ",
        "rewrite": "def light_bahdanau_attention(key, context, hidden_size, projected_align=False):\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.shape.as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = tf.tile(tf.reshape(projected_key, shape=[-1, 1, hidden_size]), [1, max_num_tokens, 1])\n\n    projected_context = tf.layers.dense(r_context, hidden_size, kernel_initializer=xav())\n    concat_h_state = tf.concat([projected_context, r_projected_key], -1)\n\n    projected_state = tf.layers.dense(concat_h_state, hidden_size, use_bias=False)"
    },
    {
        "original": "def threeQ_or_more_gates(self): \n        three_q_gates = []\n        for node in self.gate_nodes():\n            if len(node.qargs) >= 3:\n                three_q_gates.append(node)\n        return three_q_gates",
        "rewrite": "def threeQ_or_more_gates(self):\n        three_q_gates = []\n        for node in self.gate_nodes():\n            if len(node.qargs) >= 3:\n                three_q_gates.append(node)\n        return three_q_gates"
    },
    {
        "original": "def delete_file(self, filename): \n        key = os.path.join(self._bucket_root, filename)\n        objects = list(self._bucket.objects.filter(Prefix=key))\n        for obj in objects:\n            obj.delete()",
        "rewrite": "def delete_file(self, filename):\n    key = os.path.join(self._bucket_root, filename)\n    objects = list(self._bucket.objects.filter(Prefix=key))\n    for obj in objects:\n        obj.delete()"
    },
    {
        "original": "def serach_path(): \n    operating_system = get_os()\n    # 1st choice: in ~/.kerncraft/iaca-{}\n    # 2nd choice: in package directory / iaca-{}\n    return [os.path.expanduser(\"~/.kerncraft/iaca/{}/\".format(operating_system)),\n            os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/iaca/{}/'.format(\n                operating_system)]",
        "rewrite": "def search_path(): \n    operating_system = get_os()\n    return [os.path.expanduser(f\"~/.kerncraft/iaca/{operating_system}/\"),\n            os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + f'/iaca/{operating_system}/']"
    },
    {
        "original": "def _proc_ellipse(self, tokens, filled): \n\n        component = Ellipse(pen=self.pen,\n                            x_origin=tokens[\"x0\"],\n                            y_origin=tokens[\"y0\"],\n                            e_width=tokens[\"w\"],\n                            e_height=tokens[\"h\"],\n ",
        "rewrite": "def _proc_ellipse(self, tokens, filled): \n\n        component = Ellipse(pen=self.pen,\n                            x_origin=tokens[\"x0\"],\n                            y_origin=tokens[\"y0\"],\n                            e_width=tokens[\"w\"],\n                            e_height=tokens[\"h\"], filled=filled)"
    },
    {
        "original": "def _setup_template_file(self, template_file_path): \n        try:\n            template_file = template_file_path\n            template_env = get_environment_for(template_file_path)\n            template = template_env.get_template(os.path.basename(template_file))\n        except:\n            raise\n        else:\n            self._template_file = template_file\n            self._template_env = template_env\n            self.template = template",
        "rewrite": "def _setup_template_file(self, template_file_path): \n    try:\n        template_file = template_file_path\n        template_env = get_environment_for(template_file_path)\n        template = template_env.get_template(os.path.basename(template_file))\n    except:\n        raise\n    else:\n        self._template_file = template_file\n        self._template_env = template_env\n        self.template = template"
    },
    {
        "original": "def _general_error_handler(http_error): \n    message = str(http_error)\n    if http_error.respbody is not None:\n        message += '\\n' + http_error.respbody.decode('utf-8-sig')\n    raise AzureHttpError(message, http_error.status)",
        "rewrite": "def _general_error_handler(http_error): \n    message = str(http_error)\n    if http_error.respbody is not None:\n        message += '\\n' + http_error.respbody.decode('utf-8-sig')\n    raise AzureHttpError(message, http_error.status)"
    },
    {
        "original": "def get_clinvar_id(self, submission_id): \n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id",
        "rewrite": "def get_clinvar_id(self, submission_id): \n    submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n    clinvar_subm_id = submission_obj.get('clinvar_subm_id', None) \n    return clinvar_subm_id"
    },
    {
        "original": "def p_bit_list_1(self, program): \n        program[0] = program[1]\n        program[0].add_child(program[3])\n        program[3].is_bit = True\n        self.update_symtab(program[3])",
        "rewrite": "def p_bit_list_1(self, program):\n    program[0] = program[1]\n    program[0].add_child(program[3])\n    program[3].is_bit = True\n    self.update_symtab(program[3])"
    },
    {
        "original": "def _sendData(self, data): \n        d = self._callRemote(Transmit, connection=self.connection, data=data)\n        d.addErrback(log.err)",
        "rewrite": "def _send_data(self, data): \n        d = self._callRemote(Transmit, connection=self.connection, data=data)\n        d.addErrback(log.err)"
    },
    {
        "original": "def node_filter(self, name, **kwargs): \n\n        def decorator(func):\n            self.filters[name] = NodeFilter(name, func, **kwargs)\n\n        return decorator",
        "rewrite": "def node_filter(self, name, **kwargs): \n\n        def decorator(func):\n            self.filters[name] = NodeFilter(name, func, **kwargs)\n            return func\n\n        return decorator"
    },
    {
        "original": "def merge_overlaps(self): \n        overlaps = np.array(self.find_overlaps(index=True))\n\n        if not overlaps.any():\n            return\n\n        for overlap in overlaps:\n            before = self[overlap].copy()\n            after = self[overlap + 1].copy()\n\n            # Get rid of the before and after pieces.\n            del self[overlap]\n            del self[overlap]\n\n      ",
        "rewrite": "def merge_overlaps(self): \n    overlaps = np.array(self.find_overlaps(index=True))\n\n    if not overlaps.any():\n        return\n\n    for overlap in overlaps:\n        before = self[overlap].copy()\n        after = self[overlap + 1].copy()\n\n        # Get rid of the before and after pieces.\n        del self[overlap]\n        del self[overlap]"
    }
]