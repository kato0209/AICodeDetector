[
    {
        "original": "def _process_merge_request_change(self, payload, event, codebase=None):\n        \"\"\"\n        Consumes the merge_request JSON as a python object and turn it into a buildbot change.\n\n        :arguments:\n            payload\n                Python Object that represents the JSON sent by GitLab Service\n                Hook.\n        \"\"\"\n        attrs = payload['object_attributes']\n        commit = attrs['last_commit']\n        when_timestamp = dateparse(commit['timestamp'])\n        # @todo provide and document a way to choose between http and ssh url\n        repo_url = attrs['target']['git_http_url']\n        # project name from http headers is empty for me, so get it from object_attributes/target/name\n        project = attrs['target']['name']\n\n        # Filter out uninteresting events\n        state = attrs['state']\n        if re.match('^(closed|merged|approved)$', state):\n            log.msg(\"GitLab MR#{}: Ignoring because state is {}\".format(attrs['iid'], state))\n            return []\n        action = attrs['action']\n        if not re.match('^(open|reopen)$', action) and not (action == \"update\" and \"oldrev\" in attrs):\n            log.msg(\"GitLab MR#{}: Ignoring because action {} was not open or \"\n                    \"reopen or an update that added code\".format(attrs['iid'],\n                                                                 action))\n            return []\n\n        changes = [{\n            'author': '%s <%s>' % (commit['author']['name'],\n                                   commit['author']['email']),\n            'files': [],  # @todo use rest API\n            'comments': \"MR#{}: {}\\n\\n{}\".format(attrs['iid'], attrs['title'], attrs['description']),\n            'revision': commit['id'],\n            'when_timestamp': when_timestamp,\n            'branch': attrs['target_branch'],\n            'repository': repo_url,\n            'project': project,\n            'category': event,\n            'revlink': attrs['url'],\n            'properties': {\n                'source_branch': attrs['source_branch'],\n                'source_project_id': attrs['source_project_id'],\n                'source_repository': attrs['source']['git_http_url'],\n                'source_git_ssh_url': attrs['source']['git_ssh_url'],\n                'target_branch': attrs['target_branch'],\n                'target_project_id': attrs['target_project_id'],\n                'target_repository': attrs['target']['git_http_url'],\n                'target_git_ssh_url': attrs['target']['git_ssh_url'],\n                'event': event,\n            },\n        }]\n        if codebase is not None:\n            changes[0]['codebase'] = codebase\n        return changes",
        "rewrite": "target']['git_ssh_url'],\n               'merge_request_id': attrs['id'],\n               'merge_request_iid': attrs['iid'],\n               'merge_request_state': attrs['state'],\n               'merge_request_title': attrs['title'],\n               'merge_request_description': attrs['description'],\n               'merge_request_url': attrs['url'],\n               'merge_request_target': attrs['target']['name'],\n               'merge_request_source': attrs['source']['name'],\n               'merge_request_target_git_ssh_url': attrs['target']['git_ssh_url'],\n               'merge_request_target_git_http_url': attrs['target']['git_http_url'],\n               'merge"
    },
    {
        "original": "def create_project(self, key, name, description=\"\"):\n        \"\"\"\n        Create project\n        :param key:\n        :param name:\n        :param description:\n        :return:\n        \"\"\"\n        url = 'rest/api/1.0/projects'\n        data = {\"key\": key,\n                \"name\": name,\n                \"description\": description\n                }\n        return self.post(url, data=data)",
        "rewrite": "\"def create_project(self, key, name, description=\"\"):\n        \"\"\"\n        Create project\n        :param key:\n        :param name:\n        :param description:\n        :return:\n        \"\"\"\n        url ='rest/api/1.0/projects'\n        data = {\"key\": key,\n                \"name\": name,\n                \"description\": description\n                }\n        return self.post(url, data=data)\"\"def create_project(self, key, name, description=\"\"):"
    },
    {
        "original": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)",
        "rewrite": " \"def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)\"\"def scale(self,"
    },
    {
        "original": "def get_interfaces_ip(self):\n        \"\"\"\n        Get interface ip details.\n\n        Returns a dict of dicts\n\n        Example Output:\n\n        {   u'FastEthernet8': {   'ipv4': {   u'10.66.43.169': {   'prefix_length': 22}}},\n            u'Loopback555': {   'ipv4': {   u'192.168.1.1': {   'prefix_length': 24}},\n                                'ipv6': {   u'1::1': {   'prefix_length': 64},\n                                            u'2001:DB8:1::1': {   'prefix_length': 64},\n                                            u'2::': {   'prefix_length': 64},\n                                            u'FE80::3': {   'prefix_length': 10}}},\n            u'Tunnel0': {   'ipv4': {   u'10.63.100.9': {   'prefix_length': 24}}},\n            u'Tunnel1': {   'ipv4': {   u'10.63.101.9': {   'prefix_length': 24}}},\n            u'Vlan100': {   'ipv4': {   u'10.40.0.1': {   'prefix_length': 24},\n                                        u'10.41.0.1': {   'prefix_length': 24},\n                                        u'10.65.0.1': {   'prefix_length': 24}}},\n            u'Vlan200': {   'ipv4': {   u'10.63.176.57': {   'prefix_length': 29}}}}\n        \"\"\"\n        interfaces = {}\n\n        command = \"show ip interface\"\n        show_ip_interface = self._send_command(command)\n        command = \"show ipv6 interface\"\n        show_ipv6_interface = self._send_command(command)\n\n        INTERNET_ADDRESS = r\"\\s+(?:Internet address is|Secondary address)\"\n        INTERNET_ADDRESS += r\" (?P<ip>{})/(?P<prefix>\\d+)\".format(IPV4_ADDR_REGEX)\n        LINK_LOCAL_ADDRESS = (\n            r\"\\s+IPv6 is enabled, link-local address is (?P<ip>[a-fA-F0-9:]+)\"\n        )\n        GLOBAL_ADDRESS = (\n            r\"\\s+(?P<ip>[a-fA-F0-9:]+), subnet is (?:[a-fA-F0-9:]+)/(?P<prefix>\\d+)\"\n        )\n\n        interfaces = {}\n        for line in show_ip_interface.splitlines():\n            if len(line.strip()) == 0:\n                continue\n            if line[0] != \" \":\n                ipv4 = {}\n                interface_name = line.split()[0]\n            m = re.match(INTERNET_ADDRESS, line)\n            if m:\n                ip, prefix = m.groups()\n                ipv4.update({ip: {\"prefix_length\": int(prefix)}})\n                interfaces[interface_name] = {\"ipv4\": ipv4}\n\n        if \"% Invalid input detected at\" not in show_ipv6_interface:\n            for line in show_ipv6_interface.splitlines():\n                if len(line.strip()) == 0:\n                    continue\n                if line[0] != \" \":\n                    ifname = line.split()[0]\n                    ipv6 = {}\n                    if ifname not in interfaces:\n                        interfaces[ifname] = {\"ipv6\": ipv6}\n                    else:\n                        interfaces[ifname].update({\"ipv6\": ipv6})\n                m = re.match(LINK_LOCAL_ADDRESS, line)\n                if m:\n                    ip = m.group(1)\n                    ipv6.update({ip: {\"prefix_length\": 10}})\n                m = re.match(GLOBAL_ADDRESS, line)\n                if m:\n                    ip, prefix = m.groups()\n                    ipv6.update({ip: {\"prefix_length\": int(prefix)}})\n\n        # Interface without ipv6 doesn't appears in show ipv6 interface\n        return interfaces",
        "rewrite": " (?P<prefix>\\d+)\"\n        )\n        INTERFACE = r\"\\s+(?P<interface>\\S+)\\s+is\\s+(?:administratively\\s+)?(?:up|down)\"\n        INTERFACE += r\"(?:,\\s+line\\s+protocol\\s+is\\s+(?:up|down))?\"\n        INTERFACE += r\"(?:,\\s+disabled\\s+due\\s+to\\s+link\\s+flap\\s+detected)?\"\n        INTERFACE += r\"(?:,\\s+disabled\\s+due\\s+to\\s+VR\\s+not\\s+owned)?\"\n        INTERFACE += r\"(?:,\\s"
    },
    {
        "original": "def play(events, speed_factor=1.0, include_clicks=True, include_moves=True, include_wheel=True):\n    \"\"\"\n    Plays a sequence of recorded events, maintaining the relative time\n    intervals. If speed_factor is <= 0 then the actions are replayed as fast\n    as the OS allows. Pairs well with `record()`.\n\n    The parameters `include_*` define if events of that type should be inluded\n    in the replay or ignored.\n    \"\"\"\n    last_time = None\n    for event in events:\n        if speed_factor > 0 and last_time is not None:\n            _time.sleep((event.time - last_time) / speed_factor)\n        last_time = event.time\n\n        if isinstance(event, ButtonEvent) and include_clicks:\n            if event.event_type == UP:\n                _os_mouse.release(event.button)\n            else:\n                _os_mouse.press(event.button)\n        elif isinstance(event, MoveEvent) and include_moves:\n            _os_mouse.move_to(event.x, event.y)\n        elif isinstance(event, WheelEvent) and include_wheel:\n            _os_mouse.wheel(event.delta)",
        "rewrite": " \"def play(events, speed_factor=1.0, include_clicks=True, include_moves=True, include_wheel=True):\n    last_time = None\n    for event in events:\n        if speed_factor > 0 and last_time is not None:\n            _time.sleep((event.time - last_time) / speed_factor)\n        last_time = event.time\n\n        if isinstance(event, ButtonEvent) and include_clicks:\n            if event.event_type == UP:\n                _os_mouse.release(event.button)\n            else:\n                _os_mouse.press(event.button)\n        elif isinstance(event, MoveEvent) and include_moves:\n            _os"
    },
    {
        "original": "def parse_error(res):\n    \"\"\"\n    Every server error should contain a \"status\" field with a human readable explanation of\n    what went wrong as well as a \"error_type\" field indicating the kind of error that can be mapped\n    to a Python type.\n\n    There's a fallback error UnknownError for other types of exceptions (network issues, api\n    gateway problems, etc.)\n    \"\"\"\n    try:\n        body = res.json()\n    except JSONDecodeError:\n        raise UnknownApiError(res.text)\n\n    if 'error_type' not in body:\n        raise UnknownApiError(str(body))\n\n    error_type = body['error_type']\n    status = body['status']\n\n    if re.search(r\"[0-9]+ qubits were requested, but the QVM is limited to [0-9]+ qubits.\", status):\n        return TooManyQubitsError(status)\n\n    error_cls = error_mapping.get(error_type, UnknownApiError)\n    return error_cls(status)",
        "rewrite": "\n\n\ndef parse_error(res):\n    try:\n        body = res.json()\n    except JSONDecodeError:\n        raise UnknownApiError(res.text)\n\n    if 'error_type' not in body:\n        raise UnknownApiError(str(body))\n\n    error_type = body['error_type']\n    status = body['status']\n\n    if re.search(r\"[0-9]+ qubits were requested, but the QVM is limited to [0-9]+ qubits.\", status):\n        return TooManyQubitsError(status)\n\n    error_cls = error_mapping.get(error_type, UnknownApiError)\n    return error_cls(status)\n\n\ndef test_parse_error"
    },
    {
        "original": "def begin_transaction(self, transaction_type, trace_parent=None):\n        \"\"\"\n        Start a new transactions and bind it in a thread-local variable\n\n        :returns the Transaction object\n        \"\"\"\n        if trace_parent:\n            is_sampled = bool(trace_parent.trace_options.recorded)\n        else:\n            is_sampled = self._sample_rate == 1.0 or self._sample_rate > random.random()\n        transaction = Transaction(self, transaction_type, trace_parent=trace_parent, is_sampled=is_sampled)\n        if trace_parent is None:\n            transaction.trace_parent = TraceParent(\n                constants.TRACE_CONTEXT_VERSION,\n                \"%032x\" % random.getrandbits(128),\n                transaction.id,\n                TracingOptions(recorded=is_sampled),\n            )\n        execution_context.set_transaction(transaction)\n        return transaction",
        "rewrite": " \"def begin_transaction(self, transaction_type, trace_parent=None):\n        \"\"\"\n        Start a new transactions and bind it in a thread-local variable\n\n        :returns the Transaction object\n        \"\"\"\n        if trace_parent:\n            is_sampled = bool(trace_parent.trace_options.recorded)\n        else:\n            is_sampled = self._sample_rate == 1.0 or self._sample_rate > random.random()\n        transaction = Transaction(self, transaction_type, trace_parent=trace_parent, is_sampled=is_sampled)\n        if trace_parent is None:\n            transaction.trace_parent = TraceParent(\n                constants.TRACE_CONTEXT_VERSION,\n                \"%"
    },
    {
        "original": "def _split_area(self, xs, lower, upper):\n        \"\"\"\n        Splits area plots at nans and returns x- and y-coordinates for\n        each area separated by nans.\n        \"\"\"\n        xnan = np.array([np.datetime64('nat') if xs.dtype.kind == 'M' else np.nan])\n        ynan = np.array([np.datetime64('nat') if lower.dtype.kind == 'M' else np.nan])\n        split = np.where(~isfinite(xs) | ~isfinite(lower) | ~isfinite(upper))[0]\n        xvals = np.split(xs, split)\n        lower = np.split(lower, split)\n        upper = np.split(upper, split)\n        band_x, band_y = [], []\n        for i, (x, l, u) in enumerate(zip(xvals, lower, upper)):\n            if i:\n                x, l, u = x[1:], l[1:], u[1:]\n            if not len(x):\n                continue\n            band_x += [np.append(x, x[::-1]), xnan]\n            band_y += [np.append(l, u[::-1]), ynan]\n        if len(band_x):\n            xs = np.concatenate(band_x[:-1])\n            ys = np.concatenate(band_y[:-1])\n            return xs, ys\n        return [], []",
        "rewrite": "\"def _split_area(self, xs, lower, upper):\n        \"\"\"\n        Splits area plots at nans and returns x- and y-coordinates for\n        each area separated by nans.\n        \"\"\"\n        xnan = np.array([np.datetime64('nat') if xs.dtype.kind == 'M' else np.nan])\n        ynan = np.array([np.datetime64('nat') if lower.dtype.kind == 'M' else np.nan])\n        split = np.where(~isfinite(xs) | ~isfinite(lower) | ~is"
    },
    {
        "original": "def _salt_send_event(opaque, conn, data):\n    \"\"\"\n    Convenience function adding common data to the event and sending it\n    on the salt event bus.\n\n    :param opaque: the opaque data that is passed to the callback.\n                   This is a dict with 'prefix', 'object' and 'event' keys.\n    :param conn: libvirt connection\n    :param data: additional event data dict to send\n    \"\"\"\n    tag_prefix = opaque['prefix']\n    object_type = opaque['object']\n    event_type = opaque['event']\n\n    # Prepare the connection URI to fit in the tag\n    # qemu+ssh://user@host:1234/system -> qemu+ssh/user@host:1234/system\n    uri = urlparse(conn.getURI())\n    uri_tag = [uri.scheme]\n    if uri.netloc:\n        uri_tag.append(uri.netloc)\n    path = uri.path.strip('/')\n    if path:\n        uri_tag.append(path)\n    uri_str = \"/\".join(uri_tag)\n\n    # Append some common data\n    all_data = {\n        'uri': conn.getURI()\n    }\n    all_data.update(data)\n\n    tag = '/'.join((tag_prefix, uri_str, object_type, event_type))\n\n    # Actually send the event in salt\n    if __opts__.get('__role') == 'master':\n        salt.utils.event.get_master_event(\n            __opts__,\n            __opts__['sock_dir']).fire_event(all_data, tag)\n    else:\n        __salt__['event.send'](tag, all_data)",
        "rewrite": " \"def _salt_send_event(opaque, conn, data):\n    Convenience function adding common data to the event and sending it\n    on the salt event bus.\n\n    :param opaque: the opaque data that is passed to the callback.\n                   This is a dict with 'prefix', 'object' and 'event' keys.\n    :param conn: libvirt connection\n    :param data: additional event data dict to send\n    \"\"\"\n    tag_prefix = opaque['prefix']\n    object_type = opaque['object']\n    event_type = opaque['event']\n\n    # Prepare the connection URI to fit in the tag\n    # qemu"
    },
    {
        "original": "def EnumerateInterfacesFromClient(args):\n  \"\"\"Enumerate all MAC addresses of all NICs.\n\n  Args:\n    args: Unused.\n\n  Yields:\n    `rdf_client_network.Interface` instances.\n  \"\"\"\n  del args  # Unused.\n\n  pythoncom.CoInitialize()\n  for interface in (wmi.WMI().Win32_NetworkAdapterConfiguration() or []):\n    addresses = []\n    for ip_address in interface.IPAddress or []:\n      addresses.append(\n          rdf_client_network.NetworkAddress(human_readable_address=ip_address))\n\n    response = rdf_client_network.Interface(ifname=interface.Description)\n    if interface.MACAddress:\n      response.mac_address = binascii.unhexlify(\n          interface.MACAddress.replace(\":\", \"\"))\n    if addresses:\n      response.addresses = addresses\n\n    yield response",
        "rewrite": " \"def EnumerateInterfacesFromClient(args):\n  \"\"\"Enumerate all MAC addresses of all NICs.\n\n  Args:\n    args: Unused.\n\n  Yields:\n    `rdf_client_network.Interface` instances.\n  \"\"\"\n  del args  # Unused.\n\n  pythoncom.CoInitialize()\n  for interface in (wmi.WMI().Win32_NetworkAdapterConfiguration() or []):\n    addresses = []\n    for ip_address in interface.IPAddress or []:\n      addresses.append(\n          rdf_client_network.NetworkAddress(human_readable_address=ip_address))\n\n    response = rdf_client_network.Interface(ifname=interface.Description)\n    if interface.MACAddress"
    },
    {
        "original": "def _full_like_variable(other, fill_value,\n                        dtype: Union[str, np.dtype, None] = None):\n    \"\"\"Inner function of full_like, where other must be a variable\n    \"\"\"\n    from .variable import Variable\n\n    if isinstance(other.data, dask_array_type):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype,\n                               chunks=other.data.chunks)\n    else:\n        data = np.full_like(other, fill_value, dtype=dtype)\n\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)",
        "rewrite": "\"def _full_like_variable(other, fill_value,\n                        dtype: Union[str, np.dtype, None] = None):\n    \"\"\"Inner function of full_like, where other must be a variable\n    \"\"\"\n    from.variable import Variable\n\n    if isinstance(other.data, dask_array_type):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype,\n                               chunks=other.data.chunks)\n    else:\n        data = np.full_like"
    },
    {
        "original": "def prompt_user_to_select_link(self, links):\n        \"\"\"\n        Prompt the user to select a link from a list to open.\n\n        Return the link that was selected, or ``None`` if no link was selected.\n        \"\"\"\n        link_pages = self.get_link_pages(links)\n        n = 0\n        while n in range(len(link_pages)):\n            link_page = link_pages[n]\n            text = 'Select a link to open (page {} of {}):\\n\\n'\n            text = text.format(n+1, len(link_pages))\n            text += self.get_link_page_text(link_page)\n            if link_page is not link_pages[-1]:\n                text += '[j] next page...'\n            if link_page is not link_pages[0]:\n                if link_page is not link_pages[-1]:\n                    text += '\\n'\n                text += '[k] ...previous page'\n\n            try:\n                choice = chr(self.show_notification(text))\n                try:\n                    choice = int(choice)\n                except ValueError:\n                    pass\n            except ValueError:\n                return None\n            if choice == 'j':\n                if link_page is not link_pages[-1]:\n                    n += 1\n                continue\n            elif choice == 'k':\n                if link_page is not link_pages[0]:\n                    n -= 1\n                continue\n            elif choice not in range(len(link_page)):\n                return None\n            return link_page[choice]['href']",
        "rewrite": " \"def prompt_user_to_select_link(self, links):\n        \"\"\"\n        Prompt the user to select a link from a list to open.\n\n        Return the link that was selected, or ``None`` if no link was selected.\n        \"\"\"\n        link_pages = self.get_link_pages(links)\n        n = 0\n        while n in range(len(link_pages)):\n            link_page = link_pages[n]\n            text = 'Select a link to open (page {} of {}):\\n\\n'\n            text = text.format(n+1, len(link_pages))\n            text += self.get_link_page_text(link_page)\n            if link_page is not"
    },
    {
        "original": "def posterior_to_xarray(self):\n        \"\"\"Convert the posterior to an xarray dataset.\"\"\"\n        var_names = self.pymc3.util.get_default_varnames(  # pylint: disable=no-member\n            self.trace.varnames, include_transformed=False\n        )\n        data = {}\n        for var_name in var_names:\n            data[var_name] = np.array(self.trace.get_values(var_name, combine=False, squeeze=False))\n        return dict_to_dataset(data, library=self.pymc3, coords=self.coords, dims=self.dims)",
        "rewrite": " \"def posterior_to_xarray(self):\n        \"\"\"Convert the posterior to an xarray dataset.\"\"\"\n        var_names = self.pymc3.util.get_default_varnames(  # pylint: disable=no-member\n            self.trace.varnames, include_transformed=False\n        )\n        data = {}\n        for var_name in var_names:\n            data[var_name] = np.array(self.trace.get_values(var_name, combine=False, squeeze=False))\n        return dict_to_dataset(data, library=self.pymc3, coords=self.coords, dims=self.dims)\"\"def"
    },
    {
        "original": "def _extract_return(self, data):\n        \"\"\"\n        Extracts return data from the results.\n\n        :param data:\n        :return:\n        \"\"\"\n        if isinstance(data, dict):\n            data = data.get('return', data)\n\n        return data",
        "rewrite": "\"def _extract_return(self, data):\n        \"\"\"\n        Extracts return data from the results.\n\n        :param data:\n        :return:\n        \"\"\"\n        if isinstance(data, dict):\n            data = data.get('return', data)\n\n        return data\"\"def _extract_return(self, data):\n        \"\"\"\n        Extracts return data from the results.\n\n        :param data:\n        :return:\n        \"\"\"\n        if isinstance(data, dict):\n            data = data.get('return"
    },
    {
        "original": "def _label_columns_json(self, cols=None):\n        \"\"\"\n            Prepares dict with labels to be JSON serializable\n        \"\"\"\n        ret = {}\n        cols = cols or []\n        d = {k: v for (k, v) in self.label_columns.items() if k in cols}\n        for key, value in d.items():\n            ret[key] = as_unicode(_(value).encode(\"UTF-8\"))\n        return ret",
        "rewrite": " \"def _label_columns_json(self, cols=None):\n        \"\"\"\n            Prepares dict with labels to be JSON serializable\n        \"\"\"\n        ret = {}\n        cols = cols or []\n        d = {k: v for (k, v) in self.label_columns.items() if k in cols}\n        for key, value in d.items():\n            ret[key] = as_unicode(_(value).encode(\"UTF-8\"))\n        return ret\"\"def _label_columns_json(self, cols=None):\n        \"\"\"\n            Prepares dict with labels to be JSON"
    },
    {
        "original": "def RemoveLabels(self, labels_names, owner=None):\n    \"\"\"Remove specified labels from the AFF4Object.\"\"\"\n    if owner is None and not self.token:\n      raise ValueError(\"Can't remove label: No owner specified and \"\n                       \"no access token available.\")\n    if isinstance(labels_names, string_types):\n      raise ValueError(\"Label list can't be string.\")\n\n    owner = owner or self.token.username\n\n    current_labels = self.Get(self.Schema.LABELS)\n    for label_name in labels_names:\n      label = rdf_aff4.AFF4ObjectLabel(name=label_name, owner=owner)\n      current_labels.RemoveLabel(label)\n\n    self.Set(self.Schema.LABELS, current_labels)",
        "rewrite": " \"def RemoveLabels(self, labels_names, owner=None):\n    \"\"\"Remove specified labels from the AFF4Object.\"\"\"\n    if owner is None and not self.token:\n      raise ValueError(\"Can't remove label: No owner specified and \"\n                       \"no access token available.\")\n    if isinstance(labels_names, string_types):\n      raise ValueError(\"Label list can't be string.\")\n\n    owner = owner or self.token.username\n\n    current_labels = self.Get(self.Schema.LABELS)\n    for label_name in labels_names:\n      label = rdf_aff4.AFF4ObjectLabel(name=label_name, owner=owner)\n      current_labels.RemoveLabel(label"
    },
    {
        "original": "def conditional_jit(function=None, **kwargs):  # noqa: D202\n    \"\"\"Use numba's jit decorator if numba is installed.\n\n    Notes\n    -----\n        If called without arguments  then return wrapped function.\n\n        @conditional_jit\n        def my_func():\n            return\n\n        else called with arguments\n\n        @conditional_jit(nopython=True)\n        def my_func():\n            return\n\n    \"\"\"\n\n    def wrapper(function):\n        try:\n            numba = importlib.import_module(\"numba\")\n            return numba.jit(**kwargs)(function)\n\n        except ImportError:\n            return function\n\n    if function:\n        return wrapper(function)\n    else:\n        return wrapper",
        "rewrite": " \"def conditional_jit(function=None, **kwargs):  # noqa: D202\n    \"\"\"Use numba's jit decorator if numba is installed.\n\n    Notes\n    -----\n        If called without arguments  then return wrapped function.\n\n        @conditional_jit\n        def my_func():\n            return\n\n        else called with arguments\n\n        @conditional_jit(nopython=True)\n        def my_func():\n            return\n\n    \"\"\"\n\n    def wrapper(function):\n        try:\n            numba = importlib.import_module(\"numba\")\n            return numba.jit(**kwargs)(function)\n\n        except ImportError:\n            return function"
    },
    {
        "original": "def add_row(self):\n        \"\"\"\n        Return a |_Row| instance, newly added bottom-most to the table.\n        \"\"\"\n        tbl = self._tbl\n        tr = tbl.add_tr()\n        for gridCol in tbl.tblGrid.gridCol_lst:\n            tc = tr.add_tc()\n            tc.width = gridCol.w\n        return _Row(tr, self)",
        "rewrite": "\"def add_row(self):\n        \"\"\"\n        Return a |_Row| instance, newly added bottom-most to the table.\n        \"\"\"\n        tbl = self._tbl\n        tr = tbl.add_tr()\n        for gridCol in tbl.tblGrid.gridCol_lst:\n            tc = tr.add_tc()\n            tc.width = gridCol.w\n        return _Row(tr, self)\"\"def add_row(self):\n        \"\"\"\n        Return a |_Row| instance, newly added bottom-most to the"
    },
    {
        "original": "def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a\n        dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })",
        "rewrite": " \"def _get_matrix(self):\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n               'side_effect': scenario.side_effect_sequence,\n               'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n               'verify':"
    },
    {
        "original": "def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes",
        "rewrite": "\"def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if '"
    },
    {
        "original": "def read(self, length=None):\n        \"\"\"Read the given amount of bytes.\"\"\"\n        if length is None:\n            return self.reader.read()\n\n        result = self.reader.read(length)\n        if len(result) != length:\n            raise BufferError(\n                'No more data left to read (need {}, got {}: {}); last read {}'\n                .format(length, len(result), repr(result), repr(self._last))\n            )\n\n        self._last = result\n        return result",
        "rewrite": ".format(length, len(result), repr(result), repr(self._last))\"def read(self, length=None): \"\"\"Read the given amount of bytes.\"\"\" if length is None: return self.reader.read() result = self.reader.read(length) if len(result)!= length: raise BufferError( 'No more data left to read (need {}, got {}: {}); last read {}'.format(length, len(result), repr(result), repr(self._last)) self._last = result return result\""
    },
    {
        "original": "def _calc_recip(self):\n        \"\"\"\n        Perform the reciprocal space summation. Calculates the quantity\n        E_recip = 1/(2PiV) sum_{G < Gmax} exp(-(G.G/4/eta))/(G.G) S(G)S(-G)\n        where\n        S(G) = sum_{k=1,N} q_k exp(-i G.r_k)\n        S(G)S(-G) = |S(G)|**2\n\n        This method is heavily vectorized to utilize numpy's C backend for\n        speed.\n        \"\"\"\n        numsites = self._s.num_sites\n        prefactor = 2 * pi / self._vol\n        erecip = np.zeros((numsites, numsites), dtype=np.float)\n        forces = np.zeros((numsites, 3), dtype=np.float)\n        coords = self._coords\n        rcp_latt = self._s.lattice.reciprocal_lattice\n        recip_nn = rcp_latt.get_points_in_sphere([[0, 0, 0]], [0, 0, 0],\n                                                 self._gmax)\n\n        frac_coords = [fcoords for (fcoords, dist, i, img) in recip_nn if dist != 0]\n\n        gs = rcp_latt.get_cartesian_coords(frac_coords)\n        g2s = np.sum(gs ** 2, 1)\n        expvals = np.exp(-g2s / (4 * self._eta))\n        grs = np.sum(gs[:, None] * coords[None, :], 2)\n\n        oxistates = np.array(self._oxi_states)\n\n        # create array where q_2[i,j] is qi * qj\n        qiqj = oxistates[None, :] * oxistates[:, None]\n\n        # calculate the structure factor\n        sreals = np.sum(oxistates[None, :] * np.cos(grs), 1)\n        simags = np.sum(oxistates[None, :] * np.sin(grs), 1)\n\n        for g, g2, gr, expval, sreal, simag in zip(gs, g2s, grs, expvals,\n                                                   sreals, simags):\n\n            # Uses the identity sin(x)+cos(x) = 2**0.5 sin(x + pi/4)\n            m = (gr[None, :] + pi / 4) - gr[:, None]\n            np.sin(m, m)\n            m *= expval / g2\n\n            erecip += m\n\n            if self._compute_forces:\n                pref = 2 * expval / g2 * oxistates\n                factor = prefactor * pref * (\n                    sreal * np.sin(gr) - simag * np.cos(gr))\n\n                forces += factor[:, None] * g[None, :]\n\n        forces *= EwaldSummation.CONV_FACT\n        erecip *= prefactor * EwaldSummation.CONV_FACT * qiqj * 2 ** 0.5\n        return erecip, forces",
        "rewrite": " + pi / 4)[:, None]\n            n = (gr - pi / 4)[None, :]\n\n            # calculate the structure factor\n            sgs = 2 ** 0.5 * (np.sin(m) + np.cos(n))\n\n            # calculate the force\n            f = -expval * sgs * g\n\n            # calculate the energy\n            e = expval * sgs\n\n            # add the force and energy contributions\n            forces += f\n            erecip += e\n\n        # multiply by the prefactor\n        forces *= prefactor * qiqj\n        erecip *="
    },
    {
        "original": "def Parse(self, stat, file_object, knowledge_base):\n    \"\"\"Parse the History file.\"\"\"\n    _ = knowledge_base\n    # TODO(user): Convert this to use the far more intelligent plaso parser.\n    chrome = ChromeParser(file_object)\n    for timestamp, entry_type, url, data1, _, _ in chrome.Parse():\n      if entry_type == \"CHROME_DOWNLOAD\":\n        yield rdf_webhistory.BrowserHistoryItem(\n            url=url,\n            domain=urlparse.urlparse(url).netloc,\n            access_time=timestamp,\n            program_name=\"Chrome\",\n            source_path=file_object.Path(),\n            download_path=data1)\n      elif entry_type == \"CHROME_VISIT\":\n        yield rdf_webhistory.BrowserHistoryItem(\n            url=url,\n            domain=urlparse.urlparse(url).netloc,\n            access_time=timestamp,\n            program_name=\"Chrome\",\n            source_path=file_object.Path(),\n            title=data1)",
        "rewrite": " \"def Parse(self, stat, file_object, knowledge_base):\n    \"\"\"Parse the History file.\"\"\"\n    _ = knowledge_base\n    # TODO(user): Convert this to use the far more intelligent plaso parser.\n    chrome = ChromeParser(file_object)\n    for timestamp, entry_type, url, data1, _, _ in chrome.Parse():\n      if entry_type == \"CHROME_DOWNLOAD\":\n        yield rdf_webhistory.BrowserHistoryItem(\n            url=url,\n            domain=urlparse.urlparse(url).netloc,\n            access_time=timestamp,\n            program_name=\"Chrome\",\n            source_path=file_object.Path(),\n            download_path=data1"
    },
    {
        "original": "def _unpack_paths(cls, objs, items, counts):\n        \"\"\"\n        Recursively unpacks lists and ViewableTree-like objects, accumulating\n        into the supplied list of items.\n        \"\"\"\n        if type(objs) is cls:\n            objs = objs.items()\n        for item in objs:\n            path, obj = item if isinstance(item, tuple) else (None, item)\n            if type(obj) is cls:\n                cls._unpack_paths(obj, items, counts)\n                continue\n            new = path is None or len(path) == 1\n            path = util.get_path(item) if new else path\n            new_path = util.make_path_unique(path, counts, new)\n            items.append((new_path, obj))",
        "rewrite": " \"def _unpack_paths(cls, objs, items, counts):\n        \"\"\"\n        Recursively unpacks lists and ViewableTree-like objects, accumulating\n        into the supplied list of items.\n        \"\"\"\n        if type(objs) is cls:\n            objs = objs.items()\n        for item in objs:\n            path, obj = item if isinstance(item, tuple) else (None, item)\n            if type(obj) is cls:\n                cls._unpack_paths(obj, items, counts)\n                continue\n            new = path is None or len(path) == 1\n            path = util.get_path(item) if new else path"
    },
    {
        "original": "def any(self, *, collection, attribute, word, func=None, operation=None):\n        \"\"\" Performs a filter with the OData 'any' keyword on the collection\n\n        For example:\n        q.any(collection='email_addresses', attribute='address',\n        operation='eq', word='george@best.com')\n\n        will transform to a filter such as:\n\n        emailAddresses/any(a:a/address eq 'george@best.com')\n\n        :param str collection: the collection to apply the any keyword on\n        :param str attribute: the attribute of the collection to check\n        :param str word: the word to check\n        :param str func: the logical function to apply to the attribute\n         inside the collection\n        :param str operation: the logical operation to apply to the\n         attribute inside the collection\n        :rtype: Query\n        \"\"\"\n\n        return self.iterable('any', collection=collection, attribute=attribute,\n                             word=word, func=func, operation=operation)",
        "rewrite": "def any(self, *, collection, attribute, word, func=None, operation=None):\n        \"\"\" Performs a filter with the OData 'any' keyword on the collection\n\n        For example:\n        q.any(collection='email_addresses', attribute='address',\n        operation='eq', word='george@best.com')\n\n        will transform to a filter such as:\n\n        emailAddresses/any(a:a/address eq 'george@best.com')\n\n        :param str collection: the collection to apply the any keyword on\n        :param str attribute: the attribute of"
    },
    {
        "original": "def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n    r\"\"\"\n    Construct a BitFlipChannel that flips a qubit state\n    with probability of a flip given by p. If p is None, return\n    a guaranteed flip in the form of an X operation.\n\n    This channel evolves a density matrix via\n\n        $$\n        \\rho \\rightarrow M_0 \\rho M_0^\\dagger + M_1 \\rho M_1^\\dagger\n        $$\n\n    With\n\n        $$\n        \\begin{aligned}\n        M_0 =& \\sqrt{p} \\begin{bmatrix}\n                            1 & 0 \\\\\n                            0 & 1\n                       \\end{bmatrix}\n        \\\\\n        M_1 =& \\sqrt{1-p} \\begin{bmatrix}\n                            0 & 1 \\\\\n                            1 & -0\n                         \\end{bmatrix}\n        \\end{aligned}\n        $$\n\n    Args:\n        p: the probability of a bit flip.\n\n    Raises:\n        ValueError: if p is not a valid probability.\n    \"\"\"\n    if p is None:\n        return pauli_gates.X\n\n    return _bit_flip(p)",
        "rewrite": " \"def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n    r\"\"\"\n    Construct a BitFlipChannel that flips a qubit state\n    with probability of a flip given by p. If p is None, return\n    a guaranteed flip in the form of an X operation.\n\n    This channel evolves a density matrix via\n\n        $$\n        \\rho \\rightarrow M_0 \\rho M_0^\\dagger + M_1 \\rho M_1^\\dagger\n        $$\n\n    With\n\n        $$\n        \\begin{"
    },
    {
        "original": "def _write_gml(G, path):\n    \"\"\"\n    Wrapper around nx.write_gml\n    \"\"\"\n    import networkx as nx\n    return nx.write_gml(G, path, stringizer=str)",
        "rewrite": "\"def _write_gml(G, path):\n    \"\"\"\n    Wrapper around nx.write_gml\n    \"\"\"\n    import networkx as nx\n    return nx.write_gml(G, path, stringizer=str)\"\"def _write_gml(G, path):\n    \"\"\"\n    Wrapper around nx.write_gml\n    \"\"\"\n    import networkx as nx\n    return nx.write_gml(G, path, stringizer=str)\""
    },
    {
        "original": "def strip_output(nb):\n    \"\"\"strip the outputs from a notebook object\"\"\"\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for cell in _cells(nb):\n        if 'outputs' in cell:\n            cell['outputs'] = []\n        if 'prompt_number' in cell:\n            cell['prompt_number'] = None\n    return nb",
        "rewrite": " \"def strip_output(nb):\n    \"\"\"strip the outputs from a notebook object\"\"\"\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for cell in _cells(nb):\n        if 'outputs' in cell:\n            cell['outputs'] = []\n        if 'prompt_number' in cell:\n            cell['prompt_number'] = None\n    return nb\"\"def strip_output(nb):\n    \"\"\"strip the outputs from a notebook object\"\"\"\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)"
    },
    {
        "original": "def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])",
        "rewrite": " \"def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n   .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])\"\"def parse_if_range_header(value"
    },
    {
        "original": "def update(self, stats):\n        \"\"\"Update stats in the CSV output file.\"\"\"\n        # Get the stats\n        all_stats = stats.getAllExportsAsDict(plugin_list=self.plugins_to_export())\n\n        # Init data with timestamp (issue#708)\n        if self.first_line:\n            csv_header = ['timestamp']\n        csv_data = [time.strftime('%Y-%m-%d %H:%M:%S')]\n\n        # Loop over plugins to export\n        for plugin in self.plugins_to_export():\n            if isinstance(all_stats[plugin], list):\n                for stat in all_stats[plugin]:\n                    # First line: header\n                    if self.first_line:\n                        csv_header += ('{}_{}_{}'.format(\n                            plugin, self.get_item_key(stat), item) for item in stat)\n                    # Others lines: stats\n                    csv_data += itervalues(stat)\n            elif isinstance(all_stats[plugin], dict):\n                # First line: header\n                if self.first_line:\n                    fieldnames = iterkeys(all_stats[plugin])\n                    csv_header += ('{}_{}'.format(plugin, fieldname)\n                                   for fieldname in fieldnames)\n                # Others lines: stats\n                csv_data += itervalues(all_stats[plugin])\n\n        # Export to CSV\n        if self.first_line:\n            self.writer.writerow(csv_header)\n            self.first_line = False\n        self.writer.writerow(csv_data)\n        self.csv_file.flush()",
        "rewrite": " \"def update(self, stats):\n        \"\"\"Update stats in the CSV output file.\"\"\"\n        # Get the stats\n        all_stats = stats.getAllExportsAsDict(plugin_list=self.plugins_to_export())\n\n        # Init data with timestamp (issue#708)\n        if self.first_line:\n            csv_header = ['timestamp']\n        csv_data = [time.strftime('%Y-%m-%d %H:%M:%S')]\n\n        # Loop over plugins to export\n        for plugin in self.plugins_to_export():\n            if isinstance(all_stats[plugin], list):\n                for stat in all_stats[plugin]:\n                    # First line: header"
    },
    {
        "original": "def _expectation(p, constant_mean, none, kern, feat, nghp=None):\n    \"\"\"\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T K_{x_n, Z}>_p(x_n)\n        - m(x_i) = c :: Constant function\n        - K_{.,.}    :: Kernel function\n\n    :return: NxQxM\n    \"\"\"\n    with params_as_tensors_for(constant_mean):\n        c = constant_mean(p.mu)  # NxQ\n        eKxz = expectation(p, (kern, feat), nghp=nghp)  # NxM\n\n        return c[..., None] * eKxz[:, None, :]",
        "rewrite": " \"def _expectation(p, constant_mean, none, kern, feat, nghp=None):\n    \"\"\"\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T K_{x_n, Z}>_p(x_n)\n        - m(x_i) = c :: Constant function\n        - K_{.,.}    :: Kernel function\n\n    :return: NxQxM\n    \"\"\"\n    with params_as_tensors_for(constant_mean):\n        c = constant_mean(p.mu)  # NxQ\n        eKxz = expectation(p, (kern, feat), nghp=nghp)"
    },
    {
        "original": "def num_having_numId(self, numId):\n        \"\"\"\n        Return the ``<w:num>`` child element having ``numId`` attribute\n        matching *numId*.\n        \"\"\"\n        xpath = './w:num[@w:numId=\"%d\"]' % numId\n        try:\n            return self.xpath(xpath)[0]\n        except IndexError:\n            raise KeyError('no <w:num> element with numId %d' % numId)",
        "rewrite": "\"def num_having_numId(self, numId):\n        \"\"\"\n        Return the ``<w:num>`` child element having ``numId`` attribute\n        matching *numId*.\n        \"\"\"\n        xpath = './w:num[@w:numId=\"%d\"]' % numId\n        try:\n            return self.xpath(xpath)[0]\n        except IndexError:\n            raise KeyError('no <w:num> element with numId %d' % numId)\"\"def num"
    },
    {
        "original": "def compute_tls13_handshake_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for Handshake data.\n        self.handshake_messages should be ClientHello...ServerHello.\n        \"\"\"\n        if self.tls13_early_secret is None:\n            warning(\"No early secret. This is abnormal.\")\n\n        hkdf = self.prcs.hkdf\n\n        self.tls13_handshake_secret = hkdf.extract(self.tls13_early_secret,\n                                                   self.tls13_dhe_secret)\n\n        chts = hkdf.derive_secret(self.tls13_handshake_secret,\n                                  b\"client handshake traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"client_handshake_traffic_secret\"] = chts\n\n        shts = hkdf.derive_secret(self.tls13_handshake_secret,\n                                  b\"server handshake traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"server_handshake_traffic_secret\"] = shts\n\n        if self.connection_end == \"server\":\n            self.prcs.tls13_derive_keys(chts)\n            self.pwcs.tls13_derive_keys(shts)\n        elif self.connection_end == \"client\":\n            self.pwcs.tls13_derive_keys(chts)\n            self.prcs.tls13_derive_keys(shts)",
        "rewrite": " \"def compute_tls13_handshake_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for Handshake data.\n        self.handshake_messages should be ClientHello...ServerHello.\n        \"\"\"\n        if self.tls13_early_secret is None:\n            warning(\"No early secret. This is abnormal.\")\n\n        hkdf = self.prcs.hkdf\n\n        self.tls13_handshake_secret = hkdf.extract(self.tls13_early_secret,\n                                                   self.tls13_dhe_secret)\n\n        chts = hkdf.derive_secret(self.tls13_handshake_secret,\n                                  b\"client"
    },
    {
        "original": "def do_genesis(args, data_dir=None):\n    \"\"\"Given the command args, take an series of input files containing\n    GenesisData, combine all the batches into one GenesisData, and output the\n    result into a new file.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = get_data_dir()\n\n    if not os.path.exists(data_dir):\n        raise CliException(\n            \"Data directory does not exist: {}\".format(data_dir))\n\n    genesis_batches = []\n    for input_file in args.input_file:\n        print('Processing {}...'.format(input_file))\n        input_data = BatchList()\n        try:\n            with open(input_file, 'rb') as in_file:\n                input_data.ParseFromString(in_file.read())\n        except:\n            raise CliException('Unable to read {}'.format(input_file))\n\n        genesis_batches += input_data.batches\n\n    _validate_depedencies(genesis_batches)\n    _check_required_settings(genesis_batches)\n\n    if args.output:\n        genesis_file = args.output\n    else:\n        genesis_file = os.path.join(data_dir, 'genesis.batch')\n\n    print('Generating {}'.format(genesis_file))\n    output_data = GenesisData(batches=genesis_batches)\n    with open(genesis_file, 'wb') as out_file:\n        out_file.write(output_data.SerializeToString())",
        "rewrite": " \"def do_genesis(args, data_dir=None):\n    \"\"\"Given the command args, take an series of input files containing\n    GenesisData, combine all the batches into one GenesisData, and output the\n    result into a new file.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = get_data_dir()\n\n    if not os.path.exists(data_dir):\n        raise CliException(\n            \"Data directory does not exist: {}\".format(data_dir))\n\n    genesis_batches = []\n    for input_file in args.input_file:\n        print('Processing {}...'.format(input_file))\n        input_data = BatchList()\n        try"
    },
    {
        "original": "def interactive_output(f, controls):\n    \"\"\"Connect widget controls to a function.\n\n    This function does not generate a user interface for the widgets (unlike `interact`).\n    This enables customisation of the widget user interface layout.\n    The user interface layout must be defined and displayed manually.\n    \"\"\"\n\n    out = Output()\n    def observer(change):\n        kwargs = {k:v.value for k,v in controls.items()}\n        show_inline_matplotlib_plots()\n        with out:\n            clear_output(wait=True)\n            f(**kwargs)\n            show_inline_matplotlib_plots()\n    for k,w in controls.items():\n        w.observe(observer, 'value')\n    show_inline_matplotlib_plots()\n    observer(None)\n    return out",
        "rewrite": "\"def interactive_output(f, controls):\n    \"\"\"Connect widget controls to a function.\n\n    This function does not generate a user interface for the widgets (unlike `interact`).\n    This enables customisation of the widget user interface layout.\n    The user interface layout must be defined and displayed manually.\n    \"\"\"\n\n    out = Output()\n    def observer(change):\n        kwargs = {k:v.value for k,v in controls.items()}\n        show_inline_matplotlib_plots()\n        with out:\n            clear_output(wait=True)\n            f(**kwargs)\n            show_inline"
    },
    {
        "original": "def install_app(app, target='/Applications/'):\n    \"\"\"\n    Install an app file by moving it into the specified Applications directory\n\n    Args:\n        app (str): The location of the .app file\n        target (str): The target in which to install the package to\n                      Default is ''/Applications/''\n\n    Returns:\n        str: The results of the rsync command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macpackage.install_app /tmp/tmp.app /Applications/\n    \"\"\"\n\n    if target[-4:] != '.app':\n        if app[-1:] == '/':\n            base_app = os.path.basename(app[:-1])\n        else:\n            base_app = os.path.basename(app)\n\n        target = os.path.join(target, base_app)\n\n    if not app[-1] == '/':\n        app += '/'\n\n    cmd = 'rsync -a --delete \"{0}\" \"{1}\"'.format(app, target)\n    return __salt__['cmd.run'](cmd)",
        "rewrite": " \"def install_app(app, target='/Applications/'):\n    \"\"\"\n    Install an app file by moving it into the specified Applications directory\n\n    Args:\n        app (str): The location of the.app file\n        target (str): The target in which to install the package to\n                      Default is ''/Applications/''\n\n    Returns:\n        str: The results of the rsync command\n\n    CLI Example:\n\n   .. code-block:: bash\n\n        salt '*' macpackage.install_app /tmp/tmp.app /Applications/\n    \"\"\"\n\n    if target[-4:]!= '.app':\n        if app[-1:] == '/':\n            base"
    },
    {
        "original": "def _construct(self, graph, entry_node):\n        \"\"\"\n        Find post-dominators for each node in the graph.\n\n        This implementation is based on paper A Fast Algorithm for Finding Dominators in a Flow Graph by Thomas\n        Lengauer and Robert E. Tarjan from Stanford University, ACM Transactions on Programming Languages and Systems,\n        Vol. 1, No. 1, July 1979\n        \"\"\"\n\n        # Step 1\n\n        _prepared_graph, vertices, parent = self._prepare_graph(graph, entry_node)\n        # vertices is a list of ContainerNode instances\n        # parent is a dict storing the mapping from ContainerNode to ContainerNode\n        # Each node in prepared_graph is a ContainerNode instance\n\n        bucket = defaultdict(set)\n        dom = [None] * (len(vertices))\n        self._ancestor = [None] * (len(vertices) + 1)\n\n        for i in range(len(vertices) - 1, 0, -1):\n            w = vertices[i]\n\n            # Step 2\n            if w not in parent:\n                # It's one of the start nodes\n                continue\n\n            predecessors = _prepared_graph.predecessors(w)\n            for v in predecessors:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[w.index].index:\n                    self._semi[w.index] = self._semi[u.index]\n\n            bucket[vertices[self._semi[w.index].index].index].add(w)\n\n            self._pd_link(parent[w], w)\n\n            # Step 3\n            for v in bucket[parent[w].index]:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[v.index].index:\n                    dom[v.index] = u\n                else:\n                    dom[v.index] = parent[w]\n\n            bucket[parent[w].index].clear()\n\n        for i in range(1, len(vertices)):\n            w = vertices[i]\n            if w not in parent:\n                continue\n            if dom[w.index].index != vertices[self._semi[w.index].index].index:\n                dom[w.index] = dom[dom[w.index].index]\n\n        self.dom = networkx.DiGraph()  # The post-dom tree described in a directional graph\n        for i in range(1, len(vertices)):\n            if dom[i] is not None and vertices[i] is not None:\n                self.dom.add_edge(dom[i].obj, vertices[i].obj)\n\n        # Output\n        self.prepared_graph = _prepared_graph",
        "rewrite": ".dom.add_edge(dom[i], vertices[i])\n\n        self._post_dom = dom\n\n    def _pd_eval(self, v):\n        if self._ancestor[v.index] is None:\n            return v\n        self._compress(v)\n        return self._ancestor[v.index]\n\n    def _pd_link(self, u, v):\n        self._ancestor[v.index] = u\n\n    def _compress(self, v):\n        if self._ancestor[self._compress(self._ancestor[v.index]).index] is not None:\n            self._ancestor[v.index] = self._ancestor[self._compress(self._ancestor[v.index]).index]\n        return"
    },
    {
        "original": "def _aspect_preserving_resize(image, resize_min):\n  \"\"\"Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  \"\"\"\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RESIZE_ASPECT_PRESERVING,\n                          value={\"min\": resize_min})\n\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)",
        "rewrite": " \"def _smallest_size_at_least(height, width, resize_min):\n  \"\"\"Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new"
    },
    {
        "original": "def remove_small_boxes(boxlist, min_size):\n    \"\"\"\n    Only keep boxes with both sides >= min_size\n\n    Arguments:\n        boxlist (Boxlist)\n        min_size (int)\n    \"\"\"\n    # TODO maybe add an API for querying the ws / hs\n    xywh_boxes = boxlist.convert(\"xywh\").bbox\n    _, _, ws, hs = xywh_boxes.unbind(dim=1)\n    keep = (\n        (ws >= min_size) & (hs >= min_size)\n    ).nonzero().squeeze(1)\n    return boxlist[keep]",
        "rewrite": "\"def remove_small_boxes(boxlist, min_size):\n    \"\"\"\n    Only keep boxes with both sides >= min_size\n\n    Arguments:\n        boxlist (Boxlist)\n        min_size (int)\n    \"\"\"\n    # TODO maybe add an API for querying the ws / hs\n    xywh_boxes = boxlist.convert(\"xywh\").bbox\n    _, _, ws, hs = xywh_boxes.unbind(dim=1)\n    keep = (\n        (ws >= min_size) & (hs >= min_size)\n    ).nonzero().squeeze(1)"
    },
    {
        "original": "def is_parameterized(val: Any) -> bool:\n    \"\"\"Returns whether the object is parameterized with any Symbols.\n\n    A value is parameterized when it has an `_is_parameterized_` method and\n    that method returns a truthy value, or if the value is an instance of\n    sympy.Basic.\n\n    Returns:\n        True if the gate has any unresolved Symbols\n        and False otherwise. If no implementation of the magic\n        method above exists or if that method returns NotImplemented,\n        this will default to False.\n    \"\"\"\n    if isinstance(val, sympy.Basic):\n        return True\n\n    getter = getattr(val, '_is_parameterized_', None)\n    result = NotImplemented if getter is None else getter()\n\n    if result is not NotImplemented:\n        return result\n    else:\n        return False",
        "rewrite": " \"def is_parameterized(val: Any) -> bool:\n    \"\"\"Returns whether the object is parameterized with any Symbols.\n\n    A value is parameterized when it has an `_is_parameterized_` method and\n    that method returns a truthy value, or if the value is an instance of\n    sympy.Basic.\n\n    Returns:\n        True if the gate has any unresolved Symbols\n        and False otherwise. If no implementation of the magic\n        method above exists or if that method returns NotImplemented,\n        this will default to False.\n    \"\"\"\n    if isinstance(val, sympy.Basic):\n        return True\n\n    getter ="
    },
    {
        "original": "def assert_exact_text(self, text, selector=\"html\", by=By.CSS_SELECTOR,\n                          timeout=settings.SMALL_TIMEOUT):\n        \"\"\" Similar to assert_text(), but the text must be exact, rather than\n            exist as a subset of the full text.\n            (Extra whitespace at the beginning or the end doesn't count.)\n            Raises an exception if the element or the text is not found.\n            Returns True if successful. Default timeout = SMALL_TIMEOUT. \"\"\"\n        if self.timeout_multiplier and timeout == settings.SMALL_TIMEOUT:\n            timeout = self.__get_new_timeout(timeout)\n        self.wait_for_exact_text_visible(\n            text, selector, by=by, timeout=timeout)\n\n        if self.demo_mode:\n            if page_utils.is_xpath_selector(selector):\n                by = By.XPATH\n            if page_utils.is_link_text_selector(selector):\n                selector = page_utils.get_link_text_from_selector(selector)\n                by = By.LINK_TEXT\n            messenger_post = (\"ASSERT TEXT {%s} in %s: %s\"\n                              % (text, by, selector))\n            self.__highlight_with_assert_success(messenger_post, selector, by)\n        return True",
        "rewrite": " \"def assert_exact_text(self, text, selector=\"html\", by=By.CSS_SELECTOR,\n                          timeout=settings.SMALL_TIMEOUT):\n        \"\"\" Similar to assert_text(), but the text must be exact, rather than\n            exist as a subset of the full text.\n            (Extra whitespace at the beginning or the end doesn't count.)\n            Raises an exception if the element or the text is not found.\n            Returns True if successful. Default timeout = SMALL_TIMEOUT. \"\"\"\n        if self.timeout_multiplier and timeout == settings.SMALL_TIMEOUT:\n            timeout = self.__get_new_timeout(timeout)\n        self.wait_for_exact_text_visible(\n            text,"
    },
    {
        "original": "def set_font_properties(style_less,\n                        nbfont=None,\n                        tcfont=None,\n                        monofont=None,\n                        monosize=11,\n                        tcfontsize=13,\n                        nbfontsize=13,\n                        prfontsize=95,\n                        dffontsize=93,\n                        outfontsize=85,\n                        mathfontsize=100,\n                        dfonts=False):\n    \"\"\"Parent function for setting notebook, text/md, and\n    codecell font-properties\n    \"\"\"\n\n    fontsizes = [monosize, nbfontsize, tcfontsize, prfontsize, dffontsize, outfontsize]\n    monosize, nbfontsize, tcfontsize, prfontsize, dffontsize, outfontsize = convert_fontsizes(fontsizes)\n    if dfonts==True:\n        monofont, tcfont, nbfont = ['monospace', 'sans-serif', 'sans-serif']\n    else:\n        if monofont is not None:\n            monofont, monofpath = stored_font_dicts(monofont)\n            style_less = import_fonts(style_less, monofont, monofpath)\n        else:\n            monofont='monospace'\n        if tcfont is not None:\n            tcfont, tcfontpath = stored_font_dicts(tcfont)\n            style_less = import_fonts(style_less, tcfont, tcfontpath)\n        else:\n            tcfont='sans-serif'\n        if nbfont is not None:\n            if nbfont == 'proxima':\n                nbfont, tcfont = [\"'Proxima Nova'\"]*2\n                style_less = proxima_nova_imports(style_less)\n            else:\n                nbfont, nbfontpath = stored_font_dicts(nbfont)\n                style_less = import_fonts(style_less, nbfont, nbfontpath)\n        else:\n            nbfont='sans-serif'\n\n    style_less += '/* Set Font-Type and Font-Size Variables  */\\n'\n    # font names and fontfamily info for codecells, notebook & textcells\n    style_less += '@monofont: {}; \\n'.format(monofont)\n    style_less += '@notebook-fontfamily: {}; \\n'.format(nbfont)\n    style_less += '@text-cell-fontfamily: {}; \\n'.format(tcfont)\n    # font size for codecells, main notebook, notebook-sub, & textcells\n    style_less += '@monofontsize: {}pt; \\n'.format(monosize)\n    style_less += '@monofontsize-sub: {}pt; \\n'.format(float(monosize) - 1)\n    style_less += '@nb-fontsize: {}pt; \\n'.format(nbfontsize)\n    style_less += '@nb-fontsize-sub: {}pt; \\n'.format(float(nbfontsize) - 1)\n    style_less += '@text-cell-fontsize: {}pt; \\n'.format(tcfontsize)\n    style_less += '@df-header-fontsize: {}pt; \\n'.format(float(dffontsize) + 1)\n    style_less += '@df-fontsize: {}pt; \\n'.format(dffontsize)\n    style_less += '@output-font-size: {}pt; \\n'.format(outfontsize)\n    style_less += '@prompt-fontsize: {}pt; \\n'.format(prfontsize)\n    style_less += '@mathfontsize: {}%; \\n'.format(mathfontsize)\n    style_less += '\\n\\n'\n    style_less += '/* Import Theme Colors and Define Layout Variables */\\n'\n    return style_less",
        "rewrite": "    style_less += '@notebook-fontsize: {}pt; \\n'.format(nbfontsize)\n    style_less += '@text-cell-fontsize: {}pt; \\n'.format(tcfontsize)\n    # font size for printout, dataframe, and output\n    style_less += '@printout-fontsize: {}pt; \\n'.format(prfontsize)\n    style_less += '@dataframe-fontsize: {}pt; \\n'.format(dffontsize)\n    style_less += '@output-fontsize: {}pt; \\n'.format(outfontsize)\n    # font size for mathjax\n    style_less += '@math"
    },
    {
        "original": "def set(self, id, value):\n        \"\"\"\n        \u6839\u636e id \u5199\u5165\u6570\u636e\u3002\n\n        :param id: \u8981\u5199\u5165\u7684 id\n        :param value: \u8981\u5199\u5165\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a ``dict`` \u5bf9\u8c61\n                \"\"\"\n        session = json_dumps(value)\n        self.collection.replace_one(\n            {\n                \"wechat_id\": id\n            }, {\n                \"wechat_id\": id,\n                \"session\": session\n            },\n            upsert=True\n        )",
        "rewrite": " \"def get(self, id):\n        \"\"\"\n        \u6839\u636e id \u8bfb\u53d6\u6570\u636e\u3002\n\n        :param id: \u8981\u8bfb\u53d6\u7684 id\n        :return: \u8bfb\u53d6\u5230\u7684\u6570\u636e\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a ``dict`` \u5bf9\u8c61\n        \"\"\"\n        result = self.collection.find_one(\n            {\n                \"wechat_id\": id\n            }\n        )\n        if result is None:\n            return None\n        return json_loads(result[\"session\"])\"\"def delete(self, id):\n        \"\"\"\n        \u6839\u636e id \u5220\u9664\u6570\u636e\u3002\n\n        :param id: \u8981\u5220\u9664\u7684"
    }
]
