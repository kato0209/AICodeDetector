[
    {
        "original": "def add_run(self, text=None, style=None):\n        \"\"\"\n        Append a run to this paragraph containing *text* and having character\n        style identified by style ID *style*. *text* can contain tab\n        (``\\\\t``) characters, which are converted to the appropriate XML form\n        for a tab. *text* can also include newline (``\\\\n``) or carriage\n        return (``\\\\r``) characters, each of which is converted to a line\n        break.\n        \"\"\"\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run",
        "rewrite": "def add_run(self, text=None, style=None):\n        r = self._p.add_r()\n        run = Run(r, self)\n        if text:\n            run.text = text\n        if style:\n            run.style = style\n        return run"
    },
    {
        "original": "def _read_execute_info(path, parents):\n  \"\"\"Read the ExecuteInfo.txt file and return the base directory.\"\"\"\n  path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n  if os.path.exists(path):\n    with open(path, \"rb\") as f:  # Binary because the game appends a '\\0' :(.\n      for line in f:\n        parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n        if len(parts) == 2 and parts[0] == \"executable\":\n          exec_path = parts[1].replace(\"\\\\\", \"/\")  # For windows compatibility.\n          for _ in range(parents):\n            exec_path = os.path.dirname(exec_path)\n          return exec_path",
        "rewrite": "import os\n\ndef _read_execute_info(path, parents):\n    path = os.path.join(path, \"StarCraft II/ExecuteInfo.txt\")\n    if os.path.exists(path):\n        with open(path, \"rb\") as f:  \n            for line in f:\n                parts = [p.strip() for p in line.decode(\"utf-8\").split(\"=\")]\n                if len(parts) == 2 and parts[0] == \"executable\":\n                    exec_path = parts[1].replace(\"\\\\\", \"/\")  \n                    for _ in range(parents):\n                        exec_path = os.path.dirname(exec_path)\n                    return exec_path"
    },
    {
        "original": "def _modify_eni_properties(eni_id, properties=None, vm_=None):\n    \"\"\"\n    Change properties of the interface\n    with id eni_id to the values in properties dict\n    \"\"\"\n    if not isinstance(properties, dict):\n        raise SaltCloudException(\n            'ENI properties must be a dictionary'\n        )\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute',\n              'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params[k] = v\n\n    result = aws.query(params,\n                       return_root=True,\n                       location=get_location(vm_),\n                       provider=get_provider(),\n                       opts=__opts__,\n                       sigver='4')\n\n    if isinstance(result, dict) and result.get('error'):\n        raise SaltCloudException(\n            'Could not change interface <{0}> attributes <\\'{1}\\'>'.format(\n                eni_id, properties\n            )\n        )\n    else:\n        return result",
        "rewrite": "def _modify_eni_properties(eni_id, properties=None, vm_=None):\n    if not isinstance(properties, dict):\n        raise SaltCloudException('ENI properties must be a dictionary')\n\n    params = {'Action': 'ModifyNetworkInterfaceAttribute', 'NetworkInterfaceId': eni_id}\n    for k, v in six.iteritems(properties):\n        params[k] = v\n\n    result = aws.query(params, return_root=True, location=get_location(vm_), provider=get_provider(), opts=__opts__, sigver='4')\n\n    if isinstance(result, dict) and result.get('error'):\n        raise SaltCloudException('Could not change interface <{0}> attributes <\\'{1}\\'>'.format(eni_id, properties))\n    else:\n        return result"
    },
    {
        "original": "def data(self):\n        \"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\n        if self.sort:\n            xs = sorted(self.dataset, key=self.sort_key)\n        elif self.shuffle:\n            xs = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n        else:\n            xs = self.dataset\n        return xs",
        "rewrite": "def data(self):\n    if self.sort:\n        examples = sorted(self.dataset, key=self.sort_key)\n    elif self.shuffle:\n        examples = [self.dataset[i] for i in self.random_shuffler(range(len(self.dataset)))]\n    else:\n        examples = self.dataset\n    return examples"
    },
    {
        "original": "def Start(self):\n    \"\"\"This uploads the rules to the foreman and, thus, starts the hunt.\"\"\"\n    # We are already running.\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n      return\n\n    # Determine when this hunt will expire.\n    self.context.duration = self.runner_args.expiry_time\n\n    # When the next client can be scheduled. Implements gradual client\n    # recruitment rate according to the client_rate.\n    self.context.next_client_due = rdfvalue.RDFDatetime.Now()\n\n    self._CreateAuditEvent(\"HUNT_STARTED\")\n\n    # Start the hunt.\n    self.hunt_obj.Set(self.hunt_obj.Schema.STATE(\"STARTED\"))\n    self.hunt_obj.Flush()\n\n    if self.runner_args.add_foreman_rules:\n      self._AddForemanRule()",
        "rewrite": "def Start(self):\n    if self.hunt_obj.Get(self.hunt_obj.Schema.STATE) == \"STARTED\":\n        return\n\n    self.context.duration = self.runner_args.expiry_time\n    self.context.next_client_due = rdfvalue.RDFDatetime.Now()\n\n    self._CreateAuditEvent(\"HUNT_STARTED\")\n\n    self.hunt_obj.Set(self.hunt_obj.Schema.STATE(\"STARTED\"))\n    self.hunt_obj.Flush()\n\n    if self.runner_args.add_foreman_rules:\n        self._AddForemanRule()"
    },
    {
        "original": "def _partition_spec(self, shape, partition_info):\n    \"\"\"Build magic (and sparsely documented) shapes_and_slices spec string.\"\"\"\n    if partition_info is None:\n      return ''  # Empty string indicates a non-partitioned tensor.\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec",
        "rewrite": "def _partition_spec(self, shape, partition_info):\n    if partition_info is None:\n        return ''\n    ssi = tf.Variable.SaveSliceInfo(\n        full_name=self._var_name,\n        full_shape=partition_info.full_shape,\n        var_offset=partition_info.var_offset,\n        var_shape=shape)\n    return ssi.spec"
    },
    {
        "original": "def connection_id_to_endpoint(self, connection_id):\n        \"\"\"\n        Get stored public key for a connection.\n        \"\"\"\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.uri\n            except KeyError:\n                return None",
        "rewrite": "def connection_id_to_endpoint(self, connection_id):\n    with self._connections_lock:\n        try:\n            connection_info = self._connections.get(connection_id)\n            if connection_info:\n                return connection_info.uri\n            else:\n                return None\n        except KeyError:\n            return None"
    },
    {
        "original": "def _ValidateAFF4Type(aff4_type):\n  \"\"\"Validates an AFF4 type.\"\"\"\n  if aff4_type is None:\n    return\n\n  if not isinstance(aff4_type, type):\n    raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n  if not issubclass(aff4_type, AFF4Object):\n    raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" %\n                    aff4_type)",
        "rewrite": "def validate_aff4_type(aff4_type):\n    if aff4_type is None:\n        return\n\n    if not isinstance(aff4_type, type):\n        raise TypeError(\"aff4_type=%s must be a type\" % aff4_type)\n    \n    if not issubclass(aff4_type, AFF4Object):\n        raise TypeError(\"aff4_type=%s must be a subclass of AFF4Object.\" % aff4_type)"
    },
    {
        "original": "def KernelVersion():\n  \"\"\"Gets the kernel version as string, eg. \"5.1.2600\".\n\n  Returns:\n    The kernel version, or \"unknown\" in the case of failure.\n  \"\"\"\n  rtl_osversioninfoexw = RtlOSVersionInfoExw()\n  try:\n    RtlGetVersion(rtl_osversioninfoexw)\n  except OSError:\n    return \"unknown\"\n\n  return \"%d.%d.%d\" % (rtl_osversioninfoexw.dwMajorVersion,\n                       rtl_osversioninfoexw.dwMinorVersion,\n                       rtl_osversioninfoexw.dwBuildNumber)",
        "rewrite": "def KernelVersion():\n    rtl_osversioninfoexw = RtlOSVersionInfoExw()\n    try:\n        RtlGetVersion(rtl_osversioninfoexw)\n    except OSError:\n        return \"unknown\"\n    \n    return \"{}.{}.{}\".format(rtl_osversioninfoexw.dwMajorVersion,\n                             rtl_osversioninfoexw.dwMinorVersion,\n                             rtl_osversioninfoexw.dwBuildNumber)"
    },
    {
        "original": "def guid_to_squid(guid):\n    \"\"\"\n    Converts a GUID   to a compressed guid (SQUID)\n\n    Each Guid has 5 parts separated by '-'. For the first three each one will be\n    totally reversed, and for the remaining two each one will be reversed by\n    every other character. Then the final compressed Guid will be constructed by\n    concatenating all the reversed parts without '-'.\n\n    .. Example::\n\n        Input:                  2BE0FA87-5B36-43CF-95C8-C68D6673FB94\n        Reversed:               78AF0EB2-63B5-FC34-598C-6CD86637BF49\n        Final Compressed Guid:  78AF0EB263B5FC34598C6CD86637BF49\n\n    Args:\n\n        guid (str): A valid GUID\n\n    Returns:\n        str: A valid compressed GUID (SQUID)\n    \"\"\"\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w\\w)(\\w\\w)-(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match is not None:\n        for index in range(1, 12):\n            squid += guid_match.group(index)[::-1]\n    return squid",
        "rewrite": "import re\n\ndef guid_to_squid(guid):\n    guid_pattern = re.compile(r'^\\{(\\w{8})-(\\w{4})-(\\w{4})-(\\w{2})(\\w{2})-(\\w{2})(\\w{2})(\\w{2})(\\w{2})(\\w{2})(\\w{2})\\}$')\n    guid_match = guid_pattern.match(guid)\n    squid = ''\n    if guid_match:\n        for index in range(1, 12):\n            squid += guid_match.group(index)[::-1]\n    return squid"
    },
    {
        "original": "def commit_config(self, message=\"\"):\n        \"\"\"Commit configuration.\"\"\"\n        commit_args = {\"comment\": message} if message else {}\n        self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n        if not self.lock_disable and not self.session_config_lock:\n            self._unlock()",
        "rewrite": "def commit_config(self, message=\"\"):\n    commit_args = {\"comment\": message} if message else {}\n    self.device.cu.commit(ignore_warning=self.ignore_warning, **commit_args)\n    if not self.lock_disable and not self.session_config_lock:\n        self._unlock()"
    },
    {
        "original": "def fetch_metric(self, cursor, results, tags):\n        \"\"\"\n        Because we need to query the metrics by matching pairs, we can't query\n        all of them together without having to perform some matching based on\n        the name afterwards so instead we query instance by instance.\n        We cache the list of instance so that we don't have to look it up every time\n        \"\"\"\n        if self.sql_name not in results:\n            self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n            return\n\n        tags = tags + self.tags\n\n        results_list = results[self.sql_name]\n        done_instances = []\n        for ndx, row in enumerate(results_list):\n            ctype = row[0]\n            cval = row[1]\n            inst = row[2]\n            object_name = row[3]\n\n            if inst in done_instances:\n                continue\n\n            if (self.instance != ALL_INSTANCES and inst != self.instance) or (\n                self.object_name and object_name != self.object_name\n            ):\n                done_instances.append(inst)\n                continue\n\n            # find the next row which has the same instance\n            cval2 = None\n            ctype2 = None\n            for second_row in results_list[: ndx + 1]:\n                if inst == second_row[2]:\n                    cval2 = second_row[1]\n                    ctype2 = second_row[0]\n\n            if cval2 is None:\n                self.log.warning(\"Couldn't find second value for {}\".format(self.sql_name))\n                continue\n            done_instances.append(inst)\n            if ctype < ctype2:\n                value = cval\n                base = cval2\n            else:\n                value = cval2\n                base = cval\n\n            metric_tags = list(tags)\n            if self.instance == ALL_INSTANCES:\n                metric_tags.append('{}:{}'.format(self.tag_by, inst.strip()))\n            self.report_fraction(value, base, metric_tags)",
        "rewrite": "def fetch_metric(self, cursor, results, tags):\n    if self.sql_name not in results:\n        self.log.warning(\"Couldn't find {} in results\".format(self.sql_name))\n        return\n\n    tags = tags + self.tags\n\n    results_list = results[self.sql_name]\n    done_instances = []\n    for ndx, row in enumerate(results_list):\n        ctype = row[0]\n        cval = row[1]\n        inst = row[2]\n        object_name = row[3]\n\n        if inst in done_instances:\n            continue\n\n        if (self.instance != ALL_INSTANCES and inst != self.instance) or (\n            self.object_name and object_name != self.object_name\n        ):\n            done_instances.append(inst)\n            continue\n\n        cval2 = None\n        ctype2 = None\n        for second_row in results_list[: ndx + 1]:\n            if inst == second_row[2]:\n                cval2 = second_row[1]\n                ctype2 = second_row[0]\n\n        if cval2 is None:\n            self.log.warning(\"Couldn't find second value for {}\".format(self.sql_name))\n            continue\n        done_instances.append(inst)\n        if ctype < ctype2:\n            value = cval\n            base = cval2\n        else:\n            value = cval2\n            base = cval\n\n        metric_tags = list(tags)\n        if self.instance == ALL_INSTANCES:\n            metric_tags.append('{}:{}'.format(self.tag_by, inst.strip()))\n        self.report_fraction(value, base, metric_tags)"
    },
    {
        "original": "def apply(key, value):\n    \"\"\"\n    Set a single key\n\n    .. note::\n\n        This will strip comments from your config file\n    \"\"\"\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data, default_flow_style=False)",
        "rewrite": "def apply(key, value):\n    path = __opts__['conf_file']\n    if os.path.isdir(path):\n        path = os.path.join(path, 'master')\n    data = values()\n    data[key] = value\n    with salt.utils.files.fopen(path, 'w+') as fp_:\n        salt.utils.yaml.safe_dump(data, default_flow_style=False)"
    },
    {
        "original": "def refresh_access_token(self, refresh_token):\n        \"\"\"\u5237\u65b0 access token\n\n        :param refresh_token: OAuth2 refresh token\n        :return: JSON \u6570\u636e\u5305\n        \"\"\"\n        res = self._get(\n            'sns/oauth2/refresh_token',\n            params={\n                'appid': self.app_id,\n                'grant_type': 'refresh_token',\n                'refresh_token': refresh_token\n            }\n        )\n        self.access_token = res['access_token']\n        self.open_id = res['openid']\n        self.refresh_token = res['refresh_token']\n        self.expires_in = res['expires_in']\n        return res",
        "rewrite": "def refresh_access_token(self, refresh_token):\n        res = self._get(\n            'sns/oauth2/refresh_token',\n            params={\n                'appid': self.app_id,\n                'grant_type': 'refresh_token',\n                'refresh_token': refresh_token\n            }\n        )\n        self.access_token = res['access_token']\n        self.open_id = res['openid']\n        self.refresh_token = res['refresh_token']\n        self.expires_in = res['expires_in']\n        return res"
    },
    {
        "original": "def is_connection_to_a_vcenter(service_instance):\n    \"\"\"\n    Function that returns True if the connection is made to a vCenter Server and\n    False if the connection is made to an ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('api_type = %s', api_type)\n    if api_type == 'VirtualCenter':\n        return True\n    elif api_type == 'HostAgent':\n        return False\n    else:\n        raise salt.exceptions.VMwareApiError(\n            'Unexpected api type \\'{0}\\' . Supported types: '\n            '\\'VirtualCenter/HostAgent\\''.format(api_type))",
        "rewrite": "def is_connection_to_a_vcenter(service_instance):\n    try:\n        api_type = service_instance.content.about.apiType\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('api_type = %s', api_type)\n    if api_type == 'VirtualCenter':\n        return True\n    elif api_type == 'HostAgent':\n        return False\n    else:\n        raise salt.exceptions.VMwareApiError(\n            'Unexpected api type \\'{0}\\' . Supported types: '\n            '\\'VirtualCenter/HostAgent\\''.format(api_type))"
    },
    {
        "original": "def add_column(self, name, *, index=0, values=None):\n        \"\"\"\n        Adds a column to the table\n        :param str name: the name of the column\n        :param int index: the index at which the column should be added. Defaults to 0.\n        :param list values: a two dimension array of values to add to the column\n        \"\"\"\n        if name is None:\n            return None\n\n        params = {\n            'name': name,\n            'index': index\n        }\n        if values is not None:\n            params['values'] = values\n\n        url = self.build_url(self._endpoints.get('add_column'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})",
        "rewrite": "def add_column(self, name, *, index=0, values=None):\n    params = {\n        'name': name,\n        'index': index\n    }\n    \n    if values is not None:\n        params['values'] = values\n\n    url = self.build_url(self._endpoints.get('add_column'))\n    response = self.session.post(url, data=params)\n    \n    if not response:\n        return None\n\n    data = response.json()\n\n    return self.column_constructor(parent=self, **{self._cloud_data_key: data})"
    },
    {
        "original": "def __get_stat_display(self, stats, layer):\n        \"\"\"Return a dict of dict with all the stats display.\n        stats: Global stats dict\n        layer: ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n        :returns: dict of dict\n            * key: plugin name\n            * value: dict returned by the get_stats_display Plugin method\n        \"\"\"\n        ret = {}\n\n        for p in stats.getPluginsList(enable=False):\n            if p == 'quicklook' or p == 'processlist':\n                # processlist is done later\n                # because we need to know how many processes could be displayed\n                continue\n\n            # Compute the plugin max size\n            plugin_max_width = None\n            if p in self._left_sidebar:\n                plugin_max_width = max(self._left_sidebar_min_width,\n                                       self.screen.getmaxyx()[1] - 105)\n                plugin_max_width = min(self._left_sidebar_max_width,\n                                       plugin_max_width)\n\n            # Get the view\n            ret[p] = stats.get_plugin(p).get_stats_display(args=self.args,\n                                                           max_width=plugin_max_width)\n\n        return ret",
        "rewrite": "def get_stat_display(self, stats, layer):\n    ret = {}\n    \n    for p in stats.getPluginsList(enable=False):\n        if p == 'quicklook' or p == 'processlist':\n            continue\n        \n        plugin_max_width = None\n        if p in self._left_sidebar:\n            plugin_max_width = max(self._left_sidebar_min_width, self.screen.getmaxyx()[1] - 105)\n            plugin_max_width = min(self._left_sidebar_max_width, plugin_max_width)\n        \n        ret[p] = stats.get_plugin(p).get_stats_display(args=self.args, max_width=plugin_max_width)\n    \n    return ret"
    },
    {
        "original": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)",
        "rewrite": "def delete(self, id, **kwargs):\n    if id is None:\n        path = self.path\n    else:\n        if not isinstance(id, int):\n            id = id.replace('/', '%2F')\n        path = '%s/%s' % (self.path, id)\n    self.gitlab.http_delete(path, **kwargs)"
    },
    {
        "original": "def _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call",
        "rewrite": "from functools import wraps\nfrom typing import Callable\n\ndef _requires_login(func: Callable) -> Callable:\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    \n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call"
    },
    {
        "original": "def _build_next_request(self, verb, prior_request, prior_response):\n        \"\"\"Builds pagination-aware request object.\n\n        More details:\n          https://developers.google.com/api-client-library/python/guide/pagination\n\n        Args:\n            verb (str): Request verb (ex. insert, update, delete).\n            prior_request (httplib2.HttpRequest): Request that may trigger\n                paging.\n            prior_response (dict): Potentially partial response.\n\n        Returns:\n            httplib2.HttpRequest: HttpRequest or None. None is returned when\n                there is nothing more to fetch - request completed.\n        \"\"\"\n        method = getattr(self._component, verb + '_next')\n        return method(prior_request, prior_response)",
        "rewrite": "def _build_next_request(self, verb, prior_request, prior_response):\n    method = getattr(self._component, verb + '_next')\n    return method(prior_request, prior_response)"
    },
    {
        "original": "def _gpinv(probs, kappa, sigma):\n    \"\"\"Inverse Generalized Pareto distribution function.\"\"\"\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)  # pylint: disable=invalid-unary-operand-type\n        else:\n            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n        x *= sigma\n    else:\n        if np.abs(kappa) < np.finfo(float).eps:\n            x[ok] = -np.log1p(-probs[ok])  # pylint: disable=unsupported-assignment-operation, E1130\n        else:\n            x[ok] = (  # pylint: disable=unsupported-assignment-operation\n                np.expm1(-kappa * np.log1p(-probs[ok])) / kappa\n            )\n        x *= sigma\n        x[probs == 0] = 0\n        if kappa >= 0:\n            x[probs == 1] = np.inf  # pylint: disable=unsupported-assignment-operation\n        else:\n            x[probs == 1] = -sigma / kappa  # pylint: disable=unsupported-assignment-operation\n\n    return x",
        "rewrite": "import numpy as np\n\ndef inverse_generalized_pareto(probs, kappa, sigma):\n    x = np.full_like(probs, np.nan)\n    if sigma <= 0:\n        return x\n    ok = (probs > 0) & (probs < 1)\n    \n    if np.all(ok):\n        if np.abs(kappa) < np.finfo(float).eps:\n            x = -np.log1p(-probs)\n        else:\n            x = np.expm1(-kappa * np.log1p(-probs)) / kappa\n        x *= sigma\n    else:\n        if np.abs(kappa) < np.finfo(float).eps:\n            x[ok] = -np.log1p(-probs[ok])\n        else:\n            x[ok] = (np.expm1(-kappa * np.log1p(-probs[ok])) / kappa)\n        x *= sigma\n        x[probs == 0] = 0\n        if kappa >= 0:\n            x[probs == 1] = np.inf\n        else:\n            x[probs == 1] = -sigma / kappa\n\n    return x"
    },
    {
        "original": "def _create_container_args(kwargs):\n    \"\"\"\n    Convert arguments to create() to arguments to create_container().\n    \"\"\"\n    # Copy over kwargs which can be copied directly\n    create_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    host_config_kwargs = {}\n    for key in copy.copy(kwargs):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    # Process kwargs which are split over both create and host_config\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    # All kwargs should have been consumed by this point, so raise\n    # error if any are left\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    # Fill in any kwargs which need processing by create_host_config first\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        # sort to make consistent for tests\n        create_kwargs['ports'] = [tuple(p.split('/', 1))\n                                  for p in sorted(port_bindings.keys())]\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [\n                v.get('bind') for v in volumes.values()\n            ]\n        else:\n            create_kwargs['volumes'] = [\n                _host_volume_from_bind(v) for v in volumes\n            ]\n    return create_kwargs",
        "rewrite": "def _create_container_args(kwargs):\n    create_kwargs = {}\n    for key in list(kwargs.keys()):\n        if key in RUN_CREATE_KWARGS:\n            create_kwargs[key] = kwargs.pop(key)\n    \n    host_config_kwargs = {}\n    for key in list(kwargs.keys()):\n        if key in RUN_HOST_CONFIG_KWARGS:\n            host_config_kwargs[key] = kwargs.pop(key)\n\n    ports = kwargs.pop('ports', {})\n    if ports:\n        host_config_kwargs['port_bindings'] = ports\n\n    volumes = kwargs.pop('volumes', {})\n    if volumes:\n        host_config_kwargs['binds'] = volumes\n\n    network = kwargs.pop('network', None)\n    if network:\n        create_kwargs['networking_config'] = {network: None}\n        host_config_kwargs['network_mode'] = network\n\n    if kwargs:\n        raise create_unexpected_kwargs_error('run', kwargs)\n\n    create_kwargs['host_config'] = HostConfig(**host_config_kwargs)\n\n    port_bindings = create_kwargs['host_config'].get('PortBindings')\n    if port_bindings:\n        create_kwargs['ports'] = [tuple(p.split('/', 1)) for p in sorted(port_bindings.keys())]\n\n    if volumes:\n        if isinstance(volumes, dict):\n            create_kwargs['volumes'] = [v.get('bind') for v in volumes.values()]\n        else:\n            create_kwargs['volumes'] = [_host_volume_from_bind(v) for v in volumes]\n\n    return create_kwargs"
    },
    {
        "original": "def _CollectArtifact(self, artifact, apply_parsers):\n    \"\"\"Returns an `CollectedArtifact` rdf object for the requested artifact.\"\"\"\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n      parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n      parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources,\n                                                   parser_factory):\n      for response in source_result_list:\n        action_result = rdf_artifacts.ClientActionResult()\n        action_result.type = response.__class__.__name__\n        action_result.value = response\n        artifact_result.action_results.append(action_result)\n        self.UpdateKnowledgeBase(response, artifact.provides)\n\n    return artifact_result",
        "rewrite": "def _CollectArtifact(self, artifact, apply_parsers):\n    artifact_result = rdf_artifacts.CollectedArtifact(name=artifact.name)\n\n    if apply_parsers:\n        parser_factory = parsers.ArtifactParserFactory(str(artifact.name))\n    else:\n        parser_factory = None\n\n    for source_result_list in self._ProcessSources(artifact.sources, parser_factory):\n        for response in source_result_list:\n            action_result = rdf_artifacts.ClientActionResult()\n            action_result.type = response.__class__.__name__\n            action_result.value = response\n            artifact_result.action_results.append(action_result)\n            self.UpdateKnowledgeBase(response, artifact.provides)\n\n    return artifact_result"
    },
    {
        "original": "def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):\n        \"\"\"\n        Moments match of the marginal approximation in EP algorithm\n\n        :param i: number of observation (int)\n        :param tau_i: precision of the cavity distribution (float)\n        :param v_i: mean/variance of the cavity distribution (float)\n        \"\"\"\n        sigma2_hat = 1./(1./self.variance + tau_i)\n        mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n        sum_var = self.variance + 1./tau_i\n        Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5*(data_i - v_i/tau_i)**2./sum_var)\n        return Z_hat, mu_hat, sigma2_hat",
        "rewrite": "def moments_match_ep(self, data_i, tau_i, v_i, Y_metadata_i=None):        \n        sigma2_hat = 1./(1./self.variance + tau_i)\n        mu_hat = sigma2_hat*(data_i/self.variance + v_i)\n        sum_var = self.variance + 1./tau_i\n        Z_hat = 1./np.sqrt(2.*np.pi*sum_var)*np.exp(-.5*(data_i - v_i/tau_i)**2./sum_var)\n        return Z_hat, mu_hat, sigma2_hat"
    },
    {
        "original": "def validate_ports_string(ports):\n        \"\"\" Validate that provided string has proper port numbers:\n            1. port number < 65535\n            2. range start < range end\n        \"\"\"\n        pattern = re.compile('^\\\\d+(-\\\\d+)?(,\\\\d+(-\\\\d+)?)*$')\n        if pattern.match(ports) is None:\n            return False\n\n        ranges = PortsRangeHelper._get_string_port_ranges(ports)\n        for r in ranges:\n            if r.start > r.end or r.start > 65535 or r.end > 65535:\n                return False\n        return True",
        "rewrite": "import re\n\ndef validate_ports_string(ports):\n    pattern = re.compile(r'^\\d+(-\\d+)?(,\\d+(-\\d+)?)*$')\n    if pattern.match(ports) is None:\n        return False\n\n    ranges = PortsRangeHelper._get_string_port_ranges(ports)\n    for r in ranges:\n        if r.start > r.end or r.start > 65535 or r.end > 65535:\n            return False\n    return True"
    },
    {
        "original": "def strongest_match(cls, overlay, mode, backend=None):\n        \"\"\"\n        Returns the single strongest matching compositor operation\n        given an overlay. If no matches are found, None is returned.\n\n        The best match is defined as the compositor operation with the\n        highest match value as returned by the match_level method.\n        \"\"\"\n        match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                          if op.mode == mode and (not op.backends or backend in op.backends)]\n        matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n        if matches == []: return None\n        else:             return sorted(matches)[0]",
        "rewrite": "def strongest_match(cls, overlay, mode, backend=None):\n    match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                      if op.mode == mode and (not op.backends or backend in op.backends)]\n    matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n    if matches == []:\n        return None\n    else:\n        return sorted(matches)[0]"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'label') and self.label is not None:\n        _dict['label'] = self.label\n    if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n        _dict['provenance_ids'] = self.provenance_ids\n    return _dict"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)",
        "rewrite": "def _post_master_init(self, master):\n    mp_call = _metaproxy_call(self.opts, 'post_master_init')\n    return mp_call(self, master)"
    },
    {
        "original": "def get_occupation(self, atom_index, orbital):\n        \"\"\"\n        Returns the occupation for a particular orbital of a particular atom.\n\n        Args:\n            atom_num (int): Index of atom in the PROCAR. It should be noted\n                that VASP uses 1-based indexing for atoms, but this is\n                converted to 0-based indexing in this parser to be\n                consistent with representation of structures in pymatgen.\n            orbital (str): An orbital. If it is a single character, e.g., s,\n                p, d or f, the sum of all s-type, p-type, d-type or f-type\n                orbitals occupations are returned respectively. If it is a\n                specific orbital, e.g., px, dxy, etc., only the occupation\n                of that orbital is returned.\n\n        Returns:\n            Sum occupation of orbital of atom.\n        \"\"\"\n\n        orbital_index = self.orbitals.index(orbital)\n        return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None])\n                for spin, d in self.data.items()}",
        "rewrite": "def get_occupation(self, atom_index, orbital):\n    orbital_index = self.orbitals.index(orbital)\n    return {spin: np.sum(d[:, :, atom_index, orbital_index] * self.weights[:, None])\n            for spin, d in self.data.items()}"
    },
    {
        "original": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False",
        "rewrite": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n    from ..exceptions import RequestedRangeNotSatisfiable\n    \n    if accept_ranges is None:\n        return False\n    self.headers[\"Accept-Ranges\"] = accept_ranges\n    if not self._is_range_request_processable(environ) or complete_length is None:\n        return False\n    parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n    if parsed_range is None:\n        raise RequestedRangeNotSatisfiable(complete_length)\n    range_tuple = parsed_range.range_for_length(complete_length)\n    content_range_header = parsed_range.to_content_range_header(complete_length)\n    if range_tuple is None or content_range_header is None:\n        raise RequestedRangeNotSatisfiable(complete_length)\n    content_length = range_tuple[1] - range_tuple[0]\n    \n    if content_length != complete_length:\n        self.headers[\"Content-Length\"] = content_length\n        self.content_range = content_range_header\n        self.status_code = 206\n        self._wrap_response(range_tuple[0], content_length)\n        return True\n    return False"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n            _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'dialog_nodes') and self.dialog_nodes is not None:\n        _dict['dialog_nodes'] = [x._to_dict() for x in self.dialog_nodes]\n    if hasattr(self, 'pagination') and self.pagination is not None:\n        _dict['pagination'] = self.pagination._to_dict()\n    return _dict"
    },
    {
        "original": "def on_consumer_cancelled(self, method_frame):\n        \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n        receiving messages.\n\n        :param pika.frame.Method method_frame: The Basic.Cancel frame\n        \"\"\"\n        _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n        if self._channel:\n            self._channel.close()",
        "rewrite": "def on_consumer_cancelled(self, method_frame):\n    _logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n    if self._channel:\n        self._channel.close()"
    },
    {
        "original": "def record(self, timestamp, *args, **kwargs):\n        \"\"\" add custom data to data store \"\"\"\n        if self.output_file is None:\n            return\n\n        data = {'datetime': timestamp}\n\n        # append all data\n        if len(args) == 1:\n            if isinstance(args[0], dict):\n                data.update(dict(args[0]))\n            elif isinstance(args[0], pd.DataFrame):\n                data.update(args[0][-1:].to_dict(orient='records')[0])\n\n        # add kwargs\n        if kwargs:\n            data.update(dict(kwargs))\n\n        data['datetime'] = timestamp\n        # self.rows.append(pd.DataFrame(data=data, index=[timestamp]))\n\n        new_data = {}\n        if \"symbol\" not in data.keys():\n            new_data = dict(data)\n        else:\n            sym = data[\"symbol\"]\n            new_data[\"symbol\"] = data[\"symbol\"]\n            for key in data.keys():\n                if key not in ['datetime', 'symbol_group', 'asset_class']:\n                    new_data[sym + '_' + str(key).upper()] = data[key]\n\n        new_data['datetime'] = timestamp\n\n        # append to rows\n        self.rows.append(pd.DataFrame(data=new_data, index=[timestamp]))\n\n        # create dataframe\n        recorded = pd.concat(self.rows, sort=True)\n\n        if \"symbol\" not in recorded.columns:\n            return\n\n\n        # group by symbol\n        recorded['datetime'] = recorded.index\n        data = recorded.groupby(['symbol', 'datetime'], as_index=False).sum()\n        data.set_index('datetime', inplace=True)\n\n        symbols = data['symbol'].unique().tolist()\n        data.drop(columns=['symbol'], inplace=True)\n\n\n        # cleanup:\n\n        # remove symbols\n        recorded.drop(['symbol'] + [sym + '_SYMBOL' for sym in symbols],\n                      axis=1, inplace=True)\n\n        # remove non-option data if not working with options\n        for sym in symbols:\n            try:\n                opt_cols = recorded.columns[\n                    recorded.columns.str.startswith(sym + '_OPT_')].tolist()\n                if len(opt_cols) == len(recorded[opt_cols].isnull().all()):\n                    recorded.drop(opt_cols, axis=1, inplace=True)\n            except Exception as e:\n                pass\n\n        # group df\n        recorded = recorded.groupby(recorded['datetime']).first()\n\n        # shift position\n        for sym in symbols:\n            recorded[sym + '_POSITION'] = recorded[sym + '_POSITION'\n                                                   ].shift(1).fillna(0)\n\n        # make this public\n        self.recorded = recorded.copy()\n\n        # cleanup columns names before saving...\n        recorded.columns = [col.replace('_FUT_', '_').replace(\n                            '_OPT_OPT_', '_OPT_') for col in recorded.columns]\n\n        # save\n        if \".csv\" in self.output_file:\n            recorded.to_csv(self.output_file)\n        elif \".h5\" in self.output_file:\n            recorded.to_hdf(self.output_file, 0)\n        elif (\".pickle\" in self.output_file) | (\".pkl\" in self.output_file):\n            recorded.to_pickle(self.output_file)\n\n        chmod(self.output_file)",
        "rewrite": "def record(self, timestamp, *args, **kwargs):\n    if self.output_file is None:\n        return\n\n    data = {'datetime': timestamp}\n\n    if len(args) == 1:\n        if isinstance(args[0], dict):\n            data.update(dict(args[0]))\n        elif isinstance(args[0], pd.DataFrame):\n            data.update(args[0][-1:].to_dict(orient='records')[0])\n\n    if kwargs:\n        data.update(dict(kwargs))\n\n    data['datetime'] = timestamp\n\n    new_data = {}\n    if \"symbol\" not in data.keys():\n        new_data = dict(data)\n    else:\n        sym = data[\"symbol\"]\n        new_data[\"symbol\"] = data[\"symbol\"]\n        for key in data.keys():\n            if key not in ['datetime', 'symbol_group', 'asset_class']:\n                new_data[sym + '_' + str(key).upper()] = data[key]\n\n    new_data['datetime'] = timestamp\n\n    self.rows.append(pd.DataFrame(data=new_data, index=[timestamp]))\n\n    recorded = pd.concat(self.rows, sort=True)\n\n    if \"symbol\" not in recorded.columns:\n        return\n\n    recorded['datetime'] = recorded.index\n    data = recorded.groupby(['symbol', 'datetime'], as_index=False).sum()\n    data.set_index('datetime', inplace=True)\n\n    symbols = data['symbol'].unique().tolist()\n    data.drop(columns=['symbol'], inplace=True)\n\n    recorded.drop(['symbol'] + [sym + '_SYMBOL' for sym in symbols], axis=1, inplace=True)\n\n    for sym in symbols:\n        try:\n            opt_cols = recorded.columns[recorded.columns.str.startswith(sym + '_OPT_')].tolist()\n            if len(opt_cols) == len(recorded[opt_cols].isnull().all()):\n                recorded.drop(opt_cols, axis=1, inplace=True)\n        except Exception as e:\n            pass\n\n    recorded = recorded.groupby(recorded['datetime']).first()\n\n    for sym in symbols:\n        recorded[sym + '_POSITION'] = recorded[sym + '_POSITION'].shift(1).fillna(0)\n\n    self.recorded = recorded.copy()\n\n    recorded.columns = [col.replace('_FUT_', '_').replace('_OPT_OPT_', '_OPT_') for col in recorded.columns]\n\n    if \".csv\" in self.output_file:\n        recorded.to_csv(self.output_file)\n    elif \".h5\" in self.output_file:\n        recorded.to_hdf(self.output_file, 'data', mode='w')\n    elif (\".pickle\" in self.output_file) | (\".pkl\" in self.output_file):\n        recorded.to_pickle(self.output_file)\n\n    chmod(self.output_file)"
    },
    {
        "original": "def dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n        \"\"\"\n        derivative of logpdf wrt link_f param\n        .. math::\n\n        :param link_f: latent variables link(f)\n        :type link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: likelihood evaluated for this point\n        :rtype: float\n        \"\"\"\n        assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        val = np.log(y) - link_f\n        val_scaled = val/np.sqrt(self.variance)\n        val_scaled2 = val/self.variance\n        uncensored = (1-c)*(val_scaled2)\n        a = (1- stats.norm.cdf(val_scaled))\n        # llg(z) = 1. / (1 - norm_cdf(r / sqrt(s2))). * (1 / sqrt(2 * pi * s2). * exp(-1 / (2. * s2). * r. ^ 2));\n        censored = c*( 1./a) * (np.exp(-1.* val**2 /(2*self.variance)) / np.sqrt(2*np.pi*self.variance))\n        # censored = c * (1. / (1 - stats.norm.cdf(val_scaled))) * (stats.norm.pdf(val_scaled))\n        gradient = uncensored + censored\n        return gradient",
        "rewrite": "def dlogpdf_dlink(self, link_f, y, Y_metadata=None):\n    assert np.atleast_1d(link_f).shape == np.atleast_1d(y).shape\n    c = np.zeros_like(y)\n    if Y_metadata is not None and 'censored' in Y_metadata.keys():\n        c = Y_metadata['censored']\n\n    val = np.log(y) - link_f\n    val_scaled = val / np.sqrt(self.variance)\n    val_scaled2 = val / self.variance\n    uncensored = (1 - c) * (val_scaled2)\n    a = (1 - stats.norm.cdf(val_scaled))\n\n    censored = c * (1. / a) * (np.exp(-1. * (val**2) / (2 * self.variance)) / np.sqrt(2 * np.pi * self.variance))\n\n    gradient = uncensored + censored\n    return gradient"
    },
    {
        "original": "def _compare_by_version(path1, path2):\n    \"\"\"Returns the current/latest learned path.\n\n    Checks if given paths are from same source/peer and then compares their\n    version number to determine which path is received later. If paths are from\n    different source/peer return None.\n    \"\"\"\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None",
        "rewrite": "def _compare_by_version(path1, path2):\n    if path1.source == path2.source:\n        return path1 if path1.source_version_num > path2.source_version_num else path2\n    return None"
    },
    {
        "original": "def from_parts(cls, parts):\n        \"\"\"\n        Return content types XML mapping each part in *parts* to the\n        appropriate content type and suitable for storage as\n        ``[Content_Types].xml`` in an OPC package.\n        \"\"\"\n        cti = cls()\n        cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n        cti._defaults['xml'] = CT.XML\n        for part in parts:\n            cti._add_content_type(part.partname, part.content_type)\n        return cti",
        "rewrite": "def from_parts(cls, parts):\n    cti = cls()\n    cti._defaults['rels'] = CT.OPC_RELATIONSHIPS\n    cti._defaults['xml'] = CT.XML\n    for part in parts:\n        cti._add_content_type(part.partname, part.content_type)\n    return cti"
    },
    {
        "original": "def ConfigureDatastore(config):\n  \"\"\"Guides the user through configuration of the datastore.\"\"\"\n  print(\"\\n\\n-=GRR Datastore=-\\n\"\n        \"For GRR to work each GRR server has to be able to communicate with\\n\"\n        \"the datastore. To do this we need to configure a datastore.\\n\")\n\n  existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n  if not existing_datastore or existing_datastore == \"FakeDataStore\":\n    ConfigureMySQLDatastore(config)\n    return\n\n  print(\"Found existing settings:\\n  Datastore: %s\" % existing_datastore)\n  if existing_datastore == \"SqliteDataStore\":\n    set_up_mysql = RetryBoolQuestion(\n        \"The SQLite datastore is no longer supported. Would you like to\\n\"\n        \"set up a MySQL datastore? Answering 'no' will abort config \"\n        \"initialization.\", True)\n    if set_up_mysql:\n      print(\"\\nPlease note that no data will be migrated from SQLite to \"\n            \"MySQL.\\n\")\n      ConfigureMySQLDatastore(config)\n    else:\n      raise ConfigInitError()\n  elif existing_datastore == \"MySQLAdvancedDataStore\":\n    print(\"  MySQL Host: %s\\n  MySQL Port: %s\\n  MySQL Database: %s\\n\"\n          \"  MySQL Username: %s\\n\" %\n          (grr_config.CONFIG.Get(\"Mysql.host\"),\n           grr_config.CONFIG.Get(\"Mysql.port\"),\n           grr_config.CONFIG.Get(\"Mysql.database_name\"),\n           grr_config.CONFIG.Get(\"Mysql.database_username\")))\n    if grr_config.CONFIG.Get(\"Mysql.client_key_path\"):\n      print(\"  MySQL client key file: %s\\n\"\n            \"  MySQL client cert file: %s\\n\"\n            \"  MySQL ca cert file: %s\\n\" %\n            (grr_config.CONFIG.Get(\"Mysql.client_key_path\"),\n             grr_config.CONFIG.Get(\"Mysql.client_cert_path\"),\n             grr_config.CONFIG.Get(\"Mysql.ca_cert_path\")))\n\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureMySQLDatastore(config)",
        "rewrite": "def ConfigureDatastore(config):\n    print(\"\\n\\n-=GRR Datastore=-\\n\"\n          \"For GRR to work, each GRR server must be able to communicate with\\n\"\n          \"the datastore. To do this, we need to configure a datastore.\\n\")\n\n    existing_datastore = grr_config.CONFIG.Get(\"Datastore.implementation\")\n\n    if not existing_datastore or existing_datastore == \"FakeDataStore\":\n        ConfigureMySQLDatastore(config)\n        return\n\n    print(\"Found existing settings:\\n  Datastore: %s\" % existing_datastore)\n    \n    if existing_datastore == \"SqliteDataStore\":\n        set_up_mysql = RetryBoolQuestion(\n            \"The SQLite datastore is no longer supported. Would you like to\\n\"\n            \"set up a MySQL datastore? Answering 'no' will abort configuration \"\n            \"initialization.\", True)\n        if set_up_mysql:\n            print(\"\\nPlease note that no data will be migrated from SQLite to \"\n                  \"MySQL.\\n\")\n            ConfigureMySQLDatastore(config)\n        else:\n            raise ConfigInitError()\n            \n    elif existing_datastore == \"MySQLAdvancedDataStore\":\n        print(\"  MySQL Host: %s\\n  MySQL Port: %s\\n  MySQL Database: %s\\n\"\n              \"  MySQL Username: %s\\n\" %\n              (grr_config.CONFIG.Get(\"Mysql.host\"),\n               grr_config.CONFIG.Get(\"Mysql.port\"),\n               grr_config.CONFIG.Get(\"Mysql.database_name\"),\n               grr_config.CONFIG.Get(\"Mysql.database_username\")))\n        \n        if grr_config.CONFIG.Get(\"Mysql.client_key_path\"):\n            print(\"  MySQL client key file: %s\\n\"\n                  \"  MySQL client cert file: %s\\n\"\n                  \"  MySQL ca cert file: %s\\n\" %\n                  (grr_config.CONFIG.Get(\"Mysql.client_key_path\"),\n                   grr_config.CONFIG.Get(\"Mysql.client_cert_path\"),\n                   grr_config.CONFIG.Get(\"Mysql.ca_cert_path\")))\n\n        if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n            ConfigureMySQLDatastore(config)"
    },
    {
        "original": "def _get_century_code(year):\n        \"\"\"Returns the century code for a given year\"\"\"\n        if 2000 <= year < 3000:\n            separator = 'A'\n        elif 1900 <= year < 2000:\n            separator = '-'\n        elif 1800 <= year < 1900:\n            separator = '+'\n        else:\n            raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n        return separator",
        "rewrite": "def _get_century_code(year):\n        if 2000 <= year < 3000:\n            separator = 'A'\n        elif 1900 <= year < 2000:\n            separator = '-'\n        elif 1800 <= year < 1900:\n            separator = '+'\n        else:\n            raise ValueError('Finnish SSN do not support people born before the year 1800 or after the year 2999')\n        return separator"
    },
    {
        "original": "def bulk_build(jail, pkg_file, keep=False):\n    \"\"\"\n    Run bulk build on poudriere server.\n\n    Return number of pkg builds, failures, and errors, on error dump to CLI\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -N buildbox_group poudriere.bulk_build 90amd64 /root/pkg_list\n\n    \"\"\"\n    # make sure `pkg file` and jail is on file system\n    if not os.path.isfile(pkg_file):\n        return 'Could not find file {0} on filesystem'.format(pkg_file)\n    if not is_jail(jail):\n        return 'Could not find jail {0}'.format(jail)\n\n    # Generate command\n    if keep:\n        cmd = 'poudriere bulk -k -f {0} -j {1}'.format(pkg_file, jail)\n    else:\n        cmd = 'poudriere bulk -f {0} -j {1}'.format(pkg_file, jail)\n\n    # Bulk build this can take some time, depending on pkg_file ... hours\n    res = __salt__['cmd.run'](cmd)\n    lines = res.splitlines()\n    for line in lines:\n        if \"packages built\" in line:\n            return line\n    return ('There may have been an issue building packages dumping output: '\n            '{0}').format(res)",
        "rewrite": "import os\n\ndef bulk_build(jail, pkg_file, keep=False):\n    if not os.path.isfile(pkg_file):\n        return 'Could not find file {0} on filesystem'.format(pkg_file)\n    if not is_jail(jail):\n        return 'Could not find jail {0}'.format(jail)\n\n    if keep:\n        cmd = 'poudriere bulk -k -f {0} -j {1}'.format(pkg_file, jail)\n    else:\n        cmd = 'poudriere bulk -f {0} -j {1}'.format(pkg_file, jail)\n\n    res = __salt__['cmd.run'](cmd)\n    lines = res.splitlines()\n    for line in lines:\n        if \"packages built\" in line:\n            return line\n    return 'There may have been an issue building packages dumping output: {0}'.format(res)"
    },
    {
        "original": "def copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error(\n            'The copy_snapshot function must be called with -f or --function.'\n        )\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A source_snapshot_id must be specified to copy a snapshot.')\n        return False\n\n    if 'description' not in kwargs:\n        kwargs['description'] = ''\n\n    params = {'Action': 'CopySnapshot'}\n\n    if 'source_region' in kwargs:\n        params['SourceRegion'] = kwargs['source_region']\n\n    if 'source_snapshot_id' in kwargs:\n        params['SourceSnapshotId'] = kwargs['source_snapshot_id']\n\n    if 'description' in kwargs:\n        params['Description'] = kwargs['description']\n\n    log.debug(params)\n\n    data = aws.query(params,\n                     return_url=True,\n                     location=get_location(),\n                     provider=get_provider(),\n                     opts=__opts__,\n                     sigver='4')\n    return data",
        "rewrite": "def copy_snapshot(kwargs=None, call=None):\n    \"\"\"\n    Copy a snapshot\n    \"\"\"\n    if call != 'function':\n        log.error('The copy_snapshot function must be called with -f or --function.')\n        return False\n\n    if 'source_region' not in kwargs:\n        log.error('A source_region must be specified to copy a snapshot.')\n        return False\n\n    if 'source_snapshot_id' not in kwargs:\n        log.error('A source_snapshot_id must be specified to copy a snapshot.')\n        return False\n\n    if 'description' not in kwargs:\n        kwargs['description'] = ''\n\n    params = {'Action': 'CopySnapshot'}\n\n    if 'source_region' in kwargs:\n        params['SourceRegion'] = kwargs['source_region']\n\n    if 'source_snapshot_id' in kwargs:\n        params['SourceSnapshotId'] = kwargs['source_snapshot_id']\n\n    if 'description' in kwargs:\n        params['Description'] = kwargs['description']\n\n    log.debug(params)\n\n    data = aws.query(params,\n                     return_url=True,\n                     location=get_location(),\n                     provider=get_provider(),\n                     opts=__opts__,\n                     sigver='4')\n    return data"
    },
    {
        "original": "def _left_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the left in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n      return r\n    elif not self._upper:\n      return 0\n    elif self._include_diagonal:\n      return r\n    else:\n      return r + 1",
        "rewrite": "def _left_zero_blocks(self, r):\n    if not self._include_off_diagonal:\n        return r\n    elif not self._upper:\n        return 0\n    elif self._include_diagonal:\n        return r\n    else:\n        return r + 1"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "rewrite": "def execute(self):\n    command = self.command\n    self.reset()\n    return self.client.execute_command(*command)"
    },
    {
        "original": "def InferUserAndSubjectFromUrn(self):\n    \"\"\"Infers user name and subject urn from self.urn.\"\"\"\n    _, hunts_str, hunt_id, user, _ = self.urn.Split(5)\n\n    if hunts_str != \"hunts\":\n      raise access_control.UnauthorizedAccess(\n          \"Approval object has invalid urn %s.\" % self.urn,\n          requested_access=self.token.requested_access)\n\n    return (user, aff4.ROOT_URN.Add(\"hunts\").Add(hunt_id))",
        "rewrite": "def infer_user_and_subject_from_urn(self):\n    _, hunts_str, hunt_id, user, _ = self.urn.split(5)\n\n    if hunts_str != \"hunts\":\n        raise access_control.UnauthorizedAccess(\n            \"Approval object has invalid urn %s.\" % self.urn,\n            requested_access=self.token.requested_access)\n\n    return (user, aff4.ROOT_URN.add(\"hunts\").add(hunt_id))"
    },
    {
        "original": "def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json",
        "rewrite": "def _api_config_item(self, item):\n    response.content_type = 'application/json; charset=utf-8'\n\n    config_dict = self.config.as_dict()\n    if item not in config_dict:\n        abort(400, \"Unknown configuration item %s\" % item)\n\n    try:\n        args_json = json.dumps(config_dict[item])\n    except Exception as e:\n        abort(404, \"Cannot get config item (%s)\" % str(e))\n    return args_json"
    },
    {
        "original": "async def get_response(self, message=None, *, timeout=None):\n        \"\"\"\n        Returns a coroutine that will resolve once a response arrives.\n\n        Args:\n            message (`Message <telethon.tl.custom.message.Message>` | `int`, optional):\n                The message (or the message ID) for which a response\n                is expected. By default this is the last sent message.\n\n            timeout (`int` | `float`, optional):\n                If present, this `timeout` (in seconds) will override the\n                per-action timeout defined for the conversation.\n        \"\"\"\n        return await self._get_message(\n            message, self._response_indices, self._pending_responses, timeout,\n            lambda x, y: True\n        )",
        "rewrite": "async def get_response(self, message=None, *, timeout=None):\n    return await self._get_message(\n        message, self._response_indices, self._pending_responses, timeout,\n        lambda x, y: True\n    )"
    },
    {
        "original": "def _extract_field_with_regex(self, field):\n        \"\"\" extract field from response content with regex.\n            requests.Response body could be json or html text.\n\n        Args:\n            field (str): regex string that matched r\".*\\(.*\\).*\"\n\n        Returns:\n            str: matched content.\n\n        Raises:\n            exceptions.ExtractFailure: If no content matched with regex.\n\n        Examples:\n            >>> # self.text: \"LB123abcRB789\"\n            >>> filed = \"LB[\\d]*(.*)RB[\\d]*\"\n            >>> _extract_field_with_regex(field)\n            abc\n\n        \"\"\"\n        matched = re.search(field, self.text)\n        if not matched:\n            err_msg = u\"Failed to extract data with regex! => {}\\n\".format(field)\n            err_msg += u\"response body: {}\\n\".format(self.text)\n            logger.log_error(err_msg)\n            raise exceptions.ExtractFailure(err_msg)\n\n        return matched.group(1)",
        "rewrite": "def _extract_field_with_regex(self, field):\n    matched = re.search(field, self.text)\n    if not matched:\n        err_msg = u\"Failed to extract data with regex! => {}\\n\".format(field)\n        err_msg += u\"response body: {}\\n\".format(self.text)\n        logger.log_error(err_msg)\n        raise exceptions.ExtractFailure(err_msg)\n\n    return matched.group(1)"
    },
    {
        "original": "def get_accumulator_dir(cachedir):\n    \"\"\"\n    Return the directory that accumulator data is stored in, creating it if it\n    doesn't exist.\n    \"\"\"\n    fn_ = os.path.join(cachedir, 'accumulator')\n    if not os.path.isdir(fn_):\n        # accumulator_dir is not present, create it\n        os.makedirs(fn_)\n    return fn_",
        "rewrite": "import os\n\ndef get_accumulator_dir(cachedir):\n    fn_ = os.path.join(cachedir, 'accumlator')\n    if not os.path.isdir(fn_):\n        os.makedirs(fn_)\n    return fn_"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "rewrite": "def differing_functions_with_consts(self):\n    \"\"\"\n    :return: A list of function matches that appear to differ including just by constants\n    \"\"\"\n    different_funcs = []\n    for (func_a, func_b) in self.function_matches:\n        if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n            different_funcs.append((func_a, func_b))\n    return different_funcs"
    },
    {
        "original": "def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt",
        "rewrite": "def single_frame_plot(obj):\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt"
    },
    {
        "original": "def __get_cpu(self):\n        \"\"\"Update and/or return the CPU using the psutil library.\"\"\"\n        # Never update more than 1 time per cached_time\n        if self.timer_cpu.finished():\n            self.cpu_percent = psutil.cpu_percent(interval=0.0)\n            # Reset timer for cache\n            self.timer_cpu = Timer(self.cached_time)\n        return self.cpu_percent",
        "rewrite": "def __get_cpu(self):\n    if self.timer_cpu.finished():\n        self.cpu_percent = psutil.cpu_percent(interval=0.0)\n        self.timer_cpu = Timer(self.cached_time)\n    return self.cpu_percent"
    },
    {
        "original": "def text_of(relpath):\n    \"\"\"\n    Return string containing the contents of the file at *relpath* relative to\n    this file.\n    \"\"\"\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    with open(file_path) as f:\n        text = f.read()\n    return text",
        "rewrite": "import os\n\ndef text_of(relpath):\n    thisdir = os.path.dirname(__file__)\n    file_path = os.path.join(thisdir, os.path.normpath(relpath))\n    \n    with open(file_path) as f:\n        text = f.read()\n    \n    return text"
    },
    {
        "original": "def relativize(self, absolute_address, target_region_id=None):\n        \"\"\"\n        Convert an absolute address to the memory offset in a memory region.\n\n        Note that if an address belongs to heap region is passed in to a stack region map, it will be converted to an\n        offset included in the closest stack frame, and vice versa for passing a stack address to a heap region.\n        Therefore you should only pass in address that belongs to the same category (stack or non-stack) of this region\n        map.\n\n        :param absolute_address:    An absolute memory address\n        :return:                    A tuple of the closest region ID, the relative offset, and the related function\n                                    address.\n        \"\"\"\n\n        if target_region_id is None:\n            if self.is_stack:\n                # Get the base address of the stack frame it belongs to\n                base_address = next(self._address_to_region_id.irange(minimum=absolute_address, reverse=False))\n\n            else:\n                try:\n                    base_address = next(self._address_to_region_id.irange(maximum=absolute_address, reverse=True))\n\n                except StopIteration:\n                    # Not found. It belongs to the global region then.\n                    return 'global', absolute_address, None\n\n            descriptor = self._address_to_region_id[base_address]\n\n        else:\n            if target_region_id == 'global':\n                # Just return the absolute address\n                return 'global', absolute_address, None\n\n            if target_region_id not in self._region_id_to_address:\n                raise SimRegionMapError('Trying to relativize to a non-existent region \"%s\"' % target_region_id)\n\n            descriptor = self._region_id_to_address[target_region_id]\n            base_address = descriptor.base_address\n\n        return descriptor.region_id, absolute_address - base_address, descriptor.related_function_address",
        "rewrite": "def relativize(self, absolute_address, target_region_id=None):\n\n    if target_region_id is None:\n        if self.is_stack:\n            base_address = next(self._address_to_region_id.irange(minimum=absolute_address, reverse=False))\n        else:\n            try:\n                base_address = next(self._address_to_region_id.irange(maximum=absolute_address, reverse=True))\n            except StopIteration:\n                return 'global', absolute_address, None\n        descriptor = self._address_to_region_id[base_address]\n\n    else:\n        if target_region_id == 'global':\n            return 'global', absolute_address, None\n\n        if target_region_id not in self._region_id_to_address:\n            raise SimRegionMapError('Trying to relativize to a non-existent region \"%s\"' % target_region_id)\n\n        descriptor = self._region_id_to_address[target_region_id]\n        base_address = descriptor.base_address\n\n    return descriptor.region_id, absolute_address - base_address, descriptor.related_function_address"
    },
    {
        "original": "def _build(self, images):\n    \"\"\"Build dilation module.\n\n    Args:\n      images: Tensor of shape [batch_size, height, width, depth]\n        and dtype float32. Represents a set of images with an arbitrary depth.\n        Note that when using the default initializer, depth must equal\n        num_output_classes.\n\n    Returns:\n      Tensor of shape [batch_size, height, width, num_output_classes] and dtype\n        float32. Represents, for each image and pixel, logits for per-class\n        predictions.\n\n    Raises:\n      IncompatibleShapeError: If images is not rank 4.\n      ValueError: If model_size is not one of 'basic' or 'large'.\n    \"\"\"\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n      raise base.IncompatibleShapeError(\n          \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n      if self._model_size == self.BASIC:\n        self._initializers[self.WEIGHTS] = identity_kernel_initializer\n      elif self._model_size == self.LARGE:\n        self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(\n            num_classes)\n      else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n      self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n      self._conv_modules = [\n          self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    elif self._model_size == self.LARGE:\n      self._conv_modules = [\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n          self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n          self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n          self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n          self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n          self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n          self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n          self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n      ]\n    else:\n      raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)",
        "rewrite": "def build(self, images):\n    num_classes = self._num_output_classes\n\n    if len(images.get_shape()) != 4:\n        raise base.IncompatibleShapeError(\n            \"'images' must have shape [batch_size, height, width, depth].\")\n\n    if self.WEIGHTS not in self._initializers:\n        if self._model_size == self.BASIC:\n            self._initializers[self.WEIGHTS] = identity_kernel_initializer\n        elif self._model_size == self.LARGE:\n            self._initializers[self.WEIGHTS] = noisy_identity_kernel_initializer(num_classes)\n        else:\n            raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    if self.BIASES not in self._initializers:\n        self._initializers[self.BIASES] = tf.zeros_initializer()\n\n    if self._model_size == self.BASIC:\n        self._conv_modules = [\n            self._dilated_conv_layer(num_classes, 1, True, \"conv1\"),\n            self._dilated_conv_layer(num_classes, 1, True, \"conv2\"),\n            self._dilated_conv_layer(num_classes, 2, True, \"conv3\"),\n            self._dilated_conv_layer(num_classes, 4, True, \"conv4\"),\n            self._dilated_conv_layer(num_classes, 8, True, \"conv5\"),\n            self._dilated_conv_layer(num_classes, 16, True, \"conv6\"),\n            self._dilated_conv_layer(num_classes, 1, True, \"conv7\"),\n            self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n        ]\n    elif self._model_size == self.LARGE:\n        self._conv_modules = [\n            self._dilated_conv_layer(2 * num_classes, 1, True, \"conv1\"),\n            self._dilated_conv_layer(2 * num_classes, 1, True, \"conv2\"),\n            self._dilated_conv_layer(4 * num_classes, 2, True, \"conv3\"),\n            self._dilated_conv_layer(8 * num_classes, 4, True, \"conv4\"),\n            self._dilated_conv_layer(16 * num_classes, 8, True, \"conv5\"),\n            self._dilated_conv_layer(32 * num_classes, 16, True, \"conv6\"),\n            self._dilated_conv_layer(32 * num_classes, 1, True, \"conv7\"),\n            self._dilated_conv_layer(num_classes, 1, False, \"conv8\"),\n        ]\n    else:\n        raise ValueError(\"Unrecognized model_size: %s\" % self._model_size)\n\n    dilation_mod = sequential.Sequential(self._conv_modules, name=\"dilation\")\n    return dilation_mod(images)"
    },
    {
        "original": "def load(self):\n        \"\"\"\n        call this function after the file exists to populate properties\n        \"\"\"\n        # If we are given a string, open it up else assume it's something we\n        # can call read on.\n        if isinstance(self.specfile, str):\n            f = open(self.specfile, 'r')\n        else:\n            f = self.specfile\n\n        for line in f:\n            if self.v_regex.match(line):\n                self._pkg_version = self.v_regex.match(line).group(1)\n            if self.n_regex.match(line):\n                self._pkg_name = self.n_regex.match(line).group(1)\n        f.close()\n        self._loaded = True",
        "rewrite": "def load(self):\n    if isinstance(self.specfile, str):\n        f = open(self.specfile, 'r')\n    else:\n        f = self.specfile\n\n    for line in f:\n        if self.v_regex.match(line):\n            self._pkg_version = self.v_regex.match(line).group(1)\n        if self.n_regex.match(line):\n            self._pkg_name = self.n_regex.match(line).group(1)\n    f.close()\n    self._loaded = True"
    },
    {
        "original": "def order_by_line_nos(objs, line_nos):\n    \"\"\"Orders the set of `objs` by `line_nos`\n    \"\"\"\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]",
        "rewrite": "def order_by_line_nos(objs, line_nos):\n    ordering = sorted(range(len(line_nos)), key=line_nos.__getitem__)\n    return [objs[i] for i in ordering]"
    },
    {
        "original": "def get_function_subgraph(self, start, max_call_depth=None):\n        \"\"\"\n        Get a sub-graph of a certain function.\n\n        :param start: The function start. Currently it should be an integer.\n        :param max_call_depth: Call depth limit. None indicates no limit.\n        :return: A CFG instance which is a sub-graph of self.graph\n        \"\"\"\n\n        # FIXME: syscalls are not supported\n        # FIXME: start should also take a CFGNode instance\n\n        start_node = self.get_any_node(start)\n\n        node_wrapper = (start_node, 0)\n        stack = [node_wrapper]\n        traversed_nodes = {start_node}\n        subgraph_nodes = set([start_node])\n\n        while stack:\n            nw = stack.pop()\n            n, call_depth = nw[0], nw[1]\n\n            # Get successors\n            edges = self.graph.out_edges(n, data=True)\n\n            for _, dst, data in edges:\n                if dst not in traversed_nodes:\n                    # We see a new node!\n                    traversed_nodes.add(dst)\n\n                    if data['jumpkind'] == 'Ijk_Call':\n                        if max_call_depth is None or (max_call_depth is not None and call_depth < max_call_depth):\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth + 1)\n                            stack.append(new_nw)\n                    elif data['jumpkind'] == 'Ijk_Ret':\n                        if call_depth > 0:\n                            subgraph_nodes.add(dst)\n                            new_nw = (dst, call_depth - 1)\n                            stack.append(new_nw)\n                    else:\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth)\n                        stack.append(new_nw)\n\n       #subgraph = networkx.subgraph(self.graph, subgraph_nodes)\n        subgraph = self.graph.subgraph(subgraph_nodes).copy()\n\n        # Make it a CFG instance\n        subcfg = self.copy()\n        subcfg._graph = subgraph\n        subcfg._starts = (start,)\n\n        return subcfg",
        "rewrite": "def get_function_subgraph(self, start, max_call_depth=None):\n    start_node = self.get_any_node(start)\n\n    node_wrapper = (start_node, 0)\n    stack = [node_wrapper]\n    traversed_nodes = {start_node}\n    subgraph_nodes = set([start_node])\n\n    while stack:\n        nw = stack.pop()\n        n, call_depth = nw[0], nw[1]\n\n        edges = self.graph.out_edges(n, data=True)\n\n        for _, dst, data in edges:\n            if dst not in traversed_nodes:\n                traversed_nodes.add(dst)\n\n                if data['jumpkind'] == 'Ijk_Call':\n                    if max_call_depth is None or (max_call_depth is not None and call_depth < max_call_depth):\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth + 1)\n                        stack.append(new_nw)\n                elif data['jumpkind'] == 'Ijk_Ret':\n                    if call_depth > 0:\n                        subgraph_nodes.add(dst)\n                        new_nw = (dst, call_depth - 1)\n                        stack.append(new_nw)\n                else:\n                    subgraph_nodes.add(dst)\n                    new_nw = (dst, call_depth)\n                    stack.append(new_nw)\n\n    subgraph = self.graph.subgraph(subgraph_nodes).copy()\n    \n    subcfg = self.copy()\n    subcfg._graph = subgraph\n    subcfg._starts = (start,)\n\n    return subcfg"
    },
    {
        "original": "def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        endpoint = instance.get('prometheus_url')\n        if endpoint is None:\n            raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n\n        metrics = [DEFAULT_METRICS, GO_METRICS]\n        metrics.extend(instance.get('metrics', []))\n\n        instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\n\n        return instance",
        "rewrite": "def _create_core_dns_instance(self, instance):\r\n    endpoint = instance.get('prometheus_url')\r\n    if endpoint is None:\r\n        raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\r\n\r\n    metrics = [DEFAULT_METRICS, GO_METRICS]\r\n    metrics.extend(instance.get('metrics', []))\r\n\r\n    instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\r\n\r\n    return instance"
    },
    {
        "original": "def exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n        \"\"\"Omits a scheduled operation from the schedule, if present.\n\n        Args:\n            scheduled_operation: The operation to try to remove.\n\n        Returns:\n            True if the operation was present and is now removed, False if it\n            was already not present.\n        \"\"\"\n        try:\n            self.scheduled_operations.remove(scheduled_operation)\n            return True\n        except ValueError:\n            return False",
        "rewrite": "def exclude(self, scheduled_operation: ScheduledOperation) -> bool:\n    try:\n        self.scheduled_operations.remove(scheduled_operation)\n        return True\n    except ValueError:\n        return False"
    },
    {
        "original": "def server_show_libcloud(self, uuid):\n        \"\"\"\n        Make output look like libcloud output for consistency\n        \"\"\"\n        server_info = self.server_show(uuid)\n        server = next(six.itervalues(server_info))\n        server_name = next(six.iterkeys(server_info))\n        if not hasattr(self, 'password'):\n            self.password = None\n        ret = NovaServer(server_name, server, self.password)\n\n        return ret",
        "rewrite": "def server_show_libcloud(self, uuid):\n    server_info = self.server_show(uuid)\n    server = next(six.itervalues(server_info))\n    server_name = next(six.iterkeys(server_info))\n    if not hasattr(self, 'password'):\n        self.password = None\n    ret = NovaServer(server_name, server, self.password)\n\n    return ret"
    },
    {
        "original": "def extract(self, topic: str, parseNumbers=True) -> list:\n        \"\"\"\n        Extract items of given topic and return as list of objects.\n\n        The topic is a string like TradeConfirm, ChangeInDividendAccrual,\n        Order, etc.\n        \"\"\"\n        cls = type(topic, (DynamicObject,), {})\n        results = [cls(**node.attrib) for node in self.root.iter(topic)]\n        if parseNumbers:\n            for obj in results:\n                d = obj.__dict__\n                for k, v in d.items():\n                    with suppress(ValueError):\n                        d[k] = float(v)\n                        d[k] = int(v)\n        return results",
        "rewrite": "def extract(self, topic: str, parseNumbers=True) -> list:\n    cls = type(topic, (DynamicObject,), {})\n    results = [cls(**node.attrib) for node in self.root.iter(topic)]\n    if parseNumbers:\n        for obj in results:\n            d = obj.__dict__\n            for k, v in d.items():\n                with suppress(ValueError):\n                    d[k] = float(v)\n                    d[k] = int(v)\n    return results"
    },
    {
        "original": "def pin_auth(self, request):\n        \"\"\"Authenticates with the pin.\"\"\"\n        exhausted = False\n        auth = False\n        trust = self.check_pin_trust(request.environ)\n\n        # If the trust return value is `None` it means that the cookie is\n        # set but the stored pin hash value is bad.  This means that the\n        # pin was changed.  In this case we count a bad auth and unset the\n        # cookie.  This way it becomes harder to guess the cookie name\n        # instead of the pin as we still count up failures.\n        bad_cookie = False\n        if trust is None:\n            self._fail_pin_auth()\n            bad_cookie = True\n\n        # If we're trusted, we're authenticated.\n        elif trust:\n            auth = True\n\n        # If we failed too many times, then we're locked out.\n        elif self._failed_pin_auth > 10:\n            exhausted = True\n\n        # Otherwise go through pin based authentication\n        else:\n            entered_pin = request.args.get(\"pin\")\n            if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n                self._failed_pin_auth = 0\n                auth = True\n            else:\n                self._fail_pin_auth()\n\n        rv = Response(\n            json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n            mimetype=\"application/json\",\n        )\n        if auth:\n            rv.set_cookie(\n                self.pin_cookie_name,\n                \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n                httponly=True,\n            )\n        elif bad_cookie:\n            rv.delete_cookie(self.pin_cookie_name)\n        return rv",
        "rewrite": "def pin_auth(self, request):\n    exhausted = False\n    auth = False\n    trust = self.check_pin_trust(request.environ)\n    bad_cookie = False\n    \n    if trust is None:\n        self._fail_pin_auth()\n        bad_cookie = True\n    elif trust:\n        auth = True\n    elif self._failed_pin_auth > 10:\n        exhausted = True\n    else:\n        entered_pin = request.args.get(\"pin\")\n        if entered_pin.strip().replace(\"-\", \"\") == self.pin.replace(\"-\", \"\"):\n            self._failed_pin_auth = 0\n            auth = True\n        else:\n            self._fail_pin_auth()\n\n    rv = Response(\n        json.dumps({\"auth\": auth, \"exhausted\": exhausted}),\n        mimetype=\"application/json\",\n    )\n    \n    if auth:\n        rv.set_cookie(\n            self.pin_cookie_name,\n            \"%s|%s\" % (int(time.time()), hash_pin(self.pin)),\n            httponly=True,\n        )\n    elif bad_cookie:\n        rv.delete_cookie(self.pin_cookie_name)\n    \n    return rv"
    },
    {
        "original": "def set_lim(min, max, name):\n    \"\"\"Set the domain bounds of the scale associated with the provided key.\n\n    Parameters\n    ----------\n    name: hashable\n        Any variable that can be used as a key for a dictionary\n\n    Raises\n    ------\n    KeyError\n        When no context figure is associated with the provided key.\n\n    \"\"\"\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min\n    scale.max = max\n    return scale",
        "rewrite": "def set_lim(min_val, max_val, name):\n    scale = _context['scales'][_get_attribute_dimension(name)]\n    scale.min = min_val\n    scale.max = max_val\n    return scale"
    },
    {
        "original": "def ParseMultiple(self, stats, unused_file_obj, unused_kb):\n    \"\"\"Identify the init scripts and the start/stop scripts at each runlevel.\n\n    Evaluate all the stat entries collected from the system.\n    If the path name matches a runlevel spec, and if the filename matches a\n    sysv init symlink process the link as a service.\n\n    Args:\n      stats: An iterator of StatEntry rdfs.\n      unused_file_obj: An iterator of file contents. Not needed as the parser\n        only evaluates link attributes.\n      unused_kb: Unused KnowledgeBase rdf.\n\n    Yields:\n      rdf_anomaly.Anomaly if the startup link seems wierd.\n      rdf_client.LinuxServiceInformation for each detected service.\n    \"\"\"\n    services = {}\n    for stat_entry in stats:\n      path = stat_entry.pathspec.path\n      runlevel = self.runlevel_re.match(os.path.dirname(path))\n      runscript = self.runscript_re.match(os.path.basename(path))\n      if runlevel and runscript:\n        svc = runscript.groupdict()\n        service = services.setdefault(\n            svc[\"name\"],\n            rdf_client.LinuxServiceInformation(\n                name=svc[\"name\"], start_mode=\"INIT\"))\n        runlvl = GetRunlevelsNonLSB(runlevel.group(1))\n        if svc[\"action\"] == \"S\" and runlvl:\n          service.start_on.append(runlvl.pop())\n          service.starts = True\n        elif runlvl:\n          service.stop_on.append(runlvl.pop())\n        if not stat.S_ISLNK(int(stat_entry.st_mode)):\n          yield rdf_anomaly.Anomaly(\n              type=\"PARSER_ANOMALY\",\n              finding=[path],\n              explanation=\"Startup script is not a symlink.\")\n    for svc in itervalues(services):\n      yield svc",
        "rewrite": "def ParseMultiple(self, stats, unused_file_obj, unused_kb):\n    services = {}\n    for stat_entry in stats:\n        path = stat_entry.pathspec.path\n        runlevel = self.runlevel_re.match(os.path.dirname(path))\n        runscript = self.runscript_re.match(os.path.basename(path))\n        if runlevel and runscript:\n            svc = runscript.groupdict()\n            service = services.setdefault(\n                svc[\"name\"],\n                rdf_client.LinuxServiceInformation(\n                    name=svc[\"name\"], start_mode=\"INIT\"))\n            runlvl = GetRunlevelsNonLSB(runlevel.group(1))\n            if svc[\"action\"] == \"S\" and runlvl:\n                service.start_on.append(runlvl.pop())\n                service.starts = True\n            elif runlvl:\n                service.stop_on.append(runlvl.pop())\n            if not stat.S_ISLNK(int(stat_entry.st_mode)):\n                yield rdf_anomaly.Anomaly(\n                    type=\"PARSER_ANOMALY\",\n                    finding=[path],\n                    explanation=\"Startup script is not a symlink.\")\n    for svc in itervalues(services):\n        yield svc"
    },
    {
        "original": "def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs",
        "rewrite": "def get_ext_outputs(self):\n    all_outputs = []\n    ext_outputs = []\n    \n    paths = {self.bdist_dir: ''}\n    for base, dirs, files in sorted_walk(self.bdist_dir):\n        for filename in files:\n            if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                all_outputs.append(paths[base] + filename)\n        for filename in dirs:\n            paths[os.path.join(base, filename)] = (paths[base] + filename + '/')\n    \n    if self.distribution.has_ext_modules():\n        build_cmd = self.get_finalized_command('build_ext')\n        for ext in build_cmd.extensions:\n            if isinstance(ext, Library):\n                continue\n            fullname = build_cmd.get_ext_fullname(ext.name)\n            filename = build_cmd.get_ext_filename(fullname)\n            if not os.path.basename(filename).startswith('dl-'):\n                if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                    ext_outputs.append(filename)\n    \n    return all_outputs, ext_outputs"
    },
    {
        "original": "def base64_bytes(x):\n    \"\"\"Turn base64 into bytes\"\"\"\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(bytes_encode(x))",
        "rewrite": "def base64_bytes(x):\n    if six.PY2:\n        return base64.decodestring(x)\n    return base64.decodebytes(x.encode())"
    },
    {
        "original": "def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node",
        "rewrite": "def fold(self, node):\n    node = self.generic_visit(node)\n    try:\n        return nodes.Const.from_untrusted(node.as_const(), lineno=node.lineno, environment=self.environment)\n    except nodes.Impossible:\n        return node"
    },
    {
        "original": "def from_node(index, data, modify_index=None):\n        \"\"\"\n        >>> ClusterConfig.from_node(1, '{') is None\n        False\n        \"\"\"\n\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)",
        "rewrite": "def from_node(index, data, modify_index=None):\n    try:\n        data = json.loads(data)\n    except (TypeError, ValueError):\n        data = None\n        modify_index = 0\n    if not isinstance(data, dict):\n        data = {}\n    return ClusterConfig(index, data, index if modify_index is None else modify_index)"
    },
    {
        "original": "def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "rewrite": "def save_config(self, cmd=\"copy running-config startup-config\", confirm=True, confirm_response=\"y\"):\n    \"\"\"Save Config for Extreme VDX.\"\"\"\n    return super(ExtremeNosSSH, self).save_config(cmd=cmd, confirm=confirm, confirm_response=confirm_response)"
    },
    {
        "original": "def Validate(self):\n    \"\"\"GlobExpression is valid.\"\"\"\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n      raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)",
        "rewrite": "def Validate(self):\n    if len(self.RECURSION_REGEX.findall(self._value)) > 1:\n        raise ValueError(\"Only one ** is permitted per path: %s.\" % self._value)"
    },
    {
        "original": "def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        with self._lock:\n            if name not in self._gauges:\n                if self._registry._ignore_patterns and any(\n                    pattern.match(name) for pattern in self._registry._ignore_patterns\n                ):\n                    gauge = noop_metric\n                else:\n                    gauge = Gauge(name)\n                self._gauges[name] = gauge\n            return self._gauges[name]",
        "rewrite": "def gauge(self, name):\n    with self._lock:\n        if name not in self._gauges:\n            if self._registry._ignore_patterns and any(\n                pattern.match(name) for pattern in self._registry._ignore_patterns\n            ):\n                gauge = noop_metric\n            else:\n                gauge = Gauge(name)\n            self._gauges[name] = gauge\n        return self._gauges[name]"
    },
    {
        "original": "def disconnect(service_instance):\n    \"\"\"\n    Function that disconnects from the vCenter server or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)",
        "rewrite": "def disconnect(service_instance):\n    log.trace('Disconnecting')\n    try:\n        Disconnect(service_instance)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)"
    },
    {
        "original": "def h2i(self, pkt, seconds):\n        \"\"\"Convert the number of seconds since 1-Jan-70 UTC to the packed\n           representation.\"\"\"\n\n        if seconds is None:\n            seconds = 0\n\n        tmp_short = (seconds >> 32) & 0xFFFF\n        tmp_int = seconds & 0xFFFFFFFF\n\n        return struct.pack(\"!HI\", tmp_short, tmp_int)",
        "rewrite": "def h2i(self, pkt, seconds):\n    if seconds is None:\n        seconds = 0\n    \n    tmp_short = (seconds >> 32) & 0xFFFF\n    tmp_int = seconds & 0xFFFFFFFF\n    \n    return struct.pack(\"!HI\", tmp_short, tmp_int)"
    },
    {
        "original": "def notebook_start(self, **kwargs):\n        \"\"\"\n        Initialize a notebook, clearing its metadata, and save it.\n\n        When starting a notebook, this initializes and clears the metadata for\n        the notebook and its cells, and saves the notebook to the given\n        output path.\n\n        Called by Engine when execution begins.\n        \"\"\"\n        self.set_timer()\n\n        self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n        self.nb.metadata.papermill['end_time'] = None\n        self.nb.metadata.papermill['duration'] = None\n        self.nb.metadata.papermill['exception'] = None\n\n        for cell in self.nb.cells:\n            # Reset the cell execution counts.\n            if cell.get(\"execution_count\") is not None:\n                cell.execution_count = None\n\n            # Clear out the papermill metadata for each cell.\n            cell.metadata.papermill = dict(\n                exception=None,\n                start_time=None,\n                end_time=None,\n                duration=None,\n                status=self.PENDING,  # pending, running, completed\n            )\n            if cell.get(\"outputs\") is not None:\n                cell.outputs = []\n\n        self.save()",
        "rewrite": "def notebook_start(self, **kwargs):\n    self.set_timer()\n    self.nb.metadata.papermill['start_time'] = self.start_time.isoformat()\n    self.nb.metadata.papermill['end_time'] = None\n    self.nb.metadata.papermill['duration'] = None\n    self.nb.metadata.papermill['exception'] = None\n\n    for cell in self.nb.cells:\n        if cell.get(\"execution_count\") is not None:\n            cell.execution_count = None\n\n        cell.metadata.papermill = {\n            'exception': None,\n            'start_time': None,\n            'end_time': None,\n            'duration': None,\n            'status': self.PENDING\n        }\n        \n        if cell.get(\"outputs\") is not None:\n            cell.outputs = []\n            \n    self.save()"
    },
    {
        "original": "def dispatch_request(self, req):\n        \"\"\"\n        Dispatch a request object.\n        \"\"\"\n        log.debug(\"Dispatching request: {}\".format(str(req)))\n\n        # make sure it's valid\n        res = None\n        try:\n            req.validate()\n        except MissingFieldError as e:\n            res = APIMissingFieldErrorResponse(str(e))\n\n        # dispatch the request\n        if not res:\n            try:\n                res = req.dispatch()\n            except Exception as e:\n                msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n                log.exception(msg)\n                res = APIGenericErrorResponse(msg)\n\n        log.debug(\"Response: {}\".format(str(res)))\n\n        return res",
        "rewrite": "def dispatch_request(self, req):\n    log.debug(f\"Dispatching request: {str(req)}\")\n\n    res = None\n    try:\n        req.validate()\n    except MissingFieldError as e:\n        res = APIMissingFieldErrorResponse(str(e))\n\n    if not res:\n        try:\n            res = req.dispatch()\n        except Exception as e:\n            msg = f\"Exception raised while dispatching request: {repr(e)}\"\n            log.exception(msg)\n            res = APIGenericErrorResponse(msg)\n\n    log.debug(f\"Response: {str(res)}\")\n\n    return res"
    },
    {
        "original": "def _get_ngrams_with_counter(segment, max_order):\n  \"\"\"Extracts all n-grams up to a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  \"\"\"\n  ngram_counts = collections.Counter()\n  for order in xrange(1, max_order + 1):\n    for i in xrange(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i + order])\n      ngram_counts[ngram] += 1\n  return ngram_counts",
        "rewrite": "import collections\n\ndef _get_ngrams_with_counter(segment, max_order):\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts"
    },
    {
        "original": "def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c",
        "rewrite": "def replace_species(self, species_mapping):\n    species_mapping = {get_el_sp(k): v\n                       for k, v in species_mapping.items()}\n    sp_to_replace = set(species_mapping.keys())\n    sp_in_structure = set(self.composition.keys())\n    if not sp_in_structure.issuperset(sp_to_replace):\n        warnings.warn(\n            \"Some species to be substituted are not present in \"\n            \"structure. Please check your input. Species to be \"\n            \"substituted = %s; Species in structure = %s\"\n            % (sp_to_replace, sp_in_structure))\n\n    for site in self._sites:\n        if sp_to_replace.intersection(site.species):\n            c = Composition()\n            for sp, amt in site.species.items():\n                new_sp = species_mapping.get(sp, sp)\n                try:\n                    c += Composition(new_sp) * amt\n                except Exception:\n                    c += {new_sp: amt}\n            site.species = c"
    },
    {
        "original": "def show_item_dict(self, item):\n        \"\"\"Returns a json-able dict for show\"\"\"\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, string_types)):\n                v = str(v)\n            d[col] = v\n        return d",
        "rewrite": "def show_item_dict(self, item):\n        d = {}\n        for col in self.show_columns:\n            v = getattr(item, col)\n            if not isinstance(v, (int, float, str)):\n                v = str(v)\n            d[col] = v\n        return d"
    },
    {
        "original": "def FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]",
        "rewrite": "def FilterRange(self, start_time=None, stop_time=None):\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]"
    },
    {
        "original": "def get_connection(self, command_name, *keys, **options):\n        \"\"\"\n        Get a connection, blocking for ``self.timeout`` until a connection\n        is available from the pool.\n\n        If the connection returned is ``None`` then creates a new connection.\n        Because we use a last-in first-out queue, the existing connections\n        (having been returned to the pool after the initial ``None`` values\n        were added) will be returned before ``None`` values. This means we only\n        create new connections when we need to, i.e.: the actual number of\n        connections will only increase in response to demand.\n        \"\"\"\n        # Make sure we haven't changed process.\n        self._checkpid()\n\n        # Try and get a connection from the pool. If one isn't available within\n        # self.timeout then raise a ``ConnectionError``.\n        connection = None\n        try:\n            connection = self.pool.get(block=True, timeout=self.timeout)\n        except Empty:\n            # Note that this is not caught by the redis client and will be\n            # raised unless handled by application code. If you want never to\n            raise ConnectionError(\"No connection available.\")\n\n        # If the ``connection`` is actually ``None`` then that's a cue to make\n        # a new connection to add to the pool.\n        if connection is None:\n            connection = self.make_connection()\n\n        try:\n            # ensure this connection is connected to Redis\n            connection.connect()\n            # connections that the pool provides should be ready to send\n            # a command. if not, the connection was either returned to the\n            # pool before all data has been read or the socket has been\n            # closed. either way, reconnect and verify everything is good.\n            if not connection.is_ready_for_command():\n                connection.disconnect()\n                connection.connect()\n                if not connection.is_ready_for_command():\n                    raise ConnectionError('Connection not ready')\n        except:  # noqa: E722\n            # release the connection back to the pool so that we don't leak it\n            self.release(connection)\n            raise\n\n        return connection",
        "rewrite": "def get_connection(self, command_name, *keys, **options):\n    self._checkpid()\n    \n    connection = None\n    try:\n        connection = self.pool.get(block=True, timeout=self.timeout)\n    except Empty:\n        raise ConnectionError(\"No connection available.\")\n        \n    if connection is None:\n        connection = self.make_connection()\n        \n    try:\n        connection.connect()\n        \n        if not connection.is_ready_for_command():\n            connection.disconnect()\n            connection.connect()\n            if not connection.is_ready_for_command():\n                raise ConnectionError('Connection not ready')\n    except:\n        self.release(connection)\n        raise\n        \n    return connection"
    },
    {
        "original": "def check_error(res, error_enum):\n  \"\"\"Raise if the result has an error, otherwise return the result.\"\"\"\n  if res.HasField(\"error\"):\n    enum_name = error_enum.DESCRIPTOR.full_name\n    error_name = error_enum.Name(res.error)\n    details = getattr(res, \"error_details\", \"<none>\")\n    raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n  return res",
        "rewrite": "def check_error(res, error_enum):\n    if res.HasField(\"error\"):\n        enum_name = error_enum.DESCRIPTOR.full_name\n        error_name = error_enum.Name(res.error)\n        details = getattr(res, \"error_details\", \"<none>\")\n        raise RequestError(\"%s.%s: '%s'\" % (enum_name, error_name, details), res)\n    return res"
    },
    {
        "original": "def initialize_plot(self, ranges=None):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = self.layout.get(pos, None)\n            subplot = self.subplots.get(pos, None)\n            ax = self.subaxes.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if None in [view, pos, subplot]:\n                ax.set_axis_off()\n                continue\n            subplot.initialize_plot(ranges=ranges)\n\n        self.adjust_positions()\n        self.drawn = True",
        "rewrite": "def initialize_plot(self, ranges=None):\n    for pos in self.view_positions:\n        view = self.layout.get(pos, None)\n        subplot = self.subplots.get(pos, None)\n        ax = self.subaxes.get(pos, None)\n\n        if None in [view, pos, subplot]:\n            ax.set_axis_off()\n            continue\n\n        subplot.initialize_plot(ranges=ranges)\n\n    self.adjust_positions()\n    self.drawn = True"
    },
    {
        "original": "def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]",
        "rewrite": "def split_sql(sql, encoding=None):\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]"
    },
    {
        "original": "def page_str(self):\n        \"\"\"\n        The RestructuredText documentation page for the enumeration. This is\n        the only API member for the class.\n        \"\"\"\n        tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n        components = (\n            self._ms_name, self._page_title, self._intro_text,\n            self._member_defs\n        )\n        return tmpl % components",
        "rewrite": "def page_str(self):\n    tmpl = '.. _%s:\\n\\n%s\\n\\n%s\\n\\n----\\n\\n%s'\n    components = (\n        self._ms_name, self._page_title, self._intro_text,\n        self._member_defs\n    )\n    return tmpl % components"
    },
    {
        "original": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this Attachment.\n\n        :returns: This Attachment, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        attachment = {}\n        if self.file_content is not None:\n            attachment[\"content\"] = self.file_content.get()\n\n        if self.file_type is not None:\n            attachment[\"type\"] = self.file_type.get()\n\n        if self.file_name is not None:\n            attachment[\"filename\"] = self.file_name.get()\n\n        if self.disposition is not None:\n            attachment[\"disposition\"] = self.disposition.get()\n\n        if self.content_id is not None:\n            attachment[\"content_id\"] = self.content_id.get()\n        return attachment",
        "rewrite": "def get(self):\n    attachment = {}\n    if self.file_content:\n        attachment[\"content\"] = self.file_content.get()\n\n    if self.file_type:\n        attachment[\"type\"] = self.file_type.get()\n\n    if self.file_name:\n        attachment[\"filename\"] = self.file_name.get()\n\n    if self.disposition:\n        attachment[\"disposition\"] = self.disposition.get()\n\n    if self.content_id:\n        attachment[\"content_id\"] = self.content_id.get()\n    \n    return attachment"
    },
    {
        "original": "def load_ner_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"ner{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  try:\n    return pickle.load(fh)\n  except UnicodeDecodeError:\n    fh.seek(0)\n    return pickle.load(fh, encoding='latin1')",
        "rewrite": "def load_ner_model(lang=\"en\", version=\"2\"):\n    src_dir = f\"ner{version}\"\n    p = locate_resource(src_dir, lang)\n    fh = _open(p)\n    try:\n        return pickle.load(fh)\n    except UnicodeDecodeError:\n        fh.seek(0)\n        return pickle.load(fh, encoding='latin1')"
    },
    {
        "original": "def _reset_state_mode(self, state, mode):\n        \"\"\"\n        Reset the state mode to the given mode, and apply the custom state options specified with this analysis.\n\n        :param state:    The state to work with.\n        :param str mode: The state mode.\n        :return:         None\n        \"\"\"\n\n        state.set_mode(mode)\n        state.options |= self._state_add_options\n        state.options = state.options.difference(self._state_remove_options)",
        "rewrite": "def _reset_state_mode(self, state, mode):\n    state.set_mode(mode)\n    state.options |= self._state_add_options\n    state.options = state.options.difference(self._state_remove_options)"
    },
    {
        "original": "def normalize_words(self, ord=2, inplace=False):\n    \"\"\"Normalize embeddings matrix row-wise.\n\n    Args:\n      ord: normalization order. Possible values {1, 2, 'inf', '-inf'}\n    \"\"\"\n    if ord == 2:\n      ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n      self.vectors = vectors.T\n      return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)",
        "rewrite": "def normalize_words(self, ord=2, inplace=False):\n    if ord == 2:\n        ord = None\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n        self.vectors = vectors.T\n        return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)"
    },
    {
        "original": "def DEFINE_integer(self, name, default, help, constant=False):\n    \"\"\"A helper for defining integer options.\"\"\"\n    self.AddOption(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)",
        "rewrite": "def define_integer(self, name, default, help, constant=False):\n    self.add_option(\n        type_info.Integer(name=name, default=default, description=help),\n        constant=constant)"
    },
    {
        "original": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != f.data.RUNNING,\n        timeout=timeout)\n    if f.data.state != f.data.TERMINATED:\n      raise errors.FlowFailedError(\n          \"Flow %s (%s) failed: %s\" %\n          (self.flow_id, self.client_id, f.data.context.current_state))\n    return f",
        "rewrite": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the flow completes.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour). 0 means\n        no timeout (wait forever).\n\n    Returns:\n      Fresh flow object.\n    \n    Raises:\n      PollTimeoutError: if timeout is reached.\n      FlowFailedError: if the flow is not successful.\n    \"\"\"\n    f = utils.Poll(\n        generator=self.Get,\n        condition=lambda f: f.data.state != f.data.RUNNING,\n        timeout=timeout)\n        \n    if f.data.state != f.data.TERMINATED:\n      raise errors.FlowFailedError(\n          \"Flow %s (%s) failed: %s\" %\n          (self.flow_id, self.client_id, f.data.context.current_state))\n          \n    return f"
    },
    {
        "original": "def date_this_month(self, before_today=True, after_today=False):\n        \"\"\"\n        Gets a Date object for the current month.\n\n        :param before_today: include days in current month before today\n        :param after_today: include days in current month after today\n        :param tzinfo: timezone, instance of datetime.tzinfo subclass\n        :example DateTime('2012-04-04 11:02:02')\n        :return DateTime\n        \"\"\"\n        today = date.today()\n        this_month_start = today.replace(day=1)\n\n        next_month_start = this_month_start + \\\n            relativedelta.relativedelta(months=1)\n        if before_today and after_today:\n            return self.date_between_dates(this_month_start, next_month_start)\n        elif not before_today and after_today:\n            return self.date_between_dates(today, next_month_start)\n        elif not after_today and before_today:\n            return self.date_between_dates(this_month_start, today)\n        else:\n            return today",
        "rewrite": "def date_this_month(self, before_today=True, after_today=False):\n    today = date.today()\n    this_month_start = today.replace(day=1)\n    next_month_start = this_month_start + relativedelta.relativedelta(months=1)\n    \n    if before_today and after_today:\n        return self.date_between_dates(this_month_start, next_month_start)\n    elif not before_today and after_today:\n        return self.date_between_dates(today, next_month_start)\n    elif not after_today and before_today:\n        return self.date_between_dates(this_month_start, today)\n    else:\n        return today"
    },
    {
        "original": "def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    \"\"\" Update tree feature weights using decision path method.\n    \"\"\"\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff",
        "rewrite": "def _update_tree_feature_weights(X, feature_names, clf, feature_weights):\n    tree_value = clf.tree_.value\n    if tree_value.shape[1] == 1:\n        squeeze_axis = 1\n    else:\n        assert tree_value.shape[2] == 1\n        squeeze_axis = 2\n    tree_value = np.squeeze(tree_value, axis=squeeze_axis)\n    tree_feature = clf.tree_.feature\n    _, indices = clf.decision_path(X).nonzero()\n    if isinstance(clf, DecisionTreeClassifier):\n        norm = lambda x: x / x.sum()\n    else:\n        norm = lambda x: x\n    feature_weights[feature_names.bias_idx] += norm(tree_value[0])\n    for parent_idx, child_idx in zip(indices, indices[1:]):\n        assert tree_feature[parent_idx] >= 0\n        feature_idx = tree_feature[parent_idx]\n        diff = norm(tree_value[child_idx]) - norm(tree_value[parent_idx])\n        feature_weights[feature_idx] += diff"
    },
    {
        "original": "def get_team_push_restrictions(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/branches/:branch/protection/restrictions/teams <https://developer.github.com/v3/repos/branches>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Team.Team,\n            self._requester,\n            self.protection_url + \"/restrictions/teams\",\n            None\n        )",
        "rewrite": "def get_team_push_restrictions(self):\n    return github.PaginatedList.PaginatedList(\n        github.Team.Team,\n        self._requester,\n        self.protection_url + \"/restrictions/teams\",\n        None\n    )"
    },
    {
        "original": "def url(self, schemes=None):\n        \"\"\"\n        :param schemes: a list of strings to use as schemes, one will chosen randomly.\n        If None, it will generate http and https urls.\n        Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n        :returns: a random url string.\n        \"\"\"\n        if schemes is None:\n            schemes = ['http', 'https']\n\n        pattern = '{}://{}'.format(\n            self.random_element(schemes) if schemes else \"\",\n            self.random_element(self.url_formats),\n        )\n\n        return self.generator.parse(pattern)",
        "rewrite": "def url(self, schemes=None):\n    \"\"\"\n    :param schemes: a list of strings to use as schemes, one will be chosen randomly.\n    If None, it will generate http and https urls.\n    Passing an empty list will result in schemeless url generation like \"://domain.com\".\n\n    :returns: a random url string.\n    \"\"\"\n    if schemes is None:\n        schemes = ['http', 'https']\n\n    pattern = '{}://{}'.format(\n        self.random_element(schemes) if schemes else \"\",\n        self.random_element(self.url_formats),\n    )\n\n    return self.generator.parse(pattern)"
    },
    {
        "original": "def get_data():\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame\n\n\t\tI.e.,\n\t\t>>> convention_df.iloc[0]\n\t\tcategory                                                    plot\n\t\tfilename                 subjectivity_html/obj/2002/Abandon.html\n\t\ttext           A senior at an elite college (Katie Holmes), a...\n\t\tmovie_name                                               abandon\n\t\t\"\"\"\n\t\ttry:\n\t\t\tdata_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n\t\texcept:\n\t\t\turl = ROTTEN_TOMATOES_DATA_URL\n\t\t\tdata_stream = urlopen(url).read()\n\t\treturn pd.read_csv(io.BytesIO(bz2.decompress(data_stream)))",
        "rewrite": "def get_data():\n    \"\"\"\n    Returns\n    -------\n    pd.DataFrame\n\n    I.e.,\n    >>> convention_df.iloc[0]\n    category                                                    plot\n    filename                 subjectivity_html/obj/2002/Abandon.html\n    text           A senior at an elite college (Katie Holmes), a...\n    movie_name                                               abandon\n    \"\"\"\n    try:\n        data_stream = pkgutil.get_data('scattertext', 'data/rotten_tomatoes_corpus.csv.bz2')\n    except:\n        url = ROTTEN_TOMATOES_DATA_URL\n        data_stream = urlopen(url).read()\n    return pd.read_csv(io.BytesIO(bz2.decompress(data_stream)))"
    },
    {
        "original": "def add_reward_function(self):\n        \"\"\"\n        add reward function tag to pomdpx model\n\n        Return\n        ---------------\n        string containing the xml for reward function tag\n        \"\"\"\n        reward_function = self.model['reward_function']\n        for condition in reward_function:\n            condprob = etree.SubElement(self.reward_function, 'Func')\n            self.add_conditions(condition, condprob)\n        return self.__str__(self.reward_function)[:-1]",
        "rewrite": "def add_reward_function(self):\n        reward_function = self.model['reward_function']\n        for condition in reward_function:\n            condprob = etree.SubElement(self.reward_function, 'Func')\n            self.add_conditions(condition, condprob)\n        return self.__str__(self.reward_function)[:-1]"
    },
    {
        "original": "def get_all_values(self):\n        \"\"\"Returns a list of lists containing all cells' values as strings.\n\n        .. note::\n\n            Empty trailing rows and columns will not be included.\n        \"\"\"\n\n        data = self.spreadsheet.values_get(self.title)\n\n        try:\n            return fill_gaps(data['values'])\n        except KeyError:\n            return []",
        "rewrite": "def get_all_values(self):\n    data = self.spreadsheet.values_get(self.title)\n    \n    try:\n        return fill_gaps(data['values'])\n    except KeyError:\n        return []"
    },
    {
        "original": "def modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n        \"\"\"\n        \u4fee\u6539\u5c0f\u7a0b\u5e8f\u670d\u52a1\u5668\u6388\u6743\u57df\u540d\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1489138143_WPbOO\n\n        :param action: \u589e\u5220\u6539\u67e5\u7684\u64cd\u4f5c\u7c7b\u578b\uff0c\u4ec5\u652f\u6301 'add', 'delete', 'set', 'get'\n        :param request_domain: request \u5408\u6cd5\u57df\u540d\n        :param wsrequest_domain: socket \u5408\u6cd5\u57df\u540d\n        :param upload_domain: upload file \u5408\u6cd5\u57df\u540d\n        :param download_domain: download file \u5408\u6cd5\u57df\u540d\n        \"\"\"\n        return self._post(\n            'wxa/modify_domain',\n            data={\n                'action': action,\n                'requestdomain': request_domain,\n                'wsrequestdomain': wsrequest_domain,\n                'uploaddomain': upload_domain,\n                'downloaddomain': download_domain,\n            }\n        )",
        "rewrite": "def modify_domain(self, action, request_domain=(), wsrequest_domain=(), upload_domain=(), download_domain=()):\n    return self._post(\n        'wxa/modify_domain',\n        data={\n            'action': action,\n            'requestdomain': request_domain,\n            'wsrequestdomain': wsrequest_domain,\n            'uploaddomain': upload_domain,\n            'downloaddomain': download_domain,\n        }\n    )"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "rewrite": "def list_children(self, urn, limit=None, age=NEWEST_TIME):\n    _, children_urns = list(self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "rewrite": "def saveFile(self):\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving {}...\".format(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to {}!\".format(filepath))"
    },
    {
        "original": "def Copy(self, field_number=None):\n    \"\"\"Returns descriptor copy, optionally changing field number.\"\"\"\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n      new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)",
        "rewrite": "def copy(self, field_number=None):\n    new_args = self._kwargs.copy()\n    if field_number is not None:\n        new_args[\"field_number\"] = field_number\n\n    return ProtoRDFValue(\n        rdf_type=self.original_proto_type_name,\n        default=getattr(self, \"default\", None),\n        **new_args)"
    },
    {
        "original": "def _compute_labels(self, element, data, mapping):\n        \"\"\"\n        Computes labels for the nodes and adds it to the data.\n        \"\"\"\n        if element.vdims:\n            edges = Dataset(element)[element[element.vdims[0].name]>0]\n            nodes = list(np.unique([edges.dimension_values(i) for i in range(2)]))\n            nodes = element.nodes.select(**{element.nodes.kdims[2].name: nodes})\n        else:\n            nodes = element\n\n        label_dim = nodes.get_dimension(self.label_index)\n        labels = self.labels\n        if label_dim and labels:\n            if self.label_index not in [2, None]:\n                self.param.warning(\n                    \"Cannot declare style mapping for 'labels' option \"\n                    \"and declare a label_index; ignoring the label_index.\")\n        elif label_dim:\n            labels = label_dim\n        if isinstance(labels, basestring):\n            labels = element.nodes.get_dimension(labels)\n\n        if labels is None:\n            text = []\n        if isinstance(labels, dim):\n            text = labels.apply(element, flat=True)\n        else:\n            text = element.nodes.dimension_values(labels)\n            text = [labels.pprint_value(v) for v in text]\n\n        value_dim = element.vdims[0]\n        text_labels = []\n        for i, node in enumerate(element._sankey['nodes']):\n            if len(text):\n                label = text[i]\n            else:\n                label = ''\n            if self.show_values:\n                value = value_dim.pprint_value(node['value'])\n                if label:\n                    label = '%s - %s' % (label, value)\n                else:\n                    label = value\n            if value_dim.unit:\n                label += ' %s' % value_dim.unit\n            if label:\n                text_labels.append(label)\n\n        ys = nodes.dimension_values(1)\n        nodes = element._sankey['nodes']\n        if nodes:\n            offset = (nodes[0]['x1']-nodes[0]['x0'])/4.\n        else:\n            offset = 0\n        if self.label_position == 'right':\n            xs = np.array([node['x1'] for node in nodes])+offset\n        else:\n            xs = np.array([node['x0'] for node in nodes])-offset\n        data['text_1'] = dict(x=xs, y=ys, text=[str(l) for l in text_labels])\n        align = 'left' if self.label_position == 'right' else 'right'\n        mapping['text_1'] = dict(text='text', x='x', y='y', text_baseline='middle', text_align=align)",
        "rewrite": "def _compute_labels(self, element, data, mapping):\n    if element.vdims:\n        edges = Dataset(element)[element[element.vdims[0].name] > 0]\n        nodes = list(np.unique([edges.dimension_values(i) for i in range(2)]))\n        nodes = element.nodes.select(**{element.nodes.kdims[2].name: nodes})\n    else:\n        nodes = element\n\n    label_dim = nodes.get_dimension(self.label_index)\n    labels = self.labels\n\n    if isinstance(labels, str):\n        labels = element.nodes.get_dimension(labels)\n\n    if labels is None:\n        text = []\n    if isinstance(labels, dim):\n        text = labels.apply(element, flat=True)\n    else:\n        text = element.nodes.dimension_values(labels)\n        text = [labels.pprint_value(v) for v in text]\n\n    value_dim = element.vdims[0]\n    text_labels = []\n    \n    for i, node in enumerate(element._sankey['nodes']):\n        if len(text):\n            label = text[i]\n        else:\n            label = ''\n        if self.show_values:\n            value = value_dim.pprint_value(node['value'])\n            if label:\n                label = '%s - %s' % (label, value)\n            else:\n                label = value\n        if value_dim.unit:\n            label += ' %s' % value_dim.unit\n        if label:\n            text_labels.append(label)\n\n    ys = nodes.dimension_values(1)\n    nodes = element._sankey['nodes']\n    \n    if nodes:\n        offset = (nodes[0]['x1'] - nodes[0]['x0']) / 4.\n    else:\n        offset = 0\n    if self.label_position == 'right':\n        xs = np.array([node['x1'] for node in nodes]) + offset\n    else:\n        xs = np.array([node['x0'] for node in nodes]) - offset\n    data['text_1'] = dict(x=xs, y=ys, text=[str(l) for l in text_labels])\n    \n    align = 'left' if self.label_position == 'right' else 'right'\n    mapping['text_1'] = dict(text='text', x='x', y='y', text_baseline='middle', text_align=align)"
    },
    {
        "original": "def load_cli_config(args):\n    \"\"\"Modifies ARGS in-place to have the attributes defined in the CLI\n    config file if it doesn't already have them. Certain default\n    values are given if they are not in ARGS or the config file.\n    \"\"\"\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key in args and getattr(args, key) is not None:\n                pass\n            else:\n                setattr(args, key, val)",
        "rewrite": "def load_cli_config(args):\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key not in args or getattr(args, key) is None:\n                setattr(args, key, val)"
    },
    {
        "original": "async def sendAudio(self, chat_id, audio,\n                        caption=None,\n                        parse_mode=None,\n                        duration=None,\n                        performer=None,\n                        title=None,\n                        disable_notification=None,\n                        reply_to_message_id=None,\n                        reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendaudio\n\n        :param audio: Same as ``photo`` in :meth:`telepot.aio.Bot.sendPhoto`\n        \"\"\"\n        p = _strip(locals(), more=['audio'])\n        return await self._api_request_with_file('sendAudio', _rectify(p), 'audio', audio)",
        "rewrite": "async def sendAudio(self, chat_id, audio,\n                        caption=None,\n                        parse_mode=None,\n                        duration=None,\n                        performer=None,\n                        title=None,\n                        disable_notification=None,\n                        reply_to_message_id=None,\n                        reply_markup=None):\n        p = _strip(locals(), more=['audio'])\n        return await self._api_request_with_file('sendAudio', _rectify(p), 'audio', audio)"
    },
    {
        "original": "def list_datastores_full(service_instance):\n    \"\"\"\n    Returns a list of datastores associated with a given service instance.\n    The list contains basic information about the datastore:\n        name, type, url, capacity, free, used, usage, hosts\n\n    service_instance\n        The Service Instance Object from which to obtain datastores.\n    \"\"\"\n    datastores_list = list_objects(service_instance, vim.Datastore)\n\n    datastores = {}\n    for datastore in datastores_list:\n        datastores[datastore] = list_datastore_full(service_instance, datastore)\n\n    return datastores",
        "rewrite": "def list_datastores_full(service_instance):\n    datastores_list = list_objects(service_instance, vim.Datastore)\n    \n    datastores = {}\n    for datastore in datastores_list:\n        datastores[datastore] = list_datastore_full(service_instance, datastore)\n    \n    return datastores"
    },
    {
        "original": "def factorize(cls, pq):\n        \"\"\"\n        Factorizes the given large integer.\n\n        :param pq: the prime pair pq.\n        :return: a tuple containing the two factors p and q.\n        \"\"\"\n        if pq % 2 == 0:\n            return 2, pq // 2\n\n        y, c, m = randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1)\n        g = r = q = 1\n        x = ys = 0\n\n        while g == 1:\n            x = y\n            for i in range(r):\n                y = (pow(y, 2, pq) + c) % pq\n\n            k = 0\n            while k < r and g == 1:\n                ys = y\n                for i in range(min(m, r - k)):\n                    y = (pow(y, 2, pq) + c) % pq\n                    q = q * (abs(x - y)) % pq\n\n                g = cls.gcd(q, pq)\n                k += m\n\n            r *= 2\n\n        if g == pq:\n            while True:\n                ys = (pow(ys, 2, pq) + c) % pq\n                g = cls.gcd(abs(x - ys), pq)\n                if g > 1:\n                    break\n\n        p, q = g, pq // g\n        return (p, q) if p < q else (q, p)",
        "rewrite": "def factorize(cls, pq):\n    if pq % 2 == 0:\n        return 2, pq // 2\n\n    y, c, m = randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1)\n    g = r = q = 1\n    x = ys = 0\n\n    while g == 1:\n        x = y\n        for i in range(r):\n            y = (pow(y, 2, pq) + c) % pq\n\n        k = 0\n        while k < r and g == 1:\n            ys = y\n            for i in range(min(m, r - k)):\n                y = (pow(y, 2, pq) + c) % pq\n                q = q * (abs(x - y)) % pq\n\n            g = cls.gcd(q, pq)\n            k += m\n\n        r *= 2\n\n    if g == pq:\n        while True:\n            ys = (pow(ys, 2, pq) + c) % pq\n            g = cls.gcd(abs(x - ys), pq)\n            if g > 1:\n                break\n\n    p, q = g, pq // g\n    return (p, q) if p < q else (q, p)"
    },
    {
        "original": "def splits(\n        cls,\n        conn_str,\n        candidate_def,\n        word_dict=None,\n        train=0,\n        dev=1,\n        test=2,\n        use_lfs=(0, 0, 0),\n        pretrained_word_dict=None,\n        max_seq_len=125,\n    ):\n        \"\"\"\n        Create train/dev/test splits (mapped to split numbers)\n\n        :param conn_str:\n        :param candidate_def:\n        :param word_dict:\n        :param train:\n        :param dev:\n        :param test:\n        :param use_lfs:\n        :param pretrained_word_dict:\n        :param max_seq_len:\n        :return:\n\n        \"\"\"\n        # initialize word_dict if needed\n        train_set = cls(\n            conn_str,\n            candidate_def,\n            word_dict=word_dict,\n            split=train,\n            use_lfs=use_lfs[train],\n            pretrained_word_dict=pretrained_word_dict,\n            max_seq_len=max_seq_len,\n        )\n        return (\n            train_set,\n            cls(\n                conn_str,\n                candidate_def,\n                word_dict=train_set.word_dict,\n                split=dev,\n                use_lfs=use_lfs[dev],\n                max_seq_len=max_seq_len,\n            ),\n            cls(\n                conn_str,\n                candidate_def,\n                word_dict=train_set.word_dict,\n                split=test,\n                use_lfs=use_lfs[test],\n                max_seq_len=max_seq_len,\n            ),\n        )",
        "rewrite": "def splits(cls, conn_str, candidate_def, word_dict=None, train=0, dev=1, test=2, use_lfs=(0, 0, 0), pretrained_word_dict=None, max_seq_len=125):\n    train_set = cls(conn_str, candidate_def, word_dict=word_dict, split=train, use_lfs=use_lfs[train], pretrained_word_dict=pretrained_word_dict, max_seq_len=max_seq_len)\n    return (train_set, cls(conn_str, candidate_def, word_dict=train_set.word_dict, split=dev, use_lfs=use_lfs[dev], max_seq_len=max_seq_len), cls(conn_str, candidate_def, word_dict=train_set.word_dict, split=test, use_lfs=use_lfs[test], max_seq_len=max_seq_len))"
    },
    {
        "original": "def upgrade(reboot=False, at_time=None):\n    \"\"\"\n    Upgrade the kernel and optionally reboot the system.\n\n    reboot : False\n        Request a reboot if a new kernel is available.\n\n    at_time : immediate\n        Schedule the reboot at some point in the future. This argument\n        is ignored if ``reboot=False``. See\n        :py:func:`~salt.modules.system.reboot` for more details\n        on this argument.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' kernelpkg.upgrade\n        salt '*' kernelpkg.upgrade reboot=True at_time=1\n\n    .. note::\n        An immediate reboot often shuts down the system before the minion has a\n        chance to return, resulting in errors. A minimal delay (1 minute) is\n        useful to ensure the result is delivered to the master.\n    \"\"\"\n    result = __salt__['pkg.upgrade'](name=_package_name())\n    _needs_reboot = needs_reboot()\n\n    ret = {\n        'upgrades': result,\n        'active': active(),\n        'latest_installed': latest_installed(),\n        'reboot_requested': reboot,\n        'reboot_required': _needs_reboot\n    }\n\n    if reboot and _needs_reboot:\n        log.warning('Rebooting system due to kernel upgrade')\n        __salt__['system.reboot'](at_time=at_time)\n\n    return ret",
        "rewrite": "def upgrade(reboot=False, at_time=None):\n    result = __salt__['pkg.upgrade'](name=_package_name())\n    _needs_reboot = needs_reboot()\n\n    ret = {\n        'upgrades': result,\n        'active': active(),\n        'latest_installed': latest_installed(),\n        'reboot_requested': reboot,\n        'reboot_required': _needs_reboot\n    }\n\n    if reboot and _needs_reboot:\n        log.warning('Rebooting system due to kernel upgrade')\n        __salt__['system.reboot'](at_time=at_time)\n\n    return ret"
    },
    {
        "original": "def bitstring_probs_to_z_moments(p):\n    \"\"\"\n    Convert between bitstring probabilities and joint Z moment expectations.\n\n    :param np.array p: An array that enumerates bitstring probabilities. When\n        flattened out ``p = [p_00...0, p_00...1, ...,p_11...1]``. The total number of elements must\n        therefore be a power of 2. The canonical shape has a separate axis for each qubit, such that\n        ``p[i,j,...,k]`` gives the estimated probability of bitstring ``ij...k``.\n    :return: ``z_moments``, an np.array with one length-2 axis per qubit which contains the\n        expectations of all monomials in ``{I, Z_0, Z_1, ..., Z_{n-1}}``. The expectations of each\n        monomial can be accessed via::\n\n            <Z_0^j_0 Z_1^j_1 ... Z_m^j_m> = z_moments[j_0,j_1,...,j_m]\n\n    :rtype: np.array\n    \"\"\"\n    zmat = np.array([[1, 1],\n                     [1, -1]])\n    return _apply_local_transforms(p, (zmat for _ in range(p.ndim)))",
        "rewrite": "def bitstring_probs_to_z_moments(p):\n    zmat = np.array([[1, 1], [1, -1]])\n    return _apply_local_transforms(p, (zmat for _ in range(p.ndim)))"
    },
    {
        "original": "def loadavg():\n    \"\"\"\n    Return the load averages for this minion\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.loadavg\n\n        :raises CommandExecutionError: If the system cannot report loadaverages to Python\n    \"\"\"\n    if __grains__['kernel'] == 'AIX':\n        return _aix_loadavg()\n\n    try:\n        load_avg = os.getloadavg()\n    except AttributeError:\n        # Some UNIX-based operating systems do not have os.getloadavg()\n        raise salt.exceptions.CommandExecutionError('status.loadavag is not available on your platform')\n    return {'1-min': load_avg[0],\n            '5-min': load_avg[1],\n            '15-min': load_avg[2]}",
        "rewrite": "import os\nimport salt.exceptions\n\ndef loadavg():\n    if __grains__['kernel'] == 'AIX':\n        return _aix_loadavg()\n\n    try:\n        load_avg = os.getloadavg()\n    except AttributeError:\n        raise salt.exceptions.CommandExecutionError('status.loadavag is not available on your platform')\n    \n    return {'1-min': load_avg[0],\n            '5-min': load_avg[1],\n            '15-min': load_avg[2]}"
    },
    {
        "original": "def _parse_snapshot_description(vm_snapshot, unix_time=False):\n    \"\"\"\n    Parse XML doc and return a dict with the status values.\n\n    :param xmldoc:\n    :return:\n    \"\"\"\n    ret = dict()\n    tree = ElementTree.fromstring(vm_snapshot.getXMLDesc())\n    for node in tree:\n        if node.tag == 'name':\n            ret['name'] = node.text\n        elif node.tag == 'creationTime':\n            ret['created'] = datetime.datetime.fromtimestamp(float(node.text)).isoformat(' ') \\\n                                if not unix_time else float(node.text)\n        elif node.tag == 'state':\n            ret['running'] = node.text == 'running'\n\n    ret['current'] = vm_snapshot.isCurrent() == 1\n\n    return ret",
        "rewrite": "def _parse_snapshot_description(vm_snapshot, unix_time=False):\n    ret = dict()\n    tree = ElementTree.fromstring(vm_snapshot.getXMLDesc())\n    for node in tree:\n        if node.tag == 'name':\n            ret['name'] = node.text\n        elif node.tag == 'creationTime':\n            ret['created'] = datetime.datetime.fromtimestamp(float(node.text)).isoformat(' ') \\\n                                if not unix_time else float(node.text)\n        elif node.tag == 'state':\n            ret['running'] = node.text == 'running'\n\n    ret['current'] = vm_snapshot.isCurrent() == 1\n\n    return ret"
    },
    {
        "original": "def _instantiate_layers(self):\n    \"\"\"Instantiates all the linear modules used in the network.\n\n    Layers are instantiated in the constructor, as opposed to the build\n    function, because MLP implements the Transposable interface, and the\n    transpose function can be called before the module is actually connected\n    to the graph and build is called.\n\n    Notice that this is safe since layers in the transposed module are\n    instantiated using a lambda returning input_size of the mlp layers, and\n    this doesn't have to return sensible values until the original module is\n    connected to the graph.\n    \"\"\"\n\n    # Here we are entering the module's variable scope to name our submodules\n    # correctly (not to create variables). As such it's safe to not check\n    # whether we're in the same graph. This is important if we're constructing\n    # the module in one graph and connecting it in another (e.g. with `defun`\n    # the module is created in some default graph, and connected to a capturing\n    # graph in order to turn it into a graph function).\n    with self._enter_variable_scope(check_same_graph=False):\n      self._layers = [basic.Linear(self._output_sizes[i],\n                                   name=\"linear_{}\".format(i),\n                                   initializers=self._initializers,\n                                   partitioners=self._partitioners,\n                                   regularizers=self._regularizers,\n                                   use_bias=self.use_bias)\n                      for i in xrange(self._num_layers)]",
        "rewrite": "def _instantiate_layers(self):\n    with self._enter_variable_scope(check_same_graph=False):\n        self._layers = [basic.Linear(self._output_sizes[i],\n                                     name=\"linear_{}\".format(i),\n                                     initializers=self._initializers,\n                                     partitioners=self._partitioners,\n                                     regularizers=self._regularizers,\n                                     use_bias=self.use_bias)\n                        for i in range(self._num_layers)]"
    },
    {
        "original": "def get_filename_by_class(self, current_class):\n        \"\"\"\n        Returns the filename of the DEX file where the class is in.\n\n        Returns the first filename this class was present.\n        For example, if you analyzed an APK, this should return the filename of\n        the APK and not of the DEX file.\n\n        :param current_class: ClassDefItem\n        :returns: None if class was not found or the filename\n        \"\"\"\n        for digest, dx in self.analyzed_vms.items():\n            if dx.is_class_present(current_class.get_name()):\n                return self.analyzed_digest[digest]\n        return None",
        "rewrite": "def get_filename_by_class(self, current_class):\n    for digest, dx in self.analyzed_vms.items():\n        if dx.is_class_present(current_class.get_name()):\n            return self.analyzed_digest[digest]\n    return None"
    },
    {
        "original": "def _add_link(self, edge):\n        \"\"\"\n        Adds an edge to the ProbModelXML.\n        \"\"\"\n        edge_data = self.data['probnet']['edges'][edge]\n        if isinstance(edge, six.string_types):\n            edge = eval(edge)\n        link = etree.SubElement(self.links, 'Link', attrib={'var1': edge[0], 'var2': edge[1],\n                                                            'directed': edge_data['directed']})\n        try:\n            etree.SubElement(link, 'Comment').text = edge_data['Comment']\n        except KeyError:\n            pass\n        try:\n            etree.SubElement(link, 'Label').text = edge_data['Label']\n        except KeyError:\n            pass\n        try:\n            self._add_additional_properties(link, edge_data['AdditionalProperties'])\n        except KeyError:\n            etree.SubElement(link, 'AdditionalProperties')",
        "rewrite": "def _add_link(self, edge):\n    edge_data = self.data['probnet']['edges'][edge]\n    if isinstance(edge, six.string_types):\n        edge = eval(edge)\n    link = etree.SubElement(self.links, 'Link', attrib={'var1': edge[0], 'var2': edge[1], 'directed': edge_data['directed']})\n    \n    try:\n        etree.SubElement(link, 'Comment').text = edge_data.get('Comment')\n    except KeyError:\n        pass\n    \n    try:\n        etree.SubElement(link, 'Label').text = edge_data.get('Label')\n    except KeyError:\n        pass\n    \n    try:\n        self._add_additional_properties(link, edge_data.get('AdditionalProperties'))\n    except KeyError:\n        etree.SubElement(link, 'AdditionalProperties')"
    },
    {
        "original": "def get_edges(self):\n        \"\"\"\n        Returns the edges of the network\n\n        Examples\n        --------\n        >>> reader = XMLBIF.XMLBIFReader(\"xmlbif_test.xml\")\n        >>> reader.get_edges()\n        [['family-out', 'light-on'],\n         ['family-out', 'dog-out'],\n         ['bowel-problem', 'dog-out'],\n         ['dog-out', 'hear-bark']]\n        \"\"\"\n        edge_list = [[value, key] for key in self.variable_parents\n                     for value in self.variable_parents[key]]\n        return edge_list",
        "rewrite": "def get_edges(self):\n    edge_list = [[value, key] for key in self.variable_parents for value in self.variable_parents[key]]\n    return edge_list"
    },
    {
        "original": "def copy(self):\n        \"\"\"\n        Returns a copy of the state.\n        \"\"\"\n\n        if self._global_condition is not None:\n            raise SimStateError(\"global condition was not cleared before state.copy().\")\n\n        c_plugins = self._copy_plugins()\n        state = SimState(project=self.project, arch=self.arch, plugins=c_plugins, options=self.options.copy(),\n                         mode=self.mode, os_name=self.os_name)\n\n        if self._is_java_jni_project:\n            state.ip_is_soot_addr = self.ip_is_soot_addr\n\n        state.uninitialized_access_handler = self.uninitialized_access_handler\n        state._special_memory_filler = self._special_memory_filler\n        state.ip_constraints = self.ip_constraints\n\n        return state",
        "rewrite": "def copy(self):\n    if self._global_condition is not None:\n        raise SimStateError(\"global condition was not cleared before state.copy().\")\n\n    c_plugins = self._copy_plugins()\n    state = SimState(project=self.project, arch=self.arch, plugins=c_plugins, options=self.options.copy(),\n                     mode=self.mode, os_name=self.os_name)\n\n    if self._is_java_jni_project:\n        state.ip_is_soot_addr = self.ip_is_soot_addr\n\n    state.uninitialized_access_handler = self.uninitialized_access_handler\n    state._special_memory_filler = self._special_memory_filler\n    state.ip_constraints = self.ip_constraints\n\n    return state"
    },
    {
        "original": "def distribute_aars(self, arch):\n        \"\"\"Process existing .aar bundles and copy to current dist dir.\"\"\"\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)",
        "rewrite": "def distribute_aars(self, arch):\n    \"\"\"Process existing .aar bundles and copy to current dist dir.\"\"\"\n    info('Unpacking aars')\n    for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n        self._unpack_aar(aar, arch)"
    },
    {
        "original": "def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns\n        None.\n\n        :return: None\n        \"\"\"\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n\n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)",
        "rewrite": "def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns None.\n    \n        :return: None\n        \"\"\"\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n    \n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)"
    },
    {
        "original": "def renderer_doc(*args):\n    \"\"\"\n    Return the docstrings for all renderers. Optionally, specify a renderer or a\n    function to narrow the selection.\n\n    The strings are aggregated into a single document on the master for easy\n    reading.\n\n    Multiple renderers can be specified.\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc\n        salt '*' sys.renderer_doc cheetah\n        salt '*' sys.renderer_doc jinja json\n\n    Renderer names can be specified as globs.\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc 'c*' 'j*'\n\n    \"\"\"\n    renderers_ = salt.loader.render(__opts__, [])\n    docs = {}\n    if not args:\n        for func in six.iterkeys(renderers_):\n            docs[func] = renderers_[func].__doc__\n        return _strip_rst(docs)\n\n    for module in args:\n        if '*' in module or '.' in module:\n            for func in fnmatch.filter(renderers_, module):\n                docs[func] = renderers_[func].__doc__\n        else:\n            moduledot = module + '.'\n            for func in six.iterkeys(renderers_):\n                if func.startswith(moduledot):\n                    docs[func] = renderers_[func].__doc__\n    return _strip_rst(docs)",
        "rewrite": "def renderer_doc(*args):\n    renderers_ = salt.loader.render(__opts__, [])\n    docs = {}\n    if not args:\n        for func in six.iterkeys(renderers_):\n            docs[func] = renderers_[func].__doc__\n        return _strip_rst(docs)\n\n    for module in args:\n        if '*' in module or '.' in module:\n            for func in fnmatch.filter(renderers_, module):\n                docs[func] = renderers_[func].__doc__\n        else:\n            moduledot = module + '.'\n            for func in six.iterkeys(renderers_):\n                if func.startswith(moduledot):\n                    docs[func] = renderers_[func].__doc__\n    return _strip_rst(docs)"
    },
    {
        "original": "def alter_edge(self, from_index, to_index,\n                   new_weight=None, new_edge_properties=None):\n        \"\"\"\n        Alters either the weight or the edge_properties of\n        an edge in the MoleculeGraph.\n\n        :param from_index: int\n        :param to_index: int\n        :param new_weight: alter_edge does not require\n        that weight be altered. As such, by default, this\n        is None. If weight is to be changed, it should be a\n        float.\n        :param new_edge_properties: alter_edge does not require\n        that edge_properties be altered. As such, by default,\n        this is None. If any edge properties are to be changed,\n        it should be a dictionary of edge properties to be changed.\n        :return:\n        \"\"\"\n\n        existing_edge = self.graph.get_edge_data(from_index, to_index)\n\n        # ensure that edge exists before attempting to change it\n        if not existing_edge:\n            raise ValueError(\"Edge between {} and {} cannot be altered;\\\n                                no edge exists between those sites.\".format(\n                                from_index, to_index\n                                ))\n\n        # Third index should always be 0 because there should only be one edge between any two nodes\n        if new_weight is not None:\n            self.graph[from_index][to_index][0]['weight'] = new_weight\n\n        if new_edge_properties is not None:\n            for prop in list(new_edge_properties.keys()):\n                self.graph[from_index][to_index][0][prop] = new_edge_properties[prop]",
        "rewrite": "def alter_edge(self, from_index, to_index, new_weight=None, new_edge_properties=None):\n    existing_edge = self.graph.get_edge_data(from_index, to_index)\n    \n    if not existing_edge:\n        raise ValueError(\"Edge between {} and {} cannot be altered; no edge exists between those sites.\".format(from_index, to_index))\n        \n    if new_weight is not None:\n        self.graph[from_index][to_index][0]['weight'] = new_weight\n        \n    if new_edge_properties is not None:\n        for prop in list(new_edge_properties.keys()):\n            self.graph[from_index][to_index][0][prop] = new_edge_properties[prop]"
    },
    {
        "original": "def from_file(filename, file_format=\"xyz\"):\n        \"\"\"\n        Uses OpenBabel to read a molecule from a file in all supported formats.\n\n        Args:\n            filename: Filename of input file\n            file_format: String specifying any OpenBabel supported formats.\n\n        Returns:\n            BabelMolAdaptor object\n        \"\"\"\n        mols = list(pb.readfile(str(file_format), str(filename)))\n        return BabelMolAdaptor(mols[0].OBMol)",
        "rewrite": "def from_file(filename, file_format=\"xyz\"):\n    \"\"\"\n    Uses OpenBabel to read a molecule from a file in all supported formats.\n\n    Args:\n        filename: Filename of input file\n        file_format: String specifying any OpenBabel supported formats.\n\n    Returns:\n        BabelMolAdaptor object\n    \"\"\"\n    mols = list(pb.readfile(str(file_format), str(filename)))\n    return BabelMolAdaptor(mols[0].OBMol)"
    },
    {
        "original": "def on_all_ok(self):\n        \"\"\"\n        This method is called when all tasks reach S_OK\n        Ir runs `mrgddb` in sequential on the local machine to produce\n        the final DDB file in the outdir of the `Work`.\n        \"\"\"\n        # Merge DDB files.\n        out_ddb = self.merge_ddb_files()\n        return self.Results(node=self, returncode=0, message=\"DDB merge done\")",
        "rewrite": "def on_all_ok(self):\n    out_ddb = self.merge_ddb_files()\n    return self.Results(node=self, returncode=0, message=\"DDB merge done\")"
    },
    {
        "original": "def create_event(self, last_state, state, clean_server_name, replset_name):\n        \"\"\"Create an event with a message describing the replication\n            state of a mongo node\"\"\"\n\n        status = self.get_state_description(state)\n        short_status = self.get_state_name(state)\n        last_short_status = self.get_state_name(last_state)\n        hostname = self.hostname_for_event(clean_server_name)\n        msg_title = \"%s is %s for %s\" % (hostname, short_status, replset_name)\n        msg = \"MongoDB %s (%s) just reported as %s (%s) for %s; it was %s before.\"\n        msg = msg % (hostname, clean_server_name, status, short_status, replset_name, last_short_status)\n\n        self.event(\n            {\n                'timestamp': int(time.time()),\n                'source_type_name': self.SOURCE_TYPE_NAME,\n                'msg_title': msg_title,\n                'msg_text': msg,\n                'host': hostname,\n                'tags': [\n                    'action:mongo_replset_member_status_change',\n                    'member_status:' + short_status,\n                    'previous_member_status:' + last_short_status,\n                    'replset:' + replset_name,\n                ],\n            }\n        )",
        "rewrite": "def create_event(self, last_state, state, clean_server_name, replset_name):\n    status = self.get_state_description(state)\n    short_status = self.get_state_name(state)\n    last_short_status = self.get_state_name(last_state)\n    hostname = self.hostname_for_event(clean_server_name)\n    msg_title = \"%s is %s for %s\" % (hostname, short_status, replset_name)\n    msg = \"MongoDB %s (%s) just reported as %s (%s) for %s; it was %s before.\" % (hostname, clean_server_name, status, short_status, replset_name, last_short_status)\n\n    self.event(\n        {\n            'timestamp': int(time.time()),\n            'source_type_name': self.SOURCE_TYPE_NAME,\n            'msg_title': msg_title,\n            'msg_text': msg,\n            'host': hostname,\n            'tags': [\n                'action:mongo_replset_member_status_change',\n                'member_status:' + short_status,\n                'previous_member_status:' + last_short_status,\n                'replset:' + replset_name,\n            ],\n        }\n   )"
    },
    {
        "original": "def finger(match, hash_type=None):\n    \"\"\"\n    Return the matching key fingerprints. Returns a dictionary.\n\n    match\n        The key for with to retrieve the fingerprint.\n\n    hash_type\n        The hash algorithm used to calculate the fingerprint\n\n    .. code-block:: python\n\n        >>> wheel.cmd('key.finger', ['minion1'])\n        {'minions': {'minion1': '5d:f6:79:43:5e:d4:42:3f:57:b8:45:a8:7e:a4:6e:ca'}}\n\n    \"\"\"\n    if hash_type is None:\n        hash_type = __opts__['hash_type']\n\n    skey = get_key(__opts__)\n    return skey.finger(match, hash_type)",
        "rewrite": "def finger(match, hash_type=None):\n    if hash_type is None:\n        hash_type = __opts__['hash_type']\n    \n    skey = get_key(__opts__)\n    return skey.finger(match, hash_type)"
    },
    {
        "original": "def serialize_for_reading(element):\n    \"\"\"\n    Serialize *element* to human-readable XML suitable for tests. No XML\n    declaration.\n    \"\"\"\n    xml = etree.tostring(element, encoding='unicode', pretty_print=True)\n    return XmlString(xml)",
        "rewrite": "```\ndef serialize_for_reading(element):\n    xml = etree.tostring(element, encoding='unicode', pretty_print=True)\n    return XmlString(xml)\n```"
    },
    {
        "original": "def __write_aliases_file(lines):\n    \"\"\"\n    Write a new copy of the aliases file.  Lines is a list of lines\n    as returned by __parse_aliases.\n    \"\"\"\n    afn = __get_aliases_filename()\n    adir = os.path.dirname(afn)\n\n    out = tempfile.NamedTemporaryFile(dir=adir, delete=False)\n\n    if not __opts__.get('integration.test', False):\n        if os.path.isfile(afn):\n            afn_st = os.stat(afn)\n            os.chmod(out.name, stat.S_IMODE(afn_st.st_mode))\n            os.chown(out.name, afn_st.st_uid, afn_st.st_gid)\n        else:\n            os.chmod(out.name, 0o644)\n            os.chown(out.name, 0, 0)\n\n    for (line_alias, line_target, line_comment) in lines:\n        if isinstance(line_target, list):\n            line_target = ', '.join(line_target)\n        if not line_comment:\n            line_comment = ''\n        if line_alias and line_target:\n            write_line = '{0}: {1}{2}\\n'.format(\n                line_alias, line_target, line_comment\n            )\n        else:\n            write_line = '{0}\\n'.format(line_comment)\n        if six.PY3:\n            write_line = write_line.encode(__salt_system_encoding__)\n        out.write(write_line)\n\n    out.close()\n    os.rename(out.name, afn)\n\n    # Search $PATH for the newalises command\n    newaliases = salt.utils.path.which('newaliases')\n    if newaliases is not None:\n        __salt__['cmd.run'](newaliases)\n\n    return True",
        "rewrite": "def write_aliases_file(lines):\n    afn = get_aliases_filename()\n    adir = os.path.dirname(afn)\n\n    out = tempfile.NamedTemporaryFile(dir=adir, delete=False)\n\n    if not opts.get('integration.test', False):\n        if os.path.isfile(afn):\n            afn_st = os.stat(afn)\n            os.chmod(out.name, stat.S_IMODE(afn_st.st_mode))\n            os.chown(out.name, afn_st.st_uid, afn_st.st_gid)\n        else:\n            os.chmod(out.name, 0o644)\n            os.chown(out.name, 0, 0)\n\n    for line_alias, line_target, line_comment in lines:\n        if isinstance(line_target, list):\n            line_target = ', '.join(line_target)\n        if not line_comment:\n            line_comment = ''\n        write_line = f\"{line_alias}: {line_target}{line_comment}\\n\" if line_alias and line_target else f\"{line_comment}\\n\"\n        if six.PY3:\n            write_line = write_line.encode(salt_system_encoding)\n        out.write(write_line)\n\n    out.close()\n    os.rename(out.name, afn)\n\n    newaliases = salt.utils.path.which('newaliases')\n    if newaliases is not None:\n        cmd.run(newaliases)\n\n    return True"
    },
    {
        "original": "def eval_to_ast(self, e, n, extra_constraints=(), exact=None):\n        \"\"\"\n        Evaluate an expression, using the solver if necessary. Returns AST objects.\n\n        :param e: the expression\n        :param n: the number of desired solutions\n        :param extra_constraints: extra constraints to apply to the solver\n        :param exact: if False, returns approximate solutions\n        :return: a tuple of the solutions, in the form of claripy AST nodes\n        :rtype: tuple\n        \"\"\"\n        return self._solver.eval_to_ast(e, n, extra_constraints=self._adjust_constraint_list(extra_constraints), exact=exact)",
        "rewrite": "def eval_to_ast(self, e, n, extra_constraints=(), exact=None):\n    return self._solver.eval_to_ast(e, n, extra_constraints=self._adjust_constraint_list(extra_constraints), exact=exact)"
    },
    {
        "original": "def CountFlowResults(self, client_id, flow_id, with_tag=None, with_type=None):\n    \"\"\"Counts flow results of a given flow using given query options.\"\"\"\n    return len(\n        self.ReadFlowResults(\n            client_id,\n            flow_id,\n            0,\n            sys.maxsize,\n            with_tag=with_tag,\n            with_type=with_type))",
        "rewrite": "def count_flow_results(self, client_id, flow_id, with_tag=None, with_type=None):\n    return len(\n        self.read_flow_results(\n            client_id,\n            flow_id,\n            0,\n            sys.maxsize,\n            with_tag=with_tag,\n            with_type=with_type))"
    },
    {
        "original": "def _get_all_field_lines(self):\n        \"\"\"\n        Returns all lines that represent the fields of the layer (both their names and values).\n        \"\"\"\n        for field in self._get_all_fields_with_alternates():\n            # Change to yield from\n            for line in self._get_field_or_layer_repr(field):\n                yield line",
        "rewrite": "def _get_all_field_lines(self):\n    for field in self._get_all_fields_with_alternates():\n        for line in self._get_field_or_layer_repr(field):\n            yield line"
    },
    {
        "original": "def _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \"\"\"Fast Fourier transform-based Gaussian kernel density estimate (KDE).\n\n    The code was adapted from https://github.com/mfouesneau/faststats\n\n    Parameters\n    ----------\n    x : Numpy array or list\n    cumulative : bool\n        If true, estimate the cdf instead of the pdf\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    xmin : float\n        Manually set lower limit.\n    xmax : float\n        Manually set upper limit.\n\n    Returns\n    -------\n    density: A gridded 1D KDE of the input points (x)\n    xmin: minimum value of x\n    xmax: maximum value of x\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax",
        "rewrite": "import numpy as np\nimport warnings\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin is None) or (xmax is None) else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax"
    },
    {
        "original": "def uninstall(pkg,\n              user=None,\n              env=None):\n    \"\"\"\n    Uninstall a cabal package.\n\n    pkg\n        The package to uninstall\n    user\n        The user to run ghc-pkg unregister with\n    env\n        Environment variables to set when invoking cabal. Uses the\n        same ``env`` format as the :py:func:`cmd.run\n        <salt.modules.cmdmod.run>` execution function\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cabal.uninstall ShellCheck\n\n    \"\"\"\n    cmd = ['ghc-pkg unregister']\n    cmd.append('\"{0}\"'.format(pkg))\n\n    result = __salt__['cmd.run_all'](' '.join(cmd), runas=user, env=env)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(result['stderr'])\n\n    return result",
        "rewrite": "def uninstall(pkg, user=None, env=None):\n    cmd = ['ghc-pkg unregister']\n    cmd.append('\"{0}\"'.format(pkg))\n\n    result = __salt__['cmd.run_all'](' '.join(cmd), runas=user, env=env)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(result['stderr'])\n\n    return result"
    },
    {
        "original": "def api_request(request, *args, **kwargs):\n    \"\"\"\n    Create an API request.\n\n    `request_type` is the request type (string). This is used to look up a\n    plugin, whose request class is instantiated and passed the remaining\n    arguments passed to this function.\n    \"\"\"\n    plugin = pm.api_plugin_for_request(request)\n    if plugin and plugin.request_class:\n        req = plugin.request_class(*args, **kwargs)\n    else:\n        raise Exception(\"Invalid request type\")\n    return req",
        "rewrite": "def api_request(request, *args, **kwargs):\n    plugin = pm.api_plugin_for_request(request)\n    if plugin and plugin.request_class:\n        req = plugin.request_class(*args, **kwargs)\n    else:\n        raise Exception(\"Invalid request type\")\n    return req"
    },
    {
        "original": "def map_colors(arr, crange, cmap, hex=True):\n    \"\"\"\n    Maps an array of values to RGB hex strings, given\n    a color range and colormap.\n    \"\"\"\n    if isinstance(crange, np.ndarray):\n        xsorted = np.argsort(crange)\n        ypos = np.searchsorted(crange, arr)\n        arr = xsorted[ypos]\n    else:\n        if isinstance(crange, tuple):\n            cmin, cmax = crange\n        else:\n            cmin, cmax = np.nanmin(arr), np.nanmax(arr)\n        arr = (arr - cmin) / (cmax-cmin)\n        arr = np.ma.array(arr, mask=np.logical_not(np.isfinite(arr)))\n    arr = cmap(arr)\n    if hex:\n        return rgb2hex(arr)\n    else:\n        return arr",
        "rewrite": "import numpy as np\n\ndef map_colors(arr, crange, cmap, hex=True):\n    if isinstance(crange, np.ndarray):\n        xsorted = np.argsort(crange)\n        ypos = np.searchsorted(crange, arr)\n        arr = xsorted[ypos]\n    else:\n        if isinstance(crange, tuple):\n            cmin, cmax = crange\n        else:\n            cmin, cmax = np.nanmin(arr), np.nanmax(arr)\n        arr = (arr - cmin) / (cmax-cmin)\n        arr = np.ma.array(arr, mask=np.logical_not(np.isfinite(arr)))\n    arr = cmap(arr)\n    if hex:\n        return rgb2hex(arr)\n    else:\n        return arr"
    },
    {
        "original": "def _datetime64_index(self, recarr):\n        \"\"\" Given a np.recarray find the first datetime64 column \"\"\"\n        # TODO: Handle multi-indexes\n        names = recarr.dtype.names\n        for name in names:\n            if recarr[name].dtype == DTN64_DTYPE:\n                return name\n        return None",
        "rewrite": "def _datetime64_index(self, recarr):\n    names = recarr.dtype.names\n    for name in names:\n        if recarr[name].dtype == np.dtype('<M8[ns]'):\n            return name\n    return None"
    },
    {
        "original": "def decode(self, rel_codes, boxes):\n        \"\"\"\n        From a set of original boxes and encoded relative box offsets,\n        get the decoded boxes.\n\n        Arguments:\n            rel_codes (Tensor): encoded boxes\n            boxes (Tensor): reference boxes.\n        \"\"\"\n\n        boxes = boxes.to(rel_codes.dtype)\n\n        TO_REMOVE = 1  # TODO remove\n        widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE\n        heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = rel_codes[:, 0::4] / wx\n        dy = rel_codes[:, 1::4] / wy\n        dw = rel_codes[:, 2::4] / ww\n        dh = rel_codes[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(rel_codes)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2 (note: \"- 1\" is correct; don't be fooled by the asymmetry)\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n        # y2 (note: \"- 1\" is correct; don't be fooled by the asymmetry)\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n        return pred_boxes",
        "rewrite": "def decode(self, rel_codes, boxes):\n    boxes = boxes.to(rel_codes.dtype)\n\n    TO_REMOVE = 1\n    widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE\n    heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    wx, wy, ww, wh = self.weights\n    dx = rel_codes[:, 0::4] / wx\n    dy = rel_codes[:, 1::4] / wy\n    dw = rel_codes[:, 2::4] / ww\n    dh = rel_codes[:, 3::4] / wh\n\n    dw = torch.clamp(dw, max=self.bbox_xform_clip)\n    dh = torch.clamp(dh, max=self.bbox_xform_clip)\n\n    pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n    pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n    pred_w = torch.exp(dw) * widths[:, None]\n    pred_h = torch.exp(dh) * heights[:, None]\n\n    pred_boxes = torch.zeros_like(rel_codes)\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n    return pred_boxes"
    },
    {
        "original": "def _dump_additional_attributes(additional_attributes):\n    \"\"\" try to parse additional attributes, but ends up to hexdump if the scheme is unknown \"\"\"\n\n    attributes_raw = io.BytesIO(additional_attributes)\n    attributes_hex = binascii.hexlify(additional_attributes)\n\n    if not len(additional_attributes):\n        return attributes_hex\n\n    len_attribute, = unpack('<I', attributes_raw.read(4))\n    if len_attribute != 8:\n        return attributes_hex\n\n    attr_id, = unpack('<I', attributes_raw.read(4))\n    if attr_id != APK._APK_SIG_ATTR_V2_STRIPPING_PROTECTION:\n        return attributes_hex\n        \n    scheme_id, = unpack('<I', attributes_raw.read(4))\n\n    return \"stripping protection set, scheme %d\" % scheme_id",
        "rewrite": "def _dump_additional_attributes(additional_attributes):\n    attributes_raw = io.BytesIO(additional_attributes)\n    attributes_hex = binascii.hexlify(additional_attributes)\n\n    if not len(additional_attributes):\n        return attributes_hex\n\n    len_attribute, = unpack('<I', attributes_raw.read(4))\n    if len_attribute != 8:\n        return attributes_hex\n\n    attr_id, = unpack('<I', attributes_raw.read(4))\n    if attr_id != APK._APK_SIG_ATTR_V2_STRIPPING_PROTECTION:\n        return attributes_hex\n        \n    scheme_id, = unpack('<I', attributes_raw.read(4)\n\n    return \"stripping protection set, scheme %d\" % scheme_id"
    },
    {
        "original": "def generate(cls, public_keys, amount):\n        \"\"\"Generates a Output from a specifically formed tuple or list.\n\n            Note:\n                If a ThresholdCondition has to be generated where the threshold\n                is always the number of subconditions it is split between, a\n                list of the following structure is sufficient:\n\n                [(address|condition)*, [(address|condition)*, ...], ...]\n\n            Args:\n                public_keys (:obj:`list` of :obj:`str`): The public key of\n                    the users that should be able to fulfill the Condition\n                    that is being created.\n                amount (:obj:`int`): The amount locked by the Output.\n\n            Returns:\n                An Output that can be used in a Transaction.\n\n            Raises:\n                TypeError: If `public_keys` is not an instance of `list`.\n                ValueError: If `public_keys` is an empty list.\n        \"\"\"\n        threshold = len(public_keys)\n        if not isinstance(amount, int):\n            raise TypeError('`amount` must be a int')\n        if amount < 1:\n            raise AmountError('`amount` needs to be greater than zero')\n        if not isinstance(public_keys, list):\n            raise TypeError('`public_keys` must be an instance of list')\n        if len(public_keys) == 0:\n            raise ValueError('`public_keys` needs to contain at least one'\n                             'owner')\n        elif len(public_keys) == 1 and not isinstance(public_keys[0], list):\n            if isinstance(public_keys[0], Fulfillment):\n                ffill = public_keys[0]\n            else:\n                ffill = Ed25519Sha256(\n                    public_key=base58.b58decode(public_keys[0]))\n            return cls(ffill, public_keys, amount=amount)\n        else:\n            initial_cond = ThresholdSha256(threshold=threshold)\n            threshold_cond = reduce(cls._gen_condition, public_keys,\n                                    initial_cond)\n            return cls(threshold_cond, public_keys, amount=amount)",
        "rewrite": "def generate(cls, public_keys, amount):\n    threshold = len(public_keys)\n    if not isinstance(amount, int):\n        raise TypeError('`amount` must be a int')\n    if amount < 1:\n        raise AmountError('`amount` needs to be greater than zero')\n    if not isinstance(public_keys, list):\n        raise TypeError('`public_keys` must be an instance of list')\n    if len(public_keys) == 0:\n        raise ValueError('`public_keys` needs to contain at least one owner')\n    elif len(public_keys) == 1 and not isinstance(public_keys[0], list):\n        if isinstance(public_keys[0], Fulfillment):\n            ffill = public_keys[0]\n        else:\n            ffill = Ed25519Sha256(public_key=base58.b58decode(public_keys[0]))\n        return cls(ffill, public_keys, amount=amount)\n    else:\n        initial_cond = ThresholdSha256(threshold=threshold)\n        threshold_cond = reduce(cls._gen_condition, public_keys, initial_cond)\n        return cls(threshold_cond, public_keys, amount=amount)"
    },
    {
        "original": "def describe(Name,\n             region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Given a rule name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudwatch_event.describe myrule\n\n    \"\"\"\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        rule = conn.describe_rule(Name=Name)\n        if rule:\n            keys = ('Name', 'Arn', 'EventPattern',\n                    'ScheduleExpression', 'State',\n                    'Description',\n                    'RoleArn')\n            return {'rule': dict([(k, rule.get(k)) for k in keys])}\n        else:\n            return {'rule': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'RuleNotFoundException':\n            return {'error': \"Rule {0} not found\".format(Rule)}\n        return {'error': __utils__['boto3.get_error'](e)}",
        "rewrite": "def describe(Name, region=None, key=None, keyid=None, profile=None):\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        rule = conn.describe_rule(Name=Name)\n        \n        if rule:\n            keys = ('Name', 'Arn', 'EventPattern',\n                    'ScheduleExpression', 'State',\n                    'Description',\n                    'RoleArn')\n            return {'rule': dict([(k, rule.get(k)) for k in keys])}\n        else:\n            return {'rule': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        \n        if e.response.get('Error', {}).get('Code') == 'RuleNotFoundException':\n            return {'error': \"Rule {0} not found\".format(Name)}\n        \n        return {'error': __utils__['boto3.get_error'](e)}"
    },
    {
        "original": "def log_entries_from_group(session, group_name, start, end):\n    \"\"\"Get logs for a specific log group\"\"\"\n    logs = session.client('logs')\n    log.info(\"Fetching logs from group: %s\" % group_name)\n    try:\n        logs.describe_log_groups(logGroupNamePrefix=group_name)\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n            return\n        raise\n    try:\n        log_streams = logs.describe_log_streams(\n            logGroupName=group_name,\n            orderBy=\"LastEventTime\",\n            limit=3,\n            descending=True,\n        )\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n            return\n        raise\n    start = _timestamp_from_string(start)\n    end = _timestamp_from_string(end)\n    for s in reversed(log_streams['logStreams']):\n        result = logs.get_log_events(\n            logGroupName=group_name,\n            logStreamName=s['logStreamName'],\n            startTime=start,\n            endTime=end,\n        )\n        for e in result['events']:\n            yield e",
        "rewrite": "def log_entries_from_group(session, group_name, start, end):\n    try:\n        logs = session.client('logs')\n        logs.describe_log_groups(logGroupNamePrefix=group_name)\n        log.info(\"Fetching logs from group: %s\" % group_name)\n        log_streams = logs.describe_log_streams(\n            logGroupName=group_name,\n            orderBy=\"LastEventTime\",\n            limit=3,\n            descending=True,\n        )\n        start = _timestamp_from_string(start)\n        end = _timestamp_from_string(end)\n        for s in reversed(log_streams['logStreams']):\n            result = logs.get_log_events(\n                logGroupName=group_name,\n                logStreamName=s['logStreamName'],\n                startTime=start,\n                endTime=end,\n            )\n            for e in result['events']:\n                yield e\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n            return\n        raise"
    },
    {
        "original": "def sulfide_type(structure):\n    \"\"\"\n    Determines if a structure is a sulfide/polysulfide\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        (str) sulfide/polysulfide/sulfate\n    \"\"\"\n    structure = structure.copy()\n    structure.remove_oxidation_states()\n    s = Element(\"S\")\n    comp = structure.composition\n    if comp.is_element or s not in comp:\n        return None\n\n    finder = SpacegroupAnalyzer(structure, symprec=0.1)\n    symm_structure = finder.get_symmetrized_structure()\n    s_sites = [sites[0] for sites in symm_structure.equivalent_sites if\n               sites[0].specie == s]\n\n    def process_site(site):\n\n        # in an exceptionally rare number of structures, the search\n        # radius needs to be increased to find a neighbor atom\n        search_radius = 4\n        neighbors = []\n        while len(neighbors) == 0:\n            neighbors = structure.get_neighbors(site, search_radius)\n            search_radius *= 2\n            if search_radius > max(structure.lattice.abc)*2:\n                break\n\n        neighbors = sorted(neighbors, key=lambda n: n[1])\n        nn, dist = neighbors[0]\n        coord_elements = [site.specie for site, d in neighbors\n                          if d < dist + 0.4][:4]\n        avg_electroneg = np.mean([e.X for e in coord_elements])\n        if avg_electroneg > s.X:\n            return \"sulfate\"\n        elif avg_electroneg == s.X and s in coord_elements:\n            return \"polysulfide\"\n        else:\n            return \"sulfide\"\n\n    types = set([process_site(site) for site in s_sites])\n    if \"sulfate\" in types:\n        return None\n    elif \"polysulfide\" in types:\n        return \"polysulfide\"\n    else:\n        return \"sulfide\"",
        "rewrite": "def sulfide_type(structure):\n    structure = structure.copy()\n    structure.remove_oxidation_states()\n    s = Element(\"S\")\n    comp = structure.composition\n    if comp.is_element or s not in comp:\n        return None\n\n    finder = SpacegroupAnalyzer(structure, symprec=0.1)\n    symm_structure = finder.get_symmetrized_structure()\n    s_sites = [sites[0] for sites in symm_structure.equivalent_sites if\n               sites[0].specie == s]\n\n    def process_site(site):\n        search_radius = 4\n        neighbors = []\n        while len(neighbors) == 0:\n            neighbors = structure.get_neighbors(site, search_radius)\n            search_radius *= 2\n            if search_radius > max(structure.lattice.abc)*2:\n                break\n\n        neighbors = sorted(neighbors, key=lambda n: n[1])\n        nn, dist = neighbors[0]\n        coord_elements = [site.specie for site, d in neighbors if d < dist + 0.4][:4]\n        avg_electroneg = np.mean([e.X for e in coord_elements])\n        if avg_electroneg > s.X:\n            return \"sulfate\"\n        elif avg_electroneg == s.X and s in coord_elements:\n            return \"polysulfide\"\n        else:\n            return \"sulfide\"\n\n    types = set([process_site(site) for site in s_sites])\n    if \"sulfate\" in types:\n        return None\n    elif \"polysulfide\" in types:\n        return \"polysulfide\"\n    else:\n        return \"sulfide\""
    },
    {
        "original": "def send_miniprogrampage_message(\n        self, user_id, title, appid, pagepath, thumb_media_id, kf_account=None\n    ):\n        \"\"\"\n        \u53d1\u9001\u5c0f\u7a0b\u5e8f\u5361\u7247\uff08\u8981\u6c42\u5c0f\u7a0b\u5e8f\u4e0e\u516c\u4f17\u53f7\u5df2\u5173\u8054\uff09\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param title: \u5c0f\u7a0b\u5e8f\u5361\u7247\u7684\u6807\u9898\n        :param appid: \u5c0f\u7a0b\u5e8f\u7684 appid\uff0c\u8981\u6c42\u5c0f\u7a0b\u5e8f\u7684 appid \u9700\u8981\u4e0e\u516c\u4f17\u53f7\u6709\u5173\u8054\u5173\u7cfb\n        :param pagepath: \u5c0f\u7a0b\u5e8f\u7684\u9875\u9762\u8def\u5f84\uff0c\u8ddf app.json \u5bf9\u9f50\uff0c\u652f\u6301\u53c2\u6570\uff0c\u6bd4\u5982 pages/index/index?foo=bar\n        :param thumb_media_id: \u5c0f\u7a0b\u5e8f\u5361\u7247\u56fe\u7247\u7684\u5a92\u4f53 ID\uff0c\u5c0f\u7a0b\u5e8f\u5361\u7247\u56fe\u7247\u5efa\u8bae\u5927\u5c0f\u4e3a 520*416\n        :param kf_account: \u9700\u8981\u4ee5\u67d0\u4e2a\u5ba2\u670d\u5e10\u53f7\u6765\u53d1\u6d88\u606f\u65f6\u6307\u5b9a\u7684\u5ba2\u670d\u8d26\u6237\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"miniprogrampage\",\n            \"miniprogrampage\": {\n                \"title\": title,\n                \"appid\": appid,\n                \"pagepath\": pagepath,\n                \"thumb_media_id\": thumb_media_id\n            }\n        }\n        if kf_account is not None:\n            data[\"customservice\"] = {\"kf_account\": kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )",
        "rewrite": "def send_miniprogram_page_message(\n        self, user_id, title, appid, pagepath, thumb_media_id, kf_account=None\n    ):\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"miniprogrampage\",\n            \"miniprogrampage\": {\n                \"title\": title,\n                \"appid\": appid,\n                \"pagepath\": pagepath,\n                \"thumb_media_id\": thumb_media_id\n            }\n        }\n        if kf_account is not None:\n            data[\"customservice\"] = {\"kf_account\": kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n       )"
    },
    {
        "original": "def _osx_memdata():\n    \"\"\"\n    Return the memory information for BSD-like systems\n    \"\"\"\n    grains = {'mem_total': 0, 'swap_total': 0}\n\n    sysctl = salt.utils.path.which('sysctl')\n    if sysctl:\n        mem = __salt__['cmd.run']('{0} -n hw.memsize'.format(sysctl))\n        swap_total = __salt__['cmd.run']('{0} -n vm.swapusage'.format(sysctl)).split()[2].replace(',', '.')\n        if swap_total.endswith('K'):\n            _power = 2**10\n        elif swap_total.endswith('M'):\n            _power = 2**20\n        elif swap_total.endswith('G'):\n            _power = 2**30\n        swap_total = float(swap_total[:-1]) * _power\n\n        grains['mem_total'] = int(mem) // 1024 // 1024\n        grains['swap_total'] = int(swap_total) // 1024 // 1024\n    return grains",
        "rewrite": "def _osx_memdata():\n    \"\"\"\n    Return the memory information for BSD-like systems\n    \"\"\"\n    grains = {'mem_total': 0, 'swap_total': 0}\n\n    sysctl = __salt__['cmd.run']('which sysctl')\n    if sysctl:\n        mem = __salt__['cmd.run']('{0} -n hw.memsize'.format(sysctl))\n        swap_total = __salt__['cmd.run']('{0} -n vm.swapusage'.format(sysctl)).split()[2].replace(',', '.')\n        \n        if swap_total.endswith('K'):\n            swap_total_val = float(swap_total[:-1]) * 1024\n        elif swap_total.endswith('M'):\n            swap_total_val = float(swap_total[:-1]) * 1024**2\n        elif swap_total.endswith('G'):\n            swap_total_val = float(swap_total[:-1]) * 1024**3\n\n        grains['mem_total'] = int(mem) // 1024 // 1024\n        grains['swap_total'] = int(swap_total_val) // 1024 // 1024\n\n    return grains"
    },
    {
        "original": "def create_context(self, state_hash, base_contexts, inputs, outputs):\n        \"\"\"Create a ExecutionContext to run a transaction against.\n\n        Args:\n            state_hash: (str): Merkle root to base state on.\n            base_contexts (list of str): Context ids of contexts that will\n                have their state applied to make this context.\n            inputs (list of str): Addresses that can be read from.\n            outputs (list of str): Addresses that can be written to.\n        Returns:\n            context_id (str): the unique context_id of the session\n        \"\"\"\n\n        for address in inputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in inputs is not \"\n                    \"valid\".format(address))\n        for address in outputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in outputs is not \"\n                    \"valid\".format(address))\n\n        addresses_to_find = [add for add in inputs if len(add) == 70]\n\n        address_values, reads = self._find_address_values_in_chain(\n            base_contexts=base_contexts,\n            addresses_to_find=addresses_to_find)\n\n        context = ExecutionContext(\n            state_hash=state_hash,\n            read_list=inputs,\n            write_list=outputs,\n            base_context_ids=base_contexts)\n\n        contexts_asked_not_found = [cid for cid in base_contexts\n                                    if cid not in self._contexts]\n        if contexts_asked_not_found:\n            raise KeyError(\n                \"Basing a new context off of context ids {} \"\n                \"that are not in context manager\".format(\n                    contexts_asked_not_found))\n\n        context.create_initial(address_values)\n\n        self._contexts[context.session_id] = context\n\n        if reads:\n            context.create_prefetch(reads)\n            self._address_queue.put_nowait(\n                (context.session_id, state_hash, reads))\n        return context.session_id",
        "rewrite": "def create_context(self, state_hash, base_contexts, inputs, outputs):\n        for address in inputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in inputs is not valid\".format(address))\n        \n        for address in outputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in outputs is not valid\".format(address))\n        \n        addresses_to_find = [add for add in inputs if len(add) == 70]\n        \n        address_values, reads = self._find_address_values_in_chain(\n            base_contexts=base_contexts,\n            addresses_to_find=addresses_to_find)\n        \n        context = ExecutionContext(\n            state_hash=state_hash,\n            read_list=inputs,\n            write_list=outputs,\n            base_context_ids=base_contexts)\n        \n        contexts_asked_not_found = [cid for cid in base_contexts if cid not in self._contexts]\n        \n        if contexts_asked_not_found:\n            raise KeyError(\"Basing a new context off of context ids {} that are not in context manager\".format(contexts_asked_not_found))\n        \n        context.create_initial(address_values)\n        \n        self._contexts[context.session_id] = context\n        \n        if reads:\n            context.create_prefetch(reads)\n            self._address_queue.put_nowait((context.session_id, state_hash, reads))\n        \n        return context.session_id"
    },
    {
        "original": "def _CreateMethod(self, method_name):\n    \"\"\"Create a method wrapping an invocation to the SOAP service.\n\n    Args:\n      method_name: A string identifying the name of the SOAP method to call.\n\n    Returns:\n      A callable that can be used to make the desired SOAP request.\n    \"\"\"\n    soap_service_method = getattr(self.suds_client.service, method_name)\n\n    def MakeSoapRequest(*args):\n      ",
        "rewrite": "def _create_method(self, method_name):\n    \"\"\"Create a method wrapping an invocation to the SOAP service.\n\n    Args:\n      method_name: A string identifying the name of the SOAP method to call.\n\n    Returns:\n      A callable that can be used to make the desired SOAP request.\n    \"\"\"\n    soap_service_method = getattr(self.suds_client.service, method_name)\n\n    def make_soap_request(*args):\n        return soap_service_method(*args)\n\n    return make_soap_request"
    },
    {
        "original": "def parse_section_data_files(self, section_options):\n        \"\"\"Parses `data_files` configuration file section.\n\n        :param dict section_options:\n        \"\"\"\n        parsed = self._parse_section_to_dict(section_options, self._parse_list)\n        self['data_files'] = [(k, v) for k, v in parsed.items()]",
        "rewrite": "def parse_section_data_files(self, section_options):\n    parsed = self._parse_section_to_dict(section_options, self._parse_list)\n    self['data_files'] = [(k, v) for k, v in parsed.items()]"
    },
    {
        "original": "def read(self, symbol, as_of=None):\n        \"\"\"\n        Return current metadata saved for `symbol`\n\n        Parameters\n        ----------\n        symbol : `str`\n            symbol name for the item\n        as_of : `datetime.datetime`\n            return entry valid at given time\n\n        Returns\n        -------\n        metadata\n        \"\"\"\n        if as_of is not None:\n            res = self.find_one({'symbol': symbol, 'start_time': {'$lte': as_of}},\n                                sort=[('start_time', pymongo.DESCENDING)])\n        else:\n            res = self.find_one({'symbol': symbol}, sort=[('start_time', pymongo.DESCENDING)])\n        return res['metadata'] if res is not None else None",
        "rewrite": "def read(self, symbol, as_of=None):\n    if as_of is not None:\n        res = self.find_one({'symbol': symbol, 'start_time': {'$lte': as_of}},\n                            sort=[('start_time', pymongo.DESCENDING)])\n    else:\n        res = self.find_one({'symbol': symbol}, sort=[('start_time', pymongo.DESCENDING)])\n    return res['metadata'] if res is not None else None"
    },
    {
        "original": "def download_attachments(self):\n        \"\"\" Downloads this message attachments into memory.\n        Need a call to 'attachment.save' to save them on disk.\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        if not self._parent.has_attachments:\n            log.debug(\n                'Parent {} has no attachments, skipping out early.'.format(\n                    self._parent.__class__.__name__))\n            return False\n\n        if not self._parent.object_id:\n            raise RuntimeError(\n                'Attempted to download attachments of an unsaved {}'.format(\n                    self._parent.__class__.__name__))\n\n        url = self.build_url(self._endpoints.get('attachments').format(\n            id=self._parent.object_id))\n\n        response = self._parent.con.get(url)\n        if not response:\n            return False\n\n        attachments = response.json().get('value', [])\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        self.untrack = True\n        self.add({self._cloud_data_key: attachments})\n        self.untrack = False\n\n        # TODO: when it's a item attachment the attachment itself\n        # is not downloaded. We must download it...\n        # TODO: idea: retrieve the attachments ids' only with\n        # select and then download one by one.\n        return True",
        "rewrite": "def download_attachments(self):\n    if not self._parent.has_attachments:\n        return False\n\n    if not self._parent.object_id:\n        raise RuntimeError(\n            'Attempted to download attachments of an unsaved {}'.format(\n                self._parent.__class__.__name__))\n\n    url = self.build_url(self._endpoints.get('attachments').format(\n        id=self._parent.object_id))\n\n    response = self._parent.con.get(url)\n    if not response:\n        return False\n\n    attachments = response.json().get('value', [])\n\n    self.untrack = True\n    self.add({self._cloud_data_key: attachments})\n    self.untrack = False\n\n    return True"
    },
    {
        "original": "def fix_e303(self, result):\n        \"\"\"Remove extra blank lines.\"\"\"\n        delete_linenum = int(result['info'].split('(')[1].split(')')[0]) - 2\n        delete_linenum = max(1, delete_linenum)\n\n        # We need to count because pycodestyle reports an offset line number if\n        # there are comments.\n        cnt = 0\n        line = result['line'] - 2\n        modified_lines = []\n        while cnt < delete_linenum and line >= 0:\n            if not self.source[line].strip():\n                self.source[line] = ''\n                modified_lines.append(1 + line)  # Line indexed at 1\n                cnt += 1\n            line -= 1\n\n        return modified_lines",
        "rewrite": "def fix_e303(self, result):\n    delete_linenum = int(result['info'].split('(')[1].split(')')[0]) - 2\n    delete_linenum = max(1, delete_linenum)\n\n    cnt = 0\n    line = result['line'] - 2\n    modified_lines = []\n    while cnt < delete_linenum and line >= 0:\n        if not self.source[line].strip():\n            self.source[line] = ''\n            modified_lines.append(1 + line)\n            cnt += 1\n        line -= 1\n\n    return modified_lines"
    },
    {
        "original": "def _UpdateChildIndex(self, urn, mutation_pool):\n    \"\"\"Update the child indexes.\n\n    This function maintains the index for direct child relations. When we set\n    an AFF4 path, we always add an attribute like\n    index:dir/%(childname)s to its parent. This is written\n    asynchronously to its parent.\n\n    In order to query for all direct children of an AFF4 object, we then simple\n    get the attributes which match the regex index:dir/.+ which are the\n    direct children.\n\n    Args:\n      urn: The AFF4 object for which we update the index.\n      mutation_pool: A MutationPool object to write to.\n    \"\"\"\n    try:\n      # Create navigation aids by touching intermediate subject names.\n      while urn.Path() != \"/\":\n        basename = urn.Basename()\n        dirname = rdfvalue.RDFURN(urn.Dirname())\n\n        try:\n          self.intermediate_cache.Get(urn)\n          return\n        except KeyError:\n          extra_attributes = None\n          # This is a performance optimization. On the root there is no point\n          # setting the last access time since it gets accessed all the time.\n          # TODO(amoser): Can we get rid of the index in the root node entirely?\n          # It's too big to query anyways...\n          if dirname != u\"/\":\n            extra_attributes = {\n                AFF4Object.SchemaCls.LAST: [\n                    rdfvalue.RDFDatetime.Now().SerializeToDataStore()\n                ]\n            }\n\n          mutation_pool.AFF4AddChild(\n              dirname, basename, extra_attributes=extra_attributes)\n\n          self.intermediate_cache.Put(urn, 1)\n\n          urn = dirname\n\n    except access_control.UnauthorizedAccess:\n      pass",
        "rewrite": "def _UpdateChildIndex(self, urn, mutation_pool):\n    try:\n        while urn.Path() != \"/\":\n            basename = urn.Basename()\n            dirname = rdfvalue.RDFURN(urn.Dirname())\n\n            try:\n                self.intermediate_cache.Get(urn)\n                return\n            except KeyError:\n                extra_attributes = None\n                if dirname != u\"/\":\n                    extra_attributes = {\n                        AFF4Object.SchemaCls.LAST: [\n                            rdfvalue.RDFDatetime.Now().SerializeToDataStore()\n                        ]\n                    }\n\n                mutation_pool.AFF4AddChild(\n                    dirname, basename, extra_attributes=extra_attributes)\n\n                self.intermediate_cache.Put(urn, 1)\n\n                urn = dirname\n\n    except access_control.UnauthorizedAccess:\n        pass"
    },
    {
        "original": "def moment_by_moment_schedule(device: Device, circuit: Circuit):\n    \"\"\"Returns a schedule aligned with the moment structure of the Circuit.\n\n    This method attempts to create a schedule in which each moment of a circuit\n    is scheduled starting at the same time. Given the constraints of the\n    given device, such a schedule may not be possible, in this case the\n    the method will raise a ValueError with a description of the conflict.\n\n    The schedule that is produced will take each moments and schedule the\n    operations in this moment in a time slice of length equal to the maximum\n    time of an operation in the moment.\n\n    Returns:\n        A Schedule for the circuit.\n\n    Raises:\n        ValueError: if the scheduling cannot be done.\n    \"\"\"\n    schedule = Schedule(device)\n    t = Timestamp()\n    for moment in circuit:\n        if not moment.operations:\n            continue\n        for op in moment.operations:\n            scheduled_op = ScheduledOperation.op_at_on(op, t, device)\n            # Raises a ValueError describing the problem if this cannot be\n            # scheduled.\n            schedule.include(scheduled_operation=scheduled_op)\n            # Raises ValueError at first sign of a device conflict.\n            device.validate_scheduled_operation(schedule, scheduled_op)\n        # Increment time for next moment by max of ops during this moment.\n        max_duration = max(device.duration_of(op) for op in moment.operations)\n        t += max_duration\n    return schedule",
        "rewrite": "def moment_by_moment_schedule(device: Device, circuit: Circuit):\n    schedule = Schedule(device)\n    t = Timestamp()\n    for moment in circuit:\n        if not moment.operations:\n            continue\n        for op in moment.operations:\n            scheduled_op = ScheduledOperation.op_at_on(op, t, device)\n            schedule.include(scheduled_operation=scheduled_op)\n            device.validate_scheduled_operation(schedule, scheduled_op)\n        max_duration = max(device.duration_of(op) for op in moment.operations)\n        t += max_duration\n    return schedule"
    },
    {
        "original": "def to_dt(date, default_tz=None):\n    \"\"\"\n    Returns a non-naive datetime.datetime.\n\n    Interprets numbers as ms-since-epoch.\n\n    Parameters\n    ----------\n    date : `int` or `datetime.datetime`\n        The datetime to convert\n\n    default_tz : tzinfo\n        The TimeZone to use if none is found.  If not supplied, and the\n        datetime doesn't have a timezone, then we raise ValueError\n\n    Returns\n    -------\n    Non-naive datetime\n    \"\"\"\n    if isinstance(date, (int, long)):\n        return ms_to_datetime(date, default_tz)\n    elif date.tzinfo is None:\n        if default_tz is None:\n            raise ValueError(\"Must specify a TimeZone on incoming data\")\n        return date.replace(tzinfo=default_tz)\n    return date",
        "rewrite": "def to_dt(date, default_tz=None):\n    if isinstance(date, int):\n        return ms_to_datetime(date, default_tz)\n    elif date.tzinfo is None:\n        if default_tz is None:\n            raise ValueError(\"Must specify a TimeZone on incoming data\")\n        return date.replace(tzinfo=default_tz)\n    return date"
    },
    {
        "original": "def next_k_array(a):\n    \"\"\"\n    Given an array `a` of k distinct nonnegative integers, sorted in\n    ascending order, return the next k-array in the lexicographic\n    ordering of the descending sequences of the elements [1]_. `a` is\n    modified in place.\n\n    Parameters\n    ----------\n    a : ndarray(int, ndim=1)\n        Array of length k.\n\n    Returns\n    -------\n    a : ndarray(int, ndim=1)\n        View of `a`.\n\n    Examples\n    --------\n    Enumerate all the subsets with k elements of the set {0, ..., n-1}.\n\n    >>> n, k = 4, 2\n    >>> a = np.arange(k)\n    >>> while a[-1] < n:\n    ...     print(a)\n    ...     a = next_k_array(a)\n    ...\n    [0 1]\n    [0 2]\n    [1 2]\n    [0 3]\n    [1 3]\n    [2 3]\n\n    References\n    ----------\n    .. [1] `Combinatorial number system\n       <https://en.wikipedia.org/wiki/Combinatorial_number_system>`_,\n       Wikipedia.\n\n    \"\"\"\n    # Logic taken from Algotirhm T in D. Knuth, The Art of Computer\n    # Programming, Section 7.2.1.3 \"Generating All Combinations\".\n    k = len(a)\n    if k == 1 or a[0] + 1 < a[1]:\n        a[0] += 1\n        return a\n\n    a[0] = 0\n    i = 1\n    x = a[i] + 1\n\n    while i < k-1 and x == a[i+1]:\n        i += 1\n        a[i-1] = i - 1\n        x = a[i] + 1\n    a[i] = x\n\n    return a",
        "rewrite": "def next_k_array(a):\n    k = len(a)\n    if k == 1 or a[0] + 1 < a[1]:\n        a[0] += 1\n        return a\n\n    a[0] = 0\n    i = 1\n    x = a[i] + 1\n\n    while i < k-1 and x == a[i+1]:\n        i += 1\n        a[i-1] = i - 1\n        x = a[i] + 1\n    a[i] = x\n\n    return a"
    },
    {
        "original": "def _get_format(self, token):\n        \"\"\" Returns a QTextCharFormat for token or None.\n        \"\"\"\n        if token in self._formats:\n            return self._formats[token]\n\n        result = self._get_format_from_style(token, self._style)\n\n        self._formats[token] = result\n        return result",
        "rewrite": "def _get_format(self, token):\n    if token in self._formats:\n        return self._formats[token]\n\n    result = self._get_format_from_style(token, self._style)\n\n    self._formats[token] = result\n    return result"
    },
    {
        "original": "def _Stat(self, path, ext_attrs=False):\n    \"\"\"Returns stat information of a specific path.\n\n    Args:\n      path: A unicode string containing the path.\n      ext_attrs: Whether the call should also collect extended attributes.\n\n    Returns:\n      a StatResponse proto\n\n    Raises:\n      IOError when call to os.stat() fails\n    \"\"\"\n    # Note that the encoding of local path is system specific\n    local_path = client_utils.CanonicalPathToLocalPath(path)\n    result = client_utils.StatEntryFromPath(\n        local_path, self.pathspec, ext_attrs=ext_attrs)\n\n    # Is this a symlink? If so we need to note the real location of the file.\n    try:\n      result.symlink = utils.SmartUnicode(os.readlink(local_path))\n    except (OSError, AttributeError):\n      pass\n\n    return result",
        "rewrite": "def _Stat(self, path, ext_attrs=False):\n    local_path = client_utils.CanonicalPathToLocalPath(path)\n    result = client_utils.StatEntryFromPath(local_path, self.pathspec, ext_attrs=ext_attrs)\n    \n    try:\n        result.symlink = utils.SmartUnicode(os.readlink(local_path))\n    except (OSError, AttributeError):\n        pass\n        \n    return result"
    },
    {
        "original": "def get_reduced_configs(self):\n        \"\"\"Reduce the experiments to restart.\"\"\"\n        iteration_config = self.experiment_group.iteration_config\n        if iteration_config is None:\n            logger.error(\n                'Experiment group `%s` attempt to update iteration, but has no iteration',\n                self.experiment_group.id,\n                extra={'stack': True})\n            return\n        search_manager = self.experiment_group.search_manager\n\n        # Get the number of experiments to keep\n        n_configs_to_keep = search_manager.get_n_config_to_keep_for_iteration(\n            iteration=iteration_config.iteration,\n            bracket_iteration=iteration_config.bracket_iteration)\n\n        # Get the last group's experiments metrics\n        experiments_metrics = self.experiment_group.iteration_config.experiments_metrics\n        if experiments_metrics is None:\n            raise ExperimentGroupException()\n\n        # Order the experiments\n        reverse = Optimization.maximize(\n            self.experiment_group.hptuning_config.hyperband.metric.optimization)\n        experiments_metrics = sorted(experiments_metrics, key=lambda x: x[1], reverse=reverse)\n\n        # Keep n experiments\n        return [xp[0] for xp in experiments_metrics[:n_configs_to_keep]]",
        "rewrite": "def get_reduced_configs(self):\n    iteration_config = self.experiment_group.iteration_config\n    if iteration_config is None:\n        logger.error(\n            'Experiment group `%s` attempt to update iteration, but has no iteration',\n            self.experiment_group.id,\n            extra={'stack': True})\n        return\n    search_manager = self.experiment_group.search_manager\n    \n    n_configs_to_keep = search_manager.get_n_config_to_keep_for_iteration(\n        iteration=iteration_config.iteration,\n        bracket_iteration=iteration_config.bracket_iteration)\n\n    experiments_metrics = self.experiment_group.iteration_config.experiments_metrics\n    if experiments_metrics is None:\n        raise ExperimentGroupException()\n\n    reverse = Optimization.maximize(self.experiment_group.hptuning_config.hyperband.metric.optimization)\n    experiments_metrics = sorted(experiments_metrics, key=lambda x: x[1], reverse=reverse)\n\n    return [xp[0] for xp in experiments_metrics[:n_configs_to_keep]]"
    },
    {
        "original": "def get_phi_variables(self, block_addr):\n        \"\"\"\n        Get a dict of phi variables and their corresponding variables.\n\n        :param int block_addr:  Address of the block.\n        :return:                A dict of phi variables of an empty dict if there are no phi variables at the block.\n        :rtype:                 dict\n        \"\"\"\n\n        if block_addr not in self._phi_variables_by_block:\n            return dict()\n        variables = { }\n        for phi in self._phi_variables_by_block[block_addr]:\n            variables[phi] = self._phi_variables[phi]\n        return variables",
        "rewrite": "def get_phi_variables(self, block_addr):\n    if block_addr not in self._phi_variables_by_block:\n        return {}\n    \n    variables = {}\n    for phi in self._phi_variables_by_block[block_addr]:\n        variables[phi] = self._phi_variables[phi]\n    \n    return variables"
    },
    {
        "original": "def vote(self, direction=0):\n        \"\"\"Vote for the given item in the direction specified.\n\n        Note: votes must be cast by humans. That is, API clients proxying a\n        human's action one-for-one are OK, but bots deciding how to vote on\n        content or amplifying a human's vote are not. See the reddit rules for\n        more details on what constitutes vote cheating.\n\n        Source for note: http://www.reddit.com/dev/api#POST_api_vote\n\n        :returns: The json response from the server.\n\n        \"\"\"\n        url = self.reddit_session.config['vote']\n        data = {'id': self.fullname,\n                'dir': six.text_type(direction)}\n        if self.reddit_session.user:\n            # pylint: disable=W0212\n            urls = [urljoin(self.reddit_session.user._url, 'disliked'),\n                    urljoin(self.reddit_session.user._url, 'liked')]\n            # pylint: enable=W0212\n            self.reddit_session.evict(urls)\n        return self.reddit_session.request_json(url, data=data)",
        "rewrite": "def vote(self, direction=0):\n    url = self.reddit_session.config['vote']\n    data = {'id': self.fullname,\n            'dir': str(direction)}\n    if self.reddit_session.user:\n        urls = [urljoin(self.reddit_session.user._url, 'disliked'),\n                urljoin(self.reddit_session.user._url, 'liked')]\n        self.reddit_session.evict(urls)\nreturn self.reddit_session.request_json(url, data=data)"
    },
    {
        "original": "def all_experiment_groups(self):\n        \"\"\"\n        Similar to experiment_groups,\n        but uses the default manager to return archived experiments as well.\n        \"\"\"\n        from db.models.experiment_groups import ExperimentGroup\n\n        return ExperimentGroup.all.filter(project=self)",
        "rewrite": "def all_experiment_groups(self):\n    from db.models.experiment_groups import ExperimentGroup\n    return ExperimentGroup.objects.filter(project=self)"
    },
    {
        "original": "def bounce_cluster(name):\n    \"\"\"\n    Bounce all Traffic Server nodes in the cluster. Bouncing Traffic Server\n    shuts down and immediately restarts Traffic Server, node-by-node.\n\n    .. code-block:: yaml\n\n        bounce_ats_cluster:\n          trafficserver.bounce_cluster\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    if __opts__['test']:\n        ret['comment'] = 'Bouncing cluster'\n        return ret\n\n    __salt__['trafficserver.bounce_cluster']()\n\n    ret['result'] = True\n    ret['comment'] = 'Bounced cluster'\n    return ret",
        "rewrite": "def bounce_cluster(name):\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n    \n    if __opts__['test']:\n        ret['comment'] = 'Bouncing cluster'\n        return ret\n        \n    __salt__['trafficserver.bounce_cluster']()\n    \n    ret['result'] = True\n    ret['comment'] = 'Bounced cluster'\n    return ret"
    },
    {
        "original": "def matrix_exponent(M):\n    \"\"\"\n    The function computes matrix exponent and handles some special cases\n    \"\"\"\n\n    if (M.shape[0] == 1): # 1*1 matrix\n        Mexp = np.array( ((np.exp(M[0,0]) ,),) )\n\n    else: # matrix is larger\n        method = None\n        try:\n            Mexp = linalg.expm(M)\n            method = 1\n        except (Exception,) as e:\n            Mexp = linalg.expm3(M)\n            method = 2\n        finally:\n            if np.any(np.isnan(Mexp)):\n                if method == 2:\n                    raise ValueError(\"Matrix Exponent is not computed 1\")\n                else:\n                    Mexp = linalg.expm3(M)\n                    method = 2\n                    if np.any(np.isnan(Mexp)):\n                        raise ValueError(\"Matrix Exponent is not computed 2\")\n\n    return Mexp",
        "rewrite": "import numpy as np\nfrom scipy.linalg import expm, expm3\n\ndef matrix_exponent(M):\n    if (M.shape[0] == 1):\n        Mexp = np.array(((np.exp(M[0,0]),),))\n    else:\n        method = None\n        try:\n            Mexp = expm(M)\n            method = 1\n        except Exception as e:\n            Mexp = expm3(M)\n            method = 2\n        finally:\n            if np.any(np.isnan(Mexp)):\n                if method == 2:\n                    raise ValueError(\"Matrix Exponent is not computed 1\")\n                else:\n                    Mexp = expm3(M)\n                    method = 2\n                    if np.any(np.isnan(Mexp)):\n                        raise ValueError(\"Matrix Exponent is not computed 2\")\n    return Mexp"
    },
    {
        "original": "def log_likelihood(z, x, P, H, R):\n    \"\"\"\n    Returns log-likelihood of the measurement z given the Gaussian\n    posterior (x, P) using measurement function H and measurement\n    covariance error R\n    \"\"\"\n    S = np.dot(H, np.dot(P, H.T)) + R\n    return logpdf(z, np.dot(H, x), S)",
        "rewrite": "def log_likelihood(z, x, P, H, R):\n    S = np.dot(H, np.dot(P, H.T)) + R\n    return logpdf(z, np.dot(H, x), S)"
    },
    {
        "original": "def read_metadata(self, symbol):\n        \"\"\"\n        Reads user defined metadata out for the given symbol\n\n        Parameters\n        ----------\n        symbol: str\n            symbol for the given item in the DB\n\n        Returns\n        -------\n        ?\n        \"\"\"\n        sym = self._get_symbol_info(symbol)\n        if not sym:\n            raise NoDataFoundException(\"Symbol does not exist.\")\n        x = self._symbols.find_one({SYMBOL: symbol})\n        return x[USERMETA] if USERMETA in x else None",
        "rewrite": "def read_metadata(self, symbol):\n    sym = self._get_symbol_info(symbol)\n    if not sym:\n        raise NoDataFoundException(\"Symbol does not exist.\")\n    x = self._symbols.find_one({SYMBOL: symbol})\n    return x.get(USERMETA, None)"
    },
    {
        "original": "def list_streams(region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Return a list of all streams visible to the current account\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_kinesis.list_streams\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    streams = []\n    exclusive_start_stream_name = ''\n    while exclusive_start_stream_name is not None:\n        args = {'ExclusiveStartStreamName': exclusive_start_stream_name} if exclusive_start_stream_name else {}\n        ret = _execute_with_retries(conn, 'list_streams', **args)\n        if 'error' in ret:\n            return ret\n        ret = ret['result'] if ret and ret.get('result') else {}\n        streams += ret.get('StreamNames', [])\n        exclusive_start_stream_name = streams[-1] if ret.get('HasMoreStreams', False) in (True, 'true') else None\n    return {'result': streams}",
        "rewrite": "def list_streams(region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    streams = []\n    exclusive_start_stream_name = ''\n    \n    while exclusive_start_stream_name is not None:\n        args = {'ExclusiveStartStreamName': exclusive_start_stream_name} if exclusive_start_stream_name else {}\n        ret = _execute_with_retries(conn, 'list_streams', **args)\n        \n        if 'error' in ret:\n            return ret\n        \n        ret = ret['result'] if ret and ret.get('result') else {}\n        streams += ret.get('StreamNames', [])\n        exclusive_start_stream_name = streams[-1] if ret.get('HasMoreStreams', False) in (True, 'true') else None\n    \n    return {'result': streams}"
    },
    {
        "original": "def send_voice_message(self, user_id, media_id, kf_account=None):\n        \"\"\"\n        \u53d1\u9001\u8bed\u97f3\u6d88\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param media_id: \u53d1\u9001\u7684\u8bed\u97f3\u7684\u5a92\u4f53ID\u3002 \u53ef\u4ee5\u901a\u8fc7 :func:`upload_media` \u4e0a\u4f20\u3002\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"voice\",\n            \"voice\": {\n                \"media_id\": media_id\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )",
        "rewrite": "def send_voice_message(self, user_id, media_id, kf_account=None):\n    data = {\n        \"touser\": user_id,\n        \"msgtype\": \"voice\",\n        \"voice\": {\n            \"media_id\": media_id\n        }\n    }\n    if kf_account is not None:\n        data['customservice'] = {'kf_account': kf_account}\n    return self.post(\n        url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n        data=data\n   )"
    },
    {
        "original": "def create_shepherd_tour(self, name=None, theme=None):\n        \"\"\" Creates a Shepherd JS website tour.\n            @Params\n            name - If creating multiple tours at the same time,\n                   use this to select the tour you wish to add steps to.\n            theme - Sets the default theme for the tour.\n                    Choose from \"light\"/\"arrows\", \"dark\", \"default\", \"square\",\n                    and \"square-dark\". (\"light\" is used if None is selected.)\n        \"\"\"\n\n        shepherd_theme = \"shepherd-theme-arrows\"\n        if theme:\n            if theme.lower() == \"default\":\n                shepherd_theme = \"shepherd-theme-default\"\n            elif theme.lower() == \"dark\":\n                shepherd_theme = \"shepherd-theme-dark\"\n            elif theme.lower() == \"light\":\n                shepherd_theme = \"shepherd-theme-arrows\"\n            elif theme.lower() == \"arrows\":\n                shepherd_theme = \"shepherd-theme-arrows\"\n            elif theme.lower() == \"square\":\n                shepherd_theme = \"shepherd-theme-square\"\n            elif theme.lower() == \"square-dark\":\n                shepherd_theme = \"shepherd-theme-square-dark\"\n\n        if not name:\n            name = \"default\"\n\n        new_tour = (\n            ",
        "rewrite": "def create_shepherd_tour(self, name=None, theme=None):\n    \"\"\" Creates a Shepherd JS website tour.\n        @Params\n        name - Used for selecting the tour you wish to add steps to.\n        theme - Sets the default theme for the tour.\n                Choose from \"light\"/\"arrows\", \"dark\", \"default\", \"square\",\n                and \"square-dark\". (\"light\" is used if None is selected.)\n    \"\"\"\n\n    shepherd_theme = \"shepherd-theme-arrows\"\n    if theme:\n        theme_lower = theme.lower()\n        if theme_lower in [\"default\", \"dark\", \"light\", \"arrows\", \"square\", \"square-dark\"]:\n            shepherd_theme = f\"shepherd-theme-{theme_lower}\"\n\n    name = name if name else \"default\"\n\n    new_tour = (\n        \" . No need to explain. Just write code:\u201d"
    },
    {
        "original": "def manual_close(self):\n        \"\"\"\n        Close the underlying connection without returning it to the pool.\n        \"\"\"\n        if self.is_closed():\n            return False\n\n        # Obtain reference to the connection in-use by the calling thread.\n        conn = self.connection()\n\n        # A connection will only be re-added to the available list if it is\n        # marked as \"in use\" at the time it is closed. We will explicitly\n        # remove it from the \"in use\" list, call \"close()\" for the\n        # side-effects, and then explicitly close the connection.\n        self._in_use.pop(self.conn_key(conn), None)\n        self.close()\n        self._close(conn, close_conn=True)",
        "rewrite": "def manual_close(self):\n    if self.is_closed():\n        return False\n\n    conn = self.connection()\n    \n    self._in_use.pop(self.conn_key(conn), None)\n    self.close()\n    self._close(conn, close_conn=True)"
    },
    {
        "original": "def fix_e713(self, result):\n        \"\"\"Fix (trivial case of) non-membership check.\"\"\"\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        # to convert once 'not in' -> 'in'\n        before_target = target[:offset]\n        target = target[offset:]\n        match_notin = COMPARE_NEGATIVE_REGEX_THROUGH.search(target)\n        notin_pos_start, notin_pos_end = 0, 0\n        if match_notin:\n            notin_pos_start = match_notin.start(1)\n            notin_pos_end = match_notin.end()\n            target = '{}{} {}'.format(\n                target[:notin_pos_start], 'in', target[notin_pos_end:])\n\n        # fix 'not in'\n        match = COMPARE_NEGATIVE_REGEX.search(target)\n        if match:\n            if match.group(3) == 'in':\n                pos_start = match.start(1)\n                new_target = '{5}{0}{1} {2} {3} {4}'.format(\n                    target[:pos_start], match.group(2), match.group(1),\n                    match.group(3), target[match.end():], before_target)\n                if match_notin:\n                    # revert 'in' -> 'not in'\n                    pos_start = notin_pos_start + offset\n                    pos_end = notin_pos_end + offset - 4     # len('not ')\n                    new_target = '{}{} {}'.format(\n                        new_target[:pos_start], 'not in', new_target[pos_end:])\n                self.source[line_index] = new_target",
        "rewrite": "def fix_e713(self, result):\n    (line_index, offset, target) = get_index_offset_contents(result, self.source)\n\n    before_target = target[:offset]\n    target = target[offset:]\n    match_notin = COMPARE_NEGATIVE_REGEX_THROUGH.search(target)\n    notin_pos_start, notin_pos_end = 0, 0\n    if match_notin:\n        notin_pos_start = match_notin.start(1)\n        notin_pos_end = match_notin.end()\n        target = '{}{} {}'.format(\n            target[:notin_pos_start], 'in', target[notin_pos_end:])\n\n    match = COMPARE_NEGATIVE_REGEX.search(target)\n    if match and match.group(3) == 'in':\n        pos_start = match.start(1)\n        new_target = '{5}{0}{1} {2} {3} {4}'.format(\n            target[:pos_start], match.group(2), match.group(1),\n            match.group(3), target[match.end():], before_target)\n        \n        if match_notin:\n            pos_start = notin_pos_start + offset\n            pos_end = notin_pos_end + offset - 4\n            new_target = '{}{} {}'.format(\n                new_target[:pos_start], 'not in', new_target[pos_end:])\n        \n        self.source[line_index] = new_target"
    },
    {
        "original": "def pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        \"\"\"\n        Prints scheduler for user to read.\n        \"\"\"\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            # Print a message indicating what the below schedule means\n            print(\n                \"Table consists of tuples of \"\n                \"(num configs, num_resources_per_config) \"\n                \"which specify how many configs to run and \"\n                \"for how many epochs. \"\n            )\n            print(\n                \"Each bracket starts with a list of random \"\n                \"configurations which is successively halved \"\n                \"according the schedule.\"\n            )\n            print(\n                \"See the Hyperband paper \"\n                \"(https://arxiv.org/pdf/1603.06560.pdf) for more details.\"\n            )\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")",
        "rewrite": "def pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            print(\"Table consists of tuples of (num configs, num_resources_per_config) which specify how many configs to run and for how many epochs.\")\n            print(\"Each bracket starts with a list of random configurations which is successively halved according to the schedule.\")\n            print(\"See the Hyperband paper (https://arxiv.org/pdf/1603.06560.pdf) for more details.\")\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")"
    },
    {
        "original": "def evict(cls, urls):\n        \"\"\"Remove items from cache matching URLs.\n\n        Return the number of items removed.\n\n        \"\"\"\n        if isinstance(urls, text_type):\n            urls = [urls]\n        urls = set(normalize_url(url) for url in urls)\n        retval = 0\n        with cls.ca_lock:\n            for key in list(cls.cache):\n                if key[0] in urls:\n                    retval += 1\n                    del cls.cache[key]\n                    del cls.timeouts[key]\n        return retval",
        "rewrite": "def evict(cls, urls):\n    if isinstance(urls, str):\n        urls = [urls]\n    urls = set(normalize_url(url) for url in urls)\n    retval = 0\n    with cls.ca_lock:\n        for key in list(cls.cache):\n            if key[0] in urls:\n                retval += 1\n                del cls.cache[key]\n                del cls.timeouts[key]\n    return retval"
    },
    {
        "original": "def create(self, name):\n        \"\"\"\n        \u521b\u5efa\u6807\u7b7e\n\n        :param name: \u6807\u7b7e\u540d\uff0830\u4e2a\u5b57\u7b26\u4ee5\u5185\uff09\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n        \"\"\"\n        name = to_text(name)\n        return self._post(\n            'tags/create',\n            data={'tag': {'name': name}},\n            result_processor=lambda x: x['tag']\n        )",
        "rewrite": "def create(self, name):\n    name = to_text(name)\n    return self._post('tags/create', data={'tag': {'name': name}}, result_processor=lambda x: x['tag'])"
    },
    {
        "original": "def index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    posts = Post.query.order_by(Post.created.desc()).all()\n    return render_template(\"blog/index.html\", posts=posts)",
        "rewrite": "def index():\n    posts = Post.query.order_by(Post.created.desc()).all()\n    return render_template(\"blog/index.html\", posts=posts)"
    },
    {
        "original": "def fit_bounds(self, bounds, padding_top_left=None,\n                   padding_bottom_right=None, padding=None, max_zoom=None):\n        \"\"\"Fit the map to contain a bounding box with the\n        maximum zoom level possible.\n\n        Parameters\n        ----------\n        bounds: list of (latitude, longitude) points\n            Bounding box specified as two points [southwest, northeast]\n        padding_top_left: (x, y) point, default None\n            Padding in the top left corner. Useful if some elements in\n            the corner, such as controls, might obscure objects you're zooming\n            to.\n        padding_bottom_right: (x, y) point, default None\n            Padding in the bottom right corner.\n        padding: (x, y) point, default None\n            Equivalent to setting both top left and bottom right padding to\n            the same value.\n        max_zoom: int, default None\n            Maximum zoom to be used.\n\n        Examples\n        --------\n        >>> m.fit_bounds([[52.193636, -2.221575], [52.636878, -1.139759]])\n\n        \"\"\"\n        self.add_child(FitBounds(bounds,\n                                 padding_top_left=padding_top_left,\n                                 padding_bottom_right=padding_bottom_right,\n                                 padding=padding,\n                                 max_zoom=max_zoom,\n                                 )\n                       )",
        "rewrite": "def fit_bounds(self, bounds, padding_top_left=None,\n                   padding_bottom_right=None, padding=None, max_zoom=None):\n        self.add_child(FitBounds(bounds,\n                                 padding_top_left=padding_top_left,\n                                 padding_bottom_right=padding_bottom_right,\n                                 padding=padding,\n                                 max_zoom=max_zoom,\n                                 )\n                       )"
    },
    {
        "original": "def random_letters(self, length=16):\n        \"\"\"Returns a random letter (between a-z and A-Z).\"\"\"\n        return self.random_choices(\n            getattr(string, 'letters', string.ascii_letters),\n            length=length,\n        )",
        "rewrite": "def random_letters(self, length=16):\n    return self.random_choices(getattr(string, 'ascii_letters', string.ascii_letters), length=length)"
    },
    {
        "original": "def _process_dataset(name, directory, num_shards, labels_file):\n  \"\"\"Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    labels_file: string, path to the labels file.\n  \"\"\"\n  filenames, texts, labels = _find_image_files(directory, labels_file)\n  _process_image_files(name, filenames, texts, labels, num_shards)",
        "rewrite": "def _process_dataset(name, directory, num_shards, labels_file):\n    filenames, texts, labels = _find_image_files(directory, labels_file)\n    _process_image_files(name, filenames, texts, labels, num_shards)"
    },
    {
        "original": "def tgread_bytes(self):\n        \"\"\"\n        Reads a Telegram-encoded byte array, without the need of\n        specifying its length.\n        \"\"\"\n        first_byte = self.read_byte()\n        if first_byte == 254:\n            length = self.read_byte() | (self.read_byte() << 8) | (\n                self.read_byte() << 16)\n            padding = length % 4\n        else:\n            length = first_byte\n            padding = (length + 1) % 4\n\n        data = self.read(length)\n        if padding > 0:\n            padding = 4 - padding\n            self.read(padding)\n\n        return data",
        "rewrite": "def tgread_bytes(self):\n    first_byte = self.read_byte()\n    if first_byte == 254:\n        length = self.read_byte() | (self.read_byte() << 8) | (self.read_byte() << 16)\n        padding = length % 4\n    else:\n        length = first_byte\n        padding = (length + 1) % 4\n\n    data = self.read(length)\n    if padding > 0:\n        padding = 4 - padding\n        self.read(padding)\n\n    return data"
    },
    {
        "original": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this HtmlContent.\n\n        :returns: This HtmlContent, ready for use in a request body.\n        :rtype: dict\n        \"\"\"\n        content = {}\n        if self.mime_type is not None:\n            content[\"type\"] = self.mime_type\n\n        if self.content is not None:\n            content[\"value\"] = self.content\n        return content",
        "rewrite": "def get(self):\n    content = {}\n    if self.mime_type:\n        content[\"type\"] = self.mime_type\n\n    if self.content:\n        content[\"value\"] = self.content\n        \n    return content"
    },
    {
        "original": "def _verify_names(sampler, var_names, arg_names):\n    \"\"\"Make sure var_names and arg_names are assigned reasonably.\n\n    This is meant to run before loading emcee objects into InferenceData.\n    In case var_names or arg_names is None, will provide defaults. If they are\n    not None, it verifies there are the right number of them.\n\n    Throws a ValueError in case validation fails.\n\n    Parameters\n    ----------\n    sampler : emcee.EnsembleSampler\n        Fitted emcee sampler\n    var_names : list[str] or None\n        Names for the emcee parameters\n    arg_names : list[str] or None\n        Names for the args/observations provided to emcee\n\n    Returns\n    -------\n    list[str], list[str]\n        Defaults for var_names and arg_names\n    \"\"\"\n    # There are 3 possible cases: emcee2, emcee3 and sampler read from h5 file (emcee3 only)\n    if hasattr(sampler, \"args\"):\n        num_vars = sampler.chain.shape[-1]\n        num_args = len(sampler.args)\n    elif hasattr(sampler, \"log_prob_fn\"):\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = len(sampler.log_prob_fn.args)\n    else:\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = 0  # emcee only stores the posterior samples\n\n    if var_names is None:\n        var_names = [\"var_{}\".format(idx) for idx in range(num_vars)]\n    if arg_names is None:\n        arg_names = [\"arg_{}\".format(idx) for idx in range(num_args)]\n\n    if len(var_names) != num_vars:\n        raise ValueError(\n            \"The sampler has {} variables, but only {} var_names were provided!\".format(\n                num_vars, len(var_names)\n            )\n        )\n\n    if len(arg_names) != num_args:\n        raise ValueError(\n            \"The sampler has {} args, but only {} arg_names were provided!\".format(\n                num_args, len(arg_names)\n            )\n        )\n    return var_names, arg_names",
        "rewrite": "def _verify_names(sampler, var_names, arg_names):\n    if hasattr(sampler, \"args\"):\n        num_vars = sampler.chain.shape[-1]\n        num_args = len(sampler.args)\n    elif hasattr(sampler, \"log_prob_fn\"):\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = len(sampler.log_prob_fn.args)\n    else:\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = 0\n\n    if var_names is None:\n        var_names = [\"var_{}\".format(idx) for idx in range(num_vars)]\n    if arg_names is None:\n        arg_names = [\"arg_{}\".format(idx) for idx in range(num_args)]\n\n    if len(var_names) != num_vars:\n        raise ValueError(\n            \"The sampler has {} variables, but only {} var_names were provided!\".format(\n                num_vars, len(var_names)\n            )\n        )\n\n    if len(arg_names) != num_args:\n        raise ValueError(\n            \"The sampler has {} args, but only {} arg_names were provided!\".format(\n                num_args, len(arg_names)\n            )\n        )\n    return var_names, arg_names"
    },
    {
        "original": "def is_number(string):\n    \"\"\" checks if a string is a number (int/float) \"\"\"\n    string = str(string)\n    if string.isnumeric():\n        return True\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False",
        "rewrite": "def is_number(value):\n    value = str(value)\n    if value.isnumeric():\n        return True\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False"
    },
    {
        "original": "def _mod_priv_opts(object_type, privileges):\n    \"\"\"\n    Format options\n    \"\"\"\n    object_type = object_type.lower()\n    privileges = '' if privileges is None else privileges\n    _privs = re.split(r'\\s?,\\s?', privileges.upper())\n\n    return object_type, privileges, _privs",
        "rewrite": "import re\n\ndef _mod_priv_opts(object_type, privileges):\n    object_type = object_type.lower()\n    privileges = '' if privileges is None else privileges\n    _privs = re.split(r'\\s?,\\s?', privileges.upper())\n\n    return object_type, privileges, _privs"
    },
    {
        "original": "def save_flash_segment(self, f, segment, checksum=None):\n        \"\"\" Save the next segment to the image file, return next checksum value if provided \"\"\"\n        segment_end_pos = f.tell() + len(segment.data) + self.SEG_HEADER_LEN\n        segment_len_remainder = segment_end_pos % self.IROM_ALIGN\n        if segment_len_remainder < 0x24:\n            # Work around a bug in ESP-IDF 2nd stage bootloader, that it didn't map the\n            # last MMU page, if an IROM/DROM segment was < 0x24 bytes over the page boundary.\n            segment.data += b'\\x00' * (0x24 - segment_len_remainder)\n        return self.save_segment(f, segment, checksum)",
        "rewrite": "def save_flash_segment(self, f, segment, checksum=None):\n    segment_end_pos = f.tell() + len(segment.data) + self.SEG_HEADER_LEN\n    segment_len_remainder = segment_end_pos % self.IROM_ALIGN\n    if segment_len_remainder < 0x24:\n        segment.data += b'\\x00' * (0x24 - segment_len_remainder)\n    return self.save_segment(f, segment, checksum)"
    },
    {
        "original": "def alter_configs(self, config_resources):\n        \"\"\"Alter configuration parameters of one or more Kafka resources.\n\n        Warning:\n            This is currently broken for BROKER resources because those must be\n            sent to that specific broker, versus this always picks the\n            least-loaded node. See the comment in the source code for details.\n            We would happily accept a PR fixing this.\n\n        :param config_resources: A list of ConfigResource objects.\n        :return: Appropriate version of AlterConfigsResponse class.\n        \"\"\"\n        version = self._matching_api_version(AlterConfigsRequest)\n        if version == 0:\n            request = AlterConfigsRequest[version](\n                resources=[self._convert_alter_config_resource_request(config_resource) for config_resource in config_resources]\n            )\n        else:\n            raise NotImplementedError(\n                \"Support for AlterConfigs v{} has not yet been added to KafkaAdminClient.\"\n                .format(version))\n        # TODO the Java client has the note:\n        # // We must make a separate AlterConfigs request for every BROKER resource we want to alter\n        # // and send the request to that specific broker. Other resources are grouped together into\n        # // a single request that may be sent to any broker.\n        #\n        # So this is currently broken as it always sends to the least_loaded_node()\n        return self._send_request_to_node(self._client.least_loaded_node(), request)",
        "rewrite": "def alter_configs(self, config_resources):\n    version = self._matching_api_version(AlterConfigsRequest)\n    if version == 0:\n        request = AlterConfigsRequest[version](\n            resources=[self._convert_alter_config_resource_request(config_resource) for config_resource in config_resources]\n        )\n    else:\n        raise NotImplementedError(\n            \"Support for AlterConfigs v{} has not yet been added to KafkaAdminClient.\"\n            .format(version))\n    \n    return self._send_request_to_node(self._client.least_loaded_node(), request)"
    },
    {
        "original": "def _is_indirect_jump(_, sim_successors):\n        \"\"\"\n        Determine if this SimIRSB has an indirect jump as its exit\n        \"\"\"\n\n        if sim_successors.artifacts['irsb_direct_next']:\n            # It's a direct jump\n            return False\n\n        default_jumpkind = sim_successors.artifacts['irsb_default_jumpkind']\n        if default_jumpkind not in ('Ijk_Call', 'Ijk_Boring', 'Ijk_InvalICache'):\n            # It's something else, like a ret of a syscall... we don't care about it\n            return False\n\n        return True",
        "rewrite": "def _is_indirect_jump(_, sim_successors):\n\n    if sim_successors.artifacts['irsb_direct_next']:\n        return False\n\n    default_jumpkind = sim_successors.artifacts['irsb_default_jumpkind']\n    if default_jumpkind not in ('Ijk_Call', 'Ijk_Boring', 'Ijk_InvalICache'):\n        return False\n\n    return True"
    },
    {
        "original": "def volume_delete(self, name):\n        \"\"\"\n        Delete a block device\n        \"\"\"\n        if self.volume_conn is None:\n            raise SaltCloudSystemExit('No cinder endpoint available')\n        nt_ks = self.volume_conn\n        try:\n            volume = self.volume_show(name)\n        except KeyError as exc:\n            raise SaltCloudSystemExit('Unable to find {0} volume: {1}'.format(name, exc))\n        if volume['status'] == 'deleted':\n            return volume\n        response = nt_ks.volumes.delete(volume['id'])\n        return volume",
        "rewrite": "def volume_delete(self, name):\n    if self.volume_conn is None:\n        raise SaltCloudSystemExit('No cinder endpoint available')\n    nt_ks = self.volume_conn\n    try:\n        volume = self.volume_show(name)\n    except KeyError as exc:\n        raise SaltCloudSystemExit('Unable to find {0} volume: {1}'.format(name, exc))\n    if volume['status'] == 'deleted':\n        return volume\n    response = nt_ks.volumes.delete(volume['id'])\n    return volume"
    },
    {
        "original": "def get_predicted_structure(self, structure, icsd_vol=False):\n        \"\"\"\n        Given a structure, returns back the structure scaled to predicted\n        volume.\n        Args:\n            structure (Structure): structure w/unknown volume\n\n        Returns:\n            a Structure object with predicted volume\n        \"\"\"\n        new_structure = structure.copy()\n        new_structure.scale_lattice(self.predict(structure, icsd_vol=icsd_vol))\n        return new_structure",
        "rewrite": "def get_predicted_structure(self, structure, icsd_vol=False):\n    new_structure = structure.copy()\n    new_structure.scale_lattice(self.predict(structure, icsd_vol=icsd_vol))\n    return new_structure"
    },
    {
        "original": "def _local_call(self, call_conf):\n        \"\"\"\n        Execute local call\n        \"\"\"\n        try:\n            ret = self._get_caller(call_conf).call()\n        except SystemExit:\n            ret = 'Data is not available at this moment'\n            self.out.error(ret)\n        except Exception as ex:\n            ret = 'Unhandled exception occurred: {}'.format(ex)\n            log.debug(ex, exc_info=True)\n            self.out.error(ret)\n\n        return ret",
        "rewrite": "def _local_call(self, call_conf):\n    try:\n        ret = self._get_caller(call_conf).call()\n    except SystemExit:\n        ret = 'Data is not available at this moment'\n        self.out.error(ret)\n    except Exception as ex:\n        ret = 'Unhandled exception occurred: {}'.format(ex)\n        log.debug(ex, exc_info=True)\n        self.out.error(ret)\n\n    return ret"
    },
    {
        "original": "def getReqId(self) -> int:\n        \"\"\"\n        Get new request ID.\n        \"\"\"\n        if not self.isReady():\n            raise ConnectionError('Not connected')\n        newId = self._reqIdSeq\n        self._reqIdSeq += 1\n        return newId",
        "rewrite": "def get_req_id(self) -> int:\n    if not self.is_ready():\n        raise ConnectionError('Not connected')\n    new_id = self._req_id_seq\n    self._req_id_seq += 1\n    return new_id"
    },
    {
        "original": "def get_max_instability(self, min_voltage=None, max_voltage=None):\n        \"\"\"\n        The maximum instability along a path for a specific voltage range.\n\n        Args:\n            min_voltage: The minimum allowable voltage.\n            max_voltage: The maximum allowable voltage.\n\n        Returns:\n            Maximum decomposition energy of all compounds along the insertion\n            path (a subset of the path can be chosen by the optional arguments)\n        \"\"\"\n        data = []\n        for pair in self._select_in_voltage_range(min_voltage, max_voltage):\n            if pair.decomp_e_charge is not None:\n                data.append(pair.decomp_e_charge)\n            if pair.decomp_e_discharge is not None:\n                data.append(pair.decomp_e_discharge)\n        return max(data) if len(data) > 0 else None",
        "rewrite": "def get_max_instability(self, min_voltage=None, max_voltage=None):\n    data = []\n    for pair in self._select_in_voltage_range(min_voltage, max_voltage):\n        if pair.decomp_e_charge is not None:\n            data.append(pair.decomp_e_charge)\n        if pair.decomp_e_discharge is not None:\n            data.append(pair.decomp_e_discharge)\n    return max(data) if len(data) > 0 else None"
    },
    {
        "original": "def __read_master_key(self):\n        \"\"\"\n        Read in the rotating master authentication key\n        \"\"\"\n        key_user = self.salt_user\n        if key_user == 'root':\n            if self.opts.get('user', 'root') != 'root':\n                key_user = self.opts.get('user', 'root')\n        if key_user.startswith('sudo_'):\n            key_user = self.opts.get('user', 'root')\n        if salt.utils.platform.is_windows():\n            # The username may contain '\\' if it is in Windows\n            # 'DOMAIN\\username' format. Fix this for the keyfile path.\n            key_user = key_user.replace('\\\\', '_')\n        keyfile = os.path.join(self.opts['cachedir'],\n                               '.{0}_key'.format(key_user))\n        try:\n            # Make sure all key parent directories are accessible\n            salt.utils.verify.check_path_traversal(self.opts['cachedir'],\n                                                   key_user,\n                                                   self.skip_perm_errors)\n            with salt.utils.files.fopen(keyfile, 'r') as key:\n                return salt.utils.stringutils.to_unicode(key.read())\n        except (OSError, IOError, SaltClientError):\n            # Fall back to eauth\n            return ''",
        "rewrite": "def __read_master_key(self):\n        key_user = self.salt_user\n        if key_user == 'root':\n            if self.opts.get('user', 'root') != 'root':\n                key_user = self.opts.get('user', 'root')\n        if key_user.startswith('sudo_'):\n            key_user = self.opts.get('user', 'root')\n        if salt.utils.platform.is_windows():\n            key_user = key_user.replace('\\\\', '_')\n        keyfile = os.path.join(self.opts['cachedir'],\n                               '.{0}_key'.format(key_user))\n        try:\n            salt.utils.verify.check_path_traversal(self.opts['cachedir'],\n                                                   key_user,\n                                                   self.skip_perm_errors)\n            with salt.utils.files.fopen(keyfile, 'r') as key:\n                return salt.utils.stringutils.to_unicode(key.read())\n        except (OSError, IOError, SaltClientError):\n            return ''"
    },
    {
        "original": "def make_request_from_data(self, data):\n        \"\"\"Returns a Request instance from data coming from Redis.\n\n        By default, ``data`` is an encoded URL. You can override this method to\n        provide your own message decoding.\n\n        Parameters\n        ----------\n        data : bytes\n            Message from redis.\n\n        \"\"\"\n        url = bytes_to_str(data, self.redis_encoding)\n        return self.make_requests_from_url(url)",
        "rewrite": "def make_request_from_data(self, data):\n    url = bytes_to_str(data, self.redis_encoding)\n    return self.make_requests_from_url(url)"
    },
    {
        "original": "def _parse_normalization_kwargs(self, use_batch_norm, batch_norm_config,\n                                  normalization_ctor, normalization_kwargs):\n    \"\"\"Sets up normalization, checking old and new flags.\"\"\"\n    if use_batch_norm is not None:\n      # Delete this whole block when deprecation is done.\n      util.deprecation_warning(\n          \"`use_batch_norm` kwarg is deprecated. Change your code to instead \"\n          \"specify `normalization_ctor` and `normalization_kwargs`.\")\n      if not use_batch_norm:\n        # Explicitly set to False - normalization_{ctor,kwargs} has precedence.\n        self._check_and_assign_normalization_members(normalization_ctor,\n                                                     normalization_kwargs or {})\n      else:  # Explicitly set to true - new kwargs must not be used.\n        if normalization_ctor is not None or normalization_kwargs is not None:\n          raise ValueError(\n              \"if use_batch_norm is specified, normalization_ctor and \"\n              \"normalization_kwargs must not be.\")\n        self._check_and_assign_normalization_members(batch_norm.BatchNorm,\n                                                     batch_norm_config or {})\n    else:\n      # Old kwargs not set, this block will remain after removing old kwarg.\n      self._check_and_assign_normalization_members(normalization_ctor,\n                                                   normalization_kwargs or {})",
        "rewrite": "def _parse_normalization_kwargs(self, use_batch_norm, batch_norm_config,\n                                  normalization_ctor, normalization_kwargs):\n    if use_batch_norm is not None:\n        util.deprecation_warning(\n            \"`use_batch_norm` kwarg is deprecated. Change your code to instead specify `normalization_ctor` and `normalization_kwargs`.\"\n        )\n        if not use_batch_norm:\n            self._check_and_assign_normalization_members(normalization_ctor,\n                                                         normalization_kwargs or {})\n        else:\n            if normalization_ctor is not None or normalization_kwargs is not None:\n                raise ValueError(\n                    \"if use_batch_norm is specified, normalization_ctor and normalization_kwargs must not be.\"\n                )\n            self._check_and_assign_normalization_members(batch_norm.BatchNorm,\n                                                         batch_norm_config or {})\n    else:\n        self._check_and_assign_normalization_members(normalization_ctor,\n                                                     normalization_kwargs or {})"
    },
    {
        "original": "def lrem(self, name, count, value):\n        \"\"\"\n        Remove the first ``count`` occurrences of elements equal to ``value``\n        from the list stored at ``name``.\n\n        The count argument influences the operation in the following ways:\n            count > 0: Remove elements equal to value moving from head to tail.\n            count < 0: Remove elements equal to value moving from tail to head.\n            count = 0: Remove all elements equal to value.\n        \"\"\"\n        return self.execute_command('LREM', name, count, value)",
        "rewrite": "def lrem(self, name, count, value):\n    return self.execute_command('LREM', name, count, value)"
    },
    {
        "original": "def parameters_changed(self):\n        \"\"\"Update the gradients of parameters for warping function\n\n        This method is called when having new values of parameters for warping function, kernels\n        and other parameters in a normal GP\n        \"\"\"\n        # using the warped X to update\n        self.X = self.transform_data(self.X_untransformed)\n        super(InputWarpedGP, self).parameters_changed()\n        # the gradient of log likelihood w.r.t. input AFTER warping is a product of dL_dK and dK_dX\n        dL_dX = self.kern.gradients_X(self.grad_dict['dL_dK'], self.X)\n        self.warping_function.update_grads(self.X_untransformed, dL_dX)",
        "rewrite": "def parameters_changed(self):\n    self.X = self.transform_data(self.X_untransformed)\n    super(InputWarpedGP, self).parameters_changed()\n    dL_dX = self.kern.gradients_X(self.grad_dict['dL_dK'], self.X)\n    self.warping_function.update_grads(self.X_untransformed, dL_dX)"
    },
    {
        "original": "def find_cfg_file(file_name=None):\n    \"\"\"Look for .netmiko.yml in current dir, then ~/.netmiko.yml.\"\"\"\n    base_file = \".netmiko.yml\"\n    check_files = [base_file, os.path.expanduser(\"~\") + \"/\" + base_file]\n    if file_name:\n        check_files.insert(0, file_name)\n    for test_file in check_files:\n        if os.path.isfile(test_file):\n            return test_file\n    raise IOError(\"{}: file not found in current dir or home dir.\".format(base_file))",
        "rewrite": "import os\n\ndef find_cfg_file(file_name=None):\n    base_file = \".netmiko.yml\"\n    check_files = [base_file, os.path.expanduser(\"~\") + \"/\" + base_file]\n    if file_name:\n        check_files.insert(0, file_name)\n    for test_file in check_files:\n        if os.path.isfile(test_file):\n            return test_file\n    raise IOError(\"{}: file not found in current dir or home dir.\".format(base_file))"
    },
    {
        "original": "def operate_magmom(self, magmom):\n        \"\"\"\n        Apply time reversal operator on the magnetic moment. Note that\n        magnetic moments transform as axial vectors, not polar vectors. \n\n        See 'Symmetry and magnetic structures', Rodr\u00edguez-Carvajal and\n        Bour\u00e9e for a good discussion. DOI: 10.1051/epjconf/20122200010\n\n        Args:\n            magmom: Magnetic moment as electronic_structure.core.Magmom\n            class or as list or np array-like\n\n        Returns:\n            Magnetic moment after operator applied as Magmom class\n        \"\"\"\n\n        magmom = Magmom(magmom)  # type casting to handle lists as input\n\n        transformed_moment = self.apply_rotation_only(magmom.global_moment) * \\\n            np.linalg.det(self.rotation_matrix) * self.time_reversal\n\n        # retains input spin axis if different from default\n        return Magmom.from_global_moment_and_saxis(transformed_moment, magmom.saxis)",
        "rewrite": "def operate_magmom(self, magmom):\n        magmom = Magmom(magmom)\n        transformed_moment = self.apply_rotation_only(magmom.global_moment) * np.linalg.det(self.rotation_matrix) * self.time_reversal\n        return Magmom.from_global_moment_and_saxis(transformed_moment, magmom.saxis)"
    },
    {
        "original": "def lifted_gate(gate: Gate, n_qubits: int):\n    \"\"\"\n    Lift a pyquil :py:class:`Gate` in a full ``n_qubits``-qubit Hilbert space.\n\n    This function looks up the matrix form of the gate and then dispatches to\n    :py:func:`lifted_gate_matrix` with the target qubits.\n\n    :param gate: A gate\n    :param n_qubits: The total number of qubits.\n    :return: A 2^n by 2^n lifted version of the gate acting on its specified qubits.\n    \"\"\"\n    if len(gate.params) > 0:\n        matrix = QUANTUM_GATES[gate.name](*gate.params)\n    else:\n        matrix = QUANTUM_GATES[gate.name]\n\n    return lifted_gate_matrix(matrix=matrix,\n                              qubit_inds=[q.index for q in gate.qubits],\n                              n_qubits=n_qubits)",
        "rewrite": "def lifted_gate(gate: Gate, n_qubits: int):\n    if len(gate.params) > 0:\n        matrix = QUANTUM_GATES[gate.name](*gate.params)\n    else:\n        matrix = QUANTUM_GATES[gate.name]\n\n    return lifted_gate_matrix(matrix=matrix, qubit_inds=[q.index for q in gate.qubits], n_qubits=n_qubits)"
    },
    {
        "original": "def list_(consul_url=None, token=None, key=None, **kwargs):\n    \"\"\"\n    List keys in Consul\n\n    :param consul_url: The Consul server URL.\n    :param key: The key to use as the starting point for the list.\n    :return: The list of keys.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' consul.list\n        salt '*' consul.list key='web'\n\n    \"\"\"\n    ret = {}\n\n    if not consul_url:\n        consul_url = _get_config()\n        if not consul_url:\n            log.error('No Consul URL found.')\n            ret['message'] = 'No Consul URL found.'\n            ret['res'] = False\n            return ret\n\n    query_params = {}\n\n    if 'recurse' in kwargs:\n        query_params['recurse'] = 'True'\n\n    # No key so recurse and show all values\n    if not key:\n        query_params['recurse'] = 'True'\n        function = 'kv/'\n    else:\n        function = 'kv/{0}'.format(key)\n\n    query_params['keys'] = 'True'\n    query_params['separator'] = '/'\n    ret = _query(consul_url=consul_url,\n                 function=function,\n                 token=token,\n                 query_params=query_params)\n    return ret",
        "rewrite": "def list_keys(consul_url=None, token=None, key=None, **kwargs):\n    ret = {}\n\n    if not consul_url:\n        consul_url = _get_config()\n        if not consul_url:\n            log.error('No Consul URL found.')\n            ret['message'] = 'No Consul URL found.'\n            ret['res'] = False\n            return ret\n\n    query_params = {}\n\n    if 'recurse' in kwargs:\n        query_params['recurse'] = 'True'\n\n    if not key:\n        query_params['recurse'] = 'True'\n        function = 'kv/'\n    else:\n        function = 'kv/{0}'.format(key)\n\n    query_params['keys'] = 'True'\n    query_params['separator'] = '/'\n    ret = _query(consul_url=consul_url,\n                 function=function,\n                 token=token,\n                 query_params=query_params)\n    return ret"
    },
    {
        "original": "def get(self):\n        \"\"\"Return a Deferred that fires with a SourceStamp instance.\"\"\"\n        d = self.getBaseRevision()\n        d.addCallback(self.getPatch)\n        d.addCallback(self.done)\n        return d",
        "rewrite": "def get(self):\n    d = self.getBaseRevision()\n    d.addCallback(self.getPatch)\n    d.addCallback(self.done)\n    return d"
    },
    {
        "original": "def WriteArtifact(self, artifact, cursor=None):\n    \"\"\"Writes new artifact to the database.\"\"\"\n    name = Text(artifact.name)\n\n    try:\n      cursor.execute(\"INSERT INTO artifacts (name, definition) VALUES (%s, %s)\",\n                     [name, artifact.SerializeToString()])\n    except MySQLdb.IntegrityError as error:\n      if error.args[0] == mysql_error_constants.DUP_ENTRY:\n        raise db.DuplicatedArtifactError(name, cause=error)\n      else:\n        raise",
        "rewrite": "def WriteArtifact(self, artifact, cursor=None):\n  name = Text(artifact.name)\n\n  try:\n    cursor.execute(\"INSERT INTO artifacts (name, definition) VALUES (%s, %s)\",\n                   [name, artifact.SerializeToString()])\n  except MySQLdb.IntegrityError as error:\n    if error.args[0] == mysql_error_constants.DUP_ENTRY:\n      raise db.DuplicatedArtifactError(name, cause=error)\n    else:\n      raise"
    },
    {
        "original": "def run(self):\n        \"\"\"Build extensions in build directory, then copy if --inplace\"\"\"\n        old_inplace, self.inplace = self.inplace, 0\n        _build_ext.run(self)\n        self.inplace = old_inplace\n        if old_inplace:\n            self.copy_extensions_to_source()",
        "rewrite": "def run(self):\n    old_inplace, self.inplace = self.inplace, 0\n    _build_ext.run(self)\n    self.inplace = old_inplace\n    if old_inplace:\n        self.copy_extensions_to_source()"
    },
    {
        "original": "def Validate(self, value, **_):\n    \"\"\"Check that value is a list of the required type.\"\"\"\n    # Assigning from same kind can allow us to skip verification since all\n    # elements in a RepeatedFieldHelper already are coerced to the delegate\n    # type. In that case we just make a copy. This only works when the value\n    # wraps the same type as us.\n    if (value.__class__ is RepeatedFieldHelper and\n        value.type_descriptor is self.delegate):\n      result = value.Copy()\n\n    # Make sure the base class finds the value valid.\n    else:\n      # The value may be a generator here, so we just iterate over it.\n      result = RepeatedFieldHelper(type_descriptor=self.delegate)\n      result.Extend(value)\n\n    return result",
        "rewrite": "def validate(self, value, **_):\n    if (isinstance(value, RepeatedFieldHelper) and\n        value.type_descriptor is self.delegate):\n        result = value.Copy()\n    else:\n        result = RepeatedFieldHelper(type_descriptor=self.delegate)\n        result.Extend(value)\n    \n    return result"
    },
    {
        "original": "def edit(self, description=github.GithubObject.NotSet, files=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PATCH /gists/:id <http://developer.github.com/v3/gists>`_\n        :param description: string\n        :param files: dict of string to :class:`github.InputFileContent.InputFileContent`\n        :rtype: None\n        \"\"\"\n        assert description is github.GithubObject.NotSet or isinstance(description, (str, unicode)), description\n        assert files is github.GithubObject.NotSet or all(element is None or isinstance(element, github.InputFileContent) for element in files.itervalues()), files\n        post_parameters = dict()\n        if description is not github.GithubObject.NotSet:\n            post_parameters[\"description\"] = description\n        if files is not github.GithubObject.NotSet:\n            post_parameters[\"files\"] = {key: None if value is None else value._identity for key, value in files.iteritems()}\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PATCH\",\n            self.url,\n            input=post_parameters\n        )\n        self._useAttributes(data)",
        "rewrite": "def edit(self, description=github.GithubObject.NotSet, files=github.GithubObject.NotSet):\n    assert description is github.GithubObject.NotSet or isinstance(description, (str, unicode)), description\n    assert files is github.GithubObject.NotSet or all(element is None or isinstance(element, github.InputFileContent) for element in files.values()), files\n    post_parameters = {}\n    if description is not github.GithubObject.NotSet:\n        post_parameters[\"description\"] = description\n    if files is not github.GithubObject.NotSet:\n        post_parameters[\"files\"] = {key: None if value is None else value._identity for key, value in files.items()}\n    headers, data = self._requester.requestJsonAndCheck(\"PATCH\", self.url, input=post_parameters)\n    self._useAttributes(data)"
    },
    {
        "original": "def _to_numpy(Z):\n        \"\"\"Converts a None, list, np.ndarray, or torch.Tensor to np.ndarray;\n        also handles converting sparse input to dense.\"\"\"\n        if Z is None:\n            return Z\n        elif issparse(Z):\n            return Z.toarray()\n        elif isinstance(Z, np.ndarray):\n            return Z\n        elif isinstance(Z, list):\n            return np.array(Z)\n        elif isinstance(Z, torch.Tensor):\n            return Z.cpu().numpy()\n        else:\n            msg = (\n                f\"Expected None, list, numpy.ndarray or torch.Tensor, \"\n                f\"got {type(Z)} instead.\"\n            )\n            raise Exception(msg)",
        "rewrite": "def _to_numpy(Z):\n    if Z is None:\n        return Z\n    elif issparse(Z):\n        return Z.toarray()\n    elif isinstance(Z, np.ndarray):\n        return Z\n    elif isinstance(Z, list):\n        return np.array(Z)\n    elif isinstance(Z, torch.Tensor):\n        return Z.cpu().numpy()\n    else:\n        msg = f\"Expected None, list, numpy.ndarray or torch.Tensor, got {type(Z)} instead.\"\n        raise Exception(msg)"
    },
    {
        "original": "def calc_new_nonce_hash(self, new_nonce, number):\n        \"\"\"\n        Calculates the new nonce hash based on the current attributes.\n\n        :param new_nonce: the new nonce to be hashed.\n        :param number: number to prepend before the hash.\n        :return: the hash for the given new nonce.\n        \"\"\"\n        new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n        data = new_nonce + struct.pack('<BQ', number, self.aux_hash)\n\n        # Calculates the message key from the given data\n        return int.from_bytes(sha1(data).digest()[4:20], 'little', signed=True)",
        "rewrite": "def calc_new_nonce_hash(self, new_nonce, number):\n    new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n    data = new_nonce + struct.pack('<BQ', number, self.aux_hash)\n    return int.from_bytes(sha1(data).digest()[4:20], 'little', signed=True)"
    },
    {
        "original": "def batch_encode(self, iterator, *args, dim=0, **kwargs):\n        \"\"\"\n        Args:\n            iterator (iterator): Batch of text to encode.\n            *args: Arguments passed onto ``Encoder.__init__``.\n            dim (int, optional): Dimension along which to concatenate tensors.\n            **kwargs: Keyword arguments passed onto ``Encoder.__init__``.\n\n        Returns\n            torch.Tensor, list of int: Encoded and padded batch of sequences; Original lengths of\n                sequences.\n        \"\"\"\n        return stack_and_pad_tensors(\n            super().batch_encode(iterator), padding_index=self.padding_index, dim=dim)",
        "rewrite": "def batch_encode(self, iterator, *args, dim=0, **kwargs):\n    return stack_and_pad_tensors(super().batch_encode(iterator, *args, **kwargs), padding_index=self.padding_index, dim=dim)"
    },
    {
        "original": "def get_scales(scale=None, n=None):\n    \"\"\"\n    Returns a color scale \n\n    Parameters:\n    -----------\n            scale : str\n                    Color scale name\n                    If the color name is preceded by a minus (-) \n                    then the scale is inversed\n            n : int\n                    Number of colors \n                    If n < number of colors available for a given scale then \n                            the minimum number will be returned \n                    If n > number of colors available for a given scale then\n                            the maximum number will be returned \n\n    Example:\n            get_scales('accent',8)\n            get_scales('pastel1')\n    \"\"\"\n    if scale:\n        is_reverse = False\n        if scale[0] == '-':\n            scale = scale[1:]\n            is_reverse = True\n        d = copy.deepcopy(_scales_names[scale.lower()])\n        keys = list(map(int, list(d.keys())))\n        cs = None\n        if n:\n            if n in keys:\n                cs = d[str(n)]\n            elif n < min(keys):\n                cs = d[str(min(keys))]\n        if cs is None:\n            cs = d[str(max(keys))]\n        if is_reverse:\n            cs.reverse()\n        return cs\n    else:\n        d = {}\n        for k, v in list(_scales_names.items()):\n            if isinstance(v, dict):\n                keys = list(map(int, list(v.keys())))\n                d[k] = v[str(max(keys))]\n            else:\n                d[k] = v\n        return d",
        "rewrite": "def get_scales(scale=None, n=None):\n    if scale:\n        is_reverse = False\n        if scale[0] == '-':\n            scale = scale[1:]\n            is_reverse = True\n        d = copy.deepcopy(_scales_names[scale.lower()])\n        keys = list(map(int, list(d.keys())))\n        cs = None\n        if n:\n            if n in keys:\n                cs = d[str(n)]\n            elif n < min(keys):\n                cs = d[str(min(keys))]\n        if cs is None:\n            cs = d[str(max(keys))]\n        if is_reverse:\n            cs.reverse()\n        return cs\n    else:\n        d = {}\n        for k, v in list(_scales_names.items()):\n            if isinstance(v, dict):\n                keys = list(map(int, list(v.keys())))\n                d[k] = v[str(max(keys))]\n            else:\n                d[k] = v\n        return d"
    },
    {
        "original": "def calculate_sets(rules):\n    \"\"\"Calculate FOLLOW sets.\n\n    Adapted from: http://lara.epfl.ch/w/cc09:algorithm_for_first_and_follow_sets\"\"\"\n    symbols = {sym for rule in rules for sym in rule.expansion} | {rule.origin for rule in rules}\n\n    # foreach grammar rule X ::= Y(1) ... Y(k)\n    # if k=0 or {Y(1),...,Y(k)} subset of NULLABLE then\n    #   NULLABLE = NULLABLE union {X}\n    # for i = 1 to k\n    #   if i=1 or {Y(1),...,Y(i-1)} subset of NULLABLE then\n    #     FIRST(X) = FIRST(X) union FIRST(Y(i))\n    #   for j = i+1 to k\n    #     if i=k or {Y(i+1),...Y(k)} subset of NULLABLE then\n    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FOLLOW(X)\n    #     if i+1=j or {Y(i+1),...,Y(j-1)} subset of NULLABLE then\n    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FIRST(Y(j))\n    # until none of NULLABLE,FIRST,FOLLOW changed in last iteration\n\n    NULLABLE = set()\n    FIRST = {}\n    FOLLOW = {}\n    for sym in symbols:\n        FIRST[sym]={sym} if sym.is_term else set()\n        FOLLOW[sym]=set()\n\n    # Calculate NULLABLE and FIRST\n    changed = True\n    while changed:\n        changed = False\n\n        for rule in rules:\n            if set(rule.expansion) <= NULLABLE:\n                if update_set(NULLABLE, {rule.origin}):\n                    changed = True\n\n            for i, sym in enumerate(rule.expansion):\n                if set(rule.expansion[:i]) <= NULLABLE:\n                    if update_set(FIRST[rule.origin], FIRST[sym]):\n                        changed = True\n\n    # Calculate FOLLOW\n    changed = True\n    while changed:\n        changed = False\n\n        for rule in rules:\n            for i, sym in enumerate(rule.expansion):\n                if i==len(rule.expansion)-1 or set(rule.expansion[i+1:]) <= NULLABLE:\n                    if update_set(FOLLOW[sym], FOLLOW[rule.origin]):\n                        changed = True\n\n                for j in range(i+1, len(rule.expansion)):\n                    if set(rule.expansion[i+1:j]) <= NULLABLE:\n                        if update_set(FOLLOW[sym], FIRST[rule.expansion[j]]):\n                            changed = True\n\n    return FIRST, FOLLOW, NULLABLE",
        "rewrite": "def calculate_sets(rules):\n  symbols = {sym for rule in rules for sym in rule.expansion} | {rule.origin for rule in rules}\n\n  NULLABLE = set()\n  FIRST = {}\n  FOLLOW = {}\n  for sym in symbols:\n    FIRST[sym] = {sym} if sym.is_term else set()\n    FOLLOW[sym] = set()\n\n  changed = True\n  while changed:\n    changed = False\n\n    for rule in rules:\n      if set(rule.expansion) <= NULLABLE:\n        if update_set(NULLABLE, {rule.origin}):\n          changed = True\n\n      for i, sym in enumerate(rule.expansion):\n        if set(rule.expansion[:i]) <= NULLABLE:\n          if update_set(FIRST[rule.origin], FIRST[sym]):\n            changed = True\n\n  changed = True\n  while changed:\n    changed = False\n\n    for rule in rules:\n      for i, sym in enumerate(rule.expansion):\n        if i == len(rule.expansion) - 1 or set(rule.expansion[i+1:]) <= NULLABLE:\n          if update_set(FOLLOW[sym], FOLLOW[rule.origin]):\n            changed = True\n\n        for j in range(i+1, len(rule.expansion)):\n          if set(rule.expansion[i+1:j]) <= NULLABLE:\n            if update_set(FOLLOW[sym], FIRST[rule.expansion[j]]):\n              changed = True\n\n  return FIRST, FOLLOW, NULLABLE"
    },
    {
        "original": "def ip_address_delete(session, ifname, ifaddr):\n    \"\"\"\n    Deletes an IP address from interface record identified with the given\n    \"ifname\".\n\n    The arguments are similar to \"ip address delete\" command of iproute2.\n\n    :param session: Session instance connecting to database.\n    :param ifname: Name of interface.\n    :param ifaddr: IPv4 or IPv6 address.\n    :return: Instance of record or \"None\" if failed.\n    \"\"\"\n    def _remove_inet_addr(intf_inet, addr):\n        addr_list = intf_inet.split(',')\n        if addr not in addr_list:\n            LOG.debug(\n                'Interface \"%s\" does not have \"ifaddr\": %s',\n                intf.ifname, addr)\n            return intf_inet\n        else:\n            addr_list.remove(addr)\n            return ','.join(addr_list)\n\n    intf = ip_link_show(session, ifname=ifname)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', ifname)\n        return None\n\n    if ip.valid_ipv4(ifaddr):\n        intf.inet = _remove_inet_addr(intf.inet, ifaddr)\n    elif ip.valid_ipv6(ifaddr):\n        intf.inet6 = _remove_inet_addr(intf.inet6, ifaddr)\n    else:\n        LOG.debug('Invalid IP address for \"ifaddr\": %s', ifaddr)\n        return None\n\n    return intf",
        "rewrite": "def ip_address_delete(session, ifname, ifaddr):\n    def _remove_inet_addr(intf_inet, addr):\n        addr_list = intf_inet.split(',')\n        if addr not in addr_list:\n            LOG.debug('Interface \"%s\" does not have \"ifaddr\": %s', intf.ifname, addr)\n            return intf_inet\n        else:\n            addr_list.remove(addr)\n            return ','.join(addr_list)\n\n    intf = ip_link_show(session, ifname=ifname)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', ifname)\n        return None\n\n    if ip.valid_ipv4(ifaddr):\n        intf.inet = _remove_inet_addr(intf.inet, ifaddr)\n    elif ip.valid_ipv6(ifaddr):\n        intf.inet6 = _remove_inet_addr(intf.inet6, ifaddr)\n    else:\n        LOG.debug('Invalid IP address for \"ifaddr\": %s', ifaddr)\n        return None\n\n    return intf"
    },
    {
        "original": "def _marginalize_factor(self, nodes, factor):\n        \"\"\"\n        Marginalizing the factor selectively for a set of variables.\n\n        Parameters:\n        ----------\n        nodes: list, array-like\n            A container of nodes (list, dict, set, etc.).\n\n        factor: factor\n            factor which is to be marginalized.\n        \"\"\"\n        marginalizing_nodes = list(set(factor.scope()).difference(nodes))\n        return factor.marginalize(marginalizing_nodes, inplace=False)",
        "rewrite": "def _marginalize_factor(self, nodes, factor):\n    marginalizing_nodes = list(set(factor.scope()).difference(nodes))\n    return factor.marginalize(marginalizing_nodes, inplace=False)"
    },
    {
        "original": "def _receive_message(self):\n        \"\"\"\n        Internal coroutine for receiving messages\n        \"\"\"\n        while True:\n            try:\n                if self._socket.getsockopt(zmq.TYPE) == zmq.ROUTER:\n                    zmq_identity, msg_bytes = \\\n                        yield from self._socket.recv_multipart()\n                    if msg_bytes == b'':\n                        # send ACK for connection probes\n                        LOGGER.debug(\"ROUTER PROBE FROM %s\", zmq_identity)\n                        self._socket.send_multipart(\n                            [bytes(zmq_identity), msg_bytes])\n                    else:\n                        self._received_from_identity(zmq_identity)\n                        self._dispatcher_queue.put_nowait(\n                            (zmq_identity, msg_bytes))\n                else:\n                    msg_bytes = yield from self._socket.recv()\n                    self._last_message_time = time.time()\n                    self._dispatcher_queue.put_nowait((None, msg_bytes))\n                self._get_queue_size_gauge(self.connection).set_value(\n                    self._dispatcher_queue.qsize())\n\n            except CancelledError:  # pylint: disable=try-except-raise\n                # The concurrent.futures.CancelledError is caught by asyncio\n                # when the Task associated with the coroutine is cancelled.\n                # The raise is required to stop this component.\n                raise\n            except Exception as e:  # pylint: disable=broad-except\n                LOGGER.exception(\"Received a message on address %s that \"\n                                 \"caused an error: %s\", self._address, e)",
        "rewrite": "def _receive_message(self):\n    while True:\n        try:\n            if self._socket.getsockopt(zmq.TYPE) == zmq.ROUTER:\n                zmq_identity, msg_bytes = yield from self._socket.recv_multipart()\n                if msg_bytes == b'':\n                    LOGGER.debug(f\"ROUTER PROBE FROM {zmq_identity}\")\n                    self._socket.send_multipart([bytes(zmq_identity), msg_bytes])\n                else:\n                    self._received_from_identity(zmq_identity)\n                    self._dispatcher_queue.put_nowait((zmq_identity, msg_bytes))\n            else:\n                msg_bytes = yield from self._socket.recv()\n                self._last_message_time = time.time()\n                self._dispatcher_queue.put_nowait((None, msg_bytes))\n            self._get_queue_size_gauge(self.connection).set_value(self._dispatcher_queue.qsize())\n\n        except CancelledError:  \n            raise\n        except Exception as e:  \n            LOGGER.exception(f\"Received a message on address {self._address} that caused an error: {e}\")"
    },
    {
        "original": "def _TemplateNamesToFiles(self, template_str):\n        \"\"\"Parses a string of templates into a list of file handles.\"\"\"\n        template_list = template_str.split(\":\")\n        template_files = []\n        try:\n            for tmplt in template_list:\n                template_files.append(open(os.path.join(self.template_dir, tmplt), \"r\"))\n        except:  # noqa\n            for tmplt in template_files:\n                tmplt.close()\n            raise\n\n        return template_files",
        "rewrite": "def _TemplateNamesToFiles(self, template_str):\n    template_list = template_str.split(\":\")\n    template_files = []\n    try:\n        for tmplt in template_list:\n            template_files.append(open(os.path.join(self.template_dir, tmplt), \"r\"))\n    except:\n        for tmplt in template_files:\n            tmplt.close()\n        raise\n\n    return template_files"
    },
    {
        "original": "def commissionerUnregister(self):\n        \"\"\"stop commissioner\n\n        Returns:\n            True: successful to stop commissioner\n            False: fail to stop commissioner\n        \"\"\"\n        print '%s call commissionerUnregister' % self.port\n        cmd = 'commissioner stop'\n        print cmd\n        return self.__sendCommand(cmd)[0] == 'Done'",
        "rewrite": "def commissionerUnregister(self):\n    \"\"\"\n    stop commissioner\n\n    Returns:\n        True: successful to stop commissioner\n        False: fail to stop commissioner\n    \"\"\"\n    print('%s call commissionerUnregister' % self.port)\n    cmd = 'commissioner stop'\n    print(cmd)\n    return self.__sendCommand(cmd)[0] == 'Done'"
    },
    {
        "original": "def orig_py_exe(exe):  # pragma: no cover (platform specific)\n    \"\"\"A -mvenv virtualenv made from a -mvirtualenv virtualenv installs\n    packages to the incorrect location.  Attempt to find the _original_ exe\n    and invoke `-mvenv` from there.\n\n    See:\n    - https://github.com/pre-commit/pre-commit/issues/755\n    - https://github.com/pypa/virtualenv/issues/1095\n    - https://bugs.python.org/issue30811\n    \"\"\"\n    try:\n        prefix_script = 'import sys; print(sys.real_prefix)'\n        _, prefix, _ = cmd_output(exe, '-c', prefix_script)\n        prefix = prefix.strip()\n    except CalledProcessError:\n        # not created from -mvirtualenv\n        return exe\n\n    if os.name == 'nt':\n        expected = os.path.join(prefix, 'python.exe')\n    else:\n        expected = os.path.join(prefix, 'bin', os.path.basename(exe))\n\n    if os.path.exists(expected):\n        return expected\n    else:\n        return exe",
        "rewrite": "def orig_py_exe(exe):\n    try:\n        prefix_script = 'import sys; print(sys.real_prefix)'\n        _, prefix, _ = cmd_output(exe, '-c', prefix_script)\n        prefix = prefix.strip()\n    except CalledProcessError:\n        return exe\n\n    if os.name == 'nt':\n        expected = os.path.join(prefix, 'python.exe')\n    else:\n        expected = os.path.join(prefix, 'bin', os.path.basename(exe))\n\n    return expected if os.path.exists(expected) else exe"
    },
    {
        "original": "def dependency_information(include_salt_cloud=False):\n    \"\"\"\n    Report versions of library dependencies.\n    \"\"\"\n    libs = [\n        ('Python', None, sys.version.rsplit('\\n')[0].strip()),\n        ('Jinja2', 'jinja2', '__version__'),\n        ('M2Crypto', 'M2Crypto', 'version'),\n        ('msgpack-python', 'msgpack', 'version'),\n        ('msgpack-pure', 'msgpack_pure', 'version'),\n        ('pycrypto', 'Crypto', '__version__'),\n        ('pycryptodome', 'Cryptodome', 'version_info'),\n        ('PyYAML', 'yaml', '__version__'),\n        ('PyZMQ', 'zmq', '__version__'),\n        ('ZMQ', 'zmq', 'zmq_version'),\n        ('Mako', 'mako', '__version__'),\n        ('Tornado', 'tornado', 'version'),\n        ('timelib', 'timelib', 'version'),\n        ('dateutil', 'dateutil', '__version__'),\n        ('pygit2', 'pygit2', '__version__'),\n        ('libgit2', 'pygit2', 'LIBGIT2_VERSION'),\n        ('smmap', 'smmap', '__version__'),\n        ('cffi', 'cffi', '__version__'),\n        ('pycparser', 'pycparser', '__version__'),\n        ('gitdb', 'gitdb', '__version__'),\n        ('gitpython', 'git', '__version__'),\n        ('python-gnupg', 'gnupg', '__version__'),\n        ('mysql-python', 'MySQLdb', '__version__'),\n        ('cherrypy', 'cherrypy', '__version__'),\n        ('docker-py', 'docker', '__version__'),\n    ]\n\n    if include_salt_cloud:\n        libs.append(\n            ('Apache Libcloud', 'libcloud', '__version__'),\n        )\n\n    for name, imp, attr in libs:\n        if imp is None:\n            yield name, attr\n            continue\n        try:\n            imp = __import__(imp)\n            version = getattr(imp, attr)\n            if callable(version):\n                version = version()\n            if isinstance(version, (tuple, list)):\n                version = '.'.join(map(str, version))\n            yield name, version\n        except Exception:\n            yield name, None",
        "rewrite": "def dependency_information(include_salt_cloud=False):\n    libs = [\n        ('Python', None, sys.version.rsplit('\\n')[0].strip()),\n        ('Jinja2', 'jinja2', '__version__'),\n        ('M2Crypto', 'M2Crypto', 'version'),\n        ('msgpack-python', 'msgpack', 'version'),\n        ('msgpack-pure', 'msgpack_pure', 'version'),\n        ('pycrypto', 'Crypto', '__version__'),\n        ('pycryptodome', 'Cryptodome', 'version_info'),\n        ('PyYAML', 'yaml', '__version__'),\n        ('PyZMQ', 'zmq', '__version__'),\n        ('ZMQ', 'zmq', 'zmq_version'),\n        ('Mako', 'mako', '__version__'),\n        ('Tornado', 'tornado', 'version'),\n        ('timelib', 'timelib', 'version'),\n        ('dateutil', 'dateutil', '__version__'),\n        ('pygit2', 'pygit2', '__version__'),\n        ('libgit2', 'pygit2', 'LIBGIT2_VERSION'),\n        ('smmap', 'smmap', '__version__'),\n        ('cffi', 'cffi', '__version__'),\n        ('pycparser', 'pycparser', '__version__'),\n        ('gitdb', 'gitdb', '__version__'),\n        ('gitpython', 'git', '__version__'),\n        ('python-gnupg', 'gnupg', '__version__'),\n        ('mysql-python', 'MySQLdb', '__version__'),\n        ('cherrypy', 'cherrypy', '__version__'),\n        ('docker-py', 'docker', '__version__'),\n    ]\n\n    if include_salt_cloud:\n        libs.append(\n            ('Apache Libcloud', 'libcloud', '__version__'),\n        )\n\n    for name, imp, attr in libs:\n        if imp is None:\n            yield name, attr\n            continue\n        try:\n            imp = __import__(imp)\n            version = getattr(imp, attr)\n            if callable(version):\n                version = version()\n            if isinstance(version, (tuple, list)):\n                version = '.'.join(map(str, version))\n            yield name, version\n        except Exception:\n            yield name, None"
    },
    {
        "original": "def histogram(series, **kwargs):\n    \"\"\"Plot an histogram of the data.\n\n    Parameters\n    ----------\n    series: Series\n        The data to plot.\n\n    Returns\n    -------\n    str\n        The resulting image encoded as a string.\n    \"\"\"\n    imgdata = BytesIO()\n    plot = _plot_histogram(series, **kwargs)\n    plot.figure.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1, wspace=0, hspace=0)\n    plot.figure.savefig(imgdata)\n    imgdata.seek(0)\n    result_string = 'data:image/png;base64,' + quote(base64.b64encode(imgdata.getvalue()))\n    # TODO Think about writing this to disk instead of caching them in strings\n    plt.close(plot.figure)\n    return result_string",
        "rewrite": "def histogram(series, **kwargs):\n    imgdata = BytesIO()\n    plot = _plot_histogram(series, **kwargs)\n    plot.figure.subplots_adjust(left=0.15, right=0.95, top=0.9, bottom=0.1, wspace=0, hspace=0)\n    plot.figure.savefig(imgdata)\n    imgdata.seek(0)\n    result_string = 'data:image/png;base64,' + quote(base64.b64encode(imgdata.getvalue()))\n    plt.close(plot.figure)\n    return result_string"
    },
    {
        "original": "def inactive_time(self):\n        \"\"\"\n        The length of time (in seconds) that the device has been inactive for.\n        When the device is active, this is :data:`None`.\n        \"\"\"\n        if self._inactive_event.is_set():\n            return self.pin_factory.ticks_diff(self.pin_factory.ticks(),\n                                               self._last_changed)\n        else:\n            return None",
        "rewrite": "def inactive_time(self):\n    if self._inactive_event.is_set():\n        return self.pin_factory.ticks_diff(self.pin_factory.ticks(), self._last_changed)\n    else:\n        return None"
    },
    {
        "original": "def get_commits(self, sha=github.GithubObject.NotSet, path=github.GithubObject.NotSet, since=github.GithubObject.NotSet, until=github.GithubObject.NotSet, author=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/commits <http://developer.github.com/v3/repos/commits>`_\n        :param sha: string\n        :param path: string\n        :param since: datetime.datetime\n        :param until: datetime.datetime\n        :param author: string or :class:`github.NamedUser.NamedUser` or :class:`github.AuthenticatedUser.AuthenticatedUser`\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Commit.Commit`\n        \"\"\"\n        assert sha is github.GithubObject.NotSet or isinstance(sha, (str, unicode)), sha\n        assert path is github.GithubObject.NotSet or isinstance(path, (str, unicode)), path\n        assert since is github.GithubObject.NotSet or isinstance(since, datetime.datetime), since\n        assert until is github.GithubObject.NotSet or isinstance(until, datetime.datetime), until\n        assert author is github.GithubObject.NotSet or isinstance(author, (str, unicode, github.NamedUser.NamedUser, github.AuthenticatedUser.AuthenticatedUser)), author\n        url_parameters = dict()\n        if sha is not github.GithubObject.NotSet:\n            url_parameters[\"sha\"] = sha\n        if path is not github.GithubObject.NotSet:\n            url_parameters[\"path\"] = path\n        if since is not github.GithubObject.NotSet:\n            url_parameters[\"since\"] = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        if until is not github.GithubObject.NotSet:\n            url_parameters[\"until\"] = until.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        if author is not github.GithubObject.NotSet:\n            if isinstance(author, (github.NamedUser.NamedUser, github.AuthenticatedUser.AuthenticatedUser)):\n                url_parameters[\"author\"] = author.login\n            else:\n                url_parameters[\"author\"] = author\n        return github.PaginatedList.PaginatedList(\n            github.Commit.Commit,\n            self._requester,\n            self.url + \"/commits\",\n            url_parameters\n        )",
        "rewrite": "def get_commits(self, sha=github.GithubObject.NotSet, path=github.GithubObject.NotSet, since=github.GithubObject.NotSet, until=github.GithubObject.NotSet, author=github.GithubObject.NotSet):\n    assert sha is github.GithubObject.NotSet or isinstance(sha, (str, unicode)), sha\n    assert path is github.GithubObject.NotSet or isinstance(path, (str, unicode)), path\n    assert since is github.GithubObject.NotSet or isinstance(since, datetime.datetime), since\n    assert until is github.GithubObject.NotSet or isinstance(until, datetime.datetime), until\n    assert author is github.GithubObject.NotSet or isinstance(author, (str, unicode, github.NamedUser.NamedUser, github.AuthenticatedUser.AuthenticatedUser)), author\n    \n    url_parameters = {}\n    \n    if sha is not github.GithubObject.NotSet:\n        url_parameters[\"sha\"] = sha\n    if path is not github.GithubObject.NotSet:\n        url_parameters[\"path\"] = path\n    if since is not github.GithubObject.NotSet:\n        url_parameters[\"since\"] = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    if until is not github.GithubObject.NotSet:\n        url_parameters[\"until\"] = until.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    if author is not github.GithubObject.NotSet:\n        if isinstance(author, (github.NamedUser.NamedUser, github.AuthenticatedUser.AuthenticatedUser)):\n            url_parameters[\"author\"] = author.login\n        else:\n            url_parameters[\"author\"] = author\n    \n    return github.PaginatedList.PaginatedList(github.Commit.Commit, self._requester, self.url + \"/commits\", url_parameters)"
    },
    {
        "original": "def available(name):\n    \"\"\"\n    Return True if the named service is available.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.available sshd\n    \"\"\"\n    cmd = '{0} get {1}'.format(_cmd(), name)\n    if __salt__['cmd.retcode'](cmd) == 2:\n        return False\n    return True",
        "rewrite": "def available(name):\n    cmd = 'systemctl status {0}'.format(name)\n    if __salt__['cmd.retcode'](cmd) != 0:\n        return False\n    return True"
    },
    {
        "original": "def set_stack(self, stack_dump, stack_top):\n        \"\"\"\n        Stack dump is a dump of the stack from gdb, i.e. the result of the following gdb command :\n\n        ``dump binary memory [stack_dump] [begin_addr] [end_addr]``\n\n        We set the stack to the same addresses as the gdb session to avoid pointers corruption.\n\n        :param stack_dump:  The dump file.\n        :param stack_top:   The address of the top of the stack in the gdb session.\n        \"\"\"\n        data = self._read_data(stack_dump)\n        self.real_stack_top = stack_top\n        addr = stack_top - len(data) # Address of the bottom of the stack\n        l.info(\"Setting stack from 0x%x up to %#x\", addr, stack_top)\n        #FIXME: we should probably make we don't overwrite other stuff loaded there\n        self._write(addr, data)",
        "rewrite": "def set_stack(self, stack_dump, stack_top):\n    data = self._read_data(stack_dump)\n    self.real_stack_top = stack_top\n    addr = stack_top - len(data)\n    l.info(\"Setting stack from 0x%x up to %#x\", addr, stack_top)\n    self._write(addr, data)"
    },
    {
        "original": "def set_subnet_name(name):\n    \"\"\"\n    Set the local subnet name\n\n    :param str name: The new local subnet name\n\n    .. note::\n       Spaces are changed to dashes. Other special characters are removed.\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        The following will be set as 'Mikes-Mac'\n        salt '*' system.set_subnet_name \"Mike's Mac\"\n    \"\"\"\n    cmd = 'systemsetup -setlocalsubnetname \"{0}\"'.format(name)\n    __utils__['mac_utils.execute_return_success'](cmd)\n\n    return __utils__['mac_utils.confirm_updated'](\n        name,\n        get_subnet_name,\n    )",
        "rewrite": "def set_subnet_name(name):\n    cmd = 'systemsetup -setlocalsubnetname \"{0}\"'.format(name.replace(\" \", \"-\").encode('ascii', 'ignore').decode('ascii'))\n    __utils__['mac_utils.execute_return_success'](cmd)\n    return __utils__['mac_utils.confirm_updated'](\n        name,\n        get_subnet_name,\n    )"
    },
    {
        "original": "def perform_es_corr(self, lattice, q, step=1e-4):\n        \"\"\"\n        Peform Electrostatic Freysoldt Correction\n        \"\"\"\n        logger.info(\"Running Freysoldt 2011 PC calculation (should be \" \"equivalent to sxdefectalign)\")\n        logger.debug(\"defect lattice constants are (in angstroms)\" + str(lattice.abc))\n\n        [a1, a2, a3] = ang_to_bohr * np.array(lattice.get_cartesian_coords(1))\n        logging.debug(\"In atomic units, lat consts are (in bohr):\" + str([a1, a2, a3]))\n        vol = np.dot(a1, np.cross(a2, a3))  # vol in bohr^3\n\n        def e_iso(encut):\n            gcut = eV_to_k(encut)  # gcut is in units of 1/A\n            return scipy.integrate.quad(lambda g: self.q_model.rho_rec(g * g)**2, step, gcut)[0] * (q**2) / np.pi\n\n        def e_per(encut):\n            eper = 0\n            for g2 in generate_reciprocal_vectors_squared(a1, a2, a3, encut):\n                eper += (self.q_model.rho_rec(g2)**2) / g2\n            eper *= (q**2) * 2 * round(np.pi, 6) / vol\n            eper += (q**2) * 4 * round(np.pi, 6) \\\n                * self.q_model.rho_rec_limit0 / vol\n            return eper\n\n        eiso = converge(e_iso, 5, self.madetol, self.energy_cutoff)\n        logger.debug(\"Eisolated : %f\", round(eiso, 5))\n\n        eper = converge(e_per, 5, self.madetol, self.energy_cutoff)\n\n        logger.info(\"Eperiodic : %f hartree\", round(eper, 5))\n        logger.info(\"difference (periodic-iso) is %f hartree\", round(eper - eiso, 6))\n        logger.info(\"difference in (eV) is %f\", round((eper - eiso) * hart_to_ev, 4))\n\n        es_corr = round((eiso - eper) / self.dielectric * hart_to_ev, 6)\n        logger.info(\"Defect Correction without alignment %f (eV): \", es_corr)\n        return es_corr",
        "rewrite": "def perform_es_corr(self, lattice, q, step=1e-4):\n    \"\"\"\n    Perform Electrostatic Freysoldt Correction\n    \"\"\"\n    logger.info(\"Running Freysoldt 2011 PC calculation (should be equivalent to sxdefectalign)\")\n    logger.debug(\"Defect lattice constants are (in angstroms): \" + str(lattice.abc))\n\n    [a1, a2, a3] = ang_to_bohr * np.array(lattice.get_cartesian_coords(1))\n    logging.debug(\"In atomic units, lat consts are (in bohr): \" + str([a1, a2, a3]))\n    vol = np.dot(a1, np.cross(a2, a3))  # volume in bohr^3\n\n    def e_iso(encut):\n        gcut = eV_to_k(encut)  # gcut is in units of 1/A\n        return scipy.integrate.quad(lambda g: self.q_model.rho_rec(g * g)**2, step, gcut)[0] * (q**2) / np.pi\n\n    def e_per(encut):\n        eper = 0\n        for g2 in generate_reciprocal_vectors_squared(a1, a2, a3, encut):\n            eper += (self.q_model.rho_rec(g2)**2) / g2\n        eper *= (q**2) * 2 * round(np.pi, 6) / vol\n        eper += (q**2) * 4 * round(np.pi, 6) * self.q_model.rho_rec_limit0 / vol\n        return eper\n\n    eiso = converge(e_iso, 5, self.madetol, self.energy_cutoff)\n    logger.debug(\"Eisolated: %f\", round(eiso, 5))\n\n    eper = converge(e_per, 5, self.madetol, self.energy_cutoff)\n\n    logger.info(\"Eperiodic: %f hartree\", round(eper, 5))\n    logger.info(\"Difference (periodic-iso) is %f hartree\", round(eper - eiso, 6))\n    logger.info(\"Difference in (eV) is %f\", round((eper - eiso) * hart_to_ev, 4))\n\n    es_corr = round((eiso - eper) / self.dielectric * hart_to_ev, 6)\n    logger.info(\"Defect Correction without alignment: %f (eV)\", es_corr)\n    return es_corr"
    },
    {
        "original": "def add_peer_parser(subparsers, parent_parser):\n    \"\"\"Adds argument parser for the peer command\n\n        Args:\n            subparsers: Add parsers to this subparser object\n            parent_parser: The parent argparse.ArgumentParser object\n    \"\"\"\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)",
        "rewrite": "def add_peer_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\"\n    )\n\n    grand_parsers = parser.add_subparsers(title='subcommands', dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)"
    },
    {
        "original": "def revoke_grant(key_id, grant_id, region=None, key=None, keyid=None,\n                 profile=None):\n    \"\"\"\n    Revoke a grant from a key.\n\n    CLI example::\n\n        salt myminion boto_kms.revoke_grant 'alias/mykey' 8u89hf-j09j...\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if key_id.startswith('alias/'):\n        key_id = _get_key_id(key_id)\n    r = {}\n    try:\n        conn.revoke_grant(key_id, grant_id)\n        r['result'] = True\n    except boto.exception.BotoServerError as e:\n        r['result'] = False\n        r['error'] = __utils__['boto.get_error'](e)\n    return r",
        "rewrite": "def revoke_grant(key_id, grant_id, region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    \n    if key_id.startswith('alias/'):\n        key_id = _get_key_id(key_id)\n    \n    result = {}\n    \n    try:\n        conn.revoke_grant(key_id, grant_id)\n        result['result'] = True\n    except boto.exception.BotoServerError as e:\n        result['result'] = False\n        result['error'] = __utils__['boto.get_error'](e)\n    \n    return result"
    },
    {
        "original": "def shutdown(self):\n        \"\"\"Manually stop the pool.  This is only necessary from tests, as the\n        pool will stop itself when the reactor stops under normal\n        circumstances.\"\"\"\n        if not self._stop_evt:\n            return  # pool is already stopped\n        self.reactor.removeSystemEventTrigger(self._stop_evt)\n        self._stop()",
        "rewrite": "def shutdown(self):\n    if not self._stop_evt:\n        return\n    self.reactor.removeSystemEventTrigger(self._stop_evt)\n    self._stop()"
    },
    {
        "original": "def get_uvec(vec):\n    \"\"\" Gets a unit vector parallel to input vector\"\"\"\n    l = np.linalg.norm(vec)\n    if l < 1e-8:\n        return vec\n    return vec / l",
        "rewrite": "def get_uvec(vec):\n    l = np.linalg.norm(vec)\n    if l < 1e-8:\n        return vec\n    return vec / l"
    },
    {
        "original": "def login(self, username=None, password=None, **kwargs):\n        \"\"\"Login to a reddit site.\n\n        **DEPRECATED**. Will be removed in a future version of PRAW.\n\n        https://www.reddit.com/comments/2ujhkr/\n        https://www.reddit.com/comments/37e2mv/\n\n        Look for username first in parameter, then praw.ini and finally if both\n        were empty get it from stdin. Look for password in parameter, then\n        praw.ini (but only if username matches that in praw.ini) and finally\n        if they both are empty get it with getpass. Add the variables ``user``\n        (username) and ``pswd`` (password) to your praw.ini file to allow for\n        auto-login.\n\n        A successful login will overwrite any existing authentication.\n\n        \"\"\"\n        if password and not username:\n            raise Exception('Username must be provided when password is.')\n        user = username or self.config.user\n        if not user:\n            sys.stdout.write('Username: ')\n            sys.stdout.flush()\n            user = sys.stdin.readline().strip()\n            pswd = None\n        else:\n            pswd = password or self.config.pswd\n        if not pswd:\n            import getpass\n            pswd = getpass.getpass('Password for {0}: '.format(user)\n                                   .encode('ascii', 'ignore'))\n\n        data = {'passwd': pswd,\n                'user': user}\n        self.clear_authentication()\n        self.request_json(self.config['login'], data=data)\n        # Update authentication settings\n        self._authentication = True\n        self.user = self.get_redditor(user)\n        self.user.__class__ = objects.LoggedInRedditor",
        "rewrite": "def login(self, username=None, password=None, **kwargs):\n    if password and not username:\n        raise Exception('Username must be provided when password is.')\n    user = username or self.config.user\n    if not user:\n        sys.stdout.write('Username: ')\n        sys.stdout.flush()\n        user = sys.stdin.readline().strip()\n        pswd = None\n    else:\n        pswd = password or self.config.pswd\n    if not pswd:\n        import getpass\n        pswd = getpass.getpass('Password for {0}: '.format(user)\n                               .encode('ascii', 'ignore'))\n\n    data = {'passwd': pswd,\n            'user': user}\n    self.clear_authentication()\n    self.request_json(self.config['login'], data=data)\n    self._authentication = True\n    self.user = self.get_redditor(user)\n    self.user.__class__ = objects.LoggedInRedditor"
    },
    {
        "original": "def expand_paths(inputs):\n    \"\"\"Yield sys.path directories that might contain \"old-style\" packages\"\"\"\n\n    seen = {}\n\n    for dirname in inputs:\n        dirname = normalize_path(dirname)\n        if dirname in seen:\n            continue\n\n        seen[dirname] = 1\n        if not os.path.isdir(dirname):\n            continue\n\n        files = os.listdir(dirname)\n        yield dirname, files\n\n        for name in files:\n            if not name.endswith('.pth'):\n                # We only care about the .pth files\n                continue\n            if name in ('easy-install.pth', 'setuptools.pth'):\n                # Ignore .pth files that we control\n                continue\n\n            # Read the .pth file\n            f = open(os.path.join(dirname, name))\n            lines = list(yield_lines(f))\n            f.close()\n\n            # Yield existing non-dupe, non-import directory lines from it\n            for line in lines:\n                if not line.startswith(\"import\"):\n                    line = normalize_path(line.rstrip())\n                    if line not in seen:\n                        seen[line] = 1\n                        if not os.path.isdir(line):\n                            continue\n                        yield line, os.listdir(line)",
        "rewrite": "import os\n\ndef expand_paths(inputs):\n    seen = {}\n    \n    for dirname in inputs:\n        dirname = normalize_path(dirname)\n        if dirname in seen:\n            continue\n        \n        seen[dirname] = 1\n        if not os.path.isdir(dirname):\n            continue\n        \n        files = os.listdir(dirname)\n        yield dirname, files\n        \n        for name in files:\n            if name.endswith('.pth'):\n                if name not in ('easy-install.pth', 'setuptools.pth'):\n                    f = open(os.path.join(dirname, name))\n                    lines = list(yield_lines(f))\n                    f.close()\n                    \n                    for line in lines:\n                        if not line.startswith(\"import\"):\n                            line = normalize_path(line.rstrip())\n                            if line not in seen:\n                                seen[line] = 1\n                                if os.path.isdir(line):\n                                    yield line, os.listdir(line)"
    },
    {
        "original": "def __f2d(frac_coords, v):\n        \"\"\"\n        Converts fractional coordinates to discrete coordinates with respect to\n        the grid size of v\n        \"\"\"\n        # frac_coords = frac_coords % 1\n        return np.array([int(frac_coords[0] * v.shape[0]),\n                         int(frac_coords[1] * v.shape[1]),\n                         int(frac_coords[2] * v.shape[2])])",
        "rewrite": "def f2d(frac_coords, v):\n    return np.array([int(frac_coords[0] * v.shape[0]),\n                     int(frac_coords[1] * v.shape[1]),\n                     int(frac_coords[2] * v.shape[2])])"
    },
    {
        "original": "def fit_anonymous(self, struct1, struct2, niggli=True):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            True/False: Whether a species mapping can map struct1 to stuct2\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2,\n                                                              niggli)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        break_on_match=True, single_match=True)\n\n        if matches:\n            return True\n        else:\n            return False",
        "rewrite": "def fit_anonymous(self, struct1, struct2, niggli=True):\n    struct1, struct2 = self._process_species([struct1, struct2])\n    struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2, niggli)\n\n    matches = self._anonymous_match(struct1, struct2, fu, s1_supercell, break_on_match=True, single_match=True)\n\n    return True if matches else False"
    },
    {
        "original": "def label_from_attrs(da, extra=''):\n    \"\"\" Makes informative labels if variable metadata (attrs) follows\n        CF conventions. \"\"\"\n\n    if da.attrs.get('long_name'):\n        name = da.attrs['long_name']\n    elif da.attrs.get('standard_name'):\n        name = da.attrs['standard_name']\n    elif da.name is not None:\n        name = da.name\n    else:\n        name = ''\n\n    if da.attrs.get('units'):\n        units = ' [{}]'.format(da.attrs['units'])\n    else:\n        units = ''\n\n    return '\\n'.join(textwrap.wrap(name + extra + units, 30))",
        "rewrite": "def label_from_attrs(da, extra=''):\n    if da.attrs.get('long_name'):\n        name = da.attrs['long_name']\n    elif da.attrs.get('standard_name'):\n        name = da.attrs['standard_name']\n    elif da.name is not None:\n        name = da.name\n    else:\n        name = ''\n\n    if da.attrs.get('units'):\n        units = ' [{}]'.format(da.attrs['units'])\n    else:\n        units = ''\n\n    return '\\n'.join(textwrap.wrap(name + extra + units, 30))"
    },
    {
        "original": "def get_interpolated_value(self, energy):\n        \"\"\"\n        Returns interpolated density for a particular energy.\n\n        Args:\n            energy: Energy to return the density for.\n        \"\"\"\n        f = {}\n        for spin in self.densities.keys():\n            f[spin] = get_linear_interpolated_value(self.energies,\n                                                    self.densities[spin],\n                                                    energy)\n        return f",
        "rewrite": "def get_interpolated_value(self, energy):\n        f = {}\n        for spin in self.densities.keys():\n            f[spin] = get_linear_interpolated_value(self.energies, self.densities[spin], energy)\n        return f"
    },
    {
        "original": "def do_image_operations(self):\n        \"\"\"\n        If ENGINE_THREADPOOL_SIZE > 0, this will schedule the image operations\n        into a threadpool.  If not, it just executes them synchronously, and\n        calls self.done_callback when it's finished.\n\n        The actual work happens in self.img_operation_worker\n        \"\"\"\n        def inner(future):\n            self.done_callback()\n\n        self.context.thread_pool.queue(\n            operation=self.img_operation_worker,\n            callback=inner\n        )",
        "rewrite": "def do_image_operations(self):\n    def inner(future):\n        self.done_callback()\n\n    self.context.thread_pool.queue(\n        operation=self.img_operation_worker,\n        callback=inner\n   )"
    },
    {
        "original": "def set_in_selected(self, key, value):\n        \"\"\"Set the (key, value) for the selected server in the list.\"\"\"\n        # Static list then dynamic one\n        if self.screen.active_server >= len(self.static_server.get_servers_list()):\n            self.autodiscover_server.set_server(\n                self.screen.active_server - len(self.static_server.get_servers_list()),\n                key, value)\n        else:\n            self.static_server.set_server(self.screen.active_server, key, value)",
        "rewrite": "def set_in_selected(self, key, value):\n    if self.screen.active_server >= len(self.static_server.get_servers_list()):\n        self.autodiscover_server.set_server(self.screen.active_server - len(self.static_server.get_servers_list()), key, value)\n    else:\n        self.static_server.set_server(self.screen.active_server, key, value)"
    },
    {
        "original": "def total_surface_energy(self):\n        \"\"\"\n        Total surface energy of the Wulff shape.\n\n        Returns:\n            (float) sum(surface_energy_hkl * area_hkl)\n        \"\"\"\n        tot_surface_energy = 0\n        for hkl in self.miller_energy_dict.keys():\n            tot_surface_energy += self.miller_energy_dict[hkl] * \\\n                                  self.miller_area_dict[hkl]\n        return tot_surface_energy",
        "rewrite": "def total_surface_energy(self):\n    \"\"\"\n    Total surface energy of the Wulff shape.\n\n    Returns:\n        (float) sum(surface_energy_hkl * area_hkl)\n    \"\"\"\n    tot_surface_energy = 0\n    for hkl in self.miller_energy_dict.keys():\n        tot_surface_energy += self.miller_energy_dict[hkl] * self.miller_area_dict[hkl]\n    return tot_surface_energy"
    },
    {
        "original": "def RecursiveMultiListChildren(self, urns, limit=None, age=NEWEST_TIME):\n    \"\"\"Recursively lists bunch of directories.\n\n    Args:\n      urns: List of urns to list children.\n      limit: Max number of children to list (NOTE: this is per urn).\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Yields:\n       (subject<->children urns) tuples. RecursiveMultiListChildren will fetch\n       children lists for initial set of urns and then will fetch children's\n       children, etc.\n\n       For example, for the following objects structure:\n       a->\n          b -> c\n            -> d\n\n       RecursiveMultiListChildren(['a']) will return:\n       [('a', ['b']), ('b', ['c', 'd'])]\n    \"\"\"\n\n    checked_urns = set()\n    urns_to_check = urns\n    while True:\n      found_children = []\n\n      for subject, values in self.MultiListChildren(\n          urns_to_check, limit=limit, age=age):\n\n        found_children.extend(values)\n        yield subject, values\n\n      checked_urns.update(urns_to_check)\n\n      urns_to_check = set(found_children) - checked_urns\n      if not urns_to_check:\n        break",
        "rewrite": "def RecursiveMultiListChildren(self, urns, limit=None, age=NEWEST_TIME):\n    checked_urns = set()\n    urns_to_check = urns\n    while True:\n        found_children = []\n        \n        for subject, values in self.MultiListChildren(urns_to_check, limit=limit, age=age):\n            found_children.extend(values)\n            yield subject, values\n        \n        checked_urns.update(urns_to_check)\n        \n        urns_to_check = set(found_children) - checked_urns\n        if not urns_to_check:\n            break"
    },
    {
        "original": "def status(config, group, accounts=(), region=None):\n    \"\"\"report current export state status\"\"\"\n    config = validate.callback(config)\n    destination = config.get('destination')\n    client = boto3.Session().client('s3')\n\n    for account in config.get('accounts', ()):\n        if accounts and account['name'] not in accounts:\n            continue\n\n        session = get_session(account['role'], region)\n        account_id = session.client('sts').get_caller_identity()['Account']\n        prefix = destination.get('prefix', '').rstrip('/') + '/%s' % account_id\n        prefix = \"%s/flow-log\" % prefix\n\n        role = account.pop('role')\n        if isinstance(role, six.string_types):\n            account['account_id'] = role.split(':')[4]\n        else:\n            account['account_id'] = role[-1].split(':')[4]\n\n        account.pop('groups')\n\n        try:\n            tag_set = client.get_object_tagging(\n                Bucket=destination['bucket'], Key=prefix).get('TagSet', [])\n        except ClientError:\n            account['export'] = 'missing'\n            continue\n        tags = {t['Key']: t['Value'] for t in tag_set}\n\n        if 'LastExport' not in tags:\n            account['export'] = 'empty'\n        else:\n            last_export = parse(tags['LastExport'])\n            account['export'] = last_export.strftime('%Y/%m/%d')\n\n    accounts = [a for a in config.get('accounts') if a in accounts or not accounts]\n    accounts.sort(key=operator.itemgetter('export'), reverse=True)\n    print(tabulate(accounts, headers='keys'))",
        "rewrite": "def status(config, group, accounts=(), region=None):\n    config = validate.callback(config)\n    destination = config.get('destination')\n    client = boto3.Session().client('s3')\n\n    for account in config.get('accounts', ()):\n        if accounts and account['name'] not in accounts:\n            continue\n\n        session = get_session(account['role'], region)\n        account_id = session.client('sts').get_caller_identity()['Account']\n        prefix = destination.get('prefix', '').rstrip('/') + '/%s' % account_id\n        prefix = \"%s/flow-log\" % prefix\n\n        role = account.pop('role')\n        if isinstance(role, six.string_types):\n            account['account_id'] = role.split(':')[4]\n        else:\n            account['account_id'] = role[-1].split(':')[4]\n\n        account.pop('groups')\n\n        try:\n            tag_set = client.get_object_tagging(\n                Bucket=destination['bucket'], Key=prefix).get('TagSet', [])\n        except ClientError:\n            account['export'] = 'missing'\n            continue\n        tags = {t['Key']: t['Value'] for t in tag_set}\n\n        if 'LastExport' not in tags:\n            account['export'] = 'empty'\n        else:\n            last_export = parse(tags['LastExport'])\n            account['export'] = last_export.strftime('%Y/%m/%d')\n\n    accounts = [a for a in config.get('accounts') if a in accounts or not accounts]\n    accounts.sort(key=operator.itemgetter('export'), reverse=True)\n    print(tabulate(accounts, headers='keys'))"
    },
    {
        "original": "def set_startup_disk(path):\n    \"\"\"\n    Set the current startup disk to the indicated path. Use\n    ``system.list_startup_disks`` to find valid startup disks on the system.\n\n    :param str path: The valid startup disk path\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.set_startup_disk /System/Library/CoreServices\n    \"\"\"\n    if path not in list_startup_disks():\n        msg = 'Invalid value passed for path.\\n' \\\n              'Must be a valid startup disk as found in ' \\\n              'system.list_startup_disks.\\n' \\\n              'Passed: {0}'.format(path)\n        raise SaltInvocationError(msg)\n\n    cmd = 'systemsetup -setstartupdisk {0}'.format(path)\n    __utils__['mac_utils.execute_return_result'](cmd)\n\n    return __utils__['mac_utils.confirm_updated'](\n        path,\n        get_startup_disk,\n    )",
        "rewrite": "def set_startup_disk(path):\n    if path not in list_startup_disks():\n        msg = 'Invalid value passed for path.\\n' \\\n              'Must be a valid startup disk as found in ' \\\n              'system.list_startup_disks.\\n' \\\n              'Passed: {0}'.format(path)\n        raise SaltInvocationError(msg)\n\n    cmd = 'systemsetup -setstartupdisk {0}'.format(path)\n    __utils__['mac_utils.execute_return_result'](cmd)\n\n    return __utils__['mac_utils.confirm_updated'](\n        path,\n        get_startup_disk,\n   )"
    },
    {
        "original": "def boot(self, name, flavor_id=0, image_id=0, timeout=300, **kwargs):\n        \"\"\"\n        Boot a cloud server.\n        \"\"\"\n        nt_ks = self.compute_conn\n        kwargs['name'] = name\n        kwargs['flavor'] = flavor_id\n        kwargs['image'] = image_id or None\n        ephemeral = kwargs.pop('ephemeral', [])\n        block_device = kwargs.pop('block_device', [])\n        boot_volume = kwargs.pop('boot_volume', None)\n        snapshot = kwargs.pop('snapshot', None)\n        swap = kwargs.pop('swap', None)\n        kwargs['block_device_mapping_v2'] = _parse_block_device_mapping_v2(\n            block_device=block_device, boot_volume=boot_volume, snapshot=snapshot,\n            ephemeral=ephemeral, swap=swap\n        )\n        response = nt_ks.servers.create(**kwargs)\n        self.uuid = response.id\n        self.password = getattr(response, 'adminPass', None)\n\n        start = time.time()\n        trycount = 0\n        while True:\n            trycount += 1\n            try:\n                return self.server_show_libcloud(self.uuid)\n            except Exception as exc:\n                log.debug(\n                    'Server information not yet available: %s', exc\n                )\n                time.sleep(1)\n                if time.time() - start > timeout:\n                    log.error('Timed out after %s seconds '\n                              'while waiting for data', timeout)\n                    return False\n\n                log.debug(\n                    'Retrying server_show() (try %s)', trycount\n                )",
        "rewrite": "def boot(self, name, flavor_id=0, image_id=0, timeout=300, **kwargs):\n    nt_ks = self.compute_conn\n    kwargs['name'] = name\n    kwargs['flavor'] = flavor_id\n    kwargs['image'] = image_id or None\n\n    ephemeral = kwargs.pop('ephemeral', [])\n    block_device = kwargs.pop('block_device', [])\n    boot_volume = kwargs.pop('boot_volume', None)\n    snapshot = kwargs.pop('snapshot', None)\n    swap = kwargs.pop('swap', None)\n\n    kwargs['block_device_mapping_v2'] = _parse_block_device_mapping_v2(\n        block_device=block_device, boot_volume=boot_volume, snapshot=snapshot,\n        ephemeral=ephemeral, swap=swap\n    )\n\n    response = nt_ks.servers.create(**kwargs)\n    self.uuid = response.id\n    self.password = getattr(response, 'adminPass', None)\n\n    start = time.time()\n    trycount = 0\n\n    while True:\n        trycount += 1\n        try:\n            return self.server_show_libcloud(self.uuid)\n        except Exception as exc:\n            log.debug(\n                'Server information not yet available: %s', exc\n            )\n            time.sleep(1)\n\n            if time.time() - start > timeout:\n                log.error('Timed out after %s seconds while waiting for data', timeout)\n                return False\n\n            log.debug(\n                'Retrying server_show() (try %s)', trycount\n            )"
    },
    {
        "original": "def get_forks(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/forks <http://developer.github.com/v3/repos/forks>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Repository.Repository`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            Repository,\n            self._requester,\n            self.url + \"/forks\",\n            None\n        )",
        "rewrite": "def get_forks(self):\n    return github.PaginatedList.PaginatedList(\n        Repository,\n        self._requester,\n        self.url + \"/forks\",\n        None\n   )"
    },
    {
        "original": "def show_version(a_device):\n    \"\"\"Execute show version command using Netmiko.\"\"\"\n    remote_conn = ConnectHandler(**a_device)\n    print()\n    print(\"#\" * 80)\n    print(remote_conn.send_command(\"show version\"))\n    print(\"#\" * 80)\n    print()",
        "rewrite": "def show_version(a_device):\n    \"\"\"Execute show version command using Netmiko.\"\"\"\n    remote_conn = ConnectHandler(**a_device)\n    print()\n    print(\"#\" * 80)\n    print(remote_conn.send_command(\"show version\"))\n    print(\"#\" * 80)"
    },
    {
        "original": "def ReadClientFullInfo(self, client_id):\n    \"\"\"Reads full client information for a single client.\n\n    Args:\n      client_id: A GRR client id string, e.g. \"C.ea3b2b71840d6fa7\".\n\n    Returns:\n      A `ClientFullInfo` instance for given client.\n\n    Raises:\n      UnknownClientError: if no client with such id was found.\n    \"\"\"\n    result = self.MultiReadClientFullInfo([client_id])\n    try:\n      return result[client_id]\n    except KeyError:\n      raise UnknownClientError(client_id)",
        "rewrite": "def ReadClientFullInfo(self, client_id):\n    result = self.MultiReadClientFullInfo([client_id])\n    try:\n        return result[client_id]\n    except KeyError:\n        raise UnknownClientError(client_id)"
    },
    {
        "original": "def _preprocess_Y(self, Y, k=None):\n        \"\"\"Convert Y to t-length list of probabilistic labels if necessary\"\"\"\n        # If not a list, convert to a singleton list\n        if not isinstance(Y, list):\n            if self.t != 1:\n                msg = \"For t > 1, Y must be a list of n-dim or [n, K_t] tensors\"\n                raise ValueError(msg)\n            Y = [Y]\n\n        if not len(Y) == self.t:\n            msg = f\"Expected Y to be a t-length list (t={self.t}), not {len(Y)}\"\n            raise ValueError(msg)\n\n        return [EndModel._preprocess_Y(self, Y_t, self.K[t]) for t, Y_t in enumerate(Y)]",
        "rewrite": "def _preprocess_Y(self, Y, k=None):\n    if not isinstance(Y, list):\n        if self.t != 1:\n            msg = \"For t > 1, Y must be a list of n-dim or [n, K_t] tensors\"\n            raise ValueError(msg)\n        Y = [Y]\n\n    if len(Y) != self.t:\n        msg = f\"Expected Y to be a t-length list (t={self.t}), not {len(Y)}\"\n        raise ValueError(msg)\n\n    return [EndModel._preprocess_Y(self, Y_t, self.K[t]) for t, Y_t in enumerate(Y)]"
    },
    {
        "original": "def send_venue(self, chat_id, latitude, longitude, title, address, foursquare_id=None, disable_notification=None,\n                   reply_to_message_id=None, reply_markup=None):\n        \"\"\"\n        Use this method to send information about a venue.\n        :param chat_id: Integer or String : Unique identifier for the target chat or username of the target channel\n        :param latitude: Float : Latitude of the venue\n        :param longitude: Float : Longitude of the venue\n        :param title: String : Name of the venue\n        :param address: String : Address of the venue\n        :param foursquare_id: String : Foursquare identifier of the venue\n        :param disable_notification:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_venue(self.token, chat_id, latitude, longitude, title, address, foursquare_id,\n                                 disable_notification, reply_to_message_id, reply_markup)\n        )",
        "rewrite": "def send_venue(self, chat_id, latitude, longitude, title, address, foursquare_id=None, disable_notification=None,\n                   reply_to_message_id=None, reply_markup=None):\n    return types.Message.de_json(\n        apihelper.send_venue(self.token, chat_id, latitude, longitude, title, address, foursquare_id,\n                             disable_notification, reply_to_message_id, reply_markup)\n    )"
    },
    {
        "original": "def generate_substitution_structures(self, atom, target_species=[],\n                                         sub_both_sides=False, range_tol=1e-2,\n                                         dist_from_surf=0):\n        \"\"\"\n        Function that performs substitution-type doping on the surface and\n            returns all possible configurations where one dopant is substituted\n            per surface. Can substitute one surface or both.\n\n        Args:\n            atom (str): atom corresponding to substitutional dopant\n            sub_both_sides (bool): If true, substitute an equivalent\n                site on the other surface\n            target_species (list): List of specific species to substitute\n            range_tol (float): Find viable substitution sites at a specific\n                distance from the surface +- this tolerance\n            dist_from_surf (float): Distance from the surface to find viable\n                substitution sites, defaults to 0 to substitute at the surface\n        \"\"\"\n\n        # Get symmetrized structure in case we want to substitue both sides\n        sym_slab = SpacegroupAnalyzer(self.slab).get_symmetrized_structure()\n\n        # Define a function for substituting a site\n        def substitute(site, i):\n            slab = self.slab.copy()\n            props = self.slab.site_properties\n            if sub_both_sides:\n                # Find an equivalent site on the other surface\n                eq_indices = [indices for indices in\n                              sym_slab.equivalent_indices if i in indices][0]\n                for ii in eq_indices:\n                    if \"%.6f\" % (sym_slab[ii].frac_coords[2]) != \\\n                                    \"%.6f\" % (site.frac_coords[2]):\n                        props[\"surface_properties\"][ii] = \"substitute\"\n                        slab.replace(ii, atom)\n                        break\n\n            props[\"surface_properties\"][i] = \"substitute\"\n            slab.replace(i, atom)\n            slab.add_site_property(\"surface_properties\",\n                                   props[\"surface_properties\"])\n            return slab\n\n        # Get all possible substitution sites\n        substituted_slabs = []\n        # Sort sites so that we can define a range relative to the position of the\n        # surface atoms, i.e. search for sites above (below) the bottom (top) surface\n        sorted_sites = sorted(sym_slab, key=lambda site: site.frac_coords[2])\n        if sorted_sites[0].surface_properties == \"surface\":\n            d = sorted_sites[0].frac_coords[2] + dist_from_surf\n        else:\n            d = sorted_sites[-1].frac_coords[2] - dist_from_surf\n\n        for i, site in enumerate(sym_slab):\n            if d - range_tol < site.frac_coords[2] < d + range_tol:\n                if target_species and site.species_string in target_species:\n                    substituted_slabs.append(substitute(site, i))\n                elif not target_species:\n                    substituted_slabs.append(substitute(site, i))\n\n        matcher = StructureMatcher()\n        return [s[0] for s in matcher.group_structures(substituted_slabs)]",
        "rewrite": "def generate_substitution_structures(self, atom, target_species=[], sub_both_sides=False, range_tol=1e-2, dist_from_surf=0):\n    sym_slab = SpacegroupAnalyzer(self.slab).get_symmetrized_structure()\n\n    def substitute(site, i):\n        slab = self.slab.copy()\n        props = self.slab.site_properties\n        if sub_both_sides:\n            eq_indices = [indices for indices in sym_slab.equivalent_indices if i in indices][0]\n            for ii in eq_indices:\n                if \"%.6f\" % (sym_slab[ii].frac_coords[2]) != \"%.6f\" % (site.frac_coords[2]):\n                    props[\"surface_properties\"][ii] = \"substitute\"\n                    slab.replace(ii, atom)\n                    break\n        props[\"surface_properties\"][i] = \"substitute\"\n        slab.replace(i, atom)\n        slab.add_site_property(\"surface_properties\", props[\"surface_properties\"])\n        return slab\n\n    substituted_slabs = []\n    sorted_sites = sorted(sym_slab, key=lambda site: site.frac_coords[2])\n    if sorted_sites[0].surface_properties == \"surface\":\n        d = sorted_sites[0].frac_coords[2] + dist_from_surf\n    else:\n        d = sorted_sites[-1].frac_coords[2] - dist_from_surf\n\n    for i, site in enumerate(sym_slab):\n        if d - range_tol < site.frac_coords[2] < d + range_tol:\n            if target_species and site.species_string in target_species:\n                substituted_slabs.append(substitute(site, i))\n            elif not target_species:\n                substituted_slabs.append(substitute(site, i))\n\n    matcher = StructureMatcher()\n    return [s[0] for s in matcher.group_structures(substituted_slabs)]"
    },
    {
        "original": "def prepare_data(data: Data) -> Tuple[int, bytes]:\n    \"\"\"\n    Convert a string or byte-like object to an opcode and a bytes-like object.\n\n    This function is designed for data frames.\n\n    If ``data`` is a :class:`str`, return ``OP_TEXT`` and a :class:`bytes`\n    object encoding ``data`` in UTF-8.\n\n    If ``data`` is a bytes-like object, return ``OP_BINARY`` and a bytes-like\n    object.\n\n    Raise :exc:`TypeError` for other inputs.\n\n    \"\"\"\n    if isinstance(data, str):\n        return OP_TEXT, data.encode(\"utf-8\")\n    elif isinstance(data, (bytes, bytearray)):\n        return OP_BINARY, data\n    elif isinstance(data, memoryview):\n        if data.c_contiguous:\n            return OP_BINARY, data\n        else:\n            return OP_BINARY, data.tobytes()\n    else:\n        raise TypeError(\"data must be bytes-like or str\")",
        "rewrite": "from typing import Tuple\n\ndef prepare_data(data: bytes) -> Tuple[int, bytes]:\n    OP_BINARY = 0\n    return OP_BINARY, data"
    },
    {
        "original": "def tune_in_no_block(self):\n        \"\"\"\n        Executes the tune_in sequence but omits extra logging and the\n        management of the event bus assuming that these are handled outside\n        the tune_in sequence\n        \"\"\"\n        # Instantiate the local client\n        self.local = salt.client.get_local_client(\n                self.opts['_minion_conf_file'], io_loop=self.io_loop)\n\n        # add handler to subscriber\n        self.pub_channel.on_recv(self._process_cmd_socket)",
        "rewrite": "def tune_in_no_block(self):\n    self.local = salt.client.get_local_client(self.opts['_minion_conf_file'], io_loop=self.io_loop)\n    self.pub_channel.on_recv(self._process_cmd_socket)"
    },
    {
        "original": "def _maybe_abandon(self, chunk):\n        \"\"\"\n        Determine if a chunk needs to be marked as abandoned.\n\n        If it does, it marks the chunk and any other chunk belong to the same\n        message as abandoned.\n        \"\"\"\n        if chunk._abandoned:\n            return True\n\n        abandon = (\n            (chunk._max_retransmits is not None and chunk._sent_count > chunk._max_retransmits) or\n            (chunk._expiry is not None and chunk._expiry < time.time())\n        )\n        if not abandon:\n            return False\n\n        chunk_pos = self._sent_queue.index(chunk)\n        for pos in range(chunk_pos, -1, -1):\n            ochunk = self._sent_queue[pos]\n            ochunk._abandoned = True\n            ochunk._retransmit = False\n            if (ochunk.flags & SCTP_DATA_FIRST_FRAG):\n                break\n        for pos in range(chunk_pos, len(self._sent_queue)):\n            ochunk = self._sent_queue[pos]\n            ochunk._abandoned = True\n            ochunk._retransmit = False\n            if (ochunk.flags & SCTP_DATA_LAST_FRAG):\n                break\n\n        return True",
        "rewrite": "def _maybe_abandon(self, chunk):\n    if chunk._abandoned:\n        return True\n\n    abandon = (\n        (chunk._max_retransmits is not None and chunk._sent_count > chunk._max_retransmits) or\n        (chunk._expiry is not None and chunk._expiry < time.time())\n    )\n    if not abandon:\n        return False\n\n    chunk_pos = self._sent_queue.index(chunk)\n    for pos in range(chunk_pos, -1, -1):\n        ochunk = self._sent_queue[pos]\n        ochunk._abandoned = True\n        ochunk._retransmit = False\n        if (ochunk.flags & SCTP_DATA_FIRST_FRAG):\n            break\n    for pos in range(chunk_pos, len(self._sent_queue)):\n        ochunk = self._sent_queue[pos]\n        ochunk._abandoned = True\n        ochunk._retransmit = False\n        if (ochunk.flags & SCTP_DATA_LAST_FRAG):\n            break\n\n    return True"
    },
    {
        "original": "def api_config(path):\n    \"\"\"\n    Read in the Salt Master config file and add additional configs that\n    need to be stubbed out for salt-api\n    \"\"\"\n    # Let's grab a copy of salt-api's required defaults\n    opts = DEFAULT_API_OPTS.copy()\n\n    # Let's override them with salt's master opts\n    opts.update(client_config(path, defaults=DEFAULT_MASTER_OPTS.copy()))\n\n    # Let's set the pidfile and log_file values in opts to api settings\n    opts.update({\n        'pidfile': opts.get('api_pidfile', DEFAULT_API_OPTS['api_pidfile']),\n        'log_file': opts.get('api_logfile', DEFAULT_API_OPTS['api_logfile']),\n    })\n\n    prepend_root_dir(opts, [\n        'api_pidfile',\n        'api_logfile',\n        'log_file',\n        'pidfile'\n    ])\n    return opts",
        "rewrite": "def api_config(path):\n    opts = DEFAULT_API_OPTS.copy()\n    opts.update(client_config(path, defaults=DEFAULT_MASTER_OPTS.copy()))\n    opts.update({\n        'pidfile': opts.get('api_pidfile', DEFAULT_API_OPTS['api_pidfile']),\n        'log_file': opts.get('api_logfile', DEFAULT_API_OPTS['api_logfile']),\n    })\n    \n    prepend_root_dir(opts, ['api_pidfile', 'api_logfile', 'log_file', 'pidfile'])\n    return opts"
    },
    {
        "original": "def _AtNonLeaf(self, attr_value, path):\n    \"\"\"Makes dictionaries expandable when dealing with plists.\"\"\"\n    if isinstance(attr_value, dict):\n      for value in self.Expand(attr_value, path[1:]):\n        yield value\n    else:\n      for v in objectfilter.ValueExpander._AtNonLeaf(self, attr_value, path):\n        yield v",
        "rewrite": "def _AtNonLeaf(self, attr_value, path):\n    if isinstance(attr_value, dict):\n        for value in self.Expand(attr_value, path[1:]):\n            yield value\n    else:\n        for v in objectfilter.ValueExpander._AtNonLeaf(self, attr_value, path):\n            yield v"
    },
    {
        "original": "def fit(self,\n            doc,             # type: str\n            predict_proba,   # type: Callable[[Any], Any]\n            ):\n        # type: (...) -> TextExplainer\n        \"\"\"\n        Explain ``predict_proba`` probabilistic classification function\n        for the ``doc`` example. This method fits a local classification\n        pipeline following LIME approach.\n\n        To get the explanation use :meth:`show_prediction`,\n        :meth:`show_weights`, :meth:`explain_prediction` or\n        :meth:`explain_weights`.\n\n        Parameters\n        ----------\n        doc : str\n            Text to explain\n        predict_proba : callable\n            Black-box classification pipeline. ``predict_proba``\n            should be a function which takes a list of strings (documents)\n            and return a matrix of shape ``(n_samples, n_classes)`` with\n            probability values - a row per document and a column per output\n            label.\n        \"\"\"\n        self.doc_ = doc\n\n        if self.position_dependent:\n            samples, sims, mask, text = self.sampler.sample_near_with_mask(\n                doc=doc,\n                n_samples=self.n_samples\n            )\n            self.vec_ = SingleDocumentVectorizer(\n                token_pattern=self.token_pattern\n            ).fit([doc])\n            X = ~mask\n        else:\n            self.vec_ = clone(self.vec).fit([doc])\n            samples, sims = self.sampler.sample_near(\n                doc=doc,\n                n_samples=self.n_samples\n            )\n            X = self.vec_.transform(samples)\n\n        if self.rbf_sigma is not None:\n            sims = rbf(1-sims, sigma=self.rbf_sigma)\n\n        self.samples_ = samples\n        self.similarity_ = sims\n        self.X_ = X\n        self.y_proba_ = predict_proba(samples)\n        self.clf_ = clone(self.clf)\n\n        self.metrics_ = _train_local_classifier(\n            estimator=self.clf_,\n            samples=X,\n            similarity=sims,\n            y_proba=self.y_proba_,\n            expand_factor=self.expand_factor,\n            random_state=self.rng_\n        )\n        return self",
        "rewrite": "def fit(self, doc: str, predict_proba: Callable[[Any], Any]) -> TextExplainer:\n    \"\"\"\n    Explain `predict_proba` probabilistic classification function\n    for the `doc` example. This method fits a local classification\n    pipeline following LIME approach.\n    \n    To get the explanation use :meth:`show_prediction`,\n    :meth:`show_weights`, :meth:`explain_prediction` or\n    :meth:`explain_weights`.\n\n    Parameters\n    ----------\n    doc : str\n        Text to explain\n    predict_proba : callable\n        Black-box classification pipeline. `predict_proba`\n        should be a function which takes a list of strings (documents)\n        and return a matrix of shape (n_samples, n_classes) with\n        probability values - a row per document and a column per output\n        label.\n    \"\"\"\n    self.doc_ = doc\n\n    if self.position_dependent:\n        samples, sims, mask, text = self.sampler.sample_near_with_mask(\n            doc=doc,\n            n_samples=self.n_samples\n        )\n        self.vec_ = SingleDocumentVectorizer(\n            token_pattern=self.token_pattern\n        ).fit([doc])\n        X = ~mask\n    else:\n        self.vec_ = clone(self.vec).fit([doc])\n        samples, sims = self.sampler.sample_near(\n            doc=doc,\n            n_samples=self.n_samples\n        )\n        X = self.vec_.transform(samples)\n\n    if self.rbf_sigma is not None:\n        sims = rbf(1-sims, sigma=self.rbf_sigma)\n\n    self.samples_ = samples\n    self.similarity_ = sims\n    self.X_ = X\n    self.y_proba_ = predict_proba(samples)\n    self.clf_ = clone(self.clf)\n\n    self.metrics_ = _train_local_classifier(\n        estimator=self.clf_,\n        samples=X,\n        similarity=sims,\n        y_proba=self.y_proba_,\n        expand_factor=self.expand_factor,\n        random_state=self.rng_\n    )\n    \n    return self"
    },
    {
        "original": "def closest_common_ancestor(self, other):\n        \"\"\"\n        Find the common ancestor between this history node and 'other'.\n\n        :param other:    the PathHistory to find a common ancestor with.\n        :return:        the common ancestor SimStateHistory, or None if there isn't one\n        \"\"\"\n        our_history_iter = reversed(HistoryIter(self))\n        their_history_iter = reversed(HistoryIter(other))\n        sofar = set()\n\n        while True:\n            our_done = False\n            their_done = False\n\n            try:\n                our_next = next(our_history_iter)\n                if our_next in sofar:\n                    # we found it!\n                    return our_next\n                sofar.add(our_next)\n            except StopIteration:\n                # we ran out of items during iteration\n                our_done = True\n\n            try:\n                their_next = next(their_history_iter)\n                if their_next in sofar:\n                    # we found it!\n                    return their_next\n                sofar.add(their_next)\n            except StopIteration:\n                # we ran out of items during iteration\n                their_done = True\n\n            # if we ran out of both lists, there's no common ancestor\n            if our_done and their_done:\n                return None",
        "rewrite": "def closest_common_ancestor(self, other):\n    our_history_iter = reversed(HistoryIter(self))\n    their_history_iter = reversed(HistoryIter(other))\n    sofar = set()\n\n    while True:\n        our_done = False\n        their_done = False\n\n        try:\n            our_next = next(our_history_iter)\n            if our_next in sofar:\n                return our_next\n            sofar.add(our_next)\n        except StopIteration:\n            our_done = True\n\n        try:\n            their_next = next(their_history_iter)\n            if their_next in sofar:\n                return their_next\n            sofar.add(their_next)\n        except StopIteration:\n            their_done = True\n\n        if our_done and their_done:\n            return None"
    },
    {
        "original": "def write_data_as_message(self, buffer, data, content_related,\n                              *, after_id=None):\n        \"\"\"\n        Writes a message containing the given data into buffer.\n\n        Returns the message id.\n        \"\"\"\n        msg_id = self._get_new_msg_id()\n        seq_no = self._get_seq_no(content_related)\n        if after_id is None:\n            body = GzipPacked.gzip_if_smaller(content_related, data)\n        else:\n            body = GzipPacked.gzip_if_smaller(content_related,\n                bytes(InvokeAfterMsgRequest(after_id, data)))\n\n        buffer.write(struct.pack('<qii', msg_id, seq_no, len(body)))\n        buffer.write(body)\n        return msg_id",
        "rewrite": "def write_data_as_message(self, buffer, data, content_related, *, after_id=None):\n    msg_id = self._get_new_msg_id()\n    seq_no = self._get_seq_no(content_related)\n    if after_id is None:\n        body = GzipPacked.gzip_if_smaller(content_related, data)\n    else:\n        body = GzipPacked.gzip_if_smaller(content_related, bytes(InvokeAfterMsgRequest(after_id, data)))\n\n    buffer.write(struct.pack('<qii', msg_id, seq_no, len(body)))\n    buffer.write(body)\n    return msg_id"
    },
    {
        "original": "def move(x, y, absolute=True, duration=0):\n    \"\"\"\n    Moves the mouse. If `absolute`, to position (x, y), otherwise move relative\n    to the current position. If `duration` is non-zero, animates the movement.\n    \"\"\"\n    x = int(x)\n    y = int(y)\n\n    # Requires an extra system call on Linux, but `move_relative` is measured\n    # in millimiters so we would lose precision.\n    position_x, position_y = get_position()\n\n    if not absolute:\n        x = position_x + x\n        y = position_y + y\n\n    if duration:\n        start_x = position_x\n        start_y = position_y\n        dx = x - start_x\n        dy = y - start_y\n\n        if dx == 0 and dy == 0:\n            _time.sleep(duration)\n        else:\n            # 120 movements per second.\n            # Round and keep float to ensure float division in Python 2\n            steps = max(1.0, float(int(duration * 120.0)))\n            for i in range(int(steps)+1):\n                move(start_x + dx*i/steps, start_y + dy*i/steps)\n                _time.sleep(duration/steps)\n    else:\n        _os_mouse.move_to(x, y)",
        "rewrite": "def move(x, y, absolute=True, duration=0):\n    x = int(x)\n    y = int(y)\n\n    position_x, position_y = get_position()\n\n    if not absolute:\n        x += position_x\n        y += position_y\n\n    if duration:\n        start_x = position_x\n        start_y = position_y\n        dx = x - start_x\n        dy = y - start_y\n\n        if dx == 0 and dy == 0:\n            _time.sleep(duration)\n        else:\n            steps = max(1.0, float(int(duration * 120.0)))\n            for i in range(int(steps)+1):\n                move(start_x + dx*i/steps, start_y + dy*i/steps)\n                _time.sleep(duration/steps)\n    else:\n        _os_mouse.move_to(x, y)"
    },
    {
        "original": "def PlistValueToPlainValue(plist):\n  \"\"\"Takes the plist contents generated by binplist and returns a plain dict.\n\n  binplist uses rich types to express some of the plist types. We need to\n  convert them to types that RDFValueArray will be able to transport.\n\n  Args:\n    plist: A plist to convert.\n\n  Returns:\n    A simple python type.\n  \"\"\"\n\n  if isinstance(plist, dict):\n    ret_value = dict()\n    for key, value in iteritems(plist):\n      ret_value[key] = PlistValueToPlainValue(value)\n    return ret_value\n  elif isinstance(plist, list):\n    return [PlistValueToPlainValue(value) for value in plist]\n  elif isinstance(plist, datetime.datetime):\n    return (calendar.timegm(plist.utctimetuple()) * 1000000) + plist.microsecond\n  return plist",
        "rewrite": "def PlistValueToPlainValue(plist):\n    if isinstance(plist, dict):\n        ret_value = dict()\n        for key, value in plist.items():\n            ret_value[key] = PlistValueToPlainValue(value)\n        return ret_value\n    elif isinstance(plist, list):\n        return [PlistValueToPlainValue(value) for value in plist]\n    elif isinstance(plist, datetime.datetime):\n        return (calendar.timegm(plist.utctimetuple()) * 1000000) + plist.microsecond\n    return plist"
    },
    {
        "original": "def _poll(self):\n        \"\"\"\n        Poll Trusted Advisor (Support) API for limit checks.\n\n        Return a dict of service name (string) keys to nested dict vals, where\n        each key is a limit name and each value the current numeric limit.\n\n        e.g.:\n        ::\n\n            {\n                'EC2': {\n                    'SomeLimit': 10,\n                }\n            }\n\n        \"\"\"\n        logger.info(\"Beginning TrustedAdvisor poll\")\n        tmp = self._get_limit_check_id()\n        if not self.have_ta:\n            logger.info('TrustedAdvisor.have_ta is False; not polling TA')\n            return {}\n        if tmp is None:\n            logger.critical(\"Unable to find 'Service Limits' Trusted Advisor \"\n                            \"check; not using Trusted Advisor data.\")\n            return\n        check_id, metadata = tmp\n        checks = self._get_refreshed_check_result(check_id)\n        region = self.ta_region or self.conn._client_config.region_name\n        res = {}\n        if checks['result'].get('status', '') == 'not_available':\n            logger.warning(\n                'Trusted Advisor returned status \"not_available\" for '\n                'service limit check; cannot retrieve limits from TA.'\n            )\n            return {}\n        if 'flaggedResources' not in checks['result']:\n            logger.warning(\n                'Trusted Advisor returned no results for '\n                'service limit check; cannot retrieve limits from TA.'\n            )\n            return {}\n        for check in checks['result']['flaggedResources']:\n            if 'region' in check and check['region'] != region:\n                continue\n            data = dict(zip(metadata, check['metadata']))\n            if data['Service'] not in res:\n                res[data['Service']] = {}\n            try:\n                val = int(data['Limit Amount'])\n            except ValueError:\n                val = data['Limit Amount']\n                if val != 'Unlimited':\n                    logger.error('TrustedAdvisor returned unknown Limit '\n                                 'Amount %s for %s - %s', val, data['Service'],\n                                 data['Limit Name'])\n                    continue\n                else:\n                    logger.debug('TrustedAdvisor setting explicit \"Unlimited\" '\n                                 'limit for %s - %s', data['Service'],\n                                 data['Limit Name'])\n            res[data['Service']][data['Limit Name']] = val\n        logger.info(\"Finished TrustedAdvisor poll\")\n        return res",
        "rewrite": "def _poll(self):\n    logger.info(\"Beginning TrustedAdvisor poll\")\n    tmp = self._get_limit_check_id()\n    if not self.have_ta:\n        logger.info('TrustedAdvisor.have_ta is False; not polling TA')\n        return {}\n    if tmp is None:\n        logger.critical(\"Unable to find 'Service Limits' Trusted Advisor check; not using Trusted Advisor data.\")\n        return\n    check_id, metadata = tmp\n    checks = self._get_refreshed_check_result(check_id)\n    region = self.ta_region or self.conn._client_config.region_name\n    res = {}\n    if checks['result'].get('status', '') == 'not_available':\n        logger.warning(\n            'Trusted Advisor returned status \"not_available\" for service limit check; cannot retrieve limits from TA.'\n        )\n        return {}\n    if 'flaggedResources' not in checks['result']:\n        logger.warning(\n            'Trusted Advisor returned no results for service limit check; cannot retrieve limits from TA.'\n        )\n        return {}\n    for check in checks['result']['flaggedResources']:\n        if 'region' in check and check['region'] != region:\n            continue\n        data = dict(zip(metadata, check['metadata']))\n        if data['Service'] not in res:\n            res[data['Service']] = {}\n        try:\n            val = int(data['Limit Amount'])\n        except ValueError:\n            val = data['Limit Amount']\n            if val != 'Unlimited':\n                logger.error('TrustedAdvisor returned unknown Limit Amount %s for %s - %s', val, data['Service'], data['Limit Name'])\n                continue\n            else:\n                logger.debug('TrustedAdvisor setting explicit \"Unlimited\" limit for %s - %s', data['Service'], data['Limit Name'])\n        res[data['Service']][data['Limit Name']] = val\n    logger.info(\"Finished TrustedAdvisor poll\")\n    return res"
    },
    {
        "original": "def get_securitygroup(vm_):\n    \"\"\"\n    Return the security group\n    \"\"\"\n    sgs = list_securitygroup()\n    securitygroup = config.get_cloud_config_value(\n        'securitygroup', vm_, __opts__, search_global=False\n    )\n\n    if not securitygroup:\n        raise SaltCloudNotFound('No securitygroup ID specified for this VM.')\n\n    if securitygroup and six.text_type(securitygroup) in sgs:\n        return sgs[securitygroup]['SecurityGroupId']\n    raise SaltCloudNotFound(\n        'The specified security group, \\'{0}\\', could not be found.'.format(\n            securitygroup)\n    )",
        "rewrite": "def get_securitygroup(vm_):\n    sgs = list_securitygroup()\n    securitygroup = config.get_cloud_config_value('securitygroup', vm_, __opts__, search_global=False)\n    \n    if not securitygroup:\n        raise SaltCloudNotFound('No securitygroup ID specified for this VM.')\n    \n    if securitygroup and str(securitygroup) in sgs:\n        return sgs[str(securitygroup)]['SecurityGroupId']\n    raise SaltCloudNotFound(\n        'The specified security group, \\'{0}\\', could not be found.'.format(securitygroup)\n    )"
    },
    {
        "original": "def load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n        # TODO: support JSON\n\n    try:\n        enc = detect_by_bom(swag_path)\n        with codecs.open(swag_path, encoding=enc) as yaml_file:\n            return yaml_file.read()\n    except IOError:\n        # not in the same dir, add dirname\n        swag_path = os.path.join(\n            root_path or os.path.dirname(__file__), swag_path\n        )\n        try:\n            enc = detect_by_bom(swag_path)\n            with codecs.open(swag_path, encoding=enc) as yaml_file:\n                return yaml_file.read()\n        except IOError:  # pragma: no cover\n            # if package dir\n            # see https://github.com/rochacbruno/flasgger/pull/104\n            # Still not able to reproduce this case\n            # test are in examples/package_example\n            # need more detail on how to reproduce IOError here\n            swag_path = swag_path.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n            path = swag_path.replace(\n                (root_path or os.path.dirname(__file__)), ''\n            ).split(os.sep)[1:]\n            site_package = imp.find_module(path[0])[1]\n            swag_path = os.path.join(site_package, os.sep.join(path[1:]))\n            with open(swag_path) as yaml_file:\n                return yaml_file.read()",
        "rewrite": "def load_from_file(swag_path, swag_type='yml', root_path=None):\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n\n    try:\n        enc = detect_by_bom(swag_path)\n        with codecs.open(swag_path, encoding=enc) as yaml_file:\n            return yaml_file.read()\n    except IOError:\n        swag_path = os.path.join(\n            root_path or os.path.dirname(__file__), swag_path\n        )\n        try:\n            enc = detect_by_bom(swag_path)\n            with codecs.open(swag_path, encoding=enc) as yaml_file:\n                return yaml_file.read()\n        except IOError:\n            swag_path = swag_path.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n            path = swag_path.replace(\n                (root_path or os.path.dirname(__file__)), ''\n            ).split(os.sep)[1:]\n            site_package = imp.find_module(path[0])[1]\n            swag_path = os.path.join(site_package, os.sep.join(path[1:]))\n            with open(swag_path) as yaml_file:\n                return yaml_file.read()"
    },
    {
        "original": "def list_common_lookups(kwargs=None, call=None):\n    \"\"\"\n    List common lookups for a particular type of item\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n\n    args = {}\n    if 'lookup' in kwargs:\n        args['lookup'] = kwargs['lookup']\n\n    response = _query('common', 'lookup/list', args=args)\n\n    return response",
        "rewrite": "def list_common_lookups(kwargs=None, call=None):\n    if kwargs is None:\n        kwargs = {}\n\n    args = {}\n    if 'lookup' in kwargs:\n        args['lookup'] = kwargs['lookup']\n\n    response = _query('common', 'lookup/list', args=args)\n\n    return response"
    },
    {
        "original": "def doc2id(self, doc):\n        \"\"\"Get the list of token_id given doc.\n\n        Args:\n            doc (list): document.\n\n        Returns:\n            list: int id of doc.\n        \"\"\"\n        doc = map(self.process_token, doc)\n        return [self.token_to_id(token) for token in doc]",
        "rewrite": "def doc2id(self, doc):\n    doc = map(self.process_token, doc)\n    return [self.token_to_id(token) for token in doc]"
    },
    {
        "original": "def cmd_minimap(action, action_space, ability_id, queued, minimap):\n  \"\"\"Do a command that needs a point on the minimap.\"\"\"\n  action_cmd = spatial(action, action_space).unit_command\n  action_cmd.ability_id = ability_id\n  action_cmd.queue_command = queued\n  minimap.assign_to(action_cmd.target_minimap_coord)",
        "rewrite": "def cmd_minimap(action, action_space, ability_id, queued, minimap):\n    action_cmd = spatial(action, action_space).unit_command\n    action_cmd.ability_id = ability_id\n    action_cmd.queue_command = queued\n    minimap.assign_to(action_cmd.target_minimap_coord)"
    },
    {
        "original": "def transpose(self, name=None):\n    \"\"\"Returns matching `Conv2DTranspose` module.\n\n    Args:\n      name: Optional string assigning name of transpose module. The default name\n        is constructed by appending \"_transpose\" to `self.name`.\n\n    Returns:\n      `Conv2DTranspose` module.\n\n    Raises:\n     base.NotSupportedError: If `rate` in any dimension > 1.\n    \"\"\"\n    if any(x > 1 for x in self._rate):\n      raise base.NotSupportedError(\n          \"Cannot transpose a dilated convolution module.\")\n\n    if any(p != self._conv_op_padding for p in self._padding):\n      raise base.NotSupportedError(\n          \"Cannot tranpose a convolution using mixed paddings or paddings \"\n          \"other than SAME or VALID.\")\n\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    def output_shape():\n      if self._data_format == DATA_FORMAT_NCHW:\n        return self.input_shape[2:4]\n      else:  # data_format == DATA_FORMAT_NHWC\n        return self.input_shape[1:3]\n\n    return Conv2DTranspose(output_channels=lambda: self._input_channels,\n                           output_shape=output_shape,\n                           kernel_shape=self._kernel_shape,\n                           stride=self._stride,\n                           padding=self._conv_op_padding,\n                           use_bias=self._use_bias,\n                           initializers=self._initializers,\n                           partitioners=self._partitioners,\n                           regularizers=self._regularizers,\n                           data_format=self._data_format,\n                           custom_getter=self._custom_getter,\n                           name=name)",
        "rewrite": "def transpose(self, name=None):\n\n    if any(x > 1 for x in self._rate):\n        raise base.NotSupportedError(\"Cannot transpose a dilated convolution module.\")\n\n    if any(p != self._conv_op_padding for p in self._padding):\n        raise base.NotSupportedError(\"Cannot tranpose a convolution using mixed paddings or paddings other than SAME or VALID.\")\n\n    if name is None:\n        name = self.module_name + \"_transpose\"\n\n    def output_shape():\n        return self.input_shape[2:4] if self._data_format == DATA_FORMAT_NCHW else self.input_shape[1:3]\n\n    return Conv2DTranspose(output_channels=lambda: self._input_channels,\n                           output_shape=output_shape,\n                           kernel_shape=self._kernel_shape,\n                           stride=self._stride,\n                           padding=self._conv_op_padding,\n                           use_bias=self._use_bias,\n                           initializers=self._initializers,\n                           partitioners=self._partitioners,\n                           regularizers=self._regularizers,\n                           data_format=self._data_format,\n                           custom_getter=self._custom_getter,\n                           name=name)"
    },
    {
        "original": "def get_if_not_set(self, addresses):\n        \"\"\"Returns the value at an address if it was an input to the txn but\n        never set. It returns None if that address was never set in the\n        merkle database, or if the address is not within the context.\n\n        Args:\n            addresses (list of str): The full 70 character addresses.\n\n        Returns:\n            (list): bytes at that address but not set within the context\n        \"\"\"\n\n        with self._lock:\n            results = []\n            for add in addresses:\n                results.append(self._get_if_not_set(add))\n            return results",
        "rewrite": "def get_if_not_set(self, addresses):\n    with self._lock:\n        results = [self._get_if_not_set(add) for add in addresses]\n    return results"
    },
    {
        "original": "def update(self, other):\n        \"\"\"Merges other item with this object\n\n        Args:\n            other: Object containing items to merge into this object\n                Must be a dictionary or NdMapping type\n        \"\"\"\n        if isinstance(other, NdMapping):\n            dims = [d for d in other.kdims if d not in self.kdims]\n            if len(dims) == other.ndims:\n                raise KeyError(\"Cannot update with NdMapping that has\"\n                               \" a different set of key dimensions.\")\n            elif dims:\n                other = other.drop_dimension(dims)\n            other = other.data\n        for key, data in other.items():\n            self._add_item(key, data, sort=False)\n        if self.sort:\n            self._resort()",
        "rewrite": "def update(self, other):\n        if isinstance(other, NdMapping):\n            other_dims = [dim for dim in other.kdims if dim not in self.kdims]\n            if len(other_dims) == other.ndims:\n                raise KeyError(\"Cannot update with NdMapping that has a different set of key dimensions.\")\n            elif other_dims:\n                other = other.drop_dimension(other_dims)\n            other = other.data\n        for key, data in other.items():\n            self._add_item(key, data, sort=False)\n        if self.sort:\n            self._resort()"
    },
    {
        "original": "def Shape(docs, drop=0.0):\n    \"\"\"Get word shapes.\"\"\"\n    ids = numpy.zeros((sum(len(doc) for doc in docs),), dtype=\"i\")\n    i = 0\n    for doc in docs:\n        for token in doc:\n            ids[i] = token.shape\n            i += 1\n    return ids, None",
        "rewrite": "def shape(docs, drop=0.0):\n    ids = numpy.zeros((sum(len(doc) for doc in docs),), dtype=\"i\")\n    i = 0\n    for doc in docs:\n        for token in doc:\n            ids[i] = token.shape\n            i += 1\n    return ids, None"
    },
    {
        "original": "def stop(self, timeout=None):\n        \"\"\"\n        Stop the producer (async mode). Blocks until async thread completes.\n        \"\"\"\n        if timeout is not None:\n            log.warning('timeout argument to stop() is deprecated - '\n                        'it will be removed in future release')\n\n        if not self.async_send:\n            log.warning('producer.stop() called, but producer is not async')\n            return\n\n        if self.stopped:\n            log.warning('producer.stop() called, but producer is already stopped')\n            return\n\n        if self.async_send:\n            self.queue.put((STOP_ASYNC_PRODUCER, None, None))\n            self.thread_stop_event.set()\n            self.thread.join()\n\n        if hasattr(self, '_cleanup_func'):\n            # Remove cleanup handler now that we've stopped\n\n            # py3 supports unregistering\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(self._cleanup_func)  # pylint: disable=no-member\n\n            # py2 requires removing from private attribute...\n            else:\n\n                # ValueError on list.remove() if the exithandler no longer exists\n                # but that is fine here\n                try:\n                    atexit._exithandlers.remove(  # pylint: disable=no-member\n                        (self._cleanup_func, (self,), {}))\n                except ValueError:\n                    pass\n\n            del self._cleanup_func\n\n        self.stopped = True",
        "rewrite": "def stop(self, timeout=None):\n    if timeout is not None:\n        log.warning('timeout argument to stop() is deprecated - it will be removed in future release')\n\n    if not self.async_send:\n        log.warning('producer.stop() called, but producer is not async')\n        return\n\n    if self.stopped:\n        log.warning('producer.stop() called, but producer is already stopped')\n        return\n\n    if self.async_send:\n        self.queue.put((STOP_ASYNC_PRODUCER, None, None))\n        self.thread_stop_event.set()\n        self.thread.join()\n\n    if hasattr(self, '_cleanup_func'):\n        if hasattr(atexit, 'unregister'):\n            atexit.unregister(self._cleanup_func)\n        else:\n            try:\n                atexit._exithandlers.remove((self._cleanup_func, (self,), {}))\n            except ValueError:\n                pass\n\n        del self._cleanup_func\n\n    self.stopped = True"
    },
    {
        "original": "def _get_column_names(self):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        column_names = set()\n        for column in self.df:\n            column_names.add(column)\n        for column in self.unmaterialized_cols:\n            column_names.add(column)\n        return list(column_names)",
        "rewrite": "def _get_column_names(self):\n    column_names = set()\n    for column in self.df:\n        column_names.add(column)\n    for column in self.unmaterialized_cols:\n        column_names.add(column)\n    return list(column_names)"
    },
    {
        "original": "def set_threshold_override(self, service_name, limit_name,\n                               warn_percent=None, warn_count=None,\n                               crit_percent=None, crit_count=None):\n        \"\"\"\n        Set a manual override on the threshold (used for determining\n        warning/critical status) for a specific limit. See\n        :py:class:`~.AwsLimitChecker` for information on Warning and\n        Critical thresholds.\n\n        See :py:meth:`.AwsLimit.set_threshold_override`.\n\n        :param service_name: the name of the service to override limit for\n        :type service_name: str\n        :param limit_name: the name of the limit to override:\n        :type limit_name: str\n        :param warn_percent: new warning threshold, percentage used\n        :type warn_percent: int\n        :param warn_count: new warning threshold, actual count/number\n        :type warn_count: int\n        :param crit_percent: new critical threshold, percentage used\n        :type crit_percent: int\n        :param crit_count: new critical threshold, actual count/number\n        :type crit_count: int\n        \"\"\"\n        self.services[service_name].set_threshold_override(\n            limit_name,\n            warn_percent=warn_percent,\n            warn_count=warn_count,\n            crit_percent=crit_percent,\n            crit_count=crit_count\n        )",
        "rewrite": "def set_threshold_override(self, service_name, limit_name,\n                             warn_percent=None, warn_count=None,\n                             crit_percent=None, crit_count=None):\n    self.services[service_name].set_threshold_override(\n        limit_name,\n        warn_percent=warn_percent,\n        warn_count=warn_count,\n        crit_percent=crit_percent,\n        crit_count=crit_count\n    )"
    },
    {
        "original": "def _cx_state_psutil(self, tags=None):\n        \"\"\"\n        Collect metrics about connections state using psutil\n        \"\"\"\n        metrics = defaultdict(int)\n        tags = [] if tags is None else tags\n        for conn in psutil.net_connections():\n            protocol = self._parse_protocol_psutil(conn)\n            status = self.tcp_states['psutil'].get(conn.status)\n            metric = self.cx_state_gauge.get((protocol, status))\n            if metric is None:\n                self.log.warning('Metric not found for: %s,%s', protocol, status)\n            else:\n                metrics[metric] += 1\n\n        for metric, value in iteritems(metrics):\n            self.gauge(metric, value, tags=tags)",
        "rewrite": "def _cx_state_psutil(self, tags=None):\n    metrics = defaultdict(int)\n    tags = [] if tags is None else tags\n    for conn in psutil.net_connections():\n        protocol = self._parse_protocol_psutil(conn)\n        status = self.tcp_states['psutil'].get(conn.status)\n\n        metric = self.cx_state_gauge.get((protocol, status))\n        if metric is None:\n            self.log.warning('Metric not found for: %s,%s', protocol, status)\n        else:\n            metrics[metric] += 1\n\n    for metric, value in iteritems(metrics):\n        self.gauge(metric, value, tags=tags)"
    },
    {
        "original": "def broadcast_variables(*variables):\n    \"\"\"Given any number of variables, return variables with matching dimensions\n    and broadcast data.\n\n    The data on the returned variables will be a view of the data on the\n    corresponding original arrays, but dimensions will be reordered and\n    inserted so that both broadcast arrays have the same dimensions. The new\n    dimensions are sorted in order of appearance in the first variable's\n    dimensions followed by the second variable's dimensions.\n    \"\"\"\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple(var.set_dims(dims_map) if var.dims != dims_tuple else var\n                 for var in variables)",
        "rewrite": "def broadcast_variables(*variables):\n    dims_map = _unified_dims(variables)\n    dims_tuple = tuple(dims_map)\n    return tuple(var.set_dims(dims_map) if var.dims != dims_tuple else var\n                 for var in variables)"
    },
    {
        "original": "def _InitSSLContext(self, cafile=None,\n                      disable_ssl_certificate_validation=False):\n    \"\"\"Creates a ssl.SSLContext with the given settings.\n\n    Args:\n      cafile: A str identifying the resolved path to the cafile. If not set,\n        this will use the system default cafile.\n      disable_ssl_certificate_validation: A boolean indicating whether\n        certificate verification is disabled. For security purposes, it is\n        highly recommended that certificate verification remain enabled.\n\n    Returns:\n      An ssl.SSLContext instance, or None if the version of Python being used\n      doesn't support it.\n    \"\"\"\n    # Attempt to create a context; this should succeed in Python 2 versions\n    # 2.7.9+ and Python 3 versions 3.4+.\n    try:\n      if disable_ssl_certificate_validation:\n        ssl._create_default_https_context = ssl._create_unverified_context\n        ssl_context = ssl.create_default_context()\n      else:\n        ssl_context = ssl.create_default_context(cafile=cafile)\n    except AttributeError:\n      # Earlier versions lack ssl.create_default_context()\n      # Rather than raising the exception, no context will be provided for\n      # legacy support. Of course, this means no certificate validation is\n      # taking place!\n      return None\n\n    return ssl_context",
        "rewrite": "def _InitSSLContext(self, cafile=None, disable_ssl_certificate_validation=False):\n    try:\n        if disable_ssl_certificate_validation:\n            ssl._create_default_https_context = ssl._create_unverified_context\n            ssl_context = ssl.create_default_context()\n        else:\n            ssl_context = ssl.create_default_context(cafile=cafile)\n    except AttributeError:\n        return None\n\n    return ssl_context"
    },
    {
        "original": "def run(self):\n        \"\"\"\n        wait for subprocess to terminate and return subprocess' return code.\n        If timeout is reached, throw TimedProcTimeoutError\n        \"\"\"\n        def receive():\n            if self.with_communicate:\n                self.stdout, self.stderr = self.process.communicate(input=self.stdin)\n            elif self.wait:\n                self.process.wait()\n\n        if not self.timeout:\n            receive()\n        else:\n            rt = threading.Thread(target=receive)\n            rt.start()\n            rt.join(self.timeout)\n            if rt.isAlive():\n                # Subprocess cleanup (best effort)\n                self.process.kill()\n\n                def terminate():\n                    if rt.isAlive():\n                        self.process.terminate()\n                threading.Timer(10, terminate).start()\n                raise salt.exceptions.TimedProcTimeoutError(\n                    '{0} : Timed out after {1} seconds'.format(\n                        self.command,\n                        six.text_type(self.timeout),\n                    )\n                )\n        return self.process.returncode",
        "rewrite": "def run(self):\n    def receive():\n        if self.with_communicate:\n            self.stdout, self.stderr = self.process.communicate(input=self.stdin)\n        elif self.wait:\n            self.process.wait()\n\n    if not self.timeout:\n        receive()\n    else:\n        rt = threading.Thread(target=receive)\n        rt.start()\n        rt.join(self.timeout)\n        if rt.isAlive():\n            self.process.kill()\n\n            def terminate():\n                if rt.isAlive():\n                    self.process.terminate()\n            threading.Timer(10, terminate).start()\n            raise salt.exceptions.TimedProcTimeoutError(\n                '{0} : Timed out after {1} seconds'.format(\n                    self.command,\n                    six.text_type(self.timeout),\n                )\n            )\n    return self.process.returncode"
    },
    {
        "original": "def _create_event(self, alert_type, msg_title, msg, server, tags=None):\n        \"\"\"\n        Create an event object\n        \"\"\"\n        msg_title = 'Couchbase {}: {}'.format(server, msg_title)\n        msg = 'Couchbase instance {} {}'.format(server, msg)\n\n        return {\n            'timestamp': int(time.time()),\n            'event_type': 'couchbase_rebalance',\n            'msg_text': msg,\n            'msg_title': msg_title,\n            'alert_type': alert_type,\n            'source_type_name': self.SOURCE_TYPE_NAME,\n            'aggregation_key': server,\n            'tags': tags,\n        }",
        "rewrite": "def _create_event(self, alert_type, msg_title, msg, server, tags=None):\n    msg_title = 'Couchbase {}: {}'.format(server, msg_title)\n    msg = 'Couchbase instance {} {}'.format(server, msg)\n    \n    return {\n        'timestamp': int(time.time()),\n        'event_type': 'couchbase_rebalance',\n        'msg_text': msg,\n        'msg_title': msg_title,\n        'alert_type': alert_type,\n        'source_type_name': self.SOURCE_TYPE_NAME,\n        'aggregation_key': server,\n        'tags': tags,\n    }"
    },
    {
        "original": "def bin(values, bins, labels=None):\n    \"\"\"Bins data into declared bins\n\n    Bins data into declared bins. By default each bin is labelled\n    with bin center values but an explicit list of bin labels may be\n    defined.\n\n    Args:\n        values: Array of values to be binned\n        bins: List or array containing the bin boundaries\n        labels: List of labels to assign to each bin\n            If the bins are length N the labels should be length N-1\n\n    Returns:\n        Array of binned values\n    \"\"\"\n    bins = np.asarray(bins)\n    if labels is None:\n        labels = (bins[:-1] + np.diff(bins)/2.)\n    else:\n        labels = np.asarray(labels)\n    dtype = 'float' if labels.dtype.kind == 'f' else 'O'\n    binned = np.full_like(values, (np.nan if dtype == 'f' else None), dtype=dtype)\n    for lower, upper, label in zip(bins[:-1], bins[1:], labels):\n        condition = (values > lower) & (values <= upper)\n        binned[np.where(condition)[0]] = label\n    return binned",
        "rewrite": "import numpy as np\n\ndef bin(values, bins, labels=None):\n    bins = np.asarray(bins)\n    if labels is None:\n        labels = (bins[:-1] + np.diff(bins)/2.)\n    else:\n        labels = np.asarray(labels)\n    dtype = 'float' if labels.dtype.kind == 'f' else 'O'\n    binned = np.full_like(values, (np.nan if dtype == 'f' else None), dtype=dtype)\n    for lower, upper, label in zip(bins[:-1], bins[1:], labels):\n        condition = (values > lower) & (values <= upper)\n        binned[np.where(condition)[0]] = label\n    return binned"
    },
    {
        "original": "def get_scores(cat_word_counts, not_cat_word_counts,\n\t               scaler_algo=DEFAULT_SCALER_ALGO, beta=DEFAULT_BETA):\n\t\t\"\"\" Computes balanced scaled f-scores\n\t\tParameters\n\t\t----------\n\t\tcat_word_counts : np.array\n\t\t\tcategory counts\n\t\tnot_cat_word_counts : np.array\n\t\t\tnot category counts\n\t\tscaler_algo : str\n\t\t\tFunction that scales an array to a range \\in [0 and 1]. Use 'percentile', 'normcdf'. Default.\n\t\tbeta : float\n\t\t\tBeta in (1+B^2) * (Scale(P(w|c)) * Scale(P(c|w)))/(B^2*Scale(P(w|c)) + Scale(P(c|w))). Default.\n\t\tReturns\n\t\t-------\n\t\t\tnp.array\n\t\t\tHarmonic means of scaled P(word|category)\n\t\t\t and scaled P(category|word) for >median half of scores.  Low scores are harmonic means\n\t\t\t of scaled P(word|~category) and scaled P(~category|word).  Array is squashed to between\n\t\t\t 0 and 1, with 0.5 indicating a median score.\n\t\t\"\"\"\n\n\t\tcat_scores = ScaledFScore.get_scores_for_category(cat_word_counts,\n\t\t                                                  not_cat_word_counts,\n\t\t                                                  scaler_algo,\n\t\t                                                  beta)\n\t\tnot_cat_scores = ScaledFScore.get_scores_for_category(not_cat_word_counts,\n\t\t                                                      cat_word_counts,\n\t\t                                                      scaler_algo, beta)\n\t\treturn ScoreBalancer.balance_scores(cat_scores, not_cat_scores)",
        "rewrite": "def get_scores(cat_word_counts, not_cat_word_counts,\n               scaler_algo=DEFAULT_SCALER_ALGO, beta=DEFAULT_BETA):\n    cat_scores = ScaledFScore.get_scores_for_category(cat_word_counts, not_cat_word_counts, scaler_algo, beta)\n    not_cat_scores = ScaledFScore.get_scores_for_category(not_cat_word_counts, cat_word_counts, scaler_algo, beta)\n    return ScoreBalancer.balance_scores(cat_scores, not_cat_scores)"
    },
    {
        "original": "def _add_macro_map(context, package_name, macro_map):\n    \"\"\"Update an existing context in-place, adding the given macro map to the\n    appropriate package namespace. Adapter packages get inserted into the\n    global namespace.\n    \"\"\"\n    key = package_name\n    if package_name in PACKAGES:\n        key = GLOBAL_PROJECT_NAME\n    if key not in context:\n        context[key] = {}\n\n    context[key].update(macro_map)",
        "rewrite": "def _add_macro_map(context, package_name, macro_map):\n    key = package_name if package_name in PACKAGES else GLOBAL_PROJECT_NAME\n    context.setdefault(key, {}).update(macro_map)"
    },
    {
        "original": "def _validate_incongruency(self):\n        \"\"\"\n        Checks that a detected incongruency is not caused by translation backends having a different\n        idea of what constitutes a basic block.\n        \"\"\"\n\n        ot = self._throw\n\n        try:\n            self._throw = False\n            l.debug(\"Validating incongruency.\")\n\n            if (\"UNICORN\" in self.simgr.right[0].options) ^ (\"UNICORN\" in self.simgr.left[0].options):\n                if \"UNICORN\" in self.simgr.right[0].options:\n                    unicorn_stash = 'right'\n                    normal_stash = 'left'\n                else:\n                    unicorn_stash = 'left'\n                    normal_stash = 'right'\n\n                unicorn_path = self.simgr.stashes[unicorn_stash][0]\n                normal_path = self.simgr.stashes[normal_stash][0]\n\n                if unicorn_path.arch.name in (\"X86\", \"AMD64\"):\n                    # unicorn \"falls behind\" on loop and rep instructions, since\n                    # it sees them as ending a basic block. Here, we will\n                    # step the unicorn until it's caught up\n                    npg = self.project.factory.simulation_manager(unicorn_path)\n                    npg.explore(find=lambda p: p.addr == normal_path.addr, n=200)\n                    if len(npg.found) == 0:\n                        l.debug(\"Validator failed to sync paths.\")\n                        return True\n\n                    new_unicorn = npg.found[0]\n                    delta = new_unicorn.history.block_count - normal_path.history.block_count\n                    normal_path.history.recent_block_count += delta\n                    new_normal = normal_path\n                elif unicorn_path.arch.name == \"MIPS32\":\n                    # unicorn gets ahead here, because VEX falls behind for unknown reasons\n                    # for example, this block:\n                    #\n                    # 0x1016f20:      lui     $gp, 0x17\n                    # 0x1016f24:      addiu   $gp, $gp, -0x35c0\n                    # 0x1016f28:      addu    $gp, $gp, $t9\n                    # 0x1016f2c:      addiu   $sp, $sp, -0x28\n                    # 0x1016f30:      sw      $ra, 0x24($sp)\n                    # 0x1016f34:      sw      $s0, 0x20($sp)\n                    # 0x1016f38:      sw      $gp, 0x10($sp)\n                    # 0x1016f3c:      lw      $v0, -0x6cf0($gp)\n                    # 0x1016f40:      move    $at, $at\n                    npg = self.project.factory.simulation_manager(normal_path)\n                    npg.explore(find=lambda p: p.addr == unicorn_path.addr, n=200)\n                    if len(npg.found) == 0:\n                        l.debug(\"Validator failed to sync paths.\")\n                        return True\n\n                    new_normal = npg.found[0]\n                    delta = new_normal.history.block_count - unicorn_path.history.block_count\n                    unicorn_path.history.recent_block_count += delta\n                    new_unicorn = unicorn_path\n                else:\n                    l.debug(\"Dunno!\")\n                    return True\n\n                if self.compare_paths(new_unicorn, new_normal):\n                    l.debug(\"Divergence accounted for by unicorn.\")\n                    self.simgr.stashes[unicorn_stash][0] = new_unicorn\n                    self.simgr.stashes[normal_stash][0] = new_normal\n                    return False\n                else:\n                    l.warning(\"Divergence unaccounted for by unicorn.\")\n                    return True\n            else:\n                # no idea\n                l.warning(\"Divergence unaccounted for.\")\n                return True\n        finally:\n            self._throw = ot",
        "rewrite": "def _validate_incongruency(self):\n    ot = self._throw\n    try:\n        self._throw = False\n\n        if (\"UNICORN\" in self.simgr.right[0].options) ^ (\"UNICORN\" in self.simgr.left[0].options):\n            if \"UNICORN\" in self.simgr.right[0].options:\n                unicorn_stash = 'right'\n                normal_stash = 'left'\n            else:\n                unicorn_stash = 'left'\n                normal_stash = 'right'\n\n            unicorn_path = self.simgr.stashes[unicorn_stash][0]\n            normal_path = self.simgr.stashes[normal_stash][0]\n\n            if unicorn_path.arch.name in (\"X86\", \"AMD64\"):\n                npg = self.project.factory.simulation_manager(unicorn_path)\n                npg.explore(find=lambda p: p.addr == normal_path.addr, n=200)\n                if len(npg.found) == 0:\n                    l.debug(\"Validator failed to sync paths.\")\n                    return True\n\n                new_unicorn = npg.found[0]\n                delta = new_unicorn.history.block_count - normal_path.history.block_count\n                normal_path.history.recent_block_count += delta\n                new_normal = normal_path\n            elif unicorn_path.arch.name == \"MIPS32\":\n                npg = self.project.factory.simulation_manager(normal_path)\n                npg.explore(find=lambda p: p.addr == unicorn_path.addr, n=200)\n                if len(npg.found) == 0:\n                    l.debug(\"Validator failed to sync paths.\")\n                    return True\n\n                new_normal = npg.found[0]\n                delta = new_normal.history.block_count - unicorn_path.history.block_count\n                unicorn_path.history.recent_block_count += delta\n                new_unicorn = unicorn_path\n            else:\n                l.debug(\"Dunno!\")\n                return True\n\n            if self.compare_paths(new_unicorn, new_normal):\n                l.debug(\"Divergence accounted for by unicorn.\")\n                self.simgr.stashes[unicorn_stash][0] = new_unicorn\n                self.simgr.stashes[normal_stash][0] = new_normal\n                return False\n            else:\n                l.warning(\"Divergence unaccounted for by unicorn.\")\n                return True\n        else:\n            l.warning(\"Divergence unaccounted for.\")\n            return True\n    finally:\n        self._throw = ot"
    },
    {
        "original": "def get_network_instances(self, name=\"\"):\n        \"\"\"get_network_instances implementation for EOS.\"\"\"\n\n        output = self._show_vrf()\n        vrfs = {}\n        all_vrf_interfaces = {}\n        for vrf in output:\n            if (\n                vrf.get(\"route_distinguisher\", \"\") == \"<not set>\"\n                or vrf.get(\"route_distinguisher\", \"\") == \"None\"\n            ):\n                vrf[\"route_distinguisher\"] = \"\"\n            else:\n                vrf[\"route_distinguisher\"] = py23_compat.text_type(\n                    vrf[\"route_distinguisher\"]\n                )\n            interfaces = {}\n            for interface_raw in vrf.get(\"interfaces\", []):\n                interface = interface_raw.split(\",\")\n                for line in interface:\n                    if line.strip() != \"\":\n                        interfaces[py23_compat.text_type(line.strip())] = {}\n                        all_vrf_interfaces[py23_compat.text_type(line.strip())] = {}\n\n            vrfs[py23_compat.text_type(vrf[\"name\"])] = {\n                \"name\": py23_compat.text_type(vrf[\"name\"]),\n                \"type\": \"L3VRF\",\n                \"state\": {\"route_distinguisher\": vrf[\"route_distinguisher\"]},\n                \"interfaces\": {\"interface\": interfaces},\n            }\n        all_interfaces = self.get_interfaces_ip().keys()\n        vrfs[\"default\"] = {\n            \"name\": \"default\",\n            \"type\": \"DEFAULT_INSTANCE\",\n            \"state\": {\"route_distinguisher\": \"\"},\n            \"interfaces\": {\n                \"interface\": {\n                    k: {} for k in all_interfaces if k not in all_vrf_interfaces.keys()\n                }\n            },\n        }\n\n        if name:\n            if name in vrfs:\n                return {py23_compat.text_type(name): vrfs[name]}\n            return {}\n        else:\n            return vrfs",
        "rewrite": "def get_network_instances(self, name=\"\"):\n        output = self._show_vrf()\n        vrfs = {}\n        all_vrf_interfaces = {}\n\n        for vrf in output:\n            if vrf.get(\"route_distinguisher\", \"\") in [\"<not set>\", \"None\"]:\n                vrf[\"route_distinguisher\"] = \"\"\n            else:\n                vrf[\"route_distinguisher\"] = py23_compat.text_type(vrf[\"route_distinguisher\"])\n\n            interfaces = {}\n            for interface_raw in vrf.get(\"interfaces\", []):\n                interface = [line.strip() for line in interface_raw.split(\",\") if line.strip() != \"\"]\n                for line in interface:\n                    interfaces[py23_compat.text_type(line)] = {}\n                    all_vrf_interfaces[py23_compat.text_type(line)] = {}\n\n            vrfs[py23_compat.text_type(vrf[\"name\"])] = {\n                \"name\": py23_compat.text_type(vrf[\"name\"]),\n                \"type\": \"L3VRF\",\n                \"state\": {\"route_distinguisher\": vrf[\"route_distinguisher\"]},\n                \"interfaces\": {\"interface\": interfaces},\n            }\n\n        all_interfaces = self.get_interfaces_ip().keys()\n        vrfs[\"default\"] = {\n            \"name\": \"default\",\n            \"type\": \"DEFAULT_INSTANCE\",\n            \"state\": {\"route_distinguisher\": \"\"},\n            \"interfaces\": {\n                \"interface\": {\n                    k: {} for k in all_interfaces if k not in all_vrf_interfaces.keys()\n                }\n            },\n        }\n\n        if name:\n            return {py23_compat.text_type(name): vrfs.get(name, {})}\n        else:\n            return vrfs"
    },
    {
        "original": "def directive(apply_globally=False, api=None):\n    \"\"\"A decorator that registers a single hug directive\"\"\"\n    def decorator(directive_method):\n        if apply_globally:\n            hug.defaults.directives[underscore(directive_method.__name__)] = directive_method\n        else:\n            apply_to_api = hug.API(api) if api else hug.api.from_object(directive_method)\n            apply_to_api.add_directive(directive_method)\n        directive_method.directive = True\n        return directive_method\n    return decorator",
        "rewrite": "def directive(apply_globally=False, api=None):\n    def decorator(directive_method):\n        if apply_globally:\n            hug.defaults.directives[underscore(directive_method.__name__)] = directive_method\n        else:\n            apply_to_api = hug.API(api) if api else hug.api.from_object(directive_method)\n            apply_to_api.add_directive(directive_method)\n        directive_method.directive = True\n        return directive_method\n    return decorator"
    },
    {
        "original": "def _func(self, volume, params):\n        \"\"\"\n        BirchMurnaghan equation from PRB 70, 224107\n        \"\"\"\n        e0, b0, b1, v0 = tuple(params)\n        eta = (v0 / volume) ** (1. / 3.)\n        return (e0 +\n                9. * b0 * v0 / 16. * (eta ** 2 - 1)**2 *\n                (6 + b1 * (eta ** 2 - 1.) - 4. * eta ** 2))",
        "rewrite": "def _func(self, volume, params):\n    e0, b0, b1, v0 = tuple(params)\n    eta = (v0 / volume) ** (1. / 3.)\n    return (e0 + 9. * b0 * v0 / 16. * (eta ** 2 - 1) ** 2 * (6 + b1 * (eta ** 2 - 1) - 4. * eta ** 2) )"
    },
    {
        "original": "def dframe(self, dimensions=None, multi_index=False):\n        \"\"\"Convert dimension values to DataFrame.\n\n        Returns a pandas dataframe of columns along each dimension,\n        either completely flat or indexed by key dimensions.\n\n        Args:\n            dimensions: Dimensions to return as columns\n            multi_index: Convert key dimensions to (multi-)index\n\n        Returns:\n            DataFrame of columns corresponding to each dimension\n        \"\"\"\n        import pandas as pd\n        if dimensions is None:\n            outer_dimensions = self.kdims\n            inner_dimensions = None\n        else:\n            outer_dimensions = [self.get_dimension(d) for d in dimensions\n                                if d in self.kdims]\n            inner_dimensions = [d for d in dimensions\n                                if d not in outer_dimensions]\n        inds = [(d, self.get_dimension_index(d)) for d in outer_dimensions]\n\n        dframes = []\n        for key, element in self.data.items():\n            df = element.dframe(inner_dimensions, multi_index)\n            names = [d.name for d in outer_dimensions]\n            key_dims = [(d.name, key[i]) for d, i in inds]\n            if multi_index:\n                length = len(df)\n                indexes = [[v]*length for _, v in key_dims]\n                if df.index.names != [None]:\n                    indexes += [df.index]\n                    names += list(df.index.names)\n                df = df.set_index(indexes)\n                df.index.names = names\n            else:\n                for dim, val in key_dims:\n                    dimn = 1\n                    while dim in df:\n                        dim = dim+'_%d' % dimn\n                        if dim in df:\n                            dimn += 1\n                    df.insert(0, dim, val)\n            dframes.append(df)\n        return pd.concat(dframes)",
        "rewrite": "def dataframe(self, dimensions=None, multi_index=False):\n    import pandas as pd\n    if dimensions is None:\n        outer_dimensions = self.kdims\n        inner_dimensions = None\n    else:\n        outer_dimensions = [self.get_dimension(d) for d in dimensions\n                            if d in self.kdims]\n        inner_dimensions = [d for d in dimensions\n                            if d not in outer_dimensions]\n    inds = [(d, self.get_dimension_index(d)) for d in outer_dimensions]\n\n    dataframes = []\n    for key, element in self.data.items():\n        df = element.dataframe(inner_dimensions, multi_index)\n        names = [d.name for d in outer_dimensions]\n        key_dims = [(d.name, key[i]) for d, i in inds]\n        if multi_index:\n            length = len(df)\n            indexes = [[v]*length for _, v in key_dims]\n            if df.index.names != [None]:\n                indexes += [df.index]\n                names += list(df.index.names)\n            df = df.set_index(indexes)\n            df.index.names = names\n        else:\n            for dim, val in key_dims:\n                dimn = 1\n                while dim in df:\n                    dim = dim+'_%d' % dimn\n                    if dim in df:\n                        dimn += 1\n                df.insert(0, dim, val)\n        dataframes.append(df)\n    return pd.concat(dataframes)"
    },
    {
        "original": "def deep_compare(obj1, obj2):\n    \"\"\"\n    >>> deep_compare({'1': None}, {})\n    False\n    >>> deep_compare({'1': {}}, {'1': None})\n    False\n    >>> deep_compare({'1': [1]}, {'1': [2]})\n    False\n    >>> deep_compare({'1': 2}, {'1': '2'})\n    True\n    >>> deep_compare({'1': {'2': [3, 4]}}, {'1': {'2': [3, 4]}})\n    True\n    \"\"\"\n\n    if set(list(obj1.keys())) != set(list(obj2.keys())):  # Objects have different sets of keys\n        return False\n\n    for key, value in obj1.items():\n        if isinstance(value, dict):\n            if not (isinstance(obj2[key], dict) and deep_compare(value, obj2[key])):\n                return False\n        elif str(value) != str(obj2[key]):\n            return False\n    return True",
        "rewrite": "def deep_compare(obj1, obj2):\n    \n    if set(obj1.keys()) != set(obj2.keys()):  \n        return False\n\n    for key, value in obj1.items():\n        if isinstance(value, dict):\n            if not (isinstance(obj2[key], dict) and deep_compare(value, obj2[key])):\n                return False\n        elif str(value) != str(obj2[key]):\n            return False\n    return True"
    },
    {
        "original": "def parse_age(value=None):\n    \"\"\"Parses a base-10 integer count of seconds into a timedelta.\n\n    If parsing fails, the return value is `None`.\n\n    :param value: a string consisting of an integer represented in base-10\n    :return: a :class:`datetime.timedelta` object or `None`.\n    \"\"\"\n    if not value:\n        return None\n    try:\n        seconds = int(value)\n    except ValueError:\n        return None\n    if seconds < 0:\n        return None\n    try:\n        return timedelta(seconds=seconds)\n    except OverflowError:\n        return None",
        "rewrite": "def parse_age(value=None):\n    if not value:\n        return None\n    try:\n        seconds = int(value)\n        if seconds < 0:\n            return None\n        return timedelta(seconds=seconds)\n    except (ValueError, OverflowError):\n        return None"
    },
    {
        "original": "def _GetNextInterval(self):\n    \"\"\"Returns the next Range of the file that is to be hashed.\n\n    For all fingers, inspect their next expected range, and return the\n    lowest uninterrupted range of interest. If the range is larger than\n    BLOCK_SIZE, truncate it.\n\n    Returns:\n      Next range of interest in a Range namedtuple.\n    \"\"\"\n    ranges = [x.CurrentRange() for x in self.fingers]\n    starts = set([r.start for r in ranges if r])\n    ends = set([r.end for r in ranges if r])\n    if not starts:\n      return None\n    min_start = min(starts)\n    starts.remove(min_start)\n    ends |= starts\n    min_end = min(ends)\n    if min_end - min_start > self.BLOCK_SIZE:\n      min_end = min_start + self.BLOCK_SIZE\n    return Range(min_start, min_end)",
        "rewrite": "def _get_next_interval(self):\n    ranges = [x.current_range() for x in self.fingers]\n    starts = {r.start for r in ranges if r}\n    ends = {r.end for r in ranges if r}\n    \n    if not starts:\n        return None\n    \n    min_start = min(starts)\n    starts.remove(min_start)\n    ends |= starts\n    min_end = min(ends)\n    \n    if min_end - min_start > self.BLOCK_SIZE:\n        min_end = min_start + self.BLOCK_SIZE\n    \n    return Range(min_start, min_end)"
    },
    {
        "original": "def _build_from_category_spacy_doc_iter(self, category_doc_iter):\n        \"\"\"\n        Parameters\n        ----------\n        category_doc_iter : iterator of (string category name, spacy.tokens.doc.Doc) pairs\n\n        Returns\n        ----------\n        t : TermDocMatrix\n        \"\"\"\n        term_idx_store = IndexStore()\n        category_idx_store = IndexStore()\n        metadata_idx_store = IndexStore()\n        X, mX, y = self._get_features_and_labels_from_documents_and_indexes \\\n            (category_doc_iter,\n             category_idx_store,\n             term_idx_store,\n             metadata_idx_store)\n        return TermDocMatrix(X,\n                             mX,\n                             y,\n                             term_idx_store=term_idx_store,\n                             category_idx_store=category_idx_store,\n                             metadata_idx_store=metadata_idx_store)",
        "rewrite": "def _build_from_category_spacy_doc_iter(self, category_doc_iter):\n        term_idx_store = IndexStore()\n        category_idx_store = IndexStore()\n        metadata_idx_store = IndexStore()\n        X, mX, y = self._get_features_and_labels_from_documents_and_indexes(category_doc_iter, category_idx_store, term_idx_store, metadata_idx_store)\n        return TermDocMatrix(X, mX, y, term_idx_store=term_idx_store, category_idx_store=category_idx_store, metadata_idx_store=metadata_idx_store)"
    },
    {
        "original": "def _caps_add_machine(machines, node):\n    \"\"\"\n    Parse the <machine> element of the host capabilities and add it\n    to the machines list.\n    \"\"\"\n    maxcpus = node.get('maxCpus')\n    canonical = node.get('canonical')\n    name = node.text\n\n    alternate_name = \"\"\n    if canonical:\n        alternate_name = name\n        name = canonical\n\n    machine = machines.get(name)\n    if not machine:\n        machine = {'alternate_names': []}\n        if maxcpus:\n            machine['maxcpus'] = int(maxcpus)\n        machines[name] = machine\n    if alternate_name:\n        machine['alternate_names'].append(alternate_name)",
        "rewrite": "def _caps_add_machine(machines, node):\n    maxcpus = node.get('maxCpus')\n    canonical = node.get('canonical')\n    name = node.text\n\n    alternate_name = \"\"\n    if canonical:\n        alternate_name = name\n        name = canonical\n\n    machine = machines.get(name)\n    if not machine:\n        machine = {'alternate_names': []}\n        if maxcpus:\n            machine['maxcpus'] = int(maxcpus)\n        machines[name] = machine\n    if alternate_name:\n        machine['alternate_names'].append(alternate_name)"
    },
    {
        "original": "def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    \"\"\"\n    Convert a document into a list of tokens.\n\n    This lowercases, tokenizes, de-accents (optional). -- the output are final\n    tokens = unicode strings, that won't be processed any further.\n\n    \"\"\"\n    tokens = [\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\n        if min_len <= len(token) <= max_len and not token.startswith('_')\n    ]\n    return tokens",
        "rewrite": "def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    tokens = [\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\n        if min_len <= len(token) <= max_len and not token.startswith('_')\n    ]\n    return tokens"
    },
    {
        "original": "def draw(self):\n        \"\"\"\n        Draws the alpha plot based on the values on the estimator.\n        \"\"\"\n        # Search for the correct parameters on the estimator.\n        alphas = self._find_alphas_param()\n        errors = self._find_errors_param()\n\n\n        alpha = self.estimator.alpha_ # Get decision from the estimator\n        name = self.name[:-2].lower() # Remove the CV from the label\n\n        # Plot the alpha against the error\n        self.ax.plot(alphas, errors, label=name)\n\n        # Draw a dashed vline at the alpha\n        label = \"$\\\\alpha={:0.3f}$\".format(alpha)\n        self.ax.axvline(alpha, color='k', linestyle='dashed', label=label)\n\n        return self.ax",
        "rewrite": "def draw(self):\n    alphas = self._find_alphas_param()\n    errors = self._find_errors_param()\n    \n    alpha = self.estimator.alpha_\n    name = self.name[:-2].lower()\n\n    self.ax.plot(alphas, errors, label=name)\n    \n    label = \"$\\\\alpha={:0.3f}$\".format(alpha)\n    self.ax.axvline(alpha, color='k', linestyle='dashed', label=label)\n\n    return self.ax"
    },
    {
        "original": "def OSLibraries(self):\n        \"\"\"\n        Microsoft Windows SDK Libraries\n        \"\"\"\n        if self.vc_ver <= 10.0:\n            arch_subdir = self.pi.target_dir(hidex86=True, x64=True)\n            return [os.path.join(self.si.WindowsSdkDir, 'Lib%s' % arch_subdir)]\n\n        else:\n            arch_subdir = self.pi.target_dir(x64=True)\n            lib = os.path.join(self.si.WindowsSdkDir, 'lib')\n            libver = self._sdk_subdir\n            return [os.path.join(lib, '%sum%s' % (libver , arch_subdir))]",
        "rewrite": "def OSLibraries(self):\n    if self.vc_ver <= 10.0:\n        arch_subdir = self.pi.target_dir(hidex86=True, x64=True)\n        return [os.path.join(self.si.WindowsSdkDir, 'Lib%s' % arch_subdir)]\n    else:\n        arch_subdir = self.pi.target_dir(x64=True)\n        lib = os.path.join(self.si.WindowsSdkDir, 'lib')\n        libver = self._sdk_subdir\n        return [os.path.join(lib, '%sum%s' % (libver , arch_subdir))]"
    },
    {
        "original": "def _compute_block_attributes(function):\n        \"\"\"\n        :param function:    A normalized function object.\n        :returns:           A dictionary of basic block addresses to tuples of attributes.\n        \"\"\"\n        # The attributes we use are the distance form function start, distance from function exit and whether\n        # or not it has a subfunction call\n        distances_from_start = FunctionDiff._distances_from_function_start(function)\n        distances_from_exit = FunctionDiff._distances_from_function_exit(function)\n        call_sites = function.call_sites\n\n        attributes = {}\n        for block in function.graph.nodes():\n            if block in call_sites:\n                number_of_subfunction_calls = len(call_sites[block])\n            else:\n                number_of_subfunction_calls = 0\n            # there really shouldn't be blocks that can't be reached from the start, but there are for now\n            dist_start = distances_from_start[block] if block in distances_from_start else 10000\n            dist_exit = distances_from_exit[block] if block in distances_from_exit else 10000\n\n            attributes[block] = (dist_start, dist_exit, number_of_subfunction_calls)\n\n        return attributes",
        "rewrite": "def compute_block_attributes(function):\n    distances_from_start = FunctionDiff._distances_from_function_start(function)\n    distances_from_exit = FunctionDiff._distances_from_function_exit(function)\n    call_sites = function.call_sites\n\n    attributes = {}\n    for block in function.graph.nodes():\n        number_of_subfunction_calls = len(call_sites.get(block, []))\n        dist_start = distances_from_start.get(block, 10000)\n        dist_exit = distances_from_exit.get(block, 10000)\n\n        attributes[block] = (dist_start, dist_exit, number_of_subfunction_calls)\n\n    return attributes"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'document_tone') and self.document_tone is not None:\n            _dict['document_tone'] = self.document_tone._to_dict()\n        if hasattr(self, 'sentences_tone') and self.sentences_tone is not None:\n            _dict['sentences_tone'] = [\n                x._to_dict() for x in self.sentences_tone\n            ]\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'document_tone') and self.document_tone is not None:\n        _dict['document_tone'] = self.document_tone._to_dict()\n    if hasattr(self, 'sentences_tone') and self.sentences_tone is not None:\n        _dict['sentences_tone'] = [x._to_dict() for x in self.sentences_tone]\n    return _dict"
    },
    {
        "original": "def contains_raw(self, etag):\n        \"\"\"When passed a quoted tag it will check if this tag is part of the\n        set.  If the tag is weak it is checked against weak and strong tags,\n        otherwise strong only.\"\"\"\n        etag, weak = unquote_etag(etag)\n        if weak:\n            return self.contains_weak(etag)\n        return self.contains(etag)",
        "rewrite": "def contains_raw(self, etag):\n    etag, weak = unquote_etag(etag)\n    if weak:\n        return self.contains_weak(etag)\n    return self.contains(etag)"
    },
    {
        "original": "def delete(name, region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Delete an SQS queue.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_sqs.delete myqueue region=us-east-1\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        url = conn.get_queue_url(QueueName=name)['QueueUrl']\n        conn.delete_queue(QueueUrl=url)\n    except botocore.exceptions.ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}\n    return {'result': True}",
        "rewrite": "def delete(name, region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        url = conn.get_queue_url(QueueName=name)['QueueUrl']\n        conn.delete_queue(QueueUrl=url)\n    except botocore.exceptions.ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}\n    return {'result': True}"
    },
    {
        "original": "def make_inheritable(token):\n    \"\"\"Create an inheritable handle\"\"\"\n    return win32api.DuplicateHandle(\n        win32api.GetCurrentProcess(),\n        token,\n        win32api.GetCurrentProcess(),\n        0,\n        1,\n        win32con.DUPLICATE_SAME_ACCESS\n    )",
        "rewrite": "def make_inheritable(token):\n    return win32api.DuplicateHandle(win32api.GetCurrentProcess(), token, win32api.GetCurrentProcess(), 0, 1, win32con.DUPLICATE_SAME_ACCESS)"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'age') and self.age is not None:\n            _dict['age'] = self.age._to_dict()\n        if hasattr(self, 'gender') and self.gender is not None:\n            _dict['gender'] = self.gender._to_dict()\n        if hasattr(self, 'face_location') and self.face_location is not None:\n            _dict['face_location'] = self.face_location._to_dict()\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'age') and self.age is not None:\n        _dict['age'] = self.age._to_dict()\n    if hasattr(self, 'gender') and self.gender is not None:\n        _dict['gender'] = self.gender._to_dict()\n    if hasattr(self, 'face_location') and self.face_location is not None:\n        _dict['face_location'] = self.face_location._to_dict()\n    return _dict"
    },
    {
        "original": "def plot_ic_hist(ic, ax=None):\n    \"\"\"\n    Plots Spearman Rank Information Coefficient histogram for a given factor.\n\n    Parameters\n    ----------\n    ic : pd.DataFrame\n        DataFrame indexed by date, with IC for each forward return.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    \"\"\"\n\n    ic = ic.copy()\n\n    num_plots = len(ic.columns)\n\n    v_spaces = ((num_plots - 1) // 3) + 1\n\n    if ax is None:\n        f, ax = plt.subplots(v_spaces, 3, figsize=(18, v_spaces * 6))\n        ax = ax.flatten()\n\n    for a, (period_num, ic) in zip(ax, ic.iteritems()):\n        sns.distplot(ic.replace(np.nan, 0.), norm_hist=True, ax=a)\n        a.set(title=\"%s Period IC\" % period_num, xlabel='IC')\n        a.set_xlim([-1, 1])\n        a.text(.05, .95, \"Mean %.3f \\n Std. %.3f\" % (ic.mean(), ic.std()),\n               fontsize=16,\n               bbox={'facecolor': 'white', 'alpha': 1, 'pad': 5},\n               transform=a.transAxes,\n               verticalalignment='top')\n        a.axvline(ic.mean(), color='w', linestyle='dashed', linewidth=2)\n\n    if num_plots < len(ax):\n        ax[-1].set_visible(False)\n\n    return ax",
        "rewrite": "```python\ndef plot_ic_hist(ic, ax=None):\n    ic = ic.copy()\n    num_plots = len(ic.columns)\n    v_spaces = ((num_plots - 1) // 3) + 1\n\n    if ax is None:\n        f, ax = plt.subplots(v_spaces, 3, figsize=(18, v_spaces * 6))\n        ax = ax.flatten()\n\n    for a, (period_num, ic_values) in zip(ax, ic.iteritems()):\n        sns.distplot(ic_values.replace(np.nan, 0.), norm_hist=True, ax=a)\n        a.set(title=\"%s Period IC\" % period_num, xlabel='IC')\n        a.set_xlim([-1, 1])\n        a.text(.05, .95, \"Mean %.3f \\n Std. %.3f\" % (ic_values.mean(), ic_values.std()),\n               fontsize=16,\n               bbox={'facecolor': 'white', 'alpha': 1, 'pad': 5},\n               transform=a.transAxes,\n               verticalalignment='top')\n        a.axvline(ic_values.mean(), color='w', linestyle='dashed', linewidth=2)\n\n    if num_plots < len(ax):\n        ax[-1].set_visible(False)\n\n    return ax\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'tokens') and self.tokens is not None:\n            _dict['tokens'] = self.tokens._to_dict()\n        if hasattr(self, 'sentences') and self.sentences is not None:\n            _dict['sentences'] = self.sentences\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'tokens') and self.tokens is not None:\n        _dict['tokens'] = self.tokens._to_dict()\n    if hasattr(self, 'sentences') and self.sentences is not None:\n        _dict['sentences'] = self.sentences\n    return _dict"
    },
    {
        "original": "def _read_routes_c_v1():\n    \"\"\"Retrieve Windows routes through a GetIpForwardTable call.\n\n    This is compatible with XP but won't get IPv6 routes.\"\"\"\n    def _extract_ip(obj):\n        return inet_ntop(socket.AF_INET, struct.pack(\"<I\", obj))\n    routes = []\n    for route in GetIpForwardTable():\n        ifIndex = route['ForwardIfIndex']\n        dest = route['ForwardDest']\n        netmask = route['ForwardMask']\n        nexthop = _extract_ip(route['ForwardNextHop'])\n        metric = route['ForwardMetric1']\n        # Build route\n        try:\n            iface = dev_from_index(ifIndex)\n            if iface.ip == \"0.0.0.0\":\n                continue\n        except ValueError:\n            continue\n        ip = iface.ip\n        # RouteMetric + InterfaceMetric\n        metric = metric + iface.ipv4_metric\n        routes.append((dest, netmask, nexthop, iface, ip, metric))\n    return routes",
        "rewrite": "def _read_routes_c_v1():\n    def _extract_ip(obj):\n        return inet_ntop(socket.AF_INET, struct.pack(\"<I\", obj))\n    \n    routes = []\n    \n    for route in GetIpForwardTable():\n        ifIndex = route['ForwardIfIndex']\n        dest = route['ForwardDest']\n        netmask = route['ForwardMask']\n        nexthop = _extract_ip(route['ForwardNextHop'])\n        metric = route['ForwardMetric1']\n        \n        try:\n            iface = dev_from_index(ifIndex)\n            if iface.ip == \"0.0.0.0\":\n                continue\n        except ValueError:\n            continue\n        \n        ip = iface.ip\n        metric = metric + iface.ipv4_metric\n        routes.append((dest, netmask, nexthop, iface, ip, metric))\n    \n    return routes"
    },
    {
        "original": "def _inner_read_config(path):\n    \"\"\"\n    Helper to read a named config file.\n    The grossness with the global is a workaround for this python bug:\n    https://bugs.python.org/issue21591\n    The bug prevents us from defining either a local function or a lambda\n    in the scope of read_fbcode_builder_config below.\n    \"\"\"\n    global _project_dir\n    full_path = os.path.join(_project_dir, path)\n    return read_fbcode_builder_config(full_path)",
        "rewrite": "def _inner_read_config(path):\n    global _project_dir\n    full_path = os.path.join(_project_dir, path)\n    return read_fbcode_builder_config(full_path)"
    },
    {
        "original": "def memo(f):\n        \"\"\"\n        A decorator function you should apply to ``copy``\n        \"\"\"\n        def inner(self, memo=None, **kwargs):\n            if memo is None:\n                memo = {}\n            if id(self) in memo:\n                return memo[id(self)]\n            else:\n                c = f(self, memo, **kwargs)\n                memo[id(self)] = c\n                return c\n        return inner",
        "rewrite": "def memo(f):\n    def inner(self, memo=None, **kwargs):\n        if memo is None:\n            memo = {}\n        if id(self) in memo:\n            return memo[id(self)]\n        else:\n            c = f(self, memo, **kwargs)\n            memo[id(self)] = c\n            return c\n    return inner"
    },
    {
        "original": "def _update(self, response_headers):\n        \"\"\"\n        Update the state of the rate limiter based on the response headers:\n\n            X-Ratelimit-Used: Approximate number of requests used this period\n            X-Ratelimit-Remaining: Approximate number of requests left to use\n            X-Ratelimit-Reset: Approximate number of seconds to end of period\n\n        PRAW 5's rate limiting logic is structured for making hundreds of\n        evenly-spaced API requests, which makes sense for running something\n        like a bot or crawler.\n\n        This handler's logic, on the other hand, is geared more towards\n        interactive usage. It allows for short, sporadic bursts of requests.\n        The assumption is that actual users browsing reddit shouldn't ever be\n        in danger of hitting the rate limit. If they do hit the limit, they\n        will be cutoff until the period resets.\n        \"\"\"\n\n        if 'x-ratelimit-remaining' not in response_headers:\n            # This could be because the API returned an error response, or it\n            # could be because we're using something like read-only credentials\n            # which Reddit doesn't appear to care about rate limiting.\n            return\n\n        self.used = float(response_headers['x-ratelimit-used'])\n        self.remaining = float(response_headers['x-ratelimit-remaining'])\n        self.seconds_to_reset = int(response_headers['x-ratelimit-reset'])\n        _logger.debug('Rate limit: %s used, %s remaining, %s reset',\n                      self.used, self.remaining, self.seconds_to_reset)\n\n        if self.remaining <= 0:\n            self.next_request_timestamp = time.time() + self.seconds_to_reset\n        else:\n            self.next_request_timestamp = None",
        "rewrite": "def _update(self, response_headers):\n    if 'x-ratelimit-remaining' not in response_headers:\n        return\n\n    self.used = float(response_headers.get('x-ratelimit-used', 0))\n    self.remaining = float(response_headers.get('x-ratelimit-remaining', 0))\n    self.seconds_to_reset = int(response_headers.get('x-ratelimit-reset', 0))\n    _logger.debug('Rate limit: %s used, %s remaining, %s reset',\n                  self.used, self.remaining, self.seconds_to_reset)\n\n    if self.remaining <= 0:\n        self.next_request_timestamp = time.time() + self.seconds_to_reset\n    else:\n        self.next_request_timestamp = None"
    },
    {
        "original": "def getmacbyip6(ip6, chainCC=0):\n    \"\"\"Returns the MAC address corresponding to an IPv6 address\n\n    neighborCache.get() method is used on instantiated neighbor cache.\n    Resolution mechanism is described in associated doc string.\n\n    (chainCC parameter value ends up being passed to sending function\n     used to perform the resolution, if needed)\n    \"\"\"\n\n    if isinstance(ip6, Net6):\n        ip6 = str(ip6)\n\n    if in6_ismaddr(ip6):  # Multicast\n        mac = in6_getnsmac(inet_pton(socket.AF_INET6, ip6))\n        return mac\n\n    iff, a, nh = conf.route6.route(ip6)\n\n    if iff == scapy.consts.LOOPBACK_INTERFACE:\n        return \"ff:ff:ff:ff:ff:ff\"\n\n    if nh != '::':\n        ip6 = nh  # Found next hop\n\n    mac = conf.netcache.in6_neighbor.get(ip6)\n    if mac:\n        return mac\n\n    res = neighsol(ip6, a, iff, chainCC=chainCC)\n\n    if res is not None:\n        if ICMPv6NDOptDstLLAddr in res:\n            mac = res[ICMPv6NDOptDstLLAddr].lladdr\n        else:\n            mac = res.src\n        conf.netcache.in6_neighbor[ip6] = mac\n        return mac\n\n    return None",
        "rewrite": "def getmacbyip6(ip6, chainCC=0):\n    if isinstance(ip6, Net6):\n        ip6 = str(ip6)\n\n    if in6_ismaddr(ip6):\n        mac = in6_getnsmac(inet_pton(socket.AF_INET6, ip6))\n        return mac\n\n    iff, a, nh = conf.route6.route(ip6)\n\n    if iff == scapy.consts.LOOPBACK_INTERFACE:\n        return \"ff:ff:ff:ff:ff:ff\"\n\n    if nh != '::':\n        ip6 = nh\n\n    mac = conf.netcache.in6_neighbor.get(ip6)\n    if mac:\n        return mac\n\n    res = neighsol(ip6, a, iff, chainCC=chainCC)\n\n    if res is not None:\n        if ICMPv6NDOptDstLLAddr in res:\n            mac = res[ICMPv6NDOptDstLLAddr].lladdr\n        else:\n            mac = res.src\n        conf.netcache.in6_neighbor[ip6] = mac\n        return mac\n\n    return None"
    },
    {
        "original": "def compute_tls13_traffic_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for Application data.\n        self.handshake_messages should be ClientHello...ServerFinished.\n        \"\"\"\n        hkdf = self.prcs.hkdf\n\n        self.tls13_master_secret = hkdf.extract(self.tls13_handshake_secret,\n                                                None)\n\n        cts0 = hkdf.derive_secret(self.tls13_master_secret,\n                                  b\"client application traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"client_traffic_secrets\"] = [cts0]\n\n        sts0 = hkdf.derive_secret(self.tls13_master_secret,\n                                  b\"server application traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"server_traffic_secrets\"] = [sts0]\n\n        es = hkdf.derive_secret(self.tls13_master_secret,\n                                b\"exporter master secret\",\n                                b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"exporter_secret\"] = es\n\n        if self.connection_end == \"server\":\n            # self.prcs.tls13_derive_keys(cts0)\n            self.pwcs.tls13_derive_keys(sts0)\n        elif self.connection_end == \"client\":\n            # self.pwcs.tls13_derive_keys(cts0)\n            self.prcs.tls13_derive_keys(sts0)",
        "rewrite": "def compute_tls13_traffic_secrets(self):\n    hkdf = self.prcs.hkdf\n\n    self.tls13_master_secret = hkdf.extract(self.tls13_handshake_secret, None)\n\n    cts0 = hkdf.derive_secret(self.tls13_master_secret, b\"client application traffic secret\", b\"\".join(self.handshake_messages))\n    self.tls13_derived_secrets[\"client_traffic_secrets\"] = [cts0]\n\n    sts0 = hkdf.derive_secret(self.tls13_master_secret, b\"server application traffic secret\", b\"\".join(self.handshake_messages))\n    self.tls13_derived_secrets[\"server_traffic_secrets\"] = [sts0]\n\n    es = hkdf.derive_secret(self.tls13_master_secret, b\"exporter master secret\", b\"\".join(self.handshake_messages))\n    self.tls13_derived_secrets[\"exporter_secret\"] = es\n\n    if self.connection_end == \"server\":\n        self.pwcs.tls13_derive_keys(sts0)\n    elif self.connection_end == \"client\":\n        self.prcs.tls13_derive_keys(cts0)"
    },
    {
        "original": "def close_db_connections(self, instance, db_key, db_name=None):\n        \"\"\"\n        We close the db connections explicitly b/c when we don't they keep\n        locks on the db. This presents as issues such as the SQL Server Agent\n        being unable to stop.\n        \"\"\"\n        conn_key = self._conn_key(instance, db_key, db_name)\n        if conn_key not in self.connections:\n            return\n\n        try:\n            self.connections[conn_key]['conn'].close()\n            del self.connections[conn_key]\n        except Exception as e:\n            self.log.warning(\"Could not close adodbapi db connection\\n{0}\".format(e))",
        "rewrite": "def close_db_connections(self, instance, db_key, db_name=None):\n    conn_key = self._conn_key(instance, db_key, db_name)\n    if conn_key in self.connections:\n        try:\n            self.connections[conn_key]['conn'].close()\n            del self.connections[conn_key]\n        except Exception as e:\n            self.log.warning(\"Could not close adodbapi db connection\\n{0}\".format(e))"
    },
    {
        "original": "def _populate_cache(platform, pkg_cache, mount_dir):\n    \"\"\"\n    If a ``pkg_cache`` directory is specified, then use it to populate the\n    disk image.\n    \"\"\"\n    if not pkg_cache:\n        return\n    if not os.path.isdir(pkg_cache):\n        return\n\n    if platform == 'pacman':\n        cache_dir = '{0}/var/cache/pacman/pkg'.format(mount_dir)\n\n    __salt__['file.mkdir'](cache_dir, 'root', 'root', '755')\n    __salt__['file.copy'](pkg_cache, cache_dir, recurse=True, remove_existing=True)",
        "rewrite": "def _populate_cache(platform, pkg_cache, mount_dir):\n    if pkg_cache and os.path.isdir(pkg_cache):\n        if platform == 'pacman':\n            cache_dir = '{0}/var/cache/pacman/pkg'.format(mount_dir)\n            __salt__['file.mkdir'](cache_dir, 'root', 'root', '755')\n            __salt__['file.copy'](pkg_cache, cache_dir, recurse=True, remove_existing=True)"
    },
    {
        "original": "def _close_websocket(self):\n        \"\"\"Closes the websocket connection.\"\"\"\n        close_method = getattr(self._websocket, \"close\", None)\n        if callable(close_method):\n            asyncio.ensure_future(close_method(), loop=self._event_loop)\n        self._websocket = None\n        self._dispatch_event(event=\"close\")",
        "rewrite": "def _close_websocket(self):\n        close_method = getattr(self._websocket, \"close\", None)\n        if callable(close_method):\n            asyncio.ensure_future(close_method(), loop=self._event_loop)\n        self._websocket = None\n        self._dispatch_event(event=\"close\")"
    },
    {
        "original": "def satisfiable(self, extra_constraints=(), exact=None):\n        \"\"\"\n        This function does a constraint check and checks if the solver is in a sat state.\n\n        :param extra_constraints:   Extra constraints (as ASTs) to add to s for this solve\n        :param exact:               If False, return approximate solutions.\n\n        :return:                    True if sat, otherwise false\n        \"\"\"\n        if exact is False and o.VALIDATE_APPROXIMATIONS in self.state.options:\n            er = self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints))\n            ar = self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints), exact=False)\n            if er is True:\n                assert ar is True\n            return ar\n        return self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints), exact=exact)",
        "rewrite": "def satisfiable(self, extra_constraints=(), exact=None):\n        if exact is False and o.VALIDATE_APPROXIMATIONS in self.state.options:\n            er = self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints))\n            ar = self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints), exact=False)\n            if er:\n                assert ar\n            return ar\n        return self._solver.satisfiable(extra_constraints=self._adjust_constraint_list(extra_constraints), exact=exact)"
    },
    {
        "original": "def do_indent(s, width=4, indentfirst=False):\n    \"\"\"Return a copy of the passed string, each line indented by\n    4 spaces. The first line is not indented. If you want to\n    change the number of spaces or indent the first line too\n    you can pass additional parameters to the filter:\n\n    .. sourcecode:: jinja\n\n        {{ mytext|indent(2, true) }}\n            indent by two spaces and indent the first line too.\n    \"\"\"\n    indention = u' ' * width\n    rv = (u'\\n' + indention).join(s.splitlines())\n    if indentfirst:\n        rv = indention + rv\n    return rv",
        "rewrite": "def do_indent(s, width=2, indentfirst=True):\n    indention = ' ' * width\n    rv = ('\\n' + indention).join(s.splitlines())\n    if indentfirst:\n        rv = indention + rv\n    return rv"
    },
    {
        "original": "def _probs(state: np.ndarray, indices: List[int],\n           num_qubits: int) -> List[float]:\n    \"\"\"Returns the probabilities for a measurement on the given indices.\"\"\"\n    # Tensor of squared amplitudes, shaped a rank [2, 2, .., 2] tensor.\n    tensor = np.reshape(state, [2] * num_qubits)\n\n    # Calculate the probabilities for measuring the particular results.\n    probs = [\n        np.linalg.norm(\n                tensor[linalg.slice_for_qubits_equal_to(indices, b)]) ** 2\n        for b in range(2 ** len(indices))]\n\n    # To deal with rounding issues, ensure that the probabilities sum to 1.\n    probs /= sum(probs) # type: ignore\n    return probs",
        "rewrite": "import numpy as np\nfrom typing import List\n\ndef _probs(state: np.ndarray, indices: List[int],num_qubits: int) -> List[float]:\n    tensor = np.reshape(state, [2] * num_qubits)\n    probs = [np.linalg.norm(tensor[np.ix_(*[range(2) if i in indices else [slice(None)] for i in range(num_qubits)])])**2 for b in range(2 ** len(indices))]\n    probs /= sum(probs)\n    return probs"
    },
    {
        "original": "def get_connected_sites(self, n, jimage=(0, 0, 0)):\n        \"\"\"\n        Returns a named tuple of neighbors of site n:\n        periodic_site, jimage, index, weight.\n        Index is the index of the corresponding site\n        in the original structure, weight can be\n        None if not defined.\n        :param n: index of Site in Structure\n        :param jimage: lattice vector of site\n        :return: list of ConnectedSite tuples,\n        sorted by closest first\n        \"\"\"\n\n        connected_sites = set()\n        connected_site_images = set()\n\n        out_edges = [(u, v, d, 'out') for u, v, d in self.graph.out_edges(n, data=True)]\n        in_edges = [(u, v, d, 'in') for u, v, d in self.graph.in_edges(n, data=True)]\n\n        for u, v, d, dir in out_edges + in_edges:\n\n            to_jimage = d['to_jimage']\n\n            if dir == 'in':\n                u, v = v, u\n                to_jimage = np.multiply(-1, to_jimage)\n\n            to_jimage = tuple(map(int, np.add(to_jimage, jimage)))\n            site_d = self.structure[v].as_dict()\n            site_d['abc'] = np.add(site_d['abc'], to_jimage).tolist()\n            site = PeriodicSite.from_dict(site_d)\n\n            # from_site if jimage arg != (0, 0, 0)\n            relative_jimage = np.subtract(to_jimage, jimage)\n            dist = self.structure[u].distance(self.structure[v], jimage=relative_jimage)\n\n            weight = d.get('weight', None)\n\n            if (v, to_jimage) not in connected_site_images:\n\n                connected_site = ConnectedSite(site=site,\n                                               jimage=to_jimage,\n                                               index=v,\n                                               weight=weight,\n                                               dist=dist)\n\n                connected_sites.add(connected_site)\n                connected_site_images.add((v, to_jimage))\n\n        # return list sorted by closest sites first\n        connected_sites = list(connected_sites)\n        connected_sites.sort(key=lambda x: x.dist)\n\n        return connected_sites",
        "rewrite": "def get_connected_sites(self, n, jimage=(0, 0, 0)):\n    connected_sites = set()\n    connected_site_images = set()\n\n    out_edges = [(u, v, d, 'out') for u, v, d in self.graph.out_edges(n, data=True)]\n    in_edges = [(u, v, d, 'in') for u, v, d in self.graph.in_edges(n, data=True)]\n\n    for u, v, d, dir in out_edges + in_edges:\n        to_jimage = d['to_jimage']\n\n        if dir == 'in':\n            u, v = v, u\n            to_jimage = np.multiply(-1, to_jimage)\n\n        to_jimage = tuple(map(int, np.add(to_jimage, jimage)))\n        site_d = self.structure[v].as_dict()\n        site_d['abc'] = np.add(site_d['abc'], to_jimage).tolist()\n        site = PeriodicSite.from_dict(site_d)\n    \n        relative_jimage = np.subtract(to_jimage, jimage)\n        dist = self.structure[u].distance(self.structure[v], jimage=relative_jimage)\n\n        weight = d.get('weight', None)\n\n        if (v, to_jimage) not in connected_site_images:\n            connected_site = ConnectedSite(site=site,\n                                           jimage=to_jimage,\n                                           index=v,\n                                           weight=weight,\n                                           dist=dist)\n            \n            connected_sites.add(connected_site)\n            connected_site_images.add((v, to_jimage))\n\n    connected_sites = list(connected_sites)\n    connected_sites.sort(key=lambda x: x.dist)\n\n    return connected_sites"
    },
    {
        "original": "def list_devices(connection: ForestConnection = None):\n    \"\"\"\n    Query the Forest 2.0 server for a list of underlying QPU devices.\n\n    NOTE: These can't directly be used to manufacture pyQuil Device objects, but this gives a list\n          of legal values that can be supplied to list_lattices to filter its (potentially very\n          noisy) output.\n\n    :return: A list of device names.\n    \"\"\"\n    # For the record, the dictionary stored in \"devices\" that we're getting back is keyed on device\n    # names and has this structure in its values:\n    #\n    # {\n    #   \"is_online\":   a boolean indicating the availability of the device,\n    #   \"is_retuning\": a boolean indicating whether the device is busy retuning,\n    #   \"specs\":       a Specs object describing the entire device, serialized as a dictionary,\n    #   \"isa\":         an ISA object describing the entire device, serialized as a dictionary,\n    #   \"noise_model\": a NoiseModel object describing the entire device, serialized as a dictionary\n    # }\n    if connection is None:\n        connection = ForestConnection()\n\n    session = connection.session\n    url = connection.forest_cloud_endpoint + '/devices'\n    return sorted(get_json(session, url)[\"devices\"].keys())",
        "rewrite": "def list_devices(connection: ForestConnection = None):\n    if connection is None:\n        connection = ForestConnection()\n    \n    session = connection.session\n    url = connection.forest_cloud_endpoint + '/devices'\n    return sorted(get_json(session, url)[\"devices\"].keys())"
    },
    {
        "original": "def update_flagfile(flags_path, new_threshold):\n    \"\"\"Updates the flagfile at `flags_path`, changing the value for\n    `resign_threshold` to `new_threshold`\n    \"\"\"\n    if abs(new_threshold) > 1:\n        raise ValueError(\"Invalid new percentile for resign threshold\")\n    with tf.gfile.GFile(flags_path) as f:\n        lines = f.read()\n    if new_threshold > 0:\n        new_threshold *= -1\n    if not RESIGN_FLAG_REGEX.search(lines):\n        print(\"Resign threshold flag not found in flagfile {}!  Aborting.\".format(flags_path))\n        sys.exit(1)\n    old_threshold = RESIGN_FLAG_REGEX.search(lines).groups(1)\n    lines = re.sub(RESIGN_FLAG_REGEX, \"--resign_threshold={:.3f}\".format(new_threshold), lines)\n\n    if abs(float(old_threshold[0]) - new_threshold) < 0.001:\n        print(\"Not updating percentiles; {} ~= {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n    else:\n        print(\"Updated percentile from {} to {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n        with tf.gfile.GFile(flags_path, 'w') as f:\n            f.write(lines)",
        "rewrite": "import tensorflow as tf\nimport re\nimport sys\n\nRESIGN_FLAG_REGEX = re.compile(r'--resign_threshold=([-\\d.]+)')\n\n\ndef update_flagfile(flags_path, new_threshold):\n    if abs(new_threshold) > 1:\n        raise ValueError(\"Invalid new percentile for resign threshold\")\n    \n    with tf.gfile.GFile(flags_path) as f:\n        lines = f.read()\n        \n    if new_threshold > 0:\n        new_threshold *= -1\n    \n    if not RESIGN_FLAG_REGEX.search(lines):\n        print(\"Resign threshold flag not found in flagfile {}!  Aborting.\".format(flags_path))\n        sys.exit(1)\n    \n    old_threshold = RESIGN_FLAG_REGEX.search(lines).groups(1)\n    \n    lines = re.sub(RESIGN_FLAG_REGEX, \"--resign_threshold={:.3f}\".format(new_threshold), lines)\n\n    if abs(float(old_threshold[0]) - new_threshold) < 0.001:\n        print(\"Not updating percentiles; {} ~= {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n    else:\n        print(\"Updated percentile from {} to {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n        \n        with tf.gfile.GFile(flags_path, 'w') as f:\n            f.write(lines)"
    },
    {
        "original": "def setupCentral(self):\n        \"\"\"Setup empty window supporting tabs at startup. \"\"\"\n        self.central = TabsWindow(self.bin_windows, self)\n        self.setCentralWidget(self.central)",
        "rewrite": "def setupCentral(self):\n    self.central = TabsWindow(self.bin_windows, self)\n    self.setCentralWidget(self.central)"
    },
    {
        "original": "def ReadHuntObjects(self,\n                      offset,\n                      count,\n                      with_creator=None,\n                      created_after=None,\n                      with_description_match=None):\n    \"\"\"Reads all hunt objects from the database.\"\"\"\n    filter_fns = []\n    if with_creator is not None:\n      filter_fns.append(lambda h: h.creator == with_creator)\n    if created_after is not None:\n      filter_fns.append(lambda h: h.create_time > created_after)\n    if with_description_match is not None:\n      filter_fns.append(lambda h: with_description_match in h.description)\n    filter_fn = lambda h: all(f(h) for f in filter_fns)\n\n    result = [self._DeepCopy(h) for h in self.hunts.values() if filter_fn(h)]\n    return sorted(\n        result, key=lambda h: h.create_time,\n        reverse=True)[offset:offset + (count or db.MAX_COUNT)]",
        "rewrite": "def read_hunt_objects(self, offset, count, with_creator=None, created_after=None, with_description_match=None):\n    filter_fns = []\n    if with_creator is not None:\n        filter_fns.append(lambda h: h.creator == with_creator)\n    if created_after is not None:\n        filter_fns.append(lambda h: h.create_time > created_after)\n    if with_description_match is not None:\n        filter_fns.append(lambda h: with_description_match in h.description)\n    filter_fn = lambda h: all(f(h) for f in filter_fns)\n\n    result = [self._deep_copy(h) for h in self.hunts.values() if filter_fn(h)]\n    return sorted(result, key=lambda h: h.create_time, reverse=True)[offset:offset + (count or db.MAX_COUNT)]"
    },
    {
        "original": "def run(self):\n        \"\"\"\n        Run dbt for the query, based on the graph.\n        \"\"\"\n        self._runtime_initialize()\n\n        if len(self._flattened_nodes) == 0:\n            logger.warning(\"WARNING: Nothing to do. Try checking your model \"\n                           \"configs and model specification args\")\n            return []\n        else:\n            logger.info(\"\")\n\n        selected_uids = frozenset(n.unique_id for n in self._flattened_nodes)\n        result = self.execute_with_hooks(selected_uids)\n\n        result.write(self.result_path())\n\n        self.task_end_messages(result.results)\n        return result.results",
        "rewrite": "def run(self):\n    self._runtime_initialize()\n\n    if len(self._flattened_nodes) == 0:\n        logger.warning(\"WARNING: Nothing to do. Try checking your model configs and model specification args\")\n        return []\n    else:\n        logger.info(\"\")\n\n    selected_uids = frozenset(n.unique_id for n in self._flattened_nodes)\n    result = self.execute_with_hooks(selected_uids)\n\n    result.write(self.result_path())\n\n    self.task_end_messages(result.results)\n    return result.results"
    },
    {
        "original": "def from_path(cls, path, suffix=''):\n        \"\"\"\n        Convenience method to run critic2 analysis on a folder containing\n        typical VASP output files.\n        This method will:\n        1. Look for files CHGCAR, AECAR0, AECAR2, POTCAR or their gzipped\n        counterparts.\n        2. If AECCAR* files are present, constructs a temporary reference\n        file as AECCAR0 + AECCAR2\n        3. Runs critic2 analysis twice: once for charge, and a second time\n        for the charge difference (magnetization density).\n        :param path: path to folder to search in\n        :param suffix: specific suffix to look for (e.g. '.relax1' for\n        'CHGCAR.relax1.gz')\n        :return:\n        \"\"\"\n\n        def _get_filepath(filename, warning, path=path, suffix=suffix):\n            paths = glob.glob(os.path.join(path, filename + suffix + '*'))\n            if not paths:\n                warnings.warn(warning)\n                return None\n            if len(paths) > 1:\n                # using reverse=True because, if multiple files are present,\n                # they likely have suffixes 'static', 'relax', 'relax2', etc.\n                # and this would give 'static' over 'relax2' over 'relax'\n                # however, better to use 'suffix' kwarg to avoid this!\n                paths.sort(reverse=True)\n                warnings.warn('Multiple files detected, using {}'.format(os.path.basename(path)))\n            path = paths[0]\n            return path\n\n        chgcar_path = _get_filepath('CHGCAR', 'Could not find CHGCAR!')\n        chgcar = Chgcar.from_file(chgcar_path)\n\n        aeccar0_path = _get_filepath('AECCAR0', 'Could not find AECCAR0, interpret Bader results with caution.')\n        aeccar0 = Chgcar.from_file(aeccar0_path) if aeccar0_path else None\n\n        aeccar2_path = _get_filepath('AECCAR2', 'Could not find AECCAR2, interpret Bader results with caution.')\n        aeccar2 = Chgcar.from_file(aeccar2_path) if aeccar2_path else None\n\n        chgcar_ref = aeccar0.linear_add(aeccar2) if (aeccar0 and aeccar2) else None\n\n        return cls(chgcar.structure, chgcar, chgcar_ref)",
        "rewrite": "def from_path(cls, path, suffix=''):\n    def _get_filepath(filename, warning, path=path, suffix=suffix):\n        paths = glob.glob(os.path.join(path, filename + suffix + '*'))\n        if not paths:\n            warnings.warn(warning)\n            return None\n        if len(paths) > 1:\n            paths.sort(reverse=True)\n            warnings.warn('Multiple files detected, using {}'.format(os.path.basename(path)))\n        path = paths[0]\n        return path\n\n    chgcar_path = _get_filepath('CHGCAR', 'Could not find CHGCAR!')\n    chgcar = Chgcar.from_file(chgcar_path)\n\n    aeccar0_path = _get_filepath('AECCAR0', 'Could not find AECCAR0, interpret Bader results with caution.')\n    aeccar0 = Chgcar.from_file(aeccar0_path) if aeccar0_path else None\n\n    aeccar2_path = _get_filepath('AECCAR2', 'Could not find AECCAR2, interpret Bader results with caution.')\n    aeccar2 = Chgcar.from_file(aeccar2_path) if aeccar2_path else None\n\n    chgcar_ref = aeccar0.linear_add(aeccar2) if (aeccar0 and aeccar2) else None\n\n    return cls(chgcar.structure, chgcar, chgcar_ref)"
    },
    {
        "original": "def get_availability_zone(vm_):\n    \"\"\"\n    Return the availability zone to use\n    \"\"\"\n    avz = config.get_cloud_config_value(\n        'availability_zone', vm_, __opts__, search_global=False\n    )\n\n    if avz is None:\n        return None\n\n    zones = _list_availability_zones(vm_)\n\n    # Validate user-specified AZ\n    if avz not in zones:\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t valid in this region: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    # check specified AZ is available\n    elif zones[avz] != 'available':\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t currently available: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    return avz",
        "rewrite": "def get_availability_zone(vm_):\n    avz = config.get_cloud_config_value(\n        'availability_zone', vm_, __opts__, search_global=False\n    )\n\n    if avz is None:\n        return None\n\n    zones = _list_availability_zones(vm_)\n\n    if avz not in zones:\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t valid in this region: {0}\\n'.format(avz)\n        )\n    elif zones[avz] != 'available':\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t currently available: {0}\\n'.format(avz)\n        )\n\n    return avz"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "rewrite": "def validate_rpc_host(ip):\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(desc='Invalid RPC ip address: %s' % ip)\n    return ip"
    },
    {
        "original": "def default_output_name(self, input_file):\n        \"\"\" Derive a default output name from the ELF name. \"\"\"\n        irom_segment = self.get_irom_segment()\n        if irom_segment is not None:\n            irom_offs = irom_segment.addr - ESP8266ROM.IROM_MAP_START\n        else:\n            irom_offs = 0\n        return \"%s-0x%05x.bin\" % (os.path.splitext(input_file)[0],\n                                  irom_offs & ~(ESPLoader.FLASH_SECTOR_SIZE - 1))",
        "rewrite": "def default_output_name(self, input_file):\n    irom_segment = self.get_irom_segment()\n    irom_offs = irom_segment.addr - ESP8266ROM.IROM_MAP_START if irom_segment is not None else 0\n    return \"%s-0x%05x.bin\" % (os.path.splitext(input_file)[0], irom_offs & ~(ESPLoader.FLASH_SECTOR_SIZE - 1))"
    },
    {
        "original": "def __sum(self, line):\n        \"\"\"Return the IRQ sum number.\n\n        IRQ line samples:\n        1:     44487        341         44         72   IO-APIC   1-edge      i8042\n        LOC:   33549868   22394684   32474570   21855077   Local timer interrupts\n        FIQ:   usb_fiq\n        \"\"\"\n        splitted_line = line.split()\n        try:\n            ret = sum(map(int, splitted_line[1:(self.cpu_number + 1)]))\n        except ValueError:\n            # Correct issue #1007 on some conf (Raspberry Pi with Raspbian)\n            ret = 0\n        return ret",
        "rewrite": "def __sum(self, line):\n    splitted_line = line.split()\n    try:\n        ret = sum(map(int, splitted_line[1:(self.cpu_number + 1)]))\n    except ValueError:\n        ret = 0\n    return ret"
    },
    {
        "original": "def ParseFromUnicode(self, value):\n    \"\"\"Parse a string into a client URN.\n\n    Convert case so that all URNs are of the form C.[0-9a-f].\n\n    Args:\n      value: string value to parse\n    \"\"\"\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n      raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = match.group(\"clientid\")\n    clientid_correctcase = \"\".join((clientid[0].upper(), clientid[1:].lower()))\n\n    self._string_urn = self._string_urn.replace(clientid, clientid_correctcase,\n                                                1)",
        "rewrite": "def ParseFromUnicode(self, value):\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n        raise type_info.TypeValueError(f\"Client urn malformed: {value}\")\n\n    clientid = match.group(\"clientid\")\n    clientid_correctcase = \"\".join((clientid[0].upper(), clientid[1:].lower()))\n\n    self._string_urn = self._string_urn.replace(clientid, clientid_correctcase, 1)"
    },
    {
        "original": "def get_if(iff, cmd):\n    \"\"\"Ease SIOCGIF* ioctl calls\"\"\"\n\n    sck = socket.socket()\n    ifreq = ioctl(sck, cmd, struct.pack(\"16s16x\", iff.encode(\"utf8\")))\n    sck.close()\n    return ifreq",
        "rewrite": "def get_if(iff, cmd):\n    sck = socket.socket()\n    ifreq = ioctl(sck, cmd, struct.pack(\"16s16x\", iff.encode(\"utf8\")))\n    sck.close()\n    return ifreq"
    },
    {
        "original": "def solar_position_numba(unixtime, lat, lon, elev, pressure, temp, delta_t,\n                         atmos_refract, numthreads, sst=False, esd=False):\n    \"\"\"Calculate the solar position using the numba compiled functions\n    and multiple threads. Very slow if functions are not numba compiled.\n    \"\"\"\n    # these args are the same for each thread\n    loc_args = np.array([lat, lon, elev, pressure, temp, delta_t,\n                         atmos_refract, sst, esd])\n\n    # construct dims x ulength array to put the results in\n    ulength = unixtime.shape[0]\n    if sst:\n        dims = 3\n    elif esd:\n        dims = 1\n    else:\n        dims = 6\n    result = np.empty((dims, ulength), dtype=np.float64)\n\n    if unixtime.dtype != np.float64:\n        unixtime = unixtime.astype(np.float64)\n\n    if ulength < numthreads:\n        warnings.warn('The number of threads is more than the length of '\n                      'the time array. Only using %s threads.'.format(ulength))\n        numthreads = ulength\n\n    if numthreads <= 1:\n        solar_position_loop(unixtime, loc_args, result)\n        return result\n\n    # split the input and output arrays into numthreads chunks\n    split0 = np.array_split(unixtime, numthreads)\n    split2 = np.array_split(result, numthreads, axis=1)\n    chunks = [[a0, loc_args, split2[i]] for i, a0 in enumerate(split0)]\n    # Spawn one thread per chunk\n    threads = [threading.Thread(target=solar_position_loop, args=chunk)\n               for chunk in chunks]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n    return result",
        "rewrite": "def solar_position_numba(unixtime, lat, lon, elev, pressure, temp, delta_t,\n                         atmos_refract, numthreads, sst=False, esd=False):\n    loc_args = np.array([lat, lon, elev, pressure, temp, delta_t, atmos_refract, sst, esd])\n    ulength = unixtime.shape[0]\n    if sst:\n        dims = 3\n    elif esd:\n        dims = 1\n    else:\n        dims = 6\n    result = np.empty((dims, ulength), dtype=np.float64)\n\n    if unixtime.dtype != np.float64:\n        unixtime = unixtime.astype(np.float64)\n\n    if ulength < numthreads:\n        warnings.warn('The number of threads is more than the length of the time array. Only using %s threads.'.format(ulength))\n        numthreads = ulength\n\n    if numthreads <= 1:\n        solar_position_loop(unixtime, loc_args, result)\n        return result\n\n    split0 = np.array_split(unixtime, numthreads)\n    split2 = np.array_split(result, numthreads, axis=1)\n    chunks = [[a0, loc_args, split2[i]] for i, a0 in enumerate(split0)]\n    \n    threads = [threading.Thread(target=solar_position_loop, args=chunk) for chunk in chunks]\n    \n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n        \n    return result"
    },
    {
        "original": "def skip_format_url(format_, url):\n    \"\"\"\n    Checks whether a give format/url should be skipped and not downloaded.\n\n    @param format_: Filename format (extension).\n    @type format_: str (e.g. html, txt, zip, pdf)\n\n    @param url: URL.\n    @type url: str\n\n    @return: True if format/url should be skipped, False otherwise.\n    @rtype bool\n    \"\"\"\n    # Do not download empty formats\n    if format_ == '':\n        return True\n\n    # Do not download email addresses\n    if ('mailto:' in url) and ('@' in url):\n        return True\n\n    # Is this localhost?\n    parsed = urlparse(url)\n    if parsed.hostname == 'localhost':\n        return True\n\n    # These are trusted manually added formats, do not skip them\n    if RE_VALID_FORMATS.match(format_):\n        return False\n\n    # Simple formats only contain letters, numbers, \"_\" and \"-\"\n    # If this a non simple format?\n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n\n    # Is this a link to the site root?\n    if parsed.path in ('', '/'):\n        return True\n\n    # Do not skip\n    return False",
        "rewrite": "def skip_format_url(format_, url):\n    if format_ == '':\n        return True\n\n    if ('mailto:' in url) and ('@' in url):\n        return True\n\n    parsed = urlparse(url)\n    if parsed.hostname == 'localhost':\n        return True\n\n    if RE_VALID_FORMATS.match(format_):\n        return False\n\n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n\n    if parsed.path in ('', '/'):\n        return True\n\n    return False"
    },
    {
        "original": "def obj_with_unit(obj, unit):\n    \"\"\"\n    Returns a `FloatWithUnit` instance if obj is scalar, a dictionary of\n    objects with units if obj is a dict, else an instance of\n    `ArrayWithFloatWithUnit`.\n\n    Args:\n        unit: Specific units (eV, Ha, m, ang, etc.).\n    \"\"\"\n    unit_type = _UNAME2UTYPE[unit]\n\n    if isinstance(obj, numbers.Number):\n        return FloatWithUnit(obj, unit=unit, unit_type=unit_type)\n    elif isinstance(obj, collections.Mapping):\n        return {k: obj_with_unit(v, unit) for k,v in obj.items()}\n    else:\n        return ArrayWithUnit(obj, unit=unit, unit_type=unit_type)",
        "rewrite": "def obj_with_unit(obj, unit):\n    \"\"\"\n    Returns a `FloatWithUnit` instance if obj is scalar, a dictionary of\n    objects with units if obj is a dict, else an instance of\n    `ArrayWithFloatWithUnit`.\n\n    Args:\n        unit: Specific units (eV, Ha, m, ang, etc.).\n    \"\"\"\n    unit_type = _UNAME2UTYPE[unit]\n\n    if isinstance(obj, numbers.Number):\n        return FloatWithUnit(obj, unit=unit, unit_type=unit_type)\n    elif isinstance(obj, collections.Mapping):\n        return {k: obj_with_unit(v, unit) for k,v in obj.items()}\n    else:\n        return ArrayWithUnit(obj, unit=unit, unit_type=unit_type)"
    },
    {
        "original": "def stream_n_messages(n):\n    \"\"\"Stream n JSON responses\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Streamed JSON responses.\n    \"\"\"\n    response = get_dict(\"url\", \"args\", \"headers\", \"origin\")\n    n = min(n, 100)\n\n    def generate_stream():\n        for i in range(n):\n            response[\"id\"] = i\n            yield json.dumps(response) + \"\\n\"\n\n    return Response(generate_stream(), headers={\"Content-Type\": \"application/json\"})",
        "rewrite": "def stream_n_messages(n):\n    response = get_dict(\"url\", \"args\", \"headers\", \"origin\")\n    n = min(n, 100)\n\n    def generate_stream():\n        for i in range(n):\n            response[\"id\"] = i\n            yield json.dumps(response) + \"\\n\n\n    return Response(generate_stream(), headers={\"Content-Type\": \"application/json\"})"
    },
    {
        "original": "def kernel_modules(attrs=None, where=None):\n    \"\"\"\n    Return kernel_modules information from osquery\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' osquery.kernel_modules\n    \"\"\"\n    if __grains__['os_family'] in ['RedHat', 'Debian']:\n        return _osquery_cmd(table='kernel_modules', attrs=attrs, where=where)\n    return {'result': False, 'comment': 'Only available on Red Hat or Debian based systems.'}",
        "rewrite": "def kernel_modules(attrs=None, where=None):\n    if __grains__['os_family'] in ['RedHat', 'Debian']:\n        return _osquery_cmd(table='kernel_modules', attrs=attrs, where=where)\n    return {'result': False, 'comment': 'Only available on Red Hat or Debian based systems.'}"
    },
    {
        "original": "def num_compositions(m, n):\n    \"\"\"\n    The total number of m-part compositions of n, which is equal to\n    (n+m-1) choose (m-1).\n\n    Parameters\n    ----------\n    m : scalar(int)\n        Number of parts of composition.\n\n    n : scalar(int)\n        Integer to decompose.\n\n    Returns\n    -------\n    scalar(int)\n        Total number of m-part compositions of n.\n\n    \"\"\"\n    # docs.scipy.org/doc/scipy/reference/generated/scipy.special.comb.html\n    return scipy.special.comb(n+m-1, m-1, exact=True)",
        "rewrite": "import scipy.special\n\ndef num_compositions(m, n):\n    return scipy.special.comb(n+m-1, m-1, exact=True)"
    },
    {
        "original": "def _str_dtype(dtype):\n        \"\"\"\n        Represent dtypes without byte order, as earlier Java tickstore code doesn't support explicit byte order.\n        \"\"\"\n        assert dtype.byteorder != '>'\n        if dtype.kind == 'i':\n            assert dtype.itemsize == 8\n            return 'int64'\n        elif dtype.kind == 'f':\n            assert dtype.itemsize == 8\n            return 'float64'\n        elif dtype.kind == 'U':\n            return 'U%d' % (dtype.itemsize / 4)\n        else:\n            raise UnhandledDtypeException(\"Bad dtype '%s'\" % dtype)",
        "rewrite": "def _str_dtype(dtype):\n    assert dtype.byteorder != '>'\n    if dtype.kind == 'i':\n        assert dtype.itemsize == 8\n        return 'int64'\n    elif dtype.kind == 'f':\n        assert dtype.itemsize == 8\n        return 'float64'\n    elif dtype.kind == 'U':\n        return 'U%d' % (dtype.itemsize // 4)\n    else:\n        raise UnhandledDtypeException(\"Bad dtype '%s'\" % dtype)"
    },
    {
        "original": "def simple_takeoff(self, alt=None):\n        \"\"\"\n        Take off and fly the vehicle to the specified altitude (in metres) and then wait for another command.\n\n        .. note::\n\n            This function should only be used on Copter vehicles.\n\n\n        The vehicle must be in GUIDED mode and armed before this is called.\n\n        There is no mechanism for notification when the correct altitude is reached,\n        and if another command arrives before that point (e.g. :py:func:`simple_goto`) it will be run instead.\n\n        .. warning::\n\n           Apps should code to ensure that the vehicle will reach a safe altitude before\n           other commands are executed. A good example is provided in the guide topic :doc:`guide/taking_off`.\n\n        :param alt: Target height, in metres.\n        \"\"\"\n        if alt is not None:\n            altitude = float(alt)\n            if math.isnan(altitude) or math.isinf(altitude):\n                raise ValueError(\"Altitude was NaN or Infinity. Please provide a real number\")\n            self._master.mav.command_long_send(0, 0, mavutil.mavlink.MAV_CMD_NAV_TAKEOFF,\n                                               0, 0, 0, 0, 0, 0, 0, altitude)",
        "rewrite": "def simple_takeoff(self, alt=None):\n    if alt is not None:\n        altitude = float(alt)\n        if math.isnan(altitude) or math.isinf(altitude):\n            raise ValueError(\"Altitude was NaN or Infinity. Please provide a real number\")\n        self._master.mav.command_long_send(0, 0, mavutil.mavlink.MAV_CMD_NAV_TAKEOFF,\n                                           0, 0, 0, 0, 0, 0, 0, altitude)"
    },
    {
        "original": "def add_before(self, pipeline):\n        \"\"\"Add a Pipeline to be applied before this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply before this\n                Pipeline.\n        \"\"\"\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = pipeline.pipes[:] + self.pipes[:]\n        return self",
        "rewrite": "def add_before(self, pipeline):\n    if not isinstance(pipeline, Pipeline):\n        pipeline = Pipeline(pipeline)\n    self.pipes = pipeline.pipes + self.pipes\n    return self"
    },
    {
        "original": "def parse_type(defn, preprocess=True):\n    \"\"\"\n    Parse a simple type expression into a SimType\n\n    >>> parse_type('int *')\n    \"\"\"\n    if pycparser is None:\n        raise ImportError(\"Please install pycparser in order to parse C definitions\")\n\n    defn = 'typedef ' + defn.strip('; \\n\\t\\r') + ' QQQQ;'\n\n    if preprocess:\n        defn = do_preprocess(defn)\n\n    node = pycparser.c_parser.CParser().parse(make_preamble()[0] + defn)\n    if not isinstance(node, pycparser.c_ast.FileAST) or \\\n            not isinstance(node.ext[-1], pycparser.c_ast.Typedef):\n        raise ValueError(\"Something went horribly wrong using pycparser\")\n\n    decl = node.ext[-1].type\n    return _decl_to_type(decl)",
        "rewrite": "def parse_type(defn, preprocess=True):\n    if pycparser is None:\n        raise ImportError(\"Please install pycparser in order to parse C definitions\")\n\n    defn = 'typedef ' + defn.strip('; \\n\\t\\r') + ' QQQQ;'\n\n    if preprocess:\n        defn = do_preprocess(defn)\n\n    node = pycparser.c_parser.CParser().parse(make_preamble()[0] + defn)\n    if not isinstance(node, pycparser.c_ast.FileAST) or not isinstance(node.ext[-1], pycparser.c_ast.Typedef):\n        raise ValueError(\"Something went horribly wrong using pycparser\")\n\n    decl = node.ext[-1].type\n    return _decl_to_type(decl)"
    },
    {
        "original": "def resolve_method(state, method_name, class_name, params=(), ret_type=None,\n                   include_superclasses=True, init_class=True,\n                   raise_exception_if_not_found=False):\n    \"\"\"\n    Resolves the method based on the given characteristics (name, class and\n    params) The method may be defined in one of the superclasses of the given\n    class (TODO: support interfaces).\n\n    :rtype: archinfo.arch_soot.SootMethodDescriptor\n    \"\"\"\n    base_class = state.javavm_classloader.get_class(class_name)\n    if include_superclasses:\n        class_hierarchy = state.javavm_classloader.get_class_hierarchy(base_class)\n    else:\n        class_hierarchy = [base_class]\n    # walk up in class hierarchy, until method is found\n    for class_descriptor in class_hierarchy:\n        java_binary = state.project.loader.main_object\n        soot_method = java_binary.get_soot_method(method_name, class_descriptor.name,\n                                                  params, none_if_missing=True)\n        if soot_method is not None:\n            # init the class\n            if init_class:\n                state.javavm_classloader.init_class(class_descriptor)\n            return SootMethodDescriptor.from_soot_method(soot_method)\n\n    # method could not be found\n    # => we are executing code that is not loaded (typically library code)\n    # => fallback: continue with infos available from the invocation, so we\n    #              still can use SimProcedures\n    if raise_exception_if_not_found:\n        raise SootMethodNotLoadedException()\n    else:\n        return SootMethodDescriptor(class_name, method_name, params, ret_type=ret_type)",
        "rewrite": "def resolve_method(state, method_name, class_name, params=(), ret_type=None,\n                   include_superclasses=True, init_class=True,\n                   raise_exception_if_not_found=False):\n    \n    base_class = state.javavm_classloader.get_class(class_name)\n    \n    if include_superclasses:\n        class_hierarchy = state.javavm_classloader.get_class_hierarchy(base_class)\n    else:\n        class_hierarchy = [base_class]\n    \n    for class_descriptor in class_hierarchy:\n        java_binary = state.project.loader.main_object\n        soot_method = java_binary.get_soot_method(method_name, class_descriptor.name,\n                                                  params, none_if_missing=True)\n        if soot_method is not None:\n            if init_class:\n                state.javavm_classloader.init_class(class_descriptor)\n            return SootMethodDescriptor.from_soot_method(soot_method)\n    \n    if raise_exception_if_not_found:\n        raise SootMethodNotLoadedException()\n    else:\n        return SootMethodDescriptor(class_name, method_name, params, ret_type=ret_type)"
    },
    {
        "original": "def __stage1(self, image, scales: list, stage_status: StageStatus):\n        \"\"\"\n        First stage of the MTCNN.\n        :param image:\n        :param scales:\n        :param stage_status:\n        :return:\n        \"\"\"\n        total_boxes = np.empty((0, 9))\n        status = stage_status\n\n        for scale in scales:\n            scaled_image = self.__scale_image(image, scale)\n\n            img_x = np.expand_dims(scaled_image, 0)\n            img_y = np.transpose(img_x, (0, 2, 1, 3))\n\n            out = self.__pnet.feed(img_y)\n\n            out0 = np.transpose(out[0], (0, 2, 1, 3))\n            out1 = np.transpose(out[1], (0, 2, 1, 3))\n\n            boxes, _ = self.__generate_bounding_box(out1[0, :, :, 1].copy(),\n                                                    out0[0, :, :, :].copy(), scale, self.__steps_threshold[0])\n\n            # inter-scale nms\n            pick = self.__nms(boxes.copy(), 0.5, 'Union')\n            if boxes.size > 0 and pick.size > 0:\n                boxes = boxes[pick, :]\n                total_boxes = np.append(total_boxes, boxes, axis=0)\n\n        numboxes = total_boxes.shape[0]\n\n        if numboxes > 0:\n            pick = self.__nms(total_boxes.copy(), 0.7, 'Union')\n            total_boxes = total_boxes[pick, :]\n\n            regw = total_boxes[:, 2] - total_boxes[:, 0]\n            regh = total_boxes[:, 3] - total_boxes[:, 1]\n\n            qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\n            qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\n            qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\n            qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\n\n            total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\n            total_boxes = self.__rerec(total_boxes.copy())\n\n            total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\n            status = StageStatus(self.__pad(total_boxes.copy(), stage_status.width, stage_status.height),\n                                 width=stage_status.width, height=stage_status.height)\n\n        return total_boxes, status",
        "rewrite": "def __stage1(self, image, scales: list, stage_status: StageStatus):\n        total_boxes = np.empty((0, 9))\n        status = stage_status\n\n        for scale in scales:\n            scaled_image = self.__scale_image(image, scale)\n\n            img_x = np.expand_dims(scaled_image, 0)\n            img_y = np.transpose(img_x, (0, 2, 1, 3))\n\n            out = self.__pnet.feed(img_y)\n\n            out0 = np.transpose(out[0], (0, 2, 1, 3))\n            out1 = np.transpose(out[1], (0, 2, 1, 3))\n\n            boxes, _ = self.__generate_bounding_box(out1[0, :, :, 1].copy(), out0[0, :, :, :].copy(), scale, self.__steps_threshold[0])\n\n            pick = self.__nms(boxes.copy(), 0.5, 'Union')\n            if boxes.size > 0 and pick.size > 0:\n                boxes = boxes[pick, :]\n                total_boxes = np.append(total_boxes, boxes, axis=0)\n\n        numboxes = total_boxes.shape[0]\n\n        if numboxes > 0:\n            pick = self.__nms(total_boxes.copy(), 0.7, 'Union')\n            total_boxes = total_boxes[pick, :]\n\n            regw = total_boxes[:, 2] - total_boxes[:, 0]\n            regh = total_boxes[:, 3] - total_boxes[:, 1]\n\n            qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\n            qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\n            qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\n            qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\n\n            total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\n            total_boxes = self.__rerec(total_boxes.copy())\n\n            total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\n            status = StageStatus(self.__pad(total_boxes.copy(), stage_status.width, stage_status.height), width=stage_status.width, height=stage_status.height)\n\n        return total_boxes, status"
    },
    {
        "original": "def _get_YYTfactor(self, Y):\n        \"\"\"\n        find a matrix L which satisfies LLT = YYT.\n\n        Note that L may have fewer columns than Y.\n        \"\"\"\n        N, D = Y.shape\n        if (N>=D):\n            return Y.view(np.ndarray)\n        else:\n            return jitchol(tdot(Y))",
        "rewrite": "def _get_YYTfactor(self, Y):\n    N, D = Y.shape\n    if N >= D:\n        return Y.view(np.ndarray)\n    else:\n        return jitchol(tdot(Y))"
    },
    {
        "original": "def is_estimator(model):\n    \"\"\"\n    Determines if a model is an estimator using issubclass and isinstance.\n\n    Parameters\n    ----------\n    estimator : class or instance\n        The object to test if it is a Scikit-Learn clusterer, especially a\n        Scikit-Learn estimator or Yellowbrick visualizer\n    \"\"\"\n    if inspect.isclass(model):\n        return issubclass(model, BaseEstimator)\n\n    return isinstance(model, BaseEstimator)",
        "rewrite": "def is_estimator(model):\n    if inspect.isclass(model):\n        return issubclass(model, BaseEstimator)\n\n    return isinstance(model, BaseEstimator)"
    },
    {
        "original": "def save_scan_plot(self, filename=\"scan.pdf\",\n                       img_format=\"pdf\", coords=None):\n        \"\"\"\n        Save matplotlib plot of the potential energy surface to a file.\n\n        Args:\n            filename: Filename to write to.\n            img_format: Image format to use. Defaults to EPS.\n            coords: internal coordinate name to use as abcissa.\n        \"\"\"\n        plt = self.get_scan_plot(coords)\n        plt.savefig(filename, format=img_format)",
        "rewrite": "def save_scan_plot(self, filename=\"scan.pdf\", img_format=\"pdf\", coords=None):\n        plt = self.get_scan_plot(coords)\n        plt.savefig(filename, format=img_format)"
    },
    {
        "original": "def run_election_show(args, bigchain):\n    \"\"\"Retrieves information about an election\n\n    :param args: dict\n        args = {\n        'election_id': the transaction_id for an election (str)\n        }\n    :param bigchain: an instance of BigchainDB\n    \"\"\"\n\n    election = bigchain.get_transaction(args.election_id)\n    if not election:\n        logger.error(f'No election found with election_id {args.election_id}')\n        return\n\n    response = election.show_election(bigchain)\n\n    logger.info(response)\n\n    return response",
        "rewrite": "def run_election_show(args, bigchain):\n    election = bigchain.get_transaction(args['election_id'])\n    if not election:\n        logger.error(f'No election found with election_id {args[\"election_id\"]}')\n        return\n\n    response = election.show_election(bigchain)\n\n    logger.info(response)\n\n    return response"
    },
    {
        "original": "def splits(cls, datasets, batch_sizes=None, **kwargs):\n        \"\"\"Create Iterator objects for multiple splits of a dataset.\n\n        Arguments:\n            datasets: Tuple of Dataset objects corresponding to the splits. The\n                first such object should be the train set.\n            batch_sizes: Tuple of batch sizes to use for the different splits,\n                or None to use the same batch_size for all splits.\n            Remaining keyword arguments: Passed to the constructor of the\n                iterator class being used.\n        \"\"\"\n        if batch_sizes is None:\n            batch_sizes = [kwargs.pop('batch_size')] * len(datasets)\n        ret = []\n        for i in range(len(datasets)):\n            train = i == 0\n            ret.append(cls(\n                datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n        return tuple(ret)",
        "rewrite": "def splits(cls, datasets, batch_sizes=None, **kwargs):\n    if batch_sizes is None:\n        batch_sizes = [kwargs.pop('batch_size')] * len(datasets)\n    ret = []\n    for i in range(len(datasets)):\n        train = i == 0\n        ret.append(cls(\n            datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n    return tuple(ret)"
    },
    {
        "original": "def _get_computer_object():\n    \"\"\"\n    A helper function to get the object for the local machine\n\n    Returns:\n        object: Returns the computer object for the local machine\n    \"\"\"\n    with salt.utils.winapi.Com():\n        nt = win32com.client.Dispatch('AdsNameSpaces')\n    return nt.GetObject('', 'WinNT://.,computer')",
        "rewrite": "def _get_computer_object():\n    with salt.utils.winapi.Com():\n        nt = win32com.client.Dispatch('AdsNameSpaces')\n    return nt.GetObject('', 'WinNT://.,computer')"
    },
    {
        "original": "def initial_state(self, batch_size, trainable=False):\n    \"\"\"Creates the initial memory.\n\n    We should ensure each row of the memory is initialized to be unique,\n    so initialize the matrix to be the identity. We then pad or truncate\n    as necessary so that init_state is of size\n    (batch_size, self._mem_slots, self._mem_size).\n\n    Args:\n      batch_size: The size of the batch.\n      trainable: Whether the initial state is trainable. This is always True.\n\n    Returns:\n      init_state: A truncated or padded matrix of size\n        (batch_size, self._mem_slots, self._mem_size).\n    \"\"\"\n    init_state = tf.eye(self._mem_slots, batch_shape=[batch_size])\n\n    # Pad the matrix with zeros.\n    if self._mem_size > self._mem_slots:\n      difference = self._mem_size - self._mem_slots\n      pad = tf.zeros((batch_size, self._mem_slots, difference))\n      init_state = tf.concat([init_state, pad], -1)\n    # Truncation. Take the first `self._mem_size` components.\n    elif self._mem_size < self._mem_slots:\n      init_state = init_state[:, :, :self._mem_size]\n    return init_state",
        "rewrite": "def initial_state(self, batch_size, trainable=False):\n    init_state = tf.linalg.diag(tf.ones([self._mem_slots, batch_size])) # Initialize matrix to identity\n    \n    if self._mem_size > self._mem_slots:\n        difference = self._mem_size - self._mem_slots\n        pad = tf.zeros((batch_size, self._mem_slots, difference))\n        init_state = tf.concat([init_state, pad], axis=-1)\n    \n    elif self._mem_size < self._mem_slots:\n        init_state = init_state[:, :, :self._mem_size]\n    \n    return init_state"
    },
    {
        "original": "def handle_padding(self, padding):\n        \"\"\"Pads the image with transparent pixels if necessary.\"\"\"\n        left = padding[0]\n        top = padding[1]\n        right = padding[2]\n        bottom = padding[3]\n\n        offset_x = 0\n        offset_y = 0\n        new_width = self.engine.size[0]\n        new_height = self.engine.size[1]\n\n        if left > 0:\n            offset_x = left\n            new_width += left\n        if top > 0:\n            offset_y = top\n            new_height += top\n        if right > 0:\n            new_width += right\n        if bottom > 0:\n            new_height += bottom\n        new_engine = self.context.modules.engine.__class__(self.context)\n        new_engine.image = new_engine.gen_image((new_width, new_height), '#fff')\n        new_engine.enable_alpha()\n        new_engine.paste(self.engine, (offset_x, offset_y))\n        self.engine.image = new_engine.image",
        "rewrite": "def handle_padding(self, padding):\n    left, top, right, bottom = padding[0], padding[1], padding[2], padding[3]\n    \n    offset_x = 0\n    offset_y = 0\n    new_width = self.engine.size[0] + left + right\n    new_height = self.engine.size[1] + top + bottom\n\n    if left > 0:\n        offset_x = left\n    if top > 0:\n        offset_y = top\n        \n    new_engine = self.context.modules.engine.__class__(self.context)\n    new_engine.image = new_engine.gen_image((new_width, new_height), '#fff')\n    new_engine.enable_alpha()\n    new_engine.paste(self.engine, (offset_x, offset_y))\n    self.engine.image = new_engine.image"
    },
    {
        "original": "def vmstats():\n    \"\"\"\n    Return information about the virtual memory on the machine\n\n    Returns:\n        dict: A dictionary of virtual memory stats\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt * status.vmstats\n    \"\"\"\n    # Setup the SPI Structure\n    spi = SYSTEM_PERFORMANCE_INFORMATION()\n    retlen = ctypes.c_ulong()\n\n    # 2 means to query System Performance Information and return it in a\n    # SYSTEM_PERFORMANCE_INFORMATION Structure\n    ctypes.windll.ntdll.NtQuerySystemInformation(\n        2, ctypes.byref(spi), ctypes.sizeof(spi), ctypes.byref(retlen))\n\n    # Return each defined field in a dict\n    ret = {}\n    for field in spi._fields_:\n        ret.update({field[0]: getattr(spi, field[0])})\n\n    return ret",
        "rewrite": "def vmstats():\n    spi = SYSTEM_PERFORMANCE_INFORMATION()\n    retlen = ctypes.c_ulong()\n\n    ctypes.windll.ntdll.NtQuerySystemInformation(2, ctypes.byref(spi), ctypes.sizeof(spi), ctypes.byref(retlen))\n\n    ret = {field[0]: getattr(spi, field[0]) for field in spi._fields_}\n\n    return ret"
    },
    {
        "original": "def _load_account(self, acct_id, acct_dir_path):\n        \"\"\"load configuration from one per-account subdirectory\"\"\"\n        # setup a default config dict for the account\n        self._config[acct_id] = {\n            'name': None,\n            'role_name': DEFAULT_ROLE_NAME,\n            'regions': {}\n        }\n        # read the account config file\n        # @TODO unhandled exception if file doesn't exist or isn't JSON\n        with open(os.path.join(acct_dir_path, 'config.json'), 'r') as fh:\n            acct_conf = json.loads(fh.read())\n        # overwrite defaults with what we read from the account JSON\n        self._config[acct_id].update(acct_conf)\n        # iterate over contents of the per-account directory\n        for region_name in os.listdir(acct_dir_path):\n            path = os.path.join(acct_dir_path, region_name)\n            # skip anything that isn't a directory\n            if not os.path.isdir(path):\n                continue\n            # load the per-region configs for the account...\n            # @TODO - should check that it's a valid region name\n            self._load_region(acct_id, region_name, path)",
        "rewrite": "def _load_account(self, acct_id, acct_dir_path):\n    self._config[acct_id] = {\n        'name': None,\n        'role_name': DEFAULT_ROLE_NAME,\n        'regions': {}\n    }\n    \n    with open(os.path.join(acct_dir_path, 'config.json'), 'r') as fh:\n        acct_conf = json.loads(fh.read())\n        \n    self._config[acct_id].update(acct_conf)\n    \n    for region_name in os.listdir(acct_dir_path):\n        path = os.path.join(acct_dir_path, region_name)\n        \n        if not os.path.isdir(path):\n            continue\n            \n        self._load_region(acct_id, region_name, path)"
    },
    {
        "original": "def _SmallestColSize(self, text):\n        \"\"\"Finds the largest indivisible word of a string.\n\n    ...and thus the smallest possible column width that can contain that\n    word unsplit over rows.\n\n    Args:\n      text: A string of text potentially consisting of words.\n\n    Returns:\n      Integer size of the largest single word in the text.\n    \"\"\"\n        if not text:\n            return 0\n        stripped = terminal.StripAnsiText(text)\n        return max(len(word) for word in stripped.split())",
        "rewrite": "def _SmallestColSize(self, text):\n    if not text:\n        return 0\n    stripped = terminal.StripAnsiText(text)\n    return max(len(word) for word in stripped.split())"
    },
    {
        "original": "def end_block(self):\n        \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\"\n        self.current_indent -= 1\n\n        # If we did not add a new line automatically yet, now it's the time!\n        if not self.auto_added_line:\n            self.writeln()\n            self.auto_added_line = True",
        "rewrite": "def end_block(self):\n    self.current_indent -= 1\n\n    if not self.auto_added_line:\n        self.writeln()\n        self.auto_added_line = True"
    },
    {
        "original": "def check_key(self, key):\n        \"\"\"\n        Check that the key length is valid.\n\n        @param key:    a byte string\n        \"\"\"\n        if self.key_size and len(key) not in self.key_size:\n            raise TypeError('invalid key size %s, must be one of %s' %\n                            (len(key), self.key_size))",
        "rewrite": "def check_key(self, key):\n    if self.key_size and len(key) not in self.key_size:\n        raise TypeError('invalid key size %s, must be one of %s' % (len(key), self.key_size))"
    },
    {
        "original": "def _get_patterns(installed_only=None, root=None):\n    \"\"\"\n    List all known patterns in repos.\n    \"\"\"\n    patterns = {}\n    for element in __zypper__(root=root).nolock.xml.call('se', '-t', 'pattern').getElementsByTagName('solvable'):\n        installed = element.getAttribute('status') == 'installed'\n        if (installed_only and installed) or not installed_only:\n            patterns[element.getAttribute('name')] = {\n                'installed': installed,\n                'summary': element.getAttribute('summary'),\n            }\n\n    return patterns",
        "rewrite": "def _get_patterns(installed_only=None, root=None):\n    patterns = {}\n    for element in __zypper__(root=root).nolock.xml.call('se', '-t', 'pattern').getElementsByTagName('solvable'):\n        installed = element.getAttribute('status') == 'installed'\n        if (installed_only and installed) or not installed_only:\n            patterns[element.getAttribute('name')] = {\n                'installed': installed,\n                'summary': element.getAttribute('summary'),\n            }\n    return patterns"
    },
    {
        "original": "def _map_from_multiclass(self, eopatch, dst_shape, request_data):\n        \"\"\"\n        `raster_value` is a dictionary specifying the intensity values for each class and the corresponding label value.\n\n        A dictionary example for GLC30 LULC mapping is:\n        raster_value = {'no_data': (0,[0,0,0,0]),\n                        'cultivated land': (1,[193, 243, 249, 255]),\n                        'forest': (2,[73, 119, 20, 255]),\n                        'grassland': (3,[95, 208, 169, 255]),\n                        'schrubland': (4,[112, 179, 62, 255]),\n                        'water': (5,[154, 86, 1, 255]),\n                        'wetland': (6,[244, 206, 126, 255]),\n                        'thundra': (7,[50, 100, 100, 255]),\n                        'artificial surface': (8,[20, 47, 147, 255]),\n                        'bareland': (9,[202, 202, 202, 255]),\n                        'snow and ice': (10,[251, 237, 211, 255])}\n        \"\"\"\n        raster = np.ones(dst_shape, dtype=self.raster_dtype) * self.no_data_val\n\n        for key in self.raster_value.keys():\n            value, intensities = self.raster_value[key]\n            raster[np.mean(np.abs(request_data - intensities), axis=-1) < self.mean_abs_difference] = value\n\n        return self._reproject(eopatch, raster)",
        "rewrite": "def _map_from_multiclass(self, eopatch, dst_shape, request_data):\n    raster = np.ones(dst_shape, dtype=self.raster_dtype) * self.no_data_val\n\n    for key in self.raster_value.keys():\n        value, intensities = self.raster_value[key]\n        raster[np.mean(np.abs(request_data - intensities), axis=-1) < self.mean_abs_difference] = value\n\n    return self._reproject(eopatch, raster)"
    },
    {
        "original": "def _format_issue(issue):\n    \"\"\"\n    Helper function to format API return information into a more manageable\n    and useful dictionary for issue information.\n\n    issue\n        The issue to format.\n    \"\"\"\n    ret = {'id': issue.get('id'),\n           'issue_number': issue.get('number'),\n           'state': issue.get('state'),\n           'title': issue.get('title'),\n           'user': issue.get('user').get('login'),\n           'html_url': issue.get('html_url')}\n\n    assignee = issue.get('assignee')\n    if assignee:\n        assignee = assignee.get('login')\n\n    labels = issue.get('labels')\n    label_names = []\n    for label in labels:\n        label_names.append(label.get('name'))\n\n    milestone = issue.get('milestone')\n    if milestone:\n        milestone = milestone.get('title')\n\n    ret['assignee'] = assignee\n    ret['labels'] = label_names\n    ret['milestone'] = milestone\n\n    return ret",
        "rewrite": "def _format_issue(issue):\n    ret = {'id': issue.get('id'),\n           'issue_number': issue.get('number'),\n           'state': issue.get('state'),\n           'title': issue.get('title'),\n           'user': issue.get('user').get('login'),\n           'html_url': issue.get('html_url')}\n\n    assignee = issue.get('assignee')\n    if assignee:\n        assignee = assignee.get('login')\n\n    labels = [label.get('name') for label in issue.get('labels', [])]\n\n    milestone = issue.get('milestone')\n    if milestone:\n        milestone = milestone.get('title')\n\n    ret['assignee'] = assignee\n    ret['labels'] = labels\n    ret['milestone'] = milestone\n\n    return ret"
    },
    {
        "original": "def _is_monotonic(coord, axis=0):\n    \"\"\"\n    >>> _is_monotonic(np.array([0, 1, 2]))\n    True\n    >>> _is_monotonic(np.array([2, 1, 0]))\n    True\n    >>> _is_monotonic(np.array([0, 2, 1]))\n    False\n    \"\"\"\n    if coord.shape[axis] < 3:\n        return True\n    else:\n        n = coord.shape[axis]\n        delta_pos = (coord.take(np.arange(1, n), axis=axis) >=\n                     coord.take(np.arange(0, n - 1), axis=axis))\n        delta_neg = (coord.take(np.arange(1, n), axis=axis) <=\n                     coord.take(np.arange(0, n - 1), axis=axis))\n        return np.all(delta_pos) or np.all(delta_neg)",
        "rewrite": "def _is_monotonic(coord, axis=0):\n    if coord.shape[axis] < 3:\n        return True\n    n = coord.shape[axis]\n    delta_pos = (coord.take(np.arange(1, n), axis=axis) >= coord.take(np.arange(0, n - 1), axis=axis)\n    delta_neg = (coord.take(np.arange(1, n), axis=axis) <= coord.take(np.arange(0, n - 1), axis=axis)\n    return np.all(delta_pos) or np.all(delta_neg)"
    },
    {
        "original": "def update_one(self, update):\n        \"\"\"Update one document matching the selector.\n\n        :Parameters:\n          - `update` (dict): the update operations to apply\n        \"\"\"\n        self.__bulk.add_update(self.__selector,\n                               update, multi=False, upsert=True,\n                               collation=self.__collation)",
        "rewrite": "def update_one(self, update):\n    self.__bulk.add_update(self.__selector, update, multi=False, upsert=True,\n                           collation=self.__collation)"
    },
    {
        "original": "def barracks_in_middle(self) -> Point2:\n        \"\"\" Barracks position in the middle of the 2 depots \"\"\"\n        if len(self.upper2_for_ramp_wall) == 2:\n            points = self.upper2_for_ramp_wall\n            p1 = points.pop().offset((self.x_offset, self.y_offset))\n            p2 = points.pop().offset((self.x_offset, self.y_offset))\n            # Offset from top point to barracks center is (2, 1)\n            intersects = p1.circle_intersection(p2, 5 ** 0.5)\n            anyLowerPoint = next(iter(self.lower))\n            return max(intersects, key=lambda p: p.distance_to(anyLowerPoint))\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")",
        "rewrite": "def barracks_in_middle(self) -> Point2:\n        if len(self.upper2_for_ramp_wall) == 2:\n            points = self.upper2_for_ramp_wall\n            p1 = points.pop().offset((self.x_offset, self.y_offset))\n            p2 = points.pop().offset((self.x_offset, self.y_offset))\n            intersects = p1.circle_intersection(p2, 5 ** 0.5)\n            anyLowerPoint = next(iter(self.lower))\n            return max(intersects, key=lambda p: p.distance_to(anyLowerPoint))\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")"
    },
    {
        "original": "def is_serving(self) -> bool:\n        \"\"\"\n        Tell whether the server is accepting new connections or shutting down.\n\n        \"\"\"\n        try:\n            # Python \u2265 3.7\n            return self.server.is_serving()  # type: ignore\n        except AttributeError:  # pragma: no cover\n            # Python < 3.7\n            return self.server.sockets is not None",
        "rewrite": "def is_serving(self) -> bool:\n    try:\n        return self.server.is_serving()\n    except AttributeError:\n        return self.server.sockets is not None"
    },
    {
        "original": "def getUserContact(master, contact_types, uid):\n    \"\"\"\n    This is a simple getter function that returns a user attribute\n    that matches the contact_types argument, or returns None if no\n    uid/match is found.\n\n    @param master: BuildMaster used to query the database\n    @type master: BuildMaster instance\n\n    @param contact_types: list of contact attributes to look for in\n                         in a given user, such as 'email' or 'nick'\n    @type contact_types: list of strings\n\n    @param uid: user that is searched for the contact_types match\n    @type uid: integer\n\n    @returns: string of contact information or None via deferred\n    \"\"\"\n    d = master.db.users.getUser(uid)\n    d.addCallback(_extractContact, contact_types, uid)\n    return d",
        "rewrite": "def getUserContact(master, contact_types, uid):\n    d = master.db.users.getUser(uid)\n    d.addCallback(_extractContact, contact_types, uid)\n    return d"
    },
    {
        "original": "def direct_callback(self, event):\n        \"\"\"\n        This function is called for every OS keyboard event and decides if the\n        event should be blocked or not, and passes a copy of the event to\n        other, non-blocking, listeners.\n\n        There are two ways to block events: remapped keys, which translate\n        events by suppressing and re-emitting; and blocked hotkeys, which\n        suppress specific hotkeys.\n        \"\"\"\n        # Pass through all fake key events, don't even report to other handlers.\n        if self.is_replaying:\n            return True\n\n        if not all(hook(event) for hook in self.blocking_hooks):\n            return False\n\n        event_type = event.event_type\n        scan_code = event.scan_code\n\n        # Update tables of currently pressed keys and modifiers.\n        with _pressed_events_lock:\n            if event_type == KEY_DOWN:\n                if is_modifier(scan_code): self.active_modifiers.add(scan_code)\n                _pressed_events[scan_code] = event\n            hotkey = tuple(sorted(_pressed_events))\n            if event_type == KEY_UP:\n                self.active_modifiers.discard(scan_code)\n                if scan_code in _pressed_events: del _pressed_events[scan_code]\n\n        # Mappings based on individual keys instead of hotkeys.\n        for key_hook in self.blocking_keys[scan_code]:\n            if not key_hook(event):\n                return False\n\n        # Default accept.\n        accept = True\n\n        if self.blocking_hotkeys:\n            if self.filtered_modifiers[scan_code]:\n                origin = 'modifier'\n                modifiers_to_update = set([scan_code])\n            else:\n                modifiers_to_update = self.active_modifiers\n                if is_modifier(scan_code):\n                    modifiers_to_update = modifiers_to_update | {scan_code}\n                callback_results = [callback(event) for callback in self.blocking_hotkeys[hotkey]]\n                if callback_results:\n                    accept = all(callback_results)\n                    origin = 'hotkey'\n                else:\n                    origin = 'other'\n\n            for key in sorted(modifiers_to_update):\n                transition_tuple = (self.modifier_states.get(key, 'free'), event_type, origin)\n                should_press, new_accept, new_state = self.transition_table[transition_tuple]\n                if should_press: press(key)\n                if new_accept is not None: accept = new_accept\n                self.modifier_states[key] = new_state\n\n        if accept:\n            if event_type == KEY_DOWN:\n                _logically_pressed_keys[scan_code] = event\n            elif event_type == KEY_UP and scan_code in _logically_pressed_keys:\n                del _logically_pressed_keys[scan_code]\n\n        # Queue for handlers that won't block the event.\n        self.queue.put(event)\n\n        return accept",
        "rewrite": "def direct_callback(self, event):\n    if self.is_replaying:\n        return True\n\n    if not all(hook(event) for hook in self.blocking_hooks):\n        return False\n\n    event_type = event.event_type\n    scan_code = event.scan_code\n\n    with _pressed_events_lock:\n        if event_type == KEY_DOWN:\n            if is_modifier(scan_code): \n                self.active_modifiers.add(scan_code)\n            _pressed_events[scan_code] = event\n        hotkey = tuple(sorted(_pressed_events))\n        if event_type == KEY_UP:\n            self.active_modifiers.discard(scan_code)\n            if scan_code in _pressed_events: \n                del _pressed_events[scan_code]\n\n    for key_hook in self.blocking_keys[scan_code]:\n        if not key_hook(event):\n            return False\n\n    accept = True\n\n    if self.blocking_hotkeys:\n        if self.filtered_modifiers[scan_code]:\n            origin = 'modifier'\n            modifiers_to_update = set([scan_code])\n        else:\n            modifiers_to_update = self.active_modifiers\n            if is_modifier(scan_code):\n                modifiers_to_update = modifiers_to_update | {scan_code}\n            callback_results = [callback(event) for callback in self.blocking_hotkeys[hotkey]]\n            if callback_results:\n                accept = all(callback_results)\n                origin = 'hotkey'\n            else:\n                origin = 'other'\n\n        for key in sorted(modifiers_to_update):\n            transition_tuple = (self.modifier_states.get(key, 'free'), event_type, origin)\n            should_press, new_accept, new_state = self.transition_table[transition_tuple]\n            if should_press: \n                press(key)\n            if new_accept is not None: \n                accept = new_accept\n            self.modifier_states[key] = new_state\n\n    if accept:\n        if event_type == KEY_DOWN:\n            _logically_pressed_keys[scan_code] = event\n        elif event_type == KEY_UP and scan_code in _logically_pressed_keys:\n            del _logically_pressed_keys[scan_code]\n\n    self.queue.put(event)\n\n    return accept"
    },
    {
        "original": "def _get_underlayers_size(self):\n        \"\"\"\n        get the total size of all under layers\n        :return: number of bytes\n        \"\"\"\n\n        under_layer = self.underlayer\n\n        under_layers_size = 0\n\n        while under_layer and isinstance(under_layer, Dot1Q):\n            under_layers_size += 4\n            under_layer = under_layer.underlayer\n\n        if under_layer and isinstance(under_layer, Ether):\n            # ether header len + FCS len\n            under_layers_size += 14 + 4\n\n        return under_layers_size",
        "rewrite": "def _get_underlayers_size(self):\n    \"\"\"\n    get the total size of all under layers\n    :return: number of bytes\n    \"\"\"\n\n    under_layer = self.underlayer\n    under_layers_size = 0\n\n    while under_layer and isinstance(under_layer, Dot1Q):\n        under_layers_size += 4\n        under_layer = under_layer.underlayer\n\n    if under_layer and isinstance(under_layer, Ether):\n        under_layers_size += 14 + 4\n\n    return under_layers_size"
    },
    {
        "original": "def process_output(self, data, output_prompt,\n                       input_lines, output, is_doctest, image_file):\n        \"\"\"Process data block for OUTPUT token.\"\"\"\n        if is_doctest:\n            submitted = data.strip()\n            found = output\n            if found is not None:\n                found = found.strip()\n\n                # XXX - fperez: in 0.11, 'output' never comes with the prompt\n                # in it, just the actual output text.  So I think all this code\n                # can be nuked...\n\n                # the above comment does not appear to be accurate... (minrk)\n\n                ind = found.find(output_prompt)\n                if ind<0:\n                    e='output prompt=\"%s\" does not match out line=%s' % \\\n                       (output_prompt, found)\n                    raise RuntimeError(e)\n                found = found[len(output_prompt):].strip()\n\n                if found!=submitted:\n                    e = ('doctest failure for input_lines=\"%s\" with '\n                         'found_output=\"%s\" and submitted output=\"%s\"' %\n                         (input_lines, found, submitted) )\n                    raise RuntimeError(e)",
        "rewrite": "def process_output(self, data, output_prompt,\n                       input_lines, output, is_doctest, image_file):\n        if is_doctest:\n            submitted = data.strip()\n            found = output\n            if found is not None:\n                found = found.strip()\n\n                ind = found.find(output_prompt)\n                if ind < 0:\n                    e = 'output prompt=\"%s\" does not match out line=%s' % (output_prompt, found)\n                    raise RuntimeError(e)\n                found = found[len(output_prompt):].strip()\n\n                if found != submitted:\n                    e = ('doctest failure for input_lines=\"%s\" with '\n                         'found_output=\"%s\" and submitted output=\"%s\"' %\n                         (input_lines, found, submitted))\n                    raise RuntimeError(e)"
    },
    {
        "original": "def bootstrap_standby_leader(self):\n        \"\"\" If we found 'standby' key in the configuration, we need to bootstrap\n            not a real master, but a 'standby leader', that will take base backup\n            from a remote master and start follow it.\n        \"\"\"\n        clone_source = self.get_remote_master()\n        msg = 'clone from remote master {0}'.format(clone_source.conn_url)\n        result = self.clone(clone_source, msg)\n        self._post_bootstrap_task.complete(result)\n        if result:\n            self.state_handler.set_role('standby_leader')\n\n        return result",
        "rewrite": "def bootstrap_standby_leader(self):\n    clone_source = self.get_remote_master()\n    msg = 'clone from remote master {0}'.format(clone_source.conn_url)\n    result = self.clone(clone_source, msg)\n    self._post_bootstrap_task.complete(result)\n    if result:\n        self.state_handler.set_role('standby_leader')\n    return result"
    },
    {
        "original": "def ReadBytes(self, address, num_bytes):\n    \"\"\"Reads at most num_bytes starting from offset <address>.\"\"\"\n    address = int(address)\n    buf = ctypes.create_string_buffer(num_bytes)\n    bytesread = ctypes.c_size_t(0)\n    res = ReadProcessMemory(self.h_process, address, buf, num_bytes,\n                            ctypes.byref(bytesread))\n    if res == 0:\n      err = wintypes.GetLastError()\n      if err == 299:\n        # Only part of ReadProcessMemory has been done, let's return it.\n        return buf.raw[:bytesread.value]\n      raise process_error.ProcessError(\"Error in ReadProcessMemory: %d\" % err)\n\n    return buf.raw[:bytesread.value]",
        "rewrite": "def read_bytes(self, address, num_bytes):\n    address = int(address)\n    buf = ctypes.create_string_buffer(num_bytes)\n    bytesread = ctypes.c_size_t(0)\n    res = ReadProcessMemory(self.h_process, address, buf, num_bytes,\n                            ctypes.byref(bytesread))\n    if res == 0:\n        err = wintypes.GetLastError()\n        if err == 299:\n            return buf.raw[:bytesread.value]\n        raise process_error.ProcessError(\"Error in ReadProcessMemory: %d\" % err)\n\n    return buf.raw[:bytesread.value]"
    },
    {
        "original": "def uniquify_tuples(self, tuples):\n        \"\"\"\n        uniquify mimikatz tuples based on the password\n        cred format- (credType, domain, username, password, hostname, sid)\n\n        Stolen from the Empire project.\n        \"\"\"\n        seen = set()\n        return [item for item in tuples if \"{}{}{}{}\".format(item[0],item[1],item[2],item[3]) not in seen and not seen.add(\"{}{}{}{}\".format(item[0],item[1],item[2],item[3]))]",
        "rewrite": "def uniquify_tuples(self, tuples):\n    \"\"\"\n    uniquify mimikatz tuples based on the password\n    cred format- (credType, domain, username, password, hostname, sid)\n\n    Stolen from the Empire project.\n    \"\"\"\n    seen = set()\n    return [item for item in tuples if \"{}{}{}{}\".format(item[0], item[1], item[2], item[3]) not in seen and not seen.add(\"{}{}{}{}\".format(item[0], item[1], item[2], item[3]))]"
    },
    {
        "original": "def first(self, default=None, as_dict=False, as_ordereddict=False):\n        \"\"\"Returns a single record for the RecordCollection, or `default`. If\n        `default` is an instance or subclass of Exception, then raise it\n        instead of returning it.\"\"\"\n\n        # Try to get a record, or return/raise default.\n        try:\n            record = self[0]\n        except IndexError:\n            if isexception(default):\n                raise default\n            return default\n\n        # Cast and return.\n        if as_dict:\n            return record.as_dict()\n        elif as_ordereddict:\n            return record.as_dict(ordered=True)\n        else:\n            return record",
        "rewrite": "def first(self, default=None, as_dict=False, as_ordereddict=False):\n    try:\n        record = self[0]\n    except IndexError:\n        if isexception(default):\n            raise default\n        return default\n    \n    if as_dict:\n        return record.as_dict()\n    elif as_ordereddict:\n        return record.as_dict(ordered=True)\n    else:\n        return record"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'entities') and self.entities is not None:\n            _dict['entities'] = [x._to_dict() for x in self.entities]\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'entities') and self.entities is not None:\n        _dict['entities'] = [x._to_dict() for x in self.entities]\n    return _dict"
    },
    {
        "original": "def _check_command_response(response, msg=None, allowable_errors=None,\n                            parse_write_concern_error=False):\n    \"\"\"Check the response to a command for errors.\n    \"\"\"\n    if \"ok\" not in response:\n        # Server didn't recognize our message as a command.\n        raise OperationFailure(response.get(\"$err\"),\n                               response.get(\"code\"),\n                               response)\n\n    # TODO: remove, this is moving to _check_gle_response\n    if response.get(\"wtimeout\", False):\n        # MongoDB versions before 1.8.0 return the error message in an \"errmsg\"\n        # field. If \"errmsg\" exists \"err\" will also exist set to None, so we\n        # have to check for \"errmsg\" first.\n        raise WTimeoutError(response.get(\"errmsg\", response.get(\"err\")),\n                            response.get(\"code\"),\n                            response)\n\n    if parse_write_concern_error and 'writeConcernError' in response:\n        wce = response['writeConcernError']\n        raise WriteConcernError(wce['errmsg'], wce['code'], wce)\n\n    if not response[\"ok\"]:\n\n        details = response\n        # Mongos returns the error details in a 'raw' object\n        # for some errors.\n        if \"raw\" in response:\n            for shard in itervalues(response[\"raw\"]):\n                # Grab the first non-empty raw error from a shard.\n                if shard.get(\"errmsg\") and not shard.get(\"ok\"):\n                    details = shard\n                    break\n\n        errmsg = details[\"errmsg\"]\n        if allowable_errors is None or errmsg not in allowable_errors:\n\n            # Server is \"not master\" or \"recovering\"\n            if (errmsg.startswith(\"not master\")\n                    or errmsg.startswith(\"node is recovering\")):\n                raise NotMasterError(errmsg, response)\n\n            # Server assertion failures\n            if errmsg == \"db assertion failure\":\n                errmsg = (\"db assertion failure, assertion: '%s'\" %\n                          details.get(\"assertion\", \"\"))\n                raise OperationFailure(errmsg,\n                                       details.get(\"assertionCode\"),\n                                       response)\n\n            # Other errors\n            code = details.get(\"code\")\n            # findAndModify with upsert can raise duplicate key error\n            if code in (11000, 11001, 12582):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif code == 50:\n                raise ExecutionTimeout(errmsg, code, response)\n            elif code == 43:\n                raise CursorNotFound(errmsg, code, response)\n\n            msg = msg or \"%s\"\n            raise OperationFailure(msg % errmsg, code, response)",
        "rewrite": "def _check_command_response(response, msg=None, allowable_errors=None,\n                            parse_write_concern_error=False):\n    \"\"\"Check the response to a command for errors.\n    \"\"\"\n    if \"ok\" not in response:\n        raise OperationFailure(response.get(\"$err\"),\n                               response.get(\"code\"),\n                               response)\n\n    if response.get(\"wtimeout\", False):\n        raise WTimeoutError(response.get(\"errmsg\", response.get(\"err\")),\n                            response.get(\"code\"),\n                            response)\n\n    if parse_write_concern_error and 'writeConcernError' in response:\n        wce = response['writeConcernError']\n        raise WriteConcernError(wce['errmsg'], wce['code'], wce)\n\n    if not response[\"ok\"]:\n\n        details = response\n        if \"raw\" in response:\n            for shard in itervalues(response[\"raw\"]):\n                if shard.get(\"errmsg\") and not shard.get(\"ok\"):\n                    details = shard\n                    break\n\n        errmsg = details[\"errmsg\"]\n        if allowable_errors is None or errmsg not in allowable_errors:\n\n            if errmsg.startswith(\"not master\") or errmsg.startswith(\"node is recovering\"):\n                raise NotMasterError(errmsg, response)\n\n            if errmsg == \"db assertion failure\":\n                errmsg = (\"db assertion failure, assertion: '%s'\" % details.get(\"assertion\", \"\"))\n                raise OperationFailure(errmsg, details.get(\"assertionCode\"), response)\n\n            code = details.get(\"code\")\n            if code in (11000, 11001, 12582):\n                raise DuplicateKeyError(errmsg, code, response)\n            elif code == 50:\n                raise ExecutionTimeout(errmsg, code, response)\n            elif code == 43:\n                raise CursorNotFound(errmsg, code, response)\n\n            msg = msg or \"%s\"\n            raise OperationFailure(msg % errmsg, code, response)"
    },
    {
        "original": "def csrf_protect_all_post_and_cross_origin_requests():\n    \"\"\"returns None upon success\"\"\"\n    success = None\n\n    if is_cross_origin(request):\n        logger.warning(\"Received cross origin request. Aborting\")\n        abort(403)\n    if request.method in [\"POST\", \"PUT\"]:\n        token = session.get(\"csrf_token\")\n        if token == request.form.get(\"csrf_token\"):\n            return success\n\n        elif token == request.environ.get(\"HTTP_X_CSRFTOKEN\"):\n            return success\n\n        else:\n            logger.warning(\"Received invalid csrf token. Aborting\")\n            abort(403)",
        "rewrite": "def csrf_protect_all_post_and_cross_origin_requests():\n    success = None\n\n    if is_cross_origin(request):\n        logger.warning(\"Received cross origin request. Aborting\")\n        abort(403)\n    \n    if request.method in [\"POST\", \"PUT\"]:\n        token = session.get(\"csrf_token\")\n        if token == request.form.get(\"csrf_token\") or token == request.environ.get(\"HTTP_X_CSRFTOKEN\"):\n            return success\n        else:\n            logger.warning(\"Received invalid csrf token. Aborting\")\n            abort(403)"
    },
    {
        "original": "def update_cluster(cluster_ref, cluster_spec):\n    \"\"\"\n    Updates a cluster in a datacenter.\n\n    cluster_ref\n        The cluster reference.\n\n    cluster_spec\n        The cluster spec (vim.ClusterConfigSpecEx).\n        Defaults to None.\n    \"\"\"\n    cluster_name = get_managed_object_name(cluster_ref)\n    log.trace('Updating cluster \\'%s\\'', cluster_name)\n    try:\n        task = cluster_ref.ReconfigureComputeResource_Task(cluster_spec,\n                                                           modify=True)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    wait_for_task(task, cluster_name, 'ClusterUpdateTask')",
        "rewrite": "def update_cluster(cluster_ref, cluster_spec):\n    cluster_name = get_managed_object_name(cluster_ref)\n    log.trace('Updating cluster \\'%s\\'', cluster_name)\n    try:\n        task = cluster_ref.ReconfigureComputeResource_Task(cluster_spec, modify=True)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError('Not enough permissions. Required privilege: {}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    wait_for_task(task, cluster_name, 'ClusterUpdateTask')"
    },
    {
        "original": "def action(name, text, confirmation=None, icon=None, multiple=True, single=True):\n    \"\"\"\n        Use this decorator to expose actions\n\n        :param name:\n            Action name\n        :param text:\n            Action text.\n        :param confirmation:\n            Confirmation text. If not provided, action will be executed\n            unconditionally.\n        :param icon:\n            Font Awesome icon name\n        :param multiple:\n            If true will display action on list view\n        :param single:\n            If true will display action on show view\n    \"\"\"\n\n    def wrap(f):\n        f._action = (name, text, confirmation, icon, multiple, single)\n        return f\n\n    return wrap",
        "rewrite": "def action(name, text, confirmation=None, icon=None, multiple=True, single=True):\n\n    def wrap(f):\n        f._action = (name, text, confirmation, icon, multiple, single)\n        return f\n\n    return wrap"
    },
    {
        "original": "def get_default_value(self):\n        \"\"\"\n        Gets the best 'value' string this field has.\n        \"\"\"\n        val = self.show\n        if not val:\n            val = self.raw_value\n        if not val:\n            val = self.showname\n        return val",
        "rewrite": "def get_default_value(self):\n    val = self.show\n    if not val:\n        val = self.raw_value\n    if not val:\n        val = self.showname\n    return val"
    },
    {
        "original": "def get_token_accuracy(targets, outputs, ignore_index=None):\n    \"\"\" Get the accuracy token accuracy between two tensors.\n\n    Args:\n      targets (1 - 2D :class:`torch.Tensor`): Target or true vector against which to measure\n          saccuracy\n      outputs (1 - 3D :class:`torch.Tensor`): Prediction or output vector\n      ignore_index (int, optional): Specifies a target index that is ignored\n\n    Returns:\n      :class:`tuple` consisting of accuracy (:class:`float`), number correct (:class:`int`) and\n      total (:class:`int`)\n\n    Example:\n\n        >>> import torch\n        >>> from torchnlp.metrics import get_token_accuracy\n        >>> targets = torch.LongTensor([[1, 1], [2, 2], [3, 3]])\n        >>> outputs = torch.LongTensor([[1, 1], [2, 3], [4, 4]])\n        >>> accuracy, n_correct, n_total = get_token_accuracy(targets, outputs, ignore_index=3)\n        >>> accuracy\n        0.75\n        >>> n_correct\n        3.0\n        >>> n_total\n        4.0\n     \"\"\"\n    n_correct = 0.0\n    n_total = 0.0\n    for target, output in zip(targets, outputs):\n        if not torch.is_tensor(target) or is_scalar(target):\n            target = torch.LongTensor([target])\n\n        if not torch.is_tensor(output) or is_scalar(output):\n            output = torch.LongTensor([[output]])\n\n        if len(target.size()) != len(output.size()):\n            prediction = output.max(dim=0)[0].view(-1)\n        else:\n            prediction = output\n\n        if ignore_index is not None:\n            mask = target.ne(ignore_index)\n            n_correct += prediction.eq(target).masked_select(mask).sum().item()\n            n_total += mask.sum().item()\n        else:\n            n_total += len(target)\n            n_correct += prediction.eq(target).sum().item()\n\n    return n_correct / n_total, n_correct, n_total",
        "rewrite": "import torch\n\ndef get_token_accuracy(targets, outputs, ignore_index=None):\n    n_correct = 0.0\n    n_total = 0.0\n    for target, output in zip(targets, outputs):\n        if not torch.is_tensor(target) or torch.is_scalar(target):\n            target = torch.LongTensor([target])\n\n        if not torch.is_tensor(output) or torch.is_scalar(output):\n            output = torch.LongTensor([[output]])\n\n        if target.dim() != output.dim():\n            prediction = output.max(dim=0)[0].view(-1)\n        else:\n            prediction = output\n\n        if ignore_index is not None:\n            mask = target.ne(ignore_index)\n            n_correct += prediction.eq(target).masked_select(mask).sum().item()\n            n_total += mask.sum().item()\n        else:\n            n_total += len(target)\n            n_correct += prediction.eq(target).sum().item()\n\n    return n_correct / n_total, n_correct, n_total"
    },
    {
        "original": "def get_auth_constraint(self, action_id: str) -> AbstractAuthConstraint:\n        \"\"\"\n        Find rule_id for incoming action_id and return AuthConstraint instance\n        \"\"\"\n        if self.anyone_can_write_map:\n            return self._find_auth_constraint(action_id, self.anyone_can_write_map, from_local=True)\n\n        return self._find_auth_constraint(action_id, self.auth_map)",
        "rewrite": "def get_auth_constraint(self, action_id: str) -> AbstractAuthConstraint:\n        if self.anyone_can_write_map:\n            return self._find_auth_constraint(action_id, self.anyone_can_write_map, from_local=True)\n\n        return self._find_auth_constraint(action_id, self.auth_map)"
    },
    {
        "original": "def get_partition_function(self):\n        \"\"\"\n        Returns the partition function for a given undirected graph.\n\n        A partition function is defined as\n\n        .. math:: \\sum_{X}(\\prod_{i=1}^{m} \\phi_i)\n\n        where m is the number of factors present in the graph\n        and X are all the random variables present.\n\n        Examples\n        --------\n        >>> from pgmpy.models import MarkovModel\n        >>> from pgmpy.factors.discrete import DiscreteFactor\n        >>> G = MarkovModel()\n        >>> G.add_nodes_from(['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7'])\n        >>> G.add_edges_from([('x1', 'x3'), ('x1', 'x4'), ('x2', 'x4'),\n        ...                   ('x2', 'x5'), ('x3', 'x6'), ('x4', 'x6'),\n        ...                   ('x4', 'x7'), ('x5', 'x7')])\n        >>> phi = [DiscreteFactor(edge, [2, 2], np.random.rand(4)) for edge in G.edges()]\n        >>> G.add_factors(*phi)\n        >>> G.get_partition_function()\n        \"\"\"\n        self.check_model()\n\n        factor = self.factors[0]\n        factor = factor_product(factor, *[self.factors[i] for i in\n                                          range(1, len(self.factors))])\n        if set(factor.scope()) != set(self.nodes()):\n            raise ValueError('DiscreteFactor for all the random variables not defined.')\n\n        return np.sum(factor.values)",
        "rewrite": "def get_partition_function(self):\n    self.check_model()\n    factor = self.factors[0]\n    factor = factor_product(factor, *[self.factors[i] for i in range(1, len(self.factors))])\n    if set(factor.scope()) != set(self.nodes()):\n        raise ValueError('DiscreteFactor for all the random variables not defined.')\n    return np.sum(factor.values)"
    },
    {
        "original": "def update_types(config, reference, list_sep=':'):\n    \"\"\"Return a new configuration where all the values types\n    are aligned with the ones in the default configuration\n    \"\"\"\n\n    def _coerce(current, value):\n        # Coerce a value to the `current` type.\n        try:\n            # First we try to apply current to the value, since it\n            # might be a function\n            return current(value)\n        except TypeError:\n            # Then we check if current is a list AND if the value\n            # is a string.\n            if isinstance(current, list) and isinstance(value, str):\n                # If so, we use the colon as the separator\n                return value.split(list_sep)\n\n            try:\n                # If we are here, we should try to apply the type\n                # of `current` to the value\n                return type(current)(value)\n            except TypeError:\n                # Worst case scenario we return the value itself.\n                return value\n\n    def _update_type(value, path):\n        current = reference\n\n        for elem in path:\n            try:\n                current = current[elem]\n            except KeyError:\n                return value\n\n        return _coerce(current, value)\n\n    return map_leafs(_update_type, config)",
        "rewrite": "def update_types(config, reference, list_sep=':'):\n    def _coerce(current, value):\n        try:\n            return current(value)\n        except TypeError:\n            if isinstance(current, list) and isinstance(value, str):\n                return value.split(list_sep)\n            try:\n                return type(current)(value)\n            except TypeError:\n                return value\n\n    def _update_type(value, path):\n        current = reference\n        for elem in path:\n            try:\n                current = current[elem]\n            except KeyError:\n                return value\n        return _coerce(current, value)\n\n    return map_leafs(_update_type, config)"
    },
    {
        "original": "def update_agent_requirements(req_file, check, newline):\n    \"\"\"\n    Replace the requirements line for the given check\n    \"\"\"\n    package_name = get_package_name(check)\n    lines = read_file_lines(req_file)\n\n    for i, line in enumerate(lines):\n        current_package_name = line.split('==')[0]\n\n        if current_package_name == package_name:\n            lines[i] = '{}\\n'.format(newline)\n            break\n\n    write_file_lines(req_file, sorted(lines))",
        "rewrite": "def update_agent_requirements(req_file, check, newline):\n    package_name = get_package_name(check)\n    lines = read_file_lines(req_file)\n\n    for i, line in enumerate(lines):\n        current_package_name = line.split('==')[0]\n\n        if current_package_name == package_name:\n            lines[i] = '{}\\n'.format(newline)\n            break\n\n    write_file_lines(req_file, sorted(lines))"
    },
    {
        "original": "def pre_delayed(values, delay):\n    \"\"\"\n    Waits for *delay* seconds before returning each item from *values*.\n    \"\"\"\n    values = _normalize(values)\n    if delay < 0:\n        raise ValueError(\"delay must be 0 or larger\")\n    for v in values:\n        sleep(delay)\n        yield v",
        "rewrite": "def pre_delayed(values, delay):\n    values = _normalize(values)\n    if delay < 0:\n        raise ValueError(\"delay must be 0 or larger\")\n    for v in values:\n        sleep(delay)\n        yield v"
    },
    {
        "original": "def play(self, **kwargs):\n        \"\"\"Trigger a job explicitly.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobPlayError: If the job could not be triggered\n        \"\"\"\n        path = '%s/%s/play' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)",
        "rewrite": "def play(self, **kwargs):\n    path = f\"{self.manager.path}/{self.get_id()}/play\"\n    self.manager.gitlab.http_post(path, **kwargs)"
    },
    {
        "original": "def _BuildToken(self, request, execution_time):\n    \"\"\"Build an ACLToken from the request.\"\"\"\n    token = access_control.ACLToken(\n        username=request.user,\n        reason=request.args.get(\"reason\", \"\"),\n        process=\"GRRAdminUI\",\n        expiry=rdfvalue.RDFDatetime.Now() + execution_time)\n\n    for field in [\"Remote_Addr\", \"X-Forwarded-For\"]:\n      remote_addr = request.headers.get(field, \"\")\n      if remote_addr:\n        token.source_ips.append(remote_addr)\n    return token",
        "rewrite": "def _BuildToken(self, request, execution_time):\n    token = access_control.ACLToken(\n        username=request.user,\n        reason=request.args.get(\"reason\", \"\"),\n        process=\"GRRAdminUI\",\n        expiry=rdfvalue.RDFDatetime.Now() + execution_time\n    )\n\n    for field in [\"Remote_Addr\", \"X-Forwarded-For\"]:\n        remote_addr = request.headers.get(field, \"\")\n        if remote_addr:\n            token.source_ips.append(remote_addr)\n    \n    return token"
    },
    {
        "original": "def gps_0(self):\n        \"\"\"\n        GPS position information (:py:class:`GPSInfo`).\n        \"\"\"\n        return GPSInfo(self._eph, self._epv, self._fix_type, self._satellites_visible)",
        "rewrite": "def gps_0(self):\n    return GPSInfo(self._eph, self._epv, self._fix_type, self._satellites_visible)"
    },
    {
        "original": "def get_special_folder(self, name):\n        \"\"\" Returns the specified Special Folder\n\n        :return: a special Folder\n        :rtype: drive.Folder\n        \"\"\"\n\n        name = name if \\\n            isinstance(name, OneDriveWellKnowFolderNames) \\\n            else OneDriveWellKnowFolderNames(name.lower())\n        name = name.value\n\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('get_special').format(id=self.object_id,\n                                                          name=name))\n        else:\n            # we don't know the drive_id so go to the default\n            url = self.build_url(\n                self._endpoints.get('get_special_default').format(name=name))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self._classifier(data)(parent=self,\n                                      **{self._cloud_data_key: data})",
        "rewrite": "def get_special_folder(self, name):\n    name = name if isinstance(name, OneDriveWellKnowFolderNames) else OneDriveWellKnowFolderNames(name.lower())\n    name = name.value\n\n    if self.object_id:\n        url = self.build_url(self._endpoints.get('get_special').format(id=self.object_id, name=name))\n    else:\n        url = self.build_url(self._endpoints.get('get_special_default').format(name=name))\n\n    response = self.con.get(url)\n    if not response:\n        return None\n\n    data = response.json()\n\n    return self._classifier(data)(parent=self, **{self._cloud_data_key: data})"
    },
    {
        "original": "def fix_e262(self, result):\n        \"\"\"Fix spacing after comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column']\n\n        code = target[:offset].rstrip(' \\t#')\n        comment = target[offset:].lstrip(' \\t#')\n\n        fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed",
        "rewrite": "def fix_e262(self, result):\n    target = self.source[result['line'] - 1]\n    offset = result['column']\n\n    code = target[:offset].rstrip(' \\t#')\n    comment = target[offset:].lstrip(' \\t#')\n\n    fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n    self.source[result['line'] - 1] = fixed"
    },
    {
        "original": "def session_preparation(self):\n        \"\"\"\n        Prepare the session after the connection has been established.\n        Procurve uses - 'Press any key to continue'\n        \"\"\"\n        delay_factor = self.select_delay_factor(delay_factor=0)\n        output = \"\"\n        count = 1\n        while count <= 30:\n            output += self.read_channel()\n            if \"any key to continue\" in output:\n                self.write_channel(self.RETURN)\n                break\n            else:\n                time.sleep(0.33 * delay_factor)\n            count += 1\n\n        # Try one last time to past \"Press any key to continue\n        self.write_channel(self.RETURN)\n\n        # HP output contains VT100 escape codes\n        self.ansi_escape_codes = True\n\n        self._test_channel_read(pattern=r\"[>#]\")\n        self.set_base_prompt()\n        command = self.RETURN + \"no page\"\n        self.disable_paging(command=command)\n        self.set_terminal_width(command=\"terminal width 511\")\n        # Clear the read buffer\n        time.sleep(0.3 * self.global_delay_factor)\n        self.clear_buffer()",
        "rewrite": "def session_preparation(self):\n    delay_factor = self.select_delay_factor(delay_factor=0)\n    output = \"\"\n    count = 1\n    while count <= 30:\n        output += self.read_channel()\n        if \"any key to continue\" in output:\n            self.write_channel(self.RETURN)\n            break\n        else:\n            time.sleep(0.33 * delay_factor)\n        count += 1\n\n    self.write_channel(self.RETURN)\n\n    self.ansi_escape_codes = True\n\n    self._test_channel_read(pattern=r\"[>#]\")\n    self.set_base_prompt()\n    command = self.RETURN + \"no page\"\n    self.disable_paging(command=command)\n    self.set_terminal_width(command=\"terminal width 511\")\n    \n    time.sleep(0.3 * self.global_delay_factor)\n    self.clear_buffer()"
    },
    {
        "original": "def lcm( *a ):\n  \"\"\"Least common multiple.\n\n  Usage: lcm( [ 3, 4, 5 ] )\n  or:    lcm( 3, 4, 5 )\n  \"\"\"\n\n  if len( a ) > 1: return reduce( lcm2, a )\n  if hasattr( a[0], \"__iter__\" ): return reduce( lcm2, a[0] )\n  return a[0]",
        "rewrite": "def lcm(*a):\n    \"\"\"\n    Least common multiple.\n    \n    Usage: lcm([3, 4, 5])\n    or: lcm(3, 4, 5)\n    \"\"\"\n    \n    if len(a) > 1: \n        return reduce(lcm2, a)\n    if hasattr(a[0], \"__iter__\"):\n        return reduce(lcm2, a[0])\n    return a[0]"
    },
    {
        "original": "def read(self, size):\n        \"\"\"Read bytes from the stream.\"\"\"\n        buf, overflow = self._audio_stream.read(size)\n        if overflow:\n            logging.warning('SoundDeviceStream read overflow (%d, %d)',\n                            size, len(buf))\n        return bytes(buf)",
        "rewrite": "def read(self, size):\n    buf, overflow = self._audio_stream.read(size)\n    if overflow:\n        logging.warning('SoundDeviceStream read overflow (%d, %d)', size, len(buf))\n    return bytes(buf)"
    },
    {
        "original": "def predict_magnification(self, Xnew, kern=None, mean=True, covariance=True, dimensions=None):\n        \"\"\"\n        Predict the magnification factor as\n\n        sqrt(det(G))\n\n        for each point N in Xnew.\n\n        :param bool mean: whether to include the mean of the wishart embedding.\n        :param bool covariance: whether to include the covariance of the wishart embedding.\n        :param array-like dimensions: which dimensions of the input space to use [defaults to self.get_most_significant_input_dimensions()[:2]]\n        \"\"\"\n        G = self.predict_wishart_embedding(Xnew, kern, mean, covariance)\n        if dimensions is None:\n            dimensions = self.get_most_significant_input_dimensions()[:2]\n        G = G[:, dimensions][:,:,dimensions]\n        from ..util.linalg import jitchol\n        mag = np.empty(Xnew.shape[0])\n        for n in range(Xnew.shape[0]):\n            try:\n                mag[n] = np.sqrt(np.exp(2*np.sum(np.log(np.diag(jitchol(G[n, :, :]))))))\n            except:\n                mag[n] = np.sqrt(np.linalg.det(G[n, :, :]))\n        return mag",
        "rewrite": "def predict_magnification(self, Xnew, kern=None, mean=True, covariance=True, dimensions=None):\n    G = self.predict_wishart_embedding(Xnew, kern, mean, covariance)\n    if dimensions is None:\n        dimensions = self.get_most_significant_input_dimensions()[:2]\n    G = G[:, dimensions][:, :, dimensions]\n    from ..util.linalg import jitchol\n    mag = np.empty(Xnew.shape[0])\n    for n in range(Xnew.shape[0]):\n        try:\n            mag[n] = np.sqrt(np.exp(2*np.sum(np.log(np.diag(jitchol(G[n, :, :])))))\n        except:\n            mag[n] = np.sqrt(np.linalg.det(G[n, :, :]))\n    return mag"
    },
    {
        "original": "def decode(b):\n    \"\"\"\n    Decode bytes as MUTF-8\n    See https://docs.oracle.com/javase/6/docs/api/java/io/DataInput.html#modified-utf-8\n    for more information\n\n    Surrogates will be returned as two 16 bit characters.\n\n    :param b: bytes to decode\n    :rtype: unicode (py2), str (py3) of 16bit chars\n    :raises: UnicodeDecodeError if string is not decodable\n    \"\"\"\n    res = \"\"\n\n    b = iter(bytearray(b))\n\n    for x in b:\n        if x >> 7 == 0:\n            # Single char:\n            res += chr(x & 0x7f)\n        elif x >> 5 == 0b110:\n            # 2 byte Multichar\n            b2 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 2 byte sequence does not looks right.\")\n\n            res += chr((x & 0x1f) << 6 | b2 & 0x3f)\n        elif x >> 4 == 0b1110:\n            # 3 byte Multichar\n            b2 = next(b)\n            b3 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 3 byte sequence does not looks right.\")\n            if b3 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Third byte of 3 byte sequence does not looks right.\")\n\n            res += chr((x & 0xf) << 12 | (b2 & 0x3f) << 6 | b3 & 0x3f)\n        else:\n            raise UnicodeDecodeError(\"Could not decode byte\")\n\n    return res",
        "rewrite": "def decode(b):\n    res = \"\"\n    b = iter(bytearray(b))\n\n    for x in b:\n        if x >> 7 == 0:\n            res += chr(x & 0x7f)\n        elif x >> 5 == 0b110:\n            b2 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 2 byte sequence does not looks right.\")\n            res += chr((x & 0x1f) << 6 | b2 & 0x3f)\n        elif x >> 4 == 0b1110:\n            b2 = next(b)\n            b3 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 3 byte sequence does not looks right.\")\n            if b3 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Third byte of 3 byte sequence does not looks right.\")\n            res += chr((x & 0xf) << 12 | (b2 & 0x3f) << 6 | b3 & 0x3f)\n        else:\n            raise UnicodeDecodeError(\"Could not decode byte\")\n\n    return res"
    },
    {
        "original": "def _read_bytes(fp, buf):\n    \"\"\"Read bytes from a file-like object\n\n    @param fp: File-like object that implements read(int)\n    @type fp: file\n\n    @param buf: Buffer to read into\n    @type buf: bytes\n\n    @return: buf\n    \"\"\"\n\n    # Do the first read without resizing the input buffer\n    offset = 0\n    remaining = len(buf)\n    while remaining > 0:\n        l = fp.readinto((ctypes.c_char * remaining).from_buffer(buf, offset))\n        if l is None or l == 0:\n            return offset\n        offset += l\n        remaining -= l\n    return offset",
        "rewrite": "def _read_bytes(fp, buf):\n    offset = 0\n    remaining = len(buf)\n    \n    while remaining > 0:\n        l = fp.readinto((ctypes.c_char * remaining).from_buffer(buf, offset))\n        \n        if l is None or l == 0:\n            return offset\n            \n        offset += l\n        remaining -= l\n        \n    return offset"
    },
    {
        "original": "def build(self):\n        \"\"\"\n        Build vocabulary.\n        \"\"\"\n        token_freq = self._token_count.most_common(self._max_size)\n        idx = len(self.vocab)\n        for token, _ in token_freq:\n            self._token2id[token] = idx\n            self._id2token.append(token)\n            idx += 1\n        if self._unk:\n            unk = '<unk>'\n            self._token2id[unk] = idx\n            self._id2token.append(unk)",
        "rewrite": "def build(self):\n        token_freq = self._token_count.most_common(self._max_size)\n        idx = len(self.vocab)\n        for token, _ in token_freq:\n            self._token2id[token] = idx\n            self._id2token.append(token)\n            idx += 1\n        if self._unk:\n            unk = '<unk>'\n            self._token2id[unk] = idx\n            self._id2token.append(unk)"
    },
    {
        "original": "def is_prime( n ):\n  \"\"\"Return True if x is prime, False otherwise.\n\n  We use the Miller-Rabin test, as given in Menezes et al. p. 138.\n  This test is not exact: there are composite values n for which\n  it returns True.\n\n  In testing the odd numbers from 10000001 to 19999999,\n  about 66 composites got past the first test,\n  5 got past the second test, and none got past the third.\n  Since factors of 2, 3, 5, 7, and 11 were detected during\n  preliminary screening, the number of numbers tested by\n  Miller-Rabin was (19999999 - 10000001)*(2/3)*(4/5)*(6/7)\n  = 4.57 million.\n  \"\"\"\n\n  # (This is used to study the risk of false positives:)\n  global miller_rabin_test_count\n\n  miller_rabin_test_count = 0\n\n  if n <= smallprimes[-1]:\n    if n in smallprimes: return True\n    else: return False\n\n  if gcd( n, 2*3*5*7*11 ) != 1: return False\n\n  # Choose a number of iterations sufficient to reduce the\n  # probability of accepting a composite below 2**-80\n  # (from Menezes et al. Table 4.4):\n\n  t = 40\n  n_bits = 1 + int( math.log( n, 2 ) )\n  for k, tt in ( ( 100, 27 ),\n                 ( 150, 18 ),\n                 ( 200, 15 ),\n                 ( 250, 12 ),\n                 ( 300,  9 ),\n                 ( 350,  8 ),\n                 ( 400,  7 ),\n                 ( 450,  6 ),\n                 ( 550,  5 ),\n                 ( 650,  4 ),\n                 ( 850,  3 ),\n                 ( 1300, 2 ),\n                 ):\n    if n_bits < k: break\n    t = tt\n\n  # Run the test t times:\n\n  s = 0\n  r = n - 1\n  while ( r % 2 ) == 0:\n    s = s + 1\n    r = r // 2\n  for i in range( t ):\n    a = smallprimes[ i ]\n    y = modular_exp( a, r, n )\n    if y != 1 and y != n-1:\n      j = 1\n      while j <= s - 1 and y != n - 1:\n        y = modular_exp( y, 2, n )\n        if y == 1:\n          miller_rabin_test_count = i + 1\n          return False\n        j = j + 1\n      if y != n-1:\n        miller_rabin_test_count = i + 1\n        return False\n  return True",
        "rewrite": "import math\n\ndef is_prime(n):\n    global miller_rabin_test_count\n    miller_rabin_test_count = 0\n    \n    if n <= smallprimes[-1]:\n        if n in smallprimes:\n            return True\n        else:\n            return False\n    \n    if gcd(n, 2*3*5*7*11) != 1:\n        return False\n    \n    t = 40\n    n_bits = 1 + int(math.log(n, 2))\n    for k, tt in ((100, 27),\n                 (150, 18),\n                 (200, 15),\n                 (250, 12),\n                 (300, 9),\n                 (350, 8),\n                 (400, 7),\n                 (450, 6),\n                 (550, 5),\n                 (650, 4),\n                 (850, 3),\n                 (1300, 2)):\n        if n_bits < k:\n            break\n        t = tt\n    \n    s = 0\n    r = n - 1\n    while (r % 2) == 0:\n        s += 1\n        r //= 2\n    \n    for i in range(t):\n        a = smallprimes[i]\n        y = modular_exp(a, r, n)\n        if y != 1 and y != n - 1:\n            j = 1\n            while j <= s - 1 and y != n - 1:\n                y = modular_exp(y, 2, n)\n                if y == 1:\n                    miller_rabin_test_count = i + 1\n                    return False\n                j += 1\n            if y != n - 1:\n                miller_rabin_test_count = i + 1\n                return False\n    \n    return True"
    },
    {
        "original": "def get_summed_icohp_by_label_list(self, label_list, divisor=1.0, summed_spin_channels=True, spin=Spin.up):\n        \"\"\"\n        get the sum of several ICOHP values that are indicated by a list of labels (labels of the bonds are the same as in ICOHPLIST/ICOOPLIST)\n        Args:\n            label_list: list of labels of the ICOHPs/ICOOPs that should be summed\n            divisor: is used to divide the sum\n            summed_spin_channels: Boolean to indicate whether the ICOHPs/ICOOPs of both spin channels should be summed\n            spin: if summed_spin_channels is equal to False, this spin indicates which spin channel should be returned\n\n        Returns:\n             float that is a sum of all ICOHPs/ICOOPs as indicated with label_list\n        \"\"\"\n        sum_icohp = 0\n        for label in label_list:\n            icohp_here = self._icohplist[label]\n            if icohp_here.num_bonds != 1:\n                warnings.warn(\"One of the ICOHP values is an average over bonds. This is currently not considered.\")\n            # prints warning if num_bonds is not equal to 1\n            if icohp_here._is_spin_polarized:\n                if summed_spin_channels:\n                    sum_icohp = sum_icohp + icohp_here.summed_icohp\n                else:\n                    sum_icohp = sum_icohp + icohp_here.icohpvalue(spin)\n            else:\n                sum_icohp = sum_icohp + icohp_here.icohpvalue(spin)\n        return sum_icohp / divisor",
        "rewrite": "def get_summed_icohp_by_label_list(self, label_list, divisor=1.0, summed_spin_channels=True, spin=Spin.up):\n    sum_icohp = 0\n    for label in label_list:\n        icohp_here = self._icohplist[label]\n        if icohp_here.num_bonds != 1:\n            warnings.warn(\"One of the ICOHP values is an average over bonds. This is currently not considered.\")\n        if icohp_here._is_spin_polarized:\n            if summed_spin_channels:\n                sum_icohp += icohp_here.summed_icohp\n            else:\n                sum_icohp += icohp_here.icohpvalue(spin)\n        else:\n            sum_icohp += icohp_here.icohpvalue(spin)\n    return sum_icohp / divisor"
    },
    {
        "original": "def monkhorst(cls, ngkpt, shiftk=(0.5, 0.5, 0.5), chksymbreak=None, use_symmetries=True,\n                  use_time_reversal=True, comment=None):\n        \"\"\"\n        Convenient static constructor for a Monkhorst-Pack mesh.\n\n        Args:\n            ngkpt: Subdivisions N_1, N_2 and N_3 along reciprocal lattice vectors.\n            shiftk: Shift to be applied to the kpoints.\n            use_symmetries: Use spatial symmetries to reduce the number of k-points.\n            use_time_reversal: Use time-reversal symmetry to reduce the number of k-points.\n\n        Returns:\n            :class:`KSampling` object.\n        \"\"\"\n        return cls(\n            kpts=[ngkpt], kpt_shifts=shiftk,\n            use_symmetries=use_symmetries, use_time_reversal=use_time_reversal, chksymbreak=chksymbreak,\n            comment=comment if comment else \"Monkhorst-Pack scheme with user-specified shiftk\")",
        "rewrite": "def monkhorst(cls, ngkpt, shiftk=(0.5, 0.5, 0.5), chksymbreak=None, use_symmetries=True,\n              use_time_reversal=True, comment=None):\n    \n    return cls(kpts=[ngkpt], kpt_shifts=shiftk, \n               use_symmetries=use_symmetries, use_time_reversal=use_time_reversal, chksymbreak=chksymbreak, \n               comment=comment if comment else \"Monkhorst-Pack scheme with user-specified shiftk\")"
    },
    {
        "original": "def record_event(self, event: Event) -> None:\n        \"\"\"\n        Record the event async.\n        \"\"\"\n        from polyaxon.celery_api import celery_app\n        from polyaxon.settings import EventsCeleryTasks\n\n        if not event.ref_id:\n            event.ref_id = self.get_ref_id()\n        serialized_event = event.serialize(dumps=False,\n                                           include_actor_name=True,\n                                           include_instance_info=True)\n\n        celery_app.send_task(EventsCeleryTasks.EVENTS_TRACK, kwargs={'event': serialized_event})\n        celery_app.send_task(EventsCeleryTasks.EVENTS_LOG, kwargs={'event': serialized_event})\n        celery_app.send_task(EventsCeleryTasks.EVENTS_NOTIFY, kwargs={'event': serialized_event})\n        # We include the instance in the serialized event for executor\n        serialized_event['instance'] = event.instance\n        self.executor.record(event_type=event.event_type, event_data=serialized_event)",
        "rewrite": "def record_event(self, event: Event) -> None:\n        from polyaxon.celery_api import celery_app\n        from polyaxon.settings import EventsCeleryTasks\n\n        if not event.ref_id:\n            event.ref_id = self.get_ref_id()\n        \n        serialized_event = event.serialize(dumps=False, include_actor_name=True, include_instance_info=True)\n\n        celery_app.send_task(EventsCeleryTasks.EVENTS_TRACK, kwargs={'event': serialized_event})\n        celery_app.send_task(EventsCeleryTasks.EVENTS_LOG, kwargs={'event': serialized_event})\n        celery_app.send_task(EventsCeleryTasks.EVENTS_NOTIFY, kwargs={'event': serialized_event})\n        \n        serialized_event['instance'] = event.instance\n        self.executor.record(event_type=event.event_type, event_data=serialized_event)"
    },
    {
        "original": "def _update_roster(self):\n        \"\"\"\n        Update default flat roster with the passed in information.\n        :return:\n        \"\"\"\n        roster_file = self._get_roster()\n        if os.access(roster_file, os.W_OK):\n            if self.__parsed_rosters[self.ROSTER_UPDATE_FLAG]:\n                with salt.utils.files.fopen(roster_file, 'a') as roster_fp:\n                    roster_fp.write('# Automatically added by \"{s_user}\" at {s_time}\\n{hostname}:\\n    host: '\n                                    '{hostname}\\n    user: {user}'\n                                    '\\n    passwd: {passwd}\\n'.format(s_user=getpass.getuser(),\n                                                                      s_time=datetime.datetime.utcnow().isoformat(),\n                                                                      hostname=self.opts.get('tgt', ''),\n                                                                      user=self.opts.get('ssh_user', ''),\n                                                                      passwd=self.opts.get('ssh_passwd', '')))\n                log.info('The host {0} has been added to the roster {1}'.format(self.opts.get('tgt', ''),\n                                                                                roster_file))\n        else:\n            log.error('Unable to update roster {0}: access denied'.format(roster_file))",
        "rewrite": "def _update_roster(self):\n    roster_file = self._get_roster()\n    if os.access(roster_file, os.W_OK):\n        if self.__parsed_rosters[self.ROSTER_UPDATE_FLAG]:\n            with salt.utils.files.fopen(roster_file, 'a') as roster_fp:\n                roster_fp.write('# Automatically added by \"{s_user}\" at {s_time}\\n{hostname}:\\n    host: '\n                                '{hostname}\\n    user: {user}'\n                                '\\n    passwd: {passwd}\\n'.format(s_user=getpass.getuser(),\n                                                                  s_time=datetime.datetime.utcnow().isoformat(),\n                                                                  hostname=self.opts.get('tgt', ''),\n                                                                  user=self.opts.get('ssh_user', ''),\n                                                                  passwd=self.opts.get('ssh_passwd', '')))\n            log.info('The host {0} has been added to the roster {1}'.format(self.opts.get('tgt', ''),\n                                                                            roster_file))\n    else:\n        log.error('Unable to update roster {0}: access denied'.format(roster_file))"
    },
    {
        "original": "def _stats_from_measurements(bs_results: np.ndarray, qubit_index_map: Dict,\n                             setting: ExperimentSetting, n_shots: int,\n                             coeff: float = 1.0) -> Tuple[float]:\n    \"\"\"\n    :param bs_results: results from running `qc.run`\n    :param qubit_index_map: dict mapping qubit to classical register index\n    :param setting: ExperimentSetting\n    :param n_shots: number of shots in the measurement process\n    :param coeff: coefficient of the operator being estimated\n    :return: tuple specifying (mean, variance)\n    \"\"\"\n    # Identify classical register indices to select\n    idxs = [qubit_index_map[q] for q, _ in setting.out_operator]\n    # Pick columns corresponding to qubits with a non-identity out_operation\n    obs_strings = bs_results[:, idxs]\n    # Transform bits to eigenvalues; ie (+1, -1)\n    my_obs_strings = 1 - 2 * obs_strings\n    # Multiply row-wise to get operator values. Do statistics. Return result.\n    obs_vals = coeff * np.prod(my_obs_strings, axis=1)\n    obs_mean = np.mean(obs_vals)\n    obs_var = np.var(obs_vals) / n_shots\n\n    return obs_mean, obs_var",
        "rewrite": "import numpy as np\nfrom typing import Dict, Tuple\n\n\ndef _stats_from_measurements(bs_results: np.ndarray, qubit_index_map: Dict,\n                             setting: ExperimentSetting, n_shots: int,\n                             coeff: float = 1.0) -> Tuple[float]:\n    idxs = [qubit_index_map[q] for q, _ in setting.out_operator]\n    obs_strings = bs_results[:, idxs]\n    my_obs_strings = 1 - 2 * obs_strings\n    obs_vals = coeff * np.prod(my_obs_strings, axis=1)\n    obs_mean = np.mean(obs_vals)\n    obs_var = np.var(obs_vals) / n_shots\n    \n    return obs_mean, obs_var"
    },
    {
        "original": "def _get_dir(toml_config_setting, sawtooth_home_dir, windows_dir, default_dir):\n    \"\"\"Determines the directory path based on configuration.\n\n    Arguments:\n        toml_config_setting (str): The name of the config setting related\n            to the directory which will appear in path.toml.\n        sawtooth_home_dir (str): The directory under the SAWTOOTH_HOME\n            environment variable.  For example, for 'data' if the data\n            directory is $SAWTOOTH_HOME/data.\n        windows_dir (str): The windows path relative to the computed base\n            directory.\n        default_dir (str): The default path on Linux.\n\n    Returns:\n        directory (str): The path.\n    \"\"\"\n    conf_file = os.path.join(_get_config_dir(), 'path.toml')\n    if os.path.exists(conf_file):\n        with open(conf_file) as fd:\n            raw_config = fd.read()\n        toml_config = toml.loads(raw_config)\n        if toml_config_setting in toml_config:\n            return toml_config[toml_config_setting]\n\n    if 'SAWTOOTH_HOME' in os.environ:\n        return os.path.join(os.environ['SAWTOOTH_HOME'], sawtooth_home_dir)\n\n    if os.name == 'nt':\n        base_dir = \\\n            os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n        return os.path.join(base_dir, windows_dir)\n\n    return default_dir",
        "rewrite": "def _get_directory(toml_config_setting, sawtooth_home_dir, windows_dir, default_dir):\n    conf_file = os.path.join(_get_config_directory(), 'path.toml')\n    if os.path.exists(conf_file):\n        with open(conf_file) as fd:\n            raw_config = fd.read()\n        toml_config = toml.loads(raw_config)\n        if toml_config_setting in toml_config:\n            return toml_config[toml_config_setting]\n\n    if 'SAWTOOTH_HOME' in os.environ:\n        return os.path.join(os.environ['SAWTOOTH_HOME'], sawtooth_home_dir)\n\n    if os.name == 'nt':\n        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0]))\n        return os.path.join(base_dir, windows_dir)\n\n    return default_dir"
    },
    {
        "original": "def get_jsapi_params(self, prepay_id, timestamp=None, nonce_str=None, jssdk=False):\n        \"\"\"\n        \u83b7\u53d6 JSAPI \u53c2\u6570\n\n        :param prepay_id: \u7edf\u4e00\u4e0b\u5355\u63a5\u53e3\u8fd4\u56de\u7684 prepay_id \u53c2\u6570\u503c\n        :param timestamp: \u53ef\u9009\uff0c\u65f6\u95f4\u6233\uff0c\u9ed8\u8ba4\u4e3a\u5f53\u524d\u65f6\u95f4\u6233\n        :param nonce_str: \u53ef\u9009\uff0c\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u9ed8\u8ba4\u81ea\u52a8\u751f\u6210\n        :param jssdk: \u524d\u7aef\u8c03\u7528\u65b9\u5f0f\uff0c\u9ed8\u8ba4\u4f7f\u7528 WeixinJSBridge\n                      \u4f7f\u7528 jssdk \u8c03\u8d77\u652f\u4ed8\u7684\u8bdd\uff0ctimestamp \u7684 s \u4e3a\u5c0f\u5199\n                      \u4f7f\u7528 WeixinJSBridge \u8c03\u8d77\u652f\u4ed8\u7684\u8bdd\uff0ctimeStamp \u7684 S \u4e3a\u5927\u5199\n        :return: \u53c2\u6570\n        \"\"\"\n        data = {\n            'appId': self.sub_appid or self.appid,\n            'timeStamp': timestamp or to_text(int(time.time())),\n            'nonceStr': nonce_str or random_string(32),\n            'signType': 'MD5',\n            'package': 'prepay_id={0}'.format(prepay_id),\n        }\n        sign = calculate_signature(\n            data,\n            self._client.api_key if not self._client.sandbox else self._client.sandbox_api_key\n        )\n        logger.debug('JSAPI payment parameters: data = %s, sign = %s', data, sign)\n        data['paySign'] = sign\n        if jssdk:\n            data['timestamp'] = data.pop('timeStamp')\n        return data",
        "rewrite": "def get_jsapi_params(self, prepay_id, timestamp=None, nonce_str=None, jssdk=False):\n    data = {\n        'appId': self.sub_appid or self.appid,\n        'timeStamp': timestamp or to_text(int(time.time())),\n        'nonceStr': nonce_str or random_string(32),\n        'signType': 'MD5',\n        'package': 'prepay_id={0}'.format(prepay_id),\n    }\n    sign = calculate_signature(data, self._client.api_key if not self._client.sandbox else self._client.sandbox_api_key)\n    logger.debug('JSAPI payment parameters: data = %s, sign = %s', data, sign)\n    data['paySign'] = sign\n    if jssdk:\n        data['timestamp'] = data.pop('timeStamp')\n    return data"
    },
    {
        "original": "def get_label(self, name):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param name: string\n        :rtype: :class:`github.Label.Label`\n        \"\"\"\n        assert isinstance(name, (str, unicode)), name\n        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url + \"/labels/\" + urllib.quote(name)\n        )\n        return github.Label.Label(self._requester, headers, data, completed=True)",
        "rewrite": "def get_label(self, name):\n    assert isinstance(name, str), name\n    headers, data = self._requester.requestJsonAndCheck(\n        \"GET\",\n        f\"{self.url}/labels/{urllib.parse.quote(name)}\"\n    )\n    return github.Label.Label(self._requester, headers, data, completed=True)"
    },
    {
        "original": "def title(label, style=None):\n    \"\"\"Sets the title for the current figure.\n\n    Parameters\n    ----------\n    label : str\n        The new title for the current figure.\n    style: dict\n        The CSS style to be applied to the figure title\n    \"\"\"\n    fig = current_figure()\n    fig.title = label\n    if style is not None:\n        fig.title_style = style",
        "rewrite": "def title(label, style=None):\n    fig = current_figure()\n    fig.title = label\n    if style is not None:\n        fig.title_style = style"
    },
    {
        "original": "def format_bytes(bytes):\n    \"\"\"\n    Get human readable version of given bytes.\n    Ripped from https://github.com/rg3/youtube-dl\n    \"\"\"\n    if bytes is None:\n        return 'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return '{0:.2f}{1}'.format(converted, suffix)",
        "rewrite": "import math\n\ndef format_bytes(bytes):\n    if bytes is None:\n        return 'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return '{0:.2f}{1}'.format(converted, suffix)"
    },
    {
        "original": "def add(name, **kwargs):\n    \"\"\"\n    Add the specified group\n\n    Args:\n\n        name (str):\n            The name of the group to add\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.add foo\n    \"\"\"\n    if not info(name):\n        comp_obj = _get_computer_object()\n        try:\n            new_group = comp_obj.Create('group', name)\n            new_group.SetInfo()\n            log.info('Successfully created group %s', name)\n        except pywintypes.com_error as exc:\n            msg = 'Failed to create group {0}. {1}'.format(\n                name, win32api.FormatMessage(exc.excepinfo[5]))\n            log.error(msg)\n            return False\n    else:\n        log.warning('The group %s already exists.', name)\n        return False\n    return True",
        "rewrite": "def add(name, **kwargs):\n    if not info(name):\n        comp_obj = _get_computer_object()\n        try:\n            new_group = comp_obj.Create('group', name)\n            new_group.SetInfo()\n            log.info('Successfully created group %s', name)\n        except pywintypes.com_error as exc:\n            msg = 'Failed to create group {0}. {1}'.format(\n                name, win32api.FormatMessage(exc.excepinfo[5]))\n            log.error(msg)\n            return False\n    else:\n        log.warning('The group %s already exists.', name)\n        return False\n    return True"
    },
    {
        "original": "def get_direct_message(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/direct-messages/sending-and-receiving/api-reference/get-message\n            :allowed_param:'id', 'full_text'\n        \"\"\"\n        return bind_api(\n            api=self,\n            path='/direct_messages/show/{id}.json',\n            payload_type='direct_message',\n            allowed_param=['id', 'full_text'],\n            require_auth=True\n        )",
        "rewrite": "def get_direct_message(self):\n    return bind_api(\n        api=self,\n        path='/direct_messages/show/{id}.json',\n        payload_type='direct_message',\n        allowed_param=['id', 'full_text'],\n        require_auth=True\n    )"
    },
    {
        "original": "def _leave_status(self, subreddit, statusurl):\n        \"\"\"Abdicate status in a subreddit.\n\n        :param subreddit: The name of the subreddit to leave `status` from.\n        :param statusurl: The API URL which will be used in the leave request.\n            Please use :meth:`leave_contributor` or :meth:`leave_moderator`\n            rather than setting this directly.\n\n        :returns: the json response from the server.\n        \"\"\"\n        if isinstance(subreddit, six.string_types):\n            subreddit = self.get_subreddit(subreddit)\n\n        data = {'id': subreddit.fullname}\n        return self.request_json(statusurl, data=data)",
        "rewrite": "def leave_status(self, subreddit, statusurl):\n    if isinstance(subreddit, six.string_types):\n        subreddit = self.get_subreddit(subreddit)\n\n    data = {'id': subreddit.fullname}\n    return self.request_json(statusurl, data=data)"
    },
    {
        "original": "def execute_query(**kwargs):\n    \"\"\"\n      Executes a query against the connected db using pymapd\n      https://pymapd.readthedocs.io/en/latest/usage.html#querying\n\n      Kwargs:\n        query_name(str): Name of query\n        query_mapdql(str): Query to run\n        iteration(int): Iteration number\n\n      Returns:\n        query_execution(dict):::\n          result_count(int): Number of results returned\n          execution_time(float): Time (in ms) that pymapd reports\n                                 backend spent on query.\n          connect_time(float): Time (in ms) for overhead of query, calculated\n                               by subtracting backend execution time\n                               from time spent on the execution function.\n          results_iter_time(float): Time (in ms) it took to for\n                                    pymapd.fetchone() to iterate through all\n                                    of the results.\n          total_time(float): Time (in ms) from adding all above times.\n        False(bool): The query failed. Exception should be logged.\n    \"\"\"\n    start_time = timeit.default_timer()\n    try:\n        # Run the query\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(\n            \"Completed iteration \"\n            + str(kwargs[\"iteration\"])\n            + \" of query \"\n            + kwargs[\"query_name\"]\n        )\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error):\n        logging.exception(\n            \"Error running query \"\n            + kwargs[\"query_name\"]\n            + \" during iteration \"\n            + str(kwargs[\"iteration\"])\n        )\n        return False\n\n    # Calculate times\n    query_elapsed_time = (timeit.default_timer() - start_time) * 1000\n    execution_time = query_result._result.execution_time_ms\n    connect_time = round((query_elapsed_time - execution_time), 1)\n\n    # Iterate through each result from the query\n    logging.debug(\n        \"Counting results from query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n    )\n    result_count = 0\n    start_time = timeit.default_timer()\n    while query_result.fetchone():\n        result_count += 1\n    results_iter_time = round(\n        ((timeit.default_timer() - start_time) * 1000), 1\n    )\n\n    query_execution = {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": execution_time + connect_time + results_iter_time,\n    }\n    logging.debug(\n        \"Execution results for query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n        + \": \"\n        + str(query_execution)\n    )\n    return query_execution",
        "rewrite": "import timeit\nimport logging\nimport pymapd\n\ndef execute_query(**kwargs):\n    start_time = timeit.default_timer()\n    try:\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(\"Completed iteration \" + str(kwargs[\"iteration\"]) + \" of query \" + kwargs[\"query_name\"])\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error):\n        logging.exception(\"Error running query \" + kwargs[\"query_name\"] + \" during iteration \" + str(kwargs[\"iteration\"]))\n        return False\n\n    query_elapsed_time = (timeit.default_timer() - start_time) * 1000\n    execution_time = query_result._result.execution_time_ms\n    connect_time = round((query_elapsed_time - execution_time), 1)\n\n    result_count = 0\n    start_time = timeit.default_timer()\n    while query_result.fetchone():\n        result_count += 1\n    results_iter_time = round(((timeit.default_timer() - start_time) * 1000), 1)\n\n    query_execution = {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": execution_time + connect_time + results_iter_time,\n    }\n    logging.debug(\"Execution results for query \" + kwargs[\"query_name\"] + \" iteration \" + str(kwargs[\"iteration\"]) + \": \" + str(query_execution))\n    return query_execution"
    },
    {
        "original": "def _handle_job_without_successors(self, job, irsb, insn_addrs):\n        \"\"\"\n        A block without successors should still be handled so it can be added to the function graph correctly.\n\n        :param CFGJob job:  The current job that do not have any successor.\n        :param IRSB irsb:   The related IRSB.\n        :param insn_addrs:  A list of instruction addresses of this IRSB.\n        :return: None\n        \"\"\"\n\n        # it's not an empty block\n\n        # handle all conditional exits\n        ins_addr = job.addr\n        for stmt_idx, stmt in enumerate(irsb.statements):\n            if type(stmt) is pyvex.IRStmt.IMark:\n                ins_addr = stmt.addr + stmt.delta\n            elif type(stmt) is pyvex.IRStmt.Exit:\n                successor_jumpkind = stmt.jk\n                self._update_function_transition_graph(\n                    job.block_id, None,\n                    jumpkind = successor_jumpkind,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )\n\n        # handle the default exit\n        successor_jumpkind = irsb.jumpkind\n        successor_last_ins_addr = insn_addrs[-1]\n        self._update_function_transition_graph(job.block_id, None,\n                                               jumpkind=successor_jumpkind,\n                                               ins_addr=successor_last_ins_addr,\n                                               stmt_idx=DEFAULT_STATEMENT,\n                                               )",
        "rewrite": "def _handle_job_without_successors(self, job, irsb, insn_addrs):\n        # it's not an empty block\n\n        # handle all conditional exits\n        ins_addr = job.addr\n        for stmt_idx, stmt in enumerate(irsb.statements):\n            if type(stmt) is pyvex.IRStmt.IMark:\n                ins_addr = stmt.addr + stmt.delta\n            elif type(stmt) is pyvex.IRStmt.Exit:\n                successor_jumpkind = stmt.jk\n                self._update_function_transition_graph(\n                    job.block_id, None,\n                    jumpkind=successor_jumpkind,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )\n\n        # handle the default exit\n        successor_jumpkind = irsb.jumpkind\n        successor_last_ins_addr = insn_addrs[-1]\n        self._update_function_transition_graph(\n            job.block_id, None,\n            jumpkind=successor_jumpkind,\n            ins_addr=successor_last_ins_addr,\n            stmt_idx=DEFAULT_STATEMENT,\n        )"
    },
    {
        "original": "def set_release_description(self, description, **kwargs):\n        \"\"\"Set the release notes on the tag.\n\n        If the release doesn't exist yet, it will be created. If it already\n        exists, its description will be updated.\n\n        Args:\n            description (str): Description of the release.\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server fails to create the release\n            GitlabUpdateError: If the server fails to update the release\n        \"\"\"\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/release' % (self.manager.path, id)\n        data = {'description': description}\n        if self.release is None:\n            try:\n                server_data = self.manager.gitlab.http_post(path,\n                                                            post_data=data,\n                                                            **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabCreateError(e.response_code, e.error_message)\n        else:\n            try:\n                server_data = self.manager.gitlab.http_put(path,\n                                                           post_data=data,\n                                                           **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabUpdateError(e.response_code, e.error_message)\n        self.release = server_data",
        "rewrite": "def set_release_description(self, description, **kwargs):\n    id = self.get_id().replace('/', '%2F')\n    path = f\"{self.manager.path}/{id}/release\"\n    data = {'description': description}\n    \n    if self.release is None:\n        try:\n            server_data = self.manager.gitlab.http_post(path, post_data=data, **kwargs)\n        except exc.GitlabHttpError as e:\n            raise exc.GitlabCreateError(e.response_code, e.error_message)\n    else:\n        try:\n            server_data = self.manager.gitlab.http_put(path, post_data=data, **kwargs)\n        except exc.GitlabHttpError as e:\n            raise exc.GitlabUpdateError(e.response_code, e.error_message)\n    \n    self.release = server_data"
    },
    {
        "original": "def set_exp(self, claim='exp', from_time=None, lifetime=None):\n        \"\"\"\n        Updates the expiration time of a token.\n        \"\"\"\n        if from_time is None:\n            from_time = self.current_time\n\n        if lifetime is None:\n            lifetime = self.lifetime\n\n        self.payload[claim] = datetime_to_epoch(from_time + lifetime)",
        "rewrite": "def set_exp(self, claim='exp', from_time=None, lifetime=None):\n    if from_time is None:\n        from_time = self.current_time\n\n    if lifetime is None:\n        lifetime = self.lifetime\n\n    self.payload[claim] = datetime_to_epoch(from_time + lifetime)"
    },
    {
        "original": "def wrap_text(text, width):\n        \"\"\"\n        Wrap text paragraphs to the given character width while preserving\n        newlines.\n        \"\"\"\n        out = []\n        for paragraph in text.splitlines():\n            # Wrap returns an empty list when paragraph is a newline. In order\n            # to preserve newlines we substitute a list containing an empty\n            # string.\n            lines = wrap(paragraph, width=width) or ['']\n            out.extend(lines)\n        return out",
        "rewrite": "def wrap_text(text, width):\n    out = []\n    for paragraph in text.splitlines():\n        lines = wrap(paragraph, width=width) or ['']\n        out.extend(lines)\n    return out"
    },
    {
        "original": "def read_memory(self, *, region_name: str):\n        \"\"\"\n        Reads from a memory region named region_name on the QAM.\n\n        This is a shim over the eventual API and only can return memory from a region named\n        \"ro\" of type ``BIT``.\n\n        :param region_name: The string naming the declared memory region.\n        :return: A list of values of the appropriate type.\n        \"\"\"\n        assert self.status == 'done'\n        if region_name != \"ro\":\n            raise QAMError(\"Currently only allowed to read measurement data from ro.\")\n        if self._bitstrings is None:\n            raise QAMError(\"Bitstrings have not yet been populated. Something has gone wrong.\")\n\n        return self._bitstrings",
        "rewrite": "def read_memory(self, *, region_name: str):\n        assert self.status == 'done'\n        if region_name != \"ro\":\n            raise QAMError(\"Currently only allowed to read measurement data from ro.\")\n        if self._bitstrings is None:\n            raise QAMError(\"Bitstrings have not yet been populated. Something has gone wrong.\")\n\n        return self._bitstrings"
    },
    {
        "original": "def uptime():\n    \"\"\"\n    Return the uptime for this system.\n\n    .. versionchanged:: 2015.8.9\n        The uptime function was changed to return a dictionary of easy-to-read\n        key/value pairs containing uptime information, instead of the output\n        from a ``cmd.run`` call.\n\n    .. versionchanged:: 2016.11.0\n        Support for OpenBSD, FreeBSD, NetBSD, MacOS, and Solaris\n\n    .. versionchanged:: 2016.11.4\n        Added support for AIX\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.uptime\n    \"\"\"\n    curr_seconds = time.time()\n\n    # Get uptime in seconds\n    if salt.utils.platform.is_linux():\n        ut_path = \"/proc/uptime\"\n        if not os.path.exists(ut_path):\n            raise CommandExecutionError(\"File {ut_path} was not found.\".format(ut_path=ut_path))\n        with salt.utils.files.fopen(ut_path) as rfh:\n            seconds = int(float(rfh.read().split()[0]))\n    elif salt.utils.platform.is_sunos():\n        # note: some flavors/versions report the host uptime inside a zone\n        #       https://support.oracle.com/epmos/faces/BugDisplay?id=15611584\n        res = __salt__['cmd.run_all']('kstat -p unix:0:system_misc:boot_time')\n        if res['retcode'] > 0:\n            raise CommandExecutionError('The boot_time kstat was not found.')\n        seconds = int(curr_seconds - int(res['stdout'].split()[-1]))\n    elif salt.utils.platform.is_openbsd() or salt.utils.platform.is_netbsd():\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        seconds = int(curr_seconds - int(bt_data))\n    elif salt.utils.platform.is_freebsd() or salt.utils.platform.is_darwin():\n        # format: { sec = 1477761334, usec = 664698 } Sat Oct 29 17:15:34 2016\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        data = bt_data.split(\"{\")[-1].split(\"}\")[0].strip().replace(' ', '')\n        uptime = dict([(k, int(v,)) for k, v in [p.strip().split('=') for p in data.split(',')]])\n        seconds = int(curr_seconds - uptime['sec'])\n    elif salt.utils.platform.is_aix():\n        seconds = _get_boot_time_aix()\n    else:\n        return __salt__['cmd.run']('uptime')\n\n    # Setup datetime and timedelta objects\n    boot_time = datetime.datetime.utcfromtimestamp(curr_seconds - seconds)\n    curr_time = datetime.datetime.utcfromtimestamp(curr_seconds)\n    up_time = curr_time - boot_time\n\n    # Construct return information\n    ut_ret = {\n        'seconds': seconds,\n        'since_iso': boot_time.isoformat(),\n        'since_t': int(curr_seconds - seconds),\n        'days': up_time.days,\n        'time': '{0}:{1}'.format(up_time.seconds // 3600, up_time.seconds % 3600 // 60),\n    }\n\n    if salt.utils.path.which('who'):\n        who_cmd = 'who' if salt.utils.platform.is_openbsd() else 'who -s'  # OpenBSD does not support -s\n        ut_ret['users'] = len(__salt__['cmd.run'](who_cmd).split(os.linesep))\n\n    return ut_ret",
        "rewrite": "import os\nimport time\nimport datetime\nimport salt.utils.platform\nfrom salt.exceptions import CommandExecutionError\n\ndef uptime():\n    curr_seconds = time.time()\n\n    if salt.utils.platform.is_linux():\n        ut_path = \"/proc/uptime\"\n        if not os.path.exists(ut_path):\n            raise CommandExecutionError(f\"File {ut_path} was not found.\")\n        with open(ut_path, 'r') as rfh:\n            seconds = int(float(rfh.read().split()[0]))\n            \n    elif salt.utils.platform.is_sunos():\n        res = __salt__['cmd.run_all']('kstat -p unix:0:system_misc:boot_time')\n        if res['retcode'] > 0:\n            raise CommandExecutionError('The boot_time kstat was not found.')\n        seconds = int(curr_seconds - int(res['stdout'].split()[-1]))\n        \n    elif salt.utils.platform.is_openbsd() or salt.utils.platform.is_netbsd():\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        seconds = int(curr_seconds - int(bt_data))\n        \n    elif salt.utils.platform.is_freebsd() or salt.utils.platform.is_darwin():\n        bt_data = __salt__['sysctl.get']('kern.boottime')\n        if not bt_data:\n            raise CommandExecutionError('Cannot find kern.boottime system parameter')\n        data = bt_data.split(\"{\")[-1].split(\"}\")[0].strip().replace(' ', '')\n        uptime = dict([(k, int(v,)) for k, v in [p.strip().split('=') for p in data.split(',')])\n        seconds = int(curr_seconds - uptime['sec'])\n        \n    elif salt.utils.platform.is_aix():\n        seconds = _get_boot_time_aix()\n        \n    else:\n        return __salt__['cmd.run']('uptime')\n\n    boot_time = datetime.datetime.utcfromtimestamp(curr_seconds - seconds)\n    curr_time = datetime.datetime.utcfromtimestamp(curr_seconds)\n    up_time = curr_time - boot_time\n\n    ut_ret = {\n        'seconds': seconds,\n        'since_iso': boot_time.isoformat(),\n        'since_t': int(curr_seconds - seconds),\n        'days': up_time.days,\n        'time': '{0}:{1}'.format(up_time.seconds // 3600, up_time.seconds % 3600 // 60),\n    }\n\n    if salt.utils.path.which('who'):\n        who_cmd = 'who' if salt.utils.platform.is_openbsd() else 'who -s'\n        ut_ret['users'] = len(__salt__['cmd.run'](who_cmd).split(os.linesep))\n\n    return ut_ret"
    },
    {
        "original": "def _game_keys_as_array(ds):\n    \"\"\"Turn keys of a Bigtable dataset into an array.\n\n    Take g_GGG_m_MMM and create GGG.MMM numbers.\n\n    Valuable when visualizing the distribution of a given dataset in\n    the game keyspace.\n    \"\"\"\n    ds = ds.map(lambda row_key, cell: row_key)\n    # want 'g_0000001234_m_133' is '0000001234.133' and so forth\n    ds = ds.map(lambda x:\n                tf.strings.to_number(tf.strings.substr(x, 2, 10) +\n                                     '.' +\n                                     tf.strings.substr(x, 15, 3),\n                                     out_type=tf.float64))\n    return make_single_array(ds)",
        "rewrite": "def _game_keys_as_array(ds):\n    ds = ds.map(lambda row_key, cell: row_key)\n    ds = ds.map(lambda x: tf.strings.to_number(tf.strings.substr(x, 2, 10) + '.' + tf.strings.substr(x, 15, 3), out_type=tf.float64))\n    return make_single_array(ds)"
    },
    {
        "original": "def runtime_values(self):\n        \"\"\"\n        All of the concrete values used by this function at runtime (i.e., including passed-in arguments and global\n        values).\n        \"\"\"\n        constants = set()\n        for b in self.block_addrs:\n            for sirsb in self._function_manager._cfg.get_all_irsbs(b):\n                for s in sirsb.successors + sirsb.unsat_successors:\n                    for a in s.history.recent_actions:\n                        for ao in a.all_objects:\n                            if not isinstance(ao.ast, claripy.ast.Base):\n                                constants.add(ao.ast)\n                            elif not ao.ast.symbolic:\n                                constants.add(s.solver.eval(ao.ast))\n        return constants",
        "rewrite": "def runtime_values(self):\n    constants = set()\n    for b in self.block_addrs:\n        for sirsb in self._function_manager._cfg.get_all_irsbs(b):\n            for s in sirsb.successors + sirsb.unsat_successors:\n                for a in s.history.recent_actions:\n                    for ao in a.all_objects:\n                        if not isinstance(ao.ast, claripy.ast.Base):\n                            constants.add(ao.ast)\n                        elif not ao.ast.symbolic:\n                            constants.add(s.solver.eval(ao.ast))\n    return constants"
    },
    {
        "original": "def stoch(df, window=14, d=3, k=3, fast=False):\n    \"\"\"\n    compute the n period relative strength indicator\n    http://excelta.blogspot.co.il/2013/09/stochastic-oscillator-technical.html\n    \"\"\"\n\n    my_df = pd.DataFrame(index=df.index)\n\n    my_df['rolling_max'] = df['high'].rolling(window).max()\n    my_df['rolling_min'] = df['low'].rolling(window).min()\n\n    my_df['fast_k'] = 100 * (df['close'] - my_df['rolling_min'])/(my_df['rolling_max'] - my_df['rolling_min'])\n    my_df['fast_d'] = my_df['fast_k'].rolling(d).mean()\n\n    if fast:\n        return my_df.loc[:, ['fast_k', 'fast_d']]\n\n    my_df['slow_k'] = my_df['fast_k'].rolling(k).mean()\n    my_df['slow_d'] = my_df['slow_k'].rolling(d).mean()\n\n    return my_df.loc[:, ['slow_k', 'slow_d']]",
        "rewrite": "def stoch(df, window=14, d=3, k=3, fast=False):\n    my_df = pd.DataFrame(index=df.index)\n    my_df['rolling_max'] = df['high'].rolling(window).max()\n    my_df['rolling_min'] = df['low'].rolling(window).min()\n    my_df['fast_k'] = 100 * (df['close'] - my_df['rolling_min']) / (my_df['rolling_max'] - my_df['rolling_min'])\n    my_df['fast_d'] = my_df['fast_k'].rolling(d).mean()\n    if fast:\n        return my_df.loc[:, ['fast_k', 'fast_d']]\n    my_df['slow_k'] = my_df['fast_k'].rolling(k).mean()\n    my_df['slow_d'] = my_df['slow_k'].rolling(d).mean()\n    return my_df.loc[:, ['slow_k', 'slow_d']]"
    },
    {
        "original": "def inv_diagonal(S):\n    \"\"\"\n    Computes the inverse of a diagonal NxN np.array S. In general this will\n    be much faster than calling np.linalg.inv().\n\n    However, does NOT check if the off diagonal elements are non-zero. So long\n    as S is truly diagonal, the output is identical to np.linalg.inv().\n\n    Parameters\n    ----------\n    S : np.array\n        diagonal NxN array to take inverse of\n\n    Returns\n    -------\n    S_inv : np.array\n        inverse of S\n\n\n    Examples\n    --------\n\n    This is meant to be used as a replacement inverse function for\n    the KalmanFilter class when you know the system covariance S is\n    diagonal. It just makes the filter run faster, there is\n\n    >>> kf = KalmanFilter(dim_x=3, dim_z=1)\n    >>> kf.inv = inv_diagonal  # S is 1x1, so safely diagonal\n    \"\"\"\n\n    S = np.asarray(S)\n\n    if S.ndim != 2 or S.shape[0] != S.shape[1]:\n        raise ValueError('S must be a square Matrix')\n\n    si = np.zeros(S.shape)\n    for i in range(len(S)):\n        si[i, i] = 1. / S[i, i]\n    return si",
        "rewrite": "import numpy as np\n\ndef inv_diagonal(S):\n    S = np.asarray(S)\n\n    if S.ndim != 2 or S.shape[0] != S.shape[1]:\n        raise ValueError('S must be a square Matrix')\n\n    si = np.zeros(S.shape)\n    for i in range(len(S)):\n        si[i, i] = 1. / S[i, i]\n    return si"
    },
    {
        "original": "def simulate_indices(self, ts_length, init=None, num_reps=None,\n                         random_state=None):\n        \"\"\"\n        Simulate time series of state transitions, where state indices\n        are returned.\n\n        Parameters\n        ----------\n        ts_length : scalar(int)\n            Length of each simulation.\n\n        init : int or array_like(int, ndim=1), optional\n            Initial state(s). If None, the initial state is randomly\n            drawn.\n\n        num_reps : scalar(int), optional(default=None)\n            Number of repetitions of simulation.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        X : ndarray(ndim=1 or 2)\n            Array containing the state values of the sample path(s). See\n            the `simulate` method for more information.\n\n        \"\"\"\n        random_state = check_random_state(random_state)\n        dim = 1  # Dimension of the returned array: 1 or 2\n\n        msg_out_of_range = 'index {init} is out of the state space'\n\n        try:\n            k = len(init)  # init is an array\n            dim = 2\n            init_states = np.asarray(init, dtype=int)\n            # Check init_states are in the state space\n            if (init_states >= self.n).any() or (init_states < -self.n).any():\n                idx = np.where(\n                    (init_states >= self.n) + (init_states < -self.n)\n                )[0][0]\n                raise ValueError(msg_out_of_range.format(init=idx))\n            if num_reps is not None:\n                k *= num_reps\n                init_states = np.tile(init_states, num_reps)\n        except TypeError:  # init is a scalar(int) or None\n            k = 1\n            if num_reps is not None:\n                dim = 2\n                k = num_reps\n            if init is None:\n                init_states = random_state.randint(self.n, size=k)\n            elif isinstance(init, numbers.Integral):\n                # Check init is in the state space\n                if init >= self.n or init < -self.n:\n                    raise ValueError(msg_out_of_range.format(init=init))\n                init_states = np.ones(k, dtype=int) * init\n            else:\n                raise ValueError(\n                    'init must be int, array_like of ints, or None'\n                )\n\n        # === set up array to store output === #\n        X = np.empty((k, ts_length), dtype=int)\n\n        # Random values, uniformly sampled from [0, 1)\n        random_values = random_state.random_sample(size=(k, ts_length-1))\n\n        # Generate sample paths and store in X\n        if not self.is_sparse:  # Dense\n            _generate_sample_paths(\n                self.cdfs, init_states, random_values, out=X\n            )\n        else:  # Sparse\n            _generate_sample_paths_sparse(\n                self.cdfs1d, self.P.indices, self.P.indptr, init_states,\n                random_values, out=X\n            )\n\n        if dim == 1:\n            return X[0]\n        else:\n            return X",
        "rewrite": "def simulate_indices(self, ts_length, init=None, num_reps=None, random_state=None):\n        random_state = check_random_state(random_state)\n        dim = 1    # Dimension of the returned array: 1 or 2\n        msg_out_of_range = 'index {init} is out of the state space'\n        \n        try:\n            k = len(init)   # init is an array\n            dim = 2\n            init_states = np.asarray(init, dtype=int)\n            \n            if (init_states >= self.n).any() or (init_states < -self.n).any():\n                idx = np.where((init_states >= self.n) + (init_states < -self.n))[0][0]\n                raise ValueError(msg_out_of_range.format(init=idx))\n                \n            if num_reps is not None:\n                k *= num_reps\n                init_states = np.tile(init_states, num_reps)\n        \n        except TypeError:   # init is a scalar(int) or None\n            k = 1\n            if num_reps is not None:\n                dim = 2\n                k = num_reps\n            \n            if init is None:\n                init_states = random_state.randint(self.n, size=k)\n            elif isinstance(init, numbers.Integral):\n                if init >= self.n or init < -self.n:\n                    raise ValueError(msg_out_of_range.format(init=init))\n                \n                init_states = np.ones(k, dtype=int) * init\n            else:\n                raise ValueError('init must be int, array_like of ints, or None')\n        \n        X = np.empty((k, ts_length), dtype=int)\n        random_values = random_state.random_sample(size=(k, ts_length-1))\n        \n        if not self.is_sparse:  # Dense\n            _generate_sample_paths(self.cdfs, init_states, random_values, out=X)\n        else:   # Sparse\n            _generate_sample_paths_sparse(self.cdfs1d, self.P.indices, self.P.indptr, init_states,\n                                          random_values, out=X)\n        \n        if dim == 1:\n            return X[0]\n        else:\n            return X"
    },
    {
        "original": "def factor_information_coefficient(factor_data,\n                                   group_adjust=False,\n                                   by_group=False):\n    \"\"\"\n    Computes the Spearman Rank Correlation based Information Coefficient (IC)\n    between factor values and N period forward returns for each period in\n    the factor index.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_adjust : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, compute period wise IC separately for each group.\n\n    Returns\n    -------\n    ic : pd.DataFrame\n        Spearman Rank correlation between factor and\n        provided forward returns.\n    \"\"\"\n\n    def src_ic(group):\n        f = group['factor']\n        _ic = group[utils.get_forward_returns_columns(factor_data.columns)] \\\n            .apply(lambda x: stats.spearmanr(x, f)[0])\n        return _ic\n\n    factor_data = factor_data.copy()\n\n    grouper = [factor_data.index.get_level_values('date')]\n\n    if group_adjust:\n        factor_data = utils.demean_forward_returns(factor_data,\n                                                   grouper + ['group'])\n    if by_group:\n        grouper.append('group')\n\n    ic = factor_data.groupby(grouper).apply(src_ic)\n\n    return ic",
        "rewrite": "def factor_information_coefficient(factor_data, group_adjust=False, by_group=False):\n\n    def src_ic(group):\n        f = group['factor']\n        _ic = group[utils.get_forward_returns_columns(factor_data.columns)].apply(lambda x: stats.spearmanr(x, f)[0])\n        return _ic\n\n    factor_data = factor_data.copy()\n\n    grouper = [factor_data.index.get_level_values('date')]\n\n    if group_adjust:\n        factor_data = utils.demean_forward_returns(factor_data, grouper + ['group'])\n    \n    if by_group:\n        grouper.append('group')\n\n    ic = factor_data.groupby(grouper).apply(src_ic)\n\n    return ic"
    },
    {
        "original": "def _construct_w(self, inputs):\n    \"\"\"Connects the module into the graph, with input Tensor `inputs`.\n\n    Args:\n      inputs: A 4D Tensor of shape:\n          [batch_size, input_height, input_width, input_channels]\n          and of type `tf.float16`, `tf.bfloat16` or `tf.float32`.\n\n    Returns:\n      A tuple of two 4D Tensors, each with the same dtype as `inputs`:\n        1. w_dw, the depthwise weight matrix, of shape:\n          [kernel_size, input_channels, channel_multiplier]\n        2. w_pw, the pointwise weight matrix, of shape:\n          [1, 1, channel_multiplier * input_channels, output_channels].\n    \"\"\"\n    depthwise_weight_shape = self._kernel_shape + (self._input_channels,\n                                                   self._channel_multiplier)\n    pointwise_input_size = self._channel_multiplier * self._input_channels\n    pointwise_weight_shape = (1, 1, pointwise_input_size, self._output_channels)\n\n    if \"w_dw\" not in self._initializers:\n      fan_in_shape = depthwise_weight_shape[:2]\n      self._initializers[\"w_dw\"] = create_weight_initializer(fan_in_shape,\n                                                             dtype=inputs.dtype)\n\n    if \"w_pw\" not in self._initializers:\n      fan_in_shape = pointwise_weight_shape[:3]\n      self._initializers[\"w_pw\"] = create_weight_initializer(fan_in_shape,\n                                                             dtype=inputs.dtype)\n\n    w_dw = tf.get_variable(\n        \"w_dw\",\n        shape=depthwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_dw\"],\n        partitioner=self._partitioners.get(\"w_dw\", None),\n        regularizer=self._regularizers.get(\"w_dw\", None))\n\n    w_pw = tf.get_variable(\n        \"w_pw\",\n        shape=pointwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_pw\"],\n        partitioner=self._partitioners.get(\"w_pw\", None),\n        regularizer=self._regularizers.get(\"w_pw\", None))\n\n    return w_dw, w_pw",
        "rewrite": "def _construct_w(self, inputs):\n    depthwise_weight_shape = self._kernel_shape + (self._input_channels, self._channel_multiplier)\n    pointwise_input_size = self._channel_multiplier * self._input_channels\n    pointwise_weight_shape = (1, 1, pointwise_input_size, self._output_channels)\n\n    if \"w_dw\" not in self._initializers:\n        fan_in_shape = depthwise_weight_shape[:2]\n        self._initializers[\"w_dw\"] = create_weight_initializer(fan_in_shape, dtype=inputs.dtype)\n\n    if \"w_pw\" not in self._initializers:\n        fan_in_shape = pointwise_weight_shape[:3]\n        self._initializers[\"w_pw\"] = create_weight_initializer(fan_in_shape, dtype=inputs.dtype)\n\n    w_dw = tf.get_variable(\n        \"w_dw\",\n        shape=depthwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_dw\"],\n        partitioner=self._partitioners.get(\"w_dw\", None),\n        regularizer=self._regularizers.get(\"w_dw\", None)\n    )\n\n    w_pw = tf.get_variable(\n        \"w_pw\",\n        shape=pointwise_weight_shape,\n        dtype=inputs.dtype,\n        initializer=self._initializers[\"w_pw\"],\n        partitioner=self._partitioners.get(\"w_pw\", None),\n        regularizer=self._regularizers.get(\"w_pw\", None)\n    )\n\n    return w_dw, w_pw"
    },
    {
        "original": "def _cleanup_factory(self):\n        \"\"\"Build a cleanup clojure that doesn't increase our ref count\"\"\"\n        _self = weakref.proxy(self)\n        def wrapper():\n            try:\n                _self.close(timeout=0)\n            except (ReferenceError, AttributeError):\n                pass\n        return wrapper",
        "rewrite": "def _cleanup_factory(self):\n    _self = weakref.proxy(self)\n    def wrapper():\n        try:\n            _self.close(timeout=0)\n        except (ReferenceError, AttributeError):\n            pass\n    return wrapper"
    },
    {
        "original": "def resolve(self, key):\n        \"\"\"Looks up a variable like `__getitem__` or `get` but returns an\n        :class:`Undefined` object with the name of the name looked up.\n        \"\"\"\n        if key in self.vars:\n            return self.vars[key]\n        if key in self.parent:\n            return self.parent[key]\n        return self.environment.undefined(name=key)",
        "rewrite": "def resolve(self, key):\n    if key in self.vars:\n        return self.vars[key]\n    if key in self.parent:\n        return self.parent[key]\n    return self.environment.undefined(name=key)"
    },
    {
        "original": "def _flush_content(self):\n        \"\"\"\n        Flush content to the archive\n        :return:\n        \"\"\"\n        if self.__current_section is not None:\n            buff = BytesIO()\n            buff._dirty = False\n            for action_return in self.__current_section:\n                for title, ret_data in action_return.items():\n                    if isinstance(ret_data, file):\n                        self.out.put(ret_data.name, indent=4)\n                        self.__arch.add(ret_data.name, arcname=ret_data.name)\n                    else:\n                        buff.write(salt.utils.stringutils.to_bytes(title + '\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(('-' * len(title)) + '\\n\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(ret_data))\n                        buff.write(salt.utils.stringutils.to_bytes('\\n\\n\\n'))\n                        buff._dirty = True\n            if buff._dirty:\n                buff.seek(0)\n                tar_info = tarfile.TarInfo(name=\"{}/{}\".format(self.__default_root, self.__current_section_name))\n                if not hasattr(buff, 'getbuffer'):  # Py2's BytesIO is older\n                    buff.getbuffer = buff.getvalue\n                tar_info.size = len(buff.getbuffer())\n                self.__arch.addfile(tarinfo=tar_info, fileobj=buff)",
        "rewrite": "def _flush_content(self):\n    if self.__current_section is not None:\n        buff = BytesIO()\n        buff._dirty = False\n        for action_return in self.__current_section:\n            for title, ret_data in action_return.items():\n                if isinstance(ret_data, file):\n                    self.out.put(ret_data.name, indent=4)\n                    self.__arch.add(ret_data.name, arcname=ret_data.name)\n                else:\n                    buff.write(salt.utils.stringutils.to_bytes(title + '\\n'))\n                    buff.write(salt.utils.stringutils.to_bytes(('-' * len(title)) + '\\n\\n'))\n                    buff.write(salt.utils.stringutils.to_bytes(ret_data))\n                    buff.write(salt.utils.stringutils.to_bytes('\\n\\n\\n'))\n                    buff._dirty = True\n        if buff._dirty:\n            buff.seek(0)\n            tar_info = tarfile.TarInfo(name=\"{}/{}\".format(self.__default_root, self.__current_section_name))\n            if not hasattr(buff, 'getbuffer'):\n                buff.getbuffer = buff.getvalue\n            tar_info.size = len(buff.getbuffer())\n            self.__arch.addfile(tarinfo=tar_info, fileobj=buff)"
    },
    {
        "original": "def download(self, to_path=None, name=None, chunk_size='auto',\n                 convert_to_pdf=False):\n        \"\"\" Downloads this version.\n        You can not download the current version (last one).\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        return super().download(to_path=to_path, name=name,\n                                chunk_size=chunk_size,\n                                convert_to_pdf=convert_to_pdf)",
        "rewrite": "def download(self, to_path=None, name=None, chunk_size='auto', convert_to_pdf=False):\n        return super().download(to_path=to_path, name=name, chunk_size=chunk_size, convert_to_pdf=convert_to_pdf)"
    },
    {
        "original": "def _replace_nan(a, val):\n    \"\"\"\n    replace nan in a by val, and returns the replaced array and the nan\n    position\n    \"\"\"\n    mask = isnull(a)\n    return where_method(val, mask, a), mask",
        "rewrite": "def _replace_nan(a, val):\n    mask = np.isnan(a)\n    return np.where(mask, val, a), mask"
    },
    {
        "original": "def timestamping_validate(data, schema):\n    \"\"\"\n    Custom validation function which inserts a timestamp for when the\n    validation occurred\n    \"\"\"\n    jsonschema.validate(data, schema)\n    data['timestamp'] = str(time.time())",
        "rewrite": "def timestamping_validate(data, schema):\n    jsonschema.validate(data, schema)\n    data['timestamp'] = str(time.time())"
    },
    {
        "original": "def execute(self, eopatch):\n        \"\"\" Compute argmax/argmin of specified `data_feature` and `data_index`\n\n        :param eopatch: Input eopatch\n        :return: eopatch with added argmax/argmin features\n        \"\"\"\n        if self.mask_data:\n            valid_data_mask = eopatch.mask['VALID_DATA']\n        else:\n            valid_data_mask = eopatch.mask['IS_DATA']\n\n        if self.data_index is None:\n            data = eopatch.data[self.data_feature]\n        else:\n            data = eopatch.data[self.data_feature][..., self.data_index]\n\n        madata = np.ma.array(data,\n                             dtype=np.float32,\n                             mask=~valid_data_mask.astype(np.bool))\n\n        argmax_data = np.ma.MaskedArray.argmax(madata, axis=0)\n        argmin_data = np.ma.MaskedArray.argmin(madata, axis=0)\n\n        if argmax_data.ndim == 2:\n            argmax_data = argmax_data.reshape(argmax_data.shape + (1,))\n\n        if argmin_data.ndim == 2:\n            argmin_data = argmin_data.reshape(argmin_data.shape + (1,))\n\n        eopatch.data_timeless[self.amax_feature] = argmax_data\n        eopatch.data_timeless[self.amin_feature] = argmin_data\n\n        return eopatch",
        "rewrite": "def execute(self, eopatch):\n    if self.mask_data:\n        valid_data_mask = eopatch.mask['VALID_DATA']\n    else:\n        valid_data_mask = eopatch.mask['IS_DATA']\n    \n    if self.data_index is None:\n        data = eopatch.data[self.data_feature]\n    else:\n        data = eopatch.data[self.data_feature][..., self.data_index]\n    \n    madata = np.ma.array(data, dtype=np.float32, mask=~valid_data_mask.astype(np.bool))\n    \n    argmax_data = np.ma.MaskedArray.argmax(madata, axis=0)\n    argmin_data = np.ma.MaskedArray.argmin(madata, axis=0)\n    \n    if argmax_data.ndim == 2:\n        argmax_data = argmax_data.reshape(argmax_data.shape + (1,))\n    \n    if argmin_data.ndim == 2:\n        argmin_data = argmin_data.reshape(argmin_data.shape + (1,))\n    \n    eopatch.data_timeless[self.amax_feature] = argmax_data\n    eopatch.data_timeless[self.amin_feature] = argmin_data\n    \n    return eopatch"
    },
    {
        "original": "def configure():\n    \"\"\"\n    Configures the logging facility\n\n    This function will setup an initial logging facility for handling display\n    and debug outputs.  The default facility will send display messages to\n    stdout and the default debug facility will do nothing.\n\n    :returns: None\n    \"\"\"\n    root_logger = logging.getLogger()\n    root_logger.addHandler(logging.NullHandler())\n    root_logger.setLevel(99)\n\n    _display_logger.setLevel(70)\n    _debug_logger.setLevel(10)\n\n    display_handlers = [h.get_name() for h in _display_logger.handlers]\n\n    if 'stdout' not in display_handlers:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.set_name('stdout')\n        formatter = logging.Formatter('%(message)s')\n        stdout_handler.setFormatter(formatter)\n        _display_logger.addHandler(stdout_handler)",
        "rewrite": "import logging\nimport sys\n\ndef configure():\n    root_logger = logging.getLogger()\n    root_logger.addHandler(logging.NullHandler())\n    root_logger.setLevel(logging.DEBUG)\n\n    _display_logger = logging.getLogger('display_logger')\n    _debug_logger = logging.getLogger('debug_logger')\n    \n    _display_logger.setLevel(logging.INFO)\n    _debug_logger.setLevel(logging.DEBUG)\n    \n    display_handlers = [h.get_name() for h in _display_logger.handlers]\n    \n    if 'stdout' not in display_handlers:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.set_name('stdout')\n        formatter = logging.Formatter('%(message)s')\n        stdout_handler.setFormatter(formatter)\n        _display_logger.addHandler(stdout_handler)"
    },
    {
        "original": "def i2len(self, pkt, val):\n        \"\"\"get the length of the field, including the padding length\"\"\"\n        fld_len = self._fld.i2len(pkt, val)\n        return fld_len + self.padlen(fld_len)",
        "rewrite": "def i2len(self, pkt, val):\n    fld_len = self._fld.i2len(pkt, val)\n    return fld_len + self.padlen(fld_len)"
    },
    {
        "original": "def get(self, pk):\n        \"\"\"\n            Returns the object for the key\n            Override it for efficiency.\n        \"\"\"\n        for item in self.store.get(self.query_class):\n            # coverts pk value to correct type\n            pk = item.properties[item.pk].col_type(pk)\n            if getattr(item, item.pk) == pk:\n                return item",
        "rewrite": "def get(self, pk):\n        for item in self.store.get(self.query_class):\n            pk = item.properties[item.pk].col_type(pk)\n            if getattr(item, item.pk) == pk:\n                return item"
    },
    {
        "original": "def truncate(v: str, *, max_len: int = 80) -> str:\n    \"\"\"\n    Truncate a value and add a unicode ellipsis (three dots) to the end if it was too long\n    \"\"\"\n    if isinstance(v, str) and len(v) > (max_len - 2):\n        # -3 so quote + string + \u2026 + quote has correct length\n        return repr(v[: (max_len - 3)] + '\u2026')\n    v = repr(v)\n    if len(v) > max_len:\n        v = v[: max_len - 1] + '\u2026'\n    return v",
        "rewrite": "def truncate(v: str, *, max_len: int = 80) -> str:\n    if isinstance(v, str) and len(v) > (max_len - 2):\n        return repr(v[: (max_len - 3)] + '\u2026')\n    v = repr(v)\n    if len(v) > max_len:\n        v = v[: max_len - 1] + '\u2026'\n    return v"
    },
    {
        "original": "def create_filter(extended, from_id, to_id, rtr_only, rtr_too):\n        \"\"\"\n        Calculates AMR and ACR using CAN-ID as parameter.\n\n        :param bool extended:\n            if True parameters from_id and to_id contains 29-bit CAN-ID\n\n        :param int from_id:\n            first CAN-ID which should be received\n\n        :param int to_id:\n            last CAN-ID which should be received\n\n        :param bool rtr_only:\n            if True only RTR-Messages should be received, and rtr_too will be ignored\n\n        :param bool rtr_too:\n            if True CAN data frames and RTR-Messages should be received\n\n        :return: Returns list with one filter containing a \"can_id\", a \"can_mask\" and \"extended\" key.\n        \"\"\"\n        return [{\n            \"can_id\": Ucan.calculate_acr(extended, from_id, to_id, rtr_only, rtr_too),\n            \"can_mask\": Ucan.calculate_amr(extended, from_id, to_id, rtr_only, rtr_too),\n            \"extended\": extended\n        }]",
        "rewrite": "def create_filter(extended, from_id, to_id, rtr_only, rtr_too):\n    return [{\n        \"can_id\": Ucan.calculate_acr(extended, from_id, to_id, rtr_only, rtr_too),\n        \"can_mask\": Ucan.calculate_amr(extended, from_id, to_id, rtr_only, rtr_too),\n        \"extended\": extended\n    }]"
    },
    {
        "original": "def add_replace(self, selector, replacement, upsert=False,\n                    collation=None):\n        \"\"\"Create a replace document and add it to the list of ops.\n        \"\"\"\n        validate_ok_for_replace(replacement)\n        cmd = SON([('q', selector), ('u', replacement),\n                   ('multi', False), ('upsert', upsert)])\n        collation = validate_collation_or_none(collation)\n        if collation is not None:\n            self.uses_collation = True\n            cmd['collation'] = collation\n        self.ops.append((_UPDATE, cmd))",
        "rewrite": "def add_replace(self, selector, replacement, upsert=False, collation=None):\n    validate_ok_for_replace(replacement)\n    cmd = SON([('q', selector), ('u', replacement),\n               ('multi', False), ('upsert', upsert])\n    collation = validate_collation_or_none(collation)\n    if collation is not None:\n        self.uses_collation = True\n        cmd['collation'] = collation\n    self.ops.append((_UPDATE, cmd))"
    },
    {
        "original": "def _handle_eor(self, route_family):\n        \"\"\"Currently we only handle EOR for RTC address-family.\n\n        We send non-rtc initial updates if not already sent.\n        \"\"\"\n        LOG.debug('Handling EOR for %s', route_family)\n#         assert (route_family in SUPPORTED_GLOBAL_RF)\n#         assert self.is_mbgp_cap_valid(route_family)\n\n        if route_family == RF_RTC_UC:\n            self._unschedule_sending_init_updates()\n\n            # Learn all rt_nlri at the same time As RT are learned and RT\n            # filter get updated, qualifying NLRIs are automatically sent to\n            # peer including initial update\n            tm = self._core_service.table_manager\n            for rt_nlri in self._init_rtc_nlri_path:\n                tm.learn_path(rt_nlri)\n                # Give chance to process new RT_NLRI so that we have updated RT\n                # filter for all peer including this peer before we communicate\n                # NLRIs for other address-families\n                self.pause(0)\n            # Clear collection of initial RTs as we no longer need to wait for\n            # EOR for RT NLRIs and to indicate that new RT NLRIs should be\n            # handled in a regular fashion\n            self._init_rtc_nlri_path = None",
        "rewrite": "def _handle_eor(self, route_family):\n    LOG.debug('Handling EOR for %s', route_family)\n\n    if route_family == RF_RTC_UC:\n        self._unschedule_sending_init_updates()\n\n        tm = self._core_service.table_manager\n        for rt_nlri in self._init_rtc_nlri_path:\n            tm.learn_path(rt_nlri)\n            self.pause(0)\n\n        self._init_rtc_nlri_path = None"
    },
    {
        "original": "def _download_file_from_drive(filename, url):  # pragma: no cover\n    \"\"\" Download filename from google drive unless it's already in directory.\n\n    Args:\n        filename (str): Name of the file to download to (do nothing if it already exists).\n        url (str): URL to download from.\n    \"\"\"\n    confirm_token = None\n\n    # Since the file is big, drive will scan it for virus and take it to a\n    # warning page. We find the confirm token on this page and append it to the\n    # URL to start the download process.\n    confirm_token = None\n    session = requests.Session()\n    response = session.get(url, stream=True)\n    for k, v in response.cookies.items():\n        if k.startswith(\"download_warning\"):\n            confirm_token = v\n\n    if confirm_token:\n        url = url + \"&confirm=\" + confirm_token\n\n    logger.info(\"Downloading %s to %s\" % (url, filename))\n\n    response = session.get(url, stream=True)\n    # Now begin the download.\n    chunk_size = 16 * 1024\n    with open(filename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size):\n            if chunk:\n                f.write(chunk)\n\n    # Print newline to clear the carriage return from the download progress\n    statinfo = os.stat(filename)\n    logger.info(\"Successfully downloaded %s, %s bytes.\" % (filename, statinfo.st_size))",
        "rewrite": "import os\nimport requests\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef _download_file_from_drive(filename, url):  # pragma: no cover\n    confirm_token = None\n\n    session = requests.Session()\n    response = session.get(url, stream=True)\n\n    for k, v in response.cookies.items():\n        if k.startswith(\"download_warning\"):\n            confirm_token = v\n\n    if confirm_token:\n        url = url + \"&confirm=\" + confirm_token\n\n    logger.info(f\"Downloading {url} to {filename}\")\n\n    response = session.get(url, stream=True)\n    chunk_size = 16 * 1024\n\n    with open(filename, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size):\n            if chunk:\n                f.write(chunk)\n\n    statinfo = os.stat(filename)\n    logger.info(f\"Successfully downloaded {filename}, {statinfo.st_size} bytes.\")"
    },
    {
        "original": "def get_asset_balance(self, asset, **params):\n        \"\"\"Get current asset balance.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#account-information-user_data\n\n        :param asset: required\n        :type asset: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: dictionary or None if not found\n\n        .. code-block:: python\n\n            {\n                \"asset\": \"BTC\",\n                \"free\": \"4723846.89208129\",\n                \"locked\": \"0.00000000\"\n            }\n\n        :raises: BinanceRequestException, BinanceAPIException\n\n        \"\"\"\n        res = self.get_account(**params)\n        # find asset balance in list of balances\n        if \"balances\" in res:\n            for bal in res['balances']:\n                if bal['asset'].lower() == asset.lower():\n                    return bal\n        return None",
        "rewrite": "def get_asset_balance(self, asset, **params):\n    res = self.get_account(**params)\n    if \"balances\" in res:\n        for bal in res['balances']:\n            if bal['asset'].lower() == asset.lower():\n                return bal\n    return None"
    },
    {
        "original": "def from_stream(cls, stream):\n        \"\"\"\n        Return |Gif| instance having header properties parsed from GIF image\n        in *stream*.\n        \"\"\"\n        px_width, px_height = cls._dimensions_from_stream(stream)\n        return cls(px_width, px_height, 72, 72)",
        "rewrite": "def from_stream(cls, stream):\n        px_width, px_height = cls._dimensions_from_stream(stream)\n        return cls(px_width, px_height, 72, 72)"
    },
    {
        "original": "def get_maps():\n  \"\"\"Get the full dict of maps {map_name: map_class}.\"\"\"\n  maps = {}\n  for mp in Map.all_subclasses():\n    if mp.filename:\n      map_name = mp.__name__\n      if map_name in maps:\n        raise DuplicateMapException(\"Duplicate map found: \" + map_name)\n      maps[map_name] = mp\n  return maps",
        "rewrite": "def get_maps():\n    maps = {}\n    for mp in Map.all_subclasses():\n        if mp.filename:\n            map_name = mp.__name__\n            if map_name in maps:\n                raise DuplicateMapException(\"Duplicate map found: \" + map_name)\n            maps[map_name] = mp\n    return maps"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'model_id') and self.model_id is not None:\n            _dict['model_id'] = self.model_id\n        if hasattr(self, 'model_version') and self.model_version is not None:\n            _dict['model_version'] = self.model_version\n        if hasattr(self, 'documents') and self.documents is not None:\n            _dict['documents'] = [x._to_dict() for x in self.documents]\n        if hasattr(self,\n                   'aligned_elements') and self.aligned_elements is not None:\n            _dict['aligned_elements'] = [\n                x._to_dict() for x in self.aligned_elements\n            ]\n        if hasattr(\n                self,\n                'unaligned_elements') and self.unaligned_elements is not None:\n            _dict['unaligned_elements'] = [\n                x._to_dict() for x in self.unaligned_elements\n            ]\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'model_id') and self.model_id is not None:\n        _dict['model_id'] = self.model_id\n    if hasattr(self, 'model_version') and self.model_version is not None:\n        _dict['model_version'] = self.model_version\n    if hasattr(self, 'documents') and self.documents is not None:\n        _dict['documents'] = [x._to_dict() for x in self.documents]\n    if hasattr(self, 'aligned_elements') and self.aligned_elements is not None:\n        _dict['aligned_elements'] = [x._to_dict() for x in self.aligned_elements]\n    if hasattr(self, 'unaligned_elements') and self.unaligned_elements is not None:\n        _dict['unaligned_elements'] = [x._to_dict() for x in self.unaligned_elements]\n    return _dict"
    },
    {
        "original": "def dispatch_hook(cls, _pkt, _underlayer=None, *args, **kargs):\n        \"\"\"dispatch_hook to choose among different registered payloads\"\"\"\n        for klass in cls._payload_class:\n            if hasattr(klass, \"can_handle\") and \\\n                    klass.can_handle(_pkt, _underlayer):\n                return klass\n        print(\"DCE/RPC payload class not found or undefined (using Raw)\")\n        return Raw",
        "rewrite": "def dispatch_hook(cls, _pkt, _underlayer=None, *args, **kargs):\n    for klass in cls._payload_class:\n        if hasattr(klass, \"can_handle\") and klass.can_handle(_pkt, _underlayer):\n            return klass\n    print(\"DCE/RPC payload class not found or undefined (using Raw)\")\n    return Raw"
    },
    {
        "original": "def _parse_dict(features, new_names):\r\n        \"\"\"Helping function of `_parse_features` that parses a list.\"\"\"\r\n        feature_collection = OrderedDict()\r\n        for feature_type, feature_names in features.items():\r\n            try:\r\n                feature_type = FeatureType(feature_type)\r\n            except ValueError:\r\n                ValueError('Failed to parse {}, keys of the dictionary have to be instances '\r\n                           'of {}'.format(features, FeatureType.__name__))\r\n\r\n            feature_collection[feature_type] = feature_collection.get(feature_type, OrderedDict())\r\n\r\n            if feature_names is ...:\r\n                feature_collection[feature_type] = ...\r\n\r\n            if feature_type.has_dict() and feature_collection[feature_type] is not ...:\r\n                feature_collection[feature_type].update(FeatureParser._parse_feature_names(feature_names, new_names))\r\n\r\n        return feature_collection",
        "rewrite": "from collections import OrderedDict\n\ndef _parse_dict(features, new_names):\n    feature_collection = OrderedDict()\n    for feature_type, feature_names in features.items():\n        try:\n            feature_type = FeatureType(feature_type)\n        except ValueError:\n            raise ValueError('Failed to parse {}, keys of the dictionary have to be instances '\n                             'of {}'.format(features, FeatureType.__name__))\n\n        feature_collection[feature_type] = feature_collection.get(feature_type, OrderedDict())\n\n        if feature_names is None:\n            feature_collection[feature_type] = None\n\n        if feature_type.has_dict() and feature_collection[feature_type] is not None:\n            feature_collection[feature_type].update(FeatureParser._parse_feature_names(feature_names, new_names))\n\n    return feature_collection"
    },
    {
        "original": "def serialize(data):\n    \"\"\"Serialize a dict into a JSON formatted string.\n\n        This function enforces rules like the separator and order of keys.\n        This ensures that all dicts are serialized in the same way.\n\n        This is specially important for hashing data. We need to make sure that\n        everyone serializes their data in the same way so that we do not have\n        hash mismatches for the same structure due to serialization\n        differences.\n\n        Args:\n            data (dict): dict to serialize\n\n        Returns:\n            str: JSON formatted string\n\n    \"\"\"\n    return rapidjson.dumps(data, skipkeys=False, ensure_ascii=False,\n                           sort_keys=True)",
        "rewrite": "def serialize(data):\n    return json.dumps(data, separators=(',', ':'), sort_keys=True)"
    },
    {
        "original": "def getChatMembersCount(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#getchatmemberscount \"\"\"\n        p = _strip(locals())\n        return self._api_request('getChatMembersCount', _rectify(p))",
        "rewrite": "def get_chat_members_count(self, chat_id):\n    p = _strip(locals())\n    return self._api_request('getChatMembersCount', _rectify(p))"
    },
    {
        "original": "def get_post(id, check_author=True):\n    \"\"\"Get a post and its author by id.\n\n    Checks that the id exists and optionally that the current user is\n    the author.\n\n    :param id: id of post to get\n    :param check_author: require the current user to be the author\n    :return: the post with author information\n    :raise 404: if a post with the given id doesn't exist\n    :raise 403: if the current user isn't the author\n    \"\"\"\n    post = Post.query.get_or_404(id, f\"Post id {id} doesn't exist.\")\n\n    if check_author and post.author != g.user:\n        abort(403)\n\n    return post",
        "rewrite": "def get_post(id, check_author=True):\n    post = Post.query.get_or_404(id, f\"Post id {id} doesn't exist.\")\n    \n    if check_author and post.author != g.user:\n        abort(403)\n    \n    return post"
    },
    {
        "original": "def FromPath(cls, path, follow_symlink = True):\n    \"\"\"Returns stat information about the given OS path, calling os.[l]stat.\n\n    Args:\n      path: A path to perform `stat` on.\n      follow_symlink: True if `stat` of a symlink should be returned instead of\n        a file that it points to. For non-symlinks this setting has no effect.\n\n    Returns:\n      Stat instance, with information about the given path.\n    \"\"\"\n    # Note that we do not add type assertion for `path` here. The reason is that\n    # many of the existing system calls (e.g. `os.listdir`) return results as\n    # bytestrings in Python 2. This is fine because it also means that they also\n    # accept bytestring paths as arguments in Python 2 (e.g. `os.stat`). Having\n    # consistent types in both versions is certainly desired but it might be too\n    # much work for too little benefit.\n    precondition.AssertType(follow_symlink, bool)\n\n    if follow_symlink:\n      stat_obj = os.stat(path)\n    else:\n      stat_obj = os.lstat(path)\n\n    return cls(path=path, stat_obj=stat_obj)",
        "rewrite": "def FromPath(cls, path, follow_symlink=True):\n    precondition.AssertType(follow_symlink, bool)\n    \n    if follow_symlink:\n        stat_obj = os.stat(path)\n    else:\n        stat_obj = os.lstat(path)\n    \n    return cls(path=path, stat_obj=stat_obj)"
    },
    {
        "original": "def UploadFilePath(self, filepath, offset=0, amount=None):\n    \"\"\"Uploads chunks of a file on a given path to the transfer store flow.\n\n    Args:\n      filepath: A path to the file to upload.\n      offset: An integer offset at which the file upload should start on.\n      amount: An upper bound on number of bytes to stream. If it is `None` then\n        the whole file is uploaded.\n\n    Returns:\n      A `BlobImageDescriptor` object.\n    \"\"\"\n    return self._UploadChunkStream(\n        self._streamer.StreamFilePath(filepath, offset=offset, amount=amount))",
        "rewrite": "def upload_file_path(self, filepath, offset=0, amount=None):\n    return self._upload_chunk_stream(\n        self._streamer.stream_file_path(filepath, offset=offset, amount=amount))"
    },
    {
        "original": "def DIV(classical_reg, right):\n    \"\"\"\n    Produce an DIV instruction.\n\n    :param classical_reg: Left operand for the arithmetic operation. Also serves as the store target.\n    :param right: Right operand for the arithmetic operation.\n    :return: A ClassicalDiv instance.\n    \"\"\"\n    left, right = unpack_reg_val_pair(classical_reg, right)\n    return ClassicalDiv(left, right)",
        "rewrite": "def DIV(classical_reg, right):\n    left, right = unpack_reg_val_pair(classical_reg, right)\n    return ClassicalDiv(left, right)"
    },
    {
        "original": "def _build(self, ids):\n    \"\"\"Lookup embeddings.\n\n    Looks up an embedding vector for each value in `ids`. All ids must be within\n    [0, vocab_size), else an `InvalidArgumentError` is raised at runtime.\n\n    Args:\n      ids: Tensor of dtype int64.\n\n    Returns:\n      Tensor of tf.shape(ids) + [embedding_dim] and dtype float32.\n    \"\"\"\n    # Construct embeddings.\n    if self._existing_vocab is None:\n      if self.EMBEDDINGS not in self._initializers:\n        self._initializers[self.EMBEDDINGS] = tf.initializers.random_normal()\n      self._embeddings = tf.get_variable(\n          \"embeddings\",\n          shape=[self._vocab_size, self._embed_dim],\n          dtype=tf.float32,\n          initializer=self._initializers[self.EMBEDDINGS],\n          partitioner=self._partitioners.get(self.EMBEDDINGS, None),\n          regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n          trainable=self._trainable)\n    else:\n      self._embeddings = tf.get_variable(\n          \"embeddings\",\n          dtype=tf.float32,\n          initializer=self._existing_vocab,\n          regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n          trainable=self._trainable)\n\n    if self._densify_gradients:\n      # On the backwards pass, we convert the gradient from indexed-slices to a\n      # regular tensor before sending it back to the parameter server.\n      # This avoids excess computation on the parameter server.\n      # In eager mode we do not need the conversion.\n      # Add a check whether we are in eager mode when it is supported.\n      embeddings = util.convert_gradient_to_tensor(self._embeddings)\n    else:\n      embeddings = self._embeddings\n\n    # Lookup embeddings\n    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
        "rewrite": "def _build(self, ids):\n    if self._existing_vocab is None:\n        if self.EMBEDDINGS not in self._initializers:\n            self._initializers[self.EMBEDDINGS] = tf.initializers.random_normal()\n        self._embeddings = tf.get_variable(\n            \"embeddings\",\n            shape=[self._vocab_size, self._embed_dim],\n            dtype=tf.float32,\n            initializer=self._initializers[self.EMBEDDINGS],\n            partitioner=self._partitioners.get(self.EMBEDDINGS, None),\n            regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n            trainable=self._trainable)\n    else:\n        self._embeddings = tf.get_variable(\n            \"embeddings\",\n            dtype=tf.float32,\n            initializer=self._existing_vocab,\n            regularizer=self._regularizers.get(self.EMBEDDINGS, None),\n            trainable=self._trainable)\n\n    if self._densify_gradients:\n        embeddings = util.convert_gradient_to_tensor(self._embeddings)\n    else:\n        embeddings = self._embeddings\n\n    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")"
    },
    {
        "original": "def player_stats(game_id):\n    \"\"\"Return dictionary of player stats for game matching the game id.\"\"\"\n    # get information for that game\n    data = mlbgame.stats.player_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, True)",
        "rewrite": "def player_stats(game_id):\n    data = mlbgame.stats.player_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, True)"
    },
    {
        "original": "def _analyze_all_function_features(self, all_funcs_completed=False):\n        \"\"\"\n        Iteratively analyze all changed functions, update their returning attribute, until a fix-point is reached (i.e.\n        no new returning/not-returning functions are found).\n\n        :return: None\n        \"\"\"\n\n        while True:\n            new_changes = self._iteratively_analyze_function_features(all_funcs_completed=all_funcs_completed)\n            new_returning_functions = new_changes['functions_return']\n            new_not_returning_functions = new_changes['functions_do_not_return']\n\n            if not new_returning_functions and not new_not_returning_functions:\n                break\n\n            for returning_function in new_returning_functions:\n                self._pending_jobs.add_returning_function(returning_function.addr)\n                if returning_function.addr in self._function_returns:\n                    for fr in self._function_returns[returning_function.addr]:\n                        # Confirm them all\n                        if not self.kb.functions.contains_addr(fr.caller_func_addr):\n                            # FIXME: A potential bug might arise here. After post processing (phase 2), if the function\n                            # specified by fr.caller_func_addr has been merged to another function during phase 2, we\n                            # will simply skip this FunctionReturn here. It might lead to unconfirmed fake_ret edges\n                            # in the newly merged function. Fix this bug in the future when it becomes an issue.\n                            continue\n\n                        if self.kb.functions.get_by_addr(fr.caller_func_addr).returning is not True:\n                            self._updated_nonreturning_functions.add(fr.caller_func_addr)\n\n                        return_to_node = self._nodes.get(fr.return_to, None)\n                        if return_to_node is None:\n                            return_to_snippet = self._to_snippet(addr=fr.return_to, base_state=self._base_state)\n                        else:\n                            return_to_snippet = self._to_snippet(cfg_node=self._nodes[fr.return_to])\n\n                        self.kb.functions._add_return_from_call(fr.caller_func_addr, fr.callee_func_addr,\n                                                                return_to_snippet)\n\n                    del self._function_returns[returning_function.addr]\n\n            for nonreturning_function in new_not_returning_functions:\n                self._pending_jobs.add_nonreturning_function(nonreturning_function.addr)\n                if nonreturning_function.addr in self._function_returns:\n                    for fr in self._function_returns[nonreturning_function.addr]:\n                        # Remove all those FakeRet edges\n                        if self.kb.functions.contains_addr(fr.caller_func_addr) and \\\n                                self.kb.functions.get_by_addr(fr.caller_func_addr).returning is not True:\n                            self._updated_nonreturning_functions.add(fr.caller_func_addr)\n\n                    del self._function_returns[nonreturning_function.addr]",
        "rewrite": "def _analyze_all_function_features(self, all_funcs_completed=False):\n        while True:\n            new_changes = self._iteratively_analyze_function_features(all_funcs_completed=all_funcs_completed)\n            new_returning_functions = new_changes['functions_return']\n            new_not_returning_functions = new_changes['functions_do_not_return']\n\n            if not new_returning_functions and not new_not_returning_functions:\n                break\n\n            for returning_function in new_returning_functions:\n                self._pending_jobs.add_returning_function(returning_function.addr)\n                if returning_function.addr in self._function_returns:\n                    for fr in self._function_returns[returning_function.addr]:\n                        if not self.kb.functions.contains_addr(fr.caller_func_addr):\n                            continue\n\n                        if self.kb.functions.get_by_addr(fr.caller_func_addr).returning is not True:\n                            self._updated_nonreturning_functions.add(fr.caller_func_addr)\n\n                        return_to_node = self._nodes.get(fr.return_to, None)\n                        if return_to_node is None:\n                            return_to_snippet = self._to_snippet(addr=fr.return_to, base_state=self._base_state)\n                        else:\n                            return_to_snippet = self._to_snippet(cfg_node=self._nodes[fr.return_to])\n\n                        self.kb.functions._add_return_from_call(fr.caller_func_addr, fr.callee_func_addr,\n                                                                return_to_snippet)\n\n                    del self._function_returns[returning_function.addr]\n\n            for nonreturning_function in new_not_returning_functions:\n                self._pending_jobs.add_nonreturning_function(nonreturning_function.addr)\n                if nonreturning_function.addr in self._function_returns:\n                    for fr in self._function_returns[nonreturning_function.addr]:\n                        if self.kb.functions.contains_addr(fr.caller_func_addr) and \\\n                                self.kb.functions.get_by_addr(fr.caller_func_addr).returning is not True:\n                            self._updated_nonreturning_functions.add(fr.caller_func_addr)\n\n                    del self._function_returns[nonreturning_function.addr]"
    },
    {
        "original": "def resolve_dep_from_path(self, depname):\n        \"\"\" If we can find the dep in the PATH, then we consider it to\n        be a system dependency that we should not bundle in the package \"\"\"\n        if is_system_dep(depname):\n            return True\n\n        for d in self._path:\n            name = os.path.join(d, depname)\n            if os.path.exists(name):\n                return True\n\n        return False",
        "rewrite": "def resolve_dep_from_path(self, depname):\n    if is_system_dep(depname):\n        return True\n\n    for d in self._path:\n        name = os.path.join(d, depname)\n        if os.path.exists(name):\n            return True\n    \n    return False"
    },
    {
        "original": "def __make_table(yfmtfunc, fmtfunc, endline, data, fxyz, sortx=None, sorty=None, seplinefunc=None):  # noqa: E501\n    \"\"\"Core function of the make_table suite, which generates the table\"\"\"\n    vx = {}\n    vy = {}\n    vz = {}\n    vxf = {}\n\n    # Python 2 backward compatibility\n    fxyz = lambda_tuple_converter(fxyz)\n\n    tmp_len = 0\n    for e in data:\n        xx, yy, zz = [str(s) for s in fxyz(*e)]\n        tmp_len = max(len(yy), tmp_len)\n        vx[xx] = max(vx.get(xx, 0), len(xx), len(zz))\n        vy[yy] = None\n        vz[(xx, yy)] = zz\n\n    vxk = list(vx)\n    vyk = list(vy)\n    if sortx:\n        vxk.sort(key=sortx)\n    else:\n        try:\n            vxk.sort(key=int)\n        except Exception:\n            try:\n                vxk.sort(key=atol)\n            except Exception:\n                vxk.sort()\n    if sorty:\n        vyk.sort(key=sorty)\n    else:\n        try:\n            vyk.sort(key=int)\n        except Exception:\n            try:\n                vyk.sort(key=atol)\n            except Exception:\n                vyk.sort()\n\n    if seplinefunc:\n        sepline = seplinefunc(tmp_len, [vx[x] for x in vxk])\n        print(sepline)\n\n    fmt = yfmtfunc(tmp_len)\n    print(fmt % \"\", end=' ')\n    for x in vxk:\n        vxf[x] = fmtfunc(vx[x])\n        print(vxf[x] % x, end=' ')\n    print(endline)\n    if seplinefunc:\n        print(sepline)\n    for y in vyk:\n        print(fmt % y, end=' ')\n        for x in vxk:\n            print(vxf[x] % vz.get((x, y), \"-\"), end=' ')\n        print(endline)\n    if seplinefunc:\n        print(sepline)",
        "rewrite": "def make_table(yfmtfunc, fmtfunc, endline, data, fxyz, sortx=None, sorty=None, seplinefunc=None): \n    \"\"\"Core function of the make_table suite, which generates the table\"\"\"\n    vx = {}\n    vy = {}\n    vz = {}\n    vxf = {}\n\n    fxyz = lambda_tuple_converter(fxyz)\n\n    tmp_len = 0\n    for e in data:\n        xx, yy, zz = [str(s) for s in fxyz(*e)]\n        tmp_len = max(len(yy), tmp_len)\n        vx[xx] = max(vx.get(xx, 0), len(xx), len(zz))\n        vy[yy] = None\n        vz[(xx, yy)] = zz\n\n    vxk = list(vx)\n    vyk = list(vy)\n    if sortx:\n        vxk.sort(key=sortx)\n    else:\n        try:\n            vxk.sort(key=int)\n        except Exception:\n            try:\n                vxk.sort(key=atol)\n            except Exception:\n                vxk.sort()\n    if sorty:\n        vyk.sort(key=sorty)\n    else:\n        try:\n            vyk.sort(key=int)\n        except Exception:\n            try:\n                vyk.sort(key=atol)\n            except Exception:\n                vyk.sort()\n\n    if seplinefunc:\n        sepline = seplinefunc(tmp_len, [vx[x] for x in vxk])\n        print(sepline)\n\n    fmt = yfmtfunc(tmp_len)\n    print(fmt % \"\", end=' ')\n    for x in vxk:\n        vxf[x] = fmtfunc(vx[x])\n        print(vxf[x] % x, end=' ')\n    print(endline)\n    if seplinefunc:\n        print(sepline)\n    for y in vyk:\n        print(fmt % y, end=' ')\n        for x in vxk:\n            print(vxf[x] % vz.get((x, y), \"-\"), end=' ')\n        print(endline)\n    if seplinefunc:\n        print(sepline)"
    },
    {
        "original": "def has_metadata(self, name):\n        \"\"\"\n        Check if a function has either an implementation or any metadata associated with it\n\n        :param name:    The name of the function as a string\n        :return:        A bool indicating if anything is known about the function\n        \"\"\"\n        return self.has_implementation(name) or \\\n            name in self.non_returning or \\\n            name in self.prototypes",
        "rewrite": "def has_metadata(self, name):\n    return self.has_implementation(name) or \\\n           name in self.non_returning or \\\n           name in self.prototypes"
    },
    {
        "original": "def makesvg(self, right_text, status=None, left_text=None,\n                left_color=None, config=None):\n        \"\"\"Renders an SVG from the template, using the specified data\n        \"\"\"\n        right_color = config['color_scheme'].get(status, \"#9f9f9f\")  # Grey\n\n        left_text = left_text or config['left_text']\n        left_color = left_color or config['left_color']\n\n        left = {\n            \"color\": left_color,\n            \"text\": left_text,\n            \"width\": self.textwidth(left_text, config)\n        }\n        right = {\n            \"color\": right_color,\n            \"text\": right_text,\n            \"width\": self.textwidth(right_text, config)\n        }\n\n        template = self.env.get_template(config['template_name'].format(**config))\n        return template.render(left=left, right=right, config=config)",
        "rewrite": "def makesvg(self, right_text, status=None, left_text=None, left_color=None, config=None):\n    right_color = config['color_scheme'].get(status, \"#9f9f9f\")  \n\n    left_text = left_text or config['left_text']\n    left_color = left_color or config['left_color']\n\n    left = {\n        \"color\": left_color,\n        \"text\": left_text,\n        \"width\": self.textwidth(left_text, config)\n    }\n    right = {\n        \"color\": right_color,\n        \"text\": right_text,\n        \"width\": self.textwidth(right_text, config)\n    }\n\n    template = self.env.get_template(config['template_name'].format(**config))\n    return template.render(left=left, right=right, config=config)"
    },
    {
        "original": "def display(obj, raw_output=False, **kwargs):\n    \"\"\"\n    Renders any HoloViews object to HTML and displays it\n    using the IPython display function. If raw is enabled\n    the raw HTML is returned instead of displaying it directly.\n    \"\"\"\n    if not Store.loaded_backends() and isinstance(obj, Dimensioned):\n        raise RuntimeError('To use display on a HoloViews object ensure '\n                           'a backend is loaded using the holoviews '\n                           'extension.')\n\n    raw = True\n    if isinstance(obj, GridSpace):\n        with option_state(obj):\n            output = grid_display(obj)\n    elif isinstance(obj, (CompositeOverlay, ViewableElement)):\n        with option_state(obj):\n            output = element_display(obj)\n    elif isinstance(obj, (Layout, NdLayout, AdjointLayout)):\n        with option_state(obj):\n            output = layout_display(obj)\n    elif isinstance(obj, (HoloMap, DynamicMap)):\n        with option_state(obj):\n            output = map_display(obj)\n    elif isinstance(obj, Plot):\n        output = render(obj)\n    else:\n        output = obj\n        raw = kwargs.pop('raw', False)\n\n    if raw_output:\n        return output\n    elif isinstance(output, tuple):\n        data, metadata = output\n    else:\n        data, metadata = output, {}\n    return IPython.display.display(data, raw=raw, metadata=metadata, **kwargs)",
        "rewrite": "def display(obj, raw_output=False, **kwargs):\n    if not Store.loaded_backends() and isinstance(obj, Dimensioned):\n        raise RuntimeError('To use display on a HoloViews object ensure '\n                           'a backend is loaded using the holoviews '\n                           'extension.')\n\n    raw = True\n    if isinstance(obj, GridSpace):\n        with option_state(obj):\n            output = grid_display(obj)\n    elif isinstance(obj, (CompositeOverlay, ViewableElement)):\n        with option_state(obj):\n            output = element_display(obj)\n    elif isinstance(obj, (Layout, NdLayout, AdjointLayout)):\n        with option_state(obj):\n            output = layout_display(obj)\n    elif isinstance(obj, (HoloMap, DynamicMap)):\n        with option_state(obj):\n            output = map_display(obj)\n    elif isinstance(obj, Plot):\n        output = render(obj)\n    else:\n        output = obj\n        raw = kwargs.pop('raw', False)\n\n    if raw_output:\n        return output\n    elif isinstance(output, tuple):\n        data, metadata = output\n    else:\n        data, metadata = output, {}\n    return IPython.display.display(data, raw=raw, metadata=metadata, **kwargs)"
    },
    {
        "original": "def as_command(self):\n        \"\"\"Return a find command document for this query.\n\n        Should be called *after* get_message.\n        \"\"\"\n        if '$explain' in self.spec:\n            self.name = 'explain'\n            return _gen_explain_command(\n                self.coll, self.spec, self.fields, self.ntoskip,\n                self.limit, self.batch_size, self.flags,\n                self.read_concern), self.db\n        return _gen_find_command(\n            self.coll, self.spec, self.fields, self.ntoskip, self.limit,\n            self.batch_size, self.flags, self.read_concern,\n            self.collation), self.db",
        "rewrite": "def as_command(self):\n    if '$explain' in self.spec:\n        self.name = 'explain'\n        return _gen_explain_command(\n            self.coll, self.spec, self.fields, self.ntoskip,\n            self.limit, self.batch_size, self.flags,\n            self.read_concern), self.db\n    return _gen_find_command(\n        self.coll, self.spec, self.fields, self.ntoskip, self.limit,\n        self.batch_size, self.flags, self.read_concern,\n        self.collation), self.db"
    },
    {
        "original": "def from_stream(cls, stream, marker_code, offset):\n        \"\"\"\n        Extract the horizontal and vertical dots-per-inch value from the APP1\n        header at *offset* in *stream*.\n        \"\"\"\n        # field                 off  len  type   notes\n        # --------------------  ---  ---  -----  ----------------------------\n        # segment length         0    2   short\n        # Exif identifier        2    6   6 chr  'Exif\\x00\\x00'\n        # TIFF byte order        8    2   2 chr  'II'=little 'MM'=big endian\n        # meaning of universe   10    2   2 chr  '*\\x00' or '\\x00*' depending\n        # IFD0 off fr/II or MM  10   16   long   relative to ...?\n        # --------------------  ---  ---  -----  ----------------------------\n        segment_length = stream.read_short(offset)\n        if cls._is_non_Exif_APP1_segment(stream, offset):\n            return cls(marker_code, offset, segment_length, 72, 72)\n        tiff = cls._tiff_from_exif_segment(stream, offset, segment_length)\n        return cls(\n            marker_code, offset, segment_length, tiff.horz_dpi, tiff.vert_dpi\n        )",
        "rewrite": "def from_stream(cls, stream, marker_code, offset):\n    segment_length = stream.read_short(offset)\n    if cls._is_non_Exif_APP1_segment(stream, offset):\n        return cls(marker_code, offset, segment_length, 72, 72)\n    tiff = cls._tiff_from_exif_segment(stream, offset, segment_length)\n    return cls(\n        marker_code, offset, segment_length, tiff.horz_dpi, tiff.vert_dpi\n    )"
    },
    {
        "original": "def call_parallel(self, cdata, low):\n        \"\"\"\n        Call the state defined in the given cdata in parallel\n        \"\"\"\n        # There are a number of possibilities to not have the cdata\n        # populated with what we might have expected, so just be smart\n        # enough to not raise another KeyError as the name is easily\n        # guessable and fallback in all cases to present the real\n        # exception to the user\n        name = (cdata.get('args') or [None])[0] or cdata['kwargs'].get('name')\n        if not name:\n            name = low.get('name', low.get('__id__'))\n\n        proc = salt.utils.process.MultiprocessingProcess(\n                target=self._call_parallel_target,\n                args=(name, cdata, low))\n        proc.start()\n        ret = {'name': name,\n                'result': None,\n                'changes': {},\n                'comment': 'Started in a separate process',\n                'proc': proc}\n        return ret",
        "rewrite": "def call_parallel(self, cdata, low):\n    name = (cdata.get('args') or [None])[0] or cdata['kwargs'].get('name') or low.get('name', low.get('__id__'))\n    \n    proc = salt.utils.process.MultiprocessingProcess(\n        target=self._call_parallel_target,\n        args=(name, cdata, low))\n    proc.start()\n    \n    ret = {'name': name,\n           'result': None,\n           'changes': {},\n           'comment': 'Started in a separate process',\n           'proc': proc}\n    \n    return ret"
    },
    {
        "original": "def AddChild(self, path_info):\n    \"\"\"Makes the path aware of some child.\"\"\"\n\n    if self._path_type != path_info.path_type:\n      message = \"Incompatible path types: `%s` and `%s`\"\n      raise ValueError(message % (self._path_type, path_info.path_type))\n    if self._components != path_info.components[:-1]:\n      message = \"Incompatible path components, expected `%s` but got `%s`\"\n      raise ValueError(message % (self._components, path_info.components[:-1]))\n\n    self._children.add(path_info.GetPathID())",
        "rewrite": "def AddChild(self, path_info):\n    if self._path_type != path_info.path_type:\n        raise ValueError(\"Incompatible path types: `%s` and `%s`\" % (self._path_type, path_info.path_type))\n        \n    if self._components != path_info.components[:-1]:\n        raise ValueError(\"Incompatible path components, expected `%s` but got `%s`\" % (self._components, path_info.components[:-1]))\n\n    self._children.add(path_info.GetPathID())"
    },
    {
        "original": "def create_adex(self, log, dexobj):\n        \"\"\"\n        This method is called in order to create an Analysis object\n\n        :param log: an object which corresponds to a unique app\n        :param androguard.core.bytecodes.dvm.DalvikVMFormat dexobj: a :class:`DalvikVMFormat` object\n\n        :rytpe: a :class:`~androguard.core.analysis.analysis.Analysis` object\n        \"\"\"\n        vm_analysis = analysis.Analysis(dexobj)\n        vm_analysis.create_xref()\n        return vm_analysis",
        "rewrite": "def create_adex(self, log, dexobj):\n        \"\"\"\n        This method is called to create an Analysis object\n        \n        :param log: a unique app object\n        :param androguard.core.bytecodes.dvm.DalvikVMFormat dexobj: a DalvikVMFormat object\n\n        :rtype: an Analysis object\n        \"\"\"\n        vm_analysis = analysis.Analysis(dexobj)\n        vm_analysis.create_xref()\n        return vm_analysis"
    },
    {
        "original": "def get_average_along_axis(self, ind):\n        \"\"\"\n        Get the averaged total of the volumetric data a certain axis direction.\n        For example, useful for visualizing Hartree Potentials from a LOCPOT\n        file.\n\n        Args:\n            ind (int): Index of axis.\n\n        Returns:\n            Average total along axis\n        \"\"\"\n        m = self.data[\"total\"]\n        ng = self.dim\n        if ind == 0:\n            total = np.sum(np.sum(m, axis=1), 1)\n        elif ind == 1:\n            total = np.sum(np.sum(m, axis=0), 1)\n        else:\n            total = np.sum(np.sum(m, axis=0), 0)\n        return total / ng[(ind + 1) % 3] / ng[(ind + 2) % 3]",
        "rewrite": "def get_average_along_axis(self, ind):\n    m = self.data[\"total\"]\n    ng = self.dim\n    if ind == 0:\n        total = np.sum(np.sum(m, axis=1), 1)\n    elif ind == 1:\n        total = np.sum(np.sum(m, axis=0), 1)\n    else:\n        total = np.sum(np.sum(m, axis=0), 0)\n    return total / ng[(ind + 1) % 3] / ng[(ind + 2) % 3]"
    },
    {
        "original": "def paste(self):\n        \"\"\"Create a paste and return the paste id.\"\"\"\n        data = json.dumps(\n            {\n                \"description\": \"Werkzeug Internal Server Error\",\n                \"public\": False,\n                \"files\": {\"traceback.txt\": {\"content\": self.plaintext}},\n            }\n        ).encode(\"utf-8\")\n        try:\n            from urllib2 import urlopen\n        except ImportError:\n            from urllib.request import urlopen\n        rv = urlopen(\"https://api.github.com/gists\", data=data)\n        resp = json.loads(rv.read().decode(\"utf-8\"))\n        rv.close()\n        return {\"url\": resp[\"html_url\"], \"id\": resp[\"id\"]}",
        "rewrite": "def paste(self):\n    data = json.dumps({\n        \"description\": \"Werkzeug Internal Server Error\",\n        \"public\": False,\n        \"files\": {\"traceback.txt\": {\"content\": self.plaintext}},\n    }).encode(\"utf-8\")\n    \n    try:\n        from urllib2 import urlopen\n    except ImportError:\n        from urllib.request import urlopen\n    \n    rv = urlopen(\"https://api.github.com/gists\", data=data)\n    resp = json.loads(rv.read().decode(\"utf-8\"))\n    rv.close()\n    \n    return {\"url\": resp[\"html_url\"], \"id\": resp[\"id\"]}"
    },
    {
        "original": "def pillar_refresh(self, force_refresh=False, notify=False):\n        \"\"\"\n        Refresh the pillar\n        \"\"\"\n        if self.connected:\n            log.debug('Refreshing pillar. Notify: %s', notify)\n            async_pillar = salt.pillar.get_async_pillar(\n                self.opts,\n                self.opts['grains'],\n                self.opts['id'],\n                self.opts['saltenv'],\n                pillarenv=self.opts.get('pillarenv'),\n            )\n            try:\n                self.opts['pillar'] = yield async_pillar.compile_pillar()\n                if notify:\n                    evt = salt.utils.event.get_event('minion', opts=self.opts, listen=False)\n                    evt.fire_event({'complete': True}, tag=salt.defaults.events.MINION_PILLAR_COMPLETE)\n            except SaltClientError:\n                # Do not exit if a pillar refresh fails.\n                log.error('Pillar data could not be refreshed. '\n                          'One or more masters may be down!')\n            finally:\n                async_pillar.destroy()\n        self.module_refresh(force_refresh, notify)\n        self.matchers_refresh()\n        self.beacons_refresh()",
        "rewrite": "def pillar_refresh(self, force_refresh=False, notify=False):\n    if self.connected:\n        log.debug('Refreshing pillar. Notify: %s', notify)\n        async_pillar = salt.pillar.get_async_pillar(\n            self.opts,\n            self.opts['grains'],\n            self.opts['id'],\n            self.opts['saltenv'],\n            pillarenv=self.opts.get('pillarenv'),\n        )\n        try:\n            self.opts['pillar'] = await async_pillar.compile_pillar()\n            if notify:\n                evt = salt.utils.event.get_event('minion', opts=self.opts, listen=False)\n                evt.fire_event({'complete': True}, tag=salt.defaults.events.MINION_PILLAR_COMPLETE)\n        except SaltClientError:\n            log.error('Pillar data could not be refreshed. One or more masters may be down!')\n        finally:\n            async_pillar.destroy()\n    self.module_refresh(force_refresh, notify)\n    self.matchers_refresh()\n    self.beacons_refresh()"
    },
    {
        "original": "def from_api_response(cls, reddit_session, json_dict):\n        \"\"\"Return an instance of the appropriate class from the json_dict.\"\"\"\n        # The WikiPage response does not contain the necessary information\n        # in the JSON response to determine the name of the page nor the\n        # subreddit it belongs to. Thus we must extract this information\n        # from the request URL.\n        # pylint: disable=W0212\n        parts = reddit_session._request_url.split('/', 6)\n        # pylint: enable=W0212\n        subreddit = parts[4]\n        page = parts[6].split('.', 1)[0]\n        return cls(reddit_session, subreddit, page, json_dict=json_dict)",
        "rewrite": "def from_api_response(cls, reddit_session, json_dict):\n    parts = reddit_session._request_url.split('/', 6)\n    subreddit = parts[4]\n    page = parts[6].split('.', 1)[0]\n    return cls(reddit_session, subreddit, page, json_dict=json_dict)"
    },
    {
        "original": "def UnregisterFlowProcessingHandler(self, timeout=None):\n    \"\"\"Unregisters any registered flow processing handler.\"\"\"\n    self.flow_handler_target = None\n\n    if self.flow_handler_thread:\n      self.flow_handler_stop = True\n      self.flow_handler_thread.join(timeout)\n      if self.flow_handler_thread.isAlive():\n        raise RuntimeError(\"Flow processing handler did not join in time.\")\n      self.flow_handler_thread = None",
        "rewrite": "def unregister_flow_processing_handler(self, timeout=None):\n    self.flow_handler_target = None\n    \n    if self.flow_handler_thread:\n        self.flow_handler_stop = True\n        self.flow_handler_thread.join(timeout)\n        if self.flow_handler_thread.is_alive():\n            raise RuntimeError(\"Flow processing handler did not join in time.\")\n        self.flow_handler_thread = None"
    },
    {
        "original": "def posterior_to_xarray(self):\n        \"\"\"Extract posterior samples from output csv.\"\"\"\n        columns = self.posterior[0].columns\n\n        # filter posterior_predictive and log_likelihood\n        posterior_predictive = self.posterior_predictive\n        if posterior_predictive is None or (\n            isinstance(posterior_predictive, str) and posterior_predictive.lower().endswith(\".csv\")\n        ):\n            posterior_predictive = []\n        elif isinstance(posterior_predictive, str):\n            posterior_predictive = [\n                col for col in columns if posterior_predictive == col.split(\".\")[0]\n            ]\n        else:\n            posterior_predictive = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in posterior_predictive)\n            ]\n\n        log_likelihood = self.log_likelihood\n        if log_likelihood is None:\n            log_likelihood = []\n        else:\n            log_likelihood = [col for col in columns if log_likelihood == col.split(\".\")[0]]\n\n        invalid_cols = posterior_predictive + log_likelihood\n        valid_cols = [col for col in columns if col not in invalid_cols]\n        data = _unpack_dataframes([item[valid_cols] for item in self.posterior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)",
        "rewrite": "def posterior_to_xarray(self):\n    columns = self.posterior[0].columns\n\n    posterior_predictive = self.posterior_predictive\n    if posterior_predictive is None or (isinstance(posterior_predictive, str) and posterior_predictive.lower().endswith(\".csv\")):\n        posterior_predictive = []\n    elif isinstance(posterior_predictive, str):\n        posterior_predictive = [col for col in columns if posterior_predictive == col.split(\".\")[0]]\n    else:\n        posterior_predictive = [col for col in columns if any(item == col.split(\".\")[0] for item in posterior_predictive)]\n\n    log_likelihood = self.log_likelihood\n    if log_likelihood is None:\n        log_likelihood = []\n    else:\n        log_likelihood = [col for col in columns if log_likelihood == col.split(\".\")[0]]\n\n    invalid_cols = posterior_predictive + log_likelihood\n    valid_cols = [col for col in columns if col not in invalid_cols]\n    data = _unpack_dataframes([item[valid_cols] for item in self.posterior])\n    return dict_to_dataset(data, coords=self.coords, dims=self.dims)"
    },
    {
        "original": "def write(self, path):\n        \"\"\"Create a new object with the desired output schema and write it.\"\"\"\n        meta = {\n            'generated_at': self.generated_at,\n            'elapsed_time': self.elapsed_time,\n        }\n        sources = {}\n        for result in self.results:\n            unique_id = result.node.unique_id\n            if result.error is not None:\n                result_dict = {\n                    'error': result.error,\n                    'state': 'runtime error'\n                }\n            else:\n                result_dict = {\n                    'max_loaded_at': result.max_loaded_at,\n                    'snapshotted_at': result.snapshotted_at,\n                    'max_loaded_at_time_ago_in_s': result.age,\n                    'state': result.status,\n                    'criteria': result.node.freshness,\n                }\n            sources[unique_id] = result_dict\n        output = FreshnessRunOutput(meta=meta, sources=sources)\n        output.write(path)",
        "rewrite": "def write(self, path):\n    meta = {\n        'generated_at': self.generated_at,\n        'elapsed_time': self.elapsed_time,\n    }\n    sources = {}\n    for result in self.results:\n        unique_id = result.node.unique_id\n        if result.error is not None:\n            result_dict = {\n                'error': result.error,\n                'state': 'runtime error'\n            }\n        else:\n            result_dict = {\n                'max_loaded_at': result.max_loaded_at,\n                'snapshotted_at': result.snapshotted_at,\n                'max_loaded_at_time_ago_in_s': result.age,\n                'state': result.status,\n                'criteria': result.node.freshness,\n            }\n        sources[unique_id] = result_dict\n    output = FreshnessRunOutput(meta=meta, sources=sources)\n    output.write(path)"
    },
    {
        "original": "def foreach_sentence(layer, drop_factor=1.0):\n    \"\"\"Map a layer across sentences (assumes spaCy-esque .sents interface)\"\"\"\n\n    def sentence_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc.sents if len(sent)]\n            subset = [\n                s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n            ]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, sentence_bwd\n\n    model = wrap(sentence_fwd, layer)\n    return model",
        "rewrite": "def foreach_sentence(layer, drop_factor=1.0):\n\n    def sentence_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc.sents if len(sent)]\n            subset = [s for s in doc_sents if numpy.random.random() >= drop * drop_factor]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, sentence_bwd\n\n    model = wrap(sentence_fwd, layer)\n    return model"
    },
    {
        "original": "def apply_to_structure(self, structure):\n        \"\"\"\n        Apply the deformation gradient to a structure.\n\n        Args:\n            structure (Structure object): the structure object to\n                be modified by the deformation\n        \"\"\"\n        def_struct = structure.copy()\n        old_latt = def_struct.lattice.matrix\n        new_latt = np.transpose(np.dot(self, np.transpose(old_latt)))\n        def_struct.lattice = Lattice(new_latt)\n        return def_struct",
        "rewrite": "def apply_to_structure(self, structure):\n    def_struct = structure.copy()\n    old_latt = def_struct.lattice.matrix\n    new_latt = np.transpose(np.dot(self, np.transpose(old_latt)))\n    def_struct.lattice = Lattice(new_latt)\n    return def_struct"
    },
    {
        "original": "def extract_name_from_job_arn(arn):\n    \"\"\"Returns the name used in the API given a full ARN for a training job\n    or hyperparameter tuning job.\n    \"\"\"\n    slash_pos = arn.find('/')\n    if slash_pos == -1:\n        raise ValueError(\"Cannot parse invalid ARN: %s\" % arn)\n    return arn[(slash_pos + 1):]",
        "rewrite": "def extract_name_from_job_arn(arn):\n    slash_pos = arn.find('/')\n    if slash_pos == -1:\n        raise ValueError(\"Cannot parse invalid ARN: %s\" % arn)\n    return arn[(slash_pos + 1):]"
    },
    {
        "original": "def _update_services(self, ta_results):\n        \"\"\"\n        Given a dict of TrustedAdvisor check results from :py:meth:`~._poll`\n        and a dict of Service objects passed in to :py:meth:`~.update_limits`,\n        updated the TrustedAdvisor limits for all services.\n\n        :param ta_results: results returned by :py:meth:`~._poll`\n        :type ta_results: dict\n        :param services: dict of service names to _AwsService objects\n        :type services: dict\n        \"\"\"\n        logger.debug(\"Updating TA limits on all services\")\n        for svc_name in sorted(ta_results.keys()):\n            svc_results = ta_results[svc_name]\n            if svc_name not in self.ta_services:\n                logger.info(\"TrustedAdvisor returned check results for \"\n                            \"unknown service '%s'\", svc_name)\n                continue\n            svc_limits = self.ta_services[svc_name]\n            for lim_name in sorted(svc_results):\n                if lim_name not in svc_limits:\n                    logger.info(\"TrustedAdvisor returned check results for \"\n                                \"unknown limit '%s' (service %s)\",\n                                lim_name,\n                                svc_name)\n                    continue\n                val = svc_results[lim_name]\n                if val == 'Unlimited':\n                    svc_limits[lim_name]._set_ta_unlimited()\n                else:\n                    svc_limits[lim_name]._set_ta_limit(val)\n        logger.info(\"Done updating TA limits on all services\")",
        "rewrite": "def _update_services(self, ta_results):\n    for svc_name in sorted(ta_results.keys()):\n        svc_results = ta_results[svc_name]\n        if svc_name not in self.ta_services:\n            continue\n        svc_limits = self.ta_services[svc_name]\n        for lim_name in sorted(svc_results):\n            if lim_name not in svc_limits:\n                continue\n            val = svc_results[lim_name]\n            if val == 'Unlimited':\n                svc_limits[lim_name]._set_ta_unlimited()\n            else:\n                svc_limits[lim_name]._set_ta_limit(val)"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'name') and self.name is not None:\n            _dict['name'] = self.name\n        if hasattr(self, 'classifier_id') and self.classifier_id is not None:\n            _dict['classifier_id'] = self.classifier_id\n        if hasattr(self, 'classes') and self.classes is not None:\n            _dict['classes'] = [x._to_dict() for x in self.classes]\n        return _dict",
        "rewrite": "def _to_dict(self):\n        _dict = {}\n        if hasattr(self, 'name') and self.name is not None:\n            _dict['name'] = self.name\n        if hasattr(self, 'classifier_id') and self.classifier_id is not None:\n            _dict['classifier_id'] = self.classifier_id\n        if hasattr(self, 'classes') and self.classes is not None:\n            _dict['classes'] = [x._to_dict() for x in self.classes]\n        return _dict"
    },
    {
        "original": "def publish_server_closed(self, server_address, topology_id):\n        \"\"\"Publish a ServerClosedEvent to all server listeners.\n\n        :Parameters:\n         - `server_address`: The address (host/port pair) of the server.\n         - `topology_id`: A unique identifier for the topology this server\n           is a part of.\n        \"\"\"\n        event = ServerClosedEvent(server_address, topology_id)\n        for subscriber in self.__server_listeners:\n            try:\n                subscriber.closed(event)\n            except Exception:\n                _handle_exception()",
        "rewrite": "def publish_server_closed(self, server_address, topology_id):\n    event = ServerClosedEvent(server_address, topology_id)\n    for subscriber in self.__server_listeners:\n        try:\n            subscriber.closed(event)\n        except Exception:\n            _handle_exception()"
    },
    {
        "original": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n            knowledge_base):\n    \"\"\"Parse the system profiler output. We get it in the form of a plist.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused\n    self.CheckReturn(cmd, return_val)\n\n    plist = biplist.readPlist(io.BytesIO(stdout))\n\n    if len(plist) > 1:\n      raise parser.ParseError(\"SPHardwareDataType plist has too many items.\")\n\n    hardware_list = plist[0][\"_items\"][0]\n    serial_number = hardware_list.get(\"serial_number\", None)\n    system_product_name = hardware_list.get(\"machine_model\", None)\n    bios_version = hardware_list.get(\"boot_rom_version\", None)\n\n    yield rdf_client.HardwareInfo(\n        serial_number=serial_number,\n        bios_version=bios_version,\n        system_product_name=system_product_name)",
        "rewrite": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n                   knowledge_base):\n    self.CheckReturn(cmd, return_val)\n\n    plist = biplist.readPlist(io.BytesIO(stdout))\n\n    if len(plist) > 1:\n        raise parser.ParseError(\"SPHardwareDataType plist has too many items.\")\n\n    hardware_list = plist[0][\"_items\"][0]\n    serial_number = hardware_list.get(\"serial_number\", None)\n    system_product_name = hardware_list.get(\"machine_model\", None)\n    bios_version = hardware_list.get(\"boot_rom_version\", None)\n\n    yield rdf_client.HardwareInfo(\n        serial_number=serial_number,\n        bios_version=bios_version,\n        system_product_name=system_product_name)"
    },
    {
        "original": "def wait_for_vacancy(self, processor_type):\n        \"\"\"Waits for a particular processor type to have the capacity to\n        handle additional transactions or until is_cancelled is True.\n\n        Args:\n            processor_type (ProcessorType): The family, and version of\n                the transaction processor.\n\n        Returns:\n            Processor\n        \"\"\"\n\n        with self._condition:\n            self._condition.wait_for(lambda: (\n                self._processor_available(processor_type)\n                or self._cancelled_event.is_set()))\n            if self._cancelled_event.is_set():\n                raise WaitCancelledException()\n            processor = self[processor_type].next_processor()\n            return processor",
        "rewrite": "def wait_for_vacancy(self, processor_type):\n    with self._condition:\n        self._condition.wait_for(lambda: (self._processor_available(processor_type) or self._cancelled_event.is_set()))\n        if self._cancelled_event.is_set():\n            raise WaitCancelledException()\n        processor = self[processor_type].next_processor()\n        return processor"
    },
    {
        "original": "def are_equal(self, sp1, sp2):\n        \"\"\"\n        True if there is some overlap in composition between the species\n\n        Args:\n            sp1: First species. A dict of {specie/element: amt} as per the\n                definition in Site and PeriodicSite.\n            sp2: Second species. A dict of {specie/element: amt} as per the\n                definition in Site and PeriodicSite.\n\n        Returns:\n            True always\n        \"\"\"\n        set1 = set(sp1.elements)\n        set2 = set(sp2.elements)\n        return set1.issubset(set2) or set2.issubset(set1)",
        "rewrite": "def are_equal(self, sp1, sp2):\n    set1 = set(sp1.keys())\n    set2 = set(sp2.keys())\n    return set1.issubset(set2) or set2.issubset(set1)"
    },
    {
        "original": "def bind_device_pages(self, page_ids, bind, append, device_id=None,\n                          uuid=None, major=None, minor=None):\n        \"\"\"\n        \u914d\u7f6e\u8bbe\u5907\u4e0e\u9875\u9762\u7684\u5173\u8054\u5173\u7cfb\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/12/c8120214ec0ba08af5dfcc0da1a11400.html\n\n        :param page_ids: \u5f85\u5173\u8054\u7684\u9875\u9762\u5217\u8868\n        :param bind: \u5173\u8054\u64cd\u4f5c\u6807\u5fd7\u4f4d\uff0c 0\u4e3a\u89e3\u9664\u5173\u8054\u5173\u7cfb\uff0c1\u4e3a\u5efa\u7acb\u5173\u8054\u5173\u7cfb\n        :param append: \u65b0\u589e\u64cd\u4f5c\u6807\u5fd7\u4f4d\uff0c 0\u4e3a\u8986\u76d6\uff0c1\u4e3a\u65b0\u589e\n        :param device_id: \u8bbe\u5907\u7f16\u53f7\uff0c\u82e5\u586b\u4e86UUID\u3001major\u3001minor\uff0c\u5219\u53ef\u4e0d\u586b\u8bbe\u5907\u7f16\u53f7\uff0c\u82e5\u4e8c\u8005\u90fd\u586b\uff0c\u5219\u4ee5\u8bbe\u5907\u7f16\u53f7\u4e3a\u4f18\u5148\n        :param uuid: UUID\n        :param major: major\n        :param minor: minor\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        if not isinstance(page_ids, (tuple, list)):\n            page_ids = [page_ids]\n        data = {\n            'page_ids': page_ids,\n            'bind': int(bind),\n            'append': int(append),\n            'device_identifier': {\n                'device_id': device_id,\n                'uuid': uuid,\n                'major': major,\n                'minor': minor\n            }\n        }\n        return self._post(\n            'shakearound/device/bindpage',\n            data=data\n        )",
        "rewrite": "def bind_device_pages(self, page_ids, bind, append, device_id=None, uuid=None, major=None, minor=None):\n    if not isinstance(page_ids, (tuple, list)):\n        page_ids = [page_ids]\n    data = {\n        'page_ids': page_ids,\n        'bind': int(bind),\n        'append': int(append),\n        'device_identifier': {\n            'device_id': device_id,\n            'uuid': uuid,\n            'major': major,\n            'minor': minor\n        }\n    }\n    return self._post('shakearound/device/bindpage', data=data)"
    },
    {
        "original": "def build_estimator(model_dir, model_type, model_column_fn, inter_op, intra_op, ctx):\n  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n  wide_columns, deep_columns = model_column_fn()\n  hidden_units = [100, 75, 50, 25]\n\n  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n  # trains faster than GPU for this model.\n  # Note: adding device_filter to fix: https://github.com/tensorflow/tensorflow/issues/21745\n  run_config = tf.estimator.RunConfig().replace(\n      session_config=tf.ConfigProto(device_count={'GPU': 0},\n                                    device_filters=['/job:ps', '/job:%s/task:%d' % (ctx.job_name, ctx.task_index)],\n                                    inter_op_parallelism_threads=inter_op,\n                                    intra_op_parallelism_threads=intra_op))\n\n  if model_type == 'wide':\n    return tf.estimator.LinearClassifier(\n        model_dir=model_dir,\n        feature_columns=wide_columns,\n        config=run_config)\n  elif model_type == 'deep':\n    return tf.estimator.DNNClassifier(\n        model_dir=model_dir,\n        feature_columns=deep_columns,\n        hidden_units=hidden_units,\n        config=run_config)\n  else:\n    return tf.estimator.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=hidden_units,\n        config=run_config)",
        "rewrite": "def build_estimator(model_dir, model_type, model_column_fn, inter_op, intra_op, ctx):\n    wide_columns, deep_columns = model_column_fn()\n    hidden_units = [100, 75, 50, 25]\n\n    run_config = tf.estimator.RunConfig().replace(\n        session_config=tf.ConfigProto(device_count={'GPU': 0},\n            device_filters=['/job:ps', '/job:%s/task:%d' % (ctx.job_name, ctx.task_index)],\n            inter_op_parallelism_threads=inter_op,\n            intra_op_parallelism_threads=intra_op))\n\n    if model_type == 'wide':\n        return tf.estimator.LinearClassifier(\n            model_dir=model_dir,\n            feature_columns=wide_columns,\n            config=run_config)\n    elif model_type == 'deep':\n        return tf.estimator.DNNClassifier(\n            model_dir=model_dir,\n            feature_columns=deep_columns,\n            hidden_units=hidden_units,\n            config=run_config)\n    else:\n        return tf.estimator.DNNLinearCombinedClassifier(\n            model_dir=model_dir,\n            linear_feature_columns=wide_columns,\n            dnn_feature_columns=deep_columns,\n            dnn_hidden_units=hidden_units,\n            config=run_config)"
    },
    {
        "original": "def _init_valid_functions(action_dimensions):\n  \"\"\"Initialize ValidFunctions and set up the callbacks.\"\"\"\n  sizes = {\n      \"screen\": tuple(int(i) for i in action_dimensions.screen),\n      \"screen2\": tuple(int(i) for i in action_dimensions.screen),\n      \"minimap\": tuple(int(i) for i in action_dimensions.minimap),\n  }\n\n  types = actions.Arguments(*[\n      actions.ArgumentType.spec(t.id, t.name, sizes.get(t.name, t.sizes))\n      for t in actions.TYPES])\n\n  functions = actions.Functions([\n      actions.Function.spec(f.id, f.name, tuple(types[t.id] for t in f.args))\n      for f in actions.FUNCTIONS])\n\n  return actions.ValidActions(types, functions)",
        "rewrite": "def _init_valid_functions(action_dimensions):\n    sizes = {\n        \"screen\": tuple(int(i) for i in action_dimensions.screen),\n        \"screen2\": tuple(int(i) for i in action_dimensions.screen),\n        \"minimap\": tuple(int(i) for i in action_dimensions.minimap),\n    }\n\n    types = actions.Arguments(*[\n        actions.ArgumentType.spec(t.id, t.name, sizes.get(t.name, t.sizes))\n        for t in actions.TYPES])\n\n    functions = actions.Functions([\n        actions.Function.spec(f.id, f.name, tuple(types[t.id] for t in f.args))\n        for f in actions.FUNCTIONS])\n\n    return actions.ValidActions(types, functions)"
    },
    {
        "original": "def squeeze(self, dim=None):\n        \"\"\"Return a new object with squeezed data.\n\n        Parameters\n        ----------\n        dim : None or str or tuple of str, optional\n            Selects a subset of the length one dimensions. If a dimension is\n            selected with length greater than one, an error is raised. If\n            None, all length one dimensions are squeezed.\n\n        Returns\n        -------\n        squeezed : same type as caller\n            This object, but with with all or a subset of the dimensions of\n            length 1 removed.\n\n        See Also\n        --------\n        numpy.squeeze\n        \"\"\"\n        dims = common.get_squeeze_dims(self, dim)\n        return self.isel({d: 0 for d in dims})",
        "rewrite": "def squeeze(self, dim=None):\n    dims = common.get_squeeze_dims(self, dim)\n    return self.isel({d: 0 for d in dims})"
    },
    {
        "original": "def container_def(image, model_data_url=None, env=None):\n    \"\"\"Create a definition for executing a container as part of a SageMaker model.\n\n    Args:\n        image (str): Docker image to run for this container.\n        model_data_url (str): S3 URI of data required by this container,\n            e.g. SageMaker training job model artifacts (default: None).\n        env (dict[str, str]): Environment variables to set inside the container (default: None).\n    Returns:\n        dict[str, str]: A complete container definition object usable with the CreateModel API if passed via\n        `PrimaryContainers` field.\n    \"\"\"\n    if env is None:\n        env = {}\n    c_def = {'Image': image, 'Environment': env}\n    if model_data_url:\n        c_def['ModelDataUrl'] = model_data_url\n    return c_def",
        "rewrite": "def container_def(image, model_data_url=None, env=None):\n    if env is None:\n        env = {}\n    c_def = {'Image': image, 'Environment': env}\n    if model_data_url:\n        c_def['ModelDataUrl'] = model_data_url\n    return c_def"
    },
    {
        "original": "def zone_create_or_update(name, resource_group, **kwargs):\n    \"\"\"\n    .. versionadded:: Fluorine\n\n    Creates or updates a DNS zone. Does not modify DNS records within the zone.\n\n    :param name: The name of the DNS zone to create (without a terminating dot).\n\n    :param resource_group: The name of the resource group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_dns.zone_create_or_update myzone testgroup\n\n    \"\"\"\n    # DNS zones are global objects\n    kwargs['location'] = 'global'\n\n    dnsconn = __utils__['azurearm.get_client']('dns', **kwargs)\n\n    # Convert list of ID strings to list of dictionaries with id key.\n    if isinstance(kwargs.get('registration_virtual_networks'), list):\n        kwargs['registration_virtual_networks'] = [{'id': vnet} for vnet in kwargs['registration_virtual_networks']]\n\n    if isinstance(kwargs.get('resolution_virtual_networks'), list):\n        kwargs['resolution_virtual_networks'] = [{'id': vnet} for vnet in kwargs['resolution_virtual_networks']]\n\n    try:\n        zone_model = __utils__['azurearm.create_object_model']('dns', 'Zone', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        zone = dnsconn.zones.create_or_update(\n            zone_name=name,\n            resource_group_name=resource_group,\n            parameters=zone_model,\n            if_match=kwargs.get('if_match'),\n            if_none_match=kwargs.get('if_none_match')\n        )\n        result = zone.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('dns', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result",
        "rewrite": "def zone_create_or_update(name, resource_group, **kwargs):\n    kwargs['location'] = 'global'\n    dnsconn = __utils__['azurearm.get_client']('dns', **kwargs)\n\n    if isinstance(kwargs.get('registration_virtual_networks'), list):\n        kwargs['registration_virtual_networks'] = [{'id': vnet} for vnet in kwargs['registration_virtual_networks']]\n\n    if isinstance(kwargs.get('resolution_virtual_networks'), list):\n        kwargs['resolution_virtual_networks'] = [{'id': vnet} for vnet in kwargs['resolution_virtual_networks']]\n\n    try:\n        zone_model = __utils__['azurearm.create_object_model']('dns', 'Zone', **kwargs)\n    except TypeError as exc:\n        result = {'error': 'The object model could not be built. ({0})'.format(str(exc))}\n        return result\n\n    try:\n        zone = dnsconn.zones.create_or_update(\n            zone_name=name,\n            resource_group_name=resource_group,\n            parameters=zone_model,\n            if_match=kwargs.get('if_match'),\n            if_none_match=kwargs.get('if_none_match')\n        )\n        result = zone.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('dns', str(exc), **kwargs)\n        result = {'error': str(exc)}\n    except SerializationError as exc:\n        result = {'error': 'The object model could not be parsed. ({0})'.format(str(exc))}\n\n    return result"
    },
    {
        "original": "def gen_csr(\n        minion_id,\n        dns_name,\n        zone='default',\n        country=None,\n        state=None,\n        loc=None,\n        org=None,\n        org_unit=None,\n        password=None,\n    ):\n    \"\"\"\n    Generate a csr using the host's private_key.\n    Analogous to:\n\n    .. code-block:: bash\n\n        VCert gencsr -cn [CN Value] -o \"Beta Organization\" -ou \"Beta Group\" \\\n            -l \"Palo Alto\" -st \"California\" -c US\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run venafi.gen_csr <minion_id> <dns_name>\n    \"\"\"\n    tmpdir = tempfile.mkdtemp()\n    os.chmod(tmpdir, 0o700)\n\n    bank = 'venafi/domains'\n    cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n    data = cache.fetch(bank, dns_name)\n    if data is None:\n        data = {}\n    if 'private_key' not in data:\n        data['private_key'] = gen_key(minion_id, dns_name, zone, password)\n\n    tmppriv = '{0}/priv'.format(tmpdir)\n    tmpcsr = '{0}/csr'.format(tmpdir)\n    with salt.utils.files.fopen(tmppriv, 'w') as if_:\n        if_.write(salt.utils.stringutils.to_str(data['private_key']))\n\n    if country is None:\n        country = __opts__.get('venafi', {}).get('country')\n\n    if state is None:\n        state = __opts__.get('venafi', {}).get('state')\n\n    if loc is None:\n        loc = __opts__.get('venafi', {}).get('loc')\n\n    if org is None:\n        org = __opts__.get('venafi', {}).get('org')\n\n    if org_unit is None:\n        org_unit = __opts__.get('venafi', {}).get('org_unit')\n\n    subject = '/C={0}/ST={1}/L={2}/O={3}/OU={4}/CN={5}'.format(\n        country,\n        state,\n        loc,\n        org,\n        org_unit,\n        dns_name,\n    )\n\n    cmd = \"openssl req -new -sha256 -key {0} -out {1} -subj '{2}'\".format(\n        tmppriv,\n        tmpcsr,\n        subject\n    )\n    if password is not None:\n        cmd += ' -passin pass:{0}'.format(password)\n    output = __salt__['salt.cmd']('cmd.run', cmd)\n\n    if 'problems making Certificate Request' in output:\n        raise CommandExecutionError(\n            'There was a problem generating the CSR. Please ensure that you '\n            'have the following variables set either on the command line, or '\n            'in the venafi section of your master configuration file: '\n            'country, state, loc, org, org_unit'\n        )\n\n    with salt.utils.files.fopen(tmpcsr, 'r') as of_:\n        csr = salt.utils.stringutils.to_unicode(of_.read())\n\n    data['minion_id'] = minion_id\n    data['csr'] = csr\n    cache.store(bank, dns_name, data)\n    return csr",
        "rewrite": "import os\nimport tempfile\nimport salt.cache\nimport salt.utils.files\nimport salt.utils.stringutils\nfrom salt.exceptions import CommandExecutionError\n\ndef gen_csr(minion_id, dns_name, zone='default', country=None, state=None, loc=None, org=None, org_unit=None, password=None):\n    tmpdir = tempfile.mkdtemp()\n    os.chmod(tmpdir, 0o700)\n    \n    bank = 'venafi/domains'\n    cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n    data = cache.fetch(bank, dns_name) or {}\n    \n    if 'private_key' not in data:\n        data['private_key'] = gen_key(minion_id, dns_name, zone, password)\n    \n    tmppriv = '{}/priv'.format(tmpdir)\n    tmpcsr = '{}/csr'.format(tmpdir)\n    \n    with salt.utils.files.fopen(tmppriv, 'w') as ifp:\n        ifp.write(salt.utils.stringutils.to_str(data['private_key']))\n        \n    country = country or __opts__.get('venafi', {}).get('country')\n    state = state or __opts__.get('venafi', {}).get('state')\n    loc = loc or __opts__.get('venafi', {}).get('loc')\n    org = org or __opts__.get('venafi', {}).get('org')\n    org_unit = org_unit or __opts__.get('venafi', {}).get('org_unit')\n    \n    subject = '/C={}/ST={}/L={}/O={}/OU={}/CN={}'.format(country, state, loc, org, org_unit, dns_name)\n    \n    cmd = \"openssl req -new -sha256 -key {} -out {} -subj '{}'\".format(tmppriv, tmpcsr, subject)\n    \n    if password is not None:\n        cmd += ' -passin pass:{}'.format(password)\n    \n    output = __salt__['salt.cmd']('cmd.run', cmd)\n    \n    if 'problems making Certificate Request' in output:\n        raise CommandExecutionError(\n            'There was a problem generating the CSR. Please ensure that you '\n            'have the following variables set either on the command line, or '\n            'in the venafi section of your master configuration file: '\n            'country, state, loc, org, org_unit'\n        )\n        \n    with salt.utils.files.fopen(tmpcsr, 'r') as ofp:\n        csr = salt.utils.stringutils.to_unicode(ofp.read())\n    \n    data['minion_id'] = minion_id\n    data['csr'] = csr\n    cache.store(bank, dns_name, data)\n    \n    return csr"
    },
    {
        "original": "def _span_to_width(self, grid_width, top_tc, vMerge):\n        \"\"\"\n        Incorporate and then remove `w:tc` elements to the right of this one\n        until this cell spans *grid_width*. Raises |ValueError| if\n        *grid_width* cannot be exactly achieved, such as when a merged cell\n        would drive the span width greater than *grid_width* or if not enough\n        grid columns are available to make this cell that wide. All content\n        from incorporated cells is appended to *top_tc*. The val attribute of\n        the vMerge element on the single remaining cell is set to *vMerge*.\n        If *vMerge* is |None|, the vMerge element is removed if present.\n        \"\"\"\n        self._move_content_to(top_tc)\n        while self.grid_span < grid_width:\n            self._swallow_next_tc(grid_width, top_tc)\n        self.vMerge = vMerge",
        "rewrite": "def _span_to_width(self, grid_width, top_tc, vMerge):\n    self._move_content_to(top_tc)\n    while self.grid_span < grid_width:\n        self._swallow_next_tc(grid_width, top_tc)\n    self.vMerge = vMerge"
    },
    {
        "original": "def _insert_job(self, job):\n        \"\"\"\n        Insert a new job into the job queue. If the job queue is ordered, this job will be inserted at the correct\n        position.\n\n        :param job: The job to insert\n        :return:    None\n        \"\"\"\n\n        key = self._job_key(job)\n\n        if self._allow_merging:\n            if key in self._job_map:\n                job_info = self._job_map[key]\n\n                # decide if we want to trigger a widening\n                # if not, we'll simply do the merge\n                # TODO: save all previous jobs for the sake of widening\n                job_added = False\n                if self._allow_widening and self._should_widen_jobs(job_info.job, job):\n                    try:\n                        widened_job = self._widen_jobs(job_info.job, job)\n                        # remove the old job since now we have a widened one\n                        if job_info in self._job_info_queue:\n                            self._job_info_queue.remove(job_info)\n                        job_info.add_job(widened_job, widened=True)\n                        job_added = True\n                    except AngrJobWideningFailureNotice:\n                        # widening failed\n                        # fall back to merging...\n                        pass\n\n                if not job_added:\n                    try:\n                        merged_job = self._merge_jobs(job_info.job, job)\n                        # remove the old job since now we have a merged one\n                        if job_info in self._job_info_queue:\n                            self._job_info_queue.remove(job_info)\n                        job_info.add_job(merged_job, merged=True)\n                    except AngrJobMergingFailureNotice:\n                        # merging failed\n                        job_info = JobInfo(key, job)\n                        # update the job map\n                        self._job_map[key] = job_info\n\n            else:\n                job_info = JobInfo(key, job)\n                self._job_map[key] = job_info\n\n        else:\n            job_info = JobInfo(key, job)\n            self._job_map[key] = job_info\n\n        if self._order_jobs:\n            self._binary_insert(self._job_info_queue, job_info, lambda elem: self._job_sorting_key(elem.job))\n\n        else:\n            self._job_info_queue.append(job_info)",
        "rewrite": "def _insert_job(self, job):\n    key = self._job_key(job)\n\n    if self._allow_merging:\n        if key in self._job_map:\n            job_info = self._job_map[key]\n\n            job_added = False\n            if self._allow_widening and self._should_widen_jobs(job_info.job, job):\n                try:\n                    widened_job = self._widen_jobs(job_info.job, job)\n                    if job_info in self._job_info_queue:\n                        self._job_info_queue.remove(job_info)\n                    job_info.add_job(widened_job, widened=True)\n                    job_added = True\n                except AngrJobWideningFailureNotice:\n                    pass\n\n            if not job_added:\n                try:\n                    merged_job = self._merge_jobs(job_info.job, job)\n                    if job_info in self._job_info_queue:\n                        self._job_info_queue.remove(job_info)\n                    job_info.add_job(merged_job, merged=True)\n                except AngrJobMergingFailureNotice:\n                    job_info = JobInfo(key, job)\n                    self._job_map[key] = job_info\n\n        else:\n            job_info = JobInfo(key, job)\n            self._job_map[key] = job_info\n\n    else:\n        job_info = JobInfo(key, job)\n        self._job_map[key] = job_info\n\n    if self._order_jobs:\n        self._binary_insert(self._job_info_queue, job_info, lambda elem: self._job_sorting_key(elem.job))\n\n    else:\n        self._job_info_queue.append(job_info)"
    },
    {
        "original": "def init_app(self, app, session):\n        \"\"\"\n            Will initialize the Flask app, supporting the app factory pattern.\n\n            :param app:\n            :param session: The SQLAlchemy session\n\n        \"\"\"\n        app.config.setdefault(\"APP_NAME\", \"F.A.B.\")\n        app.config.setdefault(\"APP_THEME\", \"\")\n        app.config.setdefault(\"APP_ICON\", \"\")\n        app.config.setdefault(\"LANGUAGES\", {\"en\": {\"flag\": \"gb\", \"name\": \"English\"}})\n        app.config.setdefault(\"ADDON_MANAGERS\", [])\n        app.config.setdefault(\"FAB_API_MAX_PAGE_SIZE\", 20)\n        self.app = app\n        if self.update_perms:  # default is True, if False takes precedence from config\n            self.update_perms = app.config.get('FAB_UPDATE_PERMS', True)\n        _security_manager_class_name = app.config.get('FAB_SECURITY_MANAGER_CLASS', None)\n        if _security_manager_class_name is not None:\n            self.security_manager_class = dynamic_class_import(\n                _security_manager_class_name\n            )\n        if self.security_manager_class is None:\n            from flask_appbuilder.security.sqla.manager import SecurityManager\n            self.security_manager_class = SecurityManager\n\n        self._addon_managers = app.config[\"ADDON_MANAGERS\"]\n        self.session = session\n        self.sm = self.security_manager_class(self)\n        self.bm = BabelManager(self)\n        self.openapi_manager = OpenApiManager(self)\n        self._add_global_static()\n        self._add_global_filters()\n        app.before_request(self.sm.before_request)\n        self._add_admin_views()\n        self._add_addon_views()\n        if self.app:\n            self._add_menu_permissions()\n        else:\n            self.post_init()\n        self._init_extension(app)",
        "rewrite": "def init_app(self, app, session):\n    app.config.setdefault(\"APP_NAME\", \"F.A.B.\")\n    app.config.setdefault(\"APP_THEME\", \"\")\n    app.config.setdefault(\"APP_ICON\", \"\")\n    app.config.setdefault(\"LANGUAGES\", {\"en\": {\"flag\": \"gb\", \"name\": \"English\"}})\n    app.config.setdefault(\"ADDON_MANAGERS\", [])\n    app.config.setdefault(\"FAB_API_MAX_PAGE_SIZE\", 20)\n    self.app = app\n    if self.update_perms:\n        self.update_perms = app.config.get('FAB_UPDATE_PERMS', True)\n    _security_manager_class_name = app.config.get('FAB_SECURITY_MANAGER_CLASS', None)\n    if _security_manager_class_name is not None:\n        self.security_manager_class = dynamic_class_import(_security_manager_class_name)\n    if self.security_manager_class is None:\n        from flask_appbuilder.security.sqla.manager import SecurityManager\n        self.security_manager_class = SecurityManager\n\n    self._addon_managers = app.config[\"ADDON_MANAGERS\"]\n    self.session = session\n    self.sm = self.security_manager_class(self)\n    self.bm = BabelManager(self)\n    self.openapi_manager = OpenApiManager(self)\n    self._add_global_static()\n    self._add_global_filters()\n\n    app.before_request(self.sm.before_request)\n    self._add_admin_views()\n    self._add_addon_views()\n    \n    if self.app:\n        self._add_menu_permissions()\n    else:\n        self.post_init()\n    \n    self._init_extension(app)"
    },
    {
        "original": "def update_frame(self, key, ranges=None, plot=None, element=None):\n        \"\"\"\n        Updates an existing plot with data corresponding\n        to the key.\n        \"\"\"\n        reused = isinstance(self.hmap, DynamicMap) and (self.overlaid or self.batched)\n        if not reused and element is None:\n            element = self._get_frame(key)\n        elif element is not None:\n            self.current_key = key\n            self.current_frame = element\n\n        renderer = self.handles.get('glyph_renderer', None)\n        glyph = self.handles.get('glyph', None)\n        visible = element is not None\n        if hasattr(renderer, 'visible'):\n            renderer.visible = visible\n        if hasattr(glyph, 'visible'):\n            glyph.visible = visible\n\n        if ((self.batched and not element) or element is None or (not self.dynamic and self.static) or\n            (self.streaming and self.streaming[0].data is self.current_frame.data and not self.streaming[0]._triggering)):\n            return\n\n        if self.batched:\n            style_element = element.last\n            max_cycles = None\n        else:\n            style_element = element\n            max_cycles = self.style._max_cycles\n        style = self.lookup_options(style_element, 'style')\n        self.style = style.max_cycles(max_cycles) if max_cycles else style\n\n        ranges = self.compute_ranges(self.hmap, key, ranges)\n        self.param.set_param(**self.lookup_options(style_element, 'plot').options)\n        ranges = util.match_spec(style_element, ranges)\n        self.current_ranges = ranges\n        plot = self.handles['plot']\n        if not self.overlaid:\n            self._update_ranges(style_element, ranges)\n            self._update_plot(key, plot, style_element)\n            self._set_active_tools(plot)\n\n        if 'hover' in self.handles and 'hv_created' in self.handles['hover'].tags:\n            self._update_hover(element)\n\n        self._update_glyphs(element, ranges, self.style[self.cyclic_index])\n        self._execute_hooks(element)",
        "rewrite": "def update_frame(self, key, ranges=None, plot=None, element=None):\n    reused = isinstance(self.hmap, DynamicMap) and (self.overlaid or self.batched)\n    if not reused and element is None:\n        element = self._get_frame(key)\n    elif element is not None:\n        self.current_key = key\n        self.current_frame = element\n\n    renderer = self.handles.get('glyph_renderer', None)\n    glyph = self.handles.get('glyph', None)\n    visible = element is not None\n    if hasattr(renderer, 'visible'):\n        renderer.visible = visible\n    if hasattr(glyph, 'visible'):\n        glyph.visible = visible\n\n    if ((self.batched and not element) or element is None or (not self.dynamic and self.static) or\n        (self.streaming and self.streaming[0].data is self.current_frame.data and not self.streaming[0]._triggering)):\n        return\n\n    if self.batched:\n        style_element = element.last\n        max_cycles = None\n    else:\n        style_element = element\n        max_cycles = self.style._max_cycles\n    style = self.lookup_options(style_element, 'style')\n    self.style = style.max_cycles(max_cycles) if max_cycles else style\n\n    ranges = self.compute_ranges(self.hmap, key, ranges)\n    self.param.set_param(**self.lookup_options(style_element, 'plot').options)\n    ranges = util.match_spec(style_element, ranges)\n    self.current_ranges = ranges\n    plot = self.handles['plot']\n    if not self.overlaid:\n        self._update_ranges(style_element, ranges)\n        self._update_plot(key, plot, style_element)\n        self._set_active_tools(plot)\n\n    if 'hover' in self.handles and 'hv_created' in self.handles['hover'].tags:\n        self._update_hover(element)\n\n    self._update_glyphs(element, ranges, self.style[self.cyclic_index])\n    self._execute_hooks(element)"
    },
    {
        "original": "def log_env_info():\n    \"\"\"\n    Prints information about execution environment.\n    \"\"\"\n    logging.info('Collecting environment information...')\n    env_info = torch.utils.collect_env.get_pretty_env_info()\n    logging.info(f'{env_info}')",
        "rewrite": "def log_env_info():\n    \"\"\"\n    Prints information about execution environment.\n    \"\"\"\n    logging.info('Collecting environment information...')\n    env_info = torch.utils.collect_env.get_pretty_env_info()\n    logging.info(f'{env_info}')"
    },
    {
        "original": "def in_placement_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if you can place something at a position. Remember, buildings usually use 2x2, 3x3 or 5x5 of these grid points.\n        Caution: some x and y offset might be required, see ramp code:\n        https://github.com/Dentosal/python-sc2/blob/master/sc2/game_info.py#L17-L18 \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.placement_grid[pos] != 0",
        "rewrite": "def in_placement_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n    assert isinstance(pos, (Point2, Point3, Unit))\n    pos = pos.position.to2.rounded\n    return self._game_info.placement_grid[pos] != 0"
    },
    {
        "original": "def toy_heaviside(seed=default_seed, max_iters=100, optimize=True, plot=True):\n    \"\"\"\n    Simple 1D classification example using a heavy side gp transformation\n\n    :param seed: seed value for data generation (default is 4).\n    :type seed: int\n\n    \"\"\"\n\n    try:import pods\n    except ImportError:print('pods unavailable, see https://github.com/sods/ods for example datasets')\n    data = pods.datasets.toy_linear_1d_classification(seed=seed)\n    Y = data['Y'][:, 0:1]\n    Y[Y.flatten() == -1] = 0\n\n    # Model definition\n    kernel = GPy.kern.RBF(1)\n    likelihood = GPy.likelihoods.Bernoulli(gp_link=GPy.likelihoods.link_functions.Heaviside())\n    ep = GPy.inference.latent_function_inference.expectation_propagation.EP()\n    m = GPy.core.GP(X=data['X'], Y=Y, kernel=kernel, likelihood=likelihood, inference_method=ep, name='gp_classification_heaviside')\n    #m = GPy.models.GPClassification(data['X'], likelihood=likelihood)\n\n    # Optimize\n    if optimize:\n        # Parameters optimization:\n        for _ in range(5):\n            m.optimize(max_iters=int(max_iters/5))\n        print(m)\n\n    # Plot\n    if plot:\n        from matplotlib import pyplot as plt\n        fig, axes = plt.subplots(2, 1)\n        m.plot_f(ax=axes[0])\n        m.plot(ax=axes[1])\n\n    print(m)\n    return m",
        "rewrite": "def toy_heaviside(seed=default_seed, max_iters=100, optimize=True, plot=True):\n\n    import GPy\n    from GPy.inference.latent_function_inference import expectation_propagation as ep\n    from GPy.likelihoods import link_functions\n    import pods\n\n    try:\n        import pods\n    except ImportError:\n        print('pods unavailable, see https://github.com/sods/ods for example datasets')\n\n    data = pods.datasets.toy_linear_1d_classification(seed=seed)\n    Y = data['Y'][:, 0:1]\n    Y[Y.flatten() == -1] = 0\n\n    # Model definition\n    kernel = GPy.kern.RBF(1)\n    likelihood = GPy.likelihoods.Bernoulli(gp_link=link_functions.Heaviside())\n    m = GPy.models.GPClassification(data['X'], Y, kernel=kernel, likelihood=likelihood)\n\n    # Optimize\n    if optimize:\n        # Parameters optimization\n        for _ in range(5):\n            m.optimize(max_iters=int(max_iters/5))\n        print(m)\n\n    # Plot\n    if plot:\n        import matplotlib.pyplot as plt\n        fig, axes = plt.subplots(2, 1)\n        m.plot_f(ax=axes[0])\n        m.plot(ax=axes[1])\n\n    print(m)\n    return m"
    },
    {
        "original": "def replace_one(self, filter, replacement, **kwargs):\n        \"\"\"\n        See http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.replace_one\n        \"\"\"\n        self._arctic_lib.check_quota()\n        return self._collection.replace_one(filter, replacement, **kwargs)",
        "rewrite": "def replace_one(self, filter, replacement, **kwargs):\n    self._arctic_lib.check_quota()\n    return self._collection.replace_one(filter, replacement, **kwargs)"
    },
    {
        "original": "def delete_tag(self, project, repository, tag_name):\n        \"\"\"\n        Creates a tag using the information provided in the {@link RestCreateTagRequest request}\n        The authenticated user must have REPO_WRITE permission for the context repository to call this resource.\n        :param project:\n        :param repository:\n        :param tag_name:\n        :return:\n        \"\"\"\n        url = 'rest/git/1.0/projects/{project}/repos/{repository}/tags/{tag}'.format(project=project,\n                                                                                     repository=repository,\n                                                                                     tag=tag_name)\n        return self.delete(url)",
        "rewrite": "def delete_tag(self, project, repository, tag_name):\n    url = 'rest/git/1.0/projects/{project}/repos/{repository}/tags/{tag}'.format(project=project,\n                                                                                 repository=repository,\n                                                                                 tag=tag_name)\n    return self.delete(url)"
    },
    {
        "original": "def baseline_snapshot(name, number=None, tag=None, include_diff=True, config='root', ignore=None):\n    \"\"\"\n    Enforces that no file is modified comparing against a previously\n    defined snapshot identified by number.\n\n    number\n        Number of selected baseline snapshot.\n\n    tag\n        Tag of the selected baseline snapshot. Most recent baseline baseline\n        snapshot is used in case of multiple snapshots with the same tag.\n        (`tag` and `number` cannot be used at the same time)\n\n    include_diff\n        Include a diff in the response (Default: True)\n\n    config\n        Snapper config name (Default: root)\n\n    ignore\n        List of files to ignore. (Default: None)\n    \"\"\"\n    if not ignore:\n        ignore = []\n\n    ret = {'changes': {},\n           'comment': '',\n           'name': name,\n           'result': True}\n\n    if number is None and tag is None:\n        ret.update({'result': False,\n                    'comment': 'Snapshot tag or number must be specified'})\n        return ret\n\n    if number and tag:\n        ret.update({'result': False,\n                    'comment': 'Cannot use snapshot tag and number at the same time'})\n        return ret\n\n    if tag:\n        snapshot = _get_baseline_from_tag(config, tag)\n        if not snapshot:\n            ret.update({'result': False,\n                        'comment': 'Baseline tag \"{0}\" not found'.format(tag)})\n            return ret\n        number = snapshot['id']\n\n    status = __salt__['snapper.status'](\n        config, num_pre=0, num_post=number)\n\n    for target in ignore:\n        if os.path.isfile(target):\n            status.pop(target, None)\n        elif os.path.isdir(target):\n            for target_file in [target_file for target_file in status.keys() if target_file.startswith(target)]:\n                status.pop(target_file, None)\n\n    for file in status:\n        # Only include diff for modified files\n        if \"modified\" in status[file][\"status\"] and include_diff:\n            status[file].pop(\"status\")\n            status[file].update(__salt__['snapper.diff'](config,\n                                                         num_pre=0,\n                                                         num_post=number,\n                                                         filename=file).get(file, {}))\n\n    if __opts__['test'] and status:\n        ret['changes'] = status\n        ret['comment'] = \"{0} files changes are set to be undone\".format(len(status.keys()))\n        ret['result'] = None\n    elif __opts__['test'] and not status:\n        ret['changes'] = {}\n        ret['comment'] = \"Nothing to be done\"\n        ret['result'] = True\n    elif not __opts__['test'] and status:\n        undo = __salt__['snapper.undo'](config, num_pre=number, num_post=0,\n                                        files=status.keys())\n        ret['changes']['sumary'] = undo\n        ret['changes']['files'] = status\n        ret['result'] = True\n    else:\n        ret['comment'] = \"No changes were done\"\n        ret['result'] = True\n\n    return ret",
        "rewrite": "def baseline_snapshot(name, number=None, tag=None, include_diff=True, config='root', ignore=None):\n\n    if not ignore:\n        ignore = []\n\n    ret = {'changes': {},\n           'comment': '',\n           'name': name,\n           'result': True}\n\n    if number is None and tag is None:\n        ret.update({'result': False, 'comment': 'Snapshot tag or number must be specified'})\n        return ret\n\n    if number and tag:\n        ret.update({'result': False, 'comment': 'Cannot use snapshot tag and number at the same time'})\n        return ret\n\n    if tag:\n        snapshot = _get_baseline_from_tag(config, tag)\n        if not snapshot:\n            ret.update({'result': False, 'comment': 'Baseline tag \"{0}\" not found'.format(tag)})\n            return ret\n        number = snapshot['id']\n\n    status = __salt__['snapper.status'](config, num_pre=0, num_post=number)\n\n    for target in ignore:\n        if os.path.isfile(target):\n            status.pop(target, None)\n        elif os.path.isdir(target):\n            for target_file in [target_file for target_file in status.keys() if target_file.startswith(target)]:\n                status.pop(target_file, None)\n\n    for file in status:\n        if \"modified\" in status[file][\"status\"] and include_diff:\n            status[file].pop(\"status\")\n            status[file].update(__salt__['snapper.diff'](config, num_pre=0, num_post=number, filename=file).get(file, {}))\n\n    if __opts__['test']:\n        if status:\n            ret['changes'] = status\n            ret['comment'] = \"{0} files changes are set to be undone\".format(len(status.keys()))\n            ret['result'] = None\n        else:\n            ret['changes'] = {}\n            ret['comment'] = \"Nothing to be done\"\n            ret['result'] = True\n    else:\n        if status:\n            undo = __salt__['snapper.undo'](config, num_pre=number, num_post=0, files=status.keys())\n            ret['changes']['sumary'] = undo\n            ret['changes']['files'] = status\n            ret['result'] = True\n        else:\n            ret['comment'] = \"No changes were done\"\n            ret['result'] = True\n\n    return ret"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self,\n                   'matching_results') and self.matching_results is not None:\n            _dict['matching_results'] = self.matching_results\n        if hasattr(self, 'results') and self.results is not None:\n            _dict['results'] = [x._to_dict() for x in self.results]\n        if hasattr(self, 'aggregations') and self.aggregations is not None:\n            _dict['aggregations'] = [x._to_dict() for x in self.aggregations]\n        if hasattr(self, 'passages') and self.passages is not None:\n            _dict['passages'] = [x._to_dict() for x in self.passages]\n        if hasattr(\n                self,\n                'duplicates_removed') and self.duplicates_removed is not None:\n            _dict['duplicates_removed'] = self.duplicates_removed\n        return _dict",
        "rewrite": "def to_dict(self):\n    _dict = {}\n    if hasattr(self, 'matching_results') and self.matching_results is not None:\n        _dict['matching_results'] = self.matching_results\n    if hasattr(self, 'results') and self.results is not None:\n        _dict['results'] = [x.to_dict() for x in self.results]\n    if hasattr(self, 'aggregations') and self.aggregations is not None:\n        _dict['aggregations'] = [x.to_dict() for x in self.aggregations]\n    if hasattr(self, 'passages') and self.passages is not None:\n        _dict['passages'] = [x.to_dict() for x in self.passages]\n    if hasattr(self, 'duplicates_removed') and self.duplicates_removed is not None:\n        _dict['duplicates_removed'] = self.duplicates_removed\n    return _dict"
    },
    {
        "original": "def _normalize_address(self, region_id, relative_address, target_region=None):\n        \"\"\"\n        If this is a stack address, we convert it to a correct region and address\n\n        :param region_id: a string indicating which region the address is relative to\n        :param relative_address: an address that is relative to the region parameter\n        :param target_region: the ideal target region that address is normalized to. None means picking the best fit.\n        :return: an AddressWrapper object\n        \"\"\"\n        if self._stack_region_map.is_empty and self._generic_region_map.is_empty:\n            # We don't have any mapped region right now\n            return AddressWrapper(region_id, 0, relative_address, False, None)\n\n        # We wanna convert this address to an absolute address first\n        if region_id.startswith('stack_'):\n            absolute_address = self._stack_region_map.absolutize(region_id, relative_address)\n\n        else:\n            absolute_address = self._generic_region_map.absolutize(region_id, relative_address)\n\n        stack_base = self._stack_region_map.stack_base\n\n        if stack_base - self._stack_size < relative_address <= stack_base and \\\n                (target_region is not None and target_region.startswith('stack_')):\n            # The absolute address seems to be in the stack region.\n            # Map it to stack\n            new_region_id, new_relative_address, related_function_addr = self._stack_region_map.relativize(\n                absolute_address,\n                target_region_id=target_region\n            )\n\n            return AddressWrapper(new_region_id, self._region_base(new_region_id), new_relative_address, True,\n                                  related_function_addr\n                                  )\n\n        else:\n            new_region_id, new_relative_address, related_function_addr = self._generic_region_map.relativize(\n                absolute_address,\n                target_region_id=target_region\n            )\n\n            return AddressWrapper(new_region_id, self._region_base(new_region_id), new_relative_address, False, None)",
        "rewrite": "def _normalize_address(self, region_id, relative_address, target_region=None):\n        if self._stack_region_map.is_empty and self._generic_region_map.is_empty:\n            return AddressWrapper(region_id, 0, relative_address, False, None)\n\n        if region_id.startswith('stack_'):\n            absolute_address = self._stack_region_map.absolutize(region_id, relative_address)\n        else:\n            absolute_address = self._generic_region_map.absolutize(region_id, relative_address)\n\n        stack_base = self._stack_region_map.stack_base\n\n        if stack_base - self._stack_size < relative_address <= stack_base and (target_region is not None and target_region.startswith('stack_')):\n            new_region_id, new_relative_address, related_function_addr = self._stack_region_map.relativize(absolute_address, target_region_id=target_region)\n            return AddressWrapper(new_region_id, self._region_base(new_region_id), new_relative_address, True, related_function_addr)\n\n        else:\n            new_region_id, new_relative_address, related_function_addr = self._generic_region_map.relativize(absolute_address, target_region_id=target_region)\n            return AddressWrapper(new_region_id, self._region_base(new_region_id), new_relative_address, False, None)"
    },
    {
        "original": "def list(\n        self, root: str, patterns: List[str], exclude: Optional[List[str]] = None\n    ) -> List[str]:\n        \"\"\"\n            Return the list of files that match any of the patterns within root.\n            If exclude is provided, files that match an exclude pattern are omitted.\n\n            Note: The `find` command does not understand globs properly.\n                e.g. 'a/*.py' will match 'a/b/c.py'\n            For this reason, avoid calling this method with glob patterns.\n        \"\"\"\n\n        command = [\"find\", \".\"]\n        command += self._match_any(patterns)\n        if exclude:\n            command += [\"-and\", \"!\"]\n            command += self._match_any(exclude)\n        return (\n            subprocess.run(command, stdout=subprocess.PIPE, cwd=root)\n            .stdout.decode(\"utf-8\")\n            .split()\n        )",
        "rewrite": "def list(\n        self, root: str, patterns: List[str], exclude: Optional[List[str]] = None\n    ) -> List[str]:\n\n        command = [\"find\", root]\n        command += patterns\n        if exclude:\n            command.extend([\"-and\", \"!\"])\n            command += exclude\n\n        return (\n            subprocess.run(command, stdout=subprocess.PIPE, cwd=root)\n            .stdout.decode(\"utf-8\")\n            .split()\n        )"
    },
    {
        "original": "def send(self, sock, msg):\n    \"\"\"Send ``msg`` to destination ``sock``.\"\"\"\n    data = pickle.dumps(msg)\n    buf = struct.pack('>I', len(data)) + data\n    sock.sendall(buf)",
        "rewrite": "def send(self, sock, msg):\n    data = pickle.dumps(msg)\n    msg_length = struct.pack('>I', len(data))\n    sock.sendall(msg_length + data)"
    },
    {
        "original": "def with_edges(molecule, edges):\n        \"\"\"\n        Constructor for MoleculeGraph, using pre-existing or pre-defined edges\n        with optional edge parameters.\n\n        :param molecule: Molecule object\n        :param edges: dict representing the bonds of the functional\n                group (format: {(u, v): props}, where props is a dictionary of\n                properties, including weight. Props should be None if no\n                additional properties are to be specified.\n        :return: mg, a MoleculeGraph\n        \"\"\"\n\n        mg = MoleculeGraph.with_empty_graph(molecule, name=\"bonds\",\n                                            edge_weight_name=\"weight\",\n                                            edge_weight_units=\"\")\n\n        for edge, props in edges.items():\n\n            try:\n                from_index = edge[0]\n                to_index = edge[1]\n            except TypeError:\n                raise ValueError(\"Edges must be given as (from_index, to_index)\"\n                                 \"tuples\")\n\n            if props is not None:\n                if \"weight\" in props.keys():\n                    weight = props[\"weight\"]\n                    del props[\"weight\"]\n                else:\n                    weight = None\n\n                if len(props.items()) == 0:\n                    props = None\n            else:\n                weight = None\n\n            nodes = mg.graph.nodes\n            if not (from_index in nodes and to_index in nodes):\n                raise ValueError(\"Edges cannot be added if nodes are not\"\n                                 \" present in the graph. Please check your\"\n                                 \" indices.\")\n\n            mg.add_edge(from_index, to_index, weight=weight,\n                        edge_properties=props)\n\n        mg.set_node_attributes()\n        return mg",
        "rewrite": "def with_edges(molecule, edges):\n    mg = MoleculeGraph.with_empty_graph(molecule, name=\"bonds\", \n                                        edge_weight_name=\"weight\",\n                                        edge_weight_units=\"\")\n\n    for edge, props in edges.items():\n        try:\n            from_index = edge[0]\n            to_index = edge[1]\n        except TypeError:\n            raise ValueError(\"Edges must be given as (from_index, to_index) tuples\")\n\n        if props is not None:\n            weight = props.get(\"weight\", None)\n            if \"weight\" in props:\n                del props[\"weight\"]\n            if not props:\n                props = None\n        else:\n            weight = None\n\n        nodes = mg.graph.nodes\n        if from_index not in nodes or to_index not in nodes:\n            raise ValueError(\"Edges cannot be added if nodes are not present in the graph. Please check your indices.\")\n\n        mg.add_edge(from_index, to_index, weight=weight, edge_properties=props)\n\n    mg.set_node_attributes()\n    return mg"
    },
    {
        "original": "def deparse_code2str(code, out=sys.stdout, version=None,\n                     debug_opts=DEFAULT_DEBUG_OPTS,\n                     code_objects={}, compile_mode='exec',\n                     is_pypy=IS_PYPY, walker=SourceWalker):\n    \"\"\"Return the deparsed text for a Python code object. `out` is where any intermediate\n    output for assembly or tree output will be sent.\n    \"\"\"\n    return deparse_code(version, code, out, showasm=debug_opts.get('asm', None),\n                        showast=debug_opts.get('tree', None),\n                        showgrammar=debug_opts.get('grammar', None), code_objects=code_objects,\n                        compile_mode=compile_mode, is_pypy=is_pypy, walker=walker).text",
        "rewrite": "import sys\n\ndef deparse_code2str(code, out=sys.stdout, version=None, debug_opts=DEFAULT_DEBUG_OPTS, code_objects={}, compile_mode='exec', is_pypy=IS_PYPY, walker=SourceWalker):\n    return deparse_code(version, code, out, showasm=debug_opts.get('asm', None), showast=debug_opts.get('tree', None), showgrammar=debug_opts.get('grammar', None), code_objects=code_objects, compile_mode=compile_mode, is_pypy=is_pypy, walker=walker).text"
    },
    {
        "original": "def store(self, variables, attributes, check_encoding_set=frozenset(),\n              writer=None, unlimited_dims=None):\n        \"\"\"\n        Top level method for putting data on this store, this method:\n          - encodes variables/attributes\n          - sets dimensions\n          - sets variables\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        \"\"\"\n        if writer is None:\n            writer = ArrayWriter()\n\n        variables, attributes = self.encode(variables, attributes)\n\n        self.set_attributes(attributes)\n        self.set_dimensions(variables, unlimited_dims=unlimited_dims)\n        self.set_variables(variables, check_encoding_set, writer,\n                           unlimited_dims=unlimited_dims)",
        "rewrite": "def store(self, variables, attributes, check_encoding_set=frozenset(), writer=None, unlimited_dims=None):\n    if writer is None:\n        writer = ArrayWriter()\n\n    variables, attributes = self.encode(variables, attributes)\n\n    self.set_attributes(attributes)\n    self.set_dimensions(variables, unlimited_dims=unlimited_dims)\n    self.set_variables(variables, check_encoding_set, writer, unlimited_dims=unlimited_dims)"
    },
    {
        "original": "def tasks_from_nids(self, nids):\n        \"\"\"\n        Return the list of tasks associated to the given list of node identifiers (nids).\n\n        .. note::\n\n            Invalid ids are ignored\n        \"\"\"\n        if not isinstance(nids, collections.abc.Iterable): nids = [nids]\n\n        n2task = {task.node_id: task for task in self.iflat_tasks()}\n        return [n2task[n] for n in nids if n in n2task]",
        "rewrite": "def tasks_from_nids(self, nids):\n    if not isinstance(nids, collections.abc.Iterable):\n        nids = [nids]\n\n    n2task = {task.node_id: task for task in self.iflat_tasks()}\n    return [n2task.get(n) for n in nids if n in n2task]"
    },
    {
        "original": "def template(template_name):\n    \"\"\"Return a jinja template ready for rendering. If needed, global variables are initialized.\n\n    Parameters\n    ----------\n    template_name: str, the name of the template as defined in the templates mapping\n\n    Returns\n    -------\n    The Jinja template ready for rendering\n    \"\"\"\n    globals = None\n    if template_name.startswith('row_'):\n        # This is a row template setting global variable\n        globals = dict()\n        globals['vartype'] = var_type[template_name.split('_')[1].upper()]\n    return jinja2_env.get_template(templates[template_name], globals=globals)",
        "rewrite": "def template(template_name):\n    globals = None\n    if template_name.startswith('row_'):\n        globals = dict()\n        globals['vartype'] = var_type[template_name.split('_')[1].upper()]\n    return jinja2_env.get_template(templates[template_name], globals=globals)"
    },
    {
        "original": "def beginning_offsets(self, partitions):\n        \"\"\"Get the first offset for the given partitions.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The earliest available offsets for the\n            given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms.\n        \"\"\"\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets",
        "rewrite": "def beginning_offsets(self, partitions):\n    offsets = self._fetcher.beginning_offsets(partitions, self.config['request_timeout_ms'])\n    return offsets"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        Bson-serializable dict representation of the SimplestChemenvStrategy object.\n        :return: Bson-serializable dict representation of the SimplestChemenvStrategy object.\n        \"\"\"\n        return {\"@module\": self.__class__.__module__,\n                \"@class\": self.__class__.__name__,\n                \"distance_cutoff\": float(self._distance_cutoff),\n                \"angle_cutoff\": float(self._angle_cutoff),\n                \"additional_condition\": int(self._additional_condition),\n                \"continuous_symmetry_measure_cutoff\": float(self._continuous_symmetry_measure_cutoff),\n                \"symmetry_measure_type\": self._symmetry_measure_type}",
        "rewrite": "def as_dict(self):\n    return {\"@module\": self.__class__.__module__,\n            \"@class\": self.__class__.__name__,\n            \"distance_cutoff\": float(self._distance_cutoff),\n            \"angle_cutoff\": float(self._angle_cutoff),\n            \"additional_condition\": int(self._additional_condition),\n            \"continuous_symmetry_measure_cutoff\": float(self._continuous_symmetry_measure_cutoff),\n            \"symmetry_measure_type\": self._symmetry_measure_type}"
    },
    {
        "original": "def DeleteMessageHandlerRequests(self, requests):\n    \"\"\"Deletes a list of message handler requests from the database.\"\"\"\n\n    for r in requests:\n      flow_dict = self.message_handler_requests.get(r.handler_name, {})\n      if r.request_id in flow_dict:\n        del flow_dict[r.request_id]\n      flow_dict = self.message_handler_leases.get(r.handler_name, {})\n      if r.request_id in flow_dict:\n        del flow_dict[r.request_id]",
        "rewrite": "def DeleteMessageHandlerRequests(self, requests):\n    \"\"\"Deletes a list of message handler requests from the database.\"\"\"\n\n    for r in requests:\n        flow_dict = self.message_handler_requests.get(r.handler_name, {})\n        if r.request_id in flow_dict:\n            del flow_dict[r.request_id]\n        flow_dict = self.message_handler_leases.get(r.handler_name, {})\n        if r.request_id in flow_dict:\n            del flow_dict[r.request_id]"
    },
    {
        "original": "def load(self, executable):\n        \"\"\"\n        Initialize a QAM into a fresh state.\n\n        :param executable: Load a compiled executable onto the QAM.\n        \"\"\"\n        if self.status == 'loaded':\n            warnings.warn(\"Overwriting previously loaded executable.\")\n        assert self.status in ['connected', 'done', 'loaded']\n\n        self._variables_shim = {}\n        self._executable = executable\n        self._bitstrings = None\n        self.status = 'loaded'\n        return self",
        "rewrite": "def load(self, executable):\n    if self.status == 'loaded':\n        warnings.warn(\"Overwriting previously loaded executable.\")\n    assert self.status in ['connected', 'done', 'loaded']\n\n    self._variables_shim = {}\n    self._executable = executable\n    self._bitstrings = None\n    self.status = 'loaded'\n    return self"
    },
    {
        "original": "def add_text(self, text):\n        \"\"\"\n        Append the run content elements corresponding to *text* to the\n        ``<w:r>`` element of this instance.\n        \"\"\"\n        for char in text:\n            self.add_char(char)\n        self.flush()",
        "rewrite": "def add_text(self, text):\n    for char in text:\n        self.add_char(char)\n    self.flush()"
    },
    {
        "original": "def _return_pub_syndic(self, values, master_id=None):\n        \"\"\"\n        Wrapper to call the '_return_pub_multi' a syndic, best effort to get the one you asked for\n        \"\"\"\n        func = '_return_pub_multi'\n        for master, syndic_future in self.iter_master_options(master_id):\n            if not syndic_future.done() or syndic_future.exception():\n                log.error(\n                    'Unable to call %s on %s, that syndic is not connected',\n                    func, master\n                )\n                continue\n\n            future, data = self.pub_futures.get(master, (None, None))\n            if future is not None:\n                if not future.done():\n                    if master == master_id:\n                        # Targeted master previous send not done yet, call again later\n                        return False\n                    else:\n                        # Fallback master is busy, try the next one\n                        continue\n                elif future.exception():\n                    # Previous execution on this master returned an error\n                    log.error(\n                        'Unable to call %s on %s, trying another...',\n                        func, master\n                    )\n                    self._mark_master_dead(master)\n                    del self.pub_futures[master]\n                    # Add not sent data to the delayed list and try the next master\n                    self.delayed.extend(data)\n                    continue\n            future = getattr(syndic_future.result(), func)(values,\n                                                           '_syndic_return',\n                                                           timeout=self._return_retry_timer(),\n                                                           sync=False)\n            self.pub_futures[master] = (future, values)\n            return True\n        # Loop done and didn't exit: wasn't sent, try again later\n        return False",
        "rewrite": "def _return_pub_syndic(self, values, master_id=None):\n    func = '_return_pub_multi'\n    for master, syndic_future in self.iter_master_options(master_id):\n        if not syndic_future.done() or syndic_future.exception():\n            log.error('Unable to call %s on %s, that syndic is not connected', func, master)\n            continue\n\n        future, data = self.pub_futures.get(master, (None, None))\n        if future is not None:\n            if not future.done():\n                if master == master_id:\n                    return False\n                else:\n                    continue\n            elif future.exception():\n                log.error('Unable to call %s on %s, trying another...', func, master)\n                self._mark_master_dead(master)\n                del self.pub_futures[master]\n                self.delayed.extend(data)\n                continue\n\n        future = getattr(syndic_future.result(), func)(values, '_syndic_return',\n                                                       timeout=self._return_retry_timer(),\n                                                       sync=False)\n        self.pub_futures[master] = (future, values)\n        return True\n\n    return False"
    },
    {
        "original": "def decode_dict_keys_to_str(src):\n    \"\"\"\n    Convert top level keys from bytes to strings if possible.\n    This is necessary because Python 3 makes a distinction\n    between these types.\n    \"\"\"\n    if not six.PY3 or not isinstance(src, dict):\n        return src\n\n    output = {}\n    for key, val in six.iteritems(src):\n        if isinstance(key, bytes):\n            try:\n                key = key.decode()\n            except UnicodeError:\n                pass\n        output[key] = val\n    return output",
        "rewrite": "def decode_dict_keys_to_str(src):\n    if not isinstance(src, dict):\n        return src\n\n    output = {}\n    for key, val in src.items():\n        if isinstance(key, bytes):\n            try:\n                key = key.decode()\n            except UnicodeError:\n                pass\n        output[key] = val\n    return output"
    },
    {
        "original": "def ShowNotifications(self, reset=True):\n    \"\"\"A generator of current notifications.\"\"\"\n    shown_notifications = self.Schema.SHOWN_NOTIFICATIONS()\n\n    # Pending notifications first\n    pending = self.Get(self.Schema.PENDING_NOTIFICATIONS, [])\n    for notification in pending:\n      shown_notifications.Append(notification)\n\n    notifications = self.Get(self.Schema.SHOWN_NOTIFICATIONS, [])\n    for notification in notifications:\n      shown_notifications.Append(notification)\n\n    # Shall we reset the pending notification state?\n    if reset:\n      self.Set(shown_notifications)\n      self.Set(self.Schema.PENDING_NOTIFICATIONS())\n      self.Flush()\n\n    return shown_notifications",
        "rewrite": "def ShowNotifications(self, reset=True):\n    shown_notifications = self.Schema.SHOWN_NOTIFICATIONS()\n\n    pending = self.Get(self.Schema.PENDING_NOTIFICATIONS, [])\n    for notification in pending:\n        shown_notifications.Append(notification)\n\n    notifications = self.Get(self.Schema.SHOWN_NOTIFICATIONS, [])\n    for notification in notifications:\n        shown_notifications.Append(notification)\n\n    if reset:\n        self.Set(shown_notifications)\n        self.Set(self.Schema.PENDING_NOTIFICATIONS())\n        self.Flush()\n\n    return shown_notifications"
    },
    {
        "original": "def modified_policy_iteration(self, v_init=None, epsilon=None,\n                                  max_iter=None, k=20):\n        \"\"\"\n        Solve the optimization problem by modified policy iteration. See\n        the `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n        if epsilon is None:\n            epsilon = self.epsilon\n\n        def span(z):\n            return z.max() - z.min()\n\n        def midrange(z):\n            return (z.min() + z.max()) / 2\n\n        v = np.empty(self.num_states)\n        if v_init is None:\n            v[:] = self.R[self.R > -np.inf].min() / (1 - self.beta)\n        else:\n            v[:] = v_init\n\n        u = np.empty(self.num_states)\n        sigma = np.empty(self.num_states, dtype=int)\n\n        try:\n            tol = epsilon * (1-self.beta) / self.beta\n        except ZeroDivisionError:  # Raised if beta = 0\n            tol = np.inf\n\n        for i in range(max_iter):\n            # Policy improvement\n            self.bellman_operator(v, Tv=u, sigma=sigma)\n            diff = u - v\n            if span(diff) < tol:\n                v[:] = u + midrange(diff) * self.beta / (1 - self.beta)\n                break\n            # Partial policy evaluation with k iterations\n            self.operator_iteration(T=self.T_sigma(sigma), v=u, max_iter=k)\n            v[:] = u\n\n        num_iter = i + 1\n\n        res = DPSolveResult(v=v,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='modified policy iteration',\n                            epsilon=epsilon,\n                            max_iter=max_iter,\n                            k=k)\n\n        return res",
        "rewrite": "def modified_policy_iteration(self, v_init=None, epsilon=None, max_iter=None, k=20):\n    if self.beta == 1:\n        raise NotImplementedError(self._error_msg_no_discounting)\n\n    if max_iter is None:\n        max_iter = self.max_iter\n    if epsilon is None:\n        epsilon = self.epsilon\n\n    def span(z):\n        return z.max() - z.min()\n\n    def midrange(z):\n        return (z.min() + z.max()) / 2\n\n    v = np.empty(self.num_states)\n    if v_init is None:\n        v[:] = self.R[self.R > -np.inf].min() / (1 - self.beta)\n    else:\n        v[:] = v_init\n\n    u = np.empty(self.num_states)\n    sigma = np.empty(self.num_states, dtype=int)\n\n    try:\n        tol = epsilon * (1-self.beta) / self.beta\n    except ZeroDivisionError:\n        tol = np.inf\n\n    for i in range(max_iter):\n        self.bellman_operator(v, Tv=u, sigma=sigma)\n        diff = u - v\n        if span(diff) < tol:\n            v[:] = u + midrange(diff) * self.beta / (1 - self.beta)\n            break\n        self.operator_iteration(T=self.T_sigma(sigma), v=u, max_iter=k)\n        v[:] = u\n\n    num_iter = i + 1\n\n    res = DPSolveResult(v=v,\n                        sigma=sigma,\n                        num_iter=num_iter,\n                        mc=self.controlled_mc(sigma),\n                        method='modified policy iteration',\n                        epsilon=epsilon,\n                        max_iter=max_iter,\n                        k=k)\n\n    return res"
    },
    {
        "original": "def bootstraps(self, _args):\n        \"\"\"List all the bootstraps available to build with.\"\"\"\n        for bs in Bootstrap.list_bootstraps():\n            bs = Bootstrap.get_bootstrap(bs, self.ctx)\n            print('{Fore.BLUE}{Style.BRIGHT}{bs.name}{Style.RESET_ALL}'\n                  .format(bs=bs, Fore=Out_Fore, Style=Out_Style))\n            print('    {Fore.GREEN}depends: {bs.recipe_depends}{Fore.RESET}'\n                  .format(bs=bs, Fore=Out_Fore))",
        "rewrite": "def bootstraps(self, _args):\n    \"\"\"List all the bootstraps available to build with.\"\"\"\n    for bs in Bootstrap.list_bootstraps():\n        bs = Bootstrap.get_bootstrap(bs, self.ctx)\n        print('{Fore.BLUE}{Style.BRIGHT}{bs.name}{Style.RESET_ALL}'\n              .format(bs=bs, Fore=Out_Fore, Style=Out_Style))\n        print('    {Fore.GREEN}depends: {bs.recipe_depends}{Fore.RESET}'\n              .format(bs=bs, Fore=Out_Fore))"
    },
    {
        "original": "def get_role_policy(role_name, policy_name, region=None, key=None,\n                    keyid=None, profile=None):\n    \"\"\"\n    Get a role policy.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.get_role_policy myirole mypolicy\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        _policy = conn.get_role_policy(role_name, policy_name)\n        # I _hate_ you for not giving me an object boto.\n        _policy = _policy.get_role_policy_response.policy_document\n        # Policy is url encoded\n        _policy = _unquote(_policy)\n        _policy = salt.utils.json.loads(_policy, object_pairs_hook=odict.OrderedDict)\n        return _policy\n    except boto.exception.BotoServerError:\n        return {}",
        "rewrite": "def get_role_policy(role_name, policy_name, region=None, key=None,\n                    keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        _policy = conn.get_role_policy(role_name, policy_name)\n        _policy = _policy.get('RolePolicy').replace('\\n', '')\n        _policy = _unquote(_policy)\n        _policy = salt.utils.json.loads(_policy, object_pairs_hook=odict.OrderedDict)\n        return _policy\n    except boto.exception.BotoServerError:\n        return {}"
    },
    {
        "original": "def create_profile(hostname, username, password, profile_type, name, **kwargs):\n    r\"\"\"\n    A function to connect to a bigip device and create a profile.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    profile_type\n        The type of profile to create\n    name\n        The name of the profile to create\n    kwargs\n        [ arg=val ] ...\n\n        Consult F5 BIGIP user guide for specific options for each profile type.\n        Typically, tmsh arg names are used.\n\n    Special Characters ``|``, ``,`` and ``:`` must be escaped using ``\\`` when\n    used within strings.\n\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    #is this profile currently configured?\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    # if it exists\n    if existing['code'] == 200:\n\n        ret['result'] = True\n        ret['comment'] = 'A profile by this name currently exists.  No change made.'\n\n    # if it doesn't exist\n    elif existing['code'] == 404:\n\n        response = __salt__['bigip.create_profile'](hostname, username, password, profile_type, name, **kwargs)\n\n        if response['code'] == 200:\n            ret['result'] = True\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = response['content']\n            ret['comment'] = 'Profile was successfully created.'\n        else:\n            ret = _load_result(response, ret)\n\n    # else something else was returned\n    else:\n        ret = _load_result(existing, ret)\n\n    return ret",
        "rewrite": "def create_profile(hostname, username, password, profile_type, name, **kwargs):\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    if existing['code'] == 200:\n        ret['result'] = True\n        ret['comment'] = 'A profile by this name currently exists.  No change made.'\n\n    elif existing['code'] == 404:\n        response = __salt__['bigip.create_profile'](hostname, username, password, profile_type, name, **kwargs)\n\n        if response['code'] == 200:\n            ret['result'] = True\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = response['content']\n            ret['comment'] = 'Profile was successfully created.'\n        else:\n            ret = _load_result(response, ret)\n\n    else:\n        ret = _load_result(existing, ret)\n\n    return ret"
    },
    {
        "original": "def make_app(config=None):\n    \"\"\"\n    Factory function that creates a new `CoolmagicApplication`\n    object. Optional WSGI middlewares should be applied here.\n    \"\"\"\n    config = config or {}\n    app = CoolMagicApplication(config)\n\n    # static stuff\n    app = SharedDataMiddleware(\n        app, {\"/public\": path.join(path.dirname(__file__), \"public\")}\n    )\n\n    # clean up locals\n    app = local_manager.make_middleware(app)\n\n    return app",
        "rewrite": "def make_app(config=None):\n    config = config or {}\n    app = CoolMagicApplication(config)\n    app = SharedDataMiddleware(\n        app, {\"/public\": path.join(path.dirname(__file__), \"public\")}\n    )\n    app = local_manager.make_middleware(app)\n    return app"
    },
    {
        "original": "def cache_security_group_exists(name, region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Check to see if an ElastiCache security group exists.\n\n    Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_elasticache.cache_security_group_exists mysecuritygroup\n    \"\"\"\n    return bool(describe_cache_security_groups(name=name, region=region, key=key, keyid=keyid,\n                profile=profile))",
        "rewrite": "def cache_security_group_exists(name, region=None, key=None, keyid=None, profile=None):\n    return bool(describe_cache_security_groups(name=name, region=region, key=key, keyid=keyid, profile=profile))"
    },
    {
        "original": "def _sanitize_input_structure(input_structure):\n        \"\"\"\n        Sanitize our input structure by removing magnetic information\n        and making primitive.\n\n        Args:\n            input_structure: Structure\n\n        Returns: Structure\n        \"\"\"\n\n        input_structure = input_structure.copy()\n\n        # remove any annotated spin\n        input_structure.remove_spin()\n\n        # sanitize input structure: first make primitive ...\n        input_structure = input_structure.get_primitive_structure(use_site_props=False)\n\n        # ... and strip out existing magmoms, which can cause conflicts\n        # with later transformations otherwise since sites would end up\n        # with both magmom site properties and Specie spins defined\n        if \"magmom\" in input_structure.site_properties:\n            input_structure.remove_site_property(\"magmom\")\n\n        return input_structure",
        "rewrite": "def sanitize_input_structure(input_structure):\n    input_structure = input_structure.copy()\n    input_structure.remove_spin()\n    input_structure = input_structure.get_primitive_structure(use_site_props=False)\n    \n    if \"magmom\" in input_structure.site_properties:\n        input_structure.remove_site_property(\"magmom\")\n    \n    return input_structure"
    },
    {
        "original": "def _default_logfile(exe_name):\n    \"\"\"\n    Retrieve the logfile name\n    \"\"\"\n    if salt.utils.platform.is_windows():\n        tmp_dir = os.path.join(__opts__['cachedir'], 'tmp')\n        if not os.path.isdir(tmp_dir):\n            os.mkdir(tmp_dir)\n        logfile_tmp = tempfile.NamedTemporaryFile(dir=tmp_dir,\n                                                  prefix=exe_name,\n                                                  suffix='.log',\n                                                  delete=False)\n        logfile = logfile_tmp.name\n        logfile_tmp.close()\n    else:\n        logfile = salt.utils.path.join(\n            '/var/log',\n            '{0}.log'.format(exe_name)\n        )\n\n    return logfile",
        "rewrite": "def _default_logfile(exe_name):\n    if salt.utils.platform.is_windows():\n        tmp_dir = os.path.join(__opts__['cachedir'], 'tmp')\n        if not os.path.isdir(tmp_dir):\n            os.mkdir(tmp_dir)\n        logfile_tmp = tempfile.NamedTemporaryFile(dir=tmp_dir, prefix=exe_name, suffix='.log', delete=False)\n        logfile = logfile_tmp.name\n        logfile_tmp.close()\n    else:\n        logfile = salt.utils.path.join('/var/log', '{0}.log'.format(exe_name))\n\n    return logfile"
    },
    {
        "original": "def edit_repo(name,\n              description=None,\n              homepage=None,\n              private=None,\n              has_issues=None,\n              has_wiki=None,\n              has_downloads=None,\n              profile=\"github\"):\n    \"\"\"\n    Updates an existing Github repository.\n\n    name\n        The name of the team to be created.\n\n    description\n        The description of the repository.\n\n    homepage\n        The URL with more information about the repository.\n\n    private\n        The visiblity of the repository. Note that private repositories require\n        a paid GitHub account.\n\n    has_issues\n        Whether to enable issues for this repository.\n\n    has_wiki\n        Whether to enable the wiki for this repository.\n\n    has_downloads\n        Whether to enable downloads for this repository.\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.add_repo 'repo_name'\n\n    .. versionadded:: 2016.11.0\n    \"\"\"\n\n    try:\n        allow_private_change = _get_config_value(profile, 'allow_repo_privacy_changes')\n    except CommandExecutionError:\n        allow_private_change = False\n\n    if private is not None and not allow_private_change:\n        raise CommandExecutionError(\"The private field is set to be changed for \"\n                                    \"repo {0} but allow_repo_privacy_changes \"\n                                    \"disallows this.\".format(name))\n\n    try:\n        client = _get_client(profile)\n        organization = client.get_organization(\n            _get_config_value(profile, 'org_name')\n        )\n        repo = organization.get_repo(name)\n\n        given_params = {\n            'description': description,\n            'homepage': homepage,\n            'private': private,\n            'has_issues': has_issues,\n            'has_wiki': has_wiki,\n            'has_downloads': has_downloads\n        }\n        parameters = {'name': name}\n        for param_name, param_value in six.iteritems(given_params):\n            if param_value is not None:\n                parameters[param_name] = param_value\n\n        organization._requester.requestJsonAndCheck(\n            \"PATCH\",\n            repo.url,\n            input=parameters\n        )\n        get_repo_info(name, profile=profile, ignore_cache=True)  # Refresh cache\n        return True\n    except github.GithubException:\n        log.exception('Error editing a repo')\n        return False",
        "rewrite": "def edit_repo(name, description=None, homepage=None, private=None, has_issues=None, has_wiki=None, has_downloads=None, profile=\"github\"):\n    try:\n        allow_private_change = _get_config_value(profile, 'allow_repo_privacy_changes')\n    except CommandExecutionError:\n        allow_private_change = False\n\n    if private is not None and not allow_private_change:\n        raise CommandExecutionError(\"The private field is set to be changed for repo {0} but allow_repo_privacy_changes disallows this.\".format(name))\n\n    try:\n        client = _get_client(profile)\n        organization = client.get_organization(_get_config_value(profile, 'org_name'))\n        repo = organization.get_repo(name)\n\n        given_params = {'description': description, 'homepage': homepage, 'private': private, 'has_issues': has_issues, 'has_wiki': has_wiki, 'has_downloads': has_downloads}\n        parameters = {'name': name}\n        for param_name, param_value in six.iteritems(given_params):\n            if param_value is not None:\n                parameters[param_name] = param_value\n\n        organization._requester.requestJsonAndCheck(\"PATCH\", repo.url, input=parameters)\n        get_repo_info(name, profile=profile, ignore_cache=True)  \n        \n        return True\n    except github.GithubException:\n        log.exception('Error editing a repo')\n        return False"
    },
    {
        "original": "def create_introjs_tour(self, name=None):\n        \"\"\" Creates an IntroJS tour for a website.\n            @Params\n            name - If creating multiple tours at the same time,\n                   use this to select the tour you wish to add steps to.\n        \"\"\"\n        if not name:\n            name = \"default\"\n\n        new_tour = (\n            ",
        "rewrite": "def create_introjs_tour(self, name=None):\n        \"\"\" Creates an IntroJS tour for a website.\n            @Params\n            name - If creating multiple tours at the same time,\n                   use this to select the tour you wish to add steps to.\n        \"\"\"\n        if not name:\n            name = \"default\"\n\n        new_tour = (\n            \"Your introjs tour layout code here.\")"
    },
    {
        "original": "def Parse(self, persistence, knowledge_base, download_pathtype):\n    \"\"\"Convert persistence collector output to downloadable rdfvalues.\"\"\"\n    pathspecs = []\n\n    if isinstance(persistence, rdf_client.OSXServiceInformation):\n      if persistence.program:\n        pathspecs = rdf_paths.PathSpec(\n            path=persistence.program, pathtype=download_pathtype)\n      elif persistence.args:\n        pathspecs = rdf_paths.PathSpec(\n            path=persistence.args[0], pathtype=download_pathtype)\n\n    for pathspec in pathspecs:\n      yield rdf_standard.PersistenceFile(pathspec=pathspec)",
        "rewrite": "def parse(self, persistence, knowledge_base, download_pathtype):\n    pathspecs = []\n\n    if isinstance(persistence, rdf_client.OSXServiceInformation):\n        if persistence.program:\n            pathspecs.append(rdf_paths.PathSpec(\n                path=persistence.program, pathtype=download_pathtype))\n        elif persistence.args:\n            pathspecs.append(rdf_paths.PathSpec(\n                path=persistence.args[0], pathtype=download_pathtype))\n\n    for pathspec in pathspecs:\n        yield rdf_standard.PersistenceFile(pathspec=pathspec)"
    },
    {
        "original": "def _extract_asset_tags(self, text):\n        \"\"\"\n        Extract asset tags from text into a convenient form.\n\n        @param text: Text to extract asset tags from. This text contains HTML\n            code that is parsed by BeautifulSoup.\n        @type text: str\n\n        @return: Asset map.\n        @rtype: {\n            '<id>': {\n                'name': '<name>',\n                'extension': '<extension>'\n            },\n            ...\n        }\n        \"\"\"\n        soup = BeautifulSoup(text)\n        asset_tags_map = {}\n\n        for asset in soup.find_all('asset'):\n            asset_tags_map[asset['id']] = {'name': asset['name'],\n                                           'extension': asset['extension']}\n\n        return asset_tags_map",
        "rewrite": "def _extract_asset_tags(self, text):\n    soup = BeautifulSoup(text)\n    asset_tags_map = {}\n    \n    for asset in soup.find_all('asset'):\n        asset_tags_map[asset['id']] = {'name': asset['name'],\n                                       'extension': asset['extension']}\n    \n    return asset_tags_map"
    },
    {
        "original": "def do_stored_procedure_check(self, instance, proc):\n        \"\"\"\n        Fetch the metrics from the stored proc\n        \"\"\"\n\n        guardSql = instance.get('proc_only_if')\n        custom_tags = instance.get(\"tags\", [])\n\n        if (guardSql and self.proc_check_guard(instance, guardSql)) or not guardSql:\n            self.open_db_connections(instance, self.DEFAULT_DB_KEY)\n            cursor = self.get_cursor(instance, self.DEFAULT_DB_KEY)\n\n            try:\n                self.log.debug(\"Calling Stored Procedure : {}\".format(proc))\n                if self._get_connector(instance) == 'adodbapi':\n                    cursor.callproc(proc)\n                else:\n                    # pyodbc does not support callproc; use execute instead.\n                    # Reference: https://github.com/mkleehammer/pyodbc/wiki/Calling-Stored-Procedures\n                    call_proc = '{{CALL {}}}'.format(proc)\n                    cursor.execute(call_proc)\n\n                rows = cursor.fetchall()\n                self.log.debug(\"Row count ({}) : {}\".format(proc, cursor.rowcount))\n\n                for row in rows:\n                    tags = [] if row.tags is None or row.tags == '' else row.tags.split(',')\n                    tags.extend(custom_tags)\n\n                    if row.type.lower() in self.proc_type_mapping:\n                        self.proc_type_mapping[row.type](row.metric, row.value, tags)\n                    else:\n                        self.log.warning(\n                            '{} is not a recognised type from procedure {}, metric {}'.format(\n                                row.type, proc, row.metric\n                            )\n                        )\n\n            except Exception as e:\n                self.log.warning(\"Could not call procedure {}: {}\".format(proc, e))\n\n            self.close_cursor(cursor)\n            self.close_db_connections(instance, self.DEFAULT_DB_KEY)\n        else:\n            self.log.info(\"Skipping call to {} due to only_if\".format(proc))",
        "rewrite": "def do_stored_procedure_check(self, instance, proc):\n\n    guard_sql = instance.get('proc_only_if')\n    custom_tags = instance.get(\"tags\", [])\n\n    if (guard_sql and self.proc_check_guard(instance, guard_sql)) or not guard_sql:\n        self.open_db_connections(instance, self.DEFAULT_DB_KEY)\n        cursor = self.get_cursor(instance, self.DEFAULT_DB_KEY)\n\n        try:\n            self.log.debug(\"Calling Stored Procedure : {}\".format(proc))\n            if self._get_connector(instance) == 'adodbapi':\n                cursor.callproc(proc)\n            else:\n                call_proc = '{{CALL {}}}'.format(proc)\n                cursor.execute(call_proc)\n\n            rows = cursor.fetchall()\n            self.log.debug(\"Row count ({}) : {}\".format(proc, cursor.rowcount))\n\n            for row in rows:\n                tags = [] if row.tags is None or row.tags == '' else row.tags.split(',')\n                tags.extend(custom_tags)\n\n                if row.type.lower() in self.proc_type_mapping:\n                    self.proc_type_mapping[row.type](row.metric, row.value, tags)\n                else:\n                    self.log.warning(\n                        '{} is not a recognised type from procedure {}, metric {}'.format(\n                            row.type, proc, row.metric\n                        )\n                    )\n\n        except Exception as e:\n            self.log.warning(\"Could not call procedure {}: {}\".format(proc, e))\n\n        self.close_cursor(cursor)\n        self.close_db_connections(instance, self.DEFAULT_DB_KEY)\n    else:\n        self.log.info(\"Skipping call to {} due to only_if\".format(proc))"
    },
    {
        "original": "def enable_save_reply_handlers(self, delay=120, filename=\"./.handler-saves/reply.save\"):\n        \"\"\"\n        Enable saving reply handlers (by default saving disable)\n\n        :param delay: Delay between changes in handlers and saving\n        :param filename: Filename of save file\n        \"\"\"\n        self.reply_saver = Saver(self.reply_handlers, filename, delay)",
        "rewrite": "def enable_save_reply_handlers(self, delay=120, filename=\"./.handler-saves/reply.save\"):\n        self.reply_saver = Saver(self.reply_handlers, filename, delay)"
    },
    {
        "original": "def get_message_id(message):\n    \"\"\"Similar to :meth:`get_input_peer`, but for message IDs.\"\"\"\n    if message is None:\n        return None\n\n    if isinstance(message, int):\n        return message\n\n    try:\n        if message.SUBCLASS_OF_ID == 0x790009e3:\n            # hex(crc32(b'Message')) = 0x790009e3\n            return message.id\n    except AttributeError:\n        pass\n\n    raise TypeError('Invalid message type: {}'.format(type(message)))",
        "rewrite": "def get_message_id(message):\n    if message is None:\n        return None\n\n    if isinstance(message, int):\n        return message\n\n    try:\n        if message.SUBCLASS_OF_ID == 0x790009e3:\n            return message.id\n    except AttributeError:\n        pass\n\n    raise TypeError('Invalid message type: {}'.format(type(message)))"
    },
    {
        "original": "def list_releases():\n    \"\"\" Lists all releases published on pypi. \"\"\"\n    response = requests.get(PYPI_URL.format(package=PYPI_PACKAGE_NAME))\n    if response:\n        data = response.json()\n\n        releases_dict = data.get('releases', {})\n\n        if releases_dict:\n            for version, release in releases_dict.items():\n                release_formats = []\n                published_on_date = None\n                for fmt in release:\n                    release_formats.append(fmt.get('packagetype'))\n                    published_on_date = fmt.get('upload_time')\n\n                release_formats = ' | '.join(release_formats)\n                print('{:<10}{:>15}{:>25}'.format(version, published_on_date, release_formats))\n        else:\n            print('No releases found for {}'.format(PYPI_PACKAGE_NAME))\n    else:\n        print('Package \"{}\" not found on Pypi.org'.format(PYPI_PACKAGE_NAME))",
        "rewrite": "import requests\n\nPYPI_URL = 'https://pypi.org/pypi/{package}/json'\nPYPI_PACKAGE_NAME = 'requests'\n\ndef list_releases():\n    response = requests.get(PYPI_URL.format(package=PYPI_PACKAGE_NAME))\n    if response.status_code == 200:\n        data = response.json()\n\n        releases_dict = data.get('releases', {})\n\n        if releases_dict:\n            for version, release in releases_dict.items():\n                release_formats = []\n                published_on_date = None\n                for fmt in release:\n                    release_formats.append(fmt.get('packagetype'))\n                    published_on_date = fmt.get('upload_time')\n\n                release_formats = ' | '.join(release_formats)\n                print('{:<10}{:>15}{:>25}'.format(version, published_on_date, release_formats))\n        else:\n            print('No releases found for {}'.format(PYPI_PACKAGE_NAME))\n    else:\n        print('Package \"{}\" not found on Pypi.org'.format(PYPI_PACKAGE_NAME))"
    },
    {
        "original": "def _get_leader_for_partition(self, topic, partition):\n        \"\"\"\n        Returns the leader for a partition or None if the partition exists\n        but has no leader.\n\n        Raises:\n            UnknownTopicOrPartitionError: If the topic or partition is not part\n                of the metadata.\n            LeaderNotAvailableError: If the server has metadata, but there is no\n        current leader.\n        \"\"\"\n\n        key = TopicPartition(topic, partition)\n\n        # Use cached metadata if it is there\n        if self.topics_to_brokers.get(key) is not None:\n            return self.topics_to_brokers[key]\n\n        # Otherwise refresh metadata\n\n        # If topic does not already exist, this will raise\n        # UnknownTopicOrPartitionError if not auto-creating\n        # LeaderNotAvailableError otherwise until partitions are created\n        self.load_metadata_for_topics(topic)\n\n        # If the partition doesn't actually exist, raise\n        if partition not in self.topic_partitions.get(topic, []):\n            raise UnknownTopicOrPartitionError(key)\n\n        # If there's no leader for the partition, raise\n        leader = self.topic_partitions[topic][partition]\n        if leader == -1:\n            raise LeaderNotAvailableError((topic, partition))\n\n        # Otherwise return the BrokerMetadata\n        return self.brokers[leader]",
        "rewrite": "def _get_leader_for_partition(self, topic, partition):\n        key = TopicPartition(topic, partition)\n\n        if self.topics_to_brokers.get(key) is not None:\n            return self.topics_to_brokers[key]\n\n        self.load_metadata_for_topics(topic)\n\n        if partition not in self.topic_partitions.get(topic, []):\n            raise UnknownTopicOrPartitionError(key)\n\n        leader = self.topic_partitions[topic][partition]\n        if leader == -1:\n            raise LeaderNotAvailableError((topic, partition))\n\n        return self.brokers[leader]"
    },
    {
        "original": "def _filter_subgraph(self, subgraph, predicate):\n        \"\"\"\n        Given a subgraph of the manifest, and a predicate, filter\n        the subgraph using that predicate. Generates a list of nodes.\n        \"\"\"\n        to_return = []\n\n        for unique_id, item in subgraph.items():\n            if predicate(item):\n                to_return.append(item)\n\n        return to_return",
        "rewrite": "def _filter_subgraph(self, subgraph, predicate):\n    to_return = []\n\n    for unique_id, item in subgraph.items():\n        if predicate(item):\n            to_return.append(item)\n\n    return to_return"
    },
    {
        "original": "def max_langevin_fixed_point(x, r, m):\n    \"\"\"\n    Largest fixed point of dynamics  :math:argmax_x {h(x)=0}` estimated from polynomial :math:`h(x)`,\n    which has been fitted to the deterministic dynamics of Langevin model\n\n    .. math::\n        \\dot(x)(t) = h(x(t)) + R \\mathcal(N)(0,1)\n\n    as described by\n\n        Friedrich et al. (2000): Physics Letters A 271, p. 217-222\n        *Extracting model equations from experimental data*\n\n    For short time-series this method is highly dependent on the parameters.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param m: order of polynom to fit for estimating fixed points of dynamics\n    :type m: int\n    :param r: number of quantils to use for averaging\n    :type r: float\n\n    :return: Largest fixed point of deterministic dynamics\n    :return type: float\n    \"\"\"\n\n    coeff = _estimate_friedrich_coefficients(x, m, r)\n\n    try:\n        max_fixed_point = np.max(np.real(np.roots(coeff)))\n    except (np.linalg.LinAlgError, ValueError):\n        return np.nan\n\n    return max_fixed_point",
        "rewrite": "def max_langevin_fixed_point(x, r, m):\n    \n    coeff = _estimate_friedrich_coefficients(x, m, r)\n\n    try:\n        max_fixed_point = np.max(np.real(np.roots(coeff)))\n    except (np.linalg.LinAlgError, ValueError):\n        return np.nan\n\n    return max_fixed_point"
    },
    {
        "original": "def _obfuscate_inner(var):\n    \"\"\"\n    Recursive obfuscation of collection types.\n\n    Leaf or unknown Python types get replaced by the type name\n    Known collection types trigger recursion.\n    In the special case of mapping types, keys are not obfuscated\n    \"\"\"\n    if isinstance(var, (dict, salt.utils.odict.OrderedDict)):\n        return var.__class__((key, _obfuscate_inner(val))\n                             for key, val in six.iteritems(var))\n    elif isinstance(var, (list, set, tuple)):\n        return type(var)(_obfuscate_inner(v) for v in var)\n    else:\n        return '<{0}>'.format(var.__class__.__name__)",
        "rewrite": "def _obfuscate_inner(var):\n    if isinstance(var, (dict, salt.utils.odict.OrderedDict)):\n        return var.__class__((key, _obfuscate_inner(val))\n                             for key, val in six.iteritems(var))\n    elif isinstance(var, (list, set, tuple)):\n        return type(var)(_obfuscate_inner(v) for v in var)\n    else:\n        return '<{0}>'.format(var.__class__.__name__)"
    },
    {
        "original": "def from_list(cls, terms_list, coefficient=1.0):\n        \"\"\"\n        Allocates a Pauli Term from a list of operators and indices. This is more efficient than\n        multiplying together individual terms.\n\n        :param list terms_list: A list of tuples, e.g. [(\"X\", 0), (\"Y\", 1)]\n        :return: PauliTerm\n        \"\"\"\n        if not all([isinstance(op, tuple) for op in terms_list]):\n            raise TypeError(\"The type of terms_list should be a list of (name, index) \"\n                            \"tuples suitable for PauliTerm().\")\n\n        pterm = PauliTerm(\"I\", 0)\n        assert all([op[0] in PAULI_OPS for op in terms_list])\n\n        indices = [op[1] for op in terms_list]\n        assert all(_valid_qubit(index) for index in indices)\n\n        # this is because from_list doesn't call simplify in order to be more efficient.\n        if len(set(indices)) != len(indices):\n            raise ValueError(\"Elements of PauliTerm that are allocated using from_list must \"\n                             \"be on disjoint qubits. Use PauliTerm multiplication to simplify \"\n                             \"terms instead.\")\n\n        for op, index in terms_list:\n            if op != \"I\":\n                pterm._ops[index] = op\n        if not isinstance(coefficient, Number):\n            raise ValueError(\"coefficient of PauliTerm must be a Number.\")\n        pterm.coefficient = complex(coefficient)\n        return pterm",
        "rewrite": "def from_list(cls, terms_list, coefficient=1.0):\n    if not all([isinstance(op, tuple) for op in terms_list]):\n        raise TypeError(\"The type of terms_list should be a list of (name, index) tuples suitable for PauliTerm().\")\n\n    pterm = PauliTerm(\"I\", 0)\n    assert all([op[0] in PAULI_OPS for op in terms_list])\n\n    indices = [op[1] for op in terms_list]\n    assert all(_valid_qubit(index) for index in indices)\n\n    if len(set(indices)) != len(indices):\n        raise ValueError(\"Elements of PauliTerm that are allocated using from_list must be on disjoint qubits. Use PauliTerm multiplication to simplify terms instead.\")\n\n    for op, index in terms_list:\n        if op != \"I\":\n            pterm._ops[index] = op\n    if not isinstance(coefficient, Number):\n        raise ValueError(\"coefficient of PauliTerm must be a Number.\")\n    pterm.coefficient = complex(coefficient)\n    return pterm"
    },
    {
        "original": "def setPadding(self, pad):\n\t\t\"\"\"setPadding() -> bytes of length 1. Padding character.\"\"\"\n\t\t_baseDes.setPadding(self, pad)\n\t\tfor key in (self.__key1, self.__key2, self.__key3):\n\t\t\tkey.setPadding(pad)",
        "rewrite": "def setPadding(self, pad):\n    _baseDes.setPadding(self, pad)\n    for key in (self.__key1, self.__key2, self.__key3):\n        key.setPadding(pad)"
    },
    {
        "original": "def load_structure_from_file(context: InstaloaderContext, filename: str) -> JsonExportable:\n    \"\"\"Loads a :class:`Post`, :class:`Profile` or :class:`StoryItem` from a '.json' or '.json.xz' file that\n    has been saved by :func:`save_structure_to_file`.\n\n    :param context: :attr:`Instaloader.context` linked to the new object, used for additional queries if neccessary.\n    :param filename: Filename, ends in '.json' or '.json.xz'\n    \"\"\"\n    compressed = filename.endswith('.xz')\n    if compressed:\n        fp = lzma.open(filename, 'rt')\n    else:\n        fp = open(filename, 'rt')\n    json_structure = json.load(fp)\n    fp.close()\n    if 'node' in json_structure and 'instaloader' in json_structure and \\\n            'node_type' in json_structure['instaloader']:\n        node_type = json_structure['instaloader']['node_type']\n        if node_type == \"Post\":\n            return Post(context, json_structure['node'])\n        elif node_type == \"Profile\":\n            return Profile(context, json_structure['node'])\n        elif node_type == \"StoryItem\":\n            return StoryItem(context, json_structure['node'])\n        else:\n            raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))\n    elif 'shortcode' in json_structure:\n        # Post JSON created with Instaloader v3\n        return Post.from_shortcode(context, json_structure['shortcode'])\n    else:\n        raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))",
        "rewrite": "def load_structure_from_file(context: InstaloaderContext, filename: str) -> JsonExportable:\n    compressed = filename.endswith('.xz')\n    if compressed:\n        fp = lzma.open(filename, 'rt')\n    else:\n        fp = open(filename, 'rt')\n    json_structure = json.load(fp)\n    fp.close()\n    if 'node' in json_structure and 'instaloader' in json_structure and 'node_type' in json_structure['instaloader']:\n        node_type = json_structure['instaloader']['node_type']\n        if node_type == \"Post\":\n            return Post(context, json_structure['node'])\n        elif node_type == \"Profile\":\n            return Profile(context, json_structure['node'])\n        elif node_type == \"StoryItem\":\n            return StoryItem(context, json_structure['node'])\n        else:\n            raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))\n    elif 'shortcode' in json_structure:\n        return Post.from_shortcode(context, json_structure['shortcode'])\n    else:\n        raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))"
    },
    {
        "original": "def blit_np_array(self, array):\n    \"\"\"Fill this surface using the contents of a numpy array.\"\"\"\n    with sw(\"make_surface\"):\n      raw_surface = pygame.surfarray.make_surface(array.transpose([1, 0, 2]))\n    with sw(\"draw\"):\n      pygame.transform.scale(raw_surface, self.surf.get_size(), self.surf)",
        "rewrite": "def blit_np_array(self, array):\n    with sw(\"make_surface\"):\n        raw_surface = pygame.surfarray.make_surface(array.transpose([1, 0, 2]))\n    with sw(\"draw\"):\n        pygame.transform.scale(raw_surface, self.surf.get_size(), self.surf)"
    },
    {
        "original": "def fit(self, X, y=None):\n        \"\"\"\n        The fit method is the primary drawing input for the frequency\n        distribution visualization. It requires vectorized lists of\n        documents and a list of features, which are the actual words\n        from the original corpus (needed to label the x-axis ticks).\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features representing the corpus\n            of frequency vectorized documents.\n\n        y : ndarray or DataFrame of shape n\n            Labels for the documents for conditional frequency distribution.\n\n        .. note:: Text documents must be vectorized before ``fit()``.\n        \"\"\"\n\n        # Compute the conditional word frequency\n        if y is not None:\n            # Fit the frequencies\n            self.conditional_freqdist_ = {}\n\n            # Conditional frequency distribution\n            self.classes_ = [str(label) for label in set(y)]\n            for label in self.classes_:\n                self.conditional_freqdist_[label] = self.count(X[y == label])\n        else:\n            # No conditional frequencies\n            self.conditional_freqdist_ = None\n\n        # Frequency distribution of entire corpus.\n        self.freqdist_ = self.count(X)\n        self.sorted_ = self.freqdist_.argsort()[::-1] # Descending order\n\n        # Compute the number of words, vocab, and hapaxes\n        self.vocab_ = self.freqdist_.shape[0]\n        self.words_ = self.freqdist_.sum()\n        self.hapaxes_ = sum(1 for c in self.freqdist_ if c == 1)\n\n        # Draw and ensure that we return self\n        self.draw()\n        return self",
        "rewrite": "def fit(self, X, y=None):\n    if y is not None:\n        self.conditional_freqdist_ = {}\n        self.classes_ = [str(label) for label in set(y)]\n        for label in self.classes_:\n            self.conditional_freqdist_[label] = self.count(X[y == label])\n    else:\n        self.conditional_freqdist_ = None\n\n    self.freqdist_ = self.count(X)\n    self.sorted_ = self.freqdist_.argsort()[::-1]\n\n    self.vocab_ = self.freqdist_.shape[0]\n    self.words_ = self.freqdist_.sum()\n    self.hapaxes_ = sum(1 for c in self.freqdist_ if c == 1)\n\n    self.draw()\n    return self"
    },
    {
        "original": "def _decoder(self):\n    \"\"\"Transliterate a string from English to the target language.\"\"\"\n    if self.target_lang == 'en':\n      return Transliterator._dummy_coder\n    else:\n      weights = load_transliteration_table(self.target_lang)\n      decoder_weights = weights[\"decoder\"]\n      return Transliterator._transliterate_string(decoder_weights)",
        "rewrite": "def _decoder(self):\n    if self.target_lang == 'en':\n        return Transliterator._dummy_coder\n    else:\n        weights = load_transliteration_table(self.target_lang)\n        decoder_weights = weights[\"decoder\"]\n        return Transliterator._transliterate_string(decoder_weights)"
    },
    {
        "original": "def load_nb(cls, inline=True):\n        \"\"\"\n        Loads any resources required for display of plots\n        in the Jupyter notebook\n        \"\"\"\n        with param.logging_level('ERROR'):\n            cls.notebook_context = True\n            cls.comm_manager = JupyterCommManager",
        "rewrite": "def load_nb(cls, inline=True):\n    with param.logging_level('ERROR'):\n        cls.notebook_context = True\n        cls.comm_manager = JupyterCommManager"
    },
    {
        "original": "def gen_key(minion_id, dns_name=None, zone='default', password=None):\n    \"\"\"\n    Generate and return an private_key. If a ``dns_name`` is passed in, the\n    private_key will be cached under that name. The type of key and the\n    parameters used to generate the key are based on the default certificate\n    use policy associated with the specified zone.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run venafi.gen_key <minion_id> [dns_name] [zone] [password]\n    \"\"\"\n    # Get the default certificate use policy associated with the zone\n    # so we can generate keys that conform with policy\n\n    # The /v1/zones/tag/{name} API call is a shortcut to get the zoneID\n    # directly from the name\n\n    qdata = __utils__['http.query'](\n        '{0}/zones/tag/{1}'.format(_base_url(), zone),\n        method='GET',\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'tppl-api-key': _api_key(),\n            'Content-Type': 'application/json',\n        },\n    )\n\n    zone_id = qdata['dict']['id']\n\n    # the /v1/certificatepolicies?zoneId API call returns the default\n    # certificate use and certificate identity policies\n\n    qdata = __utils__['http.query'](\n        '{0}/certificatepolicies?zoneId={1}'.format(_base_url(), zone_id),\n        method='GET',\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'tppl-api-key': _api_key(),\n            'Content-Type': 'application/json',\n        },\n    )\n\n    policies = qdata['dict']['certificatePolicies']\n\n    # Extract the key length and key type from the certificate use policy\n    # and generate the private key accordingly\n\n    for policy in policies:\n        if policy['certificatePolicyType'] == \"CERTIFICATE_USE\":\n            keyTypes = policy['keyTypes']\n            # in case multiple keytypes and key lengths are supported\n            # always use the first key type and key length\n            keygen_type = keyTypes[0]['keyType']\n            key_len = keyTypes[0]['keyLengths'][0]\n\n    if int(key_len) < 2048:\n        key_len = 2048\n\n    if keygen_type == \"RSA\":\n        if HAS_M2:\n            gen = RSA.gen_key(key_len, 65537)\n            private_key = gen.as_pem(cipher='des_ede3_cbc', callback=lambda x: six.b(password))\n        else:\n            gen = RSA.generate(bits=key_len)\n            private_key = gen.exportKey('PEM', password)\n        if dns_name is not None:\n            bank = 'venafi/domains'\n            cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n            try:\n                data = cache.fetch(bank, dns_name)\n                data['private_key'] = private_key\n                data['minion_id'] = minion_id\n            except TypeError:\n                data = {'private_key': private_key,\n                        'minion_id': minion_id}\n            cache.store(bank, dns_name, data)\n    return private_key",
        "rewrite": "def gen_key(minion_id, dns_name=None, zone='default', password=None):\n\n    qdata = __utils__['http.query'](\n        '{0}/zones/tag/{1}'.format(_base_url(), zone),\n        method='GET',\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'tppl-api-key': _api_key(),\n            'Content-Type': 'application/json',\n        },\n    )\n\n    zone_id = qdata['dict']['id']\n\n    qdata = __utils__['http.query'](\n        '{0}/certificatepolicies?zoneId={1}'.format(_base_url(), zone_id),\n        method='GET',\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'tppl-api-key': _api_key(),\n            'Content-Type': 'application/json',\n        },\n    )\n\n    policies = qdata['dict']['certificatePolicies']\n\n    for policy in policies:\n        if policy['certificatePolicyType'] == \"CERTIFICATE_USE\":\n            keyTypes = policy['keyTypes']\n            keygen_type = keyTypes[0]['keyType']\n            key_len = keyTypes[0]['keyLengths'][0]\n\n    if int(key_len) < 2048:\n        key_len = 2048\n\n    if keygen_type == \"RSA\":\n        if HAS_M2:\n            gen = RSA.gen_key(key_len, 65537)\n            private_key = gen.as_pem(cipher='des_ede3_cbc', callback=lambda x: six.b(password))\n        else:\n            gen = RSA.generate(bits=key_len)\n            private_key = gen.exportKey('PEM', password)\n        if dns_name is not None:\n            bank = 'venafi/domains'\n            cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n            try:\n                data = cache.fetch(bank, dns_name)\n                data['private_key'] = private_key\n                data['minion_id'] = minion_id\n            except TypeError:\n                data = {'private_key': private_key,\n                        'minion_id': minion_id}\n            cache.store(bank, dns_name, data)\n    return private_key"
    },
    {
        "original": "def install_app(app, target='/Applications/'):\n    \"\"\"\n    Install an app file by moving it into the specified Applications directory\n\n    Args:\n        app (str): The location of the .app file\n        target (str): The target in which to install the package to\n                      Default is ''/Applications/''\n\n    Returns:\n        str: The results of the rsync command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macpackage.install_app /tmp/tmp.app /Applications/\n    \"\"\"\n\n    if target[-4:] != '.app':\n        if app[-1:] == '/':\n            base_app = os.path.basename(app[:-1])\n        else:\n            base_app = os.path.basename(app)\n\n        target = os.path.join(target, base_app)\n\n    if not app[-1] == '/':\n        app += '/'\n\n    cmd = 'rsync -a --delete \"{0}\" \"{1}\"'.format(app, target)\n    return __salt__['cmd.run'](cmd)",
        "rewrite": "def install_app(app, target='/Applications/'):\n    if target[-4:] != '.app':\n        if app[-1:] == '/':\n            base_app = os.path.basename(app[:-1])\n        else:\n            base_app = os.path.basename(app)\n\n        target = os.path.join(target, base_app)\n\n    if not app[-1] == '/':\n        app += '/'\n\n    cmd = 'rsync -a --delete \"{0}\" \"{1}\"'.format(app, target)\n    return __salt__['cmd.run'](cmd)"
    },
    {
        "original": "def _CreateSingleValueCondition(self, value, operator):\n    \"\"\"Creates a single-value condition with the provided value and operator.\"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      value = '\"%s\"' % value\n    return '%s %s %s' % (self._field, operator, value)",
        "rewrite": "def _CreateSingleValueCondition(self, value, operator):\n    if isinstance(value, str) or isinstance(value, unicode):\n        value = '\"%s\"' % value\n    return '%s %s %s' % (self._field, operator, value)"
    },
    {
        "original": "def _collect_result(self):\n        \"\"\"Collect test result.\n\n        Generate PDF, excel and pcap file\n        \"\"\"\n        # generate pdf\n        self._browser.find_element_by_class_name('save-pdf').click()\n        time.sleep(1)\n        try:\n            dialog = self._browser.find_element_by_id('Testinfo')\n        except:\n            logger.exception('Failed to get test info dialog.')\n        else:\n            if dialog.get_attribute('aria-hidden') != 'false':\n                raise Exception('Test information dialog not ready')\n\n            version = self.auto_dut and settings.DUT_VERSION or self.dut.version\n            dialog.find_element_by_id('inp_dut_manufacturer').send_keys(settings.DUT_MANUFACTURER)\n            dialog.find_element_by_id('inp_dut_firmware_version').send_keys(version)\n            dialog.find_element_by_id('inp_tester_name').send_keys(settings.TESTER_NAME)\n            dialog.find_element_by_id('inp_remarks').send_keys(settings.TESTER_REMARKS)\n            dialog.find_element_by_id('generatePdf').click()\n\n        time.sleep(1)\n        main_window = self._browser.current_window_handle\n\n        # generate excel\n        self._browser.find_element_by_class_name('save-excel').click()\n        time.sleep(1)\n        for window_handle in self._browser.window_handles:\n            if window_handle != main_window:\n                self._browser.switch_to.window(window_handle)\n                self._browser.close()\n        self._browser.switch_to.window(main_window)\n\n        # save pcap\n        self._browser.find_element_by_class_name('save-wireshark').click()\n        time.sleep(1)\n        for window_handle in self._browser.window_handles:\n            if window_handle != main_window:\n                self._browser.switch_to.window(window_handle)\n                self._browser.close()\n        self._browser.switch_to.window(main_window)\n\n        os.system('copy \"%%HOMEPATH%%\\\\Downloads\\\\NewPdf_*.pdf\" %s\\\\'\n                  % self.result_dir)\n        os.system('copy \"%%HOMEPATH%%\\\\Downloads\\\\ExcelReport_*.xlsx\" %s\\\\'\n                  % self.result_dir)\n        os.system('copy \"%s\\\\Captures\\\\*.pcapng\" %s\\\\'\n                  % (settings.HARNESS_HOME, self.result_dir))\n        os.system('copy \"%s\\\\Thread_Harness\\\\temp\\\\*.*\" \"%s\"'\n                  % (settings.HARNESS_HOME, self.result_dir))",
        "rewrite": "def _collect_result(self):\n    self._browser.find_element_by_class_name('save-pdf').click()\n    time.sleep(1)\n    try:\n        dialog = self._browser.find_element_by_id('Testinfo')\n    except:\n        logger.exception('Failed to get test info dialog.')\n    else:\n        if dialog.get_attribute('aria-hidden') != 'false':\n            raise Exception('Test information dialog not ready')\n\n        version = self.auto_dut and settings.DUT_VERSION or self.dut.version\n        dialog.find_element_by_id('inp_dut_manufacturer').send_keys(settings.DUT_MANUFACTURER)\n        dialog.find_element_by_id('inp_dut_firmware_version').send_keys(version)\n        dialog.find_element_by_id('inp_tester_name').send_keys(settings.TESTER_NAME)\n        dialog.find_element_by_id('inp_remarks').send_keys(settings.TESTER_REMARKS)\n        dialog.find_element_by_id('generatePdf').click()\n\n    time.sleep(1)\n    main_window = self._browser.current_window_handle\n\n    self._browser.find_element_by_class_name('save-excel').click()\n    time.sleep(1)\n    for window_handle in self._browser.window_handles:\n        if window_handle != main_window:\n            self._browser.switch_to.window(window_handle)\n            self._browser.close()\n    self._browser.switch_to.window(main_window)\n\n    self._browser.find_element_by_class_name('save-wireshark').click()\n    time.sleep(1)\n    for window_handle in self._browser.window_handles:\n        if window_handle != main_window:\n            self._browser.switch_to.window(window_handle)\n            self._browser.close()\n    self._browser.switch_to.window(main_window)\n\n    os.system('copy \"%%HOMEPATH%%\\\\Downloads\\\\NewPdf_*.pdf\" %s\\\\' % self.result_dir)\n    os.system('copy \"%%HOMEPATH%%\\\\Downloads\\\\ExcelReport_*.xlsx\" %s\\\\' % self.result_dir)\n    os.system('copy \"%s\\\\Captures\\\\*.pcapng\" %s\\\\' % (settings.HARNESS_HOME, self.result_dir))\n    os.system('copy \"%s\\\\Thread_Harness\\\\temp\\\\*.*\" \"%s\"' % (settings.HARNESS_HOME, self.result_dir))"
    },
    {
        "original": "def show_tricky_tasks(self, verbose=0):\n        \"\"\"\n        Print list of tricky tasks i.e. tasks that have been restarted or\n        launched more than once or tasks with corrections.\n\n        Args:\n            verbose: Verbosity level. If > 0, task history and corrections (if any) are printed.\n        \"\"\"\n        nids, tasks = [], []\n        for task in self.iflat_tasks():\n            if task.num_launches > 1 or any(n > 0 for n in (task.num_restarts, task.num_corrections)):\n                nids.append(task.node_id)\n                tasks.append(task)\n\n        if not nids:\n            cprint(\"Everything's fine, no tricky tasks found\", color=\"green\")\n        else:\n            self.show_status(nids=nids)\n            if not verbose:\n                print(\"Use --verbose to print task history.\")\n                return\n\n            for nid, task in zip(nids, tasks):\n                cprint(repr(task), **task.status.color_opts)\n                self.show_history(nids=[nid], full_history=False, metadata=False)\n                #if task.num_restarts:\n                #    self.show_restarts(nids=[nid])\n                if task.num_corrections:\n                    self.show_corrections(nids=[nid])",
        "rewrite": "def show_tricky_tasks(self, verbose=0):\n    nids, tasks = [], []\n    for task in self.iflat_tasks():\n        if task.num_launches > 1 or any(n > 0 for n in (task.num_restarts, task.num_corrections)):\n            nids.append(task.node_id)\n            tasks.append(task)\n\n    if not nids:\n        cprint(\"Everything's fine, no tricky tasks found\", color=\"green\")\n    else:\n        self.show_status(nids=nids)\n        if not verbose:\n            print(\"Use --verbose to print task history.\")\n            return\n\n        for nid, task in zip(nids, tasks):\n            cprint(repr(task), **task.status.color_opts)\n            self.show_history(nids=[nid], full_history=False, metadata=False)\n            # if task.num_restarts:\n            #    self.show_restarts(nids=[nid])\n            if task.num_corrections:\n                self.show_corrections(nids=[nid])"
    },
    {
        "original": "def gte(name, value):\n    \"\"\"\n    Only succeed if the value in the given register location is greater or equal\n    than the given value\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        foo:\n          check.gte:\n            - value: 42\n\n        run_remote_ex:\n          local.cmd:\n            - tgt: '*'\n            - func: test.ping\n            - require:\n              - check: foo\n    \"\"\"\n    ret = {'name': name,\n           'result': False,\n           'comment': '',\n           'changes': {}}\n    if name not in __reg__:\n        ret['result'] = False\n        ret['comment'] = 'Value {0} not in register'.format(name)\n        return ret\n    if __reg__[name]['val'] >= value:\n        ret['result'] = True\n    return ret",
        "rewrite": "def gte(name, value):\n    ret = {'name': name, 'result': False, 'comment': '', 'changes': {}}\n    if name not in __reg__:\n        ret['result'] = False\n        ret['comment'] = 'Value {0} not in register'.format(name)\n        return ret\n    if __reg__[name]['val'] >= value:\n        ret['result'] = True\n    return ret"
    },
    {
        "original": "def _IndexedScan(self, i, max_records=None):\n    \"\"\"Scan records starting with index i.\"\"\"\n    self._ReadIndex()\n\n    # The record number that we will read next.\n    idx = 0\n    # The timestamp that we will start reading from.\n    start_ts = 0\n    if i >= self._max_indexed:\n      start_ts = max((0, 0), (self._index[self._max_indexed][0],\n                              self._index[self._max_indexed][1] - 1))\n      idx = self._max_indexed\n    else:\n      try:\n        possible_idx = i - i % self.INDEX_SPACING\n        start_ts = (max(0, self._index[possible_idx][0]),\n                    self._index[possible_idx][1] - 1)\n        idx = possible_idx\n      except KeyError:\n        pass\n\n    if max_records is not None:\n      max_records += i - idx\n\n    with data_store.DB.GetMutationPool() as mutation_pool:\n      for (ts, value) in self.Scan(\n          after_timestamp=start_ts,\n          max_records=max_records,\n          include_suffix=True):\n        self._MaybeWriteIndex(idx, ts, mutation_pool)\n        if idx >= i:\n          yield (idx, ts, value)\n        idx += 1",
        "rewrite": "def _IndexedScan(self, i, max_records=None):\n    self._ReadIndex()\n    \n    idx = 0\n    start_ts = 0\n    if i >= self._max_indexed:\n        start_ts = max(0, self._index[self._max_indexed][0], self._index[self._max_indexed][1] - 1)\n        idx = self._max_indexed\n    else:\n        try:\n            possible_idx = i - i % self.INDEX_SPACING\n            start_ts = max(0, self._index[possible_idx][0], self._index[possible_idx][1] - 1)\n            idx = possible_idx\n        except KeyError:\n            pass\n\n    if max_records is not None:\n        max_records += i - idx\n\n    with data_store.DB.GetMutationPool() as mutation_pool:\n        for (ts, value) in self.Scan(\n            after_timestamp=start_ts,\n            max_records=max_records,\n            include_suffix=True):\n            self._MaybeWriteIndex(idx, ts, mutation_pool)\n            if idx >= i:\n                yield (idx, ts, value)\n            idx += 1"
    },
    {
        "original": "def use_general_term_frequencies(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tPriorFactory\n\t\t\"\"\"\n\t\ttdf = self._get_relevant_term_freq()\n\t\tbg_df = self.term_doc_mat.get_term_and_background_counts()[['background']]\n\t\tbg_df = pd.merge(tdf,\n\t\t                 bg_df,\n\t\t                 left_index=True,\n\t\t                 right_index=True,\n\t\t                 how='left').fillna(0.)\n\t\tself._store_priors_from_background_dataframe(bg_df)\n\t\treturn self",
        "rewrite": "def use_general_term_frequencies(self):\n    tdf = self._get_relevant_term_freq()\n    bg_df = self.term_doc_mat.get_term_and_background_counts()[['background']]\n    bg_df = pd.merge(tdf, bg_df, left_index=True, right_index=True, how='left').fillna(0.)\n    self._store_priors_from_background_dataframe(bg_df)\n    return self"
    },
    {
        "original": "def _populate_sgc_payoff_arrays(payoff_arrays):\n    \"\"\"\n    Populate the ndarrays in `payoff_arrays` with the payoff values of\n    the SGC game.\n\n    Parameters\n    ----------\n    payoff_arrays : tuple(ndarray(float, ndim=2))\n        Tuple of 2 ndarrays of shape (4*k-1, 4*k-1). Modified in place.\n\n    \"\"\"\n    n = payoff_arrays[0].shape[0]  # 4*k-1\n    m = (n+1)//2 - 1  # 2*k-1\n    for payoff_array in payoff_arrays:\n        for i in range(m):\n            for j in range(m):\n                payoff_array[i, j] = 0.75\n            for j in range(m, n):\n                payoff_array[i, j] = 0.5\n        for i in range(m, n):\n            for j in range(n):\n                payoff_array[i, j] = 0\n\n        payoff_array[0, m-1] = 1\n        payoff_array[0, 1] = 0.5\n        for i in range(1, m-1):\n            payoff_array[i, i-1] = 1\n            payoff_array[i, i+1] = 0.5\n        payoff_array[m-1, m-2] = 1\n        payoff_array[m-1, 0] = 0.5\n\n    k = (m+1)//2\n    for h in range(k):\n        i, j = m + 2*h, m + 2*h\n        payoff_arrays[0][i, j] = 0.75\n        payoff_arrays[0][i+1, j+1] = 0.75\n        payoff_arrays[1][j, i+1] = 0.75\n        payoff_arrays[1][j+1, i] = 0.75",
        "rewrite": "def _populate_sgc_payoff_arrays(payoff_arrays):\n    n = payoff_arrays[0].shape[0]  # 4*k-1\n    m = (n+1)//2 - 1  # 2*k-1\n    for payoff_array in payoff_arrays:\n        for i in range(m):\n            for j in range(m):\n                payoff_array[i, j] = 0.75\n            for j in range(m, n):\n                payoff_array[i, j] = 0.5\n        for i in range(m, n):\n            for j in range(n):\n                payoff_array[i, j] = 0\n\n        payoff_array[0, m-1] = 1\n        payoff_array[0, 1] = 0.5\n        for i in range(1, m-1):\n            payoff_array[i, i-1] = 1\n            payoff_array[i, i+1] = 0.5\n        payoff_array[m-1, m-2] = 1\n        payoff_array[m-1, 0] = 0.5\n\n    k = (m+1)//2\n    for h in range(k):\n        i, j = m + 2*h, m + 2*h\n        payoff_arrays[0][i, j] = 0.75\n        payoff_arrays[0][i+1, j+1] = 0.75\n        payoff_arrays[1][j, i+1] = 0.75\n        payoff_arrays[1][j+1, i] = 0.75"
    },
    {
        "original": "def text(self):\n        \"\"\"\n        A string representing the textual content of this run, with content\n        child elements like ``<w:tab/>`` translated to their Python\n        equivalent.\n        \"\"\"\n        text = ''\n        for child in self:\n            if child.tag == qn('w:t'):\n                t_text = child.text\n                text += t_text if t_text is not None else ''\n            elif child.tag == qn('w:tab'):\n                text += '\\t'\n            elif child.tag in (qn('w:br'), qn('w:cr')):\n                text += '\\n'\n        return text",
        "rewrite": "def text(self):\n    text = ''\n    for child in self:\n        if child.tag == qn('w:t'):\n            t_text = child.text\n            text += t_text if t_text is not None else ''\n        elif child.tag == qn('w:tab'):\n            text += '\\t'\n        elif child.tag in (qn('w:br'), qn('w:cr')):\n            text += '\\n'\n    return text"
    },
    {
        "original": "def _get_object(data, position, obj_end, opts, dummy):\n    \"\"\"Decode a BSON subdocument to opts.document_class or bson.dbref.DBRef.\"\"\"\n    obj_size = _UNPACK_INT(data[position:position + 4])[0]\n    end = position + obj_size - 1\n    if data[end:position + obj_size] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    if end >= obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    if _raw_document_class(opts.document_class):\n        return (opts.document_class(data[position:end + 1], opts),\n                position + obj_size)\n\n    obj = _elements_to_dict(data, position + 4, end, opts)\n\n    position += obj_size\n    if \"$ref\" in obj:\n        return (DBRef(obj.pop(\"$ref\"), obj.pop(\"$id\", None),\n                      obj.pop(\"$db\", None), obj), position)\n    return obj, position",
        "rewrite": "def _get_object(data, position, obj_end, opts, dummy):\n    obj_size = _UNPACK_INT(data[position:position + 4])[0]\n    end = position + obj_size - 1\n    if data[end:position + obj_size] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    if end >= obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    if _raw_document_class(opts.document_class):\n        return (opts.document_class(data[position:end + 1], opts),\n                position + obj_size)\n\n    obj = _elements_to_dict(data, position + 4, end, opts)\n\n    position += obj_size\n    if \"$ref\" in obj:\n        return (DBRef(obj.pop(\"$ref\"), obj.pop(\"$id\", None),\n                      obj.pop(\"$db\", None), obj), position)\n    return obj, position"
    },
    {
        "original": "def AdjustDescriptor(self, fields):\n    \"\"\"Payload-aware metadata processor.\"\"\"\n\n    for f in fields:\n      if f.name == \"args_rdf_name\":\n        f.name = \"payload_type\"\n\n      if f.name == \"args\":\n        f.name = \"payload\"\n\n    return fields",
        "rewrite": "def adjust_descriptor(self, fields):\n    for f in fields:\n        if f.name == \"args_rdf_name\":\n            f.name = \"payload_type\"\n        if f.name == \"args\":\n            f.name = \"payload\"\n    \n    return fields"
    },
    {
        "original": "def ProcessListDirectory(self, responses):\n    \"\"\"Processes the results of the ListDirectory client action.\n\n    Args:\n      responses: a flow Responses object.\n    \"\"\"\n    if not responses.success:\n      raise flow.FlowError(\"Unable to list directory.\")\n\n    with data_store.DB.GetMutationPool() as pool:\n      for response in responses:\n        stat_entry = rdf_client_fs.StatEntry(response)\n        filesystem.CreateAFF4Object(\n            stat_entry, self.client_urn, pool, token=self.token)\n        self.SendReply(stat_entry)",
        "rewrite": "def ProcessListDirectory(self, responses):\n    if not responses.success:\n        raise flow.FlowError(\"Unable to list directory.\")\n\n    with data_store.DB.GetMutationPool() as pool:\n        for response in responses:\n            stat_entry = rdf_client_fs.StatEntry(response)\n            filesystem.CreateAFF4Object(\n                stat_entry, self.client_urn, pool, token=self.token)\n            self.SendReply(stat_entry)"
    },
    {
        "original": "def enable_plugin(self, name, timeout=0):\n        \"\"\"\n            Enable an installed plugin.\n\n            Args:\n                name (string): The name of the plugin. The ``:latest`` tag is\n                    optional, and is the default if omitted.\n                timeout (int): Operation timeout (in seconds). Default: 0\n\n            Returns:\n                ``True`` if successful\n        \"\"\"\n        url = self._url('/plugins/{0}/enable', name)\n        params = {'timeout': timeout}\n        res = self._post(url, params=params)\n        self._raise_for_status(res)\n        return True",
        "rewrite": "def enable_plugin(self, name, timeout=0):\n    url = self._url('/plugins/{0}/enable', name)\n    params = {'timeout': timeout}\n    res = self._post(url, params=params)\n    self._raise_for_status(res)\n    return True"
    },
    {
        "original": "def _set_residue_map(self):\n        \"\"\"\n        map each residue to the corresponding molecule.\n        \"\"\"\n        self.map_residue_to_mol = {}\n        lookup = {}\n        for idx, mol in enumerate(self.mols):\n            if not mol.formula in lookup:\n                mol.translate_sites(indices=range(len(mol)),\n                                    vector=-mol.center_of_mass)\n                lookup[mol.formula] = mol.copy()\n            self.map_residue_to_mol[\"ml{}\".format(idx + 1)] = lookup[mol.formula]",
        "rewrite": "def _set_residue_map(self):\n        self.map_residue_to_mol = {}\n        lookup = {}\n        for idx, mol in enumerate(self.mols):\n            if mol.formula not in lookup:\n                mol.translate_sites(indices=range(len(mol)), vector=-mol.center_of_mass)\n                lookup[mol.formula] = mol.copy()\n            self.map_residue_to_mol[\"ml{}\".format(idx + 1)] = lookup[mol.formula]"
    },
    {
        "original": "def fit(self, y_train, y_test=None):\n        \"\"\"\n        Fit the visualizer to the the target variables, which must be 1D\n        vectors containing discrete (classification) data. Fit has two modes:\n\n        1. Balance mode: if only y_train is specified\n        2. Compare mode: if both train and test are specified\n\n        In balance mode, the bar chart is displayed with each class as its own\n        color. In compare mode, a side-by-side bar chart is displayed colored\n        by train or test respectively.\n\n        Parameters\n        ----------\n        y_train : array-like\n            Array or list of shape (n,) that containes discrete data.\n\n        y_test : array-like, optional\n            Array or list of shape (m,) that contains discrete data. If\n            specified, the bar chart will be drawn in compare mode.\n        \"\"\"\n\n        # check to make sure that y_train is not a 2D array, e.g. X\n        if y_train.ndim == 2:\n            raise YellowbrickValueError((\n                \"fit has changed to only require a 1D array, y \"\n                \"since version 0.9; please see the docs for more info\"\n            ))\n\n        # Check the target types for the y variables\n        self._validate_target(y_train)\n        self._validate_target(y_test)\n\n        # Get the unique values from the dataset\n        targets = (y_train,) if y_test is None else (y_train, y_test)\n        self.classes_ = unique_labels(*targets)\n\n        # Validate the classes with the class names\n        if self.labels is not None:\n            if len(self.labels) != len(self.classes_):\n                raise YellowbrickValueError((\n                    \"discovered {} classes in the data, does not match \"\n                    \"the {} labels specified.\"\n                ).format(len(self.classes_), len(self.labels)))\n\n        # Determine if we're in compare or balance mode\n        self._mode = BALANCE if y_test is None else COMPARE\n\n        # Compute the support values\n        if self._mode == BALANCE:\n            self.support_ = np.array([\n                (y_train == idx).sum() for idx in self.classes_\n            ])\n\n        else:\n            self.support_ = np.array([\n                [\n                    (y == idx).sum() for idx in self.classes_\n                ]\n                for y in targets\n            ])\n\n        # Draw the bar chart\n        self.draw()\n\n        # Fit returns self\n        return self",
        "rewrite": "def fit(self, y_train, y_test=None):\n\n    if y_train.ndim == 2:\n        raise YellowbrickValueError((\n            \"fit has changed to only require a 1D array, y \"\n            \"since version 0.9; please see the docs for more info\"\n        ))\n\n    self._validate_target(y_train)\n    self._validate_target(y_test)\n\n    targets = (y_train,) if y_test is None else (y_train, y_test)\n    self.classes_ = unique_labels(*targets)\n\n    if self.labels is not None:\n        if len(self.labels) != len(self.classes_):\n            raise YellowbrickValueError((\n                \"discovered {} classes in the data, does not match \"\n                \"the {} labels specified.\"\n            ).format(len(self.classes_), len(self.labels)))\n\n    self._mode = BALANCE if y_test is None else COMPARE\n\n    if self._mode == BALANCE:\n        self.support_ = np.array([\n            (y_train == idx).sum() for idx in self.classes_\n        ])\n\n    else:\n        self.support_ = np.array([\n            [\n                (y == idx).sum() for idx in self.classes_\n            ]\n            for y in targets\n        ])\n\n    self.draw()\n    return self"
    },
    {
        "original": "def _collect_peers_of_interest(self, new_best_path):\n        \"\"\"Collect all peers that qualify for sharing a path with given RTs.\n        \"\"\"\n        path_rts = new_best_path.get_rts()\n        qualified_peers = set(self._peers.values())\n\n        # Filter out peers based on RTC_AS setting if path is for RT_NLRI\n        qualified_peers = self._rt_manager.filter_by_origin_as(\n            new_best_path, qualified_peers\n        )\n\n        # We continue to filter out qualified peer based on path RTs\n        # If new best path has RTs, we need to share this UPDATE with\n        # qualifying peers\n        if path_rts:\n            # We add Default_RTC_NLRI to path RTs so that we can send it to\n            # peers that have expressed interest in all paths\n            path_rts.append(RouteTargetMembershipNLRI.DEFAULT_RT)\n            # All peers that do not have RTC capability qualify\n            qualified_peers = set(self._get_non_rtc_peers())\n            # Peers that have RTC capability and have common RT with the path\n            # also qualify\n            peer_to_rtfilter_map = self._peer_to_rtfilter_map\n            for peer, rt_filter in peer_to_rtfilter_map.items():\n                # Ignore Network Controller (its not a BGP peer)\n                if peer is None:\n                    continue\n\n                if rt_filter is None:\n                    qualified_peers.add(peer)\n                elif rt_filter.intersection(path_rts):\n                    qualified_peers.add(peer)\n\n        return qualified_peers",
        "rewrite": "def _collect_peers_of_interest(self, new_best_path):\n    path_rts = new_best_path.get_rts()\n    qualified_peers = set(self._peers.values())\n    \n    qualified_peers = self._rt_manager.filter_by_origin_as(\n        new_best_path, qualified_peers\n    )\n    \n    if path_rts:\n        path_rts.append(RouteTargetMembershipNLRI.DEFAULT_RT)\n        qualified_peers = set(self._get_non_rtc_peers())\n        \n        for peer, rt_filter in self._peer_to_rtfilter_map.items():\n            if peer is not None:\n                if rt_filter is None:\n                    qualified_peers.add(peer)\n                elif rt_filter.intersection(path_rts):\n                    qualified_peers.add(peer)\n    \n    return qualified_peers"
    },
    {
        "original": "def _send_command(self, cmd=\"\"):\n        \"\"\"\n        Handle reading/writing channel directly. It is also sanitizing the output received.\n\n        Parameters\n        ----------\n        cmd : str, optional\n            The command to send to the remote device (default : \"\", just send a new line)\n\n        Returns\n        -------\n        output : str\n            The output from the command sent\n        \"\"\"\n        self.connection.write_channel(cmd + \"\\n\")\n        time.sleep(1)\n        output = self.connection._read_channel_timing()\n        output = self.connection.strip_ansi_escape_codes(output)\n        output = self.connection.strip_backspaces(output)\n        return output",
        "rewrite": "def _send_command(self, cmd=\"\"):\n    self.connection.write_channel(cmd + \"\\n\")\n    time.sleep(1)\n    output = self.connection._read_channel_timing()\n    output = self.connection.strip_ansi_escape_codes(output)\n    output = self.connection.strip_backspaces(output)\n    return output"
    },
    {
        "original": "def lookup_users(self, user_ids=None, screen_names=None, include_entities=None, tweet_mode=None):\n        \"\"\" Perform bulk look up of users from user ID or screen_name \"\"\"\n        post_data = {}\n        if include_entities is not None:\n            include_entities = 'true' if include_entities else 'false'\n            post_data['include_entities'] = include_entities\n        if user_ids:\n            post_data['user_id'] = list_to_csv(user_ids)\n        if screen_names:\n            post_data['screen_name'] = list_to_csv(screen_names)\n        if tweet_mode:\n            post_data['tweet_mode'] = tweet_mode\n\n        return self._lookup_users(post_data=post_data)",
        "rewrite": "def lookup_users(self, user_ids=None, screen_names=None, include_entities=None, tweet_mode=None):\n    post_data = {}\n    if include_entities is not None:\n        include_entities = 'true' if include_entities else 'false'\n        post_data['include_entities'] = include_entities\n    if user_ids:\n        post_data['user_id'] = list_to_csv(user_ids)\n    if screen_names:\n        post_data['screen_name'] = list_to_csv(screen_names)\n    if tweet_mode:\n        post_data['tweet_mode'] = tweet_mode\n\n    return self._lookup_users(post_data=post_data)"
    },
    {
        "original": "def angvel(target, current, scale):\n    \"\"\"Use sigmoid function to choose a delta that will help smoothly steer from current angle to target angle.\"\"\"\n    delta = target - current\n    while delta < -180:\n        delta += 360;\n    while delta > 180:\n        delta -= 360;\n    return (old_div(2.0, (1.0 + math.exp(old_div(-delta,scale))))) - 1.0",
        "rewrite": "def angvel(target, current, scale):\n    delta = target - current\n    while delta < -180:\n        delta += 360\n    while delta > 180:\n        delta -= 360\n    return (2.0 / (1.0 + math.exp(-delta/scale))) - 1.0"
    },
    {
        "original": "def deep_copy(item_original):\n    \"\"\"Return a recursive deep-copy of item where each copy has a new ID.\"\"\"\n    item = copy.copy(item_original)\n    item._id = uuid.uuid4().hex\n    if hasattr(item, '_children') and len(item._children) > 0:\n        children_new = collections.OrderedDict()\n        for subitem_original in item._children.values():\n            subitem = deep_copy(subitem_original)\n            subitem._parent = item\n            children_new[subitem.get_name()] = subitem\n        item._children = children_new\n    return item",
        "rewrite": "import copy\nimport uuid\nimport collections\n\ndef deep_copy(item_original):\n    item = copy.copy(item_original)\n    item._id = uuid.uuid4().hex\n    if hasattr(item, '_children') and len(item._children) > 0:\n        children_new = collections.OrderedDict()\n        for subitem_original in item._children.values():\n            subitem = deep_copy(subitem_original)\n            subitem._parent = item\n            children_new[subitem.get_name()] = subitem\n        item._children = children_new\n    return item"
    },
    {
        "original": "def _parse_json(self, json, exactly_one=True):\n        \"\"\"Returns location, (latitude, longitude) from json feed.\"\"\"\n        features = json['features']\n        if features == []:\n            return None\n\n        def parse_feature(feature):\n            location = feature['place_name']\n            place = feature['text']\n            longitude = feature['geometry']['coordinates'][0]\n            latitude = feature['geometry']['coordinates'][1]\n            return Location(location, (latitude, longitude), place)\n        if exactly_one:\n            return parse_feature(features[0])\n        else:\n            return [parse_feature(feature) for feature in features]",
        "rewrite": "def _parse_json(self, json, exactly_one=True):\n    features = json['features']\n    if features == []:\n        return None\n\n    def parse_feature(feature):\n        location = feature['place_name']\n        place = feature['text']\n        longitude = feature['geometry']['coordinates'][0]\n        latitude = feature['geometry']['coordinates'][1]\n        return Location(location, (latitude, longitude), place)\n\n    if exactly_one:\n        return parse_feature(features[0])\n    else:\n        return [parse_feature(feature) for feature in features]"
    },
    {
        "original": "def _get_overlaps_tensor(self, L):\n        \"\"\"Transforms the input label matrix to a three-way overlaps tensor.\n\n        Args:\n            L: (np.array) An n x m array of LF output labels, in {0,...,k} if\n                self.abstains, else in {1,...,k}, generated by m conditionally\n                independent LFs on n data points\n\n        Outputs:\n            O: (torch.Tensor) A (m, m, m, k, k, k) tensor of the label-specific\n            empirical overlap rates; that is,\n\n                O[i,j,k,y1,y2,y3] = P(\\lf_i = y1, \\lf_j = y2, \\lf_k = y3)\n\n            where this quantity is computed empirically by this function, based\n            on the label matrix L.\n        \"\"\"\n        n, m = L.shape\n\n        # Convert from a (n,m) matrix of ints to a (k_lf, n, m) indicator tensor\n        LY = np.array([np.where(L == y, 1, 0) for y in range(self.k_0, self.k + 1)])\n\n        # Form the three-way overlaps matrix\n        O = np.einsum(\"abc,dbe,fbg->cegadf\", LY, LY, LY) / n\n        return torch.from_numpy(O).float()",
        "rewrite": "def _get_overlaps_tensor(self, L):\n    n, m = L.shape\n    LY = np.array([np.where(L == y, 1, 0) for y in range(self.k_0, self.k + 1)])\n    O = np.einsum(\"abc,dbe,fbg->cegadf\", LY, LY, LY) / n\n    return torch.from_numpy(O).float()"
    },
    {
        "original": "def _update_data_dict(self, data_dict, back_or_front):\n        \"\"\"\n        Adds spct if relevant, adds service\n        \"\"\"\n        data_dict['back_or_front'] = back_or_front\n        # The percentage of used sessions based on 'scur' and 'slim'\n        if 'slim' in data_dict and 'scur' in data_dict:\n            try:\n                data_dict['spct'] = (data_dict['scur'] / data_dict['slim']) * 100\n            except (TypeError, ZeroDivisionError):\n                pass",
        "rewrite": "def _update_data_dict(self, data_dict, back_or_front):\n    data_dict['back_or_front'] = back_or_front\n    if 'slim' in data_dict and 'scur' in data_dict:\n        try:\n            data_dict['spct'] = (data_dict['scur'] / data_dict['slim']) * 100\n        except (TypeError, ZeroDivisionError):\n            pass"
    },
    {
        "original": "def ParseSudoersEntry(self, entry, sudoers_config):\n    \"\"\"Parse an entry and add it to the given SudoersConfig rdfvalue.\"\"\"\n\n    key = entry[0]\n    if key in SudoersFieldParser.ALIAS_TYPES:\n      # Alias.\n      alias_entry = rdf_config_file.SudoersAlias(\n          type=SudoersFieldParser.ALIAS_TYPES.get(key), name=entry[1])\n\n      # Members of this alias, comma-separated.\n      members, _ = self._ExtractList(entry[2:], ignores=(\",\", \"=\"))\n      field = SudoersFieldParser.ALIAS_FIELDS.get(key)\n      getattr(alias_entry, field).Extend(members)\n\n      sudoers_config.aliases.append(alias_entry)\n    elif key.startswith(SudoersFieldParser.DEFAULTS_KEY):\n      # Default.\n      # Identify scope if one exists (Defaults<scope> ...)\n      scope = None\n      if len(key) > len(SudoersFieldParser.DEFAULTS_KEY):\n        scope = key[len(SudoersFieldParser.DEFAULTS_KEY) + 1:]\n\n      # There can be multiple defaults on a line, for the one scope.\n      entry = entry[1:]\n      defaults, _ = self._ExtractList(entry)\n      for default in defaults:\n        default_entry = rdf_config_file.SudoersDefault(scope=scope)\n\n        # Extract key name and value(s).\n        default_name = default\n        value = []\n        if \"=\" in default_name:\n          default_name, remainder = default_name.split(\"=\", 1)\n          value = [remainder]\n        default_entry.name = default_name\n        if entry:\n          default_entry.value = \" \".join(value)\n\n        sudoers_config.defaults.append(default_entry)\n    elif key in SudoersFieldParser.INCLUDE_KEYS:\n      # TODO(user): make #includedir more obvious in the RDFValue somewhere\n      target = \" \".join(entry[1:])\n      sudoers_config.includes.append(target)\n    else:\n      users, entry = self._ExtractList(entry)\n      hosts, entry = self._ExtractList(entry, terminators=(\"=\",))\n\n      # Remove = from <user> <host> = <specs>\n      if entry[0] == \"=\":\n        entry = entry[1:]\n\n      # Command specification.\n      sudoers_entry = rdf_config_file.SudoersEntry(\n          users=users, hosts=hosts, cmdspec=entry)\n\n      sudoers_config.entries.append(sudoers_entry)",
        "rewrite": "def parse_sudoers_entry(self, entry, sudoers_config):\n    key = entry[0]\n    if key in SudoersFieldParser.ALIAS_TYPES:\n        alias_entry = rdf_config_file.SudoersAlias(\n            type=SudoersFieldParser.ALIAS_TYPES.get(key), name=entry[1])\n\n        members, _ = self._ExtractList(entry[2:], ignores=(\",\", \"=\"))\n        field = SudoersFieldParser.ALIAS_FIELDS.get(key)\n        getattr(alias_entry, field).Extend(members)\n\n        sudoers_config.aliases.append(alias_entry)\n    elif key.startswith(SudoersFieldParser.DEFAULTS_KEY):\n        scope = None\n        if len(key) > len(SudoersFieldParser.DEFAULTS_KEY):\n            scope = key[len(SudoersFieldParser.DEFAULTS_KEY) + 1:]\n\n        entry = entry[1:]\n        defaults, _ = self._ExtractList(entry)\n        for default in defaults:\n            default_entry = rdf_config_file.SudoersDefault(scope=scope)\n\n            default_name = default\n            value = []\n            if \"=\" in default_name:\n                default_name, remainder = default_name.split(\"=\", 1)\n                value = [remainder]\n            default_entry.name = default_name\n            if entry:\n                default_entry.value = \" \".join(value)\n\n            sudoers_config.defaults.append(default_entry)\n    elif key in SudoersFieldParser.INCLUDE_KEYS:\n        target = \" \".join(entry[1:])\n        sudoers_config.includes.append(target)\n    else:\n        users, entry = self._ExtractList(entry)\n        hosts, entry = self._ExtractList(entry, terminators=(\"=\",))\n        \n        if entry[0] == \"=\":\n            entry = entry[1:]\n\n        sudoers_entry = rdf_config_file.SudoersEntry(\n            users=users, hosts=hosts, cmdspec=entry)\n\n        sudoers_config.entries.append(sudoers_entry)"
    },
    {
        "original": "def add_routes(app):\n    \"\"\"Add the routes to an app\"\"\"\n    for (prefix, routes) in API_SECTIONS:\n        api = Api(app, prefix=prefix)\n        for ((pattern, resource, *args), kwargs) in routes:\n            kwargs.setdefault('strict_slashes', False)\n            api.add_resource(resource, pattern, *args, **kwargs)",
        "rewrite": "def add_routes(app):\n    \"\"\"Add routes to an app\"\"\"\n    for (prefix, routes) in API_SECTIONS:\n        api = Api(app, prefix=prefix)\n        for ((pattern, resource, *args), kwargs) in routes:\n            kwargs.setdefault('strict_slashes', False)\n            api.add_resource(resource, pattern, *args, **kwargs)"
    },
    {
        "original": "def rgba_to_rgb(color, bg='rgb(255,255,255)'):\n    \"\"\"\n    Converts from rgba to rgb\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation in rgba\n            bg : string\n                    Color representation in rgb\n\n    Example:\n            rgba_to_rgb('rgb(23,25,24,.4)''\n    \"\"\"\n    def c_tup(c):\n        return eval(c[c.find('('):])\n    color = c_tup(color)\n    bg = hex_to_rgb(normalize(bg))\n    bg = c_tup(bg)\n    a = color[3]\n    r = [int((1 - a) * bg[i] + a * color[i]) for i in range(3)]\n    return 'rgb' + str(tuple(r))",
        "rewrite": "def rgba_to_rgb(color, bg='rgb(255,255,255)'):\n    def c_tup(c):\n        return eval(c[c.find('('):])\n    color = c_tup(color)\n    bg = hex_to_rgb(normalize(bg))\n    bg = c_tup(bg)\n    a = color[3]\n    r = [int((1 - a) * bg[i] + a * color[i]) for i in range(3)]\n    return 'rgb' + str(tuple(r))"
    },
    {
        "original": "def get_whitelisted_statements(self, addr):\n        \"\"\"\n        :returns: True if all statements are whitelisted\n        \"\"\"\n        if addr in self._run_statement_whitelist:\n            if self._run_statement_whitelist[addr] is True:\n                return None # This is the default value used to say\n                            # we execute all statements in this basic block. A\n                            # little weird...\n\n            else:\n                return self._run_statement_whitelist[addr]\n\n        else:\n            return []",
        "rewrite": "def get_whitelisted_statements(self, addr):\n    if addr in self._run_statement_whitelist:\n        if self._run_statement_whitelist[addr] is True:\n            return None\n        else:\n            return self._run_statement_whitelist[addr]\n    else:\n        return []"
    },
    {
        "original": "def absolute_redirect_n_times(n):\n    \"\"\"Absolutely 302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n\n    assert n > 0\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=True))\n\n    return _redirect(\"absolute\", n, True)",
        "rewrite": "def absolute_redirect_n_times(n):\n    assert n > 0\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=True))\n\n    return _redirect(\"absolute\", n, True)"
    },
    {
        "original": "def _write_regpol_data(data_to_write,\n                       policy_file_path,\n                       gpt_ini_path,\n                       gpt_extension,\n                       gpt_extension_guid):\n    \"\"\"\n    helper function to actually write the data to a Registry.pol file\n\n    also updates/edits the gpt.ini file to include the ADM policy extensions\n    to let the computer know user and/or machine registry policy files need\n    to be processed\n\n    data_to_write: data to write into the user/machine registry.pol file\n    policy_file_path: path to the registry.pol file\n    gpt_ini_path: path to gpt.ini file\n    gpt_extension: gpt extension list name from _policy_info class for this registry class gpt_extension_location\n    gpt_extension_guid: admx registry extension guid for the class\n    \"\"\"\n    try:\n        reg_pol_header = u'\\u5250\\u6765\\x01\\x00'\n        if not os.path.exists(policy_file_path):\n            __salt__['file.makedirs'](policy_file_path)\n        with salt.utils.files.fopen(policy_file_path, 'wb') as pol_file:\n            if not data_to_write.startswith(reg_pol_header.encode('utf-16-le')):\n                pol_file.write(reg_pol_header.encode('utf-16-le'))\n            pol_file.write(data_to_write)\n        try:\n            gpt_ini_data = ''\n            if os.path.exists(gpt_ini_path):\n                with salt.utils.files.fopen(gpt_ini_path, 'r') as gpt_file:\n                    gpt_ini_data = gpt_file.read()\n            if not _regexSearchRegPolData(r'\\[General\\]\\r\\n', gpt_ini_data):\n                gpt_ini_data = '[General]\\r\\n' + gpt_ini_data\n            if _regexSearchRegPolData(r'{0}='.format(re.escape(gpt_extension)),\n                                      gpt_ini_data):\n                # ensure the line contains the ADM guid\n                gpt_ext_loc = re.search(r'^{0}=.*\\r\\n'.format(re.escape(gpt_extension)),\n                                        gpt_ini_data,\n                                        re.IGNORECASE | re.MULTILINE)\n                gpt_ext_str = gpt_ini_data[gpt_ext_loc.start():gpt_ext_loc.end()]\n                if not _regexSearchRegPolData(r'{0}'.format(re.escape(gpt_extension_guid)),\n                                              gpt_ext_str):\n                    gpt_ext_str = gpt_ext_str.split('=')\n                    gpt_ext_str[1] = gpt_extension_guid + gpt_ext_str[1]\n                    gpt_ext_str = '='.join(gpt_ext_str)\n                    gpt_ini_data = gpt_ini_data[0:gpt_ext_loc.start()] + gpt_ext_str + gpt_ini_data[gpt_ext_loc.end():]\n            else:\n                general_location = re.search(r'^\\[General\\]\\r\\n',\n                                             gpt_ini_data,\n                                             re.IGNORECASE | re.MULTILINE)\n                gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                        gpt_ini_data[general_location.start():general_location.end()],\n                        gpt_extension,\n                        gpt_extension_guid,\n                        gpt_ini_data[general_location.end():])\n            # https://technet.microsoft.com/en-us/library/cc978247.aspx\n            if _regexSearchRegPolData(r'Version=', gpt_ini_data):\n                version_loc = re.search(r'^Version=.*\\r\\n',\n                                        gpt_ini_data,\n                                        re.IGNORECASE | re.MULTILINE)\n                version_str = gpt_ini_data[version_loc.start():version_loc.end()]\n                version_str = version_str.split('=')\n                version_nums = struct.unpack(b'>2H', struct.pack(b'>I', int(version_str[1])))\n                if gpt_extension.lower() == 'gPCMachineExtensionNames'.lower():\n                    version_nums = (version_nums[0], version_nums[1] + 1)\n                elif gpt_extension.lower() == 'gPCUserExtensionNames'.lower():\n                    version_nums = (version_nums[0] + 1, version_nums[1])\n                version_num = struct.unpack(b'>I', struct.pack(b'>2H', *version_nums))[0]\n                gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                        gpt_ini_data[0:version_loc.start()],\n                        'Version',\n                        version_num,\n                        gpt_ini_data[version_loc.end():])\n            else:\n                general_location = re.search(r'^\\[General\\]\\r\\n',\n                                             gpt_ini_data,\n                                             re.IGNORECASE | re.MULTILINE)\n                if gpt_extension.lower() == 'gPCMachineExtensionNames'.lower():\n                    version_nums = (0, 1)\n                elif gpt_extension.lower() == 'gPCUserExtensionNames'.lower():\n                    version_nums = (1, 0)\n                gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                        gpt_ini_data[general_location.start():general_location.end()],\n                        'Version',\n                        int(\"{0}{1}\".format(six.text_type(version_nums[0]).zfill(4),\n                                            six.text_type(version_nums[1]).zfill(4)),\n                            16),\n                        gpt_ini_data[general_location.end():])\n            if gpt_ini_data:\n                with salt.utils.files.fopen(gpt_ini_path, 'w') as gpt_file:\n                    gpt_file.write(gpt_ini_data)\n        # TODO: This needs to be more specific\n        except Exception as e:\n            msg = 'An error occurred attempting to write to {0}, the exception was {1}'.format(\n                    gpt_ini_path, e)\n            log.exception(msg)\n            raise CommandExecutionError(msg)\n    # TODO: This needs to be more specific\n    except Exception as e:\n        msg = 'An error occurred attempting to write to {0}, the exception was {1}'.format(policy_file_path, e)\n        log.exception(msg)\n        raise CommandExecutionError(msg)",
        "rewrite": "def _write_regpol_data(data_to_write, policy_file_path, gpt_ini_path, gpt_extension, gpt_extension_guid):\n    reg_pol_header = u'\\u5250\\u6765\\x01\\x00'\n    if not os.path.exists(policy_file_path):\n        __salt__['file.makedirs'](policy_file_path)\n    with salt.utils.files.fopen(policy_file_path, 'wb') as pol_file:\n        if not data_to_write.startswith(reg_pol_header.encode('utf-16-le')):\n            pol_file.write(reg_pol_header.encode('utf-16-le'))\n        pol_file.write(data_to_write)\n    try:\n        gpt_ini_data = ''\n        if os.path.exists(gpt_ini_path):\n            with salt.utils.files.fopen(gpt_ini_path, 'r') as gpt_file:\n                gpt_ini_data = gpt_file.read()\n        if not _regexSearchRegPolData(r'\\[General\\]\\r\\n', gpt_ini_data):\n            gpt_ini_data = '[General]\\r\\n' + gpt_ini_data\n        if _regexSearchRegPolData(r'{0}='.format(re.escape(gpt_extension)), gpt_ini_data):\n            gpt_ext_loc = re.search(r'^{0}=.*\\r\\n'.format(re.escape(gpt_extension)), gpt_ini_data, re.IGNORECASE | re.MULTILINE)\n            gpt_ext_str = gpt_ini_data[gpt_ext_loc.start():gpt_ext_loc.end()]\n            if not _regexSearchRegPolData(r'{0}'.format(re.escape(gpt_extension_guid)), gpt_ext_str):\n                gpt_ext_str = gpt_ext_str.split('=')\n                gpt_ext_str[1] = gpt_extension_guid + gpt_ext_str[1]\n                gpt_ext_str = '='.join(gpt_ext_str)\n                gpt_ini_data = gpt_ini_data[0:gpt_ext_loc.start()] + gpt_ext_str + gpt_ini_data[gpt_ext_loc.end():]\n        else:\n            general_location = re.search(r'^\\[General\\]\\r\\n', gpt_ini_data, re.IGNORECASE | re.MULTILINE)\n            gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                gpt_ini_data[general_location.start():general_location.end()], gpt_extension, gpt_extension_guid, gpt_ini_data[general_location.end():]\n            )\n        if _regexSearchRegPolData(r'Version=', gpt_ini_data):\n            version_loc = re.search(r'^Version=.*\\r\\n', gpt_ini_data, re.IGNORECASE | re.MULTILINE)\n            version_str = gpt_ini_data[version_loc.start():version_loc.end()]\n            version_str = version_str.split('=')\n            version_nums = struct.unpack(b'>2H', struct.pack(b'>I', int(version_str[1])))\n            if gpt_extension.lower() == 'gPCMachineExtensionNames'.lower():\n                version_nums = (version_nums[0], version_nums[1] + 1)\n            elif gpt_extension.lower() == 'gPCUserExtensionNames'.lower():\n                version_nums = (version_nums[0] + 1, version_nums[1])\n            version_num = struct.unpack(b'>I', struct.pack(b'>2H', *version_nums))[0]\n            gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                gpt_ini_data[0:version_loc.start()], 'Version', version_num, gpt_ini_data[version_loc.end()]\n            )\n        else:\n            general_location = re.search(r'^\\[General\\]\\r\\n', gpt_ini_data, re.IGNORECASE | re.MULTILINE)\n            if gpt_extension.lower() == 'gPCMachineExtensionNames'.lower():\n                version_nums = (0, 1)\n            elif gpt_extension.lower() == 'gPCUserExtensionNames'.lower():\n                version_nums = (1, 0)\n            gpt_ini_data = '{0}{1}={2}\\r\\n{3}'.format(\n                gpt_ini_data[general_location.start():general_location.end()], 'Version',\n                int(\"{0}{1}\".format(six.text_type(version_nums[0]).zfill(4), six.text_type(version_nums[1]).zfill(4)), 16),\n                gpt_ini_data[general_location.end()]\n            )\n        if gpt_ini_data:\n            with salt.utils.files.fopen(gpt_ini_path, 'w') as gpt_file:\n                gpt_file.write(gpt_ini_data)\n    except Exception as e:\n        msg = 'An error occurred attempting to write to {0}, the exception was {1}'.format(gpt_ini_path, e)\n        log.exception(msg)\n        raise CommandExecutionError(msg)\n    except Exception as e:\n        msg = 'An error occurred attempting to write to {0}, the exception was {1}'.format(policy_file_path, e)\n        log.exception(msg)\n        raise CommandExecutionError(msg)"
    },
    {
        "original": "def process_metric(self, message, **kwargs):\n        \"\"\"\n        Handle a prometheus metric message according to the following flow:\n            - search self.metrics_mapper for a prometheus.metric <--> datadog.metric mapping\n            - call check method with the same name as the metric\n            - log some info if none of the above worked\n\n        `send_histograms_buckets` is used to specify if yes or no you want to send\n        the buckets as tagged values when dealing with histograms.\n        \"\"\"\n\n        # If targeted metric, store labels\n        self.store_labels(message)\n\n        if message.name in self.ignore_metrics:\n            return  # Ignore the metric\n\n        # Filter metric to see if we can enrich with joined labels\n        self.join_labels(message)\n\n        send_histograms_buckets = kwargs.get('send_histograms_buckets', True)\n        send_monotonic_counter = kwargs.get('send_monotonic_counter', False)\n        custom_tags = kwargs.get('custom_tags')\n        ignore_unmapped = kwargs.get('ignore_unmapped', False)\n\n        try:\n            if not self._dry_run:\n                try:\n                    self._submit(\n                        self.metrics_mapper[message.name],\n                        message,\n                        send_histograms_buckets,\n                        send_monotonic_counter,\n                        custom_tags,\n                    )\n                except KeyError:\n                    if not ignore_unmapped:\n                        # call magic method (non-generic check)\n                        handler = getattr(self, message.name)  # Lookup will throw AttributeError if not found\n                        try:\n                            handler(message, **kwargs)\n                        except Exception as err:\n                            self.log.warning(\"Error handling metric: {} - error: {}\".format(message.name, err))\n                    else:\n                        # build the wildcard list if first pass\n                        if self._metrics_wildcards is None:\n                            self._metrics_wildcards = [x for x in self.metrics_mapper.keys() if '*' in x]\n                        # try matching wildcard (generic check)\n                        for wildcard in self._metrics_wildcards:\n                            if fnmatchcase(message.name, wildcard):\n                                self._submit(\n                                    message.name, message, send_histograms_buckets, send_monotonic_counter, custom_tags\n                                )\n\n        except AttributeError as err:\n            self.log.debug(\"Unable to handle metric: {} - error: {}\".format(message.name, err))",
        "rewrite": "def process_metric(self, message, **kwargs):\n    \"\"\"\n    Handle a prometheus metric message according to the following flow:\n        - search self.metrics_mapper for a prometheus.metric <--> datadog.metric mapping\n        - call check method with the same name as the metric\n        - log some info if none of the above worked\n    `send_histograms_buckets` is used to specify if you want to send buckets with histograms.\n    \"\"\"\n    self.store_labels(message)\n\n    if message.name in self.ignore_metrics:\n        return\n\n    self.join_labels(message)\n\n    send_histograms_buckets = kwargs.get('send_histograms_buckets', True)\n    send_monotonic_counter = kwargs.get('send_monotonic_counter', False)\n    custom_tags = kwargs.get('custom_tags')\n    ignore_unmapped = kwargs.get('ignore_unmapped', False)\n    try:\n        if not self._dry_run:\n            try:\n                self._submit(\n                    self.metrics_mapper[message.name],\n                    message,\n                    send_histograms_buckets,\n                    send_monotonic_counter,\n                    custom_tags,\n                )\n            except KeyError:\n                if not ignore_unmapped:\n                    handler = getattr(self, message.name)\n                    try:\n                        handler(message, **kwargs)\n                    except Exception as err:\n                        self.log.warning(\"Error handling metric: {} - error: {}\".format(message.name, err))\n                else:\n                    if self._metrics_wildcards is None:\n                        self._metrics_wildcards = [x for x in self.metrics_mapper.keys() if '*' in x]\n\n                    for wildcard in self._metrics_wildcards:\n                        if fnmatchcase(message.name, wildcard):\n                            self._submit(\n                                message.name, message, send_histograms_buckets, send_monotonic_counter, custom_tags()\n                            )\n    except AttributeError as err:\n        self.log.debug(\"Unable to handle metric: {} - error: {}\".format(message.name, err))\""
    },
    {
        "original": "def delete_endpoint(self, endpoint_name):\n        \"\"\"Delete an Amazon SageMaker ``Endpoint``.\n\n        Args:\n            endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to delete.\n        \"\"\"\n        LOGGER.info('Deleting endpoint with name: {}'.format(endpoint_name))\n        self.sagemaker_client.delete_endpoint(EndpointName=endpoint_name)",
        "rewrite": "def delete_endpoint(self, endpoint_name):\n    LOGGER.info(f\"Deleting endpoint with name: {endpoint_name}\")\n    self.sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
    },
    {
        "original": "def trim(self):\n        \"\"\"\n        Discard the ancestry of this state.\n        \"\"\"\n        new_hist = self.copy({})\n        new_hist.parent = None\n        self.state.register_plugin('history', new_hist)",
        "rewrite": "def trim(self):\n    new_hist = self.copy({})\n    new_hist.parent = None\n    self.state.register_plugin('history', new_hist)"
    },
    {
        "original": "def get_execution_role(sagemaker_session=None):\n    \"\"\"Return the role ARN whose credentials are used to call the API.\n    Throws an exception if\n    Args:\n        sagemaker_session(Session): Current sagemaker session\n    Returns:\n        (str): The role ARN\n    \"\"\"\n    if not sagemaker_session:\n        sagemaker_session = Session()\n    arn = sagemaker_session.get_caller_identity_arn()\n\n    if ':role/' in arn:\n        return arn\n    message = 'The current AWS identity is not a role: {}, therefore it cannot be used as a SageMaker execution role'\n    raise ValueError(message.format(arn))",
        "rewrite": "from sagemaker.session import Session\n\ndef get_execution_role(sagemaker_session=None):\n    if not sagemaker_session:\n        sagemaker_session = Session()\n    arn = sagemaker_session.get_caller_identity_arn()\n\n    if ':role/' in arn:\n        return arn\n    message = 'The current AWS identity is not a role: {}, therefore it cannot be used as a SageMaker execution role'\n    raise ValueError(message.format(arn))"
    },
    {
        "original": "def WriteValuesToJSONFile(self, state, values):\n    \"\"\"Write newline separated JSON dicts for each value.\n\n    We write each dict separately so we don't have to hold all of the output\n    streams in memory. We open and close the JSON array manually with [].\n\n    Args:\n      state: rdf_protodict.AttributedDict with the plugin's state.\n      values: RDF values to export.\n    \"\"\"\n    value_counters = {}\n    max_post_size = config.CONFIG[\"BigQuery.max_file_post_size\"]\n    for value in values:\n      class_name = value.__class__.__name__\n      output_tracker, created = self._GetTempOutputFileHandles(class_name)\n\n      # If our output stream is getting huge we should flush everything now and\n      # set up new output files. Only start checking when we are getting within\n      # range of the limit because we need to flush the stream to check the\n      # size. Start counting at 0 so we check each file the first time.\n      value_counters[class_name] = value_counters.get(class_name, -1) + 1\n      if not value_counters[class_name] % max_post_size // 1000:\n\n        # Flush our temp gzip handle so we can stat it to see how big it is.\n        output_tracker.gzip_filehandle.flush()\n        if os.path.getsize(output_tracker.gzip_filehandle.name) > max_post_size:\n          # Flush what we have and get new temp output handles.\n          self.Flush(state)\n          value_counters[class_name] = 0\n          output_tracker, created = self._GetTempOutputFileHandles(class_name)\n\n      if not output_tracker.schema:\n        output_tracker.schema = self.RDFValueToBigQuerySchema(value)\n\n      if created:\n        # Omit the leading newline for the first entry in the file.\n        self._WriteJSONValue(output_tracker.gzip_filehandle, value)\n      else:\n        self._WriteJSONValue(\n            output_tracker.gzip_filehandle, value, delimiter=\"\\n\")\n\n    for output_tracker in itervalues(self.temp_output_trackers):\n      output_tracker.gzip_filehandle.flush()",
        "rewrite": "def WriteValuesToJSONFile(self, state, values):\n    value_counters = {}\n    max_post_size = config.CONFIG[\"BigQuery.max_file_post_size\"]\n    for value in values:\n        class_name = value.__class__.__name__\n        output_tracker, created = self._GetTempOutputFileHandles(class_name)\n\n        value_counters[class_name] = value_counters.get(class_name, -1) + 1\n        if not value_counters[class_name] % max_post_size // 1000:\n            output_tracker.gzip_filehandle.flush()\n            if os.path.getsize(output_tracker.gzip_filehandle.name) > max_post_size:\n                self.Flush(state)\n                value_counters[class_name] = 0\n                output_tracker, created = self._GetTempOutputFileHandles(class_name)\n\n        if not output_tracker.schema:\n            output_tracker.schema = self.RDFValueToBigQuerySchema(value)\n\n        if created:\n            self._WriteJSONValue(output_tracker.gzip_filehandle, value)\n        else:\n            self._WriteJSONValue(output_tracker.gzip_filehandle, value, delimiter=\"\\n\")\n\n    for output_tracker in itervalues(self.temp_output_trackers):\n        output_tracker.gzip_filehandle.flush()"
    },
    {
        "original": "def get_batch(self, user_list):\n        \"\"\"\n        \u6279\u91cf\u83b7\u53d6\u7528\u6237\u57fa\u672c\u4fe1\u606f\n        \u5f00\u53d1\u8005\u53ef\u901a\u8fc7\u8be5\u63a5\u53e3\u6765\u6279\u91cf\u83b7\u53d6\u7528\u6237\u57fa\u672c\u4fe1\u606f\u3002\u6700\u591a\u652f\u6301\u4e00\u6b21\u62c9\u53d6100\u6761\u3002\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?t=resource/res_main&id=mp1421140839\n\n        :param user_list: user_list\uff0c\u652f\u6301\u201c\u4f7f\u7528\u793a\u4f8b\u201d\u4e2d\u4e24\u79cd\u8f93\u5165\u683c\u5f0f\n        :return: \u7528\u6237\u4fe1\u606f\u7684 list\n\n        \u4f7f\u7528\u793a\u4f8b::\n\n            from wechatpy import WeChatClient\n\n            client = WeChatClient('appid', 'secret')\n            users = client.user.get_batch(['openid1', 'openid2'])\n            users = client.user.get_batch([\n              {'openid': 'openid1', 'lang': 'zh-CN'},\n              {'openid': 'openid2', 'lang': 'en'},\n            ])\n\n        \"\"\"\n        if all((isinstance(x, six.string_types) for x in user_list)):\n            user_list = [{'openid': oid} for oid in user_list]\n        res = self._post(\n            'user/info/batchget',\n            data={'user_list': user_list},\n            result_processor=lambda x: x['user_info_list']\n        )\n        return res",
        "rewrite": "def get_batch(self, user_list):\n    if all(isinstance(x, str) for x in user_list):\n        user_list = [{'openid': oid} for oid in user_list]\n    res = self._post(\n        'user/info/batchget',\n        data={'user_list': user_list},\n        result_processor=lambda x: x['user_info_list']\n    )\n    return res"
    },
    {
        "original": "def pyobjc_method_signature(signature_str):\n        \"\"\"\n        Return a PyObjCMethodSignature object for given signature string.\n\n        :param signature_str: A byte string containing the type encoding for the method signature\n        :return: A method signature object, assignable to attributes like __block_signature__\n        :rtype: <type objc._method_signature>\n        \"\"\"\n        _objc_so.PyObjCMethodSignature_WithMetaData.restype = ctypes.py_object\n        return _objc_so.PyObjCMethodSignature_WithMetaData(ctypes.create_string_buffer(signature_str), None, False)",
        "rewrite": "def pyobjc_method_signature(signature_str):\n    _objc_so.PyObjCMethodSignature_WithMetaData.restype = ctypes.py_object\n    return _objc_so.PyObjCMethodSignature_WithMetaData(ctypes.create_string_buffer(signature_str), None, False)"
    },
    {
        "original": "def get_token(self):\n        \"\"\"\n        Retrieves the token from the File System\n        :return dict or None: The token if exists, None otherwise\n        \"\"\"\n        token = None\n        if self.token_path.exists():\n            with self.token_path.open('r') as token_file:\n                token = self.token_constructor(self.serializer.load(token_file))\n        self.token = token\n        return token",
        "rewrite": "def get_token(self):\n    token = None\n    if self.token_path.exists():\n        with self.token_path.open('r') as token_file:\n            token = self.token_constructor(self.serializer.load(token_file))\n    self.token = token\n    return token"
    },
    {
        "original": "def has_in_collaborators(self, collaborator):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/collaborators/:user <http://developer.github.com/v3/repos/collaborators>`_\n        :param collaborator: string or :class:`github.NamedUser.NamedUser`\n        :rtype: bool\n        \"\"\"\n        assert isinstance(collaborator, github.NamedUser.NamedUser) or isinstance(collaborator, (str, unicode)), collaborator\n\n        if isinstance(collaborator, github.NamedUser.NamedUser):\n            collaborator = collaborator._identity\n\n        status, headers, data = self._requester.requestJson(\n            \"GET\",\n            self.url + \"/collaborators/\" + collaborator\n        )\n        return status == 204",
        "rewrite": "def has_in_collaborators(self, collaborator):\n        assert isinstance(collaborator, github.NamedUser.NamedUser) or isinstance(collaborator, (str, unicode)), collaborator\n        if isinstance(collaborator, github.NamedUser.NamedUser):\n            collaborator = collaborator._identity\n        status, headers, data = self._requester.requestJson(\"GET\", self.url + \"/collaborators/\" + collaborator)\n        return status == 204"
    },
    {
        "original": "def Contains(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"contains\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, 'CONTAINS')\n    return self._query_builder",
        "rewrite": "def contains(self, value):\n    self._awql = self._CreateSingleValueCondition(value, 'CONTAINS')\n    return self._query_builder"
    },
    {
        "original": "def get_zone():\n    \"\"\"\n    Get current timezone (i.e. America/Denver)\n\n    .. versionchanged:: 2016.11.4\n\n    .. note::\n\n        On AIX operating systems, Posix values can also be returned\n        'CST6CDT,M3.2.0/2:00:00,M11.1.0/2:00:00'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' timezone.get_zone\n    \"\"\"\n    if salt.utils.path.which('timedatectl'):\n        ret = _timedatectl()\n\n        for line in (x.strip() for x in salt.utils.itertools.split(ret['stdout'], '\\n')):\n            try:\n                return re.match(r'Time ?zone:\\s+(\\S+)', line).group(1)\n            except AttributeError:\n                pass\n\n        msg = ('Failed to parse timedatectl output: {0}\\n'\n               'Please file an issue with SaltStack').format(ret['stdout'])\n        raise CommandExecutionError(msg)\n\n    else:\n        if __grains__['os'].lower() == 'centos':\n            return _get_zone_etc_localtime()\n        os_family = __grains__['os_family']\n        for family in ('RedHat', 'Suse'):\n            if family in os_family:\n                return _get_zone_sysconfig()\n        for family in ('Debian', 'Gentoo'):\n            if family in os_family:\n                return _get_zone_etc_timezone()\n        if os_family in ('FreeBSD', 'OpenBSD', 'NetBSD', 'NILinuxRT'):\n            return _get_zone_etc_localtime()\n        elif 'Solaris' in os_family:\n            return _get_zone_solaris()\n        elif 'AIX' in os_family:\n            return _get_zone_aix()\n    raise CommandExecutionError('Unable to get timezone')",
        "rewrite": "def get_zone():\n    if salt.utils.path.which('timedatectl'):\n        ret = _timedatectl()\n\n        for line in (x.strip() for x in salt.utils.itertools.split(ret['stdout'], '\\n')):\n            try:\n                return re.match(r'Time ?zone:\\s+(\\S+)', line).group(1)\n            except AttributeError:\n                pass\n\n        msg = ('Failed to parse timedatectl output: {0}\\n'\n               'Please file an issue with SaltStack').format(ret['stdout'])\n        raise CommandExecutionError(msg)\n\n    else:\n        if __grains__['os'].lower() == 'centos':\n            return _get_zone_etc_localtime()\n        os_family = __grains__['os_family']\n        for family in ('RedHat', 'Suse'):\n            if family in os_family:\n                return _get_zone_sysconfig()\n        for family in ('Debian', 'Gentoo'):\n            if family in os_family:\n                return _get_zone_etc_timezone()\n        if os_family in ('FreeBSD', 'OpenBSD', 'NetBSD', 'NILinuxRT'):\n            return _get_zone_etc_localtime()\n        elif 'Solaris' in os_family:\n            return _get_zone_solaris()\n        elif 'AIX' in os_family:\n            return _get_zone_aix()\n    raise CommandExecutionError('Unable to get timezone')"
    },
    {
        "original": "def build(force):\n    \"\"\" Builds the distribution files: wheels and source. \"\"\"\n    dist_path = Path(DIST_PATH)\n    if dist_path.exists() and list(dist_path.glob('*')):\n        if force or click.confirm('{} is not empty - delete contents?'.format(dist_path)):\n            dist_path.rename(DIST_PATH_DELETE)\n            shutil.rmtree(Path(DIST_PATH_DELETE))\n            dist_path.mkdir()\n        else:\n            click.echo('Aborting')\n            sys.exit(1)\n\n    subprocess.check_call(['python', 'setup.py', 'bdist_wheel'])\n    subprocess.check_call(['python', 'setup.py', 'sdist',\n                           '--formats=gztar'])",
        "rewrite": "def build(force):\n    dist_path = Path(DIST_PATH)\n    if dist_path.exists() and list(dist_path.glob('*')):\n        if force or click.confirm('{} is not empty - delete contents?'.format(dist_path)):\n            dist_path.rename(DIST_PATH_DELETE)\n            shutil.rmtree(Path(DIST_PATH_DELETE))\n            dist_path.mkdir()\n        else:\n            click.echo('Aborting')\n            sys.exit(1)\n\n    subprocess.check_call(['python', 'setup.py', 'bdist_wheel'])\n    subprocess.check_call(['python', 'setup.py', 'sdist',\n                           '--formats=gztar'])"
    },
    {
        "original": "def get_recipe_env(self, arch, with_flags_in_cc=True):\n        \"\"\" Add libgeos headers to path \"\"\"\n        env = super(ShapelyRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n        libgeos_dir = Recipe.get_recipe('libgeos', self.ctx).get_build_dir(arch.arch)\n        env['CFLAGS'] += \" -I{}/dist/include\".format(libgeos_dir)\n        return env",
        "rewrite": "def get_recipe_env(self, arch, with_flags_in_cc=True):    \n    env = super(ShapelyRecipe, self).get_recipe_env(arch, with_flags_in_cc)\n    libgeos_dir = Recipe.get_recipe('libgeos', self.ctx).get_build_dir(arch.arch)\n    env['CFLAGS'] += \" -I{}/dist/include\".format(libgeos_dir)\n    return env"
    },
    {
        "original": "def store(self):\n        \"\"\"\n        Write content of the entire cache to disk\n        \"\"\"\n        if msgpack is None:\n            log.error('Cache cannot be stored on disk: msgpack is missing')\n        else:\n            # TODO Dir hashing?\n            try:\n                with salt.utils.files.fopen(self._path, 'wb+') as fp_:\n                    cache = {\n                        \"CacheDisk_data\": self._dict,\n                        \"CacheDisk_cachetime\": self._key_cache_time\n                    }\n                    msgpack.dump(cache, fp_, use_bin_type=True)\n            except (IOError, OSError) as err:\n                log.error('Error storing cache data to the disk: %s', err)",
        "rewrite": "def store(self):\n    if msgpack is None:\n        log.error('Cache cannot be stored on disk: msgpack is missing')\n    else:\n        try:\n            with salt.utils.files.fopen(self._path, 'wb+') as fp_:\n                cache = {\n                    \"CacheDisk_data\": self._dict,\n                    \"CacheDisk_cachetime\": self._key_cache_time\n                }\n                msgpack.dump(cache, fp_, use_bin_type=True)\n        except (IOError, OSError) as err:\n            log.error('Error storing cache data to the disk: %s', err)"
    },
    {
        "original": "def communicate(self, input=None):\n        \"\"\"\n        Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached.\n        Wait for process to terminate. The optional input argument should be a string to be sent to the\n        child process, or None, if no data should be sent to the child.\n\n        communicate() returns a tuple (stdoutdata, stderrdata).\n        \"\"\"\n        stdoutdata, stderrdata = self.process.communicate(input=input)\n        self._returncode = self.process.returncode\n        self.set_status(self.S_DONE, \"status set to Done\")\n\n        return stdoutdata, stderrdata",
        "rewrite": "def communicate(self, input=None):\n    stdoutdata, stderrdata = self.process.communicate(input=input)\n    self._returncode = self.process.returncode\n    self.set_status(self.S_DONE, \"status set to Done\")\n    return stdoutdata, stderrdata"
    },
    {
        "original": "def stringify_summary(summary):\n    \"\"\" stringify summary, in order to dump json file and generate html report.\n    \"\"\"\n    for index, suite_summary in enumerate(summary[\"details\"]):\n\n        if not suite_summary.get(\"name\"):\n            suite_summary[\"name\"] = \"testcase {}\".format(index)\n\n        for record in suite_summary.get(\"records\"):\n            meta_datas = record['meta_datas']\n            __stringify_meta_datas(meta_datas)\n            meta_datas_expanded = []\n            __expand_meta_datas(meta_datas, meta_datas_expanded)\n            record[\"meta_datas_expanded\"] = meta_datas_expanded\n            record[\"response_time\"] = __get_total_response_time(meta_datas_expanded)",
        "rewrite": "def stringify_summary(summary):\n    for index, suite_summary in enumerate(summary[\"details\"]):\n\n        if not suite_summary.get(\"name\"):\n            suite_summary[\"name\"] = \"testcase {}\".format(index)\n\n        for record in suite_summary.get(\"records\"):\n            meta_datas = record['meta_datas']\n            __stringify_meta_datas(meta_datas)\n            meta_datas_expanded = []\n            __expand_meta_datas(meta_datas, meta_datas_expanded)\n            record[\"meta_datas_expanded\"] = meta_datas_expanded\n            record[\"response_time\"] = __get_total_response_time(meta_datas_expanded)"
    },
    {
        "original": "def get_body_text(self):\n        \"\"\" Parse the body html and returns the body text using bs4\n\n        :return: body text\n        :rtype: str\n        \"\"\"\n        if self.body_type != 'HTML':\n            return self.body\n\n        try:\n            soup = bs(self.body, 'html.parser')\n        except RuntimeError:\n            return self.body\n        else:\n            return soup.body.text",
        "rewrite": "def get_body_text(self):\n    if self.body_type != 'HTML':\n        return self.body\n\n    try:\n        soup = bs(self.body, 'html.parser')\n    except RuntimeError:\n        return self.body\n    else:\n        return soup.body.text"
    },
    {
        "original": "async def _disconnect(self):\n        \"\"\"\n        Disconnect only, without closing the session. Used in reconnections\n        to different data centers, where we don't want to close the session\n        file; user disconnects however should close it since it means that\n        their job with the client is complete and we should clean it up all.\n        \"\"\"\n        await self._sender.disconnect()\n        await helpers._cancel(self._log[__name__],\n                              updates_handle=self._updates_handle)",
        "rewrite": "async def _disconnect(self):\n    await self._sender.disconnect()\n    await helpers._cancel(self._log[__name__], updates_handle=self._updates_handle)"
    },
    {
        "original": "def init_database(connection=None, dbname=None):\n    \"\"\"Initialize the configured backend for use with BigchainDB.\n\n    Creates a database with :attr:`dbname` with any required tables\n    and supporting indexes.\n\n    Args:\n        connection (:class:`~bigchaindb.backend.connection.Connection`): an\n            existing connection to use to initialize the database.\n            Creates one if not given.\n        dbname (str): the name of the database to create.\n            Defaults to the database name given in the BigchainDB\n            configuration.\n    \"\"\"\n\n    connection = connection or connect()\n    dbname = dbname or bigchaindb.config['database']['name']\n\n    create_database(connection, dbname)\n    create_tables(connection, dbname)",
        "rewrite": "def init_database(connection=None, dbname=None):\n    connection = connection or connect()\n    dbname = dbname or bigchaindb.config['database']['name']\n    create_database(connection, dbname)\n    create_tables(connection, dbname)"
    },
    {
        "original": "def download(self):\n        \"\"\"\n        Download all waypoints from the vehicle.\n        The download is asynchronous. Use :py:func:`wait_ready()` to block your thread until the download is complete.\n        \"\"\"\n        self.wait_ready()\n        self._vehicle._ready_attrs.remove('commands')\n        self._vehicle._wp_loaded = False\n        self._vehicle._master.waypoint_request_list_send()",
        "rewrite": "def download(self):\n    self.wait_ready()\n    del self._vehicle._ready_attrs['commands']\n    self._vehicle._wp_loaded = False\n    self._vehicle._master.waypoint_request_list_send()"
    },
    {
        "original": "def get_distribution_path(venv, distribution):\n    \"\"\"\n    Return the path to a distribution installed inside a virtualenv\n\n    .. versionadded:: 2016.3.0\n\n    venv\n        Path to the virtualenv.\n    distribution\n        Name of the distribution. Note, all non-alphanumeric characters\n        will be converted to dashes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virtualenv.get_distribution_path /path/to/my/venv my_distribution\n    \"\"\"\n    _verify_safe_py_code(distribution)\n    bin_path = _verify_virtualenv(venv)\n\n    ret = __salt__['cmd.exec_code_all'](\n        bin_path,\n        'import pkg_resources; '\n            \"print(pkg_resources.get_distribution('{0}').location)\".format(\n                distribution\n            )\n    )\n\n    if ret['retcode'] != 0:\n        raise CommandExecutionError('{stdout}\\n{stderr}'.format(**ret))\n\n    return ret['stdout']",
        "rewrite": "def get_distribution_path(venv, distribution):\n    _verify_safe_py_code(distribution)\n    bin_path = _verify_virtualenv(venv)\n\n    ret = __salt__['cmd.exec_code_all'](\n        bin_path,\n        'import pkg_resources; ' \n        \"print(pkg_resources.get_distribution('{0}').location)\".format(\n            distribution\n        )\n    )\n\n    if ret['retcode'] != 0:\n        raise CommandExecutionError('{stdout}\\n{stderr}'.format(**ret))\n\n    return ret['stdout']"
    },
    {
        "original": "def enable_beacon(name, **kwargs):\n    \"\"\"\n    Enable a beacon on the minion.\n\n    Args:\n        name (str): Name of the beacon to enable.\n\n    Returns:\n        dict: Boolean and status message on success or failure of enable.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' beacons.enable_beacon ps\n    \"\"\"\n\n    ret = {'comment': [],\n           'result': True}\n\n    if not name:\n        ret['comment'] = 'Beacon name is required.'\n        ret['result'] = False\n        return ret\n\n    if 'test' in kwargs and kwargs['test']:\n        ret['comment'] = 'Beacon {0} would be enabled.'.format(name)\n    else:\n        _beacons = list_(return_yaml=False, **kwargs)\n        if name not in _beacons:\n            ret['comment'] = 'Beacon {0} is not currently configured.' \\\n                             ''.format(name)\n            ret['result'] = False\n            return ret\n\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({'func': 'enable_beacon',\n                                          'name': name},\n                                         'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(\n                    tag='/salt/minion/minion_beacon_enabled_complete',\n                    wait=kwargs.get('timeout', 30))\n                if event_ret and event_ret['complete']:\n                    beacons = event_ret['beacons']\n                    beacon_config_dict = _get_beacon_config_dict(beacons[name])\n\n                    if 'enabled' in beacon_config_dict and beacon_config_dict['enabled']:\n                        ret['result'] = True\n                        ret['comment'] = 'Enabled beacon {0} on minion.' \\\n                                         ''.format(name)\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to enable beacon {0} on ' \\\n                                         'minion.'.format(name)\n                elif event_ret:\n                    ret['result'] = False\n                    ret['comment'] = event_ret['comment']\n                else:\n                    ret['result'] = False\n                    ret['comment'] = 'Did not receive the manage event ' \\\n                                     'before the timeout of {0}s' \\\n                                     ''.format(kwargs.get('timeout', 30))\n                return ret\n        except KeyError:\n            # Effectively a no-op, since we can't really return without an event\n            # system\n            ret['result'] = False\n            ret['comment'] = 'Event module not available. Beacon enable job ' \\\n                             'failed.'\n    return ret",
        "rewrite": "def enable_beacon(name, **kwargs):\n    ret = {'comment': [],\n           'result': True}\n\n    if not name:\n        ret['comment'] = 'Beacon name is required.'\n        ret['result'] = False\n        return ret\n\n    if 'test' in kwargs and kwargs['test']:\n        ret['comment'] = 'Beacon {0} would be enabled.'.format(name)\n    else:\n        _beacons = list_(return_yaml=False, **kwargs)\n        if name not in _beacons:\n            ret['comment'] = 'Beacon {0} is not currently configured.'.format(name)\n            ret['result'] = False\n            return ret\n\n        try:\n            eventer = salt.utils.event.get_event('minion', opts=__opts__)\n            res = __salt__['event.fire']({'func': 'enable_beacon',\n                                          'name': name},\n                                         'manage_beacons')\n            if res:\n                event_ret = eventer.get_event(\n                    tag='/salt/minion/minion_beacon_enabled_complete',\n                    wait=kwargs.get('timeout', 30))\n                if event_ret and event_ret['complete']:\n                    beacons = event_ret['beacons']\n                    beacon_config_dict = _get_beacon_config_dict(beacons[name])\n\n                    if 'enabled' in beacon_config_dict and beacon_config_dict['enabled']:\n                        ret['result'] = True\n                        ret['comment'] = 'Enabled beacon {0} on minion.'.format(name)\n                    else:\n                        ret['result'] = False\n                        ret['comment'] = 'Failed to enable beacon {0} on minion.'.format(name)\n                elif event_ret:\n                    ret['result'] = False\n                    ret['comment'] = event_ret['comment']\n                else:\n                    ret['result'] = False\n                    ret['comment'] = 'Did not receive the manage event before the timeout of {0}s'.format(kwargs.get('timeout', 30))\n                return ret\n        except KeyError:\n            ret['result'] = False\n            ret['comment'] = 'Event module not available. Beacon enable job failed.'\n    return ret"
    },
    {
        "original": "def _verify_configs(configs):\n    \"\"\"\n    Verify a Molecule config was found and returns None.\n\n    :param configs: A list containing absolute paths to Molecule config files.\n    :return: None\n    \"\"\"\n    if configs:\n        scenario_names = [c.scenario.name for c in configs]\n        for scenario_name, n in collections.Counter(scenario_names).items():\n            if n > 1:\n                msg = (\"Duplicate scenario name '{}' found.  \"\n                       'Exiting.').format(scenario_name)\n                util.sysexit_with_message(msg)\n\n    else:\n        msg = \"'{}' glob failed.  Exiting.\".format(MOLECULE_GLOB)\n        util.sysexit_with_message(msg)",
        "rewrite": "def _verify_configs(configs):\n    if configs:\n        scenario_names = [c.scenario.name for c in configs]\n        for scenario_name, n in collections.Counter(scenario_names).items():\n            if n > 1:\n                msg = (\"Duplicate scenario name '{}' found. Exiting.\").format(scenario_name)\n                util.sysexit_with_message(msg)\n    else:\n        msg = \"'{}' glob failed. Exiting.\".format(MOLECULE_GLOB)\n        util.sysexit_with_message(msg)"
    },
    {
        "original": "def surface(self, canvas, X, Y, Z, color=None, label=None, **kwargs):\n        \"\"\"\n        Plot a surface for 3d plotting for the inputs (X, Y, Z).\n        \n        the kwargs are plotting library specific kwargs!\n        \"\"\"\n        raise NotImplementedError(\"Implement all plot functions in AbstractPlottingLibrary in order to use your own plotting library\")",
        "rewrite": "def surface(self, canvas, X, Y, Z, color=None, label=None, **kwargs):\n    \"\"\"\n    Plot a surface for 3d plotting for the inputs (X, Y, Z).\n\n    the kwargs are plotting library specific kwargs!\n    \"\"\"\n    raise NotImplementedError(\"Implement all plot functions in AbstractPlottingLibrary in order to use your own plotting library\")"
    },
    {
        "original": "def _match(self, struct1, struct2, fu, s1_supercell=True, use_rms=False,\n               break_on_match=False):\n        \"\"\"\n        Matches one struct onto the other\n        \"\"\"\n        ratio = fu if s1_supercell else 1/fu\n        if len(struct1) * ratio >= len(struct2):\n            return self._strict_match(\n                struct1, struct2, fu, s1_supercell=s1_supercell,\n                break_on_match=break_on_match, use_rms=use_rms)\n        else:\n            return self._strict_match(\n                struct2, struct1, fu, s1_supercell=(not s1_supercell),\n                break_on_match=break_on_match, use_rms=use_rms)",
        "rewrite": "def _match(self, struct1, struct2, fu, s1_supercell=True, use_rms=False, break_on_match=False):\n    ratio = fu if s1_supercell else 1/fu\n    if len(struct1) * ratio >= len(struct2):\n        return self._strict_match(struct1, struct2, fu, s1_supercell=s1_supercell, \n                                  break_on_match=break_on_match, use_rms=use_rms)\n    else:\n        return self._strict_match(struct2, struct1, fu, s1_supercell=(not s1_supercell), \n                                  break_on_match=break_on_match, use_rms=use_rms)"
    },
    {
        "original": "def get_ip_address():\n  \"\"\"Simple utility to get host IP address.\"\"\"\n  try:\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect((\"8.8.8.8\", 80))\n    ip_address = s.getsockname()[0]\n  except socket_error as sockerr:\n    if sockerr.errno != errno.ENETUNREACH:\n      raise sockerr\n    ip_address = socket.gethostbyname(socket.getfqdn())\n  finally:\n    s.close()\n\n  return ip_address",
        "rewrite": "import socket\nimport errno\n\ndef get_ip_address():\n    \"\"\"Simple utility to get host IP address.\"\"\"\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((\"8.8.8.8\", 80))\n        ip_address = s.getsockname()[0]\n    except socket.error as sockerr:\n        if sockerr.errno != errno.ENETUNREACH:\n            raise sockerr\n        ip_address = socket.gethostbyname(socket.getfqdn())\n    finally:\n        s.close()\n    \n    return ip_address"
    },
    {
        "original": "def get_lib_from(search_directory, lib_extension='.so'):\n    \"\"\"Scan directories recursively until find any file with the given\n    extension. The default extension to search is ``.so``.\"\"\"\n    for root, dirs, files in walk(search_directory):\n        for file in files:\n            if file.endswith(lib_extension):\n                print('get_lib_from: {}\\n\\t- {}'.format(\n                    search_directory, join(root, file)))\n                return join(root, file)\n    return None",
        "rewrite": "def get_lib_from(search_directory, lib_extension='.so'):\n    for root, dirs, files in os.walk(search_directory):\n        for file in files:\n            if file.endswith(lib_extension):\n                print('get_lib_from: {}\\n\\t- {}'.format(\n                    search_directory, os.path.join(root, file)))\n                return os.path.join(root, file)\n    return None"
    },
    {
        "original": "def search_word(confluence, word):\r\n    \"\"\"\r\n    Get all found pages with order by created date\r\n    :param confluence:\r\n    :param word:\r\n    :return: json answer\r\n    \"\"\"\r\n    cql = \"siteSearch ~ {} order by created\".format(word)\r\n    answers = confluence.cql(cql)\r\n    for answer in answers.get('results'):\r\n        print(answer)",
        "rewrite": "def search_word(confluence, word):\n    cql = \"siteSearch ~ {} order by created\".format(word)\n    answers = confluence.cql(cql)\n    for answer in answers.get('results'):\n        print(answer)"
    },
    {
        "original": "def utc_dt_to_local_dt(dtm):\n    \"\"\"Convert a UTC datetime to datetime in local timezone\"\"\"\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo is not None and dtm.tzinfo != utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(mktz())",
        "rewrite": "from datetime import datetime\nfrom pytz import timezone, utc\n\ndef utc_dt_to_local_dt(dtm):\n    local_tz = timezone('America/New_York')\n    utc_dt = dtm.replace(tzinfo=utc)\n    \n    return utc_dt.astimezone(local_tz)"
    },
    {
        "original": "def _check_for_api_errors(geocoding_results):\n        \"\"\"\n        Raise any exceptions if there were problems reported\n        in the api response.\n        \"\"\"\n        status_result = geocoding_results.get(\"STATUS\", {})\n        if \"NO_RESULTS\" in status_result.get(\"status\", \"\"):\n            return\n        api_call_success = status_result.get(\"status\", \"\") == \"SUCCESS\"\n        if not api_call_success:\n            access_error = status_result.get(\"access\")\n            access_error_to_exception = {\n                'API_KEY_INVALID': GeocoderAuthenticationFailure,\n                'OVER_QUERY_LIMIT': GeocoderQuotaExceeded,\n            }\n            exception_cls = access_error_to_exception.get(\n                access_error, GeocoderServiceError\n            )\n            raise exception_cls(access_error)",
        "rewrite": "def _check_for_api_errors(geocoding_results):\n    status_result = geocoding_results.get(\"STATUS\", {})\n    if \"NO_RESULTS\" in status_result.get(\"status\", \"\"):\n        return\n    api_call_success = status_result.get(\"status\", \"\") == \"SUCCESS\"\n    if not api_call_success:\n        access_error = status_result.get(\"access\")\n        access_error_to_exception = {\n            'API_KEY_INVALID': GeocoderAuthenticationFailure,\n            'OVER_QUERY_LIMIT': GeocoderQuotaExceeded,\n        }\n        exception_cls = access_error_to_exception.get(access_error, GeocoderServiceError)\n        raise exception_cls(access_error)"
    },
    {
        "original": "def get_info(self, symbol, as_of=None):\n        \"\"\"\n        Reads and returns information about the data stored for symbol\n\n        Parameters\n        ----------\n        symbol : `str`\n            symbol name for the item\n        as_of : `str` or int or `datetime.datetime`\n            Return the data as it was as_of the point in time.\n            `int` : specific version number\n            `str` : snapshot name which contains the version\n            `datetime.datetime` : the version of the data that existed as_of the requested point in time\n\n        Returns\n        -------\n        dictionary of the information (specific to the type of data)\n        \"\"\"\n        version = self._read_metadata(symbol, as_of=as_of, read_preference=None)\n        handler = self._read_handler(version, symbol)\n        if handler and hasattr(handler, 'get_info'):\n            return handler.get_info(version)\n        return {}",
        "rewrite": "def get_info(self, symbol, as_of=None):\n    version = self._read_metadata(symbol, as_of=as_of, read_preference=None)\n    handler = self._read_handler(version, symbol)\n    if handler and hasattr(handler, 'get_info'):\n        return handler.get_info(version)\n    return {}"
    },
    {
        "original": "def get_equivalent_qpoints(self, index):\n        \"\"\"\n        Returns the list of qpoint indices equivalent (meaning they are the\n        same frac coords) to the given one.\n\n        Args:\n            index: the qpoint index\n\n        Returns:\n            a list of equivalent indices\n\n        TODO: now it uses the label we might want to use coordinates instead\n        (in case there was a mislabel)\n        \"\"\"\n        #if the qpoint has no label it can\"t have a repetition along the band\n        #structure line object\n\n        if self.qpoints[index].label is None:\n            return [index]\n\n        list_index_qpoints = []\n        for i in range(self.nb_qpoints):\n            if self.qpoints[i].label == self.qpoints[index].label:\n                list_index_qpoints.append(i)\n\n        return list_index_qpoints",
        "rewrite": "def get_equivalent_qpoints(self, index):\n    if self.qpoints[index].label is None:\n        return [index]\n\n    list_index_qpoints = []\n    for i in range(self.nb_qpoints):\n        if self.qpoints[i].label == self.qpoints[index].label:\n            list_index_qpoints.append(i)\n\n    return list_index_qpoints"
    },
    {
        "original": "def score(self, model):\n        \"\"\"\n        Computes a score to measure how well the given `BayesianModel` fits to the data set.\n        (This method relies on the `local_score`-method that is implemented in each subclass.)\n\n        Parameters\n        ----------\n        model: `BayesianModel` instance\n            The Bayesian network that is to be scored. Nodes of the BayesianModel need to coincide\n            with column names of data set.\n\n        Returns\n        -------\n        score: float\n            A number indicating the degree of fit between data and model\n\n        Examples\n        -------\n        >>> import pandas as pd\n        >>> import numpy as np\n        >>> from pgmpy.estimators import K2Score\n        >>> # create random data sample with 3 variables, where B and C are identical:\n        >>> data = pd.DataFrame(np.random.randint(0, 5, size=(5000, 2)), columns=list('AB'))\n        >>> data['C'] = data['B']\n        >>> K2Score(data).score(BayesianModel([['A','B'], ['A','C']]))\n        -24242.367348745247\n        >>> K2Score(data).score(BayesianModel([['A','B'], ['B','C']]))\n        -16273.793897051042\n        \"\"\"\n\n        score = 0\n        for node in model.nodes():\n            score += self.local_score(node, model.predecessors(node))\n        score += self.structure_prior(model)\n        return score",
        "rewrite": "def score(self, model):\n    score = 0\n    for node in model.nodes():\n        score += self.local_score(node, model.predecessors(node))\n    score += self.structure_prior(model)\n    return score"
    },
    {
        "original": "def update_grads(self, X, dL_dW):\n        \"\"\"Update the gradients of marginal log likelihood with respect to the parameters of warping function\n\n        Parameters\n        ----------\n        X : array_like, shape = (n_samples, n_features)\n            The input BEFORE warping\n\n        dL_dW : array_like, shape = (n_samples, n_features)\n            The gradient of marginal log likelihood with respect to the Warped input\n\n        Math\n        ----\n        let w = f(x), the input after warping, then\n        dW_da = b * (1 - x^a)^(b - 1) * x^a * ln(x)\n        dW_db = - (1 - x^a)^b * ln(1 - x^a)\n        dL_da = dL_dW * dW_da\n        dL_db = dL_dW * dW_db\n        \"\"\"\n        for i_seq, i_fea in enumerate(self.warping_indices):\n            ai, bi = self.params[i_seq][0], self.params[i_seq][1]\n\n            # cache some value for save some computation\n            x_pow_a = np.power(self.X_normalized[:, i_fea], ai)\n\n            # compute gradient for ai, bi on all X\n            dz_dai = bi * np.power(1 - x_pow_a, bi-1) * x_pow_a * np.log(self.X_normalized[:, i_fea])\n            dz_dbi = - np.power(1 - x_pow_a, bi) * np.log(1 - x_pow_a)\n\n            # sum gradients on all the data\n            dL_dai = np.sum(dL_dW[:, i_fea] * dz_dai)\n            dL_dbi = np.sum(dL_dW[:, i_fea] * dz_dbi)\n            self.params[i_seq][0].gradient[:] = dL_dai\n            self.params[i_seq][1].gradient[:] = dL_dbi",
        "rewrite": "def update_grads(self, X, dL_dW):\n    for i_seq, i_fea in enumerate(self.warping_indices):\n        ai, bi = self.params[i_seq][0], self.params[i_seq][1]\n\n        x_pow_a = np.power(self.X_normalized[:, i_fea], ai)\n\n        dz_dai = bi * np.power(1 - x_pow_a, bi-1) * x_pow_a * np.log(self.X_normalized[:, i_fea])\n        dz_dbi = - np.power(1 - x_pow_a, bi) * np.log(1 - x_pow_a)\n\n        dL_dai = np.sum(dL_dW[:, i_fea] * dz_dai)\n        dL_dbi = np.sum(dL_dW[:, i_fea] * dz_dbi)\n        \n        self.params[i_seq][0].gradient[:] = dL_dai\n        self.params[i_seq][1].gradient[:] = dL_dbi"
    },
    {
        "original": "def _config(name, conf, default=None):\n    \"\"\"\n    Return a value for 'name' from the config file options. If the 'name' is\n    not in the config, the 'default' value is returned. This method converts\n    unicode values to str type under python 2.\n    \"\"\"\n    try:\n        value = conf[name]\n    except KeyError:\n        value = default\n    return salt.utils.data.decode(value, to_str=True)",
        "rewrite": "def _config(name, conf, default=None):\n    try:\n        value = conf[name]\n    except KeyError:\n        value = default\n    return salt.utils.data.decode(value, to_str=True)"
    },
    {
        "original": "def is_parameterized(val: Any) -> bool:\n    \"\"\"Returns whether the object is parameterized with any Symbols.\n\n    A value is parameterized when it has an `_is_parameterized_` method and\n    that method returns a truthy value, or if the value is an instance of\n    sympy.Basic.\n\n    Returns:\n        True if the gate has any unresolved Symbols\n        and False otherwise. If no implementation of the magic\n        method above exists or if that method returns NotImplemented,\n        this will default to False.\n    \"\"\"\n    if isinstance(val, sympy.Basic):\n        return True\n\n    getter = getattr(val, '_is_parameterized_', None)\n    result = NotImplemented if getter is None else getter()\n\n    if result is not NotImplemented:\n        return result\n    else:\n        return False",
        "rewrite": "from typing import Any\nimport sympy\n\ndef is_parameterized(val: Any) -> bool:\n    if isinstance(val, sympy.Basic):\n        return True\n\n    getter = getattr(val, '_is_parameterized_', None)\n    result = NotImplemented if getter is None else getter()\n\n    if result is not NotImplemented:\n        return result\n    else:\n        return False"
    },
    {
        "original": "def read_channel(self):\n        \"\"\"Generic handler that will read all the data from an SSH or telnet channel.\"\"\"\n        output = \"\"\n        self._lock_netmiko_session()\n        try:\n            output = self._read_channel()\n        finally:\n            # Always unlock the SSH channel, even on exception.\n            self._unlock_netmiko_session()\n        return output",
        "rewrite": "def read_channel(self):\n    output = \"\"\n    self._lock_netmiko_session()\n    try:\n        output = self._read_channel()\n    finally:\n        self._unlock_netmiko_session()\n    return output"
    },
    {
        "original": "def act(self, action):\n    \"\"\"Send a single action. This is a shortcut for `actions`.\"\"\"\n    if action and action.ListFields():  # Skip no-ops.\n      return self.actions(sc_pb.RequestAction(actions=[action]))",
        "rewrite": "def act(self, action):\n    if action and action.ListFields():\n        return self.actions(sc_pb.RequestAction(actions=[action]))"
    },
    {
        "original": "def format_index_raw(data):\n    \"\"\"Create DatetimeIndex for the Dataframe localized to the timezone provided\n    as the label of the third column.\n\n    Parameters\n    ----------\n    data: Dataframe\n        Must contain columns 'Year' and 'DOY'. Timezone must be found as the\n        label of the third (time) column.\n\n    Returns\n    -------\n    data: Dataframe\n        The data with a Datetime index localized to the provided timezone.\n    \"\"\"\n    tz_raw = data.columns[3]\n    timezone = TZ_MAP.get(tz_raw, tz_raw)\n    year = data.Year.apply(str)\n    jday = data.DOY.apply(lambda x: '{:03d}'.format(x))\n    time = data[tz_raw].apply(lambda x: '{:04d}'.format(x))\n    index = pd.to_datetime(year + jday + time, format=\"%Y%j%H%M\")\n    data = data.set_index(index)\n    data = data.tz_localize(timezone)\n    return data",
        "rewrite": "def format_index_raw(data):\n    tz_raw = data.columns[2]\n    timezone = TZ_MAP.get(tz_raw, tz_raw)\n    year = data['Year'].apply(str)\n    jday = data['DOY'].apply(lambda x: '{:03d}'.format(x))\n    time = data[tz_raw].apply(lambda x: '{:04d}'.format(x))\n    index = pd.to_datetime(year + jday + time, format=\"%Y%j%H%M\")\n    data = data.set_index(index)\n    data = data.tz_localize(timezone)\n    return data"
    },
    {
        "original": "def _get_col_items(mapping):\n    \"\"\"Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items",
        "rewrite": "def get_col_items(mapping):\n    from .variable import IndexVariable\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names:\n                col_items.extend(level_names)\n    return col_items"
    },
    {
        "original": "def DeleteArtifactsFromDatastore(artifact_names, reload_artifacts=True):\n  \"\"\"Deletes a list of artifacts from the data store.\"\"\"\n  artifacts_list = sorted(\n      REGISTRY.GetArtifacts(reload_datastore_artifacts=reload_artifacts))\n\n  to_delete = set(artifact_names)\n  deps = set()\n  for artifact_obj in artifacts_list:\n    if artifact_obj.name in to_delete:\n      continue\n\n    if GetArtifactDependencies(artifact_obj) & to_delete:\n      deps.add(str(artifact_obj.name))\n\n  if deps:\n    raise ValueError(\n        \"Artifact(s) %s depend(s) on one of the artifacts to delete.\" %\n        (\",\".join(deps)))\n\n  store = ArtifactCollection(rdfvalue.RDFURN(\"aff4:/artifact_store\"))\n  all_artifacts = list(store)\n\n  filtered_artifacts, found_artifact_names = set(), set()\n  for artifact_value in all_artifacts:\n    if artifact_value.name in to_delete:\n      found_artifact_names.add(artifact_value.name)\n    else:\n      filtered_artifacts.add(artifact_value)\n\n  if len(found_artifact_names) != len(to_delete):\n    not_found = to_delete - found_artifact_names\n    raise ValueError(\"Artifact(s) to delete (%s) not found.\" %\n                     \",\".join(not_found))\n\n  # TODO(user): this is ugly and error- and race-condition- prone.\n  # We need to store artifacts not in a *Collection, which is an\n  # append-only object, but in some different way that allows easy\n  # deletion. Possible option - just store each artifact in a separate object\n  # in the same folder.\n  store.Delete()\n\n  with data_store.DB.GetMutationPool() as pool:\n    for artifact_value in filtered_artifacts:\n      store.Add(artifact_value, mutation_pool=pool)\n\n  if data_store.RelationalDBEnabled():\n    for artifact_name in to_delete:\n      data_store.REL_DB.DeleteArtifact(str(artifact_name))\n\n  for artifact_value in to_delete:\n    REGISTRY.UnregisterArtifact(artifact_value)",
        "rewrite": "def delete_artifacts_from_datastore(artifact_names, reload_artifacts=True):\n    \"\"\"Deletes a list of artifacts from the data store.\"\"\"\n    artifacts_list = sorted(REGISTRY.GetArtifacts(reload_datastore_artifacts=reload_artifacts))\n\n    to_delete = set(artifact_names)\n    deps = set()\n\n    for artifact_obj in artifacts_list:\n        if artifact_obj.name in to_delete:\n            continue\n\n        if GetArtifactDependencies(artifact_obj) & to_delete:\n            deps.add(str(artifact_obj.name))\n\n    if deps:\n        raise ValueError(\"Artifact(s) %s depend(s) on one of the artifacts to delete.\" % (\",\".join(deps)))\n\n    store = ArtifactCollection(rdfvalue.RDFURN(\"aff4:/artifact_store\"))\n    all_artifacts = list(store)\n\n    filtered_artifacts, found_artifact_names = set(), set()\n\n    for artifact_value in all_artifacts:\n        if artifact_value.name in to_delete:\n            found_artifact_names.add(artifact_value.name)\n        else:\n            filtered_artifacts.add(artifact_value)\n\n    if len(found_artifact_names) != len(to_delete):\n        not_found = to_delete - found_artifact_names\n        raise ValueError(\"Artifact(s) to delete (%s) not found.\" % \",\".join(not_found))\n\n    store.Delete()\n\n    with data_store.DB.GetMutationPool() as pool:\n        for artifact_value in filtered_artifacts:\n            store.Add(artifact_value, mutation_pool=pool)\n\n    if data_store.RelationalDBEnabled():\n        for artifact_name in to_delete:\n            data_store.REL_DB.DeleteArtifact(str(artifact_name))\n\n    for artifact_value in to_delete:\n        REGISTRY.UnregisterArtifact(artifact_value)"
    },
    {
        "original": "def get_path_from_doc(full_doc):\n    \"\"\"\n    If `file:` is provided import the file.\n    \"\"\"\n    swag_path = full_doc.replace('file:', '').strip()\n    swag_type = swag_path.split('.')[-1]\n    return swag_path, swag_type",
        "rewrite": "def get_path_from_doc(full_doc):\n    swag_path = full_doc.replace('file:', '').strip()\n    swag_type = swag_path.split('.')[-1]\n    return swag_path, swag_type"
    },
    {
        "original": "def UploadImageAsset(client, url):\n  \"\"\"Uploads the image from the specified url.\n\n  Args:\n    client: An AdWordsClient instance.\n    url: The image URL.\n\n  Returns:\n    The ID of the uploaded image.\n  \"\"\"\n  # Initialize appropriate service.\n  asset_service = client.GetService('AssetService', version='v201809')\n\n  # Download the image.\n  image_request = requests.get(url)\n\n  # Create the image asset.\n  image_asset = {\n      'xsi_type': 'ImageAsset',\n      'imageData': image_request.content,\n      # This field is optional, and if provided should be unique.\n      # 'assetName': 'Image asset ' + str(uuid.uuid4()),\n  }\n\n  # Create the operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': image_asset\n  }\n\n  # Create the asset and return the ID.\n  result = asset_service.mutate([operation])\n\n  return result['value'][0]['assetId']",
        "rewrite": "def UploadImageAsset(client, url):\n    asset_service = client.GetService('AssetService', version='v201809')\n    image_request = requests.get(url)\n    image_asset = {'xsi_type': 'ImageAsset', 'imageData': image_request.content}\n    operation = {'operator': 'ADD', 'operand': image_asset}\n    result = asset_service.mutate([operation])\n    \n    return result['value'][0]['assetId']"
    },
    {
        "original": "def p_unwind(p):\n    \"\"\"\n    unwind : UNWIND_PROTECT stmt_list UNWIND_PROTECT_CLEANUP stmt_list END_UNWIND_PROTECT\n    \"\"\"\n    p[0] = node.try_catch(\n        try_stmt=p[2], catch_stmt=node.expr_list(), finally_stmt=p[4])",
        "rewrite": "def p_unwind(p):\n    p[0] = node.try_catch(\n        try_stmt = p[2], \n        catch_stmt = node.expr_list(), \n        finally_stmt = p[4])"
    },
    {
        "original": "def list_symbols(self, regex=None, as_of=None, **kwargs):\n        \"\"\"\n         Return the symbols in this library.\n\n         Parameters\n         ----------\n         as_of : `datetime.datetime`\n            filter symbols valid at given time\n         regex : `str`\n             filter symbols by the passed in regular expression\n         kwargs :\n             kwarg keys are used as fields to query for symbols with metadata matching\n             the kwargs query\n\n         Returns\n         -------\n         String list of symbols in the library\n        \"\"\"\n\n        # Skip aggregation pipeline\n        if not (regex or as_of or kwargs):\n            return self.distinct('symbol')\n\n        # Index-based query part\n        index_query = {}\n        if as_of is not None:\n            index_query['start_time'] = {'$lte': as_of}\n\n        if regex or as_of:\n            # make sure that symbol is present in query even if only as_of is specified to avoid document scans\n            # see 'Pipeline Operators and Indexes' at\n            # https://docs.mongodb.com/manual/core/aggregation-pipeline/#aggregation-pipeline-operators-and-performance\n            index_query['symbol'] = {'$regex': regex or '^'}\n\n        # Document query part\n        data_query = {}\n        if kwargs:\n            for k, v in six.iteritems(kwargs):\n                data_query['metadata.' + k] = v\n\n        # Sort using index, relying on https://docs.mongodb.com/manual/core/aggregation-pipeline-optimization/\n        pipeline = [{'$sort': {'symbol': pymongo.ASCENDING,\n                               'start_time': pymongo.DESCENDING}}]\n\n        # Index-based filter on symbol and start_time\n        if index_query:\n            pipeline.append({'$match': index_query})\n        # Group by 'symbol' and get the latest known data\n        pipeline.append({'$group': {'_id': '$symbol',\n                                    'metadata': {'$first': '$metadata'}}})\n        # Match the data fields\n        if data_query:\n            pipeline.append({'$match': data_query})\n        # Return only 'symbol' field value\n        pipeline.append({'$project': {'_id': 0, 'symbol': '$_id'}})\n\n        return sorted(r['symbol'] for r in self.aggregate(pipeline))",
        "rewrite": "def list_symbols(self, regex=None, as_of=None, **kwargs):\n    if not (regex or as_of or kwargs):\n        return self.distinct('symbol')\n\n    index_query = {}\n    if as_of is not None:\n        index_query['start_time'] = {'$lte': as_of}\n\n    if regex or as_of:\n        index_query['symbol'] = {'$regex': regex or '^'}\n\n    data_query = {}\n    if kwargs:\n        for k, v in six.iteritems(kwargs):\n            data_query['metadata.' + k] = v\n\n    pipeline = [{'$sort': {'symbol': pymongo.ASCENDING, 'start_time': pymongo.DESCENDING}}]\n\n    if index_query:\n        pipeline.append({'$match': index_query})\n    pipeline.append({'$group': {'_id': '$symbol', 'metadata': {'$first': '$metadata'}}})\n    \n    if data_query:\n        pipeline.append({'$match': data_query})\n    \n    pipeline.append({'$project': {'_id': 0, 'symbol': '$_id'}})\n\n    return sorted(r['symbol'] for r in self.aggregate(pipeline))"
    },
    {
        "original": "def select(self, *attributes):\n        \"\"\" Adds the attribute to the $select parameter\n\n        :param str attributes: the attributes tuple to select.\n         If empty, the on_attribute previously set is added.\n        :rtype: Query\n        \"\"\"\n        if attributes:\n            for attribute in attributes:\n                attribute = self.protocol.convert_case(\n                    attribute) if attribute and isinstance(attribute,\n                                                           str) else None\n                if attribute:\n                    if '/' in attribute:\n                        # only parent attribute can be selected\n                        attribute = attribute.split('/')[0]\n                    self._selects.add(attribute)\n        else:\n            if self._attribute:\n                self._selects.add(self._attribute)\n\n        return self",
        "rewrite": "def select(self, *attributes):\n    if attributes:\n        for attribute in attributes:\n            attribute = self.protocol.convert_case(attribute) if attribute and isinstance(attribute, str) else None\n            if attribute:\n                if '/' in attribute:\n                    attribute = attribute.split('/')[0]\n                self._selects.add(attribute)\n    else:\n        if self._attribute:\n            self._selects.add(self._attribute)\n    return self"
    },
    {
        "original": "def _MultiNotifyQueue(self, queue, notifications, mutation_pool=None):\n    \"\"\"Does the actual queuing.\"\"\"\n    notification_list = []\n    now = rdfvalue.RDFDatetime.Now()\n    for notification in notifications:\n      if not notification.first_queued:\n        notification.first_queued = (\n            self.frozen_timestamp or rdfvalue.RDFDatetime.Now())\n      else:\n        diff = now - notification.first_queued\n        if diff.seconds >= self.notification_expiry_time:\n          # This notification has been around for too long, we drop it.\n          logging.debug(\"Dropping notification: %s\", str(notification))\n          continue\n\n      notification_list.append(notification)\n\n    mutation_pool.CreateNotifications(\n        self.GetNotificationShard(queue), notification_list)",
        "rewrite": "def _multi_notify_queue(self, queue, notifications, mutation_pool=None):\n    notification_list = []\n    now = rdfvalue.RDFDatetime.Now()\n  \n    for notification in notifications:\n        if not notification.first_queued:\n            notification.first_queued = (self.frozen_timestamp or rdfvalue.RDFDatetime.Now())\n        else:\n            diff = now - notification.first_queued\n            if diff.seconds >= self.notification_expiry_time:\n                # This notification has been around for too long, we drop it.\n                logging.debug(\"Dropping notification: %s\", str(notification))\n                continue\n\n        notification_list.append(notification)\n\n    mutation_pool.CreateNotifications(self.GetNotificationShard(queue), notification_list)"
    },
    {
        "original": "def terminate(self):\n        \"\"\"Stops the worker processes immediately without completing\n        outstanding work. When the pool object is garbage collected\n        terminate() will be called immediately.\"\"\"\n        self.close()\n\n        # Clearing the job queue\n        try:\n            while 1:\n                self._workq.get_nowait()\n        except queue.Empty:\n            pass\n\n        # Send one sentinel for each worker thread: each thread will die\n        # eventually, leaving the next sentinel for the next thread\n        for _ in self._workers:\n            self._workq.put(SENTINEL)",
        "rewrite": "def terminate(self):\n    self.close()\n\n    try:\n        while 1:\n            self._workq.get_nowait()\n    except queue.Empty:\n        pass\n\n    for _ in self._workers:\n        self._workq.put(SENTINEL)"
    },
    {
        "original": "def get_email_confirmation_url(self, request, emailconfirmation):\n        \"\"\"Constructs the email confirmation (activation) url.\n\n        Note that if you have architected your system such that email\n        confirmations are sent outside of the request context `request`\n        can be `None` here.\n        \"\"\"\n        url = reverse(\n            \"account_confirm_email\",\n            args=[emailconfirmation.key])\n        ret = build_absolute_uri(\n            request,\n            url)\n        return ret",
        "rewrite": "def get_email_confirmation_url(self, request, emailconfirmation):\n    url = reverse(\"account_confirm_email\", args=[emailconfirmation.key])\n    ret = build_absolute_uri(request, url)\n    return ret"
    },
    {
        "original": "def engine_from_environment() -> Engine:\n    \"\"\"Returns an Engine instance configured using environment variables.\n\n    If the environment variables are set, but incorrect, an authentication\n    failure will occur when attempting to run jobs on the engine.\n\n    Required Environment Variables:\n        QUANTUM_ENGINE_PROJECT: The name of a google cloud project, with the\n            quantum engine enabled, that you have access to.\n        QUANTUM_ENGINE_API_KEY: An API key for the google cloud project named\n            by QUANTUM_ENGINE_PROJECT.\n\n    Raises:\n        EnvironmentError: The environment variables are not set.\n    \"\"\"\n    api_key = os.environ.get(ENV_API_KEY)\n    if not api_key:\n        raise EnvironmentError(\n            'Environment variable {} is not set.'.format(ENV_API_KEY))\n\n    default_project_id = os.environ.get(ENV_DEFAULT_PROJECT_ID)\n\n    return Engine(api_key=api_key, default_project_id=default_project_id)",
        "rewrite": "import os\n\ndef engine_from_environment() -> Engine:\n    api_key = os.environ.get(\"QUANTUM_ENGINE_API_KEY\")\n    if not api_key:\n        raise EnvironmentError('Environment variable QUANTUM_ENGINE_API_KEY is not set.')\n\n    default_project_id = os.environ.get(\"QUANTUM_ENGINE_PROJECT\")\n\n    return Engine(api_key=api_key, default_project_id=default_project_id)"
    },
    {
        "original": "def rate_limit(function):\n        \"\"\"Return a decorator that enforces API request limit guidelines.\n\n        We are allowed to make a API request every api_request_delay seconds as\n        specified in praw.ini. This value may differ from reddit to reddit. For\n        reddit.com it is 2. Any function decorated with this will be forced to\n        delay _rate_delay seconds from the calling of the last function\n        decorated with this before executing.\n\n        This decorator must be applied to a RateLimitHandler class method or\n        instance method as it assumes `rl_lock` and `last_call` are available.\n\n        \"\"\"\n        @wraps(function)\n        def wrapped(cls, _rate_domain, _rate_delay, **kwargs):\n            cls.rl_lock.acquire()\n            lock_last = cls.last_call.setdefault(_rate_domain, [Lock(), 0])\n            with lock_last[0]:  # Obtain the domain specific lock\n                cls.rl_lock.release()\n                # Sleep if necessary, then perform the request\n                now = timer()\n                delay = lock_last[1] + _rate_delay - now\n                if delay > 0:\n                    now += delay\n                    time.sleep(delay)\n                lock_last[1] = now\n                return function(cls, **kwargs)\n        return wrapped",
        "rewrite": "from functools import wraps\nfrom threading import Lock\nimport time\n\ndef rate_limit(function):\n    @wraps(function)\n    def wrapped(cls, _rate_domain, _rate_delay, **kwargs):\n        cls.rl_lock.acquire()\n        lock_last = cls.last_call.setdefault(_rate_domain, [Lock(), 0])\n        with lock_last[0]:\n            cls.rl_lock.release()\n            now = time.time()\n            delay = lock_last[1] + _rate_delay - now\n            if delay > 0:\n                now += delay\n                time.sleep(delay)\n            lock_last[1] = now\n            return function(cls, **kwargs)\n    return wrapped"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'dialog_node') and self.dialog_node is not None:\n            _dict['dialog_node'] = self.dialog_node\n        if hasattr(self, 'title') and self.title is not None:\n            _dict['title'] = self.title\n        if hasattr(self, 'conditions') and self.conditions is not None:\n            _dict['conditions'] = self.conditions\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'dialog_node') and self.dialog_node is not None:\n        _dict['dialog_node'] = self.dialog_node\n    if hasattr(self, 'title') and self.title is not None:\n        _dict['title'] = self.title\n    if hasattr(self, 'conditions') and self.conditions is not None:\n        _dict['conditions'] = self.conditions\n    return _dict"
    },
    {
        "original": "def grant_exists(grant,\n                 database,\n                 user,\n                 host='localhost',\n                 grant_option=False,\n                 escape=True,\n                 **connection_args):\n    \"\"\"\n    Checks to see if a grant exists in the database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mysql.grant_exists \\\n             'SELECT,INSERT,UPDATE,...' 'database.*' 'frank' 'localhost'\n    \"\"\"\n\n    server_version = salt.utils.data.decode(version(**connection_args))\n    if not server_version:\n        last_err = __context__['mysql.error']\n        err = 'MySQL Error: Unable to fetch current server version. Last error was: \"{}\"'.format(last_err)\n        log.error(err)\n        return False\n    if 'ALL' in grant:\n        if salt.utils.versions.version_cmp(server_version, '8.0') >= 0 and \\\n           'MariaDB' not in server_version:\n            grant = ','.join([i for i in __all_privileges__])\n        else:\n            grant = 'ALL PRIVILEGES'\n\n    try:\n        target = __grant_generate(\n            grant, database, user, host, grant_option, escape\n        )\n    except Exception:\n        log.error('Error during grant generation.')\n        return False\n\n    grants = user_grants(user, host, **connection_args)\n\n    if grants is False:\n        log.error('Grant does not exist or may not be ordered properly. In some cases, '\n                  'this could also indicate a connection error. Check your configuration.')\n        return False\n\n    # Combine grants that match the same database\n    _grants = {}\n    for grant in grants:\n        grant_token = _grant_to_tokens(grant)\n        if grant_token['database'] not in _grants:\n            _grants[grant_token['database']] = {'user': grant_token['user'],\n                                                'database': grant_token['database'],\n                                                'host': grant_token['host'],\n                                                'grant': grant_token['grant']}\n        else:\n            _grants[grant_token['database']]['grant'].extend(grant_token['grant'])\n\n    target_tokens = _grant_to_tokens(target)\n    for database, grant_tokens in _grants.items():\n        try:\n            _grant_tokens = {}\n            _target_tokens = {}\n\n            _grant_matches = [True if i in grant_tokens['grant']\n                              else False for i in target_tokens['grant']]\n\n            for item in ['user', 'database', 'host']:\n                _grant_tokens[item] = grant_tokens[item].replace('\"', '').replace('\\\\', '').replace('`', '')\n                _target_tokens[item] = target_tokens[item].replace('\"', '').replace('\\\\', '').replace('`', '')\n\n            if _grant_tokens['user'] == _target_tokens['user'] and \\\n                    _grant_tokens['database'] == _target_tokens['database'] and \\\n                    _grant_tokens['host'] == _target_tokens['host'] and \\\n                    all(_grant_matches):\n                return True\n            else:\n                log.debug('grants mismatch \\'%s\\'<>\\'%s\\'', grant_tokens, target_tokens)\n\n        except Exception as exc:  # Fallback to strict parsing\n            log.exception(exc)\n            if grants is not False and target in grants:\n                log.debug('Grant exists.')\n                return True\n\n    log.debug('Grant does not exist, or is perhaps not ordered properly?')\n    return False",
        "rewrite": "def grant_exists(grant, database, user, host='localhost', grant_option=False, escape=True, **connection_args):\n    server_version = salt.utils.data.decode(version(**connection_args))\n    if not server_version:\n        last_err = __context__['mysql.error']\n        err = 'MySQL Error: Unable to fetch current server version. Last error was: \"{}\"'.format(last_err)\n        log.error(err)\n        return False\n    if 'ALL' in grant:\n        if salt.utils.versions.version_cmp(server_version, '8.0') >= 0 and 'MariaDB' not in server_version:\n            grant = ','.join([i for i in __all_privileges__])\n        else:\n            grant = 'ALL PRIVILEGES'\n\n    try:\n        target = __grant_generate(grant, database, user, host, grant_option, escape)\n    except Exception:\n        log.error('Error during grant generation.')\n        return False\n\n    grants = user_grants(user, host, **connection_args)\n\n    if grants is False:\n        log.error('Grant does not exist or may not be ordered properly. In some cases, this could also indicate a connection error. Check your configuration.')\n        return False\n\n    _grants = {}\n    for grant_item in grants:\n        grant_token = _grant_to_tokens(grant_item)\n        if grant_token['database'] not in _grants:\n            _grants[grant_token['database']] = {'user': grant_token['user'],\n                                                'database': grant_token['database'],\n                                                'host': grant_token['host'],\n                                                'grant': grant_token['grant']}\n        else:\n            _grants[grant_token['database']]['grant'].extend(grant_token['grant'])\n\n    target_tokens = _grant_to_tokens(target)\n    for database, grant_tokens in _grants.items():\n        try:\n            _grant_tokens = {}\n            _target_tokens = {}\n            _grant_matches = [True if i in grant_tokens['grant'] else False for i in target_tokens['grant']]\n            \n            for item in ['user', 'database', 'host']:\n                _grant_tokens[item] = grant_tokens[item].replace('\"', '').replace('\\\\', '').replace('`', '')\n                _target_tokens[item] = target_tokens[item].replace('\"', '').replace('\\\\', '').replace('`', '')\n            \n            if _grant_tokens['user'] == _target_tokens['user'] and _grant_tokens['database'] == _target_tokens['database'] and _grant_tokens['host'] == _target_tokens['host'] and all(_grant_matches):\n                return True\n            else:\n                log.debug('grants mismatch \\'%s\\'<>\\'%s\\'', grant_tokens, target_tokens)\n        except Exception as exc:\n            log.exception(exc)\n            if grants is not False and target in grants:\n                log.debug('Grant exists.')\n                return True\n                \n    log.debug('Grant does not exist, or is perhaps not ordered properly?')\n    return False"
    },
    {
        "original": "def polynomial_multiply_mod( m1, m2, polymod, p ):\n  \"\"\"Polynomial multiplication modulo a polynomial over ints mod p.\n\n  Polynomials are represented as lists of coefficients\n  of increasing powers of x.\"\"\"\n\n  # This is just a seat-of-the-pants implementation.\n\n  # This module has been tested only by extensive use\n  # in calculating modular square roots.\n\n  # Initialize the product to zero:\n\n  prod = ( len( m1 ) + len( m2 ) - 1 ) * [0]\n\n  # Add together all the cross-terms:\n\n  for i in range( len( m1 ) ):\n    for j in range( len( m2 ) ):\n      prod[i+j] = ( prod[i+j] + m1[i] * m2[j] ) % p\n\n  return polynomial_reduce_mod( prod, polymod, p )",
        "rewrite": "def polynomial_multiply_mod(m1, m2, polymod, p):\n    prod = [0] * (len(m1) + len(m2) - 1)\n\n    for i in range(len(m1)):\n        for j in range(len(m2)):\n            prod[i+j] = (prod[i+j] + m1[i] * m2[j]) % p\n\n    return polynomial_reduce_mod(prod, polymod, p)"
    },
    {
        "original": "def next_partname(self, template):\n        \"\"\"Return a |PackURI| instance representing partname matching *template*.\n\n        The returned part-name has the next available numeric suffix to distinguish it\n        from other parts of its type. *template* is a printf (%)-style template string\n        containing a single replacement item, a '%d' to be used to insert the integer\n        portion of the partname. Example: \"/word/header%d.xml\"\n        \"\"\"\n        partnames = {part.partname for part in self.iter_parts()}\n        for n in range(1, len(partnames) + 2):\n            candidate_partname = template % n\n            if candidate_partname not in partnames:\n                return PackURI(candidate_partname)",
        "rewrite": "def next_partname(self, template):\n    partnames = {part.partname for part in self.iter_parts()}\n    for n in range(1, len(partnames) + 2):\n        candidate_partname = template % n\n        if candidate_partname not in partnames:\n            return PackURI(candidate_partname)"
    },
    {
        "original": "def filesfile_string(self):\n        \"\"\"String with the list of files and prefixes needed to execute ABINIT.\"\"\"\n        lines = []\n        app = lines.append\n\n        #optic.in     ! Name of input file\n        #optic.out    ! Unused\n        #optic        ! Root name for all files that will be produced\n        app(self.input_file.path)                           # Path to the input file\n        app(os.path.join(self.workdir, \"unused\"))           # Path to the output file\n        app(os.path.join(self.workdir, self.prefix.odata))  # Prefix for output data\n\n        return \"\\n\".join(lines)",
        "rewrite": "def filesfile_string(self):\n    lines = []\n    app = lines.append\n\n    app(self.input_file.path)\n    app(os.path.join(self.workdir, \"unused\"))\n    app(os.path.join(self.workdir, self.prefix.odata))\n\n    return \"\\n\".join(lines)"
    },
    {
        "original": "def _get_repo_details(saltenv):\n    \"\"\"\n    Return repo details for the specified saltenv as a namedtuple\n    \"\"\"\n    contextkey = 'winrepo._get_repo_details.{0}'.format(saltenv)\n\n    if contextkey in __context__:\n        (winrepo_source_dir, local_dest, winrepo_file) = __context__[contextkey]\n    else:\n        winrepo_source_dir = __opts__['winrepo_source_dir']\n        dirs = [__opts__['cachedir'], 'files', saltenv]\n        url_parts = _urlparse(winrepo_source_dir)\n        dirs.append(url_parts.netloc)\n        dirs.extend(url_parts.path.strip('/').split('/'))\n        local_dest = os.sep.join(dirs)\n\n        winrepo_file = os.path.join(local_dest, 'winrepo.p')  # Default\n        # Check for a valid windows file name\n        if not re.search(r'[\\/:*?\"<>|]',\n                         __opts__['winrepo_cachefile'],\n                         flags=re.IGNORECASE):\n            winrepo_file = os.path.join(\n                local_dest,\n                __opts__['winrepo_cachefile']\n                )\n        else:\n            log.error(\n                'minion configuration option \\'winrepo_cachefile\\' has been '\n                'ignored as its value (%s) is invalid. Please ensure this '\n                'option is set to a valid filename.',\n                __opts__['winrepo_cachefile']\n            )\n\n        # Do some safety checks on the repo_path as its contents can be removed,\n        # this includes check for bad coding\n        system_root = os.environ.get('SystemRoot', r'C:\\Windows')\n        if not salt.utils.path.safe_path(\n                path=local_dest,\n                allow_path='\\\\'.join([system_root, 'TEMP'])):\n\n            raise CommandExecutionError(\n                'Attempting to delete files from a possibly unsafe location: '\n                '{0}'.format(local_dest)\n            )\n\n        __context__[contextkey] = (winrepo_source_dir, local_dest, winrepo_file)\n\n    try:\n        os.makedirs(local_dest)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise CommandExecutionError(\n                'Failed to create {0}: {1}'.format(local_dest, exc)\n            )\n\n    winrepo_age = -1\n    try:\n        stat_result = os.stat(winrepo_file)\n        mtime = stat_result.st_mtime\n        winrepo_age = time.time() - mtime\n    except OSError as exc:\n        if exc.errno != errno.ENOENT:\n            raise CommandExecutionError(\n                'Failed to get age of {0}: {1}'.format(winrepo_file, exc)\n            )\n    except AttributeError:\n        # Shouldn't happen but log if it does\n        log.warning('st_mtime missing from stat result %s', stat_result)\n    except TypeError:\n        # Shouldn't happen but log if it does\n        log.warning('mtime of %s (%s) is an invalid type', winrepo_file, mtime)\n\n    repo_details = collections.namedtuple(\n        'RepoDetails',\n        ('winrepo_source_dir', 'local_dest', 'winrepo_file', 'winrepo_age')\n    )\n    return repo_details(winrepo_source_dir, local_dest, winrepo_file, winrepo_age)",
        "rewrite": "def _get_repo_details(saltenv):\n    contextkey = 'winrepo._get_repo_details.{0}'.format(saltenv)\n\n    if contextkey in __context__:\n        (winrepo_source_dir, local_dest, winrepo_file) = __context__[contextkey]\n    else:\n        winrepo_source_dir = __opts__['winrepo_source_dir']\n        dirs = [__opts__['cachedir'], 'files', saltenv]\n        url_parts = _urlparse(winrepo_source_dir)\n        dirs.append(url_parts.netloc)\n        dirs.extend(url_parts.path.strip('/').split('/'))\n        local_dest = os.sep.join(dirs)\n\n        winrepo_file = os.path.join(local_dest, 'winrepo.p')\n        \n        if not re.search(r'[\\/:*?\"<>|]', __opts__['winrepo_cachefile'], flags=re.IGNORECASE):\n            winrepo_file = os.path.join(local_dest, __opts__['winrepo_cachefile'])\n        else:\n            log.error(\n                'minion configuration option \\'winrepo_cachefile\\' has been '\n                'ignored as its value (%s) is invalid. Please ensure this '\n                'option is set to a valid filename.',\n                __opts__['winrepo_cachefile']\n            )\n\n        system_root = os.environ.get('SystemRoot', r'C:\\Windows')\n        if not salt.utils.path.safe_path(path=local_dest, allow_path='\\\\'.join([system_root, 'TEMP'])):\n            raise CommandExecutionError(\n                'Attempting to delete files from a possibly unsafe location: {0}'.format(local_dest)\n            )\n\n        __context__[contextkey] = (winrepo_source_dir, local_dest, winrepo_file)\n\n    try:\n        os.makedirs(local_dest)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise CommandExecutionError(\n                'Failed to create {0}: {1}'.format(local_dest, exc)\n            )\n\n    winrepo_age = -1\n    try:\n        stat_result = os.stat(winrepo_file)\n        mtime = stat_result.st_mtime\n        winrepo_age = time.time() - mtime\n    except OSError as exc:\n        if exc.errno != errno.ENOENT:\n            raise CommandExecutionError(\n                'Failed to get age of {0}: {1}'.format(winrepo_file, exc)\n            )\n    except AttributeError:\n        log.warning('st_mtime missing from stat result %s', stat_result)\n    except TypeError:\n        log.warning('mtime of %s (%s) is an invalid type', winrepo_file, mtime)\n\n    repo_details = collections.namedtuple(\n        'RepoDetails',\n        ('winrepo_source_dir', 'local_dest', 'winrepo_file', 'winrepo_age')\n    )\n    return repo_details(winrepo_source_dir, local_dest, winrepo_file, winrepo_age)"
    },
    {
        "original": "def get_user_push_restrictions(self):\n        \"\"\"\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser`\n        \"\"\"\n        if self._user_push_restrictions is github.GithubObject.NotSet:\n            return None\n        return github.PaginatedList.PaginatedList(\n            github.NamedUser.NamedUser,\n            self._requester,\n            self._user_push_restrictions,\n            None\n        )",
        "rewrite": "def get_user_push_restrictions(self):\n    if self._user_push_restrictions is github.GithubObject.NotSet:\n        return None\n    return github.PaginatedList.PaginatedList(\n        github.NamedUser.NamedUser,\n        self._requester,\n        self._user_push_restrictions,\n        None\n    )"
    },
    {
        "original": "async def _reload_message(self):\n        \"\"\"\n        Re-fetches this message to reload the sender and chat entities,\n        along with their input versions.\n        \"\"\"\n        try:\n            chat = await self.get_input_chat() if self.is_channel else None\n            msg = await self._client.get_messages(chat, ids=self.id)\n        except ValueError:\n            return  # We may not have the input chat/get message failed\n        if not msg:\n            return  # The message may be deleted and it will be None\n\n        self._sender = msg._sender\n        self._input_sender = msg._input_sender\n        self._chat = msg._chat\n        self._input_chat = msg._input_chat\n        self._via_bot = msg._via_bot\n        self._via_input_bot = msg._via_input_bot\n        self._forward = msg._forward\n        self._action_entities = msg._action_entities",
        "rewrite": "async def _reload_message(self):\n    try:\n        chat = await self.get_input_chat() if self.is_channel else None\n        msg = await self._client.get_messages(chat, ids=self.id)\n    except ValueError:\n        return\n    if not msg:\n        return\n\n    self._sender = msg._sender\n    self._input_sender = msg._input_sender\n    self._chat = msg._chat\n    self._input_chat = msg._input_chat\n    self._via_bot = msg._via_bot\n    self._via_input_bot = msg._via_input_bot\n    self._forward = msg._forward\n    self._action_entities = msg._action_entities"
    },
    {
        "original": "def _get_sliced_variables(var_list):\n  \"\"\"Separates the sliced (partitioned) and unsliced variables in var_list.\n\n  Args:\n    var_list: a list of variables.\n\n  Returns:\n    A list of unsliced variables in var_list, and a dict mapping names to parts\n    for the sliced variables in var_list.\n  \"\"\"\n  unsliced_variables = []\n  sliced_variables = collections.defaultdict(lambda: [])\n  for var in var_list:\n    if var._save_slice_info:\n      sliced_variables[var._save_slice_info.full_name].append(var)\n    else:\n      unsliced_variables.append(var)\n  return unsliced_variables, sliced_variables",
        "rewrite": "from collections import defaultdict\n\ndef get_sliced_variables(var_list):\n    unsliced_variables = []\n    sliced_variables = defaultdict(lambda: [])\n    \n    for var in var_list:\n        if var._save_slice_info:\n            sliced_variables[var._save_slice_info.full_name].append(var)\n        else:\n            unsliced_variables.append(var)\n    \n    return unsliced_variables, sliced_variables"
    },
    {
        "original": "def calculate_3D_elastic_energy(self, film, match, elasticity_tensor=None,\n                                    include_strain=False):\n        \"\"\"\n        Calculates the multi-plane elastic energy. Returns 999 if no elastic\n        tensor was given on init\n\n        Args:\n            film(Structure): conventional standard structure for the film\n            match(dictionary) : match dictionary from substrate analyzer\n            elasticity_tensor(ElasticTensor): elasticity tensor for the film\n            include_strain(bool): include strain in the output or not; changes\n             return from just the energy to a tuple with the energy and strain\n             in voigt notation\n        \"\"\"\n        if elasticity_tensor is None:\n            return 9999\n\n        # Get the appropriate surface structure\n        struc = SlabGenerator(self.film, match['film_miller'], 20, 15,\n                              primitive=False).get_slab().oriented_unit_cell\n\n        # Generate 3D lattice vectors for film super lattice\n        film_matrix = list(match['film_sl_vecs'])\n        film_matrix.append(np.cross(film_matrix[0], film_matrix[1]))\n\n        # Generate 3D lattice vectors for substrate super lattice\n        # Out of plane substrate super lattice has to be same length as\n        # Film out of plane vector to ensure no extra deformation in that\n        # direction\n        substrate_matrix = list(match['sub_sl_vecs'])\n        temp_sub = np.cross(substrate_matrix[0], substrate_matrix[1])\n        temp_sub = temp_sub * fast_norm(film_matrix[2]) / fast_norm(temp_sub)\n        substrate_matrix.append(temp_sub)\n\n        transform_matrix = np.transpose(np.linalg.solve(film_matrix,\n                                                        substrate_matrix))\n\n        dfm = Deformation(transform_matrix)\n\n        strain = dfm.green_lagrange_strain.convert_to_ieee(struc, initial_fit=False)\n\n        energy_density = elasticity_tensor.energy_density(\n            strain)\n\n        if include_strain:\n            return (film.volume * energy_density / len(film.sites), strain.von_mises_strain)\n        else:\n            return film.volume * energy_density / len(film.sites)",
        "rewrite": "def calculate_3D_elastic_energy(self, film, match, elasticity_tensor=None, \n                                include_strain=False):\n    if elasticity_tensor is None:\n        return 9999\n\n    struc = SlabGenerator(self.film, match['film_miller'], 20, 15, primitive=False).get_slab().oriented_unit_cell\n\n    film_matrix = list(match['film_sl_vecs'])\n    film_matrix.append(np.cross(film_matrix[0], film_matrix[1]))\n\n    substrate_matrix = list(match['sub_sl_vecs'])\n    temp_sub = np.cross(substrate_matrix[0], substrate_matrix[1])\n    temp_sub = temp_sub * fast_norm(film_matrix[2]) / fast_norm(temp_sub)\n    substrate_matrix.append(temp_sub)\n\n    transform_matrix = np.transpose(np.linalg.solve(film_matrix, substrate_matrix))\n\n    dfm = Deformation(transform_matrix)\n\n    strain = dfm.green_lagrange_strain.convert_to_ieee(struc, initial_fit=False)\n\n    energy_density = elasticity_tensor.energy_density(strain)\n\n    if include_strain:\n        return (film.volume * energy_density / len(film.sites), strain.von_mises_strain)\n    else:\n        return film.volume * energy_density / len(film.sites)"
    },
    {
        "original": "def _encode_datetime(name, value, dummy0, dummy1):\n    \"\"\"Encode datetime.datetime.\"\"\"\n    millis = _datetime_to_millis(value)\n    return b\"\\x09\" + name + _PACK_LONG(millis)",
        "rewrite": "def _encode_datetime(name, value, dummy0, dummy1):\n    millis = _datetime_to_millis(value)\n    return b\"\\x09\" + name + _PACK_LONG(millis)"
    },
    {
        "original": "def save(self, filename):\n        \"\"\"\n        Save the current buffer to `filename`\n\n        Exisiting files with the same name will be overwritten.\n\n        :param str filename: the name of the file to save to\n        \"\"\"\n        with open(filename, \"wb\") as fd:\n            fd.write(self.__buff)",
        "rewrite": "def save(self, filename):\n    with open(filename, \"wb\") as fd:\n        fd.write(self.__buff)"
    },
    {
        "original": "def get_dimension_type(self, dim):\n        \"\"\"Get the type of the requested dimension.\n\n        Type is determined by Dimension.type attribute or common\n        type of the dimension values, otherwise None.\n\n        Args:\n            dimension: Dimension to look up by name or by index\n\n        Returns:\n            Declared type of values along the dimension\n        \"\"\"\n        dim = self.get_dimension(dim)\n        if dim is None:\n            return None\n        elif dim.type is not None:\n            return dim.type\n        elif dim in self.vdims:\n            return np.float64\n        return self.interface.dimension_type(self, dim)",
        "rewrite": "def get_dimension_type(self, dim):\n    dim = self.get_dimension(dim)\n    if dim is None:\n        return None\n    elif dim.type is not None:\n        return dim.type\n    elif dim in self.vdims:\n        return np.float64\n    return self.interface.dimension_type(self, dim)"
    },
    {
        "original": "def set_base_prompt(\n        self, pri_prompt_terminator=\">\", alt_prompt_terminator=\"]\", delay_factor=1\n    ):\n        \"\"\"\n        Sets self.base_prompt\n\n        Used as delimiter for stripping of trailing prompt in output.\n\n        Should be set to something that is general and applies in multiple contexts. For Comware\n        this will be the router prompt with < > or [ ] stripped off.\n\n        This will be set on logging in, but not when entering system-view\n        \"\"\"\n        log.debug(\"In set_base_prompt\")\n        delay_factor = self.select_delay_factor(delay_factor)\n        self.clear_buffer()\n        self.write_channel(self.RETURN)\n        time.sleep(0.5 * delay_factor)\n\n        prompt = self.read_channel()\n        prompt = self.normalize_linefeeds(prompt)\n\n        # If multiple lines in the output take the last line\n        prompt = prompt.split(self.RESPONSE_RETURN)[-1]\n        prompt = prompt.strip()\n\n        # Check that ends with a valid terminator character\n        if not prompt[-1] in (pri_prompt_terminator, alt_prompt_terminator):\n            raise ValueError(\"Router prompt not found: {0}\".format(prompt))\n\n        # Strip off any leading HRP_. characters for USGv5 HA\n        prompt = re.sub(r\"^HRP_.\", \"\", prompt, flags=re.M)\n\n        # Strip off leading and trailing terminator\n        prompt = prompt[1:-1]\n        prompt = prompt.strip()\n        self.base_prompt = prompt\n        log.debug(\"prompt: {0}\".format(self.base_prompt))\n\n        return self.base_prompt",
        "rewrite": "def set_base_prompt(self, pri_prompt_terminator=\">\", alt_prompt_terminator=\"]\", delay_factor=1):\n    log.debug(\"In set_base_prompt\")\n    delay_factor = self.select_delay_factor(delay_factor)\n    self.clear_buffer()\n    self.write_channel(self.RETURN)\n    time.sleep(0.5 * delay_factor)\n\n    prompt = self.read_channel()\n    prompt = self.normalize_linefeeds(prompt)\n\n    prompt = prompt.split(self.RESPONSE_RETURN)[-1].strip()\n\n    if not prompt[-1] in (pri_prompt_terminator, alt_prompt_terminator):\n        raise ValueError(\"Router prompt not found: {0}\".format(prompt))\n\n    prompt = re.sub(r\"^HRP_.\", \"\", prompt, flags=re.M)\n    prompt = prompt[1:-1].strip()\n\n    self.base_prompt = prompt\n    log.debug(\"prompt: {0}\".format(self.base_prompt))\n\n    return self.base_prompt"
    },
    {
        "original": "def update(self, other):\n        \"\"\"\n        Updated the contents of the current AttrTree with the\n        contents of a second AttrTree.\n        \"\"\"\n        if not isinstance(other, AttrTree):\n            raise Exception('Can only update with another AttrTree type.')\n        fixed_status = (self.fixed, other.fixed)\n        (self.fixed, other.fixed) = (False, False)\n        for identifier, element in other.items():\n            if identifier not in self.data:\n                self[identifier] = element\n            else:\n                self[identifier].update(element)\n        (self.fixed, other.fixed) = fixed_status",
        "rewrite": "def update(self, other):\n    if not isinstance(other, AttrTree):\n        raise Exception('Can only update with another AttrTree type.')\n        \n    fixed_status = (self.fixed, other.fixed)\n    self.fixed = False\n    other.fixed = False\n    \n    for identifier, element in other.items():\n        if identifier not in self.data:\n            self[identifier] = element\n        else:\n            self[identifier].update(element)\n    \n    self.fixed, other.fixed = fixed_status"
    },
    {
        "original": "def insert_empty_columns(self, x: int, amount: int = 1) -> None:\n        \"\"\"Insert a number of columns after the given column.\"\"\"\n        def transform_columns(\n                column: Union[int, float],\n                row: Union[int, float]\n        ) -> Tuple[Union[int, float], Union[int, float]]:\n            return column + (amount if column >= x else 0), row\n        self._transform_coordinates(transform_columns)",
        "rewrite": "def insert_empty_columns(self, x: int, amount: int = 1) -> None:\n        def transform_columns(column, row):\n            return column + (amount if column >= x else 0), row\n        self._transform_coordinates(transform_columns)"
    },
    {
        "original": "def _kill(self, variable, code_loc):  # pylint:disable=no-self-use\n        \"\"\"\n        Kill previous defs. addr_list is a list of normalized addresses.\n        \"\"\"\n\n        # Case 1: address perfectly match, we kill\n        # Case 2: a is a subset of the original address\n        # Case 3: a is a superset of the original address\n\n        # the previous definition is killed. mark it in data graph.\n\n        if variable in self._live_defs:\n            for loc in self._live_defs.lookup_defs(variable):\n                pv = ProgramVariable(variable, loc, arch=self.project.arch)\n                self._data_graph_add_edge(pv, ProgramVariable(variable, code_loc, arch=self.project.arch), type='kill')\n\n        self._live_defs.kill_def(variable, code_loc)",
        "rewrite": "def _kill(self, variable, code_loc):\n    if variable in self._live_defs:\n        for loc in self._live_defs.lookup_defs(variable):\n            pv = ProgramVariable(variable, loc, arch=self.project.arch)\n            self._data_graph_add_edge(pv, ProgramVariable(variable, code_loc, arch=self.project.arch), type='kill')\n    \n    self._live_defs.kill_def(variable, code_loc)"
    },
    {
        "original": "def get_macs(vm_, **kwargs):\n    \"\"\"\n    Return a list off MAC addresses from the named vm\n\n    :param vm_: name of the domain\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.get_macs <domain>\n    \"\"\"\n    doc = ElementTree.fromstring(get_xml(vm_, **kwargs))\n    return [node.get('address') for node in doc.findall('devices/interface/mac')]",
        "rewrite": "def get_macs(vm_, **kwargs):\n    doc = ElementTree.fromstring(get_xml(vm_, **kwargs))\n    return [node.get('address') for node in doc.findall('devices/interface/mac')]"
    },
    {
        "original": "def _pick_exit(self, block_address, stmt_idx, target_ips):\n        \"\"\"\n        Include an exit in the final slice.\n\n        :param block_address:   Address of the basic block.\n        :param stmt_idx:        ID of the exit statement.\n        :param target_ips:      The target address of this exit statement.\n        \"\"\"\n\n        # TODO: Support context-sensitivity\n\n        tpl = (stmt_idx, target_ips)\n        if tpl not in self.chosen_exits[block_address]:\n            self.chosen_exits[block_address].append(tpl)",
        "rewrite": "def _pick_exit(self, block_address, stmt_idx, target_ips):\n    tpl = (stmt_idx, target_ips)\n    if tpl not in self.chosen_exits[block_address]:\n        self.chosen_exits[block_address].append(tpl)"
    },
    {
        "original": "def is_declared(self, name, local_only=False):\n        \"\"\"Check if a name is declared in this or an outer scope.\"\"\"\n        if name in self.declared_locally or name in self.declared_parameter:\n            return True\n        if local_only:\n            return False\n        return name in self.declared",
        "rewrite": "def is_declared(self, name, local_only=False):\n    if name in self.declared_locally or name in self.declared_parameter:\n        return True\n    if local_only:\n        return False\n    return name in self.declared"
    },
    {
        "original": "def _calc_g(w, aod700):\n    \"\"\"Calculate the g coefficient.\"\"\"\n\n    g = -0.0147*np.log(w) - 0.3079*aod700**2 + 0.2846*aod700 + 0.3798\n\n    return g",
        "rewrite": "def _calc_g(w, aod700):\n    g = -0.0147*np.log(w) - 0.3079*aod700**2 + 0.2846*aod700 + 0.3798\n    return g"
    },
    {
        "original": "def _get_snmpv2c(self, oid):\n        \"\"\"\n        Try to send an SNMP GET operation using SNMPv2 for the specified OID.\n\n        Parameters\n        ----------\n        oid : str\n            The SNMP OID that you want to get.\n\n        Returns\n        -------\n        string : str\n            The string as part of the value from the OID you are trying to retrieve.\n        \"\"\"\n        snmp_target = (self.hostname, self.snmp_port)\n        cmd_gen = cmdgen.CommandGenerator()\n\n        (error_detected, error_status, error_index, snmp_data) = cmd_gen.getCmd(\n            cmdgen.CommunityData(self.community),\n            cmdgen.UdpTransportTarget(snmp_target, timeout=1.5, retries=2),\n            oid,\n            lookupNames=True,\n            lookupValues=True,\n        )\n\n        if not error_detected and snmp_data[0][1]:\n            return text_type(snmp_data[0][1])\n        return \"\"",
        "rewrite": "def _get_snmpv2c(self, oid):\n        snmp_target = (self.hostname, self.snmp_port)\n        cmd_gen = cmdgen.CommandGenerator()\n\n        (error_detected, error_status, error_index, snmp_data) = cmd_gen.getCmd(\n            cmdgen.CommunityData(self.community),\n            cmdgen.UdpTransportTarget(snmp_target, timeout=1.5, retries=2),\n            oid,\n            lookupNames=True,\n            lookupValues=True,\n        )\n\n        if not error_detected and snmp_data[0][1]:\n            return text_type(snmp_data[0][1])\n        return\"\"\""
    },
    {
        "original": "def find_transitionid_by_name(self, issue, transition_name):\n        \"\"\"Get a transitionid available on the specified issue to the current user.\n\n        Look at https://developer.atlassian.com/static/rest/jira/6.1.html#d2e1074 for json reference\n\n        :param issue: ID or key of the issue to get the transitions from\n        :param trans_name: iname of transition we are looking for\n        \"\"\"\n        transitions_json = self.transitions(issue)\n        id = None\n\n        for transition in transitions_json:\n            if transition[\"name\"].lower() == transition_name.lower():\n                id = transition[\"id\"]\n                break\n        return id",
        "rewrite": "def find_transitionid_by_name(self, issue, transition_name):\n    transitions_json = self.transitions(issue)\n    id = None\n    for transition in transitions_json:\n        if transition[\"name\"].lower() == transition_name.lower():\n            id = transition[\"id\"]\n            break\n    return id"
    },
    {
        "original": "def get_baudrate_ex_message(baudrate_ex):\n        \"\"\"\n        Converts a given baud rate value for systec USB-CANmoduls to the appropriate message string.\n\n        :param BaudrateEx baudrate_ex: Bus Timing Registers (see enum :class:`BaudrateEx`)\n        :return: Baud rate message string.\n        :rtype: str\n        \"\"\"\n        baudrate_ex_msgs = {\n            Baudrate.BAUDEX_AUTO: \"auto baudrate\",\n            Baudrate.BAUDEX_10kBit: \"10 kBit/sec\",\n            Baudrate.BAUDEX_SP2_10kBit: \"10 kBit/sec\",\n            Baudrate.BAUDEX_20kBit: \"20 kBit/sec\",\n            Baudrate.BAUDEX_SP2_20kBit: \"20 kBit/sec\",\n            Baudrate.BAUDEX_50kBit: \"50 kBit/sec\",\n            Baudrate.BAUDEX_SP2_50kBit: \"50 kBit/sec\",\n            Baudrate.BAUDEX_100kBit: \"100 kBit/sec\",\n            Baudrate.BAUDEX_SP2_100kBit: \"100 kBit/sec\",\n            Baudrate.BAUDEX_125kBit: \"125 kBit/sec\",\n            Baudrate.BAUDEX_SP2_125kBit: \"125 kBit/sec\",\n            Baudrate.BAUDEX_250kBit: \"250 kBit/sec\",\n            Baudrate.BAUDEX_SP2_250kBit: \"250 kBit/sec\",\n            Baudrate.BAUDEX_500kBit: \"500 kBit/sec\",\n            Baudrate.BAUDEX_SP2_500kBit: \"500 kBit/sec\",\n            Baudrate.BAUDEX_800kBit: \"800 kBit/sec\",\n            Baudrate.BAUDEX_SP2_800kBit: \"800 kBit/sec\",\n            Baudrate.BAUDEX_1MBit: \"1 MBit/s\",\n            Baudrate.BAUDEX_SP2_1MBit: \"1 MBit/s\",\n            Baudrate.BAUDEX_USE_BTR01: \"BTR0/BTR1 is used\",\n        }\n        return baudrate_ex_msgs.get(baudrate_ex, \"BTR is unknown (user specific)\")",
        "rewrite": "def get_baudrate_ex_message(baudrate_ex):\n    baudrate_ex_msgs = {\n        Baudrate.BAUDEX_AUTO: \"auto baudrate\",\n        Baudrate.BAUDEX_10kBit: \"10 kBit/sec\",\n        Baudrate.BAUDEX_SP2_10kBit: \"10 kBit/sec\",\n        Baudrate.BAUDEX_20kBit: \"20 kBit/sec\",\n        Baudrate.BAUDEX_SP2_20kBit: \"20 kBit/sec\",\n        Baudrate.BAUDEX_50kBit: \"50 kBit/sec\",\n        Baudrate.BAUDEX_SP2_50kBit: \"50 kBit/sec\",\n        Baudrate.BAUDEX_100kBit: \"100 kBit/sec\",\n        Baudrate.BAUDEX_SP2_100kBit: \"100 kBit/sec\",\n        Baudrate.BAUDEX_125kBit: \"125 kBit/sec\",\n        Baudrate.BAUDEX_SP2_125kBit: \"125 kBit/sec\",\n        Baudrate.BAUDEX_250kBit: \"250 kBit/sec\",\n        Baudrate.BAUDEX_SP2_250kBit: \"250 kBit/sec\",\n        Baudrate.BAUDEX_500kBit: \"500 kBit/sec\",\n        Baudrate.BAUDEX_SP2_500kBit: \"500 kBit/sec\",\n        Baudrate.BAUDEX_800kBit: \"800 kBit/sec\",\n        Baudrate.BAUDEX_SP2_800kBit: \"800 kBit/sec\",\n        Baudrate.BAUDEX_1MBit: \"1 MBit/s\",\n        Baudrate.BAUDEX_SP2_1MBit: \"1 MBit/s\",\n        Baudrate.BAUDEX_USE_BTR01: \"BTR0/BTR1 is used\",\n    }\n    return baudrate_ex_msgs.get(baudrate_ex, \"BTR is unknown (user specific)\")"
    },
    {
        "original": "def log_assist_request_without_audio(assist_request):\n    \"\"\"Log AssistRequest fields without audio data.\"\"\"\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        resp_copy = embedded_assistant_pb2.AssistRequest()\n        resp_copy.CopyFrom(assist_request)\n        if len(resp_copy.audio_in) > 0:\n            size = len(resp_copy.audio_in)\n            resp_copy.ClearField('audio_in')\n            logging.debug('AssistRequest: audio_in (%d bytes)',\n                          size)\n            return\n        logging.debug('AssistRequest: %s', resp_copy)",
        "rewrite": "```python\ndef log_assist_request_without_audio(assist_request):\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        resp_copy = embedded_assistant_pb2.AssistRequest()\n        resp_copy.CopyFrom(assist_request)\n        if len(resp_copy.audio_in) > 0:\n            size = len(resp_copy.audio_in)\n            resp_copy.ClearField('audio_in')\n            logging.debug('AssistRequest: audio_in (%d bytes)', size)\n            return\n        logging.debug('AssistRequest: %s', resp_copy)\n```"
    },
    {
        "original": "def enc(data, **kwargs):\n    \"\"\"\n    Alias to `{box_type}_encrypt`\n\n    box_type: secretbox, sealedbox(default)\n    \"\"\"\n    kwargs['opts'] = __opts__\n    return salt.utils.nacl.enc(data, **kwargs)",
        "rewrite": "def enc(data, **kwargs):\n    kwargs['opts'] = __opts__\n    return salt.utils.nacl.enc(data, **kwargs)"
    },
    {
        "original": "def save(self):\n        \"\"\" Create a new event or update an existing one by checking what\n        values have changed and update them on the server\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.object_id:\n            # update event\n            if not self._track_changes:\n                return True  # there's nothing to update\n            url = self.build_url(\n                self._endpoints.get('event').format(id=self.object_id))\n            method = self.con.patch\n            data = self.to_api_data(restrict_keys=self._track_changes)\n        else:\n            # new event\n            if self.calendar_id:\n                url = self.build_url(\n                    self._endpoints.get('event_calendar').format(\n                        id=self.calendar_id))\n            else:\n                url = self.build_url(self._endpoints.get('event_default'))\n            method = self.con.post\n            data = self.to_api_data()\n\n        response = method(url, data=data)\n        if not response:\n            return False\n\n        self._track_changes.clear()  # clear the tracked changes\n\n        if not self.object_id:\n            # new event\n            event = response.json()\n\n            self.object_id = event.get(self._cc('id'), None)\n\n            self.__created = event.get(self._cc('createdDateTime'), None)\n            self.__modified = event.get(self._cc('lastModifiedDateTime'), None)\n\n            self.__created = parse(self.__created).astimezone(\n                self.protocol.timezone) if self.__created else None\n            self.__modified = parse(self.__modified).astimezone(\n                self.protocol.timezone) if self.__modified else None\n        else:\n            self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n\n        return True",
        "rewrite": "def save(self):\n    if self.object_id:\n        if not self._track_changes:\n            return True\n        url = self.build_url(self._endpoints.get('event').format(id=self.object_id))\n        method = self.con.patch\n        data = self.to_api_data(restrict_keys=self._track_changes)\n    else:\n        if self.calendar_id:\n            url = self.build_url(self._endpoints.get('event_calendar').format(id=self.calendar_id))\n        else:\n            url = self.build_url(self._endpoints.get('event_default'))\n        method = self.con.post\n        data = self.to_api_data()\n\n    response = method(url, data=data)\n    if not response:\n        return False\n\n    self._track_changes.clear()\n\n    if not self.object_id:\n        event = response.json()\n        self.object_id = event.get(self._cc('id'), None)\n        self.__created = event.get(self._cc('createdDateTime'), None)\n        self.__modified = event.get(self._cc('lastModifiedDateTime'), None)\n        self.__created = parse(self.__created).astimezone(self.protocol.timezone) if self.__created else None\n        self.__modified = parse(self.__modified).astimezone(self.protocol.timezone) if self.__modified else None\n    else:\n        self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n        \n    return True"
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Executes ``ansible-playbook`` and returns a string.\n\n        :return: str\n        \"\"\"\n        if self._ansible_command is None:\n            self.bake()\n\n        try:\n            self._config.driver.sanity_checks()\n            cmd = util.run_command(\n                self._ansible_command, debug=self._config.debug)\n            return cmd.stdout.decode('utf-8')\n        except sh.ErrorReturnCode as e:\n            out = e.stdout.decode('utf-8')\n            util.sysexit_with_message(str(out), e.exit_code)",
        "rewrite": "def execute(self):\n    if self._ansible_command is None:\n        self.bake()\n\n    try:\n        self._config.driver.sanity_checks()\n        cmd = util.run_command(\n            self._ansible_command, debug=self._config.debug)\n        return cmd.stdout.decode('utf-8')\n    except sh.ErrorReturnCode as e:\n        out = e.stdout.decode('utf-8')\n        util.sysexit_with_message(str(out), e.exit_code)"
    },
    {
        "original": "def teleport(start_index, end_index, ancilla_index):\n    \"\"\"Teleport a qubit from start to end using an ancilla qubit\n    \"\"\"\n    program = make_bell_pair(end_index, ancilla_index)\n\n    ro = program.declare('ro', memory_size=3)\n\n    # do the teleportation\n    program.inst(CNOT(start_index, ancilla_index))\n    program.inst(H(start_index))\n\n    # measure the results and store them in classical registers [0] and [1]\n    program.measure(start_index, ro[0])\n    program.measure(ancilla_index, ro[1])\n\n    program.if_then(ro[1], X(2))\n    program.if_then(ro[0], Z(2))\n\n    program.measure(end_index, ro[2])\n\n    print(program)\n    return program",
        "rewrite": "def teleport(start_index, end_index, ancilla_index):\n    program = make_bell_pair(end_index, ancilla_index)\n    ro = program.declare('ro', memory_size=3)\n    \n    program.inst(CNOT(start_index, ancilla_index))\n    program.inst(H(start_index))\n\n    program.measure(start_index, ro[0])\n    program.measure(ancilla_index, ro[1])\n\n    program.if_then(ro[1], X(2))\n    program.if_then(ro[0], Z(2))\n\n    program.measure(end_index, ro[2])\n\n    print(program)\n    return program"
    },
    {
        "original": "def _find_sink_scc(self):\n        \"\"\"\n        Set self._sink_scc_labels, which is a list containing the labels of\n        the strongly connected components.\n\n        \"\"\"\n        condensation_lil = self._condensation_lil()\n\n        # A sink SCC is a SCC such that none of its members is strongly\n        # connected to nodes in other SCCs\n        # Those k's such that graph_condensed_lil.rows[k] == []\n        self._sink_scc_labels = \\\n            np.where(np.logical_not(condensation_lil.rows))[0]",
        "rewrite": "def _find_sink_scc(self):\n    condensation_lil = self._condensation_lil()\n    self._sink_scc_labels = np.where(np.logical_not(condensation_lil.rows))[0]"
    },
    {
        "original": "def _has_definition(self):\n        \"\"\"True if a footer is defined for this section.\"\"\"\n        footerReference = self._sectPr.get_footerReference(self._hdrftr_index)\n        return False if footerReference is None else True",
        "rewrite": "def has_definition(self):\n    \"\"\"True if a footer is defined for this section.\"\"\"\n    footer_reference = self._sectPr.get_footer_reference(self._hdrftr_index)\n    return False if footer_reference is None else True"
    },
    {
        "original": "def CountHuntResults(self,\n                       hunt_id,\n                       with_tag=None,\n                       with_type=None,\n                       cursor=None):\n    \"\"\"Counts hunt results of a given hunt using given query options.\"\"\"\n    hunt_id_int = db_utils.HuntIDToInt(hunt_id)\n\n    query = \"SELECT COUNT(*) FROM flow_results WHERE hunt_id = %s \"\n\n    args = [hunt_id_int]\n\n    if with_tag is not None:\n      query += \"AND tag = %s \"\n      args.append(with_tag)\n\n    if with_type is not None:\n      query += \"AND type = %s \"\n      args.append(with_type)\n\n    cursor.execute(query, args)\n    return cursor.fetchone()[0]",
        "rewrite": "def count_hunt_results(self, hunt_id, with_tag=None, with_type=None, cursor=None):\n    hunt_id_int = db_utils.HuntIDToInt(hunt_id)\n    \n    query = \"SELECT COUNT(*) FROM flow_results WHERE hunt_id = %s\"\n    \n    args = [hunt_id_int]\n    \n    if with_tag is not None:\n        query += \" AND tag = %s\"\n        args.append(with_tag)\n    \n    if with_type is not None:\n        query += \" AND type = %s\"\n        args.append(with_type)\n    \n    cursor.execute(query, args)\n    return cursor.fetchone()[0]"
    },
    {
        "original": "def create_file(self, path, message, content,\n                    branch=github.GithubObject.NotSet,\n                    committer=github.GithubObject.NotSet,\n                    author=github.GithubObject.NotSet):\n        \"\"\"Create a file in this repository.\n\n        :calls: `PUT /repos/:owner/:repo/contents/:path <http://developer.github.com/v3/repos/contents#create-a-file>`_\n        :param path: string, (required), path of the file in the repository\n        :param message: string, (required), commit message\n        :param content: string, (required), the actual data in the file\n        :param branch: string, (optional), branch to create the commit on. Defaults to the default branch of the repository\n        :param committer: InputGitAuthor, (optional), if no information is given the authenticated user's information will be used. You must specify both a name and email.\n        :param author: InputGitAuthor, (optional), if omitted this will be filled in with committer information. If passed, you must specify both a name and email.\n        :rtype: {\n            'content': :class:`ContentFile <github.ContentFile.ContentFile>`:,\n            'commit': :class:`Commit <github.Commit.Commit>`}\n        \"\"\"\n        assert isinstance(path, (str, unicode)),                   \\\n            'path must be str/unicode object'\n        assert isinstance(message, (str, unicode)),                \\\n            'message must be str/unicode object'\n        assert isinstance(content, (str, unicode, bytes)),         \\\n            'content must be a str/unicode object'\n        assert branch is github.GithubObject.NotSet                \\\n            or isinstance(branch, (str, unicode)),                 \\\n            'branch must be a str/unicode object'\n        assert author is github.GithubObject.NotSet                \\\n            or isinstance(author, github.InputGitAuthor),          \\\n            'author must be a github.InputGitAuthor object'\n        assert committer is github.GithubObject.NotSet             \\\n            or isinstance(committer, github.InputGitAuthor),       \\\n            'committer must be a github.InputGitAuthor object'\n\n        if atLeastPython3:\n            if isinstance(content, str):\n                content = content.encode('utf-8')\n            content = b64encode(content).decode('utf-8')\n        else:\n            if isinstance(content, unicode):\n                content = content.encode('utf-8')\n            content = b64encode(content)\n        put_parameters = {'message': message, 'content': content}\n\n        if branch is not github.GithubObject.NotSet:\n            put_parameters['branch'] = branch\n        if author is not github.GithubObject.NotSet:\n            put_parameters[\"author\"] = author._identity\n        if committer is not github.GithubObject.NotSet:\n            put_parameters[\"committer\"] = committer._identity\n\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/contents/\" + urllib.quote(path),\n            input=put_parameters\n        )\n\n        return {'content': github.ContentFile.ContentFile(self._requester, headers, data[\"content\"], completed=False),\n                'commit': github.Commit.Commit(self._requester, headers, data[\"commit\"], completed=True)}",
        "rewrite": "def create_file(self, path, message, content,\n                branch=github.GithubObject.NotSet,\n                committer=github.GithubObject.NotSet,\n                author=github.GithubObject.NotSet):\n    assert isinstance(path, str), 'path must be a string'\n    assert isinstance(message, str), 'message must be a string'\n    assert isinstance(content, (str, bytes)), 'content must be a string or bytes'\n    assert branch is github.GithubObject.NotSet or isinstance(branch, str), 'branch must be a string'\n    assert author is github.GithubObject.NotSet or isinstance(author, github.InputGitAuthor), 'author must be a github.InputGitAuthor object'\n    assert committer is github.GithubObject.NotSet or isinstance(committer, github.InputGitAuthor), 'committer must be a github.InputGitAuthor object'\n\n    if isinstance(content, str):\n        content = content.encode('utf-8')\n\n    content = b64encode(content).decode('utf-8')\n\n    put_parameters = {'message': message, 'content': content}\n\n    if branch is not github.GithubObject.NotSet:\n        put_parameters['branch'] = branch\n    if author is not github.GithubObject.NotSet:\n        put_parameters[\"author\"] = author._identity\n    if committer is not github.GithubObject.NotSet:\n        put_parameters[\"committer\"] = committer._identity\n\n    headers, data = self._requester.requestJsonAndCheck(\n        \"PUT\",\n        self.url + \"/contents/\" + urllib.parse.quote(path),\n        input=put_parameters\n    )\n\n    return {'content': github.ContentFile.ContentFile(self._requester, headers, data[\"content\"], completed=False),\n            'commit': github.Commit.Commit(self._requester, headers, data[\"commit\"], completed=True)}"
    },
    {
        "original": "def insert_row(\n        self,\n        values,\n        index=1,\n        value_input_option='RAW'\n    ):\n        \"\"\"Adds a row to the worksheet at the specified index\n        and populates it with values.\n\n        Widens the worksheet if there are more values than columns.\n\n        :param values: List of values for the new row.\n        :param index: (optional) Offset for the newly inserted row.\n        :type index: int\n        :param value_input_option: (optional) Determines how input data should\n                                    be interpreted. See `ValueInputOption`_ in\n                                    the Sheets API.\n        :type value_input_option: str\n\n        .. _ValueInputOption: https://developers.google.com/sheets/api/reference/rest/v4/ValueInputOption\n\n        \"\"\"\n\n        body = {\n            \"requests\": [{\n                \"insertDimension\": {\n                    \"range\": {\n                      \"sheetId\": self.id,\n                      \"dimension\": \"ROWS\",\n                      \"startIndex\": index - 1,\n                      \"endIndex\": index\n                    }\n                }\n            }]\n        }\n\n        self.spreadsheet.batch_update(body)\n\n        range_label = '%s!%s' % (self.title, 'A%s' % index)\n\n        data = self.spreadsheet.values_update(\n            range_label,\n            params={\n                'valueInputOption': value_input_option\n            },\n            body={\n                'values': [values]\n            }\n        )\n\n        return data",
        "rewrite": "def insert_row(self, values, index=1, value_input_option='RAW'):\r\n    body = {\r\n        \"requests\": [{\r\n            \"insertDimension\": {\r\n                \"range\": {\r\n                    \"sheetId\": self.id,\r\n                    \"dimension\": \"ROWS\",\r\n                    \"startIndex\": index - 1,\r\n                    \"endIndex\": index\r\n                }\r\n            }\r\n        }]\r\n    }\r\n    self.spreadsheet.batch_update(body)\r\n    \r\n    range_label = '%s!%s' % (self.title, 'A%s' % index)\r\n    \r\n    data = self.spreadsheet.values_update(\r\n        range_label,\r\n        params={\r\n            'valueInputOption': value_input_option\r\n        },\r\n        body={\r\n            'values': [values]\r\n        }\r\n    )\r\n    \r\n    return data"
    },
    {
        "original": "def key(username, key, all):\n    \"\"\"Create an admin API key.\"\"\"\n    if username and username not in current_app.config['ADMIN_USERS']:\n        raise click.UsageError('User {} not an admin'.format(username))\n\n    def create_key(admin, key):\n        key = ApiKey(\n            user=admin,\n            key=key,\n            scopes=[Scope.admin, Scope.write, Scope.read],\n            text='Admin key created by alertad script',\n            expire_time=None\n        )\n        try:\n            db.get_db()  # init db on global app context\n            key = key.create()\n        except Exception as e:\n            click.echo('ERROR: {}'.format(e))\n        else:\n            click.echo('{} {}'.format(key.key, key.user))\n\n    if all:\n        for admin in current_app.config['ADMIN_USERS']:\n            create_key(admin, key)\n    elif username:\n        create_key(username, key)\n    else:\n        raise click.UsageError(\"Must set '--username' or use '--all'\")",
        "rewrite": "def key(username, key, all):\n    if username and username not in current_app.config['ADMIN_USERS']:\n        raise click.UsageError('User {} not an admin'.format(username))\n\n    def create_key(admin, key):\n        key = ApiKey(\n            user=admin,\n            key=key,\n            scopes=[Scope.admin, Scope.write, Scope.read],\n            text='Admin key created by alertad script',\n            expire_time=None\n        )\n        try:\n            db.get_db()  \n            key = key.create()\n        except Exception as e:\n            click.echo('ERROR: {}'.format(e))\n        else:\n            click.echo('{} {}'.format(key.key, key.user))\n\n    if all:\n        for admin in current_app.config['ADMIN_USERS']:\n            create_key(admin, key)\n    elif username:\n        create_key(username, key)\n    else:\n        raise click.UsageError(\"Must set '--username' or use '--all'\")"
    },
    {
        "original": "def ransac(npts, model, n, k, t, d):\n    \"\"\" Fit model parameters to data using the RANSAC algorithm\n\n    This implementation is written from pseudo-code found at\n    http://en.wikipedia.org/w/index.php?title=RANSAC&oldid=116358182\n\n    :param npts: A set of observed data points\n    :param model: A model that can be fitted to data points\n    :param n: The minimum number of data values required to fit the model\n    :param k: The maximum number of iterations allowed in the algorithm\n    :param t: A threshold value for determining when a data point fits a model\n    :param d: The number of close data values required to assert that a model fits well to data\n    :return: Model parameters which best fit the data (or None if no good model is found)\n    \"\"\"\n    iterations = 0\n    bestfit = None\n    besterr = np.inf\n    # best_inlier_idxs = None\n    while iterations < k:\n        maybe_idxs, test_idxs = random_partition(n, npts)\n        maybemodel = model.fit(maybe_idxs)\n        test_err = model.score(test_idxs, maybemodel)\n        also_idxs = test_idxs[test_err < t]  # select indices of rows with accepted points\n\n        LOGGER.debug('test_err.min() %f', test_err.min() if test_err.size else None)\n        LOGGER.debug('test_err.max() %f', test_err.max() if test_err.size else None)\n        LOGGER.debug('numpy.mean(test_err) %f', np.mean(test_err) if test_err.size else None)\n        LOGGER.debug('iteration %d, len(alsoinliers) = %d', iterations, len(also_idxs))\n\n        if len(also_idxs) > d:\n            betteridxs = np.concatenate((maybe_idxs, also_idxs))\n            bettermodel = model.fit(betteridxs)\n            better_errs = model.score(betteridxs, bettermodel)\n            thiserr = np.mean(better_errs)\n            if thiserr < besterr:\n                bestfit = bettermodel\n                besterr = thiserr\n                # best_inlier_idxs = np.concatenate((maybe_idxs, also_idxs))\n        iterations += 1\n    return bestfit",
        "rewrite": "import numpy as np\n\ndef ransac(npts, model, n, k, t, d):\n    iterations = 0\n    bestfit = None\n    besterr = np.inf\n    while iterations < k:\n        maybe_idxs, test_idxs = random_partition(n, npts)\n        maybemodel = model.fit(maybe_idxs)\n        test_err = model.score(test_idxs, maybemodel)\n        also_idxs = test_idxs[test_err < t]\n\n        if len(also_idxs) > d:\n            betteridxs = np.concatenate((maybe_idxs, also_idxs))\n            bettermodel = model.fit(betteridxs)\n            better_errs = model.score(betteridxs, bettermodel)\n            thiserr = np.mean(better_errs)\n            if thiserr < besterr:\n                bestfit = bettermodel\n                besterr = thiserr\n        iterations += 1\n    return bestfit"
    },
    {
        "original": "def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n    r\"\"\"\n    Construct a BitFlipChannel that flips a qubit state\n    with probability of a flip given by p. If p is None, return\n    a guaranteed flip in the form of an X operation.\n\n    This channel evolves a density matrix via\n\n        $$\n        \\rho \\rightarrow M_0 \\rho M_0^\\dagger + M_1 \\rho M_1^\\dagger\n        $$\n\n    With\n\n        $$\n        \\begin{aligned}\n        M_0 =& \\sqrt{p} \\begin{bmatrix}\n                            1 & 0 \\\\\n                            0 & 1\n                       \\end{bmatrix}\n        \\\\\n        M_1 =& \\sqrt{1-p} \\begin{bmatrix}\n                            0 & 1 \\\\\n                            1 & -0\n                         \\end{bmatrix}\n        \\end{aligned}\n        $$\n\n    Args:\n        p: the probability of a bit flip.\n\n    Raises:\n        ValueError: if p is not a valid probability.\n    \"\"\"\n    if p is None:\n        return pauli_gates.X\n\n    return _bit_flip(p)",
        "rewrite": "def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n\n    if p is None:\n        return common_gates.X\n\n    return _bit_flip(p)"
    },
    {
        "original": "def impulse_response(self, impulse_length=30):\n        \"\"\"\n        Get the impulse response corresponding to our model.\n\n        Returns\n        -------\n        psi : array_like(float)\n            psi[j] is the response at lag j of the impulse response.\n            We take psi[0] as unity.\n\n        \"\"\"\n        from scipy.signal import dimpulse\n        sys = self.ma_poly, self.ar_poly, 1\n        times, psi = dimpulse(sys, n=impulse_length)\n        psi = psi[0].flatten()  # Simplify return value into flat array\n\n        return psi",
        "rewrite": "def impulse_response(self, impulse_length=30):\n    from scipy.signal import dimpulse\n    sys = self.ma_poly, self.ar_poly, 1\n    times, psi = dimpulse(sys, n=impulse_length)\n    psi = psi[0].flatten()\n\n    return psi"
    },
    {
        "original": "def purge_obsolete_samples(self, config, now):\n        \"\"\"\n        Timeout any windows that have expired in the absence of any events\n        \"\"\"\n        expire_age = config.samples * config.time_window_ms\n        for sample in self._samples:\n            if now - sample.last_window_ms >= expire_age:\n                sample.reset(now)",
        "rewrite": "def purge_obsolete_samples(self, config, now):\n    expire_age = config.samples * config.time_window_ms\n    for sample in self._samples:\n        if now - sample.last_window_ms >= expire_age:\n            sample.reset(now)"
    },
    {
        "original": "def http(self, *args, **kwargs):\n        \"\"\"Starts the process of building a new HTTP route linked to this API instance\"\"\"\n        kwargs['api'] = self.api\n        return http(*args, **kwargs)",
        "rewrite": "def http(self, *args, **kwargs):\n    \"\"\"\n    Starts the process of building a new HTTP route linked to this API instance\n    \"\"\"\n    kwargs['api'] = self.api\n    return http(*args, **kwargs)"
    },
    {
        "original": "def _build_migrated_variables(checkpoint_reader, name_value_fn):\n  \"\"\"Builds the TensorFlow variables of the migrated checkpoint.\n\n  Args:\n    checkpoint_reader: A `tf.train.NewCheckPointReader` of the checkpoint to\n      be read from.\n    name_value_fn: Function taking two arguments, `name` and `value`, which\n      returns the pair of new name and value for that a variable of that name.\n\n  Returns:\n    Tuple of a dictionary with new variable names as keys and `tf.Variable`s as\n    values, and a dictionary that maps the old variable names to the new\n    variable names.\n  \"\"\"\n\n  names_to_shapes = checkpoint_reader.get_variable_to_shape_map()\n\n  new_name_to_variable = {}\n  name_to_new_name = {}\n\n  for name in names_to_shapes:\n    value = checkpoint_reader.get_tensor(name)\n    new_name, new_value = name_value_fn(name, value)\n    if new_name is None:\n      continue\n\n    name_to_new_name[name] = new_name\n    new_name_to_variable[new_name] = tf.Variable(new_value)\n\n  return new_name_to_variable, name_to_new_name",
        "rewrite": "def _build_migrated_variables(checkpoint_reader, name_value_fn):\n    names_to_shapes = checkpoint_reader.get_variable_to_shape_map()\n    new_name_to_variable = {}\n    name_to_new_name = {}\n    \n    for name in names_to_shapes:\n        value = checkpoint_reader.get_tensor(name)\n        new_name, new_value = name_value_fn(name, value)\n        if new_name is None:\n            continue\n        \n        name_to_new_name[name] = new_name\n        new_name_to_variable[new_name] = tf.Variable(new_value)\n    \n    return new_name_to_variable, name_to_new_name"
    },
    {
        "original": "def expand_matrix_in_orthogonal_basis(\n        m: np.ndarray,\n        basis: Dict[str, np.ndarray],\n) -> value.LinearDict[str]:\n    \"\"\"Computes coefficients of expansion of m in basis.\n\n    We require that basis be orthogonal w.r.t. the Hilbert-Schmidt inner\n    product. We do not require that basis be orthonormal. Note that Pauli\n    basis (I, X, Y, Z) is orthogonal, but not orthonormal.\n    \"\"\"\n    return value.LinearDict({\n        name: (hilbert_schmidt_inner_product(b, m) /\n               hilbert_schmidt_inner_product(b, b))\n        for name, b in basis.items()\n    })",
        "rewrite": "import numpy as np\nfrom typing import Dict\nimport value\n\ndef expand_matrix_in_orthogonal_basis(\n        m: np.ndarray,\n        basis: Dict[str, np.ndarray],\n) -> value.LinearDict[str]:\n    return value.LinearDict({\n        name: (value.hilbert_schmidt_inner_product(b, m) /\n               value.hilbert_schmidt_inner_product(b, b))\n        for name, b in basis.items()\n    })"
    },
    {
        "original": "def _kde_support(bin_range, bw, gridsize, cut, clip):\n    \"\"\"Establish support for a kernel density estimate.\"\"\"\n    kmin, kmax = bin_range[0] - bw * cut, bin_range[1] + bw * cut\n    if isfinite(clip[0]):\n        kmin = max(kmin, clip[0])\n    if isfinite(clip[1]):\n        kmax = min(kmax, clip[1])\n    return np.linspace(kmin, kmax, gridsize)",
        "rewrite": "def _kde_support(bin_range, bw, gridsize, cut, clip):\n    kmin, kmax = bin_range[0] - bw * cut, bin_range[1] + bw * cut\n    if np.isfinite(clip[0]):\n        kmin = max(kmin, clip[0])\n    if np.isfinite(clip[1]):\n        kmax = min(kmax, clip[1])\n    return np.linspace(kmin, kmax, gridsize)"
    },
    {
        "original": "async def _send_rtcp_pli(self, media_ssrc):\n        \"\"\"\n        Send an RTCP packet to report picture loss.\n        \"\"\"\n        if self.__rtcp_ssrc is not None:\n            packet = RtcpPsfbPacket(fmt=RTCP_PSFB_PLI, ssrc=self.__rtcp_ssrc, media_ssrc=media_ssrc)\n            await self._send_rtcp(packet)",
        "rewrite": "async def _send_rtcp_pli(self, media_ssrc):\n    if self.__rtcp_ssrc is not None:\n        packet = RtcpPsfbPacket(fmt=RTCP_PSFB_PLI, ssrc=self.__rtcp_ssrc, media_ssrc=media_ssrc)\n        await self._send_rtcp(packet)"
    },
    {
        "original": "def set_weights(self, weights_values: dict, ignore_missing=False):\n        \"\"\"\n        Sets the weights values of the network.\n        :param weights_values: dictionary with weights for each layer\n        \"\"\"\n        network_name = self.__class__.__name__.lower()\n\n        with tf.variable_scope(network_name):\n            for layer_name in weights_values:\n                with tf.variable_scope(layer_name, reuse=True):\n                    for param_name, data in weights_values[layer_name].items():\n                        try:\n                            var = tf.get_variable(param_name)\n                            self._session.run(var.assign(data))\n\n                        except ValueError:\n                            if not ignore_missing:\n                                raise",
        "rewrite": "def set_weights(self, weights_values: dict, ignore_missing=False):\n    network_name = self.__class__.__name__.lower()\n\n    with tf.variable_scope(network_name):\n        for layer_name in weights_values:\n            with tf.variable_scope(layer_name, reuse=True):\n                for param_name, data in weights_values[layer_name].items():\n                    try:\n                        var = tf.get_variable(param_name)\n                        self._session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise"
    },
    {
        "original": "def is_transaction_signer_authorized(self, transactions, state_root,\n                                         from_state):\n        \"\"\" Check the transaction signing key against the allowed transactor\n            permissions. The roles being checked are the following, from first\n            to last:\n                \"transactor.transaction_signer.<TP_Name>\"\n                \"transactor.transaction_signer\"\n                \"transactor\"\n                \"default\"\n\n            The first role that is set will be the one used to enforce if the\n            transaction signer is allowed.\n\n            Args:\n                transactions (List of Transactions): The transactions that are\n                    being verified.\n                state_root(string): The state root of the previous block. If\n                    this is None, the current state root hash will be\n                    retrieved.\n                from_state (bool): Whether the identity value should be read\n                    directly from state, instead of using the cached values.\n                    This should be used when the state_root passed is not from\n                    the current chain head.\n        \"\"\"\n        role = None\n        if role is None:\n            role = self._cache.get_role(\"transactor.transaction_signer\",\n                                        state_root, from_state)\n\n        if role is None:\n            role = self._cache.get_role(\"transactor\", state_root, from_state)\n\n        if role is None:\n            policy_name = \"default\"\n        else:\n            policy_name = role.policy_name\n\n        policy = self._cache.get_policy(policy_name, state_root, from_state)\n\n        family_roles = {}\n        for transaction in transactions:\n            header = TransactionHeader()\n            header.ParseFromString(transaction.header)\n            family_policy = None\n            if header.family_name not in family_roles:\n                role = self._cache.get_role(\n                    \"transactor.transaction_signer.\" + header.family_name,\n                    state_root,\n                    from_state)\n\n                if role is not None:\n                    family_policy = self._cache.get_policy(role.policy_name,\n                                                           state_root,\n                                                           from_state)\n                family_roles[header.family_name] = family_policy\n            else:\n                family_policy = family_roles[header.family_name]\n\n            if family_policy is not None:\n                if not self._allowed(header.signer_public_key, family_policy):\n                    LOGGER.debug(\"Transaction Signer: %s is not permitted.\",\n                                 header.signer_public_key)\n                    return False\n            else:\n                if policy is not None:\n                    if not self._allowed(header.signer_public_key, policy):\n                        LOGGER.debug(\n                            \"Transaction Signer: %s is not permitted.\",\n                            header.signer_public_key)\n                        return False\n        return True",
        "rewrite": "def is_transaction_signer_authorized(self, transactions, state_root, from_state):\n    role = None\n    if role is None:\n        role = self._cache.get_role(\"transactor.transaction_signer\", state_root, from_state)\n\n    if role is None:\n        role = self._cache.get_role(\"transactor\", state_root, from_state)\n\n    if role is None:\n        policy_name = \"default\"\n    else:\n        policy_name = role.policy_name\n\n    policy = self._cache.get_policy(policy_name, state_root, from_state)\n\n    family_roles = {}\n    for transaction in transactions:\n        header = TransactionHeader()\n        header.ParseFromString(transaction.header)\n        family_policy = None\n        if header.family_name not in family_roles:\n            role = self._cache.get_role(\"transactor.transaction_signer.\" + header.family_name, state_root, from_state)\n\n            if role is not None:\n                family_policy = self._cache.get_policy(role.policy_name, state_root, from_state)\n            family_roles[header.family_name] = family_policy\n\n        else:\n            family_policy = family_roles[header.family_name]\n\n        if family_policy is not None:\n            if not self._allowed(header.signer_public_key, family_policy):\n                LOGGER.debug(\"Transaction Signer: %s is not permitted.\", header.signer_public_key)\n                return False\n        else:\n            if policy is not None:\n                if not self._allowed(header.signer_public_key, policy):\n                    LOGGER.debug(\"Transaction Signer: %s is not permitted.\", header.signer_public_key)\n                    return False\n    return True"
    },
    {
        "original": "def _plot_histogram(series, bins=10, figsize=(6, 4), facecolor='#337ab7'):\n    \"\"\"Plot an histogram from the data and return the AxesSubplot object.\n\n    Parameters\n    ----------\n    series : Series\n        The data to plot\n    figsize : tuple\n        The size of the figure (width, height) in inches, default (6,4)\n    facecolor : str\n        The color code.\n\n    Returns\n    -------\n    matplotlib.AxesSubplot\n        The plot.\n    \"\"\"\n    if base.get_vartype(series) == base.TYPE_DATE:\n        # TODO: These calls should be merged\n        fig = plt.figure(figsize=figsize)\n        plot = fig.add_subplot(111)\n        plot.set_ylabel('Frequency')\n        try:\n            plot.hist(series.dropna().values, facecolor=facecolor, bins=bins)\n        except TypeError: # matplotlib 1.4 can't plot dates so will show empty plot instead\n            pass\n    else:\n        plot = series.plot(kind='hist', figsize=figsize,\n                           facecolor=facecolor,\n                           bins=bins)  # TODO when running on server, send this off to a different thread\n    return plot",
        "rewrite": "def _plot_histogram(series, bins=10, figsize=(6, 4), facecolor='#337ab7'):\n    \"\"\"Plot an histogram from the data and return the AxesSubplot object.\n\n    Parameters\n    ----------\n    series : Series\n        The data to plot\n    figsize : tuple\n        The size of the figure (width, height) in inches, default (6,4)\n    facecolor : str\n        The color code.\n\n    Returns\n    -------\n    matplotlib.AxesSubplot\n        The plot.\n    \"\"\"\n    if base.get_vartype(series) == base.TYPE_DATE:\n        fig = plt.figure(figsize=figsize)\n        plot = fig.add_subplot(111)\n        plot.set_ylabel('Frequency')\n        try:\n            plot.hist(series.dropna().values, facecolor=facecolor, bins=bins)\n        except TypeError:\n            pass\n    else:\n        plot = series.plot(kind='hist', figsize=figsize, facecolor=facecolor, bins=bins) \n    return plot"
    },
    {
        "original": "def objify(self, doc, columns=None):\n        \"\"\"\n        Decode a Pymongo SON object into an Pandas DataFrame\n        \"\"\"\n        cols = columns or doc[METADATA][COLUMNS]\n        data = {}\n\n        for col in cols:\n            # if there is missing data in a chunk, we can default to NaN\n            # and pandas will autofill the missing values to the correct length\n            if col not in doc[METADATA][LENGTHS]:\n                d = [np.nan]\n            else:\n                d = decompress(doc[DATA][doc[METADATA][LENGTHS][col][0]: doc[METADATA][LENGTHS][col][1] + 1])\n                # d is ready-only but that's not an issue since DataFrame will copy the data anyway.\n                d = np.frombuffer(d, doc[METADATA][DTYPE][col])\n\n                if MASK in doc[METADATA] and col in doc[METADATA][MASK]:\n                    mask_data = decompress(doc[METADATA][MASK][col])\n                    mask = np.frombuffer(mask_data, 'bool')\n                    d = ma.masked_array(d, mask)\n            data[col] = d\n\n        # Copy into\n        return pd.DataFrame(data, columns=cols, copy=True)[cols]",
        "rewrite": "def objify(self, doc, columns=None):\r\n    cols = columns or doc[METADATA][COLUMNS]\r\n    data = {}\r\n    \r\n    for col in cols:\r\n        if col not in doc[METADATA][LENGTHS]:\r\n            d = [np.nan]\r\n        else:\r\n            d = decompress(doc[DATA][doc[METADATA][LENGTHS][col][0]: doc[METADATA][LENGTHS][col][1] + 1])\r\n            d = np.frombuffer(d, doc[METADATA][DTYPE][col])\r\n            \r\n            if MASK in doc[METADATA] and col in doc[METADATA][MASK]:\r\n                mask_data = decompress(doc[METADATA][MASK][col])\r\n                mask = np.frombuffer(mask_data, 'bool')\r\n                d = ma.masked_array(d, mask)\r\n            \r\n            data[col] = d\r\n    \r\n    return pd.DataFrame(data, columns=cols, copy=True)[cols]"
    },
    {
        "original": "def vibrational_free_energy(self, temperature, volume):\n        \"\"\"\n        Vibrational Helmholtz free energy, A_vib(V, T).\n        Eq(4) in doi.org/10.1016/j.comphy.2003.12.001\n\n        Args:\n            temperature (float): temperature in K\n            volume (float)\n\n        Returns:\n            float: vibrational free energy in eV\n        \"\"\"\n        y = self.debye_temperature(volume) / temperature\n        return self.kb * self.natoms * temperature * (\n            9./8. * y + 3 * np.log(1 - np.exp(-y)) - self.debye_integral(y))",
        "rewrite": "def vibrational_free_energy(self, temperature, volume):\n        y = self.debye_temperature(volume) / temperature\n        return self.kb * self.natoms * temperature * (9./8. * y + 3 * np.log(1 - np.exp(-y)) - self.debye_integral(y))"
    },
    {
        "original": "def with_local_env_strategy(structure, strategy):\n        \"\"\"\n        Constructor for StructureGraph, using a strategy\n        from :Class: `pymatgen.analysis.local_env`.\n\n        :param structure: Structure object\n        :param strategy: an instance of a\n            :Class: `pymatgen.analysis.local_env.NearNeighbors` object\n        :return:\n        \"\"\"\n\n        sg = StructureGraph.with_empty_graph(structure, name=\"bonds\",\n                                             edge_weight_name=\"weight\",\n                                             edge_weight_units=\"\")\n\n        for n, neighbors in enumerate(strategy.get_all_nn_info(structure)):\n            for neighbor in neighbors:\n\n                # local_env will always try to add two edges\n                # for any one bond, one from site u to site v\n                # and another form site v to site u: this is\n                # harmless, so warn_duplicates=False\n                sg.add_edge(from_index=n,\n                            from_jimage=(0, 0, 0),\n                            to_index=neighbor['site_index'],\n                            to_jimage=neighbor['image'],\n                            weight=neighbor['weight'],\n                            warn_duplicates=False)\n\n        return sg",
        "rewrite": "def with_local_env_strategy(structure, strategy):\n    sg = StructureGraph.with_empty_graph(structure, name=\"bonds\",\n                                         edge_weight_name=\"weight\",\n                                         edge_weight_units=\"\")\n\n    for n, neighbors in enumerate(strategy.get_all_nn_info(structure)):\n        for neighbor in neighbors:\n            sg.add_edge(from_index=n,\n                        from_jimage=(0, 0, 0),\n                        to_index=neighbor['site_index'],\n                        to_jimage=neighbor['image'],\n                        weight=neighbor['weight'],\n                        warn_duplicates=False)\n\n    return sg"
    },
    {
        "original": "def _find_packages_iter(cls, where, exclude, include):\n        \"\"\"\n        All the packages found in 'where' that pass the 'include' filter, but\n        not the 'exclude' filter.\n        \"\"\"\n        for root, dirs, files in os.walk(where, followlinks=True):\n            # Copy dirs to iterate over it, then empty dirs.\n            all_dirs = dirs[:]\n            dirs[:] = []\n\n            for dir in all_dirs:\n                full_path = os.path.join(root, dir)\n                rel_path = os.path.relpath(full_path, where)\n                package = rel_path.replace(os.path.sep, '.')\n\n                # Skip directory trees that are not valid packages\n                if ('.' in dir or not cls._looks_like_package(full_path)):\n                    continue\n\n                # Should this package be included?\n                if include(package) and not exclude(package):\n                    yield package\n\n                # Keep searching subdirectories, as there may be more packages\n                # down there, even if the parent was excluded.\n                dirs.append(dir)",
        "rewrite": "def _find_packages_iter(cls, where, exclude, include):\n    for root, dirs, files in os.walk(where, followlinks=True):\n        all_dirs = dirs[:]\n        dirs[:] = []\n        \n        for dir in all_dirs:\n            full_path = os.path.join(root, dir)\n            rel_path = os.path.relpath(full_path, where)\n            package = rel_path.replace(os.path.sep, '.')\n            \n            if ('.' in dir or not cls._looks_like_package(full_path)):\n                continue\n            \n            if include(package) and not exclude(package):\n                yield package\n            \n            dirs.append(dir)"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'input') and self.input is not None:\n            _dict['input'] = self.input._to_dict()\n        if hasattr(self, 'intents') and self.intents is not None:\n            _dict['intents'] = [x._to_dict() for x in self.intents]\n        if hasattr(self, 'entities') and self.entities is not None:\n            _dict['entities'] = [x._to_dict() for x in self.entities]\n        if hasattr(self,\n                   'alternate_intents') and self.alternate_intents is not None:\n            _dict['alternate_intents'] = self.alternate_intents\n        if hasattr(self, 'context') and self.context is not None:\n            _dict['context'] = self.context._to_dict()\n        if hasattr(self, 'output') and self.output is not None:\n            _dict['output'] = self.output._to_dict()\n        if hasattr(self, 'actions') and self.actions is not None:\n            _dict['actions'] = [x._to_dict() for x in self.actions]\n        return _dict",
        "rewrite": "def _to_dict(self):\n    _dict = {}\n    if hasattr(self, 'input') and self.input is not None:\n        _dict['input'] = self.input._to_dict()\n    if hasattr(self, 'intents') and self.intents is not None:\n        _dict['intents'] = [x._to_dict() for x in self.intents]\n    if hasattr(self, 'entities') and self.entities is not None:\n        _dict['entities'] = [x._to_dict() for x in self.entities]\n    if hasattr(self, 'alternate_intents') and self.alternate_intents is not None:\n        _dict['alternate_intents'] = self.alternate_intents\n    if hasattr(self, 'context') and self.context is not None:\n        _dict['context'] = self.context._to_dict()\n    if hasattr(self, 'output') and self.output is not None:\n        _dict['output'] = self.output._to_dict()\n    if hasattr(self, 'actions') and self.actions is not None:\n        _dict['actions'] = [x._to_dict() for x in self.actions]\n    return _dict"
    },
    {
        "original": "def _get_limits_networking(self):\n        \"\"\"\n        Return a dict of VPC-related limits only.\n        This method should only be used internally by\n        :py:meth:~.get_limits`.\n\n        :rtype: dict\n        \"\"\"\n        limits = {}\n        limits['Security groups per VPC'] = AwsLimit(\n            'Security groups per VPC',\n            self,\n            500,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::VPC',\n        )\n        limits['Rules per VPC security group'] = AwsLimit(\n            'Rules per VPC security group',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::VPC',\n        )\n        limits['VPC Elastic IP addresses (EIPs)'] = AwsLimit(\n            'VPC Elastic IP addresses (EIPs)',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::EIP',\n            limit_subtype='AWS::EC2::VPC',\n            ta_service_name='VPC'  # TA shows this as VPC not EC2\n        )\n        # the EC2 limits screen calls this 'EC2-Classic Elastic IPs'\n        # but Trusted Advisor just calls it 'Elastic IP addresses (EIPs)'\n        limits['Elastic IP addresses (EIPs)'] = AwsLimit(\n            'Elastic IP addresses (EIPs)',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::EIP',\n        )\n        limits['VPC security groups per elastic network interface'] = AwsLimit(\n            'VPC security groups per elastic network interface',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::NetworkInterface',\n        )\n        return limits",
        "rewrite": "def _get_limits_networking(self):\n    limits = {}\n    limits['Security groups per VPC'] = AwsLimit(\n        'Security groups per VPC',\n        self,\n        500,\n        self.warning_threshold,\n        self.critical_threshold,\n        limit_type='AWS::EC2::SecurityGroup',\n        limit_subtype='AWS::EC2::VPC',\n    )\n    limits['Rules per VPC security group'] = AwsLimit(\n        'Rules per VPC security group',\n        self,\n        50,\n        self.warning_threshold,\n        self.critical_threshold,\n        limit_type='AWS::EC2::SecurityGroup',\n        limit_subtype='AWS::EC2::VPC',\n    )\n    limits['VPC Elastic IP addresses (EIPs)'] = AwsLimit(\n        'VPC Elastic IP addresses (EIPs)',\n        self,\n        5,\n        self.warning_threshold,\n        self.critical_threshold,\n        limit_type='AWS::EC2::EIP',\n        limit_subtype='AWS::EC2::VPC',\n        ta_service_name='VPC'\n    )\n    limits['Elastic IP addresses (EIPs)'] = AwsLimit(\n        'Elastic IP addresses (EIPs)',\n        self,\n        5,\n        self.warning_threshold,\n        self.critical_threshold,\n        limit_type='AWS::EC2::EIP'\n    )\n    limits['VPC security groups per elastic network interface'] = AwsLimit(\n        'VPC security groups per elastic network interface',\n        self,\n        5,\n        self.warning_threshold,\n        self.critical_threshold,\n        limit_type='AWS::EC2::SecurityGroup',\n        limit_subtype='AWS::EC2::NetworkInterface'\n    )\n    return limits"
    },
    {
        "original": "def coordination_geometry_symmetry_measures_sepplane_optim(self, coordination_geometry,\n                                                               points_perfect=None,\n                                                               nb_set=None, optimization=None):\n        \"\"\"\n        Returns the symmetry measures of a given coordination_geometry for a set of permutations depending on\n        the permutation setup. Depending on the parameters of the LocalGeometryFinder and on the coordination\n         geometry, different methods are called.\n        :param coordination_geometry: Coordination geometry for which the symmetry measures are looked for\n        :return: the symmetry measures of a given coordination_geometry for a set of permutations\n        :raise: NotImplementedError if the permutation_setup does not exists\n        \"\"\"\n        csms = []\n        permutations = []\n        algos = []\n        local2perfect_maps = []\n        perfect2local_maps = []\n        for algo in coordination_geometry.algorithms:\n            if algo.algorithm_type == SEPARATION_PLANE:\n                cgsm = self.coordination_geometry_symmetry_measures_separation_plane_optim(\n                    coordination_geometry,\n                    algo,\n                    points_perfect=points_perfect,\n                    nb_set=nb_set,\n                    optimization=optimization)\n                csm, perm, algo, local2perfect_map, perfect2local_map = cgsm\n\n                csms.extend(csm)\n                permutations.extend(perm)\n                algos.extend(algo)\n                local2perfect_maps.extend(local2perfect_map)\n                perfect2local_maps.extend(perfect2local_map)\n        return csms, permutations, algos, local2perfect_maps, perfect2local_maps",
        "rewrite": "def coordination_geometry_symmetry_measures_sepplane_optim(self, coordination_geometry,\n                                                               points_perfect=None,\n                                                               nb_set=None, optimization=None):\n        csms = []\n        permutations = []\n        algos = []\n        local2perfect_maps = []\n        perfect2local_maps = []\n        \n        for algo in coordination_geometry.algorithms:\n            if algo.algorithm_type == SEPARATION_PLANE:\n                cgsm = self.coordination_geometry_symmetry_measures_separation_plane_optim(\n                    coordination_geometry,\n                    algo,\n                    points_perfect=points_perfect,\n                    nb_set=nb_set,\n                    optimization=optimization)\n                \n                csm, perm, algo, local2perfect_map, perfect2local_map = cgsm\n\n                csms.extend(csm)\n                permutations.extend(perm)\n                algos.extend(algo)\n                local2perfect_maps.extend(local2perfect_map)\n                perfect2local_maps.extend(perfect2local_map)\n                \n        return csms, permutations, algos, local2perfect_maps, perfect2local_maps"
    },
    {
        "original": "def prepare_cached_fields(self, flist):\n        \"\"\"\n        Prepare the cached fields of the fields_desc dict\n        \"\"\"\n\n        cls_name = self.__class__\n\n        # Fields cache initialization\n        if flist:\n            Packet.class_default_fields[cls_name] = dict()\n            Packet.class_default_fields_ref[cls_name] = list()\n            Packet.class_fieldtype[cls_name] = dict()\n            Packet.class_packetfields[cls_name] = list()\n\n        # Fields initialization\n        for f in flist:\n            if isinstance(f, MultipleTypeField):\n                del Packet.class_default_fields[cls_name]\n                del Packet.class_default_fields_ref[cls_name]\n                del Packet.class_fieldtype[cls_name]\n                del Packet.class_packetfields[cls_name]\n                self.class_dont_cache[cls_name] = True\n                self.do_init_fields(self.fields_desc)\n                break\n\n            tmp_copy = copy.deepcopy(f.default)\n            Packet.class_default_fields[cls_name][f.name] = tmp_copy\n            Packet.class_fieldtype[cls_name][f.name] = f\n            if f.holds_packets:\n                Packet.class_packetfields[cls_name].append(f)\n\n            # Remember references\n            if isinstance(f.default, (list, dict, set, RandField, Packet)):\n                Packet.class_default_fields_ref[cls_name].append(f.name)",
        "rewrite": "def prepare_cached_fields(self, flist):\n    cls_name = self.__class__\n\n    if flist:\n        Packet.class_default_fields.setdefault(cls_name, dict())\n        Packet.class_default_fields_ref.setdefault(cls_name, list())\n        Packet.class_fieldtype.setdefault(cls_name, dict())\n        Packet.class_packetfields.setdefault(cls_name, list())\n\n    for f in flist:\n        if isinstance(f, MultipleTypeField):\n            del Packet.class_default_fields[cls_name]\n            del Packet.class_default_fields_ref[cls_name]\n            del Packet.class_fieldtype[cls_name]\n            del Packet.class_packetfields[cls_name]\n            self.class_dont_cache[cls_name] = True\n            self.do_init_fields(self.fields_desc)\n            break\n\n        tmp_copy = copy.deepcopy(f.default)\n        Packet.class_default_fields[cls_name][f.name] = tmp_copy\n        Packet.class_fieldtype[cls_name][f.name] = f\n        if f.holds_packets:\n            Packet.class_packetfields[cls_name].append(f)\n\n        if isinstance(f.default, (list, dict, set, RandField, Packet)):\n            Packet.class_default_fields_ref[cls_name].append(f.name)"
    },
    {
        "original": "def chebyshev(x, y):\n    \"\"\"Chebyshev or l-infinity distance.\n\n    ..math::\n        D(x, y) = \\max_i |x_i - y_i|\n    \"\"\"\n    result = 0.0\n    for i in range(x.shape[0]):\n        result = max(result, np.abs(x[i] - y[i]))\n\n    return result",
        "rewrite": "def chebyshev(x, y):\n    result = 0.0\n    for i in range(len(x)):\n        result = max(result, abs(x[i] - y[i]))\n\n    return result"
    },
    {
        "original": "def add_edge_lengths(G):\n    \"\"\"\n    Add length (meters) attribute to each edge by great circle distance between\n    nodes u and v.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    G : networkx multidigraph\n    \"\"\"\n\n    start_time = time.time()\n\n    # first load all the edges' origin and destination coordinates as a\n    # dataframe indexed by u, v, key\n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u', 'v', 'k']] = df_coords[['u', 'v', 'k']].astype(np.int64)\n    df_coords = df_coords.set_index(['u', 'v', 'k'])\n\n    # then calculate the great circle distance with the vectorized function\n    gc_distances = great_circle_vec(lat1=df_coords['u_y'],\n                                    lng1=df_coords['u_x'],\n                                    lat2=df_coords['v_y'],\n                                    lng2=df_coords['v_x'])\n\n    # fill nulls with zeros and round to the millimeter\n    gc_distances = gc_distances.fillna(value=0).round(3)\n    nx.set_edge_attributes(G, name='length', values=gc_distances.to_dict())\n\n    log('Added edge lengths to graph in {:,.2f} seconds'.format(time.time()-start_time))\n    return G",
        "rewrite": "def add_edge_lengths(G):\n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u', 'v', 'k']] = df_coords[['u', 'v', 'k']].astype(np.int64)\n    df_coords = df_coords.set_index(['u', 'v', 'k'])\n    gc_distances = great_circle_vec(lat1=df_coords['u_y'], lng1=df_coords['u_x'], lat2=df_coords['v_y'], lng2=df_coords['v_x'])\n    gc_distances = gc_distances.fillna(value=0).round(3)\n    nx.set_edge_attributes(G, name='length', values=gc_distances.to_dict())\n    return G"
    },
    {
        "original": "def AddFilesWithUnknownHashes(\n    client_path_blob_refs,\n    use_external_stores = True\n):\n  \"\"\"Adds new files consisting of given blob references.\n\n  Args:\n    client_path_blob_refs: A dictionary mapping `db.ClientPath` instances to\n      lists of blob references.\n    use_external_stores: A flag indicating if the files should also be added to\n      external file stores.\n\n  Returns:\n    A dictionary mapping `db.ClientPath` to hash ids of the file.\n\n  Raises:\n    BlobNotFoundError: If one of the referenced blobs cannot be found.\n  \"\"\"\n  hash_id_blob_refs = dict()\n  client_path_hash_id = dict()\n  metadatas = dict()\n\n  all_client_path_blob_refs = list()\n  for client_path, blob_refs in iteritems(client_path_blob_refs):\n    # In the special case where there is only one blob, we don't need to go to\n    # the data store to read said blob and rehash it, we have all that\n    # information already available. For empty files without blobs, we can just\n    # hash the empty string instead.\n    if len(blob_refs) <= 1:\n      if blob_refs:\n        hash_id = rdf_objects.SHA256HashID.FromBytes(\n            blob_refs[0].blob_id.AsBytes())\n      else:\n        hash_id = rdf_objects.SHA256HashID.FromData(b\"\")\n\n      client_path_hash_id[client_path] = hash_id\n      hash_id_blob_refs[hash_id] = blob_refs\n      metadatas[hash_id] = FileMetadata(\n          client_path=client_path, blob_refs=blob_refs)\n    else:\n      for blob_ref in blob_refs:\n        all_client_path_blob_refs.append((client_path, blob_ref))\n\n  client_path_offset = collections.defaultdict(lambda: 0)\n  client_path_sha256 = collections.defaultdict(hashlib.sha256)\n  verified_client_path_blob_refs = collections.defaultdict(list)\n\n  client_path_blob_ref_batches = collection.Batch(\n      items=all_client_path_blob_refs, size=_BLOBS_READ_BATCH_SIZE)\n\n  for client_path_blob_ref_batch in client_path_blob_ref_batches:\n    blob_id_batch = set(\n        blob_ref.blob_id for _, blob_ref in client_path_blob_ref_batch)\n    blobs = data_store.BLOBS.ReadBlobs(blob_id_batch)\n\n    for client_path, blob_ref in client_path_blob_ref_batch:\n      blob = blobs[blob_ref.blob_id]\n      if blob is None:\n        message = \"Could not find one of referenced blobs: {}\".format(\n            blob_ref.blob_id)\n        raise BlobNotFoundError(message)\n\n      offset = client_path_offset[client_path]\n      if blob_ref.size != len(blob):\n        raise ValueError(\n            \"Got conflicting size information for blob %s: %d vs %d.\" %\n            (blob_ref.blob_id, blob_ref.size, len(blob)))\n      if blob_ref.offset != offset:\n        raise ValueError(\n            \"Got conflicting offset information for blob %s: %d vs %d.\" %\n            (blob_ref.blob_id, blob_ref.offset, offset))\n\n      verified_client_path_blob_refs[client_path].append(blob_ref)\n      client_path_offset[client_path] = offset + len(blob)\n      client_path_sha256[client_path].update(blob)\n\n  for client_path in iterkeys(client_path_sha256):\n    sha256 = client_path_sha256[client_path].digest()\n    hash_id = rdf_objects.SHA256HashID.FromBytes(sha256)\n\n    client_path_hash_id[client_path] = hash_id\n    hash_id_blob_refs[hash_id] = verified_client_path_blob_refs[client_path]\n\n  data_store.REL_DB.WriteHashBlobReferences(hash_id_blob_refs)\n\n  if use_external_stores:\n    for client_path in iterkeys(verified_client_path_blob_refs):\n      metadatas[client_path_hash_id[client_path]] = FileMetadata(\n          client_path=client_path,\n          blob_refs=verified_client_path_blob_refs[client_path])\n\n    EXTERNAL_FILE_STORE.AddFiles(metadatas)\n\n  return client_path_hash_id",
        "rewrite": "def AddFilesWithUnknownHashes(\n    client_path_blob_refs,\n    use_external_stores=True\n):\n    hash_id_blob_refs = dict()\n    client_path_hash_id = dict()\n    metadatas = dict()\n\n    all_client_path_blob_refs = []\n    for client_path, blob_refs in client_path_blob_refs.items():\n        if len(blob_refs) <= 1:\n            if blob_refs:\n                hash_id = rdf_objects.SHA256HashID.FromBytes(blob_refs[0].blob_id.AsBytes())\n            else:\n                hash_id = rdf_objects.SHA256HashID.FromData(b\"\")\n\n            client_path_hash_id[client_path] = hash_id\n            hash_id_blob_refs[hash_id] = blob_refs\n            metadatas[hash_id] = FileMetadata(client_path=client_path, blob_refs=blob_refs)\n        else:\n            for blob_ref in blob_refs:\n                all_client_path_blob_refs.append((client_path, blob_ref))\n\n    client_path_offset = collections.defaultdict(lambda: 0)\n    client_path_sha256 = collections.defaultdict(hashlib.sha256)\n    verified_client_path_blob_refs = collections.defaultdict(list)\n\n    client_path_blob_ref_batches = collection.Batch(\n        items=all_client_path_blob_refs, size=_BLOBS_READ_BATCH_SIZE)\n\n    for client_path_blob_ref_batch in client_path_blob_ref_batches:\n        blob_id_batch = set(blob_ref.blob_id for _, blob_ref in client_path_blob_ref_batch)\n        blobs = data_store.BLOBS.ReadBlobs(blob_id_batch)\n\n        for client_path, blob_ref in client_path_blob_ref_batch:\n            blob = blobs[blob_ref.blob_id]\n            if blob is None:\n                message = \"Could not find one of referenced blobs: {}\".format(blob_ref.blob_id)\n                raise BlobNotFoundError(message)\n\n            offset = client_path_offset[client_path]\n            if blob_ref.size != len(blob):\n                raise ValueError(\"Got conflicting size information for blob %s: %d vs %d.\" % (blob_ref.blob_id, blob_ref.size, len(blob)))\n            if blob_ref.offset != offset:\n                raise ValueError(\"Got conflicting offset information for blob %s: %d vs %d.\" % (blob_ref.blob_id, blob_ref.offset, offset))\n\n            verified_client_path_blob_refs[client_path].append(blob_ref)\n            client_path_offset[client_path] = offset + len(blob)\n            client_path_sha256[client_path].update(blob)\n\n    for client_path in client_path_sha256.keys():\n        sha256 = client_path_sha256[client_path].digest()\n        hash_id = rdf_objects.SHA256HashID.FromBytes(sha256)\n\n        client_path_hash_id[client_path] = hash_id\n        hash_id_blob_refs[hash_id] = verified_client_path_blob_refs[client_path]\n\n    data_store.REL_DB.WriteHashBlobReferences(hash_id_blob_refs)\n\n    if use_external_stores:\n        for client_path in client_path_sha256.keys():\n            metadatas[client_path_hash_id[client_path]] = FileMetadata(\n                client_path=client_path,\n                blob_refs=verified_client_path_blob_refs[client_path]\n            )\n\n        EXTERNAL_FILE_STORE.AddFiles(metadatas)\n\n    return client_path_hash_id"
    },
    {
        "original": "def is_outdated(self):\n        \"\"\"Return True if a new version is available\"\"\"\n        if self.args.disable_check_update:\n            # Check is disabled by configuration\n            return False\n\n        logger.debug(\"Check Glances version (installed: {} / latest: {})\".format(self.installed_version(), self.latest_version()))\n        return LooseVersion(self.latest_version()) > LooseVersion(self.installed_version())",
        "rewrite": "def is_outdated(self):\n    if self.args.disable_check_update:\n        return False\n\n    logger.debug(\"Check Glances version (installed: {} / latest: {})\".format(self.installed_version(), self.latest_version()))\n    \n    return LooseVersion(str(self.latest_version())) > LooseVersion(str(self.installed_version()))"
    },
    {
        "original": "def add(self, cmd):\n        \"\"\"\n        Add a new command (waypoint) at the end of the command list.\n\n        .. note::\n\n            Commands are sent to the vehicle only after you call ::py:func:`upload() <Vehicle.commands.upload>`.\n\n        :param Command cmd: The command to be added.\n        \"\"\"\n        self.wait_ready()\n        self._vehicle._handler.fix_targets(cmd)\n        self._vehicle._wploader.add(cmd, comment='Added by DroneKit')\n        self._vehicle._wpts_dirty = True",
        "rewrite": "def add(self, cmd):\n    self.wait_ready()\n    self._vehicle._handler.fix_targets(cmd)\n    self._vehicle._wploader.add(cmd, comment='Added by DroneKit')\n    self._vehicle._wpts_dirty = True"
    },
    {
        "original": "def refresh_existing_encodings(self):\n        \"\"\"\n        Refresh existing encodings for messages, when encoding was changed by user in dialog\n\n        :return:\n        \"\"\"\n        update = False\n\n        for msg in self.proto_analyzer.messages:\n            i = next((i for i, d in enumerate(self.decodings) if d.name == msg.decoder.name), 0)\n            if msg.decoder != self.decodings[i]:\n                update = True\n                msg.decoder = self.decodings[i]\n                msg.clear_decoded_bits()\n                msg.clear_encoded_bits()\n\n        if update:\n            self.protocol_model.update()\n            self.label_value_model.update()",
        "rewrite": "def refresh_existing_encodings(self):\n    update = False\n    \n    for msg in self.proto_analyzer.messages:\n        i = next((i for i, d in enumerate(self.decodings) if d.name == msg.decoder.name), 0)\n        if msg.decoder != self.decodings[i]:\n            update = True\n            msg.decoder = self.decodings[i]\n            msg.clear_decoded_bits()\n            msg.clear_encoded_bits()\n    \n    if update:\n        self.protocol_model.update()\n        self.label_value_model.update()"
    },
    {
        "original": "def _run_hooks(config, hooks, args, environ):\n    \"\"\"Actually run the hooks.\"\"\"\n    skips = _get_skips(environ)\n    cols = _compute_cols(hooks, args.verbose)\n    filenames = _all_filenames(args)\n    filenames = filter_by_include_exclude(filenames, '', config['exclude'])\n    classifier = Classifier(filenames)\n    retval = 0\n    for hook in hooks:\n        retval |= _run_single_hook(classifier, hook, args, skips, cols)\n        if retval and config['fail_fast']:\n            break\n    if retval and args.show_diff_on_failure and git.has_diff():\n        if args.all_files:\n            output.write_line(\n                'pre-commit hook(s) made changes.\\n'\n                'If you are seeing this message in CI, '\n                'reproduce locally with: `pre-commit run --all-files`.\\n'\n                'To run `pre-commit` as part of git workflow, use '\n                '`pre-commit install`.',\n            )\n        output.write_line('All changes made by hooks:')\n        subprocess.call(('git', '--no-pager', 'diff', '--no-ext-diff'))\n    return retval",
        "rewrite": "def _run_hooks(config, hooks, args, environ):\n    skips = _get_skips(environ)\n    cols = _compute_cols(hooks, args.verbose)\n    filenames = _all_filenames(args)\n    filenames = filter_by_include_exclude(filenames, '', config['exclude'])\n    classifier = Classifier(filenames)\n    retval = 0\n    for hook in hooks:\n        retval |= _run_single_hook(classifier, hook, args, skips, cols)\n        if retval and config['fail_fast']:\n            break\n    if retval and args.show_diff_on_failure and git.has_diff():\n        if args.all_files:\n            output.write_line(\n                'pre-commit hook(s) made changes.\\n'\n                'If you are seeing this message in CI, '\n                'reproduce locally with: `pre-commit run --all-files`.\\n'\n                'To run `pre-commit` as part of git workflow, use '\n                '`pre-commit install`.',\n            )\n        output.write_line('All changes made by hooks:')\n        subprocess.call(('git', '--no-pager', 'diff', '--no-ext-diff'))\n    return retval"
    },
    {
        "original": "def show_pricing(kwargs=None, call=None):\n    \"\"\"\n    Show pricing for a particular profile. This is only an estimate, based on\n    unofficial pricing sources.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_pricing my-softlayerhw-config profile=my-profile\n\n    If pricing sources have not been cached, they will be downloaded. Once they\n    have been cached, they will not be updated automatically. To manually update\n    all prices, use the following command:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_pricing <provider>\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n    profile = __opts__['profiles'].get(kwargs['profile'], {})\n    if not profile:\n        return {'Error': 'The requested profile was not found'}\n\n    # Make sure the profile belongs to Softlayer HW\n    provider = profile.get('provider', '0:0')\n    comps = provider.split(':')\n    if len(comps) < 2 or comps[1] != 'softlayer_hw':\n        return {'Error': 'The requested profile does not belong to Softlayer HW'}\n\n    raw = {}\n    ret = {}\n    ret['per_hour'] = 0\n    conn = get_conn(service='SoftLayer_Product_Item_Price')\n    for item in profile:\n        if item in ('profile', 'provider', 'location'):\n            continue\n        price = conn.getObject(id=profile[item])\n        raw[item] = price\n        ret['per_hour'] += decimal.Decimal(price.get('hourlyRecurringFee', 0))\n\n    ret['per_day'] = ret['per_hour'] * 24\n    ret['per_week'] = ret['per_day'] * 7\n    ret['per_month'] = ret['per_day'] * 30\n    ret['per_year'] = ret['per_week'] * 52\n\n    if kwargs.get('raw', False):\n        ret['_raw'] = raw\n\n    return {profile['profile']: ret}",
        "rewrite": "def show_pricing(kwargs=None, call=None):\n    profile = __opts__['profiles'].get(kwargs['profile'], {})\n    if not profile:\n        return {'Error': 'The requested profile was not found'}\n\n    provider = profile.get('provider', '0:0')\n    comps = provider.split(':')\n    if len(comps) < 2 or comps[1] != 'softlayer_hw':\n        return {'Error': 'The requested profile does not belong to Softlayer HW'}\n\n    raw = {}\n    ret = {}\n    ret['per_hour'] = 0\n    conn = get_conn(service='SoftLayer_Product_Item_Price')\n    for item in profile:\n        if item in ('profile', 'provider', 'location'):\n            continue\n        price = conn.getObject(id=profile[item])\n        raw[item] = price\n        ret['per_hour'] += decimal.Decimal(price.get('hourlyRecurringFee', 0))\n\n    ret['per_day'] = ret['per_hour'] * 24\n    ret['per_week'] = ret['per_day'] * 7\n    ret['per_month'] = ret['per_day'] * 30\n    ret['per_year'] = ret['per_week'] * 52\n\n    if kwargs.get('raw', False):\n        ret['_raw'] = raw\n\n    return {profile['profile']: ret}"
    },
    {
        "original": "def stop_tuning_job(self, name):\n        \"\"\"Stop the Amazon SageMaker hyperparameter tuning job with the specified name.\n\n        Args:\n            name (str): Name of the Amazon SageMaker hyperparameter tuning job.\n\n        Raises:\n            ClientError: If an error occurs while trying to stop the hyperparameter tuning job.\n        \"\"\"\n        try:\n            LOGGER.info('Stopping tuning job: {}'.format(name))\n            self.sagemaker_client.stop_hyper_parameter_tuning_job(HyperParameterTuningJobName=name)\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            # allow to pass if the job already stopped\n            if error_code == 'ValidationException':\n                LOGGER.info('Tuning job: {} is already stopped or not running.'.format(name))\n            else:\n                LOGGER.error('Error occurred while attempting to stop tuning job: {}. Please try again.'.format(name))\n                raise",
        "rewrite": "def stop_tuning_job(self, name):\n    try:\n        LOGGER.info('Stopping tuning job: {}'.format(name))\n        self.sagemaker_client.stop_hyper_parameter_tuning_job(HyperParameterTuningJobName=name)\n    except ClientError as e:\n        error_code = e.response['Error']['Code']\n        if error_code == 'ValidationException':\n            LOGGER.info('Tuning job: {} is already stopped or not running.'.format(name))\n        else:\n            LOGGER.error('Error occurred while attempting to stop tuning job: {}. Please try again.'.format(name))\n            raise"
    },
    {
        "original": "def depolarizing_operators(p):\n    \"\"\"\n    Return the phase damping Kraus operators\n    \"\"\"\n    k0 = np.sqrt(1.0 - p) * I\n    k1 = np.sqrt(p / 3.0) * X\n    k2 = np.sqrt(p / 3.0) * Y\n    k3 = np.sqrt(p / 3.0) * Z\n    return k0, k1, k2, k3",
        "rewrite": "def depolarizing_operators(p):\n    k0 = np.sqrt(1.0 - p) * I\n    k1 = np.sqrt(p / 3.0) * X\n    k2 = np.sqrt(p / 3.0) * Y\n    k3 = np.sqrt(p / 3.0) * Z\n    return k0, k1, k2, k3"
    },
    {
        "original": "def get_proto(self):\n        \"\"\"\n        Return the prototype of the method\n\n        :rtype: string\n        \"\"\"\n        if self.proto_idx_value is None:\n            self.proto_idx_value = self.CM.get_proto(self.proto_idx)\n\n        return self.proto_idx_value",
        "rewrite": "def get_proto(self):\n    if self.proto_idx_value is None:\n        self.proto_idx_value = self.CM.get_proto(self.proto_idx)\n\n    return self.proto_idx_value"
    },
    {
        "original": "def b_operator(self, P):\n        r\"\"\"\n        The B operator, mapping P into\n\n        .. math::\n\n            B(P) := R - \\beta^2 A'PB(Q + \\beta B'PB)^{-1}B'PA + \\beta A'PA\n\n        and also returning\n\n        .. math::\n\n            F := (Q + \\beta B'PB)^{-1} \\beta B'PA\n\n        Parameters\n        ----------\n        P : array_like(float, ndim=2)\n            A matrix that should be n x n\n\n        Returns\n        -------\n        F : array_like(float, ndim=2)\n            The F matrix as defined above\n        new_p : array_like(float, ndim=2)\n            The matrix P after applying the B operator\n\n        \"\"\"\n        A, B, Q, R, beta = self.A, self.B, self.Q, self.R, self.beta\n        S1 = Q + beta * dot(B.T, dot(P, B))\n        S2 = beta * dot(B.T, dot(P, A))\n        S3 = beta * dot(A.T, dot(P, A))\n        F = solve(S1, S2) if not self.pure_forecasting else np.zeros(\n            (self.k, self.n))\n        new_P = R - dot(S2.T, F) + S3\n\n        return F, new_P",
        "rewrite": "def b_operator(self, P):\n    A, B, Q, R, beta = self.A, self.B, self.Q, self.R, self.beta\n    S1 = Q + beta * np.dot(B.T, np.dot(P, B))\n    S2 = beta * np.dot(B.T, np.dot(P, A))\n    S3 = beta * np.dot(A.T, np.dot(P, A))\n    F = np.linalg.solve(S1, S2) if not self.pure_forecasting else np.zeros((self.k, self.n))\n    new_P = R - np.dot(S2.T, F) + S3\n\n    return F, new_P"
    },
    {
        "original": "def get_token(authed_user: hug.directives.user):\n    \"\"\"\n    Get Job details\n    :param authed_user:\n    :return:\n    \"\"\"\n    user_model = Query()\n    user = db.search(user_model.username == authed_user)[0]\n\n    if user:\n        out = {\n            'user': user['username'],\n            'api_key': user['api_key']\n        }\n    else:\n        # this should never happen\n        out = {\n            'error': 'User {0} does not exist'.format(authed_user)\n        }\n\n    return out",
        "rewrite": "def get_token(authed_user: hug.directives.user):\n    user_model = Query()\n    user = db.search(user_model.username == authed_user)[0]\n\n    if user:\n        return {'user': user['username'], 'api_key': user['api_key']}\n    else:\n        return {'error': 'User {0} does not exist'.format(authed_user)}"
    },
    {
        "original": "def update_wrapper(self, process_list):\n        \"\"\"Wrapper for the children update\"\"\"\n        # Set the number of running process\n        self.set_count(len(process_list))\n        # Call the children update method\n        if self.should_update():\n            return self.update(process_list)\n        else:\n            return self.result()",
        "rewrite": "def update_wrapper(self, process_list):\n     self.set_count(len(process_list))\n     if self.should_update():\n         return self.update(process_list)\n     else:\n         return self.result()"
    },
    {
        "original": "def _keyDown(key):\n    \"\"\"Performs a keyboard key press without the release. This will put that\n    key in a held down state.\n\n    NOTE: For some reason, this does not seem to cause key repeats like would\n    happen if a keyboard key was held down on a text field.\n\n    Args:\n      key (str): The key to be pressed down. The valid names are listed in\n      pyautogui.KEY_NAMES.\n\n    Returns:\n      None\n    \"\"\"\n    if key not in keyboardMapping or keyboardMapping[key] is None:\n        return\n\n    needsShift = pyautogui.isShiftCharacter(key)\n\n    ",
        "rewrite": "def _keyDown(key):\n    if key not in keyboardMapping or keyboardMapping[key] is None:\n        return\n\n    needsShift = pyautogui.isShiftCharacter(key)"
    },
    {
        "original": "def remove_node(self, node_id, force=False):\n        \"\"\"\n        Remove a node from the swarm.\n\n        Args:\n            node_id (string): ID of the node to be removed.\n            force (bool): Force remove an active node. Default: `False`\n\n        Raises:\n            :py:class:`docker.errors.NotFound`\n                If the node referenced doesn't exist in the swarm.\n\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        Returns:\n            `True` if the request was successful.\n        \"\"\"\n        url = self._url('/nodes/{0}', node_id)\n        params = {\n            'force': force\n        }\n        res = self._delete(url, params=params)\n        self._raise_for_status(res)\n        return True",
        "rewrite": "def remove_node(self, node_id, force=False):\n        url = self._url('/nodes/{0}', node_id)\n        params = {'force': force}\n        res = self._delete(url, params=params)\n        self._raise_for_status(res)\n        return True"
    },
    {
        "original": "def StoreStat(self, responses):\n    \"\"\"Stores stat entry in the flow's state.\"\"\"\n    index = responses.request_data[\"index\"]\n    if not responses.success:\n      self.Log(\"Failed to stat file: %s\", responses.status)\n      # Report failure.\n      self._FileFetchFailed(index, responses.request_data[\"request_name\"])\n      return\n\n    tracker = self.state.pending_hashes[index]\n    tracker[\"stat_entry\"] = responses.First()",
        "rewrite": "def StoreStat(self, responses):\n    index = responses.request_data[\"index\"]\n    if not responses.success:\n      self.Log(\"Failed to stat file: %s\", responses.status)\n      self._FileFetchFailed(index, responses.request_data[\"request_name\"])\n      return\n\n    tracker = self.state.pending_hashes[index]\n    tracker[\"stat_entry\"] = responses.First()"
    },
    {
        "original": "def _yarn_node_metrics(self, rm_address, instance, addl_tags):\n        \"\"\"\n        Get metrics related to YARN nodes\n        \"\"\"\n        metrics_json = self._rest_request_to_json(rm_address, instance, YARN_NODES_PATH, addl_tags)\n\n        if metrics_json and metrics_json['nodes'] is not None and metrics_json['nodes']['node'] is not None:\n\n            for node_json in metrics_json['nodes']['node']:\n                node_id = node_json['id']\n\n                tags = ['node_id:{}'.format(str(node_id))]\n                tags.extend(addl_tags)\n\n                self._set_yarn_metrics_from_json(tags, node_json, YARN_NODE_METRICS)",
        "rewrite": "def _yarn_node_metrics(self, rm_address, instance, addl_tags):\n    metrics_json = self._rest_request_to_json(rm_address, instance, YARN_NODES_PATH, addl_tags)\n    \n    if metrics_json and 'nodes' in metrics_json and 'node' in metrics_json['nodes']:\n        for node_json in metrics_json['nodes']['node']:\n            node_id = node_json.get('id')\n\n            tags = ['node_id:{}'.format(str(node_id))]\n            tags.extend(addl_tags)\n\n            self._set_yarn_metrics_from_json(tags, node_json, YARN_NODE_METRICS)"
    },
    {
        "original": "def subscribe_to_hub(self, event, callback, secret=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `POST /hub <http://developer.github.com/>`_\n        :param event: string\n        :param callback: string\n        :param secret: string\n        :rtype: None\n        \"\"\"\n        return self._hub(\"subscribe\", event, callback, secret)",
        "rewrite": "def subscribe_to_hub(self, event, callback, secret=github.GithubObject.NotSet):\n    return self._hub(\"subscribe\", event, callback, secret)"
    },
    {
        "original": "def output_json(data, code, headers=None):\n    \"\"\"Makes a Flask response with a JSON encoded body\"\"\"\n\n    settings = current_app.config.get('RESTFUL_JSON', {})\n\n    # If we're in debug mode, and the indent is not set, we set it to a\n    # reasonable value here.  Note that this won't override any existing value\n    # that was set.  We also set the \"sort_keys\" value.\n    if current_app.debug:\n        settings.setdefault('indent', 4)\n        settings.setdefault('sort_keys', not PY3)\n\n    # always end the json dumps with a new line\n    # see https://github.com/mitsuhiko/flask/pull/1262\n    dumped = dumps(data, **settings) + \"\\n\"\n\n    resp = make_response(dumped, code)\n    resp.headers.extend(headers or {})\n    return resp",
        "rewrite": "def output_json(data, code, headers=None):\n    settings = current_app.config.get('RESTFUL_JSON', {})\n\n    if current_app.debug:\n        settings.setdefault('indent', 4)\n        settings.setdefault('sort_keys', not PY3)\n\n    dumped = dumps(data, **settings) + \"\\n\"\n\n    resp = make_response(dumped, code)\n    resp.headers.extend(headers or {})\n    return resp"
    },
    {
        "original": "def check_theme(theme):\n        \"\"\"\n        Check if the given theme is compatible with the terminal\n        \"\"\"\n        terminal_colors = curses.COLORS if curses.has_colors() else 0\n\n        if theme.required_colors > terminal_colors:\n            return False\n        elif theme.required_color_pairs > curses.COLOR_PAIRS:\n            return False\n        else:\n            return True",
        "rewrite": "def check_theme(theme):\n    terminal_colors = curses.COLORS if curses.has_colors() else 0\n\n    if theme.required_colors > terminal_colors or theme.required_color_pairs > curses.COLOR_PAIRS:\n        return False\n    return True"
    },
    {
        "original": "def start(self, build_requests=None, callback=None):\n        \"\"\"\n        Run the client using a background thread.\n        \"\"\"\n        if callback:\n            self.callback = callback\n        if build_requests:\n            self.build_requests = build_requests\n\n        # spin off requester thread\n        self.sw = threading.Thread(target=self.run)\n        self.sw.start()",
        "rewrite": "def start(self, build_requests=None, callback=None):\n        if callback:\n            self.callback = callback\n        if build_requests:\n            self.build_requests = build_requests\n\n        self.sw = threading.Thread(target=self.run)\n        self.sw.start()"
    },
    {
        "original": "def unix_device(self, prefix=None):\n        \"\"\"\n        :param prefix: sd|vd|xvd\n        \"\"\"\n        prefix = prefix or self.random_element(self.unix_device_prefixes)\n        suffix = self.random_element(string.ascii_lowercase)\n        path = '/dev/%s%s' % (prefix, suffix)\n        return path",
        "rewrite": "def unix_device(self, prefix=None):\n        prefix = prefix or self.random_element(self.unix_device_prefixes)\n        suffix = self.random_element(string.ascii_lowercase)\n        path = f'/dev/{prefix}{suffix}'\n        return path"
    },
    {
        "original": "def propagate_ids(cls, obj, match_id, new_id, applied_keys, backend=None):\n        \"\"\"\n        Recursively propagate an id through an object for components\n        matching the applied_keys. This method can only be called if\n        there is a tree with a matching id in Store.custom_options\n        \"\"\"\n        applied = []\n        def propagate(o):\n            if o.id == match_id or (o.__class__.__name__ == 'DynamicMap'):\n                setattr(o, 'id', new_id)\n                applied.append(o)\n        obj.traverse(propagate, specs=set(applied_keys) | {'DynamicMap'})\n\n        # Clean up the custom tree if it was not applied\n        if not new_id in Store.custom_options(backend=backend):\n            raise AssertionError(\"New option id %d does not match any \"\n                                 \"option trees in Store.custom_options.\"\n                                 % new_id)\n        return applied",
        "rewrite": "def propagate_ids(cls, obj, match_id, new_id, applied_keys, backend=None):\n    applied = []\n    def propagate(o):\n        if o.id == match_id or (o.__class__.__name__ == 'DynamicMap'):\n            setattr(o, 'id', new_id)\n            applied.append(o)\n    obj.traverse(propagate, specs=set(applied_keys) | {'DynamicMap'})\n\n    if new_id not in Store.custom_options(backend=backend):\n        raise AssertionError(\"New option id %d does not match any option trees in Store.custom_options.\" % new_id)\n    \n    return applied"
    },
    {
        "original": "def _system_path(self, subdir, basename=''):\n        \"\"\"\n        Gets the full path to the 'subdir/basename' file in the system binwalk directory.\n\n        @subdir   - Subdirectory inside the system binwalk directory.\n        @basename - File name inside the subdirectory.\n\n        Returns the full path to the 'subdir/basename' file.\n        \"\"\"\n        try:\n            return self._file_path(os.path.join(self.system_dir, subdir), basename)\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception:\n            return None",
        "rewrite": "def _system_path(self, subdir, basename=''):\n    try:\n        return self._file_path(os.path.join(self.system_dir, subdir), basename)\n    except KeyboardInterrupt as e:\n        raise e\n    except Exception:\n        return None"
    },
    {
        "original": "def _sentiment(self, distance=True):\n    \"\"\"Calculates the sentiment of an entity as it appears in text.\"\"\"\n    sum_pos = 0\n    sum_neg = 0\n    text = self.parent\n    entity_positions = range(self.start, self.end)\n    non_entity_positions = set(range(len(text.words))).difference(entity_positions)\n    if not distance:\n      non_entity_polarities = np.array([text.words[i].polarity for i in non_entity_positions])\n      sum_pos = sum(non_entity_polarities == 1)\n      sum_neg = sum(non_entity_polarities == -1)\n    else:\n      polarities = np.array([w.polarity for w in text.words])\n      polarized_positions = np.argwhere(polarities != 0)[0]\n      polarized_non_entity_positions = non_entity_positions.intersection(polarized_positions)\n      sentence_len = len(text.words)\n      for i in polarized_non_entity_positions:\n        min_dist = min(abs(self.start - i), abs(self.end - i))\n        if text.words[i].polarity == 1:\n          sum_pos += 1.0 - (min_dist - 1.0) / (2.0 * sentence_len)\n        else:\n          sum_neg += 1.0 - (min_dist - 1.0) / (2.0 *sentence_len)\n    return (sum_pos, sum_neg)",
        "rewrite": "def _sentiment(self, distance=True):\n    sum_pos = 0\n    sum_neg = 0\n    text = self.parent\n    entity_positions = list(range(self.start, self.end))\n    non_entity_positions = set(range(len(text.words))) - set(entity_positions)\n    \n    if not distance:\n        non_entity_polarities = np.array([text.words[i].polarity for i in non_entity_positions])\n        sum_pos = sum(non_entity_polarities == 1)\n        sum_neg = sum(non_entity_polarities == -1)\n    else:\n        polarities = np.array([w.polarity for w in text.words])\n        polarized_positions = np.argwhere(polarities != 0)[:, 0]\n        polarized_non_entity_positions = set(non_entity_positions).intersection(set(polarized_positions))\n        sentence_len = len(text.words)\n        \n        for i in polarized_non_entity_positions:\n            min_dist = min(abs(self.start - i), abs(self.end - i))\n            \n            if text.words[i].polarity == 1:\n                sum_pos += 1.0 - (min_dist - 1.0) / (2.0 * sentence_len)\n            else:\n                sum_neg += 1.0 - (min_dist - 1.0) / (2.0 * sentence_len)\n                \n    return (sum_pos, sum_neg)"
    },
    {
        "original": "def _get_mask(X, value_to_mask):\n    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n    if is_scalar_nan(value_to_mask):\n        if X.dtype.kind == \"f\":\n            return np.isnan(X)\n        elif X.dtype.kind in (\"i\", \"u\"):\n            # can't have NaNs in integer array.\n            return np.zeros(X.shape, dtype=bool)\n        else:\n            # np.isnan does not work on object dtypes.\n            return _object_dtype_isnan(X)\n    else:\n        # X == value_to_mask with object dytpes does not always perform\n        # element-wise for old versions of numpy\n        return np.equal(X, value_to_mask)",
        "rewrite": "def _get_mask(X, value_to_mask):\n    if is_scalar_nan(value_to_mask) and X.dtype.kind == \"f\":\n        return np.isnan(X)\n        \n    elif is_scalar_nan(value_to_mask) and X.dtype.kind in (\"i\", \"u\"):\n        return np.zeros(X.shape, dtype=bool)\n        \n    else:\n        return np.equal(X, value_to_mask)"
    },
    {
        "original": "def get_init_container(self,\n                           init_command,\n                           init_args,\n                           env_vars,\n                           context_mounts,\n                           persistence_outputs,\n                           persistence_data):\n        \"\"\"Pod init container for setting outputs path.\"\"\"\n        env_vars = to_list(env_vars, check_none=True)\n        if self.original_name is not None and self.cloning_strategy == CloningStrategy.RESUME:\n            return []\n        if self.original_name is not None and self.cloning_strategy == CloningStrategy.COPY:\n            command = InitCommands.COPY\n            original_outputs_path = stores.get_experiment_outputs_path(\n                persistence=persistence_outputs,\n                experiment_name=self.original_name)\n        else:\n            command = InitCommands.CREATE\n            original_outputs_path = None\n\n        outputs_path = stores.get_experiment_outputs_path(\n            persistence=persistence_outputs,\n            experiment_name=self.experiment_name)\n        _, outputs_volume_mount = get_pod_outputs_volume(persistence_outputs=persistence_outputs)\n        volume_mounts = outputs_volume_mount + to_list(context_mounts, check_none=True)\n        init_command = init_command or [\"/bin/sh\", \"-c\"]\n        init_args = init_args or to_list(\n            get_output_args(command=command,\n                            outputs_path=outputs_path,\n                            original_outputs_path=original_outputs_path))\n        init_args += to_list(get_auth_context_args(entity='experiment',\n                                                   entity_name=self.experiment_name))\n        return [\n            client.V1Container(\n                name=self.init_container_name,\n                image=self.init_docker_image,\n                image_pull_policy=self.init_docker_image_pull_policy,\n                command=init_command,\n                args=[''.join(init_args)],\n                env=env_vars,\n                volume_mounts=volume_mounts)\n        ]",
        "rewrite": "def get_init_container(self,\n                           init_command,\n                           init_args,\n                           env_vars,\n                           context_mounts,\n                           persistence_outputs,\n                           persistence_data):\n        \n        env_vars = to_list(env_vars, check_none=True)\n        if self.original_name is not None and self.cloning_strategy == CloningStrategy.RESUME:\n            return []\n        if self.original_name is not None and self.cloning_strategy == CloningStrategy.COPY:\n            command = InitCommands.COPY\n            original_outputs_path = stores.get_experiment_outputs_path(\n                persistence=persistence_outputs,\n                experiment_name=self.original_name)\n        else:\n            command = InitCommands.CREATE\n            original_outputs_path = None\n\n        outputs_path = stores.get_experiment_outputs_path(\n            persistence=persistence_outputs,\n            experiment_name=self.experiment_name)\n        _, outputs_volume_mount = get_pod_outputs_volume(persistence_outputs=persistence_outputs)\n        volume_mounts = outputs_volume_mount + to_list(context_mounts, check_none=True)\n        init_command = init_command or [\"/bin/sh\", \"-c\"]\n        init_args = init_args or to_list(\n            get_output_args(command=command,\n                            outputs_path=outputs_path,\n                            original_outputs_path=original_outputs_path))\n        init_args += to_list(get_auth_context_args(entity='experiment',\n                                                   entity_name=self.experiment_name))\n        return [\n            client.V1Container(\n                name=self.init_container_name,\n                image=self.init_docker_image,\n                image_pull_policy=self.init_docker_image_pull_policy,\n                command=init_command,\n                args=[''.join(init_args)],\n                env=env_vars,\n                volume_mounts=volume_mounts)\n        ]"
    },
    {
        "original": "def plot_helmholtz_free_energy(self, tmin, tmax, ntemp, ylim=None, **kwargs):\n        \"\"\"\n        Plots the vibrational contribution to the Helmoltz free energy in a temperature range.\n\n        Args:\n            tmin: minimum temperature\n            tmax: maximum temperature\n            ntemp: number of steps\n            ylim: tuple specifying the y-axis limits.\n            kwargs: kwargs passed to the matplotlib function 'plot'.\n        Returns:\n            matplotlib figure\n        \"\"\"\n        temperatures = np.linspace(tmin, tmax, ntemp)\n\n        if self.structure:\n            ylabel = r\"$\\Delta F$ (kJ/mol)\"\n        else:\n            ylabel = r\"$\\Delta F$ (kJ/mol-c)\"\n\n        fig = self._plot_thermo(self.dos.helmholtz_free_energy, temperatures, ylabel=ylabel, ylim=ylim,\n                                factor=1e-3, **kwargs)\n\n        return fig",
        "rewrite": "def plot_helmholtz_free_energy(self, tmin, tmax, ntemp, ylim=None, **kwargs):\n    temperatures = np.linspace(tmin, tmax, ntemp)\n    ylabel = r\"$\\Delta F$ (kJ/mol)\" if self.structure else r\"$\\Delta F$ (kJ/mol-c)\"\n    fig = self._plot_thermo(self.dos.helmholtz_free_energy, temperatures, ylabel=ylabel, ylim=ylim, factor=1e-3, **kwargs)\n    return fig"
    },
    {
        "original": "def SetPlatformArchContext():\n  \"\"\"Add the running contexts to the config system.\"\"\"\n\n  # Initialize the running platform context:\n  _CONFIG.AddContext(\"Platform:%s\" % platform.system().title())\n\n  machine = platform.uname()[4]\n  if machine in [\"x86_64\", \"AMD64\", \"i686\"]:\n    # 32 bit binaries running on AMD64 will still have a i386 arch.\n    if platform.architecture()[0] == \"32bit\":\n      arch = \"i386\"\n    else:\n      arch = \"amd64\"\n  elif machine == \"x86\":\n    arch = \"i386\"\n  else:\n    arch = machine\n\n  _CONFIG.AddContext(\"Arch:%s\" % arch)",
        "rewrite": "def SetPlatformArchContext():\n    _CONFIG.AddContext(\"Platform:%s\" % platform.system().title())\n\n    machine = platform.uname()[4]\n    if machine in [\"x86_64\", \"AMD64\", \"i686\"]:\n        if platform.architecture()[0] == \"32bit\":\n            arch = \"i386\"\n        else:\n            arch = \"amd64\"\n    elif machine == \"x86\":\n        arch = \"i386\"\n    else:\n        arch = machine\n\n    _CONFIG.AddContext(\"Arch:%s\" % arch)"
    },
    {
        "original": "def compute_mu(L_aug, Y, k, p):\n    \"\"\"Given label matrix L_aug and labels Y, compute the true mu params.\n\n    Args:\n        L: (np.array {0,1}) [n, d] The augmented (indicator) label matrix\n        Y: (np.array int) [n] The true labels in {1,...,k}\n        k: (int) Cardinality\n        p: (np.array float) [k] The class balance\n    \"\"\"\n    n, d = L_aug.shape\n    assert Y.shape[0] == n\n\n    # Compute mu\n    mu = np.zeros((d, k))\n    for y in range(1, k + 1):\n        L_y = L_aug[Y == y]\n        mu[:, y - 1] = L_y.sum(axis=0) / L_y.shape[0]\n    return mu",
        "rewrite": "import numpy as np\n\ndef compute_mu(L_aug, Y, k, p):\n    n, d = L_aug.shape\n    assert Y.shape[0] == n\n    \n    mu = np.zeros((d, k))\n    for y in range(1, k + 1):\n        L_y = L_aug[Y == y]\n        mu[:, y - 1] = L_y.sum(axis=0) / L_y.shape[0]\n    return mu"
    },
    {
        "original": "def get_reactions(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/reactions <https://developer.github.com/v3/reactions/#list-reactions-for-an-issue>`_\n        :return: :class: :class:`github.PaginatedList.PaginatedList` of :class:`github.Reaction.Reaction`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Reaction.Reaction,\n            self._requester,\n            self.url + \"/reactions\",\n            None,\n            headers={'Accept': Consts.mediaTypeReactionsPreview}\n        )",
        "rewrite": "def get_reactions(self):\n        return github.PaginatedList.PaginatedList(\n            github.Reaction.Reaction,\n            self._requester,\n            self.url + \"/reactions\",\n            None,\n            headers={'Accept': Consts.mediaTypeReactionsPreview}\n        )"
    },
    {
        "original": "def plot(self, ax_list=None, fontsize=12, **kwargs):\n        \"\"\"\n        Plot relaxation history i.e. the results of the last iteration of each SCF cycle.\n\n        Args:\n            ax_list: List of axes. If None a new figure is produced.\n            fontsize: legend fontsize.\n            kwargs: keyword arguments are passed to ax.plot\n\n        Returns: matplotlib figure\n        \"\"\"\n        history = self.history\n\n        # Build grid of plots.\n        num_plots, ncols, nrows = len(history), 1, 1\n        if num_plots > 1:\n            ncols = 2\n            nrows = num_plots // ncols + num_plots % ncols\n\n        ax_list, fig, plot = get_axarray_fig_plt(ax_list, nrows=nrows, ncols=ncols,\n                                                 sharex=True, sharey=False, squeeze=False)\n        ax_list = np.array(ax_list).ravel()\n\n        iter_num = np.array(list(range(self.num_iterations))) + 1\n        label = kwargs.pop(\"label\", None)\n\n        for i, ((key, values), ax) in enumerate(zip(history.items(), ax_list)):\n            ax.grid(True)\n            ax.set_xlabel('Relaxation Step')\n            ax.set_xticks(iter_num, minor=False)\n            ax.set_ylabel(key)\n\n            xx, yy = iter_num, values\n            if not kwargs and label is None:\n                ax.plot(xx, yy, \"-o\", lw=2.0)\n            else:\n                ax.plot(xx, yy, label=label if i == 0 else None, **kwargs)\n\n            if key in _VARS_SUPPORTING_LOGSCALE and np.all(yy > 1e-22):\n                ax.set_yscale(\"log\")\n\n            if key in _VARS_WITH_YRANGE:\n                ymin, ymax = _VARS_WITH_YRANGE[key]\n                val_min, val_max = np.min(yy), np.max(yy)\n                if abs(val_max - val_min) > abs(ymax - ymin):\n                    ax.set_ylim(ymin, ymax)\n\n            if label is not None:\n                ax.legend(loc=\"best\", fontsize=fontsize, shadow=True)\n\n        # Get around a bug in matplotlib.\n        if num_plots % ncols != 0:\n            ax_list[-1].plot(xx, yy, lw=0.0)\n            ax_list[-1].axis('off')\n\n        return fig",
        "rewrite": "def plot(self, ax_list=None, fontsize=12, **kwargs):\n    history = self.history\n\n    num_plots, ncols, nrows = len(history), 1, 1\n    if num_plots > 1:\n        ncols = 2\n        nrows = num_plots // ncols + num_plots % ncols\n\n    ax_list, fig, plot = get_axarray_fig_plt(ax_list, nrows=nrows, ncols=ncols,\n                                             sharex=True, sharey=False, squeeze=False)\n    ax_list = np.array(ax_list).ravel()\n\n    iter_num = np.array(list(range(self.num_iterations))) + 1\n    label = kwargs.pop(\"label\", None)\n\n    for i, ((key, values), ax) in enumerate(zip(history.items(), ax_list)):\n        ax.grid(True)\n        ax.set_xlabel('Relaxation Step')\n        ax.set_xticks(iter_num, minor=False)\n        ax.set_ylabel(key)\n\n        xx, yy = iter_num, values\n        if not kwargs and label is None:\n            ax.plot(xx, yy, \"-o\", lw=2.0)\n        else:\n            ax.plot(xx, yy, label=label if i == 0 else None, **kwargs)\n\n        if key in _VARS_SUPPORTING_LOGSCALE and np.all(yy > 1e-22):\n            ax.set_yscale(\"log\")\n\n        if key in _VARS_WITH_YRANGE:\n            ymin, ymax = _VARS_WITH_YRANGE[key]\n            val_min, val_max = np.min(yy), np.max(yy)\n            if abs(val_max - val_min) > abs(ymax - ymin):\n                ax.set_ylim(ymin, ymax)\n\n        if label is not None:\n            ax.legend(loc=\"best\", fontsize=fontsize, shadow=True)\n\n    if num_plots % ncols != 0:\n        ax_list[-1].plot(xx, yy, lw=0.0)\n        ax_list[-1].axis('off')\n\n    return fig"
    },
    {
        "original": "def get_repo_teams(repo_name, profile='github'):\n    \"\"\"\n    Return teams belonging to a repository.\n\n    .. versionadded:: 2017.7.0\n\n    repo_name\n        The name of the repository from which to retrieve teams.\n\n    profile\n        The name of the profile configuration to use. Defaults to ``github``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion github.get_repo_teams salt\n        salt myminion github.get_repo_teams salt profile='my-github-profile'\n    \"\"\"\n    ret = []\n    org_name = _get_config_value(profile, 'org_name')\n    client = _get_client(profile)\n\n    try:\n        repo = client.get_repo('/'.join([org_name, repo_name]))\n    except github.UnknownObjectException:\n        raise CommandExecutionError(\n            'The \\'{0}\\' repository under the \\'{1}\\' organization could not '\n            'be found.'.format(repo_name, org_name)\n        )\n    try:\n        teams = repo.get_teams()\n        for team in teams:\n            ret.append({\n                'id': team.id,\n                'name': team.name,\n                'permission': team.permission\n            })\n    except github.UnknownObjectException:\n        raise CommandExecutionError(\n            'Unable to retrieve teams for repository \\'{0}\\' under the \\'{1}\\' '\n            'organization.'.format(repo_name, org_name)\n        )\n    return ret",
        "rewrite": "def get_repo_teams(repo_name, profile='github'):\n    ret = []\n    org_name = _get_config_value(profile, 'org_name')\n    client = _get_client(profile)\n\n    try:\n        repo = client.get_repo('/'.join([org_name, repo_name]))\n    except github.UnknownObjectException:\n        raise CommandExecutionError(\n            'The \\'{0}\\' repository under the \\'{1}\\' organization could not '\n            'be found.'.format(repo_name, org_name)\n        )\n    try:\n        teams = repo.get_teams()\n        for team in teams:\n            ret.append({\n                'id': team.id,\n                'name': team.name,\n                'permission': team.permission\n            })\n    except github.UnknownObjectException:\n        raise CommandExecutionError(\n            'Unable to retrieve teams for repository \\'{0}\\' under the \\'{1}\\' '\n            'organization.'.format(repo_name, org_name)\n        )\n    return ret"
    },
    {
        "original": "def save_version_info(self, filename):\n        \"\"\"\n        Materialize the value of date into the\n        build tag. Install build keys in a deterministic order\n        to avoid arbitrary reordering on subsequent builds.\n        \"\"\"\n        egg_info = collections.OrderedDict()\n        # follow the order these keys would have been added\n        # when PYTHONHASHSEED=0\n        egg_info['tag_build'] = self.tags()\n        egg_info['tag_date'] = 0\n        edit_config(filename, dict(egg_info=egg_info))",
        "rewrite": "def save_version_info(self, filename):\n    egg_info = collections.OrderedDict()\n    egg_info['tag_build'] = self.tags()\n    egg_info['tag_date'] = 0\n    edit_config(filename, dict(egg_info=egg_info))"
    },
    {
        "original": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered",
        "rewrite": "def _filter_cache(self, dmap, kdims):\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered"
    },
    {
        "original": "def examples(path='holoviews-examples', verbose=False, force=False, root=__file__):\n    \"\"\"\n    Copies the notebooks to the supplied path.\n    \"\"\"\n    filepath = os.path.abspath(os.path.dirname(root))\n    example_dir = os.path.join(filepath, './examples')\n    if not os.path.exists(example_dir):\n        example_dir = os.path.join(filepath, '../examples')\n    if os.path.exists(path):\n        if not force:\n            print('%s directory already exists, either delete it or set the force flag' % path)\n            return\n        shutil.rmtree(path)\n    ignore = shutil.ignore_patterns('.ipynb_checkpoints', '*.pyc', '*~')\n    tree_root = os.path.abspath(example_dir)\n    if os.path.isdir(tree_root):\n        shutil.copytree(tree_root, path, ignore=ignore, symlinks=True)\n    else:\n        print('Cannot find %s' % tree_root)",
        "rewrite": "def examples(path='holoviews-examples', verbose=False, force=False, root=__file__):\n    import shutil\n    import os\n    \n    filepath = os.path.abspath(os.path.dirname(root))\n    example_dir = os.path.join(filepath, './examples')\n    \n    if not os.path.exists(example_dir):\n        example_dir = os.path.join(filepath, '../examples')\n    \n    if os.path.exists(path):\n        if not force:\n            print(f'{path} directory already exists, either delete it or set the force flag')\n            return\n        shutil.rmtree(path)\n    \n    ignore = shutil.ignore_patterns('.ipynb_checkpoints', '*.pyc', '*~')\n    tree_root = os.path.abspath(example_dir)\n    \n    if os.path.isdir(tree_root):\n        shutil.copytree(tree_root, path, ignore=ignore, symlinks=True)\n    else:\n        print(f'Cannot find {tree_root}')"
    },
    {
        "original": "def yaml_dquote(text):\n    \"\"\"\n    Make text into a double-quoted YAML string with correct escaping\n    for special characters.  Includes the opening and closing double\n    quote characters.\n    \"\"\"\n    with io.StringIO() as ostream:\n        yemitter = yaml.emitter.Emitter(ostream, width=six.MAXSIZE)\n        yemitter.write_double_quoted(six.text_type(text))\n        return ostream.getvalue()",
        "rewrite": "import io\nimport yaml.emitter\nimport six\n\ndef yaml_dquote(text):\n    with io.StringIO() as ostream:\n        yemitter = yaml.emitter.Emitter(ostream, width=six.MAXSIZE)\n        yemitter.write_double_quoted(six.text_type(text))\n        return ostream.getvalue()"
    },
    {
        "original": "def load_nb(cls, inline=True):\n        \"\"\"\n        Loads the plotly notebook resources.\n        \"\"\"\n        from IPython.display import publish_display_data\n        cls._loaded = True\n        init_notebook_mode(connected=not inline)\n        publish_display_data(data={MIME_TYPES['jlab-hv-load']:\n                                   get_plotlyjs()})",
        "rewrite": "def load_nb(cls, inline=True):\n    \"\"\"\n    Loads the plotly notebook resources.\n    \"\"\"\n    from IPython.display import publish_display_data\n    cls._loaded = True\n    init_notebook_mode(connected=not inline)\n    publish_display_data(data={MIME_TYPES['jlab-hv-load']: get_plotlyjs()})"
    },
    {
        "original": "def _is_txn_to_replay(self, txn_id, possible_successor, already_seen):\n        \"\"\"Decide if possible_successor should be replayed.\n\n        Args:\n            txn_id (str): Id of txn in failed batch.\n            possible_successor (str): Id of txn to possibly replay.\n            already_seen (list): A list of possible_successors that have\n                been replayed.\n\n        Returns:\n            (bool): If the possible_successor should be replayed.\n        \"\"\"\n\n        is_successor = self._is_predecessor_of_possible_successor(\n            txn_id,\n            possible_successor)\n        in_different_batch = not self._is_in_same_batch(txn_id,\n                                                        possible_successor)\n        has_not_been_seen = possible_successor not in already_seen\n\n        return is_successor and in_different_batch and has_not_been_seen",
        "rewrite": "def _is_txn_to_replay(self, txn_id, possible_successor, already_seen):\n    \"\"\"Decide if possible_successor should be replayed.\n\n    Args:\n        txn_id (str): Id of txn in failed batch.\n        possible_successor (str): Id of txn to possibly replay.\n        already_seen (list): A list of possible_successors that have\n            been replayed.\n\n    Returns:\n        (bool): If the possible_successor should be replayed.\n    \"\"\"\n\n    is_successor = self._is_predecessor_of_possible_successor(txn_id, possible_successor)\n    in_different_batch = not self._is_in_same_batch(txn_id, possible_successor)\n    has_not_been_seen = possible_successor not in already_seen\n\n    return is_successor and in_different_batch and has_not_been_seen"
    },
    {
        "original": "def from_native(cls, regex):\n        \"\"\"Convert a Python regular expression into a ``Regex`` instance.\n\n        Note that in Python 3, a regular expression compiled from a\n        :class:`str` has the ``re.UNICODE`` flag set. If it is undesirable\n        to store this flag in a BSON regular expression, unset it first::\n\n          >>> pattern = re.compile('.*')\n          >>> regex = Regex.from_native(pattern)\n          >>> regex.flags ^= re.UNICODE\n          >>> db.collection.insert({'pattern': regex})\n\n        :Parameters:\n          - `regex`: A regular expression object from ``re.compile()``.\n\n        .. warning::\n           Python regular expressions use a different syntax and different\n           set of flags than MongoDB, which uses `PCRE`_. A regular\n           expression retrieved from the server may not compile in\n           Python, or may match a different set of strings in Python than\n           when used in a MongoDB query.\n\n        .. _PCRE: http://www.pcre.org/\n        \"\"\"\n        if not isinstance(regex, RE_TYPE):\n            raise TypeError(\n                \"regex must be a compiled regular expression, not %s\"\n                % type(regex))\n\n        return Regex(regex.pattern, regex.flags)",
        "rewrite": "def from_native(cls, regex):\n    if not isinstance(regex, RE_TYPE):\n        raise TypeError(\"regex must be a compiled regular expression, not %s\" % type(regex))\n\n    return Regex(regex.pattern, regex.flags)"
    },
    {
        "original": "def which(program, paths=None):\n    \"\"\" takes a program name or full path, plus an optional collection of search\n    paths, and returns the full path of the requested executable.  if paths is\n    specified, it is the entire list of search paths, and the PATH env is not\n    used at all.  otherwise, PATH env is used to look for the program \"\"\"\n\n    def is_exe(fpath):\n        return (os.path.exists(fpath) and\n                os.access(fpath, os.X_OK) and\n                os.path.isfile(os.path.realpath(fpath)))\n\n    found_path = None\n    fpath, fname = os.path.split(program)\n\n    # if there's a path component, then we've specified a path to the program,\n    # and we should just test if that program is executable.  if it is, return\n    if fpath:\n        program = os.path.abspath(os.path.expanduser(program))\n        if is_exe(program):\n            found_path = program\n\n    # otherwise, we've just passed in the program name, and we need to search\n    # the paths to find where it actually lives\n    else:\n        paths_to_search = []\n\n        if isinstance(paths, (tuple, list)):\n            paths_to_search.extend(paths)\n        else:\n            env_paths = os.environ.get(\"PATH\", \"\").split(os.pathsep)\n            paths_to_search.extend(env_paths)\n\n        for path in paths_to_search:\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                found_path = exe_file\n                break\n\n    return found_path",
        "rewrite": "import os\n\ndef which(program, paths=None):\n    def is_exe(fpath):\n        return (os.path.exists(fpath) and\n                os.access(fpath, os.X_OK) and\n                os.path.isfile(os.path.realpath(fpath)))\n\n    found_path = None\n    fpath, fname = os.path.split(program)\n\n    if fpath:\n        program = os.path.abspath(os.path.expanduser(program))\n        if is_exe(program):\n            found_path = program\n    else:\n        paths_to_search = []\n\n        if isinstance(paths, (tuple, list)):\n            paths_to_search.extend(paths)\n        else:\n            env_paths = os.environ.get(\"PATH\", \"\").split(os.pathsep)\n            paths_to_search.extend(env_paths)\n\n        for path in paths_to_search:\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                found_path = exe_file\n                break\n\n    return found_path"
    },
    {
        "original": "def move_spines(ax, sides, dists):\n    \"\"\"\n    Move the entire spine relative to the figure.\n\n    Parameters:\n      ax: axes to operate on\n      sides: list of sides to move. Sides: top, left, bottom, right\n      dists: list of float distances to move. Should match sides in length.\n\n    Example:\n    move_spines(ax, sides=['left', 'bottom'], dists=[-0.02, 0.1])\n    \"\"\"\n    for side, dist in zip(sides, dists):\n        ax.spines[side].set_position((\"axes\", dist))\n    return ax",
        "rewrite": "def move_spines(ax, sides, dists):\n    for side, dist in zip(sides, dists):\n        ax.spines[side].set_position((\"axes\", dist))\n    return ax"
    },
    {
        "original": "def process_updated_files(self, paths: List[str]) -> List[str]:\n        \"\"\"\n            Return the paths in the analysis directory (symbolic links)\n            corresponding to the given paths.\n            Result also includes any files which are within a tracked directory.\n\n            This method will remove/add symbolic links for deleted/new files.\n        \"\"\"\n        tracked_files = []\n\n        deleted_paths = [path for path in paths if not os.path.isfile(path)]\n        # TODO(T40580762) use buck targets to properly identify what new files belong\n        # to the project rather than checking if they are within the current directory\n        new_paths = [\n            path\n            for path in paths\n            if path not in self._symbolic_links\n            and os.path.isfile(path)\n            and is_parent(os.getcwd(), path)\n        ]\n        updated_paths = [\n            path\n            for path in paths\n            if path not in deleted_paths and path not in new_paths\n        ]\n\n        if deleted_paths:\n            LOG.info(\"Detected deleted paths: `%s`.\", \"`,`\".join(deleted_paths))\n        for path in deleted_paths:\n            link = self._symbolic_links.pop(path, None)\n            if link:\n                try:\n                    _delete_symbolic_link(link)\n                    tracked_files.append(link)\n                except OSError:\n                    LOG.warning(\"Failed to delete link at `%s`.\", link)\n\n        if new_paths:\n            LOG.info(\"Detected new paths: %s.\", \",\".join(new_paths))\n            try:\n                for path, relative_link in buck.resolve_relative_paths(\n                    new_paths\n                ).items():\n                    link = os.path.join(self.get_root(), relative_link)\n                    try:\n                        add_symbolic_link(link, path)\n                        self._symbolic_links[path] = link\n                        tracked_files.append(link)\n                    except OSError:\n                        LOG.warning(\"Failed to add link at %s.\", link)\n            except buck.BuckException as error:\n                LOG.error(\"Exception occurred when querying buck: %s\", error)\n                LOG.error(\"No new paths will be added to the analysis directory.\")\n\n        for path in updated_paths:\n            if path in self._symbolic_links:\n                tracked_files.append(self._symbolic_links[path])\n            elif self._is_tracked(path):\n                tracked_files.append(path)\n\n        return tracked_files",
        "rewrite": "def process_updated_files(self, paths: List[str]) -> List[str]:\n    tracked_files = []\n\n    deleted_paths = [path for path in paths if not os.path.isfile(path)]\n    new_paths = [\n        path\n        for path in paths\n        if path not in self._symbolic_links\n        and os.path.isfile(path)\n        and is_parent(os.getcwd(), path)\n    ]\n    updated_paths = [\n        path\n        for path in paths\n        if path not in deleted_paths and path not in new_paths\n    ]\n\n    if deleted_paths:\n        LOG.info(\"Detected deleted paths: `%s`.\", \"`,`\".join(deleted_paths))\n    for path in deleted_paths:\n        link = self._symbolic_links.pop(path, None)\n        if link:\n            try:\n                _delete_symbolic_link(link)\n                tracked_files.append(link)\n            except OSError:\n                LOG.warning(\"Failed to delete link at `%s`.\", link)\n\n    if new_paths:\n        LOG.info(\"Detected new paths: %s.\", \",\".join(new_paths))\n        try:\n            for path, relative_link in buck.resolve_relative_paths(new_paths).items():\n                link = os.path.join(self.get_root(), relative_link)\n                try:\n                    add_symbolic_link(link, path)\n                    self._symbolic_links[path] = link\n                    tracked_files.append(link)\n                except OSError:\n                    LOG.warning(\"Failed to add link at %s.\", link)\n        except buck.BuckException as error:\n            LOG.error(\"Exception occurred when querying buck: %s\", error)\n            LOG.error(\"No new paths will be added to the analysis directory.\")\n\n    for path in updated_paths:\n        if path in self._symbolic_links:\n            tracked_files.append(self._symbolic_links[path])\n        elif self._is_tracked(path):\n            tracked_files.append(path)\n\n    return tracked_files"
    },
    {
        "original": "def parsers(self):\n        \"\"\"Metadata item name to parser function mapping.\"\"\"\n        parse_list = self._parse_list\n        parse_list_semicolon = partial(self._parse_list, separator=';')\n        parse_bool = self._parse_bool\n        parse_dict = self._parse_dict\n\n        return {\n            'zip_safe': parse_bool,\n            'use_2to3': parse_bool,\n            'include_package_data': parse_bool,\n            'package_dir': parse_dict,\n            'use_2to3_fixers': parse_list,\n            'use_2to3_exclude_fixers': parse_list,\n            'convert_2to3_doctests': parse_list,\n            'scripts': parse_list,\n            'eager_resources': parse_list,\n            'dependency_links': parse_list,\n            'namespace_packages': parse_list,\n            'install_requires': parse_list_semicolon,\n            'setup_requires': parse_list_semicolon,\n            'tests_require': parse_list_semicolon,\n            'packages': self._parse_packages,\n            'entry_points': self._parse_file,\n            'py_modules': parse_list,\n        }",
        "rewrite": "def parsers(self):\n    parse_list = self._parse_list\n    parse_list_semicolon = partial(self._parse_list, separator=';')\n    parse_bool = self._parse_bool\n    parse_dict = self._parse_dict\n\n    return {\n        'zip_safe': parse_bool,\n        'use_2to3': parse_bool,\n        'include_package_data': parse_bool,\n        'package_dir': parse_dict,\n        'use_2to3_fixers': parse_list,\n        'use_2to3_exclude_fixers': parse_list,\n        'convert_2to3_doctests': parse_list,\n        'scripts': parse_list,\n        'eager_resources': parse_list,\n        'dependency_links': parse_list,\n        'namespace_packages': parse_list,\n        'install_requires': parse_list_semicolon,\n        'setup_requires': parse_list_semicolon,\n        'tests_require': parse_list_semicolon,\n        'packages': self._parse_packages,\n        'entry_points': self._parse_file,\n        'py_modules': parse_list,\n    }"
    },
    {
        "original": "def uhash(self, val):\n        \"\"\"Calculate hash from unicode value and return hex value as unicode\"\"\"\n\n        if not isinstance(val, string_types):\n            raise _TypeError(\"val\", \"str\", val)\n\n        return codecs.encode(self.hash(val.encode(\"utf-8\")), \"hex_codec\").decode(\"utf-8\")",
        "rewrite": "def uhash(self, val):\n    if not isinstance(val, str):\n        raise TypeError(\"val must be a string\")\n    \n    return codecs.encode(self.hash(val.encode(\"utf-8\")), \"hex_codec\").decode(\"utf-8\")"
    },
    {
        "original": "def _get_running_app_ids(self, rm_address, auth, ssl_verify):\n        \"\"\"\n        Return a dictionary of {app_id: (app_name, tracking_url)} for the running MapReduce applications\n        \"\"\"\n        metrics_json = self._rest_request_to_json(\n            rm_address,\n            auth,\n            ssl_verify,\n            self.YARN_APPS_PATH,\n            self.YARN_SERVICE_CHECK,\n            states=self.YARN_APPLICATION_STATES,\n            applicationTypes=self.YARN_APPLICATION_TYPES,\n        )\n\n        running_apps = {}\n\n        if metrics_json.get('apps'):\n            if metrics_json['apps'].get('app') is not None:\n\n                for app_json in metrics_json['apps']['app']:\n                    app_id = app_json.get('id')\n                    tracking_url = app_json.get('trackingUrl')\n                    app_name = app_json.get('name')\n\n                    if app_id and tracking_url and app_name:\n                        running_apps[app_id] = (app_name, tracking_url)\n\n        return running_apps",
        "rewrite": "def _get_running_app_ids(self, rm_address, auth, ssl_verify):\n    metrics_json = self._rest_request_to_json(rm_address, auth, ssl_verify,\n                                              self.YARN_APPS_PATH, self.YARN_SERVICE_CHECK,\n                                              states=self.YARN_APPLICATION_STATES,\n                                              applicationTypes=self.YARN_APPLICATION_TYPES)\n\n    running_apps = {}\n\n    if 'apps' in metrics_json:\n        if 'app' in metrics_json['apps']:\n            for app_json in metrics_json['apps']['app']:\n                app_id = app_json.get('id')\n                tracking_url = app_json.get('trackingUrl')\n                app_name = app_json.get('name')\n\n                if app_id and tracking_url and app_name:\n                    running_apps[app_id] = (app_name, tracking_url)\n\n    return running_apps"
    },
    {
        "original": "def get_languages(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/languages <http://developer.github.com/v3/repos>`_\n        :rtype: dict of string to integer\n        \"\"\"\n        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url + \"/languages\"\n        )\n        return data",
        "rewrite": "def get_languages(self):\n    headers, data = self._requester.requestJsonAndCheck(\n        \"GET\",\n        self.url + \"/languages\"\n    )\n    return data"
    },
    {
        "original": "def clean_lines(string_list, remove_empty_lines=True):\n    \"\"\"\n    Strips whitespace, carriage returns and empty lines from a list of strings.\n\n    Args:\n        string_list: List of strings\n        remove_empty_lines: Set to True to skip lines which are empty after\n            stripping.\n\n    Returns:\n        List of clean strings with no whitespaces.\n    \"\"\"\n\n    for s in string_list:\n        clean_s = s\n        if '#' in s:\n            ind = s.index('#')\n            clean_s = s[:ind]\n        clean_s = clean_s.strip()\n        if (not remove_empty_lines) or clean_s != '':\n            yield clean_s",
        "rewrite": "def clean_lines(string_list, remove_empty_lines=True):\n    for s in string_list:\n        clean_s = s\n        if '#' in s:\n            ind = s.index('#')\n            clean_s = s[:ind]\n        clean_s = clean_s.strip()\n        if (not remove_empty_lines) or clean_s != '':\n            yield clean_s"
    },
    {
        "original": "def execute_paged_query(self, verb, verb_arguments):\n        \"\"\"Executes query (ex. list) via a dedicated http object.\n\n        Args:\n            verb (str): Method to execute on the component (ex. get, list).\n            verb_arguments (dict): key-value pairs to be passed to _BuildRequest.\n\n        Yields:\n            dict: Service Response.\n\n        Raises:\n            PaginationNotSupportedError: When an API does not support paging.\n        \"\"\"\n        if not self.supports_pagination(verb=verb):\n            raise PaginationNotSupported('{} does not support pagination')\n\n        request = self._build_request(verb, verb_arguments)\n\n        number_of_pages_processed = 0\n        while request is not None:\n            response = self._execute(request)\n            number_of_pages_processed += 1\n            log.debug('Executing paged request #%s', number_of_pages_processed)\n            request = self._build_next_request(verb, request, response)\n            yield response",
        "rewrite": "def execute_paged_query(self, verb, verb_arguments):\n    if not self.supports_pagination(verb=verb):\n        raise PaginationNotSupportedError('{} does not support pagination')\n\n    request = self._build_request(verb, verb_arguments)\n\n    number_of_pages_processed = 0\n    while request is not None:\n        response = self._execute(request)\n        number_of_pages_processed += 1\n        log.debug('Executing paged request #%s', number_of_pages_processed)\n        request = self._build_next_request(verb, request, response)\n        yield response"
    },
    {
        "original": "def get_most_frequent_value(values: list):\n    \"\"\"\n    Return the most frequent value in list.\n    If there is no unique one, return the maximum of the most frequent values\n\n    :param values:\n    :return:\n    \"\"\"\n    if len(values) == 0:\n        return None\n\n    most_common = Counter(values).most_common()\n    result, max_count = most_common[0]\n    for value, count in most_common:\n        if count < max_count:\n            return result\n        else:\n            result = value\n\n    return result",
        "rewrite": "from collections import Counter\n\ndef get_most_frequent_value(values: list):\n    if len(values) == 0:\n        return None\n\n    most_common = Counter(values).most_common()\n    result, max_count = most_common[0]\n    for value, count in most_common:\n        if count < max_count:\n            return result\n        else:\n            result = value\n\n    return result"
    },
    {
        "original": "def create(self, data, **kwargs):\n        \"\"\"Creates a new object.\n\n        Args:\n            data (dict): Parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n\n        Returns:\n            RESTObject: A new instance of the managed object class build with\n                the data sent by the server\n        \"\"\"\n        path = self.path[:-1]  # drop the 's'\n        return CreateMixin.create(self, data, path=path, **kwargs)",
        "rewrite": "def create(self, data, **kwargs):\n    path = self.path[:-1]\n    return CreateMixin.create(self, data, path=path, **kwargs)"
    },
    {
        "original": "async def getUpdates(self,\n                         offset=None,\n                         limit=None,\n                         timeout=None,\n                         allowed_updates=None):\n        \"\"\" See: https://core.telegram.org/bots/api#getupdates \"\"\"\n        p = _strip(locals())\n        return await self._api_request('getUpdates', _rectify(p))",
        "rewrite": "async def get_updates(self, offset=None, limit=None, timeout=None, allowed_updates=None):\n    p = _strip(locals())\n    return await self._api_request('getUpdates', _rectify(p))"
    },
    {
        "original": "def is_available_extension(name,\n                           user=None,\n                           host=None,\n                           port=None,\n                           maintenance_db=None,\n                           password=None,\n                           runas=None):\n    \"\"\"\n    Test if a specific extension is available\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.is_available_extension\n\n    \"\"\"\n    exts = available_extensions(user=user,\n                                host=host,\n                                port=port,\n                                maintenance_db=maintenance_db,\n                                password=password,\n                                runas=runas)\n    if name.lower() in [\n        a.lower()\n        for a in exts\n    ]:\n        return True\n    return False",
        "rewrite": "def is_available_extension(name, user=None, host=None, port=None, maintenance_db=None, password=None, runas=None):\n    exts = available_extensions(user=user, host=host, port=port, maintenance_db=maintenance_db, password=password, runas=runas)\n    if name.lower() in [a.lower() for a in exts]:\n        return True\n    return False"
    },
    {
        "original": "def topics(self):\n        \"\"\"\n        Get the set of topics that can be extracted from this report.\n        \"\"\"\n        return set(node.tag for node in self.root.iter() if node.attrib)",
        "rewrite": "def topics(self):\n    return set(node.tag for node in self.root.iter() if node.attrib)"
    },
    {
        "original": "def dpsi2_dtheta(self, dL_dpsi2, Z, mu, S, target):\n        \"\"\"Shape N,num_inducing,num_inducing,Ntheta\"\"\"\n        self._psi_computations(Z, mu, S)\n        d_var = 2.*self._psi2 / self.variance\n        # d_length = 2.*self._psi2[:, :, :, None] * (self._psi2_Zdist_sq * self._psi2_denom + self._psi2_mudist_sq + S[:, None, None, :] / self.lengthscale2) / (self.lengthscale * self._psi2_denom)\n        d_length = -2.*self._psi2[:, :, :, None] * (self._psi2_Zdist_sq * self._psi2_denom + self._psi2_mudist_sq + S[:, None, None, :] * self.inv_lengthscale2) / (self.inv_lengthscale * self._psi2_denom)\n        target[0] += np.sum(dL_dpsi2 * d_var)\n        dpsi2_dlength = d_length * dL_dpsi2[:, :, :, None]\n        if not self.ARD:\n            target[1] += dpsi2_dlength.sum() # *(-self.lengthscale2)\n        else:\n            target[1:] += dpsi2_dlength.sum(0).sum(0).sum(0)",
        "rewrite": "def dpsi2_dtheta(self, dL_dpsi2, Z, mu, S, target):\n    self._psi_computations(Z, mu, S)\n    d_var = 2. * self._psi2 / self.variance\n    d_length = -2. * self._psi2[:, :, :, None] * (self._psi2_Zdist_sq * self._psi2_denom + self._psi2_mudist_sq + S[:, None, None, :] * self.inv_lengthscale2) / (self.inv_lengthscale * self._psi2_denom)\n    target[0] += np.sum(dL_dpsi2 * d_var)\n    dpsi2_dlength = d_length * dL_dpsi2[:, :, :, None]\n    if not self.ARD:\n        target[1] += dpsi2_dlength.sum()\n    else:\n        target[1:] += dpsi2_dlength.sum(0).sum(0).sum(0)"
    },
    {
        "original": "def mu_so(species, motif, spin_state):\n        \"\"\"\n        Calculates the spin-only magnetic moment for a\n        given species. Only supports transition metals.\n\n        :param species: str or Species\n        :param motif: \"oct\" or \"tet\"\n        :param spin_state: \"high\" or \"low\"\n        :return: spin-only magnetic moment in Bohr magnetons\n        \"\"\"\n        try:\n            sp = get_el_sp(species)\n            n = sp.get_crystal_field_spin(coordination=motif, spin_config=spin_state)\n            # calculation spin-only magnetic moment for this number of unpaired spins\n            return np.sqrt(n * (n + 2))\n        except AttributeError:\n            return None",
        "rewrite": "def mu_so(species, motif, spin_state):\n    try:\n        sp = get_el_sp(species)\n        n = sp.get_crystal_field_spin(coordination=motif, spin_config=spin_state)\n        return np.sqrt(n * (n + 2))\n    except AttributeError:\n        return None"
    },
    {
        "original": "def _ParseMatchGrp(self, key, val):\n    \"\"\"Adds valid match group parameters to the configuration.\"\"\"\n    if key in self._match_keywords:\n      self._ParseEntry(key, val)",
        "rewrite": "def _ParseMatchGrp(self, key, val):\n    if key in self._match_keywords:\n        self._ParseEntry(key, val)"
    },
    {
        "original": "def _create_lacp(self, datapath, port, req):\n        \"\"\"create a LACP packet.\"\"\"\n        actor_system = datapath.ports[datapath.ofproto.OFPP_LOCAL].hw_addr\n        res = slow.lacp(\n            actor_system_priority=0xffff,\n            actor_system=actor_system,\n            actor_key=req.actor_key,\n            actor_port_priority=0xff,\n            actor_port=port,\n            actor_state_activity=req.LACP_STATE_PASSIVE,\n            actor_state_timeout=req.actor_state_timeout,\n            actor_state_aggregation=req.actor_state_aggregation,\n            actor_state_synchronization=req.actor_state_synchronization,\n            actor_state_collecting=req.actor_state_collecting,\n            actor_state_distributing=req.actor_state_distributing,\n            actor_state_defaulted=req.LACP_STATE_OPERATIONAL_PARTNER,\n            actor_state_expired=req.LACP_STATE_NOT_EXPIRED,\n            partner_system_priority=req.actor_system_priority,\n            partner_system=req.actor_system,\n            partner_key=req.actor_key,\n            partner_port_priority=req.actor_port_priority,\n            partner_port=req.actor_port,\n            partner_state_activity=req.actor_state_activity,\n            partner_state_timeout=req.actor_state_timeout,\n            partner_state_aggregation=req.actor_state_aggregation,\n            partner_state_synchronization=req.actor_state_synchronization,\n            partner_state_collecting=req.actor_state_collecting,\n            partner_state_distributing=req.actor_state_distributing,\n            partner_state_defaulted=req.actor_state_defaulted,\n            partner_state_expired=req.actor_state_expired,\n            collector_max_delay=0)\n        self.logger.info(\"SW=%s PORT=%d LACP sent.\",\n                         dpid_to_str(datapath.id), port)\n        self.logger.debug(str(res))\n        return res",
        "rewrite": "def _create_lacp(self, datapath, port, req):\n    actor_system = datapath.ports[datapath.ofproto.OFPP_LOCAL].hw_addr\n    res = slow.lacp(\n        actor_system_priority=0xffff,\n        actor_system=actor_system,\n        actor_key=req.actor_key,\n        actor_port_priority=0xff,\n        actor_port=port,\n        actor_state_activity=req.LACP_STATE_PASSIVE,\n        actor_state_timeout=req.actor_state_timeout,\n        actor_state_aggregation=req.actor_state_aggregation,\n        actor_state_synchronization=req.actor_state_synchronization,\n        actor_state_collecting=req.actor_state_collecting,\n        actor_state_distributing=req.actor_state_distributing,\n        actor_state_defaulted=req.LACP_STATE_OPERATIONAL_PARTNER,\n        actor_state_expired=req.LACP_STATE_NOT_EXPIRED,\n        partner_system_priority=req.actor_system_priority,\n        partner_system=req.actor_system,\n        partner_key=req.actor_key,\n        partner_port_priority=req.actor_port_priority,\n        partner_port=req.actor_port,\n        partner_state_activity=req.actor_state_activity,\n        partner_state_timeout=req.actor_state_timeout,\n        partner_state_aggregation=req.actor_state_aggregation,\n        partner_state_synchronization=req.actor_state_synchronization,\n        partner_state_collecting=req.actor_state_collecting,\n        partner_state_distributing=req.actor_state_distributing,\n        partner_state_defaulted=req.actor_state_defaulted,\n        partner_state_expired=req.actor_state_expired,\n        collector_max_delay=0)\n    self.logger.info(\"SW=%s PORT=%d LACP sent.\", dpid_to_str(datapath.id), port)\n    self.logger.debug(str(res))\n    return res"
    },
    {
        "original": "def uint32_gt(a: int, b: int) -> bool:\n    \"\"\"\n    Return a > b.\n    \"\"\"\n    half_mod = 0x80000000\n    return (((a < b) and ((b - a) > half_mod)) or\n            ((a > b) and ((a - b) < half_mod)))",
        "rewrite": "def uint32_gt(a: int, b: int) -> bool:\n    half_mod = 0x80000000\n    return (((a < b) and ((b - a) > half_mod)) or ((a > b) and ((a - b) < half_mod)))"
    },
    {
        "original": "def add_find_links(self, urls):\n        \"\"\"Add `urls` to the list that will be prescanned for searches\"\"\"\n        for url in urls:\n            if (\n                self.to_scan is None  # if we have already \"gone online\"\n                or not URL_SCHEME(url)  # or it's a local file/directory\n                or url.startswith('file:')\n                or list(distros_for_url(url))  # or a direct package link\n            ):\n                # then go ahead and process it now\n                self.scan_url(url)\n            else:\n                # otherwise, defer retrieval till later\n                self.to_scan.append(url)",
        "rewrite": "def add_find_links(self, urls):\n    for url in urls:\n        if (\n            self.to_scan is None\n            or not URL_SCHEME(url)\n            or url.startswith('file:')\n            or list(distros_for_url(url))\n        ):\n        self.scan_url(url)\n    else:\n        self.to_scan.append(url)"
    },
    {
        "original": "def read_hierarchy(self, fid):\n        \"\"\"Read hierarchy information from acclaim skeleton file stream.\"\"\"\n\n        lin = self.read_line(fid)\n                    \n        while lin != 'end':\n            parts = lin.split()\n            if lin != 'begin':\n                ind = self.get_index_by_name(parts[0])\n                for i in range(1, len(parts)):\n                    self.vertices[ind].children.append(self.get_index_by_name(parts[i]))\n            lin = self.read_line(fid)\n        lin = self.read_line(fid)\n        return lin",
        "rewrite": "def read_hierarchy(self, fid):\n    lin = self.read_line(fid)\n\n    while lin != 'end':\n        parts = lin.split()\n        if lin != 'begin':\n            ind = self.get_index_by_name(parts[0])\n            for i in range(1, len(parts)):\n                self.vertices[ind].children.append(self.get_index_by_name(parts[i]))\n        lin = self.read_line(fid)\n    lin = self.read_line(fid)\n    return lin"
    },
    {
        "original": "def discard_plugin_preset(self):\n        \"\"\"\n        Discard the current active preset. Will release any active plugins that could have come from the old preset.\n        \"\"\"\n        if self.has_plugin_preset:\n            for name, plugin in list(self._active_plugins.items()):\n                if id(plugin) in self._provided_by_preset:\n                    self.release_plugin(name)\n            self._active_preset.deactivate(self)\n        self._active_preset = None",
        "rewrite": "def discard_plugin_preset(self):\n\tif self.has_plugin_preset:\n\t\tfor name, plugin in list(self._active_plugins.items()):\n\t\t\tif id(plugin) in self._provided_by_preset:\n\t\t\t\tself.release_plugin(name)\n\t\tself._active_preset.deactivate(self)\n\tself._active_preset = None"
    },
    {
        "original": "def ddb_filepath(self):\n        \"\"\"Returns (at runtime) the absolute path of the input DDB file.\"\"\"\n        # This is not very elegant! A possible approach could to be path self.ddb_node.outdir!\n        if isinstance(self.ddb_node, FileNode): return self.ddb_node.filepath\n        path = self.ddb_node.outdir.has_abiext(\"DDB\")\n        return path if path else \"DDB_FILE_DOES_NOT_EXIST\"",
        "rewrite": "def ddb_filepath(self):\n        if isinstance(self.ddb_node, FileNode):\n            return self.ddb_node.filepath\n        path = self.ddb_node.outdir.has_abiext(\"DDB\")\n        return path if path else \"DDB_FILE_DOES_NOT_EXIST\""
    },
    {
        "original": "def RunOnce(self, token=None, force=False, names=None):\n    \"\"\"Tries to lock and run cron jobs.\n\n    Args:\n      token: security token\n      force: If True, force a run\n      names: List of job names to run.  If unset, run them all\n    \"\"\"\n    names = names or self.ListJobs(token=token)\n    urns = [self.CRON_JOBS_PATH.Add(name) for name in names]\n\n    for cron_job_urn in urns:\n      try:\n        with aff4.FACTORY.OpenWithLock(\n            cron_job_urn, blocking=False, token=token,\n            lease_time=600) as cron_job:\n          try:\n            logging.info(\"Running cron job: %s\", cron_job.urn)\n            cron_job.Run(force=force)\n          except Exception as e:  # pylint: disable=broad-except\n            logging.exception(\"Error processing cron job %s: %s\", cron_job.urn,\n                              e)\n            stats_collector_instance.Get().IncrementCounter(\n                \"cron_internal_error\")\n\n      except aff4.LockError:\n        pass",
        "rewrite": "def RunOnce(self, token=None, force=False, names=None):\n    names = names or self.ListJobs(token=token)\n    urns = [self.CRON_JOBS_PATH.Add(name) for name in names]\n\n    for cron_job_urn in urns:\n        try:\n            with aff4.FACTORY.OpenWithLock(\n                cron_job_urn, blocking=False, token=token, lease_time=600) as cron_job:\n                try:\n                    logging.info(\"Running cron job: %s\", cron_job.urn)\n                    cron_job.Run(force=force)\n                except Exception as e:\n                    logging.exception(\"Error processing cron job %s: %s\", cron_job.urn, e)\n                    stats_collector_instance.Get().IncrementCounter(\"cron_internal_error\")\n        except aff4.LockError:\n            pass"
    },
    {
        "original": "def _to_patches(self, X):\r\n        \"\"\"\r\n        Reshapes input to patches of the size of classifier's receptive field.\r\n\r\n        For example:\r\n\r\n        input X shape: [n_samples, n_pixels_y, n_pixels_x, n_bands]\r\n\r\n        output: [n_samples * n_pixels_y/receptive_field_y * n_pixels_x/receptive_field_x,\r\n                 receptive_field_y, receptive_field_x, n_bands]\r\n        \"\"\"\r\n\r\n        window = self.patch_size\r\n        asteps = self.patch_size\r\n\r\n        if len(X.shape) == 4:\r\n            window += (0,)\r\n            asteps += (1,)\r\n\r\n        image_view = rolling_window(X, window, asteps)\r\n\r\n        new_shape = image_view.shape\r\n\r\n        return image_view, new_shape",
        "rewrite": "def _to_patches(self, X):\r\n    window = self.patch_size\r\n    asteps = self.patch_size\r\n\r\n    if len(X.shape) == 4:\r\n        window += (0,)\r\n        asteps += (1,)\r\n\r\n    image_view = rolling_window(X, window, asteps)\r\n\r\n    new_shape = image_view.shape\r\n\r\n    return image_view, new_shape"
    },
    {
        "original": "def clean_all_trash_pages_from_all_spaces(confluence):\n    \"\"\"\n    Main function for retrieve space keys and provide space for cleaner\n    :param confluence:\n    :return:\n    \"\"\"\n    limit = 50\n    flag = True\n    i = 0\n    while flag:\n        space_lists = confluence.get_all_spaces(start=i * limit, limit=limit)\n        if space_lists and len(space_lists) != 0:\n            i += 1\n            for space_list in space_lists:\n                print(\"Start review the space with key = \" + space_list['key'])\n                clean_pages_from_space(confluence=confluence, space_key=space_list['key'])\n        else:\n            flag = False\n    return 0",
        "rewrite": "def clean_all_trash_pages_from_all_spaces(confluence):\n    limit = 50\n    flag = True\n    i = 0\n    while flag:\n        space_lists = confluence.get_all_spaces(start=i * limit, limit=limit)\n        if space_lists and len(space_lists) != 0:\n            i += 1\n            for space_list in space_lists:\n                print(\"Start review the space with key = \" + space_list['key'])\n                clean_pages_from_space(confluence=confluence, space_key=space_list['key'])\n        else:\n            flag = False\n    return 0"
    },
    {
        "original": "def _set_namespace(self, namespaces):\n        \"\"\"Set the name space for use when calling eval. This needs to contain all the relvant functions for mapping from symbolic python to the numerical python. It also contains variables, cached portions etc.\"\"\"\n        self.namespace = {}\n        for m in namespaces[::-1]:\n            buf = _get_namespace(m)\n            self.namespace.update(buf)\n        self.namespace.update(self.__dict__)",
        "rewrite": "def _set_namespace(self, namespaces):\n    self.namespace = {}\n    for m in namespaces[::-1]:\n        buf = _get_namespace(m)\n        self.namespace.update(buf)\n    self.namespace.update(self.__dict__)"
    },
    {
        "original": "def get_authorize_url(self, state, scope='identity', refreshable=False):\n        \"\"\"Return the URL to send the user to for OAuth2 authorization.\n\n        :param state: a unique string of your choice that represents this\n            individual client\n        :param scope: the reddit scope to ask permissions for. Multiple scopes\n            can be enabled by passing in a container of strings.\n        :param refreshable: when True, a permanent \"refreshable\" token is\n            issued\n\n        \"\"\"\n        params = {'client_id': self.client_id, 'response_type': 'code',\n                  'redirect_uri': self.redirect_uri, 'state': state,\n                  'scope': _to_reddit_list(scope)}\n        params['duration'] = 'permanent' if refreshable else 'temporary'\n        request = Request('GET', self.config['authorize'], params=params)\n        return request.prepare().url",
        "rewrite": "def get_authorize_url(self, state, scope='identity', refreshable=False):\n    params = {'client_id': self.client_id, 'response_type': 'code',\n              'redirect_uri': self.redirect_uri, 'state': state,\n              'scope': _to_reddit_list(scope)}\n    params['duration'] = 'permanent' if refreshable else 'temporary'\n    request = Request('GET', self.config['authorize'], params=params)\n    return request.prepare().url"
    },
    {
        "original": "def preconstrain(self, value, variable):\n        \"\"\"\n        Add a preconstraint that ``variable == value`` to the state.\n\n        :param value:       The concrete value. Can be a bitvector or a bytestring or an integer.\n        :param variable:    The BVS to preconstrain.\n        \"\"\"\n        if not isinstance(value, claripy.ast.Base):\n            value = self.state.solver.BVV(value, len(variable))\n        elif value.op != 'BVV':\n            raise ValueError(\"Passed a value to preconstrain that was not a BVV or a string\")\n\n        if variable.op not in claripy.operations.leaf_operations:\n            l.warning(\"The variable %s to preconstrain is not a leaf AST. This may cause replacement failures in the \"\n                      \"claripy replacement backend.\", variable)\n            l.warning(\"Please use a leaf AST as the preconstraining variable instead.\")\n\n        constraint = variable == value\n        l.debug(\"Preconstraint: %s\", constraint)\n\n        # add the constraint for reconstraining later\n        self.variable_map[next(iter(variable.variables))] = constraint\n        self.preconstraints.append(constraint)\n        if o.REPLACEMENT_SOLVER in self.state.options:\n            self.state.solver._solver.add_replacement(variable, value, invalidate_cache=False)\n        else:\n            self.state.add_constraints(*self.preconstraints)\n        if not self.state.satisfiable():\n            l.warning(\"State went unsat while adding preconstraints\")",
        "rewrite": "def preconstrain(self, value, variable):\n    if not isinstance(value, claripy.ast.Base):\n        value = self.state.solver.BVV(value, len(variable))\n    elif value.op != 'BVV':\n        raise ValueError(\"Passed a value to preconstrain that was not a BVV or a string\")\n\n    if variable.op not in claripy.operations.leaf_operations:\n        l.warning(\"The variable %s to preconstrain is not a leaf AST. This may cause replacement failures in the \"\n                  \"claripy replacement backend.\", variable)\n        l.warning(\"Please use a leaf AST as the preconstraining variable instead.\")\n\n    constraint = claripy.BV(operation=claripy.OPERATION_EQ, arguments=[variable, value])\n    l.debug(\"Preconstraint: %s\", constraint)\n    \n    self.variable_map[next(iter(variable.variables))] = constraint\n    self.preconstraints.append(constraint)\n    \n    if o.REPLACEMENT_SOLVER in self.state.options:\n        self.state.solver._solver.add_replacement(variable, value, invalidate_cache=False)\n    else:\n        self.state.add_constraints(*self.preconstraints)\n        \n    if not self.state.satisfiable():\n        l.warning(\"State went unsat while adding preconstraints\")"
    },
    {
        "original": "def dump_age(age=None):\n    \"\"\"Formats the duration as a base-10 integer.\n\n    :param age: should be an integer number of seconds,\n                a :class:`datetime.timedelta` object, or,\n                if the age is unknown, `None` (default).\n    \"\"\"\n    if age is None:\n        return\n    if isinstance(age, timedelta):\n        # do the equivalent of Python 2.7's timedelta.total_seconds(),\n        # but disregarding fractional seconds\n        age = age.seconds + (age.days * 24 * 3600)\n\n    age = int(age)\n    if age < 0:\n        raise ValueError(\"age cannot be negative\")\n\n    return str(age)",
        "rewrite": "from datetime import timedelta\n\ndef dump_age(age=None):\n    if age is None:\n        return\n    if isinstance(age, timedelta):\n        age = age.seconds + (age.days * 24 * 3600)\n\n    age = int(age)\n    if age < 0:\n        raise ValueError(\"age cannot be negative\")\n\n    return str(age)"
    },
    {
        "original": "def delete_stream(stream_name, region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Delete the stream with name stream_name. This cannot be undone! All data will be lost!!\n\n    CLI example::\n\n        salt myminion boto_kinesis.delete_stream my_stream region=us-east-1\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    r = _execute_with_retries(conn,\n                              \"delete_stream\",\n                              StreamName=stream_name)\n    if 'error' not in r:\n        r['result'] = True\n    return r",
        "rewrite": "def delete_stream(stream_name, region=None, key=None, keyid=None, profile=None):\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    result = _execute_with_retries(conn, \"delete_stream\", StreamName=stream_name)\n    if 'error' not in result:\n        result['result'] = True\n    return result"
    },
    {
        "original": "def route_absent(name, route_table, resource_group, connection_auth=None):\n    \"\"\"\n    .. versionadded:: 2019.2.0\n\n    Ensure a route table does not exist in the resource group.\n\n    :param name:\n        Name of the route table.\n\n    :param route_table:\n        The name of the existing route table containing the route.\n\n    :param resource_group:\n        The resource group assigned to the route table.\n\n    :param connection_auth:\n        A dict with subscription and authentication parameters to be used in connecting to the\n        Azure Resource Manager API.\n    \"\"\"\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    route = __salt__['azurearm_network.route_get'](\n        name,\n        route_table,\n        resource_group,\n        azurearm_log_level='info',\n        **connection_auth\n    )\n\n    if 'error' in route:\n        ret['result'] = True\n        ret['comment'] = 'Route {0} was not found.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'Route {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': route,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_network.route_delete'](name, route_table, resource_group, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'Route {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': route,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete route {0}!'.format(name)\n    return ret",
        "rewrite": "def route_absent(name, route_table, resource_group, connection_auth=None):\n    ret = {\n        'name': name,\n        'result': False,\n        'comment': '',\n        'changes': {}\n    }\n\n    if not isinstance(connection_auth, dict):\n        ret['comment'] = 'Connection information must be specified via connection_auth dictionary!'\n        return ret\n\n    route = __salt__['azurearm_network.route_get'](\n        name,\n        route_table,\n        resource_group,\n        azurearm_log_level='info',\n        **connection_auth\n    )\n\n    if 'error' in route:\n        ret['result'] = True\n        ret['comment'] = 'Route {0} was not found.'.format(name)\n        return ret\n\n    elif __opts__['test']:\n        ret['comment'] = 'Route {0} would be deleted.'.format(name)\n        ret['result'] = None\n        ret['changes'] = {\n            'old': route,\n            'new': {},\n        }\n        return ret\n\n    deleted = __salt__['azurearm_network.route_delete'](name, route_table, resource_group, **connection_auth)\n\n    if deleted:\n        ret['result'] = True\n        ret['comment'] = 'Route {0} has been deleted.'.format(name)\n        ret['changes'] = {\n            'old': route,\n            'new': {}\n        }\n        return ret\n\n    ret['comment'] = 'Failed to delete route {0}!'.format(name)\n    return ret"
    },
    {
        "original": "def team_stats(game_id):\n    \"\"\"Return dictionary of team stats for game matching the game id.\"\"\"\n    # get data\n    data = mlbgame.stats.team_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, False)",
        "rewrite": "def team_stats(game_id):\n    data = mlbgame.stats.team_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, False)"
    },
    {
        "original": "def parse(self, tokenized):  # pylint: disable=invalid-name\n        \"\"\"Parses input, which is a list of tokens.\"\"\"\n        table, trees = _parse(tokenized, self.grammar)\n        # Check if the parse succeeded.\n        if all(r.lhs != self.start for r in table[(0, len(tokenized) - 1)]):\n            raise ParseError('Parsing failed.')\n        parse = trees[(0, len(tokenized) - 1)][self.start]\n        return self._to_tree(revert_cnf(parse))",
        "rewrite": "def parse_tokens(self, tokenized):  \n        table, trees = _parse_tokens(tokenized, self.grammar)\n        \n        if all(rule.lhs != self.start for rule in table[(0, len(tokenized) - 1)]):\n            raise ParseError('Parsing failed.')\n        \n        parse_tree = trees[(0, len(tokenized) - 1)][self.start]\n        return self._convert_to_tree(revert_cnf(parse_tree))"
    },
    {
        "original": "def sigprocmask(self, how, new_mask, sigsetsize, valid_ptr=True):\n        \"\"\"\n        Updates the signal mask.\n\n        :param how: the \"how\" argument of sigprocmask (see manpage)\n        :param new_mask: the mask modification to apply\n        :param sigsetsize: the size (in *bytes* of the sigmask set)\n        :param valid_ptr: is set if the new_mask was not NULL\n        \"\"\"\n        oldmask = self.sigmask(sigsetsize)\n        self._sigmask = self.state.solver.If(valid_ptr,\n            self.state.solver.If(how == self.SIG_BLOCK,\n                oldmask | new_mask,\n                self.state.solver.If(how == self.SIG_UNBLOCK,\n                    oldmask & (~new_mask),\n                    self.state.solver.If(how == self.SIG_SETMASK,\n                        new_mask,\n                        oldmask\n                     )\n                )\n            ),\n            oldmask\n        )",
        "rewrite": "def sigprocmask(self, how, new_mask, sigsetsize, valid_ptr=True):\n    oldmask = self.sigmask(sigsetsize)\n    self._sigmask = self.state.solver.If(valid_ptr,\n        self.state.solver.If(how == self.SIG_BLOCK,\n            oldmask | new_mask,\n            self.state.solver.If(how == self.SIG_UNBLOCK,\n                oldmask & (~new_mask),\n                self.state.solver.If(how == self.SIG_SETMASK,\n                    new_mask,\n                    oldmask\n                 )\n            )\n        ),\n        oldmask\n    )"
    },
    {
        "original": "def getAllAsDict(self):\n        \"\"\"Return all the stats (dict).\"\"\"\n        return {p: self._plugins[p].get_raw() for p in self._plugins}",
        "rewrite": "def getAllAsDict(self):\n    return {p: self._plugins[p].get_raw() for p in self._plugins}"
    },
    {
        "original": "def get_pem_entry(text, pem_type=None):\n    \"\"\"\n    Returns a properly formatted PEM string from the input text fixing\n    any whitespace or line-break issues\n\n    text:\n        Text containing the X509 PEM entry to be returned or path to\n        a file containing the text.\n\n    pem_type:\n        If specified, this function will only return a pem of a certain type,\n        for example 'CERTIFICATE' or 'CERTIFICATE REQUEST'.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' x509.get_pem_entry \"-----BEGIN CERTIFICATE REQUEST-----MIICyzCC Ar8CAQI...-----END CERTIFICATE REQUEST\"\n    \"\"\"\n    text = _text_or_file(text)\n    # Replace encoded newlines\n    text = text.replace('\\\\n', '\\n')\n\n    _match = None\n\n    if len(text.splitlines()) == 1 and text.startswith(\n            '-----') and text.endswith('-----'):\n        # mine.get returns the PEM on a single line, we fix this\n        pem_fixed = []\n        pem_temp = text\n        while pem_temp:\n            if pem_temp.startswith('-----'):\n                # Grab ----(.*)---- blocks\n                pem_fixed.append(pem_temp[:pem_temp.index('-----', 5) + 5])\n                pem_temp = pem_temp[pem_temp.index('-----', 5) + 5:]\n            else:\n                # grab base64 chunks\n                if pem_temp[:64].count('-') == 0:\n                    pem_fixed.append(pem_temp[:64])\n                    pem_temp = pem_temp[64:]\n                else:\n                    pem_fixed.append(pem_temp[:pem_temp.index('-')])\n                    pem_temp = pem_temp[pem_temp.index('-'):]\n        text = \"\\n\".join(pem_fixed)\n\n    _dregex = _make_regex('[0-9A-Z ]+')\n    errmsg = 'PEM text not valid:\\n{0}'.format(text)\n    if pem_type:\n        _dregex = _make_regex(pem_type)\n        errmsg = ('PEM does not contain a single entry of type {0}:\\n'\n                  '{1}'.format(pem_type, text))\n\n    for _match in _dregex.finditer(text):\n        if _match:\n            break\n    if not _match:\n        raise salt.exceptions.SaltInvocationError(errmsg)\n    _match_dict = _match.groupdict()\n    pem_header = _match_dict['pem_header']\n    proc_type = _match_dict['proc_type']\n    dek_info = _match_dict['dek_info']\n    pem_footer = _match_dict['pem_footer']\n    pem_body = _match_dict['pem_body']\n\n    # Remove all whitespace from body\n    pem_body = ''.join(pem_body.split())\n\n    # Generate correctly formatted pem\n    ret = pem_header + '\\n'\n    if proc_type:\n        ret += proc_type + '\\n'\n    if dek_info:\n        ret += dek_info + '\\n' + '\\n'\n    for i in range(0, len(pem_body), 64):\n        ret += pem_body[i:i + 64] + '\\n'\n    ret += pem_footer + '\\n'\n\n    return salt.utils.stringutils.to_bytes(ret, encoding='ascii')",
        "rewrite": "def get_pem_entry(text, pem_type=None):\n    text = _text_or_file(text)\n    text = text.replace('\\\\n', '\\n')\n    _match = None\n\n    if len(text.splitlines()) == 1 and text.startswith('-----') and text.endswith('-----'):\n        pem_fixed = []\n        pem_temp = text\n        while pem_temp:\n            if pem_temp.startswith('-----'):\n                pem_fixed.append(pem_temp[:pem_temp.index('-----', 5) + 5])\n                pem_temp = pem_temp[pem_temp.index('-----', 5) + 5:]\n            else:\n                if pem_temp[:64].count('-') == 0:\n                    pem_fixed.append(pem_temp[:64])\n                    pem_temp = pem_temp[64:]\n                else:\n                    pem_fixed.append(pem_temp[:pem_temp.index('-')])\n                    pem_temp = pem_temp[pem_temp.index('-'):]\n        text = \"\\n\".join(pem_fixed)\n\n    _dregex = _make_regex('[0-9A-Z ]+')\n    errmsg = 'PEM text not valid:\\n{0}'.format(text)\n    if pem_type:\n        _dregex = _make_regex(pem_type)\n        errmsg = ('PEM does not contain a single entry of type {0}:\\n {1}'.format(pem_type, text))\n\n    for _match in _dregex.finditer(text):\n        if _match:\n            break\n    if not _match:\n        raise salt.exceptions.SaltInvocationError(errmsg)\n    _match_dict = _match.groupdict()\n    pem_header = _match_dict['pem_header']\n    proc_type = _match_dict['proc_type']\n    dek_info = _match_dict['dek_info']\n    pem_footer = _match_dict['pem_footer']\n    pem_body = _match_dict['pem_body']\n    pem_body = ''.join(pem_body.split())\n\n    ret = pem_header + '\\n'\n    if proc_type:\n        ret += proc_type + '\\n'\n    if dek_info:\n        ret += dek_info + '\\n' + '\\n'\n    for i in range(0, len(pem_body), 64):\n        ret += pem_body[i:i + 64] + '\\n'\n    ret += pem_footer + '\\n'\n\n    return salt.utils.stringutils.to_bytes(ret, encoding='ascii')"
    },
    {
        "original": "def root_rhx_gis(self) -> Optional[str]:\n        \"\"\"rhx_gis string returned in the / query.\"\"\"\n        if self.is_logged_in:\n            # At the moment, rhx_gis seems to be required for anonymous requests only. By returning None when logged\n            # in, we can save the root_rhx_gis lookup query.\n            return None\n        if not self._root_rhx_gis:\n            self._root_rhx_gis = self.get_json('', {})['rhx_gis']\n        return self._root_rhx_gis",
        "rewrite": "def root_rhx_gis(self) -> Optional[str]:\n    if self.is_logged_in:\n        return None\n    if not self._root_rhx_gis:\n        self._root_rhx_gis = self.get_json('', {})['rhx_gis']\n    return self._root_rhx_gis"
    },
    {
        "original": "def set_timezone(tz=None, deploy=False):\n    \"\"\"\n    Set the timezone of the Palo Alto proxy minion. A commit will be required before this is processed.\n\n    CLI Example:\n\n    Args:\n        tz (str): The name of the timezone to set.\n\n        deploy (bool): If true then commit the full candidate configuration, if false only set pending change.\n\n    .. code-block:: bash\n\n        salt '*' panos.set_timezone UTC\n        salt '*' panos.set_timezone UTC deploy=True\n\n    \"\"\"\n\n    if not tz:\n        raise CommandExecutionError(\"Timezone name option must not be none.\")\n\n    ret = {}\n\n    query = {'type': 'config',\n             'action': 'set',\n             'xpath': '/config/devices/entry[@name=\\'localhost.localdomain\\']/deviceconfig/system/timezone',\n             'element': '<timezone>{0}</timezone>'.format(tz)}\n\n    ret.update(__proxy__['panos.call'](query))\n\n    if deploy is True:\n        ret.update(commit())\n\n    return ret",
        "rewrite": "def set_timezone(tz=None, deploy=False):\n\n    if not tz:\n        raise CommandExecutionError(\"Timezone name option must not be none.\")\n\n    ret = {}\n\n    query = {'type': 'config',\n             'action': 'set',\n             'xpath': '/config/devices/entry[@name=\\'localhost.localdomain\\']/deviceconfig/system/timezone',\n             'element': '<timezone>{0}</timezone>'.format(tz)}\n\n    ret.update(__proxy__['panos.call'](query))\n\n    if deploy is True:\n        ret.update(commit())\n\n    return ret"
    },
    {
        "original": "def cache():\n    \"\"\"Returns a 304 if an If-Modified-Since header or If-None-Match is present. Returns the same as a GET otherwise.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: header\n        name: If-Modified-Since\n      - in: header\n        name: If-None-Match\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Cached response\n      304:\n        description: Modified\n\n    \"\"\"\n    is_conditional = request.headers.get(\"If-Modified-Since\") or request.headers.get(\n        \"If-None-Match\"\n    )\n\n    if is_conditional is None:\n        response = view_get()\n        response.headers[\"Last-Modified\"] = http_date()\n        response.headers[\"ETag\"] = uuid.uuid4().hex\n        return response\n    else:\n        return status_code(304)",
        "rewrite": "def cache():\n    is_conditional = request.headers.get(\"If-Modified-Since\") or request.headers.get(\"If-None-Match\")\n\n    if is_conditional is None:\n        response = view_get()\n        response.headers[\"Last-Modified\"] = http_date()\n        response.headers[\"ETag\"] = uuid.uuid4().hex\n        return response\n    else:\n        return status_code(304)"
    },
    {
        "original": "def _rfind(lst, item):\n        \"\"\"\n        Reverse look-up.\n\n        :param list lst: The list to look up in.\n        :param item: The item to look for.\n        :return: Offset of the item if found. A ValueError is raised if the item is not in the list.\n        :rtype: int\n        \"\"\"\n\n        try:\n            return dropwhile(lambda x: lst[x] != item,\n                             next(reversed(range(len(lst)))))\n        except Exception:\n            raise ValueError(\"%s not in the list\" % item)",
        "rewrite": "def _rfind(lst, item):\n    \"\"\"\n    Reverse look-up.\n\n    :param list lst: The list to look up in.\n    :param item: The item to look for.\n    :return: Offset of the item if found. A ValueError is raised if the item is not in the list.\n    :rtype: int\n    \"\"\"\n\n    try:\n        return next(dropwhile(lambda x: lst[x] != item, reversed(range(len(lst)))))\n    except Exception:\n        raise ValueError(\"%s not in the list\" % item)"
    },
    {
        "original": "def use_technique(self, tech):\n        \"\"\"\n        Use an exploration technique with this SimulationManager.\n\n        Techniques can be found in :mod:`angr.exploration_techniques`.\n\n        :param tech:    An ExplorationTechnique object that contains code to modify\n                        this SimulationManager's behavior.\n        :type tech:     ExplorationTechnique\n        :return:        The technique that was added, for convenience\n        \"\"\"\n        if not isinstance(tech, ExplorationTechnique):\n            raise SimulationManagerError\n\n        # XXX: as promised\n        tech.project = self._project\n        tech.setup(self)\n\n        HookSet.install_hooks(self, **tech._get_hooks())\n        self._techniques.append(tech)\n        return tech",
        "rewrite": "def use_technique(self, tech):\n    if not isinstance(tech, ExplorationTechnique):\n        raise SimulationManagerError\n\n    tech.project = self._project\n    tech.setup(self)\n\n    HookSet.install_hooks(self, **tech._get_hooks())\n    self._techniques.append(tech)\n    return tech"
    },
    {
        "original": "def getPollingRate(self):\n        \"\"\"get data polling rate for sleepy end device\"\"\"\n        print '%s call getPollingRate' % self.port\n        sPollingRate = self.__sendCommand('pollperiod')[0]\n        try:\n            iPollingRate = int(sPollingRate)/1000\n            fPollingRate = round(float(sPollingRate)/1000, 3)\n            return fPollingRate if fPollingRate > iPollingRate else iPollingRate\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger(\"getPollingRate() Error: \" + str(e))",
        "rewrite": "def getPollingRate(self):\n    print('%s call getPollingRate' % self.port)\n    sPollingRate = self.__sendCommand('pollperiod')[0]\n    try:\n        iPollingRate = int(sPollingRate) / 1000\n        fPollingRate = round(float(sPollingRate) / 1000, 3)\n        return fPollingRate if fPollingRate > iPollingRate else iPollingRate\n    except Exception as e:\n        ModuleHelper.WriteIntoDebugLogger(\"getPollingRate() Error: \" + str(e))"
    },
    {
        "original": "def is_iequivalent(self, model):\n        \"\"\"\n        Checks whether the given model is I-equivalent\n\n        Two graphs G1 and G2 are said to be I-equivalent if they have same skeleton\n        and have same set of immoralities.\n\n        Note: For same skeleton different names of nodes can work but for immoralities\n        names of nodes must be same\n\n        Parameters\n        ----------\n        model : A DAG object, for which you want to check I-equivalence\n\n        Returns\n        --------\n        boolean : True if both are I-equivalent, False otherwise\n\n        Examples\n        --------\n        >>> from pgmpy.base import DAG\n        >>> G = DAG()\n        >>> G.add_edges_from([('V', 'W'), ('W', 'X'),\n        ...                   ('X', 'Y'), ('Z', 'Y')])\n        >>> G1 = DAG()\n        >>> G1.add_edges_from([('W', 'V'), ('X', 'W'),\n        ...                    ('X', 'Y'), ('Z', 'Y')])\n        >>> G.is_iequivalent(G1)\n        True\n\n        \"\"\"\n        if not isinstance(model, DAG):\n            raise TypeError('model must be an instance of DAG')\n        skeleton = nx.algorithms.isomorphism.GraphMatcher(self.to_undirected(), model.to_undirected())\n        if skeleton.is_isomorphic() and self.get_immoralities() == model.get_immoralities():\n            return True\n        return False",
        "rewrite": "def is_iequivalent(self, model):\n    if not isinstance(model, DAG):\n        raise TypeError('model must be an instance of DAG')\n    skeleton = nx.algorithms.isomorphism.GraphMatcher(self.to_undirected(), model.to_undirected())\n    if skeleton.is_isomorphic() and self.get_immoralities() == model.get_immoralities():\n        return True\n    return False"
    },
    {
        "original": "def issue(self, issue_instance_id):\n        \"\"\"Select an issue.\n\n        Parameters:\n            issue_instance_id: int    id of the issue instance to select\n\n        Note: We are selecting issue instances, even though the command is called\n        issue.\n        \"\"\"\n        with self.db.make_session() as session:\n            selected_issue = (\n                session.query(IssueInstance)\n                .filter(IssueInstance.id == issue_instance_id)\n                .scalar()\n            )\n\n            if selected_issue is None:\n                self.warning(\n                    f\"Issue {issue_instance_id} doesn't exist. \"\n                    \"Type 'issues' for available issues.\"\n                )\n                return\n\n            self.sources = self._get_leaves_issue_instance(\n                session, issue_instance_id, SharedTextKind.SOURCE\n            )\n\n            self.sinks = self._get_leaves_issue_instance(\n                session, issue_instance_id, SharedTextKind.SINK\n            )\n\n        self.current_issue_instance_id = int(selected_issue.id)\n        self.current_frame_id = -1\n        self.current_trace_frame_index = 1  # first one after the source\n\n        print(f\"Set issue to {issue_instance_id}.\")\n        if int(selected_issue.run_id) != self.current_run_id:\n            self.current_run_id = int(selected_issue.run_id)\n            print(f\"Set run to {self.current_run_id}.\")\n        print()\n\n        self._generate_trace_from_issue()\n        self.show()",
        "rewrite": "def issue(self, issue_instance_id):\n    with self.db.make_session() as session:\n        selected_issue = session.query(IssueInstance).filter(IssueInstance.id == issue_instance_id).scalar()\n\n        if selected_issue is None:\n            self.warning(f\"Issue {issue_instance_id} doesn't exist. Type 'issues' for available issues.\")\n            return\n\n        self.sources = self._get_leaves_issue_instance(session, issue_instance_id, SharedTextKind.SOURCE)\n        self.sinks = self._get_leaves_issue_instance(session, issue_instance_id, SharedTextKind.SINK)\n\n    self.current_issue_instance_id = int(selected_issue.id)\n    self.current_frame_id = -1\n    self.current_trace_frame_index = 1  # first one after the source\n\n    print(f\"Set issue to {issue_instance_id}.\")\n    if int(selected_issue.run_id) != self.current_run_id:\n        self.current_run_id = int(selected_issue.run_id)\n        print(f\"Set run to {self.current_run_id}.\\n\")\n\n    self._generate_trace_from_issue()\n    self.show()"
    },
    {
        "original": "def issues(self, **kwargs):\n        \"\"\"List issues related to this milestone.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of issues\n        \"\"\"\n\n        path = '%s/%s/issues' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = GroupIssueManager(self.manager.gitlab,\n                                    parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, GroupIssue, data_list)",
        "rewrite": "def issues(self, **kwargs):\n    path = f'{self.manager.path}/{self.get_id()}/issues'\n    data_list = self.manager.gitlab.http_list(path, as_list=False, **kwargs)\n    manager = GroupIssueManager(self.manager.gitlab, parent=self.manager._parent)\n    return RESTObjectList(manager, GroupIssue, data_list)"
    },
    {
        "original": "def get_action(self, parent, undo_stack: QUndoStack, sel_range, protocol: ProtocolAnalyzer, view: int):\n        \"\"\"\n        :type parent: QTableView\n        :type undo_stack: QUndoStack\n        :type protocol_analyzers: list of ProtocolAnalyzer\n        \"\"\"\n        min_row, max_row, start, end = sel_range\n        if min_row == -1 or max_row == -1 or start == -1 or end == -1:\n            return None\n\n        if max_row != min_row:\n            return None\n\n        end = protocol.convert_index(end, view, 0, True, message_indx=min_row)[0]\n        # factor = 1 if view == 0 else 4 if view == 1 else 8\n\n        self.command = MessageBreakAction(protocol, max_row, end)\n        action = QAction(self.command.text(), parent)\n        action.triggered.connect(self.action_triggered)\n        self.undo_stack = undo_stack\n        return action",
        "rewrite": "def get_action(self, parent, undo_stack: QUndoStack, sel_range, protocol: ProtocolAnalyzer, view: int):\n        min_row, max_row, start, end = sel_range\n        if min_row == -1 or max_row == -1 or start == -1 or end == -1:\n            return None\n\n        if max_row != min_row:\n            return None\n\n        end = protocol.convert_index(end, view, 0, True, message_indx=min_row)[0]\n        \n        self.command = MessageBreakAction(protocol, max_row, end)\n        action = QAction(self.command.text(), parent)\n        action.triggered.connect(self.action_triggered)\n        self.undo_stack = undo_stack\n        return action"
    },
    {
        "original": "def _contour_data(data, length_scales, log_SNRs, kernel_call=GPy.kern.RBF):\n    \"\"\"\n    Evaluate the GP objective function for a given data set for a range of\n    signal to noise ratios and a range of lengthscales.\n\n    :data_set: A data set from the utils.datasets director.\n    :length_scales: a list of length scales to explore for the contour plot.\n    :log_SNRs: a list of base 10 logarithm signal to noise ratios to explore for the contour plot.\n    :kernel: a kernel to use for the 'signal' portion of the data.\n    \"\"\"\n\n    lls = []\n    total_var = np.var(data['Y'])\n    kernel = kernel_call(1, variance=1., lengthscale=1.)\n    model = GPy.models.GPRegression(data['X'], data['Y'], kernel=kernel)\n    for log_SNR in log_SNRs:\n        SNR = 10.**log_SNR\n        noise_var = total_var / (1. + SNR)\n        signal_var = total_var - noise_var\n        model.kern['.*variance'] = signal_var\n        model.likelihood.variance = noise_var\n        length_scale_lls = []\n\n        for length_scale in length_scales:\n            model['.*lengthscale'] = length_scale\n            length_scale_lls.append(model.log_likelihood())\n\n        lls.append(length_scale_lls)\n\n    return np.array(lls)",
        "rewrite": "def _contour_data(data, length_scales, log_SNRs, kernel_call=GPy.kern.RBF):\n    lls = []\n    total_var = np.var(data['Y'])\n    kernel = kernel_call(1, variance=1., lengthscale=1.)\n    model = GPy.models.GPRegression(data['X'], data['Y'], kernel=kernel)\n    for log_SNR in log_SNRs:\n        SNR = 10.**log_SNR\n        noise_var = total_var / (1. + SNR)\n        signal_var = total_var - noise_var\n        model.kern['.*variance'] = signal_var\n        model.likelihood.variance = noise_var\n        length_scale_lls = []\n\n        for length_scale in length_scales:\n            model['.*lengthscale'] = length_scale\n            length_scale_lls.append(model.log_likelihood())\n\n        lls.append(length_scale_lls)\n\n    return np.array(lls)"
    },
    {
        "original": "def get_pos_infinity(dtype):\n    \"\"\"Return an appropriate positive infinity for this dtype.\n\n    Parameters\n    ----------\n    dtype : np.dtype\n\n    Returns\n    -------\n    fill_value : positive infinity value corresponding to this dtype.\n    \"\"\"\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return np.inf + 1j * np.inf\n\n    return INF",
        "rewrite": "def get_pos_infinity(dtype):\n    if issubclass(dtype.type, (np.floating, np.integer)):\n        return np.inf\n\n    if issubclass(dtype.type, np.complexfloating):\n        return np.inf + 1j * np.inf\n\n    return float('inf')"
    },
    {
        "original": "def disconnect(self, sid, namespace):\n        \"\"\"Register a client disconnect from a namespace.\"\"\"\n        if namespace not in self.rooms:\n            return\n        rooms = []\n        for room_name, room in six.iteritems(self.rooms[namespace].copy()):\n            if sid in room:\n                rooms.append(room_name)\n        for room in rooms:\n            self.leave_room(sid, namespace, room)\n        if sid in self.callbacks and namespace in self.callbacks[sid]:\n            del self.callbacks[sid][namespace]\n            if len(self.callbacks[sid]) == 0:\n                del self.callbacks[sid]\n        if namespace in self.pending_disconnect and \\\n                sid in self.pending_disconnect[namespace]:\n            self.pending_disconnect[namespace].remove(sid)\n            if len(self.pending_disconnect[namespace]) == 0:\n                del self.pending_disconnect[namespace]",
        "rewrite": "def disconnect(self, sid, namespace):\n    if namespace not in self.rooms:\n        return\n    rooms = []\n    for room_name, room in six.iteritems(self.rooms[namespace].copy()):\n        if sid in room:\n            rooms.append(room_name)\n    for room in rooms:\n        self.leave_room(sid, namespace, room)\n    if sid in self.callbacks and namespace in self.callbacks[sid]:\n        del self.callbacks[sid][namespace]\n        if len(self.callbacks[sid]) == 0:\n            del self.callbacks[sid]\n    if namespace in self.pending_disconnect and sid in self.pending_disconnect[namespace]:\n        self.pending_disconnect[namespace].remove(sid)\n        if len(self.pending_disconnect[namespace]) == 0:\n            del self.pending_disconnect[namespace]"
    },
    {
        "original": "def analyze(self, text, tokenizer=str.split):\n        \"\"\"Analyze text and return pretty format.\n\n        Args:\n            text: string, the input text.\n            tokenizer: Tokenize input sentence. Default tokenizer is `str.split`.\n\n        Returns:\n            res: dict.\n        \"\"\"\n        if not self.tagger:\n            self.tagger = Tagger(self.model,\n                                 preprocessor=self.p,\n                                 tokenizer=tokenizer)\n\n        return self.tagger.analyze(text)",
        "rewrite": "def analyze(self, text, tokenizer=str.split):\n    if not self.tagger:\n        self.tagger = Tagger(self.model, preprocessor=self.p, tokenizer=tokenizer)\n\n    return self.tagger.analyze(text)"
    },
    {
        "original": "def options(self, context, module_options):\n        \"\"\"\n        CONTYPE   Specifies the VNC connection type, choices are: reverse, bind (default: reverse).\n        PORT      VNC Port (default: 5900)\n        PASSWORD  Specifies the connection password.\n        \"\"\"\n\n        self.contype = 'reverse'\n        self.port = 5900\n        self.password = None\n\n        if 'PASSWORD' not in module_options:\n            context.log.error('PASSWORD option is required!')\n            exit(1)\n\n        if 'CONTYPE' in module_options:\n            self.contype    =  module_options['CONTYPE']\n\n        if 'PORT' in module_options:\n            self.port = int(module_options['PORT'])\n\n        self.password = module_options['PASSWORD']\n\n        self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1')\n        self.ps_script2 = obfs_ps_script('invoke-vnc/Invoke-Vnc.ps1')",
        "rewrite": "def options(self, context, module_options):\n\n    self.contype = 'reverse'\n    self.port = 5900\n    self.password = None\n\n    if 'PASSWORD' not in module_options:\n        context.log.error('PASSWORD option is required!')\n        exit(1)\n\n    self.contype = module_options.get('CONTYPE', self.contype)\n    self.port = int(module_options.get('PORT', self.port))\n    self.password = module_options['PASSWORD']\n\n    self.ps_script1 = obfs_ps_script('cme_powershell_scripts/Invoke-PSInject.ps1')\n    self.ps_script2 = obfs_ps_script('invoke-vnc/Invoke-Vnc.ps1')"
    },
    {
        "original": "def dKd_dLen(self, X, dimension, lengthscale, X2=None):\n\t\t\"\"\"\n\t\tDerivate of Kernel function wrt lengthscale applied on inputs X and X2.\n\t\tIn the stationary case there is an inner function depending on the\n\t\tdistances from X to X2, called r.\n\n\t\tdKd_dLen(X, X2) = dKdLen_of_r((X-X2)**2)\n\t\t\"\"\"\n\t\tr = self._scaled_dist(X, X2)\n\t\treturn self.dKdLen_of_r(r, dimension, lengthscale)",
        "rewrite": "def dKd_dLen(self, X, dimension, lengthscale, X2=None):\n    r = self._scaled_dist(X, X2)\n    return self.dKdLen_of_r(r, dimension, lengthscale)"
    },
    {
        "original": "def update_chain(graph, loc, du, ud):\n    \"\"\"\n    Updates the DU chain of the instruction located at loc such that there is\n    no more reference to it so that we can remove it.\n    When an instruction is found to be dead (i.e it has no side effect, and the\n    register defined is not used) we have to update the DU chain of all the\n    variables that may me used by the dead instruction.\n    \"\"\"\n    ins = graph.get_ins_from_loc(loc)\n    for var in ins.get_used_vars():\n        # We get the definition points of the current variable\n        for def_loc in set(ud[var, loc]):\n            # We remove the use of the variable at loc from the DU chain of\n            # the variable definition located at def_loc\n            du[var, def_loc].remove(loc)\n            ud[var, loc].remove(def_loc)\n            if not ud.get((var, loc)):\n                ud.pop((var, loc))\n            # If the DU chain of the defined variable is now empty, this means\n            # that we may have created a new dead instruction, so we check that\n            # the instruction has no side effect and we update the DU chain of\n            # the new dead instruction, and we delete it.\n            # We also make sure that def_loc is not < 0. This is the case when\n            # the current variable is a method parameter.\n            if def_loc >= 0 and not du[var, def_loc]:\n                du.pop((var, def_loc))\n                def_ins = graph.get_ins_from_loc(def_loc)\n                if def_ins.is_call():\n                    def_ins.remove_defined_var()\n                elif def_ins.has_side_effect():\n                    continue\n                else:\n                    update_chain(graph, def_loc, du, ud)\n                    graph.remove_ins(def_loc)",
        "rewrite": "def update_chain(graph, loc, du, ud):\n    \n    ins = graph.get_ins_from_loc(loc)\n    \n    for var in ins.get_used_vars():\n        \n        for def_loc in set(ud[var, loc]):\n            \n            du[var, def_loc].remove(loc)\n            ud[var, loc].remove(def_loc)\n            \n            if not ud.get((var, loc)):\n                ud.pop((var, loc))\n            \n            if def_loc >= 0 and not du[var, def_loc]:\n                du.pop((var, def_loc))\n                def_ins = graph.get_ins_from_loc(def_loc)\n                \n                if def_ins.is_call():\n                    def_ins.remove_defined_var()\n                \n                elif def_ins.has_side_effect():\n                    continue\n                \n                else:\n                    update_chain(graph, def_loc, du, ud)\n                    graph.remove_ins(def_loc)"
    },
    {
        "original": "def microsoft(self, key, x86=False):\n        \"\"\"\n        Return key in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        x86: str\n            Force x86 software registry.\n\n        Return\n        ------\n        str: value\n        \"\"\"\n        node64 = '' if self.pi.current_is_x86() or x86 else 'Wow6432Node'\n        return os.path.join('Software', node64, 'Microsoft', key)",
        "rewrite": "def microsoft(self, key, x86=False):\n    node64 = '' if self.pi.current_is_x86() or x86 else 'Wow6432Node'\n    return os.path.join('Software', node64, 'Microsoft', key)"
    },
    {
        "original": "def rename(name, kwargs, call=None):\n    \"\"\"\n    Properly rename a node. Pass in the new name as \"new name\".\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a rename mymachine newname=yourmachine\n    \"\"\"\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The rename action must be called with -a or --action.'\n        )\n\n    log.info('Renaming %s to %s', name, kwargs['newname'])\n\n    set_tags(name, {'Name': kwargs['newname']}, call='action')\n\n    salt.utils.cloud.rename_key(\n        __opts__['pki_dir'], name, kwargs['newname']\n    )",
        "rewrite": "def rename(name, newname, call=None):\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The rename action must be called with -a or --action.'\n        )\n\n    log.info('Renaming %s to %s', name, newname)\n\n    set_tags(name, {'Name': newname}, call='action')\n\n    salt.utils.cloud.rename_key(\n        __opts__['pki_dir'], name, newname\n    )"
    },
    {
        "original": "def get_pr(pr_num, config=None, repo=DEFAULT_REPO, raw=False):\n    \"\"\"\n    Get the payload for the given PR number. Let exceptions bubble up.\n    \"\"\"\n    response = requests.get(PR_ENDPOINT.format(repo, pr_num), auth=get_auth_info(config))\n\n    if raw:\n        return response\n    else:\n        response.raise_for_status()\n        return response.json()",
        "rewrite": "def get_pr(pr_num, config=None, repo=DEFAULT_REPO, raw=False):\n    response = requests.get(PR_ENDPOINT.format(repo, pr_num), auth=get_auth_info(config))\n\n    if raw:\n        return response\n    else:\n        response.raise_for_status()\n        return response.json()"
    },
    {
        "original": "def owns_endpoint(self, endpoint):\n        \"\"\"Tests if an endpoint name (not path) belongs to this Api.  Takes\n        in to account the Blueprint name part of the endpoint name.\n\n        :param endpoint: The name of the endpoint being checked\n        :return: bool\n        \"\"\"\n\n        if self.blueprint:\n            if endpoint.startswith(self.blueprint.name):\n                endpoint = endpoint.split(self.blueprint.name + '.', 1)[-1]\n            else:\n                return False\n        return endpoint in self.endpoints",
        "rewrite": "def owns_endpoint(self, endpoint):\n        if self.blueprint:\n            if endpoint.startswith(self.blueprint.name):\n                endpoint = endpoint.split(self.blueprint.name + '.', 1)[-1]\n            else:\n                return False\n        return endpoint in self.endpoints"
    },
    {
        "original": "def get(self, card_id):\n        \"\"\"\n        \u67e5\u8be2\u5361\u5238\u8be6\u60c5\n        \"\"\"\n        result = self._post(\n            'card/get',\n            data={\n                'card_id': card_id\n            },\n            result_processor=lambda x: x['card']\n        )\n        return result",
        "rewrite": "def get_card_details(self, card_id):\n    result = self._post(\n        'card/get',\n        data={\n            'card_id': card_id\n        },\n        result_processor=lambda x: x['card']\n    )\n    return result"
    },
    {
        "original": "def copy(self):\n        \"\"\" Creates a copy of this RequestParser with the same set of arguments \"\"\"\n        parser_copy = self.__class__(self.argument_class, self.namespace_class)\n        parser_copy.args = deepcopy(self.args)\n        parser_copy.trim = self.trim\n        parser_copy.bundle_errors = self.bundle_errors\n        return parser_copy",
        "rewrite": "def copy(self):\n    parser_copy = self.__class__(self.argument_class, self.namespace_class)\n    parser_copy.args = deepcopy(self.args)\n    parser_copy.trim = self.trim\n    parser_copy.bundle_errors = self.bundle_errors\n    return parser_copy"
    },
    {
        "original": "def row_factory(cursor, row):\n    \"\"\"Returns a sqlite row factory that returns a dictionary\"\"\"\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d",
        "rewrite": "def row_factory(cursor, row):\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d"
    },
    {
        "original": "def _ohlc_dict(df_or_figure,open='',high='',low='',close='',volume='',\n\t\t\t   validate='',**kwargs):\n\t\"\"\"\n\tReturns a dictionary with the actual column names that \n\tcorrespond to each of the OHLCV values.\n\n\tdf_or_figure :  DataFrame or Figure\n\topen : string\n\t\tColumn name to be used for OPEN values\n\thigh : string\n\t\tColumn name to be used for HIGH values\n\tlow : string\n\t\tColumn name to be used for LOW values\n\tclose : string\n\t\tColumn name to be used for CLOSE values\n\tvolume : string\n\t\tColumn name to be used for VOLUME values\n\tvalidate : string\n\t\tValidates that the stated column exists\n\t\tExample:\n\t\t\tvalidate='ohv' | Will ensure Open, High\n\t\t\t\t\t\t\t and close values exist. \n\t\"\"\"\n\tc_dir={}\n\tohlcv=['open','high','low','close','volume']\n\tif type(df_or_figure)==pd.DataFrame:\n\t\tcnames=df_or_figure.columns\n\telif type(df_or_figure)==Figure or type(df_or_figure) == dict:\n\t\tcnames=df_or_figure.axis['ref'].keys()\n\telif type(df_or_figure)==pd.Series:\n\t\tcnames=[df_or_figure.name]\n\tc_min=dict([(v.lower(),v) for v in cnames])\n\tfor _ in ohlcv:\n\t\tif _ in c_min.keys():\n\t\t\tc_dir[_]=c_min[_]\n\t\telse:\n\t\t\tfor c in cnames:\n\t\t\t\tif _ in c.lower():\n\t\t\t\t\tc_dir[_]=c\n\n\tif open:\n\t\tc_dir['open']=open\n\tif high:\n\t\tc_dir['high']=high\n\tif low:\n\t\tc_dir['low']=low\n\tif close:\n\t\tc_dir['close']=close\n\tif volume:\n\t\tc_dir['volume']=volume\n\t\t\n\tfor v in list(c_dir.values()):\n\t\tif v not in cnames:\n\t\t\traise StudyError('{0} is not a valid column name'.format(v))\n\n\tif validate:\n\t\t\terrs=[]\n\t\t\tval=validate.lower()\n\t\t\ts_names=dict([(_[0],_) for _ in ohlcv])\n\t\t\tcols=[_[0] for _ in c_dir.keys()]\n\t\t\tfor _ in val:\n\t\t\t\tif _ not in cols:\n\t\t\t\t\terrs.append(s_names[_])\n\t\t\tif errs:\n\t\t\t\traise StudyError('Missing Columns: {0}'.format(', '.join(errs)))\n\n\treturn c_dir",
        "rewrite": "def _ohlc_dict(df_or_figure, open_col='', high_col='', low_col='', close_col='', volume_col='',\n                validate='', **kwargs):\n    c_dir = {}\n    ohlcv = ['open', 'high', 'low', 'close', 'volume']\n\n    if type(df_or_figure) == pd.DataFrame:\n        cnames = df_or_figure.columns\n    elif type(df_or_figure) == Figure or type(df_or_figure) == dict:\n        cnames = df_or_figure.axis['ref'].keys()\n    elif type(df_or_figure) == pd.Series:\n        cnames = [df_or_figure.name]\n\n    c_min = dict([(v.lower(), v) for v in cnames])\n\n    for _ in ohlcv:\n        if _ in c_min.keys():\n            c_dir[_] = c_min[_]\n        else:\n            for c in cnames:\n                if _ in c.lower():\n                    c_dir[_] = c\n\n    if open_col:\n        c_dir['open'] = open_col\n    if high_col:\n        c_dir['high'] = high_col\n    if low_col:\n        c_dir['low'] = low_col\n    if close_col:\n        c_dir['close'] = close_col\n    if volume_col:\n        c_dir['volume'] = volume_col\n\n    for v in list(c_dir.values()):\n        if v not in cnames:\n            raise StudyError('{0} is not a valid column name'.format(v))\n\n    if validate:\n        errs = []\n        val = validate.lower()\n        s_names = dict([(v[0], v) for v in ohlcv])\n        cols = [v[0] for v in c_dir.items()]\n\n        for _ in val:\n            if _ not in cols:\n                errs.append(s_names[_])\n        if errs:\n            raise StudyError('Missing Columns: {0}'.format(', '.join(errs)))\n\n    return c_dir"
    },
    {
        "original": "def get_list_from_file(file_name):\n    \"\"\"read the lines from a file into a list\"\"\"\n    with open(file_name, mode='r', encoding='utf-8') as f1:\n        lst = f1.readlines()\n    return lst",
        "rewrite": "def get_list_from_file(file_name):\n    with open(file_name, mode='r', encoding='utf-8') as file:\n        lst = file.readlines()\n    return lst"
    },
    {
        "original": "def get_function_name(s):\n    \"\"\"\n    Get the function name from a C-style function declaration string.\n\n    :param str s: A C-style function declaration string.\n    :return:      The function name.\n    :rtype:       str\n    \"\"\"\n\n    s = s.strip()\n    if s.startswith(\"__attribute__\"):\n        # Remove \"__attribute__ ((foobar))\"\n        if \"))\" not in s:\n            raise ValueError(\"__attribute__ is present, but I cannot find double-right parenthesis in the function \"\n                             \"declaration string.\")\n\n        s = s[s.index(\"))\") + 2 : ].strip()\n\n    if '(' not in s:\n        raise ValueError(\"Cannot find any left parenthesis in the function declaration string.\")\n\n    func_name = s[:s.index('(')].strip()\n\n    for i, ch in enumerate(reversed(func_name)):\n        if ch == ' ':\n            pos = len(func_name) - 1 - i\n            break\n    else:\n        raise ValueError('Cannot find any space in the function declaration string.')\n\n    func_name = func_name[pos + 1 : ]\n    return func_name",
        "rewrite": "def get_function_name(s):\n    s = s.strip()\n    if s.startswith(\"__attribute__\"):\n        if \"))\" not in s:\n            raise ValueError(\"__attribute__ is present, but I cannot find double-right parenthesis in the function declaration string.\")\n\n        s = s[s.index(\"))\") + 2 : ].strip()\n\n    if '(' not in s:\n        raise ValueError(\"Cannot find any left parenthesis in the function declaration string.\")\n\n    func_name = s[:s.index('(')].strip()\n\n    for i, ch in enumerate(reversed(func_name)):\n        if ch == ' ':\n            pos = len(func_name) - 1 - i\n            break\n    else:\n        raise ValueError('Cannot find any space in the function declaration string.')\n\n    func_name = func_name[pos + 1 : ]\n    return func_name"
    },
    {
        "original": "def print_result(\n    result: Result,\n    host: Optional[str] = None,\n    vars: List[str] = None,\n    failed: bool = False,\n    severity_level: int = logging.INFO,\n) -> None:\n    \"\"\"\n    Prints the :obj:`nornir.core.task.Result` from a previous task to screen\n\n    Arguments:\n        result: from a previous task\n        host: # TODO\n        vars: Which attributes you want to print\n        failed: if ``True`` assume the task failed\n        severity_level: Print only errors with this severity level or higher\n    \"\"\"\n    LOCK.acquire()\n    try:\n        _print_result(result, host, vars, failed, severity_level)\n    finally:\n        LOCK.release()",
        "rewrite": "def print_result(\n    result: Result,\n    host: Optional[str] = None,\n    vars: List[str] = None,\n    failed: bool = False,\n    severity_level: int = logging.INFO,\n) -> None:\n    LOCK.acquire()\n    try:\n        _print_result(result, host, vars, failed, severity_level)\n    finally:\n        LOCK.release()"
    },
    {
        "original": "def do_build(self):\n        \"\"\"\n        We need this hack, else 'self' would be replaced by __iter__.next().\n        \"\"\"\n        tmp = self.explicit\n        self.explicit = True\n        b = super(KeyShareEntry, self).do_build()\n        self.explicit = tmp\n        return b",
        "rewrite": "def do_build(self):\n        \"\"\"\n        This hack is needed because 'self' would otherwise be replaced by __iter__.next().\n        \"\"\"\n        tmp = self.explicit\n        self.explicit = True\n        b = super(KeyShareEntry, self).do_build()\n        self.explicit = tmp\n        return b."
    },
    {
        "original": "def _to_seconds(timestr):\n    \"\"\"\n    Converts a time value to seconds.\n\n    As per RFC1035 (page 45), max time is 1 week, so anything longer (or\n    unreadable) will be set to one week (604800 seconds).\n    \"\"\"\n    timestr = timestr.upper()\n    if 'H' in timestr:\n        seconds = int(timestr.replace('H', '')) * 3600\n    elif 'D' in timestr:\n        seconds = int(timestr.replace('D', '')) * 86400\n    elif 'W' in timestr:\n        seconds = 604800\n    else:\n        try:\n            seconds = int(timestr)\n        except ValueError:\n            seconds = 604800\n    if seconds > 604800:\n        seconds = 604800\n    return seconds",
        "rewrite": "def _to_seconds(timestr):\n    timestr = timestr.upper()\n    if 'H' in timestr:\n        seconds = int(timestr.replace('H', '')) * 3600\n    elif 'D' in timestr:\n        seconds = int(timestr.replace('D', '')) * 86400\n    elif 'W' in timestr:\n        seconds = 604800\n    else:\n        try:\n            seconds = int(timestr)\n        except ValueError:\n            seconds = 604800\n    if seconds > 604800:\n        seconds = 604800\n    return seconds"
    },
    {
        "original": "def create(self, data, **kwargs):\n        \"\"\"Create a new object.\n\n        Args:\n            data (dict): parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            RESTObject, RESTObject: The source and target issues\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n        \"\"\"\n        self._check_missing_create_attrs(data)\n        server_data = self.gitlab.http_post(self.path, post_data=data,\n                                            **kwargs)\n        source_issue = ProjectIssue(self._parent.manager,\n                                    server_data['source_issue'])\n        target_issue = ProjectIssue(self._parent.manager,\n                                    server_data['target_issue'])\n        return source_issue, target_issue",
        "rewrite": "def create(self, data, **kwargs):\n    self._check_missing_create_attrs(data)\n    server_data = self.gitlab.http_post(self.path, post_data=data, **kwargs)\n    source_issue = ProjectIssue(self._parent.manager, server_data['source_issue'])\n    target_issue = ProjectIssue(self._parent.manager, server_data['target_issue'])\n    return source_issue, target_issue"
    },
    {
        "original": "def saveAsTFRecords(df, output_dir):\n  \"\"\"Save a Spark DataFrame as TFRecords.\n\n  This will convert the DataFrame rows to TFRecords prior to saving.\n\n  Args:\n    :df: Spark DataFrame\n    :output_dir: Path to save TFRecords\n  \"\"\"\n  tf_rdd = df.rdd.mapPartitions(toTFExample(df.dtypes))\n  tf_rdd.saveAsNewAPIHadoopFile(output_dir, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n                                keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                valueClass=\"org.apache.hadoop.io.NullWritable\")",
        "rewrite": "def saveAsTFRecords(df, output_dir):\n    tf_rdd = df.rdd.mapPartitions(toTFExample(df.dtypes))\n    tf_rdd.saveAsNewAPIHadoopFile(output_dir, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n                                  keyClass=\"org.apache.hadoop.io.BytesWritable\",\n                                  valueClass=\"org.apache.hadoop.io.NullWritable\")"
    },
    {
        "original": "def Lookup(self,\n             keywords,\n             start_time=FIRST_TIMESTAMP,\n             end_time=LAST_TIMESTAMP,\n             last_seen_map=None):\n    \"\"\"Finds objects associated with keywords.\n\n    Find the names related to all keywords.\n\n    Args:\n      keywords: A collection of keywords that we are interested in.\n      start_time: Only considers keywords added at or after this point in time.\n      end_time: Only considers keywords at or before this point in time.\n      last_seen_map: If present, is treated as a dict and populated to map pairs\n        (keyword, name) to the timestamp of the latest connection found.\n    Returns:\n      A set of potentially relevant names.\n\n    \"\"\"\n    posting_lists = self.ReadPostingLists(\n        keywords,\n        start_time=start_time,\n        end_time=end_time,\n        last_seen_map=last_seen_map)\n\n    results = list(itervalues(posting_lists))\n    relevant_set = results[0]\n\n    for hits in results:\n      relevant_set &= hits\n\n      if not relevant_set:\n        return relevant_set\n\n    return relevant_set",
        "rewrite": "def lookup(self, keywords, start_time=FIRST_TIMESTAMP, end_time=LAST_TIMESTAMP, last_seen_map=None):\n    posting_lists = self.read_posting_lists(keywords, start_time=start_time, end_time=end_time, last_seen_map=last_seen_map)\n    \n    results = list(itervalues(posting_lists))\n    relevant_set = results[0]\n    \n    for hits in results:\n        relevant_set &= hits\n        if not relevant_set:\n            return relevant_set\n            \n    return relevant_set"
    },
    {
        "original": "def _load_schema(name, path=__file__):\n    \"\"\"Load a schema from disk\"\"\"\n    path = os.path.join(os.path.dirname(path), name + '.yaml')\n    with open(path) as handle:\n        schema = yaml.safe_load(handle)\n    fast_schema = rapidjson.Validator(rapidjson.dumps(schema))\n    return path, (schema, fast_schema)",
        "rewrite": "import os\nimport yaml\nimport rapidjson\n\ndef load_schema(name, path=__file__):\n    path = os.path.join(os.path.dirname(path), name + '.yaml')\n    with open(path) as handle:\n        schema = yaml.safe_load(handle)\n    fast_schema = rapidjson.Validator(rapidjson.dumps(schema))\n    return path, (schema, fast_schema)"
    },
    {
        "original": "def _deserialize(self, value, attr, data, partial=None, **kwargs):\n        \"\"\"Same as :meth:`Field._deserialize` with additional ``partial`` argument.\n\n        :param bool|tuple partial: For nested schemas, the ``partial``\n            parameter passed to `Schema.load`.\n\n        .. versionchanged:: 3.0.0\n            Add ``partial`` parameter\n        \"\"\"\n        self._test_collection(value)\n        return self._load(value, data, partial=partial)",
        "rewrite": "def _deserialize(self, value, attr, data, partial=None, **kwargs):\n    self._test_collection(value)\n    return self._load(value, data, partial=partial)"
    },
    {
        "original": "def convert_tensor(input_, device=None, non_blocking=False):\n    \"\"\"Move tensors to relevant device.\"\"\"\n    def _func(tensor):\n        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n\n    return apply_to_tensor(input_, _func)",
        "rewrite": "def convert_tensor(input_, device=None, non_blocking=False):\n    def _func(tensor):\n        return tensor.to(device=device, non_blocking=non_blocking) if device else tensor\n\n    return apply_to_tensor(input_, _func)"
    },
    {
        "original": "def subsets_changed(last_observed_subsets, subsets):\n        \"\"\"\n        >>> Kubernetes.subsets_changed([], [])\n        False\n        >>> Kubernetes.subsets_changed([], [k8s_client.V1EndpointSubset()])\n        True\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip='1.2.3.4')])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=[k8s_client.V1EndpointAddress(ip='1.2.3.5')])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> a = [k8s_client.V1EndpointAddress(ip='1.2.3.4')]\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(protocol='TCP', port=1)])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[k8s_client.V1EndpointPort(port=5432)])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> p1 = k8s_client.V1EndpointPort(name='port1', port=1)\n        >>> p2 = k8s_client.V1EndpointPort(name='port2', port=2)\n        >>> p3 = k8s_client.V1EndpointPort(name='port3', port=3)\n        >>> s1 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p1, p2])]\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p3])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        True\n        >>> s2 = [k8s_client.V1EndpointSubset(addresses=a, ports=[p2, p1])]\n        >>> Kubernetes.subsets_changed(s1, s2)\n        False\n        \"\"\"\n        if len(last_observed_subsets) != len(subsets):\n            return True\n        if subsets == []:\n            return False\n        if len(last_observed_subsets[0].addresses or []) != 1 or \\\n                last_observed_subsets[0].addresses[0].ip != subsets[0].addresses[0].ip or \\\n                len(last_observed_subsets[0].ports) != len(subsets[0].ports):\n            return True\n        if len(subsets[0].ports) == 1:\n            return not Kubernetes.compare_ports(last_observed_subsets[0].ports[0], subsets[0].ports[0])\n        observed_ports = {p.name: p for p in last_observed_subsets[0].ports}\n        for p in subsets[0].ports:\n            if p.name not in observed_ports or not Kubernetes.compare_ports(p, observed_ports.pop(p.name)):\n                return True\n        return False",
        "rewrite": "def subsets_changed(last_observed_subsets, subsets):\n        if len(last_observed_subsets) != len(subsets):\n            return True\n        if subsets == []:\n            return False\n        if len(last_observed_subsets[0].addresses or []) != 1 or \\\n                last_observed_subsets[0].addresses[0].ip != subsets[0].addresses[0].ip or \\\n                len(last_observed_subsets[0].ports) != len(subsets[0].ports):\n            return True\n        if len(subsets[0].ports) == 1:\n            return not Kubernetes.compare_ports(last_observed_subsets[0].ports[0], subsets[0].ports[0])\n        observed_ports = {p.name: p for p in last_observed_subsets[0].ports}\n        for p in subsets[0].ports:\n            if p.name not in observed_ports or not Kubernetes.compare_ports(p, observed_ports.pop(p.name)):\n                return True\n        return False"
    },
    {
        "original": "def mapping_get(index, doc_type, hosts=None, profile=None):\n    \"\"\"\n    Retrieve mapping definition of index or index/type\n\n    index\n        Index for the mapping\n    doc_type\n        Name of the document type\n\n    CLI example::\n\n        salt myminion elasticsearch.mapping_get testindex user\n    \"\"\"\n    es = _get_instance(hosts, profile)\n\n    try:\n        return es.indices.get_mapping(index=index, doc_type=doc_type)\n    except elasticsearch.exceptions.NotFoundError:\n        return None\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot retrieve mapping {0}, server returned code {1} with message {2}\".format(index, e.status_code, e.error))",
        "rewrite": "def mapping_get(index, doc_type, hosts=None, profile=None):\n    es = _get_instance(hosts, profile)\n    try:\n        return es.indices.get_mapping(index=index, doc_type=doc_type)\n    except elasticsearch.exceptions.NotFoundError:\n        return None\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot retrieve mapping {0}, server returned code {1} with message {2}\".format(index, e.status_code, e.error))"
    },
    {
        "original": "def process_eni_metrics(\n        stream_eni, myips, stream,\n        start, end, period, sample_size,\n        resolver, sink_uri):\n    \"\"\"ENI flow stream processor that rollups, enhances,\n       and indexes the stream by time period.\"\"\"\n    stats = Counter()\n    period_counters = flow_stream_stats(myips, stream, period)\n    client = InfluxDBClient.from_dsn(sink_uri)\n    resource = resolver.resolve_resource(stream_eni)\n    points = []\n\n    for period in sorted(period_counters):\n        pc = period_counters[period]\n        pd = datetime.fromtimestamp(period)\n\n        for t in ('inbytes', 'outbytes'):\n            tpc = pc[t]\n            ips = [ip for ip, _ in tpc.most_common(sample_size)]\n            resolved = resolver.resolve(ips, pd - timedelta(900), pd + timedelta(900))\n            logical_counter = rollup_logical(tpc, resolved, ('app', 'env'))\n            for (app, env), v in logical_counter.items():\n                p = {}\n#                rinfo = resolved.get(ip, {})\n                p['fields'] = {'Bytes': v}\n                p['measurement'] = 'traffic_%s' % t\n                p['time'] = datetime.fromtimestamp(period)\n                p['tags'] = {\n                    'Kind': resource['type'],\n                    'AccountId': resource['account_id'],\n                    'App': resource['app'],\n                    'Env': resource['env'],\n                    'ForeignApp': app,\n                    'ForeignEnv': env}\n                points.append(p)\n\n        if len(points) > 2000:\n            client.write_points(points)\n            stats['Points'] += len(points)\n            points = []\n\n    client.write_points(points)\n    stats['Points'] += len(points)\n    log.info('periods:%d resource:%s points:%d',\n             len(period_counters), resource, stats['Points'])\n    return stats",
        "rewrite": "def process_eni_metrics(stream_eni, myips, stream, start, end, period, sample_size, resolver, sink_uri):\n    stats = Counter()\n    period_counters = flow_stream_stats(myips, stream, period)\n    client = InfluxDBClient.from_dsn(sink_uri)\n    resource = resolver.resolve_resource(stream_eni)\n    points = []\n\n    for period in sorted(period_counters):\n        pc = period_counters[period]\n        pd = datetime.fromtimestamp(period)\n\n        for t in ('inbytes', 'outbytes'):\n            tpc = pc[t]\n            ips = [ip for ip, _ in tpc.most_common(sample_size)]\n            resolved = resolver.resolve(ips, pd - timedelta(900), pd + timedelta(900))\n            logical_counter = rollup_logical(tpc, resolved, ('app', 'env'))\n            for (app, env), v in logical_counter.items():\n                p = {}\n                p['fields'] = {'Bytes': v}\n                p['measurement'] = 'traffic_%s' % t\n                p['time'] = datetime.fromtimestamp(period)\n                p['tags'] = {\n                    'Kind': resource['type'],\n                    'AccountId': resource['account_id'],\n                    'App': resource['app'],\n                    'Env': resource['env'],\n                    'ForeignApp': app,\n                    'ForeignEnv': env}\n                points.append(p)\n\n        if len(points) > 2000:\n            client.write_points(points)\n            stats['Points'] += len(points)\n            points = []\n\n    client.write_points(points)\n    stats['Points'] += len(points)\n    log.info('periods:%d resource:%s points:%d', len(period_counters), resource, stats['Points'])\n    return stats"
    },
    {
        "original": "def delete_row_range(self, format_str, start_game, end_game):\n        \"\"\"Delete rows related to the given game range.\n\n        Args:\n          format_str:  a string to `.format()` by the game numbers\n            in order to create the row prefixes.\n          start_game:  the starting game number of the deletion.\n          end_game:  the ending game number of the deletion.\n        \"\"\"\n        row_keys = make_single_array(\n            self.tf_table.keys_by_range_dataset(\n                format_str.format(start_game),\n                format_str.format(end_game)))\n        row_keys = list(row_keys)\n        if not row_keys:\n            utils.dbg('No rows left for games %d..%d' % (\n                start_game, end_game))\n            return\n        utils.dbg('Deleting %d rows:  %s..%s' % (\n            len(row_keys), row_keys[0], row_keys[-1]))\n\n        # Reverse the keys so that the queue is left in a more\n        # sensible end state if you change your mind (say, due to a\n        # mistake in the timestamp) and abort the process: there will\n        # be a bit trimmed from the end, rather than a bit\n        # trimmed out of the middle.\n        row_keys.reverse()\n        total_keys = len(row_keys)\n        utils.dbg('Deleting total of %d keys' % total_keys)\n        concurrency = min(MAX_BT_CONCURRENCY,\n                          multiprocessing.cpu_count() * 2)\n        with multiprocessing.Pool(processes=concurrency) as pool:\n            batches = []\n            with tqdm(desc='Keys', unit_scale=2, total=total_keys) as pbar:\n                for b in utils.iter_chunks(bigtable.row.MAX_MUTATIONS,\n                                           row_keys):\n                    pbar.update(len(b))\n                    batches.append((self.btspec, b))\n                    if len(batches) >= concurrency:\n                        pool.map(_delete_rows, batches)\n                        batches = []\n                pool.map(_delete_rows, batches)\n                batches = []",
        "rewrite": "def delete_row_range(self, format_str, start_game, end_game):\n    row_keys = list(make_single_array(self.tf_table.keys_by_range_dataset(\n        format_str.format(start_game), format_str.format(end_game))))\n    if not row_keys:\n        return\n    row_keys.reverse()\n    concurrency = min(MAX_BT_CONCURRENCY, multiprocessing.cpu_count() * 2)\n    with multiprocessing.Pool(processes=concurrency) as pool:\n        with tqdm(desc='Keys', unit_scale=2, total=len(row_keys)) as pbar:\n            for batch in utils.iter_chunks(bigtable.row.MAX_MUTATIONS, row_keys):\n                pbar.update(len(batch))\n                pool.map(_delete_rows, [(self.btspec, batch)])"
    },
    {
        "original": "def widget_from_tuple(o):\n        \"\"\"Make widgets from a tuple abbreviation.\"\"\"\n        if _matches(o, (Real, Real)):\n            min, max, value = _get_min_max_value(o[0], o[1])\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max)\n        elif _matches(o, (Real, Real, Real)):\n            step = o[2]\n            if step <= 0:\n                raise ValueError(\"step must be >= 0, not %r\" % step)\n            min, max, value = _get_min_max_value(o[0], o[1], step=step)\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max, step=step)",
        "rewrite": "def widget_from_tuple(o):\n    if _matches(o, (Real, Real)):\n        min, max, value = _get_min_max_value(o[0], o[1])\n        if all(isinstance(_, Integral) for _ in o):\n            cls = IntSlider\n        else:\n            cls = FloatSlider\n        return cls(value=value, min=min, max=max)\n    \n    elif _matches(o, (Real, Real, Real)):\n        step = o[2]\n        if step <= 0:\n            raise ValueError(\"step must be >= 0, not %r\" % step)\n        \n        min, max, value = _get_min_max_value(o[0], o[1], step=step)\n        if all(isinstance(_, Integral) for _ in o):\n            cls = IntSlider\n        else:\n            cls = FloatSlider\n        \n        return cls(value=value, min=min, max=max, step=step)"
    },
    {
        "original": "def if_then(self, classical_reg, if_program, else_program=None):\n        \"\"\"\n        If the classical register at index classical reg is 1, run if_program, else run\n        else_program.\n\n        Equivalent to the following construction:\n\n        .. code::\n\n            IF [c]:\n               instrA...\n            ELSE:\n               instrB...\n            =>\n              JUMP-WHEN @THEN [c]\n              instrB...\n              JUMP @END\n              LABEL @THEN\n              instrA...\n              LABEL @END\n\n        :param int classical_reg: The classical register to check as the condition\n        :param Program if_program: A Quil program to execute if classical_reg is 1\n        :param Program else_program: A Quil program to execute if classical_reg is 0. This\n            argument is optional and defaults to an empty Program.\n        :returns: The Quil Program with the branching instructions added.\n        :rtype: Program\n        \"\"\"\n        else_program = else_program if else_program is not None else Program()\n\n        label_then = LabelPlaceholder(\"THEN\")\n        label_end = LabelPlaceholder(\"END\")\n        self.inst(JumpWhen(target=label_then, condition=unpack_classical_reg(classical_reg)))\n        self.inst(else_program)\n        self.inst(Jump(label_end))\n        self.inst(JumpTarget(label_then))\n        self.inst(if_program)\n        self.inst(JumpTarget(label_end))\n        return self",
        "rewrite": "def if_then(self, classical_reg, if_program, else_program=None):\n        else_program = else_program if else_program is not None else Program()\n\n        label_then = LabelPlaceholder(\"THEN\")\n        label_end = LabelPlaceholder(\"END\")\n        self.inst(JumpWhen(target=label_then, condition=unpack_classical_reg(classical_reg)))\n        self.inst(else_program)\n        self.inst(Jump(label_end))\n        self.inst(JumpTarget(label_then))\n        self.inst(if_program)\n        self.inst(JumpTarget(label_end))\n        return self"
    },
    {
        "original": "def _result_to_dict(data, result, conf, source):\n    \"\"\"\n    Aggregates LDAP search result based on rules, returns a dictionary.\n\n    Rules:\n    Attributes tagged in the pillar config as 'attrs' or 'lists' are\n    scanned for a 'key=value' format (non matching entries are ignored.\n\n    Entries matching the 'attrs' tag overwrite previous values where\n    the key matches a previous result.\n\n    Entries matching the 'lists' tag are appended to list of values where\n    the key matches a previous result.\n\n    All Matching entries are then written directly to the pillar data\n    dictionary as data[key] = value.\n\n    For example, search result:\n\n        { saltKeyValue': ['ntpserver=ntp.acme.local', 'foo=myfoo'],\n          'saltList': ['vhost=www.acme.net', 'vhost=www.acme.local'] }\n\n    is written to the pillar data dictionary as:\n\n        { 'ntpserver': 'ntp.acme.local', 'foo': 'myfoo',\n           'vhost': ['www.acme.net', 'www.acme.local'] }\n    \"\"\"\n    attrs = _config('attrs', conf) or []\n    lists = _config('lists', conf) or []\n    dict_key_attr = _config('dict_key_attr', conf) or 'dn'\n    # TODO:\n    # deprecate the default 'mode: split' and make the more\n    # straightforward 'mode: map' the new default\n    mode = _config('mode', conf) or 'split'\n    if mode == 'map':\n        data[source] = []\n        for record in result:\n            ret = {}\n            if 'dn' in attrs or 'distinguishedName' in attrs:\n                log.debug('dn: %s', record[0])\n                ret['dn'] = record[0]\n            record = record[1]\n            log.debug('record: %s', record)\n            for key in record:\n                if key in attrs:\n                    for item in record.get(key):\n                        ret[key] = item\n                if key in lists:\n                    ret[key] = record.get(key)\n            data[source].append(ret)\n    elif mode == 'dict':\n        data[source] = {}\n        for record in result:\n            ret = {}\n            distinguished_name = record[0]\n            log.debug('dn: %s', distinguished_name)\n            if 'dn' in attrs or 'distinguishedName' in attrs:\n                ret['dn'] = distinguished_name\n            record = record[1]\n            log.debug('record: %s', record)\n            for key in record:\n                if key in attrs:\n                    for item in record.get(key):\n                        ret[key] = item\n                if key in lists:\n                    ret[key] = record.get(key)\n            if dict_key_attr in ['dn', 'distinguishedName']:\n                dict_key = distinguished_name\n            else:\n                dict_key = ','.join(sorted(record.get(dict_key_attr, [])))\n            try:\n                data[source][dict_key].append(ret)\n            except KeyError:\n                data[source][dict_key] = [ret]\n    elif mode == 'split':\n        for key in result[0][1]:\n            if key in attrs:\n                for item in result.get(key):\n                    skey, sval = item.split('=', 1)\n                    data[skey] = sval\n            elif key in lists:\n                for item in result.get(key):\n                    if '=' in item:\n                        skey, sval = item.split('=', 1)\n                        if skey not in data:\n                            data[skey] = [sval]\n                        else:\n                            data[skey].append(sval)\n    return data",
        "rewrite": "def _result_to_dict(data, result, conf, source):\n    attrs = _config('attrs', conf) or []\n    lists = _config('lists', conf) or []\n    dict_key_attr = _config('dict_key_attr', conf) or 'dn'\n    mode = _config('mode', conf) or 'split'\n\n    if mode == 'map':\n        data[source] = []\n        for record in result:\n            ret = {}\n            if 'dn' in attrs or 'distinguishedName' in attrs:\n                log.debug('dn: %s', record[0])\n                ret['dn'] = record[0]\n            record = record[1]\n            log.debug('record: %s', record)\n            for key in record:\n                if key in attrs:\n                    for item in record.get(key):\n                        ret[key] = item\n                if key in lists:\n                    ret[key] = record.get(key)\n            data[source].append(ret)\n    elif mode == 'dict':\n        data[source] = {}\n        for record in result:\n            ret = {}\n            distinguished_name = record[0]\n            log.debug('dn: %s', distinguished_name)\n            if 'dn' in attrs or 'distinguishedName' in attrs:\n                ret['dn'] = distinguished_name\n            record = record[1]\n            log.debug('record: %s', record)\n            for key in record:\n                if key in attrs:\n                    for item in record.get(key):\n                        ret[key] = item\n                if key in lists:\n                    ret[key] = record.get(key)\n            if dict_key_attr in ['dn', 'distinguishedName']:\n                dict_key = distinguished_name\n            else:\n                dict_key = ','.join(sorted(record.get(dict_key_attr, [])))\n            try:\n                data[source][dict_key].append(ret)\n            except KeyError:\n                data[source][dict_key] = [ret]\n    elif mode == 'split':\n        for key in result[0][1]:\n            if key in attrs:\n                for item in result[key]:\n                    skey, sval = item.split('=', 1)\n                    data[skey] = sval\n            elif key in lists:\n                for item in result[key]:\n                    if '=' in item:\n                        skey, sval = item.split('=', 1)\n                        if skey not in data:\n                            data[skey] = [sval]\n                        else:\n                            data[skey].append(sval)\n    return data"
    },
    {
        "original": "def to_index(self):\n        \"\"\"Convert this variable to a pandas.Index\"\"\"\n        # n.b. creating a new pandas.Index from an old pandas.Index is\n        # basically free as pandas.Index objects are immutable\n        assert self.ndim == 1\n        index = self._data.array\n        if isinstance(index, pd.MultiIndex):\n            # set default names for multi-index unnamed levels so that\n            # we can safely rename dimension / coordinate later\n            valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i)\n                                 for i, name in enumerate(index.names)]\n            index = index.set_names(valid_level_names)\n        else:\n            index = index.set_names(self.name)\n        return index",
        "rewrite": "def to_index(self):\n    assert self.ndim == 1\n    index = self._data.array\n    if isinstance(index, pd.MultiIndex):\n        valid_level_names = [name or '{}_level_{}'.format(self.dims[0], i)\n                             for i, name in enumerate(index.names)]\n        index = index.set_names(valid_level_names)\n    else:\n        index = index.set_names(self.name)\n    return index"
    },
    {
        "original": "def get_all_entries(self, charge_to_discharge=True):\n        \"\"\"\n        Return all entries input for the electrode.\n\n        Args:\n            charge_to_discharge:\n                order from most charge to most discharged state? Defaults to\n                True.\n\n        Returns:\n            A list of all entries in the electrode (both stable and unstable),\n            ordered by amount of the working ion.\n        \"\"\"\n        all_entries = list(self.get_stable_entries())\n        all_entries.extend(self.get_unstable_entries())\n        # sort all entries by amount of working ion ASC\n        fsrt = lambda e: e.composition.get_atomic_fraction(self.working_ion)\n        all_entries = sorted([e for e in all_entries],\n                             key=fsrt)\n        return all_entries if charge_to_discharge else all_entries.reverse()",
        "rewrite": "def get_all_entries(self, charge_to_discharge=True):\n    all_entries = list(self.get_stable_entries())\n    all_entries.extend(self.get_unstable_entries())\n    fsrt = lambda e: e.composition.get_atomic_fraction(self.working_ion)\n    all_entries = sorted(all_entries, key=fsrt)\n    \n    return all_entries if charge_to_discharge else all_entries[::-1]"
    },
    {
        "original": "def _submit_gauge(self, metric_name, val, metric, custom_tags=None, hostname=None):\n        \"\"\"\n        Submit a metric as a gauge, additional tags provided will be added to\n        the ones from the label provided via the metrics object.\n\n        `custom_tags` is an array of 'tag:value' that will be added to the\n        metric when sending the gauge to Datadog.\n        \"\"\"\n        _tags = self._metric_tags(metric_name, val, metric, custom_tags, hostname)\n        self.check.gauge('{}.{}'.format(self.NAMESPACE, metric_name), val, _tags, hostname=hostname)",
        "rewrite": "def _submit_gauge(self, metric_name, val, metric, custom_tags=None, hostname=None):\n    _tags = self._metric_tags(metric_name, val, metric, custom_tags, hostname)\n    self.check.gauge('{}.{}'.format(self.NAMESPACE, metric_name), val, _tags, hostname=hostname)"
    },
    {
        "original": "def _volume_get(self, volume_id):\n        \"\"\"\n        Organize information about a volume from the volume_id\n        \"\"\"\n        if self.volume_conn is None:\n            raise SaltCloudSystemExit('No cinder endpoint available')\n        nt_ks = self.volume_conn\n        volume = nt_ks.volumes.get(volume_id)\n        response = {'name': volume.display_name,\n                    'size': volume.size,\n                    'id': volume.id,\n                    'description': volume.display_description,\n                    'attachments': volume.attachments,\n                    'status': volume.status\n                    }\n        return response",
        "rewrite": "def _volume_get(self, volume_id):\n    if self.volume_conn is None:\n        raise SaltCloudSystemExit('No cinder endpoint available')\n    \n    nt_ks = self.volume_conn\n    volume = nt_ks.volumes.get(volume_id)\n    \n    response = {\n        'name': volume.display_name,\n        'size': volume.size,\n        'id': volume.id,\n        'description': volume.display_description,\n        'attachments': volume.attachments,\n        'status': volume.status\n    }\n    \n    return response"
    },
    {
        "original": "def check(frame) -> None:\n        \"\"\"\n        Check that this frame contains acceptable values.\n\n        Raise :exc:`~websockets.exceptions.WebSocketProtocolError` if this\n        frame contains incorrect values.\n\n        \"\"\"\n        # The first parameter is called `frame` rather than `self`,\n        # but it's the instance of class to which this method is bound.\n\n        if frame.rsv1 or frame.rsv2 or frame.rsv3:\n            raise WebSocketProtocolError(\"Reserved bits must be 0\")\n\n        if frame.opcode in DATA_OPCODES:\n            return\n        elif frame.opcode in CTRL_OPCODES:\n            if len(frame.data) > 125:\n                raise WebSocketProtocolError(\"Control frame too long\")\n            if not frame.fin:\n                raise WebSocketProtocolError(\"Fragmented control frame\")\n        else:\n            raise WebSocketProtocolError(f\"Invalid opcode: {frame.opcode}\")",
        "rewrite": "def check(frame) -> None:\n    if frame.rsv1 or frame.rsv2 or frame.rsv3:\n        raise WebSocketProtocolError(\"Reserved bits must be 0\")\n\n    if frame.opcode in DATA_OPCODES:\n        return\n    elif frame.opcode in CTRL_OPCODES:\n        if len(frame.data) > 125:\n            raise WebSocketProtocolError(\"Control frame too long\")\n        if not frame.fin:\n            raise WebSocketProtocolError(\"Fragmented control frame\")\n    else:\n        raise WebSocketProtocolError(f\"Invalid opcode: {frame.opcode}\")"
    },
    {
        "original": "def sample_measurements(\n            self,\n            indices: List[int],\n            repetitions: int=1) -> List[List[bool]]:\n        \"\"\"Samples from measurements in the computational basis.\n\n        Note that this does not collapse the wave function.\n\n        Args:\n            indices: Which qubits are measured.\n\n        Returns:\n            Measurement results with True corresponding to the |1> state.\n            The outer list is for repetitions, and the inner corresponds to\n            measurements ordered by the input indices.\n\n        Raises:\n            ValueError if repetitions is less than one.\n        \"\"\"\n        # Stepper uses little endian while sample_state uses big endian.\n        reversed_indices = [self._num_qubits - 1 - index for index in indices]\n        return sim.sample_state_vector(self._current_state(), reversed_indices,\n                                       repetitions)",
        "rewrite": "def sample_measurements(\n        self,\n        indices: List[int],\n        repetitions: int=1) -> List[List[bool]]:\n        \n    reversed_indices = [self._num_qubits - 1 - index for index in indices]    \n    return sim.sample_state_vector(self._current_state(), reversed_indices, repetitions)"
    },
    {
        "original": "def CreateCampaignWithBiddingStrategy(client, bidding_strategy_id, budget_id):\n  \"\"\"Create a Campaign with a Shared Bidding Strategy.\n\n  Args:\n    client: AdWordsClient the client to run the example with.\n    bidding_strategy_id: string the bidding strategy ID to use.\n    budget_id: string the shared budget ID to use.\n\n  Returns:\n    dict An object representing a campaign.\n  \"\"\"\n  # Initialize appropriate service.\n  campaign_service = client.GetService('CampaignService', version='v201809')\n\n  # Create campaign.\n  campaign = {\n      'name': 'Interplanetary Cruise #%s' % uuid.uuid4(),\n      'budget': {\n          'budgetId': budget_id\n      },\n      'biddingStrategyConfiguration': {\n          'biddingStrategyId': bidding_strategy_id\n      },\n      'advertisingChannelType': 'SEARCH',\n      'networkSetting': {\n          'targetGoogleSearch': 'true',\n          'targetSearchNetwork': 'true',\n          'targetContentNetwork': 'true'\n      }\n  }\n\n  # Create operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': campaign\n  }\n\n  response = campaign_service.mutate([operation])\n  new_campaign = response['value'][0]\n\n  print ('Campaign with name \"%s\", ID \"%s\" and bidding scheme ID \"%s\" '\n         'was created.' %\n         (new_campaign['name'], new_campaign['id'],\n          new_campaign['biddingStrategyConfiguration']['biddingStrategyId']))\n\n  return new_campaign",
        "rewrite": "def create_campaign_with_bidding_strategy(client, bidding_strategy_id, budget_id):\n    campaign_service = client.GetService('CampaignService', version='v201809')\n\n    campaign = {\n        'name': 'Interplanetary Cruise #%s' % uuid.uuid4(),\n        'budget': {\n            'budgetId': budget_id\n        },\n        'biddingStrategyConfiguration': {\n            'biddingStrategyId': bidding_strategy_id\n        },\n        'advertisingChannelType': 'SEARCH',\n        'networkSetting': {\n            'targetGoogleSearch': 'true',\n            'targetSearchNetwork': 'true',\n            'targetContentNetwork': 'true'\n        }\n    }\n\n    operation = {\n        'operator': 'ADD',\n        'operand': campaign\n    }\n\n    response = campaign_service.mutate([operation])\n    new_campaign = response['value'][0]\n\n    print('Campaign with name \"%s\", ID \"%s\" and bidding scheme ID \"%s\" '\n          'was created.' %\n          (new_campaign['name'], new_campaign['id'],\n           new_campaign['biddingStrategyConfiguration']['biddingStrategyId']))\n\n    return new_campaign"
    },
    {
        "original": "def schedule_from_proto_dicts(\n        device: 'xmon_device.XmonDevice',\n        ops: Iterable[Dict],\n) -> Schedule:\n    \"\"\"Convert proto dictionaries into a Schedule for the given device.\"\"\"\n    scheduled_ops = []\n    last_time_picos = 0\n    for op in ops:\n        delay_picos = 0\n        if 'incremental_delay_picoseconds' in op:\n            delay_picos = op['incremental_delay_picoseconds']\n        time_picos = last_time_picos + delay_picos\n        last_time_picos = time_picos\n        xmon_op = xmon_op_from_proto_dict(op)\n        scheduled_ops.append(ScheduledOperation.op_at_on(\n            operation=xmon_op,\n            time=Timestamp(picos=time_picos),\n            device=device,\n        ))\n    return Schedule(device, scheduled_ops)",
        "rewrite": "def schedule_from_proto_dicts(device: 'xmon_device.XmonDevice', ops: Iterable[Dict]) -> Schedule:\n    scheduled_ops = []\n    last_time_picos = 0\n    for op in ops:\n        delay_picos = op.get('incremental_delay_picoseconds', 0)\n        time_picos = last_time_picos + delay_picos\n        last_time_picos = time_picos\n        xmon_op = xmon_op_from_proto_dict(op)\n        scheduled_ops.append(ScheduledOperation.op_at_on(operation=xmon_op, time=Timestamp(picos=time_picos), device=device))\n    return Schedule(device, scheduled_ops)"
    },
    {
        "original": "def _build_machine_uri(machine, cwd):\n    \"\"\"\n    returns string used to fetch id names from the sdb store.\n\n    the cwd and machine name are concatenated with '?' which should\n    never collide with a Salt node id -- which is important since we\n    will be storing both in the same table.\n    \"\"\"\n    key = '{}?{}'.format(machine, os.path.abspath(cwd))\n    return _build_sdb_uri(key)",
        "rewrite": "def _build_machine_uri(machine, cwd):\n    key = '{}?{}'.format(machine, os.path.abspath(cwd))\n    return _build_sdb_uri(key)"
    },
    {
        "original": "def create_cache_security_group(name, region=None, key=None, keyid=None, profile=None, **args):\n    \"\"\"\n    Create a cache security group.\n\n    Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_elasticache.create_cache_security_group mycachesecgrp Description='My Cache Security Group'\n    \"\"\"\n    return _create_resource(name, name_param='CacheSecurityGroupName', desc='cache security group',\n                            res_type='cache_security_group',\n                            region=region, key=key, keyid=keyid, profile=profile, **args)",
        "rewrite": "def create_cache_security_group(name, region=None, key=None, keyid=None, profile=None, **args):\n    \"\"\"\n    Create a cache security group.\n\n    Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_elasticache.create_cache_security_group mycachesecgrp Description='My Cache Security Group'\n    \"\"\"\n    return _create_resource(name, name_param='CacheSecurityGroupName', desc='cache security group',\n                            res_type='cache_security_group',\n                            region=region, key=key, keyid=keyid, profile=profile, **args)"
    },
    {
        "original": "def xmlinfo(self, id):\n    \"\"\"Return the XML info record for the given item\"\"\"\n    self._update_index()\n    for package in self._index.findall('packages/package'):\n      if package.get('id') == id:\n        return package\n    for collection in self._index.findall('collections/collection'):\n      if collection.get('id') == id:\n        return collection\n    raise ValueError('Package %r not found in index' % id)",
        "rewrite": "def xmlinfo(self, id):\n    self._update_index()\n    for package in self._index.findall('packages/package'):\n        if package.get('id') == id:\n            return package\n    for collection in self._index.findall('collections/collection'):\n        if collection.get('id') == id:\n            return collection\n    raise ValueError(f'Package {id} not found in index')"
    },
    {
        "original": "def _merge_states(self, states):\n        \"\"\"\n        Merges a list of states.\n\n        :param states:      the states to merge\n        :returns SimState:  the resulting state\n        \"\"\"\n\n        if self._hierarchy:\n            optimal, common_history, others = self._hierarchy.most_mergeable(states)\n        else:\n            optimal, common_history, others = states, None, []\n\n        if len(optimal) >= 2:\n            # We found optimal states (states that share a common ancestor) to merge.\n            # Compute constraints for each state starting from the common ancestor,\n            # and use them as merge conditions.\n            constraints = [s.history.constraints_since(common_history) for s in optimal]\n\n            o = optimal[0]\n            m, _, _ = o.merge(*optimal[1:],\n                              merge_conditions=constraints,\n                              common_ancestor=common_history.strongref_state\n                              )\n\n        else:\n            l.warning(\n                \"Cannot find states with common history line to merge. Fall back to the naive merging strategy \"\n                \"and merge all states.\"\n                )\n            s = states[0]\n            m, _, _ = s.merge(*states[1:])\n\n            others = []\n\n        if self._hierarchy:\n            self._hierarchy.add_state(m)\n\n        if len(others):\n            others.append(m)\n            return self._merge_states(others)\n        else:\n            return m",
        "rewrite": "def _merge_states(self, states):\n    if self._hierarchy:\n        optimal, common_history, others = self._hierarchy.most_mergeable(states)\n    else:\n        optimal, common_history, others = states, None, []\n    \n    if len(optimal) >= 2:\n        constraints = [s.history.constraints_since(common_history) for s in optimal]\n        o = optimal[0]\n        m, _, _ = o.merge(*optimal[1:], merge_conditions=constraints, common_ancestor=common_history.strongref_state)\n    else:\n        l.warning(\"Cannot find states with common history line to merge. Fall back to the naive merging strategy and merge all states.\")\n        s = states[0]\n        m, _, _ = s.merge(*states[1:])\n        others = []\n    \n    if self._hierarchy:\n        self._hierarchy.add_state(m)\n    \n    if len(others):\n        others.append(m)\n        return self._merge_states(others)\n    else:\n        return m"
    },
    {
        "original": "def list_values(hive, key=None, use_32bit_registry=False, include_default=True):\n    r\"\"\"\n    Enumerates the values in a registry key or hive.\n\n    Args:\n\n        hive (str):\n            The name of the hive. Can be one of the following:\n\n                - HKEY_LOCAL_MACHINE or HKLM\n                - HKEY_CURRENT_USER or HKCU\n                - HKEY_USER or HKU\n                - HKEY_CLASSES_ROOT or HKCR\n                - HKEY_CURRENT_CONFIG or HKCC\n\n        key (str):\n            The key (looks like a path) to the value name. If a key is not\n            passed, the values under the hive will be returned.\n\n        use_32bit_registry (bool):\n            Accesses the 32bit portion of the registry on 64 bit installations.\n            On 32bit machines this is ignored.\n\n        include_default (bool):\n            Toggle whether to include the '(Default)' value.\n\n    Returns:\n        list: A list of values under the hive or key.\n\n    CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' reg.list_values HKLM 'SYSTEM\\\\CurrentControlSet\\\\Services\\\\Tcpip'\n    \"\"\"\n    return __utils__['reg.list_values'](hive=hive,\n                                        key=key,\n                                        use_32bit_registry=use_32bit_registry,\n                                        include_default=include_default)",
        "rewrite": "def list_values(hive, key=None, use_32bit_registry=False, include_default=True):\n    return __utils__['reg.list_values'](hive=hive, key=key, use_32bit_registry=use_32bit_registry, include_default=include_default)"
    },
    {
        "original": "def AddBlob(self, blob_id, length):\n    \"\"\"Add another blob to this image using its hash.\n\n    Once a blob is added that is smaller than the chunksize we finalize the\n    file, since handling adding more blobs makes the code much more complex.\n\n    Args:\n      blob_id: rdf_objects.BlobID object.\n      length: int length of blob\n\n    Raises:\n      IOError: if blob has been finalized.\n    \"\"\"\n    if self.finalized and length > 0:\n      raise IOError(\"Can't add blobs to finalized BlobImage\")\n\n    self.content_dirty = True\n    self.index.seek(0, 2)\n    self.index.write(blob_id.AsBytes())\n    self.size += length\n\n    if length < self.chunksize:\n      self.finalized = True",
        "rewrite": "def AddBlob(self, blob_id, length):\n    if self.finalized and length > 0:\n        raise IOError(\"Can't add blobs to finalized BlobImage\")\n\n    self.content_dirty = True\n    self.index.seek(0, 2)\n    self.index.write(blob_id.AsBytes())\n    self.size += length\n\n    if length < self.chunksize:\n        self.finalized = True"
    },
    {
        "original": "def balance_scores_and_dont_scale(cat_scores, not_cat_scores):\n\t\t\"\"\"\n\t\tmedian = np.median(cat_scores)\n\t\tscores = np.zeros(len(cat_scores)).astype(np.float)\n\t\tscores[cat_scores > median] = cat_scores[cat_scores > median]\n\t\tnot_cat_mask = cat_scores < median if median != 0 else cat_scores <= median\n\t\tscores[not_cat_mask] = -not_cat_scores[not_cat_mask]\n\t\t\"\"\"\n\t\tscores = np.zeros(len(cat_scores)).astype(np.float)\n\t\tscores[cat_scores > not_cat_scores] = cat_scores[cat_scores > not_cat_scores]\n\t\tscores[cat_scores < not_cat_scores] = -not_cat_scores[cat_scores < not_cat_scores]\n\t\treturn scores",
        "rewrite": "import numpy as np\n\ndef balance_scores_and_dont_scale(cat_scores, not_cat_scores):\n    scores = np.zeros(len(cat_scores)).astype(np.float)\n    scores[cat_scores > not_cat_scores] = cat_scores[cat_scores > not_cat_scores]\n    scores[cat_scores < not_cat_scores] = -not_cat_scores[cat_scores < not_cat_scores]\n    return scores"
    },
    {
        "original": "def get_rudder_scores_vs_background(self):\n        \"\"\"\n        Returns\n        -------\n        pd.DataFrame of rudder scores vs background\n        \"\"\"\n        df = self.get_term_and_background_counts()\n        corpus_percentiles = self._get_percentiles_from_freqs(df['corpus'])\n        background_percentiles = self._get_percentiles_from_freqs(df['background'])\n        df['Rudder'] = (self._get_rudder_scores_for_percentile_pair(corpus_percentiles,\n                                                                    background_percentiles))\n        df = df.sort_values(by='Rudder', ascending=True)\n        return df",
        "rewrite": "def get_rudder_scores_vs_background(self):\n    df = self.get_term_and_background_counts()\n    corpus_percentiles = self._get_percentiles_from_freqs(df['corpus'])\n    background_percentiles = self._get_percentiles_from_freqs(df['background'])\n    df['Rudder'] = (self._get_rudder_scores_for_percentile_pair(corpus_percentiles,\n                                                                background_percentiles))\n    df = df.sort_values(by='Rudder', ascending=True)\n    return df"
    },
    {
        "original": "def _create_memory_variable(self, action, addr, addrs):\n        \"\"\"\n        Create a SimStackVariable or SimMemoryVariable based on action objects and its address.\n\n        :param SimAction action: The action to work with.\n        :param int addr:         The address of the memory variable in creation.\n        :param list addrs:       A list of all addresses that the action was effective on.\n        :return:\n        \"\"\"\n\n        variable = None\n        if len(addrs) == 1 and len(action.addr.tmp_deps) == 1:\n            addr_tmp = list(action.addr.tmp_deps)[0]\n            if addr_tmp in self._temp_register_symbols:\n                # it must be a stack variable\n                sort, offset = self._temp_register_symbols[addr_tmp]\n                variable = SimStackVariable(offset, action.size.ast // 8, base=sort, base_addr=addr - offset)\n\n        if variable is None:\n            variable = SimMemoryVariable(addr, action.size.ast // 8)\n\n        return variable",
        "rewrite": "def _create_memory_variable(self, action, addr, addrs):\n    variable = None\n    if len(addrs) == 1 and len(action.addr.tmp_deps) == 1:\n        addr_tmp = list(action.addr.tmp_deps)[0]\n        if addr_tmp in self._temp_register_symbols:\n            sort, offset = self._temp_register_symbols[addr_tmp]\n            variable = SimStackVariable(offset, action.size.ast // 8, base=sort, base_addr=addr - offset)\n    if variable is None:\n        variable = SimMemoryVariable(addr, action.size.ast // 8)\n    return variable"
    },
    {
        "original": "def remove_content_history_in_cloud(self, page_id, version_id):\n        \"\"\"\n        Remove content history. It works in CLOUD\n        :param page_id:\n        :param version_id:\n        :return:\n        \"\"\"\n        url = 'rest/api/content/{id}/version/{versionId}'.format(id=page_id, versionId=version_id)\n        self.delete(url)",
        "rewrite": "def remove_content_history_in_cloud(self, page_id, version_id):\n    url = 'rest/api/content/{id}/version/{versionId}'.format(id=page_id, versionId=version_id)\n    self.delete(url)"
    },
    {
        "original": "def ParseFileHash(hash_obj, result):\n    \"\"\"Parses Hash rdfvalue into ExportedFile's fields.\"\"\"\n    if hash_obj.HasField(\"md5\"):\n      result.hash_md5 = str(hash_obj.md5)\n\n    if hash_obj.HasField(\"sha1\"):\n      result.hash_sha1 = str(hash_obj.sha1)\n\n    if hash_obj.HasField(\"sha256\"):\n      result.hash_sha256 = str(hash_obj.sha256)\n\n    if hash_obj.HasField(\"pecoff_md5\"):\n      result.pecoff_hash_md5 = str(hash_obj.pecoff_md5)\n\n    if hash_obj.HasField(\"pecoff_sha1\"):\n      result.pecoff_hash_sha1 = str(hash_obj.pecoff_sha1)\n\n    if hash_obj.HasField(\"signed_data\"):\n      StatEntryToExportedFileConverter.ParseSignedData(hash_obj.signed_data[0],\n                                                       result)",
        "rewrite": "def ParseFileHash(hash_obj, result):\n    if hash_obj.HasField(\"md5\"):\n        result.hash_md5 = str(hash_obj.md5)\n\n    if hash_obj.HasField(\"sha1\"):\n        result.hash_sha1 = str(hash_obj.sha1)\n\n    if hash_obj.HasField(\"sha256\"):\n        result.hash_sha256 = str(hash_obj.sha256)\n\n    if hash_obj.HasField(\"pecoff_md5\"):\n        result.pecoff_hash_md5 = str(hash_obj.pecoff_md5)\n\n    if hash_obj.HasField(\"pecoff_sha1\"):\n        result.pecoff_hash_sha1 = str(hash_obj.pecoff_sha1)\n\n    if hash_obj.HasField(\"signed_data\"):\n        StatEntryToExportedFileConverter.ParseSignedData(hash_obj.signed_data[0], result)"
    },
    {
        "original": "def shorthand(self):\n        \"\"\"Return the 6-tuple (a,b,c,d,e,f) that describes this matrix\"\"\"\n        return (self.a, self.b, self.c, self.d, self.e, self.f)",
        "rewrite": "def shorthand(self):\n    return (self.a, self.b, self.c, self.d, self.e, self.f)"
    },
    {
        "original": "def user_config_dir():\n    r\"\"\"Return the per-user config dir (full path).\n\n    - Linux, *BSD, SunOS: ~/.config/glances\n    - macOS: ~/Library/Application Support/glances\n    - Windows: %APPDATA%\\glances\n    \"\"\"\n    if WINDOWS:\n        path = os.environ.get('APPDATA')\n    elif MACOS:\n        path = os.path.expanduser('~/Library/Application Support')\n    else:\n        path = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n    if path is None:\n        path = ''\n    else:\n        path = os.path.join(path, 'glances')\n\n    return path",
        "rewrite": "def user_config_dir():\n    if WINDOWS:\n        path = os.environ.get('APPDATA')\n    elif MACOS:\n        path = os.path.expanduser('~/Library/Application Support')\n    else:\n        path = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n    \n    return os.path.join(path, 'glances') if path else ''"
    },
    {
        "original": "def ensure_sink(self):\n        \"\"\"Ensure the log sink and its pub sub topic exist.\"\"\"\n        topic_info = self.pubsub.ensure_topic()\n        scope, sink_path, sink_info = self.get_sink(topic_info)\n        client = self.session.client('logging', 'v2', '%s.sinks' % scope)\n        try:\n            sink = client.execute_command('get', {'sinkName': sink_path})\n        except HttpError as e:\n            if e.resp.status != 404:\n                raise\n            sink = client.execute_command('create', sink_info)\n        else:\n            delta = delta_resource(sink, sink_info['body'])\n            if delta:\n                sink_info['updateMask'] = ','.join(delta)\n                sink_info['sinkName'] = sink_path\n                sink_info.pop('parent')\n                sink = client.execute_command('update', sink_info)\n            else:\n                return sink_path\n\n        self.pubsub.ensure_iam(publisher=sink['writerIdentity'])\n        return sink_path",
        "rewrite": "def ensure_sink(self):\n    topic_info = self.pubsub.ensure_topic()\n    scope, sink_path, sink_info = self.get_sink(topic_info)\n    client = self.session.client('logging', 'v2', f'{scope}.sinks')\n    try:\n        sink = client.execute_command('get', {'sinkName': sink_path})\n    except HttpError as e:\n        if e.resp.status != 404:\n            raise\n        sink = client.execute_command('create', sink_info)\n    else:\n        delta = delta_resource(sink, sink_info['body'])\n        if delta:\n            sink_info['updateMask'] = ','.join(delta)\n            sink_info['sinkName'] = sink_path\n            sink_info.pop('parent')\n            sink = client.execute_command('update', sink_info)\n        else:\n            return sink_path\n\n    self.pubsub.ensure_iam(publisher=sink['writerIdentity'])\n    return sink_path"
    },
    {
        "original": "def handle_channel_disconnected(self):\n        \"\"\" Handles a channel being disconnected. \"\"\"\n        for namespace in self.app_namespaces:\n            if namespace in self._handlers:\n                self._handlers[namespace].channel_disconnected()\n\n        self.app_namespaces = []\n        self.destination_id = None\n        self.session_id = None",
        "rewrite": "def handle_channel_disconnected(self):\n    for namespace in self.app_namespaces:\n        if namespace in self._handlers:\n            self._handlers[namespace].channel_disconnected()\n\n    self.app_namespaces.clear()\n    self.destination_id = None\n    self.session_id = None"
    },
    {
        "original": "def end_profiling(profiler, filename, sorting=None):\n    \"\"\"\n    Helper function to stop the profiling process and write out the profiled\n    data into the given filename. Before this, sort the stats by the passed sorting.\n\n    :param profiler: An already started profiler (probably by start_profiling).\n    :type profiler: cProfile.Profile\n    :param filename: The name of the output file to save the profile.\n    :type filename: basestring\n    :param sorting: The sorting of the statistics passed to the sort_stats function.\n    :type sorting: basestring\n\n    :return: None\n    :rtype: None\n\n    Start and stop the profiler with:\n\n    >>> profiler = start_profiling()\n    >>> # Do something you want to profile\n    >>> end_profiling(profiler, \"out.txt\", \"cumulative\")\n    \"\"\"\n    profiler.disable()\n    s = six.StringIO()\n    ps = pstats.Stats(profiler, stream=s).sort_stats(sorting)\n    ps.print_stats()\n\n    with open(filename, \"w+\") as f:\n        _logger.info(\"[calculate_ts_features] Finished profiling of time series feature extraction\")\n        f.write(s.getvalue())",
        "rewrite": "def end_profiling(profiler, filename, sorting=None):\n    profiler.disable()\n    s = six.StringIO()\n    ps = pstats.Stats(profiler, stream=s).sort_stats(sorting)\n    ps.print_stats()\n\n    with open(filename, \"w+\") as f:\n        f.write(s.getvalue())"
    },
    {
        "original": "def main():\n    \"\"\"\n    Launches translation (inference).\n    Inference is executed on a single GPU, implementation supports beam search\n    with length normalization and coverage penalty.\n    \"\"\"\n    args = parse_args()\n    utils.set_device(args.cuda, args.local_rank)\n    utils.init_distributed(args.cuda)\n    setup_logging()\n\n    if args.env:\n        utils.log_env_info()\n\n    logging.info(f'Run arguments: {args}')\n\n    if not args.cuda and torch.cuda.is_available():\n        warnings.warn('cuda is available but not enabled')\n    if not args.cudnn:\n        torch.backends.cudnn.enabled = False\n\n    # load checkpoint and deserialize to CPU (to save GPU memory)\n    checkpoint = torch.load(args.model, map_location={'cuda:0': 'cpu'})\n\n    # build GNMT model\n    tokenizer = Tokenizer()\n    tokenizer.set_state(checkpoint['tokenizer'])\n    vocab_size = tokenizer.vocab_size\n    model_config = checkpoint['model_config']\n    model_config['batch_first'] = args.batch_first\n    model = GNMT(vocab_size=vocab_size, **model_config)\n    model.load_state_dict(checkpoint['state_dict'])\n\n    for (math, batch_size, beam_size) in product(args.math, args.batch_size,\n                                                 args.beam_size):\n        logging.info(f'math: {math}, batch size: {batch_size}, '\n                     f'beam size: {beam_size}')\n        if math == 'fp32':\n            dtype = torch.FloatTensor\n        if math == 'fp16':\n            dtype = torch.HalfTensor\n        model.type(dtype)\n\n        if args.cuda:\n            model = model.cuda()\n        model.eval()\n\n        # construct the dataset\n        test_data = TextDataset(src_fname=args.input,\n                                tokenizer=tokenizer,\n                                sort=args.sort)\n\n        # build the data loader\n        test_loader = test_data.get_loader(batch_size=batch_size,\n                                           batch_first=args.batch_first,\n                                           shuffle=False,\n                                           pad=True,\n                                           num_workers=0)\n\n        # build the translator object\n        translator = Translator(model=model,\n                                tokenizer=tokenizer,\n                                loader=test_loader,\n                                beam_size=beam_size,\n                                max_seq_len=args.max_seq_len,\n                                len_norm_factor=args.len_norm_factor,\n                                len_norm_const=args.len_norm_const,\n                                cov_penalty_factor=args.cov_penalty_factor,\n                                cuda=args.cuda,\n                                print_freq=args.print_freq,\n                                dataset_dir=args.dataset_dir)\n\n        # execute the inference\n        translator.run(calc_bleu=args.bleu, eval_path=args.output,\n                       reference_path=args.reference, summary=True)",
        "rewrite": "def main():\n    args = parse_args()\n    utils.set_device(args.cuda, args.local_rank)\n    utils.init_distributed(args.cuda)\n    setup_logging()\n\n    if args.env:\n        utils.log_env_info()\n\n    logging.info(f'Run arguments: {args}')\n\n    if not args.cuda and torch.cuda.is_available():\n        warnings.warn('cuda is available but not enabled')\n    if not args.cudnn:\n        torch.backends.cudnn.enabled = False\n\n    checkpoint = torch.load(args.model, map_location={'cuda:0': 'cpu'})\n\n    tokenizer = Tokenizer()\n    tokenizer.set_state(checkpoint['tokenizer'])\n    vocab_size = tokenizer.vocab_size\n    model_config = checkpoint['model_config']\n    model_config['batch_first'] = args.batch_first\n    model = GNMT(vocab_size=vocab_size, **model_config)\n    model.load_state_dict(checkpoint['state_dict'])\n\n    for (math, batch_size, beam_size) in product(args.math, args.batch_size, args.beam_size):\n        logging.info(f'math: {math}, batch size: {batch_size}, beam size: {beam_size}')\n        if math == 'fp32':\n            dtype = torch.FloatTensor\n        if math == 'fp16':\n            dtype = torch.HalfTensor\n        model.type(dtype)\n\n        if args.cuda:\n            model = model.cuda()\n        model.eval()\n\n        test_data = TextDataset(src_fname=args.input, tokenizer=tokenizer, sort=args.sort)\n\n        test_loader = test_data.get_loader(batch_size=batch_size, \n                                           batch_first=args.batch_first, \n                                           shuffle=False, \n                                           pad=True, \n                                           num_workers=0)\n\n        translator = Translator(model=model, tokenizer=tokenizer, loader=test_loader, \n                                beam_size=beam_size, max_seq_len=args.max_seq_len, \n                                len_norm_factor=args.len_norm_factor, len_norm_const=args.len_norm_const, \n                                cov_penalty_factor=args.cov_penalty_factor, cuda=args.cuda, \n                                print_freq=args.print_freq, dataset_dir=args.dataset_dir)\n\n        translator.run(calc_bleu=args.bleu, eval_path=args.output, reference_path=args.reference, summary=True)"
    },
    {
        "original": "def set_all_variables(self, delu_dict, delu_default):\n        \"\"\"\n        Sets all chemical potential values and returns a dictionary where\n            the key is a sympy Symbol and the value is a float (chempot).\n\n        Args:\n            entry (SlabEntry): Computed structure entry of the slab\n            delu_dict (Dict): Dictionary of the chemical potentials to be set as\n                constant. Note the key should be a sympy Symbol object of the\n                format: Symbol(\"delu_el\") where el is the name of the element.\n            delu_default (float): Default value for all unset chemical potentials\n\n        Returns:\n            Dictionary of set chemical potential values\n        \"\"\"\n\n        # Set up the variables\n        all_delu_dict = {}\n        for du in self.list_of_chempots:\n            if delu_dict and du in delu_dict.keys():\n                all_delu_dict[du] = delu_dict[du]\n            elif du == 1:\n                all_delu_dict[du] = du\n            else:\n                all_delu_dict[du] = delu_default\n\n        return all_delu_dict",
        "rewrite": "def set_all_variables(self, delu_dict, delu_default):\n    all_delu_dict = {}\n    for du in self.list_of_chempots:\n        if delu_dict and du in delu_dict.keys():\n            all_delu_dict[du] = delu_dict[du]\n        elif du == 1:\n            all_delu_dict[du] = du\n        else:\n            all_delu_dict[du] = delu_default\n    return all_delu_dict"
    },
    {
        "original": "def get_service_info(service_instance):\n    \"\"\"\n    Returns information of the vCenter or ESXi host\n\n    service_instance\n        The Service Instance from which to obtain managed object references.\n    \"\"\"\n    try:\n        return service_instance.content.about\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)",
        "rewrite": "def get_service_info(service_instance):\n    try:\n        return service_instance.content.about\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)"
    },
    {
        "original": "def create_connection(address):\n    \"\"\"\n    Wrapper for socket.create_connection() function.\n\n    If *address* (a 2-tuple ``(host, port)``) contains a valid IPv4/v6\n    address, passes *address* to socket.create_connection().\n    If *host* is valid path to Unix Domain socket, tries to connect to\n    the server listening on the given socket.\n\n    :param address: IP address or path to Unix Domain socket.\n    :return: Socket instance.\n    \"\"\"\n    host, _port = address\n\n    if ip.valid_ipv4(host) or ip.valid_ipv6(host):\n        return socket.create_connection(address)\n    elif os.path.exists(host):\n        sock = None\n        try:\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n            sock.connect(host)\n        except socket.error as e:\n            if sock is not None:\n                sock.close()\n            raise e\n        return sock\n    else:\n        raise ValueError('Invalid IP address or Unix Socket: %s' % host)",
        "rewrite": "def create_connection(address):\n    host, _port = address\n\n    if ip.valid_ipv4(host) or ip.valid_ipv6(host):\n        return socket.create_connection(address)\n    elif os.path.exists(host):\n        sock = None\n        try:\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n            sock.connect(host)\n        except socket.error as e:\n            if sock is not None:\n                sock.close()\n            raise e\n        return sock\n    else:\n        raise ValueError('Invalid IP address or Unix Socket: %s' % host)"
    },
    {
        "original": "def setKey(self, key):\n\t\t\"\"\"Will set the crypting key for this object. Either 16 or 24 bytes long.\"\"\"\n\t\tself.key_size = 24  # Use DES-EDE3 mode\n\t\tif len(key) != self.key_size:\n\t\t\tif len(key) == 16: # Use DES-EDE2 mode\n\t\t\t\tself.key_size = 16\n\t\t\telse:\n\t\t\t\traise ValueError(\"Invalid triple DES key size. Key must be either 16 or 24 bytes long\")\n\t\tif self.getMode() == CBC:\n\t\t\tif not self.getIV():\n\t\t\t\t# Use the first 8 bytes of the key\n\t\t\t\tself._iv = key[:self.block_size]\n\t\t\tif len(self.getIV()) != self.block_size:\n\t\t\t\traise ValueError(\"Invalid IV, must be 8 bytes in length\")\n\t\tself.__key1 = des(key[:8], self._mode, self._iv,\n\t\t\t\t  self._padding, self._padmode)\n\t\tself.__key2 = des(key[8:16], self._mode, self._iv,\n\t\t\t\t  self._padding, self._padmode)\n\t\tif self.key_size == 16:\n\t\t\tself.__key3 = self.__key1\n\t\telse:\n\t\t\tself.__key3 = des(key[16:], self._mode, self._iv,\n\t\t\t\t\t  self._padding, self._padmode)\n\t\t_baseDes.setKey(self, key)",
        "rewrite": "def setKey(self, key):\n\tself.key_size = 24\n\tif len(key) != self.key_size:\n\t\tif len(key) == 16:\n\t\t\tself.key_size = 16\n\t\telse:\n\t\t\traise ValueError(\"Invalid triple DES key size. Key must be either 16 or 24 bytes long\")\n\tif self.getMode() == CBC:\n\t\tif not self.getIV():\n\t\t\tself._iv = key[:self.block_size]\n\t\tif len(self.getIV()) != self.block_size:\n\t\t\traise ValueError(\"Invalid IV, must be 8 bytes in length\")\n\tself.__key1 = des(key[:8], self._mode, self._iv, self._padding, self._padmode)\n\tself.__key2 = des(key[8:16], self._mode, self._iv, self._padding, self._padmode)\n\tif self.key_size == 16:\n\t\tself.__key3 = self.__key1\n\telse:\n\t\tself.__key3 = des(key[16:], self._mode, self._iv, self._padding, self._padmode)\n\t_baseDes.setKey(self, key)"
    },
    {
        "original": "def ArtifactsFromYaml(self, yaml_content):\n    \"\"\"Get a list of Artifacts from yaml.\"\"\"\n    raw_list = yaml.ParseMany(yaml_content)\n\n    # TODO(hanuszczak): I am very sceptical about that \"doing the right thing\"\n    # below. What are the real use cases?\n\n    # Try to do the right thing with json/yaml formatted as a list.\n    if (isinstance(raw_list, list) and len(raw_list) == 1 and\n        isinstance(raw_list[0], list)):\n      raw_list = raw_list[0]\n\n    # Convert json into artifact and validate.\n    valid_artifacts = []\n    for artifact_dict in raw_list:\n      # In this case we are feeding parameters directly from potentially\n      # untrusted yaml/json to our RDFValue class. However, safe_load ensures\n      # these are all primitive types as long as there is no other\n      # deserialization involved, and we are passing these into protobuf\n      # primitive types.\n      try:\n        artifact_value = rdf_artifacts.Artifact(**artifact_dict)\n        valid_artifacts.append(artifact_value)\n      except (TypeError, AttributeError, type_info.TypeValueError) as e:\n        name = artifact_dict.get(\"name\")\n        raise rdf_artifacts.ArtifactDefinitionError(\n            name, \"invalid definition\", cause=e)\n\n    return valid_artifacts",
        "rewrite": "def artifacts_from_yaml(self, yaml_content):\n    raw_list = yaml.ParseMany(yaml_content)\n\n    if isinstance(raw_list, list) and len(raw_list) == 1 and isinstance(raw_list[0], list):\n        raw_list = raw_list[0]\n\n    valid_artifacts = []\n    for artifact_dict in raw_list:\n        try:\n            artifact_value = rdf_artifacts.Artifact(**artifact_dict)\n            valid_artifacts.append(artifact_value)\n        except (TypeError, AttributeError, type_info.TypeValueError) as e:\n            name = artifact_dict.get(\"name\")\n            raise rdf_artifacts.ArtifactDefinitionError(\n                name, \"invalid definition\", cause=e)\n\n    return valid_artifacts"
    },
    {
        "original": "def _filter_and_bucket_subtokens(subtoken_counts, min_count):\n  \"\"\"Return a bucketed list of subtokens that are filtered by count.\n\n  Args:\n    subtoken_counts: defaultdict mapping subtokens to their counts\n    min_count: int count used to filter subtokens\n\n  Returns:\n    List of subtoken sets, where subtokens in set i have the same length=i.\n  \"\"\"\n  # Create list of buckets, where subtokens in bucket i have length i.\n  subtoken_buckets = []\n  for subtoken, count in six.iteritems(subtoken_counts):\n    if count < min_count:  # Filter out subtokens that don't appear enough\n      continue\n    while len(subtoken_buckets) <= len(subtoken):\n      subtoken_buckets.append(set())\n    subtoken_buckets[len(subtoken)].add(subtoken)\n  return subtoken_buckets",
        "rewrite": "def _filter_and_bucket_subtokens(subtoken_counts, min_count):\n    subtoken_buckets = []\n    \n    for subtoken, count in subtoken_counts.items():\n        if count < min_count:\n            continue\n        \n        while len(subtoken_buckets) <= len(subtoken):\n            subtoken_buckets.append(set())\n        \n        subtoken_buckets[len(subtoken)].add(subtoken)\n    \n    return subtoken_buckets"
    },
    {
        "original": "def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\n    \"\"\"Called after each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n      run_values: A SessionRunValues object.\n    \"\"\"\n    global_step = run_values.results\n\n    if self._timer.should_trigger_for_step(\n        global_step) and global_step > self._warm_steps:\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        # average examples per second is based on the total (accumulative)\n        # training steps and training time so far\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        # current examples per second is based on the elapsed training steps\n        # and training time per batch\n        current_examples_per_sec = self._batch_size * (\n            elapsed_steps / elapsed_time)\n        # Current examples/sec followed by average examples/sec\n        tf.logging.info('Batch [%g]:  current exp/sec = %g, average exp/sec = '\n                        '%g', self._total_steps, current_examples_per_sec,\n                        average_examples_per_sec)",
        "rewrite": "def after_run(self, run_context, run_values):  \n    global_step = run_values.results\n\n    if self._timer.should_trigger_for_step(global_step) and global_step > self._warm_steps:\n        elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(global_step)\n        if elapsed_time is not None:\n            self._step_train_time += elapsed_time\n            self._total_steps += elapsed_steps\n\n            average_examples_per_sec = self._batch_size * (self._total_steps / self._step_train_time)\n            current_examples_per_sec = self._batch_size * (elapsed_steps / elapsed_time)\n\n            tf.logging.info('Batch [%g]: current exp/sec = %g, average exp/sec = %g', self._total_steps, current_examples_per_sec, average_examples_per_sec)"
    },
    {
        "original": "def ParseMultiple(self, result_dicts):\n    \"\"\"Parse the WMI packages output.\"\"\"\n    for result_dict in result_dicts:\n      result = result_dict.ToDict()\n      winvolume = rdf_client_fs.WindowsVolume(\n          drive_letter=result.get(\"DeviceID\"),\n          drive_type=result.get(\"DriveType\"))\n\n      try:\n        size = int(result.get(\"Size\"))\n      except (ValueError, TypeError):\n        size = None\n\n      try:\n        free_space = int(result.get(\"FreeSpace\"))\n      except (ValueError, TypeError):\n        free_space = None\n\n      # Since we don't get the sector sizes from WMI, we just set them at 1 byte\n      yield rdf_client_fs.Volume(\n          windowsvolume=winvolume,\n          name=result.get(\"VolumeName\"),\n          file_system_type=result.get(\"FileSystem\"),\n          serial_number=result.get(\"VolumeSerialNumber\"),\n          sectors_per_allocation_unit=1,\n          bytes_per_sector=1,\n          total_allocation_units=size,\n          actual_available_allocation_units=free_space)",
        "rewrite": "def ParseMultiple(self, result_dicts):\n    for result_dict in result_dicts:\n        result = result_dict.ToDict()\n        winvolume = rdf_client_fs.WindowsVolume(\n            drive_letter=result.get(\"DeviceID\"),\n            drive_type=result.get(\"DriveType\"))\n\n        try:\n            size = int(result.get(\"Size\"))\n        except (ValueError, TypeError):\n            size = None\n\n        try:\n            free_space = int(result.get(\"FreeSpace\"))\n        except (ValueError, TypeError):\n            free_space = None\n\n        yield rdf_client_fs.Volume(\n            windowsvolume=winvolume,\n            name=result.get(\"VolumeName\"),\n            file_system_type=result.get(\"FileSystem\"),\n            serial_number=result.get(\"VolumeSerialNumber\"),\n            sectors_per_allocation_unit=1,\n            bytes_per_sector=1,\n            total_allocation_units=size,\n            actual_available_allocation_units=free_space)"
    },
    {
        "original": "def get_structure_by_id(self, cod_id, **kwargs):\n        \"\"\"\n        Queries the COD for a structure by id.\n\n        Args:\n            cod_id (int): COD id.\n            kwargs: All kwargs supported by\n                :func:`pymatgen.core.structure.Structure.from_str`.\n\n        Returns:\n            A Structure.\n        \"\"\"\n        r = requests.get(\"http://www.crystallography.net/cod/%s.cif\" % cod_id)\n        return Structure.from_str(r.text, fmt=\"cif\", **kwargs)",
        "rewrite": "def get_structure_by_id(self, cod_id, **kwargs):\n    r = requests.get(\"http://www.crystallography.net/cod/%s.cif\" % cod_id)\n    return Structure.from_str(r.text, fmt=\"cif\", **kwargs)"
    },
    {
        "original": "def AddHashEntry(self, hash_entry, timestamp):\n    \"\"\"Registers hash entry at a given timestamp.\"\"\"\n\n    if timestamp in self._hash_entries:\n      message = (\"Duplicated hash entry write for path '%s' of type '%s' at \"\n                 \"timestamp '%s'. Old: %s. New: %s.\")\n      message %= (\"/\".join(self._components), self._path_type, timestamp,\n                  self._hash_entries[timestamp], hash_entry)\n      raise db.Error(message)\n\n    if timestamp not in self._path_infos:\n      path_info = rdf_objects.PathInfo(\n          path_type=self._path_type,\n          components=self._components,\n          timestamp=timestamp,\n          hash_entry=hash_entry)\n      self.AddPathInfo(path_info)\n    else:\n      self._path_infos[timestamp].hash_entry = hash_entry",
        "rewrite": "def add_hash_entry(self, hash_entry, timestamp):\n    if timestamp in self._hash_entries:\n        message = f\"Duplicated hash entry write for path '{'/'.join(self._components)}' of type '{self._path_type}' at timestamp '{timestamp}'. Old: {self._hash_entries[timestamp]}. New: {hash_entry}.\"\n        raise db.Error(message)\n\n    if timestamp not in self._path_infos:\n        path_info = rdf_objects.PathInfo(\n            path_type=self._path_type,\n            components=self._components,\n            timestamp=timestamp,\n            hash_entry=hash_entry)\n        self.addPathInfo(path_info)\n    else:\n        self._path_infos[timestamp].hash_entry = hash_entry"
    },
    {
        "original": "def _draw_ap_score(self, score, label=None):\n        \"\"\"\n        Helper function to draw the AP score annotation\n        \"\"\"\n        label = label or \"Avg Precision={:0.2f}\".format(score)\n        if self.ap_score:\n            self.ax.axhline(\n                y=score, color=\"r\", ls=\"--\", label=label\n            )",
        "rewrite": "def _draw_ap_score(self, score, label=None):\n    label = label or \"Avg Precision={:0.2f}\".format(score)\n    if self.ap_score:\n        self.ax.axhline(\n            y=score, color=\"r\", ls=\"--\", label=label\n        )"
    },
    {
        "original": "def PopState(self, **_):\n    \"\"\"Pop the previous state from the stack.\"\"\"\n    try:\n      self.state = self.state_stack.pop()\n      if self.verbose:\n        logging.debug(\"Returned state to %s\", self.state)\n\n      return self.state\n    except IndexError:\n      self.Error(\"Tried to pop the state but failed - possible recursion error\")",
        "rewrite": "def PopState(self, **_):\n    try:\n        self.state = self.state_stack.pop()\n        if self.verbose:\n            logging.debug(\"Returned state to %s\", self.state)\n\n        return self.state\n    except IndexError:\n        self.Error(\"Tried to pop the state but failed - possible recursion error\")"
    },
    {
        "original": "async def store_their_did(wallet_handle: int,\n                          identity_json: str) -> None:\n    \"\"\"\n    Saves their DID for a pairwise connection in a secured Wallet,\n    so that it can be used to verify transaction.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param identity_json: Identity information as json. Example:\n        {\n           \"did\": string, (required)\n           \"verkey\": string (optional, if only pk is provided),\n           \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\n                  currently only 'ed25519' value is supported for this field)\n        }\n    :return: None\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"store_their_did: >>> wallet_handle: %r, identity_json: %r\",\n                 wallet_handle,\n                 identity_json)\n\n    if not hasattr(store_their_did, \"cb\"):\n        logger.debug(\"store_their_did: Creating callback\")\n        store_their_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_identity_json = c_char_p(identity_json.encode('utf-8'))\n\n    res = await do_call('indy_store_their_did',\n                        c_wallet_handle,\n                        c_identity_json,\n                        store_their_did.cb)\n\n    logger.debug(\"store_their_did: <<< res: %r\", res)\n    return res",
        "rewrite": "async def store_their_did(wallet_handle: int,\n                          identity_json: str) -> None:\n    \"\"\"\n    Saves their DID for a pairwise connection in a secured Wallet,\n    so that it can be used to verify transactions.\n\n    :param wallet_handle: Wallet handler (created by open_wallet).\n    :param identity_json: Identity information as JSON.\n    Example:\n        {\n           \"did\": string, (required)\n           \"verkey\": string (optional, if only public key is provided),\n           \"crypto_type\": string, (optional; if not set then ed25519 curve is used;\n                  currently only 'ed25519' value is supported for this field)\n        }\n    :return: None\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"store_their_did: >>> wallet_handle: %r, identity_json: %r\",\n                 wallet_handle,\n                 identity_json)\n\n    if not hasattr(store_their_did, \"cb\"):\n        logger.debug(\"store_their_did: Creating callback\")\n        store_their_did.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_identity_json = c_char_p(identity_json.encode('utf-8'))\n\n    res = await do_call('indy_store_their_did',\n                        c_wallet_handle,\n                        c_identity_json,\n                        store_their_did.cb)\n\n    logger.debug(\"store_their_did: <<< res: %r\", res)\n    return res"
    },
    {
        "original": "def reset(self):\n        \"\"\"\n        Reset the Quantum Abstract Machine to its initial state, which is particularly useful\n        when it has gotten into an unwanted state. This can happen, for example, if the QAM\n        is interrupted in the middle of a run.\n        \"\"\"\n        self._variables_shim = {}\n        self._executable = None\n        self._bitstrings = None\n\n        self.status = 'connected'",
        "rewrite": "def reset(self):\n    self._variables_shim = {}\n    self._executable = None\n    self._bitstrings = None\n\n    self.status = 'connected'"
    },
    {
        "original": "def mark_all_as_done(self, **kwargs):\n        \"\"\"Mark all the todos as done.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabTodoError: If the server failed to perform the request\n\n        Returns:\n            int: The number of todos maked done\n        \"\"\"\n        result = self.gitlab.http_post('/todos/mark_as_done', **kwargs)\n        try:\n            return int(result)\n        except ValueError:\n            return 0",
        "rewrite": "def mark_all_as_done(self, **kwargs):\n    result = self.gitlab.http_post('/todos/mark_as_done', **kwargs)\n    try:\n        return int(result)\n    except ValueError:\n        return 0"
    },
    {
        "original": "def _ssl_PRF(secret, seed, req_len):\n    \"\"\"\n    Provides the implementation of SSLv3 PRF function:\n\n     SSLv3-PRF(secret, seed) =\n        MD5(secret || SHA-1(\"A\" || secret || seed)) ||\n        MD5(secret || SHA-1(\"BB\" || secret || seed)) ||\n        MD5(secret || SHA-1(\"CCC\" || secret || seed)) || ...\n\n    req_len should not be more than  26 x 16 = 416.\n    \"\"\"\n    if req_len > 416:\n        warning(\"_ssl_PRF() is not expected to provide more than 416 bytes\")\n        return \"\"\n\n    d = [b\"A\", b\"B\", b\"C\", b\"D\", b\"E\", b\"F\", b\"G\", b\"H\", b\"I\", b\"J\", b\"K\", b\"L\",  # noqa: E501\n         b\"M\", b\"N\", b\"O\", b\"P\", b\"Q\", b\"R\", b\"S\", b\"T\", b\"U\", b\"V\", b\"W\", b\"X\",  # noqa: E501\n         b\"Y\", b\"Z\"]\n    res = b\"\"\n    hash_sha1 = _tls_hash_algs[\"SHA\"]()\n    hash_md5 = _tls_hash_algs[\"MD5\"]()\n    rounds = (req_len + hash_md5.hash_len - 1) // hash_md5.hash_len\n\n    for i in range(rounds):\n        label = d[i] * (i + 1)\n        tmp = hash_sha1.digest(label + secret + seed)\n        res += hash_md5.digest(secret + tmp)\n\n    return res[:req_len]",
        "rewrite": "def _ssl_PRF(secret, seed, req_len):\n    if req_len > 416:\n        warning(\"_ssl_PRF() is not expected to provide more than 416 bytes\")\n        return \"\"\n\n    d = [b\"A\", b\"B\", b\"C\", b\"D\", b\"E\", b\"F\", b\"G\", b\"H\", b\"I\", b\"J\", b\"K\", b\"L\",\n         b\"M\", b\"N\", b\"O\", b\"P\", b\"Q\", b\"R\", b\"S\", b\"T\", b\"U\", b\"V\", b\"W\", b\"X\",\n         b\"Y\", b\"Z\"]\n    res = b\"\"\n    hash_sha1 = _tls_hash_algs[\"SHA\"]()\n    hash_md5 = _tls_hash_algs[\"MD5\"]()\n    rounds = (req_len + hash_md5.hash_len - 1) // hash_md5.hash_len\n\n    for i in range(rounds):\n        label = d[i] * (i + 1)\n        tmp = hash_sha1.digest(label + secret + seed)\n        res += hash_md5.digest(secret + tmp)\n\n    return res[:req_len]"
    },
    {
        "original": "def apply_transformation(self, structure, return_ranked_list=False):\n        \"\"\"\n        Return either a single ordered structure or a sequence of all ordered\n        structures.\n\n        Args:\n            structure: Structure to order.\n            return_ranked_list (bool): Whether or not multiple structures are\n                returned. If return_ranked_list is a number, that number of\n                structures is returned.\n\n        Returns:\n            Depending on returned_ranked list, either a transformed structure\n            or a list of dictionaries, where each dictionary is of the form\n            {\"structure\" = .... , \"other_arguments\"}\n\n            The list of ordered structures is ranked by ewald energy / atom, if\n            the input structure is an oxidation state decorated structure.\n            Otherwise, it is ranked by number of sites, with smallest number of\n            sites first.\n        \"\"\"\n        try:\n            num_to_return = int(return_ranked_list)\n        except ValueError:\n            num_to_return = 1\n\n        if self.refine_structure:\n            finder = SpacegroupAnalyzer(structure, self.symm_prec)\n            structure = finder.get_refined_structure()\n\n        contains_oxidation_state = all(\n            [hasattr(sp, \"oxi_state\") and sp.oxi_state != 0 for sp in\n             structure.composition.elements]\n        )\n\n        structures = None\n\n        if structure.is_ordered:\n            warn(\"Enumeration skipped for structure with composition {} \"\n                 \"because it is ordered\".format(structure.composition))\n            structures = [structure.copy()]\n\n        if self.max_disordered_sites:\n            ndisordered = sum([1 for site in structure if not site.is_ordered])\n            if ndisordered > self.max_disordered_sites:\n                raise ValueError(\n                    \"Too many disordered sites! ({} > {})\".format(\n                        ndisordered, self.max_disordered_sites))\n            max_cell_sizes = range(self.min_cell_size, int(\n                math.floor(self.max_disordered_sites / ndisordered)) + 1)\n        else:\n            max_cell_sizes = [self.max_cell_size]\n\n        for max_cell_size in max_cell_sizes:\n            adaptor = EnumlibAdaptor(\n                structure, min_cell_size=self.min_cell_size,\n                max_cell_size=max_cell_size,\n                symm_prec=self.symm_prec, refine_structure=False,\n                enum_precision_parameter=self.enum_precision_parameter,\n                check_ordered_symmetry=self.check_ordered_symmetry,\n                timeout=self.timeout)\n            try:\n                adaptor.run()\n            except EnumError:\n                warn(\"Unable to enumerate for max_cell_size = %d\".format(\n                    max_cell_size))\n            structures = adaptor.structures\n            if structures:\n                break\n\n        if structures is None:\n            raise ValueError(\"Unable to enumerate\")\n\n        original_latt = structure.lattice\n        inv_latt = np.linalg.inv(original_latt.matrix)\n        ewald_matrices = {}\n        all_structures = []\n        for s in structures:\n            new_latt = s.lattice\n            transformation = np.dot(new_latt.matrix, inv_latt)\n            transformation = tuple([tuple([int(round(cell)) for cell in row])\n                                    for row in transformation])\n            if contains_oxidation_state and self.sort_criteria == \"ewald\":\n                if transformation not in ewald_matrices:\n                    s_supercell = structure * transformation\n                    ewald = EwaldSummation(s_supercell)\n                    ewald_matrices[transformation] = ewald\n                else:\n                    ewald = ewald_matrices[transformation]\n                energy = ewald.compute_sub_structure(s)\n                all_structures.append({\"num_sites\": len(s), \"energy\": energy,\n                                       \"structure\": s})\n            else:\n                all_structures.append({\"num_sites\": len(s), \"structure\": s})\n\n        def sort_func(s):\n            return s[\"energy\"] / s[\"num_sites\"] \\\n                if contains_oxidation_state and self.sort_criteria == \"ewald\" \\\n                else s[\"num_sites\"]\n\n        self._all_structures = sorted(all_structures, key=sort_func)\n\n        if return_ranked_list:\n            return self._all_structures[0:num_to_return]\n        else:\n            return self._all_structures[0][\"structure\"]",
        "rewrite": "def apply_transformation(self, structure, return_ranked_list=False):\n    try:\n        num_to_return = int(return_ranked_list)\n    except ValueError:\n        num_to_return = 1\n\n    if self.refine_structure:\n        finder = SpacegroupAnalyzer(structure, self.symm_prec)\n        structure = finder.get_refined_structure()\n\n    contains_oxidation_state = all([hasattr(sp, \"oxi_state\") and sp.oxi_state != 0 for sp in structure.composition.elements])\n\n    structures = None\n\n    if structure.is_ordered:\n        warn(\"Enumeration skipped for structure with composition {} because it is ordered\".format(structure.composition))\n        structures = [structure.copy()]\n\n    if self.max_disordered_sites:\n        ndisordered = sum([1 for site in structure if not site.is_ordered])\n        if ndisordered > self.max_disordered_sites:\n            raise ValueError(\"Too many disordered sites! ({} > {})\".format(ndisordered, self.max_disordered_sites))\n        max_cell_sizes = range(self.min_cell_size, int(math.floor(self.max_disordered_sites / ndisordered)) + 1)\n    else:\n        max_cell_sizes = [self.max_cell_size]\n\n    for max_cell_size in max_cell_sizes:\n        adaptor = EnumlibAdaptor(\n            structure, min_cell_size=self.min_cell_size,\n            max_cell_size=max_cell_size,\n            symm_prec=self.symm_prec, refine_structure=False,\n            enum_precision_parameter=self.enum_precision_parameter,\n            check_ordered_symmetry=self.check_ordered_symmetry,\n            timeout=self.timeout)\n        try:\n            adaptor.run()\n        except EnumError:\n            warn(\"Unable to enumerate for max_cell_size = {}\".format(max_cell_size))\n        structures = adaptor.structures\n        if structures:\n            break\n\n    if structures is None:\n        raise ValueError(\"Unable to enumerate\")\n\n    original_latt = structure.lattice\n    inv_latt = np.linalg.inv(original_latt.matrix)\n    ewald_matrices = {}\n    all_structures = []\n    for s in structures:\n        new_latt = s.lattice\n        transformation = np.dot(new_latt.matrix, inv_latt)\n        transformation = tuple([tuple([int(round(cell)) for cell in row]) for row in transformation])\n        if contains_oxidation_state and self.sort_criteria == \"ewald\":\n            if transformation not in ewald_matrices:\n                s_supercell = structure * transformation\n                ewald = EwaldSummation(s_supercell)\n                ewald_matrices[transformation] = ewald\n            else:\n                ewald = ewald_matrices[transformation]\n            energy = ewald.compute_sub_structure(s)\n            all_structures.append({\"num_sites\": len(s), \"energy\": energy, \"structure\": s})\n        else:\n            all_structures.append({\"num_sites\": len(s), \"structure\": s})\n\n    def sort_func(s):\n        return s[\"energy\"] / s[\"num_sites\"] if contains_oxidation_state and self.sort_criteria == \"ewald\" else s[\"num_sites\"]\n\n    self._all_structures = sorted(all_structures, key=sort_func)\n\n    if return_ranked_list:\n        return self._all_structures[0:num_to_return]\n    else:\n        return self._all_structures[0][\"structure\"]"
    },
    {
        "original": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        box = Span(level=properties.get('level', 'glyph'), **mapping)\n        plot.renderers.append(box)\n        return None, box",
        "rewrite": "def _init_glyph(self, plot, mapping, properties):\n    box = Span(level=properties.get('level', 'glyph'), **mapping)\n    plot.renderers.append(box)\n    return None, box"
    },
    {
        "original": "def sample_stats_prior_to_xarray(self):\n        \"\"\"Convert sample_stats_prior samples to xarray.\"\"\"\n        data = self.sample_stats_prior\n        if not isinstance(data, dict):\n            raise TypeError(\"DictConverter.sample_stats_prior is not a dictionary\")\n\n        return dict_to_dataset(data, library=None, coords=self.coords, dims=self.dims)",
        "rewrite": "def sample_stats_prior_to_xarray(self):\n    data = self.sample_stats_prior\n    if not isinstance(data, dict):\n        raise TypeError(\"DictConverter.sample_stats_prior is not a dictionary\")\n\n    return dict_to_dataset(data, library=None, coords=self.coords, dims=self.dims)"
    },
    {
        "original": "def _find_classes_param(self):\n        \"\"\"\n        Searches the wrapped model for the classes_ parameter.\n        \"\"\"\n        for attr in [\"classes_\"]:\n            try:\n                return getattr(self.estimator, attr)\n            except AttributeError:\n                continue\n\n        raise YellowbrickTypeError(\n            \"could not find classes_ param on {}\".format(\n                self.estimator.__class__.__name__\n            )\n        )",
        "rewrite": "def _find_classes_param(self):\n    \"\"\"\n    Searches the wrapped model for the classes_ parameter.\n    \"\"\"\n    for attr in [\"classes_\"]:\n        try:\n            return getattr(self.estimator, attr)\n        except AttributeError:\n            continue\n\n    raise YellowbrickTypeError(\n        \"could not find classes_ param on {}\".format(\n            self.estimator.__class__.__name__\n        )\n    )"
    },
    {
        "original": "def polarity(self):\n    \"\"\"Return the polarity score as a float within the range [-1.0, 1.0]\n    \"\"\"\n    scores = [w.polarity for w in self.words if w.polarity != 0]\n    if not scores:\n      return 0.0\n    return sum(scores) / float(len(scores))",
        "rewrite": "def polarity(self):\n    scores = [w.polarity for w in self.words if w.polarity != 0]\n    if not scores:\n        return 0.0\n    return sum(scores) / len(scores) * 1.0"
    },
    {
        "original": "def register_variant(cls):\n        \"\"\"\n        Registers the RADIUS attributes defined in this module.\n        \"\"\"\n\n        if hasattr(cls, \"val\"):\n            cls.registered_attributes[cls.val] = cls\n        else:\n            cls.registered_attributes[cls.type.default] = cls",
        "rewrite": "def register_variant(cls):\n    if hasattr(cls, \"val\"):\n        cls.registered_attributes[cls.val] = cls\n    else:\n        cls.registered_attributes[cls.type.default] = cls"
    },
    {
        "original": "async def read_line(stream: asyncio.StreamReader) -> bytes:\n    \"\"\"\n    Read a single line from ``stream``.\n\n    ``stream`` is an :class:`~asyncio.StreamReader`.\n\n    Return :class:`bytes` without CRLF.\n\n    \"\"\"\n    # Security: this is bounded by the StreamReader's limit (default = 32\u00a0KiB).\n    line = await stream.readline()\n    # Security: this guarantees header values are small (hard-coded = 4\u00a0KiB)\n    if len(line) > MAX_LINE:\n        raise ValueError(\"Line too long\")\n    # Not mandatory but safe - https://tools.ietf.org/html/rfc7230#section-3.5\n    if not line.endswith(b\"\\r\\n\"):\n        raise ValueError(\"Line without CRLF\")\n    return line[:-2]",
        "rewrite": "async def read_line(stream: asyncio.StreamReader) -> bytes:\n    \"\"\"\n    Read a single line from ``stream``.\n\n    ``stream`` is an :class:`~asyncio.StreamReader`.\n\n    Return :class:`bytes` without CRLF.\n\n    \"\"\"\n    line = await stream.readline()\n    if len(line) > MAX_LINE:\n        raise ValueError(\"Line too long\")\n    if not line.endswith(b\"\\r\\n\"):\n        raise ValueError(\"Line without CRLF\")\n    return line[:-2]"
    },
    {
        "original": "def sign_hmac(secret, payload):\n    \"\"\"Returns a base64-encoded HMAC-SHA1 signature of a given string.\n\n    :param secret: The key used for the signature, base64 encoded.\n    :type secret: string\n\n    :param payload: The payload to sign.\n    :type payload: string\n\n    :rtype: string\n    \"\"\"\n    payload = payload.encode('ascii', 'strict')\n    secret = secret.encode('ascii', 'strict')\n    sig = hmac.new(base64.urlsafe_b64decode(secret), payload, hashlib.sha1)\n    out = base64.urlsafe_b64encode(sig.digest())\n    return out.decode('utf-8')",
        "rewrite": "import hmac\nimport hashlib\nimport base64\n\ndef sign_hmac(secret, payload):\n    payload = payload.encode('utf-8')\n    secret = secret.encode('utf-8')\n    sig = hmac.new(base64.urlsafe_b64decode(secret), payload, hashlib.sha1)\n    out = base64.urlsafe_b64encode(sig.digest())\n    return out.decode('utf-8')"
    },
    {
        "original": "def collect_metrics(self, instance):\n        \"\"\"\n        Calls asynchronously _collect_metrics_async on all MORs, as the\n        job queue is processed the Aggregator will receive the metrics.\n        \"\"\"\n        i_key = self._instance_key(instance)\n        if not self.mor_cache.contains(i_key):\n            self.log.debug(\"Not collecting metrics for instance '{}', nothing to do yet.\".format(i_key))\n            return\n\n        vm_count = 0\n        custom_tags = instance.get('tags', [])\n        tags = [\"vcenter_server:{}\".format(ensure_unicode(instance.get('name')))] + custom_tags\n\n        n_mors = self.mor_cache.instance_size(i_key)\n        if not n_mors:\n            self.gauge('vsphere.vm.count', vm_count, tags=tags)\n            self.log.debug(\"No Mor objects to process for instance '{}', skip...\".format(i_key))\n            return\n\n        self.log.debug(\"Collecting metrics for {} mors\".format(ensure_unicode(n_mors)))\n\n        # Request metrics for several objects at once. We can limit the number of objects with batch_size\n        # If batch_size is 0, process everything at once\n        batch_size = self.batch_morlist_size or n_mors\n        for batch in self.mor_cache.mors_batch(i_key, batch_size):\n            query_specs = []\n            for _, mor in iteritems(batch):\n                if mor['mor_type'] == 'vm':\n                    vm_count += 1\n                if mor['mor_type'] not in REALTIME_RESOURCES and ('metrics' not in mor or not mor['metrics']):\n                    continue\n\n                query_spec = vim.PerformanceManager.QuerySpec()\n                query_spec.entity = mor[\"mor\"]\n                query_spec.intervalId = mor[\"interval\"]\n                query_spec.maxSample = 1\n                if mor['mor_type'] in REALTIME_RESOURCES:\n                    query_spec.metricId = self.metadata_cache.get_metric_ids(i_key)\n                else:\n                    query_spec.metricId = mor[\"metrics\"]\n                query_specs.append(query_spec)\n\n            if query_specs:\n                self.pool.apply_async(self._collect_metrics_async, args=(instance, query_specs))\n\n        self.gauge('vsphere.vm.count', vm_count, tags=tags)",
        "rewrite": "def collect_metrics(self, instance):\n    i_key = self._instance_key(instance)\n    if not self.mor_cache.contains(i_key):\n        self.log.debug(f\"Not collecting metrics for instance '{i_key}', nothing to do yet.\")\n        return\n\n    vm_count = 0\n    custom_tags = instance.get('tags', [])\n    tags = [f\"vcenter_server:{ensure_unicode(instance.get('name'))}\"] + custom_tags\n\n    n_mors = self.mor_cache.instance_size(i_key)\n    if not n_mors:\n        self.gauge('vsphere.vm.count', vm_count, tags=tags)\n        self.log.debug(f\"No Mor objects to process for instance '{i_key}', skip...\")\n        return\n\n    self.log.debug(f\"Collecting metrics for {ensure_unicode(n_mors)} mors\")\n\n    batch_size = self.batch_morlist_size or n_mors\n    for batch in self.mor_cache.mors_batch(i_key, batch_size):\n        query_specs = []\n        for _, mor in batch.items():\n            if mor['mor_type'] == 'vm':\n                vm_count += 1\n            if mor['mor_type'] not in REALTIME_RESOURCES and ('metrics' not in mor or not mor['metrics']):\n                continue\n\n            query_spec = vim.PerformanceManager.QuerySpec()\n            query_spec.entity = mor[\"mor\"]\n            query_spec.intervalId = mor[\"interval\"]\n            query_spec.maxSample = 1\n            if mor['mor_type'] in REALTIME_RESOURCES:\n                query_spec.metricId = self.metadata_cache.get_metric_ids(i_key)\n            else:\n                query_spec.metricId = mor[\"metrics\"]\n            query_specs.append(query_spec)\n\n        if query_specs:\n            self.pool.apply_async(self._collect_metrics_async, args=(instance, query_specs))\n\n    self.gauge('vsphere.vm.count', vm_count, tags=tags)"
    },
    {
        "original": "def translate_doc(filename, destination='zh-CN', mix=True):\n    \"\"\"\n    translate a word document type of file and save the result as document and keep the exactly same file format. \n        :param filename: word doc file \n        :param destination='zh-CN': \n        :param mix=True: if True, will have original language and target language into the same doc. paragraphs by paragraphs.\n    \"\"\"\n    def tx(t): return Translator().translate(t, dest=destination).text\n    doc = Document(filename)\n    for p in doc.paragraphs:\n        txd = tx(p.text)\n\n        p.text = p.text + ('\\n' + txd if mix else '')\n\n    for table in doc.tables:\n        for row in table.rows:\n            for cell in row.cells:\n                txd = tx(cell.text)\n                p.text = cell.text + ('\\n' + txd if mix else '')\n\n    f = filename.replace('.doc', destination.lower() + '.doc')\n    doc.save(f)",
        "rewrite": "def translate_doc(filename, destination='zh-CN', mix=True):\n    def tx(t): return Translator().translate(t, dest=destination).text\n    doc = Document(filename)\n    for p in doc.paragraphs:\n        txd = tx(p.text)\n        p.text = p.text + ('\\n' + txd if mix else '')\n\n    for table in doc.tables:\n        for row in table.rows:\n            for cell in row.cells:\n                txd = tx(cell.text)\n                cell.text = cell.text + ('\\n' + txd if mix else '')\n\n    f = filename.replace('.doc', f'{destination.lower()}.doc')\n    doc.save(f)"
    },
    {
        "original": "def get_headers(hide_env=True):\n    \"\"\"Returns headers dict from request context.\"\"\"\n\n    headers = dict(request.headers.items())\n\n    if hide_env and ('show_env' not in request.args):\n        for key in ENV_HEADERS:\n            try:\n                del headers[key]\n            except KeyError:\n                pass\n\n    return CaseInsensitiveDict(headers.items())",
        "rewrite": "def get_headers(hide_env=True):\n    headers = dict(request.headers.items())\n    \n    if hide_env and ('show_env' not in request.args):\n        for key in ENV_HEADERS:\n            try:\n                del headers[key]\n            except KeyError:\n                pass\n\n    return CaseInsensitiveDict(headers)"
    },
    {
        "original": "def batch(data, batch_size, batch_size_fn=None):\n    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n    if batch_size_fn is None:\n        def batch_size_fn(new, count, sofar):\n            return count\n    minibatch, size_so_far = [], 0\n    for ex in data:\n        minibatch.append(ex)\n        size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n        if size_so_far == batch_size:\n            yield minibatch\n            minibatch, size_so_far = [], 0\n        elif size_so_far > batch_size:\n            yield minibatch[:-1]\n            minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n    if minibatch:\n        yield minibatch",
        "rewrite": "def batch(data, batch_size, batch_size_fn=None):\n    if batch_size_fn is None:\n        def batch_size_fn(new, count, sofar):\n            return count\n    minibatch, size_so_far = [], 0\n    for ex in data:\n        minibatch.append(ex)\n        size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n        if size_so_far == batch_size:\n            yield minibatch\n            minibatch, size_so_far = [], 0\n        elif size_so_far > batch_size:\n            yield minibatch[:-1]\n            minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n    if minibatch:\n        yield minibatch"
    },
    {
        "original": "def GetAnnotatedMethods(cls):\n    \"\"\"Returns a dictionary of annotated router methods.\"\"\"\n\n    result = {}\n\n    # We want methods with the highest call-order to be processed last,\n    # so that their annotations have precedence.\n    for i_cls in reversed(inspect.getmro(cls)):\n      for name in compatibility.ListAttrs(i_cls):\n        cls_method = getattr(i_cls, name)\n\n        if not callable(cls_method):\n          continue\n\n        if not hasattr(cls_method, \"__http_methods__\"):\n          continue\n\n        result[name] = RouterMethodMetadata(\n            name=name,\n            doc=cls_method.__doc__,\n            args_type=getattr(cls_method, \"__args_type__\", None),\n            result_type=getattr(cls_method, \"__result_type__\", None),\n            category=getattr(cls_method, \"__category__\", None),\n            http_methods=getattr(cls_method, \"__http_methods__\", set()),\n            no_audit_log_required=getattr(cls_method,\n                                          \"__no_audit_log_required__\", False))\n\n    return result",
        "rewrite": "def get_annotated_methods(cls):\n    result = {}\n\n    for i_cls in reversed(inspect.getmro(cls)):\n        for name in compatibility.ListAttrs(i_cls):\n            cls_method = getattr(i_cls, name)\n\n            if not callable(cls_method):\n                continue\n\n            if not hasattr(cls_method, \"__http_methods__\"):\n                continue\n\n            result[name] = RouterMethodMetadata(\n                name=name,\n                doc=cls_method.__doc__,\n                args_type=getattr(cls_method, \"__args_type__\", None),\n                result_type=getattr(cls_method, \"__result_type__\", None),\n                category=getattr(cls_method, \"__category__\", None),\n                http_methods=getattr(cls_method, \"__http_methods__\", set()),\n                no_audit_log_required=getattr(cls_method, \"__no_audit_log_required__\", False))\n\n    return result"
    },
    {
        "original": "def with_settings(self,\n            stop: Optional[int] = None,\n            start: int = 0,\n            step: int = 1) -> 'Loop':\n        \"\"\"\n        Set start, stop and step loop configuration.\n\n            :param stop: Looop stop iteration integer. If None then loop\n                becomes infinite.\n            :param start: Loop iteration start integer.\n            :param step: Loop iteration interval integer.\n            :return: Loop itself.\n        \"\"\"\n        self.start = start\n        self.stop = stop\n        self.step = step\n        return self",
        "rewrite": "def with_settings(self, stop: Optional[int] = None, start: int = 0, step: int = 1) -> 'Loop':\n    \"\"\"\n    Set start, stop, and step loop configuration.\n    \n    :param stop: Loop stop iteration integer. If None then loop becomes infinite.\n    :param start: Loop iteration start integer.\n    :param step: Loop iteration interval integer.\n    :return: Loop itself.\n    \"\"\"\n    self.start = start\n    self.stop = stop\n    self.step = step\n    return self"
    },
    {
        "original": "def _clitable_to_dict(objects, fsm_handler):\n    \"\"\"\n    Converts TextFSM cli_table object to list of dictionaries.\n    \"\"\"\n    objs = []\n    log.debug('Cli Table:')\n    log.debug(objects)\n    log.debug('FSM handler:')\n    log.debug(fsm_handler)\n    for row in objects:\n        temp_dict = {}\n        for index, element in enumerate(row):\n            temp_dict[fsm_handler.header[index].lower()] = element\n        objs.append(temp_dict)\n    log.debug('Extraction result:')\n    log.debug(objs)\n    return objs",
        "rewrite": "def cli_table_to_dict(objects, fsm_handler):\n    objs = []\n    log.debug('Cli Table:')\n    log.debug(objects)\n    log.debug('FSM handler:')\n    log.debug(fsm_handler)\n    for row in objects:\n        temp_dict = {}\n        for index, element in enumerate(row):\n            temp_dict[fsm_handler.header[index].lower()] = element\n        objs.append(temp_dict)\n    log.debug('Extraction result:')\n    log.debug(objs)\n    return objs"
    },
    {
        "original": "def plan_branches(self, plan_key, expand=None, favourite=False, clover_enabled=False, max_results=25):\n        \"\"\"api/1.0/plan/{projectKey}-{buildKey}/branch\"\"\"\n        resource = 'plan/{}/branch'.format(plan_key)\n        return self.base_list_call(resource, expand, favourite, clover_enabled, max_results,\n                                   elements_key='branches', element_key='branch')",
        "rewrite": "def plan_branches(self, plan_key, expand=None, favourite=False, clover_enabled=False, max_results=25):\n        resource = 'plan/{}/branch'.format(plan_key)\n        return self.base_list_call(resource, expand, favourite, clover_enabled, max_results,\n                                   elements_key='branches', element_key='branch')"
    },
    {
        "original": "def delete_role(self, name):\n        # type: (str) -> None\n        \"\"\"Delete a role by first deleting all inline policies.\"\"\"\n        client = self._client('iam')\n        inline_policies = client.list_role_policies(\n            RoleName=name\n        )['PolicyNames']\n        for policy_name in inline_policies:\n            self.delete_role_policy(name, policy_name)\n        client.delete_role(RoleName=name)",
        "rewrite": "def delete_role(self, name):\n    client = self._client('iam')\n    inline_policies = client.list_role_policies(RoleName=name)['PolicyNames']\n    for policy_name in inline_policies:\n        self.delete_role_policy(name, policy_name)\n    client.delete_role(RoleName=name)"
    },
    {
        "original": "def triggering_streams(streams):\n    \"\"\"\n    Temporarily declares the streams as being in a triggered state.\n    Needed by DynamicMap to determine whether to memoize on a Callable,\n    i.e. if a stream has memoization disabled and is in triggered state\n    Callable should disable lookup in the memoization cache. This is\n    done by the dynamicmap_memoization context manager.\n    \"\"\"\n    for stream in streams:\n        stream._triggering = True\n    try:\n        yield\n    except:\n        raise\n    finally:\n        for stream in streams:\n            stream._triggering = False",
        "rewrite": "def triggering_streams(streams):\n    for stream in streams:\n        stream._triggering = True\n    try:\n        yield\n    except:\n        raise\n    finally:\n        for stream in streams:\n            stream._triggering = False"
    },
    {
        "original": "def has_in_assignees(self, assignee):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/assignees/:assignee <http://developer.github.com/v3/issues/assignees>`_\n        :param assignee: string or :class:`github.NamedUser.NamedUser`\n        :rtype: bool\n        \"\"\"\n        assert isinstance(assignee, github.NamedUser.NamedUser) or isinstance(assignee, (str, unicode)), assignee\n\n        if isinstance(assignee, github.NamedUser.NamedUser):\n            assignee = assignee._identity\n\n        status, headers, data = self._requester.requestJson(\n            \"GET\",\n            self.url + \"/assignees/\" + assignee\n        )\n        return status == 204",
        "rewrite": "def has_in_assignees(self, assignee):\n    assert isinstance(assignee, github.NamedUser.NamedUser) or isinstance(assignee, (str, unicode)), assignee\n\n    if isinstance(assignee, github.NamedUser.NamedUser):\n        assignee = assignee._identity\n\n    status, headers, data = self._requester.requestJson(\n        \"GET\",\n        self.url + \"/assignees/\" + assignee\n    )\n    return status == 204"
    },
    {
        "original": "def _int_to_words(self, pattern):\n        \"\"\"\n        Given a bit-pattern expressed an integer number, return a sequence of\n        the individual words that make up the pattern. The number of bits per\n        word will be obtained from the internal SPI interface.\n        \"\"\"\n        try:\n            bits_required = int(ceil(log(pattern, 2))) + 1\n        except ValueError:\n            # pattern == 0 (technically speaking, no bits are required to\n            # transmit the value zero ;)\n            bits_required = 1\n        shifts = range(0, bits_required, self._spi.bits_per_word)[::-1]\n        mask = 2 ** self._spi.bits_per_word - 1\n        return [(pattern >> shift) & mask for shift in shifts]",
        "rewrite": "def _int_to_words(self, pattern):\n    try:\n        bits_required = int(ceil(log(pattern, 2))) + 1\n    except ValueError:\n        bits_required = 1\n    \n    shifts = range(bits_required - self._spi.bits_per_word, -1, -self._spi.bits_per_word)\n    mask = 2 ** self._spi.bits_per_word - 1\n    \n    return [(pattern >> shift) & mask for shift in shifts]"
    },
    {
        "original": "def close(self) -> None:\n        \"\"\"\n        Close the server and terminate connections with close code 1001.\n\n        This method is idempotent.\n\n        \"\"\"\n        if self.close_task is None:\n            self.close_task = self.loop.create_task(self._close())",
        "rewrite": "def close(self) -> None:\n    if self.close_task is None:\n        self.close_task = self.loop.create_task(self._close())"
    },
    {
        "original": "def find_primitive(self):\n        \"\"\"\n        Find a primitive version of the unit cell.\n\n        Returns:\n            A primitive cell in the input cell is searched and returned\n            as an Structure object. If no primitive cell is found, None is\n            returned.\n        \"\"\"\n        lattice, scaled_positions, numbers = spglib.find_primitive(\n            self._cell, symprec=self._symprec)\n\n        species = [self._unique_species[i - 1] for i in numbers]\n\n        return Structure(lattice, species, scaled_positions,\n                         to_unit_cell=True).get_reduced_structure()",
        "rewrite": "def find_primitive(self):\n    lattice, scaled_positions, numbers = spglib.find_primitive(\n        self._cell, symprec=self._symprec)\n\n    species = [self._unique_species[i - 1] for i in numbers]\n\n    return Structure(lattice, species, scaled_positions,\n                     to_unit_cell=True).get_reduced_structure()"
    },
    {
        "original": "def _spark_job_metrics(self, instance, running_apps, addl_tags, requests_config):\n        \"\"\"\n        Get metrics for each Spark job.\n        \"\"\"\n        for app_id, (app_name, tracking_url) in iteritems(running_apps):\n\n            base_url = self._get_request_url(instance, tracking_url)\n            response = self._rest_request_to_json(\n                base_url, SPARK_APPS_PATH, SPARK_SERVICE_CHECK, requests_config, addl_tags, app_id, 'jobs'\n            )\n\n            for job in response:\n\n                status = job.get('status')\n\n                tags = ['app_name:%s' % str(app_name)]\n                tags.extend(addl_tags)\n                tags.append('status:%s' % str(status).lower())\n\n                self._set_metrics_from_json(tags, job, SPARK_JOB_METRICS)\n                self._set_metric('spark.job.count', COUNT, 1, tags)",
        "rewrite": "def _spark_job_metrics(self, instance, running_apps, addl_tags, requests_config):\n    for app_id, (app_name, tracking_url) in iteritems(running_apps):\n        base_url = self._get_request_url(instance, tracking_url)\n        response = self._rest_request_to_json(\n            base_url, SPARK_APPS_PATH, SPARK_SERVICE_CHECK, requests_config, addl_tags, app_id, 'jobs'\n        )\n\n        for job in response:\n            status = job.get('status')\n\n            tags = ['app_name:%s' % str(app_name)]\n            tags.extend(addl_tags)\n            tags.append('status:%s' % str(status).lower())\n\n            self._set_metrics_from_json(tags, job, SPARK_JOB_METRICS)\n            self._set_metric('spark.job.count', COUNT, 1, tags)"
    },
    {
        "original": "def update_disk(kwargs=None, conn=None, call=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Update a disk's properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_disk my-azure name=my_disk label=my_disk\n        salt-cloud -f update_disk my-azure name=my_disk new_name=another_disk\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_disk function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A name must be specified as \"name\"')\n\n    old_data = show_disk(kwargs={'name': kwargs['name']}, call='function')\n    data = conn.update_disk(\n        disk_name=kwargs['name'],\n        has_operating_system=kwargs.get('has_operating_system', old_data['has_operating_system']),\n        label=kwargs.get('label', old_data['label']),\n        media_link=kwargs.get('media_link', old_data['media_link']),\n        name=kwargs.get('new_name', old_data['name']),\n        os=kwargs.get('os', old_data['os']),\n    )\n    return show_disk(kwargs={'name': kwargs['name']}, call='function')",
        "rewrite": "def update_disk(kwargs=None, conn=None, call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_disk function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A name must be specified as \"name\"')\n\n    old_data = show_disk(kwargs={'name': kwargs['name']}, call='function')\n    data = conn.update_disk(\n        disk_name=kwargs['name'],\n        has_operating_system=kwargs.get('has_operating_system', old_data.get('has_operating_system', '')),\n        label=kwargs.get('label', old_data.get('label', '')),\n        media_link=kwargs.get('media_link', old_data.get('media_link', '')),\n        name=kwargs.get('new_name', old_data.get('name', '')),\n        os=kwargs.get('os', old_data.get('os', '')),\n    )\n    return show_disk(kwargs={'name': kwargs['name']}, call='function')"
    },
    {
        "original": "def include_callback_query_chat_id(fn=pair, types='all'):\n    \"\"\"\n    :return:\n        a pair producer that enables static callback query capturing\n        across seeder and delegator.\n\n    :param types:\n        ``all`` or a list of chat types (``private``, ``group``, ``channel``)\n    \"\"\"\n    @_ensure_seeders_list\n    def p(seeders, delegator_factory, *args, **kwargs):\n        return fn(seeders + [per_callback_query_chat_id(types=types)],\n                  delegator_factory, *args, include_callback_query=True, **kwargs)\n    return p",
        "rewrite": "def include_callback_query_chat_id(fn=pair, types='all'):\n    @_ensure_seeders_list\n    def p(seeders, delegator_factory, *args, **kwargs):\n        return fn(seeders + [per_callback_query_chat_id(types=types)],\n                  delegator_factory, *args, include_callback_query=True, **kwargs)\n    return p"
    },
    {
        "original": "def StreamFile(self, filedesc, offset=0, amount=None):\n    \"\"\"Streams chunks of a given file starting at given offset.\n\n    Args:\n      filedesc: A `file` object to stream.\n      offset: An integer offset at which the file stream should start on.\n      amount: An upper bound on number of bytes to read.\n\n    Returns:\n      Generator over `Chunk` instances.\n    \"\"\"\n    reader = FileReader(filedesc, offset=offset)\n    return self.Stream(reader, amount=amount)",
        "rewrite": "def StreamFile(self, filedesc, offset=0, amount=None):\n    reader = FileReader(filedesc, offset=offset)\n    return self.Stream(reader, amount=amount)"
    },
    {
        "original": "def _next_move_direction(self):\n        \"\"\"\n        pick a move at random from the list of moves\n        \"\"\"\n        nmoves = len(self.moves)\n        move = np.random.randint(1, nmoves+1)\n        while self.prev_move == (move + 3) % nmoves:\n            move = np.random.randint(1, nmoves+1)\n        self.prev_move = move\n        return np.array(self.moves[move])",
        "rewrite": "def _next_move_direction(self):\n    nmoves = len(self.moves)\n    move = np.random.randint(0, nmoves)\n    \n    while self.prev_move == (move + 3) % nmoves:\n        move = np.random.randint(0, nmoves)\n        \n    self.prev_move = move\n    return np.array(self.moves[move])"
    },
    {
        "original": "def do_get(self, line):\n        \"\"\"get <peer>\n        eg. get sw1\n        \"\"\"\n\n        def f(p, args):\n            print(p.get())\n\n        self._request(line, f)",
        "rewrite": "def do_get(self, line):\n    def f(p, args):\n        print(p.get())\n\n    self._request(line, f)"
    },
    {
        "original": "def variational_expectations(self, Fmu, Fvar, Y, epsilon=None):\n        r\"\"\"\n        Compute the expected log density of the data, given a Gaussian\n        distribution for the function values.\n\n        if\n            q(f) = N(Fmu, Fvar)  - Fmu: N x D  Fvar: N x D\n\n        and this object represents\n\n            p(y|f)  - Y: N x 1\n\n        then this method computes\n\n           \\int (\\log p(y|f)) q(f) df.\n\n\n        Here, we implement a default Monte Carlo quadrature routine.\n        \"\"\"\n        return self._mc_quadrature(self.logp, Fmu, Fvar, Y=Y, epsilon=epsilon)",
        "rewrite": "def variational_expectations(self, Fmu, Fvar, Y, epsilon=None):\n        return self._mc_quadrature(self.logp, Fmu, Fvar, Y=Y, epsilon=epsilon)"
    },
    {
        "original": "def find_region_end(self, lines):\n        \"\"\"Find the end of the region started with start and end markers\"\"\"\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            # New code region\n            # Simple code pattern in LightScripts must be preceded with a blank line\n            if self.start_code_re.match(line) or (\n                    self.simple_start_code_re and self.simple_start_code_re.match(line) and\n                    (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n\n                if self.explicit_end_marker_required:\n                    # Metadata here was conditioned on finding an explicit end marker\n                    # before the next start marker. So we dismiss it.\n                    self.metadata = None\n                    self.language = None\n\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if not self.ignore_end_marker and self.end_code_re:\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n            elif _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False",
        "rewrite": "def find_region_end(self, lines):\n    if self.metadata and 'cell_type' in self.metadata:\n        self.cell_type = self.metadata.pop('cell_type')\n    else:\n        self.cell_type = 'code'\n\n    parser = StringParser(self.language or self.default_language)\n    for i, line in enumerate(lines):\n        if self.metadata is not None and i == 0:\n            continue\n\n        if parser.is_quoted():\n            parser.read_line(line)\n            continue\n\n        parser.read_line(line)\n\n        if self.start_code_re.match(line) or (self.simple_start_code_re and self.simple_start_code_re.match(line) and (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n            if self.explicit_end_marker_required:\n                self.metadata = None\n                self.language = None\n\n            if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                    return i - 2, i, False\n                return i - 1, i, False\n            return i, i, False\n\n        if not self.ignore_end_marker and self.end_code_re:\n            if self.end_code_re.match(line):\n                return i, i + 1, True\n        elif _BLANK_LINE.match(line):\n            if not next_code_is_indented(lines[i:]):\n                if i > 0:\n                    return i, i + 1, False\n                if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                    return 1, 1, False\n                return 1, 2, False\n\n    return len(lines), len(lines), False"
    },
    {
        "original": "def _results(r):\n    r\"\"\"Select from a tuple of(root, funccalls, iterations, flag)\"\"\"\n    x, funcalls, iterations, flag = r\n    return results(x, funcalls, iterations, flag == 0)",
        "rewrite": "def results(r):\n    x, funcalls, iterations, flag = r\n    return results(x, funcalls, iterations, flag == 0)"
    },
    {
        "original": "def create_container(container_name, profile, **libcloud_kwargs):\n    \"\"\"\n    Create a container in the cloud\n\n    :param container_name: Container name\n    :type  container_name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's create_container method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.create_container MyFolder profile1\n    \"\"\"\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    container = conn.create_container(container_name, **libcloud_kwargs)\n    return {\n        'name': container.name,\n        'extra': container.extra\n        }",
        "rewrite": "def create_container(container_name, profile, **libcloud_kwargs):\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    container = conn.create_container(container_name, **libcloud_kwargs)\n    return {\n        'name': container.name,\n        'extra': container.extra\n    }"
    },
    {
        "original": "def GetReportData(self, get_report_args, token):\n    \"\"\"Filter the last week of user actions.\"\"\"\n    ret = rdf_report_plugins.ApiReportData(\n        representation_type=RepresentationType.STACK_CHART)\n\n    week_duration = rdfvalue.Duration(\"7d\")\n    num_weeks = math.ceil(get_report_args.duration.seconds /\n                          week_duration.seconds)\n    weeks = range(0, num_weeks)\n    start_time = get_report_args.start_time\n    end_time = start_time + num_weeks * week_duration\n    user_activity = collections.defaultdict(lambda: {week: 0 for week in weeks})\n\n    entries = self._LoadUserActivity(\n        start_time=get_report_args.start_time, end_time=end_time, token=token)\n\n    for username, timestamp, count in entries:\n      week = (timestamp - start_time).seconds // week_duration.seconds\n      if week in user_activity[username]:\n        user_activity[username][week] += count\n\n    user_activity = sorted(iteritems(user_activity))\n    user_activity = [(user, data)\n                     for user, data in user_activity\n                     if user not in aff4_users.GRRUser.SYSTEM_USERS]\n\n    ret.stack_chart.data = [\n        rdf_report_plugins.ApiReportDataSeries2D(\n            label=user,\n            points=(rdf_report_plugins.ApiReportDataPoint2D(x=x, y=y)\n                    for x, y in sorted(data.items())))\n        for user, data in user_activity\n    ]\n\n    return ret",
        "rewrite": "def GetReportData(self, get_report_args, token):\n    ret = rdf_report_plugins.ApiReportData(representation_type=RepresentationType.STACK_CHART)\n    \n    week_duration = rdfvalue.Duration(\"7d\")\n    num_weeks = math.ceil(get_report_args.duration.seconds / week_duration.seconds)\n    weeks = range(0, num_weeks)\n    start_time = get_report_args.start_time\n    end_time = start_time + num_weeks * week_duration\n    user_activity = collections.defaultdict(lambda: {week: 0 for week in weeks})\n    \n    entries = self._LoadUserActivity(start_time=get_report_args.start_time, end_time=end_time, token=token)\n    \n    for username, timestamp, count in entries:\n        week = (timestamp - start_time).seconds // week_duration.seconds\n        if week in user_activity[username]:\n            user_activity[username][week] += count\n    \n    user_activity = sorted(iteritems(user_activity))\n    user_activity = [(user, data) for user, data in user_activity if user not in aff4_users.GRRUser.SYSTEM_USERS]\n    \n    ret.stack_chart.data = [\n        rdf_report_plugins.ApiReportDataSeries2D(label=user, \n            points=(rdf_report_plugins.ApiReportDataPoint2D(x=x, y=y) for x, y in sorted(data.items())))\n        for user, data in user_activity\n    ]\n    \n    return ret"
    },
    {
        "original": "def _next_image_partname(self, ext):\n        \"\"\"\n        The next available image partname, starting from\n        ``/word/media/image1.{ext}`` where unused numbers are reused. The\n        partname is unique by number, without regard to the extension. *ext*\n        does not include the leading period.\n        \"\"\"\n        def image_partname(n):\n            return PackURI('/word/media/image%d.%s' % (n, ext))\n        used_numbers = [image_part.partname.idx for image_part in self]\n        for n in range(1, len(self)+1):\n            if n not in used_numbers:\n                return image_partname(n)\n        return image_partname(len(self)+1)",
        "rewrite": "def _next_image_partname(self, ext):\n    def image_partname(n):\n        return PackURI('/word/media/image%d.%s' % (n, ext))\n    \n    used_numbers = [image_part.partname.idx for image_part in self]\n    \n    for n in range(1, len(self)+1):\n        if n not in used_numbers:\n            return image_partname(n)\n    \n    return image_partname(len(self)+1)"
    },
    {
        "original": "def _check(self, sock_info):\n        \"\"\"This side-effecty function checks if this socket has been idle for\n        for longer than the max idle time, or if the socket has been closed by\n        some external network error, and if so, attempts to create a new\n        socket. If this connection attempt fails we raise the\n        ConnectionFailure.\n\n        Checking sockets lets us avoid seeing *some*\n        :class:`~pymongo.errors.AutoReconnect` exceptions on server\n        hiccups, etc. We only check if the socket was closed by an external\n        error if it has been > 1 second since the socket was checked into the\n        pool, to keep performance reasonable - we can't avoid AutoReconnects\n        completely anyway.\n        \"\"\"\n        # How long since socket was last checked in.\n        idle_time_seconds = _time() - sock_info.last_checkin\n        # If socket is idle, open a new one.\n        if (self.opts.max_idle_time_ms is not None and\n                idle_time_seconds * 1000 > self.opts.max_idle_time_ms):\n            sock_info.close()\n            return self.connect()\n\n        if (self._check_interval_seconds is not None and (\n                0 == self._check_interval_seconds or\n                idle_time_seconds > self._check_interval_seconds)):\n            if self.socket_checker.socket_closed(sock_info.sock):\n                sock_info.close()\n                return self.connect()\n\n        return sock_info",
        "rewrite": "def _check(self, sock_info):\n    idle_time_seconds = _time() - sock_info.last_checkin\n\n    if (self.opts.max_idle_time_ms is not None and idle_time_seconds * 1000 > self.opts.max_idle_time_ms):\n        sock_info.close()\n        return self.connect()\n\n    if (self._check_interval_seconds is not None and (\n            0 == self._check_interval_seconds or idle_time_seconds > self._check_interval_seconds)):\n        if self.socket_checker.socket_closed(sock_info.sock):\n            sock_info.close()\n            return self.connect()\n\n    return sock_info"
    },
    {
        "original": "def _get_destination(self, estimated_size):\n        \"\"\"\n        Invoked from the C level, this function will return either the name of\n        the folder where the datatable is to be saved; or None, indicating that\n        the datatable should be read into RAM. This function may also raise an\n        exception if it determines that it cannot find a good strategy to\n        handle a dataset of the requested size.\n        \"\"\"\n        global _psutil_load_attempted\n        if not _psutil_load_attempted:\n            _psutil_load_attempted = True\n            try:\n                import psutil\n            except ImportError:\n                psutil = None\n\n        if self.verbose and estimated_size > 1:\n            self.logger.debug(\"The Frame is estimated to require %s bytes\"\n                              % humanize_bytes(estimated_size))\n        if estimated_size < 1024 or psutil is None:\n            return None\n        vm = psutil.virtual_memory()\n        if self.verbose:\n            self.logger.debug(\"Memory available = %s (out of %s)\"\n                              % (humanize_bytes(vm.available),\n                                 humanize_bytes(vm.total)))\n        if (estimated_size < vm.available and self._save_to is None or\n                self._save_to == \"memory\"):\n            if self.verbose:\n                self.logger.debug(\"Frame will be loaded into memory\")\n            return None\n        else:\n            if self._save_to:\n                tmpdir = self._save_to\n                os.makedirs(tmpdir)\n            else:\n                tmpdir = tempfile.mkdtemp()\n            du = psutil.disk_usage(tmpdir)\n            if self.verbose:\n                self.logger.debug(\"Free disk space on drive %s = %s\"\n                                  % (os.path.splitdrive(tmpdir)[0] or \"/\",\n                                     humanize_bytes(du.free)))\n            if du.free > estimated_size or self._save_to:\n                if self.verbose:\n                    self.logger.debug(\"Frame will be stored in %s\"\n                                      % tmpdir)\n                return tmpdir\n        raise RuntimeError(\"The Frame is estimated to require at lest %s \"\n                           \"of memory, and you don't have that much available \"\n                           \"either in RAM or on a hard drive.\"\n                           % humanize_bytes(estimated_size))",
        "rewrite": "def _get_destination(self, estimated_size):\n    global _psutil_load_attempted\n    if not _psutil_load_attempted:\n        _psutil_load_attempted = True\n        try:\n            import psutil\n        except ImportError:\n            psutil = None\n\n    if self.verbose and estimated_size > 1:\n        self.logger.debug(\"The Frame is estimated to require %s bytes\" % humanize_bytes(estimated_size))\n    \n    if estimated_size < 1024 or psutil is None:\n        return None\n        \n    vm = psutil.virtual_memory()\n    \n    if self.verbose:\n        self.logger.debug(\"Memory available = %s (out of %s)\" % (humanize_bytes(vm.available), humanize_bytes(vm.total)))\n    \n    if (estimated_size < vm.available and (self._save_to is None or self._save_to == \"memory\")):\n        if self.verbose:\n            self.logger.debug(\"Frame will be loaded into memory\")\n        return None\n    else:\n        if self._save_to:\n            tmpdir = self._save_to\n            os.makedirs(tmpdir)\n        else:\n            tmpdir = tempfile.mkdtemp()\n        du = psutil.disk_usage(tmpdir)\n        \n        if self.verbose:\n            self.logger.debug(\"Free disk space on drive %s = %s\" % (os.path.splitdrive(tmpdir)[0] or \"/\", humanize_bytes(du.free)))\n            \n        if du.free > estimated_size or self._save_to:\n            if self.verbose:\n                self.logger.debug(\"Frame will be stored in %s\" % tmpdir)\n            return tmpdir\n    \n    raise RuntimeError(\"The Frame is estimated to require at least %s of memory, and you don't have that much available either in RAM or on a hard drive.\" % humanize_bytes(estimated_size))"
    },
    {
        "original": "def to_data(value):\n    \"\"\"Standardize data types. Converts PyTorch tensors to Numpy arrays,\n    and Numpy scalars to Python scalars.\"\"\"\n    # TODO: Use get_framework() for better detection.\n    if value.__class__.__module__.startswith(\"torch\"):\n        import torch\n        if isinstance(value, torch.nn.parameter.Parameter):\n            value = value.data\n        if isinstance(value, torch.Tensor):\n            if value.requires_grad:\n                value = value.detach()\n            value = value.cpu().numpy().copy()\n        # If 0-dim array, convert to scalar\n        if not value.shape:\n            value = value.item()\n    # Convert Numpy scalar types to Python types\n    if value.__class__.__module__ == \"numpy\" and value.__class__.__name__ != \"ndarray\":\n        value = value.item()\n    return value",
        "rewrite": "def to_data(value):\n    if \"torch\" in str(type(value)):\n        import torch\n        if isinstance(value, torch.nn.parameter.Parameter):\n            value = value.data\n        if isinstance(value, torch.Tensor):\n            if value.requires_grad:\n                value = value.detach()\n            value = value.cpu().numpy().copy()\n        if not value.shape:\n            value = value.item()\n    if \"numpy\" in str(type(value)) and type(value).__name__ != \"ndarray\":\n        value = value.item()\n    return value"
    },
    {
        "original": "def get_link_page_text(link_page):\n        \"\"\"\n        Construct the dialog box to display a list of links to the user.\n        \"\"\"\n        text = ''\n        for i, link in enumerate(link_page):\n            capped_link_text = (link['text'] if len(link['text']) <= 20\n                                else link['text'][:19] + '\u2026')\n            text += '[{}] [{}]({})\\n'.format(i, capped_link_text, link['href'])\n        return text",
        "rewrite": "def get_link_page_text(link_page):\n    text = ''\n    for i, link in enumerate(link_page):\n        capped_link_text = (link['text'] if len(link['text']) <= 20 else link['text'][:19] + '\u2026')\n        text += '[{}] [{}]({})\\n'.format(i, capped_link_text, link['href'])\n    return text"
    },
    {
        "original": "def get_parameters(self, packet_count=None):\n        \"\"\"\n        Returns the special tshark parameters to be used according to the configuration of this class.\n        \"\"\"\n        params = super(InMemCapture, self).get_parameters(packet_count=packet_count)\n        params += ['-i', '-']\n        return params",
        "rewrite": "def get_parameters(self, packet_count=None):\n    params = super(InMemCapture, self).get_parameters(packet_count=packet_count)\n    params += ['-i', '-']\n    return params"
    },
    {
        "original": "def get_zmatrix(self):\n        \"\"\"\n        Returns a z-matrix representation of the molecule.\n        \"\"\"\n        output = []\n        outputvar = []\n        for i, site in enumerate(self._mol):\n            if i == 0:\n                output.append(\"{}\".format(site.specie))\n            elif i == 1:\n                nn = self._find_nn_pos_before_site(i)\n                bondlength = self._mol.get_distance(i, nn[0])\n                output.append(\"{} {} B{}\".format(self._mol[i].specie,\n                                                 nn[0] + 1, i))\n                outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n            elif i == 2:\n                nn = self._find_nn_pos_before_site(i)\n                bondlength = self._mol.get_distance(i, nn[0])\n                angle = self._mol.get_angle(i, nn[0], nn[1])\n                output.append(\"{} {} B{} {} A{}\".format(self._mol[i].specie,\n                                                        nn[0] + 1, i,\n                                                        nn[1] + 1, i))\n                outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n                outputvar.append(\"A{}={:.6f}\".format(i, angle))\n            else:\n                nn = self._find_nn_pos_before_site(i)\n                bondlength = self._mol.get_distance(i, nn[0])\n                angle = self._mol.get_angle(i, nn[0], nn[1])\n                dih = self._mol.get_dihedral(i, nn[0], nn[1], nn[2])\n                output.append(\"{} {} B{} {} A{} {} D{}\"\n                              .format(self._mol[i].specie, nn[0] + 1, i,\n                                      nn[1] + 1, i, nn[2] + 1, i))\n                outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n                outputvar.append(\"A{}={:.6f}\".format(i, angle))\n                outputvar.append(\"D{}={:.6f}\".format(i, dih))\n        return \"\\n\".join(output) + \"\\n\\n\" + \"\\n\".join(outputvar)",
        "rewrite": "def get_zmatrix(self):\n    output = []\n    outputvar = []\n    for i, site in enumerate(self._mol):\n        if i == 0:\n            output.append(\"{}\".format(site.specie))\n        elif i == 1:\n            nn = self._find_nn_pos_before_site(i)\n            bondlength = self._mol.get_distance(i, nn[0])\n            output.append(\"{} {} B{}\".format(self._mol[i].specie, nn[0] + 1, i))\n            outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n        elif i == 2:\n            nn = self._find_nn_pos_before_site(i)\n            bondlength = self._mol.get_distance(i, nn[0])\n            angle = self._mol.get_angle(i, nn[0], nn[1])\n            output.append(\"{} {} B{} {} A{}\".format(self._mol[i].specie, nn[0] + 1, i, nn[1] + 1, i))\n            outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n            outputvar.append(\"A{}={:.6f}\".format(i, angle))\n        else:\n            nn = self._find_nn_pos_before_site(i)\n            bondlength = self._mol.get_distance(i, nn[0])\n            angle = self._mol.get_angle(i, nn[0], nn[1])\n            dih = self._mol.get_dihedral(i, nn[0], nn[1], nn[2])\n            output.append(\"{} {} B{} {} A{} {} D{}\".format(self._mol[i].specie, nn[0] + 1, i, nn[1] + 1, i, nn[2] + 1, i))\n            outputvar.append(\"B{}={:.6f}\".format(i, bondlength))\n            outputvar.append(\"A{}={:.6f}\".format(i, angle))\n            outputvar.append(\"D{}={:.6f}\".format(i, dih))\n    return \"\\n\".join(output) + \"\\n\\n\" + \"\\n\".join(outputvar)"
    },
    {
        "original": "def GetMacAddresses(self):\n    \"\"\"MAC addresses from all interfaces.\"\"\"\n    result = set()\n    for interface in self.interfaces:\n      if (interface.mac_address and\n          interface.mac_address != b\"\\x00\" * len(interface.mac_address)):\n        result.add(Text(interface.mac_address.human_readable_address))\n    return sorted(result)",
        "rewrite": "def get_mac_addresses(self):\n    \"\"\"MAC addresses from all interfaces.\"\"\"\n    result = set()\n    for interface in self.interfaces:\n        if interface.mac_address and interface.mac_address != b\"\\x00\" * len(interface.mac_address):\n            result.add(Text(interface.mac_address.human_readable_address))\n    return sorted(result)"
    },
    {
        "original": "def list_pkgs(versions_as_list=False, with_origin=False, **kwargs):\n    \"\"\"\n    List the packages currently installed as a dict::\n\n        {'<package_name>': '<version>'}\n\n    with_origin : False\n        Return a nested dictionary containing both the origin name and version\n        for each installed package.\n\n        .. versionadded:: 2014.1.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.list_pkgs\n    \"\"\"\n    versions_as_list = salt.utils.data.is_true(versions_as_list)\n    # not yet implemented or not applicable\n    if any([salt.utils.data.is_true(kwargs.get(x))\n            for x in ('removed', 'purge_desired')]):\n        return {}\n\n    if 'pkg.list_pkgs' in __context__:\n        ret = copy.deepcopy(__context__['pkg.list_pkgs'])\n        if not versions_as_list:\n            __salt__['pkg_resource.stringify'](ret)\n        if salt.utils.data.is_true(with_origin):\n            origins = __context__.get('pkg.origin', {})\n            return dict([\n                (x, {'origin': origins.get(x, ''), 'version': y})\n                for x, y in six.iteritems(ret)\n            ])\n        return ret\n\n    ret = {}\n    origins = {}\n    out = __salt__['cmd.run_stdout'](['pkg_info', '-ao'],\n                                     output_loglevel='trace',\n                                     python_shell=False)\n    pkgs_re = re.compile(r'Information for ([^:]+):\\s*Origin:\\n([^\\n]+)')\n    for pkg, origin in pkgs_re.findall(out):\n        if not pkg:\n            continue\n        try:\n            pkgname, pkgver = pkg.rsplit('-', 1)\n        except ValueError:\n            continue\n        __salt__['pkg_resource.add_pkg'](ret, pkgname, pkgver)\n        origins[pkgname] = origin\n\n    __salt__['pkg_resource.sort_pkglist'](ret)\n    __context__['pkg.list_pkgs'] = copy.deepcopy(ret)\n    __context__['pkg.origin'] = origins\n    if not versions_as_list:\n        __salt__['pkg_resource.stringify'](ret)\n    if salt.utils.data.is_true(with_origin):\n        return dict([\n            (x, {'origin': origins.get(x, ''), 'version': y})\n            for x, y in six.iteritems(ret)\n        ])\n    return ret",
        "rewrite": "def list_pkgs(versions_as_list=False, with_origin=False, **kwargs):\n    versions_as_list = salt.utils.data.is_true(versions_as_list)\n\n    if any([salt.utils.data.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]):\n        return {}\n\n    if 'pkg.list_pkgs' in __context__:\n        ret = copy.deepcopy(__context__['pkg.list_pkgs'])\n        if not versions_as_list:\n            __salt__['pkg_resource.stringify'](ret)\n        if salt.utils.data.is_true(with_origin):\n            origins = __context__.get('pkg.origin', {})\n            return dict([(x, {'origin': origins.get(x, ''), 'version': y}) for x, y in six.iteritems(ret)])\n        return ret\n\n    ret = {}\n    origins = {}\n    out = __salt__['cmd.run_stdout'](['pkg_info', '-ao'], output_loglevel='trace', python_shell=False)\n    pkgs_re = re.compile(r'Information for ([^:]+):\\s*Origin:\\n([^\\n]+)')\n    for pkg, origin in pkgs_re.findall(out):\n        if not pkg:\n            continue\n        try:\n            pkgname, pkgver = pkg.rsplit('-', 1)\n        except ValueError:\n            continue\n        __salt__['pkg_resource.add_pkg'](ret, pkgname, pkgver)\n        origins[pkgname] = origin\n\n    __salt__['pkg_resource.sort_pkglist'](ret)\n    __context__['pkg.list_pkgs'] = copy.deepcopy(ret)\n    __context__['pkg.origin'] = origins\n    if not versions_as_list:\n        __salt__['pkg_resource.stringify'](ret)\n    if salt.utils.data.is_true(with_origin):\n        return dict([(x, {'origin': origins.get(x, ''), 'version': y}) for x, y in six.iteritems(ret)])\n    return ret"
    },
    {
        "original": "def spm_config(path):\n    \"\"\"\n    Read in the salt master config file and add additional configs that\n    need to be stubbed out for spm\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n    # Let's grab a copy of salt's master default opts\n    defaults = DEFAULT_MASTER_OPTS.copy()\n    # Let's override them with spm's required defaults\n    defaults.update(DEFAULT_SPM_OPTS)\n\n    overrides = load_config(path, 'SPM_CONFIG', DEFAULT_SPM_OPTS['spm_conf_file'])\n    default_include = overrides.get('spm_default_include',\n                                    defaults['spm_default_include'])\n    include = overrides.get('include', [])\n\n    overrides.update(include_config(default_include, path, verbose=False))\n    overrides.update(include_config(include, path, verbose=True))\n    defaults = apply_master_config(overrides, defaults)\n    defaults = apply_spm_config(overrides, defaults)\n    return client_config(path, env_var='SPM_CONFIG', defaults=defaults)",
        "rewrite": "def spm_config(path):\n    defaults = DEFAULT_MASTER_OPTS.copy()\n    defaults.update(DEFAULT_SPM_OPTS)\n\n    overrides = load_config(path, 'SPM_CONFIG', DEFAULT_SPM_OPTS['spm_conf_file'])\n    default_include = overrides.get('spm_default_include', defaults['spm_default_include'])\n    include = overrides.get('include', [])\n\n    overrides.update(include_config(default_include, path, verbose=False))\n    overrides.update(include_config(include, path, verbose=True))\n    defaults = apply_master_config(overrides, defaults)\n    defaults = apply_spm_config(overrides, defaults)\n    return client_config(path, env_var='SPM_CONFIG', defaults=defaults)"
    },
    {
        "original": "def size(self, key, resource_type):\n        \"\"\"\n        Return the size of the queue for a given key and resource type.\n        If the key is not in the cache, this will raise a KeyError.\n        \"\"\"\n        with self._objects_queue_lock:\n            return len(self._objects_queue[key].get(resource_type, []))",
        "rewrite": "def size(self, key, resource_type):\n    with self._objects_queue_lock:\n        return len(self._objects_queue[key].get(resource_type, []))"
    },
    {
        "original": "def _fetch_rrd_meta(self, connection, rrd_path_root, whitelist, field_names, tags):\n        \"\"\" Fetch metadata about each RRD in this Cacti DB, returning a list of\n            tuples of (hostname, device_name, rrd_path)\n        \"\"\"\n\n        def _in_whitelist(rrd):\n            path = rrd.replace('<path_rra>/', '')\n            for p in whitelist:\n                if fnmatch(path, p):\n                    return True\n            return False\n\n        c = connection.cursor()\n\n        and_parameters = \" OR \".join([\"hsc.field_name = '%s'\" % field_name for field_name in field_names])\n\n        # Check for the existence of the `host_snmp_cache` table\n        rrd_query = ",
        "rewrite": "def fetch_rrd_meta(self, connection, rrd_path_root, whitelist, field_names, tags):\n    \"\"\"\n    Fetch metadata about each RRD in this Cacti DB, returning a list of tuples of (hostname, device_name, rrd_path)\n    \"\"\"\n    \n    def in_whitelist(rrd):\n        path = rrd.replace('<path_rra>/', '')\n        for p in whitelist:\n            if fnmatch(path, p):\n                return True\n        return False\n    \n    c = connection.cursor()\n    \n    and_parameters = \" OR \".join([\"hsc.field_name = '%s'\" % field_name for field_name in field_names])\n    \n    # Check for the existence of the `host_snmp_cache` table\n    rrd_query = \". No need to explain. Just write code.\""
    },
    {
        "original": "def get_page_statistics(self, page_id, begin_date, end_date):\n        \"\"\"\n        \u4ee5\u9875\u9762\u4e3a\u7ef4\u5ea6\u7684\u6570\u636e\u7edf\u8ba1\u63a5\u53e3\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/0/8a24bcacad40fe7ee98d1573cb8a6764.html\n\n        :param page_id: \u9875\u9762 ID\n        :param begin_date: \u8d77\u59cb\u65f6\u95f4\uff0c\u6700\u957f\u65f6\u95f4\u8de8\u5ea6\u4e3a30\u5929\n        :param end_date: \u7ed3\u675f\u65f6\u95f4\uff0c\u6700\u957f\u65f6\u95f4\u8de8\u5ea6\u4e3a30\u5929\n        :return: \u7edf\u8ba1\u6570\u636e\n        \"\"\"\n        res = self._post(\n            'shakearound/statistics/page',\n            data={\n                'page_id': page_id,\n                'begin_date': self._to_timestamp(begin_date),\n                'end_date': self._to_timestamp(end_date),\n            },\n            result_processor=lambda x: x['data']\n        )\n        return res",
        "rewrite": "def get_page_statistics(self, page_id, begin_date, end_date):\n        res = self._post(\n            'shakearound/statistics/page',\n            data={\n                'page_id': page_id,\n                'begin_date': self._to_timestamp(begin_date),\n                'end_date': self._to_timestamp(end_date),\n            },\n            result_processor=lambda x: x['data']\n        )\n        return res"
    },
    {
        "original": "def _get_existing_template_c_list(component, parent_id, **kwargs):\n    \"\"\"\n    Make a list of given component type not inherited from other templates because Zabbix API returns only list of all\n    and list of inherited component items so we have to do a difference list.\n\n    :param component: Template component (application, item, etc...)\n    :param parent_id: ID of existing template the component is assigned to\n    :return List of non-inherited (own) components\n    \"\"\"\n    c_def = TEMPLATE_COMPONENT_DEF[component]\n    q_params = dict(c_def['output'])\n    q_params.update({c_def['qselectpid']: parent_id})\n\n    existing_clist_all = __salt__['zabbix.run_query'](c_def['qtype'] + '.get', q_params, **kwargs)\n\n    # in some cases (e.g. templatescreens) the logic is reversed (even name of the flag is different!)\n    if c_def['inherited'] == 'inherited':\n        q_params.update({c_def['inherited']: 'true'})\n        existing_clist_inherited = __salt__['zabbix.run_query'](c_def['qtype'] + '.get', q_params, **kwargs)\n    else:\n        existing_clist_inherited = []\n\n    if existing_clist_inherited:\n        return [c_all for c_all in existing_clist_all if c_all not in existing_clist_inherited]\n\n    return existing_clist_all",
        "rewrite": "def _get_existing_template_c_list(component, parent_id, **kwargs):\n    c_def = TEMPLATE_COMPONENT_DEF[component]\n    q_params = dict(c_def['output'])\n    q_params.update({c_def['qselectpid']: parent_id})\n\n    existing_clist_all = __salt__['zabbix.run_query'](c_def['qtype'] + '.get', q_params, **kwargs)\n\n    if c_def['inherited'] == 'inherited':\n        q_params.update({c_def['inherited']: 'true'})\n        existing_clist_inherited = __salt__['zabbix.run_query'](c_def['qtype'] + '.get', q_params, **kwargs)\n    else:\n        existing_clist_inherited = []\n\n    if existing_clist_inherited:\n        return [c_all for c_all in existing_clist_all if c_all not in existing_clist_inherited]\n\n    return existing_clist_all"
    },
    {
        "original": "def _split(rule):\n    \"\"\"Splits a rule whose len(rhs) > 2 into shorter rules.\"\"\"\n    rule_str = str(rule.lhs) + '__' + '_'.join(str(x) for x in rule.rhs)\n    rule_name = '__SP_%s' % (rule_str) + '_%d'\n    yield Rule(rule.lhs, [rule.rhs[0], NT(rule_name % 1)], weight=rule.weight, alias=rule.alias)\n    for i in xrange(1, len(rule.rhs) - 2):\n        yield Rule(NT(rule_name % i), [rule.rhs[i], NT(rule_name % (i + 1))], weight=0, alias='Split')\n    yield Rule(NT(rule_name % (len(rule.rhs) - 2)), rule.rhs[-2:], weight=0, alias='Split')",
        "rewrite": "def _split(rule):\n    rule_str = str(rule.lhs) + '__' + '_'.join(str(x) for x in rule.rhs)\n    rule_name = '__SP_%s' % (rule_str) + '_%d'\n    yield Rule(rule.lhs, [rule.rhs[0], NT(rule_name % 1)], weight=rule.weight, alias=rule.alias)\n    for i in range(1, len(rule.rhs) - 2):\n        yield Rule(NT(rule_name % i), [rule.rhs[i], NT(rule_name % (i + 1))], weight=0, alias='Split')\n    yield Rule(NT(rule_name % (len(rule.rhs) - 2)), rule.rhs[-2:], weight=0, alias='Split')"
    },
    {
        "original": "def WritePathInfos(self, client_id, path_infos):\n    \"\"\"Writes a collection of path_info records for a client.\"\"\"\n    try:\n      self._MultiWritePathInfos({client_id: path_infos})\n    except MySQLdb.IntegrityError as error:\n      raise db.UnknownClientError(client_id=client_id, cause=error)",
        "rewrite": "def WritePathInfos(self, client_id, path_infos):\n    try:\n        self._MultiWritePathInfos({client_id: path_infos})\n    except MySQLdb.IntegrityError as error:\n        raise db.UnknownClientError(client_id=client_id, cause=error)"
    },
    {
        "original": "def create_open(self, appid):\n        \"\"\"\n        \u521b\u5efa\u5f00\u653e\u5e73\u53f0\u8d26\u53f7\uff0c\u5e76\u7ed1\u5b9a\u516c\u4f17\u53f7/\u5c0f\u7a0b\u5e8f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://open.weixin.qq.com/cgi-bin/showdocument?action=dir_list&id=open1498704199_1bcax\n\n        :param appid: \u6388\u6743\u516c\u4f17\u53f7\u6216\u5c0f\u7a0b\u5e8f\u7684 appid\n        :return: \u5f00\u653e\u5e73\u53f0\u7684 appid\n        \"\"\"\n        return self._post(\n            'cgi-bin/open/create',\n            data={\n                'appid': appid,\n            },\n            result_processor=lambda x: x['open_appid'],\n        )",
        "rewrite": "def create_open(self, appid):\n    return self._post('cgi-bin/open/create',\n                      data={\n                          'appid': appid,\n                      },\n                      result_processor=lambda x: x['open_appid'])"
    },
    {
        "original": "def update(self, **kwargs):\n        \"\"\"\n        Update a service's configuration. Similar to the ``docker service\n        update`` command.\n\n        Takes the same parameters as :py:meth:`~ServiceCollection.create`.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        # Image is required, so if it hasn't been set, use current image\n        if 'image' not in kwargs:\n            spec = self.attrs['Spec']['TaskTemplate']['ContainerSpec']\n            kwargs['image'] = spec['Image']\n\n        if kwargs.get('force_update') is True:\n            task_template = self.attrs['Spec']['TaskTemplate']\n            current_value = int(task_template.get('ForceUpdate', 0))\n            kwargs['force_update'] = current_value + 1\n\n        create_kwargs = _get_create_service_kwargs('update', kwargs)\n\n        return self.client.api.update_service(\n            self.id,\n            self.version,\n            **create_kwargs\n        )",
        "rewrite": "def update(self, **kwargs):\n    if 'image' not in kwargs:\n        spec = self.attrs['Spec']['TaskTemplate']['ContainerSpec']\n        kwargs['image'] = spec['Image']\n\n    if kwargs.get('force_update') is True:\n        task_template = self.attrs['Spec']['TaskTemplate']\n        current_value = int(task_template.get('ForceUpdate', 0))\n        kwargs['force_update'] = current_value + 1\n\n    create_kwargs = _get_create_service_kwargs('update', kwargs)\n\n    return self.client.api.update_service(\n        self.id,\n        self.version,\n        **create_kwargs\n    )"
    },
    {
        "original": "def forward(self, inputs, context, inference=False):\n        \"\"\"\n        Execute the decoder.\n\n        :param inputs: tensor with inputs to the decoder\n        :param context: state of encoder, encoder sequence lengths and hidden\n            state of decoder's LSTM layers\n        :param inference: if True stores and repackages hidden state\n        \"\"\"\n        self.inference = inference\n\n        enc_context, enc_len, hidden = context\n        hidden = self.init_hidden(hidden)\n\n        x = self.embedder(inputs)\n\n        x, h, attn, scores = self.att_rnn(x, hidden[0], enc_context, enc_len)\n        self.append_hidden(h)\n\n        x = torch.cat((x, attn), dim=2)\n        x = self.dropout(x)\n        x, h = self.rnn_layers[0](x, hidden[1])\n        self.append_hidden(h)\n\n        for i in range(1, len(self.rnn_layers)):\n            residual = x\n            x = torch.cat((x, attn), dim=2)\n            x = self.dropout(x)\n            x, h = self.rnn_layers[i](x, hidden[i + 1])\n            self.append_hidden(h)\n            x = x + residual\n\n        x = self.classifier(x)\n        hidden = self.package_hidden()\n\n        return x, scores, [enc_context, enc_len, hidden]",
        "rewrite": "def forward(self, inputs, context, inference=False):\n    self.inference = inference\n\n    enc_context, enc_len, hidden = context\n    hidden = self.init_hidden(hidden)\n\n    x = self.embedder(inputs)\n\n    x, h, attn, scores = self.att_rnn(x, hidden[0], enc_context, enc_len)\n    self.append_hidden(h)\n\n    x = torch.cat((x, attn), dim=2)\n    x = self.dropout(x)\n    x, h = self.rnn_layers[0](x, hidden[1])\n    self.append_hidden(h)\n\n    for i in range(1, len(self.rnn_layers)):\n        residual = x\n        x = torch.cat((x, attn), dim=2)\n        x = self.dropout(x)\n        x, h = self.rnn_layers[i](x, hidden[i + 1])\n        self.append_hidden(h)\n        x = x + residual\n\n    x = self.classifier(x)\n    hidden = self.package_hidden()\n\n    return x, scores, [enc_context, enc_len, hidden]"
    },
    {
        "original": "def GetArtifactDependencies(rdf_artifact, recursive=False, depth=1):\n  \"\"\"Return a set of artifact dependencies.\n\n  Args:\n    rdf_artifact: RDF object artifact.\n    recursive: If True recurse into dependencies to find their dependencies.\n    depth: Used for limiting recursion depth.\n\n  Returns:\n    A set of strings containing the dependent artifact names.\n\n  Raises:\n    RuntimeError: If maximum recursion depth reached.\n  \"\"\"\n  deps = set()\n  for source in rdf_artifact.sources:\n    # ARTIFACT is the legacy name for ARTIFACT_GROUP\n    # per: https://github.com/ForensicArtifacts/artifacts/pull/143\n    # TODO(user): remove legacy support after migration.\n    if source.type in (rdf_artifacts.ArtifactSource.SourceType.ARTIFACT,\n                       rdf_artifacts.ArtifactSource.SourceType.ARTIFACT_GROUP):\n      if source.attributes.GetItem(\"names\"):\n        deps.update(source.attributes.GetItem(\"names\"))\n\n  if depth > 10:\n    raise RuntimeError(\"Max artifact recursion depth reached.\")\n\n  deps_set = set(deps)\n  if recursive:\n    for dep in deps:\n      artifact_obj = REGISTRY.GetArtifact(dep)\n      new_dep = GetArtifactDependencies(artifact_obj, True, depth=depth + 1)\n      if new_dep:\n        deps_set.update(new_dep)\n\n  return deps_set",
        "rewrite": "def get_artifact_dependencies(rdf_artifact, recursive=False, depth=1):\n    deps = set()\n    for source in rdf_artifact.sources:\n        if source.type in (rdf_artifacts.ArtifactSource.SourceType.ARTIFACT,\n                           rdf_artifacts.ArtifactSource.SourceType.ARTIFACT_GROUP):\n            if source.attributes.get(\"names\"):\n                deps.update(source.attributes.get(\"names\"))\n\n    if depth > 10:\n        raise RuntimeError(\"Max artifact recursion depth reached.\")\n\n    deps_set = set(deps)\n    if recursive:\n        for dep in deps:\n            artifact_obj = REGISTRY.GetArtifact(dep)\n            new_dep = get_artifact_dependencies(artifact_obj, recursive=True, depth=depth + 1)\n            if new_dep:\n                deps_set.update(new_dep)\n\n    return deps_set"
    },
    {
        "original": "def get_config_from_string(parts):\n    \"\"\"\n    Helper function to extract the configuration of a certain function from the column name.\n    The column name parts (split by \"__\") should be passed to this function. It will skip the\n    kind name and the function name and only use the parameter parts. These parts will be split up on \"_\"\n    into the parameter name and the parameter value. This value is transformed into a python object\n    (for example is \"(1, 2, 3)\" transformed into a tuple consisting of the ints 1, 2 and 3).\n\n    Returns None of no parameters are in the column name.\n\n    :param parts: The column name split up on \"__\"\n    :type parts: list\n    :return: a dictionary with all parameters, which are encoded in the column name.\n    :rtype: dict\n    \"\"\"\n    relevant_parts = parts[2:]\n    if not relevant_parts:\n        return\n\n    config_kwargs = [s.rsplit(\"_\", 1)[0] for s in relevant_parts]\n    config_values = [s.rsplit(\"_\", 1)[1] for s in relevant_parts]\n\n    dict_if_configs = {}\n\n    for key, value in zip(config_kwargs, config_values):\n        if value.lower() == \"nan\":\n            dict_if_configs[key] = np.NaN\n        elif value.lower() == \"-inf\":\n            dict_if_configs[key] = np.NINF\n        elif value.lower() == \"inf\":\n            dict_if_configs[key] = np.PINF\n        else:\n            dict_if_configs[key] = ast.literal_eval(value)\n\n    return dict_if_configs",
        "rewrite": "import numpy as np\nimport ast\n\ndef get_config_from_string(parts):\n    relevant_parts = parts[2:]\n    if not relevant_parts:\n        return\n\n    config_kwargs = [s.rsplit(\"_\", 1)[0] for s in relevant_parts]\n    config_values = [s.rsplit(\"_\", 1)[1] for s in relevant_parts]\n\n    dict_if_configs = {}\n\n    for key, value in zip(config_kwargs, config_values):\n        if value.lower() == \"nan\":\n            dict_if_configs[key] = np.NaN\n        elif value.lower() == \"-inf\":\n            dict_if_configs[key] = np.NINF\n        elif value.lower() == \"inf\":\n            dict_if_configs[key] = np.PINF\n        else:\n            dict_if_configs[key] = ast.literal_eval(value)\n\n    return dict_if_configs"
    },
    {
        "original": "def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst",
        "rewrite": "def _get_values(self, lst, list_columns):\n    retlst = []\n    for item in lst:\n        retdict = {}\n        for col in list_columns:\n            retdict[col] = self._get_attr_value(item, col)\n        retlst.append(retdict)\n    return retlst"
    },
    {
        "original": "def _Initialize(self):\n    \"\"\"Initialize the filter configuration from a validated configuration.\n\n    The configuration is read. Active filters are added to the matcher list,\n    which is used to process the Stat values.\n    \"\"\"\n\n    if self.cfg.mask:\n      self.mask = int(self.cfg.mask[0], 8)\n    else:\n      self.mask = 0o7777\n    if self.cfg.mode:\n      self.mode = int(self.cfg.mode[0], 8)\n      self.matchers.append(self._MatchMode)\n\n    if self.cfg.gid:\n      for gid in self.cfg.gid:\n        matched = self._UID_GID_RE.match(gid)\n        if matched:\n          o, v = matched.groups()\n          self.gid_matchers.append((self._Comparator(o), int(v)))\n      self.matchers.append(self._MatchGid)\n\n    if self.cfg.uid:\n      for uid in self.cfg.uid:\n        matched = self._UID_GID_RE.match(uid)\n        if matched:\n          o, v = matched.groups()\n          self.uid_matchers.append((self._Comparator(o), int(v)))\n      self.matchers.append(self._MatchUid)\n\n    if self.cfg.file_re:\n      self.file_re = re.compile(self.cfg.file_re[0])\n      self.matchers.append(self._MatchFile)\n\n    if self.cfg.path_re:\n      self.path_re = re.compile(self.cfg.path_re[0])\n      self.matchers.append(self._MatchPath)\n\n    if self.cfg.file_type:\n      self.file_type = self._TYPES.get(self.cfg.file_type[0].upper())\n      self.matchers.append(self._MatchType)",
        "rewrite": "def _initialize(self):\n    if self.cfg.mask:\n        self.mask = int(self.cfg.mask[0], 8)\n    else:\n        self.mask = 0o7777\n    if self.cfg.mode:\n        self.mode = int(self.cfg.mode[0], 8)\n        self.matchers.append(self._match_mode)\n\n    if self.cfg.gid:\n        for gid in self.cfg.gid:\n            matched = self._UID_GID_RE.match(gid)\n            if matched:\n                o, v = matched.groups()\n                self.gid_matchers.append((self._Comparator(o), int(v)))\n        self.matchers.append(self._match_gid)\n\n    if self.cfg.uid:\n        for uid in self.cfg.uid:\n            matched = self._UID_GID_RE.match(uid)\n            if matched:\n                o, v = matched.groups()\n                self.uid_matchers.append((self._Comparator(o), int(v)))\n        self.matchers.append(self._match_uid)\n\n    if self.cfg.file_re:\n        self.file_re = re.compile(self.cfg.file_re[0])\n        self.matchers.append(self._match_file)\n\n    if self.cfg.path_re:\n        self.path_re = re.compile(self.cfg.path_re[0])\n        self.matchers.append(self._match_path)\n\n    if self.cfg.file_type:\n        self.file_type = self._TYPES.get(self.cfg.file_type[0].upper())\n        self.matchers.append(self._match_type)"
    },
    {
        "original": "def _build_list(option_value, item_kind):\n    \"\"\"\n    pass in an option to check for a list of items, create a list of dictionary of items to set\n    for this option\n    \"\"\"\n    #specify profiles if provided\n    if option_value is not None:\n\n        items = []\n\n        #if user specified none, return an empty list\n        if option_value == 'none':\n            return items\n\n        #was a list already passed in?\n        if not isinstance(option_value, list):\n            values = option_value.split(',')\n        else:\n            values = option_value\n\n        for value in values:\n            # sometimes the bigip just likes a plain ol list of items\n            if item_kind is None:\n                items.append(value)\n            # other times it's picky and likes key value pairs...\n            else:\n                items.append({'kind': item_kind, 'name': value})\n        return items\n    return None",
        "rewrite": "def _build_list(option_value, item_kind):\n    if option_value is not None:\n        items = []\n        if option_value == 'none':\n            return items\n        if not isinstance(option_value, list):\n            values = option_value.split(',')\n        else:\n            values = option_value\n        for value in values:\n            if item_kind is None:\n                items.append(value)\n            else:\n                items.append({'kind': item_kind, 'name': value})\n        return items\n    return None"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'intents') and self.intents is not None:\n            _dict['intents'] = [x._to_dict() for x in self.intents]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict",
        "rewrite": "def to_dict(self):\n        _dict = {}\n        if hasattr(self, 'intents') and self.intents is not None:\n            _dict['intents'] = [x.to_dict() for x in self.intents]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination.to_dict()\n        return _dict"
    },
    {
        "original": "def resolve_group_names(self, r, target_group_ids, groups):\n        \"\"\"Resolve any security group names to the corresponding group ids\n\n        With the context of a given network attached resource.\n        \"\"\"\n        names = self.get_group_names(target_group_ids)\n        if not names:\n            return target_group_ids\n\n        target_group_ids = list(target_group_ids)\n        vpc_id = self.vpc_expr.search(r)\n        if not vpc_id:\n            raise PolicyExecutionError(self._format_error(\n                \"policy:{policy} non vpc attached resource used \"\n                \"with modify-security-group: {resource_id}\",\n                resource_id=r[self.manager.resource_type.id]))\n\n        found = False\n        for n in names:\n            for g in groups:\n                if g['GroupName'] == n and g['VpcId'] == vpc_id:\n                    found = g['GroupId']\n            if not found:\n                raise PolicyExecutionError(self._format_error((\n                    \"policy:{policy} could not resolve sg:{name} for \"\n                    \"resource:{resource_id} in vpc:{vpc}\"),\n                    name=n,\n                    resource_id=r[self.manager.resource_type.id], vpc=vpc_id))\n            target_group_ids.remove(n)\n            target_group_ids.append(found)\n        return target_group_ids",
        "rewrite": "def resolve_group_names(self, r, target_group_ids, groups):\n    names = self.get_group_names(target_group_ids)\n    if not names:\n        return target_group_ids\n    \n    target_group_ids = list(target_group_ids)\n    vpc_id = self.vpc_expr.search(r)\n    if not vpc_id:\n        raise PolicyExecutionError(self._format_error(\n            \"policy:{policy} non vpc attached resource used \"\n            \"with modify-security-group: {resource_id}\",\n            resource_id=r[self.manager.resource_type.id]))\n\n    found = False\n    for n in names:\n        for g in groups:\n            if g['GroupName'] == n and g['VpcId'] == vpc_id:\n                found = g['GroupId']\n                break\n        if not found:\n            raise PolicyExecutionError(self._format_error(\n                \"policy:{policy} could not resolve sg:{name} for \"\n                \"resource:{resource_id} in vpc:{vpc}\",\n                name=n,\n                resource_id=r[self.manager.resource_type.id], vpc=vpc_id))\n        target_group_ids.remove(n)\n        target_group_ids.append(found)\n        \n    return target_group_ids"
    },
    {
        "original": "def update_affinity_group(kwargs=None, conn=None, call=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Update an affinity group's properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_affinity_group my-azure name=my_group label=my_group\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The update_affinity_group function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A name must be specified as \"name\"')\n\n    if 'label' not in kwargs:\n        raise SaltCloudSystemExit('A label must be specified as \"label\"')\n\n    conn.update_affinity_group(\n        affinity_group_name=kwargs['name'],\n        label=kwargs['label'],\n        description=kwargs.get('description', None),\n    )\n    return show_affinity_group(kwargs={'name': kwargs['name']}, call='function')",
        "rewrite": "def update_affinity_group(kwargs=None, conn=None, call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit('The update_affinity_group function must be called with -f or --function.')\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A name must be specified as \"name\"')\n\n    if 'label' not in kwargs:\n        raise SaltCloudSystemExit('A label must be specified as \"label\"')\n\n    conn.update_affinity_group(\n        affinity_group_name=kwargs['name'],\n        label=kwargs['label'],\n        description=kwargs.get('description', None),\n    )\n    return show_affinity_group(kwargs={'name': kwargs['name']}, call='function')"
    },
    {
        "original": "def get_order(self):\n        \"\"\" Returns the result order by clauses\n\n        :rtype: str or None\n        \"\"\"\n        # first get the filtered attributes in order as they must appear\n        # in the order_by first\n        if not self.has_order:\n            return None\n        filter_order_clauses = OrderedDict([(filter_attr[0], None)\n                                            for filter_attr in self._filters\n                                            if isinstance(filter_attr, tuple)])\n\n        # any order_by attribute that appears in the filters is ignored\n        order_by_dict = self._order_by.copy()\n        for filter_oc in filter_order_clauses.keys():\n            direction = order_by_dict.pop(filter_oc, None)\n            filter_order_clauses[filter_oc] = direction\n\n        filter_order_clauses.update(\n            order_by_dict)  # append any remaining order_by clause\n\n        if filter_order_clauses:\n            return ','.join(['{} {}'.format(attribute,\n                                            direction if direction else '')\n                            .strip()\n                             for attribute, direction in\n                             filter_order_clauses.items()])\n        else:\n            return None",
        "rewrite": "def get_order(self):\n    if not self.has_order:\n        return None\n    filter_order_clauses = OrderedDict([(filter_attr[0], None)\n                                        for filter_attr in self._filters\n                                        if isinstance(filter_attr, tuple)])\n    order_by_dict = self._order_by.copy()\n    for filter_oc in filter_order_clauses.keys():\n        direction = order_by_dict.pop(filter_oc, None)\n        filter_order_clauses[filter_oc] = direction\n    filter_order_clauses.update(order_by_dict)\n    if filter_order_clauses:\n        return ','.join(['{} {}'.format(attribute,\n                                        direction if direction else '')\n                        .strip()\n                        for attribute, direction in\n                        filter_order_clauses.items()])\n    else:\n        return None"
    },
    {
        "original": "def channels_rename(self, *, channel: str, name: str, **kwargs) -> SlackResponse:\n        \"\"\"Renames a channel.\n\n        Args:\n            channel (str): The channel id. e.g. 'C1234567890'\n            name (str): The new channel name. e.g. 'newchannel'\n        \"\"\"\n        self._validate_xoxp_token()\n        kwargs.update({\"channel\": channel, \"name\": name})\n        return self.api_call(\"channels.rename\", json=kwargs)",
        "rewrite": "def channels_rename(self, *, channel: str, name: str, **kwargs) -> SlackResponse:\n    self._validate_xoxp_token()\n    data = {\"channel\": channel, \"name\": name}\n    data.update(kwargs)\n    return self.api_call(\"channels.rename\", json=data)"
    },
    {
        "original": "def CheckApprovalRequest(approval_request):\n  \"\"\"Checks if an approval request is granted.\"\"\"\n\n  at = rdf_objects.ApprovalRequest.ApprovalType\n\n  if approval_request.approval_type == at.APPROVAL_TYPE_CLIENT:\n    return CheckClientApprovalRequest(approval_request)\n  elif approval_request.approval_type == at.APPROVAL_TYPE_HUNT:\n    return CheckHuntApprovalRequest(approval_request)\n  elif approval_request.approval_type == at.APPROVAL_TYPE_CRON_JOB:\n    return CheckCronJobApprovalRequest(approval_request)\n  else:\n    raise ValueError(\n        \"Invalid approval type: %s\" % approval_request.approval_type)",
        "rewrite": "def CheckApprovalRequest(approval_request):\n    at = rdf_objects.ApprovalRequest.ApprovalType\n\n    if approval_request.approval_type == at.APPROVAL_TYPE_CLIENT:\n        return CheckClientApprovalRequest(approval_request)\n    elif approval_request.approval_type == at.APPROVAL_TYPE_HUNT:\n        return CheckHuntApprovalRequest(approval_request)\n    elif approval_request.approval_type == at.APPROVAL_TYPE_CRON_JOB:\n        return CheckCronJobApprovalRequest(approval_request)\n    else:\n        raise ValueError(\n            \"Invalid approval type: %s\" % approval_request.approval_type)"
    },
    {
        "original": "def reminders_list(self, **kwargs) -> SlackResponse:\n        \"\"\"Lists all reminders created by or for a given user.\"\"\"\n        self._validate_xoxp_token()\n        return self.api_call(\"reminders.list\", http_verb=\"GET\", params=kwargs)",
        "rewrite": "def reminders_list(self, **kwargs) -> SlackResponse:\n    self._validate_xoxp_token()\n    return self.api_call(\"reminders.list\", http_verb=\"GET\", params=kwargs)"
    },
    {
        "original": "def build_arch(self, arch):\n        \"\"\"\n        Creates expected build and symlinks system Python version.\n        \"\"\"\n        self.ctx.hostpython = '/usr/bin/false'\n        # creates the sub buildir (used by other recipes)\n        # https://github.com/kivy/python-for-android/issues/1154\n        sub_build_dir = join(self.get_build_dir(), 'build')\n        shprint(sh.mkdir, '-p', sub_build_dir)\n        python3crystax = self.get_recipe('python3crystax', self.ctx)\n        system_python = sh.which(\"python\" + python3crystax.version)\n        if system_python is None:\n            raise OSError(\n                ('Trying to use python3crystax=={} but this Python version '\n                 'is not installed locally.').format(python3crystax.version))\n        link_dest = join(self.get_build_dir(), 'hostpython')\n        shprint(sh.ln, '-sf', system_python, link_dest)",
        "rewrite": "def build_arch(self, arch):\n    self.ctx.hostpython = '/usr/bin/false'\n    sub_build_dir = os.path.join(self.get_build_dir(), 'build')\n    shprint(sh.mkdir, '-p', sub_build_dir)\n    python3crystax = self.get_recipe('python3crystax', self.ctx)\n    system_python = sh.which(\"python\" + python3crystax.version)\n    \n    if system_python is None:\n        raise OSError(\n            ('Trying to use python3crystax=={} but this Python version '\n             'is not installed locally.').format(python3crystax.version))\n    \n    link_dest = os.path.join(self.get_build_dir(), 'hostpython')\n    shprint(sh.ln, '-sf', system_python, link_dest)"
    },
    {
        "original": "def freq_units(units):\n    \"\"\"\n    Returns conversion factor from THz to the requred units and the label in the form of a namedtuple\n    Accepted values: thz, ev, mev, ha, cm-1, cm^-1\n    \"\"\"\n\n    d = {\"thz\": FreqUnits(1, \"THz\"),\n         \"ev\": FreqUnits(const.value(\"hertz-electron volt relationship\") * const.tera, \"eV\"),\n         \"mev\": FreqUnits(const.value(\"hertz-electron volt relationship\") * const.tera / const.milli, \"meV\"),\n         \"ha\": FreqUnits(const.value(\"hertz-hartree relationship\") * const.tera, \"Ha\"),\n         \"cm-1\": FreqUnits(const.value(\"hertz-inverse meter relationship\") * const.tera * const.centi, \"cm^{-1}\"),\n         'cm^-1': FreqUnits(const.value(\"hertz-inverse meter relationship\") * const.tera * const.centi, \"cm^{-1}\")\n         }\n    try:\n        return d[units.lower().strip()]\n    except KeyError:\n        raise KeyError('Value for units `{}` unknown\\nPossible values are:\\n {}'.format(units, list(d.keys())))",
        "rewrite": "from collections import namedtuple\nfrom scipy.constants import value, tera, milli, centi\n\nFreqUnits = namedtuple('FreqUnits', ['conversion_factor', 'label'])\n\ndef freq_units(units):\n    d = {\"thz\": FreqUnits(1, \"THz\"),\n         \"ev\": FreqUnits(value(\"hertz-electron volt relationship\") * tera, \"eV\"),\n         \"mev\": FreqUnits(value(\"hertz-electron volt relationship\") * tera / milli, \"meV\"),\n         \"ha\": FreqUnits(value(\"hertz-hartree relationship\") * tera, \"Ha\"),\n         \"cm-1\": FreqUnits(value(\"hertz-inverse meter relationship\") * tera * centi, \"cm^{-1}\"),\n         'cm^-1': FreqUnits(value(\"hertz-inverse meter relationship\") * tera * centi, \"cm^{-1}\")\n         }\n    try:\n        return d[units.lower().strip()]\n    except KeyError:\n        raise KeyError('Value for units `{}` unknown\\nPossible values are:\\n {}'.format(units, list(d.keys())))"
    },
    {
        "original": "def _phi_node_contains(self, phi_variable, variable):\n        \"\"\"\n        Checks if `phi_variable` is a phi variable, and if it contains `variable` as a sub-variable.\n\n        :param phi_variable:\n        :param variable:\n        :return:\n        \"\"\"\n\n        if self.variable_manager[self.function.addr].is_phi_variable(phi_variable):\n            return variable in self.variable_manager[self.function.addr].get_phi_subvariables(phi_variable)\n        return False",
        "rewrite": "def _phi_node_contains(self, phi_variable, variable):\n    if self.variable_manager[self.function.addr].is_phi_variable(phi_variable):\n        return variable in self.variable_manager[self.function.addr].get_phi_subvariables(phi_variable)\n    return False"
    },
    {
        "original": "def get_parents(self):\n        \"\"\"\n        Add the parents to BIF\n\n        Returns\n        -------\n        dict: dict of type {variable: a list of parents}\n\n        Example\n        -------\n        >>> from pgmpy.readwrite import BIFReader, BIFWriter\n        >>> model = BIFReader('dog-problem.bif').get_model()\n        >>> writer = BIFWriter(model)\n        >>> writer.get_parents()\n        {'bowel-problem': [],\n         'dog-out': ['bowel-problem', 'family-out'],\n         'family-out': [],\n         'hear-bark': ['dog-out'],\n         'light-on': ['family-out']}\n        \"\"\"\n        cpds = self.model.get_cpds()\n        cpds.sort(key=lambda x: x.variable)\n        variable_parents = {}\n        for cpd in cpds:\n            variable_parents[cpd.variable] = []\n            for parent in sorted(cpd.variables[:0:-1]):\n                variable_parents[cpd.variable].append(parent)\n        return variable_parents",
        "rewrite": "def get_parents(self):\n    cpds = self.model.get_cpds()\n    cpds.sort(key=lambda x: x.variable)\n    variable_parents = {}\n    for cpd in cpds:\n        variable_parents[cpd.variable] = []\n        for parent in sorted(cpd.variables[:0:-1]):\n            variable_parents[cpd.variable].append(parent)\n    return variable_parents"
    },
    {
        "original": "def changes(self, **kwargs):\n        \"\"\"List the merge request changes.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: List of changes\n        \"\"\"\n        path = '%s/%s/changes' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)",
        "rewrite": "def changes(self, **kwargs):\n    path = f\"{self.manager.path}/{self.get_id()}/changes\"\n    return self.manager.gitlab.http_get(path, **kwargs)"
    },
    {
        "original": "def _do_refresh_session(self):\n        \"\"\":returns: `!True` if it had to create new session\"\"\"\n        if self._session and self._last_session_refresh + self._loop_wait > time.time():\n            return False\n\n        if self._session:\n            try:\n                self._client.session.renew(self._session)\n            except NotFound:\n                self._session = None\n        ret = not self._session\n        if ret:\n            try:\n                self._session = self._client.session.create(name=self._scope + '-' + self._name,\n                                                            checks=self.__session_checks,\n                                                            lock_delay=0.001, behavior='delete')\n            except InvalidSessionTTL:\n                logger.exception('session.create')\n                self.adjust_ttl()\n                raise\n\n        self._last_session_refresh = time.time()\n        return ret",
        "rewrite": "def _do_refresh_session(self):\n    if self._session and self._last_session_refresh + self._loop_wait > time.time():\n        return False\n\n    if self._session:\n        try:\n            self._client.session.renew(self._session)\n        except NotFound:\n            self._session = None\n    ret = not self._session\n    if ret:\n        try:\n            self._session = self._client.session.create(name=self._scope + '-' + self._name,\n                                                        checks=self.__session_checks,\n                                                        lock_delay=0.001, behavior='delete')\n        except InvalidSessionTTL:\n            logger.exception('session.create')\n            self.adjust_ttl()\n            raise\n\n    self._last_session_refresh = time.time()\n    return ret"
    },
    {
        "original": "def embed_data(views, drop_defaults=True, state=None):\n    \"\"\"Gets data for embedding.\n\n    Use this to get the raw data for embedding if you have special\n    formatting needs.\n\n    Parameters\n    ----------\n    {views_attribute}\n    drop_defaults: boolean\n        Whether to drop default values from the widget states.\n    state: dict or None (default)\n        The state to include. When set to None, the state of all widgets\n        know to the widget manager is included. Otherwise it uses the\n        passed state directly. This allows for end users to include a\n        smaller state, under the responsibility that this state is\n        sufficient to reconstruct the embedded views.\n\n    Returns\n    -------\n    A dictionary with the following entries:\n        manager_state: dict of the widget manager state data\n        view_specs: a list of widget view specs\n    \"\"\"\n    if views is None:\n        views = [w for w in Widget.widgets.values() if isinstance(w, DOMWidget)]\n    else:\n        try:\n            views[0]\n        except (IndexError, TypeError):\n            views = [views]\n\n    if state is None:\n        # Get state of all known widgets\n        state = Widget.get_manager_state(drop_defaults=drop_defaults, widgets=None)['state']\n\n    # Rely on ipywidget to get the default values\n    json_data = Widget.get_manager_state(widgets=[])\n    # but plug in our own state\n    json_data['state'] = state\n\n    view_specs = [w.get_view_spec() for w in views]\n\n    return dict(manager_state=json_data, view_specs=view_specs)",
        "rewrite": "def embed_data(views=None, drop_defaults=True, state=None):\n    if views is None:\n        views = [w for w in Widget.widgets.values() if isinstance(w, DOMWidget)]\n    else:\n        try:\n            views[0]\n        except (IndexError, TypeError):\n            views = [views]\n\n    if state is None:\n        state = Widget.get_manager_state(drop_defaults=drop_defaults, widgets=None)['state']\n\n    json_data = Widget.get_manager_state(widgets=[])\n    json_data['state'] = state\n\n    view_specs = [w.get_view_spec() for w in views]\n\n    return dict(manager_state=json_data, view_specs=view_specs)"
    },
    {
        "original": "async def recv(self) -> Data:\n        \"\"\"\n        This coroutine receives the next message.\n\n        It returns a :class:`str` for a text frame and :class:`bytes` for a\n        binary frame.\n\n        When the end of the message stream is reached, :meth:`recv` raises\n        :exc:`~websockets.exceptions.ConnectionClosed`. This can happen after\n        a normal connection closure, a protocol error or a network failure.\n\n        .. versionchanged:: 3.0\n\n            :meth:`recv` used to return ``None`` instead. Refer to the\n            changelog for details.\n\n        Canceling :meth:`recv` is safe. There's no risk of losing the next\n        message. The next invocation of :meth:`recv` will return it. This\n        makes it possible to enforce a timeout by wrapping :meth:`recv` in\n        :func:`~asyncio.wait_for`.\n\n        .. versionchanged:: 7.0\n\n            Calling :meth:`recv` concurrently raises :exc:`RuntimeError`.\n\n        \"\"\"\n        if self._pop_message_waiter is not None:\n            raise RuntimeError(\n                \"cannot call recv() while another coroutine \"\n                \"is already waiting for the next message\"\n            )\n\n        # Don't await self.ensure_open() here:\n        # - messages could be available in the queue even if the connection\n        #   is closed;\n        # - messages could be received before the closing frame even if the\n        #   connection is closing.\n\n        # Wait until there's a message in the queue (if necessary) or the\n        # connection is closed.\n        while len(self.messages) <= 0:\n            pop_message_waiter: asyncio.Future[None] = self.loop.create_future()\n            self._pop_message_waiter = pop_message_waiter\n            try:\n                # If asyncio.wait() is canceled, it doesn't cancel\n                # pop_message_waiter and self.transfer_data_task.\n                await asyncio.wait(\n                    [pop_message_waiter, self.transfer_data_task],\n                    loop=self.loop,\n                    return_when=asyncio.FIRST_COMPLETED,\n                )\n            finally:\n                self._pop_message_waiter = None\n\n            # If asyncio.wait(...) exited because self.transfer_data_task\n            # completed before receiving a new message, raise a suitable\n            # exception (or return None if legacy_recv is enabled).\n            if not pop_message_waiter.done():\n                if self.legacy_recv:\n                    return None  # type: ignore\n                else:\n                    assert self.state in [State.CLOSING, State.CLOSED]\n                    # Wait until the connection is closed to raise\n                    # ConnectionClosed with the correct code and reason.\n                    await self.ensure_open()\n\n        # Pop a message from the queue.\n        message = self.messages.popleft()\n\n        # Notify transfer_data().\n        if self._put_message_waiter is not None:\n            self._put_message_waiter.set_result(None)\n            self._put_message_waiter = None\n\n        return message",
        "rewrite": "async def recv(self) -> Data:\n        if self._pop_message_waiter is not None:\n            raise RuntimeError(\n                \"cannot call recv() while another coroutine \"\n                \"is already waiting for the next message\"\n            )\n\n        while len(self.messages) <= 0:\n            pop_message_waiter = self.loop.create_future()\n            self._pop_message_waiter = pop_message_waiter\n            try:\n                await asyncio.wait(\n                    [pop_message_waiter, self.transfer_data_task],\n                    loop=self.loop,\n                    return_when=asyncio.FIRST_COMPLETED,\n                )\n            finally:\n                self._pop_message_waiter = None\n\n            if not pop_message_waiter.done():\n                if self.legacy_recv:\n                    return None\n                else:\n                    assert self.state in [State.CLOSING, State.CLOSED]\n                    await self.ensure_open()\n\n        message = self.messages.popleft()\n\n        if self._put_message_waiter is not None:\n            self._put_message_waiter.set_result(None)\n            self._put_message_waiter = None\n\n        return message"
    },
    {
        "original": "def input(self, data):\n        \"\"\" \u5c0f\u6570\u636e\u7247\u6bb5\u62fc\u63a5\u6210\u5b8c\u6574\u6570\u636e\u5305\n            \u5982\u679c\u5185\u5bb9\u8db3\u591f\u5219yield\u6570\u636e\u5305\n        \"\"\"\n        self.buf += data\n        while len(self.buf) > HEADER_SIZE:\n            data_len = struct.unpack('i', self.buf[0:HEADER_SIZE])[0]\n            if len(self.buf) >= data_len + HEADER_SIZE:\n                content = self.buf[HEADER_SIZE:data_len + HEADER_SIZE]\n                self.buf = self.buf[data_len + HEADER_SIZE:]\n                yield content\n            else:\n                break",
        "rewrite": "def input(self, data):\n    self.buf += data\n    while len(self.buf) > HEADER_SIZE:\n        data_len = struct.unpack('i', self.buf[0:HEADER_SIZE])[0]\n        if len(self.buf) >= data_len + HEADER_SIZE:\n            content = self.buf[HEADER_SIZE:data_len + HEADER_SIZE]\n            self.buf = self.buf[data_len + HEADER_SIZE:]\n            yield content\n        else:\n            break"
    },
    {
        "original": "def _gpdfit(x):\n    \"\"\"Estimate the parameters for the Generalized Pareto Distribution (GPD).\n\n    Empirical Bayes estimate for the parameters of the generalized Pareto\n    distribution given the data.\n\n    Parameters\n    ----------\n    x : array\n        sorted 1D data array\n\n    Returns\n    -------\n    k : float\n        estimated shape parameter\n    sigma : float\n        estimated scale parameter\n    \"\"\"\n    prior_bs = 3\n    prior_k = 10\n    len_x = len(x)\n    m_est = 30 + int(len_x ** 0.5)\n\n    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n    b_ary /= prior_bs * x[int(len_x / 4 + 0.5) - 1]\n    b_ary += 1 / x[-1]\n\n    k_ary = np.log1p(-b_ary[:, None] * x).mean(axis=1)  # pylint: disable=no-member\n    len_scale = len_x * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n\n    # remove negligible weights\n    real_idxs = weights >= 10 * np.finfo(float).eps\n    if not np.all(real_idxs):\n        weights = weights[real_idxs]\n        b_ary = b_ary[real_idxs]\n    # normalise weights\n    weights /= weights.sum()\n\n    # posterior mean for b\n    b_post = np.sum(b_ary * weights)\n    # estimate for k\n    k_post = np.log1p(-b_post * x).mean()  # pylint: disable=invalid-unary-operand-type,no-member\n    # add prior for k_post\n    k_post = (len_x * k_post + prior_k * 0.5) / (len_x + prior_k)\n    sigma = -k_post / b_post\n\n    return k_post, sigma",
        "rewrite": "import numpy as np\n\ndef _gpdfit(x):\n    prior_bs = 3\n    prior_k = 10\n    len_x = len(x)\n    m_est = 30 + int(len_x ** 0.5)\n\n    b_ary = 1 - np.sqrt(m_est / (np.arange(1, m_est + 1, dtype=float) - 0.5))\n    b_ary /= prior_bs * x[int(len_x / 4 + 0.5) - 1]\n    b_ary += 1 / x[-1]\n\n    k_ary = np.log1p(-b_ary[:, None] * x).mean(axis=1)\n    len_scale = len_x * (np.log(-(b_ary / k_ary)) - k_ary - 1)\n    weights = 1 / np.exp(len_scale - len_scale[:, None]).sum(axis=1)\n\n    real_idxs = weights >= 10 * np.finfo(float).eps\n    if not np.all(real_idxs):\n        weights = weights[real_idxs]\n        b_ary = b_ary[real_idxs]\n   \n    weights /= weights.sum()\n\n    b_post = np.sum(b_ary * weights)\n    k_post = np.log1p(-b_post * x).mean()\n    k_post = (len_x * k_post + prior_k * 0.5) / (len_x + prior_k)\n    sigma = -k_post / b_post\n\n    return k_post, sigma"
    },
    {
        "original": "def build_loss(model_logits, sparse_targets):\n  \"\"\"Compute the log loss given predictions and targets.\"\"\"\n  time_major_shape = [FLAGS.unroll_steps, FLAGS.batch_size]\n  flat_batch_shape = [FLAGS.unroll_steps * FLAGS.batch_size, -1]\n  xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=tf.reshape(model_logits, flat_batch_shape),\n      labels=tf.reshape(sparse_targets, flat_batch_shape[:-1]))\n  xent = tf.reshape(xent, time_major_shape)\n  # Sum over the sequence.\n  sequence_neg_log_prob = tf.reduce_sum(xent, axis=0)\n  # Average over the batch.\n  return tf.reduce_mean(sequence_neg_log_prob, axis=0)",
        "rewrite": "def build_loss(model_logits, sparse_targets):\n    time_major_shape = [FLAGS.unroll_steps, FLAGS.batch_size]\n    flat_batch_shape = [FLAGS.unroll_steps * FLAGS.batch_size, -1]\n    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=tf.reshape(model_logits, flat_batch_shape),\n        labels=tf.reshape(sparse_targets, flat_batch_shape[:-1]))\n    xent = tf.reshape(xent, time_major_shape)\n    sequence_neg_log_prob = tf.reduce_sum(xent, axis=0)\n    return tf.reduce_mean(sequence_neg_log_prob, axis=0)"
    },
    {
        "original": "def pred_from_list(self, species_list):\n        \"\"\"\n        There are an exceptionally large number of substitutions to\n        look at (260^n), where n is the number of species in the\n        list. We need a more efficient than brute force way of going\n        through these possibilities. The brute force method would be::\n\n            output = []\n            for p in itertools.product(self._sp.species_list\n                                       , repeat = len(species_list)):\n                if self._sp.conditional_probability_list(p, species_list)\n                                       > self._threshold:\n                    output.append(dict(zip(species_list,p)))\n            return output\n\n        Instead of that we do a branch and bound.\n\n        Args:\n            species_list:\n                list of species in the starting structure\n\n        Returns:\n            list of dictionaries, each including a substitutions\n            dictionary, and a probability value\n        \"\"\"\n        species_list = get_el_sp(species_list)\n        # calculate the highest probabilities to help us stop the recursion\n        max_probabilities = []\n        for s2 in species_list:\n            max_p = 0\n            for s1 in self._sp.species:\n                max_p = max([self._sp.cond_prob(s1, s2), max_p])\n            max_probabilities.append(max_p)\n        output = []\n\n        def _recurse(output_prob, output_species):\n            best_case_prob = list(max_probabilities)\n            best_case_prob[:len(output_prob)] = output_prob\n            if functools.reduce(mul, best_case_prob) > self._threshold:\n                if len(output_species) == len(species_list):\n                    odict = {\n                        'substitutions':\n                            dict(zip(species_list, output_species)),\n                        'probability': functools.reduce(mul, best_case_prob)}\n                    output.append(odict)\n                    return\n                for sp in self._sp.species:\n                    i = len(output_prob)\n                    prob = self._sp.cond_prob(sp, species_list[i])\n                    _recurse(output_prob + [prob], output_species + [sp])\n\n        _recurse([], [])\n        logging.info('{} substitutions found'.format(len(output)))\n        return output",
        "rewrite": "def pred_from_list(self, species_list):\n        species_list = get_el_sp(species_list)\n        max_probabilities = []\n        for s2 in species_list:\n            max_p = 0\n            for s1 in self._sp.species:\n                max_p = max([self._sp.cond_prob(s1, s2), max_p])\n            max_probabilities.append(max_p)\n        output = []\n        \n        def _recurse(output_prob, output_species):\n            best_case_prob = list(max_probabilities)\n            best_case_prob[:len(output_prob)] = output_prob\n            if functools.reduce(mul, best_case_prob) > self._threshold:\n                if len(output_species) == len(species_list):\n                    odict = {\n                        'substitutions': dict(zip(species_list, output_species)),\n                        'probability': functools.reduce(mul, best_case_prob)\n                    }\n                    output.append(odict)\n                    return\n                for sp in self._sp.species:\n                    i = len(output_prob)\n                    prob = self._sp.cond_prob(sp, species_list[i])\n                    _recurse(output_prob + [prob], output_species + [sp])\n        \n        _recurse([], [])\n        logging.info('{} substitutions found'.format(len(output)))\n        return output"
    },
    {
        "original": "def _FlowProcessingRequestHandlerLoop(self, handler):\n    \"\"\"The main loop for the flow processing request queue.\"\"\"\n    while not self.flow_processing_request_handler_stop:\n      try:\n        msgs = self._LeaseFlowProcessingReqests()\n        if msgs:\n          for m in msgs:\n            self.flow_processing_request_handler_pool.AddTask(\n                target=handler, args=(m,))\n        else:\n          time.sleep(self._FLOW_REQUEST_POLL_TIME_SECS)\n\n      except Exception as e:  # pylint: disable=broad-except\n        logging.exception(\"_FlowProcessingRequestHandlerLoop raised %s.\", e)\n        break",
        "rewrite": "def _FlowProcessingRequestHandlerLoop(self, handler):\n    while not self.flow_processing_request_handler_stop:\n        try:\n            msgs = self._LeaseFlowProcessingRequests()\n            if msgs:\n                for m in msgs:\n                    self.flow_processing_request_handler_pool.AddTask(\n                        target=handler, args=(m,))\n            else:\n                time.sleep(self._FLOW_REQUEST_POLL_TIME_SECS)\n\n        except Exception as e:\n            logging.exception(\"_FlowProcessingRequestHandlerLoop raised %s.\", e)\n            break"
    },
    {
        "original": "def keepvol_on_destroy(name, kwargs=None, call=None):\n    \"\"\"\n    Do not delete all/specified EBS volumes upon instance termination\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a keepvol_on_destroy mymachine\n    \"\"\"\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The keepvol_on_destroy action must be called with -a or --action.'\n        )\n\n    if not kwargs:\n        kwargs = {}\n\n    device = kwargs.get('device', None)\n    volume_id = kwargs.get('volume_id', None)\n\n    return _toggle_delvol(name=name, device=device,\n                          volume_id=volume_id, value='false')",
        "rewrite": "def keepvol_on_destroy(name, kwargs=None, call=None):\n    if call != 'action':\n        raise SaltCloudSystemExit('The keepvol_on_destroy action must be called with -a or --action.')\n\n    if not kwargs:\n        kwargs = {}\n\n    device = kwargs.get('device', None)\n    volume_id = kwargs.get('volume_id', None)\n\n    return _toggle_delvol(name=name, device=device, volume_id=volume_id, value='false')"
    },
    {
        "original": "async def _download_web_document(cls, web, file, progress_callback):\n        \"\"\"\n        Specialized version of .download_media() for web documents.\n        \"\"\"\n        if not aiohttp:\n            raise ValueError(\n                'Cannot download web documents without the aiohttp '\n                'dependency install it (pip install aiohttp)'\n            )\n\n        # TODO Better way to get opened handles of files and auto-close\n        in_memory = file is bytes\n        if in_memory:\n            f = io.BytesIO()\n        elif isinstance(file, str):\n            kind, possible_names = cls._get_kind_and_names(web.attributes)\n            file = cls._get_proper_filename(\n                file, kind, utils.get_extension(web),\n                possible_names=possible_names\n            )\n            f = open(file, 'wb')\n        else:\n            f = file\n\n        try:\n            with aiohttp.ClientSession() as session:\n                # TODO Use progress_callback; get content length from response\n                # https://github.com/telegramdesktop/tdesktop/blob/c7e773dd9aeba94e2be48c032edc9a78bb50234e/Telegram/SourceFiles/ui/images.cpp#L1318-L1319\n                async with session.get(web.url) as response:\n                    while True:\n                        chunk = await response.content.read(128 * 1024)\n                        if not chunk:\n                            break\n                        f.write(chunk)\n        finally:\n            if isinstance(file, str) or file is bytes:\n                f.close()\n\n        return f.getvalue() if in_memory else file",
        "rewrite": "async def _download_web_document(cls, web, file, progress_callback):\n    \"\"\"\n    Specialized version of .download_media() for web documents.\n    \"\"\"\n    if not aiohttp:\n        raise ValueError(\n            'Cannot download web documents without the aiohttp '\n            'dependency install it (pip install aiohttp)'\n        )\n\n    in_memory = isinstance(file, bytes)\n    if in_memory:\n        f = io.BytesIO()\n    elif isinstance(file, str):\n        kind, possible_names = cls._get_kind_and_names(web.attributes)\n        file = cls._get_proper_filename(\n            file, kind, utils.get_extension(web),\n            possible_names=possible_names\n        )\n        f = open(file, 'wb')\n    else:\n        f = file\n\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.get(web.url) as response:\n                while True:\n                    chunk = await response.content.read(128 * 1024)\n                    if not chunk:\n                        break\n                    f.write(chunk)\n    finally:\n        if isinstance(file, str) or isinstance(file, bytes):\n            f.close()\n\n    return f.getvalue() if in_memory else file"
    },
    {
        "original": "def ensure_parent_dir_exists(file_path):\n    \"\"\"Ensures that the parent directory exists\"\"\"\n    parent = os.path.dirname(file_path)\n    if parent:\n        os.makedirs(parent, exist_ok=True)",
        "rewrite": "import os\n\ndef ensure_parent_dir_exists(file_path):\n    parent = os.path.dirname(file_path)\n    if parent:\n        os.makedirs(parent, exist_ok=True)"
    },
    {
        "original": "def get_corrections_dict(self, entry):\n        \"\"\"\n        Returns the corrections applied to a particular entry.\n\n        Args:\n            entry: A ComputedEntry object.\n\n        Returns:\n            ({correction_name: value})\n        \"\"\"\n        corrections = {}\n        for c in self.corrections:\n            val = c.get_correction(entry)\n            if val != 0:\n                corrections[str(c)] = val\n        return corrections",
        "rewrite": "def get_corrections_dict(self, entry):\n        \"\"\"\n        Returns the corrections applied to a particular entry.\n\n        Args:\n            entry: A ComputedEntry object.\n\n        Returns:\n            ({correction_name: value})\n        \"\"\"\n        corrections = {}\n        for correction in self.corrections:\n            value = correction.get_correction(entry)\n            if value != 0:\n                corrections[str(correction)] = value\n        return corrections"
    },
    {
        "original": "def split_indexes(\n    dims_or_levels,  # type: Union[Any, List[Any]]\n    variables,  # type: OrderedDict[Any, Variable]\n    coord_names,  # type: Set\n    level_coords,  # type: Dict[Any, Any]\n    drop=False,  # type: bool\n):\n    # type: (...) -> Tuple[OrderedDict[Any, Variable], Set]\n    \"\"\"Extract (multi-)indexes (levels) as variables.\n\n    Not public API. Used in Dataset and DataArray reset_index\n    methods.\n    \"\"\"\n    if isinstance(dims_or_levels, str):\n        dims_or_levels = [dims_or_levels]\n\n    dim_levels = defaultdict(list)  # type: Dict[Any, list]\n    dims = []\n    for k in dims_or_levels:\n        if k in level_coords:\n            dim_levels[level_coords[k]].append(k)\n        else:\n            dims.append(k)\n\n    vars_to_replace = {}\n    vars_to_create = OrderedDict()  # type: OrderedDict[Any, Variable]\n    vars_to_remove = []\n\n    for d in dims:\n        index = variables[d].to_index()\n        if isinstance(index, pd.MultiIndex):\n            dim_levels[d] = index.names\n        else:\n            vars_to_remove.append(d)\n            if not drop:\n                vars_to_create[d + '_'] = Variable(d, index)\n\n    for d, levs in dim_levels.items():\n        index = variables[d].to_index()\n        if len(levs) == index.nlevels:\n            vars_to_remove.append(d)\n        else:\n            vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n\n        if not drop:\n            for lev in levs:\n                idx = index.get_level_values(lev)\n                vars_to_create[idx.name] = Variable(d, idx)\n\n    new_variables = variables.copy()\n    for v in set(vars_to_remove):\n        del new_variables[v]\n    new_variables.update(vars_to_replace)\n    new_variables.update(vars_to_create)\n    new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n\n    return new_variables, new_coord_names",
        "rewrite": "def split_indexes(\n        dims_or_levels: Union[Any, List[Any]], \n        variables: OrderedDict[Any, Variable], \n        coord_names: Set[Any], \n        level_coords: Dict[Any, Any], \n        drop: bool = False\n) -> Tuple[OrderedDict[Any, Variable], Set[Any]]:\n    \n    if isinstance(dims_or_levels, str):\n        dims_or_levels = [dims_or_levels]\n\n    dim_levels = defaultdict(list)\n    dims = []\n    \n    for k in dims_or_levels:\n        if k in level_coords:\n            dim_levels[level_coords[k]].append(k)\n        else:\n            dims.append(k)\n\n    vars_to_replace = {}\n    vars_to_create = OrderedDict() \n    vars_to_remove = []\n\n    for d in dims:\n        index = variables[d].to_index()\n        if isinstance(index, pd.MultiIndex):\n            dim_levels[d] = index.names\n        else:\n            vars_to_remove.append(d)\n            if not drop:\n                vars_to_create[d + '_'] = Variable(d, index)\n\n    for d, levs in dim_levels.items():\n        index = variables[d].to_index()\n        if len(levs) == index.nlevels:\n            vars_to_remove.append(d)\n        else:\n            vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n\n        if not drop:\n            for lev in levs:\n                idx = index.get_level_values(lev)\n                vars_to_create[idx.name] = Variable(d, idx)\n\n    new_variables = variables.copy()\n    for v in set(vars_to_remove):\n        del new_variables[v]\n    new_variables.update(vars_to_replace)\n    new_variables.update(vars_to_create)\n    new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n\n    return new_variables, new_coord_names"
    },
    {
        "original": "def _load(self,\n              url,\n              headers=CaseInsensitiveDict(),\n              params=None,\n              path=None,\n              ):\n        \"\"\" Load a resource.\n\n        :type url: str\n        :type headers: CaseInsensitiveDict\n        :type params: Optional[Dict[str,str]]\n        :type path: Optional[str]\n\n        \"\"\"\n        r = self._session.get(url, headers=headers, params=params)\n        try:\n            j = json_loads(r)\n        except ValueError as e:\n            logging.error(\"%s:\\n%s\" % (e, r.text))\n            raise e\n        if path:\n            j = j[path]\n        self._parse_raw(j)",
        "rewrite": "def _load(self, url, headers=CaseInsensitiveDict(), params=None, path=None):\n    r = self._session.get(url, headers=headers, params=params)\n    try:\n        j = json.loads(r)\n    except ValueError as e:\n        logging.error(\"%s:\\n%s\" % (e, r.text))\n        raise e\n    if path:\n        j = j[path]\n    self._parse_raw(j)"
    },
    {
        "original": "def get_quoted_strings(quoted_string):\n    \"\"\"\n    Returns a string comprised of all data in between double quotes.\n\n    @quoted_string - String to get quoted data from.\n\n    Returns a string of quoted data on success.\n    Returns a blank string if no quoted data is present.\n    \"\"\"\n    try:\n        # This regex grabs all quoted data from string.\n        # Note that this gets everything in between the first and last double quote.\n        # This is intentional, as printed (and quoted) strings from a target file may contain\n        # double quotes, and this function should ignore those. However, it also means that any\n        # data between two quoted strings (ex: '\"quote 1\" non-quoted data\n        # \"quote 2\"') will also be included.\n        return re.findall(r'\\\"(.*)\\\"', quoted_string)[0]\n    except KeyboardInterrupt as e:\n        raise e\n    except Exception:\n        return ''",
        "rewrite": "def get_quoted_strings(quoted_string):\n    try:\n        return re.findall(r'\\\"(.*)\\\"', quoted_string)[0]\n    except:\n        return ''"
    },
    {
        "original": "def untokenize_without_newlines(tokens):\n    \"\"\"Return source code based on tokens.\"\"\"\n    text = ''\n    last_row = 0\n    last_column = -1\n\n    for t in tokens:\n        token_string = t[1]\n        (start_row, start_column) = t[2]\n        (end_row, end_column) = t[3]\n\n        if start_row > last_row:\n            last_column = 0\n        if (\n            (start_column > last_column or token_string == '\\n') and\n            not text.endswith(' ')\n        ):\n            text += ' '\n\n        if token_string != '\\n':\n            text += token_string\n\n        last_row = end_row\n        last_column = end_column\n\n    return text.rstrip()",
        "rewrite": "def untokenize_without_newlines(tokens):\n    text = ''\n    last_row = 0\n    last_column = -1\n\n    for t in tokens:\n        token_string = t[1]\n        (start_row, start_column) = t[2]\n        (end_row, end_column) = t[3]\n\n        if start_row > last_row:\n            last_column = 0\n        if start_column > last_column or token_string == '\\n':\n            if not text.endswith(' '):\n                text += ' '\n\n        if token_string != '\\n':\n            text += token_string\n\n        last_row = end_row\n        last_column = end_column\n\n    return text.rstrip()"
    },
    {
        "original": "def hash_password(password, salt):\n    \"\"\"\n    Securely hash a password using a provided salt\n    :param password:\n    :param salt:\n    :return: Hex encoded SHA512 hash of provided password\n    \"\"\"\n    password = str(password).encode('utf-8')\n    salt = str(salt).encode('utf-8')\n    return hashlib.sha512(password + salt).hexdigest()",
        "rewrite": "import hashlib\n\ndef hash_password(password, salt):\n    password = str(password).encode('utf-8')\n    salt = str(salt).encode('utf-8')\n    return hashlib.sha512(password + salt).hexdigest()"
    },
    {
        "original": "def list_users(host=None,\n               admin_username=None,\n               admin_password=None,\n               module=None):\n    \"\"\"\n    List all DRAC users\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt dell dracr.list_users\n    \"\"\"\n    users = {}\n    _username = ''\n\n    for idx in range(1, 17):\n        cmd = __execute_ret('getconfig -g '\n                            'cfgUserAdmin -i {0}'.format(idx),\n                            host=host, admin_username=admin_username,\n                            admin_password=admin_password)\n\n        if cmd['retcode'] != 0:\n            log.warning('racadm returned an exit code of %s', cmd['retcode'])\n\n        for user in cmd['stdout'].splitlines():\n            if not user.startswith('cfg'):\n                continue\n\n            (key, val) = user.split('=')\n\n            if key.startswith('cfgUserAdminUserName'):\n                _username = val.strip()\n\n                if val:\n                    users[_username] = {'index': idx}\n                else:\n                    break\n            else:\n                if _username:\n                    users[_username].update({key: val})\n\n    return users",
        "rewrite": "def list_users(host=None, admin_username=None, admin_password=None, module=None):\n    users = {}\n    _username = ''\n\n    for idx in range(1, 17):\n        cmd = __execute_ret(f'getconfig -g cfgUserAdmin -i {idx}', host=host, admin_username=admin_username, admin_password=admin_password)\n\n        if cmd['retcode'] != 0:\n            log.warning('racadm returned an exit code of %s', cmd['retcode'])\n\n        for user in cmd['stdout'].splitlines():\n            if not user.startswith('cfg'):\n                continue\n\n            key, val = user.split('=')\n\n            if key.startswith('cfgUserAdminUserName'):\n                _username = val.strip()\n\n                if val:\n                    users[_username] = {'index': idx}\n                else:\n                    break\n            else:\n                if _username:\n                    users[_username].update({key: val})\n\n    return users"
    },
    {
        "original": "def secgroup_delete(self, name):\n        \"\"\"\n        Delete a security group\n        \"\"\"\n        nt_ks = self.compute_conn\n        for item in nt_ks.security_groups.list():\n            if item.name == name:\n                nt_ks.security_groups.delete(item.id)\n                return {name: 'Deleted security group: {0}'.format(name)}\n        return 'Security group not found: {0}'.format(name)",
        "rewrite": "def secgroup_delete(self, name):\n    nt_ks = self.compute_conn\n    for item in nt_ks.security_groups.list():\n        if item.name == name:\n            nt_ks.security_groups.delete(item.id)\n            return {name: 'Deleted security group: {0}'.format(name)}\n    return 'Security group not found: {0}'.format(name)"
    },
    {
        "original": "def answers(self, other):\n        \"\"\"DEV: true if self is an answer from other\"\"\"\n        if other.__class__ == self.__class__:\n            return (other.service + 0x40) == self.service or \\\n                   (self.service == 0x7f and\n                    self.request_service_id == other.service)\n        return False",
        "rewrite": "def answers(self, other):\n    if other.__class__ == self.__class__:\n        return (other.service + 0x40) == self.service or (self.service == 0x7f and self.request_service_id == other.service)\n    return False"
    },
    {
        "original": "def port_add(br, port, may_exist=False, internal=False):\n    \"\"\"\n    Creates on bridge a new port named port.\n\n    Returns:\n        True on success, else False.\n\n    Args:\n        br: A string - bridge name\n        port: A string - port name\n        may_exist: Bool, if False - attempting to create a port that exists returns False.\n        internal: A boolean to create an internal interface if one does not exist.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' openvswitch.port_add br0 8080\n    \"\"\"\n    param_may_exist = _param_may_exist(may_exist)\n    cmd = 'ovs-vsctl {2}add-port {0} {1}'.format(br, port, param_may_exist)\n    if internal:\n        cmd += ' -- set interface {0} type=internal'.format(port)\n    result = __salt__['cmd.run_all'](cmd)\n    retcode = result['retcode']\n    return _retcode_to_bool(retcode)",
        "rewrite": "def port_add(br, port, may_exist=False, internal=False):\n    param_may_exist = _param_may_exist(may_exist)\n    cmd = 'ovs-vsctl {2}add-port {0} {1}'.format(br, port, param_may_exist)\n    if internal:\n        cmd += ' -- set interface {0} type=internal'.format(port)\n    result = __salt__['cmd.run_all'](cmd)\n    retcode = result['retcode']\n    return _retcode_to_bool(retcode)"
    },
    {
        "original": "def clean_download_cache(self, args):\n        \"\"\" Deletes a download cache for recipes passed as arguments. If no\n        argument is passed, it'll delete *all* downloaded caches. ::\n\n            p4a clean_download_cache kivy,pyjnius\n\n        This does *not* delete the build caches or final distributions.\n        \"\"\"\n        ctx = self.ctx\n        if hasattr(args, 'recipes') and args.recipes:\n            for package in args.recipes:\n                remove_path = join(ctx.packages_path, package)\n                if exists(remove_path):\n                    shutil.rmtree(remove_path)\n                    info('Download cache removed for: \"{}\"'.format(package))\n                else:\n                    warning('No download cache found for \"{}\", skipping'.format(\n                        package))\n        else:\n            if exists(ctx.packages_path):\n                shutil.rmtree(ctx.packages_path)\n                info('Download cache removed.')\n            else:\n                print('No cache found at \"{}\"'.format(ctx.packages_path))",
        "rewrite": "def clean_download_cache(self, args):\n    ctx = self.ctx\n    if hasattr(args, 'recipes') and args.recipes:\n        for package in args.recipes:\n            remove_path = os.path.join(ctx.packages_path, package)\n            if os.path.exists(remove_path):\n                shutil.rmtree(remove_path)\n                info('Download cache removed for: \"{}\"'.format(package))\n            else:\n                warning('No download cache found for \"{}\", skipping'.format(package))\n    else:\n        if os.path.exists(ctx.packages_path):\n            shutil.rmtree(ctx.packages_path)\n            info('Download cache removed.')\n        else:\n            print('No cache found at \"{}\"'.format(ctx.packages_path))"
    },
    {
        "original": "def move(self, x, y):\n        \"\"\"Move window top-left corner to position\"\"\"\n        SetWindowPos(self._hwnd, None, x, y, 0, 0, SWP_NOSIZE)",
        "rewrite": "def move(self, x, y):\n        SetWindowPos(self._hwnd, None, x, y, 0, 0, SWP_NOSIZE)"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        A JSON serializable dict representation of self.\n        \"\"\"\n        return {\"@module\": self.__class__.__module__,\n                \"@class\": self.__class__.__name__,\n                \"operation\": self.operation, \"title\": self.title,\n                \"xc\": self.xc.as_dict(), \"basis_set\": self.basis_set.as_dict(),\n                \"units\": self.units.as_dict(), \"scf\": self.scf.as_dict(),\n                \"geo\": self.geo.as_dict(),\n                \"others\": [k.as_dict() for k in self.other_directives]}",
        "rewrite": "def as_dict(self):\n    return {\"@module\": self.__class__.__module__,\n            \"@class\": self.__class__.__name__,\n            \"operation\": self.operation, \n            \"title\": self.title,\n            \"xc\": self.xc.as_dict(), \n            \"basis_set\": self.basis_set.as_dict(),\n            \"units\": self.units.as_dict(), \n            \"scf\": self.scf.as_dict(),\n            \"geo\": self.geo.as_dict(),\n            \"others\": [k.as_dict() for k in self.other_directives]}"
    },
    {
        "original": "def link_page_size_filter(self, page_size, modelview_name):\n        \"\"\"\n        Arguments are passed like: psize_<VIEW_NAME>=<PAGE_NUMBER>\n        \"\"\"\n        new_args = request.view_args.copy()\n        args = request.args.copy()\n        args[\"psize_\" + modelview_name] = page_size\n        return url_for(\n            request.endpoint,\n            **dict(list(new_args.items()) + list(args.to_dict().items()))\n        )",
        "rewrite": "def link_page_size_filter(self, page_size, modelview_name):\n    new_args = request.view_args.copy()\n    args = request.args.copy()\n    args[\"psize_\" + modelview_name] = page_size\n    return url_for(\n        request.endpoint,\n        **dict(list(new_args.items()) + list(args.to_dict().items()))\n    )"
    },
    {
        "original": "def _qnwgamma1(n, a=1.0, b=1.0, tol=3e-14):\n    \"\"\"\n    1d quadrature weights and nodes for Gamma distributed random variable\n\n    Parameters\n    ----------\n    n : scalar : int\n        The number of quadrature points\n\n    a : scalar : float, optional(default=1.0)\n        Shape parameter of the gamma distribution parameter. Must be positive\n\n    b : scalar : float, optional(default=1.0)\n        Scale parameter of the gamma distribution parameter. Must be positive\n\n    tol : scalar : float, optional(default=3e-14)\n        Tolerance parameter for newton iterations for each node\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float, ndim=1)\n        The quadrature points\n\n    weights : np.ndarray(dtype=float, ndim=1)\n        The quadrature weights that correspond to nodes\n\n    Notes\n    -----\n    Based of original function ``qnwgamma1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    a -= 1\n\n    maxit = 25\n\n    factor = -math.exp(gammaln(a+n) - gammaln(n) - gammaln(a+1))\n    nodes = np.zeros(n)\n    weights = np.zeros(n)\n\n    # Create nodes\n    for i in range(n):\n        # Reasonable starting values\n        if i == 0:\n            z = (1+a) * (3+0.92*a) / (1 + 2.4*n + 1.8*a)\n        elif i == 1:\n            z = z + (15 + 6.25*a) / (1 + 0.9*a + 2.5*n)\n        else:\n            j = i-1\n            z = z + ((1 + 2.55*j) / (1.9*j) + 1.26*j*a / (1 + 3.5*j)) * \\\n                (z - nodes[j-1]) / (1 + 0.3*a)\n\n        # root finding iterations\n        its = 0\n        z1 = -10000\n        while abs(z - z1) > tol and its < maxit:\n            p1 = 1.0\n            p2 = 0.0\n            for j in range(1, n+1):\n                # Recurrance relation for Laguerre polynomials\n                p3 = p2\n                p2 = p1\n                p1 = ((2*j - 1 + a - z)*p2 - (j - 1 + a)*p3) / j\n\n            pp = (n*p1 - (n+a)*p2) / z\n            z1 = z\n            z = z1 - p1/pp\n            its += 1\n\n        if its == maxit:\n            raise ValueError('Failure to converge')\n\n        nodes[i] = z\n        weights[i] = factor / (pp*n*p2)\n\n    return nodes*b, weights",
        "rewrite": "import math\nimport numpy as np\n\ndef _qnwgamma1(n, a=1.0, b=1.0, tol=3e-14):\n\n    a -= 1\n\n    maxit = 25\n\n    factor = -math.exp(math.lgamma(a+n) - math.lgamma(n) - math.lgamma(a+1))\n    nodes = np.zeros(n)\n    weights = np.zeros(n)\n\n    for i in range(n):\n        if i == 0:\n            z = (1+a) * (3+0.92*a) / (1 + 2.4*n + 1.8*a)\n        elif i == 1:\n            z = z + (15 + 6.25*a) / (1 + 0.9*a + 2.5*n)\n        else:\n            j = i-1\n            z = z + ((1 + 2.55*j) / (1.9*j) + 1.26*j*a / (1 + 3.5*j)) * \\\n                (z - nodes[j-1]) / (1 + 0.3*a)\n\n        its = 0\n        z1 = -10000\n        while abs(z - z1) > tol and its < maxit:\n            p1 = 1.0\n            p2 = 0.0\n            for j in range(1, n+1):\n                p3 = p2\n                p2 = p1\n                p1 = ((2*j - 1 + a - z)*p2 - (j - 1 + a)*p3) / j\n\n            pp = (n*p1 - (n+a)*p2) / z\n            z1 = z\n            z = z1 - p1/pp\n            its += 1\n\n        if its == maxit:\n            raise ValueError('Failure to converge')\n\n        nodes[i] = z\n        weights[i] = factor / (pp*n*p2)\n\n    return nodes*b, weights"
    },
    {
        "original": "def get_sla_by_id(self, issue_id_or_key, sla_id):\n        \"\"\"\n        Get the SLA information for a customer request for a given request ID or key and SLA metric ID\n        IMPORTANT: The calling user must be an agent\n\n        :param issue_id_or_key: str\n        :param sla_id: str\n        :return: SLA information\n        \"\"\"\n        url = 'rest/servicedeskapi/request/{0}/sla/{1}'.format(issue_id_or_key, sla_id)\n\n        return self.get(url)",
        "rewrite": "def get_sla_by_id(self, issue_id_or_key, sla_id):\n    url = 'rest/servicedeskapi/request/{0}/sla/{1}'.format(issue_id_or_key, sla_id)\n    return self.get(url)"
    },
    {
        "original": "def _styles_part(self):\n        \"\"\"\n        Instance of |StylesPart| for this document. Creates an empty styles\n        part if one is not present.\n        \"\"\"\n        try:\n            return self.part_related_by(RT.STYLES)\n        except KeyError:\n            styles_part = StylesPart.default(self.package)\n            self.relate_to(styles_part, RT.STYLES)\n            return styles_part",
        "rewrite": "def _styles_part(self):\r\n    try:\r\n        return self.part_related_by(RT.STYLES)\r\n    except KeyError:\r\n        styles_part = StylesPart.default(self.package)\r\n        self.relate_to(styles_part, RT.STYLES)\r\n        return styles_part"
    },
    {
        "original": "def create_from_raw_data(self, klass, raw_data, headers={}):\n        \"\"\"\n        Creates an object from raw_data previously obtained by :attr:`github.GithubObject.GithubObject.raw_data`,\n        and optionaly headers previously obtained by :attr:`github.GithubObject.GithubObject.raw_headers`.\n\n        :param klass: the class of the object to create\n        :param raw_data: dict\n        :param headers: dict\n        :rtype: instance of class ``klass``\n        \"\"\"\n        return klass(self.__requester, headers, raw_data, completed=True)",
        "rewrite": "def create_from_raw_data(self, klass, raw_data, headers={}):\n    return klass(self.__requester, headers, raw_data, completed=True)"
    },
    {
        "original": "def power_under_cph(n_exp, n_con, p_exp, p_con, postulated_hazard_ratio, alpha=0.05):\n    \"\"\"\n    This computes the power of the hypothesis test that the two groups, experiment and control,\n    have different hazards (that is, the relative hazard ratio is different from 1.)\n\n    Parameters\n    ----------\n\n    n_exp : integer\n        size of the experiment group.\n\n    n_con : integer\n        size of the control group.\n\n    p_exp : float\n        probability of failure in experimental group over period of study.\n\n    p_con : float\n        probability of failure in control group over period of study\n\n    postulated_hazard_ratio : float\n    the postulated hazard ratio\n\n    alpha : float, optional (default=0.05)\n        type I error rate\n\n    Returns\n    -------\n\n    float:\n        power to detect the magnitude of the hazard ratio as small as that specified by postulated_hazard_ratio.\n\n\n    Notes\n    -----\n    `Reference <https://cran.r-project.org/web/packages/powerSurvEpi/powerSurvEpi.pdf>`_.\n\n\n    See Also\n    --------\n    sample_size_necessary_under_cph\n    \"\"\"\n\n    def z(p):\n        return stats.norm.ppf(p)\n\n    m = n_exp * p_exp + n_con * p_con\n    k = float(n_exp) / float(n_con)\n    return stats.norm.cdf(\n        np.sqrt(k * m) * abs(postulated_hazard_ratio - 1) / (k * postulated_hazard_ratio + 1) - z(1 - alpha / 2.0)\n    )",
        "rewrite": "from scipy import stats\nimport numpy as np\n\ndef power_under_cph(n_exp, n_con, p_exp, p_con, postulated_hazard_ratio, alpha=0.05):\n\n    def z(p):\n        return stats.norm.ppf(p)\n\n    m = n_exp * p_exp + n_con * p_con\n    k = float(n_exp) / float(n_con)\n    return stats.norm.cdf(\n        np.sqrt(k * m) * abs(postulated_hazard_ratio - 1) / (k * postulated_hazard_ratio + 1) - z(1 - alpha / 2.0)\n    )"
    },
    {
        "original": "def from_node(index, value):\n        \"\"\"\n        >>> SyncState.from_node(1, None).leader is None\n        True\n        >>> SyncState.from_node(1, '{}').leader is None\n        True\n        >>> SyncState.from_node(1, '{').leader is None\n        True\n        >>> SyncState.from_node(1, '[]').leader is None\n        True\n        >>> SyncState.from_node(1, '{\"leader\": \"leader\"}').leader == \"leader\"\n        True\n        >>> SyncState.from_node(1, {\"leader\": \"leader\"}).leader == \"leader\"\n        True\n        \"\"\"\n        if isinstance(value, dict):\n            data = value\n        elif value:\n            try:\n                data = json.loads(value)\n                if not isinstance(data, dict):\n                    data = {}\n            except (TypeError, ValueError):\n                data = {}\n        else:\n            data = {}\n        return SyncState(index, data.get('leader'), data.get('sync_standby'))",
        "rewrite": "def from_node(index, value):\n        if isinstance(value, dict):\n            data = value\n        elif value:\n            try:\n                data = json.loads(value)\n                if not isinstance(data, dict):\n                    data = {}\n            except (TypeError, ValueError):\n                data = {}\n        else:\n            data = {}\n        return SyncState(index, data.get('leader'), data.get('sync_standby'))"
    },
    {
        "original": "def unpack(data):\n        \"\"\" return length, content\n        \"\"\"\n        length = struct.unpack('i', data[0:HEADER_SIZE])\n        return length[0], data[HEADER_SIZE:]",
        "rewrite": "def unpack(data):\n    \"\"\" return length, content \"\"\"\n    length = struct.unpack('i', data[0:HEADER_SIZE])[0]\n    return length, data[HEADER_SIZE:]"
    },
    {
        "original": "def compare_md5(self):\n        \"\"\"Compare md5 of file on network device to md5 of local file.\"\"\"\n        if self.direction == \"put\":\n            remote_md5 = self.remote_md5()\n            return self.source_md5 == remote_md5\n        elif self.direction == \"get\":\n            local_md5 = self.file_md5(self.dest_file)\n            return self.source_md5 == local_md5",
        "rewrite": "def compare_md5(self):\n    if self.direction == \"put\":\n        remote_md5 = self.remote_md5()\n        return self.source_md5 == remote_md5\n    elif self.direction == \"get\":\n        local_md5 = self.file_md5(self.dest_file)\n        return self.source_md5 == local_md5"
    },
    {
        "original": "def get_term_category_frequencies(self, scatterchartdata):\n        \"\"\"\n        Applies the ranker in scatterchartdata to term-category frequencies.\n\n        Parameters\n        ----------\n        scatterchartdata : ScatterChartData\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        term_ranker = scatterchartdata.term_ranker(self)\n        if scatterchartdata.use_non_text_features:\n            term_ranker.use_non_text_features()\n        return term_ranker.get_ranks()",
        "rewrite": "def get_term_category_frequencies(self, scatterchartdata):\n    term_ranker = scatterchartdata.term_ranker(self)\n    if scatterchartdata.use_non_text_features:\n        term_ranker.use_non_text_features()\n    return term_ranker.get_ranks()"
    },
    {
        "original": "def publish(self):\n        \"\"\"Publish new events to the subscribers.\"\"\"\n\n        while True:\n            event = yield from self.event_source.get()\n            str_buffer = []\n\n            if event == POISON_PILL:\n                return\n\n            if isinstance(event, str):\n                str_buffer.append(event)\n\n            elif event.type == EventTypes.BLOCK_VALID:\n                str_buffer = map(json.dumps, eventify_block(event.data))\n\n            for str_item in str_buffer:\n                for _, websocket in self.subscribers.items():\n                    yield from websocket.send_str(str_item)",
        "rewrite": "def publish(self):\n    while True:\n        event = yield from self.event_source.get()\n        str_buffer = []\n        \n        if event == POISON_PILL:\n            return\n        \n        if isinstance(event, str):\n            str_buffer.append(event)\n        \n        elif event.type == EventTypes.BLOCK_VALID:\n            str_buffer = map(json.dumps, eventify_block(event.data))\n            \n        for str_item in str_buffer:\n            for _, websocket in self.subscribers.items():\n                yield from websocket.send_str(str_item)"
    },
    {
        "original": "def roc_auc_score(gold, probs, ignore_in_gold=[], ignore_in_pred=[]):\n    \"\"\"Compute the ROC AUC score, given the gold labels and predicted probs.\n\n    Args:\n        gold: A 1d array-like of gold labels\n        probs: A 2d array-like of predicted probabilities\n        ignore_in_gold: A list of labels for which elements having that gold\n            label will be ignored.\n\n    Returns:\n        roc_auc_score: The (float) roc_auc score\n    \"\"\"\n    gold = arraylike_to_numpy(gold)\n\n    # Filter out the ignore_in_gold (but not ignore_in_pred)\n    # Note the current sub-functions (below) do not handle this...\n    if len(ignore_in_pred) > 0:\n        raise ValueError(\"ignore_in_pred not defined for ROC-AUC score.\")\n    keep = [x not in ignore_in_gold for x in gold]\n    gold = gold[keep]\n    probs = probs[keep, :]\n\n    # Convert gold to one-hot indicator format, using the k inferred from probs\n    gold_s = pred_to_prob(torch.from_numpy(gold), k=probs.shape[1]).numpy()\n    return skm.roc_auc_score(gold_s, probs)",
        "rewrite": "def roc_auc_score(gold, probs, ignore_in_gold=[], ignore_in_pred=[]):\n    gold = arraylike_to_numpy(gold)\n    \n    if len(ignore_in_pred) > 0:\n        raise ValueError(\"ignore_in_pred not defined for ROC-AUC score.\")\n    \n    keep = [x not in ignore_in_gold for x in gold]\n    gold = gold[keep]\n    probs = probs[keep, :]\n    \n    gold_s = pred_to_prob(torch.from_numpy(gold), k=probs.shape[1]).numpy()\n    return skm.roc_auc_score(gold_s, probs)"
    },
    {
        "original": "def Get(self):\n    \"\"\"Fetch client's data and return a proper Client object.\"\"\"\n\n    args = client_pb2.ApiGetClientArgs(client_id=self.client_id)\n    result = self._context.SendRequest(\"GetClient\", args)\n    return Client(data=result, context=self._context)",
        "rewrite": "def get_client(self):\n    args = client_pb2.ApiGetClientArgs(client_id=self.client_id)\n    result = self._context.SendRequest(\"GetClient\", args)\n    return Client(data=result, context=self._context)"
    },
    {
        "original": "def _compute_tick_mapping(self, kind, order, bins):\n        \"\"\"Helper function to compute tick mappings based on `ticks` and\n        default orders and bins.\n\n        \"\"\"\n\n        if kind == \"angle\":\n            ticks = self.xticks\n            reverse = True\n        elif kind == \"radius\":\n            ticks = self.yticks\n            reverse = False\n\n        if callable(ticks):\n            text_nth = [x for x in order if ticks(x)]\n\n        elif isinstance(ticks, (tuple, list)):\n            bins = self._get_bins(kind, ticks, reverse)\n            text_nth = ticks\n\n        elif ticks:\n            nth_label = np.ceil(len(order) / float(ticks)).astype(int)\n            text_nth = order[::nth_label]\n\n        return {x: bins[x] for x in text_nth}",
        "rewrite": "def _compute_tick_mapping(self, kind, order, bins):\n\n    if kind == \"angle\":\n        ticks = self.xticks\n        reverse = True\n    elif kind == \"radius\":\n        ticks = self.yticks\n        reverse = False\n\n    if callable(ticks):\n        text_nth = [x for x in order if ticks(x)]\n\n    elif isinstance(ticks, (tuple, list)):\n        bins = self._get_bins(kind, ticks, reverse)\n        text_nth = ticks\n\n    elif ticks:\n        nth_label = np.ceil(len(order) / float(ticks)).astype(int)\n        text_nth = order[::nth_label]\n\n    return {x: bins[x] for x in text_nth}"
    },
    {
        "original": "def _find_image_bounding_boxes(filenames, image_to_bboxes):\n  \"\"\"Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  \"\"\"\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print('Found %d images with bboxes out of %d images' % (\n      num_image_bbox, len(filenames)))\n  return bboxes",
        "rewrite": "def find_image_bounding_boxes(filenames, image_to_bboxes):\n    num_images_with_bbox = 0\n    bboxes = []\n    for file_name in filenames:\n        basename = os.path.basename(file_name)\n        if basename in image_to_bboxes:\n            bboxes.append(image_to_bboxes[basename])\n            num_images_with_bbox += 1\n        else:\n            bboxes.append([])\n    print('Found %d images with bboxes out of %d images' % (\n          num_images_with_bbox, len(filenames)))\n    return bboxes"
    },
    {
        "original": "def delete_reply_comment(self, msg_data_id, index, user_comment_id):\n        \"\"\"\n        \u5220\u9664\u56de\u590d\n        \"\"\"\n        return self._post(\n            'comment/reply/delete',\n            data={\n                'msg_data_id': msg_data_id,\n                'index': index,\n                'user_comment_id': user_comment_id,\n            })",
        "rewrite": "def delete_reply_comment(self, msg_data_id, index, user_comment_id):\n    return self._post(\n        'comment/reply/delete',\n        data={\n            'msg_data_id': msg_data_id,\n            'index': index,\n            'user_comment_id': user_comment_id,\n        }\n    )"
    },
    {
        "original": "def lf_conflicts(L, normalize_by_overlaps=False):\n    \"\"\"Return the **fraction of items each LF labels that are also given a\n    different (non-abstain) label by at least one other LF.**\n\n    Note that the maximum possible conflict fraction for an LF is the LF's\n        overlaps fraction, unless `normalize_by_overlaps=True`, in which case it\n        is 1.\n\n    Args:\n        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n            jth LF to the ith candidate\n        normalize_by_overlaps: Normalize by overlaps of the LF, so that it\n            returns the percent of LF overlaps that have conflicts.\n    \"\"\"\n    conflicts = (L != 0).T @ _conflicted_data_points(L) / L.shape[0]\n    if normalize_by_overlaps:\n        conflicts /= lf_overlaps(L)\n    return np.nan_to_num(conflicts)",
        "rewrite": "def lf_conflicts(L, normalize_by_overlaps=False):\n    conflicts = (L != 0).T @ _conflicted_data_points(L) / L.shape[0]\n    if normalize_by_overlaps:\n        conflicts /= lf_overlaps(L)\n    return np.nan_to_num(conflicts)"
    },
    {
        "original": "def show_service_certificate(kwargs=None, conn=None, call=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Return information about a service certificate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_service_certificate my-azure name=my_service_certificate \\\\\n            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The get_service_certificate function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A service name must be specified as \"name\"')\n\n    if 'thumbalgorithm' not in kwargs:\n        raise SaltCloudSystemExit('A thumbalgorithm must be specified as \"thumbalgorithm\"')\n\n    if 'thumbprint' not in kwargs:\n        raise SaltCloudSystemExit('A thumbprint must be specified as \"thumbprint\"')\n\n    data = conn.get_service_certificate(\n        kwargs['name'],\n        kwargs['thumbalgorithm'],\n        kwargs['thumbprint'],\n    )\n    return object_to_dict(data)",
        "rewrite": "def show_service_certificate(kwargs=None, conn=None, call=None):\n    if call != 'function':\n        raise SaltCloudSystemExit('The get_service_certificate function must be called with -f or --function.')\n    \n    if not conn:\n        conn = get_conn()\n    \n    if kwargs is None:\n        kwargs = {}\n    \n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A service name must be specified as \"name\"')\n    \n    if 'thumbalgorithm' not in kwargs:\n        raise SaltCloudSystemExit('A thumbalgorithm must be specified as \"thumbalgorithm\"')\n    \n    if 'thumbprint' not in kwargs:\n        raise SaltCloudSystemExit('A thumbprint must be specified as \"thumbprint\"')\n    \n    data = conn.get_service_certificate(\n        kwargs['name'],\n        kwargs['thumbalgorithm'],\n        kwargs['thumbprint'],\n    )\n    return object_to_dict(data)"
    },
    {
        "original": "def get_download(self, id):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/downloads/:id <http://developer.github.com/v3/repos/downloads>`_\n        :param id: integer\n        :rtype: :class:`github.Download.Download`\n        \"\"\"\n        assert isinstance(id, (int, long)), id\n        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url + \"/downloads/\" + str(id)\n        )\n        return github.Download.Download(self._requester, headers, data, completed=True)",
        "rewrite": "def get_download(self, id):\n    assert isinstance(id, (int, int)), id\n    headers, data = self._requester.requestJsonAndCheck(\n        \"GET\",\n        self.url + \"/downloads/\" + str(id)\n    )\n    return github.Download.Download(self._requester, headers, data, completed=True)"
    },
    {
        "original": "def Run(self):\n    \"\"\"Create FileStore and HashFileStore namespaces.\"\"\"\n    if not data_store.AFF4Enabled():\n      return\n\n    try:\n      filestore = aff4.FACTORY.Create(\n          FileStore.PATH, FileStore, mode=\"rw\", token=aff4.FACTORY.root_token)\n      filestore.Close()\n      hash_filestore = aff4.FACTORY.Create(\n          HashFileStore.PATH,\n          HashFileStore,\n          mode=\"rw\",\n          token=aff4.FACTORY.root_token)\n      hash_filestore.Close()\n      nsrl_filestore = aff4.FACTORY.Create(\n          NSRLFileStore.PATH,\n          NSRLFileStore,\n          mode=\"rw\",\n          token=aff4.FACTORY.root_token)\n      nsrl_filestore.Close()\n    except access_control.UnauthorizedAccess:\n      # The aff4:/files area is ACL protected, this might not work on components\n      # that have ACL enforcement.\n      pass",
        "rewrite": "def Run(self):\n    if not data_store.AFF4Enabled():\n        return\n\n    try:\n        filestore = aff4.FACTORY.Create(FileStore.PATH, FileStore, mode=\"rw\", token=aff4.FACTORY.root_token)\n        filestore.Close()\n        \n        hash_filestore = aff4.FACTORY.Create(HashFileStore.PATH, HashFileStore, mode=\"rw\", token=aff4.FACTORY.root_token)\n        hash_filestore.Close()\n        \n        nsrl_filestore = aff4.FACTORY.Create(NSRLFileStore.PATH, NSRLFileStore, mode=\"rw\", token=aff4.FACTORY.root_token)\n        nsrl_filestore.Close()\n    except access_control.UnauthorizedAccess:\n        pass"
    },
    {
        "original": "def closest(self, obj, group, defaults=True):\n        \"\"\"\n        This method is designed to be called from the root of the\n        tree. Given any LabelledData object, this method will return\n        the most appropriate Options object, including inheritance.\n\n        In addition, closest supports custom options by checking the\n        object\n        \"\"\"\n        components = (obj.__class__.__name__,\n                      group_sanitizer(obj.group),\n                      label_sanitizer(obj.label))\n        target = '.'.join([c for c in components if c])\n        return self.find(components).options(group, target=target,\n                                             defaults=defaults)",
        "rewrite": "def closest(self, obj, group, defaults=True):\n    components = (obj.__class__.__name__,\n                  group_sanitizer(obj.group),\n                  label_sanitizer(obj.label))\n    target = '.'.join([c for c in components if c])\n    return self.find(components).options(group, target=target, defaults=defaults)"
    },
    {
        "original": "def execute(self, eopatch):\n        \"\"\" Execute function which adds new vector layer to the EOPatch\n\n        :param eopatch: input EOPatch\n        :type eopatch: EOPatch\n        :return: New EOPatch with added vector layer\n        :rtype: EOPatch\n        \"\"\"\n        bbox_map = self._get_submap(eopatch)\n        height, width = self._get_shape(eopatch)\n        data_transform = rasterio.transform.from_bounds(*eopatch.bbox, width=width, height=height)\n\n        if self.feature_name in eopatch[self.feature_type]:\n            raster = eopatch[self.feature_type][self.feature_name].squeeze()\n        else:\n            raster = np.ones((height, width), dtype=self.raster_dtype) * self.no_data_value\n\n        if not bbox_map.empty:\n            rasterio.features.rasterize([(bbox_map.cascaded_union.buffer(0), self.raster_value)], out=raster,\n                                        transform=data_transform, dtype=self.raster_dtype)\n\n        eopatch[self.feature_type][self.feature_name] = raster[..., np.newaxis]\n\n        return eopatch",
        "rewrite": "def execute(self, eopatch):\n    bbox_map = self._get_submap(eopatch)\n    height, width = self._get_shape(eopatch)\n    data_transform = rasterio.transform.from_bounds(*eopatch.bbox, width=width, height=height)\n\n    if self.feature_name in eopatch[self.feature_type]:\n        raster = eopatch[self.feature_type][self.feature_name].squeeze()\n    else:\n        raster = np.ones((height, width), dtype=self.raster_dtype) * self.no_data_value\n\n    if not bbox_map.empty:\n        rasterio.features.rasterize([(bbox_map.cascaded_union.buffer(0), self.raster_value)], out=raster,\n                                    transform=data_transform, dtype=self.raster_dtype)\n\n    eopatch[self.feature_type][self.feature_name] = raster[..., np.newaxis]\n\n    return eopatch"
    },
    {
        "original": "def dispatch_command(self, command, params=None):\n        \"\"\"Dispatch device commands to the appropriate handler.\"\"\"\n        try:\n            if command in self.handlers:\n                self.handlers[command](**params)\n            else:\n                logging.warning('Unsupported command: %s: %s',\n                                command, params)\n        except Exception as e:\n            logging.warning('Error during command execution',\n                            exc_info=sys.exc_info())\n            raise e",
        "rewrite": "def dispatch_command(self, command, params=None):\n    try:\n        if command in self.handlers:\n            self.handlers[command](**params)\n        else:\n            logging.warning('Unsupported command: %s: %s',\n                            command, params)\n    except Exception as e:\n        logging.warning('Error during command execution',\n                        exc_info=sys.exc_info())\n        raise e"
    },
    {
        "original": "def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes",
        "rewrite": "def modified_recipes(branch='origin/master'):\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes"
    },
    {
        "original": "def from_node(index, name, session, data):\n        \"\"\"\n        >>> Member.from_node(-1, '', '', '{\"conn_url\": \"postgres://foo@bar/postgres\"}') is not None\n        True\n        >>> Member.from_node(-1, '', '', '{')\n        Member(index=-1, name='', session='', data={})\n        \"\"\"\n        if data.startswith('postgres'):\n            conn_url, api_url = parse_connection_string(data)\n            data = {'conn_url': conn_url, 'api_url': api_url}\n        else:\n            try:\n                data = json.loads(data)\n            except (TypeError, ValueError):\n                data = {}\n        return Member(index, name, session, data)",
        "rewrite": "```python\ndef from_node(index, name, session, data):\n    if data.startswith('postgres'):\n        conn_url, api_url = parse_connection_string(data)\n        data = {'conn_url': conn_url, 'api_url': api_url}\n    else:\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = {}\n    return Member(index, name, session, data)\n```"
    },
    {
        "original": "def _common(ret, name, service_name, kwargs):\n    \"\"\"\n    Returns: tuple whose first element is a bool indicating success or failure\n             and the second element is either a ret dict for salt or an object\n    \"\"\"\n    if 'interface' not in kwargs and 'public_url' not in kwargs:\n        kwargs['interface'] = name\n    service = __salt__['keystoneng.service_get'](name_or_id=service_name)\n\n    if not service:\n        ret['comment'] = 'Cannot find service'\n        ret['result'] = False\n        return (False, ret)\n\n    filters = kwargs.copy()\n    filters.pop('enabled', None)\n    filters.pop('url', None)\n    filters['service_id'] = service.id\n    kwargs['service_name_or_id'] = service.id\n    endpoints = __salt__['keystoneng.endpoint_search'](filters=filters)\n\n    if len(endpoints) > 1:\n        ret['comment'] = \"Multiple endpoints match criteria\"\n        ret['result'] = False\n        return ret\n    endpoint = endpoints[0] if endpoints else None\n    return (True, endpoint)",
        "rewrite": "def _common(ret, name, service_name, kwargs):\n    if 'interface' not in kwargs and 'public_url' not in kwargs:\n        kwargs['interface'] = name\n    service = __salt__['keystoneng.service_get'](name_or_id=service_name)\n\n    if not service:\n        ret['comment'] = 'Cannot find service'\n        ret['result'] = False\n        return (False, ret)\n\n    filters = {k: v for k, v in kwargs.items() if k not in ['enabled', 'url']}\n    filters['service_id'] = service.id\n    kwargs['service_name_or_id'] = service.id\n    endpoints = __salt__['keystoneng.endpoint_search'](filters=filters)\n\n    if len(endpoints) > 1:\n        ret['comment'] = \"Multiple endpoints match criteria\"\n        ret['result'] = False\n        return (False, ret)\n    \n    endpoint = endpoints[0] if endpoints else None\n    return (True, endpoint)"
    },
    {
        "original": "def root_urns_for_deletion(self):\n    \"\"\"Roots of the graph of urns marked for deletion.\"\"\"\n    roots = set()\n    for urn in self._urns_for_deletion:\n      new_root = True\n\n      str_urn = utils.SmartUnicode(urn)\n      fake_roots = []\n      for root in roots:\n        str_root = utils.SmartUnicode(root)\n\n        if str_urn.startswith(str_root):\n          new_root = False\n          break\n        elif str_root.startswith(str_urn):\n          fake_roots.append(root)\n\n      if new_root:\n        roots -= set(fake_roots)\n        roots.add(urn)\n\n    return roots",
        "rewrite": "def root_urns_for_deletion(self):\n    roots = set()\n    for urn in self._urns_for_deletion:\n        new_root = True\n        str_urn = utils.SmartUnicode(urn)\n        fake_roots = []\n        for root in roots:\n            str_root = utils.SmartUnicode(root)\n            if str_urn.startswith(str_root):\n                new_root = False\n                break\n            elif str_root.startswith(str_urn):\n                fake_roots.append(root)\n        if new_root:\n            roots -= set(fake_roots)\n            roots.add(urn)\n    return roots"
    },
    {
        "original": "def ext(external, pillar=None):\n    \"\"\"\n    .. versionchanged:: 2016.3.6,2016.11.3,2017.7.0\n        The supported ext_pillar types are now tunable using the\n        :conf_master:`on_demand_ext_pillar` config option. Earlier releases\n        used a hard-coded default.\n\n    Generate the pillar and apply an explicit external pillar\n\n\n    external\n        A single ext_pillar to add to the ext_pillar configuration. This must\n        be passed as a single section from the ext_pillar configuration (see\n        CLI examples below). For more complicated ``ext_pillar``\n        configurations, it can be helpful to use the Python shell to load YAML\n        configuration into a dictionary, and figure out\n\n        .. code-block:: python\n\n            >>> import salt.utils.yaml\n            >>> ext_pillar = salt.utils.yaml.safe_load(\"\"\"\n            ... ext_pillar:\n            ...   - git:\n            ...     - issue38440 https://github.com/terminalmage/git_pillar:\n            ...       - env: base\n            ... ",
        "rewrite": "def ext(external, pillar=None):\n    \"\"\"\n    .. versionchanged:: 2016.3.6,2016.11.3,2017.7.0\n        The supported ext_pillar types are now tunable using the\n        :conf_master:`on_demand_ext_pillar` config option. Earlier releases\n        used a hard-coded default.\n\n    Generate the pillar and apply an explicit external pillar\n\n\n    external\n        A single ext_pillar to add to the ext_pillar configuration. This must\n        be passed as a single section from the ext_pillar configuration (see\n        CLI examples below). For more complicated ``ext_pillar``\n        configurations, it can be helpful to use the Python shell to load YAML\n        configuration into a dictionary, and figure out\n\n        .. code-block:: python\n\n            >>> import salt.utils.yaml\n            >>> ext_pillar = salt.utils.yaml.safe_load(\"\"\"\n            ... ext_pillar:\n            ...   - git:\n            ...     - issue38440 https://github.com/terminalmage/git_pillar:\n            ...       - env: base\n            \"\"\")\n\n        '''\n        No need to explain. Just write code.\n        '''\""
    },
    {
        "original": "def get_message(self, dummy0, dummy1, use_cmd=False):\n        \"\"\"Get a getmore message.\"\"\"\n\n        ns = _UJOIN % (self.db, self.coll)\n\n        if use_cmd:\n            ns = _UJOIN % (self.db, \"$cmd\")\n            spec = self.as_command()[0]\n\n            return query(0, ns, 0, -1, spec, None, self.codec_options)\n\n        return get_more(ns, self.ntoreturn, self.cursor_id)",
        "rewrite": "def get_message(self, dummy0, dummy1, use_cmd=False):\n        ns = _UJOIN % (self.db, self.coll)\n\n        if use_cmd:\n            ns = _UJOIN % (self.db, \"$cmd\")\n            spec = self.as_command()[0]\n\n            return query(0, ns, 0, -1, spec, None, self.codec_options)\n\n        return get_more(ns, self.ntoreturn, self.cursor_id)"
    },
    {
        "original": "def rename_to_tmp_name(self):\n        \"\"\"Rename the container to a hopefully unique temporary container name\n        by prepending the short id.\n        \"\"\"\n        self.client.rename(\n            self.id,\n            '%s_%s' % (self.short_id, self.name)\n        )",
        "rewrite": "def rename_to_tmp_name(self):\n        self.client.rename(\n            self.id,\n            f'{self.short_id}_{self.name}'\n        )"
    },
    {
        "original": "def get_simple_split(branchfile):\n    \"\"\"Splits the branchfile argument and assuming branch is\n       the first path component in branchfile, will return\n       branch and file else None.\"\"\"\n\n    index = branchfile.find('/')\n    if index == -1:\n        return None, None\n    branch, file = branchfile.split('/', 1)\n    return branch, file",
        "rewrite": "def get_simple_split(branchfile):\n    index = branchfile.find('/')\n    if index == -1:\n        return None, None\n    branch, file = branchfile.split('/', 1)\n    return branch, file"
    },
    {
        "original": "def migrate_non_shared(vm_, target, ssh=False):\n    \"\"\"\n    Attempt to execute non-shared storage \"all\" migration\n\n    :param vm_: domain name\n    :param target: target libvirt host name\n    :param ssh: True to connect over ssh\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.migrate_non_shared <vm name> <target hypervisor>\n\n    A tunnel data migration can be performed by setting this in the\n    configuration:\n\n    .. code-block:: yaml\n\n        virt:\n            tunnel: True\n\n    For more details on tunnelled data migrations, report to\n    https://libvirt.org/migration.html#transporttunnel\n    \"\"\"\n    cmd = _get_migrate_command() + ' --copy-storage-all ' + vm_\\\n        + _get_target(target, ssh)\n\n    stdout = subprocess.Popen(cmd,\n                              shell=True,\n                              stdout=subprocess.PIPE).communicate()[0]\n    return salt.utils.stringutils.to_str(stdout)",
        "rewrite": "def migrate_non_shared(vm_, target, ssh=False):\n    cmd = _get_migrate_command() + ' --copy-storage-all ' + vm_ + _get_target(target, ssh)\n\n    stdout = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).communicate()[0]\n    return salt.utils.stringutils.to_str(stdout)"
    },
    {
        "original": "def auth_user_oid(self, email):\n        \"\"\"\n            OpenID user Authentication\n\n            :param email: user's email to authenticate\n            :type self: User model\n        \"\"\"\n        user = self.find_user(email=email)\n        if user is None or (not user.is_active):\n            log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(email))\n            return None\n        else:\n            self.update_user_auth_stat(user)\n            return user",
        "rewrite": "def auth_user_oid(self, email):\n    user = self.find_user(email=email)\n    if user is None or not user.is_active:\n        log.info(LOGMSG_WAR_SEC_LOGIN_FAILED.format(email))\n        return None\n    else:\n        self.update_user_auth_stat(user)\n        return user"
    },
    {
        "original": "def _get_values(cls, diff_dict, type='new'):\n        \"\"\"\n        Returns a dictionaries with the 'new' values in a diff dict.\n\n        type\n            Which values to return, 'new' or 'old'\n        \"\"\"\n        ret_dict = {}\n        for p in diff_dict.keys():\n            if type in diff_dict[p].keys():\n                ret_dict.update({p: diff_dict[p][type]})\n            else:\n                ret_dict.update(\n                    {p: cls._get_values(diff_dict[p], type=type)})\n        return ret_dict",
        "rewrite": "def _get_values(cls, diff_dict, type='new'):\n    ret_dict = {}\n    for p in diff_dict.keys():\n        if type in diff_dict[p].keys():\n            ret_dict.update({p: diff_dict[p][type]})\n        else:\n            ret_dict.update({p: cls._get_values(cls, diff_dict[p], type=type)})\n    return ret_dict"
    },
    {
        "original": "def get_location(conn, vm_):\n    \"\"\"\n    Return the node location to use\n    \"\"\"\n    locations = conn.list_locations()\n    # Default to Dallas if not otherwise set\n    loc = config.get_cloud_config_value('location', vm_, __opts__, default=2)\n    for location in locations:\n        if six.text_type(loc) in (six.text_type(location.id), six.text_type(location.name)):\n            return location",
        "rewrite": "def get_location(conn, vm_):\n    locations = conn.list_locations()\n    loc = config.get_cloud_config_value('location', vm_, __opts__, default=2)\n    for location in locations:\n        if str(loc) in (str(location.id), str(location.name)):\n            return location"
    },
    {
        "original": "def extend_variables(raw_variables, override_variables):\n    \"\"\" extend raw_variables with override_variables.\n        override_variables will merge and override raw_variables.\n\n    Args:\n        raw_variables (list):\n        override_variables (list):\n\n    Returns:\n        dict: extended variables mapping\n\n    Examples:\n        >>> raw_variables = [{\"var1\": \"val1\"}, {\"var2\": \"val2\"}]\n        >>> override_variables = [{\"var1\": \"val111\"}, {\"var3\": \"val3\"}]\n        >>> extend_variables(raw_variables, override_variables)\n            {\n                'var1', 'val111',\n                'var2', 'val2',\n                'var3', 'val3'\n            }\n\n    \"\"\"\n    if not raw_variables:\n        override_variables_mapping = ensure_mapping_format(override_variables)\n        return override_variables_mapping\n\n    elif not override_variables:\n        raw_variables_mapping = ensure_mapping_format(raw_variables)\n        return raw_variables_mapping\n\n    else:\n        raw_variables_mapping = ensure_mapping_format(raw_variables)\n        override_variables_mapping = ensure_mapping_format(override_variables)\n        raw_variables_mapping.update(override_variables_mapping)\n        return raw_variables_mapping",
        "rewrite": "def extend_variables(raw_variables, override_variables):\n    \"\"\"\n    extend raw_variables with override_variables.\n    override_variables will merge and override raw_variables.\n    \"\"\"\n    \n    if not raw_variables:\n        return ensure_mapping_format(override_variables)\n    \n    elif not override_variables:\n        return ensure_mapping_format(raw_variables)\n    \n    else:\n        raw_variables_mapping = ensure_mapping_format(raw_variables)\n        override_variables_mapping = ensure_mapping_format(override_variables)\n        raw_variables_mapping.update(override_variables_mapping)\n        return raw_variables_mapping"
    },
    {
        "original": "def make_linear_workflow(*tasks, **kwargs):\r\n        \"\"\"Factory method for creating linear workflows.\r\n\r\n        :param tasks: EOTask's t1,t2,...,tk with dependencies t1->t2->...->tk\r\n        :param kwargs: Optional keyword arguments (such as workflow name) forwarded to the constructor\r\n        :return: A new EO workflow instance\r\n        :rtype: EOWorkflow\r\n        \"\"\"\r\n        warnings.warn(\"Method 'make_linear_workflow' will soon be removed. Use LinearWorkflow class instead\",\r\n                      DeprecationWarning, stacklevel=2)\r\n\r\n        return LinearWorkflow(*tasks, **kwargs)",
        "rewrite": "def make_linear_workflow(*tasks, **kwargs):\r\n    warnings.warn(\"Method 'make_linear_workflow' will soon be removed. Use LinearWorkflow class instead\",\r\n                  DeprecationWarning, stacklevel=2)\r\n\r\n    return LinearWorkflow(*tasks, **kwargs)"
    },
    {
        "original": "def prepare_subprocess_cmd(subprocess_cmd):\n    \"\"\"Prepares a subprocess command by running --helpfull and masking flags.\n\n    Args:\n        subprocess_cmd: List[str], what would be passed into subprocess.call()\n            i.e. ['python', 'train.py', '--flagfile=flags']\n\n    Returns:\n        ['python', 'train.py', '--train_flag=blah', '--more_flags']\n    \"\"\"\n    help_cmd = subprocess_cmd + ['--helpfull']\n    help_output = subprocess.run(help_cmd, stdout=subprocess.PIPE).stdout\n    help_output = help_output.decode('ascii')\n    if 'python' in subprocess_cmd[0]:\n        valid_flags = parse_helpfull_output(help_output)\n    else:\n        valid_flags = parse_helpfull_output(help_output, regex=FLAG_HELP_RE_CC)\n    parsed_flags = flags.FlagValues().read_flags_from_files(subprocess_cmd[1:])\n\n    filtered_flags = filter_flags(parsed_flags, valid_flags)\n    return [subprocess_cmd[0]] + filtered_flags",
        "rewrite": "def prepare_subprocess_cmd(subprocess_cmd):\n    help_cmd = subprocess_cmd + ['--helpfull']\n    help_output = subprocess.run(help_cmd, stdout=subprocess.PIPE).stdout\n    help_output = help_output.decode('ascii')\n    \n    if 'python' in subprocess_cmd[0]:\n        valid_flags = parse_helpfull_output(help_output)\n    else:\n        valid_flags = parse_helpfull_output(help_output, regex=FLAG_HELP_RE_CC)\n    \n    parsed_flags = flags.FlagValues().read_flags_from_files(subprocess_cmd[1:])\n    \n    filtered_flags = filter_flags(parsed_flags, valid_flags)\n    return [subprocess_cmd[0]] + filtered_flags"
    },
    {
        "original": "def time_recommendation(move_num, seconds_per_move=5, time_limit=15 * 60,\n                        decay_factor=0.98):\n    \"\"\"Given the current move number and the 'desired' seconds per move, return\n    how much time should actually be used. This is intended specifically for\n    CGOS time controls, which has an absolute 15-minute time limit.\n\n    The strategy is to spend the maximum possible moves using seconds_per_move,\n    and then switch to an exponentially decaying time usage, calibrated so that\n    we have enough time for an infinite number of moves.\"\"\"\n\n    # Divide by two since you only play half the moves in a game.\n    player_move_num = move_num / 2\n\n    # Sum of geometric series maxes out at endgame_time seconds.\n    endgame_time = seconds_per_move / (1 - decay_factor)\n\n    if endgame_time > time_limit:\n        # There is so little main time that we're already in 'endgame' mode.\n        base_time = time_limit * (1 - decay_factor)\n        core_moves = 0\n    else:\n        # Leave over endgame_time seconds for the end, and play at\n        # seconds_per_move for as long as possible.\n        base_time = seconds_per_move\n        core_moves = (time_limit - endgame_time) / seconds_per_move\n\n    return base_time * decay_factor ** max(player_move_num - core_moves, 0)",
        "rewrite": "def time_recommendation(move_num, seconds_per_move=5, time_limit=15 * 60, decay_factor=0.98):\n    player_move_num = move_num / 2\n    endgame_time = seconds_per_move / (1 - decay_factor)\n    if endgame_time > time_limit:\n        base_time = time_limit * (1 - decay_factor)\n        core_moves = 0\n    else:\n        base_time = seconds_per_move\n        core_moves = (time_limit - endgame_time) / seconds_per_move\n    return base_time * decay_factor ** max(player_move_num - core_moves, 0)"
    },
    {
        "original": "def readat(self, off):\n        \"\"\"\n        Read all bytes from the start of `off` until the end of the buffer\n\n        This method can be used to determine a checksum of a buffer from a given\n        point on.\n\n        :param int off: starting offset\n        :rtype: bytearray\n        \"\"\"\n        if isinstance(off, SV):\n            off = off.value\n\n        return self.__buff[off:]",
        "rewrite": "def read_at(self, off):\n        if isinstance(off, SV):\n            off = off.value\n\n        return self.__buff[off:]"
    },
    {
        "original": "def _group_similar(items: List[T],\n                   comparer: Callable[[T, T], bool]) -> List[List[T]]:\n    \"\"\"Combines similar items into groups.\n\n  Args:\n    items: The list of items to group.\n    comparer: Determines if two items are similar.\n\n  Returns:\n    A list of groups of items.\n  \"\"\"\n    groups = []  # type: List[List[T]]\n    used = set()  # type: Set[int]\n    for i in range(len(items)):\n        if i not in used:\n            group = [items[i]]\n            for j in range(i + 1, len(items)):\n                if j not in used and comparer(items[i], items[j]):\n                    used.add(j)\n                    group.append(items[j])\n            groups.append(group)\n    return groups",
        "rewrite": "from typing import List, Callable\n\ndef _group_similar(items: List[T], comparer: Callable[[T, T], bool]) -> List[List[T]]:\n    groups = []  \n    used = set()  \n    for i in range(len(items)):\n        if i not in used:\n            group = [items[i]]\n            for j in range(i + 1, len(items)):\n                if j not in used and comparer(items[i], items[j]):\n                    used.add(j)\n                    group.append(items[j])\n            groups.append(group)\n    return groups"
    },
    {
        "original": "def pydot__tree_to_png(tree, filename, rankdir=\"LR\"):\n    \"\"\"Creates a colorful image that represents the tree (data+children, without meta)\n\n    Possible values for `rankdir` are \"TB\", \"LR\", \"BT\", \"RL\", corresponding to\n    directed graphs drawn from top to bottom, from left to right, from bottom to\n    top, and from right to left, respectively. See:\n    https://www.graphviz.org/doc/info/attrs.html#k:rankdir\n    \"\"\"\n\n    import pydot\n    graph = pydot.Dot(graph_type='digraph', rankdir=rankdir)\n\n    i = [0]\n\n    def new_leaf(leaf):\n        node = pydot.Node(i[0], label=repr(leaf))\n        i[0] += 1\n        graph.add_node(node)\n        return node\n\n    def _to_pydot(subtree):\n        color = hash(subtree.data) & 0xffffff\n        color |= 0x808080\n\n        subnodes = [_to_pydot(child) if isinstance(child, Tree) else new_leaf(child)\n                    for child in subtree.children]\n        node = pydot.Node(i[0], style=\"filled\", fillcolor=\"#%x\"%color, label=subtree.data)\n        i[0] += 1\n        graph.add_node(node)\n\n        for subnode in subnodes:\n            graph.add_edge(pydot.Edge(node, subnode))\n\n        return node\n\n    _to_pydot(tree)\n    graph.write_png(filename)",
        "rewrite": "def pydot_tree_to_png(tree, filename, rankdir=\"LR\"):\n    import pydot\n    graph = pydot.Dot(graph_type='digraph', rankdir=rankdir)\n\n    i = [0]\n\n    def new_leaf(leaf):\n        node = pydot.Node(i[0], label=repr(leaf))\n        i[0] += 1\n        graph.add_node(node)\n        return node\n\n    def _to_pydot(subtree):\n        color = hash(subtree.data) & 0xffffff\n        color |= 0x808080\n\n        subnodes = [_to_pydot(child) if isinstance(child, Tree) else new_leaf(child) for child in subtree.children]\n        node = pydot.Node(i[0], style=\"filled\", fillcolor=\"#%x\" % color, label=subtree.data)\n        i[0] += 1\n        graph.add_node(node)\n\n        for subnode in subnodes:\n            graph.add_edge(pydot.Edge(node, subnode))\n\n        return node\n\n    _to_pydot(tree)\n    graph.write_png(filename)"
    },
    {
        "original": "def Get(self, key):\n    \"\"\"Fetch the object from cache.\n\n    Objects may be flushed from cache at any time. Callers must always\n    handle the possibility of KeyError raised here.\n\n    Args:\n      key: The key used to access the object.\n\n    Returns:\n      Cached object.\n\n    Raises:\n      KeyError: If the object is not present in the cache.\n    \"\"\"\n    if key not in self._hash:\n      raise KeyError(key)\n\n    node = self._hash[key]\n\n    self._age.Unlink(node)\n    self._age.AppendNode(node)\n\n    return node.data",
        "rewrite": "def get(self, key):\n    if key not in self._hash:\n        raise KeyError(key)\n\n    node = self._hash[key]\n\n    self._age.unlink(node)\n    self._age.append_node(node)\n\n    return node.data"
    },
    {
        "original": "def sample_cc(self, nsamples=1, weighted=True):\n        \"\"\"\n        Returns a random polygon of any class. The probability of each polygon to be sampled\n        is proportional to its area if weighted is True.\n        \"\"\"\n        weights = self.areas / np.sum(self.areas) if weighted else None\n        for index in np.random.choice(a=len(self.geometries), size=nsamples, p=weights):\n            yield self.geometries[index]",
        "rewrite": "def sample_cc(self, nsamples=1, weighted=True):\n        weights = self.areas / np.sum(self.areas) if weighted else None\n        for index in np.random.choice(a=len(self.geometries), size=nsamples, p=weights):\n            yield self.geometries[index]"
    },
    {
        "original": "def remove_argument(self, name):\n        \"\"\" Remove the argument matching the given name. \"\"\"\n        for index, arg in enumerate(self.args[:]):\n            if name == arg.name:\n                del self.args[index]\n                break\n        return self",
        "rewrite": "def remove_argument(self, name):\n        for index, arg in enumerate(self.args[:]):\n            if name == arg.name:\n                del self.args[index]\n                break\n        return self"
    },
    {
        "original": "def preprocess(ops, nlp, rows, get_ids):\n    \"\"\"Parse the texts with spaCy. Make one-hot vectors for the labels.\"\"\"\n    Xs = []\n    ys = []\n    for (text1, text2), label in rows:\n        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n        ys.append(label)\n    return Xs, to_categorical(ys, nb_classes=2)",
        "rewrite": "def preprocess(ops, nlp, rows, get_ids):\n    Xs = []\n    ys = []\n    for (text1, text2), label in rows:\n        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n        ys.append(label)\n    return Xs, to_categorical(ys, nb_classes=2)"
    },
    {
        "original": "def GetIpForwardTable():\n    \"\"\"Return all Windows routes (IPv4 only) from iphlpapi\"\"\"\n    # We get the size first\n    size = ULONG()\n    res = _GetIpForwardTable(None, byref(size), False)\n    if res != 0x7a:  # ERROR_INSUFFICIENT_BUFFER -> populate size\n        raise RuntimeError(\"Error getting structure length (%d)\" % res)\n    # Now let's build our buffer\n    pointer_type = PMIB_IPFORWARDTABLE\n    buffer = create_string_buffer(size.value)\n    pIpForwardTable = ctypes.cast(buffer, pointer_type)\n    # And call GetAdaptersAddresses\n    res = _GetIpForwardTable(pIpForwardTable, byref(size), True)\n    if res != NO_ERROR:\n        raise RuntimeError(\"Error retrieving table (%d)\" % res)\n    results = []\n    for i in range(pIpForwardTable.contents.NumEntries):\n        results.append(_struct_to_dict(pIpForwardTable.contents.Table[i]))\n    del(pIpForwardTable)\n    return results",
        "rewrite": "def get_ip_forward_table():\n    size = ULONG()\n    res = _GetIpForwardTable(None, byref(size), False)\n    if res != 0x7a:\n        raise RuntimeError(\"Error getting structure length (%d)\" % res)\n    \n    pointer_type = PMIB_IPFORWARDTABLE\n    buffer = create_string_buffer(size.value)\n    pIpForwardTable = ctypes.cast(buffer, pointer_type)\n\n    res = _GetIpForwardTable(pIpForwardTable, byref(size), True)\n    if res != NO_ERROR:\n        raise RuntimeError(\"Error retrieving table (%d)\" % res)\n    \n    results = []\n    for i in range(pIpForwardTable.contents.NumEntries):\n        results.append(_struct_to_dict(pIpForwardTable.contents.Table[i]))\n    \n    del(pIpForwardTable)\n    return results"
    },
    {
        "original": "def ProcessClientResourcesStats(self, client_id, status):\n    \"\"\"Process status message from a client and update the stats.\n\n    Args:\n      client_id: Client id.\n      status: The status object returned from the client.\n    \"\"\"\n    if hasattr(status, \"child_session_id\"):\n      flow_path = status.child_session_id\n    else:\n      flow_path = \"aff4:/%s/flows/%s\" % (status.client_id, status.flow_id)\n\n    resources = rdf_client_stats.ClientResources()\n    resources.client_id = client_id\n    resources.session_id = flow_path\n    resources.cpu_usage.user_cpu_time = status.cpu_time_used.user_cpu_time\n    resources.cpu_usage.system_cpu_time = status.cpu_time_used.system_cpu_time\n    resources.network_bytes_sent = status.network_bytes_sent\n    self.context.usage_stats.RegisterResources(resources)",
        "rewrite": "def ProcessClientResourcesStats(self, client_id, status):\n    if hasattr(status, \"child_session_id\"):\n        flow_path = status.child_session_id\n    else:\n        flow_path = \"aff4:/%s/flows/%s\" % (status.client_id, status.flow_id)\n\n    resources = rdf_client_stats.ClientResources()\n    resources.client_id = client_id\n    resources.session_id = flow_path\n    resources.cpu_usage.user_cpu_time = status.cpu_time_used.user_cpu_time\n    resources.cpu_usage.system_cpu_time = status.cpu_time_used.system_cpu_time\n    resources.network_bytes_sent = status.network_bytes_sent\n    self.context.usage_stats.RegisterResources(resources)"
    },
    {
        "original": "def commits(self, **kwargs):\n        \"\"\"List the merge request commits.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of commits\n        \"\"\"\n\n        path = '%s/%s/commits' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = ProjectCommitManager(self.manager.gitlab,\n                                       parent=self.manager._parent)\n        return RESTObjectList(manager, ProjectCommit, data_list)",
        "rewrite": "def commits(self, **kwargs):\n    path = f'{self.manager.path}/{self.get_id()}/commits'\n    data_list = self.manager.gitlab.http_list(path, as_list=False, **kwargs)\n    manager = ProjectCommitManager(self.manager.gitlab, parent=self.manager._parent)\n    return RESTObjectList(manager, ProjectCommit, data_list)"
    },
    {
        "original": "def model_to_pdag(model):\n        \"\"\"Construct the DAG pattern (representing the I-equivalence class) for\n        a given DAG. This is the \"inverse\" to pdag_to_dag.\n        \"\"\"\n\n        if not isinstance(model, DAG):\n            raise TypeError(\"model: Expected DAG instance, \" +\n                            \"got type {model_type}\".format(model_type=type(model)))\n\n        skel, separating_sets = ConstraintBasedEstimator.build_skeleton(\n                                    model.nodes(),\n                                    model.get_independencies())\n        pdag = ConstraintBasedEstimator.skeleton_to_pdag(skel, separating_sets)\n\n        return pdag",
        "rewrite": "def model_to_pdag(model):\n    if not isinstance(model, DAG):\n        raise TypeError(\"model: Expected DAG instance, got type {model_type}\".format(model_type=type(model)))\n\n    skel, separating_sets = ConstraintBasedEstimator.build_skeleton(model.nodes(), model.get_independencies())\n    pdag = ConstraintBasedEstimator.skeleton_to_pdag(skel, separating_sets)\n\n    return pdag"
    },
    {
        "original": "def json_splitter(buffer):\n    \"\"\"Attempt to parse a json object from a buffer. If there is at least one\n    object, return it and the rest of the buffer, otherwise return None.\n    \"\"\"\n    buffer = buffer.strip()\n    try:\n        obj, index = json_decoder.raw_decode(buffer)\n        rest = buffer[json.decoder.WHITESPACE.match(buffer, index).end():]\n        return obj, rest\n    except ValueError:\n        return None",
        "rewrite": "def json_splitter(buffer):\n    buffer = buffer.strip()\n    try:\n        obj, index = json_decoder.raw_decode(buffer)\n        rest = buffer[json.decoder.WHITESPACE.match(buffer, index).end():]\n        return obj, rest\n    except ValueError:\n        return None"
    },
    {
        "original": "def get(self, key, index=None):\n        \"\"\"Retrieves a value associated with a key from the database\n\n        Args:\n            key (str): The key to retrieve\n        \"\"\"\n        records = self.get_multi([key], index=index)\n\n        try:\n            return records[0][1]  # return the value from the key/value tuple\n        except IndexError:\n            return None",
        "rewrite": "def get(self, key, index=None):\n    records = self.get_multi([key], index=index)\n\n    try:\n        return records[0][1]\n    except IndexError:\n        return None"
    },
    {
        "original": "def _sha256(path):\n    \"\"\"Calculate the sha256 hash of the file at path.\"\"\"\n    sha256hash = hashlib.sha256()\n    chunk_size = 8192\n    with open(path, \"rb\") as buff:\n        while True:\n            buffer = buff.read(chunk_size)\n            if not buffer:\n                break\n            sha256hash.update(buffer)\n    return sha256hash.hexdigest()",
        "rewrite": "import hashlib\n\ndef _sha256(path):\n    sha256hash = hashlib.sha256()\n    chunk_size = 8192\n    with open(path, \"rb\") as file:\n        while True:\n            buffer = file.read(chunk_size)\n            if not buffer:\n                break\n            sha256hash.update(buffer)\n    return sha256hash.hexdigest()"
    },
    {
        "original": "async def query_pathing(\n        self, start: Union[Unit, Point2, Point3], end: Union[Point2, Point3]\n    ) -> Optional[Union[int, float]]:\n        \"\"\" Caution: returns 0 when path not found \"\"\"\n        assert isinstance(start, (Point2, Unit))\n        assert isinstance(end, Point2)\n        if isinstance(start, Point2):\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(\n                            start_pos=common_pb.Point2D(x=start.x, y=start.y),\n                            end_pos=common_pb.Point2D(x=end.x, y=end.y),\n                        )\n                    ]\n                )\n            )\n        else:\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(unit_tag=start.tag, end_pos=common_pb.Point2D(x=end.x, y=end.y))\n                    ]\n                )\n            )\n        distance = float(result.query.pathing[0].distance)\n        if distance <= 0.0:\n            return None\n        return distance",
        "rewrite": "async def query_pathing(\n        self, start: Union[Unit, Point2, Point3], end: Union[Point2, Point3]\n    ) -> Optional[Union[int, float]]:\n        \"\"\" Caution: returns 0 when path not found \"\"\"\n        assert isinstance(start, (Point2, Unit))\n        assert isinstance(end, Point2)\n        if isinstance(start, Point2):\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(\n                            start_pos=common_pb.Point2D(x=start.x, y=start.y),\n                            end_pos=common_pb.Point2D(x=end.x, y=end.y),\n                        )\n                    ]\n                )\n            )\n        else:\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(unit_tag=start.tag, end_pos=common_pb.Point2D(x=end.x, y=end.y))\n                    ]\n                )\n            )\n        distance = float(result.query.pathing[0].distance)\n        if distance <= 0.0:\n            return None\n        return distance"
    },
    {
        "original": "def ofp_instruction_from_jsondict(dp, jsonlist, encap=True):\n    \"\"\"\n    This function is intended to be used with\n    ryu.lib.ofctl_string.ofp_instruction_from_str.\n    It is very similar to ofp_msg_from_jsondict, but works on\n    a list of OFPInstructions/OFPActions. It also encapsulates\n    OFPAction into OFPInstructionActions, as >OF1.0 OFPFlowMod\n    requires that.\n\n    This function takes the following arguments.\n\n    ======== ==================================================\n    Argument Description\n    ======== ==================================================\n    dp       An instance of ryu.controller.Datapath.\n    jsonlist A list of JSON style dictionaries.\n    encap    Encapsulate OFPAction into OFPInstructionActions.\n             Must be false for OF10.\n    ======== ==================================================\n    \"\"\"\n    proto = dp.ofproto\n    parser = dp.ofproto_parser\n    actions = []\n    result = []\n    for jsondict in jsonlist:\n        assert len(jsondict) == 1\n        k, v = list(jsondict.items())[0]\n        cls = getattr(parser, k)\n        if issubclass(cls, parser.OFPAction):\n            if encap:\n                actions.append(cls.from_jsondict(v))\n                continue\n        else:\n            ofpinst = getattr(parser, 'OFPInstruction', None)\n            if not ofpinst or not issubclass(cls, ofpinst):\n                raise ValueError(\"Supplied jsondict is of wrong type: %s\",\n                                 jsondict)\n        result.append(cls.from_jsondict(v))\n\n    if not encap:\n        return result\n\n    if actions:\n        # Although the OpenFlow spec says Apply Actions is executed first,\n        # let's place it in the head as a precaution.\n        result = [parser.OFPInstructionActions(\n            proto.OFPIT_APPLY_ACTIONS, actions)] + result\n    return result",
        "rewrite": "def ofp_instruction_from_jsondict(dp, jsonlist, encap=True):\n    proto = dp.ofproto\n    parser = dp.ofproto_parser\n    actions = []\n    result = []\n    for jsondict in jsonlist:\n        assert len(jsondict) == 1\n        k, v = list(jsondict.items())[0]\n        cls = getattr(parser, k)\n        if issubclass(cls, parser.OFPAction):\n            if encap:\n                actions.append(cls.from_jsondict(v))\n            continue\n        else:\n            ofpinst = getattr(parser, 'OFPInstruction', None)\n            if not ofpinst or not issubclass(cls, ofpinst):\n                raise ValueError(\"Supplied jsondict is of wrong type: %s\" % jsondict)\n        result.append(cls.from_jsondict(v))\n\n    if not encap:\n        return result\n\n    if actions:\n        result = [parser.OFPInstructionActions(\n            proto.OFPIT_APPLY_ACTIONS, actions)] + result\n    return result"
    },
    {
        "original": "def IterIfaddrs(ifaddrs):\n  \"\"\"Iterates over contents of the intrusive linked list of `ifaddrs`.\n\n  Args:\n    ifaddrs: A pointer to the first node of `ifaddrs` linked list. Can be NULL.\n\n  Yields:\n    Instances of `Ifaddr`.\n  \"\"\"\n  precondition.AssertOptionalType(ifaddrs, ctypes.POINTER(Ifaddrs))\n\n  while ifaddrs:\n    yield ifaddrs.contents\n    ifaddrs = ifaddrs.contents.ifa_next",
        "rewrite": "def IterIfaddrs(ifaddrs):\n    precondition.AssertOptionalType(ifaddrs, ctypes.POINTER(Ifaddrs))\n\n    while ifaddrs:\n        yield ifaddrs.contents\n        ifaddrs = ifaddrs.contents.ifa_next"
    },
    {
        "original": "def _process_one_indirect_jump(self, jump):\n        \"\"\"\n        Resolve a given indirect jump.\n\n        :param IndirectJump jump:  The IndirectJump instance.\n        :return:        A set of resolved indirect jump targets (ints).\n        \"\"\"\n\n        resolved = False\n        resolved_by = None\n        targets = None\n\n        block = self._lift(jump.addr, opt_level=1)\n\n        for resolver in self.indirect_jump_resolvers:\n            resolver.base_state = self._base_state\n\n            if not resolver.filter(self, jump.addr, jump.func_addr, block, jump.jumpkind):\n                continue\n\n            resolved, targets = resolver.resolve(self, jump.addr, jump.func_addr, block, jump.jumpkind)\n            if resolved:\n                resolved_by = resolver\n                break\n\n        if resolved:\n            self._indirect_jump_resolved(jump, jump.addr, resolved_by, targets)\n        else:\n            self._indirect_jump_unresolved(jump)\n\n        return set() if targets is None else set(targets)",
        "rewrite": "def _process_one_indirect_jump(self, jump):\n    resolved = False\n    resolved_by = None\n    targets = None\n\n    block = self._lift(jump.addr, opt_level=1)\n\n    for resolver in self.indirect_jump_resolvers:\n        resolver.base_state = self._base_state\n\n        if not resolver.filter(self, jump.addr, jump.func_addr, block, jump.jumpkind):\n            continue\n\n        resolved, targets = resolver.resolve(self, jump.addr, jump.func_addr, block, jump.jumpkind)\n        if resolved:\n            resolved_by = resolver\n            break\n\n    if resolved:\n        self._indirect_jump_resolved(jump, jump.addr, resolved_by, targets)\n    else:\n        self._indirect_jump_unresolved(jump)\n\n    return set() if targets is None else set(targets)"
    },
    {
        "original": "def ReadClientStartupInfo(self, client_id, cursor=None):\n    \"\"\"Reads the latest client startup record for a single client.\"\"\"\n    query = (\n        \"SELECT startup_info, UNIX_TIMESTAMP(timestamp) \"\n        \"FROM clients, client_startup_history \"\n        \"WHERE clients.last_startup_timestamp=client_startup_history.timestamp \"\n        \"AND clients.client_id=client_startup_history.client_id \"\n        \"AND clients.client_id=%s\")\n    cursor.execute(query, [db_utils.ClientIDToInt(client_id)])\n    row = cursor.fetchone()\n    if row is None:\n      return None\n\n    startup_info, timestamp = row\n    res = rdf_client.StartupInfo.FromSerializedString(startup_info)\n    res.timestamp = mysql_utils.TimestampToRDFDatetime(timestamp)\n    return res",
        "rewrite": "def ReadClientStartupInfo(self, client_id, cursor=None):\n    query = (\n        \"SELECT startup_info, UNIX_TIMESTAMP(timestamp) \"\n        \"FROM clients, client_startup_history \"\n        \"WHERE clients.last_startup_timestamp=client_startup_history.timestamp \"\n        \"AND clients.client_id=client_startup_history.client_id \"\n        \"AND clients.client_id=%s\"\n    )\n    cursor.execute(query, [db_utils.ClientIDToInt(client_id)])\n    row = cursor.fetchone()\n    if row is None:\n        return None\n\n    startup_info, timestamp = row\n    res = rdf_client.StartupInfo.FromSerializedString(startup_info)\n    res.timestamp = mysql_utils.TimestampToRDFDatetime(timestamp)\n    return res"
    },
    {
        "original": "def _encode_binary(name, value, dummy0, dummy1):\n    \"\"\"Encode bson.binary.Binary.\"\"\"\n    subtype = value.subtype\n    if subtype == 2:\n        value = _PACK_INT(len(value)) + value\n    return b\"\\x05\" + name + _PACK_LENGTH_SUBTYPE(len(value), subtype) + value",
        "rewrite": "def _encode_binary(name, value, dummy0, dummy1):\n    subtype = value.subtype\n    if subtype == 2:\n        value = _PACK_INT(len(value)) + value\n    return b\"\\x05\" + name + _PACK_LENGTH_SUBTYPE(len(value), subtype) + value"
    },
    {
        "original": "def translate(self, text, model_id=None, source=None, target=None,\n                  **kwargs):\n        \"\"\"\n        Translate.\n\n        Translates the input text from the source language to the target language.\n\n        :param list[str] text: Input text in UTF-8 encoding. Multiple entries will result\n        in multiple translations in the response.\n        :param str model_id: A globally unique string that identifies the underlying model\n        that is used for translation.\n        :param str source: Translation source language code.\n        :param str target: Translation target language code.\n        :param dict headers: A `dict` containing the request headers\n        :return: A `DetailedResponse` containing the result, headers and HTTP status code.\n        :rtype: DetailedResponse\n        \"\"\"\n\n        if text is None:\n            raise ValueError('text must be provided')\n\n        headers = {}\n        if 'headers' in kwargs:\n            headers.update(kwargs.get('headers'))\n        sdk_headers = get_sdk_headers('language_translator', 'V3', 'translate')\n        headers.update(sdk_headers)\n\n        params = {'version': self.version}\n\n        data = {\n            'text': text,\n            'model_id': model_id,\n            'source': source,\n            'target': target\n        }\n\n        url = '/v3/translate'\n        response = self.request(\n            method='POST',\n            url=url,\n            headers=headers,\n            params=params,\n            json=data,\n            accept_json=True)\n        return response",
        "rewrite": "def translate(self, text, model_id=None, source=None, target=None, **kwargs):\n    if text is None:\n        raise ValueError('text must be provided')\n\n    headers = {}\n    if 'headers' in kwargs:\n        headers.update(kwargs.get('headers'))\n    sdk_headers = get_sdk_headers('language_translator', 'V3', 'translate')\n    headers.update(sdk_headers)\n\n    params = {'version': self.version}\n\n    data = {\n        'text': text,\n        'model_id': model_id,\n        'source': source,\n        'target': target\n    }\n\n    url = '/v3/translate'\n    response = self.request(\n        method='POST',\n        url=url,\n        headers=headers,\n        params=params,\n        json=data,\n        accept_json=True)\n    return response"
    },
    {
        "original": "def parse_helpfull_output(help_output, regex=FLAG_HELP_RE_PY):\n    \"\"\"Parses the output of --helpfull.\n\n    Args:\n        help_output: str, the full output of --helpfull.\n\n    Returns:\n        A set of flags that are valid flags.\n    \"\"\"\n    valid_flags = set()\n    for _, no_prefix, flag_name in regex.findall(help_output):\n        valid_flags.add('--' + flag_name)\n        if no_prefix:\n            valid_flags.add('--no' + flag_name)\n    return valid_flags",
        "rewrite": "def parse_helpful_output(help_output, regex=FLAG_HELP_RE_PY):\n    valid_flags = set()\n    for _, no_prefix, flag_name in regex.findall(help_output):\n        valid_flags.add('--' + flag_name)\n        if no_prefix:\n            valid_flags.add('--no' + flag_name)\n    return valid_flags"
    },
    {
        "original": "def get_values(self):\n        \"\"\"\n        Returns the CPD of the variables present in the network\n\n        Examples\n        --------\n        >>> reader = XMLBIF.XMLBIFReader(\"xmlbif_test.xml\")\n        >>> reader.get_values()\n        {'bowel-problem': array([[ 0.01],\n                                 [ 0.99]]),\n         'dog-out': array([[ 0.99,  0.01,  0.97,  0.03],\n                           [ 0.9 ,  0.1 ,  0.3 ,  0.7 ]]),\n         'family-out': array([[ 0.15],\n                              [ 0.85]]),\n         'hear-bark': array([[ 0.7 ,  0.3 ],\n                             [ 0.01,  0.99]]),\n         'light-on': array([[ 0.6 ,  0.4 ],\n                            [ 0.05,  0.95]])}\n        \"\"\"\n        variable_CPD = {definition.find('FOR').text: list(map(float, table.text.split()))\n                        for definition in self.network.findall('DEFINITION')\n                        for table in definition.findall('TABLE')}\n        for variable in variable_CPD:\n            arr = np.array(variable_CPD[variable])\n            arr = arr.reshape((len(self.variable_states[variable]),\n                               arr.size // len(self.variable_states[variable])), order='F')\n            variable_CPD[variable] = arr\n        return variable_CPD",
        "rewrite": "def get_values(self):\n    variable_CPD = {definition.find('FOR').text: list(map(float, table.text.split()))\n                    for definition in self.network.findall('DEFINITION')\n                    for table in definition.findall('TABLE')}\n    \n    for variable in variable_CPD:\n        arr = np.array(variable_CPD[variable])\n        arr = arr.reshape((len(self.variable_states[variable]), arr.size // len(self.variable_states[variable])), order='F')\n        variable_CPD[variable] = arr\n    \n    return variable_CPD"
    },
    {
        "original": "def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n        \"\"\" A (private) method to convert datetime array to numeric dtype\n        See duck_array_ops.datetime_to_numeric\n        \"\"\"\n        numeric_array = duck_array_ops.datetime_to_numeric(\n            self.data, offset, datetime_unit, dtype)\n        return type(self)(self.dims, numeric_array, self._attrs)",
        "rewrite": "def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n        numeric_array = duck_array_ops.datetime_to_numeric(self.data, offset, datetime_unit, dtype)\n        return type(self)(self.dims, numeric_array, self._attrs)"
    },
    {
        "original": "def format_timestamp(t):\n    \"\"\"Cast given object to a Timestamp and return a nicely formatted string\"\"\"\n    # Timestamp is only valid for 1678 to 2262\n    try:\n        datetime_str = str(pd.Timestamp(t))\n    except OutOfBoundsDatetime:\n        datetime_str = str(t)\n\n    try:\n        date_str, time_str = datetime_str.split()\n    except ValueError:\n        # catch NaT and others that don't split nicely\n        return datetime_str\n    else:\n        if time_str == '00:00:00':\n            return date_str\n        else:\n            return '{}T{}'.format(date_str, time_str)",
        "rewrite": "def format_timestamp(t):\n    try:\n        datetime_str = str(pd.Timestamp(t))\n    except (OutOfBoundsDatetime, ValueError):\n        return str(t)\n\n    try:\n        date_str, time_str = datetime_str.split()\n    except ValueError:\n        return datetime_str\n\n    if time_str == '00:00:00':\n        return date_str\n    else:\n        return '{}T{}'.format(date_str, time_str)"
    },
    {
        "original": "def k8s_events_handle_job_statuses(self: 'celery_app.task', payload: Dict) -> None:\n    \"\"\"Project jobs statuses\"\"\"\n    details = payload['details']\n    job_uuid = details['labels']['job_uuid']\n    job_name = details['labels']['job_name']\n    project_name = details['labels'].get('project_name')\n    logger.debug('handling events status for job %s', job_name)\n\n    try:\n        job = Job.objects.get(uuid=job_uuid)\n    except Job.DoesNotExist:\n        logger.debug('Job `%s` does not exist', job_name)\n        return\n\n    try:\n        job.project\n    except Project.DoesNotExist:\n        logger.debug('Project for job `%s` does not exist', project_name)\n        return\n\n    # Set the new status\n    try:\n        set_node_scheduling(job, details['node_name'])\n        job.set_status(status=payload['status'],\n                       message=payload['message'],\n                       traceback=payload.get('traceback'),\n                       details=details)\n    except IntegrityError:\n        # Due to concurrency this could happen, we just retry it\n        self.retry(countdown=Intervals.EXPERIMENTS_SCHEDULER)",
        "rewrite": "def k8s_events_handle_job_statuses(self: 'celery_app.task', payload: Dict) -> None:\n    details = payload['details']\n    job_uuid = details['labels']['job_uuid']\n    job_name = details['labels']['job_name']\n    project_name = details['labels'].get('project_name')\n    logger.debug('handling events status for job %s', job_name)\n\n    try:\n        job = Job.objects.get(uuid=job_uuid)\n    except Job.DoesNotExist:\n        logger.debug('Job `%s` does not exist', job_name)\n        return\n\n    try:\n        job.project\n    except Project.DoesNotExist:\n        logger.debug('Project for job `%s` does not exist', project_name)\n        return\n\n    try:\n        set_node_scheduling(job, details['node_name'])\n        job.set_status(status=payload['status'],\n                       message=payload['message'],\n                       traceback=payload.get('traceback'),\n                       details=details)\n    except IntegrityError:\n        self.retry(countdown=Intervals.EXPERIMENTS_SCHEDULER)"
    },
    {
        "original": "def get_parser(scheduler, err_file, out_file=None, run_err_file=None, batch_err_file=None):\n    \"\"\"\n    Factory function to provide the parser for the specified scheduler. If the scheduler is not implemented None is\n    returned. The files, string, correspond to file names of the out and err files:\n    err_file        stderr of the scheduler\n    out_file        stdout of the scheduler\n    run_err_file    stderr of the application\n    batch_err_file  stderr of the submission\n\n    Returns:\n        None if scheduler is not supported.\n    \"\"\"\n    cls = ALL_PARSERS.get(scheduler)\n    return cls if cls is None else cls(err_file, out_file, run_err_file, batch_err_file)",
        "rewrite": "def get_parser(scheduler, err_file, out_file=None, run_err_file=None, batch_err_file=None):\n    cls = ALL_PARSERS.get(scheduler)\n    return cls(err_file, out_file, run_err_file, batch_err_file) if cls else None"
    },
    {
        "original": "def min_volatility(self):\n        \"\"\"\n        Minimise volatility.\n\n        :return: asset weights for the volatility-minimising portfolio\n        :rtype: dict\n        \"\"\"\n        args = (self.cov_matrix, self.gamma)\n        result = sco.minimize(\n            objective_functions.volatility,\n            x0=self.initial_guess,\n            args=args,\n            method=\"SLSQP\",\n            bounds=self.bounds,\n            constraints=self.constraints,\n        )\n        self.weights = result[\"x\"]\n        return dict(zip(self.tickers, self.weights))",
        "rewrite": "def min_volatility(self):\n    args = (self.cov_matrix, self.gamma)\n    result = sco.minimize(\n        objective_functions.volatility,\n        x0=self.initial_guess,\n        args=args,\n        method=\"SLSQP\",\n        bounds=self.bounds,\n        constraints=self.constraints,\n    )\n    self.weights = result[\"x\"]\n    return dict(zip(self.tickers, self.weights))"
    },
    {
        "original": "def get_structure_with_only_magnetic_atoms(self, make_primitive=True):\n        \"\"\"\n        Returns a Structure with only magnetic atoms present.\n        :return: Structure\n        \"\"\"\n\n        sites = [site for site in self.structure if abs(site.properties[\"magmom\"]) > 0]\n\n        structure = Structure.from_sites(sites)\n\n        if make_primitive:\n            structure = structure.get_primitive_structure(use_site_props=True)\n\n        return structure",
        "rewrite": "def get_structure_with_only_magnetic_atoms(self, make_primitive=True):\n    sites = [site for site in self.structure if abs(site.properties[\"magmom\"]) > 0]\n    \n    structure = Structure.from_sites(sites)\n    \n    if make_primitive:\n        structure = structure.get_primitive_structure(use_site_props=True)\n    \n    return structure"
    },
    {
        "original": "def create_baseline(tag=\"baseline\", config='root'):\n    \"\"\"\n    Creates a snapshot marked as baseline\n\n    tag\n        Tag name for the baseline\n\n    config\n        Configuration name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' snapper.create_baseline\n        salt '*' snapper.create_baseline my_custom_baseline\n    \"\"\"\n    return __salt__['snapper.create_snapshot'](config=config,\n                                               snapshot_type='single',\n                                               description=\"baseline snapshot\",\n                                               cleanup_algorithm=\"number\",\n                                               userdata={\"baseline_tag\": tag})",
        "rewrite": "def create_baseline(tag=\"baseline\", config='root'):\n    return __salt__['snapper.create_snapshot'](config=config,\n                                               snapshot_type='single',\n                                               description=\"baseline snapshot\",\n                                               cleanup_algorithm=\"number\",\n                                               userdata={\"baseline_tag\": tag})"
    },
    {
        "original": "def accept_moderator_invite(self, subreddit):\n        \"\"\"Accept a moderator invite to the given subreddit.\n\n        Callable upon an instance of Subreddit with no arguments.\n\n        :returns: The json response from the server.\n\n        \"\"\"\n        data = {'r': six.text_type(subreddit)}\n        # Clear moderated subreddits and cache\n        self.user._mod_subs = None  # pylint: disable=W0212\n        self.evict(self.config['my_mod_subreddits'])\n        return self.request_json(self.config['accept_mod_invite'], data=data)",
        "rewrite": "def accept_moderator_invite(self, subreddit):\n        data = {'r': str(subreddit)}\n        self.user._mod_subs = None  \n        self.evict(self.config['my_mod_subreddits'])\n        return self.request_json(self.config['accept_mod_invite'], data=data)"
    },
    {
        "original": "def upgrade_available(name, **kwargs):\n    \"\"\"\n    Check if there is an upgrade available for a certain package\n    Accepts full or partial FMRI. Returns all matches found.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.upgrade_available apache-22\n    \"\"\"\n    version = None\n    cmd = ['pkg', 'list', '-Huv', name]\n    lines = __salt__['cmd.run_stdout'](cmd).splitlines()\n    if not lines:\n        return {}\n    ret = {}\n    for line in lines:\n        ret[_ips_get_pkgname(line)] = _ips_get_pkgversion(line)\n    return ret",
        "rewrite": "def upgrade_available(name, **kwargs):\n    version = None\n    cmd = ['pkg', 'list', '-Huv', name]\n    lines = __salt__['cmd.run_stdout'](cmd).splitlines()\n    if not lines:\n        return {}\n    ret = {}\n    for line in lines:\n        ret[_ips_get_pkgname(line)] = _ips_get_pkgversion(line)\n    return ret"
    },
    {
        "original": "def get_all():\n    \"\"\"\n    Return all installed services.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all\n    \"\"\"\n    ret = []\n    service = _cmd()\n    for svc in __salt__['cmd.run']('{0} ls all'.format(service)).splitlines():\n        ret.append(svc)\n    return sorted(ret)",
        "rewrite": "def get_all():\n    \"\"\"\n    Return all installed services.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.get_all\n    \"\"\"\n    ret = []\n    service = _cmd()\n    for svc in __salt__['cmd.run']('{0} ls all'.format(service)).splitlines():\n        ret.append(svc)\n    return sorted(ret)"
    },
    {
        "original": "def loadCats(self, ids=[]):\n        \"\"\"\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        \"\"\"\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]",
        "rewrite": "def loadCats(self, ids=[]):\n    if _isArrayLike(ids):\n        return [self.cats[id] for id in ids]\n    elif isinstance(ids, int):\n        return [self.cats[ids]]"
    },
    {
        "original": "def register_targets(name,\n                     targets,\n                     region=None,\n                     key=None,\n                     keyid=None,\n                     profile=None):\n    \"\"\"\n    Register targets to a target froup of an ALB. ``targets`` is either a\n    instance id string or a list of instance id's.\n\n    Returns:\n\n    - ``True``: instance(s) registered successfully\n    - ``False``: instance(s) failed to be registered\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_elbv2.register_targets myelb instance_id\n        salt myminion boto_elbv2.register_targets myelb \"[instance_id,instance_id]\"\n    \"\"\"\n    targetsdict = []\n    if isinstance(targets, six.string_types) or isinstance(targets, six.text_type):\n        targetsdict.append({\"Id\": targets})\n    else:\n        for target in targets:\n            targetsdict.append({\"Id\": target})\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        registered_targets = conn.register_targets(TargetGroupArn=name, Targets=targetsdict)\n        if registered_targets:\n            return True\n        return False\n    except ClientError as error:\n        log.warning(error)\n        return False",
        "rewrite": "def register_targets(name, targets, region=None, key=None, keyid=None, profile=None):\n    targetsdict = []\n    if isinstance(targets, six.string_types) or isinstance(targets, six.text_type):\n        targetsdict.append({\"Id\": targets})\n    else:\n        for target in targets:\n            targetsdict.append({\"Id\": target})\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        registered_targets = conn.register_targets(TargetGroupArn=name, Targets=targetsdict)\n        if registered_targets:\n            return True\n        return False\n    except ClientError as error:\n        log.warning(error)\n        return False"
    },
    {
        "original": "def cell(self, row_idx, col_idx):\n        \"\"\"\n        Return |_Cell| instance correponding to table cell at *row_idx*,\n        *col_idx* intersection, where (0, 0) is the top, left-most cell.\n        \"\"\"\n        cell_idx = col_idx + (row_idx * self._column_count)\n        return self._cells[cell_idx]",
        "rewrite": "def cell(self, row_idx, col_idx):\n    cell_idx = col_idx + (row_idx * self._column_count)\n    return self._cells[cell_idx]"
    },
    {
        "original": "def get_polarization_change_norm(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n        \"\"\"\n        Get magnitude of difference between nonpolar and polar same branch\n        polarization.\n        \"\"\"\n        polar = self.structures[-1]\n        a, b, c = polar.lattice.matrix\n        a, b, c = a / np.linalg.norm(a), b / np.linalg.norm(\n            b), c / np.linalg.norm(c)\n        P = self.get_polarization_change(convert_to_muC_per_cm2=convert_to_muC_per_cm2,\n                                         all_in_polar=all_in_polar).ravel()\n        P_norm = np.linalg.norm(a * P[0] + b * P[1] + c * P[2])\n        return P_norm",
        "rewrite": "def get_polarization_change_norm(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n    polar = self.structures[-1]\n    a, b, c = polar.lattice.matrix\n    a, b, c = a / np.linalg.norm(a), b / np.linalg.norm(b), c / np.linalg.norm(c)\n    P = self.get_polarization_change(convert_to_muC_per_cm2=convert_to_muC_per_cm2, all_in_polar=all_in_polar).ravel()\n    P_norm = np.linalg.norm(a * P[0] + b * P[1] + c * P[2])\n    return P_norm"
    },
    {
        "original": "def from_rfc(datestring, use_dateutil=True):\n    \"\"\"Parse a RFC822-formatted datetime string and return a datetime object.\n\n    Use dateutil's parser if possible.\n\n    https://stackoverflow.com/questions/885015/how-to-parse-a-rfc-2822-date-time-into-a-python-datetime\n    \"\"\"\n    # Use dateutil's parser if possible\n    if dateutil_available and use_dateutil:\n        return parser.parse(datestring)\n    else:\n        parsed = parsedate(datestring)  # as a tuple\n        timestamp = time.mktime(parsed)\n        return datetime.datetime.fromtimestamp(timestamp)",
        "rewrite": "from dateutil import parser\nfrom email.utils import parsedate\nimport datetime\nimport time\n\n\ndef from_rfc(datestring, use_dateutil=True):\n    if use_dateutil:\n        return parser.parse(datestring)\n    else:\n        parsed = parsedate(datestring)  # as a tuple\n        timestamp = time.mktime(parsed)\n        return datetime.datetime.fromtimestamp(timestamp)"
    },
    {
        "original": "def create_image(data: np.ndarray, colormap, data_min=None, data_max=None, normalize=True) -> QImage:\n        \"\"\"\n        Create QImage from ARGB array.\n        The ARGB must have shape (width, height, 4) and dtype=ubyte.\n        NOTE: The order of values in the 3rd axis must be (blue, green, red, alpha).\n        :return:\n        \"\"\"\n        image_data = Spectrogram.apply_bgra_lookup(data, colormap, data_min, data_max, normalize)\n\n        if not image_data.flags['C_CONTIGUOUS']:\n            logger.debug(\"Array was not C_CONTIGUOUS. Converting it.\")\n            image_data = np.ascontiguousarray(image_data)\n\n        try:\n            # QImage constructor needs inverted row/column order\n            image = QImage(image_data.ctypes.data, image_data.shape[1], image_data.shape[0], QImage.Format_ARGB32)\n        except Exception as e:\n            logger.error(\"could not create image \" + str(e))\n            return QImage()\n\n        image.data = image_data\n        return image",
        "rewrite": "import numpy as np\nfrom PyQt5.QtGui import QImage\nfrom Spectrogram import Spectrogram\n\ndef create_image(data: np.ndarray, colormap, data_min=None, data_max=None, normalize=True) -> QImage:\n    \"\"\"\n    Create QImage from ARGB array.\n    The ARGB must have shape (width, height, 4) and dtype=ubyte.\n    NOTE: The order of values in the 3rd axis must be (blue, green, red, alpha).\n    :return:\n    \"\"\"\n    image_data = Spectrogram.apply_bgra_lookup(data, colormap, data_min, data_max, normalize)\n\n    if not image_data.flags['C_CONTIGUOUS']:\n        logger.debug(\"Array was not C_CONTIGUOUS. Converting it.\")\n        image_data = np.ascontiguousarray(image_data)\n\n    try:\n        # QImage constructor needs inverted row/column order\n        image = QImage(image_data.ctypes.data, image_data.shape[1], image_data.shape[0], QImage.Format_ARGB32)\n    except Exception as e:\n        logger.error(\"could not create image \" + str(e))\n        return QImage()\n\n    image.data = image_data\n    return image"
    },
    {
        "original": "def import_status(handler, host=None, core_name=None, verbose=False):\n    \"\"\"\n    Submits an import command to the specified handler using specified options.\n    This command can only be run if the minion is configured with\n    solr.type: 'master'\n\n    handler : str\n        The name of the data import handler.\n    host : str (None)\n        The solr host to query. __opts__['host'] is default.\n    core : str (None)\n        The core the handler belongs to.\n    verbose : boolean (False)\n        Specifies verbose output\n\n    Return : dict<str,obj>::\n\n        {'success':boolean, 'data':dict, 'errors':list, 'warnings':list}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' solr.import_status dataimport None music False\n    \"\"\"\n    if not _is_master() and _get_none_or_value(host) is None:\n        errors = ['solr.import_status can only be called by \"master\" minions']\n        return _get_return_dict(False, errors=errors)\n\n    extra = [\"command=status\"]\n    if verbose:\n        extra.append(\"verbose=true\")\n    url = _format_url(handler, host=host, core_name=core_name, extra=extra)\n    return _http_request(url)",
        "rewrite": "def import_status(handler, host=None, core_name=None, verbose=False):\n    if not _is_master() and _get_none_or_value(host) is None:\n        errors = ['solr.import_status can only be called by \"master\" minions']\n        return _get_return_dict(False, errors=errors)\n\n    extra = [\"command=status\"]\n    if verbose:\n        extra.append(\"verbose=true\")\n    url = _format_url(handler, host=host, core_name=core_name, extra=extra)\n    return _http_request(url)"
    },
    {
        "original": "def RestrictFeedItemToAdGroup(client, feed_item, adgroup_id):\n  \"\"\"Restricts the feed item to an ad group.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed_item: The feed item.\n    adgroup_id: The ad group ID.\n  \"\"\"\n  # Get the FeedItemTargetService\n  feed_item_target_service = client.GetService(\n      'FeedItemTargetService', 'v201809')\n\n  # Optional: Restrict the first feed item to only serve with ads for the\n  # specified ad group ID.\n  ad_group_target = {\n      'xsi_type': 'FeedItemAdGroupTarget',\n      'feedId': feed_item['feedId'],\n      'feedItemId': feed_item['feedItemId'],\n      'adGroupId': adgroup_id\n  }\n\n  operation = {'operator': 'ADD', 'operand': ad_group_target}\n\n  response = feed_item_target_service.mutate([operation])\n  new_ad_group_target = response['value'][0]\n\n  print('Feed item target for feed ID %s and feed item ID %s was created to '\n        'restrict serving to ad group ID %s' %\n        (new_ad_group_target['feedId'],\n         new_ad_group_target['feedItemId'],\n         new_ad_group_target['adGroupId']))",
        "rewrite": "def restrict_feed_item_to_ad_group(client, feed_item, adgroup_id):\n    feed_item_target_service = client.GetService('FeedItemTargetService', 'v201809')\n    \n    ad_group_target = {\n        'xsi_type': 'FeedItemAdGroupTarget',\n        'feedId': feed_item['feedId'],\n        'feedItemId': feed_item['feedItemId'],\n        'adGroupId': adgroup_id\n    }\n\n    operation = {'operator': 'ADD', 'operand': ad_group_target}\n\n    response = feed_item_target_service.mutate([operation])\n    new_ad_group_target = response['value'][0]\n\n    print('Feed item target for feed ID %s and feed item ID %s was created to restrict serving to ad group ID %s' % (new_ad_group_target['feedId'], new_ad_group_target['feedItemId'], new_ad_group_target['adGroupId']))"
    },
    {
        "original": "def linear_trend_timewise(x, param):\n    \"\"\"\n    Calculate a linear least-squares regression for the values of the time series versus the sequence from 0 to\n    length of the time series minus one.\n    This feature uses the index of the time series to fit the model, which must be of a datetime\n    dtype.\n    The parameters control which of the characteristics are returned.\n\n    Possible extracted attributes are \"pvalue\", \"rvalue\", \"intercept\", \"slope\", \"stderr\", see the documentation of\n    linregress for more information.\n\n    :param x: the time series to calculate the feature of. The index must be datetime.\n    :type x: pandas.Series\n    :param param: contains dictionaries {\"attr\": x} with x an string, the attribute name of the regression model\n    :type param: list\n    :return: the different feature values\n    :return type: list\n    \"\"\"\n    ix = x.index\n\n    # Get differences between each timestamp and the first timestamp in seconds.\n    # Then convert to hours and reshape for linear regression\n    times_seconds = (ix - ix[0]).total_seconds()\n    times_hours = np.asarray(times_seconds / float(3600))\n\n    linReg = linregress(times_hours, x.values)\n\n    return [(\"attr_\\\"{}\\\"\".format(config[\"attr\"]), getattr(linReg, config[\"attr\"]))\n            for config in param]",
        "rewrite": "def linear_trend_timewise(x, param):\n    ix = x.index\n    times_seconds = (ix - ix[0]).total_seconds()\n    times_hours = np.asarray(times_seconds / float(3600))\n    linReg = linregress(times_hours, x.values)\n    return [(\"attr_{}\".format(config[\"attr\"]), getattr(linReg, config[\"attr\"])) for config in param]"
    }
]