[
    {
        "original": "def create_iterator(self, start=0, step=1, security_level=1): \n            reaches the end of the key space.\n\n        :param security_level:\n            The security level of the keys to generate.\n\n            This value must be between 1 and 4, inclusive.\n\n        :return:\n            A new :class:`KeyIterator` instance.\n        \"\"\"\n        if security_level < 1 or security_level > 4:\n          ",
        "rewrite": "def create_iterator(self, start=0, step=1, security_level=1): \n        \"\"\"\n        Creates a new KeyIterator instance with specified security level\n        \n        :param start:\n            The starting point of the key space.\n\n        :param step:\n            The step size to increment the iterator.\n\n        :param security_level:\n            The security level of the keys to generate.\n            It must be between 1 and 4, inclusive.\n\n        :return:\n            A new KeyIterator instance.\n        \"\"\"\n        if security_level < 1 or security_level > 4:\n            raise ValueError(\"Security level must be between 1 and 4.\")\n        \n        return KeyIterator(start, step, security_level)"
    },
    {
        "original": "def remove_property(self, key=None, value=None): \n        if key is None:\n            self._properties = []\n        else:\n            self._properties = [p for p in self._properties if p.key != key]\n\n        if value is not None:\n            self._properties = [p for p in self._properties if p.value != value]\n\n    def remove_properties(self, key=None, value=None):\n        \"\"\"Remove all properties matching both key and value.\n\n        :param str key: Key of",
        "rewrite": "def remove_property(self, key=None, value=None): \n        if key is None:\n            self._properties = []\n        else:\n            self._properties = [p for p in self._properties if p.key != key]\n\n        if value is not None:\n            self._properties = [p for p in self._properties if p.value != value]\n\n    def remove_properties(self, key=None, value=None):\n        self._properties = [p for p in self._properties if p.key != key or p.value != value]"
    },
    {
        "original": "def round_values(df, *, column: str, decimals: int, new_column: str = None): is created and the original column is modified\n\n    ---\n\n    ### Example\n\n    ```python\n    df = pd.DataFrame({\n        'A': [1.23456789, 2.3456789, 3.456789, 4.56789, 5.6789]\n    })\n\n    df = round_values(df, column='A', decimals=2)\n    ```\n\n    ---\n\n    ### Returns\n\n    A pandas dataframe with",
        "rewrite": "def round_values(df, *, column: str, decimals: int, new_column: str = None):\n    if new_column is None:\n        new_column = f\"{column}_rounded\"\n    df[new_column] = df[column].round(decimals)\n    return df"
    },
    {
        "original": "def get_organism(self): \n        organism = self.organism\n        if organism == 'Human & Mouse':\n            organism = 'Homo sapiens'\n        elif organism == 'Fly':\n            organism = 'Drosophila melanogaster'\n        elif organism == 'Yeast':\n            organism = 'Saccharomyces cerevisiae'\n        elif organism == 'Worm':\n            organism = 'Caenorhabditis elegans'\n        elif organism",
        "rewrite": "def get_organism(self):\n        organism = self.organism\n        if organism == 'Human & Mouse':\n            organism = 'Homo sapiens'\n        elif organism == 'Fly':\n            organism = 'Drosophila melanogaster'\n        elif organism == 'Yeast':\n            organism = 'Saccharomyces cerevisiae'\n        elif organism == 'Worm':\n            organism = 'Caenorhabditis elegans'\n        elif organism: # add condition for all other cases\n            pass # do nothing for other cases as required."
    },
    {
        "original": "def _general_error_handler(http_error): \n    print(\"HTTP error: {0}\".format(http_error))\n\n\ndef _general_exception_handler(exception):\n    \"\"\" Simple exception handler for azure.\"\"\"\n    print(\"Exception: {0}\".format(exception))\n\n\ndef _general_response_handler(response):\n    \"\"\" Simple response handler for azure.\"\"\"\n    print(\"Response: {0}\".format(response))\n\n\ndef _general_status_handler(status):\n    \"\"\" Simple status handler for azure.\"\"\"\n    print(\"Status: {0}\".format(status))",
        "rewrite": "def _general_error_handler(http_error): \n    print(\"HTTP error: {0}\".format(http_error))\n\n\ndef _general_exception_handler(exception):\n    print(\"Exception: {0}\".format(exception))\n\n\ndef _general_response_handler(response):\n    print(\"Response: {0}\".format(response))\n\n\ndef _general_status_handler(status):\n    print(\"Status: {0}\".format(status))"
    },
    {
        "original": "def calc_translations_parallel(images): \n    from skimage.feature import register_translation\n    from joblib import Parallel, delayed\n\n    # Calculate translations in parallel\n    translations = Parallel(n_jobs=4)(\n        delayed(register_translation)(image, images[i])\n        for i, image in enumerate(images))\n\n    # Convert to array\n    translations = np.array(translations)\n\n    # Extract translations\n    ty = translations[:, 0]\n    tx = translations[:, 1]\n\n    return ty",
        "rewrite": "import numpy as np\nfrom skimage.feature import register_translation\nfrom joblib import Parallel, delayed\n\ndef calc_translations_parallel(images):\n    translations = Parallel(n_jobs=4)(\n        delayed(register_translation)(image, images[i])\n        for i, image in enumerate(images))\n\n    translations = np.array(translations)\n\n    ty = translations[:, 0]\n\n    return ty"
    },
    {
        "original": "def file(self): \n        if not self.args.file:\n            return\n\n        with open(self.args.file, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                self.domains.append(line)\n\n    def",
        "rewrite": "def read_file(self): \n        if not self.args.file:\n            return\n\n        with open(self.args.file, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                self.domains.append(line)"
    },
    {
        "original": "def add_netnode_plugin_name(plugin_name): \n    idc.AddNetNodePluginName(plugin_name)\n\n\ndef get_netnode_plugin_names():\n    \"\"\"\n    Return the list of plugin names registered in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"\n    return idc.GetNetNodePluginNames()\n\n\ndef get_netnode_plugin_name(netnode_id):\n    \"\"\"\n    Return the plugin name associated with the given netnode ID.\n    Note that this implicitly uses the open IDB via the id",
        "rewrite": "def add_netnode_plugin_name(plugin_name): \n    idc.AddNetNodePluginName(plugin_name)\n\ndef get_netnode_plugin_names():\n    return idc.GetNetNodePluginNames()\n\ndef get_netnode_plugin_name(netnode_id):    \n    return idc.GetNetNodePluginName(netnode_id)"
    },
    {
        "original": "def open_shift_managed_clusters(self): \n        api_version = self._get_api_version('open_shift_managed_clusters')\n        if api_version == '2018-09-30-preview':\n            from .v2018_09_30_preview.operations import OpenShiftManagedClustersOperations as OperationClass\n        else:\n            raise ValueError(\"API version {} does not have operation group 'open_shift_managed_clusters'\".format(api_version))\n        return OperationClass(self._client, self._config, Serial",
        "rewrite": "def open_shift_managed_clusters(self): \n    api_version = self._get_api_version('open_shift_managed_clusters')\n    if api_version == '2018-09-30-preview':\n        from .v2018_09_30_preview.operations import OpenShiftManagedClustersOperations as OperationClass\n    else:\n        raise ValueError(\"API version {} does not have operation group 'open_shift_managed_clusters'\".format(api_version))\n    return OperationClass(self._client, self._config)"
    },
    {
        "original": "def run(self, dag): \n        if dag.width() > len(self.coupling_map.get_edges()):\n            raise TranspilerError('Layout cannot be found for a dag with more qubits than edges in the coupling map.')\n\n        # initialize the layout\n        layout = Layout()\n        for qreg in dag.qregs.values():\n            for i in range(qreg.size):\n                layout[qreg] = i\n\n        # map each qubit in the dag to the layout\n   ",
        "rewrite": "def run(self, dag): \n    if dag.width() > len(self.coupling_map.get_edges()):\n        raise TranspilerError('Layout cannot be found for a dag with more qubits than edges in the coupling map.')\n\n    # initialize the layout\n    layout = Layout()\n    for qreg in dag.qregs.values():\n        for i in range(qreg.size):\n            layout[qreg] = i\n\n    # map each qubit in the dag to the layout\n    for node in dag.topological_nodes():\n        for qubit in node.qargs:\n            node.qargs[node.qargs.index(qubit)] = Qubit(layout[qubit.register], qubit.index)"
    },
    {
        "original": "def maybe_check_wont_broadcast(flat_xs, validate_args): \n  if not validate_args:\n    return\n\n  for x in flat_xs:\n    if x.shape.ndims is not None and x.shape.ndims > 0:\n      if x.shape.dims[0] != 1:\n        raise ValueError(\n            'The first dimension of a `Tensor` passed to `concatenate` '\n            'must be size 1, but saw: %s' % x.shape)\n\n\ndef _concat_parts(parts, axis, validate_args):\n  \"\"\"Con",
        "rewrite": "def maybe_check_wont_broadcast(flat_xs, validate_args):\n    if not validate_args:\n        return\n\n    for x in flat_xs:\n        if x.shape.ndims is not None and x.shape.ndims > 0:\n            if x.shape.dims[0] != 1:\n                raise ValueError(\n                    'The first dimension of a `Tensor` passed to `concatenate` '\n                    'must be size 1, but saw: %s' % x.shape)\n\n\ndef _concat_parts(parts, axis, validate_args):\n    \"\"\"Concatenate parts along axis if possible.\"\"\"\n    # Code implementation goes here."
    },
    {
        "original": "def set_variable(self, name, type_, size): \n        if type_ not in self.datatypes_size:\n            raise ValueError(\"Unknown type %s\" % type_)\n        if size is not None:\n            if not isinstance(size, tuple):\n                raise ValueError(\"Size must be a tuple\")\n            if not all(isinstance(x, int) for x in size):\n                raise ValueError(\"Size must be a tuple of ints\")\n   ",
        "rewrite": "def set_variable(self, name, type_, size): \n    if type_ not in self.datatypes_size:\n        raise ValueError(\"Unknown type %s\" % type_)\n    if size is not None:\n        if not isinstance(size, tuple):\n            raise ValueError(\"Size must be a tuple\")\n        if not all(isinstance(x, int) for x in size):\n            raise ValueError(\"Size must be a tuple of ints\")"
    },
    {
        "original": "def print_stats(self): \n        if self.n_samples == 0:\n            return\n        self.stats['mean'] = self.stats['sum'] / self.n_samples\n        self.stats['std'] = np.sqrt(\n            (self.stats['sum_sq'] - self.stats['sum']**2 / self.n_samples) /\n            (self.n_samples - 1))\n        self.stats['min'] = self.stats['min_val']\n        self.stats['max'] = self.stats['max_val']\n\n    def print",
        "rewrite": "_stats(self): \n        if self.n_samples == 0:\n            return\n        self.stats['mean'] = self.stats['sum'] / self.n_samples\n        self.stats['std'] = np.sqrt(\n            (self.stats['sum_sq'] - self.stats['sum']**2 / self.n_samples) /\n            (self.n_samples - 1))\n        self.stats['min'] = self.stats['min_val']\n        self.stats['max'] = self.stats['max_val']"
    },
    {
        "original": "def available_products(cls, user, category=None, products=None): \n        if not products:\n            products = []\n        if not category:\n            category = []\n        for cat in category:\n            products.extend(cls.objects.filter(\n                category=cat,\n                is_available=True,\n                is_active=True,\n   ",
        "rewrite": "def available_products(cls, user, category=None, products=None):\n    if products is None:\n        products = []\n    if category is None:\n        category = []\n    \n    for cat in category:\n        products.extend(cls.objects.filter(\n            category=cat,\n            is_available=True,\n            is_active=True,\n        ))"
    },
    {
        "original": "def plfit_lsq(x,y): \n    x = np.array(x)\n    y = np.array(y)\n    x = x[~np.isnan(y)]\n    y = y[~np.isnan(y)]\n    x = x[~np.isinf(y)]\n    y = y[~np.isinf(y)]\n    x = x[~np.isnan(x)]\n    y = y[~np.isnan(x)]\n    x = x[~np.isinf(x)]\n    y = y[~np.is",
        "rewrite": "def plfit_lsq(x,y): \n    x = np.array(x)\n    y = np.array(y)\n    mask = np.logical_and(~np.isnan(y), ~np.isinf(y))\n    x = x[mask]\n    y = y[mask]\n    mask = np.logical_and(~np.isnan(x), ~np.isinf(x))\n    x = x[mask]\n    y = y[mask]"
    },
    {
        "original": "def light_bahdanau_attention(key, context, hidden_size, projected_align=False): context_length, context_size]\n        hidden_size: The size of the hidden layer\n        projected_align: If True, the attention mechanism will project the alignments to the hidden_size dimension\n    Returns:\n        A tensorflow tensor with dimensionality [None, None, hidden_size]\n    \"\"\"\n    # Calculate the attention weights\n    align = tf.matmul(key, tf.transpose(context, perm=[0, 2, 1]))\n    align = tf.nn.softmax(align, axis=2)\n    if projected_align",
        "rewrite": "== True:\n        align = tf.layers.dense(align, hidden_size, use_bias=False)\n    \n    # Calculate the context vector\n    context_vector = tf.matmul(align, context)\n    \n    return context_vector"
    },
    {
        "original": " :type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds\n        :type max_ingestion_time: int\n        :return: the created endpoint\n        :rtype: dict\n        \"\"\"\n        endpoint_name = config['EndpointConfigName']\n        endpoint_config_name = config['EndpointConfigName']\n        endpoint_config = self.describe_endpoint_config(endpoint_config_name)\n        if endpoint_config is None:\n            raise ValueError('EndpointConfig {} does not exist'.format(endpoint_",
        "rewrite": ":type check_interval: int\n        :param max_ingestion_time: the maximum ingestion time in seconds\n        :type max_ingestion_time: int\n        :return: the created endpoint\n        :rtype: dict\n        \"\"\"\n        endpoint_name = config['EndpointConfigName']\n        endpoint_config_name = config['EndpointConfigName']\n        endpoint_config = self.describe_endpoint_config(endpoint_config_name)\n        if endpoint_config is None:\n            raise ValueError('EndpointConfig {} does not exist'.format(endpoint_\")"
    },
    {
        "original": "def sysinfo(self): \n        return [\n            (\"version\", self.version),\n            (\"platform\", self.platform),\n            (\"machine\", self.machine),\n            (\"node\", self.node),\n            (\"release\", self.release),\n            (\"system\", self.system),\n            (\"version_info\", self.version_info),\n            (\"maxsize\", self.maxsize),\n            (\"maxunicode\",",
        "rewrite": "def sysinfo(self): \n        return [\n            (\"version\", self.version),\n            (\"platform\", self.platform),\n            (\"machine\", self.machine),\n            (\"node\", self.node),\n            (\"release\", self.release),\n            (\"system\", self.system),\n            (\"version_info\", self.version_info),\n            (\"maxsize\", self.maxsize),\n            (\"maxunicode\", self.maxunicode)\n        ]"
    },
    {
        "original": "def calc_correlation(self, x_analyte, y_analyte, window=15, filt=True, recalc=True): \n        recalc : bool\n            Whether or not to recalculate the filter.\n\n        Returns\n        -------\n        correlation : float\n            The correlation between the two analytes.\n        \"\"\"\n        if filt:\n            self.apply_filters()\n        if recalc:\n           ",
        "rewrite": "self.calc_filter()"
    },
    {
        "original": "def compress_dhist(dh): \n    dhist = dh[:]\n    dhist.sort()\n    dhist = [dhist[i] for i in range(len(dhist)) if i % 2 == 0]\n    dhist = [dhist[i] for i in range(len(dhist)) if i % 10 != 0]\n    dhist.sort()\n    return dhist\n\n\ndef get_dhist_from_file(filename):\n    \"\"\"Get a directory history from a file.\n\n    Return a list of",
        "rewrite": "def compress_dhist(dh): \n    dhist = dh[:]\n    dhist.sort()\n    dhist = [dhist[i] for i in range(len(dhist)) if i % 2 == 0]\n    dhist = [dhist[i] for i in range(len(dhist)) if i % 10 != 0]\n    dhist.sort()\n    return dhist\n\ndef get_dhist_from_file(filename):\n    with open(filename, 'r') as file:\n        dhist = file.readlines()\n    return dhist"
    },
    {
        "original": "def remove_empty(self, tag): \n        if tag.name in self.empty_tags:\n            return True\n        if tag.name in self.empty_tags_with_content:\n            if tag.text and tag.text.strip():\n                return False\n            if tag.tail and tag.tail.strip():\n                return False\n            if len(tag) > 0:\n          ",
        "rewrite": "def remove_empty(self, tag):\n    if tag.name in self.empty_tags:\n        return True\n    if tag.name in self.empty_tags_with_content:\n        if tag.text and tag.text.strip():\n            return False\n        if tag.tail and tag.tail.strip():\n            return False\n        if len(tag) > 0:\n            return False"
    },
    {
        "original": "def print_inplace(msg): \n    print('\\r' + msg, end='')\n\n\ndef print_progress(msg, total, current):\n    \"\"\"Prints a progress bar.\"\"\"\n    print_inplace('[{:<{}}] {:.0f}%'.format(\n        '=' * int(current * total / 100),\n        total,\n        current * 100 / total\n    ))\n\n\ndef print_error(msg):\n    \"\"\"Prints an error message.\"\"\"\n    print_inplace('Error: ' + msg)\n\n\ndef print",
        "rewrite": "def print_inplace(msg):\n    print('\\r' + msg, end='')\n\n\ndef print_progress(msg, total, current):\n    print_inplace('[{:<{}}] {:.0f}%'.format(\n        '=' * int((current / total) * 50),\n        50,\n        (current / total) * 100\n    ))\n\n\ndef print_error(msg):\n    print_inplace('Error: ' + msg)"
    },
    {
        "original": "def to_string(obj): \n    if isinstance(obj, str):\n        return obj\n    if isinstance(obj, bytes):\n        return obj.decode('utf-8')\n    if isinstance(obj, (list, tuple)):\n        return ', '.join(to_string(o) for o in obj)\n    if isinstance(obj, dict):\n        return ', '.join('%s=%s' % (k, to_string(v)) for k, v in obj.items())\n    if isinstance(obj, (int, float)):\n        return str(obj)",
        "rewrite": "def to_string(obj):\n    if isinstance(obj, str):\n        return obj\n    if isinstance(obj, bytes):\n        return obj.decode('utf-8')\n    if isinstance(obj, (list, tuple)):\n        return ', '.join(to_string(o) for o in obj)\n    if isinstance(obj, dict):\n        return ', '.join('%s=%s' % (k, to_string(v)) for k, v in obj.items())\n    if isinstance(obj, (int, float)):\n        return str(obj)"
    },
    {
        "original": "def off_scheme(self, year): \n        return self.schemes[year]['off']\n\n    def def_scheme(self, year):\n        \"\"\"Returns the name of the defensive scheme the team ran in the given\n        year.\n\n        :year: Int representing the season year.\n        :returns: A string representing the defensive scheme.\n        \"\"\"\n        return self.schemes[year]['def']\n\n    def off_scheme_desc(self, year):\n        \"\"\"Returns the description of the offensive scheme the team ran in the\n        given year.\n\n   ",
        "rewrite": "def off_scheme(self, year): \n        return self.schemes[year]['off']\n\n    def def_scheme(self, year):\n        return self.schemes[year]['def']\n\n    def off_scheme_desc(self, year):\n        return self.schemes[year]['desc']"
    },
    {
        "original": "def parse_frequencies(variant, transcripts): \n    frequencies = {}\n    for transcript in transcripts:\n        if transcript['transcript_id'] in variant.INFO.get('ANN', []):\n            frequencies[transcript['transcript_id']] = transcript['frequency']\n\n    if not frequencies:\n        for transcript in transcripts:\n            if transcript['transcript_id'] in variant.INFO.get('CSQ', []):\n                frequencies[transcript['transcript_id']] = transcript['frequency']\n\n    return frequencies\n\n\ndef parse",
        "rewrite": "def parse_frequencies(variant, transcripts):\n    frequencies = {}\n    for transcript in transcripts:\n        if transcript['transcript_id'] in variant.INFO.get('ANN', []):\n            frequencies[transcript['transcript_id']] = transcript['frequency']\n\n    if not frequencies:\n        for transcript in transcripts:\n            if transcript['transcript_id'] in variant.INFO.get('CSQ', []):\n                frequencies[transcript['transcript_id']] = transcript['frequency']\n\n    return frequencies"
    },
    {
        "original": "def force_unicode(raw): \n    try:\n        return BeautifulSoup.UnicodeDammit(raw).unicode_markup\n    except UnicodeDecodeError:\n        return raw.decode('utf-8', 'ignore')\n\n\ndef get_text(element):\n    \"\"\"Get the text of an element.\n\n    :param element: element to get text from\n    :type element: :class:`BeautifulSoup.Tag`\n    :return: text of `element`\n    :rtype: :class:`unicode`\n\n    \"\"\"\n    return force_unicode",
        "rewrite": "def force_unicode(raw):\n    try:\n        return str(BeautifulSoup.UnicodeDammit(raw).unicode_markup)\n    except UnicodeDecodeError:\n        return raw.decode('utf-8', 'ignore')\n\n\ndef get_text(element):\n    return force_unicode(element.text)"
    },
    {
        "original": "def import_app_sitetree_module(app): \n    try:\n        return import_module('{}.sitetree'.format(app))\n    except ImportError:\n        return None\n\n\ndef get_app_sitetree_module(app):\n    \"\"\"Gets sitetree module from a given app.\n\n    :param str|unicode app: Application name\n    :return: module|None\n    \"\"\"\n    try:\n        return import_app_sitetree_module(app)\n    except ImportError:\n        return None\n\n\ndef get_app_sitetree_models(",
        "rewrite": "def get_app_sitetree_models(app):\n    \"\"\"\n    Gets sitetree module from a given app.\n\n    :param str|unicode app: Application name\n    :return: module|None\n    \"\"\"\n    try:\n        return import_module('{}.sitetree'.format(app))\n    except ImportError:\n        return None"
    },
    {
        "original": "def map(self, func, value_shape=None, dtype=None): from the map\n\n        Returns\n        -------\n        mapped : Array\n            The result of applying the function to each subarray\n\n        Notes\n        -----\n        The function is applied to subarrays of the array. The output of the\n        function must be an array of the same shape as the input.\n\n        Examples\n        --------\n   ",
        "rewrite": "def map(self, func, value_shape=None, dtype=None):\n        \"\"\"\n        Returns\n        -------\n        mapped : Array\n            The result of applying the function to each subarray\n\n        Notes\n        -----\n        The function is applied to subarrays of the array. The output of the\n        function must be an array of the same shape as the input.\n\n        Examples\n        --------\n        \"\"\"\n        # Your code here\n        pass"
    },
    {
        "original": "def attach_to_tree(self): \n        for node in self.tree.traverse():\n            if node.is_leaf():\n                continue\n            else:\n                node.branch_length_interpolator.cost = self.cost_function(node)\n\n    def cost_function(self, node):\n        \"\"\"\n        calculates the cost of the merger of the two branches attached to the node.\n        \"\"\"\n        if node.is_leaf():\n   ",
        "rewrite": "def attach_to_tree(self):\n    for node in self.tree.traverse():\n        if node.is_leaf():\n            continue\n        else:\n            node.branch_length_interpolator.cost = self.cost_function(node)\n\ndef cost_function(self, node):\n    \"\"\"\n    calculates the cost of the merger of the two branches attached to the node.\n    \"\"\"\n    if node.is_leaf():\n        return 0  # or any other appropriate value for leaf nodes\n\n    # Add code for calculating cost for non-leaf nodes here."
    },
    {
        "original": "def format_location(proc_obj): \n    if proc_obj.location:\n        return proc_obj.location\n    else:\n        return proc_obj.file_name\n\n\ndef format_time(proc_obj):\n    \"\"\"Show when we started. GUI's and front-end interfaces often\n    use this to update displays. So it is helpful to make sure\n    we give at least some time that's located in a file.\n    \"\"\"\n    if proc_obj.time:\n        return proc_obj.time\n    else:\n        return proc_obj.file_name",
        "rewrite": "def format_location(proc_obj): \n    location = proc_obj.location if proc_obj.location else proc_obj.file_name\n    return location\n\ndef format_time(proc_obj):\n    if proc_obj.time:\n        return proc_obj.time\n    else:\n        return proc_obj.file_name"
    },
    {
        "original": "def observations(store, loqusdb, case_obj, variant_obj): \n    query = \"\"\"\n        SELECT\n            loqusdb_variant.variant_id,\n            loqusdb_variant.variant_id_str,\n            loqusdb_variant.chrom,\n            loqusdb_variant.pos,\n            loqusdb_variant.ref,\n            loqusdb_variant.alt,\n            loqusdb_variant.genotype,\n            loqusdb_variant.genotype_str,\n            loqusdb_variant.genotype_str_alt,\n",
        "rewrite": "def observations(store, loqusdb, case_obj, variant_obj):\n    query = \"\"\"\n        SELECT\n            loqusdb_variant.variant_id,\n            loqusdb_variant.variant_id_str,\n            loqusdb_variant.chrom,\n            loqusdb_variant.pos,\n            loqusdb_variant.ref,\n            loqusdb_variant.alt,\n            loqusdb_variant.genotype,\n            loqusdb_variant.genotype_str,\n            loqusdb_variant.genotype_str_alt,\n            \"\"\""
    },
    {
        "original": "def read_vlrs(self): \n        vlrs = []\n        while True:\n            try:\n                vlr = self.read_vlr()\n                vlrs.append(vlr)\n            except EOFError:\n                break\n        return vlrs\n\n    def read_vlr(self):\n        \"\"\" Reads and returns a single vlr\n   ",
        "rewrite": "def read_vlrs(self): \n        vlrs = []\n        while True:\n            try:\n                vlr = self.read_vlr()\n                vlrs.append(vlr)\n            except EOFError:\n                break\n        return vlrs\n\n    def read_vlr(self):\n        \"\"\" Reads and returns a single vlr \"\"\"\n        # Code for reading and returning a single vlr would go here"
    },
    {
        "original": "def process_alignment_dict(self): const\n            dictionary specifying the differences from the reference sequence\n            to construct the reduced alignment with variable sites only\n        \"\"\"\n        # get the reference sequence\n        ref_seq = self.alignment.reference_sequence\n        # get the alignment\n        alignment = self.alignment.alignment\n        # get the reference sequence\n        ref_seq = self.alignment.reference_sequence\n        # get the reference sequence\n ",
        "rewrite": "def process_alignment_dict(self):\n    \"\"\"\n    const dictionary specifying the differences from the reference sequence\n    to construct the reduced alignment with variable sites only\n    \"\"\"\n    \n    # get the reference sequence\n    ref_seq = self.alignment.reference_sequence\n    \n    # get the alignment\n    alignment = self.alignment.alignment\n    # get the reference sequence\n    ref_seq = self.alignment.reference_sequence"
    },
    {
        "original": " \n    return v.is_macro()\n\n\ndef _is_macro_or_var(v: Var) -> bool:\n    \"\"\"Return True if the Var holds a macro function or a variable.\"\"\"\n    return v.is_macro() or v.is_var()\n\n\ndef _is_var(v: Var) -> bool:\n    \"\"\"Return True if the Var holds a variable.\"\"\"\n    return v.is_var()\n\n\ndef _is_var_or_macro(v: Var) -> bool:\n    \"\"\"Return True if the Var",
        "rewrite": "def _is_var_or_macro(v: Var) -> bool:\n    \"\"\"Return True if the Var holds a variable or a macro function.\"\"\"\n    return v.is_var() or v.is_macro()"
    },
    {
        "original": "def _normalize_params(params): \n    if isinstance(params, dict):\n        params = params.items()\n    params = [(k, v) for k, v in params if k not in ('realm', 'oauth_signature')]\n    params.sort()\n    return urlencode(params)\n\n\ndef _normalize_url(url):\n    \"\"\"\n    Returns a normalized URL as specified here: http://oauth.net/core/1.0a/#rfc.section.9.1.2.\n\n    :param url:\n        :class:`str`",
        "rewrite": "def _normalize_url(url):\n    return url.strip()"
    },
    {
        "original": "def container_search(self, query, across_collections=False): \n    if across_collections:\n        return self.search(query)\n    else:\n        return self.search(query, show_details=False)\n\n\ndef container_search_across_collections(self, query):\n    \"\"\"search for a specific container across collections. If across\n    collections is False, the query is parsed as a full container name\n    and a specific container is returned. If across collections is True,\n    the container is searched for across collections. If across collections\n    is True, details are not shown\"\"\"\n    return self.search(query,",
        "rewrite": "def container_search(self, query, across_collections=False): \n    if across_collections:\n        return self.search(query)\n    else:\n        return self.search(query, show_details=False)\n\n\ndef container_search_across_collections(self, query):\n    return self.search(query)"
    },
    {
        "original": "def is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms): (PM1\u2013 PM6) OR\n      (iii) 1 Pathogenic (PP1\u2013PP3) AND 1\u20132 moderate (PM1\u2013 PM6)\n\n    Args:\n        pvs (list): List of pvs\n        ps_terms (list): List of ps terms\n        pm_terms (list): List of pm terms\n        pp_terms (list): List of pp terms\n\n    Returns:\n        bool: True if likely pathogenic, False otherwise\n    \"\"\"\n    if len(pvs) == 1 and",
        "rewrite": "def is_likely_pathogenic(pvs, ps_terms, pm_terms, pp_terms):\n    #Check for the presence of 1 pathogenic variant (PVS1)\n    if len(pvs) == 1:\n        return True\n    else:\n        return False"
    },
    {
        "original": "def parse_osm_file(f, parse_timestamps=True): \n    from .osm import OSM\n    from .osm import OSMNode\n    from .osm import OSMWay\n    from .osm import OSMRelation\n    from .osm import OSMRelationMember\n    from .osm import OSMRelationMemberRole\n    from .osm import OSMRelationMemberType\n    from .osm import OSMRelationMemberRef\n    from .osm import OSMRelationMemberRefRole\n    from .osm import OSMRelationMemberRefType\n    from .osm import OSM",
        "rewrite": "def parse_osm_file(f, parse_timestamps=True): \n    from .osm import OSM, OSMNode, OSMWay, OSMRelation, OSMRelationMember, OSMRelationMemberRole, OSMRelationMemberType, OSMRelationMemberRef, OSMRelationMemberRefRole, OSMRelationMemberRefType"
    },
    {
        "original": "def addsitedir(sitedir, known_paths=None): \n    if known_paths is None:\n        known_paths = []\n    sitedirs = [sitedir]\n    if sitedir not in known_paths:\n        for name in os.listdir(sitedir):\n            if name.endswith('.pth'):\n                sitedirs.append(os.path.join(sitedir, name))\n    for sitedir in sitedirs:\n        if sitedir not in sys.path:\n            sys.path.append(sitedir)\n\n\ndef _get_python_",
        "rewrite": "def addsitedir(sitedir, known_paths=None): \n    if known_paths is None:\n        known_paths = []\n    sitedirs = [sitedir]\n    if sitedir not in known_paths:\n        for name in os.listdir(sitedir):\n            if name.endswith('.pth'):\n                sitedirs.append(os.path.join(sitedir, name))\n    for sitedir in sitedirs:\n        if sitedir not in sys.path:\n            sys.path.append(sitedir)"
    },
    {
        "original": "def Bin(self): \n    if self.bin_time is None:\n        raise ValueError(\"Bin time array not set\")\n    if self.bin_time.shape[0] != self.time.shape[0]:\n        raise ValueError(\"Bin time array has different length than time array\")\n    if self.bin_time.shape[0] != self.flux.shape[0]:\n        raise ValueError(\"Bin time array has different length than flux array\")\n    if self.bin_time.shape[0] != self.flux_err.shape[0]:",
        "rewrite": "raise ValueError(\"Bin time array has different length than flux error array\")"
    },
    {
        "original": "def categories(self, limit=20, offset=0, country=None, locale=None): locale : Optional[str]\n            The desired language, consisting of a lowercase ISO 639 language code and an uppercase ISO 3166-1 alpha-2 country code, joined by an underscore.\n\n        Returns\n        -------\n        Categories\n            A list of categories used to tag items in Spotify.\n        \"\"\"\n        return self.http.get(\n            \"browse/categories\",\n           ",
        "rewrite": "def categories(self, limit=20, offset=0, country=None, locale=None):\n        return self.http.get(\"browse/categories\")"
    },
    {
        "original": "def cli_run(): \n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument('--input', type=str, help='input file')\n    parser.add_argument('--output', type=str, help='output file')\n    parser.add_argument('--mode', type=str, help='mode')\n    parser.add_argument('--model', type=str, help='model')\n    parser.add_argument('--batch_size', type=int, help='batch size')\n    parser.add_argument('--epoch",
        "rewrite": "def cli_run(): \n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument('--input', type=str, help='input file')\n    parser.add_argument('--output', type=str, help='output file')\n    parser.add_argument('--mode', type=str, help='mode')\n    parser.add_argument('--model', type=str, help='model')\n    parser.add_argument('--batch_size', type=int, help='batch size')\n    parser.add_argument('--epoch', type=int, help='epoch')"
    },
    {
        "original": " \n    if user:\n        raise NotImplementedError(\n            \"user installs are not supported by distutils\")\n    if home is None:\n        home = os.path.expanduser('~')\n    if root is None:\n        root = home\n    if isolated:\n        raise NotImplementedError(\n            \"isolated installs are not supported by distutils\")\n    return {'user': False,\n            'home': home,\n         ",
        "rewrite": "if user:\n    raise NotImplementedError(\"user installs are not supported by distutils\")\nif home is None:\n    home = os.path.expanduser('~')\nif root is None:\n    root = home\nif isolated:\n    raise NotImplementedError(\"isolated installs are not supported by distutils\")\nreturn {'user': False,\n        'home': home,\n        'root': root,\n        'isolated': isolated}"
    },
    {
        "original": "def send(self, message): Error: provider credit exhausted\n            :raises ProviderError: provider error\n        \"\"\"\n        if not isinstance(message, OutgoingMessage):\n            raise AssertionError('send() expects an OutgoingMessage object')\n        if not message.provider:\n            raise AssertionError('send() expects a provider name')\n        if message.provider != self.provider:\n            raise AssertionError('send() expects a message for provider %s' % self.provider)\n        if not message.to:\n ",
        "rewrite": "def send(self, message):\n    if not isinstance(message, OutgoingMessage):\n        raise AssertionError('send() expects an OutgoingMessage object')\n    if not message.provider:\n        raise AssertionError('send() expects a provider name')\n    if message.provider != self.provider:\n        raise AssertionError('send() expects a message for provider %s' % self.provider)\n    if not message.to:\n        raise AssertionError('send() expects a message recipient')"
    },
    {
        "original": " \n    params = {\n        'uid': uid,\n        'days_back': days_back,\n        'cumulative': cumulative,\n        'frequency': frequency,\n        'min_val': min_val,\n        'max_val': max_val,\n        'chart_type': chart_type,\n        'percentage': percentage,\n        'sort': sort,\n    }\n    return params\n\n\ndef get_gecko_params_from_request(request):\n    \"\"\"\n    Returns the default GET parameters for a particular Ge",
        "rewrite": "def get_gecko_params_from_request(request):\n    params = {\n        'uid': request.GET.get('uid', None),\n        'days_back': request.GET.get('days_back', None),\n        'cumulative': request.GET.get('cumulative', None),\n        'frequency': request.GET.get('frequency', None),\n        'min_val': request.GET.get('min_val', None),\n        'max_val': request.GET.get('max_val', None),\n        'chart_type': request.GET.get('chart_type', None),\n        'percentage': request.GET.get('percentage', None),\n        'sort': request.GET.get('sort', None),\n    }\n    return params"
    },
    {
        "original": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_3): \n        Raises:\n            ValueError: Raised if the data type of the value attribute is\n                unknown.\n        \"\"\"\n        local_buffer = utils.BytearrayStream()\n\n        if kmip_version == enums.KMIPVersion.KMIP_1_0:\n            self._write_kmip_1_0(local_buffer)\n        elif kmip_version == enums.KMIPVersion.KMIP_1_1:\n            self._write_kmip_1_1(local_buffer)",
        "rewrite": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_3):\n        \"\"\"\n        Raises:\n            ValueError: Raised if the data type of the value attribute is\n                unknown.\n        \"\"\"\n        local_buffer = utils.BytearrayStream()\n\n        if kmip_version == enums.KMIPVersion.KMIP_1_0:\n            self._write_kmip_1_0(local_buffer)\n        elif kmip_version == enums.KMIPVersion.KMIP_1_1:\n            self._write_kmip_1_1(local_buffer)"
    },
    {
        "original": "def fetch_items(self, category, **kwargs): \n        return self._fetch_pages(category, **kwargs)\n\n    def _fetch_pages(self, category, **kwargs):\n        \"\"\"Fetch the pages\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"\n        # Fetch the first page\n        page = self._fetch_page(category, **kwargs)\n\n        # Fetch the next pages\n        while page:\n      ",
        "rewrite": "def fetch_items(self, category, **kwargs): \n    return self._fetch_pages(category, **kwargs)\n\ndef _fetch_pages(self, category, **kwargs):\n    \"\"\"Fetch the pages\n\n    :param category: the category of items to fetch\n    :param kwargs: backend arguments\n\n    :returns: a generator of items\n    \"\"\"\n    # Fetch the first page\n    page = self._fetch_page(category, **kwargs)\n\n    # Fetch the next pages\n    while page:\n        yield page\n        page = self._fetch_page(category, **kwargs)"
    },
    {
        "original": "def parse_token_stream(stream, soft_delimiter, hard_delimiter): \n    # TODO: This is a bit of a hack, but it works for now.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a proper parser.\n    # TODO: This should be replaced by a",
        "rewrite": "def parse_token_stream(stream, soft_delimiter, hard_delimiter): \n    raise NotImplementedError(\"Proper parser not implemented yet\")"
    },
    {
        "original": "def load_exons(self, exons, genes=None, build='37'): \n        if genes is None:\n            genes = []\n        for exon in exons:\n            exon_id = exon['exon_id']\n            gene_id = exon['gene_id']\n            chrom = exon['chrom']\n            strand = exon['strand']\n            start = exon['start']\n            end = exon['end']\n  ",
        "rewrite": "def load_exons(self, exons, genes=None, build='37'): \n    if genes is None:\n        genes = []\n    for exon in exons:\n        exon_id = exon['exon_id']\n        gene_id = exon['gene_id']\n        chrom = exon['chrom']\n        strand = exon['strand']\n        start = exon['start']\n        end = exon['end']"
    },
    {
        "original": "def get_except_handler_name(self, node): \n        if node.type == 'except_handler':\n            if node.type_name:\n                return node.type_name.name\n            else:\n                return 'Exception'\n        elif node.type == 'except_handler_py3':\n            return node.type_name.name\n        else:\n            raise ValueError('Unknown node type: %s' % node.type)\n\n    def",
        "rewrite": "def get_except_handler_name(self, node):\n    if node.type == 'except_handler':\n        if node.type_name:\n            return node.type_name.name\n        else:\n            return 'Exception'\n    elif node.type == 'except_handler_py3':\n        return node.type_name.name\n    else:\n        raise ValueError('Unknown node type: %s' % node.type)"
    },
    {
        "original": "def name(self, src=None): \n        return self.type.name\n\n    def __str__(self):\n        \"\"\"Return string representing this type.\"\"\"\n        return self.name()\n\n    def __repr__(self):\n        \"\"\"Return string representing this type.\"\"\"\n        return self.name()\n\n    def __eq__(self, other):\n        \"\"\"Return True if this type is equal to other.\"\"\"\n        return self.type == other.type\n\n    def __ne__(self, other):\n        \"\"\"Return True if this type is not equal to other.\"\"\"\n       ",
        "rewrite": "def name(self, src=None):\n    return self.type.name\n\ndef __str__(self):\n    return self.name()\n\ndef __repr__(self):\n    return self.name()\n\ndef __eq__(self, other):\n    return self.type == other.type\n\ndef __ne__(self, other):\n    return not self == other"
    },
    {
        "original": "def get_data_node(self): \n        return self._data_node\n\n    def get_data_node_path(self):\n        \"\"\"Returns the path of the underlying data node.\n\n        In case one uses HDF5 this will be the HDF5 leaf node path.\n\n        \"\"\"\n        return self._data_node_path\n\n    def get_data_node_name(self):\n        \"\"\"Returns the name of the underlying data node.\n\n        In case one uses HDF5 this will be the HDF5 leaf node name.\n\n        \"\"\"\n        return self._data",
        "rewrite": "def get_data_node(self):\n    return self._data_node\n\ndef get_data_node_path(self):\n    return self._data_node_path\n\ndef get_data_node_name(self):\n    return self._data_node_name"
    },
    {
        "original": "def feed_forward(self, x, **kwargs): examples, num-variables)\n            A list of arrays containing the output of each layer.\n        \"\"\"\n        # Build the graph\n        layers = self.build_graph(x, **kwargs)\n        # Return the output of each layer\n        return layers\n\n    def build_graph(self, x, **kwargs):\n        \"\"\"Build the graph for the given input.\n\n        All keyword arguments are passed directly to :func:`feed_forward`.\n\n        Parameters\n    ",
        "rewrite": "def feed_forward(self, x, **kwargs):\n        \"\"\"\n        Feed forward the input through the network.\n\n        Parameters:\n        x (ndarray): Input data\n        **kwargs: Additional keyword arguments\n\n        Returns:\n        list: A list of arrays containing the output of each layer.\n        \"\"\"\n        # Build the graph\n        layers = self.build_graph(x, **kwargs)\n        # Return the output of each layer\n        return layers\n\ndef build_graph(self, x, **kwargs):\n    \"\"\"\n    Build the graph for the given input.\n\n    All keyword arguments are passed directly to feed_forward.\n\n    Parameters:\n    x (ndarray): Input data\n    **kwargs: Additional keyword arguments\n\n    Returns:\n    None\n    \"\"\"\n    # Code for building the graph goes here"
    },
    {
        "original": "def cp_files(self, source, target, delete_source=False): \n    # Check if source is a directory\n    if source.endswith('/'):\n      # If source is a directory, get all files in the directory\n      source_files = self.list_files(source)\n      # If there is no file in the directory, return\n      if not source_files:\n        return\n      # If there is only one file in the directory, copy the file\n      elif len(source_files) == 1:\n        self.cp_file(source_files[0], target)\n      # If there are multiple files in the directory,",
        "rewrite": "copy all files if target directory does not exist then create and copy all files in target directory, if delete_source is true then delete files from source directory\"."
    },
    {
        "original": " file is found while processing\n        the directory, it will behave much like a ``.gitignore``,\n        ignoring files that match any of the regex patterns specified\n        in the file.\n\n        :param dag_folder: a file path or a folder\n        :type dag_folder: str\n        :param only_if_updated: only collect dags if last DagBag collection\n            is older than current folder mtime, otherwise no-op\n        :type only_if_updated: bool\n        :param include_examples:",
        "rewrite": "\"\"\"\nfile is found while processing\nthe directory, it will behave much like a `.gitignore`,\nignoring files that match any of the regex patterns specified\nin the file.\n\n:param dag_folder: a file path or a folder\n:type dag_folder: str\n:param only_if_updated: only collect dags if last DagBag collection\nis older than current folder mtime, otherwise no-op\n:type only_if_updated: bool\n:param include_examples:\n\"\"\""
    },
    {
        "original": "def get(self, task_id): \n        try:\n            return self._tasks[task_id]\n        except KeyError:\n            raise NotFoundError(task_id)\n\n    def get_all(self):\n        \"\"\"Get all tasks from the registry.\n\n        Retrieve all tasks from the registry.\n\n        :returns: a list of task objects\n        \"\"\"\n        return list(self._tasks.values())\n\n    def get_by_name(self, name):\n        \"\"\"Get a task from the registry by",
        "rewrite": "def get(self, task_id):\n    try:\n        return self._tasks[task_id]\n    except KeyError:\n        raise NotFoundError(task_id)\n\ndef get_all(self):\n    return list(self._tasks.values())\n\ndef get_by_name(self, name):\n    return next((task for task in self._tasks.values() if task.name == name), None)"
    },
    {
        "original": "def generate_random_128bit_string():    \"\"\"\n    return generate_random_64bit_string() + generate_random_64bit_string()\n\n\ndef generate_random_64bit_string():\n    \"\"\"Returns a 64 bit UTF-8 encoded string.\n\n    :returns: 16-character hex string\n    \"\"\"\n    return hex(random.randint(0, 2 ** 64))[2:-1]\n\n\ndef generate_random_32bit_string():\n    \"\"\"Returns a 32 bit UTF-8 encoded string.",
        "rewrite": "def generate_random_128bit_string():\n    return generate_random_64bit_string() + generate_random_64bit_string()\n\n\ndef generate_random_64bit_string():\n    return hex(random.randint(0, 2 ** 64))[2:18]\n\n\ndef generate_random_32bit_string():\n    return hex(random.randint(0, 2 ** 32))[2:10]"
    },
    {
        "original": "def flags2text(self): \n        text = []\n        for flag in self.flags:\n            if flag == 0:\n                break\n            if flag & self.CKF_RW_SESSION:\n                text.append('CKF_RW_SESSION')\n            if flag & self.CKF_SERIAL_SESSION:\n                text.append('CKF_SERIAL_SESSION')\n           ",
        "rewrite": "def flags2text(self): \n        text = []\n        for flag in self.flags:\n            if flag == 0:\n                break\n            if flag & self.CKF_RW_SESSION:\n                text.append('CKF_RW_SESSION')\n            if flag & self.CKF_SERIAL_SESSION:\n                text.append('CKF_SERIAL_SESSION')"
    },
    {
        "original": "def sina_download_by_vid(vid, title=None, output_dir='.', merge=True, info_only=False): \n\n    # Get video info\n    html = get_html('http://video.sina.com.cn/v/v/' + vid)\n    title = match1(html, r'<meta property=\"og:title\" content=\"([^\"]+)\"')\n    if not title:\n        title = match1(html, r'<title>([^<]+)</title>')\n    title = unescape_html(title)\n    if not title:\n        title = 'Sina Video'\n    title = title.strip()\n    if not title",
        "rewrite": ": title = 'Sina Video'"
    },
    {
        "original": "def path_dispatch_kwarg(mname, path_default, returns_model): \n    def decorator(f):\n        @wraps(f)\n        def wrapper(self, path=path_default, **kwargs):\n            if path is None:\n                path = path_default\n            if path is not None:\n                path = self._check_path(path)\n            return f(self, path, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef path_dispatch_arg(mname, path_default, returns_model):\n",
        "rewrite": "from functools import wraps\n\ndef path_dispatch_kwarg(mname, path_default, returns_model): \n    def decorator(f):\n        @wraps(f)\n        def wrapper(self, path=path_default, **kwargs):\n            if path is None:\n                path = path_default\n            if path is not None:\n                path = self._check_path(path)\n            return f(self, path, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef path_dispatch_arg(mname, path_default, returns_model):\n    pass"
    },
    {
        "original": "def harmonic(y, **kwargs): librosa.decompose.hpss\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> y_harmonic = librosa.effects.harmonic(y)\n    \"\"\"\n\n    return librosa.decompose.hpss(y, **kwargs)[0]\n\n\ndef percussive(y, **kwargs):\n    \"\"\"Extract percussive elements from an audio time-series.\n\n    Parameters\n    ----------",
        "rewrite": "def percussive(y, **kwargs):\n    return librosa.decompose.hpss(y, **kwargs)[1]"
    },
    {
        "original": "def _setup_chans(self): \n        self.chans = np.zeros(self.nchan, dtype=np.int)\n        self.chans[0] = 0\n        for i in range(1, self.nchan):\n            self.chans[i] = self.chans[i - 1] + self.chans_per_beam[i - 1]\n\n    def _setup_beams(self):\n        \"\"\"Setup beam borders\n        \"\"\"\n        self.beams = np.zeros(self.nbeam, dtype=np.",
        "rewrite": "def _setup_chans(self): \n        self.chans = np.zeros(self.nchan, dtype=np.int)\n        self.chans[0] = 0\n        for i in range(1, self.nchan):\n            self.chans[i] = self.chans[i - 1] + self.chans_per_beam[i - 1]\n\n    def _setup_beams(self):\n        self.beams = np.zeros(self.nbeam, dtype=np.int)"
    },
    {
        "original": "def _chunk_to_long(self, chunk): \n        return int.from_bytes(chunk, 'big')\n\n    def _chunk_to_float(self, chunk):\n        \"\"\"\n        parses a chunk of bytes to float using big-endian representation\n        \"\"\"\n        return struct.unpack('>f', chunk)[0]\n\n    def _chunk_to_double(self, chunk):\n        \"\"\"\n        parses a chunk of bytes to float using big-endian representation\n        \"\"\"\n        return struct.unpack('>d', chunk)[0]\n\n    def _chunk_to",
        "rewrite": "_long(self, chunk):\n        return int.from_bytes(chunk, 'big')\n\n    def _chunk_to_float(self, chunk):\n        return struct.unpack('>f', chunk)[0]\n\n    def _chunk_to_double(self, chunk):\n        return struct.unpack('>d', chunk)[0]"
    },
    {
        "original": "def _deep_tuple(self, x): \n    if isinstance(x, (tuple, list)):\n        return tuple(map(self._deep_tuple, x))\n    elif isinstance(x, dict):\n        return tuple(map(self._deep_tuple, x.items()))\n    else:\n        return x\n\n\ndef _deep_list(self, x):\n    \"\"\"Converts nested `tuple`, `list`, or `dict` to nested `list`.\"\"\"\n    if isinstance(x, (tuple, list)):\n        return list(map(self._deep_list, x))",
        "rewrite": "def _deep_tuple(self, x): \n    if isinstance(x, (tuple, list)):\n        return tuple(map(lambda y: self._deep_tuple(y), x))\n    elif isinstance(x, dict):\n        return tuple(map(lambda item: self._deep_tuple(item), x.items()))\n    else:\n        return x\n\n\ndef _deep_list(self, x):\n    if isinstance(x, (tuple, list)):\n        return list(map(lambda y: self._deep_list(y), x))"
    },
    {
        "original": "def _get_volume(self, volume_id): \r\n        return self.get_volume(volume_id)\r\n\r\n    def _get_volumes(self):\r\n        \"\"\"Returns all volumes\"\"\" \r\n        return self.get_volumes()\r\n\r\n    def _get_volume_snapshot(self, volume_id, snapshot_id):\r\n        \"\"\"Returns a specific volume snapshot\"\"\" \r\n        return self.get_volume_snapshot(volume_id, snapshot_id)\r\n\r\n    def _get_volume_snapshots(self, volume_id):",
        "rewrite": "def _get_volume(self, volume_id):\n    return self.get_volume(volume_id)\n\ndef _get_volumes(self):\n    return self.get_volumes()\n\ndef _get_volume_snapshot(self, volume_id, snapshot_id):\n    return self.get_volume_snapshot(volume_id, snapshot_id)\n\ndef _get_volume_snapshots(self, volume_id):"
    },
    {
        "original": "def _get_url(self, ticker, frequency): \n        if frequency == 'daily':\n            return 'https://api.tiingo.com/tiingo/daily/{}/prices?startDate=2010-01-01&endDate=2019-01-01&resampleFreq=daily&token={}'.format(ticker, self.token)\n        elif frequency == 'weekly':\n            return 'https://api.tiingo.com/tiingo/daily/{}/prices?startDate=2010-01-01&end",
        "rewrite": "def _get_url(self, ticker, frequency):\n    if frequency == 'daily':\n        return 'https://api.tiingo.com/tiingo/daily/{}/prices?startDate=2010-01-01&endDate=2019-01-01&resampleFreq=daily&token={}'.format(ticker, self.token)\n    elif frequency == 'weekly':\n        return 'https://api.tiingo.com/tiingo/daily/{}/prices?startDate=2010-01-01&endDate=2019-01-01&resampleFreq=weekly&token={}'.format(ticker, self.token)"
    },
    {
        "original": "def create_url_adapter(self, request): \n        return self.url_map.bind_to_environ(request.environ,\n                                            server_name=request.host,\n                                            app_root=self.import_name)\n\n    def make_default_options_response(self):\n        \"\"\"Creates a default 200 response for the OPTIONS request.\"\"\"\n      ",
        "rewrite": "def create_url_adapter(self, request):\n        return self.url_map.bind_to_environ(request.environ,\n                                            server_name=request.host,\n                                            app_root=self.import_name)\n\n    def make_default_options_response(self):\n        return Response(status=200)"
    },
    {
        "original": "def import_file(self, path, **kwargs): \n        return DatasetImport(self, path, **kwargs)\n\n    def import_url(self, url, **kwargs):\n        \"\"\"\n        This is a shortcut to creating a DatasetImport. Can't use \"import()\"\n        because of Python.\n        \"\"\"\n        return DatasetImport(self, url, **kwargs)\n\n    def import_data(self, data, **kwargs):\n        \"\"\"\n        This is a shortcut to creating a DatasetImport. Can't use \"import()\"\n        because of Python.\n       ",
        "rewrite": "def import_file(self, path, **kwargs): \n        return DatasetImport(self, path, **kwargs)\n\ndef import_url(self, url, **kwargs):        \n        return DatasetImport(self, url, **kwargs)\n\ndef import_data(self, data, **kwargs):   \n        return DatasetImport(self, data, **kwargs)"
    },
    {
        "original": "def patterns(prefix, *args): \n    return [prefix + arg for arg in args]\n\n\ndef get_user_model():\n    \"\"\"As get_user_model() in django.\"\"\"\n    from django.contrib.auth import get_user_model\n    return get_user_model()\n\n\ndef get_user_model_name():\n    \"\"\"As get_user_model_name() in django.\"\"\"\n    return get_user_model()._meta.model_name\n\n\ndef get_user_model_verbose_name():\n    \"\"\"As get_user_model_ver",
        "rewrite": "def patterns(prefix, *args): \n    return [prefix + arg for arg in args]\n\n\ndef get_user_model():\n    from django.contrib.auth import get_user_model\n    return get_user_model()\n\n\ndef get_user_model_name():\n    return get_user_model()._meta.model_name\n\n\ndef get_user_model_verbose_name():\n    return get_user_model()._meta.verbose_name"
    },
    {
        "original": "def get_kvlayer_stream_ids_by_doc_id(client, doc_id):    \"\"\"\n    return client.get_stream_ids_by_doc_id(doc_id)\n\n\ndef get_kvlayer_stream_by_id(client, stream_id):\n    \"\"\"Retrieve stream from :mod:`kvlayer`.\n\n    :param client: kvlayer client object\n    :type client: :class:`kvlayer.AbstractStorage`\n    :param str stream_id: stream id of stream to retrieve\n    :return: stream object\n    :rtype: :class:`kvlayer.Stream`\n    \"\"\"\n    return client.",
        "rewrite": "def get_kvlayer_stream_ids_by_doc_id(client, doc_id):\n    return client.get_stream_ids_by_doc_id(doc_id)\n\n\ndef get_kvlayer_stream_by_id(client, stream_id):\n    return client.get_stream_by_id(stream_id)"
    }
]