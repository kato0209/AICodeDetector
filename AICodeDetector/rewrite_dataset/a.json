[
    {
        "original": "\nfrom kafka import KafkaConsumer\n\ndef beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {}\n    for p in partitions:\n        topic_partition = p.topic, p.partition\n        try:\n            offset = consumer.beginning_offset(topic_partition)\n            offsets[p] = offset\n        except Exception as e:\n            if isinstance(e, UnsupportedVersionError) or isinstance(e, KafkaTimeoutError):\n                raise e\n    return offsets\n",
        "rewrite": "\nfrom kafka import KafkaConsumer, UnsupportedVersionError, KafkaTimeoutError\n\ndef get_beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {}\n    for p in partitions:\n        topic_partition = p.topic, p.partition\n        try:\n            offset = consumer.beginning_offsets([topic_partition])[0].offset\n            offsets[p] = offset\n        except (UnsupportedVersionError, KafkaTimeoutError) as e:\n            raise e \n    return offsets"
    },
    {
        "original": "\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0] if type == 'new' else value[1] for key, value in diff_dict.items() if len(value) == 2}\n",
        "rewrite": "\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0] if type == 'new' else value[-1] for key, value in diff_dict.items() if len(value) > 0}\n"
    },
    {
        "original": "\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    \"\"\"\n    Resolve the field within the given state.\n    \"\"\"\n    obj = state[obj_alloc_id]\n    if not hasattr(obj, field_class_name):\n        raise AttributeError(f\"Object {obj_alloc_id} does not have class {field_class_name}\")\n    field_cls = getattr(obj, field_class_name)\n    if not hasattr(field_cls, field_name):\n        raise AttributeError(f\"",
        "rewrite": "\n\n\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name):\n    obj = state.get(obj_alloc_id)\n    if obj and hasattr(obj, field_class_name):\n        field_cls = getattr(obj, field_class_name)\n        if hasattr(field_cls, 'get') and callable(getattr(field_cls, 'get')):\n            return getattr(field_cls(), '_inst').__dict__[field_type][field_name]\n        elif hasattr(field_cls(), '_inst'):\n            return getattr(field_cls(), '_inst').__dict__.get(field_type).get[field_name]\n"
    },
    {
        "original": "\ndef pd_coords(self, comp):\n    n_elements = len(comp)\n    coords = [0] * (n_elements - 1)\n    sum_comp = sum(comp)\n    \n    for i in range(n_elements - 1):\n        coords[i] = comp[i] / sum_comp\n    \n    return coords\n",
        "rewrite": "\n\n\ndef pd_coords(self, comp):\n    return [(i / sum(comp)) for i in comp[:-1]]\n"
    },
    {
        "original": "\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    i = 0\n    while i < len(base_path) and i < len(target_path) and base_path[i] == target_path[i]:\n        i += 1\n    rel_ref = '../' * (len(base_path) - i) + '/'.join(target_path[i:])\n    return rel_ref if rel_ref else '.'\n",
        "rewrite": "\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    common_prefix = sum(a == b for a, b in zip(base_path, target_path))\n    rel_ref = '../' * (len(base_path) - common_prefix) + '/'.join(target_path[common_prefix:])\n    return rel_ref if rel_ref else '.'\n"
    },
    {
        "original": "\ndef is_enhanced_rr_cap_valid(self):\n    open_messages_sent = self.open_messages_sent\n    open_messages_received = self.open_messages_received\n\n    if open_messages_sent and open_messages_received:\n        return 'enhanced-route-refresh-capability' in open_messages_sent and 'enhanced-route-refresh-capability' in open_messages_received\n    else:\n        return False\n",
        "rewrite": "\ndef is_enhanced_rr_cap_valid(self):\n    return ('open_messages_sent' in self.__dict__ and 'open_messages_received' in self.__dict__ \n            and 'enhanced-route-refresh-capability' in self.open_messages_sent \n            and 'enhanced-route-refresh-capability' in self.open_media_sreceived)\n"
    },
    {
        "original": "\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = element\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags = {k: v for k, v in element.items() if k != 'nodes' and k !=",
        "rewrite": "\n\n\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = {'id': element['id'], 'lat': element['lat'], 'lon': element ['lon']}\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags ={k: v for k, v in.element.items() if k not in ['nodes', '.']}\n                ways_refs =["
    },
    {
        "original": "\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    # Get all possible permutations of mapping between species of two structures\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n\n    # Initialize minimum difference in electronegativity and best mapping\n    min_diff = float('inf')\n    best_mapping = None\n\n    # Iterate over",
        "rewrite": "\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n    min_diff = float('inf')\n    best_mapping = None\n    \n    for perm in perms:\n        diff = sum(abs(MoleculeClass.get_el_sp\ub152(species) - MoleculeClass.get_el_sp\u78a9(species)) for species, _ in zip(struct1.species, (struct2.species[i] for i in perm)))\n        if diff < min_diff"
    },
    {
        "original": "\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    pass\n\nclass HPackIndexedHdr(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithIncrIndexing(HPackHeaders):\n    pass\n\n\ndef _convert_a_header_to_a_h2_header(\n    hdr_name: str,\n    hdr_value: str,\n    is_sensitive: Callable[[str, str], bool",
        "rewrite": "\n\n\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    pass\n\n\nclass HPackIndexedHdr(HPackHeaders):\n    pass\n\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    pass\n\n\nclass HPackLitHdrWithIncrIndexing(HPackHeaders):\n    pass\n\n\ndef convert_header_to_h2_header(\n        hdr_name: str,\n        hdr_value: str,\n        is_sensitive: Callable[[str, str], bool]\n) -> Tuple[str, str]:\n    ...\n"
    },
    {
        "original": "\nclass BigchainTransactionCreator:\n    def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        from bigchaindb.common.transaction import Transaction\n        \n        # Initialize an empty transaction object \n        transaction = Transaction()\n\n        # Set transaction type to 'CREATE'\n        transaction.operation = 'CREATE'\n\n        # Add signers to transaction inputs \n        for signer in tx_signers:\n            transaction.add_input(signer)\n\n        # Add recipients and amounts to transaction outputs \n       ",
        "rewrite": "\n\n\nclass BigchainTransactionCreator:\n    def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        from bigchaindb.common.transaction import Transaction\n        \n        transaction = Transaction(operation='CREATE')\n        \n        for signer in tx_signers:\n            transaction.add_input(signer)\n        \n        for recipient, amount in recipients.items():\n            transaction.add_output(recipient, amount)\n        \n        if metadata:\n            transaction.metadata = metadata\n        if asset:\n            transaction.asset = asset\n        \n        return transaction\n"
    },
    {
        "original": "\nfrom datetime import datetime\nimport pytz\n\ndef utc_dt_to_local_dt(dtm):\n    utc_dt = datetime.strptime(dtm, '%Y-%m-%d %H:%M:%S')\n    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)\n    local_dt = utc_dt.astimezone()\n    return local_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
        "rewrite": "\nfrom datetime import datetime\nimport pytz\n\ndef utc_to_local(dtm, tz_str):\n    dt_obj = datetime.strptime(dtm, '%Y-%m-%d %H:%M:%S')\n    dt_obj = dt_obj.replace(tzinfo=pytz.UTC)\n    tz_obj = pytz.timezone(tz_str)\n    localDtObj = dt_obj.astimezone(tz_obj)\n    return localDtObj.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n\n# usage:\nprint(utc_to_local('2022-01-01 00:00:00',"
    },
    {
        "original": "\ndef _getScriptSettingsFrom IniFile(policy_info):\n    # Open and read the file\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    # Initialize variables\n    scripts = {}\n    current_script = None\n\n    # Parse lines\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            # Section header found, extract script name\n            start_br",
        "rewrite": "\n\n\ndef _getScriptSettingsFromFile(policy_info):\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    scripts = {}\n    current_script = None\n\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            start_bracket_index = 0\n            end_bracket_index = 0\n            for i, char in enumerate(line):\n                if char == '[':\n                    start_bracket_index += 1\n                elif char == ']':\n                    end_bracket_index +="
    },
    {
        "original": "\ndef _get_triplet_scores(self, triangles_list):\n    scores = {}\n    for triangle in triangles_list:\n        triangle_set = frozenset(triangle)\n        # assuming the score calculation is a placeholder, replace with actual logic\n        score = len(triangle) * 2.0\n        scores[triangle_set] = score\n    return scores\n",
        "rewrite": "\ndef _get_triplet_scores(self, triangles_list):\n    return {frozenset(triangle): len(triangle) * 2.0 for triangle in triangles_list}\n"
    },
    {
        "original": "\nasync def vcx_messages_update_status(msg_json: str):\n    import json\n    import asyncio\n    from aiohttp import ClientSession\n\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n\n        for message in messages:\n            url = f'https://example.com/update_message_status/{connection_id}'\n            data = {'message_id': message['uid'], 'status",
        "rewrite": "\n\n\nasync def vcx_messages_update_status(msg_json: str):\n    import json\n    from aiohttp import ClientSession\n\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n\n        for message in messages:\n            url = f'https://example.com/update_message_status/{connection_id}'\n            data = {'message_id': message['uid'], 'status': 'updated'}\n            async with session.post(url, json=data) as response:\n                await response.json()\n"
    },
    {
        "original": "\nimport socket\nimport time\n\nclass IsoTPSocket:\n    def __init__(self):\n        self.receive_buffer = b''\n\n    def recv_with_timeout(self, timeout=1):\n        start_time = time.time()\n        while True:\n            if self.receive_buffer:\n                return self.receive_buffer.pop(0)\n            elif time.time() - start_time > timeout:\n                return None\n",
        "rewrite": "\nimport socket\nimport select\nimport queue\n\nclass IsoTPSocket(socket.socket):\n    def __init__(self, *connection):\n        super().__init__(socket.AF_INET, socket.SOCK_STREAM)\n        self.connect(connection)\n        self.receive_buffer = queue.Queue()\n\n    def recv_with_timeout(self, timeout=1):\n        readable, _, _ = select.select([self], [], [], timeout)\n        if readable:\n            data = self.recv(4096)\n            for byte in data:\n                self.receive_buffer.put(byte)\n            return data\n        else:\n            return None\n"
    },
    {
        "original": "\nimport math\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, (p-1) * (p**(e-1)))\n    return lambda_val\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a, b)\n",
        "rewrite": "\n\n\nimport math\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, euler_totient(p,e))\n    return lambda_val\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a,b)\n\ndef euler_totient(n,e):\n    phi_n=math.pow(n-1,e)\n    phi_n*=n-1\nreturn int(phi_n)\n"
    },
    {
        "original": "\nimport requests\n\ndef absent(name, profile=\"splunk\"):\n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    response = requests.delete(url, auth=(\"username\", \"password\"))\n    if response.status_code == 200:\n        return True\n    else:\n        return False\n",
        "rewrite": "\nimport requests\n\ndef delete_job(name, profile=\"splunk\", username=\"your_username\", password=\"your_password\"):\n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    response = requests.delete(url, auth=(username, password))\n    return response.status_code == 200\n"
    },
    {
        "original": "\ndef GetNotificationsForAllShards(self, queue):\n    notifications = []\n    for shard in self.GetShardNames(queue):\n        notifications.extend(self.GetNotificationsForShard(queue, shard))\n    return notifications\n",
        "rewrite": "\ndef GetNotificationsForAllShards(self, queue):\n    return [n for shard in self.GetShardNames(queue) for n in self.GetNotificationsForShard(queue, shard)]\n"
    },
    {
        "original": "\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n\n    def differing_functions_with_consts(self):\n        matches = []\n        for i in range(len(self.functions)):\n            for j in range(i + 1, len(self.functions)):\n                x = symbols('x')\n                eq1 = Eq(eval(self.functions[i]), 0)\n                eq2 = Eq(eval(self.functions[j]), 0)\n               ",
        "rewrite": "Here is the revised code with some improvements and best practices:\n\n\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n\n    def differing_functions_with_consts(self):\n        matches = []\n        x = symbols('x')  # declare x as a symbol only once\n        for i in range(len(self-functions)):\n            for j in range(i + 1, len(self-functions)):\n                eq1 = Eq(eval(self.functions[i]), 0)\n                eq2 = Eq(eval(self-functions[j]), 0)\n                solution_set = solve(("
    },
    {
        "original": "\nclass EOPatchSaver:\n    def save(self, eopatch, use_tmp=True):\n        if use_tmp:\n            # Save to temporary file\n            with open(\"temp.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n        else:\n            # Save to intended location\n            with open(\"eopatch.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n",
        "rewrite": "\nimport os\nimport pickle\n\nclass EOPatchSaver:\n    def __init__(self, tmp_dir, final_dir):\n        self.tmp_dir = tmp_dir\n        self.final_dir = final_dir\n\n    def save(self, eopatch, filename, use_tmp=True):\n        if use_tmp:\n            filepath = os.path.join(self.tmp_dir, f\"{filename}.eopatch\")\n        else:\n            filepath = os.path.join(self.final_dir, f\"{filename}.eopatch\")\n        \n        with open(filepath, \"wb\") as f:\n            pickle.dump(eopatch, f)\n"
    },
    {
        "original": "\nclass StateTrie:\n    def __init__(self):\n        self.trie = {}\n\n    def _addAttr(self, txn, isCommitted=False) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.tr",
        "rewrite": "\n\n\nclass StateTrie:\n    def __init__(self):\n        self.trie = {}\n\n    def _addAttr(self, txn) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.trie[key] = hash(data)\n"
    },
    {
        "original": "\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ET\n\ndef _find_packages(root):\n    for subdir, dirs, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                zf = zipfile.ZipFile(os.path.join(subdir, file))\n                pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                yield (pkg_xml, zf, os.path.relpath(subdir, root))\n",
        "rewrite": "\nimport os\nimport zipfile\nfrom xml.etree import ElementTree as ET\n\ndef find_packages(root: str) -> iter:\n    for subdir, dirs, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                with zipfile.ZipFile(os.path.join(subdir, file)) as zf:\n                    pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                    yield pkg_xml, zf, os.path.relpath(subdir, root)\n"
    },
    {
        "original": "\nimport bluetooth\n\ndef srbt1(bt_address, pkts, *_args={}, **_kargs={}):\n    sock = bluetoothAMP.get_socket()\n    sock.connect((bt_address, 1))\n    sock.send(pkts)\n    data = sock.recv(1024)\n    return data\n",
        "rewrite": "\nimport bluetooth\n\ndef srbt1(bt_address: str, packets: bytes, *_args={}) -> bytes:\n    with bluetooth.discover_devices(lookup_names=True)[0] as device:\n        port = device.services[0][2]\n        socket = bluetooth.Socket(device IServiceAvailable=False)\n        socket.connect((bt_address, port))\n        socket.send(packets)\n        data = socket.recv(1024)\n        return data\n"
    },
    {
        "original": "\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def GetIPAddresses(self):\n        ip_array = []\n        for interface in netifaces.interfaces():\n            if interface == 'lo':  # ignore loopback interface\n                continue\n            lst = netifaces.ifaddresses(interface)\n            teste = {}\n            for item in lst:\n                if(item == 2): # AF_INET \n                    for thing in lst[item]:\n                        teste={\n                            \"iname\":interface,\n                           ",
        "rewrite": "\n\n\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def get_ip_addresses(self):\n        ip_array = []\n        for interface in netifaces.interfaces():\n            if interface == 'lo':\n                continue\n            addresses = netifaces.ifaddresses(interface)\n            for family, addr in [(socket.AF_INET, 'ipv4'), (socket.AF_INET6, 'ipv6')]:\n                try:\n                    addr_info = addresses[family][0]\n                    ip_array.append({\n                        'iname': interface,\n                        'ip': addr_info['addr'],\n                        'family': family,\n                        '"
    },
    {
        "original": "\ndef operate(self, point):\n    # Assuming operate function is defined elsewhere\n    pass\n\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    \"\"\"\n    Checks if two points are symmetrically related.\n\n    Args:\n        point_a (3x1 array): First point.\n        point_b (3x1 array): Second point.\n        tol (float): Absolute tolerance for checking distance.\n\n    Returns:\n        True if self.operate(point",
        "rewrite": "\n\n\ndef operate(self, point):\n    raise NotImplementedError(\"Subclass must implement this method\")\n\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    return False \n"
    },
    {
        "original": "\ndef validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    parts = ip.split(\".\")\n    if len(parts) != 4:\n        return False\n    for part in parts:\n        if not part.isdigit():\n            return False\n        i = int(part)\n        if i < 0 or i > 255:\n            return False\n    return True\n",
        "rewrite": "\ndef validate_rpc_host(ip):\n    parts = ip.split(\".\")\n    if len(parts) != 4:\n        return False\n    for part in parts:\n        if not part.isdigit() or not 0 <= int(part) <= 255:\n            return False\n    return True"
    },
    {
        "original": "\nimport subprocess\nimport logging\n\ndef find_available_interfaces():\n    \"\"\"\n    Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n    \n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-",
        "rewrite": "\n\n\nimport subprocess\n\ndef find_available_interfaces():\n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-8\").splitlines()\n        interfaces = [line.split(\": \")[1] for line in lines if \"-can\" in line.lower()]\n        return interfaces\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n        return []\n"
    },
    {
        "original": "\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {}\n\n    def save_session(self, sid, session, namespace=None):\n        if namespace is None:\n            namespace = self.namespace\n        if namespace not in self.sessions:\n            self.sessions[namespace] = {}\n        self.sessions[namespace][sid] = session\n",
        "rewrite": "\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {} if namespace is None else {namespace: {}}\n\n    def save_session(self, sid, session):\n        namespace = self.namespace\n        if not hasattr(self.sessions,(namespace)):\n            setattr(self.sessions,namespace,{})\n        getattr(self.sessions,namespace)[sid] = session \n"
    },
    {
        "original": "\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    public_keys = []\n    if self.v3_signature_block:\n        for cert in self.v3_signature_block.certs:\n            pub_key = cert.public_key()\n            der_pub_key = pub_key.public_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n            )\n            public_keys.append(der",
        "rewrite": "\n\n\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    public_keys = []\n    if self.v3_signature_block:\n        for cert in self.v3_signature_block.certs:\n            pub_key = cert.public_key()\n            der_pub_key = pub_key.public_bytes(encoding=serialization.Encoding.DER, \n                                               format=serialization.PublicFormat.SubjectPublicKeyInfo)\n            public_keys.append(der_pub_key)\n    return public_keys\n"
    },
    {
        "original": "\ndef chemical_symbols(atom_species, symbol_length):\n    symbols = []\n    ascii_offset = 97  # ASCII value of 'a'\n    for i in range(atom_species):\n        symbol = \"\"\n        for j in range(symbol_length):\n            symbol += chr(ascii_offset + ((i + j) % 26))\n        symbols.append(symbol)\n    return symbols\n",
        "rewrite": "\n\ndef chemical_symbols(atom_species, symbol_length):\n    return [''.join(chr(97 + ((i + j) % 26) for j in range(symbol_length)) for i in range(atom_species)]\n"
    },
    {
        "original": "\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    if stream is None:\n        return yaml.safe_dump_all(documents, default_flow_style=True, **kwds)\n    else:\n        yaml.safe_dump_all(documents, stream=stream, default_flow_style=True, **kwds)\n",
        "rewrite": "\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **(**kwds):\n    yaml.safe_dump_all(documents, stream=stream or None, default_flow_style=True, **kwds)\n"
    },
    {
        "original": "\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit,\n                        param_resolver,\n                        qubit_order,\n                        initial_state: Union[int, np.ndarray]) -> Iterator:\n    # Check if initial_state is an integer \n    if isinstance(initial_state, int):\n      # Set initial_state to computational basis corresponding to this integer \n      pass\n  \n    # Check if initial_state is a numpy array  \n    elif isinstance",
        "rewrite": "\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit, \n                         param_resolver, \n                         qubit_order, \n                         initial_state: Union[int, np.ndarray]) -> Iterator:\n    if isinstance(initial_state, int):\n        num_qubits = len(qubit_order)\n        initial_state = np.zeros(2 ** num_qubits)\n        initial_state[initial_state] = 1\n    \n    elif isinstance(initial_state, np.ndarray):\n        pass  # Assuming it's a valid state vector \n\n"
    },
    {
        "original": "\ndef predictive_variance(self, mu, variance, predictive_mean=None, Y_metadata=None):\n    if predictive_mean is None:\n      # If no predictive mean is provided assume it's 0\n      predictive_mean = 0  \n    expectation_squared = (mu - predictive_mean) ** 2 \n    variance_squared = variance ** 2 \n    return expectation_squared + variance_squared\n",
        "rewrite": "\ndef predictive_variance(self, mu, variance, predictive_mean=0, Y_metadata=None):\n    return (mu - predictive_mean) ** 2 + variance ** 2"
    },
    {
        "original": "\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id):\n        if id in self.configs:\n            del self.configs[id]\n            return True\n        else:\n            raise docker.errors.NotFound(\"No config with that ID exists\")\n",
        "rewrite": "\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id: str) -> bool:\n        if id in self.configs:\n            del self.configs[id]\n            return True\n        raise docker.errors.NotFound(\"No config with that ID exists\")\n"
    },
    {
        "original": "\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    \n    \nimport pyVmomi\n    \ndef get_service_instance():\n",
        "rewrite": "\n\n\nimport pyVmomi\n\ndef get_service_instance():\n    return pyVmomi.VMwareSoapApi\u0660((service=\"https://localhost/sdk\"))\n\ndef get_mo_by_moid(si, obj_type, obj_moid):\n    if not si:\n        si = get_service_instance()\n        \n    mo_ref = si.content.searchIndex.FindByObjectUUID(obj_moid, True)\n    \n    if not mo_ref:\n        raise Exception(f\"Object {obj_type} with MOID {obj_moid} not found\")\n        \n    return mo_ref[0]\n"
    },
    {
        "original": "\ndef ConfigureUrls(config, external_hostname = None):\n    if external_hostname is None:\n        external_hostname = input(\"Enter the external hostname: \")\n    config[\"AdminUI.url\"] = f\"http://{external_hostname}:8000\"\n    config[\"Client.frontend_url\"] = f\"http://{external_hostname}:8080\"\n    config[\"ClientPoll.url\"] = f\"http://{external_hostname}:8081\"\n    return config\n",
        "rewrite": "\ndef configure_urls(config, external_hostname=None):\n    external_hostname = external_hostname or input(\"Enter the external hostname: \")\n    config[\"AdminUI.url\"] = f\"http://{external.hostname}:8000\"\n    config[\"Client.frontend_url\"] = f\"http://{external.hostname}:8080\"\n    config[\"ClientPoll.url\"] = f\"http://{external.hostname}:8081\"\n    return config\n"
    },
    {
        "original": "\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def Parse(self, cmd, args, stdout, stderr, return_val, time_taken):\n        self.knowledge_base[cmd] = {\n            'args': args,\n            'stdout': stdout,\n            'stderr': stderr,\n            'return_val': return_val,\n            'time_taken': time_taken\n        }\n",
        "rewrite": "\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def parse(self, cmd: str, *: list[str], stdout: str, stderr: str, return_val: int, time_taken: float) -> None:\n        self.knowledge_base[cmd] = {'args': argscopy(args), 'stdout': stdout, 'stderr': stderr, 'return_val': return_val, 'time_taken': time_taken}\n\ndef argscopy(args: list[str]) -> tuple[str]:\n    return tuple(arg for arg in args)\n"
    },
    {
        "original": "\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as f:\n        content = f.read()\n        # parse ASF content into skeleton structure\n        # TO DO: implement parsing logic\n        pass\n",
        "rewrite": "\nimport xml.etree.ElementTree as ET\n\ndef load_skel(self, file_name):\n    tree = ET.parse(file_name)\n    root = tree.getroot()\n    self.skeleton = {}\n    for child in root:\n        if child.tag == 'Vertex':\n            self.skeleton[child.attrib['id']] = {\n                'x': float(child.attrib['x']),\n                'y': float(child.attrib['y']),\n                'z': float(child.attrib['z'])\n            }\n        elif child.tag == 'Segment':\n            self.skeleton[child.attrib['id']] = {\n                'type': childattrib['type'],\n                '"
    },
    {
        "original": "\ndef _ruby_installed(ret, ruby, user=None):\n    if user is not None:\n        cmd = f\"su {user} -c 'ruby -v | grep {ruby}'\"\n    else:\n        cmd = f\"ruby -v | grep {ruby}\"\n    ret[\"installed\"] = True if os.system(cmd) == 0 else False\n",
        "rewrite": "\nimport subprocess\n\ndef _ruby_installed(ret, ruby, user=None):\n    if user:\n        cmd = [\"su\", \"-c\", f\"ruby -v | grep {ruby}\"]\n        subprocess.run([\"su\", user, *\"-c\"] + cmd)\n    else:\n        cmd = [\"ruby\", \"-v\"]\n        out = subprocess.check_output(cmd)\n        ret[\"installed\"] = b\"{} {}\".format(ruby, ruby) in out\n"
    },
    {
        "original": "\nclass Structure:\n    # assuming Structure class is defined somewhere\n    pass\n\nclass Element:\n    # assuming Element class is defined somewhere\n    pass\n\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        A dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    \n    projection = {}\n    \n    # iterate over",
        "rewrite": "Here's a revised version of the code:\n\n\nclass ProjectionGetter:\n    def get_projection_on_elements(self, structure):\n        projection = {}\n        for spin in [Spin.up, Spin.down]:\n            projection[spin] = {}\n            for k_index in range(structure.n_kpoints):\n                projection[spin][k_index] = {}\n                for b_index in range(structure.n_bands):\n                    element_projections = {}\n                    for element in structure.elements_at_kpoint(k_index):\n                        element_projections[element] = self.calculate_projection(spin, k_index, b_index, element)\n                    projection[spin][k_index"
    },
    {
        "original": "\nclass VectorArgs:\n    def __init__(self, *args):\n        self.args = args\n\n    def vector_args(self):\n        lanes = self.args.split(',')\n        lane_pairs = [lane.split('..') for lane in lanes]\n        lane_pairs.sort(key=lambda x: int(x[1]), reverse=True)\n        return [(int(pair[0]), int(pair[1])) for pair in lane_pairs]\n\n# Example usage:\nvector_args_instance = VectorArgs('0..10,",
        "rewrite": "\n\n\nclass VectorArgs:\n    def __init__(self, args_string):\n        self.args_string = args_string\n\n    def parse_vector_args(self):\n        lanes = [lane.strip() for lane in self.args_string.replace('\"', '').replace('(', '').replace(')', '').split(',')]\n        lane_pairs = [lane.split('..') for lane in lanes]\n        lane_pairs.sort(key=lambda x: int(x[1]), reverse=True)\n        return [(int(pair[0]), int(pair[1])) for pair in lane_pairs]\n\n# Example usage:\nvector_args_instance = VectorArgs('0.."
    },
    {
        "original": "\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key, value in list(from_kwargs.items()):\n        if key.startswith(keyword + '_'):\n            to_kwargs[key.replace(keyword + '_', '')] = value\n            if clean_origin:\n                del from_kwargs[key]\n    return to_kwargs\n",
        "rewrite": "\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key in list(from_kwargs.keys()):\n        if key.startswith(f\"{keyword}_\"):\n            new_key = key.replace(f\"{keyword}_\", \"\")\n            to_kwargs[new_key] = from.kwargs.pop(key) if clean_origin else from_kwargs[key]\n    return to.kwargs \n"
    },
    {
        "original": "\nclass ActionExecutor:\n    def __init__(self):\n        self.actions = {}\n\n    def register_action(self, action_name, func):\n        self.actions[action_name] = func\n\n    def _RunAction(self, rule, client_id):\n        count = 0\n        for action in rule['actions']:\n            if action['name'] in self.actions:\n                self.actions[action['name']](client_id)\n                count += 1\n        return count\n",
        "rewrite": "\n\n\nclass ActionExecutor:\n    def __init__(self):\n        self._actions = {}\n\n    def register_action(self, action_name: str, func):\n        self._actions[action_name] = func\n\n    def run_actions(self, rule: dict, client_id) -> int:\n        count = 0\n        for action in rule.get('actions', []):\n            if 'name' in action and action['name'] in self._actions:\n                self._actions[action['name']](client_id)\n                count += 1\n        return count\n"
    },
    {
        "original": "\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def df(self):\n        try:\n            info = self.client.info()\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': len(self.client.volumes.list()),\n                'Networks': len(self.client.networks.list())\n            }\n        except docker.errors.APIError as e:\n            raise e\n",
        "rewrite": "\n\n\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def df(self):\n        try:\n            info = self.client.info()\n            volumes_count = len(self.client.volumes.list())\n            networks_count = len(self.client.networks.list())\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': volumes_count,\n                'Networks': networks_count\n            }\n        except docker.errors.APIError as e:\n            raise\n"
    },
    {
        "original": "\ndef file_extension(category=None):\n    extensions = {\n        'audio': ['mp3'],\n        'image': ['jpg', 'jpeg', 'png', 'gif'],\n        'office': ['docx', 'pdf', 'pptx'],\n        'text': ['txt', 'doc'],\n        'video': ['mp4']\n    }\n    if category:\n        return extensions.get(category.lower(), [])\n    else:\n        return []\n",
        "rewrite": "\ndef file_extension(category=None):\n    extensions = {\n            \"audio\": [\"mp3\", \"wav\"],\n            \"image\": [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"tiff\"],\n            \"office\": [\"docx\", \"docm\", \"pdf\", \"pptx\", \"pptm\"],\n            \"text\": [\"txt\", \"doc\", Splash\n                    ],\n            video\":[\"mp4\",\"avi\",\"mkv\"]\n    }\n    return extensions.get(category.lower(), []) if category else []\n"
    },
    {
        "original": "\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    import numpy as np\n    \n    # Calculate air mass\n    am = pressure / (101325 * np.exp(-0.000121 * surface_tilt))\n    \n    # Calculate relative air mass\n    mam = am /",
        "rewrite": "\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    import numpy as np\n    \n    am = pressure / (101325 * np.exp(-0.000121 * surface_tilt))\n    mam = am / 1.00271 - 0.014 * (1 - albedo) ** 1.261 \n"
    },
    {
        "original": "\nclass RDFURN:\n    def __init__(self, value):\n        self.value = value\n\nclass Client:\n    def ListChildren(self, urn, limit=None, age=\"NEWEST_TIME\"):\n        # assuming we have a function get_children that returns all children\n        all_children = self.get_children(urn)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children = all_children\n        elif age == \"NEWEST_TIME\":\n            filtered_children = [all_children[0]]",
        "rewrite": "\n\n\nclass RDFURN:\n    def __init__(self, value):\n        self.value = value\n\nclass Client:\n    def __init__(self):\n        pass\n    \n    def get_children(self, urn):\n        # implement this method to return all children\n        pass\n\n    def ListChildren(self, urn: RDFURN, limit=None, age=\"NEWEST_TIME\"):\n        all_children = self.get_children(urn.value)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children = all_children\n        elif age == \"NEWEST_TIME\":\n            filtered.Children = [all.children[0]]\n        \n"
    },
    {
        "original": "\ndef _convert_validators_to_mapping(validators):\n    validator_mapping = {}\n    for validator in validators:\n        key = (str(validator[\"check\"]), validator[\"comparator\"])\n        validator_mapping[key] = validator\n    return validator_mapping\n",
        "rewrite": "\ndef convert_validators_to_mapping(validators):\n    return {(str(v['check']), v['comparator']): v for v in validators}\n"
    },
    {
        "original": "\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n    if users is None:\n        user_path = knowledge_base.interpolate_string(path, **path_args)\n        return user_path\n    else:\n        result = []\n        for user in users:\n            user_path_args = path_args.copy()\n            user_path_args['user'] = user\n            user_path = knowledge_base.interpolate_string(path",
        "rewrite": "\n\n\ndef interpolate_path(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n    if users is None:\n        return knowledge_base.interpolate_string(path, path_args)\n    else:\n        result = []\n        for user in users:\n            user_path_args = {** + dict(user=user) | (path_args or {})\n            result.append(knowledge_base.interpolate_string(path, user_path_ARGS))\n        return result\n"
    },
    {
        "original": "\nimport numpy as np\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \n     from scipy.stats import gaussian_kde\n    \n     x = np.array(x)\n     if xmin is None:\n         xmin = np.min(x)\n     if xmax is None:\n         xmax = np.max(x)\n         \n     kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method='silverman')\n     xi = np.linspace(xmin,xmax",
        "rewrite": "\n\n\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    x = np.array(x)\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n    kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method='silverman')\n    xi = np.linspace(xmin, xmax, 400)\n    if cumulative:\n        yi = kde.integrate_box_1d(xmin, xi)\n    else"
    },
    {
        "original": "\ndef md_options_to_metadata(options):\n    metadata = {}\n    language = None\n    for option in options:\n        if option.startswith('language:'):\n            language = option.split('language:')[1].strip()\n        elif '=' in option:\n            key, value = option.split('=', 1)\n            metadata[key.strip()] = value.strip()\n    return language, metadata\n",
        "rewrite": "\ndef md_options_to_metadata(options):\n    metadata = {}\n    for option in options:\n        if '=' in option:\n            key, value = map(str.strip, option.split('=', 1))\n            metadata[key] = value\n        elif 'language:' in option:\n            return {'language': option[9:].strip()}, metadata\n    return {'language': None}, metadata\n"
    },
    {
        "original": "\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\nclass Trace:\n    def __init__(self):\n        self.frames = []\n\n    def add_frame(self,",
        "rewrite": "\n\n\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\nclass Trace:\n    def __init__(self):\n        self.frames : List[TraceFrame] = []\n\n    def add_frame(self, frame: TraceFrame) -> None:\n        self.frames.append(frame)\n"
    },
    {
        "original": "\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n        batch_Y = self.Y[start:end]\n        \n        self.index += self.batch_size",
        "rewrite": "\n\n\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n\t\tbatch_Y=\tself.Y[start:end]\n\t\t\n\t\tself.index=((self.index+batch_size) % len(X)) if (self.index+batch_size) >= len(X) else ("
    },
    {
        "original": "\ndef createSummary(self, log):\n    \"\"\"\n    Create nice summary logs.\n\n    @param log: log to create summary off of.\n    \"\"\"\n    result = {}\n    for entry in log:\n        key = entry['type']\n        if key not in result:\n            result[key] = 1\n        else:\n            result[key] += 1\n    return result\n",
        "rewrite": "\ndef create_summary(self, log):\n    return {entry['type']: count for count, _ in enumerate(dict.fromkeys(set(entry['type'] for entry in log), range(len(log))).items())}\n"
    },
    {
        "original": "\nimport requests\n\nclass GitRepository:\n    def __init__(self):\n        self.url = \"https://api.bitbucket.org/2.0\"\n    \n    def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        auth_token = (\"your_username\", \"your_password\")\n        \n\t\theaders = {\n\t\t\t\"Content-Type\": \"application/json\"\n\t\t}\n\t\t\n\t\tdata = {\n\t\t\t\"name\": name,\n\t\t\t\"start\": {\n\t\t\t",
        "rewrite": "\n\nimport requests\n\nclass GitRepository:\n    def __init__(self, username, password):\n        self.url = \"https://api.bitbucket.org/2.0\"\n        self.auth_token = (username, password)\n\n    def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        \n        data = {\n            \"name\": name,\n            \"start\": {\n                \"type\": \"commit\",\n                \"hash\": start_point\n            }\n        }\n        \n        if message:\n            data[\""
    },
    {
        "original": "\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(list(old_OPTS.keys()) + list(new_OPTS.keys()))\n    \n    for key in all_keys:\n        merged[key] = {}\n        \n        if key in old_OPTS:\n            merged[key].update(old_OPTS[key])\n        \n        if key in new_OPTS:\n            merged[key].update(newOpts[key])\n    \n    return merged\n\n# Test case\nold.opts = {'a':{'x':'old','",
        "rewrite": "\n\n\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(list(old_opts.keys()) + list(new_opts.keys()))\n    \n    for key in all_keys:\n        merged[key] = {}\n        \n        if key in old_opts:\n            merged[key].update(old_opts[key])\n        \n        if key in new_opts:\n            merged[key].update(new_opts[key])\n    \n    return merged\n\n# Test case\nold_opts = {'a':{'x':'old'}, 'b':{'y':'old2'}}\nnew_opts = {'a':{'x':'new'}, 'c':{'z"
    },
    {
        "original": "\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def get(self, file_path, ref, **kwargs):\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        \n        if 'sudo' in kwargs",
        "rewrite": "\n\n\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def get(self, file_path, ref, **kwargs):\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        if 'sudo' in kwargs:\n            params['sudo'] = kwargs['sudo']\n        response = requests.get(f'{self.url}/{file_path"
    },
    {
        "original": "\ndef clean_recipe_build(self, args):\n    import os\n    import shutil\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    if os.path.exists(recipe_build_dir):\n        shutil.rmtree(recipe_build_dir)\n    print(f\"Deleted build files for recipe {args.recipe_name}\")\n",
        "rewrite": "\nimport os\nimport shutil\n\ndef clean_recipe_build(self, args):\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    if os.path.isdir(recipe_build_dir):\n        shutil.rmtree(recipe_build_dir)\n    print(f\"Deleted build files for recipe {args.recipe_name}\")\n"
    },
    {
        "original": "\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    bias_shape = [input_shape[i] if i in bias_dims else 1 for i in range(len(input_shape))]\n    return tuple(bias_shape)\n",
        "rewrite": "\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    return tuple( input_shape[i] if i in set(bias_dims) else 1 for i in range(len(input_shape)) )\n"
    },
    {
        "original": "\ndef read_metadata(text, ext):\n    if ext == 'txt':\n        lines = text.split('\\n')\n        metadata = {}\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                metadata[key.strip()] = value.strip()\n        return metadata\n    else:\n        return {}\n",
        "rewrite": "\ndef read_metadata(text, ext):\n    if ext == 'txt':\n        metadata = {k.strip(): v.strip() for line in text.split('\\n') for k, v in [line.split(':', 1)] if ':' in line}\n        return metadata\n    return {}"
    },
    {
        "original": "\ndef _is_process_filtered(self, process, key=None):\n    \"\"\"\n    Return True if the process[key] should be filtered according to the current filter\n    \"\"\"\n    # Assuming self.filter is set elsewhere in your class\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n\n    if key is None:\n        return any(str(d) in str(self.filter) for d in process.values())\n    else:\n        return str(process.get(key)) in str",
        "rewrite": "\n\n\ndef _is_process_filtered(self, process, key=None):\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n    if key is None:\n        return any(str(d) in str(self.filter) for d in process.values())\n    return str(process.get(key)) in str(self.filter)\n"
    },
    {
        "original": "\ndef modulation_type(self, value: int):\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    if 0 <= value <= 3:\n        return modulation_types[value]\n    else:\n        return None\n",
        "rewrite": "\ndef modulation_type(self, value: int) -> str | None:\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    return modulation_types.get(value) if 0 <= value <= 3 else None\n"
    },
    {
        "original": "\ndef load(self, fname):\n    with open(fname, 'r') as f:\n        self.signatures = [line.strip() for line in f.readlines()]\n",
        "rewrite": "\ndef load(self, fname):\n    with open(fname, 'r', encoding='utf-8', errors='ignore') as f:\n        self.signatures = [line.rstrip() for line in (l for l in (ll.strip() for ll in f) if ll)]\n"
    },
    {
        "original": "\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid, room, namespace=None):\n        if namespace is None:\n            namespace = self.__class__.__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n",
        "rewrite": "\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid, room, namespace=None):\n        namespace = namespace or type(self).__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n"
    },
    {
        "original": "\ndef _get_args(self, **):\n    return {key: value for key, value in kwargs.items() if hasattr(self.__class__, key)}\n",
        "rewrite": "\ndef _get_args(self, **, kwargs):\n    return {key: kwargs[key] for key in dir(self.__class__) if key in kwargs}\n"
    },
    {
        "original": "\nfrom pyogram import Client\nfrom pyogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self, caption: str, parse_mode: str = \"\", reply_markup: Union[InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id, message_id=self.message_id,c",
        "rewrite": "\n\n\nfrom pyogram import Client\nfrom pyogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self,\n                      caption: str,\n                      parse_mode: str = \"\",\n                      reply_markup: Union[InlineKeyboardMarkup,\n                                          ReplyKeyboardMarkup,\n                                          Reply KeyboardRemove,\n                                          ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id,\n                                         message_id=self.message_id,\n                                         caption=caption,\n                                         parse_mode=parse_mode,\n                                         reply_markup"
    },
    {
        "original": "\nimport numpy as np\nfrom scipy.linalg import kron\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    # Get the size of sub-matrices\n    n = int(np.sqrt(mat4x4.shape[0]))\n\n    # Reshape mat4x4 into 3D array (n*n matrices)\n    mats = mat4x4.reshape(n, n, n, n)\n\n    # Flip kron order by swapping last two axes",
        "rewrite": "\n\n\nimport numpy as np\nfrom scipy.linalg import kron\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    n = int(np.sqrt(mat4x4.shape[0]))\n    mats = mat4x4.reshape(n, n, n, n)\n    return mats.transpose(0, 1, 3, 2).reshape(n*n, n*n)\n"
    },
    {
        "original": "\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\" Hyper-Band Parameters: \")\n    print(\"     - Try {} possible sets of hyperparameters\".format(len(hyperband_schedule)))\n    if describe_hyperband:\n        print(\"     - With an average of {} iterations per trial\".format(sum([len(trial) for trial in hyperband_schedule]) / len(hyperband_schedule)))\n    print(\"\")\n    \n    max_r = max(max(trial)",
        "rewrite": "\n\n\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\"Hyper-Band Parameters:\")\n    print(\"     - Try {} possible sets of hyperparameters\".format(len(hyperband_schedule)))\n    if describe_hyperband:\n        avg_iterations_per_trial = sum(len(trial) for trial in hyperband_schedule) / len(hyperband_schedule)\n        print(\"     - With an average of {:.2f} iterations per trial\".format(avg_iterations_per_trial))\n    print()\n    \n    max_r = max(max(len(trial) for trial in hyperband_scale))"
    },
    {
        "original": "\nclass Matrix:\n    def __init__(self, a, b, c, d, e, f):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        self.e = e\n        self.f = f\n\n    def shorthand(self):\n        return (self.a, self.b, self.c, self.d, self.e, self.f)\n",
        "rewrite": "\nclass Matrix:\n    def __init__(self, *args):\n        self.elements = args\n\n    def shorthand(self):\n        return self.elements\n"
    },
    {
        "original": "\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited:\n            if dfs",
        "rewrite": "\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited:\n            if dfs(node):\n                return True\n    return False"
    },
    {
        "original": "\nfrom email import policy\nfrom email.parser import BytesParser\n\ndef get_header_items(self):\n    \"\"\"Get an iterable list of key/value pairs representing headers.\"\"\"\n    parser = BytesParser(policy=policy.default)\n    msg = parser.parsestr(self.request_text)\n    return [(k, v) for k, v in msg.items()]\n",
        "rewrite": "\nfrom email import policy\nfrom email.parser import BytesParser\n\nclass EmailParser:\n    def __init__(self, request_text):\n        self.request_text = request_text\n\n    def get_header_items(self):\n        parser = BytesParser(policy=policy.default)\n        msg = parser.parsestr(self.request_text)\n        return [(k, v) for k, v in msg.items()]\n"
    }
]
