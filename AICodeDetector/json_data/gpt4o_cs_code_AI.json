[
    {
        "original": "\n\n    \"\"\"Eagerly converts struct to Tensor, recursing upon failure.\"\"\"\n    try:\n        return tf.convert_to_tensor(struct, dtype=dtype, name=name)\n    except (TypeError, ValueError):\n        if isinstance(struct, dict):\n            return {k: _nested_convert_to_tensor(v, dtype, name) for k, v in struct.items()}\n        elif isinstance(struct, (list, tuple)):\n            return type(struct)(_nested_convert_to_tensor(v, dtype, name) for v in struct)\n        else:\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Construct the Discord JSON payload. All relevant parameters are combined here\n    to a valid Discord JSON payload.\n\n    :return: Discord payload (str) to send\n    \"\"\"\n    payload = {\n        \"content\": self.content,\n        \"username\": self.username,\n        \"avatar_url\": self.avatar_url,\n        \"embeds\": self.embeds\n    }\n    return json.dumps(payload)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Executes the sql and returns the first resulting row.\n\n    :param sql: the sql statement to be executed (str) or a list of\n        sql statements to execute\n    :type sql: str or list\n    :param parameters: The parameters to render the SQL query with.\n    :type parameters: mapping or iterable\n    \"\"\"\n    if isinstance(sql, list):\n        sql = ' '.join(sql)\n    \n    cursor = self.connection.cursor()\n    cursor.execute(sql, parameters)\n    result = cursor.fetchone()\n    cursor.close()\n    return result",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Transfers the remote file to a local location.\n    If local_full_path is a string path, the file will be put\n    at that location\n    :param remote_full_path: full path to the remote file\n    :type remote_full_path: str\n    :param local_full_path: full path to the local file\n    :type local_full_path: str\n    \"\"\"\n\n    # Assuming self.client is an instance of paramiko.SFTPClient\n    sftp_client = self.client.open_sftp()\n",
        "rewrite": ""
    },
    {
        "original": "\n\n            execution_date,\n            key=None,\n            task_id=None,\n            dag_id=None,\n            include_prior_dates=False,\n            session=None):\n    \"\"\"\n    Retrieve an XCom value, optionally meeting certain criteria.\n    TODO: \"pickling\" has been deprecated and JSON is preferred.\n    \"pickling\" will be removed in Airflow 2.0.\n\n    :return: XCom value\n    \"\"\"\n    filters = [cls.execution_date == execution_date]\n    \n    if key:\n        filters.append(cls.key == key)\n    if task_id:\n        filters.append",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Walks the tree of loggers and tries to set the context for each handler\n    :param logger: logger\n    :param value: value to set\n    \"\"\"\n    for handler in logger.handlers:\n        if hasattr(handler, 'set_context'):\n            handler.set_context(value)\n    for child_logger in logger.manager.loggerDict.values():\n        if isinstance(child_logger, logging.Logger):\n            set_context(child_logger, value)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Given task instance, try_number, filename_template, return the rendered log\n    filename\n\n    :param ti: task instance\n    :param try_number: try_number of the task\n    :param filename_template: filename template, which can be jinja template or\n        python string template\n    \"\"\"\n    context = {\n        'dag_id': ti.dag_id,\n        'task_id': ti.task_id,\n        'execution_date': ti.execution_date.isoformat(),\n        'try_number': try",
        "rewrite": ""
    },
    {
        "original": "\n\ntfd = tfp.distributions\n\n    distribution_b,\n    use_exact_kl=False,\n    test_points_reduce_axis=(),  # `None` == \"all\"; () == \"none\".\n    test_points_fn=tf.convert_to_tensor,\n    weight=None):\n  \"\"\"Creates a callable computing `KL[a,b]` from `a`, a `tfd.Distribution`.\"\"\"\n  \n    if use_exact_kl:\n      kl_divergence = t",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Waits for the named operation to complete - checks status of the async call.\n\n    :param operation_name: name of the operation\n    :type operation_name: str\n    :param zone: optional region of the request (might be None for global operations)\n    :type zone: str\n    :return: None\n    \"\"\"\n\n    compute = discovery.build('compute', 'v1')\n\n    while True:\n        if zone:\n            result =",
        "rewrite": ""
    },
    {
        "original": "\n\n    s3 = boto3.resource('s3')\n    bucket = s3.Bucket(bucket_name)\n    \n    for obj in bucket.objects.all():\n        if fnmatch.fnmatch(obj.key, wildcard_key):\n            return obj\n    return None\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    video_info_url = f\"http://s.video.sina.com.cn/video/h5play?video_id={vid}\"\n    response = requests.get(video_info_url)\n    video_info = response.json()\n    \n    if not title:\n        title = video_info['data']['title']\n    \n    video_url = video_info['data']['videos']['hd']\n    video_response = requests.get(video_url, stream=True)\n    \n",
        "rewrite": ""
    },
    {
        "original": "\n\n    @wraps(func)\n    @provide_session\n        result = func(*args, **kwargs)\n        \n        # Assuming lineage information is available in the function's return value\n        lineage_info = result.get('lineage', {})\n        \n        # Save lineage to XCom\n        task_instance = kwargs.get('ti')\n        if task_instance:\n            task_instance.xcom_push(key='lineage', value=lineage_info)\n        \n        # Send",
        "rewrite": ""
    },
    {
        "original": "\n\n    x = np.asarray(x, dtype=dtype)\n    concentration = np.asarray(concentration, dtype=dtype)\n    \n    # Ensure x is within the range [0, 2*pi]\n    x = np.mod(x, 2 * np.pi)\n    \n    # Initialize the series expansion\n    cdf = np.zeros_like(x)\n    derivative = np.zeros_like(x)\n    \n    for n in range(1, num_terms + 1):\n        term",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.name_scope(name or 'MixtureDistribution'):\n        # Split the params into mixture weights and component parameters\n        mixture_weights, component_params = tf.split(params, [num_components, -1], axis=-1)\n        \n        # Create the mixture distribution\n        mixture_distribution = tfp.distributions.Categorical(logits=mixture_weights)\n        \n        # Create the component distributions\n        component_distributions",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Get summary ops for the magnitude of gradient updates.\"\"\"\n    summaries = []\n    for grad, var in grads:\n        if grad is not None:\n            grad_update = opt.apply_gradients([(grad, var)], global_step=tf.train.get_or_create_global_step())\n            with tf.control_dependencies([grad_update]):\n                grad_norm = tf.norm(grad)\n                update_norm = tf.norm(lr * grad)\n                summaries.append(tf.summary.scalar(f'{var.op.name}/grad_norm', grad_norm))\n                summaries.append(tf.summary.scalar(f'{",
        "rewrite": ""
    },
    {
        "original": "\n\n    base_url = 'https://www.veoh.com/watch/'\n    video_url = urljoin(base_url, item_id)\n    \n    response = requests.get(video_url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch video page: {response.status_code}\")\n    \n    # Extract video information (this is a simplified example, actual extraction may vary)\n    video_info = extract_video_info(response.text)\n    \n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n# Example setup (replace with your actual database URL)\nDATABASE_URL = \"sqlite:///example.db\"\nengine = create_engine(DATABASE_URL)\nSession = scoped_session(sessionmaker(bind=engine))\n\n    \"\"\" Properly close pooled database connections \"\"\"\n    Session.remove()\n    engine.dispose()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Returns list of assertions related to `lu_solve` assumptions.\"\"\"\n    assertions = []\n    if validate_args:\n        lower_upper_shape = tf.shape(lower_upper)\n        perm_shape = tf.shape(perm)\n        rhs_shape = tf.shape(rhs)\n\n        assertions.append(tf.debugging.assert_equal(\n            lower_upper_shape[-1], lower_upper_shape[-2],\n            message=\"`lower_upper` must be a square matrix.\"))\n        \n        assertions.append(tf.debugging.assert_equal(\n            perm_shape[-1], lower_upper_shape[-1",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Email server connection details\n    imap_host = context['imap_host']\n    imap_user = context['imap_user']\n    imap_pass = context['imap_pass']\n    \n    # S3 connection details\n    s3_bucket = context['s3_bucket']\n    s3_folder = context['s3_folder']\n    \n    # Connect to the email server\n    mail = imaplib.IMAP4_SSL(imap_host)\n    mail.login(imap_user, imap_pass",
        "rewrite": ""
    },
    {
        "original": "\nclass EcommerceAgent:\n        self.inventory = {}\n        self.cart = []\n\n        if item in self.inventory:\n            self.inventory[item]['quantity'] += quantity\n        else:\n            self.inventory[item] = {'price': price, 'quantity': quantity}\n\n        if item in self.inventory and self.inventory[item]['quantity'] >= quantity:\n            self.cart.append({'item': item, 'quantity': quantity, 'price': self.inventory[item]['price']})\n            self.inventory[item",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Calls `fn`, appropriately reshaping its input `x` and output.\"\"\"\n    original_shape = x.shape\n    x = x.reshape(-1, x.shape[-1])  # Flatten the input except for the last dimension\n\n    if extra_kwargs is None:\n        extra_kwargs = {}\n\n    y = fn(x, **extra_kwargs)\n\n    output_shape = list(original_shape)\n    output_shape[-1] = y.shape[-1]\n    y = y.reshape(output_shape)\n\n    return y\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    for ax in sorted(axis):\n        x = tf.expand_dims(x, axis=ax)\n    return x\n",
        "rewrite": ""
    },
    {
        "original": "\n        self, location, product_set_id, project_id=None, retry=None, timeout=None, metadata=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetDeleteOperator`\n        \"\"\"\n        client = vision_v1.ProductSearchClient()\n\n        if not project_id:\n            project_id = self.project_id\n\n        name = client.product_set_path(project_id, location, product_set_id)\n\n        client.delete_product_set(name=name, retry=retry, timeout=timeout",
        "rewrite": ""
    },
    {
        "original": "\n\n    md5 = hashlib.md5()\n    md5.update(upid.encode('utf-8'))\n    digest = md5.digest()\n    mimi = ''.join(f'{(digest[i] ^ digest[i + 8]):02x}' for i in range(8))\n    return mimi\n",
        "rewrite": ""
    },
    {
        "original": "\n    return num_components + num_components * component_params_size\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    try:\n        waiter = client.get_waiter('task_ended')\n        waiter.wait(TaskId=task_id)\n    except botocore.exceptions.WaiterError:\n        # Exponential backoff\n        max_attempts = 10\n        base_delay = 1  # starting delay in seconds\n        for attempt in range(max_attempts):\n            try:\n                response = client.describe_tasks(TaskIds=[task_id])\n                if response['Tasks'][0",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    return tfp.distributions.MultivariateNormalDiag(\n        loc=params[:event_shape[0]],\n        scale_diag=tf.math.softplus(params[event_shape[0]:]),\n        validate_args=validate_args,\n        allow_nan_stats=True,\n        name=name\n    )\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Ensures `self.distribution.mean()` has `[batch, event]` shape.\"\"\"\n    mean = self.distribution.mean()\n    if mean.shape.ndims is None:\n        raise ValueError(\"The rank of the mean tensor must be known.\")\n    while mean.shape.ndims < self.batch_ndims + self.event_ndims:\n        mean = tf.expand_dims(mean, axis=0)\n    return mean\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Model inference based on the given data.\n    You need to invoke collect() to trigger those actions\n    as the returning result is an RDD.\n\n    :param data_rdd: the data to be predicted.\n    :param batch_size: total batch size of prediction.\n    :return: An RDD representing the prediction result.\n    \"\"\"\n    if batch_size == -1:\n        batch_size = data_rdd.count()\n    \n        partition_data = list(partition)\n        if len(partition",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Call the SparkSubmitHook to run the provided spark job\n    \"\"\"\n\n    hook = SparkSubmitHook(\n        application=self.application,\n        conf=self.conf,\n        conn_id=self.conn_id,\n        files=self.files,\n        py_files=self.py_files,\n        jars=self.jars,\n        driver_class_path=self.driver_class_path,\n        packages=self.packages,\n        exclude_packages=self.exclude_packages,\n        repositories=self.repositories,\n        total_executor_cores=self.total_executor_cores,\n        executor_cores=self.executor_",
        "rewrite": ""
    },
    {
        "original": "\n\n                                     iterator: Union[DataLearningIterator, DataFittingIterator] = None, *,\n                                     to_train: bool = True,\n                                     evaluation_targets: Optional[Iterable[str]] = None,\n                                     to_validate: Optional[bool] = None,\n                                     download: bool = False,\n                                     start_epoch_num: Optional[int] = None,\n                                     recursive: bool = False) -> Dict[str, Dict[str, float]]:\n    \"\"\"Make training and evaluation of",
        "rewrite": ""
    },
    {
        "original": "\n                   val_c_input,\n                   active,\n                   step_size_shrink_param):\n    \"\"\"Shrinks the input step size until the value and grad become finite.\"\"\"\n    step_size = 1.0\n    while True:\n        val, grad = value_and_gradients_function(val_c_input * step_size, active)\n        if np.isfinite(val) and np.all(np.isfinite(grad)):\n            break\n        step_size *= step_size_shrink_param\n    return step_size\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Visualizes the reconstruction of inputs in TensorBoard.\n\n    Args:\n        inputs: A tensor of the original inputs, of shape [batch, timesteps,\n            h, w, c].\n        reconstruct: A tensor of a reconstruction of inputs, of shape\n            [batch, timesteps, h, w, c].\n        num: Integer for the number of examples to visualize.\n        name: String name of this summary.\n    \"\"\"\n    with tf.name_scope(name):\n        # Select the first `",
        "rewrite": ""
    },
    {
        "original": "\n\n    model = Inception3(**kwargs)\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(\n            'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n            progress=True\n        )\n        model.load_state_dict(state_dict)\n    return model\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if file_name.startswith(\"gs://\"):\n        client = storage.Client()\n        bucket_name, blob_name = file_name[5:].split(\"/\", 1)\n        bucket = client.bucket(bucket_name)\n        blob = bucket.blob(blob_name)\n        \n        local_file_name = os.path.basename(blob_name)\n        blob.download_to_filename(local_file_name)\n        \n        return os.path.abspath(local_file_name)\n    else:\n        return os.path.abspath(file_name)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    manifest_path = os.path.join(app.static_folder, 'manifest.json')\n    \n    with open(manifest_path) as manifest_file:\n        manifest = json.load(manifest_file)\n    \n    @app.context_processor\n            return url_for('static', filename=manifest.get(filename, filename))\n        return dict(url_for_asset_=url_for_asset_)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    SlackAPIOperator calls will not fail even if the call is not unsuccessful.\n    It should not prevent a DAG from completing in success\n    \"\"\"\n    try:\n        # Your Slack API call logic here\n        response = self.call_slack_api(**kwargs)\n        if not response['ok']:\n            self.log.warning(\"Slack API call was not successful: %s\", response['error'])\n    except Exception as e:\n        self.log.error(\"An error occurred during Slack API call: %s\", str(e))\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    ec2 = boto3.resource('ec2', region_name=region)\n    \n    instances = ec2.create_instances(\n        MinCount=count,\n        MaxCount=count,\n        **ec2_config\n    )\n    \n    if tags:\n        for instance in instances:\n            instance.create_tags(Tags=tags)\n    \n    if waitForSSH:\n        for instance in instances:\n            instance.wait_until_running()\n            instance.reload()\n            while True:\n                try:\n                    response = instance.describe_instance",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Computes `model_matrix @ model_coefficients + offset`.\"\"\"\n    linear_predictor = np.dot(model_matrix, model_coefficients)\n    if offset is not None:\n        linear_predictor += offset\n    return linear_predictor\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Takes a cursor, and writes the BigQuery schema for the results to a\n    local file system.\n\n    :return: A dictionary where key is a filename to be used as an object\n        name in GCS, and values are file handles to local files that\n        contains the BigQuery schema fields in .json format.\n    \"\"\"\n    schema = cursor.description\n    schema_fields = []\n\n    for field in schema:\n        field_info = {\n            \"name\": field[0],\n            \"type\": field[1",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Convert a vector size to a matrix size.\"\"\"\n    with tf.name_scope(name or 'vector_size_to_square_matrix_size'):\n        d = tf.convert_to_tensor(d, name='d')\n        \n        if validate_args:\n            d = tf.debugging.assert_non_negative(d, message='d must be non-negative.')\n            d = tf.debugging.assert_integer(d, message='d must be an integer.')\n        \n        matrix_size = tf.sqrt(tf.cast(d, tf.float32))\n        \n        if validate_args:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n\n    sc = SparkContext.getOrCreate()\n    \n    # Load IMDB dataset\n    (training_data, training_labels), (testing_data, testing_labels) = imdb.load_data(nb_words=5000)\n    \n    # Pad sequences to the same length\n    max_len = 500\n    training_data = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in training_data])\n    testing_data = np",
        "rewrite": ""
    },
    {
        "original": "\n\nclass MNISTDownloader:\n        self.processed_folder = processed_folder\n        self.url_base = 'http://yann.lecun.com/exdb/mnist/'\n        self.files = {\n            'train_images': 'train-images-idx3-ubyte.gz',\n            'train_labels': 'train-labels-idx1-ubyte.gz',\n            'test_images': 't10k-images-idx3-ubyte.gz',\n            'test_labels': 't10k-labels-",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Start H2O cluster.\n    The cluster is not up until wait_for_cloud_to_be_up() is called and returns.\n\n    :return none\n    \"\"\"\n    self._start_cluster()\n    self.wait_for_cloud_to_be_up()\n",
        "rewrite": ""
    },
    {
        "original": "\n        self,\n        location,\n        product_id,\n        reference_image_id,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionReferenceImageCreateOperator`\n        \"\"\"\n        client = vision_v1.ProductSearchClient()\n\n        if not project_id:\n            project_id = self.project_id\n\n        name = client.reference_image_path(\n            project=project_id,\n            location=location,\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Function decorator that intercepts HTTP Errors and raises AirflowException\n    with more informative message.\n    \"\"\"\n    @wraps(func)\n        try:\n            return func(*args, **kwargs)\n        except requests.exceptions.HTTPError as e:\n            raise AirflowException(f\"HTTP error occurred: {e.response.status_code} - {e.response.reason}\")\n        except requests.exceptions.RequestException as e:\n            raise AirflowException(f\"Request error occurred:",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Create Y-axis\n    \"\"\"\n    y_axis = {\n        'name': name,\n        'label': label if label else name,\n        'format': format if custom_format else 'default'\n    }\n    return y_axis\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    hash_md5 = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest() == md5\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n    \n    if filename is None:\n        filename = os.path.basename(url)\n    \n    file",
        "rewrite": ""
    },
    {
        "original": "\n\n    with tf.variable_scope(scope):\n        # Linear transformation of the state\n        state_proj = tf.layers.dense(state, att_size, activation=tf.tanh, name=\"state_proj\")\n        \n        # Linear transformation of the inputs\n        inputs_proj = tf.layers.dense(inputs, att_size, activation=tf.tanh, name=\"inputs_proj\")\n        \n        # Compute the attention scores\n        scores = tf.reduce_sum(tf.tanh(state_proj + inputs_proj), axis=-1)\n        \n        # Apply mask to the scores",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Checks that input is a `float` matrix.\"\"\"\n    if validate_args:\n        if not isinstance(a, np.ndarray):\n            raise ValueError(\"Input must be a numpy array.\")\n        if not np.issubdtype(a.dtype, np.floating):\n            raise ValueError(\"Input matrix must be of float type.\")\n    return a\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not self.task.downstream_task_ids:\n        return True\n\n    if not session:\n        with create_session() as session:\n            return self.are_dependents_done(session=session)\n\n    TI = TaskInstance\n    downstream_task_ids = self.task.downstream_task_ids\n    downstream_tis = session.query(TI).filter(\n        TI.dag_id == self.dag_id,\n        TI.task_id.in_(downstream_task_ids",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Helper to validate block sizes.\"\"\"\n    if len(block_sizes) != len(bijectors):\n        raise ValueError(\"block_sizes and bijectors must have the same length.\")\n    \n    if validate_args:\n        for size in block_sizes:\n            if size <= 0:\n                raise ValueError(\"All block sizes must be positive integers.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Calculate the batched KL divergence `KL(a || b)` with `a` Deterministic.\n\n    Args:\n        a: instance of a Deterministic distribution object.\n        b: instance of a Distribution distribution object.\n        name: (optional) Name to use for created operations. Default is\n            \"kl_deterministic_distribution\".\n\n    Returns:\n        Batchwise `KL(a || b)`.\n    \"\"\"\n\n    with tf.name_scope(name or \"kl",
        "rewrite": ""
    },
    {
        "original": "\n        self,\n        product_set_id,\n        product_id,\n        location=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionAddProductToProductSetOperator`\n        \"\"\"\n        client = vision_v1.ProductSearchClient()\n\n        if not location:\n            location = 'us-west1'\n\n        if not project_id:\n            project_id = 'your-project-id'\n\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Placeholder for the actual bot framework execution logic\n    print(\"Running MS Bot Framework...\")\n\n    \"\"\"Parse parameters and run ms bot framework\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run MS Bot Framework\")\n    parser.add_argument('--config', type=str, help='Path to the configuration file')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    args = parser.parse_args()\n    \n    if args.verbose:\n        print(f\"Configuration file: {args.config}\")\n        print(\"Verbose mode enabled\")\n    \n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n                           expect_ndims=None,\n                           expect_ndims_at_least=None,\n                           expect_ndims_no_more_than=None):\n  \"\"\"Get static ndims if possible.  Fallback on `tf.rank(x)`.\"\"\"\n  ndims = x.shape.ndims\n  if ndims is not None:\n    if expect_ndims is not None and ndims != expect_ndims:\n      raise ValueError(f\"Expected ndims to be {expect_ndims} but got {ndims}\")\n    if expect_ndims_at_least is not None and",
        "rewrite": ""
    },
    {
        "original": "\n    for key, value in editing_dict.items():\n        if isinstance(value, dict) and key in editable_dict and isinstance(editable_dict[key], dict):\n            update_dict_recursive(editable_dict[key], value)\n        else:\n            editable_dict[key] = value\n",
        "rewrite": ""
    },
    {
        "original": "\n                  volatility_fn,\n                  state,\n                  step_size,\n                  target_log_prob=None,\n                  grads_target_log_prob=None,\n                  volatility=None,\n                  grads_volatility_fn=None,\n                  diffusion_drift=None,\n                  parallel_iterations=10):\n  \"\"\"Helper which processes input args to meet list-like assumptions.\"\"\"\n  state = list(state) if isinstance(state, (list, tuple)) else [state]\n  step_size = list(step_size) if isinstance(step_size, (list, tuple)) else [step_size]\n\n  if target_log_prob is None or grads_target_log_prob is None",
        "rewrite": ""
    },
    {
        "original": "\n\n        prediction_tokens = prediction.split()\n        ground_truth_tokens = ground_truth.split()\n        common_tokens = set(prediction_tokens) & set(ground_truth_tokens)\n        if len(common_tokens) == 0:\n            return 0.0\n        precision = len(common_tokens) / len(prediction_tokens)\n        recall = len(common_tokens) / len(ground_truth_tokens)\n        return ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    This function creates the command list from available information\n    \"\"\"\n    cmd = [self.base_command]\n    if self.options:\n        for option, value in self.options.items():\n            cmd.append(f\"--{option}\")\n            if value is not None:\n                cmd.append(str(value))\n    if self.arguments:\n        cmd.extend(self.arguments)\n    return cmd\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n    \n    if isinstance(json_api_content, str):\n        json_api_content = json.loads(json_api_content)\n    \n    course_title = json_api_content.get('title', 'Unknown Course')\n    course_dir = os.path.join(output_dir, course_title)\n    os.makedirs(course_dir, exist_ok=True)\n    \n    for chapter in json_api_content.get",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Download video from Yixia\"\"\"\n    # Extract video information\n    video_info = get_video_info(url)\n    if info_only:\n        return video_info\n\n    # Download video segments\n    segments = video_info['segments']\n    downloaded_files = []\n    for i, segment in enumerate(segments):\n        segment_url = segment['url']\n        segment_file = os.path.join(output_dir, f'segment_{i}.ts')\n        download_file(segment_url, segment_file)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Executes command received and stores result state in queue.\n    :param key: the key to identify the TI\n    :type key: tuple(dag_id, task_id, execution_date)\n    :param command: the command to execute\n    :type command: str\n    \"\"\"\n\n    # Create a queue to store the result state\n    result_queue = Queue()\n\n    try:\n        # Execute the command\n        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if session is None:\n        session = Session()\n\n    try:\n        # Delete TaskInstances\n        session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id).delete()\n\n        # Delete DagRuns\n        session.query(DagRun).filter(DagRun.dag_id == dag_id).delete()\n\n        # Optionally delete logs\n        if not keep_records_in_log:\n            session.query(Log).filter(Log.d",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Remote Popen to execute the spark-submit job\n\n    :param application: Submitted application, jar or py file\n    :type application: str\n    :param kwargs: extra arguments to Popen (see subprocess.Popen)\n    \"\"\"\n    command = [\"spark-submit\", application]\n    process = subprocess.Popen(command, **kwargs)\n    return process\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Pod:\n        self.name = name\n        self.state = 'Pending'\n        self.logs = ''\n\n        # Simulate pod starting\n        time.sleep(2)\n        self.state = 'Running'\n\n        # Simulate waiting for pod to complete\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if self.state == 'Running':\n                time.sleep(2)  # Simulate some running time\n               ",
        "rewrite": ""
    },
    {
        "original": "\n\n                         current_objective_values,\n                         objective_function=None,\n                         dim=None,\n                         func_tolerance=None,\n                         position_tolerance=None,\n                         batch_evaluate_objective=False,\n                         reflection=1.0,\n                         expansion=2.0,\n                         contraction=0.5,\n                         shrinkage=0.5,\n                         name=None):\n    \"\"\"A single iteration of the Nelder Mead algorithm.\"\"\"\n    \n    # Sort the simplex and objective values\n    indices = np.argsort(current_objective_values)\n    current_simplex = current_simplex",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Wrapper for tf.Print which supports lists and namedtuples for printing.\"\"\"\n    if isinstance(values, (list, tuple)):\n        values = [tf.convert_to_tensor(v) for v in values]\n    else:\n        values = [tf.convert_to_tensor(values)]\n    \n    return tf.print(pass_through_tensor, values)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Set validation and checkpoint for distributed optimizer.\n    \"\"\"\n\n    if not isinstance(optimizer, Optimizer):\n        raise ValueError(\"The optimizer must be an instance of torch.optim.Optimizer\")\n\n    if not isinstance(test_data, DataLoader):\n        raise ValueError(\"The test_data must be an instance of torch.utils.data.DataLoader\")\n\n    # Set the model to evaluation mode\n    model = options.get('model')\n    if model is None:\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Execute the python dataflow job.\"\"\"\n\n    # Define your pipeline options\n    options = PipelineOptions(\n        runner='DataflowRunner',\n        project='your-gcp-project-id',\n        region='your-gcp-region',\n        temp_location='gs://your-temp-bucket/temp',\n        staging_location='gs://your-staging-bucket/staging',\n        job_name='your-dataflow-job-name'\n    )\n\n    # Define your pipeline\n    with beam.Pipeline(options=options)",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Applies a Householder rotation to `samples`.\"\"\"\n    # Generate a random vector for the Householder transformation\n    v = np.random.randn(samples.shape[1])\n    v = v / np.linalg.norm(v)  # Normalize the vector\n\n    # Compute the Householder matrix\n    H = np.eye(samples.shape[1]) - 2 * np.outer(v, v)\n\n    # Apply the Householder transformation\n    rotated_samples = np.dot(samples, H)\n\n    return rotated_samples\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Integrate plugins to the context\"\"\"\n    plugins = load_plugins()  # Assuming load_plugins is a function that loads available plugins\n    context = get_context()   # Assuming get_context is a function that retrieves the current context\n\n    for plugin in plugins:\n        if hasattr(plugin, 'integrate'):\n            plugin.integrate(context)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    download_path = Path(download_path)\n    if extract_paths is None:\n        extract_paths = [download_path]\n    elif isinstance(extract_paths, (str, Path)):\n        extract_paths = [Path(extract_paths)]\n    else:\n        extract_paths = [Path(p) for p in extract_paths]\n\n    # Ensure download directory exists\n    download_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the file\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Return a list (preferred) or 1d Tensor from values, if values.ndims < 2.\"\"\"\n    if isinstance(values, tf.Tensor):\n        if values.ndim < 2:\n            return values\n        else:\n            raise ValueError(\"Tensor has more than 1 dimension\")\n    elif isinstance(values, (list, tuple)):\n        return list(values)\n    else:\n        raise TypeError(\"Input must be a Tensor, list, or tuple\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Sleep for the time specified in the exception. If not specified, wait\n    for 60 seconds.\n    \"\"\"\n    wait_time = getattr(rate_limit_exception, 'retry_after', 60)\n    time.sleep(wait_time)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    :param dag_id: DAG ID\n    :type dag_id: unicode\n    :return: if the given DAG ID exists in the bag, return the BaseDag\n    corresponding to that ID. Otherwise, throw an Exception\n    :rtype: airflow.utils.dag_processing.SimpleDag\n    \"\"\"\n    if dag_id in self.dags:\n        return self.dags[dag_id]\n    else:\n        raise Exception(f\"DAG with ID {dag_id} does not exist.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Defines any necessary secrets for the pod executor\"\"\"\n    secrets = {\n        'database_password': 'supersecretpassword',\n        'api_key': '12345-abcde-67890-fghij',\n        'encryption_key': 's3cr3tk3y'\n    }\n    return secrets\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass H2OImporter:\n        self.h2o_ip = h2o_ip\n        self.h2o_port = h2o_port\n\n        url = f\"http://{self.h2o_ip}:{self.h2o_port}/3/ImportFiles\"\n        params = {'path': path}\n        start_time = time.time()\n        \n        while time.time() - start_time < timeoutSecs:\n            response = requests",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Learnable Gamma via concentration and scale parameterization.\"\"\"\n    concentration = tf.Variable(\n        initial_value=tf.random.uniform(shape, minval=min_concentration, maxval=1.0),\n        trainable=True,\n        name=name + '_concentration' if name else None\n    )\n    scale = tf.Variable(\n        initial_value=tf.random.uniform(shape, minval=min_scale, maxval=1.0),\n        trainable=True,\n        name=name +",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Computes `log(sum(exp(input_tensor)))` along the specified axis.\"\"\"\n    max_input = np.max(input_tensor, axis=axis, keepdims=True)\n    if keepdims:\n        return np.log(np.sum(np.exp(input_tensor - max_input), axis=axis, keepdims=True)) + max_input\n    else:\n        return np.log(np.sum(np.exp(input_tensor - max_input), axis=axis)) + np.squeeze(max_input, axis=axis)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Deletes an existing collection in the CosmosDB database.\n    \"\"\"\n    try:\n        # Initialize the Cosmos client\n        client = CosmosClient(self.endpoint, self.key)\n        \n        # Get the database\n        database = client.get_database_client(database_name)\n        \n        # Get the container (collection)\n        container = database.get_container_client(collection_name)\n        \n        # Delete the container\n        container.delete_container()\n        print(f\"Collection '{collection_name}' deleted successfully.\")\n    except exceptions.CosmosResourceNot",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Ensure the mask is a boolean tensor\n    broadcast_mask = tf.cast(broadcast_mask, tf.bool)\n    \n    # Create a mask for the first unmasked entry\n    first_unmasked_mask = tf.logical_and(broadcast_mask, tf.cumsum(broadcast_mask, axis=-1) == 1)\n    \n    # Use the mask to gather the first unmasked entries\n    initial_values = tf.reduce_sum(tf.where(first_unmasked_mask, time_series_tensor, tf.zeros_like(time_series_tensor)), axis=-",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = bigquery.Client(project=project_id)\n    try:\n        dataset = client.get_dataset(dataset_id)\n        return dataset\n    except NotFound:\n        raise Exception(f\"Dataset {dataset_id} not found.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Call the SparkSubmitHook to run the provided spark job\n    \"\"\"\n\n    hook = SparkSubmitHook(\n        application=self.application,\n        conf=self.conf,\n        conn_id=self.conn_id,\n        files=self.files,\n        py_files=self.py_files,\n        jars=self.jars,\n        driver_class_path=self.driver_class_path,\n        packages=self.packages,\n        exclude_packages=self.exclude_packages,\n        repositories=self.repositories,\n        total_executor_cores=self.total_executor_cores,\n        executor_cores=self.executor_",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"The Squared-Hellinger Csiszar-function in log-space.\n\n    Args:\n      logu: `float`-like `Tensor` representing `log(u)` from above.\n      name: Python `str` name prefixed to Ops created by this function.\n\n    Returns:\n      squared_hellinger_of_u: `float`-like `Tensor` of the Csiszar-function\n        evaluated at `u = exp(logu)`.\n    \"\"\"\n    with tf.name_scope(name or 'squared_hellinger'):\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Reloads the current dagrun from the database\n    :param session: database session\n    \"\"\"\n    if session is None:\n        session = provide_session()\n\n    session.expire(self)\n    session.refresh(self)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Constructs function encapsulated in the graph and the session.\"\"\"\n        with graph.as_default():\n            with session.as_default():\n                return func(*args, **kwargs)\n    return wrapper\n",
        "rewrite": ""
    },
    {
        "original": "\n    answer = [{} for _ in range(len(dictionary))]\n\n    for i in range(len(dictionary)):\n        for a in insertion_costs.keys():\n            answer[i][a] = [float('inf')] * n\n\n    for i in range(len(dictionary)):\n        for a in insertion_costs.keys():\n            for j in range(n):\n                min_cost = insertion_costs[a]\n                if allow_spaces:\n                    min_cost = min(min_cost, insertion_costs.get(' ', float('inf')) +",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    If a parameter is not stored in parms dict save it there (even though the value is None).\n    Else check if the parameter has been already set during initialization of estimator. If yes, check the new value is the same or not. If the values are different, set the last passed value to params dict and throw UserWarning.\n    \"\"\"\n    if parameter_name not in parms:\n        parms[parameter_name] = parameter_value\n    else:\n        if parms[parameter_name] != parameter_value:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Check if a file exists on Azure Data Lake.\n\n    :param file_path: Path and name of the file.\n    :type file_path: str\n    :return: True if the file exists, False otherwise.\n    :rtype: bool\n    \"\"\"\n    try:\n        file_system_client = self.service_client.get_file_system_client(file_system=self.file_system_name)\n        file_client = file_system_client.get_file_client(file_path)\n        file_properties = file_client.get_file_properties()\n        return True",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Compute the norm of the given (possibly batched) value.\n\n    Args:\n        value: A `Tensor` of real dtype.\n        dims: An Python integer with the number of non-batching dimensions in the\n              value, i.e. `dims=0` (scalars), `dims=1` (vectors), `dims=2` (matrices).\n        order: Order of the norm, defaults to `np.inf`.\n    \"\"\"\n    if order is None:\n        order = np.inf\n\n    if dims == ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Creates a transfer job that runs periodically.\n\n    :param body: (Required) A request body, as described in\n        https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n    :type body: dict\n    :return: transfer job.\n        See:\n        https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n    :rtype: dict\n    \"\"\"\n\n    # Initialize the Storage Transfer service\n    storagetrans",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Constructs function encapsulated in the graph.\"\"\"\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return wrapped_func\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Get a pandas dataframe from a sql query.\n    \"\"\"\n    conn = hive.Connection(host='your_hive_host', port=10000, username='your_username')\n    try:\n        df = pd.read_sql(hql, conn, params=parameters)\n    finally:\n        conn.close()\n    return df\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\" Dropout with the same drop mask for all fixed_mask_dims\n\n    Args:\n        units: a tensor, usually with shapes [B x T x F], where\n            B - batch size\n            T - tokens dimension\n            F - feature dimension\n        keep_prob: keep probability\n        fixed_mask_dims: in these dimensions the mask will be the same\n\n    Returns:\n        dropped units tensor\n    \"\"\"\n    noise_shape = [units.shape[dim] if dim in fixed_mask_dims else",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(g0 || g1) with g0 and g1 Gamma.\n\n    Args:\n        g0: instance of a Gamma distribution object.\n        g1: instance of a Gamma distribution object.\n        name: (optional) Name to use for created operations. Default is\n          \"kl_gamma_gamma\".\n\n    Returns:\n        kl_gamma_gamma: `Tensor`. The batchwise KL(g0 || g1).\n    \"\"\"\n    with tf.name_scope(name",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Processes Alexa requests from skill server and returns responses to Alexa.\n\n    Args:\n        request: Dict with Alexa request payload and metadata.\n    Returns:\n        result: Alexa formatted or error response.\n    \"\"\"\n    try:\n        request_type = request['request']['type']\n        \n        if request_type == 'LaunchRequest':\n            return self._handle_launch_request(request)\n        elif request_type == 'IntentRequest':\n            return self._handle_intent_request(request)\n        elif request_type == 'SessionEndedRequest':\n            return self._handle_session_ended_request",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Converts a sequence of productions into a string of terminal symbols.\n\n    Args:\n      productions: Tensor of shape [1, num_productions, num_production_rules].\n        Slices along the `num_productions` dimension represent one-hot vectors.\n\n    Returns:\n      str that concatenates all terminal symbols from `productions`.\n\n    Raises:\n      ValueError: If the first production rule does not begin with\n        `self.start_symbol`.\n    \"\"\"\n    # Ensure the first production rule starts with the start symbol\n    if not productions[0, 0",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Specialized inversion sampler for 3D.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    u = np.random.uniform(0, 1, n)\n    v = np.random.uniform(0, 1, n)\n    w = np.random.uniform(0, 1, n)\n    \n    x = self.inverse_transform_x(u)\n    y = self.inverse_transform_y(v)\n    z = self.inverse_transform_z(w)\n    \n    return np.column_stack((x, y, z",
        "rewrite": ""
    },
    {
        "original": "\n\n    dag_bag = DagBag()\n    dag = dag_bag.get_dag(dag_id)\n    task = dag.get_task(task_id)\n    \n    task_info = {var: getattr(task, var) for var in vars(task) if not var.startswith('_')}\n    \n    return json.dumps(task_info, default=str)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Gets the DAG out of the dictionary, and refreshes it if expired\n    \"\"\"\n    dag = self.dag_dict.get(dag_id)\n    if dag and dag.is_expired():\n        dag.refresh()\n    return dag\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Returns sorted array of primes such that `2 <= prime < n`.\"\"\"\n    sieve = [True] * n\n    for i in range(2, int(n**0.5) + 1):\n        if sieve[i]:\n            for j in range(i*i, n, i):\n                sieve[j] = False\n    return [i for i in range(2, n) if sieve[i]]\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Assert x is a non-negative tensor, and optionally of integers.\"\"\"\n    with tf.name_scope(name):\n        x = tf.convert_to_tensor(x, name=\"x\")\n        if not x.dtype.is_integer:\n            raise TypeError(f\"Expected integer type for {name}, but got {x.dtype}\")\n        if tf.reduce_any(x < 0):\n            raise ValueError(f\"Tensor {name} contains negative values\")\n    return x\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Run an optimizer within the graph to minimize a loss function.\"\"\"\n    if optimizer is None:\n        optimizer = tf.optimizers.Adam()\n\n    # Build the loss function\n    loss = build_loss_fn()\n\n    # Get the trainable variables\n    trainable_variables = tf.compat.v1.trainable_variables()\n\n    # Create the training operation\n    train_op = optimizer.minimize(loss, var_list=trainable_variables)\n\n    # Initialize variables\n    init_op = tf.compat.v1.global_variables_initializer()\n\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    One method to fetch connection params as a dict\n    used in get_uri() and get_connection()\n    \"\"\"\n    return {\n        'host': self.host,\n        'port': self.port,\n        'user': self.user,\n        'password': self.password,\n        'database': self.database\n    }\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Save model parameters to self.save_path\"\"\"\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n    \n    # Filter out parameters to exclude\n    state_dict = {k: v for k, v in self.state_dict().items() if not any(scope in k for scope in exclude_scopes)}\n    \n    # Save the state_dict\n    torch.save(state_dict, self.save_path)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    start_time = time.time()\n    end_time = start_time + timeoutSecs\n    job_status = None\n\n    while time.time() < end_time:\n        response = requests.get(f\"{self.base_url}/Jobs/{job_key}\", params=kwargs)\n        if response.status_code == 200:\n            job_status = response.json().get('status')\n            if job_status in [\"DONE\", \"CANCELLED\", \"FAILED\"]:\n                break\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Get broadcast shape as a Python list of integers (preferred) or `Tensor`.\n\n    Args:\n        *tensors:  One or more `Tensor` objects (already converted!).\n\n    Returns:\n        broadcast shape:  Python list (if shapes determined statically), otherwise\n        an `int32` `Tensor`.\n    \"\"\"\n    shapes = [tf.shape(tensor) for tensor in tensors]\n    broadcast_shape = tf.broadcast_static_shape(*[tensor.shape for tensor in tensors])\n    \n    if broadcast_shape.is_fully_defined():\n        return",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Traverse a directory and look for Python files.\n\n    :param directory: the directory to traverse\n    :type directory: unicode\n    :param safe_mode: whether to use a heuristic to determine whether a file\n        contains Airflow DAG definitions\n    :return: a list of paths to Python files in the specified directory\n    :rtype: list[unicode]\n    \"\"\"\n    py_file_paths = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Handles IntentRequest Alexa request.\n\n    Args:\n        request: Alexa request.\n    Returns:\n        response: \"response\" part of response dict conforming Alexa specification.\n    \"\"\"\n    intent_name = request['request']['intent']['name']\n    \n    if intent_name == \"HelloWorldIntent\":\n        response_text = \"Hello, world!\"\n    else:\n        response_text = \"Sorry, I don't know that one.\"\n\n    response = {\n        \"version\": \"1.0\",\n        \"response\": {\n            \"outputSpeech\": {\n                \"",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Logger:\n        self.log_file_path = log_file_path\n        self.bucket_name = bucket_name\n        self.s3_key = s3_key\n        self.s3_client = boto3.client('s3')\n\n        \"\"\"\n        Close and upload local log file to remote storage S3.\n        \"\"\"\n        # Ensure the log file is closed if it's open\n        if hasattr(self, 'log_file') and not self.log_file.closed:\n            self.log_file.close()\n\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Heartbeat DAG file processor and start it if it is not alive.\n    :return:\n    \"\"\"\n    if not self._process.is_alive():\n        self.log.warning(\"Processor for %s is not alive. Restarting...\", self._file_path)\n        self._start_processor()\n    else:\n        self.log.debug(\"Processor for %s is alive.\", self._file_path)\n    self._last_heartbeat = time.time()\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass FileProcessor:\n        self.start_time = time.time()\n        self.processed_files = 0\n\n        # Simulate file processing\n        time.sleep(0.1)\n        self.processed_files += 1\n        self._print_stat()\n\n        elapsed_time = time.time() - self.start_time\n        if elapsed_time > 0:\n            files_per_second = self.processed_files / elapsed_time\n            print(f\"Processed {self.processed_files} files in {elapsed_time",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Ensure all logging output has been flushed\n    \"\"\"\n    for handler in self.handlers:\n        handler.flush()\n",
        "rewrite": ""
    },
    {
        "original": "\n        self, image, max_results=None, retry=None, timeout=None, additional_properties=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectImageSafeSearchOperator`\n        \"\"\"\n        client = vision.ImageAnnotatorClient()\n\n        if isinstance(image, str):\n            image = vision.Image(content=image)\n        elif isinstance(image, dict):\n            image = vision.Image(**image)\n\n        response = client.safe_search_detection(\n            image=image,\n            max_results=max_results,\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Chi.\n\n    Args:\n        a: instance of a Chi distribution object.\n        b: instance of a Chi distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_chi_chi\".\n\n    Returns:\n        Batchwise KL(a || b)\n    \"\"\"\n    with tf.name_scope(name or \"kl_chi_chi\"):\n        a_df = a.df\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Computes the multivariate digamma function; Psi_p(a).\"\"\"\n    with tf.name_scope(name):\n        a = tf.convert_to_tensor(a, name=\"a\")\n        p = tf.convert_to_tensor(p, name=\"p\")\n        return tf.reduce_sum(tf.math.digamma(a - tf.range(p, dtype=a.dtype) / 2), axis=-1)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"20 newsgroups as a tf.data.Dataset.\"\"\"\n    # Load the 20 Newsgroups dataset\n    dataset, info = tfds.load('20_newsgroups', split=split_name, with_info=True, data_dir=directory)\n    \n    # Tokenize the text data\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n    \n        text = example['text'].numpy().decode('utf-8",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Initializes all components required to run a dag for a specified date range and\n    calls helper method to execute the tasks.\n    \"\"\"\n    # Initialize components\n    self._initialize_components(session)\n\n    # Execute tasks\n    self._execute_tasks(session)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Create a Python Model based on the given java value\n    :param jvalue: Java object created by Py4j\n    :param bigdl_type: Data type, default is \"float\"\n    :return: A Python Model\n    \"\"\"\n    if not isinstance(jvalue, JavaObject):\n        raise TypeError(\"jvalue should be a JavaObject, but got {}\".format(type(jvalue)))\n    \n    # Assuming `Model` is a class that can be instantiated with a Java",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = bigquery.Client()\n\n    job_config = bigquery.QueryJobConfig()\n    for key, value in configuration.items():\n        setattr(job_config, key, value)\n\n    query = configuration.get('query')\n    if not query:\n        raise ValueError(\"The configuration must include a 'query' field.\")\n\n    query_job = client.query(query, job_config=job_config)\n\n    result = query_job.result()  # Waits for the query to finish\n\n    return result\n",
        "rewrite": ""
    },
    {
        "original": "\n        self,\n        location,\n        product_id,\n        reference_image,\n        reference_image_id=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :py:class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionReferenceImageCreateOperator`\n        \"\"\"\n        client = vision_v1.ProductSearchClient()\n\n        if project_id is None:\n            project_id = self.project_id\n\n        parent = f\"projects/{project_id}/locations/{location",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Inserts a single document into a mongo collection\n    https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_one\n    \"\"\"\n    if mongo_db:\n        collection = mongo_db[mongo_collection]\n    else:\n        collection = self.db[mongo_collection]\n    \n    result = collection.insert_one(doc, **kwargs)\n    return result.inserted_id\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Converts a user type to RECORD that contains n fields, where n is the\n    number of attributes. Each element in the user type class will be converted to its\n    corresponding data type in BQ.\n    \"\"\"\n\n        type_map = {\n            int: 'INTEGER',\n            float: 'FLOAT',\n            str: 'STRING',\n            bool: 'BOOLEAN',\n            dict: 'RECORD',\n            list: 'RECORD'\n        }\n        return type_map.get(py",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Create a directory to save the population configs\n    os.makedirs('population_configs', exist_ok=True)\n    \n    # Save each individual's config in the population\n    for i, individual in enumerate(population):\n        config_path = f'population_configs/individual_{i}.json'\n        with open(config_path, 'w') as f:\n            json.dump(individual, f)\n    \n    # Save the evolution config\n    evolution_config_path = 'population_configs/evolution_config.json'\n    with open(evolution_config",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Shared init logic for `amplitude` and `length_scale` params.\n\n    Args:\n      amplitude: `Tensor` (or convertible) or `None` to convert, validate.\n      length_scale: `Tensor` (or convertible) or `None` to convert, validate.\n      validate_args: If `True`, parameters are checked for validity despite\n        possibly degrading runtime performance\n\n    Returns:\n      dtype: The common `DType` of the parameters.\n    \"\"\"\n    if amplitude is not None:\n        amplitude = tf.convert_to",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"\n    metrics = {\n        'function_name': func_name,\n        'dag_id': getattr(namespace, 'dag_id', None),\n        'task_id': getattr(namespace, 'task_id',",
        "rewrite": ""
    },
    {
        "original": "\n\nclass JTensor:\n        self.storage = storage\n        self.shape = shape\n        self.bigdl_type = bigdl_type\n\n    @classmethod\n        storage = a_ndarray.flatten()\n        shape = np.array(a_ndarray.shape)\n        return cls(storage, shape, bigdl_type)\n\n        return self.storage.reshape(self.shape)\n",
        "rewrite": ""
    },
    {
        "original": "\n                                       initial_simplex,\n                                       objective_at_initial_simplex,\n                                       batch_evaluate_objective):\n    \"\"\"Evaluates the objective function at the specified initial simplex.\"\"\"\n    if objective_at_initial_simplex is None:\n        if batch_evaluate_objective:\n            objective_at_initial_simplex = objective_function(initial_simplex)\n        else:\n            objective_at_initial_simplex = [objective_function(x) for x in initial_simplex]\n    return initial_simplex, objective_at_initial_simplex\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Load a pre-trained Caffe model.\n\n    :param model: A bigdl model definition which equivalent to the pre-trained caffe model.\n    :param defPath: The path containing the caffe model definition.\n    :param modelPath: The path containing the pre-trained caffe model.\n    :param match_all: Whether to match all layers or not.\n    :param bigdl_type: Data type, default is \"float\".\n    :return: A pre-trained",
        "rewrite": ""
    },
    {
        "original": "\n\n    try:\n        # normal Vimeo video\n        html = get_content('https://vimeo.com/' + id)\n        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n        cfg = json.loads(match1(html, cfg_patt))\n        video_page = get_content(cfg['player']['config_url'], headers=f",
        "rewrite": ""
    },
    {
        "original": "\n            self,\n            mark_success=False,\n            ignore_all_deps=False,\n            ignore_depends_on_past=False,\n            ignore_task_deps=False,\n            ignore_ti_state=False,\n            local=False,\n            pickle_id=None,\n            raw=False,\n            job_id=None,\n            pool=None,\n            cfg_path=None):\n        \"\"\"\n        Returns a command that can be executed anywhere where airflow is\n        installed. This command is part of the message sent to executors by\n        the orchestrator.\n        \"\"\"\n        cmd = [\"airflow\", \"tasks\", \"run\"]\n        \n        if mark_success:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if dropout < 0. or dropout >= 1.:\n        raise ValueError(\"Dropout must be in the range [0, 1).\")\n    \n    if dropout == 0.:\n        return inputs\n    \n    # inputs shape: (batch_size, timesteps, features)\n    batch_size, timesteps, features = inputs.shape\n    \n    # Generate dropout mask\n    dropout_mask = np.random.rand(batch_size, timesteps) >= dropout\n    dropout_mask = dropout_mask[:, :, np.newaxis]  #",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Computes graph and static `sample_shape`.\"\"\"\n    if isinstance(x, tf.Tensor):\n        return tf.shape(x)\n    elif isinstance(x, np.ndarray):\n        return x.shape\n    else:\n        raise TypeError(\"Input must be a TensorFlow tensor or a NumPy array.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Runs an aggregation pipeline and returns the results\n    https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate\n    https://api.mongodb.com/python/current/examples/aggregation.html\n    \"\"\"\n    if mongo_db:\n        collection = self.client[mongo_db][mongo_collection]\n    else:\n        collection = self.client[self.default_db][mongo_collection]\n    \n    return list(collection.aggregate(aggregate_query, **kwargs))\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Checks that a key matching a wildcard expression exists in a bucket\n\n    :param wildcard_key: the path to the key\n    :type wildcard_key: str\n    :param bucket_name: the name of the bucket\n    :type bucket_name: str\n    :param delimiter: the delimiter marks key hierarchy\n    :type delimiter: str\n    \"\"\"\n\n    s3 = boto3.client('s3')\n    if not bucket_name:\n        bucket_name",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Processes the log files and extracts useful information out of it.\n\n    If the deploy-mode is 'client', log the output of the submit command as those\n    are the output logs of the Spark worker directly.\n\n    Remark: If the driver needs to be tracked for its status, the log-level of the\n    spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)\n\n    :param itr: An iterator which iterates over the input of the subprocess\n    \"\"\"\n    for line in itr:\n        decoded_line",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Read logs of given task instance and try_number from GCS.\n    If failed, read the log from task instance host machine.\n    :param ti: task instance object\n    :param try_number: task instance try_number to read logs from\n    :param metadata: log metadata,\n                     can be used for steaming log reading and auto-tailing.\n    \"\"\"\n    try:\n        # Attempt to read log from GCS\n        log = self.gcs_client.read_log(ti, try_number, metadata)\n    except Exception as e:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Execute the bash command in a temporary directory\n    which will be cleaned afterwards\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            result = subprocess.run(self.bash_command, shell=True, cwd=temp_dir, check=True, capture_output=True, text=True)\n            return result.stdout\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(f\"Command '{self.bash_command}' failed with error: {e.stderr}\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    output_path = os.path.join(output_dir, f\"{title}.{ext}\")\n    ffmpeg_command = ['ffmpeg']\n\n    for key, value in params.items():\n        ffmpeg_command.extend([f\"-{key}\", str(value)])\n\n    if stream:\n        ffmpeg_command.extend(['-i', files, output_path])\n    else:\n        ffmpeg_command.extend(['-i', files, '-c', 'copy', output_path])\n\n    subprocess.run(ffmpeg_command, check=True)\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass DAGDirectoryRefresher:\n        self.dag_dir = dag_dir\n        self.refresh_interval = refresh_interval\n        self.last_refresh_time = 0\n        self.file_paths = []\n\n        current_time = time.time()\n        if current_time - self.last_refresh_time > self.refresh_interval:\n            self.file_paths = [os.path.join(self.dag_dir, f) for f in os.listdir(self.dag_dir) if os.path.isfile(os.path.join(self.d",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Finding all tasks that have SLAs defined, and sending alert emails\n    where needed. New SLA misses are also recorded in the database.\n\n    Where assuming that the scheduler runs often, so we only check for\n    tasks that should have succeeded in the past hour.\n    \"\"\"\n\n    now = datetime.utcnow()\n    one_hour_ago = now - timedelta(hours=1)\n\n    # Query for tasks with SL",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Broadcasts the event or samples.\"\"\"\n    event_shape = np.shape(event)\n    samples_shape = np.shape(samples)\n    \n    if len(event_shape) < event_ndims:\n        raise ValueError(\"Event dimensions are less than event_ndims\")\n    \n    if event_shape[-event_ndims:] != samples_shape[-event_ndims:]:\n        raise ValueError(\"Event shape and samples shape do not match in the last event_ndims dimensions\")\n    \n    broadcast_shape = np.broadcast_shapes(event_shape[:-event_ndims],",
        "rewrite": ""
    },
    {
        "original": "\n\n    method, endpoint = endpoint_info\n    url = f\"https://api.example.com/{endpoint}\"\n    headers = {'Content-Type': 'application/json'}\n    retries = 3\n\n    for attempt in range(retries):\n        try:\n            if method.upper() == 'GET':\n                response = requests.get(url, headers=headers, json=json)\n            elif method.upper() == 'POST':\n                response = requests.post(url, headers=headers, json=json)\n            else:\n                raise AirflowException(f\"",
        "rewrite": ""
    },
    {
        "original": "\n\n    file_path = f\"{data_dir}/{info_file}\"\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n    \n    point_ids = [int(line.split()[0]) for line in lines]\n    return tf.constant(point_ids)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Build the transition matrix for a semi-local linear trend model.\n\n    Args:\n    autoregressive_coef (float): The autoregressive coefficient.\n\n    Returns:\n    np.ndarray: The transition matrix.\n    \"\"\"\n    return np.array([\n        [1, 1],\n        [0, autoregressive_coef]\n    ])\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass DeepExponentialFamilyVariational(tf.keras.Model):\n        super(DeepExponentialFamilyVariational, self).__init__()\n        self.data_size = data_size\n        self.feature_size = feature_size\n        self.units = units\n\n        self.encoder = tf.keras.Sequential([\n            layers.InputLayer(input_shape=(data_size, feature_size)),\n            layers.Dense(units, activation='relu'),\n            layers.Dense(units, activation='relu')\n        ])\n\n        self.z_mean = layers.Dense(units",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Builds the model function for use in an estimator.\n\n    Arguments:\n        features: The input features for the estimator.\n        labels: The labels, unused here.\n        mode: Signifies whether it is train or test or predict.\n        params: Some hyperparameters as a dictionary.\n        config: The RunConfig, unused here.\n\n    Returns:\n        EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n    \"\"\"\n    # Define the input layer\n    input_layer = tf.feature_column.input_layer(features, params",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Inserts many docs into a mongo collection.\n    https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_many\n    \"\"\"\n    if mongo_db:\n        collection = mongo_db[mongo_collection]\n    else:\n        collection = self.db[mongo_collection]\n    \n    result = collection.insert_many(docs, **kwargs)\n    return result.inserted_ids\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass ModelLoader:\n        self.model_path = model_path\n        self.model = None\n\n        \"\"\"Checks existence of the model file, loads the model if the file exists\"\"\"\n        if os.path.exists(self.model_path):\n            with open(self.model_path, 'rb') as file:\n                self.model = pickle.load(file)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n                           length=8, shuffle=False, fake_data=False):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_png(image, channels=channels)\n        return image\n\n        return tf.random.uniform([length, 64, 64, channels]), 0, 0, 0, 0, 0, 'skin', 'hair', 'top', 'pants', 'action'\n\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Model:\n        self.latent_size = latent_size\n        self.dense_layer = tf.keras.layers.Dense(latent_size * 2)  # For mean and log_std\n\n        batch_shape = tf.shape(inputs)[:-1]\n        hidden_size = inputs.shape[-1]\n        \n        # Flatten the inputs to apply the dense layer\n        flat_inputs = tf.reshape(inputs, [-1, hidden_size])\n        \n        # Apply the dense layer to get the parameters for the",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    if not isinstance(num_components, int) or num_components <= 0:\n        raise ValueError(\"num_components must be a positive integer.\")\n    if not isinstance(event_shape, tuple):\n        raise ValueError(\"event_shape must be a tuple.\")\n    \n    event_size = 1\n    for dim in event_shape:\n        event_size *= dim\n    \n    return num_components * (event_size + 1)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    keyfile_path = extras.get('keyfile_path')\n    keyfile_json = extras.get('keyfile_json')\n\n    if keyfile_path:\n        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = keyfile_path\n    elif keyfile_json:\n        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.json') as temp_file:\n            temp_file.write(keyfile_json)\n            temp_file_path = temp_file.name\n        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = temp_file_path",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Stable evaluation of `Log[exp{big} - exp{small}]`.\n\n    To work correctly, we should have the pointwise relation:  `small <= big`.\n\n    Args:\n        big: Floating-point `Tensor`\n        small: Floating-point `Tensor` with same `dtype` as `big` and broadcastable\n            shape.\n\n    Returns:\n        `Tensor` of same `dtype` of `big` and broadcast shape.\n    \"\"\"\n    return big + tf.math.log1p(-tf.exp",
        "rewrite": ""
    },
    {
        "original": "\n\n    if command == 'start':\n        print(\"Starting the process...\")\n    elif command == 'stop':\n        print(\"Stopping the process...\")\n    else:\n        print(f\"Unknown command: {command}\")\n\n    \"\"\"\n    Parse options and process commands\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Process some commands.\")\n    parser.add_argument('command', type=str, help='Command to execute')\n    \n    args = parser.parse_args()\n    process_command(args.command)\n\nif __name__ == \"__main__\":\n    _main()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Takes a cursor, and writes results to a local file.\n\n    :return: A dictionary where keys are filenames to be used as object\n        names in GCS, and values are file handles to local files that\n        contain the data for the GCS objects.\n    \"\"\"\n\n    result_files = {}\n    headers = [desc[0] for desc in cursor.description]\n\n    while True:\n        rows = cursor.fetchmany(size=1000)\n        if not rows:\n            break\n\n        temp_file = tempfile.Named",
        "rewrite": ""
    },
    {
        "original": "\n\n            transpose_a=False, transpose_b=False,\n            adjoint_a=False, adjoint_b=False,\n            a_is_sparse=False, b_is_sparse=False,\n            name=None):  # pylint: disable=unused-argument\n    \"\"\"Numpy matmul wrapper.\"\"\"\n    if transpose_a and adjoint_a:\n        raise ValueError(\"Only one of transpose_a and adjoint_a can be True.\")\n    if transpose_b and adjoint_b:\n        raise ValueError(\"Only one of transpose_b and adjoint_b can be True.\")\n    \n    if transpose_a",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns a map of task instance key to task instance object for the tasks to\n    run in the given dag run.\n\n    :param dag_run: the dag run to get the tasks from\n    :type dag_run: airflow.models.DagRun\n    :param session: the database session object\n    :type session: sqlalchemy.orm.session.Session\n    \"\"\"\n\n    task_instances = (\n        session.query(TaskInstance)\n        .filter(TaskInstance.dag_id == dag_run.dag_id",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Sets tasks instances to skipped from the same dag run.\n\n    :param dag_run: the DagRun for which to set the tasks to skipped\n    :param execution_date: execution_date\n    :param tasks: tasks to skip (not task_ids)\n    :param session: db session to use\n    \"\"\"\n    if session is None:\n        session = provide_session()\n\n    task_ids = [task.task_id for",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"\n    if isinstance(config, (str, Path)):\n        with open(config, 'r') as file:\n            config = json.load(file)\n    \n    # Assuming the config is now a dictionary\n    model_name = config.get('model_name')\n    model_params = config.get('model_params', {})\n    \n    # Placeholder for model loading and interaction logic\n    print(f\"Loading model: {model_name} with parameters",
        "rewrite": ""
    },
    {
        "original": "\n\n                              dense_vector,\n                              validate_args=False,\n                              name=None,\n                              **kwargs):\n    with tf.name_scope(name or \"sparse_or_dense_matvecmul\"):\n        if validate_args:\n            matrix_shape = tf.shape(sparse_or_dense_matrix)\n            vector_shape = tf.shape(dense_vector)\n            assertions = [\n                tf.assert_equal(matrix_shape[:-2], vector_shape[:-1],\n                                message=\"Batch shapes must be equal\"),\n                tf.assert_equal(matrix_shape[-1], vector_shape[-1],\n                                message=\"Matrix and vector",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Populate settings directory with default settings files\n\n    Args:\n        force: if ``True``, replace existing settings files with default ones\n\n    Returns:\n        ``True`` if any files were copied and ``False`` otherwise\n    \"\"\"\n    settings_dir = 'settings'\n    default_files = {\n        'config.json': 'default_config.json',\n        'preferences.json': 'default_preferences.json'\n    }\n    \n    if not os.path.exists(settings_dir):\n        os.makedirs(settings_dir)\n    \n    files_copied",
        "rewrite": ""
    },
    {
        "original": "\n\n    url = f\"https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}\"\n    headers = {\n        \"Accept\": \"application/json;pk=YOUR_POLICY_KEY\",\n        \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"\n    }\n    \n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        return []\n    \n    data = response.json()\n    streams = data.get('sources', [])\n    \n    https_streams = [stream for stream in streams if stream",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"self, str->None\n    \n    Keyword arguments:\n    self: self\n    vid: The video ID for BokeCC cloud, something like\n    FE3BB999594978049C33DC5901307461\n    \n    Calls the prepare() to download the video.\n    \n    If no title is provided, this method shall try to find a proper title\n    with the information providin within the\n    returned content of the API.\"\"\"\n    \n    if not vid:\n        raise",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"DeepPavlov console configuration utility.\"\"\"\n\n        with open(config_path, 'r') as file:\n            return json.load(file)\n\n        with open(config_path, 'w') as file:\n            json.dump(config, file, indent=4)\n\n    parser = argparse.ArgumentParser(description=\"DeepPavlov console configuration utility.\")\n    parser.add_argument('config_path', type=str, help='Path to the configuration file.')\n    parser.add_argument('--set', nargs",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Returns a BigQuery service object.\n    \"\"\"\n    credentials = service_account.Credentials.from_service_account_file('path/to/your/service-account-file.json')\n    service = build('bigquery', 'v2', credentials=credentials)\n    return service\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    block_blob_service = BlockBlobService(account_name='your_account_name', account_key='your_account_key')\n    block_blob_service.get_blob_to_path(container_name, blob_name, file_path, **kwargs)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n             source_project_dataset_tables,\n             destination_project_dataset_table,\n             write_disposition='WRITE_EMPTY',\n             create_disposition='CREATE_IF_NEEDED',\n             labels=None):\n    client = bigquery.Client()\n\n    if isinstance(source_project_dataset_tables, str):\n        source_project_dataset_tables = [source_project_dataset_tables]\n\n    sources = [bigquery.TableReference.from_string(table) for table in source_project_dataset_tables]\n    destination = bigquery.TableReference.from_string(destination_project_dataset_table)\n\n    job_config = bigquery.CopyJobConfig(\n        write_disposition=write",
        "rewrite": ""
    },
    {
        "original": "\n\n# Assuming you have a database URL\nDATABASE_URL = \"sqlite:///example.db\"\n\n# Create an engine and a sessionmaker\nengine = create_engine(DATABASE_URL)\nSession = sessionmaker(bind=engine)\n\n    @wraps(func)\n        session = kwargs.get('session')\n        if session is None:\n            session = Session()\n            kwargs['session'] = session\n            try:\n                result = func(*args, **kwargs)\n                session.commit()\n               ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Dump the trained weights from a model to a HDF5 file.\n\n    Args:\n        tf_save_dir (str): Directory where the TensorFlow model is saved.\n        outfile (str): Path to the output HDF5 file.\n        options (dict, optional): Additional options for saving weights.\n    \"\"\"\n    # Load the model\n    model = tf.keras.models.load_model(tf_save_dir)\n\n    # Create HDF5 file\n    with h5py.File(outfile, 'w') as",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Add a learning rate scheduler to the contained `schedules`\n\n    :param scheduler: learning rate scheduler to be add\n    :param max_iteration: iteration numbers this scheduler will run\n    \"\"\"\n    if not hasattr(self, 'schedules'):\n        self.schedules = []\n    self.schedules.append((scheduler, max_iteration, bigdl_type))\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Loads a tab-delimited file into a database table\n    \"\"\"\n    conn = None\n    try:\n        # Connect to your postgres DB\n        conn = psycopg2.connect(\"dbname=yourdbname user=yourusername password=yourpassword host=yourhost\")\n\n        # Open a cursor to perform database operations\n        cur = conn.cursor()\n\n        # Open the file\n        with open(tmp_file, 'r') as f:\n            # Use the copy_expert method to load the file into the table\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Saves model to the save_path, provided in config. The directory is\n    already created by super().__init__, which is called in __init__ of this class\"\"\"\n    save_path = self.config.get('save_path')\n    if save_path:\n        with open(save_path, 'wb') as f:\n            pickle.dump(self.model, f)\n    else:\n        raise ValueError(\"Save path not provided in config\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass YourClassNameHere:\n    @staticmethod\n        \"\"\"\n        Extract hadoop sequence files from an HDFS path as ImageFrame\n        :param url: sequence files folder path\n        :param sc: spark context\n        :param class_num: class number of data\n        :param partition_num: partition number, default: Engine.nodeNumber() * Engine.coreNumber()\n        \"\"\"\n        return call",
        "rewrite": ""
    },
    {
        "original": "\n                  state,\n                  step_size,\n                  target_log_prob=None,\n                  grads_target_log_prob=None,\n                  maybe_expand=False,\n                  state_gradients_are_stopped=False):\n  \"\"\"Helper which processes input args to meet list-like assumptions.\"\"\"\n  if not isinstance(state, (list, tuple)):\n    state = [state]\n  if not isinstance(step_size, (list, tuple)):\n    step_size = [step_size] * len(state)\n  if target_log_prob is None:\n    target_log_prob = target_log_prob_fn(*state)\n  if grads_target_log_prob is None",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    if name is None:\n        raise ValueError(\"name must be provided\")\n    \n    if name == \"GaussianMixture\":\n        # For Gaussian Mixture, we need means, variances, and mixing coefficients\n        return num_components * (2 * event_size + 1)\n    elif name == \"Categorical\":\n        # For Categorical, we need the probabilities for each category\n        return event_size\n    elif name == \"Bernoulli\":\n        # For Bern",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    \u041e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u0442\u043e\u043f\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430\n    \"\"\"\n        visited.add(node)\n        for neighbor in trie[node]:\n            if neighbor not in visited:\n                dfs(neighbor, visited, stack)\n        stack.append(node)\n\n    visited = set()\n    stack = []\n\n    for node in trie:\n        if node not in visited:\n            dfs(node, visited, stack)\n\n    return stack[::-1]\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Construct the Slack message. All relevant parameters are combined here to a valid\n    Slack json message\n    :return: Slack message (str) to send\n    \"\"\"\n    message = {\n        \"channel\": self.channel,\n        \"username\": self.username,\n        \"text\": self.text,\n        \"icon_emoji\": self.icon_emoji,\n        \"attachments\": self.attachments\n    }\n    return json.dumps(message)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Get parameters for affine transformation\n\n    Returns:\n        sequence: params to be passed to the affine transformation\n    \"\"\"\n    angle = random.uniform(degrees[0], degrees[1])\n    max_dx = translate[0] * img_size[0]\n    max_dy = translate[1] * img_size[1]\n    translations = (random.uniform(-max_dx, max_dx), random.uniform(-max_dy, max_dy))\n    scale = random.uniform(scale_ranges[0], scale_ranges[",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n    \n    # Create TensorFlow Dataset objects for training and heldout data\n    train_dataset = tf.data.Dataset.from_tensor_slices((mnist_data.train.images, mnist_data.train.labels))\n    heldout_dataset = tf.data.Dataset.from_tensor_slices((mnist_data.validation.images, mnist_data.validation.labels))\n    \n    # Shuffle and batch the datasets\n    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n    held",
        "rewrite": ""
    },
    {
        "original": "\nclass _PrettyDict(dict):\n        return '{' + ', '.join(f'{k}: {v}' for k, v in self.items()) + '}'\n\n    \"\"\"Recursively replace `dict`s with `_PrettyDict`.\"\"\"\n    if isinstance(x, dict):\n        return _PrettyDict({k: _recursively_replace_dict_for_pretty_dict(v) for k, v in x.items()})\n    elif isinstance(x, list):\n        return [_recursively_replace_dict_for_pretty_dict(i) for i in x]\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p].\"\"\"\n    \n    with tf.name_scope(name):\n        a = tf.convert_to_tensor(a, name=\"a\")\n        p = tf.convert_to_tensor(p, dtype=tf.int32, name=\"p\")\n        \n        # Create a range tensor from 0 to p\n        seq = tf.range(tf.cast(p, a.dtype), dtype=a.dtype)\n        \n        # Expand dimensions to match the shape of `a`\n",
        "rewrite": ""
    },
    {
        "original": "\n\nDEFAULT_TIME_TO_WAIT_AFTER_SIGTERM = 5\n\n    try:\n        # Send the initial signal to the process group\n        os.killpg(os.getpgid(pid), sig)\n    except ProcessLookupError:\n        log.warning(f\"Process group {pid} does not exist.\")\n        return\n\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            # Check if the process group is still alive",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"OneHotCategorical helper computing probs, cdf, etc over its support.\"\"\"\n    with tf.name_scope(name or 'eval_all_one_hot'):\n        # Create a tensor of indices for the categorical distribution\n        indices = tf.range(dist.event_shape_tensor()[-1])\n        # Convert indices to one-hot encoded format\n        one_hot = tf.one_hot(indices, depth=dist.event_shape_tensor()[-1])\n        # Apply the function to the one-hot encoded tensor\n        return fn(one_hot)\n",
        "rewrite": ""
    },
    {
        "original": "\n\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\n                   num_hidden_layers=2,\n                   seed=None,\n                   dtype=tf.float32):\n  \"\"\"Creates an stacked IAF bijector.\n\n  This bijector operates on vector-valued events.\n\n  Args:\n    total_event_size: Number of dimensions to operate over.\n    num_hidden_layers: How many hidden layers to use in each IAF.\n    seed: Random seed for the initializers.\n    dtype: DType for the",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Defines the security context\"\"\"\n    security_context = {\n        'user': self.current_user,\n        'roles': self.user_roles,\n        'permissions': self.user_permissions,\n        'is_authenticated': self.is_authenticated,\n        'session_id': self.session_id,\n    }\n    return security_context\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Adds the DAG into the bag, recurses into sub dags.\n    Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags\n    \"\"\"\n    if dag in self.bagged_dags:\n        raise AirflowDagCycleException(f\"Cycle detected in DAG: {dag.dag_id}\")\n    \n    self.bagged_dags.add(dag)\n    \n    for subdag in dag.subdags:\n        self.bag_dag(subdag, dag, root_dag)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    stop the input gradient of layers that match the given names\n    their input gradient are not computed.\n    And they will not contributed to the input gradient computation of\n    layers that depend on them.\n    :param stop_layers:  an array of layer names\n    :param bigdl_type:\n    :return:\n    \"\"\"\n    for layer in self.layers:\n        if layer.name in stop_layers:\n            layer.requires_grad = False\n",
        "rewrite": ""
    },
    {
        "original": "\n              current_state,\n              current_grads_target_log_prob,\n              current_momentum,\n              step_size):\n  \"\"\"Runs one step of leapfrog integration.\"\"\"\n  # Half step update for momentum\n  new_momentum = [m + 0.5 * step_size * g for m, g in zip(current_momentum, current_grads_target_log_prob)]\n\n  # Full step update for position\n  new_state = [s + step_size * m for s, m in zip(current_state, new_momentum)]\n\n  # Compute new gradients\n  new_target",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    \u041d\u0430\u0445\u043e\u0434\u0438\u0442 \u0432\u0441\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f s = s_1 ... s_m \u043d\u0430 \u0441\u043b\u043e\u0432\u0430\u0440\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 s_1, ..., s_m\n    \u0434\u043b\u044f m <= max_count\n    \"\"\"\n        # This function should check if the word is in the dictionary\n        # For example, it could be implemented as:\n        # return word in dictionary\n        pass\n\n        if start == len(s):\n            if len(path) <= max_count:\n                result.append(path[:])\n            return",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Fetches a field from extras, and returns it. This is some Airflow\n    magic. The google_cloud_platform hook type adds custom UI elements\n    to the hook page, which allow admins to specify service_account,\n    key_path, etc. They get formatted as shown below.\n    \"\"\"\n    if 'extra__google_cloud_platform__{}'.format(f) in self.extra_dejson:\n        return self.extra_dejson['extra__google_cloud_platform__{}'.format(f)]\n    return default\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    return tfp.distributions.MultivariateNormalDiag(\n        loc=params,\n        scale_diag=tf.ones_like(params),\n        validate_args=validate_args,\n        name=name\n    )\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"\n    if tol is None:\n        return tf.constant(0, dtype=dtype)\n    tol = tf.convert_to_tensor(tol, dtype=dtype)\n    if validate_args:\n        tol = tf.debugging.assert_non_negative(tol)\n    return tol\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    dag_bag = DagBag()\n    dag = dag_bag.get_dag(dag_id)\n    if dag is None:\n        return None\n    task = dag.get_task(task_id)\n    if task is None:\n        return None\n    execution_date = datetime.strptime(execution_date, '%Y-%m-%d')\n    task_instance = TaskInstance(task, execution_date)\n    task_instance.refresh_from_db()\n    return task_instance.state",
        "rewrite": ""
    },
    {
        "original": "\n\n    if session is None:\n        session = create_session()\n\n    # Initialize the maps\n    total_map = defaultdict(int)\n    state_map = defaultdict(int)\n\n    # Query to get the counts of task instances grouped by dag_id and task_id\n    query = session.query(\n        TaskInstance.dag_id,\n        TaskInstance.task_id,\n        func.count(TaskInstance.task_id)\n    ).group_by(\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = storage.Client()\n    bucket = client.bucket(bucket_name, user_project=user_project)\n    acl = bucket.acl\n    acl.entity(entity).grant(role)\n    acl.save()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Check if remote_log_location exists in remote storage\n    :param remote_log_location: log's location in remote storage\n    :return: True if location exists else False\n    \"\"\"\n\n    try:\n        blob_service_client = BlobServiceClient.from_connection_string(self.connection_string)\n        container_name, blob_name = self._parse_wasb_url(remote_log_location)\n        container_client = blob_service_client.get_container_client(container_name)\n        blob_client = container_client.get_blob_client(blob_name)\n        return",
        "rewrite": ""
    },
    {
        "original": "\n        validation_split=0., validation_data=None, shuffle=True,\n        class_weight=None, sample_weight=None, initial_epoch=0, is_distributed=False):\n    \"\"\"Optimize the model by the given options\n\n    :param x: ndarray or list of ndarray for local mode.\n              RDD[Sample] for distributed mode\n    :param y: ndarray or list of ndarray for local mode and would be None for cluster mode.\n        is_distributed: used to control run in local or",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    \u0421\u043f\u0443\u0441\u043a \u0438\u0437 \u0432\u0435\u0440\u0448\u0438\u043d\u044b curr \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0435 s \u0441 \u043a\u044d\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c\n    \"\"\"\n    for char in s:\n        if char in self.cache[curr]:\n            curr = self.cache[curr][char]\n        else:\n            if char in self.edges[curr]:\n                next_node = self.edges[curr][char]\n                self.cache[curr][char] = next_node\n                curr = next_node\n            else:\n                return None\n    return curr\n",
        "rewrite": ""
    },
    {
        "original": "\n\ntfd = tfp.distributions\n\n    \"\"\"Unnormalized log-posterior for the text messages model.\"\"\"\n    n = len(data)\n    log_prob = (alpha - 1) * tf.math.log(data).sum() + (beta - 1) * tf.math.log(1 - data).sum()\n    log_prob += (alpha + beta - 2) * tf.math.log(data).sum()\n    return log_prob\n\n   ",
        "rewrite": ""
    },
    {
        "original": "\n                       next_position,\n                       current_objective,\n                       next_objective,\n                       next_gradient,\n                       grad_tolerance,\n                       f_relative_tolerance,\n                       x_tolerance):\n  \"\"\"Checks if the algorithm satisfies the convergence criteria.\"\"\"\n  # Check gradient tolerance\n  if abs(next_gradient) < grad_tolerance:\n    return True\n\n  # Check relative function tolerance\n  if abs(next_objective - current_objective) < f_relative_tolerance * (abs(current_objective) + f_relative_tolerance):\n    return True\n\n  # Check position tolerance\n  if abs(next_position",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Factory for making summary statistics, eg, mean, mode, stddev.\"\"\"\n        if attr == 'mean':\n            return np.mean(data)\n        elif attr == 'median':\n            return np.median(data)\n        elif attr == 'mode':\n            return stats.mode(data).mode[0]\n        elif attr == 'stddev':\n            return np.std(data)\n        elif attr == 'variance':\n            return np.var(data)\n        else:\n            raise ValueError(f\"Unknown attribute:",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Convenience function which statically broadcasts shape when possible.\n\n    Args:\n        shape1:  `1-D` integer `Tensor`.  Already converted to tensor!\n        shape2:  `1-D` integer `Tensor`.  Already converted to tensor!\n        name:  A string name to prepend to created ops.\n\n    Returns:\n        The broadcast shape, either as `TensorShape` (if broadcast can be done\n        statically), or as a `Tensor`.\n    \"\"\"\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Component:\n        self.params = params\n        self.mode = mode\n        self.serialized = serialized\n        self.extra_args = kwargs\n\n    @classmethod\n        return cls(params, mode, serialized, **kwargs)\n",
        "rewrite": ""
    },
    {
        "original": "\n    if project_id is None:\n        project_id = self.gcp_connection.default_project_id\n\n    url = f\"https://www.googleapis.com/sql/v1beta4/projects/{project_id}/instances/{instance}/databases/{database}\"\n    response = self.gcp_connection.request(\"GET\", url)\n    \n    if response.status_code != 200:\n        raise Exception(f\"Failed to retrieve database: {response.content}\")\n    \n    return response.json()\n",
        "rewrite": ""
    },
    {
        "original": "\nclass DecayType:\n    @classmethod\n        decay_types = {\n            \"linear\": 0,\n            \"cosine\": 1,\n            \"exponential\": 2,\n            \"onecycle\": 3,\n            \"trapezoid\": 4\n        }\n        \n        if label.startswith(\"polynomial\"):\n            try:\n                _, k = label.split(',')\n                return int(k.strip())\n            except ValueError:\n                raise ValueError(\"Invalid polynomial format. Use 'polynomial, K' where K is an integer.\")\n        \n        if",
        "rewrite": ""
    },
    {
        "original": "\n\nclass AuthenticationError(Exception):\n    pass\n\nclass PasswordUser:\n    # Assuming PasswordUser is a SQLAlchemy model with fields 'username' and 'password_hash'\n    pass\n\n    try:\n        user = session.query(PasswordUser).filter_by(username=username).one()\n        if not check_password_hash(user.password_hash, password):\n            raise AuthenticationError(\"Invalid password\")\n        return user\n    except NoResultFound:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Returns a mssql connection object\n    \"\"\"\n    server = 'your_server'\n    database = 'your_database'\n    username = 'your_username'\n    password = 'your_password'\n    conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n    return pyodbc.connect(conn_str)\n",
        "rewrite": ""
    },
    {
        "original": "\n                          exchange_proposed_n, sampled_replica_states,\n                          sampled_replica_results):\n    \"\"\"Get list of TensorArrays holding exchanged states, and zeros.\"\"\"\n    exchanged_states = []\n    for i in range(len(old_states)):\n        exchanged_state = tf.where(exchange_proposed,\n                                   sampled_replica_states[exchange_proposed_n],\n                                   old_states[i])\n        exchanged_states.append(exchanged_state)\n    return exchanged_states\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Build fake MNIST-style data for unit testing.\"\"\"\n    # MNIST images are 28x28 pixels\n    image_shape = (28, 28)\n    # Generate random pixel values for images\n    images = np.random.randint(0, 256, size=(num_examples, *image_shape), dtype=np.uint8)\n    # Generate random labels (0-9) for each image\n    labels = np.random.randint(0, 10, size=(num_examples,), dtype=np.uint8)\n    return images, labels\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not isinstance(from_structure, Iterable) or isinstance(from_structure, (str, bytes)):\n        # If from_structure is a singleton, broadcast it to match to_structure\n        if isinstance(to_structure, dict):\n            return {k: from_structure for k in to_structure}\n        elif isinstance(to_structure, (list, tuple)):\n            return type(to_structure)(from_structure for _ in to_structure)\n        else:\n            return from_structure\n    else:\n        return from_structure\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if isinstance(size, int):\n        size = (size, size)\n    \n    w, h = img.size\n    crop_w, crop_h = size\n    \n    # Define the coordinates for the 5 crops\n    tl = img.crop((0, 0, crop_w, crop_h))\n    tr = img.crop((w - crop_w, 0, w, crop_h))\n    bl = img.crop((0, h - crop_h, crop_w, h))\n    br = img.crop((w - crop_w,",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Respond to executor events.\n    \"\"\"\n\n    @provide_session\n        executor = self.executor\n        if not executor.queued_tasks:\n            return\n\n        for key, event in executor.get_event_buffer().items():\n            dag_id, task_id, execution_date, try_number = key\n            if event ==",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\" Convert Python object into Java \"\"\"\n    if obj is None:\n        return None\n    elif isinstance(obj, bool):\n        return gateway.jvm.java.lang.Boolean(obj)\n    elif isinstance(obj, int):\n        return gateway.jvm.java.lang.Long(obj)\n    elif isinstance(obj, float):\n        return gateway.jvm.java.lang.Double(obj)\n    elif isinstance(obj, str):\n        return gateway.jvm.java.lang.String(obj)\n    elif isinstance(obj, list):\n        array_list = gateway.jvm.java.util.ArrayList()\n        for item in obj:\n            array_list.add(_py2java(g",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Update the BGFS state by computing the next inverse hessian estimate.\"\"\"\n    s = next_state['x'] - prev_state['x']\n    y = next_state['grad'] - prev_state['grad']\n    rho_inv = np.dot(y, s)\n    \n    if rho_inv == 0:\n        raise ValueError(\"Division by zero in BFGS update.\")\n    \n    rho = 1.0 / rho_inv\n    I = np.eye(len(s))\n    V = I - rho * np.outer(s,",
        "rewrite": ""
    },
    {
        "original": "\n\n                           dense_b,\n                           validate_args=False,\n                           name=None,\n                           **kwargs):\n  \"\"\"Returns (batched) matmul of a SparseTensor (or Tensor) with a Tensor.\"\"\"\n  with tf.name_scope(name or \"sparse_or_dense_matmul\"):\n    if isinstance(sparse_or_dense_a, tf.SparseTensor):\n      product = tf.sparse.sparse_dense_matmul(sparse_or_dense_a, dense_b, **kwargs)\n    else:\n      product = tf.matmul(sparse_or_dense_a, dense_b, **",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Check that a shape Tensor is int-type and otherwise sane.\"\"\"\n    if validate_args:\n        shape = tf.convert_to_tensor(shape, dtype=tf.int32)\n        tf.debugging.assert_type(shape, tf.int32, message=\"Shape must be an int32 tensor\")\n        tf.debugging.assert_non_negative(shape, message=\"Shape values must be non-negative\")\n    return shape\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass DAGParser:\n        self.parse_interval = parse_interval\n\n    async def parse_dag_files(self):\n        while True:\n            print(\"Parsing DAG files...\")\n            # Add your DAG parsing logic here\n            await asyncio.sleep(self.parse_interval)\n\n        \"\"\"\n        Parse DAG files repeatedly in a standalone loop.\n        \"\"\"\n        asyncio.run(self.parse_dag_files())\n\n# Example usage:\n# parser = DAGParser()\n# parser.start_in_async()\n",
        "rewrite": ""
    },
    {
        "original": "\n\n                    dryrun=False, cc=None, bcc=None,\n                    mime_subtype='mixed', mime_charset='utf-8',\n                    **kwargs):\n    msg = MIMEMultipart(mime_subtype)\n    msg['From'] = kwargs.get('from', 'no-reply@example.com')\n    msg['To'] = to\n    msg",
        "rewrite": ""
    },
    {
        "original": "\n                       static_shape,\n                       dynamic_shape,\n                       static_target_shape,\n                       dynamic_target_shape=None):\n    \"\"\"Check that source and target shape match, statically if possible.\"\"\"\n    if static_shape != static_target_shape:\n        raise ValueError(f\"Static shapes do not match for {name}: \"\n                         f\"{static_shape} vs {static_target_shape}\")\n    \n    if dynamic_target_shape is not None and dynamic_shape != dynamic_target_shape:\n        raise ValueError(f\"Dynamic shapes do not match for {name}: \"\n                         f\"{dynamic_shape} vs {dynamic_target_shape}\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Parse the JSON content\n    course_data = json.loads(json_api_content)\n    \n    # Extract the specific part of the course\n    try:\n        topic = course_data['topics'][tIndex]\n        part = topic['parts'][pIndex]\n    except IndexError:\n        raise ValueError(\"Invalid topic index or part index\")\n    \n    # Get the download URL and filename\n    video_url = part",
        "rewrite": ""
    },
    {
        "original": "\n\n    with create_session() as session:\n        session.query(DagModel).update({DagModel.is_paused: is_paused})\n        session.commit()\n",
        "rewrite": ""
    },
    {
        "original": "\n        self, location, product, project_id=None, product_id=None, retry=None, timeout=None, metadata=None\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductCreateOperator`\n        \"\"\"\n        client = vision.ProductSearchClient()\n\n        parent = f\"projects/{project_id}/locations/{location}\"\n\n        if product_id:\n            response = client.create_product(\n                parent=parent,\n                product=product,\n                product_id=product_id,\n                retry=retry,\n                timeout=timeout,\n                metadata=",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Make func to expand left/right (of axis) dims of tensors shaped like x.\"\"\"\n        shape = list(x.shape)\n        for i in range(len(shape)):\n            if i < axis:\n                shape[i] = y_ref.shape[i]\n            elif i > axis:\n                shape[i] = y_ref.shape[i + 1]\n        return x.reshape(shape)\n    return expand_fn\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Creates the basic network architecture,\n    transforming word embeddings to intermediate outputs\n    \"\"\"\n\n    # Define a simple feed-forward neural network\n    hidden_layer = tf.keras.layers.Dense(units=128, activation='relu')(word_outputs)\n    dropout_layer = tf.keras.layers.Dropout(rate=0.5)(hidden_layer)\n    output_layer = tf.keras.layers.Dense(units=64, activation='relu')(dropout_layer)\n\n    return output_layer\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\" Helper method that escapes parameters to a SQL query. \"\"\"\n    if not isinstance(s, str):\n        raise TypeError(\"Input must be a string\")\n    # Escape single quotes by replacing them with two single quotes\n    return re.sub(r\"'\", \"''\", s)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns all task reschedules for the task instance and try number,\n    in ascending order.\n\n    :param task_instance: the task instance to find task reschedules for\n    :type task_instance: airflow.models.TaskInstance\n    \"\"\"\n\n    return session.query(TaskReschedule).filter(\n        TaskReschedule.dag_id == task_instance.dag_id,\n        TaskReschedule.task_id == task_instance.task_id,\n        TaskReschedule.execution_date == task_instance.execution_date,\n        TaskReschedule.try",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Transfers a local file to the remote location.\n    If local_full_path_or_buffer is a string path, the file will be read\n    from that location\n    :param remote_full_path: full path to the remote file\n    :type remote_full_path: str\n    :param local_full_path: full path to the local file\n    :type local_full_path: str\n    \"\"\"\n    with open(local_full_path, 'rb') as local_file:\n        with open(remote_full_path, 'wb') as remote_file:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\" Helper method that binds parameters to a SQL query. \"\"\"\n    if isinstance(parameters, dict):\n        for key, value in parameters.items():\n            placeholder = f\":{key}\"\n            if isinstance(value, str):\n                value = f\"'{value}'\"\n            operation = operation.replace(placeholder, str(value))\n    elif isinstance(parameters, (list, tuple)):\n        for value in parameters:\n            placeholder = re.search(r'\\?', operation)\n            if placeholder:\n                if isinstance(value, str):\n                    value = f\"'{value}'\"\n                operation = operation",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Will test the filepath result and test if its size is at least self.filesize\n\n    :param result: a list of dicts returned by Snakebite ls\n    :param size: the file size in MB a file should be at least to trigger True\n    :return: (bool) depending on the matching criteria\n    \"\"\"\n    if size is None:\n        return True\n    \n    size_in_bytes = size * 1024 * 1024\n    \n    for file_info in result:\n        if 'length' in file_info and",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = bigquery.Client()\n    table_ref = client.dataset(dataset_id).table(table_id)\n    table = client.get_table(table_ref)\n    return table.schema\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    sagemaker_client = boto3.client('sagemaker')\n    logs_client = boto3.client('logs')\n\n    # Describe the training job\n    response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n    last_description = response\n    last_describe_job_call = time.time()\n\n    # Print CloudWatch logs\n    for i in range(instance_count):\n        log_stream_name = f\"{stream",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not inplace:\n        tensor = tensor.clone()\n    \n    dtype = tensor.dtype\n    mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n    \n    if mean.ndim == 1:\n        mean = mean.view(-1, 1, 1)\n    if std.ndim == 1:\n        std = std.view(-1, 1, 1)\n    \n    tensor.sub_(mean).div_(std)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n    files = [\n        \"train-images-idx3-ubyte.gz\",\n        \"train-labels-idx1-ubyte.gz\",\n        \"t10k-images-idx3-ubyte.gz\",\n        \"t10k-labels-idx1-ubyte.gz\"\n    ]\n    \n    os.makedirs(location, exist_ok=True)\n    \n    for file in files:\n        file_path = os.path.join",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    transform ImageFeature\n    \"\"\"\n    if bigdl_type == \"float\":\n        # Perform transformation for float type\n        transformed_feature = image_feature.astype('float32')\n    elif bigdl_type == \"double\":\n        # Perform transformation for double type\n        transformed_feature = image_feature.astype('float64')\n    else:\n        raise ValueError(f\"Unsupported bigdl_type: {bigdl_type}\")\n    \n    # Additional transformation logic can be added here\n    \n    return transformed_feature\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Defers an operator overload to `attr`.\n\n    Args:\n        attr: Operator attribute to use.\n\n    Returns:\n        Function calling operator attribute.\n    \"\"\"\n        return getattr(self, attr)(*args, **kwargs)\n    return operator_func\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    get prediction rdd from ImageFrame\n    \"\"\"\n    return self.get_image(key)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Defines any necessary environment variables for the pod executor\"\"\"\n    environment = {\n        'ENV_VAR_1': 'value1',\n        'ENV_VAR_2': 'value2',\n        'ENV_VAR_3': 'value3'\n    }\n    return environment\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    transformImageFrame\n    \"\"\"\n\n    if not isinstance(transformer, ImageFrame):\n        raise TypeError(\"transformer should be an instance of ImageFrame\")\n\n    return transformer.transform(self, bigdl_type)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    dagbag = DagBag()\n    dag = dagbag.get_dag(dag_id)\n    if not dag:\n        raise ValueError(f\"DAG with id {dag_id} not found\")\n\n    execution_date = timezone.utcnow()\n    dag_run = DagRun(\n        dag_id=dag_id,\n        execution_date=execution_date,\n        state=State.RUNNING,\n        run_id=f\"manual__{execution_date.isoformat()}\",\n    )\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n              seq_lengths=None, input_initial_h=None, name='cudnn_gru', reuse=False):\n    with tf.variable_scope(name, reuse=reuse):\n        # Create initial hidden state\n        if input_initial_h is not None:\n            initial_h = input_initial_h\n        elif trainable_initial_states:\n            initial_h = tf.get_variable('initial_h', shape=[n_layers, units.shape[0],",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"The Triangular Csiszar-function in log-space.\n\n    A Csiszar-function is a member of,\n\n    none\n    F = { f:R_+ to R : f convex }.\n    \n\n    The Triangular Csiszar-function is:\n\n    none\n    f(u) = (u - 1)**2 / (1 + u)\n    \n\n    This Csiszar-function induces a symmetric f-Divergence, i.e.,\n    `D_f[p, q] = D_f[q, p",
        "rewrite": ""
    },
    {
        "original": "\n\n    glove_url = f\"http://nlp.stanford.edu/data/glove.6B.zip\"\n    local_zip_file = os.path.join(source_dir, \"glove.6B.zip\")\n    \n    if not os.path.exists(local_zip_file):\n        print(\"Downloading GloVe vectors...\")\n        response = requests.get(glove_url, stream=True)\n        with open(local_zip_file, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=128):\n                f.write(chunk)\n        print(\"Download",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Takes task_instances, which should have been set to queued, and enqueues them\n    with the executor.\n\n    :param simple_task_instances: TaskInstances to enqueue\n    :type simple_task_instances: list[SimpleTaskInstance]\n    :param simple_dag_bag: Should contains all of the task_instances' dags\n    :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag\n    \"\"\"\n    for simple_task_instance in simple_task_instances:\n        dag = simple_d",
        "rewrite": ""
    },
    {
        "original": "\n\n                                      dtype,\n                                      validate_args,\n                                      name=None):\n  with tf.name_scope(name or 'process_quadrature_grid_and_probs'):\n    if quadrature_grid_and_probs is None:\n      grid, probs = np.polynomial.hermite.hermgauss(deg=8)\n      grid = tf.convert_to_tensor(grid, dtype=dtype)\n      probs = tf.convert_to_tensor(probs, dtype=dtype)\n    else:\n      grid, probs = quadrature_grid_and_probs\n      grid =",
        "rewrite": ""
    },
    {
        "original": "\n                  log_likelihood=None, description='log_likelihood'):\n  \"\"\"Processes input args to meet list-like assumptions.\"\"\"\n  if log_likelihood is None:\n    log_likelihood = [log_likelihood_fn(s) for s in state]\n  elif not isinstance(log_likelihood, (list, tuple)):\n    raise TypeError(f'{description} must be a list or tuple.')\n  return log_likelihood\n",
        "rewrite": ""
    },
    {
        "original": "\n\n__all__ = ['AlexNet', 'alexnet']\n\nmodel_urls = {\n    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n}\n\nclass AlexNet(nn.Module):\n\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Returns json compatible state of the ButtonsFrame instance.\n\n    Returns json compatible state of the ButtonsFrame instance including\n    all nested buttons.\n\n    Returns:\n        control_json: Json representation of ButtonsFrame state.\n    \"\"\"\n    control_json = {\n        'buttons': [button.json() for button in self.buttons]\n    }\n    return control_json\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batchwise KL divergence KL(d1 || d2) with d1 and d2 Beta.\n\n    Args:\n        d1: instance of a Beta distribution object.\n        d2: instance of a Beta distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_beta_beta\".\n\n    Returns:\n        Batchwise KL(d1 || d2)\n    \"\"\"\n    with tf.name_scope(name or \"kl_beta_beta\"):\n        return (",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Harvest DAG parsing results from result queue and sync metadata from stat queue.\n    :return: List of parsing result in SimpleDag format.\n    \"\"\"\n    simple_dags = []\n\n    # Harvest results from the result queue\n    while not self.result_queue.empty():\n        try:\n            simple_dag = self.result_queue.get_nowait()\n            simple_dags.append(simple_dag)\n        except queue.Empty:\n            break\n\n    # Sync metadata from the stat queue\n    while not self.stat_queue.empty():\n        try:\n            stat = self.stat_queue.get_nowait()\n           ",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    filepath = os.path.join(data_dir, filename)\n    if not os.path.exists(filepath):\n        urllib.request.urlretrieve(url, filepath)\n        with zipfile.ZipFile(filepath, 'r') as zip_ref:\n            zip_ref.extractall(data_dir)\n\n    url = 'http://files.grouplens.org/datasets/movielens/ml-",
        "rewrite": ""
    },
    {
        "original": "\n\nclass DAGProcessor:\n        self.dags = dags\n\n        # Placeholder for the actual parsing and task generation logic\n        print(f\"Processing DAG: {dag}\")\n\n        with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n            futures = [executor.submit(self.parse_and_generate_tasks, dag) for dag in self.dags]\n            for future in futures:\n                future.result()  # Wait for all futures to complete\n\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Convert native python ``datetime.date`` object to a format supported by the API\n    \"\"\"\n    if isinstance(field_date, date):\n        return {\n            \"year\": field_date.year,\n            \"month\": field_date.month,\n            \"day\": field_date.day\n        }\n    else:\n        raise ValueError(\"Input must be a datetime.date object\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\" function to check expected type and raise\n    error if type is not correct \"\"\"\n    if not isinstance(value, expected_type):\n        raise TypeError(f\"Expected {expected_type} for key '{key}', but got {type(value)}.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Return a model builder or all of the model builders known to the\n    h2o cluster. The model builders are contained in a dictionary\n    called \"model_builders\" at the top level of the result. The\n    dictionary maps algorithm names to parameters lists. Each of the\n    parameters contains all the metadata required by a client to\n    present a model building interface to the user.\n    \n    if parameters = True, return the parameters?\n    \"\"\"\n    endpoint = \"ModelBuilders\"\n   ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Verifies the DagRun by checking for removed tasks or tasks that are not in the\n    database yet. It will set state to removed or add the task if required.\n    \"\"\"\n\n    if session is None:\n        session = Session()\n\n    # Get the current tasks in the DAG\n    current_task_ids = {task.task_id for task in self.dag.tasks}\n\n    # Get the task instances from the database\n    tis = session.query",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Builds fake data for unit testing.\"\"\"\n        features = {\n            'feature1': tf.random.uniform([batch_size, 10]),\n            'feature2': tf.random.uniform([batch_size, 5])\n        }\n        labels = tf.random.uniform([batch_size, 1], maxval=2, dtype=tf.int32)\n        return features, labels\n\n    return input_fn\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Create a Model. Blocks until finished.\n    \"\"\"\n    client = automl.AutoMlClient()\n\n    # Get the full path of the project.\n    project_location = client.location_path(project_id, \"us-central1\")\n    \n    # Create the model\n    response = client.create_model(parent=project_location, model=model)\n\n    print(\"Training operation name: {}\".format(response.operation.name))\n    print(\"Training started...\")\n\n    # Wait until the operation is done\n    result =",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Execute sqoop job\n    \"\"\"\n\n    sqoop_command = [\n        'sqoop', 'import',\n        '--connect', self.jdbc_url,\n        '--username', self.username,\n        '--password', self.password,\n        '--table', self.table,\n        '--target-dir', self.target_dir,\n        '--num-mappers', str(self.num_mappers)\n    ]\n\n    try:\n        result = subprocess.run(sqoop_command, check=True, capture_output=True, text=True)\n        context['task_instance'].xcom_push(key='sqoop_output', value",
        "rewrite": ""
    },
    {
        "original": "\n\ntfd = tfp.distributions\n\n# Data for the eight schools problem\ntreatment_effects = np.array([28,  8, -3,  7, -1,  1, 18, 12], dtype=np.float32)\ntreatment_stddevs = np.array([15, 10, 16, 11,  9, 11, 10, 18], dtype=np.float32)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Load a pre-trained Bigdl model.\n\n    :param path: The path containing the pre-trained model.\n    :return: A pre-trained model.\n    \"\"\"\n    return Model.loadModel(path, bigdl_type)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    This method schedules the tasks for a single DAG by looking at the\n    active DAG runs and adding task instances that should run to the\n    queue.\n    \"\"\"\n\n    # Get active DAG runs\n    dag_runs = session.query(DagRun).filter(\n        DagRun.dag_id == dag.dag_id,\n        DagRun.state == State.RUNNING\n    ).all()\n\n    for dag_run in dag_runs:\n        # Get task",
        "rewrite": ""
    },
    {
        "original": "\n\nclass UTCDateTime(TypeDecorator):\n    impl = DateTime\n\n        if value is not None:\n            # Assuming the datetime from the DB is naive and in UTC\n            return value.replace(tzinfo=pytz.UTC)\n        return value\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    return tf.reduce_all(tf.equal(tf.shape(a), tf.shape(b)))\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Set this layer in the training mode or in prediction mode if is_training=False\n    \"\"\"\n    self.is_training = is_training\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    enhancer = ImageEnhance.Color(img)\n    return enhancer.enhance(saturation_factor)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Executes `model`, creating both samples and distributions.\"\"\"\n\n    # Ensure sample_shape is a Tensor\n    sample_shape = tf.convert_to_tensor(sample_shape, dtype=tf.int32)\n\n    # Set the seed if provided\n    if seed is not None:\n        tf.random.set_seed(seed)\n\n    # Execute the model to get the distributions\n    distributions = self.model()\n\n    # Check if distributions is a list or a single distribution\n    if",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Create a Python Layer based on the given java value and the real type.\n    :param jvalue: Java object created by Py4j\n    :return: A Python Layer\n    \"\"\"\n    \n    # Call the BigDL function to create a layer from the Java value\n    py_layer = callBigDlFunc(bigdl_type, \"createLayer\", jvalue)\n    \n    # Return the created Python Layer\n    return Layer.of(py_layer)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Checks that `distributions` satisfies all assumptions.\"\"\"\n    if not isinstance(distributions, (list, tuple)):\n        raise TypeError(\"`distributions` must be a list or tuple.\")\n    \n    if not distributions:\n        raise ValueError(\"`distributions` must not be empty.\")\n    \n    for dist in distributions:\n        if not hasattr(dist, 'dtype'):\n            raise ValueError(\"Each distribution must have a `dtype` attribute.\")\n        \n        if dtype_override is not None and dist.dtype != dtype_override:\n            raise TypeError",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Build different type of Dingding message\n    As most commonly used type, text message just need post message content\n    rather than a dict like ``{'content': 'message'}``\n    \"\"\"\n    if self.message_type == 'text':\n        return self.message_content\n    elif self.message_type == 'markdown':\n        return {\n            'msgtype': 'markdown',\n            'markdown': {\n                'title': self.message_title,\n                'text': self.message_content\n            }\n        }\n    elif self.message_type == 'link':\n        return {\n            'msgtype",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Replicate the input tensor n times along a new (major) dimension.\"\"\"\n    return np.tile(tensor, (n,) + (1,) * tensor.ndim)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns an iterable of strings that explain why this dependency wasn't met.\n\n    :param ti: the task instance to see if this dependency is met for\n    :type ti: airflow.models.TaskInstance\n    :param session: database session\n    :type session: sqlalchemy.orm.session.Session\n    :param dep_context: The context this dependency is being checked under that stores\n        state that can be used by this dependency.\n    :type dep_context: BaseDepContext\n    \"\"\"\n    reasons = []\n    \n    # Example check:",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Publish the message to SQS queue\n\n    :param context: the context object\n    :type context: dict\n    :return: dict with information about the message sent\n        For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`\n    :rtype: dict\n    \"\"\"\n    sqs = boto3.client('sqs')\n    queue_url = context.get('queue_url')\n    message_body = context.get('message_body')\n    \n    response = sqs.send_message(\n        QueueUrl=queue_url",
        "rewrite": ""
    },
    {
        "original": "\n\n    return platform.system()\n\n    if os == 'Windows':\n        # Remove invalid characters for Windows filenames\n        text = re.sub(r'[<>:\"/\\\\|?*]', '', text)\n    else:\n        # Remove invalid characters for Unix-like filenames\n        text = re.sub(r'[/]', '', text)\n    \n    # Remove leading and trailing whitespace\n    text = text.strip()\n    \n    # Replace spaces with underscores\n    text = re.sub(r'\\s+', '_', text)\n    \n    return text\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Convenience function to efficiently construct a MultivariateNormalDiag.\"\"\"\n    return tfp.distributions.MultivariateNormalDiag(*args, **kwargs)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Computes I_v(z)*exp(-abs(z)) using a recurrence relation, where z > 0.\"\"\"\n    if cache is None:\n        cache = {}\n    \n    if (v, z) in cache:\n        return cache[(v, z)]\n    \n    if v == 0:\n        result = ive(0, z)\n    elif v == 1:\n        result = ive(1, z)\n    else:\n        result = (2 * (v - 1)",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Chainer:\n        self.config = config\n        self.mode = mode\n        self.load_trained = load_trained\n        self.download = download\n        self.serialized = serialized\n        self.model = self._build_model()\n\n        # Placeholder for actual model building logic\n        return f\"Model built with config: {self.config}, mode: {self.mode}\"\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Helper method that set dag run state in the DB.\n    :param dag_id: dag_id of target dag run\n    :param execution_date: the execution date from which to start looking\n    :param state: target state\n    :param session: database session\n    \"\"\"\n    if session is None:\n        raise ValueError(\"A valid session must be provided\")\n\n    dag_run = session",
        "rewrite": ""
    },
    {
        "original": "\n                     position_delta,\n                     next_objective,\n                     next_gradient,\n                     grad_tolerance,\n                     f_relative_tolerance,\n                     x_tolerance):\n    \"\"\"Updates the state advancing its position by a given position_delta.\"\"\"\n    new_position = state['position'] + position_delta\n    new_gradient_norm = np.linalg.norm(next_gradient)\n    relative_change_in_objective = abs(next_objective - state['objective']) / max(1.0, abs(state['objective']))\n    position_change_norm = np.linalg.norm(position_delta)\n\n    converged = (new_gradient_norm < grad_tolerance or\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Takes a cursor, and writes results to a local file.\n\n    :return: A dictionary where keys are filenames to be used as object\n        names in GCS, and values are file handles to local files that\n        contain the data for the GCS objects.\n    \"\"\"\n\n    # Create a temporary directory to store the files\n    temp_dir = tempfile.mkdtemp()\n    result_files = {}\n\n    # Fetch all rows from the cursor\n    rows = cursor.fetchall()\n\n    # Get column names from the cursor",
        "rewrite": ""
    },
    {
        "original": "\n    rightmost_transposed_ndims, validate_args, name=None):\n  \"\"\"Checks that `rightmost_transposed_ndims` is valid.\"\"\"\n  if validate_args:\n    rightmost_transposed_ndims = tf.convert_to_tensor(\n        rightmost_transposed_ndims, name='rightmost_transposed_ndims')\n    assertions = [\n        tf.debugging.assert_rank(\n            rightmost_transposed_ndims, 0,\n            message='Argument `rightmost_transposed_ndims` must be a scalar.'),\n        tf.debugging.assert_non_negative(\n            right",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Execute the Discord webhook call\n    \"\"\"\n    webhook_url = \"YOUR_DISCORD_WEBHOOK_URL\"\n    data = {\n        \"content\": \"Hello, this is a message from your webhook!\"\n    }\n    \n    response = requests.post(webhook_url, json=data)\n    \n    if response.status_code == 204:\n        print(\"Message sent successfully.\")\n    else:\n        print(f\"Failed to send message. Status code: {response.status_code}\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    prior_variables = [\n        tf.Variable(initial_value * tf.ones([num_topics]), name='prior_mean'),\n        tf.Variable(initial_value * tf.ones([num_topics]), name='prior_stddev')\n    ]\n\n        return tfp.distributions.Normal(loc=prior_variables[0], scale=tf.nn.softplus(prior_variables[1]))\n\n    return prior, prior_variables\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    method = params.get('method', 'GET')\n    headers = params.get('headers', {})\n    data = params.get('data', {})\n    url = params.get('url', '')\n    category = params.get('category', '')\n    pageno = params.get('pageno', 1)\n\n    if method == 'POST':\n        response = requests.post(url, headers=headers, data=data)\n    else:\n        response = requests.get(url, headers=headers, params={'query': query, 'category': category, 'pageno': pageno})\n\n    return response.json",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Validate `outcomes`, `logits` and `probs`'s shapes.\"\"\"\n    if not validate_args:\n        return\n\n    if logits is not None and probs is not None:\n        raise ValueError(\"Cannot specify both `logits` and `probs`.\")\n\n    if logits is not None:\n        if outcomes.shape[-1] != logits.shape[-1]:\n            raise ValueError(\"The last dimension of `outcomes` must match the last dimension of `logits`.\")\n    elif probs is not None:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    try:\n        # Get the latest tag\n        tag = subprocess.check_output(['git', 'describe', '--tags', '--abbrev=0']).strip().decode('utf-8')\n        prefix = f'release:{tag}'\n    except subprocess.CalledProcessError:\n        # No tags found\n        prefix = 'dev0'\n\n    # Get the current commit hash\n    commit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip().decode('utf-8')\n\n    # Check for uncommitted changes\n    try:\n        subprocess.check",
        "rewrite": ""
    },
    {
        "original": "\n\n    max_log_value = tf.reduce_max(log_values, axis=0, keepdims=True)\n    log_mean = max_log_value + tf.math.log(tf.reduce_mean(tf.exp(log_values - max_log_value), axis=0))\n    return log_mean\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Network block for VGG.\"\"\"\n    x = tfp.layers.Convolution2DFlipout(\n        filters,\n        kernel,\n        strides=stride,\n        padding='same',\n        kernel_posterior_fn=kernel_posterior_fn,\n        activation='relu'\n    )(x)\n    x = tfp.layers.Convolution2DFlipout(\n        filters,\n        kernel,\n        strides=1,\n        padding='same',\n        kernel_posterior",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    if not is_master:\n        builtins.print = lambda *args, **kwargs: None\n\n# Example usage:\n# setup_for_distributed(os.getenv('RANK') == '0')\n",
        "rewrite": ""
    },
    {
        "original": "\n    # Assuming data is a tuple where the first element is the word sequences\n    word_sequences = data[0]\n    \n    # Placeholder for predictions\n    predictions = []\n    \n    for sequence in word_sequences:\n        # Predict tags for each word in the sequence\n        predicted_tags = self.model.predict(sequence)\n        \n        if return_indexes:\n            predictions.append(predicted_tags)\n        else:\n            # Convert indexes to actual tags\n            tags = [self.vocab.get_tag(index) for index",
        "rewrite": ""
    },
    {
        "original": "\n\n    if series_order < 1:\n        raise ValueError(\"series_order must be >= 1\")\n    \n    x_2 = x * x\n    return_sum = -0.5 * np.log(2 * np.pi) - 0.5 * x_2 - np.log(x)\n    if series_order == 1:\n        return return_sum\n\n    # Precompute the odd powers of x\n    x_m = 1. / x_2\n    x_2i = x_m",
        "rewrite": ""
    },
    {
        "original": "\n\n             execution_date,\n             key=None,\n             task_ids=None,\n             dag_ids=None,\n             include_prior_dates=False,\n             limit=100,\n             session=None):\n    query = session.query(cls).filter(cls.execution_date == execution_date)\n    \n    if key:\n        query = query.filter(cls.key == key)\n    \n    if task_ids:\n        if isinstance(task_ids, str):\n            task_ids = [task_ids]\n        query = query.filter(cls.task_id.in_(task_ids))\n    \n    if dag_ids:\n        if isinstance(d",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"generate HTML div\"\"\"\n    return '<div></div>'\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n\n    :param operator: Databricks operator being handled\n    :param context: Airflow context\n    \"\"\"\n    try:\n        log.info(\"Submitting Databricks run for operator: %s\", operator.task_id)\n        run_id = hook.submit_run(operator.json)\n        log.info(\"Run submitted with run_id: %s\", run_id)\n\n        while not hook.check_run_status(run_id):\n            log.info(\"Waiting for Databr",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Checks if a task is either queued or running in this executor\n\n    :param task_instance: TaskInstance\n    :return: True if the task is known to this executor\n    \"\"\"\n    return task_instance in self.queued_tasks or task_instance in self.running_tasks\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns the task instances for this dag run\n    \"\"\"\n\n    @provide_session\n        query = session.query(TaskInstance).filter(TaskInstance.dag_id == self.dag_id, TaskInstance.execution_date == self.execution_date)\n        if state:\n            query = query.filter(TaskInstance.state == state)\n        return query.all()\n\n    return _get_task_instances(session=session)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n                                        state,\n                                        volatility_fn_results=None,\n                                        grads_volatility_fn=None,\n                                        sample_shape=None,\n                                        parallel_iterations=10):\n  \"\"\"Helper which computes `volatility_fn` results and grads, if needed.\"\"\"\n  if volatility_fn_results is None:\n    with tf.GradientTape() as tape:\n      tape.watch(state)\n      volatility_fn_results = volatility_fn(state)\n    grads_volatility_fn = tape.gradient(volatility_fn_results, state)\n  return volatility_fn_results, grads_volatility_fn\n",
        "rewrite": ""
    },
    {
        "original": "\n\n               normalize=False, range=None, scale_each=False, pad_value=0):\n    grid = vutils.make_grid(tensor, nrow=nrow, padding=padding, \n                            normalize=normalize, range=range, \n                            scale_each=scale_each, pad_value=pad_value)\n    ndarr = grid.mul(255).add(0.5).clamp(0, 255).byte().permute(1, 2, ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Create perm-vm if not exist and insert into FAB security model for all-dags.\n    \"\"\"\n\n    session: Session = self.get_session()\n    security_manager: AirflowSecurityManager = self.appbuilder.sm\n\n    all_dags = session.query(DagModel).all()\n    for dag in all_dags:\n        view_menu",
        "rewrite": ""
    },
    {
        "original": "\n\n    try:\n        # Validate the certificate URL\n        if not signature_chain_url.startswith(\"https://s3.amazonaws.com/echo.api/\"):\n            return False\n\n        # Download the certificate\n        with urllib.request.urlopen(signature",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Load model architecture from a JSON file and weights from an HDF5 file.\n\n    Parameters:\n    def_json (str): Path to the JSON file containing the model definition.\n    weights_hdf5 (str): Path to the HDF5 file containing the model weights.\n    by_name (bool): Whether to load weights by name or by topological order.\n\n    Returns:\n    model: A Keras model instance with loaded",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(a || b) with a and b `HalfNormal`.\n\n    Args:\n        a: Instance of a `HalfNormal` distribution object.\n        b: Instance of a `HalfNormal` distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_half_normal_half_normal\".\n\n    Returns:\n        Batchwise KL(a || b)\n    \"\"\"\n    with tf.name_scope(name or \"kl_half_normal_half_normal",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Provide filename context to airflow task handler.\n    :param filename: filename in which the dag is located\n    \"\"\"\n    self.filename = filename\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Returns opened file object for writing dialog logs.\n\n    Returns:\n        log_file: opened Python file object.\n    \"\"\"\n    log_file_path = \"dialog_logs.txt\"\n    log_file = open(log_file_path, \"a\")\n    return log_file\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    date_format = \"%Y-%m-%d\"\n    date_obj = datetime.strptime(ds, date_format)\n    new_date_obj = date_obj + timedelta(days=days)\n    return new_date_obj.strftime(date_format)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    num_seasons, is_last_day_of_season, dtype,\n    basis_change_matrix=None, basis_change_matrix_inv=None):\n  \"\"\"Build a function computing transitions for a seasonal effect model.\"\"\"\n  \n  identity_matrix = np.eye(num_seasons, dtype=dtype)\n  transition_matrix = np.roll(identity_matrix, shift=1, axis=0)\n  \n  if basis_change_matrix is not None and basis_change_matrix_inv is not None:\n    transition_matrix = np.dot(\n        np.dot(basis_change_matrix, transition_matrix), basis_change_matrix_inv)\n",
        "rewrite": ""
    },
    {
        "original": "\n    transformed_state = previous_kernel_results.transformed_state\n    inner_results = self.inner_kernel.one_step(transformed_state, previous_kernel_results.inner_results)\n    next_transformed_state = inner_results[0]\n    next_state = self.bijector.inverse(next_transformed_state)\n    new_kernel_results = previous_kernel_results._replace(\n        transformed_state=next_transformed_state,\n        inner_results=inner_results[1]\n    )\n    return next_state, new_kernel_results\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Read logs of given task instance and try_number from Wasb remote storage.\n    If failed, read the log from task instance host machine.\n    :param ti: task instance object\n    :param try_number: task instance try_number to read logs from\n    :param metadata: log metadata,\n                     can be used for steaming log reading and auto-tailing.\n    \"\"\"\n    try:\n        # Attempt to read the log from Wasb remote storage\n        log = self.wasb_hook.read_log(ti, try_number)\n        return log",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Deletes Conversation instance.\n\n    Args:\n        conversation_key: Conversation key.\n    \"\"\"\n    if conversation_key in self.conversations:\n        del self.conversations[conversation_key]\n",
        "rewrite": ""
    },
    {
        "original": "\n\n        label = data[0]\n        features = data[1:]\n        normalized_features = [float(x) / 255.0 for x in features]\n        return LabeledPoint(label, Vectors.dense(normalized_features))\n\n    # Load the MNIST dataset\n    mnist_rdd = sc.textFile(options['mnist_path'])\n\n    # Split the data by commas\n    mnist_rdd",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Convenience function which chooses the condition based on the predicate.\"\"\"\n    return cond_true if pred else cond_false\n",
        "rewrite": ""
    },
    {
        "original": "\n    if input_shape[-1] is None:\n        raise ValueError(\"The innermost dimension of `input_shape` must be defined.\")\n    \n    # Example logic for computing output shape\n    # This will vary depending on the specific layer's logic\n    output_shape = list(input_shape)\n    output_shape[-1] = self.units  # Assuming 'units' is an attribute of the layer\n    \n    return tuple(output_shape)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n    lifecycle per task.\n\n    :return: The Cloud SQL Proxy runner.\n    :rtype: CloudSqlProxyRunner\n    \"\"\"\n    if not hasattr(self, '_sqlproxy_runner'):\n        self._sqlproxy_runner = CloudSqlProxyRunner()\n    return self._sqlproxy_runner\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    drift_scale, num_seasons, is_last_day_of_season):\n  \"\"\"Build the transition noise model for a SeasonalStateSpaceModel.\"\"\"\n  return tfp.distributions.MultivariateNormalDiag(\n      scale_diag=drift_scale * tf.ones([num_seasons]) * is_last_day_of_season)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    shape = tf.shape(element)\n    dtype = element.dtype\n    return tf.zeros((k,) + tuple(shape), dtype=dtype)\n\n# Example usage:\nelement = tf.constant([[0., 1., 2., 3., 4.],\n                       [5., 6., 7., 8., 9.]])\nqueue = _make_empty_queue_for(3, element)\nprint(queue)\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass ImageFrameWriter:\n    @classmethod\n        \"\"\"\n        Write ImageFrame as parquet file\n        \"\"\"\n        spark = SparkSession.builder.getOrCreate()\n        df = spark.createDataFrame(output)\n        df.repartition(partition_num).write.parquet(path)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Uploads the file to Google cloud storage\n    \"\"\"\n\n    # Initialize a client\n    client = storage.Client()\n\n    # Define the bucket and the file to be uploaded\n    bucket_name = 'your-bucket-name'\n    source_file_name = 'local/path/to/file'\n    destination_blob_name = 'storage-object-name'\n\n    # Get the bucket\n    bucket = client.get_bucket(bucket_name)\n\n    # Create a blob object from the file path\n    blob = bucket.blob(destination_blob_name)\n\n    # Upload the file to a",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Generate sequences\n    generated_sequences = []\n    for _ in range(samples):\n        z = model.sample_prior(batch_size)\n        generated_sequence = model.sample_sequence(z, length)\n        generated_sequences.append(generated_sequence)\n\n    # Plot original inputs\n    fig, axes = plt.subplots(batch_size, length, figsize=(length * 2, batch_size * 2))\n    for i in range(batch_size):\n        for",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Returns a Redis connection.\n    \"\"\"\n    return redis.Redis(host='localhost', port=6379, db=0)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n                         batch_size, valid_size):\n    # Create TensorFlow datasets from the input arrays\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    \n    # Shuffle and batch the training dataset\n    train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n    \n    # Batch the test dataset\n    test_dataset = test_dataset.batch(batch_size)\n    \n    # Create an",
        "rewrite": ""
    },
    {
        "original": "\n\n    with tf.variable_scope(scope):\n        memory_shape = memory.get_shape().as_list()\n        query = tf.get_variable(\"query\", [att_size], dtype=tf.float32)\n        \n        keys = tf.layers.dense(memory, att_size, use_bias=False, name=\"keys\")\n        logits = tf.reduce_sum(keys * query, axis=-1)\n        \n        if mask is not None:\n            logits = tf.where(mask, logits, tf.fill(tf.shape(logits), float('-inf",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    \u0412\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043a\u044d\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043a descend\n    \"\"\"\n    original_descend = self.descend\n    cache = {}\n\n        if args in cache:\n            return cache[args]\n        result = original_descend(*args)\n        cache[args] = result\n        return result\n\n    self.descend = cached_descend\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Extract the S3 operations from the configuration and execute them.\n\n    :param config: config of SageMaker operation\n    :type config: dict\n    :rtype: dict\n    \"\"\"\n    s3 = boto3.client('s3')\n    results = {}\n\n    for operation in config.get('s3_operations', []):\n        op_type = operation.get('type')\n        params = operation.get('params', {})\n\n        if op_type == 'upload_file':\n            s3.upload_file(**params)\n            results[op_type] =",
        "rewrite": ""
    },
    {
        "original": "\n\n    response = requests.get(url, stream=True)\n    if response.status_code == 200:\n        file_name = os.path.join(output_dir, url.split('/')[-1])\n        if info_only:\n            print(f\"File name: {file_name}\")\n            print(f\"File size: {response.headers.get('content-length', 'unknown')}\")\n            return\n        with open(file_name, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=1024):\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    target_date = datetime.now() - timedelta(days=n)\n    return target_date.replace(hour=hour, minute=minute, second=second, microsecond=microsecond)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    command = self.command\n    if join_args:\n        command = [' '.join(command)]\n    if run_with:\n        command = run_with + command\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return process\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Compute the harmonic number from its analytic continuation.\n\n    Derivation from [here](\n    https://en.wikipedia.org/wiki/Digamma_function#Relation_to_harmonic_numbers)\n    and [Euler's constant](\n    https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).\n\n    Args:\n        x: input float.\n\n    Returns:\n        z: The analytic continuation of the harmonic number for the input.\n    \"\"\"\n    return math.psi(x + 1) + math.euler_gamma\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = storage.Client()\n    bucket = client.bucket(bucket_name, user_project=user_project)\n    blob = bucket.blob(object_name)\n    acl = blob.acl\n    acl.entity(entity).grant(role)\n    acl.save()\n",
        "rewrite": ""
    },
    {
        "original": "\n                      global_scale_variance,\n                      global_scale_noncentered,\n                      local_scale_variances,\n                      local_scales_noncentered,\n                      weights_noncentered):\n    \"\"\"Build regression weights from model parameters.\"\"\"\n    \n    # Compute the global scale\n    global_scale = global_scale_noncentered * (global_scale_variance ** 0.5)\n    \n    # Compute the local scales\n    local_scales = local_scales_noncentered * (local_scale_variances ** 0.5)\n    \n    # Compute the weights\n    weights = weights_noncentered * local_sc",
        "rewrite": ""
    },
    {
        "original": "\n\n    processed_folder = 'path/to/processed_folder'\n    url = 'http://path.to/emnist/data.gz'\n    file_path = os.path.join(processed_folder, 'data.gz')\n\n    if not os.path.exists(processed_folder):\n        os.makedirs(processed_folder)\n\n    if not os.path.exists(file_path):\n        print(\"Downloading EMNIST data...\")\n        urllib.request.urlretrieve(url, file_path)\n        print(\"Download complete.\")\n\n        with gzip.open(file_path, 'rb') as f_in:\n            with open(os",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Construct the Opsgenie JSON payload. All relevant parameters are combined here\n    to a valid Opsgenie JSON payload.\n\n    :return: Opsgenie payload (dict) to send\n    \"\"\"\n    payload = {\n        \"message\": self.message,\n        \"alias\": self.alias,\n        \"description\": self.description,\n        \"responders\": self.responders,\n        \"visibleTo\": self.visible_to,\n        \"actions\": self.actions,\n        \"tags\": self.tags,\n        \"details\": self.details,\n        \"entity\": self.entity,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Helper to __init__ which makes or raises assertions.\"\"\"\n    if not isinstance(distribution, str):\n        raise TypeError(\"distribution must be a string\")\n    if not isinstance(batch_shape, (list, tuple)):\n        raise TypeError(\"batch_shape must be a list or tuple\")\n    if not all(isinstance(dim, int) and dim > 0 for dim in batch_shape):\n        raise ValueError(\"All dimensions in batch_shape must be positive integers\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if isinstance(config, (str, Path)):\n        with open(config, 'r') as file:\n            return json.load(file)\n    elif isinstance(config, dict):\n        return config\n    else:\n        raise ValueError(\"Invalid configuration format\")\n\n    # Dummy function to represent model loading\n    return \"Loaded model based on config\"\n\n    # Dummy function to represent data",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Call the SparkSqlHook to run the provided sql query\n    \"\"\"\n\n    spark_sql_hook = SparkSqlHook(\n        sql=self.sql,\n        conn_id=self.spark_conn_id,\n        total_executor_cores=self.total_executor_cores,\n        executor_cores=self.executor_cores,\n        executor_memory=self.executor_memory,\n        driver_memory=self.driver_memory,\n        name=self.name,\n        verbose=self.verbose,\n        yarn_queue=self.yarn_queue,\n        conf=self.conf,\n        files=self.files,\n        py_files",
        "rewrite": ""
    },
    {
        "original": "\n    client = vision_v1.ProductSearchClient()\n\n    if project_id is None:\n        project_id = client.project\n\n    product_path = client.product_path(project_id, location, product_id)\n\n    client.delete_product(name=product_path, retry=retry, timeout=timeout, metadata=metadata)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Tar the local file or directory and upload to s3\n\n    :param path: local file or directory\n    :type path: str\n    :param key: s3 key\n    :type key: str\n    :param bucket: s3 bucket\n    :type bucket: str\n    :return: None\n    \"\"\"\n    tar_path = '/tmp/temp.tar.gz'\n    \n    with tarfile.open(tar_path, 'w:gz') as tar:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    signature = inspect.signature(f)\n    valid_kwargs = {k: v for k, v in src_kwargs.items() if k in signature.parameters}\n    return valid_kwargs\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    onehot = np.zeros_like(proba)\n    for i, sample in enumerate(proba):\n        max_prob = np.max(sample)\n        if max_prob >= confident_threshold:\n            onehot[i, np.argmax(sample)] = 1\n    return onehot\n",
        "rewrite": ""
    },
    {
        "original": "\n    while True:\n        result = self.client.get_operation(project_id=project_id, operation_id=operation.name)\n        if result.status == google.cloud.container_v1.enums.Operation.Status.DONE:\n            if result.error:\n                raise Exception(f\"Operation failed with error: {result.error.message}\")\n            return result\n        time.sleep(5)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Makes a function which applies a list of Bijectors' `forward`s.\"\"\"\n        for b in bijector:\n            x = b.forward(x)\n        return x\n    return transform\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    for dag_run in dag_runs:\n        unfinished_tasks = session.query(TaskInstance).filter(\n            TaskInstance.dag_id == dag_run.dag_id,\n            TaskInstance.execution_date == dag_run.execution_date,\n            TaskInstance.state.in_([State.NONE, State.SCHEDULED, State.QUEUED, State.RUNNING])\n        ).all()\n\n        if unfinished_tasks:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Creates lists of callables suitable for JDSeq.\"\"\"\n    flattened = []\n    for name, maker in named_makers:\n        if isinstance(maker, list):\n            flattened.extend(maker)\n        else:\n            flattened.append(maker)\n    return flattened\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Network block for ResNet.\"\"\"\n    conv1 = tfp.layers.Convolution2DFlipout(\n        filters, kernel, strides=stride, padding='same',\n        kernel_posterior_fn=kernel_posterior_fn)(x)\n    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n    conv1 = tf.keras.layers.ReLU()(conv1)\n\n    conv2 = tfp.layers.Convolution2DFlipout(\n        filters, kernel",
        "rewrite": ""
    },
    {
        "original": "\nclass TaskState:\n    NOT_STARTED = 'not_started'\n    IN_PROGRESS = 'in_progress'\n    COMPLETED = 'completed'\n    FAILED = 'failed'\n\n    \"\"\"\n    A list of states indicating that a task either has not completed\n    a run or has not even started.\n    \"\"\"\n    return [cls.NOT_STARTED, cls.IN_PROGRESS]\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not database_name or not collection_name:\n        raise ValueError(\"Both database_name and collection_name must be provided\")\n\n    client = CosmosClient(self.endpoint, self.key)\n    database = client.get_database_client(database_name)\n    container = database.get_container_client(collection_name)\n\n    if document_id:\n        document['id'] = document_id\n\n    try:\n        response = container.upsert_item(body=document)\n        return response\n    except exceptions.CosmosHttpResponseError",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Create X-axis\"\"\"\n    axis = {'name': name}\n    \n    if label:\n        axis['label'] = label\n    \n    if format:\n        axis['format'] = format\n    \n    if date:\n        axis['type'] = 'date'\n    \n    if custom_format:\n        axis['custom_format'] = custom_format\n    \n    return axis\n",
        "rewrite": ""
    },
    {
        "original": "\n    if event_name == \"after_validation\":\n        self.learning_rate = data.get(\"learning_rate\", self.learning_rate)\n        self.momentum = data.get(\"momentum\", self.momentum)\n    elif event_name == \"after_batch\":\n        self.learning_rate = data.get(\"learning_rate\", self.learning_rate)\n    elif event_name == \"after_epoch\":\n        self.momentum = data.get(\"momentum\", self.momentum)\n    elif event_name == \"after_train_log\":\n        self.learning_rate = data.get(\"learning_rate\",",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Computes `rank` given a `Tensor`'s `shape`.\"\"\"\n    if tensorshape is not None:\n        return len(tensorshape)\n    shape_tensor = shape_tensor_fn()\n    return shape_tensor.shape[0]\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Load a pre-trained Caffe model.\n\n    :param defPath: The path containing the caffe model definition.\n    :param modelPath: The path containing the pre-trained caffe model.\n    :return: A pre-trained model.\n    \"\"\"\n    return Model.load_caffe(defPath, modelPath, bigdl_type)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Uses arg names to resolve distribution names.\"\"\"\n    resolved_names = {}\n    for arg in dist_fn_args:\n        if arg in dist_names:\n            resolved_names[arg] = dist_names[arg]\n        else:\n            resolved_names[arg] = f\"{leaf_name}_{arg}\"\n    return resolved_names\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    model = Sequential()\n    \n    # Convolutional Layer 1\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    # Convolutional Layer 2\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Pareto.\n\n    Args:\n        a: instance of a Pareto distribution object.\n        b: instance of a Pareto distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_pareto_pareto\".\n\n    Returns:\n        Batchwise KL(a || b)\n    \"\"\"\n    with tf.name_scope(name or \"kl_pareto_pareto\"):\n        a_scale = tf",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Insert a list of new documents into an existing collection in the CosmosDB database.\n    \"\"\"\n    if not database_name or not collection_name:\n        raise ValueError(\"Both database_name and collection_name must be provided\")\n\n    client = CosmosClient(self.endpoint, self.key)\n    database = client.get_database_client(database_name)\n    container = database.get_container_client(collection_name)\n\n    for document in documents:\n        try:\n            container.create_item(body=document)\n        except exceptions.CosmosHttpResponse",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Returns Gcp Video Intelligence Service client\n\n    :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient\n    \"\"\"\n    return videointelligence_v1.VideoIntelligenceServiceClient()\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if isinstance(dest_file_path, (str, Path)):\n        dest_file_path = [dest_file_path]\n\n    response = requests.get(source_url)\n    response.raise_for_status()\n\n    for path in dest_file_path:\n        path = Path(path)\n        if not force_download and path.exists():\n            continue\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with open(path, 'wb') as file:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n    onehot = np.zeros((len(labels), len(classes)), dtype=int)\n    \n    for i, sample in enumerate(labels):\n        if isinstance(sample, str):\n            sample = [sample]\n        for label in sample:\n            onehot[i, class_to_index[label]] = 1\n            \n    return onehot\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Call the SlackWebhookHook to post the provided Slack message\n    \"\"\"\n\n    slack_webhook_token = self.slack_webhook_token\n    message = self.message\n\n    hook = SlackWebhookHook(\n        http_conn_id=slack_webhook_token,\n        message=message\n    )\n    hook.execute()\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    s3 = boto3.client('s3')\n    \n    if isinstance(keys, str):\n        keys = [keys]\n    \n    objects = [{'Key': key} for key in keys]\n    \n    response = s3.delete_objects(\n        Bucket=bucket,\n        Delete={\n            'Objects': objects\n        }\n    )\n    \n    return response\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Run Presto Query on Athena\n    \"\"\"\n\n    # Initialize the Athena client\n    client = boto3.client('athena')\n\n    # Define the query and the database\n    query = \"SELECT * FROM your_table LIMIT 10;\"\n    database = \"your_database\"\n    output_location = \"s3://your-output-bucket/\"\n\n    try:\n        # Start the query execution\n        response = client.start_query_execution(\n            QueryString=query,\n            QueryExecutionContext={'Database': database},\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if ti.state != State.NONE:\n        yield self._passing_status(reason=\"Task instance state is not NONE.\")\n        return\n\n    reschedules = session.query(TaskReschedule).filter(\n        TaskReschedule.dag_id == ti.dag_id,\n        TaskReschedule.task_id == ti.task_id,\n        TaskReschedule.execution_date == ti.execution",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Provide task_instance context to airflow task handler.\n    :param ti: task instance object\n    \"\"\"\n    self.context = {\n        'dag_id': ti.dag_id,\n        'task_id': ti.task_id,\n        'execution_date': ti.execution_date,\n        'try_number': ti.try_number,\n        'log': self.log,\n    }\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Normal node shutdown.\n    Ignore failures for now.\n\n    :return none\n    \"\"\"\n    try:\n        # Add your shutdown logic here\n        pass\n    except Exception:\n        # Ignore any exceptions for now\n        pass\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Build fake CIFAR10-style data for unit testing.\n    \n    Parameters:\n    - num_samples: Number of samples to generate.\n    - image_size: Size of each image (height, width, channels).\n    - num_classes: Number of classes.\n    \n    Returns:\n    - X: NumPy array of shape (num_samples, height, width, channels) containing the fake images.\n    - y: NumPy array of shape (num_samples,)",
        "rewrite": ""
    },
    {
        "original": "\n\nclass H2OCluster:\n        self.base_url = base_url\n\n        url = f\"{self.base_url}/3/Frames/{key}\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeoutSecs:\n            response = requests.delete(url, **kwargs)\n            if response.status_code == 200:\n                return True\n            elif response.status_code == 404 and ignoreMissingKey:\n                return False",
        "rewrite": ""
    },
    {
        "original": "\n\n    converted_row = []\n    for col, value in zip(schema, row):\n        col_type = col_type_dict.get(col)\n        if value is None:\n            converted_row.append(value)\n        elif col_type in ('DATE', 'DATETIME', 'TIMESTAMP'):\n            if isinstance(value, datetime.datetime):\n                value = calendar.timegm(value.utctimetuple())\n            elif isinstance(value, datetime.date):\n                value = calendar.timegm(value.timetuple())\n            converted_row.append(value)\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Builds an Iterator switching between train and heldout data.\"\"\"\n    \n        # Define your `tf.parse_single_example` function here\n        keys_to_features = {\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        }\n        parsed_features = tf.parse_single_example(proto, keys_to_features)\n        image = tf.decode_raw(parsed_features['image_raw'], tf.uint8)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Computes E[log(det(X))] under this Wishart distribution.\"\"\"\n    with tf.name_scope(name):\n        # Ensure the scale matrix is positive definite\n        scale_matrix = self.scale.to_dense()\n        \n        # Compute the dimension of the scale matrix\n        p = tf.cast(tf.shape(scale_matrix)[-1], dtype=self.dtype)\n        \n        # Compute the digamma function for the degrees of freedom\n        df = tf.cast(self.df, dtype=self.dtype)\n        digamma_sum = tf.reduce_sum(tf.math.digamma(0.",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"\n    return self.mean_direction\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Like tf.where but works on namedtuples.\"\"\"\n    if not isinstance(tval, type(fval)):\n        raise TypeError(\"tval and fval must be of the same type\")\n    \n    if isinstance(tval, tuple) and hasattr(tval, '_fields'):\n        # Assuming namedtuples\n        return type(tval)(*(tv if c else fv for c, tv, fv in zip(cond, tval, fval)))\n    else:\n        raise TypeError(\"tval and fval must be named",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Chooses default step sizes according to [Gao and Han(2010)][3].\"\"\"\n\n    n = len(reference_vertex)\n    step_sizes = np.zeros(n)\n    \n    for i in range(n):\n        if reference_vertex[i] != 0:\n            step_sizes[i] = 0.1 * abs(reference_vertex[i])\n        else:\n            step_sizes[i] = 0.1\n    \n    return step_sizes\n",
        "rewrite": ""
    },
    {
        "original": "\n        self,\n        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"\n\n        client = ProductSearchClient()\n\n        if not project_id:\n            project_id = self.project_id\n\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\ntfd = tfp.distributions\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    if dtype is None:\n        dtype = tf.float32\n\n    params = tf.convert_to_tensor(params, dtype=dtype, name=\"params\")\n    loc = params[..., :event_size]\n    scale_diag = tf.nn.softplus(params[..., event_size:])\n\n    return tfd.MultivariateNormalDiag(\n        loc=loc,\n        scale_diag=scale_diag,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns a string contains start time and the secondary training job status message.\n\n    :param job_description: Returned response from DescribeTrainingJob call\n    :type job_description: dict\n    :param prev_description: Previous job description from DescribeTrainingJob call\n    :type prev_description: dict\n\n    :return: Job status string to be printed.\n    \"\"\"\n    start_time = job_description.get('TrainingStartTime', 'N/A')\n    job_status = job_description.get('SecondaryStatus', 'N/A')\n    prev_status = prev_description.get('",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Check a dictionary of model builder parameters on the h2o cluster \n    using the given algorithm and model parameters.\n    \"\"\"\n\n    # Initialize H2O if not already initialized\n    if not h2o.connection():\n        h2o.init()\n\n    # Prepare the parameters for validation\n    params = {\n        'algo': algo,\n        'training_frame': training_frame.frame_id,\n        **parameters\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    base_url = \"http://video.sina.com/\"\n    video_url = urljoin(base_url, f\"vkey={vkey}\")\n    \n    response = requests.get(video_url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch video info: {response.status_code}\")\n    \n    video_info = response.json()\n    if not video_info.get('data'):\n        raise Exception(\"No video data found\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    :return: a mapping between the subsystem name to the cgroup name\n    :rtype: dict[str, str]\n    \"\"\"\n    cgroup_mapping = {}\n    try:\n        with open('/proc/self/cgroup', 'r') as f:\n            for line in f:\n                parts = line.strip().split(':')\n                if len(parts) == 3:\n                    _, subsystems, cgroup_name = parts\n                    for subsystem in subsystems.split(','):\n                        cgroup_mapping[subsystem] = cgroup_name\n    except FileNotFoundError",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Refreshes the task instance from the database based on the primary key\n\n    :param lock_for_update: if True, indicates that the database should\n        lock the TaskInstance (issuing a FOR UPDATE clause) until the\n        session is committed.\n    \"\"\"\n    if session is None:\n        session = provide_session()\n\n    query = session.query(self.__class__).filter_by(\n        dag_id=self.dag_id,\n        task_id=self.task_id,\n        execution_date=self.execution_date\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Load model parameters from self.load_path\"\"\"\n\n    if not os.path.exists(self.load_path):\n        raise FileNotFoundError(f\"Load path {self.load_path} does not exist\")\n\n    checkpoint = torch.load(self.load_path, map_location=self.device)\n    model_state_dict = checkpoint['model_state_dict']\n\n    # Filter out parameters that are in the exclude_scopes\n    filtered_state_dict = {k: v for k, v in model_state_dict.items() if not any(scope",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Load the model from the weight file\n    model = tf.keras.models.load_model(weight_file, custom_objects=options.get('custom_objects', None))\n\n    # Create the directory if it doesn't exist\n    if not os.path.exists(hub_dir):\n        os.makedirs(hub_dir)\n\n    # Export the model to TF-Hub format\n    tf.saved_model.save(model, hub_dir)\n\n    print(f\"Model exported to {hub_dir}\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Helper which checks validity of `loc` and `scale` init args.\"\"\"\n    if validate_args:\n        param = tf.convert_to_tensor(param, name=name)\n        if param.dtype.is_floating:\n            return param\n        else:\n            raise TypeError(f\"{name} must be a floating point tensor.\")\n    return param\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    check if aws conn exists already or create one and return it\n\n    :return: boto3 session\n    \"\"\"\n    if not hasattr(self, '_aws_conn'):\n        self._aws_conn = boto3.Session()\n    return self._aws_conn\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if session is None:\n        session = Session()\n\n    # Find the DagRun for the given execution date\n    dag_run = session.query(DagRun).filter(\n        DagRun.dag_id == dag.dag_id,\n        DagRun.execution_date == execution_date\n    ).first()\n\n    if not dag_run:\n        return []\n\n    # Set the state of the DagRun to running\n   ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Helper to `maybe_call_fn_and_grads`.\"\"\"\n    if result is None:\n        result = fn(*fn_arg_list)\n    if grads is None:\n        with tf.GradientTape() as tape:\n            tape.watch(fn_arg_list)\n            result = fn(*fn_arg_list)\n        grads = tape.gradient(result, fn_arg_list)\n    return result, grads\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Builds iterators for train and evaluation data.\n\n    Each object is represented as a bag-of-words vector.\n\n    Arguments:\n        data_dir: Folder in which to store the data.\n        batch_size: Batch size for both train and evaluation.\n\n    Returns:\n        train_input_fn: A function that returns an iterator over the training data.\n        eval_input_fn: A function that returns an iterator over the evaluation data.\n        vocabulary: A mapping of word's integer index to the corresponding string.\n    \"\"\"\n    \n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = texttospeech_v1.TextToSpeechClient()\n\n    if isinstance(input_data, dict):\n        input_data = texttospeech_v1.SynthesisInput(**input_data)\n    if isinstance(voice, dict):\n        voice = texttospeech_v1.VoiceSelectionParams(**voice)\n    if isinstance(audio_config, dict):\n        audio_config = texttospeech_v1.AudioConfig(**audio_config)\n\n    response",
        "rewrite": ""
    },
    {
        "original": "\n    if len(args) == 0:\n        self.train = False\n        return self\n    elif len(args) == 3:\n        dataset, batch_size, val_methods = args\n        results = []\n        for method in val_methods:\n            result = method.evaluate(dataset, batch_size)\n            results.append(result)\n        return results\n    else:\n        raise ValueError(\"Invalid number of arguments passed to evaluate method\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    model = Sequential()\n    \n    # Convolutional layers\n    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(None, 64, 64, 3)))\n    model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n    model",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Takes a cursor, and writes the BigQuery schema in .json format for the\n    results to a local file system.\n\n    :return: A dictionary where key is a filename to be used as an object\n        name in GCS, and values are file handles to local files that\n        contains the BigQuery schema fields in .json format.\n    \"\"\"\n    schema = cursor.description\n    schema_fields = []\n\n    for field in schema:\n        field_info = {\n            \"name\": field[0],\n            \"type",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Build the HTML page\n    Create the htmlheader with css / js\n    Create html page\n    Add Js code for nvd3\n    \"\"\"\n    html_header = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>My NVD3 Chart</title>\n        <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/nvd3/1.8.6",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Creates a snapshot of a cluster\n\n    :param snapshot_identifier: unique identifier for a snapshot of a cluster\n    :type snapshot_identifier: str\n    :param cluster_identifier: unique identifier of a cluster\n    :type cluster_identifier: str\n    \"\"\"\n    client = boto3.client('redshift')\n    response = client.create_cluster_snapshot(\n        SnapshotIdentifier=snapshot_identifier,\n        ClusterIdentifier=cluster_identifier\n    )\n    return response\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"\n        # Generate fake images (28x28 pixels, 1 channel)\n        images = np.random.rand(batch_size, 28, 28, 1).astype(np.float32)\n        # Generate fake labels (10 classes)\n        labels = np.random.randint(0, 10, size=(batch_size,)).astype(np.int32)\n        return images, labels\n\n        dataset = tf.data.Dataset.from",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Returns samples from a Bernoulli distribution.\"\"\"\n    with tf.name_scope(name or \"random_bernoulli\"):\n        probs = tf.convert_to_tensor(probs, name=\"probs\")\n        return tf.cast(tf.less(tf.random.uniform(shape, seed=seed), probs), dtype)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Sample shape of random variable as a 1-D `Tensor`.\n\n    Args:\n      name: name to give to the op\n\n    Returns:\n      sample_shape: `Tensor`.\n    \"\"\"\n    with tf.name_scope(name):\n        return tf.convert_to_tensor(self.sample_shape, dtype=tf.int32, name=\"sample_shape\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Convert tensorflow model to bigdl model\n    :param input_ops: operation list used for input, should be placeholders\n    :param output_ops: operations list used for output\n    :param byte_order: byte order for the conversion, default is 'little'\n    :param bigdl_type: data type for bigdl model, default is 'float'\n    :return: bigdl model\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n    conn = sqlite3.connect('example.db')\n    cursor = conn.cursor()\n    \n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n    tables = cursor.fetchall()\n    \n    for table in tables:\n        cursor.execute(f\"DROP TABLE IF EXISTS {table[0]}\")\n    \n    conn.commit()\n    conn.close()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Find the best learning rate schedule, and set obtained values of learning rate\n    and momentum for further model training. Best learning rate will be divided\n    by `fit_learning_rate_div` for further training model.\n\n    Args:\n        *args: arguments\n\n    Returns:\n\n    \"\"\"\n    # Example implementation\n    best_learning_rate = self.find_best_learning_rate(*args)\n    best_momentum = self.find_best_momentum(*args)\n    \n    self.learning_rate = best_learning_rate / self.fit_learning_rate_div\n    self.momentum = best_momentum\n    \n    return",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Hook:\n        # Placeholder for the actual run implementation\n        pass\n\n        retry_decorator = tenacity.retry(**_retry_args)\n        decorated_run = retry_decorator(self.run)\n        return decorated_run(*args, **kwargs)\n\n# Example usage\nhook = Hook()\nretry_args = dict(\n    wait=tenacity.wait_exponential(),\n    stop=tenacity.stop_after_attempt(10),\n    retry=",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Make a query to Salesforce.\n\n    :param query: The query to make to Salesforce.\n    :type query: str\n    :return: The query result.\n    :rtype: dict\n    \"\"\"\n    response = self.session.get(f\"{self.instance_url}/services/data/vXX.X/query\", params={'q': query}, headers=self.headers)\n    response.raise_for_status()\n    return response.json()\n",
        "rewrite": ""
    },
    {
        "original": "\n                   fn_arg_list,\n                   fn_result=None,\n                   description='target_log_prob'):\n    \"\"\"Helper which computes `fn_result` if needed.\"\"\"\n    if fn_result is None:\n        fn_result = fn(*fn_arg_list)\n    return fn_result\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Takes a cursor, and writes results to a local file.\n\n    :return: A dictionary where keys are filenames to be used as object\n        names in GCS, and values are file handles to local files that\n        contain the data for the GCS objects.\n    \"\"\"\n\n    result_files = {}\n    for i, row in enumerate(cursor):\n        # Create a temporary file\n        temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.csv')\n        filename = os.path.basename(temp_file",
        "rewrite": ""
    },
    {
        "original": "\n\n    url = \"http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\"\n    filename = os.path.join(source_dir, \"20news-18828.tar.gz\")\n    \n    if not os.path.exists(source_dir):\n        os.makedirs(source_dir)\n    \n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n    \n    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "rewrite": ""
    },
    {
        "original": "\n\nclass MultivariateNormalModel(tf.keras.Model):\n        super(MultivariateNormalModel, self).__init__()\n        self.dimensions = dimensions\n\n        # Unused argument\n        del inputs\n        \n        # Define the mean and standard deviation for the distribution\n        mean = tf.zeros(self.dimensions)\n        stddev = tf.ones(self.dimensions)\n        \n        # Create the MultivariateNormalDiag distribution\n        distribution = tfp.distributions.MultivariateNormalDiag(loc=mean, scale_diag",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Updates the counters per state of the tasks that were running. Can re-add\n    to tasks to run in case required.\n\n    :param ti_status: the internal status of the backfill job tasks\n    :type ti_status: BackfillJob._DagRunTaskStatus\n    \"\"\"\n    for key, value in ti_status.running.items():\n        task_instance, _ = value\n        state = task_instance.state\n        if state == State.SUCCESS:\n            ti_status.succeeded.add(key)\n            ti_status.running.pop(key)\n        elif state == State.F",
        "rewrite": ""
    },
    {
        "original": "\n    task_id = context['task_instance'].xcom_pull(task_ids='your_task_id')\n    result = AsyncResult(task_id)\n    return result.ready()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Computes the log normalizing constant, log(Z).\"\"\"\n\n    # Assuming self.logits is a numpy array of logits\n    max_logit = np.max(self.logits)\n    log_sum_exp = np.log(np.sum(np.exp(self.logits - max_logit))) + max_logit\n    return log_sum_exp\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n    Args:\n        n_a: instance of a Normal distribution object.\n        n_b: instance of a Normal distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_normal_normal\".\n\n    Returns:\n        Batchwise KL(n_a || n_b)\n    \"\"\"\n    with tf.name_scope(name or \"kl_normal_normal\"):\n        mean_a = n_a.mean()\n        mean",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Handles all unsupported types of Alexa requests. Returns standard message.\n\n    Args:\n        request: Alexa request.\n    Returns:\n        response: \"response\" part of response dict conforming Alexa specification.\n    \"\"\"\n    return {\n        \"outputSpeech\": {\n            \"type\": \"PlainText\",\n            \"text\": \"Sorry, I can't handle that request.\"\n        },\n        \"shouldEndSession\": True\n    }\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Summarize the parameters of a distribution.\n\n    Args:\n        dist: A Distribution object with mean and standard deviation\n              parameters.\n        name: The name of the distribution.\n        name_scope: The name scope of this summary.\n    \"\"\"\n    with tf.name_scope(name_scope):\n        mean = dist.mean()\n        stddev = dist.stddev()\n        tf.summary.scalar(f\"{name}_mean\", mean)\n        tf.summary.scalar(f\"{name}_stddev\", stddev)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n@tf.custom_gradient\n            raise LookupError(\"Second derivative is not allowed.\")\n        return dy, second_grad\n    return x, grad\n",
        "rewrite": ""
    },
    {
        "original": "\n    # Assuming self.model is a pre-defined model that processes the inputs\n    # and generates the intermediate representation.\n    intermediate_representation = self.model(inputs)\n    \n    # Reshape the output to the desired shape\n    sample_shape, batch_size, timesteps, height, width, channels = inputs.shape\n    hidden_size = intermediate_representation.shape[-1]\n    \n    # The intermediate representation should have the shape:\n    # [sample_shape, batch_size, timesteps, hidden_size]\n    intermediate_representation = intermediate_representation.reshape(\n        sample_shape, batch_size, timesteps",
        "rewrite": ""
    },
    {
        "original": "\n\n    while True:\n        response = self.get_operation_status(operation_name)\n        if response['status'] == 'DONE':\n            if 'error' in response:\n                raise AirflowException(f\"Operation {operation_name} failed: {response['error']}\")\n            return response\n        time.sleep(5)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Clears a set of task instances, but makes sure the running ones\n    get killed.\n\n    :param tis: a list of task instances\n    :param session: current session\n    :param activate_dag_runs: flag to check for active dag run\n    :param dag: DAG object\n    \"\"\"\n    if not tis:\n        return\n\n    task_ids = [ti.task_id for ti in tis]\n    dag_ids",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Visualizes sequences as TensorBoard summaries.\n\n    Args:\n        seqs: A tensor of shape [n, t, h, w, c].\n        name: String name of this summary.\n        num: Integer for the number of examples to visualize. Defaults to\n            all examples.\n    \"\"\"\n    if num is None:\n        num = seqs.shape[0]\n    \n    # Select the first `num` sequences\n    seqs = seqs[:num]\n    \n    # Reshape the sequences to combine the time dimension",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Get a set of records from Presto\n    \"\"\"\n    conn = prestodb.dbapi.connect(\n        host='your-presto-host',\n        port=8080,\n        user='your-username',\n        catalog='your-catalog',\n        schema='your-schema',\n    )\n    cur = conn.cursor()\n    cur.execute(hql, parameters or [])\n    records = cur.fetchall()\n    cur.close()\n    conn.close()\n    return records\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Execute the bash command in a temporary directory\n    which will be cleaned afterwards\n    \"\"\"\n    command = context.get('command')\n    if not command:\n        raise ValueError(\"No command provided in context\")\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            result = subprocess.run(command, shell=True, cwd=temp_dir, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            return result.stdout.decode('utf-8')\n        except subprocess.CalledProcessError as e:\n            raise RuntimeError(f\"",
        "rewrite": ""
    },
    {
        "original": "\n\n                            fn_arg_list,\n                            result=None,\n                            grads=None,\n                            check_non_none_grads=True,\n                            name=None):\n    \"\"\"Calls `fn` and computes the gradient of the result wrt `args_list`.\"\"\"\n    with tf.name_scope(name or \"maybe_call_fn_and_grads\"):\n        if result is None:\n            with tf.GradientTape() as tape:\n                tape.watch(fn_arg_list)\n                result = fn(*fn_arg_list)\n        else:\n            tape = tf.GradientTape()\n            tape.watch(fn_arg_list)\n        \n",
        "rewrite": ""
    },
    {
        "original": "\n@classmethod\n    \"\"\"\n    Store an XCom value.\n    TODO: \"pickling\" has been deprecated and JSON is preferred.\n    \"pickling\" will be removed in Airflow 2.0.\n\n    :return: None\n    \"\"\"\n\n    @provide_session\n        if session is None:\n            raise ValueError(\"Session must be provided\")\n\n        # Convert value to",
        "rewrite": ""
    },
    {
        "original": "\n\n    if not callable(true_fn):\n        raise TypeError(\"`true_fn` must be callable.\")\n    if not callable(false_fn):\n        raise TypeError(\"`false_fn` must be callable.\")\n    \n    if isinstance(pred, bool) or tf.is_tensor(pred) and pred.shape == ():\n        return true_fn() if pred else false_fn()\n    else:\n        return tf.cond(pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Helper function that maps from MySQL fields to BigQuery fields. Used\n    when a schema_filename is set.\n    \"\"\"\n    mysql_to_bq = {\n        'TINYINT': 'INTEGER',\n        'SMALLINT': 'INTEGER',\n        'MEDIUMINT': 'INTEGER',\n        'INT': 'INTEGER',\n        'BIGINT': 'INTEGER',\n        'FLOAT': 'FLOAT',\n        'DOUBLE': 'FLOAT',\n        'DECIMAL': 'NUMERIC',\n        'DATE': 'DATE',\n        'DATETIME': 'DATETIME',\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Calculates the reshaped dimensions (replacing up to one -1 in reshape).\"\"\"\n    if validate:\n        original_size = 1\n        for dim in original_shape:\n            original_size *= dim\n\n    new_size = 1\n    unknown_index = -1\n    for i, dim in enumerate(new_shape):\n        if dim == -1:\n            if unknown_index != -1:\n                raise ValueError(f\"Only one dimension can be unknown (-1) in {name or 'reshape'}.\")\n            unknown",
        "rewrite": ""
    },
    {
        "original": "\n\n@provide_session\n    dag_id = args.dag_id\n\n    session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id).delete(synchronize_session='fetch')\n    session.query(DagRun).filter(DagRun.dag_id == dag_id).delete(synchronize_session='fetch')\n    session.query(DagModel).filter(DagModel.dag_id == dag_id).delete(synchronize_session='fetch')\n\n    session.commit()\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Upload a file to Azure Blob Storage.\"\"\"\n\n    # Retrieve connection string and other parameters from context or self\n    connection_string = self.connection_string\n    container_name = self.container_name\n    blob_name = self.blob_name\n    file_path = self.file_path\n\n    # Create the BlobServiceClient object\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\n    # Create a blob client using the local file name as the name for the blob\n   ",
        "rewrite": ""
    },
    {
        "original": "\n                          initial_args,\n                          val_0,\n                          val_c,\n                          f_lim,\n                          sufficient_decrease_param,\n                          curvature_param):\n  \"\"\"Helper function for secant-square step.\"\"\"\n    denom = (grad_c - grad_0)\n    if denom == 0:\n      step = step_c\n    else:\n      step = step_c - grad_c * (step_c - step_0) / denom\n\n    new_args",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    1. Init the default role(Admin, Viewer, User, Op, public)\n       with related permissions.\n    2. Init the custom role(dag-user) with related permissions.\n\n    :return: None.\n    \"\"\"\n    default_roles = ['Admin', 'Viewer', 'User', 'Op', 'public']\n    custom_roles = ['dag-user']\n    permissions = {\n        'Admin': ['all_permissions'],\n        'Viewer': ['view'],\n        'User': ['view', 'edit'],\n        'Op': ['view', 'edit', 'operate'],\n        '",
        "rewrite": ""
    },
    {
        "original": "\n\n    video_id = url.split('/')[-1].split('_')[0]\n    api_url = f'https://www.dailymotion.com/player/metadata/video/{video_id}'\n    \n    response = requests.get(api_url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch video metadata: {response.status_code}\")\n    \n    metadata = response.json()\n    title = metadata['title']\n    stream_url = metadata['qualities']['auto'][0]['",
        "rewrite": ""
    },
    {
        "original": "\n\n    block_blob_service = BlockBlobService(account_name='your_account_name', account_key='your_account_key')\n    block_blob_service.create_blob_from_text(container_name, blob_name, string_data, **kwargs)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"The Jeffreys Csiszar-function in log-space.\"\"\"\n    with tf.name_scope(name or 'jeffreys'):\n        u = tf.exp(logu)\n        jeffreys_of_u = 0.5 * (u * logu - logu)\n        return jeffreys_of_u\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    \u041f\u0440\u0435\u0434\u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u0431\u0443\u0434\u0443\u0449\u0438\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u0438 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0441 \u043d\u0438\u043c\u0438\n    \u0434\u043b\u044f h-\u044d\u0432\u0440\u0438\u0441\u0442\u0438\u043a\u0438\n    \"\"\"\n    self.future_symbols = {}\n    self.operation_costs = {}\n\n    for state in self.states:\n        self.future_symbols[state] = self._calculate_future_symbols(state)\n        self.operation_costs[state] = self._calculate_operation_costs(state)\n\n    # Placeholder for actual future symbols calculation logic\n    return []\n\n    # Placeholder for actual operation costs",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Applies a sequence of slice or copy-with-overrides operations to `dist`.\"\"\"\n    for slice_overrides in slice_overrides_seq:\n        if isinstance(slice_overrides, dict):\n            dist = dist.copy(**slice_overrides)\n        else:\n            dist = dist[slice_overrides]\n    return dist\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Helper to broadcast a tensor using a list of target tensors.\"\"\"\n    # Determine the target shape by finding the maximum shape along each dimension\n    target_shape = np.broadcast(*target_tensors).shape\n    \n    # Broadcast the tensor to the target shape\n    broadcasted_tensor = np.broadcast_to(tensor_to_broadcast, target_shape)\n    \n    return broadcasted_tensor\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Creates a dag run for the specified dag\n    :param dag_id: The id of the dag to trigger\n    :param run_id: The run id for this dag run\n    :param conf: Configuration for the dag run\n    :param execution_date: Execution date for the dag run\n    :",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Initialize the client\n    credential = DefaultAzureCredential()\n    client = ContainerInstanceManagementClient(credential, self.subscription_id)\n\n    # Get the container group\n    container_group = client.container_groups.get(resource_group, name)\n\n    # Extract the state, exitcode, and details\n    state = container_group.instance_view.state\n    exitcode = 0\n    details = \"\"\n\n    if container_group.containers:\n        container = container",
        "rewrite": ""
    },
    {
        "original": "\n\nclass Sample:\n        self.features = features\n        self.labels = labels\n\n    @classmethod\n            return callBigDlFunc(bigdl_type, \"createSample\", ndarray)\n\n        if isinstance(features, np.ndarray):\n            features = [to_sample(features)]\n        else:\n            features = [to_sample(f) for f in features]\n\n        if isinstance(labels, (np",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Returns a connection object\"\"\"\n    # Assuming self.connection_string is defined and contains the necessary connection info\n    return sqlite3.connect(self.connection_string)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Laplace.\n\n    Args:\n        a: instance of a Laplace distribution object.\n        b: instance of a Laplace distribution object.\n        name: (optional) Name to use for created operations.\n            default is \"kl_laplace_laplace\".\n\n    Returns:\n        Batchwise KL(a || b)\n    \"\"\"\n    with tf.name_scope(name or \"kl_laplace_laplace\"):\n        # Extract the parameters",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    An endpoint helping check the health status of the Airflow instance,\n    including metadatabase and scheduler.\n    \"\"\"\n\n    health_status = {\n        'metadatabase': 'unhealthy',\n        'scheduler': 'unhealthy'\n    }\n\n    # Check metadatabase health\n    try:\n        session.execute('SELECT 1')\n        health_status['metadatabase'] = 'healthy'\n    except Exception",
        "rewrite": ""
    },
    {
        "original": "\n\nclass LeapFrogStepState(NamedTuple):\n    state: tf.Tensor\n    momentum: tf.Tensor\n    target_log_prob: tf.Tensor\n    target_log_prob_grad: tf.Tensor\n\nclass LeapFrogStepExtras(NamedTuple):\n    target_log_prob: tf.Tensor\n    kinetic_energy: tf.Tensor\n\nFloatTensor = tf.Tensor\nPotentialFn = callable\n\n                  step_size: FloatTensor, target_log_prob_fn: PotentialFn,\n                  kinetic_energy_fn: Potential",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Takes a cursor, and writes results to a local file.\n\n    :return: A dictionary where keys are filenames to be used as object\n        names in GCS, and values are file handles to local files that\n        contain the data for the GCS objects.\n    \"\"\"\n\n    result_files = {}\n    try:\n        # Fetch all rows from the cursor\n        rows = cursor.fetchall()\n        # Get column names from the cursor\n        column_names = [desc[0] for desc in cursor.description]\n\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\n        self,\n        location,\n        product_set,\n        project_id=None,\n        product_set_id=None,\n        retry=Retry(),\n        timeout=DEFAULT,\n        metadata=None,\n    ):\n    client = vision_v1.ProductSearchClient()\n    \n    if project_id is None:\n        project_id = client.project\n    \n    parent = f\"projects/{project_id}/locations/{location}\"\n    \n    response = client.create_product_set(\n        parent=parent,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Helper method that casts a BigQuery row to the appropriate data types.\n    This is useful because BigQuery returns all fields as strings.\n    \"\"\"\n    if string_field is None:\n        return None\n\n    if bq_type == 'STRING':\n        return string_field\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        return string_field.lower() == 'true'\n    elif bq_type == '",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Computes log(sum(x**2)).\"\"\"\n    return np.log(np.sum(np.square(x), axis=axis))\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Returns an image tensor.\"\"\"\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_image(image, channels=3)\n    return image\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Get mnist dataset and parallelize into RDDs.\n    Data would be downloaded automatically if it doesn't present at the specific location.\n\n    :param sc: SparkContext.\n    :param data_type: \"train\" for training data and \"test\" for testing data.\n    :param location: Location to store mnist dataset.\n    :return: RDD of (features: ndarray, label: ndarray).\n    \"\"\"\n",
        "rewrite": ""
    },
    {
        "original": "\n      self,\n      fn,\n      event_shape_list=None,\n      static_event_shape_list=None,\n      extra_kwargs=None):\n    \"\"\"Calls `fn` and appropriately reshapes its output.\"\"\"\n    # Call the function with extra_kwargs if provided\n    if extra_kwargs is None:\n        output = fn()\n    else:\n        output = fn(**extra_kwargs)\n    \n    # If event_shape_list is provided, reshape the output accordingly\n    if event_shape_list is not None:\n        reshaped_output = []\n        for out, event_shape in zip(output, event_shape_list):\n            resh",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Returns a Hive thrift client.\n    \"\"\"\n    transport = TSocket.TSocket('localhost', 9083)\n    transport = TTransport.TBufferedTransport(transport)\n    protocol = TBinaryProtocol.TBinaryProtocol(transport)\n    client = ThriftHiveMetastore.Client(protocol)\n    transport.open()\n    return client\n",
        "rewrite": ""
    },
    {
        "original": "\n    dtypes = {arg.dtype for arg in arg_list if arg is not None}\n    return dtypes.pop() if len(dtypes) == 1 else None\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Extracts video ID from live.qq.com.\"\"\"\n    match = re.search(r'live\\.qq\\.com/(\\d+)', url)\n    if match:\n        return match.group(1)\n    return None\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Establish an AWS connection for retrieving logs during training\n\n    :rtype: CloudWatchLogs.Client\n    \"\"\"\n    return boto3.client('logs')\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    credentials = service_account.Credentials.from_service_account_file('path/to/your/service-account-file.json')\n    service = build('sqladmin', 'v1beta4', credentials=credentials)\n\n    if project_id is None:\n        project_id = 'your-default-project-id'  # Replace with your default project ID\n\n    request = service.databases().delete(\n        project=project_id,\n        instance=instance,\n        database=database\n    )\n    response =",
        "rewrite": ""
    },
    {
        "original": "\n\n               files=None, dryrun=False, cc=None, bcc=None,\n               mime_subtype='mixed', mime_charset='utf-8', **kwargs):\n    \"\"\"\n    Send email using backend specified in EMAIL_BACKEND.\n    \"\"\"\n    msg = MIMEMultipart(mime_subtype",
        "rewrite": ""
    },
    {
        "original": "\n\n        validate_args=False, name=None):\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    with tf.name_scope(name or 'MixtureDistribution'):\n        params = tf.convert_to_tensor(params, name='params')\n        mixture_weights = params[..., :num_components]\n        component_params = params[..., num_components:]\n        components = tfp.distributions.Independent(\n            tfp.distributions.Normal(loc=component_params[..., :num_components],\n                                     scale=tf.nn.softplus(component_params[..., num_components:]",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    The result would contain all of the layers including nested layers.\n    :param kmodel: a keras model which can be Sequential or Model\n    :param node_id_to_config_layer: a container to store the result\n    \"\"\"\n        for layer in layers:\n            node_id_to_config_layer[layer.name] = layer\n            if hasattr(layer, 'layers'):  # If the layer has nested layers\n                _traverse_layers(layer.layers",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Checks that `perm` is valid.\"\"\"\n    if validate_args:\n        perm = np.array(perm)\n        if perm.ndim != 1:\n            raise ValueError(f\"{name} must be a 1D array.\")\n        if not np.array_equal(np.sort(perm), np.arange(len(perm))):\n            raise ValueError(f\"{name} is not a valid permutation.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    if isinstance(input_shape, list):\n        return [self._compute_single_output_shape(shape) for shape in input_shape]\n    else:\n        return self._compute_single_output_shape(input_shape)\n\n    # Example implementation, replace with actual logic\n    output_shape = list(shape)\n    output_shape[-1] = self.units  # Assuming 'units' is an attribute of the layer\n    return tuple(output_shape)\n",
        "rewrite": ""
    },
    {
        "original": "\n                  target_log_prob=None, maybe_expand=False,\n                  description='target_log_prob'):\n  \"\"\"Processes input args to meet list-like assumptions.\"\"\"\n  if not isinstance(state, (list, tuple)):\n    state = [state]\n  if not isinstance(step_size, (list, tuple)):\n    step_size = [step_size] * len(state)\n  if target_log_prob is None:\n    target_log_prob = target_log_prob_fn(*state)\n  if maybe_expand:\n    state = [s[tf.newaxis, ...] for s in state]\n",
        "rewrite": ""
    },
    {
        "original": "\n                                   validate_args, name):\n    \"\"\"Helper to __init__ which ensures override batch/event_shape are valid.\"\"\"\n    if override_shape is None:\n        return\n    if not validate_args:\n        return\n    override_shape = tf.convert_to_tensor(override_shape, dtype=tf.int32, name=name)\n    if override_shape.shape.ndims == 0:\n        override_shape = tf.expand_dims(override_shape, axis=0)\n    if base_is_scalar:\n        if tf.reduce_any(override_shape != 1):\n            raise ValueError(f",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Builds the network using Keras.\n    \"\"\"\n\n    model = Sequential()\n\n    # Example architecture\n    model.add(Dense(64, input_dim=self.input_dim))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(self.output_dim))\n    model.add(Activation('softmax'))\n\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = speech_v1.SpeechClient()\n    \n    if isinstance(config, dict):\n        config = speech_v1.RecognitionConfig(**config)\n    \n    if isinstance(audio, dict):\n        audio = speech_v1.RecognitionAudio(**audio)\n    \n    response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n    \n    return response\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    return tfp.distributions.MultivariateNormalDiag(\n        loc=params[:event_shape[0]],\n        scale_diag=params[event_shape[0]:],\n        validate_args=validate_args,\n        name=name\n    )\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    client = pubsub_v1.PublisherClient()\n    topic_path = client.topic_path(project, topic)\n    \n    try:\n        client.create_topic(request={\"name\": topic_path})\n    except AlreadyExists:\n        if fail_if_exists:\n            raise\n",
        "rewrite": ""
    },
    {
        "original": "\n\n                      transition_matrix, transition_noise):\n    \"\"\"Propagate a filtered distribution through a transition model.\"\"\"\n    predicted_mean = np.dot(transition_matrix, filtered_mean)\n    predicted_cov = np.dot(np.dot(transition_matrix, filtered_cov), transition_matrix.T) + transition_noise\n    return predicted_mean, predicted_cov\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Build a zero-dimensional MVNDiag object.\"\"\"\n    loc = tf.constant(0.0, dtype=dtype)\n    scale_diag = tf.constant([1.0], dtype=dtype)\n    return tfp.distributions.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    if num_output_channels not in (1, 3):\n        raise ValueError(\"num_output_channels should be either 1 or 3\")\n    \n    grayscale_img = img.convert(\"L\")\n    \n    if num_output_channels == 1:\n        return grayscale_img\n    else:\n        return grayscale_img.convert(\"RGB\")\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Sample shape of random variable as a `TensorShape`.\"\"\"\n    return tf.TensorShape(self._sample_shape)\n",
        "rewrite": ""
    },
    {
        "original": "\n        self,\n        product,\n        location=None,\n        product_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductUpdateOperator`\n        \"\"\"\n        client = vision_v1.ProductSearchClient()\n\n        if project_id is None:\n            project_id = self.project_id\n\n        name = client.product_path(project_id, location, product_id)\n\n        product.name",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\" Return a JavaRDD of Object by unpickling\n\n    It will convert each Python object into Java object by Pyrolite, whenever\n    the RDD is serialized in batch or not.\n    \"\"\"\n    \n    # Ensure the RDD is serialized with BatchedSerializer\n    ser = BatchedSerializer(PickleSerializer(), batchSize=10)\n    ",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Checks that a prefix exists in a bucket\n\n    :param bucket_name: the name of the bucket\n    :type bucket_name: str\n    :param prefix: a key prefix\n    :type prefix: str\n    :param delimiter: the delimiter marks key hierarchy.\n    :type delimiter: str\n    \"\"\"\n    s3_client = boto3.client('s3')\n    response = s3_client.list_objects_v2(\n        Bucket=bucket_name,\n        Prefix=prefix,\n        Delimiter=delimiter\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    :param X: X can be a ndarray or list of ndarray if the model has multiple inputs.\n              The first dimension of X should be batch.\n    :param batch_size: total batch size of prediction.\n    :return: a ndarray as the prediction result.\n    \"\"\"\n\n    # Ensure X is a list of ndarrays\n    if not isinstance(X, list):\n        X = [X]\n\n    # Determine the total number of samples\n    num_samples = X[0].shape[0]\n\n    # If",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Find zombie task instances, which are tasks haven't heartbeated for too long.\n    :return: Zombie task instances in SimpleTaskInstance format.\n    \"\"\"\n\n    zombie_threshold = self.zombie_threshold if hasattr(self, 'zombie_threshold') else timedelta(minutes=10)\n    current_time = datetime.utcnow()\n\n    with create_session() as session:\n        zombie",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Ensure that observation data and locations have consistent shapes.\n\n    This basically means that the batch shapes are broadcastable. We can only\n    ensure this when those shapes are fully statically defined.\n\n    Args:\n        kernel: The GP kernel.\n        observation_index_points: the observation data locations in the index set.\n        observations: the observation data.\n\n    Raises:\n        ValueError: if the observations' batch shapes are not broadcastable.\n    \"\"\"\n    observation_index_points = tf.convert_to_tensor(observation_index_points)\n   ",
        "rewrite": ""
    },
    {
        "original": "\n                      initial_population,\n                      initial_position,\n                      population_size,\n                      population_stddev,\n                      max_iterations,\n                      func_tolerance,\n                      position_tolerance,\n                      differential_weight,\n                      crossover_prob,\n                      seed):\n  \"\"\"Processes initial args.\"\"\"\n  if not callable(objective_function):\n    raise ValueError(\"objective_function must be callable\")\n  \n  if initial_population is not None and initial_position is not None:\n    raise ValueError(\"Only one of initial_population or initial_position should be provided\")\n  \n  if initial_population is None and initial_position is None:\n    raise ValueError",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Wrapper around the private _get_dep_statuses method that contains some global\n    checks for all dependencies.\n\n    :param ti: the task instance to get the dependency status for\n    :type ti: airflow.models.TaskInstance\n    :param session: database session\n    :type session: sqlalchemy.orm.session.Session\n    :param dep_context: the context for which this dependency should be evaluated for\n    :type dep_context: DepContext\n    \"\"\"\n    # Perform any global checks here\n    if not ti.are_dependencies_met",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Creates initial `previous_kernel_results` using a supplied `state`.\"\"\"\n    # Assuming `self` has a method or attribute to create initial kernel results\n    return self.create_initial_kernel_results(init_state)\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Build HTML content only, no header or body tags. To be useful this\n    will usually require the attribute `jquery_on_ready` to be set which\n    will wrap the js in $(function(){<regular_js>};)\n    \"\"\"\n    html_content = \"<div>Your HTML content here</div>\"\n    if hasattr(self, 'jquery_on_ready') and self.jquery_on_ready:\n        js_content = \"$(function(){\" + self.jquery_on_ready + \"});\"\n        html_content += f\"<script>{js_content}</script>\"\n    return html_content\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    api_url = f'https://api.vimeo.com/channels/{channel_id}/videos'\n    headers = {\n        'Authorization': 'Bearer YOUR_ACCESS_TOKEN'\n    }\n    \n    response = requests.get(api_url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch videos for channel {channel_id}: {response.status_code}\")\n    \n    videos = response.json()['data']\n    \n    if",
        "rewrite": ""
    },
    {
        "original": "\n\n# this is the Alembic Config object, which provides access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(config.config_file_name)\n\n# add your model's MetaData object here for 'autogenerate' support\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n\n    \"\"\"Run",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Helper to merge which handles merging one value.\"\"\"\n    if use_equals:\n        return new if new != old else old\n    else:\n        return new if new is not None else old\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Returns the maximum representable value in this data type.\"\"\"\n    return np.iinfo(dtype).max if np.issubdtype(dtype, np.integer) else np.finfo(dtype).max\n",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Print out stats about how files are getting processed.\n\n    :param known_file_paths: a list of file paths that may contain Airflow\n        DAG definitions\n    :type known_file_paths: list[unicode]\n    :return: None\n    \"\"\"\n    total_files = len(known_file_paths)\n    processed_files = sum(1 for path in known_file_paths if self._is_file_processed(path))\n    unprocessed_files = total_files - processed_files\n\n    print(f\"Total files: {total_files}\")\n    print(f\"Processed files",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"generate javascript code for the chart\"\"\"\n    chart_data = {\n        'type': 'bar',\n        'data': {\n            'labels': ['January', 'February', 'March', 'April', 'May', 'June', 'July'],\n            'datasets': [{\n                'label': 'My First Dataset',\n                'data': [65, 59, 80, 81, 56, 55, 40],\n                'backgroundColor': [\n                    'rgba(255, 99, 132, 0.2)',\n                    'rgba(54,",
        "rewrite": ""
    },
    {
        "original": "\n\n    # Parse the URL to get the video ID\n    parsed_url = urlparse(url)\n    video_id = parsed_url.path.split('/')[-1]\n\n    # Fetch the video page\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch the video page: {response.status_code}\")\n\n    # Parse the HTML to find the video URL\n    soup =",
        "rewrite": ""
    },
    {
        "original": "\n\n__all__ = ['SqueezeNet', 'squeezenet1_1']\n\nmodel_urls = {\n    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth',\n}\n\nclass Fire(nn.Module):\n        super(Fire, self).__init__()\n        self",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Self destruct task if state has been moved away from running externally\"\"\"\n    if session is None:\n        session = self.get_session()\n\n    task_instance = session.query(TaskInstance).filter(\n        TaskInstance.dag_id == self.dag_id,\n        TaskInstance.task_id == self.task_id,\n        TaskInstance.execution_date == self.execution_date\n    ).one_or_none()\n\n    if task_instance is None:\n        return\n\n    if task_instance.state != State.RUNNING:\n        self.log.info(\"Task state is no longer running. Self-destructing.\")\n        self.clear",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Returns true if training job's secondary status message has changed.\n\n    :param current_job_description: Current job description, returned from DescribeTrainingJob call.\n    :type current_job_description: dict\n    :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.\n    :type prev_job_description: dict\n\n    :return: Whether the secondary status message of a training job changed or not.\n    \"\"\"\n    current_secondary_status = current_job_description.get('SecondaryStatusTransitions', [])\n    prev_secondary_status = prev_job_description.get",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Transforms a SQLAlchemy model instance into a dictionary\n    \"\"\"\n    return {c.name: getattr(obj, c.name) for c in obj.__table__.columns}\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Creates a character sprite from a set of attribute sprites.\"\"\"\n    # Open the images\n    skin_img = Image.open(skin).convert(\"RGBA\")\n    hair_img = Image.open(hair).convert(\"RGBA\")\n    top_img = Image.open(top).convert(\"RGBA\")\n    pants_img = Image.open(pants).convert(\"RGBA\")\n    \n    # Create a blank canvas with the same size as the skin image\n    character = Image.new(\"RGBA\", skin_img.size)\n    \n    # Paste the images on the canvas in",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Save a synthetic image as a PNG file.\n\n    Args:\n        images: samples of synthetic images generated by the generative network.\n        fname: Python `str`, filename to save the plot to.\n    \"\"\"\n    # Determine the grid size\n    grid_size = int(np.ceil(np.sqrt(images.shape[0])))\n\n    # Create a figure with subplots\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n\n    # Plot each image\n    for i, ax",
        "rewrite": ""
    },
    {
        "original": "\n\n_T = Any\n\n    \"\"\"Recursively apply config's variables values to its property\"\"\"\n    if isinstance(item, dict):\n        return {key: _parse_config_property(value, variables) for key, value in item.items()}\n    elif isinstance(item, list):\n        return [_parse_config_property(element, variables) for element in item]\n    elif isinstance(item, str):\n        for var, val in variables",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    return tfp.distributions.MultivariateNormalDiag(\n        loc=params[..., :event_size],\n        scale_diag=tf.nn.softplus(params[..., event_size:]),\n        validate_args=validate_args,\n        name=name\n    )\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    with tf.name_scope(name or \"event_size\"):\n        event_shape = tf.convert_to_tensor(event_shape, dtype=tf.int32, name=\"event_shape\")\n        if event_shape.shape.is_fully_defined():\n            return event_shape.shape.num_elements()\n        else:\n            return tf.reduce_prod(event_shape)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    video_id = parse_qs(urlparse(url).query).get('vid', [None])[0]\n    if not video_id:\n        raise ValueError(\"Invalid URL: Video ID not found\")\n\n    api_url = f\"http://api.sina.com.cn/video/{video_id}\"\n    response = requests.get(api_url)\n    if response.status_code != 200:\n        raise Exception(\"Failed to fetch video info\")\n\n    video_info =",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n    e.g. kill -s QUIT <PID> or CTRL+\\\n    \"\"\"\n    id2name = {th.ident: th.name for th in threading.enumerate()}\n    code = []\n    for thread_id, stack in sys._current_frames().items():\n        code.append(f\"\\n# Thread: {id2name.get(thread_id, thread_id)}({thread_id})\")\n        for filename, lineno, name, line",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"\n    Evaluate a model on a given dataset in distributed mode.\n\n    # Arguments\n    x: Input data. A Numpy array or RDD of Sample.\n    y: Labels. A Numpy array. Default is None if x is already RDD of Sample.\n    batch_size: Number of samples per gradient update.\n    \"\"\"\n    if isinstance(x, np.ndarray):\n        if y is None:\n            raise ValueError(\"Labels (y) must be provided when x is a Numpy array.\")\n        data = list(zip(x, y",
        "rewrite": ""
    },
    {
        "original": "\n\nclass CosmosDBClient:\n        self.client = CosmosClient(endpoint, key)\n\n        try:\n            if database_name is None:\n                raise ValueError(\"Database name must be provided\")\n\n            database = self.client.get_database_client(database_name)\n            database.create_container_if_not_exists(id=collection_name, partition_key=PartitionKey(path=\"/id\"))\n            print(f\"Collection '{collection_name}' created successfully in database '{database_name}'\")\n        except exceptions.Cosmos",
        "rewrite": ""
    },
    {
        "original": "\n    \"\"\"Checks the validity of a sample.\"\"\"\n    if not isinstance(x, (int, float)):\n        raise ValueError(\"Sample must be an integer or float.\")\n    if x < 0:\n        raise ValueError(\"Sample must be non-negative.\")\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"Create the distribution instance from a `params` vector.\"\"\"\n    if dtype is None:\n        dtype = tf.float32\n\n    params = tf.convert_to_tensor(params, dtype=dtype, name=\"params\")\n    with tf.name_scope(name or \"MixtureDistribution\"):\n        # Split the params into the mixture weights and the component parameters\n        mixture_weights, component_params = tf.split(params, [num_components, event_size * num_components], axis=-1",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Sleeps until all the processors are done.\n    \"\"\"\n    while not self.all_processors_done():\n        time.sleep(1)\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    \"\"\"\n    Get datetime of the next retry if the task instance fails. For exponential\n    backoff, retry_delay is used as base and will be converted to seconds.\n    \"\"\"\n    if self.try_number <= 1:\n        return self.start_date + self.retry_delay\n\n    delay_seconds = self.retry_delay.total_seconds()\n    backoff_factor = math.pow(2, self.try_number - 1)\n    next_retry_delay = timedelta(seconds=delay_seconds * backoff_factor)\n\n    return self.start_date + next_retry_delay\n",
        "rewrite": ""
    }
]