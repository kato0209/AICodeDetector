[
    {
        "original": "\n    repo = None\n    try:\n        import git\n        repo = git.Repo('.git')\n    except ImportError:\n        logger.warning('gitpython not found: Cannot compute the git version.')\n        return ''\n    except Exception as e:\n        logger.warning('Cannot compute the git version. {}'.format(e))\n        return ''\n    if repo:\n        sha = repo.head.commit.hexsha\n        if repo.is_dirty():\n            return '.dev0+{sha}.dirty'.format(sha=sha)\n   ",
        "rewrite": ""
    },
    {
        "original": "\n    single_draw_shape = concat_vectors(self.batch_shape_tensor(),\n                                       self.event_shape_tensor())\n    m = tf.reshape(\n        self.distribution.mean(),  # A scalar.\n        shape=tf.ones_like(single_draw_shape, dtype=tf.int32))\n    m = tf.tile(m, multiples=single_draw_shape)\n    tensorshape_util.set_shape(\n        m, tensorshape_util.concatenate(self.batch_shape, self.event_shape))\n    return m",
        "rewrite": ""
    },
    {
        "original": "\n        local_loc = self._init_file(filename)\n        self.handler = logging.FileHandler(local_loc)\n        self.handler.setFormatter(self.formatter)\n        self.handler.setLevel(self.level)\n\n        if self._cur_date < datetime.today():\n            self._symlink_latest_log_directory()\n            self._cur_date = datetime.today()",
        "rewrite": ""
    },
    {
        "original": "\n\n  with tf.compat.v1.name_scope(name, \"jeffreys\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return 0.5 * tf.math.expm1(logu) * logu",
        "rewrite": ""
    },
    {
        "original": "\n\n    def parse_manifest_json():\n        # noinspection PyBroadException\n        try:\n            global manifest\n            manifest_file = os.path.join(os.path.dirname(__file__),\n                                         'static/dist/manifest.json')\n            with open(manifest_file, 'r') as f:\n                manifest.update(json.load(f))\n\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        task = self.task\n\n        if not task.downstream_task_ids:\n            return True\n\n        ti = session.query(func.count(TaskInstance.task_id)).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.task_id.in_(task.downstream_task_ids),\n            TaskInstance.execution_date == self.execution_date,\n            TaskInstance.state == State.SUCCESS,\n        )\n        count = ti[0][0]\n        return count == len(task.downstream_task_ids)",
        "rewrite": ""
    },
    {
        "original": "\n    config = parse_config(config)\n\n    if serialized:\n        serialized: list = pickle.loads(serialized)\n\n    if download:\n        deep_download(config)\n\n    import_packages(config.get('metadata', {}).get('imports', []))\n\n    model_config = config['chainer']\n\n    model = Chainer(model_config['in'], model_config['out'], model_config.get('in_y'))\n\n    for component_config in model_config['pipe']:\n        if load_trained and ('fit_on' in component_config or 'in_y' in component_config):\n            try:\n                component_config['load_path'] = component_config['save_path']\n            except KeyError:\n     ",
        "rewrite": ""
    },
    {
        "original": "\n    return MixtureSameFamily.params_size(\n        num_components,\n        IndependentNormal.params_size(event_shape, name=name),\n        name=name)",
        "rewrite": ""
    },
    {
        "original": "\n    event_dim = (\n        tf.compat.dimension_value(self.event_shape[0]) or\n        self._event_shape_tensor()[0])\n    basis = tf.concat([[1.], tf.zeros([event_dim - 1], dtype=self.dtype)],\n                      axis=0),\n    u = tf.nn.l2_normalize(basis - self.mean_direction, axis=-1)\n    return samples - 2 * tf.reduce_sum(\n        input_tensor=samples * u, axis=-1, keepdims=True) * u",
        "rewrite": ""
    },
    {
        "original": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.insert_one(doc, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n        converted_row = []\n        for col_name, col_val in zip(schema, row):\n            if type(col_val) in (datetime, date):\n                col_val = time.mktime(col_val.timetuple())\n            elif isinstance(col_val, Decimal):\n                col_val = float(col_val)\n            elif col_type_dict.get(col_name) == \"BYTES\":\n                col_val = base64.standard_b64encode(col_val).decode('ascii')\n     ",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_conn()\n        self.log.info('Retrieving file from FTP: %s', remote_full_path)\n        conn.get(remote_full_path, local_full_path)\n        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)",
        "rewrite": ""
    },
    {
        "original": "\n        if isinstance(keys, list):\n            keys = keys\n        else:\n            keys = [keys]\n\n        delete_dict = {\"Objects\": [{\"Key\": k} for k in keys]}\n        response = self.get_conn().delete_objects(Bucket=bucket,\n                                                  Delete=delete_dict)\n     ",
        "rewrite": ""
    },
    {
        "original": "\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[List[str]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer",
        "rewrite": ""
    },
    {
        "original": "\n        local_loc = self._init_file(ti)\n        self.handler = logging.FileHandler(local_loc)\n        self.handler.setFormatter(self.formatter)\n        self.handler.setLevel(self.level)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'sparse_or_dense_matvecmul',\n                               [sparse_or_dense_matrix, dense_vector]):\n    dense_vector = tf.convert_to_tensor(\n        value=dense_vector, dtype_hint=tf.float32, name='dense_vector')\n    return tf.squeeze(\n        sparse_or_dense_matmul(\n            sparse_or_dense_matrix,\n            dense_vector[..., tf.newaxis],\n            validate_args=validate_args,\n            **kwargs),\n        axis=[-1])",
        "rewrite": ""
    },
    {
        "original": "\n    model = SqueezeNet(version=1.1, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['squeezenet1_1']))\n    return model",
        "rewrite": ""
    },
    {
        "original": "\n        if not hasattr(self, 'sess'):\n            raise RuntimeError('Your TensorFlow model {} must'\n                               ' have sess attribute!'.format(self.__class__.__name__))\n        path = str(self.load_path.resolve())\n        # Check presence of the model files\n        if tf.train.checkpoint_exists(path):\n            log.info('[loading model from {}]'.format(path))\n            # Exclude optimizer variables from saved",
        "rewrite": ""
    },
    {
        "original": "\n        self._hook = SparkSqlHook(sql=self._sql,\n                                  conf=self._conf,\n                                  conn_id=self._conn_id,\n                                  total_executor_cores=self._total_executor_cores,\n               ",
        "rewrite": ""
    },
    {
        "original": "\n    args = parser.parse_args()\n    path = get_settings_path()\n\n    if args.default:\n        if populate_settings_dir(force=True):\n            print(f'Populated {path} with default settings files')\n        else:\n            print(f'{path} is already a default settings directory')\n    else:\n        print(f'Current DeepPavlov settings path: {path}')",
        "rewrite": ""
    },
    {
        "original": "\n        if not tasks:\n            return\n\n        task_ids = [d.task_id for d in tasks]\n        now = timezone.utcnow()\n\n        if dag_run:\n            session.query(TaskInstance).filter(\n                TaskInstance.dag_id == dag_run.dag_id,\n                TaskInstance.execution_date == dag_run.execution_date,\n                TaskInstance.task_id.in_(task_ids)\n        ",
        "rewrite": ""
    },
    {
        "original": "\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n\n        dag = self.get_dag()\n        tis = self.get_task_instances(session=session)\n\n        # check for removed or restored tasks\n        task_ids = []\n        for ti in tis:\n            task_ids.append(ti.task_id)\n            task = None\n            try:\n                task = dag.get_task(ti.task_id)\n",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL',\n                                 [params, event_size]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      scale_tril = tfb.ScaleTriL(\n          diag_shift=np.array(1e-5, params.dtype.as_numpy_dtype()),\n          validate_args=validate_args)\n      return tfd.MultivariateNormalTriL(\n          loc=params[..., :event_size],\n          scale_tril=scale_tril(params[..., event_size:]),\n          validate_args=validate_args)",
        "rewrite": ""
    },
    {
        "original": "\n\n    if job_description is None or job_description.get('SecondaryStatusTransitions') is None\\\n            or len(job_description.get('SecondaryStatusTransitions')) == 0:\n        return ''\n\n    prev_description_secondary_transitions = prev_description.get('SecondaryStatusTransitions')\\\n        if prev_description is not None else None\n    prev_transitions_num = len(prev_description['SecondaryStatusTransitions'])\\\n        if prev_description_secondary_transitions is not None else 0\n    current_transitions = job_description['SecondaryStatusTransitions']\n\n    transitions_to_print = current_transitions[-1:] if len(current_transitions) == prev_transitions_num else \\\n        current_transitions[prev_transitions_num - len(current_transitions):]\n\n    status_strs = []\n    for transition in transitions_to_print:\n        message = transition['StatusMessage']\n ",
        "rewrite": ""
    },
    {
        "original": "\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  if stride != 1 or filters != x.shape[1]:\n    shortcut = _projection_shortcut(x, filters, stride, kernel_posterior_fn)\n  else:\n    shortcut = x\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=stride,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.BatchNormalization()(x)\n  x = tf.keras.layers.Activation('relu')(x)\n\n  x = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      strides=1,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  x = tf.keras.layers.add([x, shortcut])\n  return x",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n\n        product_name = ProductSearchClient.product_path(project_id, location, product_id)\n        product_set_name = ProductSearchClient.product_set_path(project_id, location, product_set_id)\n\n        self.log.info('Add Product[name=%s] to Product Set[name=%s]', product_name, product_set_name)\n\n        client.add_product_to_product_set(\n            name=product_set_name, product=product_name, retry=retry, timeout=timeout, metadata=metadata\n        )\n\n        self.log.info('Product added to Product Set')",
        "rewrite": ""
    },
    {
        "original": "\n        run_with = run_with or []\n        cmd = [\" \".join(self._command)] if join_args else self._command\n        full_cmd = run_with + cmd\n\n        self.log.info('Running: %s', full_cmd)\n        proc = subprocess.Popen(\n            full_cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n            close_fds=True,\n        ",
        "rewrite": ""
    },
    {
        "original": "\n        hook = WasbHook(wasb_conn_id=self.wasb_conn_id)\n        self.log.info(\n            'Uploading %s to wasb://%s '\n            'as %s'.format(self.file_path, self.container_name, self.blob_name)\n        )\n        hook.load_file(self.file_path, self.container_name,\n                       self.blob_name, **self.load_options)",
        "rewrite": ""
    },
    {
        "original": "\n  fig = plt.figure(figsize=(4, 4))\n  canvas = backend_agg.FigureCanvasAgg(fig)\n\n  for i, image in enumerate(images):\n    ax = fig.add_subplot(4, 4, i + 1)\n    plt.axis('off')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.imshow(image.reshape(IMAGE_SHAPE[:-1]), cmap='Greys_r')\n\n  fig.tight_layout()\n  plt.subplots_adjust(wspace=0.05, hspace=0.05)\n  canvas.print_figure(fname, format='png')",
        "rewrite": ""
    },
    {
        "original": "\n        if self.container:\n            return\n\n        # Create SVG div with style\n        if self.width:\n            if self.width[-1] != '%':\n                self.style += 'width:%spx;' % self.width\n            else:\n                self.style += 'width:%s;' % self.width\n        if self.height:\n        ",
        "rewrite": ""
    },
    {
        "original": "\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn`, `event_shape_list`, `static_event_shape_list` and/or\n    # `extra_kwargs` as keys.\n    with tf.control_dependencies(self._runtime_assertions):\n      if event_shape_list is None:\n        event_shape_list = [self._event_shape_tensor()]\n      if static_event_shape_list is None:\n        static_event_shape_list = [self.event_shape]\n      new_shape = tf.concat(\n          [self._batch_shape_unexpanded] + event_shape_list, axis=0)\n      result = tf.reshape(fn(**extra_kwargs) if extra_kwargs else fn(),\n      ",
        "rewrite": ""
    },
    {
        "original": "\n        body = self._inject_project_id(body, BODY, PROJECT_ID)\n        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)",
        "rewrite": ""
    },
    {
        "original": "\n    with self._name_scope(name):\n      seq = self._multi_gamma_sequence(a, p)\n      return tf.reduce_sum(input_tensor=tf.math.digamma(seq), axis=[-1])",
        "rewrite": ""
    },
    {
        "original": "\n        payload = {}\n\n        for key in [\n            \"message\", \"alias\", \"description\", \"responders\",\n            \"visibleTo\", \"actions\", \"tags\", \"details\", \"entity\",\n            \"source\", \"priority\", \"user\", \"note\"\n        ]:\n            val = getattr(self, key)\n            if val:\n                payload[key] = val\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  # TODO(axch) Does this already exist somewhere?  Should it get contributed?\n  multiples = tf.concat([[n], tf.ones_like(tensor.shape)], axis=0)\n  return tf.tile(tf.expand_dims(tensor, axis=0), multiples)",
        "rewrite": ""
    },
    {
        "original": "\n    symbols = []\n    for production in tf.unstack(productions, axis=1):\n      lhs, rhs = self.production_rules[tf.argmax(input=production, axis=-1)]\n      if not symbols:  # first iteration\n        if lhs != self.start_symbol:\n          raise ValueError(\"`productions` must begin with `self.start_symbol`.\")\n        symbols = rhs\n      else:\n        # Greedily unroll the nonterminal symbols based on the first occurrence\n        # in a linear sequence.\n        index = symbols.index(lhs)\n        symbols =",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"process_quadrature_grid_and_probs\"):\n    if quadrature_grid_and_probs is None:\n      grid, probs = np.polynomial.hermite.hermgauss(deg=8)\n      grid = grid.astype(dtype_util.as_numpy_dtype(dtype))\n      probs = probs.astype(dtype_util.as_numpy_dtype(dtype))\n      probs /= np.linalg.norm(probs, ord=1, keepdims=True)\n      grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n      probs = tf.convert_to_tensor(value=probs, name=\"probs\", dtype=dtype)\n      return grid, probs\n\n    grid, probs = tuple(quadrature_grid_and_probs)\n    grid = tf.convert_to_tensor(value=grid, name=\"grid\", dtype=dtype)\n    probs = tf.convert_to_tensor(\n        value=probs, name=\"unnormalized_probs\", dtype=dtype)\n    probs /= tf.norm(tensor=probs, ord=1, axis=-1, keepdims=True, name=\"probs\")\n\n    def _static_event_size(x):\n      ",
        "rewrite": ""
    },
    {
        "original": "\n    _logger = logger\n    while _logger:\n        for handler in _logger.handlers:\n            try:\n                handler.set_context(value)\n            except AttributeError:\n                # Not all handlers need to have context passed in so we ignore\n                # the error when handlers do not have set_context defined.\n           ",
        "rewrite": ""
    },
    {
        "original": "\n  # Keep the number of terms as a float. It should be a small integer, so\n  # exactly representable as a float.\n  num_terms = tf.cast(num_terms, dtype=dtype)\n\n  def loop_body(n, rn, drn_dconcentration, vn, dvn_dconcentration):\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(\n        name, 'CategoricalMixtureOfOneHotCategorical_params_size',\n        [event_size, num_components]):\n      return MixtureSameFamily.params_size(\n          num_components,\n          OneHotCategorical.params_size(event_size, name=name),\n          name=name)",
        "rewrite": ""
    },
    {
        "original": "\n    filename_template, filename_jinja_template = parse_template_string(filename_template)\n    if filename_jinja_template:\n        jinja_context = ti.get_template_context()\n        jinja_context['try_number'] = try_number\n        return filename_jinja_template.render(**jinja_context)\n\n    return filename_template.format(dag_id=ti.dag_id,\n                                    task_id=ti.task_id,\n                                    execution_date=ti.execution_date.isoformat(),\n         ",
        "rewrite": ""
    },
    {
        "original": "\n        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))\n        col_type_dict = self._get_col_type_dict()\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)\n        if self.export_format == 'csv':\n            file_mime_type = 'text/csv'\n        else:\n            file_mime_type = 'application/json'\n        files_to_upload = [{\n            'file_name': self.filename.format(file_no),\n            'file_handle': tmp_file_handle,\n ",
        "rewrite": ""
    },
    {
        "original": "\n        for dag_run in dag_runs:\n            dag_run.update_state()\n            if dag_run.state not in State.finished():\n                dag_run.set_state(State.FAILED)\n            session.merge(dag_run)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n        tf.math.log(a.concentration)",
        "rewrite": ""
    },
    {
        "original": "\n    try:\n        info = get_task(dag_id, task_id)\n    except AirflowException as err:\n        _log.info(err)\n        response = jsonify(error=\"{}\".format(err))\n        response.status_code = err.status_code\n        return response\n\n    # JSONify and return.\n    fields = {k: str(v)\n              for k, v in vars(info).items()\n              if not k.startswith('_')}\n    return jsonify(fields)",
        "rewrite": ""
    },
    {
        "original": "\n    webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1'.format(item_id = item_id)\n\n    #grab download URL\n    a = get_content(webpage_url, decoded=True)\n    url = match1(a, r'<source src=\"(.*?)\\\"\\W')\n\n    #grab title\n    title = match1(a, r'<meta property=\"og:title\" content=\"([^\"]*)\"')\n\n    type_, ext, size = url_info(url)\n    print_info(site_info, title, type_, size)\n    if not info_only:\n        download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.debug('Start syncing user roles.')\n        # Create global all-dag VM\n        self.create_perm_vm_for_all_dag()\n\n        # Create default user role.\n        for config in self.ROLE_CONFIGS:\n            role = config['role']\n            vms = config['vms']\n            perms = config['perms']\n            self.init_role(role, vms, perms)\n        self.create_custom_dag_permission_view()\n\n        # init",
        "rewrite": ""
    },
    {
        "original": "\n  def _fn(self):\n    if any(self._dist_fn_args):  # pylint: disable=protected-access\n      raise ValueError(\n          'Can only compute ' + attr + ' when all distributions are '\n          'independent; {}'.format(self.model))\n    return self._unflatten(getattr(d(), attr)() for d in self._dist_fn_wrapped)  # pylint: disable=protected-access\n  return _fn",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.annotator_client\n\n        self.log.info(\"Detecting safe search\")\n\n        if additional_properties is None:\n            additional_properties = {}\n\n        response = client.safe_search_detection(\n            image=image, max_results=max_results, retry=retry, timeout=timeout, **additional_properties\n        )\n        response = MessageToDict(response)\n        self._check_for_error(response)\n\n        self.log.info(\"Safe search detection finished\")\n        return response",
        "rewrite": ""
    },
    {
        "original": "\n\n    args = parser.parse_args()\n    run_ms_bot_framework_server(agent_generator=make_agent,\n                                app_id=args.ms_id,\n                                app_secret=args.ms_secret,\n                                stateful=True)",
        "rewrite": ""
    },
    {
        "original": "\n        word_inputs = kl.Input(shape=(None, MAX_WORD_LENGTH+2), dtype=\"int32\")\n        inputs = [word_inputs]\n        word_outputs = self._build_word_cnn(word_inputs)\n        if len(self.word_vectorizers) > 0:\n            additional_word_inputs = [kl.Input(shape=(None, input_dim), dtype=\"float32\")\n                                      for input_dim, dense_dim in self.word_vectorizers]\n            inputs.extend(additional_word_inputs)\n            additional_word_embeddings = [kl.Dense(dense_dim)(additional_word_inputs[i])\n",
        "rewrite": ""
    },
    {
        "original": "\n    X, Y = mnist.read_data_sets(location, data_type)\n    return X, Y + 1",
        "rewrite": ""
    },
    {
        "original": "\n    assert key is not None, '\"key\" parameter is null'\n\n    result = self.do_json_request('/3/Frames.json/' + key, cmd='delete', timeout=timeoutSecs)\n\n    # TODO: look for what?\n    if not ignoreMissingKey and 'f00b4r' in result:\n        raise ValueError('Frame key not found: ' + key)\n    return result",
        "rewrite": ""
    },
    {
        "original": "\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c",
        "rewrite": ""
    },
    {
        "original": "\n        from airflow.models.dag import DagModel  # Avoid circular import\n\n        # If asking for a known subdag, we want to refresh the parent\n        root_dag_id = dag_id\n        if dag_id in self.dags:\n            dag = self.dags[dag_id]\n            if dag.is_subdag:\n                root_dag_id = dag.parent_dag.dag_id\n\n        # If the dag corresponding to root_dag_id is absent or expired\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    model = AlexNet(**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n    return model",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        name = ProductSearchClient.product_set_path(project_id, location, product_set_id)\n        self.log.info('Deleting ProductSet: %s', name)\n        client.delete_product_set(name=name, retry=retry, timeout=timeout, metadata=metadata)\n        self.log.info('ProductSet with the name [%s] deleted.', name)",
        "rewrite": ""
    },
    {
        "original": "\n    f1_total = 0.0\n    for ground_truth, prediction in zip(y_true, y_predicted):\n        prediction_tokens = normalize_answer(prediction).split()\n        f1s = []\n        for gt in ground_truth:\n            gt_tokens = normalize_answer(gt).split()\n            if len(gt_tokens) == 0 or len(prediction_tokens) == 0:\n                f1s.append(float(gt_tokens == prediction_tokens))\n                continue\n            common = Counter(prediction_tokens)",
        "rewrite": ""
    },
    {
        "original": "\n    a = self.do_json_request('3/ImportFiles.json',\n        timeout=timeoutSecs,\n        params={\"path\": path}\n    )\n    verboseprint(\"\\nimport_files result:\", dump_json(a))\n    h2o_sandbox.check_sandbox_for_errors()\n    return a",
        "rewrite": ""
    },
    {
        "original": "\n        from airflow.models.taskinstance import TaskInstance  # Avoid circular import\n        tis = session.query(TaskInstance).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.execution_date == self.execution_date,\n        )\n        if state:\n            if isinstance(state, six.string_types):\n                tis = tis.filter(TaskInstance.state == state)\n            else:\n          ",
        "rewrite": ""
    },
    {
        "original": "\n    e = s\n    e = e.replace('\\\\', '\\\\\\\\')\n    e = e.replace('\\n', '\\\\n')\n    e = e.replace('\\r', '\\\\r')\n    e = e.replace(\"'\", \"\\\\'\")\n    e = e.replace('\"', '\\\\\"')\n    return e",
        "rewrite": ""
    },
    {
        "original": "\n  return tf.reduce_logsumexp(\n      input_tensor=2. * tf.math.log(tf.abs(x)), axis=axis)",
        "rewrite": ""
    },
    {
        "original": "\n        schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description))\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)\n        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}\n\n        for row in cursor:\n            # Convert if needed\n            row = map(self.convert_types, row)\n            row_dict = dict(zip(schema, row))\n\n            s = json.dumps(row_dict, sort_keys=True)\n          ",
        "rewrite": ""
    },
    {
        "original": "\n        jmodel = callBigDlFunc(bigdl_type, \"loadCaffe\", model, defPath, modelPath, match_all)\n        return Layer.of(jmodel)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_chi_chi\"):\n    # Consistent with\n    # https://mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 118\n    # The paper introduces an additional scaling parameter; setting that\n    # parameter to 1 and simplifying yields the expression we use here.\n    return (0.5 * tf.math.digamma(0.5 * a.df) * (a.df - b.df) +\n            tf.math.lgamma(0.5 * b.df) - tf.math.lgamma(0.5 * a.df))",
        "rewrite": ""
    },
    {
        "original": "\n    config = parse_config(config)\n\n    if download:\n        deep_download(config)\n\n    if to_train and recursive:\n        for subconfig in get_all_elems_from_json(config['chainer'], 'config_path'):\n            log.info(f'Training \"{subconfig}\"')\n            train_evaluate_model_from_config(subconfig, download=False, recursive=True)\n\n    import_packages(config.get('metadata', {}).get('imports', []))\n\n    if iterator is None:\n        try:\n            data = read_data_by_config(config)\n        except ConfigError as e:\n            to_train = False\n  ",
        "rewrite": ""
    },
    {
        "original": "\n        self.value = callBigDlFunc(bigdl_type,\n                                 \"transformImageFrame\", transformer, self.value)\n        return self",
        "rewrite": ""
    },
    {
        "original": "\n        hit = re.search(r'live.qq.com/(\\d+)', url)\n        if hit is not None:\n            return hit.group(1)\n        hit = re.search(r'live.qq.com/directory/match/(\\d+)', url)\n        if hit is not None:\n            return self.get_room_id_from_url(hit.group(1))\n        html = get_content(url)\n        room_id = match1(html, r'room_id\\\":(\\d+)')\n        if room_id is None:\n            log.wtf('Unknown page {}'.format(url))\n        return room_id",
        "rewrite": ""
    },
    {
        "original": "\n        payload = {}\n\n        if self.username:\n            payload['username'] = self.username\n        if self.avatar_url:\n            payload['avatar_url'] = self.avatar_url\n\n        payload['tts'] = self.tts\n\n        if len(self.message) <= 2000:\n            payload['content'] = self.message\n        else:\n            raise AirflowException('Discord message length must be 2000 or fewer '\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.info('Tmp dir root location: \\n %s', gettempdir())\n\n        # Prepare env for child process.\n        if self.env is None:\n            self.env = os.environ.copy()\n\n        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)\n        self.log.info('Exporting the following env vars:\\n%s',\n                      '\\n'.join([\"{}={}\".format(k, v)\n                             ",
        "rewrite": ""
    },
    {
        "original": "\n    image_shape = tf.shape(input=inputs)[-3:]\n    collapsed_shape = tf.concat(([-1], image_shape), axis=0)\n    out = tf.reshape(inputs, collapsed_shape)  # (sample*batch*T, h, w, c)\n    out = self.conv1(out)\n    out = self.conv2(out)\n    out = self.conv3(out)\n    out = self.conv4(out)\n    expanded_shape = tf.concat((tf.shape(input=inputs)[:-3], [-1]), axis=0)\n    return tf.reshape(out, expanded_shape)",
        "rewrite": ""
    },
    {
        "original": "\n  def delta(fn, is_property=True):\n    fn1 = getattr(d1, fn)\n    fn2 = getattr(d2, fn)\n    return (fn2 - fn1) if is_property else (fn2() - fn1())\n\n  with tf.name_scope(name or \"kl_beta_beta\"):\n    return (delta(\"_log_normalization\", is_property=False) -\n            tf.math.digamma(d1.concentration1) * delta(\"concentration1\") -\n            tf.math.digamma(d1.concentration0) * delta(\"concentration0\") +\n            (tf.math.digamma(d1.total_concentration) *\n             delta(\"total_concentration\")))",
        "rewrite": ""
    },
    {
        "original": "\n    if isinstance(item, str):\n        return item.format(**variables)\n    elif isinstance(item, list):\n        return [_parse_config_property(item, variables) for item in item]\n    elif isinstance(item, dict):\n        return {k: _parse_config_property(v, variables) for k, v in item.items()}\n    else:\n        return item",
        "rewrite": ""
    },
    {
        "original": "\n  num_words = 1000\n  vocabulary = [str(i) for i in range(num_words)]\n\n  random_sample = np.random.randint(\n      10, size=(batch_size, num_words)).astype(np.float32)\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(random_sample)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn, vocabulary",
        "rewrite": ""
    },
    {
        "original": "\n  if dist_names is None:\n    dist_names = []\n  else:\n    dist_names = dist_names.copy()\n  n = len(dist_fn_args)\n  dist_names.extend([None]*(n - len(dist_names)))\n  for i_, args in enumerate(reversed(dist_fn_args)):\n    if not args:\n      continue  # There's no args to analyze.\n    i = n - i_ - 1\n    for j, arg_name in enumerate(args):\n      dist_names[i - j - 1] = arg_name\n  j = 0\n  for i_ in range(len(dist_names)):\n    i = n - i_ - 1\n    if dist_names[i] is None:\n      dist_names[i] = leaf_name if j == 0 else leaf_name + str(j)\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        curr_agenda = [(self.root, [], 0)]\n        for i, a in enumerate(s):\n            next_agenda = []\n            for curr, borders, cost in curr_agenda:\n                if cost >= max_count:\n                    continue\n                child = self.graph[curr][self.alphabet_codes[a]]\n               ",
        "rewrite": ""
    },
    {
        "original": "\n        return [\n            cls.NONE,\n            cls.SCHEDULED,\n            cls.QUEUED,\n            cls.RUNNING,\n            cls.SHUTDOWN,\n            cls.UP_FOR_RETRY,\n            cls.UP_FOR_RESCHEDULE\n        ]",
        "rewrite": ""
    },
    {
        "original": "\n\n        @functools.wraps(func)\n        def wrapper_decorator(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except GoogleAPICallError as e:\n                if isinstance(e, AlreadyExists):\n                    raise e\n                else:\n       ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or 'maybe_validate_rightmost_transposed_ndims'):\n    assertions = []\n    if not dtype_util.is_integer(rightmost_transposed_ndims.dtype):\n      raise TypeError('`rightmost_transposed_ndims` must be integer type.')\n\n    if tensorshape_util.rank(rightmost_transposed_ndims.shape) is not None:\n      if tensorshape_util.rank(rightmost_transposed_ndims.shape) != 0:\n        raise ValueError('`rightmost_transposed_ndims` must be a scalar, '\n                         'saw rank: {}.'.format(\n                             tensorshape_util.rank(\n            ",
        "rewrite": ""
    },
    {
        "original": "\n  if not dtype_util.is_integer(shape.dtype):\n    raise TypeError('{} dtype ({}) should be `int`-like.'.format(\n        shape, dtype_util.name(shape.dtype)))\n\n  assertions = []\n\n  message = '`{}` rank should be <= 1.'\n  if tensorshape_util.rank(shape.shape) is not None:\n    if tensorshape_util.rank(shape.shape) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.rank(shape), 2, message=message.format(shape)))\n\n  shape_ = tf.get_static_value(shape)\n\n  message = '`{}` elements must have at most one `-1`.'\n  if shape_ is not None:\n    if sum(shape_ == -1) > 1:\n      raise ValueError(message.format(shape))\n  elif validate_args:\n    assertions.append(assert_util.assert_less(\n        tf.reduce_sum(input_tensor=tf.cast(tf.equal(shape, -1), tf.int32)),\n",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'event_size', [event_shape]):\n    event_shape = tf.convert_to_tensor(\n        value=event_shape, dtype=tf.int32, name='event_shape')\n\n    event_shape_const = tf.get_static_value(event_shape)\n    if event_shape_const is not None:\n      return np.prod(event_shape_const)\n    else:\n      return tf.reduce_prod(input_tensor=event_shape)",
        "rewrite": ""
    },
    {
        "original": "\n        try:\n            files = self.connection.glob(file_path, details=False, invalidate_cache=True)\n            return len(files) == 1\n        except FileNotFoundError:\n            return False",
        "rewrite": ""
    },
    {
        "original": "\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n ",
        "rewrite": ""
    },
    {
        "original": "\n    if not _is_tensor_image(tensor):\n        raise TypeError('tensor is not a torch image.')\n\n    if not inplace:\n        tensor = tensor.clone()\n\n    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)\n    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n    return tensor",
        "rewrite": ""
    },
    {
        "original": "\n    today = timezone.utcnow().replace(\n        hour=hour,\n        minute=minute,\n        second=second,\n        microsecond=microsecond)\n    return today - timedelta(days=n)",
        "rewrite": ""
    },
    {
        "original": "\n        retry_after = int(\n            rate_limit_exception.response.headers.get('Retry-After', 60))\n        self.log.info(\n            \"Hit Zendesk API rate limit. Pausing for %s seconds\",\n            retry_after\n        )\n        time.sleep(retry_after)",
        "rewrite": ""
    },
    {
        "original": "\n  # Note that `all` defaults to `True` if `arg_list` is empty.\n  if all(a is None for a in arg_list):\n    return None\n  return dtype_util.common_dtype(arg_list, tf.float32)",
        "rewrite": ""
    },
    {
        "original": "\n\n    def _get_outname(tf_name):\n        outname = re.sub(':0$', '', tf_name)\n        outname = outname.lstrip('lm/')\n        outname = re.sub('/rnn/', '/RNN/', outname)\n        outname = re.sub('/multi_rnn_cell/', '/MultiRNNCell/', outname)\n        outname = re.sub('/cell_', '/Cell', outname)\n        outname = re.sub('/lstm_cell/', '/LSTMCell/', outname)\n        if '/RNN/' in outname:\n            if 'projection' in outname:\n                outname = re.sub('projection/kernel', 'W_P_0', outname)\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  # This expansion is to help x broadcast with `y`, the output.\n  # In the non-batch case, the output shape is going to be\n  #   y_ref.shape[:axis] + x.shape + y_ref.shape[axis+1:]\n\n  # Recall we made axis non-negative\n  y_ref_shape = tf.shape(input=y_ref)\n  y_ref_shape_left = y_ref_shape[:axis]\n  y_ref_shape_right = y_ref_shape[axis + 1:]\n\n  def expand_ends(x, broadcast=False):\n    ",
        "rewrite": ""
    },
    {
        "original": "\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n        ",
        "rewrite": ""
    },
    {
        "original": "\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
        "rewrite": ""
    },
    {
        "original": "\n    print(\"Dumping stack traces for all threads in PID {}\".format(os.getpid()))\n    id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()])\n    code = []\n    for thread_id, stack in sys._current_frames().items():\n        code.append(\"\\n# Thread: {}({})\"\n                    .format(id_to_name.get(thread_id, \"\"), thread_id))\n        for filename, line_number, name, line in traceback.extract_stack(stack):\n            code.append('File: \"{}\", line {}, in {}'\n                        .format(filename, line_number, name))\n ",
        "rewrite": ""
    },
    {
        "original": "\n        # Reorder the argument order from airflow.hooks.S3_hook.load_string.\n        self.connection.create_blob_from_text(container_name, blob_name,\n                                              string_data, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n        # Explicitly getting log relative path is necessary as the given\n        # task instance might be different than task instance passed in\n        # in set_context method.\n        log_relative_path = self._render_filename(ti, try_number)\n        remote_loc = os.path.join(self.remote_base, log_relative_path)\n\n        try:\n            remote_log = self.gcs_read(remote_loc)\n            log = '*** Reading remote log from {}.\\n{}\\n'.format(\n                remote_loc, remote_log)\n",
        "rewrite": ""
    },
    {
        "original": "\n  fn_arg_list = (list(fn_arg_list) if mcmc_util.is_list_like(fn_arg_list)\n                 else [fn_arg_list])\n  if fn_result is None:\n    fn_result = fn(*fn_arg_list)\n  if not fn_result.dtype.is_floating:\n    raise TypeError('`{}` must be a `Tensor` with `float` `dtype`.'.format(\n        description))\n  return fn_result",
        "rewrite": ""
    },
    {
        "original": "\n    # what is passed in json:\n    config_params = {k: _resolve(v) for k, v in params.items()}\n\n    # get component by reference (if any)\n    if 'ref' in config_params:\n        try:\n            component = _refs[config_params['ref']]\n            if serialized is not None:\n                component.deserialize(serialized)\n            return component\n        except KeyError:\n            e =",
        "rewrite": ""
    },
    {
        "original": "\n        session.expunge_all()\n\n        enable_pickling = configuration.getboolean('core', 'enable_xcom_pickling')\n        if enable_pickling:\n            value = pickle.dumps(value)\n        else:\n            try:\n                value = json.dumps(value).encode('UTF-8')\n            except ValueError:\n                log = LoggingMixin().log\n                log.error(\"Could",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        self.log.info('Deleting ReferenceImage')\n        name = ProductSearchClient.reference_image_path(\n            project=project_id, location=location, product=product_id, reference_image=reference_image_id\n        )\n        response = client.delete_reference_image(name=name, retry=retry, timeout=timeout, metadata=metadata)\n        self.log.info('ReferenceImage with the name [%s] deleted.', name)\n\n        return MessageToDict(response)",
        "rewrite": ""
    },
    {
        "original": "\n        return callBigDlFunc(bigdl_type, \"addScheduler\", self.value, scheduler, max_iteration)",
        "rewrite": ""
    },
    {
        "original": "\n\n    offset = (params['pageno'] - 1)\n    params['url'] = search_url.format(offset=offset, query=quote(query))\n    return params",
        "rewrite": ""
    },
    {
        "original": "\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)",
        "rewrite": ""
    },
    {
        "original": "\n        self._descendance_cash = [dict() for _ in self.graph]\n        self.descend = self._descend_cashed",
        "rewrite": ""
    },
    {
        "original": "\n\n  x_train = x_train.astype(\"float32\")\n  x_test = x_test.astype(\"float32\")\n\n  x_train /= 255\n  x_test /= 255\n\n  y_train = y_train.flatten()\n  y_test = y_test.flatten()\n\n  if FLAGS.subtract_pixel_mean:\n    x_train_mean = np.mean(x_train, axis=0)\n    x_train -= x_train_mean\n    x_test -= x_train_mean\n\n  print(\"x_train shape:\" + str(x_train.shape))\n  print(str(x_train.shape[0]) + \" train samples\")\n  print(str(x_test.shape[0]) + \" test samples\")\n\n  # Build an iterator over training batches.\n  training_dataset = tf.data.Dataset.from_tensor_slices(\n      (x_train, np.int32(y_train)))\n  training_batches = training_dataset.shuffle(\n      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n\n  # Build a iterator over the heldout set with batch_size=heldout_size,\n  # i.e., return the entire heldout set as a constant.\n  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n    ",
        "rewrite": ""
    },
    {
        "original": "\n  optimizer = tf.compat.v1.train.AdamOptimizer(\n      0.1) if optimizer is None else optimizer\n\n  def train_loop_body(step):\n    train_op = optimizer.minimize(\n        build_loss_fn if tf.executing_eagerly() else build_loss_fn())\n    return tf.tuple(tensors=[tf.add(step, 1)], control_inputs=[train_op])\n\n  minimize_op = tf.compat.v1.while_loop(\n      cond=lambda step: step < num_steps,\n      body=train_loop_body,\n      loop_vars=[tf.constant(0)],\n      return_same_structure=True)[0]  # Always return a single op.\n  return minimize_op",
        "rewrite": ""
    },
    {
        "original": "\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n      ",
        "rewrite": ""
    },
    {
        "original": "\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            evaluation_data = to_sample_rdd(x, y)\n        elif isinstance(x, RDD) and not y:\n            evaluation_data = x\n        else:\n            raise TypeError(\"Unsupported evaluation data type: %s\" % type(x))\n        return callBigDlFunc(self.bigdl_type, \"evaluate\",\n                             self.value,\n      ",
        "rewrite": ""
    },
    {
        "original": "\n\n        if self._check_exists():\n            return\n\n        makedir_exist_ok(self.raw_folder)\n        makedir_exist_ok(self.processed_folder)\n\n        # download files\n        for url in self.urls:\n            filename = url.rpartition('/')[2]\n            file_path = os.path.join(self.raw_folder, filename)\n            download_url(url, root=self.raw_folder, filename=filename, md5=None)\n            self.extract_gzip(gzip_path=file_path, remove_finished=True)\n\n        # process and save as",
        "rewrite": ""
    },
    {
        "original": "\n  if a_is_sparse or b_is_sparse:\n    raise NotImplementedError('Numpy backend does not support sparse matmul.')\n  if transpose_a or adjoint_a:\n    a = _matrix_transpose(a, conjugate=adjoint_a)\n  if transpose_b or adjoint_b:\n    b = _matrix_transpose(b, conjugate=adjoint_b)\n  return np.matmul(a, b)",
        "rewrite": ""
    },
    {
        "original": "\n        db = self.get_connection(getattr(self, self.conn_name_attr))\n        return self.connector.connect(\n            host=db.host,\n            port=db.port,\n            username=db.login,\n            schema=db.schema)",
        "rewrite": ""
    },
    {
        "original": "\n  @functools.wraps(attr)\n  def func(a, *args):\n    return attr(a.value, *args)\n  return func",
        "rewrite": ""
    },
    {
        "original": "\n        # Consume the iterator\n        for line in itr:\n            line = line.strip()\n            # If we run yarn cluster mode, we want to extract the application id from\n            # the logs so we can kill the application when we stop it unexpectedly\n            if self._is_yarn and self._connection['deploy_mode'] == 'cluster':\n                match = re.search('(application[0-9_]+)', line)\n    ",
        "rewrite": ""
    },
    {
        "original": "\n    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)\n    module = importlib.import_module(path)\n    backend = getattr(module, attr)\n    to = get_email_address_list(to)\n    to = \", \".join(to)\n\n    return backend(to, subject, html_content, files=files,\n                   dryrun=dryrun, cc=cc, bcc=bcc,\n                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
        "rewrite": ""
    },
    {
        "original": "\n    n_classes = len(classes)\n    y = []\n    for sample in labels:\n        curr = np.zeros(n_classes)\n        if isinstance(sample, list):\n            for intent in sample:\n                if intent not in classes:\n                    log.warning('Unknown intent {} detected. Assigning no class'.format(intent))\n                else:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size =",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_gamma_gamma\"):\n    # Result from:\n    #   http://www.fil.ion.ucl.ac.uk/~wpenny/publications/densities.ps\n    # For derivation see:\n    #   http://stats.stackexchange.com/questions/11646/kullback-leibler-divergence-between-two-gamma-distributions   pylint: disable=line-too-long\n    return (((g0.concentration - g1.concentration) *\n             tf.math.digamma(g0.concentration)) +\n            tf.math.lgamma(g1.concentration) -\n            tf.math.lgamma(g0.concentration) +\n            g1.concentration * tf.math.log(g0.rate) -\n            g1.concentration * tf.math.log(g1.rate) + g0.concentration *\n           ",
        "rewrite": ""
    },
    {
        "original": "\n        if len(self._buffer) > 0:\n            self.logger.log(self.level, self._buffer)\n            self._buffer = str()",
        "rewrite": ""
    },
    {
        "original": "\n        if not self.conn:\n            self.conn = self.get_client_type('athena')\n        return self.conn",
        "rewrite": ""
    },
    {
        "original": "\n  # Note: This function is only valid if all of pred, cond_true, and cond_false\n  # are scalars. This means its semantics are arguably more like tf.cond than\n  # tf.where even though we use tf.where to implement it.\n  pred_ = tf.get_static_value(tf.convert_to_tensor(value=pred))\n  if pred_ is None:\n    return tf.where(pred, cond_true, cond_false)\n  return cond_true if pred_ else cond_false",
        "rewrite": ""
    },
    {
        "original": "\n  failed = state.failed | ~tf.math.is_finite(next_objective) | ~tf.reduce_all(\n      input_tensor=tf.math.is_finite(next_gradient), axis=-1)\n\n  next_position = state.position + position_delta\n  converged = ~failed & _check_convergence(state.position,\n                                           next_position,\n                                           state.objective_value,\n              ",
        "rewrite": ""
    },
    {
        "original": "\n  batch_shape_static = tensorshape_util.constant_value_as_shape(new_shape)\n  if tensorshape_util.is_fully_defined(batch_shape_static):\n    return np.int32(batch_shape_static), batch_shape_static, []\n  with tf.name_scope(name or \"calculate_reshape\"):\n    original_size = tf.reduce_prod(input_tensor=original_shape)\n    implicit_dim = tf.equal(new_shape, -1)\n    size_implicit_dim = (\n        original_size // tf.maximum(1, -tf.reduce_prod(input_tensor=new_shape)))\n    new_ndims = tf.shape(input=new_shape)\n    expanded_new_shape = tf.where(  # Assumes exactly one `-1`.\n        implicit_dim, tf.fill(new_ndims, size_implicit_dim), new_shape)\n    validations = [] if not validate else [  # pylint: disable=g-long-ternary\n        assert_util.assert_rank(\n            original_shape, 1, message=\"Original shape must be a vector.\"),\n       ",
        "rewrite": ""
    },
    {
        "original": "\n  # This is the shape of self.samples, without the samples axis, i.e. the shape\n  # of the result of a call to dist.sample(). This way we can broadcast it with\n  # event to get a properly-sized event, then add the singleton dim back at\n  # -event_ndims - 1.\n  samples_shape = tf.concat(\n      [tf.shape(input=samples)[:-event_ndims - 1],\n       tf.shape(input=samples)[tf.rank(samples) - event_ndims:]],\n      axis=0)\n  event *= tf.ones(samples_shape, dtype=event.dtype)\n  event = tf.expand_dims(event, axis=-event_ndims - 1)\n  samples *= tf.ones_like(event, dtype=samples.dtype)\n\n  return event, samples",
        "rewrite": ""
    },
    {
        "original": "\n  # Build an iterator over training batches.\n  if mnist_type in [MnistType.FAKE_DATA, MnistType.THRESHOLD]:\n    if mnist_type == MnistType.FAKE_DATA:\n      mnist_data = build_fake_data()\n    else:\n      mnist_data = mnist.read_data_sets(data_dir)\n    training_dataset = tf.data.Dataset.from_tensor_slices(\n        (mnist_data.train.images, np.int32(mnist_data.train.labels)))\n    heldout_dataset = tf.data.Dataset.from_tensor_slices(\n        (mnist_data.validation.images,\n         np.int32(mnist_data.validation.labels)))\n  elif mnist_type == MnistType.BERNOULLI:\n    training_dataset = load_bernoulli_mnist_dataset(data_dir, \"train\")\n    heldout_dataset = load_bernoulli_mnist_dataset(data_dir, \"valid\")\n  else:\n    raise ValueError(\"Unknown MNIST type.\")\n\n  training_batches = training_dataset.repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n\n  # Build a iterator over the heldout set with batch_size=heldout_size,\n ",
        "rewrite": ""
    },
    {
        "original": "\n    html = get_content(url)\n    # resourceID is UUID\n    resourceID = re.findall( r'resourceID\":\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', html)[0]\n    assert resourceID != '', 'Cannot find resourceID!'\n\n    title = match1(html, r'<div class=\"bc-h\">(.+)</div>')\n    url_lists = _ucas_get_url_lists_by_resourceID(resourceID)\n    assert url_lists, 'Cannot find any URL of such class!'\n    \n    for k, part in enumerate(url_lists):\n        part_title = title + '_' + str(k)\n        print_info(site_info, part_title, 'flv', 0)\n        if not info_only:\n            download_urls(part, part_title, 'flv', total_size=None, output_dir=output_dir, merge=merge)",
        "rewrite": ""
    },
    {
        "original": "\n  # We want to write the following 2 x 2 matrix:\n  #  [[1., 1., ],    # level(t+1) = level(t) + slope(t)\n  #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)\n  # but it's slightly tricky to properly incorporate the batch shape of\n  # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want\n  # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its\n  # fixed entries, written explicitly, and then the autoregressive_coef part\n  # which we add in after using a mask to broadcast to the correct matrix shape.\n\n  fixed_entries = tf.constant(\n      [[1., 1.],\n       [0.,",
        "rewrite": ""
    },
    {
        "original": "\n        if not self._conn:\n            self._conn = VideoIntelligenceServiceClient(credentials=self._get_credentials())\n        return self._conn",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',\n                               [model_matrix, model_coefficients, offset]):\n    predicted_linear_response = tf.linalg.matvec(model_matrix,\n                                                 model_coefficients)\n    if offset is not None:\n      predicted_linear_response += offset\n    return predicted_linear_response",
        "rewrite": ""
    },
    {
        "original": "\n        filters = []\n        if key:\n            filters.append(cls.key == key)\n        if task_ids:\n            filters.append(cls.task_id.in_(as_tuple(task_ids)))\n        if dag_ids:\n            filters.append(cls.dag_id.in_(as_tuple(dag_ids)))\n        if include_prior_dates:\n            filters.append(cls.execution_date <= execution_date)\n        else:\n            filters.append(cls.execution_date == execution_date)\n\n       ",
        "rewrite": ""
    },
    {
        "original": "\n\n  if use_exact_kl is None:\n    kl_divergence_fn = tfd.kl_divergence\n  else:\n    # Closure over: test_points_fn, test_points_reduce_axis.\n    def kl_divergence_fn(distribution_a, distribution_b):\n      z = test_points_fn(distribution_a)\n      return tf.reduce_mean(\n          input_tensor=distribution_a.log_prob(z) - distribution_b.log_prob(z),\n          axis=test_points_reduce_axis)\n\n  # Closure over: distribution_b, kl_divergence_fn, weight.\n  def _fn(distribution_a):\n    ",
        "rewrite": ""
    },
    {
        "original": "\n    # inspired by MySQL Python Connector (conversion.py)\n    string_parameters = {}\n    for (name, value) in iteritems(parameters):\n        if value is None:\n            string_parameters[name] = 'NULL'\n        elif isinstance(value, basestring):\n            string_parameters[name] = \"'\" + _escape(value) + \"'\"\n        else:\n            string_parameters[name] = str(value)\n    return operation % string_parameters",
        "rewrite": ""
    },
    {
        "original": "\n  grad_converged = norm(next_gradient, dims=1) <= grad_tolerance\n  x_converged = norm(next_position - current_position, dims=1) <= x_tolerance\n  f_converged = (norm(next_objective - current_objective, dims=0) <=\n                 f_relative_tolerance * current_objective)\n  return grad_converged | x_converged | f_converged",
        "rewrite": ""
    },
    {
        "original": "\n\n    spec = make_module_spec(options, str(weight_file))\n\n    try:\n        with tf.Graph().as_default():\n            module = hub.Module(spec)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                if hub_dir.exists():\n                    shutil.rmtree(hub_dir)\n                module.export(str(hub_dir), sess)\n    finally:\n    ",
        "rewrite": ""
    },
    {
        "original": "\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n      ",
        "rewrite": ""
    },
    {
        "original": "\n        if key is None:\n            return\n        self.log.info(\"%s running %s\", self.__class__.__name__, command)\n        try:\n            subprocess.check_call(command, close_fds=True)\n            state = State.SUCCESS\n        except subprocess.CalledProcessError as e:\n            state = State.FAILED\n            self.log.error(\"Failed to execute task %s.\", str(e))\n            # TODO: Why is",
        "rewrite": ""
    },
    {
        "original": "\n        bmodel = DefinitionLoader.from_json_path(def_json)\n        def_value = BCommon.text_from_path(def_json)\n        kmodel = model_from_json(def_value)\n        WeightLoader.load_weights_from_hdf5(bmodel, kmodel, weights_hdf5, by_name)\n        return bmodel",
        "rewrite": ""
    },
    {
        "original": "\n    site = VimeoExtractor()\n    site.download_by_vid(id, info_only=info_only, output_dir=output_dir, merge=merge, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n        TI = TaskInstance\n\n        qry = session.query(TI).filter(\n            TI.dag_id == self.dag_id,\n            TI.task_id == self.task_id,\n            TI.execution_date == self.execution_date)\n\n        if lock_for_update:\n            ti = qry.with_for_update().first()\n        else:\n            ti = qry.first()\n        if ti:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    return MixtureSameFamily.new(\n        params,\n        num_components,\n        IndependentLogistic(\n            event_shape, validate_args=validate_args, name=name),\n        validate_args=validate_args,\n        name=name)",
        "rewrite": ""
    },
    {
        "original": "\n        self.jschart = ''\n\n        # add custom tooltip string in jschart\n        # default condition (if build_custom_tooltip is not called explicitly with date_flag=True)\n        if self.tooltip_condition_string == '':\n            self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'\n\n        # Include data\n        self.series_js = json.dumps(self.series)",
        "rewrite": ""
    },
    {
        "original": "\n  # Only update the inverse Hessian if not already failed or converged.\n  should_update = ~next_state.converged & ~next_state.failed\n\n  # Compute the normalization term (y^T . s), should not update if is singular.\n  gradient_delta = next_state.objective_gradient - prev_state.objective_gradient\n  position_delta = next_state.position - prev_state.position\n  normalization_factor = tf.reduce_sum(\n      input_tensor=gradient_delta * position_delta, axis=-1)\n  should_update = should_update & ~tf.equal(normalization_factor, 0)\n\n  def _do_update_inv_hessian():\n    next_inv_hessian = _bfgs_inv_hessian_update(\n        gradient_delta, position_delta, normalization_factor,\n        prev_state.inverse_hessian_estimate)\n    return bfgs_utils.update_fields(\n        next_state,\n        inverse_hessian_estimate=tf.where(should_update,\n             ",
        "rewrite": ""
    },
    {
        "original": "\n    if not self.validate_args:\n      return x\n    return distribution_util.with_dependencies([\n        assert_util.assert_positive(x, message=\"samples must be positive\"),\n        assert_util.assert_near(\n            tf.ones([], dtype=self.dtype),\n            tf.reduce_sum(input_tensor=x, axis=-1),\n            message=\"sample last-dimension must sum to `1`\"),\n    ], x)",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.conn\n        hive_bin = 'hive'\n        cmd_extra = []\n\n        if self.use_beeline:\n            hive_bin = 'beeline'\n            jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\".format(\n                host=conn.host, port=conn.port, schema=conn.schema)\n            if configuration.conf.get('core', 'security') == 'kerberos':\n                template = conn.extra_dejson.get(\n        ",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope('get_exchanged_states'):\n\n      target_log_probs = []\n      for replica in range(self.num_replica):\n        replica_log_prob = _get_field(sampled_replica_results[replica],\n                                      'target_log_prob')\n        inverse_temp = self.inverse_temperatures[replica]\n        target_log_probs.append(replica_log_prob / inverse_temp)\n      target_log_probs = tf.stack(target_log_probs, axis=0)\n\n      dtype = target_log_probs.dtype\n      num_state_parts = len(sampled_replica_states[0])\n      # exchanged_states[k][i] is Tensor of (new) state",
        "rewrite": ""
    },
    {
        "original": "\n        import hmsclient\n        from thrift.transport import TSocket, TTransport\n        from thrift.protocol import TBinaryProtocol\n        ms = self.metastore_conn\n        auth_mechanism = ms.extra_dejson.get('authMechanism', 'NOSASL')\n        if configuration.conf.get('core', 'security') == 'kerberos':\n            auth_mechanism = ms.extra_dejson.get('authMechanism', 'GSSAPI')\n            kerberos_service_name = ms.extra_dejson.get('kerberos_service_name', 'hive')\n\n        socket = TSocket.TSocket(ms.host, ms.port)\n        if configuration.conf.get('core', 'security') == 'kerberos' \\\n        ",
        "rewrite": ""
    },
    {
        "original": "\n        try:\n            return self.hook.check_for_blob(self.wasb_container, remote_log_location)\n        except Exception:\n            pass\n        return False",
        "rewrite": ""
    },
    {
        "original": "\n\n    input_names = map(lambda x: x.name.split(\":\")[0], input_ops)\n    output_names = map(lambda x: x.name.split(\":\")[0], output_ops)\n    temp = tempfile.mkdtemp()\n\n    dump_model(path=temp)\n    model_path = temp + '/model.pb'\n    bin_path = temp + '/model.bin'\n\n    model = Model.load_tensorflow(model_path, input_names, output_names,\n                                  byte_order, bin_path, bigdl_type)\n\n    try:\n        shutil.rmtree(temp)\n    except OSError as e:\n        if e.errno != errno.ENOENT:\n         ",
        "rewrite": ""
    },
    {
        "original": "\n    x_ndims = (\n        tf.rank(x) if tensorshape_util.rank(x.shape) is None else\n        tensorshape_util.rank(x.shape))\n    event_ndims = (\n        tf.size(input=self.event_shape_tensor())\n        if tensorshape_util.rank(self.event_shape) is None else\n        tensorshape_util.rank(self.event_shape))\n    batch_ndims = (\n        tf.size(input=self._batch_shape_unexpanded)\n        if tensorshape_util.rank(self.batch_shape) is None else\n        tensorshape_util.rank(self.batch_shape))\n    sample_ndims = x_ndims - batch_ndims - event_ndims\n    if isinstance(sample_ndims, int):\n      static_sample_shape = x.shape[:sample_ndims]\n    else:\n    ",
        "rewrite": ""
    },
    {
        "original": "\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.variable_scope(name, reuse=reuse):\n        gru = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=n_layers,\n                                            num_units=n_hidden)\n\n        if trainable_initial_states:\n            init_h = tf.get_variable('init_h', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n",
        "rewrite": ""
    },
    {
        "original": "\n        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >\n                self.print_stats_interval):\n            if len(self._file_paths) > 0:\n                self._log_file_processing_stats(self._file_paths)\n            self.last_stat_print_time = timezone.utcnow()",
        "rewrite": ""
    },
    {
        "original": "\n        request_body: bytes = request['request_body']\n        signature_chain_url: str = request['signature_chain_url']\n        signature: str = request['signature']\n        alexa_request: dict = request['alexa_request']\n\n        if not self._verify_request(signature_chain_url, signature, request_body):\n            return {'error': 'failed certificate/signature check'}\n\n        timestamp_str = alexa_request['request']['timestamp']\n        timestamp_datetime = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%SZ')\n        now = datetime.utcnow()\n\n        delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now\n\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        ti = context['ti']\n        celery_result = ti.xcom_pull(task_ids=self.target_task_id)\n        return celery_result.ready()",
        "rewrite": ""
    },
    {
        "original": "\n        # When application exit, system shuts down all handlers by\n        # calling close method. Here we check if logger is already\n        # closed to prevent uploading the log to remote storage multiple\n        # times when `logging.shutdown` is called.\n        if self.closed:\n            return\n\n        super().close()\n\n        if not self.upload_on_close:\n            return\n\n        local_loc = os.path.join(self.local_base,",
        "rewrite": ""
    },
    {
        "original": "\n        for node in self.nodes:\n            node.start()\n\n        for node in self.client_nodes:\n            node.start()",
        "rewrite": ""
    },
    {
        "original": "\n    global_scale = (global_scale_noncentered *\n                    tf.sqrt(global_scale_variance) *\n                    self.weights_prior_scale)\n\n    local_scales = local_scales_noncentered * tf.sqrt(local_scale_variances)\n    return weights_noncentered * local_scales * global_scale[..., tf.newaxis]",
        "rewrite": ""
    },
    {
        "original": "\n\n        jresults = callBigDlFunc(self.bigdl_type,\n                             \"predictLocal\",\n                               self.value,\n                               self._to_jtensors(X),\n                          ",
        "rewrite": ""
    },
    {
        "original": "\n    seed = seed_stream.SeedStream(seed, salt='von_mises_fisher_3d')\n    u_shape = tf.concat([[n], self._batch_shape_tensor()], axis=0)\n    z = tf.random.uniform(u_shape, seed=seed(), dtype=self.dtype)\n    # TODO(bjp): Higher-order odd dim analytic CDFs are available in [1], could\n    # be bisected for bounded sampling runtime (i.e. not rejection sampling).\n    # [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/\n    # The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa\n    # We must protect against both kappa and z being zero.\n    safe_conc = tf.where(self.concentration > 0,\n                         self.concentration,\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        file_no = 0\n        tmp_file_handle = NamedTemporaryFile(delete=True)\n        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}\n        for row in cursor:\n            row_dict = self.generate_data_dict(row._fields, row)\n            s = json.dumps(row_dict).encode('utf-8')\n            tmp_file_handle.write(s)\n\n            # Append newline to make dumps BigQuery compatible.\n            tmp_file_handle.write(b'\\n')\n\n            if tmp_file_handle.tell()",
        "rewrite": ""
    },
    {
        "original": "\n        self.buildcontainer()\n        # if the subclass has a method buildjs this method will be\n        # called instead of the method defined here\n        # when this subclass method is entered it does call\n        # the method buildjschart defined here\n        self.buildjschart()\n        self.htmlcontent = self.template_chart_nvd3.render(chart=self)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(\"check_\" + name):\n    assertions = []\n    if tensorshape_util.rank(param.shape) is not None:\n      if tensorshape_util.rank(param.shape) == 0:\n        raise ValueError(\"Mixing params must be a (batch of) vector; \"\n                         \"{}.rank={} is not at least one.\".format(\n                             name, tensorshape_util.rank(param.shape)))\n    elif validate_args:\n      assertions.append(\n          assert_util.assert_rank_at_least(\n ",
        "rewrite": ""
    },
    {
        "original": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n     ",
        "rewrite": ""
    },
    {
        "original": "\n  one = tf.ones([], dtype=x.dtype)\n  return tf.math.digamma(x + one) - tf.math.digamma(one)",
        "rewrite": ""
    },
    {
        "original": "\n    res = []\n    if not dag or not execution_date:\n        return res\n\n    # Mark the dag run to running.\n    if commit:\n        _set_dag_run_state(dag.dag_id, execution_date, State.RUNNING, session)\n\n    # To keep the return type consistent with the other similar functions.\n    return res",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'value_and_gradients',\n                               [fn_arg_list, result, grads]):\n\n    def _convert_to_tensor(x, name):\n      ctt = lambda x_: x_ if x_ is None else tf.convert_to_tensor(\n          value=x_, name=name)\n      return [ctt(x_) for x_ in x] if is_list_like(x) else ctt(x)\n\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    fn_arg_list = _convert_to_tensor(fn_arg_list, 'fn_arg')\n\n    if result is None:\n",
        "rewrite": ""
    },
    {
        "original": "\n    return labels2onehot(proba2labels(proba, confident_threshold, classes), classes)",
        "rewrite": ""
    },
    {
        "original": "\n        service = self.get_conn()\n        full_topic = _format_topic(project, topic)\n        try:\n            service.projects().topics().create(\n                name=full_topic, body={}).execute(num_retries=self.num_retries)\n        except HttpError as e:\n            # Status code 409 indicates that the topic already exists.\n            if str(e.resp['status']) == '409':\n                message = 'Topic already exists: {}'.format(full_topic)\n ",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.info('Creating a new ACL entry for object: %s in bucket: %s',\n                      object_name, bucket_name)\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        blob = bucket.blob(object_name)\n        # Reload fetches the current ACL from Cloud Storage.\n        blob.acl.reload()\n        blob.acl.entity_from_dict(entity_dict={\"entity\": entity, \"role\": role})\n        if user_project:\n            blob.acl.user_project",
        "rewrite": ""
    },
    {
        "original": "\n        long_f = 'extra__google_cloud_platform__{}'.format(f)\n        if hasattr(self, 'extras') and long_f in self.extras:\n            return self.extras[long_f]\n        else:\n            return default",
        "rewrite": ""
    },
    {
        "original": "\n    params_dict = {}\n    # merge kwargs into params_dict\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'poll_job', False)\n\n    start_time = time.time()\n    pollCount = 0\n    while True:\n        result = self.do_json_request('3/Jobs.json/' + job_key, timeout=timeoutSecs, params=params_dict)\n        # print 'Job: ', dump_json(result)\n\n        if key:\n            frames_result = self.frames(key=key)\n            print 'frames_result for key:', key, dump_json(result)\n\n        jobs = result['jobs'][0]\n        description = jobs['description']\n   ",
        "rewrite": ""
    },
    {
        "original": "\n\n  if not tf.executing_eagerly():\n    tf.compat.v1.reset_default_graph()\n\n  # Build a static, pretend dataset.\n  count_data = tf.cast(\n      tf.concat(\n          [tfd.Poisson(rate=15.).sample(43),\n           tfd.Poisson(rate=25.).sample(31)],\n          axis=0),\n      dtype=tf.float32)\n  if tf.executing_eagerly():\n    count_data = count_data.numpy()\n  else:\n    with tf.compat.v1.Session():\n      count_data = count_data.eval()\n\n  # Define a closure over our joint_log_prob.\n  def unnormalized_log_posterior(lambda1, lambda2, tau):\n    return text_messages_joint_log_prob(count_data, lambda1, lambda2, tau)\n\n  if tf.executing_eagerly():\n    sample_chain = tf.function(tfp.mcmc.sample_chain)\n  else:\n    sample_chain = tfp.mcmc.sample_chain\n\n  # Initialize",
        "rewrite": ""
    },
    {
        "original": "\n        label_norm = label.replace('1', 'one').upper()\n        if label_norm in cls.__members__:\n            return DecayType[label_norm]\n        else:\n            raise NotImplementedError",
        "rewrite": ""
    },
    {
        "original": "\n\n    config_path = find_config('tfidf_retrieve')\n    skill = build_model(config_path)\n    agent = EcommerceAgent(skills=[skill])\n    return agent",
        "rewrite": ""
    },
    {
        "original": "\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    if num_output_channels == 1:\n        img = img.convert('L')\n    elif num_output_channels == 3:\n        img = img.convert('L')\n        np_img = np.array(img, dtype=np.uint8)\n        np_img = np.dstack([np_img, np_img, np_img])\n        img = Image.fromarray(np_img, 'RGB')\n    else:\n        raise ValueError('num_output_channels should be either 1 or 3')\n\n    return img",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_connection(self.redis_conn_id)\n        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        # Metadata and results to be harvested can be inconsistent,\n        # but it should not be a big problem.\n        self._sync_metadata()\n        # Heartbeating after syncing metadata so we do not restart manager\n        # if it processed all files for max_run times and exit normally.\n        self._heartbeat_manager()\n        simple_dags = []\n        # multiprocessing.Queue().qsize will not work on MacOS.\n        if sys.platform == \"darwin\":\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name):\n\n    def make_shape_tensor(x):\n      return tf.convert_to_tensor(value=x, name=\"shape\", dtype=tf.int32)\n\n    def get_tensor_shape(s):\n      if isinstance(s, tf.TensorShape):\n        return s\n      s_ = tf.get_static_value(make_shape_tensor(s))\n      if s_ is not None:\n        return tf.TensorShape(s_)\n      return None\n\n    def get_shape_tensor(s):\n      if not isinstance(s, tf.TensorShape):\n        return make_shape_tensor(s)\n      if tensorshape_util.is_fully_defined(s):\n        return make_shape_tensor(tensorshape_util.as_list(s))\n      raise ValueError(\"Cannot broadcast from partially \"\n    ",
        "rewrite": ""
    },
    {
        "original": "\n  for slices, overrides in slice_overrides_seq:\n    dist = _apply_single_step(dist, params_event_ndims, slices, overrides)\n  return dist",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        product = self.product_name_determiner.get_entity_with_name(product, product_id, location, project_id)\n        self.log.info('Updating ProductSet: %s', product.name)\n        response = client.update_product(\n            product=product, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('Product updated: %s', response.name if response else '')\n        self.log.debug('Product updated:\\n%s', response)\n        return MessageToDict(response)",
        "rewrite": ""
    },
    {
        "original": "\n  # TODO(b/67497980): Switch to a more numerically faithful implementation.\n  z = tf.convert_to_tensor(value=z)\n\n  wrap = lambda result: tf.debugging.check_numerics(result, 'besseli{}'.format(v\n                                                                              ))\n\n  if float(v) >= 2:\n    raise ValueError(\n        'Evaluating bessel_i by recurrence becomes imprecise for large v')\n\n  cache",
        "rewrite": ""
    },
    {
        "original": "\n\n        self.log.info(\"Processing files using up to %s processes at a time \", self._parallelism)\n        self.log.info(\"Process each file at most once every %s seconds\", self._file_process_interval)\n        self.log.info(\n            \"Checking for new files in %s every %s seconds\", self._dag_directory, self.dag_dir_list_interval\n        )\n\n        if self._async_mode:\n            self.log.debug(\"Starting DagFileProcessorManager in async mode\")\n            self.start_in_async()\n        else:\n        ",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        parent = ProductSearchClient.location_path(project_id, location)\n        self.log.info('Creating a new ProductSet under the parent: %s', parent)\n        response = client.create_product_set(\n            parent=parent,\n            product_set=product_set,\n            product_set_id=product_set_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n",
        "rewrite": ""
    },
    {
        "original": "\n\n  # If the current season has just ended, increase the variance of its effect\n  # following drift_scale. (the just-ended seasonal effect will always be the\n  # bottom element of the vector). Otherwise, do nothing.\n  drift_scale_diag = tf.stack(\n      [tf.zeros_like(drift_scale)] * (num_seasons - 1) + [drift_scale],\n      axis=-1)\n  def seasonal_transition_noise(t):\n    noise_scale_diag = dist_util.pick_scalar_condition(\n        is_last_day_of_season(t),\n        drift_scale_diag,\n        tf.zeros_like(drift_scale_diag))\n    return tfd.MultivariateNormalDiag(\n        loc=tf.zeros(num_seasons, dtype=drift_scale.dtype),\n        scale_diag=noise_scale_diag)\n  return seasonal_transition_noise",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.variable_scope(None, default_name=\"trainable_gamma\"):\n    unconstrained_concentration = tf.compat.v1.get_variable(\n        \"unconstrained_concentration\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(\n            mean=0.5, stddev=0.1))\n    unconstrained_scale = tf.compat.v1.get_variable(\n        \"unconstrained_scale\",\n        shape,\n        initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))\n    concentration = tf.maximum(tf.nn.softplus(unconstrained_concentration),\n                               min_concentration)\n    rate = tf.maximum(1. / tf.nn.softplus(unconstrained_scale), 1. / min_scale)\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn",
        "rewrite": ""
    },
    {
        "original": "\n  assertions = []\n\n  if not _is_iterable(distributions) or not distributions:\n    raise ValueError('`distributions` must be a list of one or more '\n                     'distributions.')\n\n  if dtype_override is None:\n    dts = [\n        dtype_util.base_dtype(d.dtype)\n        for d in distributions\n        if d.dtype is not None\n    ]\n    if dts[1:] != dts[:-1]:\n      raise TypeError('Distributions must have same dtype; found: {}.'.format(\n          set(dtype_util.name(dt) for dt in dts)))\n\n ",
        "rewrite": ""
    },
    {
        "original": "\n    for k, v in editing_dict.items():\n        if isinstance(v, collections.Mapping):\n            update_dict_recursive(editable_dict.get(k, {}), v)\n        else:\n            editable_dict[k] = v",
        "rewrite": ""
    },
    {
        "original": "\n        if self.euristics is None:\n            return\n        # \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438,\n        # \u043f\u0440\u0438\u0432\u043e\u0434\u044f\u0449\u0435\u0439 \u043a \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044e ('+') \u0438\u043b\u0438 \u0438\u0441\u0447\u0435\u0437\u043d\u043e\u0432\u0435\u043d\u0438\u044e ('-') \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0430\n        removal_costs = {a : np.inf for a in self.alphabet}\n        insertion_costs = {a : np.inf for a in self.alphabet}\n        if self.allow_spaces:\n            removal_costs[' '] = np.inf\n            insertion_costs[' '] = np.inf\n    ",
        "rewrite": ""
    },
    {
        "original": "\n    WHOLE_DATA = 'ml-1m.zip'\n    local_file = base.maybe_download(WHOLE_DATA, data_dir, SOURCE_URL + WHOLE_DATA)\n    zip_ref = zipfile.ZipFile(local_file, 'r')\n    extracted_to = os.path.join(data_dir, \"ml-1m\")\n    if not os.path.exists(extracted_to):\n        print(\"Extracting %s to %s\" % (local_file, data_dir))\n        zip_ref.extractall(data_dir)\n        zip_ref.close()\n    rating_files = os.path.join(extracted_to,\"ratings.dat\")\n\n    rating_list = [i.strip().split(\"::\") for i in open(rating_files,\"r\").readlines()]    \n    movielens_data = np.array(rating_list).astype(int)\n    return movielens_data",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily',\n                                 [params, num_components, component_layer]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n\n      components_dist = component_layer(\n          tf.reshape(\n              params[..., num_components:],\n              tf.concat([tf.shape(input=params)[:-1], [num_components, -1]],\n        ",
        "rewrite": ""
    },
    {
        "original": "\n  def _softplus_inverse(x):\n    return np.log(np.expm1(x))\n\n  logit_concentration = tf.compat.v1.get_variable(\n      \"logit_concentration\",\n      shape=[1, num_topics],\n      initializer=tf.compat.v1.initializers.constant(\n          _softplus_inverse(initial_value)))\n  concentration = _clip_dirichlet_parameters(\n      tf.nn.softplus(logit_concentration))\n\n  def prior():\n    return tfd.Dirichlet(concentration=concentration,\n                         name=\"topics_prior\")\n\n  prior_variables = [logit_concentration]\n\n  return prior, prior_variables",
        "rewrite": ""
    },
    {
        "original": "\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  state_parts = [\n      tf.convert_to_tensor(value=s, name='current_state') for s in state_parts\n  ]\n  if state_gradients_are_stopped:\n    state_parts = [tf.stop_gradient(x) for x in state_parts]\n  target_log_prob, grads_target_log_prob = mcmc_util.maybe_call_fn_and_grads(\n      target_log_prob_fn,\n      state_parts,\n      target_log_prob,\n      grads_target_log_prob)\n  step_sizes = (list(step_size) if mcmc_util.is_list_like(step_size)\n                else [step_size])\n  step_sizes = [\n      tf.convert_to_tensor(\n          value=s, name='step_size', dtype=target_log_prob.dtype)\n      for s in step_sizes\n  ]\n  if len(step_sizes) ==",
        "rewrite": ""
    },
    {
        "original": "\n    if isinstance(self._sample_shape, tf.Tensor):\n      return tf.TensorShape(tf.get_static_value(self._sample_shape))\n    return tf.TensorShape(self._sample_shape)",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'IndependentPoisson',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      output_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ],\n",
        "rewrite": ""
    },
    {
        "original": "\n        self.hook = SqoopHook(\n            conn_id=self.conn_id,\n            verbose=self.verbose,\n            num_mappers=self.num_mappers,\n            hcatalog_database=self.hcatalog_database,\n            hcatalog_table=self.hcatalog_table,\n            properties=self.properties\n        )\n\n        if self.cmd_type == 'export':\n            self.hook.export_table(\n              ",
        "rewrite": ""
    },
    {
        "original": "\n        schema_str = None\n        schema_file_mime_type = 'application/json'\n        tmp_schema_file_handle = NamedTemporaryFile(delete=True)\n        if self.schema is not None and isinstance(self.schema, string_types):\n            schema_str = self.schema.encode('utf-8')\n        elif self.schema is not None and isinstance(self.schema, list):\n            schema_str = json.dumps(self.schema).encode('utf-8')\n        else:\n            schema = []\n            for field in cursor.description:\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(\"logsum_expbig_minus_expsmall\"):\n    return tf.math.log1p(-tf.exp(small - big)) + big",
        "rewrite": ""
    },
    {
        "original": "\n    input_shape = tf.TensorShape(input_shape).as_list()\n    if self.data_format == 'channels_last':\n      space = input_shape[1:-1]\n      new_space = []\n      for i in range(len(space)):\n        new_dim = tf_layers_util.conv_output_length(\n            space[i],\n            self.kernel_size[i],\n            padding=self.padding,\n            stride=self.strides[i],\n            dilation=self.dilation_rate[i])\n        new_space.append(new_dim)\n      return tf.TensorShape([input_shape[0]] + new_space + [self.filters])\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        axis = {}\n\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n            axis['axisLabel'] = \"'\" + label + \"'\"\n\n        # Add new axis to list of axis\n        self.axislist[name] = axis",
        "rewrite": ""
    },
    {
        "original": "\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n               ",
        "rewrite": ""
    },
    {
        "original": "\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
        "rewrite": ""
    },
    {
        "original": "\n        DR = DagRun\n\n        exec_date = func.cast(self.execution_date, DateTime)\n\n        dr = session.query(DR).filter(\n            DR.dag_id == self.dag_id,\n            func.cast(DR.execution_date, DateTime) == exec_date,\n            DR.run_id == self.run_id\n        ).one()\n\n        self.id = dr.id\n        self.state = dr.state",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.info('Creating a new ACL entry in bucket: %s', bucket_name)\n        client = self.get_conn()\n        bucket = client.bucket(bucket_name=bucket_name)\n        bucket.acl.reload()\n        bucket.acl.entity_from_dict(entity_dict={\"entity\": entity, \"role\": role})\n        if user_project:\n            bucket.acl.user_project = user_project\n        bucket.acl.save()\n\n        self.log.info('A new ACL entry created in bucket: %s', bucket_name)",
        "rewrite": ""
    },
    {
        "original": "\n        content = {}\n\n        if self.text:\n            content['text'] = self.text\n\n        content['controls'] = [control.json() for control in self.content]\n\n        self.control_json['content'] = content\n\n        return self.control_json",
        "rewrite": ""
    },
    {
        "original": "\n    # TO DO: adapt for >3D tensors\n    if dropout == 0.0:\n        return inputs\n    inputs_func = lambda x: kb.ones_like(inputs[:, :, 0:1])\n    inputs_mask = kl.Lambda(inputs_func)(inputs)\n    inputs_mask = kl.Dropout(dropout)(inputs_mask)\n    tiling_shape = [1, 1, kb.shape(inputs)[2]] + [1] * (kb.ndim(inputs) - 3)\n    inputs_mask = kl.Lambda(kb.tile, arguments={\"n\": tiling_shape},\n                            output_shape=inputs._keras_shape[1:])(inputs_mask)\n    answer = kl.Multiply()([inputs, inputs_mask])\n    return answer",
        "rewrite": ""
    },
    {
        "original": "\n        d = {\n            FIELD_TYPE.INT24: 'INTEGER',\n            FIELD_TYPE.TINY: 'INTEGER',\n            FIELD_TYPE.BIT: 'INTEGER',\n            FIELD_TYPE.DATETIME: 'TIMESTAMP',\n            FIELD_TYPE.DATE: 'TIMESTAMP',\n            FIELD_TYPE.DECIMAL: 'FLOAT',\n            FIELD_TYPE.NEWDECIMAL: 'FLOAT',\n            FIELD_TYPE.DOUBLE: 'FLOAT',\n            FIELD_TYPE.FLOAT: 'FLOAT',\n",
        "rewrite": ""
    },
    {
        "original": "\n    if not _is_pil_image(img):\n        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        self.log.info('Creating ReferenceImage')\n        parent = ProductSearchClient.product_path(project=project_id, location=location, product=product_id)\n\n        response = client.create_reference_image(\n            parent=parent,\n            reference_image=reference_image,\n            reference_image_id=reference_image_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n\n      ",
        "rewrite": ""
    },
    {
        "original": "\n\n  # Build an iterator over training batches.\n  training_dataset = tf.data.Dataset.from_tensor_slices(\n      (mnist_data.train.images, np.int32(mnist_data.train.labels)))\n  training_batches = training_dataset.shuffle(\n      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n\n  # Build a iterator over the heldout set with batch_size=heldout_size,\n  # i.e., return the entire heldout set as a constant.\n  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n      (mnist_data.validation.images,\n       np.int32(mnist_data.validation.labels)))\n  heldout_frozen = (heldout_dataset.take(heldout_size).\n                    repeat().batch(heldout_size))\n  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_frozen)\n\n  # Combine these into a feedable iterator that can switch between training\n  # and validation inputs.\n  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n  feedable_iterator =",
        "rewrite": ""
    },
    {
        "original": "\n    file_name = Path(urlparse(url).path).name\n    download_path = Path(download_path)\n\n    if extract_paths is None:\n        extract_paths = [download_path]\n    elif isinstance(extract_paths, list):\n        extract_paths = [Path(path) for path in extract_paths]\n    else:\n        extract_paths = [Path(extract_paths)]\n\n    cache_dir = os.getenv('DP_CACHE_DIR')\n    extracted = False\n    if cache_dir:\n        cache_dir = Path(cache_dir)\n        url_hash = md5(url.encode('utf8')).hexdigest()[:15]\n        arch_file_path = cache_dir / url_hash\n        extracted_path = cache_dir / (url_hash + '_extracted')\n ",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_conn()\n        conn.put(local_full_path, remote_full_path)",
        "rewrite": ""
    },
    {
        "original": "\n        names = value._fields\n        values = [cls.convert_value(name, getattr(value, name)) for name in names]\n        return cls.generate_data_dict(names, values)",
        "rewrite": ""
    },
    {
        "original": "\n        method, endpoint = endpoint_info\n        url = 'https://{host}/{endpoint}'.format(\n            host=self._parse_host(self.databricks_conn.host),\n            endpoint=endpoint)\n        if 'token' in self.databricks_conn.extra_dejson:\n            self.log.info('Using token auth.')\n            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])\n        else:\n            self.log.info('Using basic auth.')\n            auth = (self.databricks_conn.login, self.databricks_conn.password)\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  if isinstance(d, (float, int, np.generic, np.ndarray)):\n    n = (-1 + np.sqrt(1 + 8 * d)) / 2.\n    if float(int(n)) != n:\n      raise ValueError(\"Vector length is not a triangular number.\")\n    return int(n)\n  else:\n    with tf.name_scope(name or \"vector_size_to_square_matrix_size\") as name:\n      n = (-1. + tf.sqrt(1 + 8. * tf.cast(d, dtype=tf.float32))) / 2.\n      if validate_args:\n        with tf.control_dependencies([\n            assert_util.assert_equal(\n                tf.cast(tf.cast(n, dtype=tf.int32), dtype=tf.float32),\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        with closing(self.get_conn()) as conn:\n            with closing(conn.cursor()) as cur:\n                if parameters is not None:\n                    cur.execute(sql, parameters)\n                else:\n                    cur.execute(sql)\n                return cur.fetchone()",
        "rewrite": ""
    },
    {
        "original": "\n  seqs = tf.clip_by_value(seqs, 0., 1.)\n  seqs = tf.unstack(seqs[:num])\n  joined_seqs = [tf.concat(tf.unstack(seq), 1) for seq in seqs]\n  joined_seqs = tf.expand_dims(tf.concat(joined_seqs, 0), 0)\n  tf.compat.v2.summary.image(\n      name,\n      joined_seqs,\n      max_outputs=1,\n      step=tf.compat.v1.train.get_or_create_global_step())",
        "rewrite": ""
    },
    {
        "original": "\n        jobs = self.service.jobs()\n        job_data = {'configuration': configuration}\n\n        # Send query and wait for reply.\n        query_reply = jobs \\\n            .insert(projectId=self.project_id, body=job_data) \\\n            .execute(num_retries=self.num_retries)\n        self.running_job_id = query_reply['jobReference']['jobId']\n        if 'location' in query_reply['jobReference']:\n            location = query_reply['jobReference']['location']\n        else:\n            location",
        "rewrite": ""
    },
    {
        "original": "\n        import shutil\n        import zipfile\n\n        if self._check_exists():\n            return\n\n        makedir_exist_ok(self.raw_folder)\n        makedir_exist_ok(self.processed_folder)\n\n        # download files\n        filename = self.url.rpartition('/')[2]\n        file_path = os.path.join(self.raw_folder, filename)\n        download_url(self.url, root=self.raw_folder, filename=filename, md5=None)\n\n        print('Extracting zip archive')\n        with zipfile.ZipFile(file_path) as zip_f:\n         ",
        "rewrite": ""
    },
    {
        "original": "\n    with self._name_scope(name):\n      return (self._multi_digamma(0.5 * self.df, self.dimension) +\n              self.dimension * math.log(2.) +\n              2 * self.scale_operator.log_abs_determinant())",
        "rewrite": ""
    },
    {
        "original": "\n    train_data = get_mnist(sc, \"train\", options.dataPath)\\\n        .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n                                rec_tuple[1]))\\\n        .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n    test_data = get_mnist(sc, \"test\", options.dataPath)\\\n        .map(lambda rec_tuple: (normalizer(rec_tuple[0], mnist.TEST_MEAN, mnist.TEST_STD),\n                                rec_tuple[1]))\\\n        .map(lambda t: Sample.from_ndarray(t[0], t[1]))\n ",
        "rewrite": ""
    },
    {
        "original": "\n        # Explicitly getting log relative path is necessary as the given\n        # task instance might be different than task instance passed in\n        # in set_context method.\n        log_relative_path = self._render_filename(ti, try_number)\n        remote_loc = os.path.join(self.remote_base, log_relative_path)\n\n        if self.wasb_log_exists(remote_loc):\n            # If Wasb remote file exists, we do not fetch logs from task instance\n            # local machine even if there are errors reading remote logs, as\n ",
        "rewrite": ""
    },
    {
        "original": "\n\n    ec2params = inheritparams(ec2_config, EC2_API_RUN_INSTANCE)\n    ec2params.setdefault('min_count', count)\n    ec2params.setdefault('max_count', count)\n\n    reservation = None\n    conn = ec2_connect(region)\n    try:\n        reservation = conn.run_instances(**ec2params)\n        log('Reservation: {0}'.format(reservation.id))\n        log('Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.'.format(len(reservation.instances), reservation.instances))\n        start = time.time()\n        time.sleep(1)\n        for instance in reservation.instances:\n            while instance.update() == 'pending':\n         ",
        "rewrite": ""
    },
    {
        "original": "\n    input_shape = tf.TensorShape(input_shape)\n    input_shape = input_shape.with_rank_at_least(2)\n    if tf.compat.dimension_value(input_shape[-1]) is None:\n      raise ValueError(\n          'The innermost dimension of `input_shape` must be defined, '\n          'but saw: {}'.format(input_shape))\n    return input_shape[:-1].concatenate(self.units)",
        "rewrite": ""
    },
    {
        "original": "\n    if isinstance(obj, RDD):\n        obj = _to_java_object_rdd(obj)\n    elif isinstance(obj, DataFrame):\n        obj = obj._jdf\n    elif isinstance(obj, SparkContext):\n        obj = obj._jsc\n    elif isinstance(obj, (list, tuple)):\n        obj = ListConverter().convert([_py2java(gateway, x) for x in obj],\n                                      gateway._gateway_client)\n    elif isinstance(obj, dict):\n        result = {}\n   ",
        "rewrite": ""
    },
    {
        "original": "\n  # Try static.\n  s_shape = tensors[0].shape\n  for t in tensors[1:]:\n    s_shape = tf.broadcast_static_shape(s_shape, t.shape)\n  if tensorshape_util.is_fully_defined(s_shape):\n    return tensorshape_util.as_list(s_shape)\n\n  # Fallback on dynamic.\n  d_shape = tf.shape(input=tensors[0])\n  for t in tensors[1:]:\n    d_shape = tf.broadcast_dynamic_shape(d_shape, tf.shape(input=t))\n  return d_shape",
        "rewrite": ""
    },
    {
        "original": "\n        response = self.get_conn().create_cluster_snapshot(\n            SnapshotIdentifier=snapshot_identifier,\n            ClusterIdentifier=cluster_identifier,\n        )\n        return response['Snapshot'] if response['Snapshot'] else None",
        "rewrite": ""
    },
    {
        "original": "\n  a = tf.convert_to_tensor(value=a, name=\"a\")\n  b = tf.convert_to_tensor(value=b, name=\"b\")\n\n  # Here we can't just do tf.equal(a.shape, b.shape), since\n  # static shape inference may break the equality comparison between\n  # shape(a) and shape(b) in tf.equal.\n  def all_shapes_equal():\n    return tf.reduce_all(\n        input_tensor=tf.equal(\n            tf.concat([tf.shape(input=a), tf.shape(input=b)], 0),\n            tf.concat([tf.shape(input=b), tf.shape(input=a)], 0)))\n\n  # One of the shapes isn't fully defined, so we need to use the dynamic\n  # shape.\n  return tf.cond(\n      pred=tf.equal(tf.rank(a), tf.rank(b)),\n      true_fn=all_shapes_equal,\n      false_fn=lambda: tf.constant(False))",
        "rewrite": ""
    },
    {
        "original": "\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print",
        "rewrite": ""
    },
    {
        "original": "\n\n    ds = datetime.strptime(ds, '%Y-%m-%d')\n    if days:\n        ds = ds + timedelta(days)\n    return ds.isoformat()[:10]",
        "rewrite": ""
    },
    {
        "original": "\n  # Faster than using `tfd.MultivariateNormalDiag`.\n  return tfd.Independent(tfd.Normal(*args, **kwargs),\n                         reinterpreted_batch_ndims=1)",
        "rewrite": ""
    },
    {
        "original": "\n    hostname = urlparse(url).hostname\n    if 'n.miaopai.com' == hostname: \n        smid = match1(url, r'n\\.miaopai\\.com/media/([^.]+)') \n        miaopai_download_by_smid(smid, output_dir, merge, info_only)\n        return\n    elif 'miaopai.com' in hostname:  #Miaopai\n        yixia_download_by_scid = yixia_miaopai_download_by_scid\n        site_info = \"Yixia Miaopai\"\n\n        scid = match1(url, r'miaopai\\.com/show/channel/([^.]+)\\.htm') or \\\n               match1(url, r'miaopai\\.com/show/([^.]+)\\.htm') or \\\n               match1(url, r'm\\.miaopai\\.com/show/channel/([^.]+)\\.htm') or \\\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_connection(self.snowflake_conn_id)\n        account = conn.extra_dejson.get('account', None)\n        warehouse = conn.extra_dejson.get('warehouse', None)\n        database = conn.extra_dejson.get('database', None)\n        region = conn.extra_dejson.get(\"region\", None)\n        role = conn.extra_dejson.get('role', None)\n\n        conn_config = {\n            \"user\": conn.login,\n            \"password\": conn.password or '',\n            \"schema\": conn.schema or '',\n         ",
        "rewrite": ""
    },
    {
        "original": "\n        while True:\n            loop_start_time = time.time()\n\n            if self._signal_conn.poll():\n                agent_signal = self._signal_conn.recv()\n                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:\n                    self.terminate()\n                    break\n              ",
        "rewrite": ""
    },
    {
        "original": "\n        if size:\n            log = LoggingMixin().log\n            log.debug(\n                'Filtering for file size >= %s in files: %s',\n                size, map(lambda x: x['path'], result)\n            )\n            size *= settings.MEGABYTE\n            result = [x for x in result if x['length'] >= size]\n",
        "rewrite": ""
    },
    {
        "original": "\n  npdt = dtype_util.as_numpy_dtype(x.dtype)\n  if series_order <= 0:\n    return npdt(1)\n  x_2 = tf.square(x)\n  even_sum = tf.zeros_like(x)\n  odd_sum = tf.zeros_like(x)\n  x_2n = x_2  # Start with x^{2*1} = x^{2*n} with n = 1.\n  for n in range(1, series_order + 1):\n    y = npdt(_double_factorial(2 * n - 1)) / x_2n\n    if n % 2:\n      odd_sum += y\n    else:\n      even_sum += y\n    x_2n *= x_2\n  return 1. + even_sum - odd_sum",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'IndependentLogistic',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      output_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ],\n",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'CategoricalMixtureOfOneHotCategorical',\n                                 [params, event_size, num_components]):\n      dist = MixtureSameFamily.new(\n          params,\n          num_components,\n          OneHotCategorical(\n              event_size,\n              validate_args=False,  # So we can eval on simplex interior.\n           ",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        name = ProductSearchClient.product_path(project_id, location, product_id)\n        self.log.info('Deleting ProductSet: %s', name)\n        client.delete_product(name=name, retry=retry, timeout=timeout, metadata=metadata)\n        self.log.info('Product with the name [%s] deleted:', name)",
        "rewrite": ""
    },
    {
        "original": "\n    import sys\n    from airflow.plugins_manager import operators_modules\n    for operators_module in operators_modules:\n        sys.modules[operators_module.__name__] = operators_module\n        globals()[operators_module._name] = operators_module",
        "rewrite": ""
    },
    {
        "original": "\n\n  seed = tfd.SeedStream(seed, 'make_iaf_stack')\n\n  def make_iaf():\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        jvalue = callBigDlFunc(bigdl_type,\n                               \"seqFilesToImageFrame\",\n                               url,\n                               sc,\n                        ",
        "rewrite": ""
    },
    {
        "original": "\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  needs_volatility_fn_gradients = grads_volatility_fn is None\n\n  # Convert `volatility_fn_results` to a list\n  if volatility_fn_results is None:\n    volatility_fn_results = volatility_fn(*state_parts)\n\n  volatility_fn_results = (list(volatility_fn_results)\n                           if mcmc_util.is_list_like(volatility_fn_results)\n                           else [volatility_fn_results])\n  if len(volatility_fn_results) == 1:\n    volatility_fn_results *= len(state_parts)\n  if len(state_parts) != len(volatility_fn_results):\n    raise ValueError('`volatility_fn` should return a tensor or a list '\n      ",
        "rewrite": ""
    },
    {
        "original": "\n    if override_shape is None:\n      override_shape = []\n\n    override_shape = tf.convert_to_tensor(\n        value=override_shape, dtype=tf.int32, name=name)\n\n    if not dtype_util.is_integer(override_shape.dtype):\n      raise TypeError(\"shape override must be an integer\")\n\n    override_is_scalar = _is_scalar_from_shape_tensor(override_shape)\n    if tf.get_static_value(override_is_scalar):\n      return self._empty\n\n    dynamic_assertions = []\n\n    if tensorshape_util.rank(override_shape.shape) is not None:\n      if tensorshape_util.rank(override_shape.shape) != 1:\n        raise ValueError(\"shape override must be a vector\")\n    elif validate_args:\n      dynamic_assertions += [\n          assert_util.assert_rank(\n",
        "rewrite": ""
    },
    {
        "original": "\n    DR = DagRun\n    dr = session.query(DR).filter(\n        DR.dag_id == dag_id,\n        DR.execution_date == execution_date\n    ).one()\n    dr.state = state\n    if state == State.RUNNING:\n        dr.start_date = timezone.utcnow()\n        dr.end_date = None\n    else:\n        dr.end_date = timezone.utcnow()\n    session.merge(dr)",
        "rewrite": ""
    },
    {
        "original": "\n  reconstruct = tf.clip_by_value(reconstruct, 0., 1.)\n  inputs_and_reconstruct = tf.concat((inputs[:num], reconstruct[:num]), axis=0)\n  image_summary(inputs_and_reconstruct, name)",
        "rewrite": ""
    },
    {
        "original": "\n\n    # strategy:\n    # make a dict of variable name -> [variable, grad, adagrad slot]\n    vars_grads = {}\n    for v in tf.trainable_variables():\n        vars_grads[v.name] = [v, None, None]\n    for g, v in grads:\n        vars_grads[v.name][1] = g\n        vars_grads[v.name][2] = opt.get_slot(v, 'accumulator')\n\n    # now make summaries\n    ret = []\n    for vname, (v, g, a) in vars_grads.items():\n\n        if g is None:\n            continue\n\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.insert_many(docs, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n  block_sizes_shape = block_sizes.shape\n  if tensorshape_util.is_fully_defined(block_sizes_shape):\n    if (tensorshape_util.rank(block_sizes_shape) != 1 or\n        (tensorshape_util.num_elements(block_sizes_shape) != len(bijectors))):\n      raise ValueError(\n          '`block_sizes` must be `None`, or a vector of the same length as '\n          '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of '\n          'length {}'.format(block_sizes_shape, len(bijectors)))\n    return block_sizes\n  elif validate_args:\n    message = ('`block_sizes` must be `None`, or a vector of the same length '\n               'as `bijectors`.')\n",
        "rewrite": ""
    },
    {
        "original": "\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout, Activation, Flatten\n    from keras.layers import Convolution2D, MaxPooling2D\n\n    keras_model = Sequential()\n    keras_model.add(Convolution2D(32, 3, 3, border_mode='valid',\n                                  input_shape=input_shape))\n    keras_model.add(Activation('relu'))\n    keras_model.add(Convolution2D(32, 3, 3))\n    keras_model.add(Activation('relu'))\n    keras_model.add(MaxPooling2D(pool_size=(2, 2)))\n    keras_model.add(Dropout(0.25))\n    keras_model.add(Flatten())\n    keras_model.add(Dense(128))\n    keras_model.add(Activation('relu'))\n    keras_model.add(Dropout(0.5))\n    keras_model.add(Dense(10))\n    keras_model.add(Activation('softmax'))\n    return keras_model",
        "rewrite": ""
    },
    {
        "original": "\n        if len(args) == 0:\n            callBigDlFunc(self.bigdl_type,\n                          \"evaluate\", self.value)\n            return self\n        elif len(args) == 3:\n            dataset, batch_size, val_methods = args\n            if (isinstance(dataset, ImageFrame)):\n                return callBigDlFunc(self.bigdl_type,\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  mid_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(current_momentum, step_size, current_grads_target_log_prob)]\n  next_state = [\n      s + step * m for s, step, m in\n      zip(current_state, step_size, mid_momentum)]\n  next_target_log_prob, next_grads_target_log_prob = value_and_gradients_fn(\n      *next_state)\n  next_momentum = [\n      m + 0.5 * step * g for m, step, g in\n      zip(mid_momentum, step_size, next_grads_target_log_prob)]\n  return [\n      next_state,\n      next_target_log_prob,\n      next_grads_target_log_prob,\n      next_momentum,\n  ]",
        "rewrite": ""
    },
    {
        "original": "\n    params_dict = {}\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'model_builders', False)\n\n    request = '3/ModelBuilders.json' \n    if algo:\n        request += \"/\" + algo\n\n    result = self.do_json_request(request, timeout=timeoutSecs, params=params_dict)\n    # verboseprint(request, \"result:\", dump_json(result))\n    h2o_sandbox.check_sandbox_for_errors()\n    return result",
        "rewrite": ""
    },
    {
        "original": "\n    res = False\n    if _default_settings_path == _settings_path:\n        return res\n\n    for src in list(_default_settings_path.glob('**/*.json')):\n        dest = _settings_path / src.relative_to(_default_settings_path)\n        if not force and dest.exists():\n            continue\n        res = True\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(src, dest)\n    return res",
        "rewrite": ""
    },
    {
        "original": "\n  # center = Max[Log[values]],  with stop-gradient\n  # The center hopefully keep the exponentiated term small.  It is canceled\n  # from the final result, so putting stop gradient on it will not change the\n  # final result.  We put stop gradient on to eliminate unnecessary computation.\n  center = tf.stop_gradient(_sample_max(log_values))\n\n  # centered_values = exp{Log[values] - E[Log[values]]}\n  centered_values = tf.math.exp(log_values - center)\n\n  # log_mean_of_values = Log[ E[centered_values] ] + center\n  #                    = Log[ E[exp{log_values - E[log_values]}] ] + center\n  #                  ",
        "rewrite": ""
    },
    {
        "original": "\n    if not username or not password:\n        raise AuthenticationError()\n\n    user = session.query(PasswordUser).filter(\n        PasswordUser.username == username).first()\n\n    if not user:\n        raise AuthenticationError()\n\n    if not user.authenticate(password):\n        raise AuthenticationError()\n\n    log.info(\"User %s successfully authenticated\", username)\n    return user",
        "rewrite": ""
    },
    {
        "original": "\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
        "rewrite": ""
    },
    {
        "original": "\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n",
        "rewrite": ""
    },
    {
        "original": "\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                ",
        "rewrite": ""
    },
    {
        "original": "\n  dummy_mvndiag = tfd.MultivariateNormalDiag(\n      scale_diag=tf.ones([0], dtype=dtype))\n  dummy_mvndiag.covariance = lambda: dummy_mvndiag.variance()[..., tf.newaxis]\n  return dummy_mvndiag",
        "rewrite": ""
    },
    {
        "original": "\n\n  with tf.compat.v1.name_scope(name, \"squared_hellinger\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return pearson(0.5 * logu)",
        "rewrite": ""
    },
    {
        "original": "\n        ti_status = BackfillJob._DagRunTaskStatus()\n\n        start_date = self.bf_start_date\n\n        # Get intervals between the start/end dates, which will turn into dag runs\n        run_dates = self.dag.get_run_dates(start_date=start_date,\n                                           end_date=self.bf_end_date)\n        if self.run_backwards:\n            tasks_that_depend_on_past = [t.task_id for t in self.dag.task_dict.values() if t.depends_on_past]\n     ",
        "rewrite": ""
    },
    {
        "original": "\n  output = tensor_to_broadcast\n  for tensor in target_tensors:\n    output += tf.zeros_like(tensor)\n  return output",
        "rewrite": ""
    },
    {
        "original": "\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data",
        "rewrite": ""
    },
    {
        "original": "\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  state_parts = [tf.convert_to_tensor(s, name='current_state')\n                 for s in state_parts]\n\n  log_likelihood = _maybe_call_fn(\n      log_likelihood_fn,\n      state_parts,\n      log_likelihood,\n      description)\n  return [state_parts, log_likelihood]",
        "rewrite": ""
    },
    {
        "original": "\n    from PIL import Image\n    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n                     normalize=normalize, range=range, scale_each=scale_each)\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n    im = Image.fromarray(ndarr)\n    im.save(filename)",
        "rewrite": ""
    },
    {
        "original": "\n    dtype = util.maybe_get_common_dtype(\n        [amplitude, length_scale])\n    if amplitude is not None:\n      amplitude = tf.convert_to_tensor(\n          value=amplitude, name='amplitude', dtype=dtype)\n    self._amplitude = _validate_arg_if_not_none(\n        amplitude, tf.compat.v1.assert_positive, validate_args)\n    if length_scale is not None:\n      length_scale = tf.convert_to_tensor(\n          value=length_scale, name='length_scale', dtype=dtype)\n    self._length_scale = _validate_arg_if_not_none(\n        length_scale, tf.compat.v1.assert_positive, validate_args)\n    return dtype",
        "rewrite": ""
    },
    {
        "original": "\n        # create perm for global logical dag\n        for dag_vm in self.DAG_VMS:\n            for perm in self.DAG_PERMS:\n                self._merge_perm(permission_name=perm,\n                                 view_menu_name=dag_vm)",
        "rewrite": ""
    },
    {
        "original": "\n    log = LoggingMixin().log\n    if args.yes or input(\n            \"This will drop all existing records related to the specified DAG. \"\n            \"Proceed? (y/n)\").upper() == \"Y\":\n        try:\n            message = api_client.delete_dag(dag_id=args.dag_id)\n        except IOError as err:\n            log.error(err)\n            raise AirflowException(err)\n        log.info(message)\n    else:\n      ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_deterministic_distribution\"):\n    return -b.log_prob(a.loc)",
        "rewrite": ""
    },
    {
        "original": "\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n\n  [\n      target_log_prob,\n      grads_target_log_prob,\n  ] = mcmc_util.maybe_call_fn_and_grads(\n      target_log_prob_fn,\n      state_parts,\n      target_log_prob,\n      grads_target_log_prob)\n  [\n      volatility_parts,\n      grads_volatility,\n  ] = _maybe_call_volatility_fn_and_grads(\n      volatility_fn,\n      state_parts,\n      volatility,\n      grads_volatility_fn,\n      distribution_util.prefer_static_shape(target_log_prob),\n      parallel_iterations)\n\n  step_sizes = (list(step_size) if mcmc_util.is_list_like(step_size)\n                else [step_size])\n",
        "rewrite": ""
    },
    {
        "original": "\n        TI = models.TaskInstance\n        # actually enqueue them\n        for simple_task_instance in simple_task_instances:\n            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)\n            command = TI.generate_command(\n                simple_task_instance.dag_id,\n                simple_task_instance.task_id,\n                simple_task_instance.execution_date,\n                local=True,\n   ",
        "rewrite": ""
    },
    {
        "original": "\n  # The maximum iterations permitted are determined as the number of halvings\n  # it takes to reduce 1 to 0 in the given dtype.\n  iter_max = np.ceil(-np.log2(_machine_eps(val_c_input.x.dtype)))\n\n  def _cond(i, val_c, to_fix):\n    del val_c  # Unused.\n    return (i < iter_max) & tf.reduce_any(input_tensor=to_fix)\n\n  def _body(i, val_c, to_fix):\n    next_c = tf.where(to_fix, val_c.x * step_size_shrink_param, val_c.x)\n    next_val_c = value_and_gradients_function(next_c)\n    still_to_fix = to_fix & ~hzl.is_finite(next_val_c)\n    return (i + 1, next_val_c, still_to_fix)\n\n  to_fix = active & ~hzl.is_finite(val_c_input)\n  return tf.while_loop(\n      cond=_cond, body=_body, loop_vars=(0, val_c_input, to_fix))",
        "rewrite": ""
    },
    {
        "original": "\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
        "rewrite": ""
    },
    {
        "original": "\n        self._hook = SparkSubmitHook(\n            conf=self._conf,\n            conn_id=self._conn_id,\n            files=self._files,\n            py_files=self._py_files,\n            archives=self._archives,\n            driver_class_path=self._driver_class_path,\n            jars=self._jars,\n            java_class=self._java_class,\n            packages=self._packages,\n         ",
        "rewrite": ""
    },
    {
        "original": "\n        if s == \"\":\n            return curr\n        curr_cash = self._descendance_cash[curr]\n        answer = curr_cash.get(s, None)\n        if answer is not None:\n            return answer\n        # \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0443\u0431\u043b\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u0434\n        res = curr\n        for a in s:\n            res = self.graph[res][self.alphabet_codes[a]]\n          ",
        "rewrite": ""
    },
    {
        "original": "\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n         ",
        "rewrite": ""
    },
    {
        "original": "\n\n  num_schools = 8\n  treatment_effects = tf.constant(\n      [28, 8, -3, 7, -1, 1, 18, 12],\n      dtype=np.float32,\n      name='treatment_effects')\n  treatment_stddevs = tf.constant(\n      [15, 10, 16, 11, 9, 11, 10, 18],\n      dtype=np.float32,\n      name='treatment_stddevs')\n\n  def unnormalized_posterior_log_prob(\n      avg_effect, avg_stddev, school_effects_standard):\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.info(\n            'Transferring mail attachment %s from mail server via imap to s3 key %s...',\n            self.imap_attachment_name, self.s3_key\n        )\n\n        with ImapHook(imap_conn_id=self.imap_conn_id) as imap_hook:\n            imap_mail_attachments = imap_hook.retrieve_mail_attachments(\n                name=self.imap_attachment_name,\n                mail_folder=self.imap_mail_folder,\n                check_regex=self.imap_check_regex,\n ",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        parent = ProductSearchClient.location_path(project_id, location)\n        self.log.info('Creating a new Product under the parent: %s', parent)\n        response = client.create_product(\n            parent=parent,\n            product=product,\n            product_id=product_id,\n            retry=retry,\n            timeout=timeout,\n            metadata=metadata,\n        )\n",
        "rewrite": ""
    },
    {
        "original": "\n  assertions = _lu_reconstruct_assertions(lower_upper, perm, validate_args)\n\n  message = 'Input `rhs` must have at least 2 dimensions.'\n  if rhs.shape.ndims is not None:\n    if rhs.shape.ndims < 2:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_rank_at_least(rhs, rank=2, message=message))\n\n  message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.'\n  if (tf.compat.dimension_value(lower_upper.shape[-1]) is not None and\n      tf.compat.dimension_value(rhs.shape[-2]) is not None):\n    if lower_upper.shape[-1] != rhs.shape[-2]:\n      raise ValueError(message)\n  elif validate_args:\n    assertions.append(\n        tf.compat.v1.assert_equal(\n            tf.shape(input=lower_upper)[-1],\n         ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_laplace_laplace\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 38\n    distance = tf.abs(a.loc - b.loc)\n    ratio = a.scale / b.scale\n\n    return (-tf.math.log(ratio) - 1 + distance / b.scale +\n            ratio * tf.exp(-distance / a.scale))",
        "rewrite": ""
    },
    {
        "original": "\n  state_parts = list(state) if mcmc_util.is_list_like(state) else [state]\n  state_parts = [\n      tf.convert_to_tensor(value=s, name='current_state') for s in state_parts\n  ]\n\n  target_log_prob = _maybe_call_fn(\n      target_log_prob_fn,\n      state_parts,\n      target_log_prob,\n      description)\n  step_sizes = (list(step_size) if mcmc_util.is_list_like(step_size)\n                else [step_size])\n  step_sizes = [\n      tf.convert_to_tensor(\n          value=s, name='step_size', dtype=target_log_prob.dtype)\n      for s in step_sizes\n  ]\n  if len(step_sizes) == 1:\n    step_sizes *= len(state_parts)\n  if len(state_parts) != len(step_sizes):\n  ",
        "rewrite": ""
    },
    {
        "original": "\n    with self._name_scope(name):\n      return (self.df * self.scale_operator.log_abs_determinant() +\n              0.5 * self.df * self.dimension * math.log(2.) +\n              self._multi_lgamma(0.5 * self.df, self.dimension))",
        "rewrite": ""
    },
    {
        "original": "\n    strSeed = \"gGddgPfeaf_gzyr\"\n    prehash = upid + \"_\" + strSeed\n    return md5(prehash.encode('utf-8')).hexdigest()",
        "rewrite": ""
    },
    {
        "original": "\n    data = request.get_json(force=True)\n\n    run_id = None\n    if 'run_id' in data:\n        run_id = data['run_id']\n\n    conf = None\n    if 'conf' in data:\n        conf = data['conf']\n\n    execution_date = None\n    if 'execution_date' in data and data['execution_date'] is not None:\n        execution_date = data['execution_date']\n\n        # Convert string datetime into actual datetime\n        try:\n            execution_date = timezone.parse(execution_date)\n        except ValueError:\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'eval_all_one_hot'):\n    event_size = dist.event_shape_tensor()[-1]\n    batch_ndims = tf.size(input=dist.batch_shape_tensor())\n    # Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`.\n    x = tf.reshape(\n        tf.eye(event_size, dtype=dist.dtype),\n        shape=tf.pad(\n            tensor=tf.ones(batch_ndims, tf.int32),\n            paddings=[[1, 1]],\n            constant_values=event_size))\n    # Compute `fn(x)` then cyclically left-transpose one dim.\n    perm = tf.pad(tensor=tf.range(1, batch_ndims + 1), paddings=[[0, 1]])\n    return tf.transpose(a=fn(dist, x), perm=perm)",
        "rewrite": ""
    },
    {
        "original": "\n    output = title + '.' + ext\n\n    if not (output_dir == '.'):\n        output = output_dir + '/' + output\n\n    print('Downloading streaming content with FFmpeg, press q to stop recording...')\n    if stream:\n        ffmpeg_params = [FFMPEG] + ['-y', '-re', '-i']\n    else:\n        ffmpeg_params = [FFMPEG] + ['-y', '-i']\n    ffmpeg_params.append(files)  #not the same here!!!!\n\n    if FFMPEG == 'avconv':  #who cares?\n        ffmpeg_params += ['-c', 'copy', output]\n    else:\n        ffmpeg_params += ['-c', 'copy',",
        "rewrite": ""
    },
    {
        "original": "\n  # Check that observation index points and observation counts broadcast.\n  ndims = kernel.feature_ndims\n  if (tensorshape_util.is_fully_defined(\n      observation_index_points.shape[:-ndims]) and\n      tensorshape_util.is_fully_defined(observations.shape)):\n    index_point_count = observation_index_points.shape[:-ndims]\n    observation_count = observations.shape\n    try:\n      tf.broadcast_static_shape(index_point_count, observation_count)\n    except ValueError:\n      # Re-raise with our own more contextual error message.\n      raise ValueError(\n          'Observation index point and observation counts are not '\n          'broadcastable: {} and {}, respectively.'.format(\n              index_point_count, observation_count))",
        "rewrite": ""
    },
    {
        "original": "\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
        "rewrite": ""
    },
    {
        "original": "\n  # We use \"PrettyDict\" because collections.OrderedDict repr/str has the word\n  # \"OrderedDict\" in it. We only want to print \"OrderedDict\" if in fact the\n  # input really is an OrderedDict.\n  if isinstance(x, dict):\n    return _PrettyDict({\n        k: _recursively_replace_dict_for_pretty_dict(v)\n        for k, v in x.items()})\n  if (isinstance(x, collections.Sequence) and\n      not isinstance(x, six.string_types)):\n    args = (_recursively_replace_dict_for_pretty_dict(x_) for x_ in x)\n    is_named_tuple = (isinstance(x, tuple) and\n                      hasattr(x, \"_asdict\") and\n         ",
        "rewrite": ""
    },
    {
        "original": "\n\n  static_target_shape = tf.TensorShape(static_target_shape)\n  if tensorshape_util.is_fully_defined(\n      static_shape) and tensorshape_util.is_fully_defined(static_target_shape):\n    if static_shape != static_target_shape:\n      raise ValueError(\"{}: required shape {} but found {}\".\n                       format(name, static_target_shape, static_shape))\n    return None\n  else:\n    if dynamic_target_shape is None:\n      if tensorshape_util.is_fully_defined(static_target_shape):\n        dynamic_target_shape = tensorshape_util.as_list(static_target_shape)\n      else:\n        raise ValueError(\"{}: cannot infer target shape: no dynamic shape \"\n             ",
        "rewrite": ""
    },
    {
        "original": "\n    xml = api_req(vid)\n    urls, name, size = video_info(xml)\n    if urls is None:\n        log.wtf(name)\n    title = name\n    print_info(site_info, title, 'flv', size)\n    if not info_only:\n        download_urls(urls, title, 'flv', size, output_dir = output_dir, merge = merge)",
        "rewrite": ""
    },
    {
        "original": "\n        spark_submit_cmd = self._build_spark_submit_command(application)\n\n        if hasattr(self, '_env'):\n            env = os.environ.copy()\n            env.update(self._env)\n            kwargs[\"env\"] = env\n\n        self._submit_sp = subprocess.Popen(spark_submit_cmd,\n                                           stdout=subprocess.PIPE,\n              ",
        "rewrite": ""
    },
    {
        "original": "\n        if not file_name.startswith('gs://'):\n            return file_name\n\n        # Extracts bucket_id and object_id by first removing 'gs://' prefix and\n        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n             ",
        "rewrite": ""
    },
    {
        "original": "\n        jmodel = callBigDlFunc(bigdl_type, \"loadBigDL\", path)\n        return Layer.of(jmodel)",
        "rewrite": ""
    },
    {
        "original": "\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn` and/or `x` as a key.\n    with tf.control_dependencies(self._runtime_assertions +\n                                 self._validate_sample_arg(x)):\n      sample_shape, static_sample_shape = self._sample_shape(x)\n      old_shape = tf.concat(\n          [\n              sample_shape,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)",
        "rewrite": ""
    },
    {
        "original": "\n  dtype = skin.dtype\n  hair_mask = tf.cast(hair[..., -1:] <= 0, dtype)\n  top_mask = tf.cast(top[..., -1:] <= 0, dtype)\n  pants_mask = tf.cast(pants[..., -1:] <= 0, dtype)\n  char = (skin * hair_mask) + hair\n  char = (char * top_mask) + top\n  char = (char * pants_mask) + pants\n  return char",
        "rewrite": ""
    },
    {
        "original": "\n  data = np.load(download(directory, FILE_TEMPLATE.format(split=split_name)))\n  # The last row is empty in both train and test.\n  data = data[:-1]\n\n  # Each row is a list of word ids in the document. We first convert this to\n  # sparse COO matrix (which automatically sums the repeating words). Then,\n  # we convert this COO matrix to CSR format which allows for fast querying of\n  # documents.\n  num_documents = data.shape[0]\n  indices = np.array([(row_idx, column_idx)\n                      for row_idx, row in enumerate(data)\n                      for",
        "rewrite": ""
    },
    {
        "original": "\n        return self.get_wildcard_key(wildcard_key=wildcard_key,\n                                     bucket_name=bucket_name,\n                                     delimiter=delimiter) is not None",
        "rewrite": ""
    },
    {
        "original": "\n\n  if tensorshape is None:\n    shape_tensor = (shape_tensor_fn() if callable(shape_tensor_fn)\n                    else shape_tensor_fn)\n    if (hasattr(shape_tensor, 'shape') and\n        hasattr(shape_tensor.shape, 'num_elements')):\n      ndims_ = tensorshape_util.num_elements(shape_tensor.shape)\n    else:\n      ndims_ = len(shape_tensor)\n    ndims_fn = lambda: tf.size(input=shape_tensor)\n  else:\n    ndims_ = tensorshape_util.rank(tensorshape)\n    ndims_fn = lambda: tf.size(input=shape_tensor_fn()  # pylint: disable=g-long-lambda\n                               if",
        "rewrite": ""
    },
    {
        "original": "\n        for key, ti in list(ti_status.running.items()):\n            ti.refresh_from_db()\n            if ti.state == State.SUCCESS:\n                ti_status.succeeded.add(key)\n                self.log.debug(\"Task instance %s succeeded. Don't rerun.\", ti)\n                ti_status.running.pop(key)\n                continue\n            elif ti.state == State.SKIPPED:\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        if self.word_dropout > 0.0:\n            lstm_outputs = kl.Dropout(self.word_dropout)(word_outputs)\n        else:\n            lstm_outputs = word_outputs\n        for j in range(self.word_lstm_layers-1):\n            lstm_outputs = kl.Bidirectional(\n                kl.LSTM(self.word_lstm_units[j], return_sequences=True,\n                        dropout=self.lstm_dropout))(lstm_outputs)\n        lstm_outputs = kl.Bidirectional(\n    ",
        "rewrite": ""
    },
    {
        "original": "\n  if tensorshape_util.rank(batch_shape.shape) is not None:\n    if tensorshape_util.rank(batch_shape.shape) != 1:\n      raise ValueError(\"`batch_shape` must be a vector \"\n                       \"(saw rank: {}).\".format(\n                           tensorshape_util.rank(batch_shape.shape)))\n\n  batch_shape_static = tensorshape_util.constant_value_as_shape(batch_shape)\n  batch_size_static = tensorshape_util.num_elements(batch_shape_static)\n  dist_batch_size_static = tensorshape_util.num_elements(\n      distribution.batch_shape)\n\n  if batch_size_static is not None and dist_batch_size_static is not None:\n    if batch_size_static != dist_batch_size_static:\n      raise ValueError(\"`batch_shape` size ({}) must match \"\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  dtype = tf.as_dtype(dtype)\n  if hasattr(dtype, 'max'):\n    return dtype.max\n  use_finfo = is_floating(dtype) or is_complex(dtype)\n  return np.finfo(dtype).max if use_finfo else np.iinfo(dtype).max",
        "rewrite": ""
    },
    {
        "original": "\n  del labels, config\n\n  if params[\"analytic_kl\"] and params[\"mixture_components\"] != 1:\n    raise NotImplementedError(\n        \"Using `analytic_kl` is only supported when `mixture_components = 1` \"\n        \"since there's no closed form otherwise.\")\n\n  encoder = make_encoder(params[\"activation\"],\n                         params[\"latent_size\"],\n                         params[\"base_depth\"])\n  decoder = make_decoder(params[\"activation\"],\n                       ",
        "rewrite": ""
    },
    {
        "original": "\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  try:\n    return scipy_special.logsumexp(\n        input_tensor, axis=_astuple(axis), keepdims=keepdims)\n  except NotImplementedError:\n    # We offer a non SP version just in case SP isn't installed and this\n    # because logsumexp is often used.\n    m = _max_mask_non_finite(input_tensor, axis=axis, keepdims=True)\n    y = input_tensor - m\n    y = np.exp(y, out=y)\n    return m + np.log(np.sum(y, axis=_astuple(axis), keepdims=keepdims))",
        "rewrite": ""
    },
    {
        "original": "\n    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')\n\n    to = get_email_address_list(to)\n\n    msg = MIMEMultipart(mime_subtype)\n    msg['Subject'] = subject\n    msg['From'] = smtp_mail_from\n    msg['To'] = \", \".join(to)\n    recipients = to\n    if cc:\n        cc = get_email_address_list(cc)\n        msg['CC'] = \", \".join(cc)\n        recipients = recipients + cc\n\n    if bcc:\n        # don't add bcc in header\n        bcc = get_email_address_list(bcc)\n        recipients = recipients + bcc\n\n    msg['Date'] =",
        "rewrite": ""
    },
    {
        "original": "\n\n  with tf.compat.v1.name_scope('build_seasonal_transition_matrix'):\n    # If the season is changing, the transition matrix permutes the latent\n    # state to shift all seasons up by a dimension, and sends the current\n    # season's effect to the bottom.\n    seasonal_permutation = np.concatenate(\n        [np.arange(1, num_seasons), [0]], axis=0)\n    seasonal_permutation_matrix = tf.constant(\n        np.eye(num_seasons)[seasonal_permutation], dtype=dtype)\n\n    # Optionally transform the transition matrix into a reparameterized space,\n    # enforcing the zero-sum constraint for ConstrainedSeasonalStateSpaceModel.\n    if basis_change_matrix is not None:\n      seasonal_permutation_matrix = tf.matmul(\n          basis_change_matrix,\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  def _make(dist_fn, args):\n    if args is None:\n      return lambda *_: dist_fn\n    if not args:\n      return lambda *_: dist_fn()\n    def _fn(*xs):\n      kwargs = dict(zip(args, reversed(xs[-len(args):])))\n      kwargs.pop('_', None)\n      return dist_fn(**kwargs)\n    return _fn\n  named_makers = _convert_to_dict(named_makers)\n  g = {k: (None if distribution_util.is_distribution_instance(v)\n           else joint_distribution_sequential._get_required_args(v))  # pylint: disable=protected-access\n       for k, v in named_makers.items()}\n  g = _best_order(g)\n  dist_fn_name, dist_fn_args = zip(*g)\n  dist_fn_args = tuple(None if a is None else tuple(a) for a",
        "rewrite": ""
    },
    {
        "original": "\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
        "rewrite": ""
    },
    {
        "original": "\n  num_examples = 10\n  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n  return (x_train, y_train), (x_test, y_test)",
        "rewrite": ""
    },
    {
        "original": "\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
        "rewrite": ""
    },
    {
        "original": "\n        if not any([isinstance(ti.sla, timedelta) for ti in dag.tasks]):\n            self.log.info(\"Skipping SLA check for %s because no tasks in DAG have SLAs\", dag)\n            return\n\n        TI = models.TaskInstance\n        sq = (\n            session\n            .query(\n                TI.task_id,\n                func.max(TI.execution_date).label('max_ti'))\n",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(\n        name=mcmc_util.make_name(self.name, 'transformed_kernel', 'one_step'),\n        values=[previous_kernel_results]):\n      transformed_next_state, kernel_results = self._inner_kernel.one_step(\n          previous_kernel_results.transformed_state,\n          previous_kernel_results.inner_results)\n      transformed_next_state_parts = (\n          transformed_next_state\n          if mcmc_util.is_list_like(transformed_next_state) else\n          [transformed_next_state])\n      next_state_parts = self._forward_transform(transformed_next_state_parts)\n      next_state = (\n          next_state_parts if mcmc_util.is_list_like(transformed_next_state)\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        return callBigDlFunc(bigdl_type, \"writeParquet\", path, output, sc, partition_num)",
        "rewrite": ""
    },
    {
        "original": "\n  qw2 = trainable_positive_deterministic([units[2], units[1]], name=\"qw2\")\n  qw1 = trainable_positive_deterministic([units[1], units[0]], name=\"qw1\")\n  qw0 = trainable_positive_deterministic([units[0], feature_size], name=\"qw0\")\n  qz2 = trainable_gamma([data_size, units[2]], name=\"qz2\")\n  qz1 = trainable_gamma([data_size, units[1]], name=\"qz1\")\n  qz0 = trainable_gamma([data_size, units[0]], name=\"qz0\")\n  return qw2, qw1, qw0, qz2, qz1, qz0",
        "rewrite": ""
    },
    {
        "original": "\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout, Activation\n    from keras.layers import Embedding\n    from keras.layers import LSTM\n    from keras.layers import Convolution1D, MaxPooling1D\n    keras_model = Sequential()\n    keras_model.add(Embedding(20000, 128, input_length=100))\n    keras_model.add(Dropout(0.25))\n    keras_model.add(Convolution1D(nb_filter=64,\n                                  filter_length=5,\n                                  border_mode='valid',\n ",
        "rewrite": ""
    },
    {
        "original": "\n        if self._process and not self._process.is_alive() and not self.done:\n            self.start()",
        "rewrite": ""
    },
    {
        "original": "\n        data = list(zip(*args))\n        self.save()\n        if self._fit_batch_size is None:\n            raise ConfigError(\"in order to use fit() method\"\n                              \" set `fit_batch_size` parameter\")\n        bs = int(self._fit_batch_size)\n        data_len = len(data)\n        num_batches = self._fit_max_batches or ((data_len - 1) // bs + 1)\n\n        avg_loss =",
        "rewrite": ""
    },
    {
        "original": "\n        # this avoids a circular dependency\n        from airflow.ti_deps.dep_context import DepContext\n\n        if dep_context is None:\n            dep_context = DepContext()\n\n        if self.IGNOREABLE and dep_context.ignore_all_deps:\n            yield self._passing_status(\n                reason=\"Context specified all dependencies should be ignored.\")\n            return\n\n        if self.IS_TASK_DEP and dep_context.ignore_task_deps:\n         ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, \"random_bernoulli\", [shape, probs]):\n    probs = tf.convert_to_tensor(value=probs)\n    random_uniform = tf.random.uniform(shape, dtype=probs.dtype, seed=seed)\n    return tf.cast(tf.less(random_uniform, probs), dtype)",
        "rewrite": ""
    },
    {
        "original": "\n  if isinstance(tval, tf.Tensor):\n    return tf.where(cond, tval, fval)\n  elif isinstance(tval, tuple):\n    cls = type(tval)\n    return cls(*(val_where(cond, t, f) for t, f in zip(tval, fval)))\n  else:\n    raise Exception(TypeError)",
        "rewrite": ""
    },
    {
        "original": "\n    news_dir = download_news20(source_dir)\n    texts = []  # list of text samples\n    label_id = 0\n    for name in sorted(os.listdir(news_dir)):\n        path = os.path.join(news_dir, name)\n        label_id += 1\n        if os.path.isdir(path):\n            for fname in sorted(os.listdir(path)):\n                if fname.isdigit():\n                    fpath = os.path.join(path, fname)\n           ",
        "rewrite": ""
    },
    {
        "original": "\n        if dag_id not in self.dag_id_to_simple_dag:\n            raise AirflowException(\"Unknown DAG ID {}\".format(dag_id))\n        return self.dag_id_to_simple_dag[dag_id]",
        "rewrite": ""
    },
    {
        "original": "\n\n        # Checks presence of the model files\n        if self.load_path.exists():\n            path = str(self.load_path.resolve())\n            log.info('[loading model from {}]'.format(path))\n            self._net.load(path)",
        "rewrite": ""
    },
    {
        "original": "\n        return self.connection.get_blob_to_path(container_name, blob_name,\n                                                file_path, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n  state = leapfrog_step_state.state\n  state_grads = leapfrog_step_state.state_grads\n  momentum = leapfrog_step_state.momentum\n  step_size = maybe_broadcast_structure(step_size, state)\n\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n  momentum = tf.nest.map_structure(tf.convert_to_tensor, momentum)\n  state = tf.nest.map_structure(tf.convert_to_tensor, state)\n\n  if state_grads is None:\n    _, _, state_grads = call_and_grads(target_log_prob_fn, state)\n  else:\n    state_grads = tf.nest.map_structure(tf.convert_to_tensor, state_grads)\n\n  momentum = tf.nest.map_structure(lambda m, sg, s: m + 0.5 * sg * s, momentum,\n                                   state_grads, step_size)\n\n  kinetic_energy, kinetic_energy_extra, momentum_grads = call_and_grads(\n      kinetic_energy_fn, momentum)\n\n  state = tf.nest.map_structure(lambda x, mg, s: x",
        "rewrite": ""
    },
    {
        "original": "\n  im_bytes = tf.io.read_file(filepath)\n  im = tf.image.decode_image(im_bytes, channels=CHANNELS)\n  im = tf.image.convert_image_dtype(im, tf.float32)\n  return im",
        "rewrite": ""
    },
    {
        "original": "\n        tasks_to_run = {}\n\n        if dag_run is None:\n            return tasks_to_run\n\n        # check if we have orphaned tasks\n        self.reset_state_for_orphaned_tasks(filter_by_dag_run=dag_run, session=session)\n\n        # for some reason if we don't refresh the reference to run is lost\n        dag_run.refresh_from_db()\n        make_transient(dag_run)\n\n        # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf\n        for ti in dag_run.get_task_instances():\n     ",
        "rewrite": ""
    },
    {
        "original": "\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])",
        "rewrite": ""
    },
    {
        "original": "\n    ds = []\n    values_out = []\n    seed = seed_stream.SeedStream('JointDistributionCoroutine', seed)\n    gen = self._model()\n    index = 0\n    d = next(gen)\n    try:\n      while True:\n        actual_distribution = d.distribution if isinstance(d, self.Root) else d\n        ds.append(actual_distribution)\n        if (value is not None and len(value) > index and\n            value[index] is not None):\n          seed()\n          next_value = value[index]\n  ",
        "rewrite": ""
    },
    {
        "original": "\n\n    if isinstance(dest_file_path, list):\n        dest_file_paths = [Path(path) for path in dest_file_path]\n    else:\n        dest_file_paths = [Path(dest_file_path).absolute()]\n\n    if not force_download:\n        to_check = list(dest_file_paths)\n        dest_file_paths = []\n        for p in to_check:\n            if p.exists():\n                log.info(f'File already exists in {p}')\n            else:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    del inputs  # unused\n    with tf.compat.v1.name_scope(self._name):\n      return tfd.MultivariateNormalDiag(self.loc, self.scale_diag)",
        "rewrite": ""
    },
    {
        "original": "\n    html = get_content('https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}'.format(channel_id=channel_id, access_token=access_token))\n    data = loads(html)\n    id_list = []\n\n    #print(data)\n    for i in data['data']:\n        id_list.append(match1(i['uri'], r'/videos/(\\w+)'))\n\n    for id in id_list:\n        try:\n            vimeo_download_by_id(id, None, output_dir, merge, info_only, **kwargs)\n        except urllib.error.URLError as e:\n            log.w('{} failed with {}'.format(id, e))",
        "rewrite": ""
    },
    {
        "original": "\n    if include_examples is None:\n        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')\n    file_paths = []\n    if directory is None:\n        return []\n    elif os.path.isfile(directory):\n        return [directory]\n    elif os.path.isdir(directory):\n        patterns_by_dir = {}\n        for root, dirs, files in os.walk(directory, followlinks=True):\n            patterns = patterns_by_dir.get(root, [])\n            ignore_file = os.path.join(root, '.airflowignore')\n            if os.path.isfile(ignore_file):\n ",
        "rewrite": ""
    },
    {
        "original": "\n    current_secondary_status_transitions = current_job_description.get('SecondaryStatusTransitions')\n    if current_secondary_status_transitions is None or len(current_secondary_status_transitions) == 0:\n        return False\n\n    prev_job_secondary_status_transitions = prev_job_description.get('SecondaryStatusTransitions') \\\n        if prev_job_description is not None else None\n\n    last_message = prev_job_secondary_status_transitions[-1]['StatusMessage'] \\\n        if prev_job_secondary_status_transitions is not None \\\n        and len(prev_job_secondary_status_transitions) > 0 else ''\n\n    message = current_job_description['SecondaryStatusTransitions'][-1]['StatusMessage']\n\n    return message != last_message",
        "rewrite": ""
    },
    {
        "original": "\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.config['unsupported_message']\n                },\n            ",
        "rewrite": ""
    },
    {
        "original": "\n        if is_training:\n            callJavaFunc(self.value.training)\n        else:\n            callJavaFunc(self.value.evaluate)\n        return self",
        "rewrite": ""
    },
    {
        "original": "\n        self._retry_obj = tenacity.Retrying(\n            **_retry_args\n        )\n\n        self._retry_obj(self.run, *args, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.variable_scope(scope):\n        u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [1, tf.shape(inputs)[1], 1]), inputs], axis=2)\n        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)\n        logits = softmax_mask(tf.squeeze(logits, [2]), mask)\n        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n        res = tf.reduce_sum(att_weights * inputs, axis=1)\n        return res, logits",
        "rewrite": ""
    },
    {
        "original": "\n        node_id_to_config_layer[kmodel.name] = kmodel  # include itself as well\n        def gather_result(layers):\n            if layers:  # layers maybe None here.\n                for layer in layers:\n                    if layer.name not in node_id_to_config_layer:\n                        node_id_to_config_layer[layer.name] = layer\n              ",
        "rewrite": ""
    },
    {
        "original": "\n    (images, labels) = mnist.read_data_sets(location, data_type)\n    images = sc.parallelize(images)\n    labels = sc.parallelize(labels + 1)  # Target start from 1 in BigDL\n    record = images.zip(labels)\n    return record",
        "rewrite": ""
    },
    {
        "original": "\n\n        assert vid\n\n        self.prepare(vid = vid, title = title, **kwargs)\n\n        self.extract(**kwargs)\n\n        self.download(output_dir = output_dir, \n                    merge = merge, \n                    info_only = info_only, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n\n        BJ = jobs.BaseJob\n        payload = {}\n        scheduler_health_check_threshold = timedelta(seconds=conf.getint('scheduler',\n                                                                         'scheduler_health_check_threshold'\n                        ",
        "rewrite": ""
    },
    {
        "original": "\n        schema = []\n        for field in cursor.description:\n            # See PEP 249 for details about the description tuple.\n            field_name = field[0]\n            field_type = self.type_map(field[1])\n            field_mode = 'REPEATED' if field[1] in (1009, 1005, 1007,\n                                     ",
        "rewrite": ""
    },
    {
        "original": "\n    backend = _get_backend()\n\n    @wraps(func)\n    def wrapper(self, context, *args, **kwargs):\n        self.log.debug(\"Backend: %s, Lineage called with inlets: %s, outlets: %s\",\n                       backend, self.inlets, self.outlets)\n        ret_val = func(self, context, *args, **kwargs)\n\n        outlets = [x.as_dict() for x in self.outlets]\n        inlets = [x.as_dict() for x in self.inlets]\n\n        if len(self.outlets) > 0:\n            self.xcom_push(context,\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_conn()\n\n        self.log.info(\"Querying for all objects\")\n        query_results = conn.query_all(query)\n\n        self.log.info(\"Received results: Total size: %s; Done: %s\",\n                      query_results['totalSize'], query_results['done'])\n\n        return query_results",
        "rewrite": ""
    },
    {
        "original": "\n        self.buildcontent()\n        self.content = self.htmlcontent\n        self.htmlcontent = self.template_page_nvd3.render(chart=self)",
        "rewrite": ""
    },
    {
        "original": "\n        bucket_helper = GoogleCloudBucketHelper(\n            self.gcp_conn_id, self.delegate_to)\n        self.py_file = bucket_helper.google_cloud_to_local(self.py_file)\n        hook = DataFlowHook(gcp_conn_id=self.gcp_conn_id,\n                            delegate_to=self.delegate_to,\n                            poll_sleep=self.poll_sleep)\n        dataflow_options = self.dataflow_default_options.copy()\n        dataflow_options.update(self.options)\n        # Convert argument",
        "rewrite": ""
    },
    {
        "original": "\n    assert algo is not None, '\"algo\" parameter is null'\n    # Allow this now: assert training_frame is not None, '\"training_frame\" parameter is null'\n    assert parameters is not None, '\"parameters\" parameter is null'\n\n    model_builders = self.model_builders(timeoutSecs=timeoutSecs)\n    assert model_builders is not None, \"/ModelBuilders REST call failed\"\n    assert algo in model_builders['model_builders']\n    builder = model_builders['model_builders'][algo]\n    \n    # TODO: test this assert, I don't think this is working. . .\n    if training_frame is not None:\n        frames = self.frames(key=training_frame)\n        assert frames is not None, \"/Frames/{0} REST call failed\".format(training_frame)\n\n  ",
        "rewrite": ""
    },
    {
        "original": "\n    w2v_dir = download_glove_w2v(source_dir)\n    w2v_path = os.path.join(w2v_dir, \"glove.6B.%sd.txt\" % dim)\n    if sys.version_info < (3,):\n        w2v_f = open(w2v_path)\n    else:\n        w2v_f = open(w2v_path, encoding='latin-1')\n    pre_w2v = {}\n    for line in w2v_f.readlines():\n        items = line.split(\" \")\n        pre_w2v[items[0]] = [float(i) for i in items[1:]]\n    w2v_f.close()\n    return pre_w2v",
        "rewrite": ""
    },
    {
        "original": "\n        order, stack = [], []\n        stack.append(trie.root)\n        colors = ['white'] * len(trie)\n        while len(stack) > 0:\n            index = stack[-1]\n            color = colors[index]\n            if color == 'white': # \u0432\u0435\u0440\u0448\u0438\u043d\u0430 \u0435\u0449\u0451 \u043d\u0435 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u043b\u0430\u0441\u044c\n                colors[index] = 'grey'\n                for child in",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_conn()\n        cur = conn.cursor()\n        cur.execute(",
        "rewrite": ""
    },
    {
        "original": "\n\n        dag.test_cycle()  # throws if a task cycle is found\n\n        dag.resolve_template_files()\n        dag.last_loaded = timezone.utcnow()\n\n        for task in dag.tasks:\n            settings.policy(task)\n\n        subdags = dag.subdags\n\n        try:\n            for subdag in subdags:\n                subdag.full_filepath = dag.full_filepath\n                subdag.parent_dag = dag\n",
        "rewrite": ""
    },
    {
        "original": "\n        if signature_chain_url not in self.valid_certificates.keys():\n            amazon_cert: X509 = verify_cert(signature_chain_url)\n            if amazon_cert:\n                amazon_cert_lifetime: timedelta = self.config['amazon_cert_lifetime']\n                expiration_timestamp = datetime.utcnow() + amazon_cert_lifetime\n                validated_cert = ValidatedCert(cert=amazon_cert, expiration_timestamp=expiration_timestamp)\n                self.valid_certificates[signature_chain_url] = validated_cert\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    connectable = settings.engine\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            transaction_per_migration=True,\n            target_metadata=target_metadata,\n            compare_type=COMPARE_TYPE,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()",
        "rewrite": ""
    },
    {
        "original": "\n  assertions = []\n  if not a.dtype.is_floating:\n    raise TypeError('Input `a` must have `float`-like `dtype` '\n                    '(saw {}).'.format(a.dtype.name))\n  if a.shape.ndims is not None:\n    if a.shape.ndims < 2:\n      raise ValueError('Input `a` must have at least 2 dimensions '\n                       '(saw: {}).'.format(a.shape.ndims))\n  elif validate_args:\n    assertions.append(tf.compat.v1.assert_rank_at_least(\n        a, rank=2, message='Input `a` must have at least 2 dimensions.'))\n  return assertions",
        "rewrite": ""
    },
    {
        "original": "\n        http_authorized = self._authorize()\n        return build(\n            'bigquery', 'v2', http=http_authorized, cache_discovery=False)",
        "rewrite": ""
    },
    {
        "original": "\n        callBigDlFunc(bigdl_type, \"setStopGradient\", self.value, stop_layers)\n        return self",
        "rewrite": ""
    },
    {
        "original": "\n        resp = self.run_pod_async(pod)\n        curr_time = dt.now()\n        if resp.status.start_time is None:\n            while self.pod_not_started(pod):\n                delta = dt.now() - curr_time\n                if delta.seconds >= startup_timeout:\n                    raise AirflowException(\"Pod took too long to start\")\n                time.sleep(1)\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    assertions = [\n        assert_util.assert_non_negative(\n            x, message=\"'{}' must be non-negative.\".format(x)),\n    ]\n    if not dtype_util.is_integer(x.dtype):\n      assertions += [\n          assert_integer_form(\n              x,\n              message=\"'{}' cannot contain fractional components.\".format(x)),\n      ]\n    return with_dependencies(assertions, x)",
        "rewrite": ""
    },
    {
        "original": "\n  values = tf.convert_to_tensor(value=values, name='values')\n  values_ = tf.get_static_value(values)\n\n  # Static didn't work.\n  if values_ is None:\n    # Cheap way to bring to at least 1d.\n    return values + tf.zeros([1], dtype=values.dtype)\n\n  # Static worked!\n  if values_.ndim > 1:\n    raise ValueError('values had > 1 dim: {}'.format(values_.shape))\n  # Cheap way to bring to at least 1d.\n  values_ = values_ + np.zeros([1], dtype=values_.dtype)\n  return list(values_)",
        "rewrite": ""
    },
    {
        "original": "\n        self.hook = self.get_hook()\n        self.hook.get_conn()\n\n        self.query_execution_context['Database'] = self.database\n        self.result_configuration['OutputLocation'] = self.output_location\n        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,\n                                                      self.result_configuration, self.client_request_token)\n        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)\n\n        if query_status in AWSAthenaHook.FAILURE_STATES:\n ",
        "rewrite": ""
    },
    {
        "original": "\n    import keras.backend as K\n\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            K.set_session(session)\n            return func(*args, **kwargs)\n    return _wrapped",
        "rewrite": ""
    },
    {
        "original": "\n        axis = {}\n        if custom_format and format:\n            axis['tickFormat'] = format\n        elif format:\n            if format == 'AM_PM':\n                axis['tickFormat'] = \"function(d) { return get_am_pm(parseInt(d)); }\"\n            else:\n                axis['tickFormat'] = \"d3.format(',%s')\" % format\n\n        if label:\n     ",
        "rewrite": ""
    },
    {
        "original": "\n        config = botocore.config.Config(retries={'max_attempts': 15})\n        return self.get_client_type('logs', config=config)",
        "rewrite": ""
    },
    {
        "original": "\n    if not isinstance(value, expected_type):\n        raise TypeError(\"{} argument must have a type {} not {}\".format(\n            key, expected_type, type(value)))",
        "rewrite": ""
    },
    {
        "original": "\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.variable_scope(scope):\n        BS, ML, MH = tf.unstack(tf.shape(memory))\n        memory_do = tf.nn.dropout(memory, keep_prob=keep_prob, noise_shape=[BS, 1, MH])\n        logits = tf.layers.dense(tf.layers.dense(memory_do, att_size, activation=tf.nn.tanh), 1, use_bias=False)\n        logits = softmax_mask(tf.squeeze(logits, [2]), mask)\n        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n        res = tf.reduce_sum(att_weights * memory, axis=1)\n        return res",
        "rewrite": ""
    },
    {
        "original": "\n    job_ids = []\n    for ti in tis:\n        if ti.state == State.RUNNING:\n            if ti.job_id:\n                ti.state = State.SHUTDOWN\n                job_ids.append(ti.job_id)\n        else:\n            task_id = ti.task_id\n            if dag and dag.has_task(task_id):\n                task = dag.get_task(task_id)\n ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n  ",
        "rewrite": ""
    },
    {
        "original": "\n        current_state = self._get_instance_view(resource_group, name).current_state\n        return (current_state.state,\n                current_state.exit_code,\n                current_state.detail_status)",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'IndependentBernoulli',\n                                 [params, event_shape]):\n      params = tf.convert_to_tensor(value=params, name='params')\n      event_shape = dist_util.expand_to_vector(\n          tf.convert_to_tensor(\n              value=event_shape, name='event_shape', dtype_hint=tf.int32),\n          tensor_name='event_shape')\n      new_shape = tf.concat([\n          tf.shape(input=params)[:-1],\n          event_shape,\n      ],",
        "rewrite": ""
    },
    {
        "original": "\n        if self.pid > 0:\n            print(\"Killing JVM with PID {}\".format(self.pid))\n            try:\n                self.child.terminate()\n                self.child.wait()\n            except OSError:\n                pass\n            self.pid = -1",
        "rewrite": ""
    },
    {
        "original": "\n        service = self.get_conn()\n        while True:\n            operation_response = service.operations().get(\n                name=operation_name,\n            ).execute(num_retries=self.num_retries)\n            if operation_response.get(\"done\"):\n                response = operation_response.get(\"response\")\n                error = operation_response.get(\"error\")\n                # Note,",
        "rewrite": ""
    },
    {
        "original": "\n  if not mcmc_util.is_list_like(bijector):\n    bijector = [bijector]\n\n  def fn(transformed_state_parts):\n    return [b.forward(sp) for b, sp in zip(bijector, transformed_state_parts)]\n\n  return fn",
        "rewrite": ""
    },
    {
        "original": "\n        if isinstance(features, np.ndarray):\n            features = [features]\n        else:\n            assert all(isinstance(feature, np.ndarray) for feature in features), \\\n                \"features should be a list of np.ndarray, not %s\" % type(features)\n        if np.isscalar(labels):  # in case labels is a scalar.\n            labels = [np.array(labels)]\n        elif isinstance(labels, np.ndarray):\n         ",
        "rewrite": ""
    },
    {
        "original": "\n        self._hook = SparkJDBCHook(\n            spark_app_name=self._spark_app_name,\n            spark_conn_id=self._spark_conn_id,\n            spark_conf=self._spark_conf,\n            spark_py_files=self._spark_py_files,\n            spark_files=self._spark_files,\n            spark_jars=self._spark_jars,\n            num_executors=self._num_executors,\n            executor_cores=self._executor_cores,\n            executor_memory=self._executor_memory,\n         ",
        "rewrite": ""
    },
    {
        "original": "\n\n  predicted_mean = _propagate_mean(filtered_mean,\n                                   transition_matrix,\n                                   transition_noise)\n  predicted_cov = _propagate_cov(filtered_cov,\n                                 transition_matrix,\n                ",
        "rewrite": ""
    },
    {
        "original": "\n        conn = self.get_connection(self.mssql_conn_id)\n        conn = pymssql.connect(\n            server=conn.host,\n            user=conn.login,\n            password=conn.password,\n            database=self.schema or conn.schema,\n            port=conn.port)\n        return conn",
        "rewrite": ""
    },
    {
        "original": "\n    if isinstance(size, numbers.Number):\n        size = (int(size), int(size))\n    else:\n        assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n\n    first_five = five_crop(img, size)\n\n    if vertical_flip:\n        img = vflip(img)\n    else:\n        img = hflip(img)\n\n    second_five = five_crop(img, size)\n    return first_five + second_five",
        "rewrite": ""
    },
    {
        "original": "\n  def grad(dy):\n    return array_ops.prevent_gradient(\n        dy, message=\"Second derivative is not implemented.\")\n\n  return tf.identity(x), grad",
        "rewrite": ""
    },
    {
        "original": "\n\n  with tf.compat.v1.name_scope(name, \"triangular\", [logu]):\n    logu = tf.convert_to_tensor(value=logu, name=\"logu\")\n    return pearson(logu) / (1. + tf.exp(logu))",
        "rewrite": ""
    },
    {
        "original": "\n        if a_ndarray is None:\n            return None\n        assert isinstance(a_ndarray, np.ndarray), \\\n            \"input should be a np.ndarray, not %s\" % type(a_ndarray)\n        return cls(a_ndarray,\n                   a_ndarray.shape if a_ndarray.shape else (a_ndarray.size),\n                   bigdl_type)",
        "rewrite": ""
    },
    {
        "original": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.aggregate(aggregate_query, **kwargs)",
        "rewrite": ""
    },
    {
        "original": "\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input('{}::'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {'exit', 'stop', 'quit', 'q'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred",
        "rewrite": ""
    },
    {
        "original": "\n        path = str(self.save_path.absolute())\n        log.info('[saving model to {}]'.format(path))\n        self._net.save(path)",
        "rewrite": ""
    },
    {
        "original": "\n        if self.message_type in ['text', 'markdown']:\n            data = {\n                'msgtype': self.message_type,\n                self.message_type: {\n                    'content': self.message\n                } if self.message_type == 'text' else self.message,\n                'at': {\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)\n    log.info('Run submitted with run_id: %s', operator.run_id)\n    run_page_url = hook.get_run_page_url(operator.run_id)\n    if operator.do_xcom_push:\n        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)\n\n    log.info('View run status, Spark UI, and logs at %s', run_page_url)\n    while True:\n        run_state = hook.get_run_state(operator.run_id)\n        if run_state.is_terminal:\n            if run_state.is_successful:\n                log.info('%s completed successfully.', operator.task_id)\n               ",
        "rewrite": ""
    },
    {
        "original": "\n        model = Model([], [], jvalue=jvalue)\n        model.value = jvalue\n        return model",
        "rewrite": ""
    },
    {
        "original": "\n        env = {}\n\n        for env_var_name, env_var_val in six.iteritems(self.kube_config.kube_env_vars):\n            env[env_var_name] = env_var_val\n\n        env[\"AIRFLOW__CORE__EXECUTOR\"] = \"LocalExecutor\"\n\n        if self.kube_config.airflow_configmap:\n            env['AIRFLOW_HOME'] = self.worker_airflow_home\n            env['AIRFLOW__CORE__DAGS_FOLDER'] = self.worker_airflow_dags\n        if (not self.kube_config.airflow_configmap and\n                'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self.kube_config.kube_secrets):\n            env['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = conf.get(\"core\",",
        "rewrite": ""
    },
    {
        "original": "\n        schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description))\n        tmp_file_handles = {}\n        row_no = 0\n\n        def _create_new_file():\n            handle = NamedTemporaryFile(delete=True)\n            filename = self.filename.format(len(tmp_file_handles))\n            tmp_file_handles[filename] = handle\n            return handle\n\n        # Don't create a file if there is nothing to write\n        if cursor.rowcount > 0:\n ",
        "rewrite": ""
    },
    {
        "original": "\n        source_project_dataset_tables = ([\n            source_project_dataset_tables\n        ] if not isinstance(source_project_dataset_tables, list) else\n            source_project_dataset_tables)\n\n        source_project_dataset_tables_fixup = []\n        for source_project_dataset_table in source_project_dataset_tables:\n            source_project, source_dataset, source_table = \\\n                _split_tablename(table_input=source_project_dataset_table,\n                           ",
        "rewrite": ""
    },
    {
        "original": "\n    DM = models.DagModel\n    dag = session.query(DM).filter(DM.dag_id == dag_id).first()\n    if dag is None:\n        raise DagNotFound(\"Dag id {} not found\".format(dag_id))\n\n    if dag.fileloc and os.path.exists(dag.fileloc):\n        raise DagFileExists(\"Dag id {} is still in DagBag. \"\n                            \"Remove the DAG file first: {}\".format(dag_id, dag.fileloc))\n\n    count = 0\n\n    # noinspection PyUnresolvedReferences,PyProtectedMember\n    for m in models.base.Base._decl_class_registry.values():\n        if hasattr(m, \"dag_id\"):\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        for dep_status in self.get_dep_statuses(ti, session, dep_context):\n            if not dep_status.passed:\n                yield dep_status.reason",
        "rewrite": ""
    },
    {
        "original": "\n        log_dir: Path = Path(self.config['log_path']).expanduser().resolve() / self.agent_name\n        log_dir.mkdir(parents=True, exist_ok=True)\n        log_file_path = Path(log_dir, f'{self._get_timestamp_utc_str()}_{self.agent_name}.log')\n        log_file = open(log_file_path, 'a', buffering=1, encoding='utf8')\n        return log_file",
        "rewrite": ""
    },
    {
        "original": "\n        intent_name = self.config['intent_name']\n        slot_name = self.config['slot_name']\n\n        request_id = request['request']['requestId']\n        request_intent: dict = request['request']['intent']\n\n        if intent_name != request_intent['name']:\n            log.error(f\"Wrong intent name received: {request_intent['name']} in request {request_id}\")\n            return {'error': 'wrong intent name'}\n\n        if slot_name not in request_intent['slots'].keys():\n            log.error(f'No slot named {slot_name} found in request {request_id}')\n         ",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name):\n      if isinstance(self._sample_shape, tf.Tensor):\n        return self._sample_shape\n      return tf.convert_to_tensor(\n          value=self.sample_shape.as_list(), dtype=tf.int32)",
        "rewrite": ""
    },
    {
        "original": "\n        # Assign unique ID if one isn't provided\n        if document_id is None:\n            document_id = str(uuid.uuid4())\n\n        if document is None:\n            raise AirflowBadRequest(\"You cannot insert a None document\")\n\n        # Add document id if isn't found\n        if 'id' in document:\n            if document['id'] is None:\n                document['id'] = document_id\n",
        "rewrite": ""
    },
    {
        "original": "\n  if dims == 0:\n    return tf.math.abs(value)\n  elif dims == 1:\n    axis = -1\n  elif dims == 2:\n    axis = [-1, -2]\n  else:\n    ValueError(dims)\n  if order is None:\n    order = np.inf\n  return tf.norm(tensor=value, axis=axis, ord=order)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'maybe_call_fn_and_grads',\n                               [fn_arg_list, result, grads]):\n    fn_arg_list = (list(fn_arg_list) if is_list_like(fn_arg_list)\n                   else [fn_arg_list])\n    result, grads = _value_and_gradients(fn, fn_arg_list, result, grads)\n    if not all(r.dtype.is_floating\n               for r in (result if is_list_like(result) else [result])):  # pylint: disable=superfluous-parens\n      raise TypeError('Function result must be a `Tensor` with `float` '\n     ",
        "rewrite": ""
    },
    {
        "original": "\n  if dtype is not None or not tf.nest.is_nested(struct):\n    return tf.convert_to_tensor(struct, dtype=dtype)\n\n  if _maybe_convertible_to_tensor(struct):\n    try:\n      # Try converting the structure wholesale.\n      return tf.convert_to_tensor(value=struct, name=name)\n    except (ValueError, TypeError):\n      # Unfortunately Eager/Graph mode don't agree on the error type.\n      pass\n  # Try converting all of its children.\n  shallow_struct = _get_shallow_structure(struct)\n  return nest.map_structure_up_to(\n      shallow_struct, lambda s: _nested_convert_to_tensor(s, name=name), struct)",
        "rewrite": ""
    },
    {
        "original": "\n\n        # update the state of the previously active dag runs\n        dag_runs = DagRun.find(dag_id=dag.dag_id, state=State.RUNNING, session=session)\n        active_dag_runs = []\n        for run in dag_runs:\n            self.log.info(\"Examining DAG run %s\", run)\n            # don't consider runs that are executed in the future\n            if run.execution_date > timezone.utcnow():\n                self.log.error(\n         ",
        "rewrite": ""
    },
    {
        "original": "\n\n    url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey\n    type, ext, size = url_info(url)\n\n    print_info(site_info, title, 'flv', size)\n    if not info_only:\n        download_urls([url], title, 'flv', size, output_dir = output_dir, merge = merge)",
        "rewrite": ""
    },
    {
        "original": "\n    # Parse arguments\n    usage = \"usage: nvd3.py [options]\"\n    parser = OptionParser(usage=usage,\n                          version=(\"python-nvd3 - Charts generator with \"\n                                   \"nvd3.js and d3.js\"))\n    parser.add_option(\"-q\", \"--quiet\",\n                      action=\"store_false\", dest=\"verbose\", default=True,\n          ",
        "rewrite": ""
    },
    {
        "original": "\n    if string_field is None:\n        return None\n    elif bq_type == 'INTEGER':\n        return int(string_field)\n    elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP':\n        return float(string_field)\n    elif bq_type == 'BOOLEAN':\n        if string_field not in ['true', 'false']:\n            raise ValueError(\"{} must have value 'true' or 'false'\".format(\n                string_field))\n        return string_field == 'true'\n    else:\n     ",
        "rewrite": ""
    },
    {
        "original": "\n    if file_path is None or file_path == '-':\n        if sys.stdin.isatty():\n            raise RuntimeError('To process data from terminal please use interact mode')\n        f = sys.stdin\n    else:\n        f = open(file_path, encoding='utf8')\n\n    model: Chainer = build_model(config)\n\n    args_count = len(model.in_x)\n    while True:\n        batch = list((l.strip() for l in islice(f, batch_size * args_count)))\n\n        if not batch:\n            break\n\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        if parameter_name not in parms:\n            parms[parameter_name] = parameter_value\n        elif parameter_value is not None and parms[parameter_name] != parameter_value:\n            parms[parameter_name] = parameter_value\n            warnings.warn(\"\\n\\n\\t`%s` parameter has been already set and had a different value in `train` method. The last passed value \\\"%s\\\" is used.\" % (parameter_name, parameter_value), UserWarning, stacklevel=2)",
        "rewrite": ""
    },
    {
        "original": "\n    answer = [dict() for node in dictionary.data]\n    if n == 0:\n        return answer\n    curr_alphabet = copy.copy(dictionary.alphabet)\n    if allow_spaces:\n        curr_alphabet += [' ']\n    for l, (costs_in_node, node) in enumerate(zip(answer, dictionary.data)):\n        # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u044f \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432\n        curr_node_removal_costs = np.empty(dtype=np.float64, shape=(n,))\n        if len(node[0]) > 0:\n            curr_node_removal_costs[0] = min(removal_costs[symbol] for symbol in node[0])\n            for j, symbols in enumerate(node[1:],",
        "rewrite": ""
    },
    {
        "original": "\n\n        if self.terminating:\n            # ensure termination if processes are created later\n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if",
        "rewrite": ""
    },
    {
        "original": "\n        filters = []\n        if key:\n            filters.append(cls.key == key)\n        if task_id:\n            filters.append(cls.task_id == task_id)\n        if dag_id:\n            filters.append(cls.dag_id == dag_id)\n        if include_prior_dates:\n            filters.append(cls.execution_date <= execution_date)\n        else:\n            filters.append(cls.execution_date == execution_date)\n\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        if not hasattr(self, 'sess'):\n            raise RuntimeError('Your TensorFlow model {} must'\n                               ' have sess attribute!'.format(self.__class__.__name__))\n        path = str(self.save_path.resolve())\n        log.info('[saving model to {}]'.format(path))\n        var_list = self._get_saveable_variables(exclude_scopes)\n        saver = tf.train.Saver(var_list)\n        saver.save(self.sess, path)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'sparse_or_dense_matmul',\n                               [sparse_or_dense_a, dense_b]):\n    dense_b = tf.convert_to_tensor(\n        value=dense_b, dtype_hint=tf.float32, name='dense_b')\n\n    if validate_args:\n      assert_a_rank_at_least_2 = tf.compat.v1.assert_rank_at_least(\n          sparse_or_dense_a,\n          rank=2,\n          message='Input `sparse_or_dense_a` must have at least 2 dimensions.')\n      assert_b_rank_at_least_2 = tf.compat.v1.assert_rank_at_least(\n          dense_b,\n       ",
        "rewrite": ""
    },
    {
        "original": "\n        def get_py_name(jclass_name):\n            if jclass_name == \"StaticGraph\" or jclass_name == \"DynamicGraph\":\n                return \"Model\"\n            elif jclass_name == \"Input\":\n                return \"Layer\"\n            else:\n                return jclass_name\n\n        jname = callBigDlFunc(bigdl_type,\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    from keras.preprocessing import sequence\n    from keras.datasets import imdb\n    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=20000)\n    X_train = sequence.pad_sequences(X_train, maxlen=100)\n    X_test = sequence.pad_sequences(X_test, maxlen=100)\n    return X_train, y_train, X_test, y_test",
        "rewrite": ""
    },
    {
        "original": "\n        proxies = {}\n        if self.proxy:\n            # we only need https proxy for Discord\n            proxies = {'https': self.proxy}\n\n        discord_payload = self._build_discord_payload()\n\n        self.run(endpoint=self.webhook_endpoint,\n                 data=discord_payload,\n                 headers={'Content-type': 'application/json'},\n                 extra_options={'proxies': proxies})",
        "rewrite": ""
    },
    {
        "original": "\n        if dep_context.ignore_in_reschedule_period:\n            yield self._passing_status(\n                reason=\"The context specified that being in a reschedule period was \"\n                       \"permitted.\")\n            return\n\n        if ti.state not in self.RESCHEDULEABLE_STATES:\n            yield self._passing_status(\n                reason=\"The task instance is",
        "rewrite": ""
    },
    {
        "original": "\n        try:\n            waiter = self.client.get_waiter('job_execution_complete')\n            waiter.config.max_attempts = sys.maxsize  # timeout is managed by airflow\n            waiter.wait(jobs=[self.jobId])\n        except ValueError:\n            # If waiter not available use expo\n            retry = True\n            retries = 0\n\n            while retries < self.max_retries and retry:\n",
        "rewrite": ""
    },
    {
        "original": "\n        elapsed_time_since_refresh = (timezone.utcnow() -\n                                      self.last_dag_dir_refresh_time).total_seconds()\n        if elapsed_time_since_refresh > self.dag_dir_list_interval:\n            # Build up a list of Python files that could contain DAGs\n            self.log.info(\"Searching for files in %s\", self._dag_directory)\n            self._file_paths = list_py_file_paths(self._dag_directory)\n            self.last_dag_dir_refresh_time =",
        "rewrite": ""
    },
    {
        "original": "\n        key_path = self._get_field(extras, 'key_path', False)\n        keyfile_json_str = self._get_field(extras, 'keyfile_dict', False)\n\n        if not key_path and not keyfile_json_str:\n            self.log.info('Using gcloud with application default credentials.')\n        elif key_path:\n            os.environ[G_APP_CRED] = key_path\n        else:\n            # Write service account JSON to secure file for gcloud to reference\n            service_key = tempfile.NamedTemporaryFile(delete=False)\n     ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_half_normal_half_normal\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 119\n    return (tf.math.log(b.scale) - tf.math.log(a.scale) +\n            (a.scale**2 - b.scale**2) / (2 * b.scale**2))",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        self.log.info(\"Synthesizing input: %s\" % input_data)\n        return client.synthesize_speech(\n            input_=input_data, voice=voice, audio_config=audio_config, retry=retry, timeout=timeout\n        )",
        "rewrite": ""
    },
    {
        "original": "\n  for i in sorted(axis):\n    x = tf.expand_dims(x, axis=i)\n  return x",
        "rewrite": ""
    },
    {
        "original": "\n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])",
        "rewrite": ""
    },
    {
        "original": "\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
        "rewrite": ""
    },
    {
        "original": "\n    log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())\n    global engine\n    global Session\n\n    if Session:\n        Session.remove()\n        Session = None\n    if engine:\n        engine.dispose()\n        engine = None",
        "rewrite": ""
    },
    {
        "original": "\n        return \" \".join(self.command_as_list(\n            mark_success=mark_success,\n            ignore_all_deps=ignore_all_deps,\n            ignore_depends_on_past=ignore_depends_on_past,\n            ignore_task_deps=ignore_task_deps,\n            ignore_ti_state=ignore_ti_state,\n            local=local,\n            pickle_id=pickle_id,\n            raw=raw,\n            job_id=job_id,\n         ",
        "rewrite": ""
    },
    {
        "original": "\n        predicts = callBigDlFunc(self.bigdl_type, \"distributedImageFrameToPredict\", self.value, key)\n        return predicts.map(lambda predict: (predict[0], predict[1].to_ndarray()) if predict[1] else (predict[0], None))",
        "rewrite": ""
    },
    {
        "original": "\n        callBigDlFunc(bigdl_type, \"transformImageFeature\", self.value, image_feature)\n        return image_feature",
        "rewrite": ""
    },
    {
        "original": "\n    optimizer.set_validation(\n        batch_size=options.batchSize,\n        val_rdd=test_data,\n        trigger=EveryEpoch(),\n        val_method=[Top1Accuracy()]\n    )\n    optimizer.set_checkpoint(EveryEpoch(), options.checkpointPath)",
        "rewrite": ""
    },
    {
        "original": "\n        if event_name == \"after_validation\":\n            if data['impatience'] > self._learning_rate_last_impatience:\n                self._learning_rate_cur_impatience += 1\n            else:\n                self._learning_rate_cur_impatience = 0\n\n            self._learning_rate_last_impatience = data['impatience']\n\n            if (self._learning_rate_drop_patience is not None) and\\\n                    (self._learning_rate_cur_impatience >=\n ",
        "rewrite": ""
    },
    {
        "original": "\n  flat_values = []\n  for value in values:\n    # Checks if it is a namedtuple.\n    if hasattr(value, '_fields'):\n      for field in value._fields:\n        flat_values.extend([field, _to_str(getattr(value, field))])\n      continue\n    if isinstance(value, (list, tuple)):\n      for v in value:\n        flat_values.append(_to_str(v))\n      continue\n    flat_values.append(_to_str(value))\n  return tf.compat.v1.Print(pass_through_tensor, flat_values)",
        "rewrite": ""
    },
    {
        "original": "\n    out = self.dense(inputs)  # (..., batch, time, hidden)\n    out = self.output_layer(out)  # (..., batch, time, 2*latent)\n    loc = out[..., :self.latent_size]\n    scale_diag = tf.nn.softplus(out[..., self.latent_size:]) + 1e-5  # keep > 0\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)",
        "rewrite": ""
    },
    {
        "original": "\n        # TODO: this shares quite a lot of code with _manage_executor_state\n\n        TI = models.TaskInstance\n        for key, state in list(self.executor.get_event_buffer(simple_dag_bag.dag_ids)\n                                   .items()):\n            dag_id, task_id, execution_date, try_number = key\n            self.log.info(\n                \"Executor reports execution of %s.%s execution_date=%s \"\n ",
        "rewrite": ""
    },
    {
        "original": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n             ",
        "rewrite": ""
    },
    {
        "original": "\n        if not self.api_params:\n            self.construct_api_call_params()\n        slack = SlackHook(token=self.token, slack_conn_id=self.slack_conn_id)\n        slack.call(self.method, self.api_params)",
        "rewrite": ""
    },
    {
        "original": "\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(x)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tfp.layers.Convolution2DFlipout(\n      filters,\n      kernel,\n      padding='same',\n      kernel_posterior_fn=kernel_posterior_fn)(out)\n  out = tf.keras.layers.BatchNormalization()(out)\n  out = tf.keras.layers.Activation('relu')(out)\n\n  out = tf.keras.layers.MaxPooling2D(\n      pool_size=(2, 2), strides=stride)(out)\n  return out",
        "rewrite": ""
    },
    {
        "original": "\n  if not callable(true_fn):\n    raise TypeError('`true_fn` must be callable.')\n  if not callable(false_fn):\n    raise TypeError('`false_fn` must be callable.')\n\n  pred_value = _get_static_predicate(pred)\n  if pred_value is not None:\n    if pred_value:\n      return true_fn()\n    else:\n      return false_fn()\n  else:\n    return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn, name=name)",
        "rewrite": ""
    },
    {
        "original": "\n\n    def on_terminate(p):\n        log.info(\"Process %s (%s) terminated with exit code %s\", p, p.pid, p.returncode)\n\n    if pid == os.getpid():\n        raise RuntimeError(\"I refuse to kill myself\")\n\n    parent = psutil.Process(pid)\n\n    children = parent.children(recursive=True)\n    children.append(parent)\n\n    try:\n        pg = os.getpgid(pid)\n    except OSError as err:\n        # Skip if not such process - we experience a race and it just terminated\n        if err.errno == errno.ESRCH:\n            return\n  ",
        "rewrite": ""
    },
    {
        "original": "\n\n  with open(download(data_dir, \"vocab.pkl\"), \"r\") as f:\n    words_to_idx = pickle.load(f)\n  num_words = len(words_to_idx)\n\n  vocabulary = [None] * num_words\n  for word, idx in words_to_idx.items():\n    vocabulary[idx] = word\n\n  # Build an iterator over training batches.\n  def train_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"train\", num_words, shuffle_and_repeat=True)\n    # Prefetching makes training about 1.5x faster.\n    dataset = dataset.batch(batch_size).prefetch(32)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  # Build an iterator over the heldout set.\n  def eval_input_fn():\n    dataset = newsgroups_dataset(\n        data_dir, \"test\", num_words, shuffle_and_repeat=False)\n    dataset = dataset.batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn,",
        "rewrite": ""
    },
    {
        "original": "\n        TR = TaskReschedule\n        return (\n            session\n            .query(TR)\n            .filter(TR.dag_id == task_instance.dag_id,\n                    TR.task_id == task_instance.task_id,\n                    TR.execution_date == task_instance.execution_date,\n                    TR.try_number == task_instance.try_number)\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        worker_secrets = []\n\n        for env_var_name, obj_key_pair in six.iteritems(self.kube_config.kube_secrets):\n            k8s_secret_obj, k8s_secret_key = obj_key_pair.split('=')\n            worker_secrets.append(\n                Secret('env', env_var_name, k8s_secret_obj, k8s_secret_key)\n            )\n\n        if self.kube_config.env_from_secret_ref:\n            for secret_ref in self.kube_config.env_from_secret_ref.split(','):\n                worker_secrets.append(\n       ",
        "rewrite": ""
    },
    {
        "original": "\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)",
        "rewrite": ""
    },
    {
        "original": "\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    ",
        "rewrite": ""
    },
    {
        "original": "\n        return {DAY: field_date.day, MONTH: field_date.month, YEAR: field_date.year}",
        "rewrite": ""
    },
    {
        "original": "\n\n        # File Path: Path to the file containing the DAG definition\n        # PID: PID associated with the process that's processing the file. May\n        # be empty.\n        # Runtime: If the process is currently running, how long it's been\n        # running for in seconds.\n        # Last Runtime: If the process ran before, how long did it take to\n        # finish in seconds\n        # Last Run: When the file finished processing in the",
        "rewrite": ""
    },
    {
        "original": "\n        for file_path, processor in self._processors.items():\n            while not processor.done:\n                time.sleep(0.1)",
        "rewrite": ""
    },
    {
        "original": "\n        if conversation_key in self.conversations.keys():\n            del self.conversations[conversation_key]\n            log.info(f'Deleted conversation, key: {conversation_key}')",
        "rewrite": ""
    },
    {
        "original": "\n        self.hook = SlackWebhookHook(\n            self.http_conn_id,\n            self.webhook_token,\n            self.message,\n            self.attachments,\n            self.channel,\n            self.username,\n            self.icon_emoji,\n            self.link_names,\n            self.proxy\n        )\n ",
        "rewrite": ""
    },
    {
        "original": "\n        result = callBigDlFunc(self.bigdl_type,\n                               \"modelPredictRDD\", self.value, data_rdd, batch_size)\n        return result.map(lambda data: data.to_ndarray())",
        "rewrite": ""
    },
    {
        "original": "\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                   ",
        "rewrite": ""
    },
    {
        "original": "\n  ndims_static = _get_static_ndims(\n      x,\n      expect_ndims=expect_ndims,\n      expect_ndims_at_least=expect_ndims_at_least,\n      expect_ndims_no_more_than=expect_ndims_no_more_than)\n  if ndims_static is not None:\n    return ndims_static\n  return tf.rank(x)",
        "rewrite": ""
    },
    {
        "original": "\n  # Step size to choose when the coordinate is zero.\n  small_sizes = tf.ones_like(reference_vertex) * 0.00025\n  # Step size to choose when the coordinate is non-zero.\n  large_sizes = reference_vertex * 0.05\n  return tf.where(tf.abs(reference_vertex) < _EPSILON,\n                  small_sizes,\n                  large_sizes)",
        "rewrite": ""
    },
    {
        "original": "\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
        "rewrite": ""
    },
    {
        "original": "\n\n        hook = SQSHook(aws_conn_id=self.aws_conn_id)\n\n        result = hook.send_message(queue_url=self.sqs_queue,\n                                   message_body=self.message_content,\n                                   delay_seconds=self.delay_seconds,\n                                   message_attributes=self.message_attributes)\n\n  ",
        "rewrite": ""
    },
    {
        "original": "\n  initial_simplex = tf.convert_to_tensor(value=initial_simplex)\n\n  # If d is the dimension of the problem, the number of vertices in the\n  # simplex should be d+1. From this, we can infer the number of dimensions\n  # as n - 1 where n is the number of vertices specified.\n  num_vertices = tf.shape(input=initial_simplex)[0]\n  dim = num_vertices - 1\n  num_evaluations = 0\n\n  if objective_at_initial_simplex is None:\n    objective_at_initial_simplex, n_evals = _evaluate_objective_multiple(\n        objective_function, initial_simplex, batch_evaluate_objective)\n    num_evaluations += n_evals\n  objective_at_initial_simplex = tf.convert_to_tensor(\n      value=objective_at_initial_simplex)\n  return (dim,\n          num_vertices,\n          initial_simplex,\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        if callbacks:\n            raise Exception(\"We don't support callbacks in fit for now\")\n        if class_weight:\n            unsupport_exp(\"class_weight\")\n        if sample_weight:\n            unsupport_exp(\"sample_weight\")\n        if initial_epoch != 0:\n            unsupport_exp(\"initial_epoch\")\n        if shuffle != True:\n            unsupport_exp(\"shuffle\")\n        if validation_split !=",
        "rewrite": ""
    },
    {
        "original": "\n        bash_command = self.bash_command\n        self.log.info(\"Tmp dir root location: \\n %s\", gettempdir())\n        with TemporaryDirectory(prefix='airflowtmp') as tmp_dir:\n            with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as f:\n                f.write(bytes(bash_command, 'utf_8'))\n                f.flush()\n                fname = f.name\n                script_location = tmp_dir + \"/\" + fname\n   ",
        "rewrite": ""
    },
    {
        "original": "\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol",
        "rewrite": ""
    },
    {
        "original": "\n        s3_operations = config.pop('S3Operations', None)\n\n        if s3_operations is not None:\n            create_bucket_ops = s3_operations.get('S3CreateBucket', [])\n            upload_ops = s3_operations.get('S3Upload', [])\n            for op in create_bucket_ops:\n                self.s3_hook.create_bucket(bucket_name=op['Bucket'])\n            for op in upload_ops:\n                if op['Tar']:\n           ",
        "rewrite": ""
    },
    {
        "original": "\n    if pretrained:\n        if 'transform_input' not in kwargs:\n            kwargs['transform_input'] = True\n        if 'aux_logits' in kwargs:\n            original_aux_logits = kwargs['aux_logits']\n            kwargs['aux_logits'] = True\n        else:\n            original_aux_logits = True\n        model = Inception3(**kwargs)\n        model.load_state_dict(model_zoo.load_url(model_urls['inception_v3_google']))\n        if not original_aux_logits:\n       ",
        "rewrite": ""
    },
    {
        "original": "\n    if old is None:\n      return new\n    if new is None:\n      return old\n    if (old == new) if use_equals else (old is new):\n      return old\n    raise ValueError(\"Incompatible values: %s != %s\" % (old, new))",
        "rewrite": ""
    },
    {
        "original": "\n  small_primes = np.array((2, 3, 5))\n  if n <= 6:\n    return small_primes[small_primes < n]\n  sieve = np.ones(n // 3 + (n % 6 == 2), dtype=np.bool)\n  sieve[0] = False\n  m = int(n ** 0.5) // 3 + 1\n  for i in range(m):\n    if not sieve[i]:\n      continue\n    k = 3 * i + 1 | 1\n    sieve[k ** 2 // 3::2 * k] = False\n    sieve[(k ** 2 + 4 * k - 2 * k * (i & 1)) // 3::2 * k] = False\n  return np.r_[2, 3, 3 * np.nonzero(sieve)[0] + 1 | 1]",
        "rewrite": ""
    },
    {
        "original": "\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
        "rewrite": ""
    },
    {
        "original": "\n    labels = []\n    with open(os.path.join(data_dir, info_file), 'r') as f:\n        labels = [int(line.split()[0]) for line in f]\n    return torch.LongTensor(labels)",
        "rewrite": ""
    },
    {
        "original": "\n        TI = models.TaskInstance\n        ti_concurrency_query = (\n            session\n            .query(TI.task_id, TI.dag_id, func.count('*'))\n            .filter(TI.state.in_(states))\n            .group_by(TI.task_id, TI.dag_id)\n        ).all()\n        dag_map = defaultdict(int)\n        task_map = defaultdict(int)\n        for result in ti_concurrency_query:\n            task_id, dag_id, count = result\n ",
        "rewrite": ""
    },
    {
        "original": "\n    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n    return \\\n        rdd.ctx._jvm.org.apache.spark.bigdl.api.python.BigDLSerDe.pythonToJava(\n            rdd._jrdd, True)",
        "rewrite": ""
    },
    {
        "original": "\n    kernel_results = self._impl.bootstrap_results(init_state)\n    if self.step_size_update_fn is not None:\n      step_size_assign = self.step_size_update_fn(self.step_size, None)  # pylint: disable=not-callable\n      kernel_results = kernel_results._replace(\n          extra=HamiltonianMonteCarloExtraKernelResults(\n              step_size_assign=step_size_assign))\n    return kernel_results",
        "rewrite": ""
    },
    {
        "original": "\n        self.log.info(\"Waiting for OPERATION_NAME %s\", operation.name)\n        time.sleep(OPERATIONAL_POLL_INTERVAL)\n        while operation.status != Operation.Status.DONE:\n            if operation.status == Operation.Status.RUNNING or operation.status == \\\n                    Operation.Status.PENDING:\n                time.sleep(OPERATIONAL_POLL_INTERVAL)\n            else:\n                raise exceptions.GoogleCloudError(\n            ",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'OneHotCategorical',\n                                 [params, event_size]):\n      return tfd.OneHotCategorical(\n          logits=params,\n          dtype=dtype or params.dtype.base_dtype,\n          validate_args=validate_args)",
        "rewrite": ""
    },
    {
        "original": "\n  assertions = []\n\n  def validate_equal_last_dim(tensor_a, tensor_b, message):\n    if tensor_a.shape.is_fully_defined() and tensor_b.shape.is_fully_defined():\n      if tensor_a.shape[-1] != tensor_b.shape[-1]:\n        raise ValueError(message)\n    elif validate_args:\n      assertions.append(\n          tf.compat.v1.assert_equal(\n              tf.shape(input=tensor_a)[-1],\n              tf.shape(input=tensor_b)[-1],\n              message=message))\n\n  if logits is not None:\n    validate_equal_last_dim(\n        outcomes,\n        logits,\n    ",
        "rewrite": ""
    },
    {
        "original": "\n    log = LoggingMixin().log\n    try:\n        message = api_client.trigger_dag(dag_id=args.dag_id,\n                                         run_id=args.run_id,\n                                         conf=args.conf,\n                         ",
        "rewrite": ""
    },
    {
        "original": "\n        security_context = {}\n\n        if self.kube_config.worker_run_as_user:\n            security_context['runAsUser'] = self.kube_config.worker_run_as_user\n\n        if self.kube_config.worker_fs_group:\n            security_context['fsGroup'] = self.kube_config.worker_fs_group\n\n        # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth\n        if self.kube_config.git_ssh_key_secret_name and security_context.get('fsGroup') is None:\n            security_context['fsGroup'] = 65533\n\n        return security_context",
        "rewrite": ""
    },
    {
        "original": "\n        tables_resource = self.service.tables() \\\n            .get(projectId=self.project_id, datasetId=dataset_id, tableId=table_id) \\\n            .execute(num_retries=self.num_retries)\n        return tables_resource['schema']",
        "rewrite": ""
    },
    {
        "original": "\n        with open(\"/proc/self/cgroup\") as f:\n            lines = f.readlines()\n            d = {}\n            for line in lines:\n                line_split = line.rstrip().split(\":\")\n                subsystem = line_split[1]\n                group_name = line_split[2]\n                d[subsystem] = group_name\n ",
        "rewrite": ""
    },
    {
        "original": "\n  was_iterable = False\n  if initial_position is not None:\n    initial_position, was_iterable = _ensure_list(initial_position)\n\n  if initial_population is not None:\n    initial_population, was_iterable = _ensure_list(initial_population)\n\n  population = _get_starting_population(initial_population,\n                                        initial_position,\n                                        population_size,\n             ",
        "rewrite": ""
    },
    {
        "original": "\n        now = timezone.utcnow()\n        zombies = []\n        if (now - self._last_zombie_query_time).total_seconds() \\\n                > self._zombie_query_interval:\n            # to avoid circular imports\n            from airflow.jobs import LocalTaskJob as LJ\n            self.log.info(\"Finding 'running' jobs without a recent heartbeat\")\n            TI = airflow.models.TaskInstance\n            limit_dttm =",
        "rewrite": ""
    },
    {
        "original": "\n    units_shape = tf.shape(units)\n    noise_shape = [units_shape[n] for n in range(len(units.shape))]\n    for dim in fixed_mask_dims:\n        noise_shape[dim] = 1\n    return tf.nn.dropout(units, keep_prob, noise_shape)",
        "rewrite": ""
    },
    {
        "original": "\n        log_group = '/aws/sagemaker/TrainingJobs'\n\n        if len(stream_names) < instance_count:\n            # Log streams are created whenever a container starts writing to stdout/err, so this list\n            # may be dynamic until we have a stream for every instance.\n            logs_conn = self.get_log_conn()\n            try:\n                streams = logs_conn.describe_log_streams(\n             ",
        "rewrite": ""
    },
    {
        "original": "\n        client = self.get_conn()\n        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)\n        self.log.info(\"Recognised speech: %s\" % response)\n        return response",
        "rewrite": ""
    },
    {
        "original": "\n        if not bucket_name:\n            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)\n\n        prefix = re.split(r'[*]', wildcard_key, 1)[0]\n        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)\n        if klist:\n            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]\n            if key_matches:\n                return self.get_key(key_matches[0], bucket_name)",
        "rewrite": ""
    },
    {
        "original": "\n        if value is not None:\n            if value.tzinfo is None:\n                value = value.replace(tzinfo=utc)\n            else:\n                value = value.astimezone(utc)\n\n        return value",
        "rewrite": ""
    },
    {
        "original": "\n  # Fail if `val_c` is no longer finite.\n  new_failed = initial_args.active & ~is_finite(val_c)\n  active = initial_args.active & ~new_failed\n  failed = initial_args.failed | new_failed\n\n  # We converge when we find a point satisfying the Wolfe conditions, in those\n  # cases we set `val_left = val_right = val_c`.\n  found_wolfe = active & _satisfies_wolfe(\n      val_0, val_c, f_lim, sufficient_decrease_param, curvature_param)\n  val_left = val_where(found_wolfe, val_c, initial_args.left)\n  val_right = val_where(found_wolfe, val_c, initial_args.right)\n  converged = initial_args.converged | found_wolfe\n  active = active & ~found_wolfe\n\n  # If any active batch members remain, we apply the `update` function to\n  # squeeze further their corresponding left/right bracketing interval.\n  def _apply_update():\n    update_result = update(\n   ",
        "rewrite": ""
    },
    {
        "original": "\n        jmodel = callBigDlFunc(bigdl_type, \"loadCaffeModel\", defPath, modelPath)\n        return Layer.of(jmodel)",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name_scope):\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"mean\"),\n        data=dist.mean(),\n        step=tf.compat.v1.train.get_or_create_global_step())\n    tf.compat.v2.summary.histogram(\n        name=\"{}/{}\".format(name, \"stddev\"),\n        data=dist.stddev(),\n        step=tf.compat.v1.train.get_or_create_global_step())",
        "rewrite": ""
    },
    {
        "original": "\n        endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}'.format(account_number = account_number, video_id = video_id)\n        fake_header_id = fake_headers\n        #is this somehow related to the time? Magic....\n        fake_header_id['Accept'] ='application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ'\n\n        html = get_content(endpoint, headers= fake_header_id)\n        html_json = json.loads(html)\n\n        link_list = []\n\n        for i in html_json['sources']:\n            if 'src' in i:  #to avoid KeyError\n                if",
        "rewrite": ""
    },
    {
        "original": "\n  if hasattr(f, \"_func\"):  # functions returned by tf.make_template\n    f = f._func  # pylint: disable=protected-access\n\n  try:  # getargspec was deprecated in Python 3.6\n    argspec = inspect.getfullargspec(f)\n  except AttributeError:\n    argspec = inspect.getargspec(f)\n\n  fkwargs = {k: v for k, v in six.iteritems(src_kwargs) if k in argspec.args}\n  return fkwargs",
        "rewrite": ""
    },
    {
        "original": "\n\n        if not dataset_id or not isinstance(dataset_id, str):\n            raise ValueError(\"dataset_id argument must be provided and has \"\n                             \"a type 'str'. You provided: {}\".format(dataset_id))\n\n        dataset_project_id = project_id if project_id else self.project_id\n\n        try:\n            dataset_resource = self.service.datasets().get(\n                datasetId=dataset_id, projectId=dataset_project_id).execute(num_retries=self.num_retries)\n      ",
        "rewrite": ""
    },
    {
        "original": "\n\n  num_timesteps = tf.shape(input=time_series_tensor)[-1]\n\n  # Compute the index of the first unmasked entry for each series in the batch.\n  unmasked_negindices = (\n      tf.cast(~broadcast_mask, tf.int32) *\n      tf.range(num_timesteps, 0, -1))\n  first_unmasked_indices = num_timesteps - tf.reduce_max(\n      input_tensor=unmasked_negindices, axis=-1)\n\n  if first_unmasked_indices.shape.ndims is None:\n    raise NotImplementedError(\n        'Cannot compute initial values of a masked time series with'\n        'dynamic rank.')  # `batch_gather` requires static rank\n\n  # Extract the initial value for each series in the batch.\n  return tf.squeeze(tf.compat.v1.batch_gather(\n      params=time_series_tensor,\n      indices=first_unmasked_indices[..., tf.newaxis]), axis=-1)",
        "rewrite": ""
    },
    {
        "original": "\n    with tf.compat.v1.name_scope(name, 'MixtureSameFamily_params_size',\n                                 [num_components, component_params_size]):\n      num_components = tf.convert_to_tensor(\n          value=num_components, name='num_components', dtype_hint=tf.int32)\n      component_params_size = tf.convert_to_tensor(\n          value=component_params_size, name='component_params_size')\n\n      num_components = dist_util.prefer_static_value(num_components)\n      component_params_size = dist_util.prefer_static_value(\n          component_params_size)\n\n      return num_components + num_components * component_params_size",
        "rewrite": ""
    },
    {
        "original": "\n    population_size = len(population)\n    for k in range(population_size // len(gpus) + 1):\n        procs = []\n        for j in range(len(gpus)):\n            i = k * len(gpus) + j\n            if i < population_size:\n                save_path = expand_path(\n                    evolution.get_value_from_config(parse_config(population[i]),\n                  ",
        "rewrite": ""
    },
    {
        "original": "\n        service = self.get_conn()\n        while True:\n            if zone is None:\n                # noinspection PyTypeChecker\n                operation_response = self._check_global_operation_status(\n                    service, operation_name, project_id)\n            else:\n                # noinspection PyTypeChecker\n     ",
        "rewrite": ""
    },
    {
        "original": "\n        cmd = {}\n\n        if self.channel:\n            cmd['channel'] = self.channel\n        if self.username:\n            cmd['username'] = self.username\n        if self.icon_emoji:\n            cmd['icon_emoji'] = self.icon_emoji\n        if self.link_names:\n            cmd['link_names'] = 1\n        if self.attachments:\n            cmd['attachments'] = self.attachments\n\n  ",
        "rewrite": ""
    },
    {
        "original": "\n    if not obj:\n        return None\n    d = {}\n    for c in obj.__table__.columns:\n        value = getattr(obj, c.name)\n        if type(value) == datetime:\n            value = value.isoformat()\n        d[c.name] = value\n    return d",
        "rewrite": ""
    },
    {
        "original": "\n  with tf.compat.v1.name_scope(name, 'nelder_mead_one_step'):\n    domain_dtype = current_simplex.dtype.base_dtype\n    order = tf.argsort(\n        current_objective_values, direction='ASCENDING', stable=True)\n    (\n        best_index,\n        worst_index,\n        second_worst_index\n    ) = order[0], order[-1], order[-2]\n\n    worst_vertex = current_simplex[worst_index]\n\n    (\n        best_objective_value,\n        worst_objective_value,\n        second_worst_objective_value\n    ) = (\n        current_objective_values[best_index],\n        current_objective_values[worst_index],\n        current_objective_values[second_worst_index]\n ",
        "rewrite": ""
    }
]