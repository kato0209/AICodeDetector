[
    {
        "original": "def vm2json(vm):\n    \"\"\"\n    Get a JSON representation of a DEX file\n\n    :param vm: :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`\n    :return:\n    \"\"\"\n    d = {\"name\": \"root\", \"children\": []}\n\n    for _class in vm.get_classes():\n        c_class = {\"name\": _class.get_name(), \"children\": []}\n\n        for method in _class.get_methods():\n            c_method = {\"name\": method.get_name(), \"children\": []}\n\n            c_class[\"children\"].append(c_method)\n\n        d[\"children\"].append(c_class)\n\n    return json.dumps(d)",
        "rewrite": " all_classes = []\n\n    for _class in vm.get_classes():\n        c_class = {\"name\": _class.get_name(), \"methods\": []}\n\n        for method in _class.get_methods():\n            c_method = {\"name\": method.get_name(), \"code\": method.get_code()}\n\n            c_class[\"methods\"].append(c_method)\n\n        all_classes.append(c_class)\n\n    d = {\"name\": \"root\", \"children\": all_classes}\n\n    return json.dumps(d)assistant\n\nAlternatively, you could use a recursive function to build the JSON structure:\n\n```python\ndef vm2json(vm):\n    all_classes ="
    },
    {
        "original": "def _split_arrs(array_2d, slices):\n    \"\"\"\n    Equivalent to numpy.split(array_2d, slices),\n    but avoids fancy indexing\n    \"\"\"\n    if len(array_2d) == 0:\n        return np.empty(0, dtype=np.object)\n\n    rtn = np.empty(len(slices) + 1, dtype=np.object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn",
        "rewrite": "```python\nimport numpy as np\n\ndef _split_arrs(array_2d, slices):\n    if len(array_2d) == 0:\n        return np.empty(0, dtype=np.object)\n\n    rtn = np.empty(len(slices) + 1, dtype=np.object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn\n```assistant\n\n```python\nimport numpy as np\n\ndef _split_arrs"
    },
    {
        "original": "def storeSenderKey(self, senderKeyName, senderKeyRecord):\n        \"\"\"\n        :type senderKeyName: SenderKeName\n        :type senderKeyRecord: SenderKeyRecord\n        \"\"\"\n        q = \"INSERT INTO sender_keys (group_id, sender_id, record) VALUES(?,?, ?)\"\n        cursor = self.dbConn.cursor()\n        serialized = senderKeyRecord.serialize()\n        if sys.version_info < (2,7):\n            serialized = buffer(serialized)\n        try:\n            cursor.execute(q, (senderKeyName.getGroupId(), senderKeyName.getSender().getName(), serialized))\n            self.dbConn.commit()\n        except sqlite3.IntegrityError as e:\n            q = \"UPDATE sender_keys set record = ? WHERE group_id = ? and sender_id = ?\"\n            cursor = self.dbConn.cursor()\n            cursor.execute(q, (serialized, senderKeyName.getGroupId(), senderKeyName.getSender().getName()))\n            self.dbConn.commit()",
        "rewrite": "```python\ndef storeSenderKey(self, senderKeyName, senderKeyRecord):\n    q = \"INSERT INTO sender_keys (group_id, sender_id, record) VALUES(?,?,?)\"\n    cursor = self.dbConn.cursor()\n    serialized = senderKeyRecord.serialize()\n    if sys.version_info < (2,7):\n        serialized = buffer(serialized)\n    try:\n        cursor.execute(q, (senderKeyName.getGroupId(), senderKeyName.getSender().getName(), serialized))\n        self.dbConn.commit()\n    except sqlite3.IntegrityError:\n        q = \"UPDATE sender_keys set record =? WHERE group_id =? and sender_id"
    },
    {
        "original": "def Refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\n\n    Raises:\n      google.auth.exceptions.RefreshError: If the refresh fails.\n    \"\"\"\n    with requests.Session() as session:\n      session.proxies = self.proxy_config.proxies\n      session.verify = not self.proxy_config.disable_certificate_validation\n      session.cert = self.proxy_config.cafile\n\n      self.creds.refresh(\n          google.auth.transport.requests.Request(session=session))",
        "rewrite": "assistant\n\n```python\nimport requests\nfrom google.auth import credentials as creds\n\ndef refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\"\"\"\n    with requests.Session() as session:\n        session.proxies = self.proxy_config.proxies\n        session.verify = not self.proxy_config.disable_certificate_validation\n        session.cert = self.proxy_config.cafile\n\n        self.creds.refresh(\n            creds.transport.requests.Request(session=session))\n```assistant\n\n```python\nimport requests\nfrom google.auth import credentials as creds\n\ndef refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new"
    },
    {
        "original": "def arg_locs(self, is_fp=None, sizes=None):\n        \"\"\"\n        Pass this a list of whether each parameter is floating-point or not, and get back a list of\n        SimFunctionArguments. Optionally, pass a list of argument sizes (in bytes) as well.\n\n        If you've customized this CC, this will sanity-check the provided locations with the given list.\n        \"\"\"\n        session = self.arg_session\n        if self.func_ty is None:\n            # No function prototype is provided. `is_fp` must be provided.\n            if is_fp is None:\n                raise ValueError('\"is_fp\" must be provided when no function prototype is available.')\n        else:\n            # let's rely on the func_ty for the number of arguments and whether each argument is FP or not\n            is_fp = [ True if isinstance(arg, (SimTypeFloat, SimTypeDouble)) else False for arg in self.func_ty.args ]\n\n        if sizes is None: sizes = [self.arch.bytes] * len(is_fp)\n        return [session.next_arg(ifp, size=sz) for ifp, sz in zip(is_fp, sizes)]",
        "rewrite": "```python\ndef arg_locs(self, is_fp=None, sizes=None):\n    \"\"\"\n    Pass this a list of whether each parameter is floating-point or not, and get back a list of\n    SimFunctionArguments. Optionally, pass a list of argument sizes (in bytes) as well.\n\n    If you've customized this CC, this will sanity-check the provided locations with the given list.\n    \"\"\"\n    session = self.arg_session\n    if self.func_ty is None:\n        # No function prototype is provided. `is_fp` must be provided.\n        if is_fp is None:\n            raise ValueError('\"is_fp\" must be"
    },
    {
        "original": "def get_or_add_image(self, image_descriptor):\n        \"\"\"Return (rId, image) pair for image identified by *image_descriptor*.\n\n        *rId* is the str key (often like \"rId7\") for the relationship between this story\n        part and the image part, reused if already present, newly created if not.\n        *image* is an |Image| instance providing access to the properties of the image,\n        such as dimensions and image type.\n        \"\"\"\n        image_part = self._package.get_or_add_image_part(image_descriptor)\n        rId = self.relate_to(image_part, RT.IMAGE)\n        return rId, image_part.image",
        "rewrite": "assistant\n\n```python\ndef get_or_add_image(self, image_descriptor):\n    image_part = self._package.get_or_add_image_part(image_descriptor)\n    rId = self.relate_to(image_part, RT.IMAGE)\n    return rId, image_part.image\n```assistant\n\n```python\ndef get_or_add_image(self, image_descriptor):\n    image_part = self._package.get_or_add_image_part(image_descriptor)\n    rId = self.relate_to(image_part, RT.IMAGE)\n    return rId, image_part.image\n```assistant\n\n```python\ndef get_or_add_image(self,"
    },
    {
        "original": "def get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    exec_dir = os.path.dirname(os.path.realpath(__file__))\n    xml_path = os.path.join(exec_dir, 'data', 'FacebookLocales.xml')\n\n    fb_locales = _build_locale_table(xml_path)\n\n    def default_locale(request):\n        ",
        "rewrite": "assistant\n\n```python\nimport os\nfrom functools import lru_cache\n\ndef get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    @lru_cache(maxsize=None)\n    def _build_locale_table(xml_path):\n        # Assuming the locale table is built from the XML file\n        # This is a placeholder, you should implement the actual logic\n        return {}\n\n    @lru_cache(maxsize=None)\n    def _get_fb_locales():\n        exec_dir = os.path.dirname(os.path.realpath(__file__))\n        xml_path = os.path.join(exec_dir"
    },
    {
        "original": "def _pack(cls, tensors):\n    \"\"\"Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\"\"\"\n    if not tensors:\n      return None\n    elif len(tensors) == 1:\n      return array_ops.reshape(tensors[0], [-1])\n    else:\n      flattened = [array_ops.reshape(tensor, [-1]) for tensor in tensors]\n      return array_ops.concat(flattened, 0)",
        "rewrite": "assistant\n\n```python\nimport tensorflow as tf\n\ndef _pack(cls, tensors):\n    if not tensors:\n        return None\n    elif len(tensors) == 1:\n        return tf.reshape(tensors[0], [-1])\n    else:\n        flattened = [tf.reshape(tensor, [-1]) for tensor in tensors]\n        return tf.concat(flattened, axis=0)\n```assistant\n\n```python\nimport numpy as np\n\ndef _pack(cls, tensors):\n    if not tensors:\n        return None\n    elif len(tensors) == 1:\n        return np.ravel(tensors["
    },
    {
        "original": "def close(self):\n        \"\"\"Close method.\n\n        Closes the underlying comm.\n        When the comm is closed, all of the widget views are automatically\n        removed from the front-end.\"\"\"\n        if self.comm is not None:\n            Widget.widgets.pop(self.model_id, None)\n            self.comm.close()\n            self.comm = None\n            self._ipython_display_ = None",
        "rewrite": "assistant\n\n```python\ndef close(self):\n    \"\"\"Close method.\"\"\"\n    if self.comm is not None:\n        Widget.widgets.pop(self.model_id, None)\n        self.comm.close()\n        self.comm = None\n        self._ipython_display_ = None\n```assistant\n\n```python\ndef close(self):\n    if self.comm is not None:\n        Widget.widgets.pop(self.model_id, None)\n        self._ipython_display_ = None\n        self.comm.close()\n        self.comm = None\n```assistant\n\n```python\ndef close(self):\n    if self.comm is not None"
    },
    {
        "original": "def cartesian_product(arrays, flat=True, copy=False):\n    \"\"\"\n    Efficient cartesian product of a list of 1D arrays returning the\n    expanded array views for each dimensions. By default arrays are\n    flattened, which may be controlled with the flat flag. The array\n    views can be turned into regular arrays with the copy flag.\n    \"\"\"\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(arr.flatten() if copy else arr.flat for arr in arrays)\n    return tuple(arr.copy() if copy else arr for arr in arrays)",
        "rewrite": "```python\nimport numpy as np\n\ndef cartesian_product(arrays, flat=True, copy=False):\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(arr.flatten() if copy else arr.flat for arr in arrays)\n    return tuple(arr.copy() if copy else arr for arr in arrays)\n```assistant\n\n```python\nimport numpy as np\n\ndef cartesian_product(arrays, flat=True, copy=False):\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(np.atleast_1d(arr).flatten() if copy else np"
    },
    {
        "original": "def _get_cluster_sizes(self):\n        \"\"\"\n        Returns the marker size (in points, e.g. area of the circle) based on\n        the scores, using the prop_to_size scaling mechanism.\n        \"\"\"\n        # NOTE: log and power are hardcoded, should we allow the user to specify?\n        return prop_to_size(\n            self.scores_, mi=self.min_size, ma=self.max_size, log=False, power=0.5\n        )",
        "rewrite": "def _get_cluster_sizes(self):\n    return prop_to_size(\n        self.scores_, mi=self.min_size, ma=self.max_size, log=False, power=0.5\n    )assistant\n\nIf you want to allow the user to specify the parameters, you could modify the function like this:\n\n```python\ndef _get_cluster_sizes(self, log=False, power=0.5):\n    return prop_to_size(\n        self.scores_, mi=self.min_size, ma=self.max_size, log=log, power=power\n    )\n```assistant\n\nThis version of the function takes two optional parameters"
    },
    {
        "original": "def fix_e262(self, result):\n        \"\"\"Fix spacing after comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column']\n\n        code = target[:offset].rstrip(' \\t#')\n        comment = target[offset:].lstrip(' \\t#')\n\n        fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed",
        "rewrite": "```python\ndef fix_e262(self, result):\n    \"\"\"Fix spacing after comment hash.\"\"\"\n    target = self.source[result['line'] - 1]\n    offset = result['column']\n\n    code = target[:offset].rstrip('#')\n    comment = target[offset:].lstrip('#')\n\n    fixed = code + ('  #'+ comment if comment.strip() else '\\n')\n\n    self.source[result['line'] - 1] = fixed\n```assistant\n\n```python\ndef fix_e262(self, result):\n    \"\"\"Fix spacing after comment hash.\"\"\"\n    target = self.source[result['line'] -"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'language') and self.language is not None:\n            _dict['language'] = self.language\n        if hasattr(self, 'analyzed_text') and self.analyzed_text is not None:\n            _dict['analyzed_text'] = self.analyzed_text\n        if hasattr(self, 'retrieved_url') and self.retrieved_url is not None:\n            _dict['retrieved_url'] = self.retrieved_url\n        if hasattr(self, 'usage') and self.usage is not None:\n            _dict['usage'] = self.usage._to_dict()\n        if hasattr(self, 'concepts') and self.concepts is not None:\n            _dict['concepts'] = [x._to_dict() for x in self.concepts]\n        if hasattr(self, 'entities') and self.entities is not None:\n            _dict['entities'] = [x._to_dict() for x in self.entities]\n        if hasattr(self, 'keywords') and self.keywords is not None:\n            _dict['keywords'] = [x._to_dict() for x in self.keywords]\n        if hasattr(self, 'categories') and self.categories is not None:\n            _dict['categories'] = [x._to_dict() for x in self.categories]\n        if hasattr(self, 'emotion') and self.emotion is not None:\n            _dict['emotion'] = self.emotion._to_dict()\n        if hasattr(self, 'metadata') and self.metadata is not None:\n            _dict['metadata'] = self.metadata._to_dict()\n        if hasattr(self, 'relations') and self.relations is not None:\n            _dict['relations'] = [x._to_dict() for x in self.relations]\n        if hasattr(self, 'semantic_roles') and self.semantic_roles is not None:\n            _dict['semantic_roles'] = [\n                x._to_dict() for x in self.semantic_roles\n            ]\n        if hasattr(self, 'sentiment') and self.sentiment is not None:\n            _dict['sentiment'] = self.sentiment._to_dict()\n        if hasattr(self, 'syntax') and self.syntax is not None:\n            _dict['syntax'] = self.syntax._to_dict()\n        return _dict",
        "rewrite": "            _dict['syntax'] = self.syntax._to_dict()\n        return _dict\"\n\nThe code is a method of a class, and it is used to convert the object into a dictionary. The code is quite long and repetitive.assistant\n\nHere's a revised version of the code that reduces repetition and makes it more concise:\n\n```python\ndef _to_dict(self):\n    \"\"\"Return a json dictionary representing this model.\"\"\"\n    _dict = {}\n    for attr_name in dir(self):\n        attr = getattr(self, attr_name)\n        if attr is not None and not attr_name.startswith('_'):\n            if isinstance(attr,"
    },
    {
        "original": "def validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    \"\"\"\n    Ensure that the field's name does not shadow an existing attribute of the model.\n    \"\"\"\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )",
        "rewrite": "from typing import List, Type, TypeVar\n\nT = TypeVar('T')\n\ndef validate_field_name(bases: List[T], field_name: str) -> None:\n    for base in bases:\n        if hasattr(base, field_name):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )assistant\n\nI've revised the code to use `hasattr` instead of `getattr` to check if the attribute exists. This is because `getattr` returns the attribute's value"
    },
    {
        "original": "def _get_col_items(mapping):\n    \"\"\"Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items",
        "rewrite": "```python\ndef _get_col_items(mapping):\n    from.variable import IndexVariable\n\n    col_items = []\n    for key, value in mapping.items():\n        col_items.append(key)\n        var = getattr(value, 'variable', value)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items.extend(level_names)\n    return col_items\n```assistant\n\n```python\ndef _get_col_items(mapping):\n    from.variable import IndexVariable\n\n    col_items = []\n    for key, value in mapping.items():"
    },
    {
        "original": "def get_nb_strings(self):\n        \"\"\"\n        Return the total number of strings in all Analysis objects\n        \"\"\"\n        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb",
        "rewrite": "assistant\n\n```python\ndef get_nb_strings(self):\n    \"\"\"\n    Return the total number of strings in all Analysis objects\n    \"\"\"\n    nb = 0\n    seen = set()\n    for digest, dx in self.analyzed_vms.items():\n        if dx in seen:\n            continue\n        seen.add(dx)\n        nb += len(dx.get_strings_analysis())\n    return nb\n```assistant\n\n```python\ndef get_nb_strings(self):\n    \"\"\"\n    Return the total number of strings in all Analysis objects\n    \"\"\"\n    return sum(len(dx.get_strings_analysis()) for dx in self.analyzed_vms"
    },
    {
        "original": "def read_cs_g0_contribution(self):\n        \"\"\"\n            Parse the  G0 contribution of NMR chemical shielding.\n\n            Returns:\n            G0 contribution matrix as list of list.\n        \"\"\"\n        header_pattern = r'^\\s+G\\=0 CONTRIBUTION TO CHEMICAL SHIFT \\(field along BDIR\\)\\s+$\\n' \\\n                         r'^\\s+-{50,}$\\n' \\\n                         r'^\\s+BDIR\\s+X\\s+Y\\s+Z\\s*$\\n' \\\n                         r'^\\s+-{50,}\\s*$\\n'\n        row_pattern = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 3)\n        footer_pattern = r'\\s+-{50,}\\s*$'\n        self.read_table_pattern(header_pattern, row_pattern, footer_pattern, postprocess=float,\n                                last_one_only=True, attribute_name=\"cs_g0_contribution\")",
        "rewrite": "```python\ndef read_cs_g0_contribution(self):\n    \"\"\"\n    Parse the  G0 contribution of NMR chemical shielding.\n\n    Returns:\n    G0 contribution matrix as list of list.\n    \"\"\"\n    header_pattern = r'^\\s+G\\=0 CONTRIBUTION TO CHEMICAL SHIFT \\(field along BDIR\\)\\s+\\n' \\\n                     r'^\\s+-{50,}\\n' \\\n                     r'^\\s+BDIR\\s+X\\s+Y\\s+Z\\s*\\n' \\\n                     r'^\\s+-{50,}\\s*\\n'\n    row_pattern = r"
    },
    {
        "original": "def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        assert isinstance(label, (github.Label.Label, str, unicode)), label\n        if isinstance(label, github.Label.Label):\n            label = label._identity\n        else:\n            label = urllib.quote(label)\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.issue_url + \"/labels/\" + label\n        )",
        "rewrite": "assistant\n\n```python\ndef remove_from_labels(self, label):\n    \"\"\"\n    :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n    :param label: :class:`github.Label.Label` or string\n    :rtype: None\n    \"\"\"\n    assert isinstance(label, (github.Label.Label, str)), label\n    if isinstance(label, github.Label.Label):\n        label = label._identity\n    else:\n        label = urllib.parse.quote(label)\n    headers, data = self._requester.requestJsonAndCheck("
    },
    {
        "original": "def GetValueRepresentation(cls, value,\n                             version=sorted(_SERVICE_MAP.keys())[-1]):\n    \"\"\"Converts a single python value to its PQL representation.\n\n    Args:\n      value: A python value.\n      version: A string identifying the Ad Manager version the value object\n          is compatible with. This defaults to what is currently the latest\n          version. This will be updated in future releases to point to what is\n          then the latest version.\n\n    Returns:\n      The value formatted for PQL statements which are compatible with a\n      particular API version.\n    \"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      return {'value': value, 'xsi_type': 'TextValue'}\n    elif isinstance(value, bool):\n      return {'value': value, 'xsi_type': 'BooleanValue'}\n    elif isinstance(value, numbers.Number):\n      return {'value': value, 'xsi_type': 'NumberValue'}\n    # It's important that datetime is checked for before date\n    # because isinstance(datetime.datetime.now(), datetime.date) is True\n    elif isinstance(value, datetime.datetime):\n      if value.tzinfo is None:\n        raise googleads.errors.GoogleAdsValueError(\n            'Datetime %s is not timezone aware.' % value\n        )\n\n      return {\n          'xsi_type': 'DateTimeValue',\n          'value': {\n              'date': {\n                  'year': value.year,\n                  'month': value.month,\n                  'day': value.day,\n              },\n              'hour': value.hour,\n              'minute': value.minute,\n              'second': value.second,\n              'timeZoneId' if version >= 'v201811' else 'timeZoneID':\n                  value.tzinfo.zone,\n          }\n      }\n    elif isinstance(value, datetime.date):\n      return {\n          'xsi_type': 'DateValue',\n          'value': {\n              'year': value.year,\n              'month': value.month,\n              'day': value.day,\n          }\n      }\n    elif isinstance(value, list):\n      if value and not all(isinstance(x, type(value[0])) for x in value):\n        raise googleads.errors.GoogleAdsValueError('Cannot pass more than one '\n                                                   'type in a set.')\n\n      return {\n          'xsi_type': 'SetValue',\n          'values': [cls.GetValueRepresentation(v, version) for v in value]\n      }\n    else:\n      raise googleads.errors.GoogleAdsValueError(\n          'Can\\'t represent unknown type: %s.' % type(value))",
        "rewrite": "si_type': 'SetValue',\n          'value': {\n              'list': [\n                  GetValueRepresentation(cls, x, version) for x in value\n              ]\n          }\n      }\n    elif isinstance(value, dict):\n      return {\n          'xsi_type': 'MapValue',\n          'value': {\n              'key': GetValueRepresentation(cls, value.keys()[0], version),\n              'value': GetValueRepresentation(cls, value.values()[0], version)\n          }\n      }\n    elif isinstance(value, set):\n      return {\n          'xsi_type': 'SetValue',\n          'value': {\n              'list': [\n                  GetValueRepresentation(cls"
    },
    {
        "original": "def CheckTemplates(self, base_dir, version):\n    \"\"\"Verify we have at least one template that matches maj.minor version.\"\"\"\n    major_minor = \".\".join(version.split(\".\")[0:2])\n    templates = glob.glob(\n        os.path.join(base_dir, \"templates/*%s*.zip\" % major_minor))\n    required_templates = set(\n        [x.replace(\"maj.minor\", major_minor) for x in self.REQUIRED_TEMPLATES])\n\n    # Client templates have an extra version digit, e.g. 3.1.0.0\n    templates_present = set([\n        re.sub(r\"_%s[^_]+_\" % major_minor, \"_%s_\" % major_minor,\n               os.path.basename(x)) for x in templates\n    ])\n\n    difference = required_templates - templates_present\n    if difference:\n      raise RuntimeError(\"Missing templates %s\" % difference)",
        "rewrite": "```python\ndef CheckTemplates(self, base_dir, version):\n    \"\"\"Verify we have at least one template that matches maj.minor version.\"\"\"\n    major_minor = \".\".join(version.split(\".\")[0:2])\n    templates = glob.glob(os.path.join(base_dir, \"templates/*%s*.zip\" % major_minor))\n    required_templates = set([x.replace(\"maj.minor\", major_minor) for x in self.REQUIRED_TEMPLATES])\n\n    # Client templates have an extra version digit, e.g. 3.1.0.0\n    templates_present = set([re.sub(r\"_%s[^"
    },
    {
        "original": "def build(format='qcow2', path='/tmp/'):\n    \"\"\"\n    Build an image from a current system description.\n    The image is a system image can be output in bootable ISO or QCOW2 formats.\n\n    Node uses the image building library Kiwi to perform the actual build.\n\n    Parameters:\n\n    * **format**: Specifies output format: \"qcow2\" or \"iso. Default: `qcow2`.\n    * **path**: Specifies output path where to store built image. Default: `/tmp`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion inspector.build\n        salt myminion inspector.build format=iso path=/opt/builds/\n    \"\"\"\n    try:\n        _(\"collector\").Inspector(cachedir=__opts__['cachedir'],\n                                 piddir=os.path.dirname(__opts__['pidfile']),\n                                 pidfilename='').reuse_snapshot().build(format=format, path=path)\n    except InspectorKiwiProcessorException as ex:\n        raise CommandExecutionError(ex)\n    except Exception as ex:\n        log.error(_get_error_message(ex))\n        raise Exception(ex)",
        "rewrite": "```python\ndef build(format='qcow2', path='/tmp/'):\n    try:\n        _(\"collector\").Inspector(cachedir=__opts__['cachedir'],\n                                 piddir=os.path.dirname(__opts__['pidfile']),\n                                 pidfilename='').reuse_snapshot().build(format=format, path=path)\n    except InspectorKiwiProcessorException as ex:\n        raise CommandExecutionError(ex)\n    except Exception as ex:\n        log.error(_get_error_message(ex))\n        raise Exception(ex)\n```assistant\n\n```python\ndef build(format='qcow2', path='/tmp/'):\n    try:\n        inspector ="
    },
    {
        "original": "def execute_query(**kwargs):\n    \"\"\"\n      Executes a query against the connected db using pymapd\n      https://pymapd.readthedocs.io/en/latest/usage.html#querying\n\n      Kwargs:\n        query_name(str): Name of query\n        query_mapdql(str): Query to run\n        iteration(int): Iteration number\n\n      Returns:\n        query_execution(dict):::\n          result_count(int): Number of results returned\n          execution_time(float): Time (in ms) that pymapd reports\n                                 backend spent on query.\n          connect_time(float): Time (in ms) for overhead of query, calculated\n                               by subtracting backend execution time\n                               from time spent on the execution function.\n          results_iter_time(float): Time (in ms) it took to for\n                                    pymapd.fetchone() to iterate through all\n                                    of the results.\n          total_time(float): Time (in ms) from adding all above times.\n        False(bool): The query failed. Exception should be logged.\n    \"\"\"\n    start_time = timeit.default_timer()\n    try:\n        # Run the query\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(\n            \"Completed iteration \"\n            + str(kwargs[\"iteration\"])\n            + \" of query \"\n            + kwargs[\"query_name\"]\n        )\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error):\n        logging.exception(\n            \"Error running query \"\n            + kwargs[\"query_name\"]\n            + \" during iteration \"\n            + str(kwargs[\"iteration\"])\n        )\n        return False\n\n    # Calculate times\n    query_elapsed_time = (timeit.default_timer() - start_time) * 1000\n    execution_time = query_result._result.execution_time_ms\n    connect_time = round((query_elapsed_time - execution_time), 1)\n\n    # Iterate through each result from the query\n    logging.debug(\n        \"Counting results from query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n    )\n    result_count = 0\n    start_time = timeit.default_timer()\n    while query_result.fetchone():\n        result_count += 1\n    results_iter_time = round(\n        ((timeit.default_timer() - start_time) * 1000), 1\n    )\n\n    query_execution = {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": execution_time + connect_time + results_iter_time,\n    }\n    logging.debug(\n        \"Execution results for query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n        + \": \"\n        + str(query_execution)\n    )\n    return query_execution",
        "rewrite": " - start_time) * 1000), 1\n    )\n\n    # Calculate total time\n    total_time = round(\n        (query_elapsed_time + execution_time + connect_time + results_iter_time),\n        1\n    )\n\n    # Return the results\n    return {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": total_time,\n    }\n\"\nThe code you are given is a function that executes a query against a connected database using pymapd. The function takes keyword arguments"
    },
    {
        "original": "def clitable_to_dict(cli_table):\n    \"\"\"Converts TextFSM cli_table object to list of dictionaries.\"\"\"\n    objs = []\n    for row in cli_table:\n        temp_dict = {}\n        for index, element in enumerate(row):\n            temp_dict[cli_table.header[index].lower()] = element\n        objs.append(temp_dict)\n    return objs",
        "rewrite": "assistant\n\n```python\ndef cli_table_to_dict(cli_table):\n    return [dict(zip(map(lambda x: x.lower(), cli_table.header), row)) for row in cli_table]\n```assistant\n\n```python\ndef cli_table_to_dict(cli_table):\n    return [{key.lower(): value for key, value in zip(cli_table.header, row)} for row in cli_table]\n```assistant\n\n```python\ndef cli_table_to_dict(cli_table):\n    return [{k.lower(): v for k, v in zip(cli_table.header, row)} for row in cli_table]\n```assistant"
    },
    {
        "original": "def _build_likelihood(self):\n        \"\"\"\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood. For a derivation of the terms in here, see the associated\n        SGPR notebook.\n        \"\"\"\n\n        num_inducing = len(self.feature)\n        num_data = tf.cast(tf.shape(self.Y)[0], settings.float_type)\n        output_dim = tf.cast(tf.shape(self.Y)[1], settings.float_type)\n\n        err = self.Y - self.mean_function(self.X)\n        Kdiag = self.kern.Kdiag(self.X)\n        Kuf = features.Kuf(self.feature, self.kern, self.X)\n        Kuu = features.Kuu(self.feature, self.kern, jitter=settings.numerics.jitter_level)\n        L = tf.cholesky(Kuu)\n        sigma = tf.sqrt(self.likelihood.variance)\n\n        # Compute intermediate matrices\n        A = tf.matrix_triangular_solve(L, Kuf, lower=True) / sigma\n        AAT = tf.matmul(A, A, transpose_b=True)\n        B = AAT + tf.eye(num_inducing, dtype=settings.float_type)\n        LB = tf.cholesky(B)\n        Aerr = tf.matmul(A, err)\n        c = tf.matrix_triangular_solve(LB, Aerr, lower=True) / sigma\n\n        # compute log marginal bound\n        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)\n        bound += tf.negative(output_dim) * tf.reduce_sum(tf.log(tf.matrix_diag_part(LB)))\n        bound -= 0.5 * num_data * output_dim * tf.log(self.likelihood.variance)\n        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance\n        bound += 0.5 * tf.reduce_sum(tf.square(c))\n        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance\n        bound += 0.5 * output_dim * tf.reduce_sum(tf.matrix_diag_part(AAT))\n\n        return bound",
        "rewrite": "```python\ndef _build_likelihood(self):\n    num_inducing = len(self.feature)\n    num_data = tf.cast(tf.shape(self.Y)[0], settings.float_type)\n    output_dim = tf.cast(tf.shape(self.Y)[1], settings.float_type)\n\n    err = self.Y - self.mean_function(self.X)\n    Kdiag = self.kern.Kdiag(self.X)\n    Kuf = features.Kuf(self.feature, self.kern, self.X)\n    Kuu = features.Kuu(self.feature, self.kern, jitter=settings.numerics.jitter_level)\n    L = tf.cholesky(Kuu)"
    },
    {
        "original": "def read_channel(self):\n        \"\"\"Generic handler that will read all the data from an SSH or telnet channel.\"\"\"\n        output = \"\"\n        self._lock_netmiko_session()\n        try:\n            output = self._read_channel()\n        finally:\n            # Always unlock the SSH channel, even on exception.\n            self._unlock_netmiko_session()\n        return output",
        "rewrite": "assistant\n\n```python\ndef read_channel(self):\n    output = \"\"\n    self._lock_netmiko_session()\n    try:\n        output = self._read_channel()\n    except Exception as e:\n        self._unlock_netmiko_session()\n        raise e\n    finally:\n        self._unlock_netmiko_session()\n    return output\n```assistant\n\n```python\ndef read_channel(self):\n    output = \"\"\n    self._lock_netmiko_session()\n    try:\n        output = self._read_channel()\n    except Exception as e:\n        self._unlock_netmiko_session()\n        raise e"
    },
    {
        "original": "def automatic_gamma_density(structure, kppa):\n        \"\"\"\n        Returns an automatic Kpoint object based on a structure and a kpoint\n        density. Uses Gamma centered meshes always. For GW.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n\n        Args:\n            structure:\n                Input structure\n            kppa:\n                Grid density\n        \"\"\"\n\n        latt = structure.lattice\n        lengths = latt.abc\n        ngrid = kppa / structure.num_sites\n\n        mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n        num_div = [int(round(mult / l)) for l in lengths]\n\n        # ensure that numDiv[i] > 0\n        num_div = [i if i > 0 else 1 for i in num_div]\n\n        # VASP documentation recommends to use even grids for n <= 8 and odd\n        # grids for n > 8.\n        num_div = [i + i % 2 if i <= 8 else i - i % 2 + 1 for i in num_div]\n\n        style = Kpoints.supported_modes.Gamma\n\n        comment = \"pymatgen 4.7.6+ generated KPOINTS with grid density = \" + \\\n                  \"{} / atom\".format(kppa)\n        num_kpts = 0\n        return Kpoints(comment, num_kpts, style, [num_div], [0, 0, 0])",
        "rewrite": "```python\ndef automatic_gamma_density(structure, kppa):\n    latt = structure.lattice\n    lengths = latt.abc\n    ngrid = kppa / structure.num_sites\n\n    mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n    num_div = [int(round(mult / l)) for l in lengths]\n\n    num_div = [i if i > 0 else 1 for i in num_div]\n\n    num_div = [i + i % 2 if i <= 8 else i - i %"
    },
    {
        "original": "def render(template, dest, **kwargs):\n    \"\"\"Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    \"\"\"\n\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    f = open(dest, 'wb')\n    f.write(text.encode('utf-8'))\n    f.close()",
        "rewrite": "assistant\n\n```python\ndef render(template, dest, **kwargs):\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    with open(dest, 'wb') as f:\n        f.write(text.encode('utf-8'))\n```\n\nNote: The `with` statement is used to ensure the file is properly closed after it is no longer needed.assistant\n\n```python\ndef render(template, dest, **kwargs):\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    with open(dest, 'w', encoding='utf-8') as f:"
    },
    {
        "original": "def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n        @returns: list of builder names to build on\n        \"\"\"\n\n        # self.builderNames is the configured list of builders\n        # available for try.  If the user supplies a list of builders,\n        # it must be restricted to the configured list.  If not, build\n        # on all of the configured builders.\n        if builderNames:\n            for b in builderNames:\n                if b not in self.builderNames:\n                    log.msg(\"%s got with builder %s\" % (self, b))\n                    log.msg(\" but that wasn't in our list: %s\"\n                            % (self.builderNames,))\n                    return []\n        else:\n            builderNames = self.builderNames\n        return builderNames",
        "rewrite": "assistant\n\n```python\ndef filterBuilderList(self, builderNames):\n    \"\"\"\n    Make sure that C{builderNames} is a subset of the configured\n    C{self.builderNames}, returning an empty list if not.  If\n    C{builderNames} is empty, use C{self.builderNames}.\n\n    @returns: list of builder names to build on\n    \"\"\"\n\n    if not builderNames:\n        return self.builderNames\n\n    filtered_names = [b for b in builderNames if b not in self.builderNames]\n    if filtered_names:\n        log.msg(\"%s got with builder %s"
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            ifrealname = i['interface_name'].split(':')[0]\n            # Convert rate in bps ( to be able to compare to interface speed)\n            bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n            bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n            # Decorate the bitrate with the configuration file thresolds\n            alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n            alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')\n            # If nothing is define in the configuration file...\n            # ... then use the interface speed (not available on all systems)\n            if alert_rx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_rx = self.get_alert(current=bps_rx,\n                                          maximum=i['speed'],\n                                          header='rx')\n            if alert_tx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_tx = self.get_alert(current=bps_tx,\n                                          maximum=i['speed'],\n                                          header='tx')\n            # then decorates\n            self.views[i[self.get_key()]]['rx']['decoration'] = alert_rx\n            self.views[i[self.get_key()]]['tx']['decoration'] = alert_tx",
        "rewrite": "```python\ndef update_views(self):\n    \"\"\"Update stats views.\"\"\"\n    super(Plugin, self).update_views()\n\n    for i in self.stats:\n        ifrealname = i['interface_name'].split(':')[0]\n        bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n        bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n        alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n        alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')"
    },
    {
        "original": "def add_pool_member(hostname, username, password, name, member):\n    \"\"\"\n    A function to connect to a bigip device and add a new member to an existing pool.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    name\n        The name of the pool to modify\n    member\n        The member to add to the pool\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': member\n        }\n        )\n\n    #is this pool member currently configured?\n    existing_pool = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n    if existing_pool['code'] == 200:\n\n        # for some reason iControl REST doesn't support listing a single pool member.\n        # the response from GET for listing a member will return 200 even if it doesn't exists.\n        # because of this we have to do some rather \"unnecessary\" searching within a pool.\n\n        #what are the current members?\n        current_members = existing_pool['content']['membersReference']['items']\n\n        #loop through them\n        exists = False\n        for current_member in current_members:\n            if current_member['name'] == member['name']:\n                exists = True\n                break\n\n        if exists:\n            ret['result'] = True\n            ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = {}\n        else:\n            new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)\n\n            if new_member['code'] == 200:\n                ret['result'] = True\n                ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])\n                ret['changes']['old'] = {}\n\n                #look up the member again...\n                pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n                if pool_listing['code'] != 200:\n                    ret = _load_result(new_member, ret)\n                    return ret\n\n                members = pool_listing['content']['membersReference']['items']\n                #loop through them\n                for current_member in members:\n                    if current_member['name'] == member['name']:\n                        added_member = current_member\n                        break\n\n                ret['changes']['new'] = added_member\n\n            # member wasn't added\n            else:\n                ret = _load_result(new_member, ret)\n\n    #pool does not exists\n    elif existing_pool['code'] == 404:\n        ret['comment'] = 'A pool with this name was not found.'\n    else:\n        ret = _load_result(existing_pool, ret)\n\n    return ret",
        "rewrite": "['changes']['old'] = {}\n                ret['changes']['new'] = new_member['content']\n            else:\n                ret['result'] = False\n                ret['comment'] = 'Failed to add member: {name} to the pool.'.format(name=member['name'])\n                ret['changes']['old'] = {}\n                ret['changes']['new'] = {}\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to retrieve pool information.  Please check the hostname, username, and password.'\n        ret['changes']['old'] = {}\n        ret['changes']['new'] = {}"
    },
    {
        "original": "def _hook(self, hook_name, doc_uri=None, **kwargs):\n        \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n        doc = self.workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=self.workspace, document=doc, **kwargs)",
        "rewrite": "assistant\n\n```python\ndef _hook(self, hook_name, doc_uri=None, **kwargs):\n    \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n    doc = self.workspace.get_document(doc_uri) if doc_uri else None\n    hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n    return [handler(config=self.config, workspace=self.workspace, document=doc, **kwargs) for handler in hook_handlers]\n```assistant\n\n```python\ndef _hook(self, hook_name, doc_uri=None, **kwargs):\n    \"\"\"Calls"
    },
    {
        "original": "def _validate_disallowed(self, disallowed, field, value):\n        \"\"\" Readonly but with a custom error.\n\n        The rule's arguments are validated against this schema:\n        {'type': 'boolean'}\n        \"\"\"\n        if disallowed:\n            msg = 'disallowed user provided config option'\n            self._error(field, msg)",
        "rewrite": "assistant\n\n```python\ndef _validate_disallowed(self, disallowed, field, value):\n    if not isinstance(disallowed, bool):\n        msg = 'disallowed user provided config option'\n        self._error(field, msg)\n```assistant\n\n```python\ndef _validate_disallowed(self, disallowed, field, value):\n    if not isinstance(disallowed, bool) or not isinstance(value, bool):\n        msg = 'disallowed user provided config option'\n        self._error(field, msg)\n```assistant\n\n```python\ndef _validate_disallowed(self, disallowed, field,"
    },
    {
        "original": "def updateRouterStatus(self):\n        \"\"\"force update to router as if there is child id request\"\"\"\n        print '%s call updateRouterStatus' % self.port\n        cmd = 'state'\n        while True:\n            state = self.__sendCommand(cmd)[0]\n            if state == 'detached':\n                continue\n            elif state == 'child':\n                break\n            else:\n                return False\n\n        cmd = 'state router'\n        return self.__sendCommand(cmd)[0] == 'Done'",
        "rewrite": "```python\ndef updateRouterStatus(self):\n    \"\"\"force update to router as if there is child id request\"\"\"\n    print(f\"{self.port} call updateRouterStatus\")\n    cmd ='state'\n    while True:\n        state = self.__sendCommand(cmd)[0]\n        if state == 'detached':\n            continue\n        elif state == 'child':\n            break\n        else:\n            return False\n\n    cmd ='state router'\n    return self.__sendCommand(cmd)[0] == 'Done'\n```assistant\n\n```python\ndef updateRouterStatus(self):\n    \"\"\"force update to router as if"
    },
    {
        "original": "def load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n        # TODO: support JSON\n\n    try:\n        enc = detect_by_bom(swag_path)\n        with codecs.open(swag_path, encoding=enc) as yaml_file:\n            return yaml_file.read()\n    except IOError:\n        # not in the same dir, add dirname\n        swag_path = os.path.join(\n            root_path or os.path.dirname(__file__), swag_path\n        )\n        try:\n            enc = detect_by_bom(swag_path)\n            with codecs.open(swag_path, encoding=enc) as yaml_file:\n                return yaml_file.read()\n        except IOError:  # pragma: no cover\n            # if package dir\n            # see https://github.com/rochacbruno/flasgger/pull/104\n            # Still not able to reproduce this case\n            # test are in examples/package_example\n            # need more detail on how to reproduce IOError here\n            swag_path = swag_path.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n            path = swag_path.replace(\n                (root_path or os.path.dirname(__file__)), ''\n            ).split(os.sep)[1:]\n            site_package = imp.find_module(path[0])[1]\n            swag_path = os.path.join(site_package, os.sep.join(path[1:]))\n            with open(swag_path) as yaml_file:\n                return yaml_file.read()",
        "rewrite": "assistant\n\n```python\nimport os\nimport codecs\nimport yaml\nimport imp\nimport sys\n\ndef load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n\n    try:\n        with codecs.open(swag_path, encoding='utf-8', errors='ignore') as yaml_file:\n            return yaml.safe_load(yaml_file)\n    except IOError:\n        # not in the same dir, add dirname"
    },
    {
        "original": "def share_with_invite(self, recipients, require_sign_in=True,\n                          send_email=True, message=None, share_type='view'):\n        \"\"\" Sends an invitation to access or edit this DriveItem\n\n        :param recipients: a string or Contact or a list of the former\n         representing recipients of this invitation\n        :type recipients: list[str] or list[Contact] or str or Contact\n        :param bool require_sign_in: if True the recipients\n         invited will need to log in to view the contents\n        :param bool send_email: if True an email will be send to the recipients\n        :param str message: the body text of the message emailed\n        :param str share_type: 'view': will allow to read the contents.\n         'edit' will allow to modify the contents\n        :return: link to share\n        :rtype: DriveItemPermission\n        \"\"\"\n        if not self.object_id:\n            return None\n\n        to = []\n        if recipients is None:\n            raise ValueError('Provide a valid to parameter')\n        elif isinstance(recipients, (list, tuple)):\n            for x in recipients:\n                if isinstance(x, str):\n                    to.append({'email': x})\n                elif isinstance(x, Contact):\n                    to.append({'email': x.main_email})\n                else:\n                    raise ValueError(\n                        'All the recipients must be either strings or Contacts')\n        elif isinstance(recipients, str):\n            to.append({'email': recipients})\n        elif isinstance(recipients, Contact):\n            to.append({'email': recipients.main_email})\n        else:\n            raise ValueError(\n                'All the recipients must be either strings or Contacts')\n\n        url = self.build_url(\n            self._endpoints.get('share_invite').format(id=self.object_id))\n\n        data = {\n            'recipients': to,\n            self._cc('requireSignIn'): require_sign_in,\n            self._cc('sendInvitation'): send_email,\n        }\n        if share_type in {'view', 'read'}:\n            data['roles'] = ['read']\n        elif share_type == {'edit', 'write'}:\n            data['roles'] = ['write']\n        else:\n            raise ValueError(\n                '\"{}\" is not a valid share_type'.format(share_type))\n        if send_email and message:\n            data['message'] = message\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return DriveItemPermission(parent=self, **{self._cloud_data_key: data})",
        "rewrite": ":\n            data['body'] = {'text': message}\n\n        response = self._post(url, data=data)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            return None\n        \"\"\"\n        I will revise the code with my best effort to make it more readable and maintainable.\n\n```\ndef share_with_invite(self, recipients, require_sign_in=True, send_email=True, message=None, share_type='view'):\n    \"\"\"\n    Sends an invitation to access or edit this DriveItem.\n\n    Args:\n        recipients (list[str] or list[Contact] or str or Contact): A string or"
    },
    {
        "original": "def _mod_repo_in_file(repo, repostr, filepath):\n    \"\"\"\n    Replace a repo entry in filepath with repostr\n    \"\"\"\n    with salt.utils.files.fopen(filepath) as fhandle:\n        output = []\n        for line in fhandle:\n            cols = salt.utils.args.shlex_split(\n                salt.utils.stringutils.to_unicode(line).strip()\n            )\n            if repo not in cols:\n                output.append(line)\n            else:\n                output.append(salt.utils.stringutils.to_str(repostr + '\\n'))\n    with salt.utils.files.fopen(filepath, 'w') as fhandle:\n        fhandle.writelines(output)",
        "rewrite": "```python\ndef _mod_repo_in_file(repo, repostr, filepath):\n    with salt.utils.files.fopen(filepath, 'r') as fhandle:\n        output = []\n        for line in fhandle:\n            cols = salt.utils.args.shlex_split(salt.utils.stringutils.to_unicode(line).strip())\n            if repo not in cols:\n                output.append(line)\n            else:\n                output.append(salt.utils.stringutils.to_str(repostr + '\\n'))\n    with salt.utils.files.fopen(filepath, 'w') as fhandle:\n        fhandle.writelines(output)\n```assistant\n\n```python\ndef _"
    },
    {
        "original": "def config_mode(self, config_command=\"config term\", pattern=\"\"):\n        \"\"\"\n        Enter into configuration mode on remote device.\n\n        Cisco IOS devices abbreviate the prompt at 20 chars in config mode\n        \"\"\"\n        if not pattern:\n            pattern = re.escape(self.base_prompt[:16])\n        return super(CiscoBaseConnection, self).config_mode(\n            config_command=config_command, pattern=pattern\n        )",
        "rewrite": "assistant\n\n```python\ndef config_mode(self, config_command=\"config term\", pattern=\"\"):\n    if not pattern:\n        pattern = re.escape(self.base_prompt[:20])\n    return super(CiscoBaseConnection, self).config_mode(\n        config_command=config_command, pattern=pattern\n    )\n```assistant\n\nI changed the length of the base prompt from 16 to 20 as per the comment in the function.assistant\n\n```python\ndef config_mode(self, config_command=\"config term\", pattern=\"\"):\n    if not pattern:\n        pattern = re.escape(self.base_prompt[:20])"
    },
    {
        "original": "def ParseFromUnicode(self, value):\n    \"\"\"Parse a string into a client URN.\n\n    Convert case so that all URNs are of the form C.[0-9a-f].\n\n    Args:\n      value: string value to parse\n    \"\"\"\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n      raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = match.group(\"clientid\")\n    clientid_correctcase = \"\".join((clientid[0].upper(), clientid[1:].lower()))\n\n    self._string_urn = self._string_urn.replace(clientid, clientid_correctcase,\n                                                1)",
        "rewrite": "```python\ndef ParseFromUnicode(self, value):\n    \"\"\"Parse a string into a client URN.\n\n    Convert case so that all URNs are of the form C.[0-9a-f].\n\n    Args:\n      value: string value to parse\n    \"\"\"\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n      raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = match.group(\"clientid\")"
    },
    {
        "original": "def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        \"\"\"\n        label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label",
        "rewrite": "```python\nimport scipy.io\nimport numpy as np\n\ndef load_label(self, idx):\n    label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n    label = np.zeros_like(label_400, dtype=np.uint8)\n    for idx, l in enumerate(self.labels_59):\n        idx_400 = self.labels_400.index(l) + 1\n        label[label_400 == idx_400] = idx + 1\n    label = label[np.newaxis,...]\n    return label\n```assistant\n\n```python"
    },
    {
        "original": "def endian_swap_words(source):\n    \"\"\" Endian-swap each word in 'source' bitstring \"\"\"\n    assert len(source) % 4 == 0\n    words = \"I\" * (len(source) // 4)\n    return struct.pack(\"<\" + words, *struct.unpack(\">\" + words, source))",
        "rewrite": "assistant\n\n```python\nimport struct\n\ndef endian_swap_words(source):\n    \"\"\" Endian-swap each word in'source' bitstring \"\"\"\n    assert len(source) % 4 == 0\n    words = len(source) // 4\n    return struct.pack(\"<\" + \"I\" * words, *struct.unpack(\">\" + \"I\" * words, source))\n```assistant\n\n```python\nimport struct\n\ndef endian_swap_words(source):\n    \"\"\" Endian-swap each word in'source' bitstring \"\"\"\n    assert len(source) % 4 == 0\n    return struct"
    },
    {
        "original": "def put_group_policy(group_name, policy_name, policy_json, region=None, key=None,\n                     keyid=None, profile=None):\n    \"\"\"\n    Adds or updates the specified policy document for the specified group.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.put_group_policy mygroup policyname policyrules\n    \"\"\"\n    group = get_group(group_name, region=region, key=key, keyid=keyid,\n                      profile=profile)\n    if not group:\n        log.error('Group %s does not exist', group_name)\n        return False\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        if not isinstance(policy_json, six.string_types):\n            policy_json = salt.utils.json.dumps(policy_json)\n        created = conn.put_group_policy(group_name, policy_name,\n                                        policy_json)\n        if created:\n            log.info('Created policy for IAM group %s.', group_name)\n            return True\n        log.error('Could not create policy for IAM group %s', group_name)\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        log.error('Failed to create policy for IAM group %s', group_name)\n    return False",
        "rewrite": "```python\ndef put_group_policy(group_name, policy_name, policy_json, region=None, key=None,\n                     keyid=None, profile=None):\n    \"\"\"\n    Adds or updates the specified policy document for the specified group.\n\n   .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n   .. code-block:: bash\n\n        salt myminion boto_iam.put_group_policy mygroup policyname policyrules\n    \"\"\"\n    group = get_group(group_name, region=region, key=key, keyid=keyid,\n                      profile=profile)\n    if not group:\n        log.error('Group %s does not"
    },
    {
        "original": "def bitphase_flip_operators(p):\n    \"\"\"\n    Return the bitphase flip kraus operators\n    \"\"\"\n    k0 = np.sqrt(1 - p) * I\n    k1 = np.sqrt(p) * Y\n    return k0, k1",
        "rewrite": "assistant\n\n```python\nimport numpy as np\n\ndef bitphase_flip_operators(p):\n    \"\"\"\n    Return the bitphase flip kraus operators\n    \"\"\"\n    I = np.array([[1, 0], [0, 1]])\n    Y = np.array([[0, -1j], [1j, 0]])\n    k0 = np.sqrt(1 - p) * I\n    k1 = np.sqrt(p) * Y\n    return k0, k1\n```assistant\n\n```python\nimport numpy as np\n\ndef bitphase_flip_operators(p):\n    \"\"\""
    },
    {
        "original": "def _SignedBinaryIDFromURN(binary_urn\n                          ):\n  \"\"\"Converts an AFF4 URN for a signed binary to a SignedBinaryID.\"\"\"\n  if binary_urn.RelativeName(GetAFF4PythonHackRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK,\n        path=binary_urn.RelativeName(GetAFF4PythonHackRoot()))\n  elif binary_urn.RelativeName(GetAFF4ExecutablesRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE,\n        path=binary_urn.RelativeName(GetAFF4ExecutablesRoot()))\n  else:\n    raise ValueError(\"Unable to determine type of signed binary: %s.\" %\n                     binary_urn)",
        "rewrite": "```python\ndef _SignedBinaryIDFromURN(binary_urn):\n    if binary_urn.RelativeName(GetAFF4PythonHackRoot()):\n        return rdf_objects.SignedBinaryID(\n            binary_type=rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK,\n            path=binary_urn.RelativeName(GetAFF4PythonHackRoot()))\n    elif binary_urn.RelativeName(GetAFF4ExecutablesRoot()):\n        return rdf_objects.SignedBinaryID(\n            binary_type=rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE,\n            path=binary_urn.RelativeName(GetAFF4ExecutablesRoot()))"
    },
    {
        "original": "def add_candidate_peer_endpoints(self, peer_endpoints):\n        \"\"\"Adds candidate endpoints to the list of endpoints to\n        attempt to peer with.\n\n        Args:\n            peer_endpoints ([str]): A list of public uri's which the\n                validator can attempt to peer with.\n        \"\"\"\n        with self._lock:\n            for endpoint in peer_endpoints:\n                if endpoint not in self._candidate_peer_endpoints:\n                    self._candidate_peer_endpoints.append(endpoint)",
        "rewrite": "assistant\n\n```python\ndef add_candidate_peer_endpoints(self, peer_endpoints):\n    with self._lock:\n        self._candidate_peer_endpoints.extend(peer_endpoints)\n        self._candidate_peer_endpoints = list(set(self._candidate_peer_endpoints))\n```assistant\n\n```python\ndef add_candidate_peer_endpoints(self, peer_endpoints):\n    with self._lock:\n        self._candidate_peer_endpoints.update(set(peer_endpoints) - set(self._candidate_peer_endpoints))\n```assistant\n\n```python\ndef add_candidate_peer_endpoints(self, peer_endpoints):\n    with self._"
    },
    {
        "original": "def set_tags(name=None,\n             tags=None,\n             call=None,\n             location=None,\n             instance_id=None,\n             resource_id=None,\n             kwargs=None):  # pylint: disable=W0613\n    \"\"\"\n    Set tags for a resource. Normally a VM name or instance_id is passed in,\n    but a resource_id may be passed instead. If both are passed in, the\n    instance_id will be used.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -a set_tags mymachine tag1=somestuff tag2='Other stuff'\n        salt-cloud -a set_tags resource_id=vol-3267ab32 tag=somestuff\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n\n    if location is None:\n        location = get_location()\n\n    if instance_id is None:\n        if 'resource_id' in kwargs:\n            resource_id = kwargs['resource_id']\n            del kwargs['resource_id']\n\n        if 'instance_id' in kwargs:\n            instance_id = kwargs['instance_id']\n            del kwargs['instance_id']\n\n        if resource_id is None:\n            if instance_id is None:\n                instance_id = _get_node(name=name, instance_id=None, location=location)['instanceId']\n        else:\n            instance_id = resource_id\n\n    # This second check is a safety, in case the above still failed to produce\n    # a usable ID\n    if instance_id is None:\n        return {\n            'Error': 'A valid instance_id or resource_id was not specified.'\n        }\n\n    params = {'Action': 'CreateTags',\n              'ResourceId.1': instance_id}\n\n    log.debug('Tags to set for %s: %s', name, tags)\n\n    if kwargs and not tags:\n        tags = kwargs\n\n    for idx, (tag_k, tag_v) in enumerate(six.iteritems(tags)):\n        params['Tag.{0}.Key'.format(idx)] = tag_k\n        params['Tag.{0}.Value'.format(idx)] = tag_v\n\n    attempts = 0\n    while attempts < aws.AWS_MAX_RETRIES:\n        aws.query(params,\n                           setname='tagSet',\n                           location=location,\n                           provider=get_provider(),\n                           opts=__opts__,\n                           sigver='4')\n\n        settags = get_tags(\n            instance_id=instance_id, call='action', location=location\n        )\n\n        log.debug('Setting the tags returned: %s', settags)\n\n        failed_to_set_tags = False\n        for tag in settags:\n            if tag['key'] not in tags:\n                # We were not setting this tag\n                continue\n\n            if tag.get('value') is None and tags.get(tag['key']) == '':\n                # This is a correctly set tag with no value\n                continue\n\n            if six.text_type(tags.get(tag['key'])) != six.text_type(tag['value']):\n                # Not set to the proper value!?\n                log.debug(\n                    'Setting the tag %s returned %s instead of %s',\n                    tag['key'], tags.get(tag['key']), tag['value']\n                )\n                failed_to_set_tags = True\n                break\n\n        if failed_to_set_tags:\n            log.warning('Failed to set tags. Remaining attempts %s', attempts)\n            attempts += 1\n            aws.sleep_exponential_backoff(attempts)\n            continue\n\n        return settags\n\n    raise SaltCloudSystemExit(\n        'Failed to set tags on {0}!'.format(name)\n    )",
        "rewrite": "        if aws.get_result()['CreateTagsResponse']['CreateTagsResult']['ResponseMetadata']['HTTPStatusCode'] == 200:\n            break\n        attempts += 1\n\n    if attempts >= aws.AWS_MAX_RETRIES:\n        return {\n            'Error': 'Failed to set tags after %s attempts.' % aws.AWS_MAX_RETRIES\n        }\n\n    return {'result': 'Tags set successfully'}\"\n\n    \"def get_location():\n    \"\"\"\n    Get the location from the salt-cloud configuration.\n\n    CLI Examples:\n\n   .. code-block:: bash\n\n        salt-cloud -a get_location\n    \"\"\"\n    return __opts__['location']\""
    },
    {
        "original": "def getPluginsList(self, enable=True):\n        \"\"\"Return the plugins list.\n\n        if enable is True, only return the active plugins (default)\n        if enable is False, return all the plugins\n\n        Return: list of plugin name\n        \"\"\"\n        if enable:\n            return [p for p in self._plugins if self._plugins[p].is_enable()]\n        else:\n            return [p for p in self._plugins]",
        "rewrite": "assistant\n\n```python\ndef getPluginsList(self, enable=True):\n    return [p for p in self._plugins if (enable and self._plugins[p].is_enable()) or not enable]\n```assistant\n\n```python\ndef getPluginsList(self, enable=True):\n    return [p for p in self._plugins if enable and self._plugins[p].is_enable() or not enable]\n```assistant\n\n```python\ndef getPluginsList(self, enable=True):\n    return [p for p in self._plugins if enable and self._plugins[p].is_enabled() or not enable"
    },
    {
        "original": "def annToRLE(self, ann):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        t = self.imgs[ann['image_id']]\n        h, w = t['height'], t['width']\n        segm = ann['segmentation']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm['counts']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle",
        "rewrite": "```python\ndef annToRLE(self, ann):\n    t = self.imgs[ann['image_id']]\n    h, w = t['height'], t['width']\n    segm = ann['segmentation']\n    if isinstance(segm, list):\n        rles = maskUtils.frPyObjects(segm, h, w)\n        rle = maskUtils.merge(rles)\n    elif isinstance(segm['counts'], list):\n        rle = maskUtils.frPyObjects(segm, h, w)\n    else:\n        rle = ann['segmentation']\n    return rle\n```assistant"
    },
    {
        "original": "def detect_cadvisor(kubelet_url, cadvisor_port):\n        \"\"\"\n        Tries to connect to the cadvisor endpoint, with given params\n        :return: url if OK, raises exception if NOK\n        \"\"\"\n        if cadvisor_port == 0:\n            raise ValueError(\"cAdvisor port set to 0 in configuration\")\n        kubelet_hostname = urlparse(kubelet_url).hostname\n        if not kubelet_hostname:\n            raise ValueError(\"kubelet hostname empty\")\n        url = \"http://{}:{}{}\".format(kubelet_hostname, cadvisor_port, LEGACY_CADVISOR_METRICS_PATH)\n\n        # Test the endpoint is present\n        r = requests.head(url, timeout=1)\n        r.raise_for_status()\n\n        return url",
        "rewrite": "```python\nimport requests\nfrom urllib.parse import urlparse\n\ndef detect_cadvisor(kubelet_url, cadvisor_port):\n    if cadvisor_port == 0:\n        raise ValueError(\"cAdvisor port set to 0 in configuration\")\n    kubelet_hostname = urlparse(kubelet_url).hostname\n    if not kubelet_hostname:\n        raise ValueError(\"kubelet hostname empty\")\n    url = f\"http://{kubelet_hostname}:{cadvisor_port}{LEGACY_CADVISOR_METRICS_PATH}\"\n\n    try:\n        r = requests.head(url, timeout=1)\n        r.raise_for_status()\n    except requests.RequestException as e"
    },
    {
        "original": "def imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n        \"\"\"\n        Show the image stored in X on the canvas.\n        \n        The origin of the image show is (0,0), such that X[0,0] gets plotted at [0,0] of the image!\n        \n        the kwargs are plotting library specific kwargs!\n        \"\"\"\n        raise NotImplementedError(\"Implement all plot functions in AbstractPlottingLibrary in order to use your own plotting library\")",
        "rewrite": "```python\ndef imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n    \"\"\"\n    Show the image stored in X on the canvas.\n    \n    The origin of the image show is (0,0), such that X[0,0] gets plotted at [0,0] of the image!\n    \n    the kwargs are plotting library specific kwargs!\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"X must be a numpy array\")\n    \n    if extent is None:\n        extent = [0, X.shape[1], 0, X.shape"
    },
    {
        "original": "def make_type_consistent(s1, s2):\n    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n        return s1, s2\n\n    else:\n        return unicode(s1), unicode(s2)",
        "rewrite": "assistant\n\n```python\ndef make_type_consistent(s1, s2):\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    else:\n        return s1.__class__(s1), s1.__class__(s2)\n```assistant\n\n```python\ndef make_type_consistent(s1, s2):\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2"
    }
]