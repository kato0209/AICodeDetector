[
    {
        "original": "def _RunAction(self, rule, client_id):\n    \"\"\"Run all the actions specified in the rule.\n\n    Args:\n      rule: Rule which actions are to be executed.\n      client_id: Id of a client where rule's actions are to be executed.\n\n    Returns:\n      Number of actions started.\n    \"\"\"\n    actions_count = 0\n\n    try:\n      if self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n        logging.info(\n            \"Foreman: ignoring hunt %s on client %s: was started \"\n            \"here before\", client_id, rule.hunt_id)\n      else:\n        logging.info(\"Foreman: Starting hunt %s on client %s.\", rule.hunt_id,\n                     client_id)\n\n        # hunt_name is only used for legacy hunts.\n        if rule.hunt_name:\n          flow_cls = registry.AFF4FlowRegistry.FlowClassByName(rule.hunt_name)\n          hunt_urn = rdfvalue.RDFURN(\"aff4:/hunts/%s\" % rule.hunt_id)\n          flow_cls.StartClients(hunt_urn, [client_id])\n        else:\n          hunt.StartHuntFlowOnClient(client_id, rule.hunt_id)\n\n        actions_count += 1\n\n    # There could be all kinds of errors we don't know about when starting the\n    # hunt so we catch everything here.\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(\"Failure running foreman action on client %s: %s\",\n                        rule.hunt_id, e)\n\n    return actions_count",
        "rewrite": "```python\ndef _RunAction(self, rule, client_id):\n    \"\"\"Run all the actions specified in the rule.\n\n    Args:\n      rule: Rule which actions are to be executed.\n      client_id: Id of a client where rule's actions are to be executed.\n\n    Returns:\n      Number of actions started.\n    \"\"\"\n    actions_count = 0\n\n    try:\n        if self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n            logging.info(\n                \"Foreman: ignoring hunt %s on client %s: was started \"\n                \"here before\", client_id, rule.hunt"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "rewrite": "```python\ndef list_children(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    children_urns = list(self.multi_list_children([urn], limit=limit, age=age))[0][1]\n    return children_urns\n```assistant"
    },
    {
        "original": "def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        \"\"\"\n        Checks if two points are symmetrically related.\n\n        Args:\n            point_a (3x1 array): First point.\n            point_b (3x1 array): Second point.\n            tol (float): Absolute tolerance for checking distance.\n\n        Returns:\n            True if self.operate(point_a) == point_b or vice versa.\n        \"\"\"\n        if np.allclose(self.operate(point_a), point_b, atol=tol):\n            return True\n        if np.allclose(self.operate(point_b), point_a, atol=tol):\n            return True\n        return False",
        "rewrite": "```python\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    return np.allclose(self.operate(point_a), point_b, atol=tol) or np.allclose(self.operate(point_b), point_a, atol=tol)\n```assistant\n\n```python\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    return np.allclose(self.operate(point_a), point_b, atol=tol) or np.allclose(self.operate(point_b), point_a, atol=tol)\n```assistant"
    },
    {
        "original": "def absent(name, profile=\"splunk\"):\n    \"\"\"\n    Ensure a search is absent\n\n    .. code-block:: yaml\n\n        API Error Search:\n          splunk_search.absent\n\n    The following parameters are required:\n\n    name\n        This is the name of the search in splunk\n    \"\"\"\n    ret = {\n        'name': name,\n        'changes': {},\n        'result': True,\n        'comment': '{0} is absent.'.format(name)\n    }\n\n    target = __salt__['splunk_search.get'](name, profile=profile)\n    if target:\n        if __opts__['test']:\n            ret = {}\n            ret[\"name\"] = name\n            ret['comment'] = \"Would delete {0}\".format(name)\n            ret['result'] = None\n            return ret\n\n        result = __salt__['splunk_search.delete'](name, profile=profile)\n        if result:\n            ret['comment'] = '{0} was deleted'.format(name)\n        else:\n            ret['comment'] = 'Failed to delete {0}'.format(name)\n            ret['result'] = False\n    return ret",
        "rewrite": "```python\ndef absent(name, profile=\"splunk\"):\n    \"\"\"\n    Ensure a search is absent\n\n   .. code-block:: yaml\n\n        API Error Search:\n          splunk_search.absent\n\n    The following parameters are required:\n\n    name\n        This is the name of the search in splunk\n    \"\"\"\n    ret = {\n        'name': name,\n        'changes': {},\n       'result': True,\n        'comment': '{0} is absent.'.format(name)\n    }\n\n    target = __salt__['splunk_search.get'](name, profile=profile)\n    if target:\n        if __opts__['"
    },
    {
        "original": "def _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n                log.debug('Have deserialized data %s', _existingData)\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecutionError(error)\n            if 'Section' in policy_info['ScriptIni'] and policy_info['ScriptIni']['Section'].lower() in [z.lower() for z in _existingData.keys()]:\n                if 'SettingName' in policy_info['ScriptIni']:\n                    log.debug('Need to look for %s', policy_info['ScriptIni']['SettingName'])\n                    if policy_info['ScriptIni']['SettingName'].lower() in [z.lower() for z in _existingData[policy_info['ScriptIni']['Section']].keys()]:\n                        return _existingData[policy_info['ScriptIni']['Section']][policy_info['ScriptIni']['SettingName'].lower()]\n                    else:\n                        return None\n                else:\n                    return _existingData[policy_info['ScriptIni']['Section']]\n            else:\n                return None\n\n    return None",
        "rewrite": "```python\ndef _getScriptSettingsFromIniFile(policy_info):\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecution"
    },
    {
        "original": "async def vcx_messages_update_status(msg_json: str):\n    \"\"\"\n    Update the status of messages from the specified connection\n    :param msg_json:\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,\n                           c_msg_json,\n                           vcx_messages_update_status.cb)\n\n    logger.debug(\"vcx_messages_update_status completed\")\n    return result",
        "rewrite": "assistant\n\n```python\nasync def vcx_messages_update_status(msg_json: str):\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,"
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "rewrite": "```python\ndef save(self, eopatch, use_tmp=True):\n    \"\"\" Method which does the saving\n\n    :param eopatch: EOPatch containing the data which will be saved\n    :type eopatch: EOPatch\n    :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\n    (i.e. final) location\n    :type use_tmp: bool\n    \"\"\"\n    filename = self.tmp_filename if use_tmp else self.final_filename\n\n    if self.feature_name is None:\n        data = eopatch[self.feature_type"
    },
    {
        "original": "def utc_dt_to_local_dt(dtm):\n    \"\"\"Convert a UTC datetime to datetime in local timezone\"\"\"\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo is not None and dtm.tzinfo != utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(mktz())",
        "rewrite": "```python\nfrom datetime import datetime\nimport pytz\n\ndef utc_dt_to_local_dt(dtm):\n    utc_zone = pytz.UTC\n    if dtm.tzinfo is not None and dtm.tzinfo!= utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(pytz.timezone(\"US/Pacific\"))  # Replace with your local"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "rewrite": "def differing_functions_with_consts(self):\n    different_funcs = []\n    for func_a, func_b in self.function_matches:\n        if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n            different_funcs.append((func_a, func_b))\n    return different_funcsassistant\n\nAlternatively, you could use a list comprehension for a more concise version:\n\n```python\ndef differing_functions_with_consts(self):\n    return [(func_a, func_b) for func_a, func_b in self.function_matches \n            if not self.functions_probably_identical(func_a, func_b, check_consts=True)]\n```"
    },
    {
        "original": "def vector_args(self, args):\n        \"\"\"\n         Yields each of the individual lane pairs from the arguments, in\n         order from most significan to least significant\n        \"\"\"\n        for i in reversed(range(self._vector_count)):\n            pieces = []\n            for vec in args:\n                pieces.append(vec[(i+1) * self._vector_size - 1 : i * self._vector_size])\n            yield pieces",
        "rewrite": "```python\ndef vector_args(self, args):\n    for i in reversed(range(self._vector_count)):\n        yield [vec[(i+1) * self._vector_size - 1 : i * self._vector_size] for vec in args]\n```assistant\n\n```python\ndef vector_args(self, args):\n    for i in range(self._vector_count):\n        yield [vec[i * self._vector_size : (i+1) * self._vector_size] for vec in args]\n```assistant\n\n```python\ndef vector_args(self, args):\n    for i in range(self._vector"
    },
    {
        "original": "def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks is enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with\n        peer is capable of enhanced route refresh capability.\n        \"\"\"\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled",
        "rewrite": "```python\ndef is_enhanced_rr_cap_valid(self):\n    \"\"\"Checks is enhanced route refresh capability is enabled/valid.\"\"\"\n    if not self.recv_open_msg:\n        raise ValueError('Did not yet receive peers open message.')\n\n    local_caps = self.sent_open_msg.opt_param\n    peer_caps = self.recv_open_msg.opt_param\n\n    local_cap = next((cap for cap in local_caps if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH), None)\n    peer_cap = next((cap for cap in peer_caps if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH), None)\n\n    err_cap_enabled"
    },
    {
        "original": "def _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \"\"\"\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists'\n            ret['default'] = default == ruby\n            break\n\n    return ret",
        "rewrite": "assistant\n\n```python\ndef _ruby_installed(ret, ruby, user=None):\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = f'Requested ruby {ruby} exists'\n            ret['default'] = default == ruby\n            break\n    else:\n        ret['result'] = False\n        ret['comment'] = f'Requested ruby {ruby} does not exist'\n    return ret\n```assistant"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "rewrite": "```python\ndef create(cls, tx_signers, recipients, metadata=None, asset=None):\n    \"\"\"A simple way to generate a `CREATE` transaction.\"\"\"\n    \n    inputs, outputs = cls.validate_create(tx_signers, recipients, asset, metadata)\n    return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)\n```assistant\n\n```python\ndef create(cls, tx_signers, recipients, metadata=None, asset=None):\n    \"\"\"A simple way to generate a `CREATE` transaction.\"\"\"\n    \n    inputs, outputs = cls.validate_create(tx_signers, recipients, asset, metadata)\n    return cls"
    },
    {
        "original": "def df(self):\n        \"\"\"\n        Get data usage information.\n\n        Returns:\n            (dict): A dictionary representing different resource categories\n            and their respective data usage.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/system/df')\n        return self._result(self._get(url), True)",
        "rewrite": "```python\ndef df(self):\n    \"\"\"\n    Get data usage information.\n    \"\"\"\n    url = self._url('/system/df')\n    return self._result(self._get(url), True)\n```assistant\n\n```python\ndef df(self):\n    \"\"\"\n    Get data usage information.\n    \"\"\"\n    url = self._url('/system/df')\n    return self._result(self._get(url), True)\n```assistant\n\n```python\ndef df(self):\n    \"\"\"\n    Get data usage information.\n    \"\"\"\n    url = self._url('/system/df')\n    return self._result(self._get"
    },
    {
        "original": "def predictive_variance(self, mu,variance, predictive_mean=None, Y_metadata=None):\n        \"\"\"\n        Approximation to the predictive variance: V(Y_star)\n\n        The following variance decomposition is used:\n        V(Y_star) = E( V(Y_star|f_star)**2 ) + V( E(Y_star|f_star) )**2\n\n        :param mu: mean of posterior\n        :param sigma: standard deviation of posterior\n        :predictive_mean: output's predictive mean, if None _predictive_mean function will be called.\n\n        \"\"\"\n        #sigma2 = sigma**2\n        normalizer = np.sqrt(2*np.pi*variance)\n\n        fmin_v = -np.inf\n        fmin_m = np.inf\n        fmin = -np.inf\n        fmax = np.inf\n\n        from ..util.misc import safe_exp\n        # E( V(Y_star|f_star) )\n        def int_var(f,m,v):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = safe_exp(exponent)\n            #If p is zero then conditional_variance will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_variance(f)*p\n        scaled_exp_variance = [quad(int_var, fmin_v, fmax,args=(mj,s2j))[0] for mj,s2j in zip(mu,variance)]\n        exp_var = np.array(scaled_exp_variance)[:,None] / normalizer\n\n        #V( E(Y_star|f_star) ) =  E( E(Y_star|f_star)**2 ) - E( E(Y_star|f_star) )**2\n\n        #E( E(Y_star|f_star) )**2\n        if predictive_mean is None:\n            predictive_mean = self.predictive_mean(mu,variance)\n        predictive_mean_sq = predictive_mean**2\n\n        #E( E(Y_star|f_star)**2 )\n        def int_pred_mean_sq(f,m,v,predictive_mean_sq):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = np.exp(exponent)\n            #If p is zero then conditional_mean**2 will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_mean(f)**2*p\n\n        scaled_exp_exp2 = [quad(int_pred_mean_sq, fmin_m, fmax,args=(mj,s2j,pm2j))[0] for mj,s2j,pm2j in zip(mu,variance,predictive_mean_sq)]\n        exp_exp2 = np.array(scaled_exp_exp2)[:,None] / normalizer\n\n        var_exp = exp_exp2 - predictive_mean_sq\n\n        # V(Y_star) = E[ V(Y_star|f_star) ] + V[ E(Y_star|f_star) ]\n        # V(Y_star) = E[ V(Y_star|f_star) ] + E(Y_star**2|f_star) - E[Y_star|f_star]**2\n        return exp_var + var_exp",
        "rewrite": " 1e-10:\n                return 0.\n            else:\n                return predictive_mean_sq * p\n        scaled_pred_mean_sq = [quad(int_pred_mean_sq, fmin_v, fmax, args=(mj,s2j,predictive_mean_sq))[0] for mj,s2j in zip(mu,variance)]\n        pred_mean_sq = np.array(scaled_pred_mean_sq)[:,None] / normalizer\n\n        #V( E(Y_star|f_star) ) = E( E(Y_star|f_star)**2 ) - E( E(Y_star|f_star) )**2\n        pred_var"
    },
    {
        "original": "def _get_values(cls, diff_dict, type='new'):\n        \"\"\"\n        Returns a dictionaries with the 'new' values in a diff dict.\n\n        type\n            Which values to return, 'new' or 'old'\n        \"\"\"\n        ret_dict = {}\n        for p in diff_dict.keys():\n            if type in diff_dict[p].keys():\n                ret_dict.update({p: diff_dict[p][type]})\n            else:\n                ret_dict.update(\n                    {p: cls._get_values(diff_dict[p], type=type)})\n        return ret_dict",
        "rewrite": "```python\ndef _get_values(cls, diff_dict, type='new'):\n    ret_dict = {}\n    for p in diff_dict.keys():\n        if type in diff_dict[p].keys():\n            ret_dict[p] = diff_dict[p][type]\n        else:\n            ret_dict[p] = cls._get_values(diff_dict[p], type=type)\n    return ret_dict\n```assistant\n\n```python\ndef _get_values(cls, diff_dict, type='new'):\n    ret_dict = {}\n    for key, value in diff_dict.items():\n        if type in value.keys():\n            ret_dict[key] = value"
    },
    {
        "original": "def _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    \"\"\"\n    GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.\n\n    See gti_dirint signature for parameter details.\n    \"\"\"\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370, 'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)\n    disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)\n\n    dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,\n                                          False, temp_dew)\n\n    dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)\n    cos_surface_tilt = tools.cosd(surface_tilt)\n\n    # isotropic sky plus ground diffuse\n    dhi_gte_90 = (\n        (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /\n        (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))\n\n    ghi_gte_90 = dni_gte_90_proj + dhi_gte_90\n\n    return ghi_gte_90, dni_gte_90, dhi_gte_90",
        "rewrite": "```python\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370,'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith,"
    },
    {
        "original": "def srbt1(bt_address, pkts, *args, **kargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kargs)\n    if len(a) > 0:\n        return a[0][1]",
        "rewrite": "\n\n```python\ndef srbt1(bt_address, pkts, *args, **kwargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kwargs)\n    if len(a) > 0:\n        return a[0][1]\n```assistant\n\n```python\ndef srbt1(bt_address, pkts, *args, **kwargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args,"
    },
    {
        "original": "def pd_coords(self, comp):\n        \"\"\"\n        The phase diagram is generated in a reduced dimensional space\n        (n_elements - 1). This function returns the coordinates in that space.\n        These coordinates are compatible with the stored simplex objects.\n        \"\"\"\n        if set(comp.elements).difference(self.elements):\n            raise ValueError('{} has elements not in the phase diagram {}'\n                             ''.format(comp, self.elements))\n        return np.array(\n            [comp.get_atomic_fraction(el) for el in self.elements[1:]])",
        "rewrite": "```python\nimport numpy as np\n\ndef pd_coords(self, comp):\n    if set(comp.elements).difference(self.elements):\n        raise ValueError('{} has elements not in the phase diagram {}'\n                         ''.format(comp, self.elements))\n    return np.array(\n        [comp.get_atomic_fraction(el) for el in self.elements[1:]])\n```assistant\n\n```python\nimport numpy as np\n\ndef pd_coords(self, comp):\n    if not set(comp.elements).issubset(self.elements):\n        raise ValueError('{} has elements not in the phase diagram {}'\n                         ''.format(comp, self.elements))\n    return np.array("
    },
    {
        "original": "def relative_ref(self, baseURI):\n        \"\"\"\n        Return string containing relative reference to package item from\n        *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n        return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n        \"\"\"\n        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath",
        "rewrite": "```python\nimport posixpath\n\ndef relative_ref(self, baseURI):\n    \"\"\"\n    Return string containing relative reference to package item from\n    *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n    return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n    \"\"\"\n    if baseURI == '/':\n        relpath = self[1:]\n    else:\n        relpath = posixpath.relpath(self, baseURI)\n    return relpath\n```assistant\n\n```python\nimport pathlib\n\ndef relative_ref(self, base"
    },
    {
        "original": "def _get_triplet_scores(self, triangles_list):\n        \"\"\"\n        Returns the score of each of the triplets found in the current model\n\n        Parameters\n        ---------\n        triangles_list: list\n                        The list of variables forming the triangles to be updated. It is of the form of\n                        [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]\n\n        Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'var_5', 'var_4', 'var_7'}): 10.23}\n        \"\"\"\n        triplet_scores = {}\n        for triplet in triangles_list:\n\n            # Find the intersection sets of the current triplet\n            triplet_intersections = [intersect for intersect in it.combinations(triplet, 2)]\n\n            # Independent maximization\n            ind_max = sum([np.amax(self.objective[frozenset(intersect)].values) for intersect in triplet_intersections])\n\n            # Joint maximization\n            joint_max = self.objective[frozenset(triplet_intersections[0])]\n            for intersect in triplet_intersections[1:]:\n                joint_max += self.objective[frozenset(intersect)]\n            joint_max = np.amax(joint_max.values)\n            # score = Independent maximization solution - Joint maximization solution\n            score = ind_max - joint_max\n            triplet_scores[frozenset(triplet)] = score\n\n        return triplet_scores",
        "rewrite": "```python\ndef _get_triplet_scores(self, triangles_list):\n    \"\"\"\n    Returns the score of each of the triplets found in the current model\n\n    Parameters\n    ---------\n    triangles_list: list\n                    The list of variables forming the triangles to be updated. It is of the form of\n                    [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]\n\n    Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'"
    },
    {
        "original": "def _find_packages(root):\n  \"\"\"\n  Helper for ``build_index()``: Yield a list of tuples\n  ``(pkg_xml, zf, subdir)``, where:\n    - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n    package\n    - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n    - ``subdir`` is the subdirectory (relative to ``root``) where\n    the package was found (e.g. 'corpora' or 'grammars').\n  \"\"\"\n  # Find all packages.\n  packages = []\n  for dirname, subdirs, files in os.walk(root):\n    relpath = '/'.join(_path_from(root, dirname))\n    for filename in files:\n      if filename.endswith('.xml'):\n        xmlfilename = os.path.join(dirname, filename)\n        zipfilename = xmlfilename[:-4]+'.zip'\n        try: zf = zipfile.ZipFile(zipfilename)\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (zipfilename, e))\n        try: pkg_xml = ElementTree.parse(xmlfilename).getroot()\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (xmlfilename, e))\n\n        # Check that the UID matches the filename\n        uid = os.path.split(xmlfilename[:-4])[1]\n        if pkg_xml.get('id') != uid:\n          raise ValueError('package identifier mismatch (%s '\n                   'vs %s)' % (pkg_xml.get('id'), uid))\n\n        # Check that the zipfile expands to a subdir whose\n        # name matches the uid.\n        if sum( (name!=uid and not name.startswith(uid+'/'))\n            for name in zf.namelist() ):\n          raise ValueError('Zipfile %s.zip does not expand to a '\n                   'single subdirectory %s/' % (uid, uid))\n\n        yield pkg_xml, zf, relpath\n    # Don't recurse into svn subdirectories:\n    try: subdirs.remove('.svn')\n    except ValueError: pass",
        "rewrite": "```python\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ElementTree\n\ndef _path_from(root, path):\n    \"\"\"Helper function to get the relative path from root.\"\"\"\n    return os.path.relpath(path, root)\n\ndef _find_packages(root):\n    \"\"\"\n    Helper for ``build_index()``: Yield a list of tuples\n    ``(pkg_xml, zf, subdir)``, where:\n      - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n        package\n      - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n      - ``"
    },
    {
        "original": "def _addAttr(self, txn, isCommitted=False) -> None:\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        assert get_type(txn) == ATTRIB\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.attributeStore.set(hashed_value, value)",
        "rewrite": "assistant\n\n```python\ndef _addAttr(self, txn, isCommitted=False) -> None:\n    assert get_type(txn) == ATTRIB\n    attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n    self.state.set(path, value_bytes)\n    if attr_type!= HASH:\n        self.attributeStore.set(hashed_value, value)\n```assistant\n\n```python\ndef _addAttr(self, txn, isCommitted=False) -> None:\n    assert get_type(txn) == ATTRIB\n    attr_type, path, value, hashed"
    },
    {
        "original": "def parse_osm_nodes_paths(osm_data):\n    \"\"\"\n    Construct dicts of nodes and paths with key=osmid and value=dict of\n    attributes.\n\n    Parameters\n    ----------\n    osm_data : dict\n        JSON response from from the Overpass API\n\n    Returns\n    -------\n    nodes, paths : tuple\n    \"\"\"\n\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            key = element['id']\n            nodes[key] = get_node(element)\n        elif element['type'] == 'way': #osm calls network paths 'ways'\n            key = element['id']\n            paths[key] = get_path(element)\n\n    return nodes, paths",
        "rewrite": "assistant\n\n```python\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            nodes[element['id']] = get_node(element)\n        elif element['type'] == 'way':\n            paths[element['id']] = get_path(element)\n\n    return nodes, paths\n```assistant\n\n```python\ndef get_node(element):\n    return {\n        'id': element['id'],\n        'tags': element.get('tags', {}),\n        'lat': element['lat'],\n        'lon"
    },
    {
        "original": "def ConfigureUrls(config, external_hostname = None):\n  \"\"\"Guides the user through configuration of various URLs used by GRR.\"\"\"\n  print(\"\\n\\n-=GRR URLs=-\\n\"\n        \"For GRR to work each client has to be able to communicate with the\\n\"\n        \"server. To do this we normally need a public dns name or IP address\\n\"\n        \"to communicate with. In the standard configuration this will be used\\n\"\n        \"to host both the client facing server and the admin user interface.\\n\")\n\n  existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\", default=None)\n  existing_frontend_urns = grr_config.CONFIG.Get(\"Client.server_urls\")\n  if not existing_frontend_urns:\n    # Port from older deprecated setting Client.control_urls.\n    existing_control_urns = grr_config.CONFIG.Get(\n        \"Client.control_urls\", default=None)\n    if existing_control_urns is not None:\n      existing_frontend_urns = []\n      for existing_control_urn in existing_control_urns:\n        if not existing_control_urn.endswith(\"control\"):\n          raise RuntimeError(\"Invalid existing control URL: %s\" %\n                             existing_control_urn)\n\n        existing_frontend_urns.append(\n            existing_control_urn.rsplit(\"/\", 1)[0] + \"/\")\n\n      config.Set(\"Client.server_urls\", existing_frontend_urns)\n      config.Set(\"Client.control_urls\", [\"deprecated use Client.server_urls\"])\n\n  if not existing_frontend_urns or not existing_ui_urn:\n    ConfigureHostnames(config, external_hostname=external_hostname)\n  else:\n    print(\"Found existing settings:\\n  AdminUI URL: %s\\n  \"\n          \"Frontend URL(s): %s\\n\" % (existing_ui_urn, existing_frontend_urns))\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureHostnames(config, external_hostname=external_hostname)",
        "rewrite": "```python\ndef ConfigureUrls(config, external_hostname=None):\n  \"\"\"Guides the user through configuration of various URLs used by GRR.\"\"\"\n  print(\"\\n\\n-=GRR URLs=-\\n\"\n        \"For GRR to work each client has to be able to communicate with the\\n\"\n        \"server. To do this we normally need a public dns name or IP address\\n\"\n        \"to communicate with. In the standard configuration this will be used\\n\"\n        \"to host both the client facing server and the admin user interface.\\n\")\n\n  existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\","
    },
    {
        "original": "def file_extension(self, category=None):\n        \"\"\"\n        :param category: audio|image|office|text|video\n        \"\"\"\n        category = category if category else self.random_element(\n            list(self.file_extensions.keys()))\n        return self.random_element(self.file_extensions[category])",
        "rewrite": "assistant\n\n```python\ndef file_extension(self, category=None):\n    category = category if category else self.random_element(list(self.file_extensions.keys()))\n    return self.random_element(self.file_extensions[category])\n```assistant\n\n```python\ndef file_extension(self, category=None):\n    category = category or self.random_element(list(self.file_extensions.keys()))\n    return self.random_element(self.file_extensions.get(category, []))\n```assistant\n\n```python\ndef file_extension(self, category=None):\n    category = category or self.random_element(list(self.file_extensions.keys()))\n    return self.random_element(self.file_extensions.get(category,"
    },
    {
        "original": "def InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \"\"\"Take a string as a path on a client and interpolate with client data.\n\n  Args:\n    path: A single string/unicode to be interpolated.\n    knowledge_base: An rdf_client.KnowledgeBase object.\n    users: A list of string usernames, or None.\n    path_args: A dict of additional args to use in interpolation. These take\n      precedence over any system provided variables.\n    depth: A counter for recursion depth.\n\n  Returns:\n    A single string if users is None, otherwise a list of strings.\n  \"\"\"\n\n  sys_formatters = {\n      # TODO(user): Collect this during discovery from the registry.\n      # HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\\n      # Value: SystemRoot\n      \"systemroot\": \"c:\\\\Windows\"\n  }\n\n  # Override any system formatters with path_args.\n  if path_args:\n    sys_formatters.update(path_args)\n\n  if users:\n    results = []\n    for user in users:\n      # Extract and interpolate user specific formatters.\n      user = GetUserInfo(knowledge_base, user)\n      if user:\n        formatters = dict((x.name, y) for x, y in user.ListSetFields())\n        formatters.update(sys_formatters)\n        try:\n          results.append(path.format(**formatters))\n        except KeyError:\n          pass  # We may be missing values for some users.\n    return results\n  else:\n    try:\n      path = path.format(**sys_formatters)\n    except KeyError:\n      logging.warning(\"Failed path interpolation on %s\", path)\n      return \"\"\n    if \"{\" in path and depth < 10:\n      path = InterpolatePath(\n          path,\n          knowledge_base=knowledge_base,\n          users=users,\n          path_args=path_args,\n          depth=depth + 1)\n    return path",
        "rewrite": "```python\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \"\"\"Take a string as a path on a client and interpolate with client data.\n\n  Args:\n    path: A single string/unicode to be interpolated.\n    knowledge_base: An rdf_client.KnowledgeBase object.\n    users: A list of string usernames, or None.\n    path_args: A dict of additional args to use in interpolation. These take\n      precedence over any system provided variables.\n    depth: A counter for recursion depth.\n\n  Returns:\n    A single string if users is None, otherwise a list"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "rewrite": "assistant\n\n```python\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as fid:\n        self.read_skel(fid)\n    self.name = file_name\n```assistant\n\nThis code does the same thing as the original, but it's more Pythonic and efficient. The `with` statement automatically closes the file when you're done with it, so you don't need to call `close()` explicitly.assistant\n\n```python\ndef load_skel(self, file_name):\n    with open(file_name, 'rb') as fid: # Assuming binary file"
    },
    {
        "original": "def remove_config(self, id):\n        \"\"\"\n            Remove a config\n\n            Args:\n                id (string): Full ID of the config to remove\n\n            Returns (boolean): True if successful\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no config with that ID exists\n        \"\"\"\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True",
        "rewrite": "```python\ndef remove_config(self, id):\n    url = self._url('/configs/{0}', id)\n    res = self._delete(url)\n    self._raise_for_status(res)\n    return True\n```assistant\n\n```python\ndef remove_config(self, id):\n    try:\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True\n    except docker.errors.NotFound:\n        return False\n```assistant\n\n```python\ndef remove_config(self, id):\n    try:\n        url = self"
    },
    {
        "original": "def carmichael_of_factorized( f_list ):\n  \"\"\"Return the Carmichael function of a number that is\n  represented as a list of (prime,exponent) pairs.\n  \"\"\"\n\n  if len( f_list ) < 1: return 1\n\n  result = carmichael_of_ppower( f_list[0] )\n  for i in range( 1, len( f_list ) ):\n    result = lcm( result, carmichael_of_ppower( f_list[i] ) )\n\n  return result",
        "rewrite": "```python\ndef carmichael_of_factorized(f_list):\n    if len(f_list) < 1: return 1\n\n    def lcm(a, b):\n        return a * b // math.gcd(a, b)\n\n    result = carmichael_of_ppower(f_list[0])\n    for i in range(1, len(f_list)):\n        result = lcm(result, carmichael_of_ppower(f_list[i]))\n\n    return result\n\ndef carmichael_of_ppower(p, e):\n    if e == 0: return 1\n    if e == 1: return p\n    if p"
    },
    {
        "original": "def _simulator_iterator(self, circuit: circuits.Circuit,\n                            param_resolver: study.ParamResolver,\n                            qubit_order: ops.QubitOrderOrList,\n                            initial_state: Union[int, np.ndarray]) -> Iterator:\n        \"\"\"See definition in `cirq.SimulatesIntermediateState`.\n\n        If the initial state is an int, the state is set to the computational\n        basis state corresponding to this state. Otherwise  if the initial\n        state is a np.ndarray it is the full initial state, either a pure state\n        or the full density matrix.  If it is the pure state it must be the\n        correct size, be normalized (an L2 norm of 1), and be safely castable\n        to an appropriate dtype for the simulator.  If it is a mixed state\n        it must be correctly sized and positive semidefinite with trace one.\n        \"\"\"\n        param_resolver = param_resolver or study.ParamResolver({})\n        resolved_circuit = protocols.resolve_parameters(circuit, param_resolver)\n        actual_initial_state = 0 if initial_state is None else initial_state\n        return self._base_iterator(resolved_circuit,\n                                   qubit_order,\n                                   actual_initial_state)",
        "rewrite": "```python\ndef _simulator_iterator(self, circuit: circuits.Circuit,\n                         param_resolver: study.ParamResolver,\n                         qubit_order: ops.QubitOrderOrList,\n                         initial_state: Union[int, np.ndarray]) -> Iterator:\n    param_resolver = param_resolver or study.ParamResolver({})\n    resolved_circuit = protocols.resolve_parameters(circuit, param_resolver)\n    actual_initial_state = 0 if initial_state is None else initial_state\n    return self._base_iterator(resolved_circuit,\n                               qubit_order,\n                               actual_initial_state)\n```assistant\n\n```python\ndef _simulator_iterator(self, circuit"
    },
    {
        "original": "def save_session(self, sid, session, namespace=None):\n        \"\"\"Store the user session for a client.\n\n        The only difference with the :func:`socketio.Server.save_session`\n        method is that when the ``namespace`` argument is not given the\n        namespace associated with the class is used.\n        \"\"\"\n        return self.server.save_session(\n            sid, session, namespace=namespace or self.namespace)",
        "rewrite": "assistant\n\n```python\ndef save_session(self, sid, session, namespace=None):\n    return self.server.save_session(sid, session, namespace=namespace or self.namespace)\n```assistant\n\nThis code is identical to the original, but it's formatted to be more concise and readable.assistant\n\n```python\ndef save_session(self, sid, session, namespace=None):\n    self.server.save_session(sid, session, namespace or self.namespace)\n```assistant\n\nThis code is identical to the original, but it's formatted to be more concise and readable.assistant\n\n```python"
    },
    {
        "original": "def get_best_electronegativity_anonymous_mapping(self, struct1, struct2):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar. If multiple substitutions are within tolerance\n        this will return the one which minimizes the difference in\n        electronegativity between the matches species.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            min_mapping (Dict): Mapping of struct1 species to struct2 species\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        use_rms=True, break_on_match=True)\n\n        if matches:\n            min_X_diff = np.inf\n            for m in matches:\n                X_diff = 0\n                for k, v in m[0].items():\n                    X_diff += struct1.composition[k] * (k.X - v.X) ** 2\n                if X_diff < min_X_diff:\n                    min_X_diff = X_diff\n                    best = m[0]\n            return best",
        "rewrite": "```python\ndef get_best_electronegativity_anonymous_mapping(self, struct1, struct2):\n    struct1, struct2 = self._process_species([struct1, struct2])\n    struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n    matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                    use_rms=True, break_on_match=True)\n\n    if matches:\n        min_X_diff = np.inf\n        best = None\n        for m in matches:\n            X_diff = 0"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "rewrite": "```python\nimport json\nimport collections\n\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping\n```assistant\n\n```python\nimport json\nimport collections\n\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance("
    },
    {
        "original": "def kwargs_from_keyword(from_kwargs,to_kwargs,keyword,clean_origin=True):\n\t\"\"\"\n\tLooks for keys of the format keyword_value. \n\tAnd return a dictionary with {keyword:value} format\n\n\tParameters:\n\t-----------\n\t\tfrom_kwargs : dict\n\t\t\tOriginal dictionary\n\t\tto_kwargs : dict\n\t\t\tDictionary where the items will be appended\n\t\tkeyword : string\n\t\t\tKeyword to look for in the orginal dictionary\n\t\tclean_origin : bool\n\t\t\tIf True then the k,v pairs from the original \n\t\t\tdictionary are deleted\n\t\"\"\"\n\tfor k in list(from_kwargs.keys()):\n\t\tif '{0}_'.format(keyword) in k:\n\t\t\tto_kwargs[k.replace('{0}_'.format(keyword),'')]=from_kwargs[k]\n\t\t\tif clean_origin:\n\t\t\t\tdel from_kwargs[k]\n\treturn to_kwargs",
        "rewrite": "assistant\n\n```python\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for k in list(from_kwargs.keys()):\n        if '{0}_'.format(keyword) in k:\n            to_kwargs[k.replace('{0}_'.format(keyword), '')] = from_kwargs[k]\n            if clean_origin:\n                del from_kwargs[k]\n    return to_kwargs\n```assistant\n\n```python\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for k in list(from_kwargs.keys()):\n        if keyword + '_' in k:\n            to_kwargs[k.replace(keyword +"
    },
    {
        "original": "def chemical_symbols(self):\n        \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n        charr = self.read_value(\"chemical_symbols\")\n        symbols = []\n        for v in charr:\n            s = \"\".join(c.decode(\"utf-8\") for c in v)\n            symbols.append(s.strip())\n\n        return symbols",
        "rewrite": "assistant\n\n```python\ndef chemical_symbols(self):\n    \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n    charr = self.read_value(\"chemical_symbols\")\n    symbols = [s.strip().decode(\"utf-8\") for v in charr for s in (\"\".join(c.decode(\"utf-8\") for c in v))]\n    return symbols\n```assistant\n\n```python\ndef chemical_symbols(self):\n    \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n    charr = self.read_value(\"chemical_symbols\")\n    symbols = [\"\".join(c.decode"
    },
    {
        "original": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n            knowledge_base):\n    \"\"\"Parse the sysctl output.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused.\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    # The KeyValueParser generates an ordered dict by default. The sysctl vals\n    # aren't ordering dependent, but there's no need to un-order it.\n    for k, v in iteritems(self.lexer.ParseToOrderedDict(stdout)):\n      key = k.replace(\".\", \"_\")\n      if len(v) == 1:\n        v = v[0]\n      result[key] = v\n    return [result]",
        "rewrite": "```python\ndef Parse(self, cmd, args, stdout, stderr, return_val, time_taken, knowledge_base):\n    \"\"\"Parse the sysctl output.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused.\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    for k, v in self.lexer.ParseToOrderedDict(stdout).items():\n        key = k.replace(\".\", \"_\")\n        if len(v) == 1:\n            v = v[0]\n        result[key] = v\n    return [result]\n```assistant"
    },
    {
        "original": "def GetNotificationsForAllShards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\n\n    Used by worker_test_lib.MockWorker to cover all shards with a single worker.\n\n    Args:\n      queue: usually rdfvalue.RDFURN(\"aff4:/W\")\n\n    Returns:\n      List of rdf_flows.GrrNotification objects\n    \"\"\"\n    notifications_by_session_id = {}\n    for queue_shard in self.GetAllNotificationShards(queue):\n      self._GetUnsortedNotifications(\n          queue_shard, notifications_by_session_id=notifications_by_session_id)\n\n    return notifications_by_session_id.values()",
        "rewrite": "assistant\n\n```python\ndef get_notifications_for_all_shards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\"\"\"\n    notifications_by_session_id = {}\n    for queue_shard in self.get_all_notification_shards(queue):\n        self._get_unsorted_notifications(\n            queue_shard, notifications_by_session_id=notifications_by_session_id)\n\n    return list(notifications_by_session_id.values())\n```assistant\n\n```python\ndef get_notifications_for_all_shards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\"\"\"\n    notifications_by_session_id = {}\n    for queue"
    },
    {
        "original": "def get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    for item in container.view:\n        if item._moId == obj_moid:\n            return item\n    return None",
        "rewrite": "```python\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    for item in container.view:\n        if item._moId == obj_moid:\n            return item\n    return None\n```assistant\n\n```python\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    return next(("
    },
    {
        "original": "def get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n        \"\"\"\n        Resolve the field within the given state.\n        \"\"\"\n        # resolve field\n        field_class = state.javavm_classloader.get_class(field_class_name)\n        field_id = resolve_field(state, field_class, field_name, field_type)\n        # return field ref\n        return cls.from_field_id(obj_alloc_id, field_id)",
        "rewrite": "assistant\n\n```python\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    field_class = state.javavm_classloader.get_class(field_class_name)\n    field_id = resolve_field(state, field_class, field_name, field_type)\n    return cls.from_field_id(obj_alloc_id, field_id)\n```assistant\n\nPlease note that the function `resolve_field` is not defined in the provided code snippet. It is assumed that this function is defined elsewhere in the codebase.assistant\n\n```python\ndef resolve_field(state, field_class,"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "rewrite": "                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        # The header name is not indexed\n        if is_sensitive(\n            hdr_name,\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name,\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=None,\n            hdr_value=hdr_value\n        ), len(\n            HPack"
    },
    {
        "original": "def safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)",
        "rewrite": "assistant\n\n```python\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    return yaml.dump_all(documents, stream, default_flow_style=False, Dumper=yaml.SafeDumper, **kwds)\n```assistant\n\n```python\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    return yaml.dump_all(documents, stream, default_flow_style=False, Dumper=yaml.SafeDumper, allow_unicode=True, **kwds)\n```assistant\n\n```python\nimport yaml\n\ndef safe_dump_all(documents, stream"
    },
    {
        "original": "def find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        # it might be good to add \"type vcan\", but that might (?) exclude physical can devices\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True)\n\n    except Exception as e: # subprocess.CalledProcessError was too specific\n        log.error(\"failed to fetch opened can devices: %s\", e)\n        return []\n\n    else:\n        #log.debug(\"find_available_interfaces(): output=\\n%s\", output)\n        # output contains some lines like \"1: vcan42: <NOARP,UP,LOWER_UP> ...\"\n        # extract the \"vcan42\" of each line\n        interface_names = [line.split(\": \", 3)[1] for line in output.splitlines()]\n        log.debug(\"find_available_interfaces(): detected: %s\", interface_names)\n        return filter(_PATTERN_CAN_INTERFACE.match, interface_names)",
        "rewrite": "```python\nimport subprocess\nimport logging\nimport re\n\nlog = logging.getLogger(__name__)\n\n_PATTERN_CAN_INTERFACE = re.compile(r'^vcan\\d+$')\n\ndef find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"IP addresses from all interfaces.\"\"\"\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n      for address in interface.addresses:\n        if address.human_readable_address not in filtered_ips:\n          result.append(Text(address.human_readable_address))\n    return sorted(result)",
        "rewrite": "assistant\n\n```python\ndef get_ip_addresses(self):\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n        for address in interface.addresses:\n            if address.human_readable_address not in filtered_ips:\n                result.append(address.human_readable_address)\n    return sorted(result)\n```assistant\n\n```python\ndef get_ip_addresses(self):\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:"
    },
    {
        "original": "def recv_with_timeout(self, timeout=1):\n        \"\"\"Receive a complete ISOTP message, blocking until a message is\n        received or the specified timeout is reached.\n        If timeout is 0, then this function doesn't block and returns the\n        first frame in the receive buffer or None if there isn't any.\"\"\"\n        msg = self.ins.recv(timeout)\n        t = time.time()\n        if msg is None:\n            raise Scapy_Exception(\"Timeout\")\n        return self.basecls, msg, t",
        "rewrite": "```python\ndef recv_with_timeout(self, timeout=1):\n    \"\"\"Receive a complete ISOTP message, blocking until a message is\n    received or the specified timeout is reached.\n    If timeout is 0, then this function doesn't block and returns the\n    first frame in the receive buffer or None if there isn't any.\"\"\"\n    msg = self.ins.recv(timeout)\n    if msg is None:\n        raise Exception(\"Timeout\")\n    return self.basecls, msg\n```assistant\n\n```python\nimport time\n\ndef recv_with_timeout(self, timeout=1):\n    \"\"\"Receive a complete ISOTP message,"
    },
    {
        "original": "def beginning_offsets(self, partitions):\n        \"\"\"Get the first offset for the given partitions.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The earliest available offsets for the\n            given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms.\n        \"\"\"\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets",
        "rewrite": "```python\ndef beginning_offsets(self, partitions):\n    return self._fetcher.beginning_offsets(partitions)\n```assistant\n\n```python\ndef beginning_offsets(self, partitions):\n    return self._fetcher.beginning_offsets(partitions, self.config.get('request_timeout_ms'))\n```assistant\n\n```python\ndef beginning_offsets(self, partitions):\n    return self._fetcher.beginning_offsets(partitions, self.config.get('request_timeout_ms', 1000))\n```assistant\n\n```python\ndef beginning_offsets(self, partitions):\n    try:\n        return self._fetcher.beginning_offsets(partitions"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "rewrite": "```python\ndef get_projection_on_elements(self, structure):\n    dico = {}\n    for spin in self.data.keys():\n        dico[spin] = [[defaultdict(float) for _ in range(self.nkpoints)]\n                      for _ in range(self.nbands)]\n\n    for iat in range(self.nions):\n        name = structure.species[iat].symbol\n        for spin, d in self.data.items():\n            for k, b in itertools.product(range(self.nkpoints), range(self.nbands)):\n                dico[spin][b][k][name] = np.sum(d[k, b, iat, :"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "rewrite": "assistant\n\n```python\ndef is_valid_ipv4(ip):\n    parts = ip.split('.')\n    if len(parts)!= 4:\n        return False\n    for item in parts:\n        if not item.isdigit():\n            return False\n        if not 0 <= int(item) <= 255:\n            return False\n    return True\n\ndef is_valid_ipv6(ip):\n    parts = ip.split(':')\n    if len(parts)!= 8:\n        return False\n    for item in parts:\n        if not all(c in '0123456789abcdefABCDEF' for c in item):\n            return False"
    },
    {
        "original": "def _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \"\"\"Fast Fourier transform-based Gaussian kernel density estimate (KDE).\n\n    The code was adapted from https://github.com/mfouesneau/faststats\n\n    Parameters\n    ----------\n    x : Numpy array or list\n    cumulative : bool\n        If true, estimate the cdf instead of the pdf\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    xmin : float\n        Manually set lower limit.\n    xmax : float\n        Manually set upper limit.\n\n    Returns\n    -------\n    density: A gridded 1D KDE of the input points (x)\n    xmin: minimum value of x\n    xmax: maximum value of x\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax",
        "rewrite": ")\n    scotts_bandwidth = scotts_factor * d_x\n\n    if cumulative:\n        density = np.convolve(grid, np.exp(-np.arange(n_bins) * scotts_bandwidth), mode='full')\n    else:\n        density = np.convolve(grid, np.exp(-np.arange(n_bins) * scotts_bandwidth ** 2), mode='full')\n\n    return density, xmin, xmax\n\"\nThe code has several issues:\n\n1.  The function name starts with an underscore, which is not a conventional Python naming convention.\n2.  The function does not handle the case where `x` is a list"
    },
    {
        "original": "def get_public_keys_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 public keys from the v3 signature block\n        \"\"\"\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        public_keys = []\n\n        for signer in self._v3_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys",
        "rewrite": "assistant\n\n```python\ndef get_public_keys_der_v3(self):\n    if self._v3_signing_data is None:\n        self.parse_v3_signing_block()\n\n    return [signer.public_key for signer in self._v3_signing_data]\n```assistant\n\n```python\ndef get_public_keys_der_v3(self):\n    if self._v3_signing_data is None:\n        self.parse_v3_signing_block()\n\n    return list(map(lambda signer: signer.public_key, self._v3_signing_data))\n```assistant\n\n```python\ndef get_public_keys_der_v"
    }
]