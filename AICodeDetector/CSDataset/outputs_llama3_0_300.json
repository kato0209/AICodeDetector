[
    {
        "original": "def beginning_offsets(self, partitions):\n        \"\"\"Get the first offset for the given partitions.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The earliest available offsets for the\n            given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms.\n        \"\"\"\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets",
        "sampled": "```python\nfrom kafka import KafkaConsumer\n\ndef beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {}\n    for p in partitions:\n        topic_partition = p.topic, p.partition\n        try:\n            offset = consumer.beginning_offset(topic_partition)\n            offsets[p] = offset\n        except Exception as e:\n            if isinstance(e, UnsupportedVersionError) or isinstance(e, KafkaTimeoutError):\n                raise e\n    return offsets\n```"
    },
    {
        "original": "def _get_values(cls, diff_dict, type='new'):\n        \"\"\"\n        Returns a dictionaries with the 'new' values in a diff dict.\n\n        type\n            Which values to return, 'new' or 'old'\n        \"\"\"\n        ret_dict = {}\n        for p in diff_dict.keys():\n            if type in diff_dict[p].keys():\n                ret_dict.update({p: diff_dict[p][type]})\n            else:\n                ret_dict.update(\n                    {p: cls._get_values(diff_dict[p], type=type)})\n        return ret_dict",
        "sampled": "```\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0] if type == 'new' else value[1] for key, value in diff_dict.items() if len(value) == 2}\n```"
    },
    {
        "original": "def get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n        \"\"\"\n        Resolve the field within the given state.\n        \"\"\"\n        # resolve field\n        field_class = state.javavm_classloader.get_class(field_class_name)\n        field_id = resolve_field(state, field_class, field_name, field_type)\n        # return field ref\n        return cls.from_field_id(obj_alloc_id, field_id)",
        "sampled": "```\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    \"\"\"\n    Resolve the field within the given state.\n    \"\"\"\n    obj = state[obj_alloc_id]\n    if not hasattr(obj, field_class_name):\n        raise AttributeError(f\"Object {obj_alloc_id} does not have class {field_class_name}\")\n    field_cls = getattr(obj, field_class_name)\n    if not hasattr(field_cls, field_name):\n        raise AttributeError(f\""
    },
    {
        "original": "def pd_coords(self, comp):\n        \"\"\"\n        The phase diagram is generated in a reduced dimensional space\n        (n_elements - 1). This function returns the coordinates in that space.\n        These coordinates are compatible with the stored simplex objects.\n        \"\"\"\n        if set(comp.elements).difference(self.elements):\n            raise ValueError('{} has elements not in the phase diagram {}'\n                             ''.format(comp, self.elements))\n        return np.array(\n            [comp.get_atomic_fraction(el) for el in self.elements[1:]])",
        "sampled": "```\ndef pd_coords(self, comp):\n    n_elements = len(comp)\n    coords = [0] * (n_elements - 1)\n    sum_comp = sum(comp)\n    \n    for i in range(n_elements - 1):\n        coords[i] = comp[i] / sum_comp\n    \n    return coords\n```"
    },
    {
        "original": "def relative_ref(self, baseURI):\n        \"\"\"\n        Return string containing relative reference to package item from\n        *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n        return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n        \"\"\"\n        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath",
        "sampled": "```\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    i = 0\n    while i < len(base_path) and i < len(target_path) and base_path[i] == target_path[i]:\n        i += 1\n    rel_ref = '../' * (len(base_path) - i) + '/'.join(target_path[i:])\n    return rel_ref if rel_ref else '.'\n```"
    },
    {
        "original": "def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks is enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with\n        peer is capable of enhanced route refresh capability.\n        \"\"\"\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled",
        "sampled": "```\ndef is_enhanced_rr_cap_valid(self):\n    open_messages_sent = self.open_messages_sent\n    open_messages_received = self.open_messages_received\n\n    if open_messages_sent and open_messages_received:\n        return 'enhanced-route-refresh-capability' in open_messages_sent and 'enhanced-route-refresh-capability' in open_messages_received\n    else:\n        return False\n```"
    },
    {
        "original": "def parse_osm_nodes_paths(osm_data):\n    \"\"\"\n    Construct dicts of nodes and paths with key=osmid and value=dict of\n    attributes.\n\n    Parameters\n    ----------\n    osm_data : dict\n        JSON response from from the Overpass API\n\n    Returns\n    -------\n    nodes, paths : tuple\n    \"\"\"\n\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            key = element['id']\n            nodes[key] = get_node(element)\n        elif element['type'] == 'way': #osm calls network paths 'ways'\n            key = element['id']\n            paths[key] = get_path(element)\n\n    return nodes, paths",
        "sampled": "```\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = element\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags = {k: v for k, v in element.items() if k != 'nodes' and k !="
    },
    {
        "original": "def get_best_electronegativity_anonymous_mapping(self, struct1, struct2):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar. If multiple substitutions are within tolerance\n        this will return the one which minimizes the difference in\n        electronegativity between the matches species.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            min_mapping (Dict): Mapping of struct1 species to struct2 species\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        use_rms=True, break_on_match=True)\n\n        if matches:\n            min_X_diff = np.inf\n            for m in matches:\n                X_diff = 0\n                for k, v in m[0].items():\n                    X_diff += struct1.composition[k] * (k.X - v.X) ** 2\n                if X_diff < min_X_diff:\n                    min_X_diff = X_diff\n                    best = m[0]\n            return best",
        "sampled": "```python\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    # Get all possible permutations of mapping between species of two structures\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n\n    # Initialize minimum difference in electronegativity and best mapping\n    min_diff = float('inf')\n    best_mapping = None\n\n    # Iterate over"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "sampled": "```\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    pass\n\nclass HPackIndexedHdr(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithIncrIndexing(HPackHeaders):\n    pass\n\n\ndef _convert_a_header_to_a_h2_header(\n    hdr_name: str,\n    hdr_value: str,\n    is_sensitive: Callable[[str, str], bool"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "sampled": "```python\nclass BigchainTransactionCreator:\n    def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        from bigchaindb.common.transaction import Transaction\n        \n        # Initialize an empty transaction object \n        transaction = Transaction()\n\n        # Set transaction type to 'CREATE'\n        transaction.operation = 'CREATE'\n\n        # Add signers to transaction inputs \n        for signer in tx_signers:\n            transaction.add_input(signer)\n\n        # Add recipients and amounts to transaction outputs \n       "
    },
    {
        "original": "def utc_dt_to_local_dt(dtm):\n    \"\"\"Convert a UTC datetime to datetime in local timezone\"\"\"\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo is not None and dtm.tzinfo != utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(mktz())",
        "sampled": "```\nfrom datetime import datetime\nimport pytz\n\ndef utc_dt_to_local_dt(dtm):\n    utc_dt = datetime.strptime(dtm, '%Y-%m-%d %H:%M:%S')\n    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)\n    local_dt = utc_dt.astimezone()\n    return local_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n```"
    },
    {
        "original": "def _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n                log.debug('Have deserialized data %s', _existingData)\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecutionError(error)\n            if 'Section' in policy_info['ScriptIni'] and policy_info['ScriptIni']['Section'].lower() in [z.lower() for z in _existingData.keys()]:\n                if 'SettingName' in policy_info['ScriptIni']:\n                    log.debug('Need to look for %s', policy_info['ScriptIni']['SettingName'])\n                    if policy_info['ScriptIni']['SettingName'].lower() in [z.lower() for z in _existingData[policy_info['ScriptIni']['Section']].keys()]:\n                        return _existingData[policy_info['ScriptIni']['Section']][policy_info['ScriptIni']['SettingName'].lower()]\n                    else:\n                        return None\n                else:\n                    return _existingData[policy_info['ScriptIni']['Section']]\n            else:\n                return None\n\n    return None",
        "sampled": "```\ndef _getScriptSettingsFrom IniFile(policy_info):\n    # Open and read the file\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    # Initialize variables\n    scripts = {}\n    current_script = None\n\n    # Parse lines\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            # Section header found, extract script name\n            start_br"
    },
    {
        "original": "def _get_triplet_scores(self, triangles_list):\n        \"\"\"\n        Returns the score of each of the triplets found in the current model\n\n        Parameters\n        ---------\n        triangles_list: list\n                        The list of variables forming the triangles to be updated. It is of the form of\n                        [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]\n\n        Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'var_5', 'var_4', 'var_7'}): 10.23}\n        \"\"\"\n        triplet_scores = {}\n        for triplet in triangles_list:\n\n            # Find the intersection sets of the current triplet\n            triplet_intersections = [intersect for intersect in it.combinations(triplet, 2)]\n\n            # Independent maximization\n            ind_max = sum([np.amax(self.objective[frozenset(intersect)].values) for intersect in triplet_intersections])\n\n            # Joint maximization\n            joint_max = self.objective[frozenset(triplet_intersections[0])]\n            for intersect in triplet_intersections[1:]:\n                joint_max += self.objective[frozenset(intersect)]\n            joint_max = np.amax(joint_max.values)\n            # score = Independent maximization solution - Joint maximization solution\n            score = ind_max - joint_max\n            triplet_scores[frozenset(triplet)] = score\n\n        return triplet_scores",
        "sampled": "```\ndef _get_triplet_scores(self, triangles_list):\n    scores = {}\n    for triangle in triangles_list:\n        triangle_set = frozenset(triangle)\n        # assuming the score calculation is a placeholder, replace with actual logic\n        score = len(triangle) * 2.0\n        scores[triangle_set] = score\n    return scores\n```"
    },
    {
        "original": "async def vcx_messages_update_status(msg_json: str):\n    \"\"\"\n    Update the status of messages from the specified connection\n    :param msg_json:\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,\n                           c_msg_json,\n                           vcx_messages_update_status.cb)\n\n    logger.debug(\"vcx_messages_update_status completed\")\n    return result",
        "sampled": "```\nasync def vcx_messages_update_status(msg_json: str):\n    import json\n    import asyncio\n    from aiohttp import ClientSession\n\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n\n        for message in messages:\n            url = f'https://example.com/update_message_status/{connection_id}'\n            data = {'message_id': message['uid'], 'status"
    },
    {
        "original": "def recv_with_timeout(self, timeout=1):\n        \"\"\"Receive a complete ISOTP message, blocking until a message is\n        received or the specified timeout is reached.\n        If timeout is 0, then this function doesn't block and returns the\n        first frame in the receive buffer or None if there isn't any.\"\"\"\n        msg = self.ins.recv(timeout)\n        t = time.time()\n        if msg is None:\n            raise Scapy_Exception(\"Timeout\")\n        return self.basecls, msg, t",
        "sampled": "```\nimport socket\nimport time\n\nclass IsoTPSocket:\n    def __init__(self):\n        self.receive_buffer = b''\n\n    def recv_with_timeout(self, timeout=1):\n        start_time = time.time()\n        while True:\n            if self.receive_buffer:\n                return self.receive_buffer.pop(0)\n            elif time.time() - start_time > timeout:\n                return None\n```"
    },
    {
        "original": "def carmichael_of_factorized( f_list ):\n  \"\"\"Return the Carmichael function of a number that is\n  represented as a list of (prime,exponent) pairs.\n  \"\"\"\n\n  if len( f_list ) < 1: return 1\n\n  result = carmichael_of_ppower( f_list[0] )\n  for i in range( 1, len( f_list ) ):\n    result = lcm( result, carmichael_of_ppower( f_list[i] ) )\n\n  return result",
        "sampled": "```\nimport math\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, (p-1) * (p**(e-1)))\n    return lambda_val\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a, b)\n```"
    },
    {
        "original": "def absent(name, profile=\"splunk\"):\n    \"\"\"\n    Ensure a search is absent\n\n    .. code-block:: yaml\n\n        API Error Search:\n          splunk_search.absent\n\n    The following parameters are required:\n\n    name\n        This is the name of the search in splunk\n    \"\"\"\n    ret = {\n        'name': name,\n        'changes': {},\n        'result': True,\n        'comment': '{0} is absent.'.format(name)\n    }\n\n    target = __salt__['splunk_search.get'](name, profile=profile)\n    if target:\n        if __opts__['test']:\n            ret = {}\n            ret[\"name\"] = name\n            ret['comment'] = \"Would delete {0}\".format(name)\n            ret['result'] = None\n            return ret\n\n        result = __salt__['splunk_search.delete'](name, profile=profile)\n        if result:\n            ret['comment'] = '{0} was deleted'.format(name)\n        else:\n            ret['comment'] = 'Failed to delete {0}'.format(name)\n            ret['result'] = False\n    return ret",
        "sampled": "```\nimport requests\n\ndef absent(name, profile=\"splunk\"):\n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    response = requests.delete(url, auth=(\"username\", \"password\"))\n    if response.status_code == 200:\n        return True\n    else:\n        return False\n```"
    },
    {
        "original": "def GetNotificationsForAllShards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\n\n    Used by worker_test_lib.MockWorker to cover all shards with a single worker.\n\n    Args:\n      queue: usually rdfvalue.RDFURN(\"aff4:/W\")\n\n    Returns:\n      List of rdf_flows.GrrNotification objects\n    \"\"\"\n    notifications_by_session_id = {}\n    for queue_shard in self.GetAllNotificationShards(queue):\n      self._GetUnsortedNotifications(\n          queue_shard, notifications_by_session_id=notifications_by_session_id)\n\n    return notifications_by_session_id.values()",
        "sampled": "```\ndef GetNotificationsForAllShards(self, queue):\n    notifications = []\n    for shard in self.GetShardNames(queue):\n        notifications.extend(self.GetNotificationsForShard(queue, shard))\n    return notifications\n```"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "sampled": "```\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n\n    def differing_functions_with_consts(self):\n        matches = []\n        for i in range(len(self.functions)):\n            for j in range(i + 1, len(self.functions)):\n                x = symbols('x')\n                eq1 = Eq(eval(self.functions[i]), 0)\n                eq2 = Eq(eval(self.functions[j]), 0)\n               "
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "sampled": "```\nclass EOPatchSaver:\n    def save(self, eopatch, use_tmp=True):\n        if use_tmp:\n            # Save to temporary file\n            with open(\"temp.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n        else:\n            # Save to intended location\n            with open(\"eopatch.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n```"
    },
    {
        "original": "def _addAttr(self, txn, isCommitted=False) -> None:\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        assert get_type(txn) == ATTRIB\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.attributeStore.set(hashed_value, value)",
        "sampled": "```\nclass StateTrie:\n    def __init__(self):\n        self.trie = {}\n\n    def _addAttr(self, txn, isCommitted=False) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.tr"
    },
    {
        "original": "def _find_packages(root):\n  \"\"\"\n  Helper for ``build_index()``: Yield a list of tuples\n  ``(pkg_xml, zf, subdir)``, where:\n    - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n    package\n    - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n    - ``subdir`` is the subdirectory (relative to ``root``) where\n    the package was found (e.g. 'corpora' or 'grammars').\n  \"\"\"\n  # Find all packages.\n  packages = []\n  for dirname, subdirs, files in os.walk(root):\n    relpath = '/'.join(_path_from(root, dirname))\n    for filename in files:\n      if filename.endswith('.xml'):\n        xmlfilename = os.path.join(dirname, filename)\n        zipfilename = xmlfilename[:-4]+'.zip'\n        try: zf = zipfile.ZipFile(zipfilename)\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (zipfilename, e))\n        try: pkg_xml = ElementTree.parse(xmlfilename).getroot()\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (xmlfilename, e))\n\n        # Check that the UID matches the filename\n        uid = os.path.split(xmlfilename[:-4])[1]\n        if pkg_xml.get('id') != uid:\n          raise ValueError('package identifier mismatch (%s '\n                   'vs %s)' % (pkg_xml.get('id'), uid))\n\n        # Check that the zipfile expands to a subdir whose\n        # name matches the uid.\n        if sum( (name!=uid and not name.startswith(uid+'/'))\n            for name in zf.namelist() ):\n          raise ValueError('Zipfile %s.zip does not expand to a '\n                   'single subdirectory %s/' % (uid, uid))\n\n        yield pkg_xml, zf, relpath\n    # Don't recurse into svn subdirectories:\n    try: subdirs.remove('.svn')\n    except ValueError: pass",
        "sampled": "```\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ET\n\ndef _find_packages(root):\n    for subdir, dirs, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                zf = zipfile.ZipFile(os.path.join(subdir, file))\n                pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                yield (pkg_xml, zf, os.path.relpath(subdir, root))\n```"
    },
    {
        "original": "def srbt1(bt_address, pkts, *args, **kargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kargs)\n    if len(a) > 0:\n        return a[0][1]",
        "sampled": "```\nimport bluetooth\n\ndef srbt1(bt_address, pkts, *_args={}, **_kargs={}):\n    sock = bluetoothAMP.get_socket()\n    sock.connect((bt_address, 1))\n    sock.send(pkts)\n    data = sock.recv(1024)\n    return data\n```"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"IP addresses from all interfaces.\"\"\"\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n      for address in interface.addresses:\n        if address.human_readable_address not in filtered_ips:\n          result.append(Text(address.human_readable_address))\n    return sorted(result)",
        "sampled": "```\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def GetIPAddresses(self):\n        ip_array = []\n        for interface in netifaces.interfaces():\n            if interface == 'lo':  # ignore loopback interface\n                continue\n            lst = netifaces.ifaddresses(interface)\n            teste = {}\n            for item in lst:\n                if(item == 2): # AF_INET \n                    for thing in lst[item]:\n                        teste={\n                            \"iname\":interface,\n                           "
    },
    {
        "original": "def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        \"\"\"\n        Checks if two points are symmetrically related.\n\n        Args:\n            point_a (3x1 array): First point.\n            point_b (3x1 array): Second point.\n            tol (float): Absolute tolerance for checking distance.\n\n        Returns:\n            True if self.operate(point_a) == point_b or vice versa.\n        \"\"\"\n        if np.allclose(self.operate(point_a), point_b, atol=tol):\n            return True\n        if np.allclose(self.operate(point_b), point_a, atol=tol):\n            return True\n        return False",
        "sampled": "```\ndef operate(self, point):\n    # Assuming operate function is defined elsewhere\n    pass\n\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    \"\"\"\n    Checks if two points are symmetrically related.\n\n    Args:\n        point_a (3x1 array): First point.\n        point_b (3x1 array): Second point.\n        tol (float): Absolute tolerance for checking distance.\n\n    Returns:\n        True if self.operate(point"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "sampled": "```\ndef validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    parts = ip.split(\".\")\n    if len(parts) != 4:\n        return False\n    for part in parts:\n        if not part.isdigit():\n            return False\n        i = int(part)\n        if i < 0 or i > 255:\n            return False\n    return True\n```"
    },
    {
        "original": "def find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        # it might be good to add \"type vcan\", but that might (?) exclude physical can devices\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True)\n\n    except Exception as e: # subprocess.CalledProcessError was too specific\n        log.error(\"failed to fetch opened can devices: %s\", e)\n        return []\n\n    else:\n        #log.debug(\"find_available_interfaces(): output=\\n%s\", output)\n        # output contains some lines like \"1: vcan42: <NOARP,UP,LOWER_UP> ...\"\n        # extract the \"vcan42\" of each line\n        interface_names = [line.split(\": \", 3)[1] for line in output.splitlines()]\n        log.debug(\"find_available_interfaces(): detected: %s\", interface_names)\n        return filter(_PATTERN_CAN_INTERFACE.match, interface_names)",
        "sampled": "```python\nimport subprocess\nimport logging\n\ndef find_available_interfaces():\n    \"\"\"\n    Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n    \n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-"
    },
    {
        "original": "def save_session(self, sid, session, namespace=None):\n        \"\"\"Store the user session for a client.\n\n        The only difference with the :func:`socketio.Server.save_session`\n        method is that when the ``namespace`` argument is not given the\n        namespace associated with the class is used.\n        \"\"\"\n        return self.server.save_session(\n            sid, session, namespace=namespace or self.namespace)",
        "sampled": "```\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {}\n\n    def save_session(self, sid, session, namespace=None):\n        if namespace is None:\n            namespace = self.namespace\n        if namespace not in self.sessions:\n            self.sessions[namespace] = {}\n        self.sessions[namespace][sid] = session\n```"
    },
    {
        "original": "def get_public_keys_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 public keys from the v3 signature block\n        \"\"\"\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        public_keys = []\n\n        for signer in self._v3_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys",
        "sampled": "```\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    public_keys = []\n    if self.v3_signature_block:\n        for cert in self.v3_signature_block.certs:\n            pub_key = cert.public_key()\n            der_pub_key = pub_key.public_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n            )\n            public_keys.append(der"
    },
    {
        "original": "def chemical_symbols(self):\n        \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n        charr = self.read_value(\"chemical_symbols\")\n        symbols = []\n        for v in charr:\n            s = \"\".join(c.decode(\"utf-8\") for c in v)\n            symbols.append(s.strip())\n\n        return symbols",
        "sampled": "```\ndef chemical_symbols(atom_species, symbol_length):\n    symbols = []\n    ascii_offset = 97  # ASCII value of 'a'\n    for i in range(atom_species):\n        symbol = \"\"\n        for j in range(symbol_length):\n            symbol += chr(ascii_offset + ((i + j) % 26))\n        symbols.append(symbol)\n    return symbols\n```"
    },
    {
        "original": "def safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)",
        "sampled": "```\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    if stream is None:\n        return yaml.safe_dump_all(documents, default_flow_style=True, **kwds)\n    else:\n        yaml.safe_dump_all(documents, stream=stream, default_flow_style=True, **kwds)\n```"
    },
    {
        "original": "def _simulator_iterator(self, circuit: circuits.Circuit,\n                            param_resolver: study.ParamResolver,\n                            qubit_order: ops.QubitOrderOrList,\n                            initial_state: Union[int, np.ndarray]) -> Iterator:\n        \"\"\"See definition in `cirq.SimulatesIntermediateState`.\n\n        If the initial state is an int, the state is set to the computational\n        basis state corresponding to this state. Otherwise  if the initial\n        state is a np.ndarray it is the full initial state, either a pure state\n        or the full density matrix.  If it is the pure state it must be the\n        correct size, be normalized (an L2 norm of 1), and be safely castable\n        to an appropriate dtype for the simulator.  If it is a mixed state\n        it must be correctly sized and positive semidefinite with trace one.\n        \"\"\"\n        param_resolver = param_resolver or study.ParamResolver({})\n        resolved_circuit = protocols.resolve_parameters(circuit, param_resolver)\n        actual_initial_state = 0 if initial_state is None else initial_state\n        return self._base_iterator(resolved_circuit,\n                                   qubit_order,\n                                   actual_initial_state)",
        "sampled": "```\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit,\n                        param_resolver,\n                        qubit_order,\n                        initial_state: Union[int, np.ndarray]) -> Iterator:\n    # Check if initial_state is an integer \n    if isinstance(initial_state, int):\n      # Set initial_state to computational basis corresponding to this integer \n      pass\n  \n    # Check if initial_state is a numpy array  \n    elif isinstance"
    },
    {
        "original": "def predictive_variance(self, mu,variance, predictive_mean=None, Y_metadata=None):\n        \"\"\"\n        Approximation to the predictive variance: V(Y_star)\n\n        The following variance decomposition is used:\n        V(Y_star) = E( V(Y_star|f_star)**2 ) + V( E(Y_star|f_star) )**2\n\n        :param mu: mean of posterior\n        :param sigma: standard deviation of posterior\n        :predictive_mean: output's predictive mean, if None _predictive_mean function will be called.\n\n        \"\"\"\n        #sigma2 = sigma**2\n        normalizer = np.sqrt(2*np.pi*variance)\n\n        fmin_v = -np.inf\n        fmin_m = np.inf\n        fmin = -np.inf\n        fmax = np.inf\n\n        from ..util.misc import safe_exp\n        # E( V(Y_star|f_star) )\n        def int_var(f,m,v):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = safe_exp(exponent)\n            #If p is zero then conditional_variance will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_variance(f)*p\n        scaled_exp_variance = [quad(int_var, fmin_v, fmax,args=(mj,s2j))[0] for mj,s2j in zip(mu,variance)]\n        exp_var = np.array(scaled_exp_variance)[:,None] / normalizer\n\n        #V( E(Y_star|f_star) ) =  E( E(Y_star|f_star)**2 ) - E( E(Y_star|f_star) )**2\n\n        #E( E(Y_star|f_star) )**2\n        if predictive_mean is None:\n            predictive_mean = self.predictive_mean(mu,variance)\n        predictive_mean_sq = predictive_mean**2\n\n        #E( E(Y_star|f_star)**2 )\n        def int_pred_mean_sq(f,m,v,predictive_mean_sq):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = np.exp(exponent)\n            #If p is zero then conditional_mean**2 will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_mean(f)**2*p\n\n        scaled_exp_exp2 = [quad(int_pred_mean_sq, fmin_m, fmax,args=(mj,s2j,pm2j))[0] for mj,s2j,pm2j in zip(mu,variance,predictive_mean_sq)]\n        exp_exp2 = np.array(scaled_exp_exp2)[:,None] / normalizer\n\n        var_exp = exp_exp2 - predictive_mean_sq\n\n        # V(Y_star) = E[ V(Y_star|f_star) ] + V[ E(Y_star|f_star) ]\n        # V(Y_star) = E[ V(Y_star|f_star) ] + E(Y_star**2|f_star) - E[Y_star|f_star]**2\n        return exp_var + var_exp",
        "sampled": "```\ndef predictive_variance(self, mu, variance, predictive_mean=None, Y_metadata=None):\n    if predictive_mean is None:\n      # If no predictive mean is provided assume it's 0\n      predictive_mean = 0  \n    expectation_squared = (mu - predictive_mean) ** 2 \n    variance_squared = variance ** 2 \n    return expectation_squared + variance_squared\n```"
    },
    {
        "original": "def remove_config(self, id):\n        \"\"\"\n            Remove a config\n\n            Args:\n                id (string): Full ID of the config to remove\n\n            Returns (boolean): True if successful\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no config with that ID exists\n        \"\"\"\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True",
        "sampled": "```\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id):\n        if id in self.configs:\n            del self.configs[id]\n            return True\n        else:\n            raise docker.errors.NotFound(\"No config with that ID exists\")\n```"
    },
    {
        "original": "def get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    for item in container.view:\n        if item._moId == obj_moid:\n            return item\n    return None",
        "sampled": "```\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    \n```python    \nimport pyVmomi\n    \ndef get_service_instance():\n"
    },
    {
        "original": "def ConfigureUrls(config, external_hostname = None):\n  \"\"\"Guides the user through configuration of various URLs used by GRR.\"\"\"\n  print(\"\\n\\n-=GRR URLs=-\\n\"\n        \"For GRR to work each client has to be able to communicate with the\\n\"\n        \"server. To do this we normally need a public dns name or IP address\\n\"\n        \"to communicate with. In the standard configuration this will be used\\n\"\n        \"to host both the client facing server and the admin user interface.\\n\")\n\n  existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\", default=None)\n  existing_frontend_urns = grr_config.CONFIG.Get(\"Client.server_urls\")\n  if not existing_frontend_urns:\n    # Port from older deprecated setting Client.control_urls.\n    existing_control_urns = grr_config.CONFIG.Get(\n        \"Client.control_urls\", default=None)\n    if existing_control_urns is not None:\n      existing_frontend_urns = []\n      for existing_control_urn in existing_control_urns:\n        if not existing_control_urn.endswith(\"control\"):\n          raise RuntimeError(\"Invalid existing control URL: %s\" %\n                             existing_control_urn)\n\n        existing_frontend_urns.append(\n            existing_control_urn.rsplit(\"/\", 1)[0] + \"/\")\n\n      config.Set(\"Client.server_urls\", existing_frontend_urns)\n      config.Set(\"Client.control_urls\", [\"deprecated use Client.server_urls\"])\n\n  if not existing_frontend_urns or not existing_ui_urn:\n    ConfigureHostnames(config, external_hostname=external_hostname)\n  else:\n    print(\"Found existing settings:\\n  AdminUI URL: %s\\n  \"\n          \"Frontend URL(s): %s\\n\" % (existing_ui_urn, existing_frontend_urns))\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureHostnames(config, external_hostname=external_hostname)",
        "sampled": "```\ndef ConfigureUrls(config, external_hostname = None):\n    if external_hostname is None:\n        external_hostname = input(\"Enter the external hostname: \")\n    config[\"AdminUI.url\"] = f\"http://{external_hostname}:8000\"\n    config[\"Client.frontend_url\"] = f\"http://{external_hostname}:8080\"\n    config[\"ClientPoll.url\"] = f\"http://{external_hostname}:8081\"\n    return config\n```"
    },
    {
        "original": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n            knowledge_base):\n    \"\"\"Parse the sysctl output.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused.\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    # The KeyValueParser generates an ordered dict by default. The sysctl vals\n    # aren't ordering dependent, but there's no need to un-order it.\n    for k, v in iteritems(self.lexer.ParseToOrderedDict(stdout)):\n      key = k.replace(\".\", \"_\")\n      if len(v) == 1:\n        v = v[0]\n      result[key] = v\n    return [result]",
        "sampled": "```\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def Parse(self, cmd, args, stdout, stderr, return_val, time_taken):\n        self.knowledge_base[cmd] = {\n            'args': args,\n            'stdout': stdout,\n            'stderr': stderr,\n            'return_val': return_val,\n            'time_taken': time_taken\n        }\n```"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "sampled": "```\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as f:\n        content = f.read()\n        # parse ASF content into skeleton structure\n        # TO DO: implement parsing logic\n        pass\n```"
    },
    {
        "original": "def _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \"\"\"\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists'\n            ret['default'] = default == ruby\n            break\n\n    return ret",
        "sampled": "```\ndef _ruby_installed(ret, ruby, user=None):\n    if user is not None:\n        cmd = f\"su {user} -c 'ruby -v | grep {ruby}'\"\n    else:\n        cmd = f\"ruby -v | grep {ruby}\"\n    ret[\"installed\"] = True if os.system(cmd) == 0 else False\n```"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "sampled": "```\nclass Structure:\n    # assuming Structure class is defined somewhere\n    pass\n\nclass Element:\n    # assuming Element class is defined somewhere\n    pass\n\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        A dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    \n    projection = {}\n    \n    # iterate over"
    },
    {
        "original": "def vector_args(self, args):\n        \"\"\"\n         Yields each of the individual lane pairs from the arguments, in\n         order from most significan to least significant\n        \"\"\"\n        for i in reversed(range(self._vector_count)):\n            pieces = []\n            for vec in args:\n                pieces.append(vec[(i+1) * self._vector_size - 1 : i * self._vector_size])\n            yield pieces",
        "sampled": "```\nclass VectorArgs:\n    def __init__(self, *args):\n        self.args = args\n\n    def vector_args(self):\n        lanes = self.args.split(',')\n        lane_pairs = [lane.split('..') for lane in lanes]\n        lane_pairs.sort(key=lambda x: int(x[1]), reverse=True)\n        return [(int(pair[0]), int(pair[1])) for pair in lane_pairs]\n\n# Example usage:\nvector_args_instance = VectorArgs('0..10,"
    },
    {
        "original": "def kwargs_from_keyword(from_kwargs,to_kwargs,keyword,clean_origin=True):\n\t\"\"\"\n\tLooks for keys of the format keyword_value. \n\tAnd return a dictionary with {keyword:value} format\n\n\tParameters:\n\t-----------\n\t\tfrom_kwargs : dict\n\t\t\tOriginal dictionary\n\t\tto_kwargs : dict\n\t\t\tDictionary where the items will be appended\n\t\tkeyword : string\n\t\t\tKeyword to look for in the orginal dictionary\n\t\tclean_origin : bool\n\t\t\tIf True then the k,v pairs from the original \n\t\t\tdictionary are deleted\n\t\"\"\"\n\tfor k in list(from_kwargs.keys()):\n\t\tif '{0}_'.format(keyword) in k:\n\t\t\tto_kwargs[k.replace('{0}_'.format(keyword),'')]=from_kwargs[k]\n\t\t\tif clean_origin:\n\t\t\t\tdel from_kwargs[k]\n\treturn to_kwargs",
        "sampled": "```\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key, value in list(from_kwargs.items()):\n        if key.startswith(keyword + '_'):\n            to_kwargs[key.replace(keyword + '_', '')] = value\n            if clean_origin:\n                del from_kwargs[key]\n    return to_kwargs\n```"
    },
    {
        "original": "def _RunAction(self, rule, client_id):\n    \"\"\"Run all the actions specified in the rule.\n\n    Args:\n      rule: Rule which actions are to be executed.\n      client_id: Id of a client where rule's actions are to be executed.\n\n    Returns:\n      Number of actions started.\n    \"\"\"\n    actions_count = 0\n\n    try:\n      if self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n        logging.info(\n            \"Foreman: ignoring hunt %s on client %s: was started \"\n            \"here before\", client_id, rule.hunt_id)\n      else:\n        logging.info(\"Foreman: Starting hunt %s on client %s.\", rule.hunt_id,\n                     client_id)\n\n        # hunt_name is only used for legacy hunts.\n        if rule.hunt_name:\n          flow_cls = registry.AFF4FlowRegistry.FlowClassByName(rule.hunt_name)\n          hunt_urn = rdfvalue.RDFURN(\"aff4:/hunts/%s\" % rule.hunt_id)\n          flow_cls.StartClients(hunt_urn, [client_id])\n        else:\n          hunt.StartHuntFlowOnClient(client_id, rule.hunt_id)\n\n        actions_count += 1\n\n    # There could be all kinds of errors we don't know about when starting the\n    # hunt so we catch everything here.\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(\"Failure running foreman action on client %s: %s\",\n                        rule.hunt_id, e)\n\n    return actions_count",
        "sampled": "```\nclass ActionExecutor:\n    def __init__(self):\n        self.actions = {}\n\n    def register_action(self, action_name, func):\n        self.actions[action_name] = func\n\n    def _RunAction(self, rule, client_id):\n        count = 0\n        for action in rule['actions']:\n            if action['name'] in self.actions:\n                self.actions[action['name']](client_id)\n                count += 1\n        return count\n```"
    },
    {
        "original": "def df(self):\n        \"\"\"\n        Get data usage information.\n\n        Returns:\n            (dict): A dictionary representing different resource categories\n            and their respective data usage.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/system/df')\n        return self._result(self._get(url), True)",
        "sampled": "```\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def df(self):\n        try:\n            info = self.client.info()\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': len(self.client.volumes.list()),\n                'Networks': len(self.client.networks.list())\n            }\n        except docker.errors.APIError as e:\n            raise e\n```"
    },
    {
        "original": "def file_extension(self, category=None):\n        \"\"\"\n        :param category: audio|image|office|text|video\n        \"\"\"\n        category = category if category else self.random_element(\n            list(self.file_extensions.keys()))\n        return self.random_element(self.file_extensions[category])",
        "sampled": "```\ndef file_extension(category=None):\n    extensions = {\n        'audio': ['mp3'],\n        'image': ['jpg', 'jpeg', 'png', 'gif'],\n        'office': ['docx', 'pdf', 'pptx'],\n        'text': ['txt', 'doc'],\n        'video': ['mp4']\n    }\n    if category:\n        return extensions.get(category.lower(), [])\n    else:\n        return []\n```"
    },
    {
        "original": "def _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    \"\"\"\n    GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.\n\n    See gti_dirint signature for parameter details.\n    \"\"\"\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370, 'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)\n    disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)\n\n    dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,\n                                          False, temp_dew)\n\n    dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)\n    cos_surface_tilt = tools.cosd(surface_tilt)\n\n    # isotropic sky plus ground diffuse\n    dhi_gte_90 = (\n        (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /\n        (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))\n\n    ghi_gte_90 = dni_gte_90_proj + dhi_gte_90\n\n    return ghi_gte_90, dni_gte_90, dhi_gte_90",
        "sampled": "```\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    import numpy as np\n    \n    # Calculate air mass\n    am = pressure / (101325 * np.exp(-0.000121 * surface_tilt))\n    \n    # Calculate relative air mass\n    mam = am /"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "sampled": "```\nclass RDFURN:\n    def __init__(self, value):\n        self.value = value\n\nclass Client:\n    def ListChildren(self, urn, limit=None, age=\"NEWEST_TIME\"):\n        # assuming we have a function get_children that returns all children\n        all_children = self.get_children(urn)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children = all_children\n        elif age == \"NEWEST_TIME\":\n            filtered_children = [all_children[0]]"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "sampled": "```\ndef _convert_validators_to_mapping(validators):\n    validator_mapping = {}\n    for validator in validators:\n        key = (str(validator[\"check\"]), validator[\"comparator\"])\n        validator_mapping[key] = validator\n    return validator_mapping\n```"
    },
    {
        "original": "def InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \"\"\"Take a string as a path on a client and interpolate with client data.\n\n  Args:\n    path: A single string/unicode to be interpolated.\n    knowledge_base: An rdf_client.KnowledgeBase object.\n    users: A list of string usernames, or None.\n    path_args: A dict of additional args to use in interpolation. These take\n      precedence over any system provided variables.\n    depth: A counter for recursion depth.\n\n  Returns:\n    A single string if users is None, otherwise a list of strings.\n  \"\"\"\n\n  sys_formatters = {\n      # TODO(user): Collect this during discovery from the registry.\n      # HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\\n      # Value: SystemRoot\n      \"systemroot\": \"c:\\\\Windows\"\n  }\n\n  # Override any system formatters with path_args.\n  if path_args:\n    sys_formatters.update(path_args)\n\n  if users:\n    results = []\n    for user in users:\n      # Extract and interpolate user specific formatters.\n      user = GetUserInfo(knowledge_base, user)\n      if user:\n        formatters = dict((x.name, y) for x, y in user.ListSetFields())\n        formatters.update(sys_formatters)\n        try:\n          results.append(path.format(**formatters))\n        except KeyError:\n          pass  # We may be missing values for some users.\n    return results\n  else:\n    try:\n      path = path.format(**sys_formatters)\n    except KeyError:\n      logging.warning(\"Failed path interpolation on %s\", path)\n      return \"\"\n    if \"{\" in path and depth < 10:\n      path = InterpolatePath(\n          path,\n          knowledge_base=knowledge_base,\n          users=users,\n          path_args=path_args,\n          depth=depth + 1)\n    return path",
        "sampled": "```\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n    if users is None:\n        user_path = knowledge_base.interpolate_string(path, **path_args)\n        return user_path\n    else:\n        result = []\n        for user in users:\n            user_path_args = path_args.copy()\n            user_path_args['user'] = user\n            user_path = knowledge_base.interpolate_string(path"
    },
    {
        "original": "def _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \"\"\"Fast Fourier transform-based Gaussian kernel density estimate (KDE).\n\n    The code was adapted from https://github.com/mfouesneau/faststats\n\n    Parameters\n    ----------\n    x : Numpy array or list\n    cumulative : bool\n        If true, estimate the cdf instead of the pdf\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    xmin : float\n        Manually set lower limit.\n    xmax : float\n        Manually set upper limit.\n\n    Returns\n    -------\n    density: A gridded 1D KDE of the input points (x)\n    xmin: minimum value of x\n    xmax: maximum value of x\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax",
        "sampled": "```\nimport numpy as np\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \n     from scipy.stats import gaussian_kde\n    \n     x = np.array(x)\n     if xmin is None:\n         xmin = np.min(x)\n     if xmax is None:\n         xmax = np.max(x)\n         \n     kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method='silverman')\n     xi = np.linspace(xmin,xmax"
    },
    {
        "original": "def md_options_to_metadata(options):\n    \"\"\"Parse markdown options and return language and metadata\"\"\"\n    metadata = parse_md_code_options(options)\n\n    if metadata:\n        language = metadata[0][0]\n        for lang in _JUPYTER_LANGUAGES + ['julia', 'scheme', 'c++']:\n            if language.lower() == lang.lower():\n                return lang, dict(metadata[1:])\n\n    return None, dict(metadata)",
        "sampled": "```\ndef md_options_to_metadata(options):\n    metadata = {}\n    language = None\n    for option in options:\n        if option.startswith('language:'):\n            language = option.split('language:')[1].strip()\n        elif '=' in option:\n            key, value = option.split('=', 1)\n            metadata[key.strip()] = value.strip()\n    return language, metadata\n```"
    },
    {
        "original": "def frames(\n        self,\n        *,\n        callers: Optional[Union[str, List[str]]] = None,\n        callees: Optional[Union[str, List[str]]] = None,\n        kind: Optional[TraceKind] = None,\n        limit: Optional[int] = 10,\n    ):\n        \"\"\"Display trace frames independent of the current issue.\n\n        Parameters (all optional):\n            callers: str or list[str]            filter traces by this caller name\n            callees: str or list[str]            filter traces by this callee name\n            kind: precondition|postcondition    the type of trace frames to show\n            limit: int (default: 10)            how many trace frames to display\n                                                (specify limit=None for all)\n\n        Sample usage:\n            frames callers=\"module.function\", kind=postcondition\n\n        String filters support LIKE wildcards (%, _) from SQL:\n            % matches anything (like .* in regex)\n            _ matches 1 character (like . in regex)\n        \"\"\"\n        with self.db.make_session() as session:\n            query = (\n                session.query(\n                    TraceFrame.id,\n                    CallerText.contents.label(\"caller\"),\n                    TraceFrame.caller_port,\n                    CalleeText.contents.label(\"callee\"),\n                    TraceFrame.callee_port,\n                )\n                .filter(TraceFrame.run_id == self.current_run_id)\n                .join(CallerText, CallerText.id == TraceFrame.caller_id)\n                .join(CalleeText, CalleeText.id == TraceFrame.callee_id)\n            )\n\n            if callers is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callers, query, CallerText.contents, \"callers\"\n                )\n\n            if callees is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callees, query, CalleeText.contents, \"callees\"\n                )\n\n            if kind is not None:\n                if kind not in {TraceKind.PRECONDITION, TraceKind.POSTCONDITION}:\n                    raise UserError(\n                        \"Try 'frames kind=postcondition'\"\n                        \" or 'frames kind=precondition'.\"\n                    )\n                query = query.filter(TraceFrame.kind == kind)\n\n            if limit is not None and not isinstance(limit, int):\n                raise UserError(\"'limit' should be an int or None.\")\n\n            trace_frames = query.group_by(TraceFrame.id).order_by(\n                CallerText.contents, CalleeText.contents\n            )\n\n            total_trace_frames = trace_frames.count()\n            limit = limit or total_trace_frames\n\n            self._output_trace_frames(\n                self._group_trace_frames(trace_frames, limit), limit, total_trace_frames\n            )",
        "sampled": "```\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\nclass Trace:\n    def __init__(self):\n        self.frames = []\n\n    def add_frame(self,"
    },
    {
        "original": "def new_batch(self):\n        \"\"\"\n        Return a new batch of X and Y by taking a chunk of data from the complete X and Y\n        \"\"\"\n        i = next(self.slicer)\n        return self.X_all[i], self.Y_all[i]",
        "sampled": "```\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n        batch_Y = self.Y[start:end]\n        \n        self.index += self.batch_size"
    },
    {
        "original": "def createSummary(self, log):\n        \"\"\"\n        Create nice summary logs.\n\n        @param log: log to create summary off of.\n        \"\"\"\n        warnings = self.obs.warnings\n        errors = self.obs.errors\n\n        if warnings:\n            self.addCompleteLog('%d Warnings' % len(warnings), \"\\n\".join(warnings))\n            self.warnCount = len(warnings)\n        if errors:\n            self.addCompleteLog('%d Errors' % len(errors), \"\\n\".join(errors))\n            self.errCount = len(errors)",
        "sampled": "```\ndef createSummary(self, log):\n    \"\"\"\n    Create nice summary logs.\n\n    @param log: log to create summary off of.\n    \"\"\"\n    result = {}\n    for entry in log:\n        key = entry['type']\n        if key not in result:\n            result[key] = 1\n        else:\n            result[key] += 1\n    return result\n```"
    },
    {
        "original": "def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        \"\"\"Creates a branch using the information provided in the request.\n\n        The authenticated user must have REPO_WRITE permission for the context repository to call this resource.\n\n        :param project_key: The project matching the projectKey supplied in the resource path as shown in URL.\n        :type project_key: str\n        :param repository: Name of repository where branch is created (i.e. \"my_repo\").\n        :type repository: str\n        :param name: Name of branch to create (i.e. \"my_branch\").\n        :type name: str\n        :param start_point: Name of branch to branch from.\n        :type start_point: str\n        :param message: Branch message.\n        :type message: str\n        :return:\n            200 - application/json (repository)\n            401 - application/json (errors)\n            404 - application/json (errors)\n        :rtype: requests.Response\n        \"\"\"\n\n        url = 'rest/api/1.0/projects/{projectKey}/repos/{repository}/branches'.format(projectKey=project_key,\n                                                                                      repository=repository)\n        data = {\n            \"name\": name,\n            \"startPoint\": start_point,\n            \"message\": message\n        }\n        return self.post(url, data=data)",
        "sampled": "```\nimport requests\n\nclass GitRepository:\n    def __init__(self):\n        self.url = \"https://api.bitbucket.org/2.0\"\n    \n    def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        auth_token = (\"your_username\", \"your_password\")\n        \n\t\theaders = {\n\t\t\t\"Content-Type\": \"application/json\"\n\t\t}\n\t\t\n\t\tdata = {\n\t\t\t\"name\": name,\n\t\t\t\"start\": {\n\t\t\t"
    },
    {
        "original": "def merge_option_dicts(old_opts, new_opts):\n    \"\"\"\n    Update the old_opts option dictionary with the options defined in\n    new_opts. Instead of a shallow update as would be performed by calling\n    old_opts.update(new_opts), this updates the dictionaries of all option\n    types separately.\n\n    Given two dictionaries\n        old_opts = {'a': {'x': 'old', 'y': 'old'}}\n    and\n        new_opts = {'a': {'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    this returns a dictionary\n        {'a': {'x': 'old', 'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    \"\"\"\n    merged = dict(old_opts)\n\n    for option_type, options in new_opts.items():\n        if option_type not in merged:\n            merged[option_type] = {}\n\n        merged[option_type].update(options)\n\n    return merged",
        "sampled": "```python\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(list(old_OPTS.keys()) + list(new_OPTS.keys()))\n    \n    for key in all_keys:\n        merged[key] = {}\n        \n        if key in old_OPTS:\n            merged[key].update(old_OPTS[key])\n        \n        if key in new_OPTS:\n            merged[key].update(newOpts[key])\n    \n    return merged\n\n# Test case\nold.opts = {'a':{'x':'old','"
    },
    {
        "original": "def get(self, file_path, ref, **kwargs):\n        \"\"\"Retrieve a single file.\n\n        Args:\n            file_path (str): Path of the file to retrieve\n            ref (str): Name of the branch, tag or commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            object: The generated RESTObject\n        \"\"\"\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)",
        "sampled": "```python\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def get(self, file_path, ref, **kwargs):\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        \n        if 'sudo' in kwargs"
    },
    {
        "original": "def clean_recipe_build(self, args):\n        \"\"\"Deletes the build files of the given recipe.\n\n        This is intended for debug purposes. You may experience\n        strange behaviour or problems with some recipes if their\n        build has made unexpected state changes. If this happens, run\n        clean_builds, or attempt to clean other recipes until things\n        work again.\n        \"\"\"\n        recipe = Recipe.get_recipe(args.recipe, self.ctx)\n        info('Cleaning build for {} recipe.'.format(recipe.name))\n        recipe.clean_build()\n        if not args.no_clean_dists:\n            self.clean_dists(args)",
        "sampled": "```\ndef clean_recipe_build(self, args):\n    import os\n    import shutil\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    if os.path.exists(recipe_build_dir):\n        shutil.rmtree(recipe_build_dir)\n    print(f\"Deleted build files for recipe {args.recipe_name}\")\n```"
    },
    {
        "original": "def calculate_bias_shape(input_shape, bias_dims):\n  \"\"\"Calculate `bias_shape` based on the `input_shape` and `bias_dims`.\n\n  Args:\n    input_shape: Shape of the input being passed into the module. The leading\n        dimension is the minibatch size.\n    bias_dims: The dimensions that bias should be applied over. The remaining\n        dimensions will get broadcasted over.\n\n  Returns:\n    bias_shape: Tuple corresponding to the shape of bias Variable to create.\n\n  Raises:\n    ValueError: If the user attempts to add bias over the minibatch dimension,\n        e.g. `bias_dims=[0]`.\n  \"\"\"\n  input_rank = len(input_shape)\n  # If None, default is to use all dimensions.\n  if bias_dims is None:\n    return input_shape[1:]\n  # If empty list, use a scalar bias.\n  elif not bias_dims:\n    return ()\n  # Otherwise, calculate bias_shape from bias_dims.\n  else:\n    bias_shape = [1] * input_rank\n    # Populate bias dimensions.\n    for dim in bias_dims:\n      dim %= input_rank\n      if dim == 0:\n        raise ValueError(\"Cannot apply bias across the minibatch dimension.\")\n      bias_shape[dim] = input_shape[dim]\n    # Strip leading unit dimensions.\n    start = input_rank\n    for dim in xrange(1, input_rank):\n      if bias_shape[dim] != 1:\n        start = dim\n        break\n    return tuple(bias_shape[start:])",
        "sampled": "```\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    bias_shape = [input_shape[i] if i in bias_dims else 1 for i in range(len(input_shape))]\n    return tuple(bias_shape)\n```"
    },
    {
        "original": "def read_metadata(text, ext):\n    \"\"\"Return the header metadata\"\"\"\n    ext = '.' + ext.split('.')[-1]\n    lines = text.splitlines()\n\n    if ext in ['.md', '.Rmd']:\n        comment = ''\n    else:\n        comment = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment', '#')\n\n    metadata, _, _, _ = header_to_metadata_and_cell(lines, comment, ext)\n    if ext in ['.r', '.R'] and not metadata:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, \"#'\", ext)\n\n    return metadata",
        "sampled": "```\ndef read_metadata(text, ext):\n    if ext == 'txt':\n        lines = text.split('\\n')\n        metadata = {}\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                metadata[key.strip()] = value.strip()\n        return metadata\n    else:\n        return {}\n```"
    },
    {
        "original": "def _is_process_filtered(self, process, key=None):\n        \"\"\"Return True if the process[key] should be filtered according to the current filter\"\"\"\n        if key is None:\n            key = self.filter_key\n        try:\n            # If the item process[key] is a list, convert it to a string\n            # in order to match it with the current regular expression\n            if isinstance(process[key], list):\n                value = ' '.join(process[key])\n            else:\n                value = process[key]\n        except KeyError:\n            # If the key did not exist\n            return False\n        try:\n            return self._filter_re.match(value) is None\n        except (AttributeError, TypeError):\n            # AttributeError\n            # Filter processes crashs with a bad regular expression pattern (issue #665)\n            # TypeError\n            # Filter processes crashs if value is None (issue #1105)\n            return False",
        "sampled": "```\ndef _is_process_filtered(self, process, key=None):\n    \"\"\"\n    Return True if the process[key] should be filtered according to the current filter\n    \"\"\"\n    # Assuming self.filter is set elsewhere in your class\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n\n    if key is None:\n        return any(str(d) in str(self.filter) for d in process.values())\n    else:\n        return str(process.get(key)) in str"
    },
    {
        "original": "def modulation_type(self, value: int):\n        \"\"\"\n        0 - \"ASK\", 1 - \"FSK\", 2 - \"PSK\", 3 - \"APSK (QAM)\"\n\n        :param value:\n        :return:\n        \"\"\"\n        if self.__modulation_type != value:\n            self.__modulation_type = value\n            self._qad = None\n\n            self.modulation_type_changed.emit(self.__modulation_type)\n            if not self.block_protocol_update:\n                self.protocol_needs_update.emit()",
        "sampled": "```\ndef modulation_type(self, value: int):\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    if 0 <= value <= 3:\n        return modulation_types[value]\n    else:\n        return None\n```"
    },
    {
        "original": "def load(self, fname):\n        \"\"\"\n        Load signatures from a file.\n\n        @fname - Path to signature file.\n\n        Returns None.\n        \"\"\"\n        # Magic files must be ASCII, else encoding issues can arise.\n        fp = open(fname, \"r\")\n        lines = fp.readlines()\n        self.parse(lines)\n        fp.close()",
        "sampled": "```\ndef load(self, fname):\n    with open(fname, 'r') as f:\n        self.signatures = [line.strip() for line in f.readlines()]\n```"
    },
    {
        "original": "def enter_room(self, sid, room, namespace=None):\n        \"\"\"Enter a room.\n\n        The only difference with the :func:`socketio.Server.enter_room` method\n        is that when the ``namespace`` argument is not given the namespace\n        associated with the class is used.\n        \"\"\"\n        return self.server.enter_room(sid, room,\n                                      namespace=namespace or self.namespace)",
        "sampled": "```python\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid, room, namespace=None):\n        if namespace is None:\n            namespace = self.__class__.__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n```"
    },
    {
        "original": "def _get_args(self, kwargs):\n        \"\"\"\n        Discard all keywords which aren't function-specific from the kwargs.\n\n        :param kwargs:\n        :return:\n        \"\"\"\n        _args = list()\n        _kwargs = salt.utils.args.clean_kwargs(**kwargs)\n\n        return _args, _kwargs",
        "sampled": "```\ndef _get_args(self, **):\n    return {key: value for key, value in kwargs.items() if hasattr(self.__class__, key)}\n```"
    },
    {
        "original": "def edit_caption(\n        self,\n        caption: str,\n        parse_mode: str = \"\",\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *edit_caption* of :obj:`Message <pyrogram.Message>`\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.edit_message_caption(\n                chat_id=message.chat.id,\n                message_id=message.message_id,\n                caption=\"hello\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.edit_caption(\"hello\")\n\n        Args:\n            caption (``str``):\n                New caption of the message.\n\n            parse_mode (``str``, *optional*):\n                Use :obj:`MARKDOWN <pyrogram.ParseMode.MARKDOWN>` or :obj:`HTML <pyrogram.ParseMode.HTML>`\n                if you want Telegram apps to show bold, italic, fixed-width text or inline URLs in your message.\n                Defaults to Markdown.\n\n            reply_markup (:obj:`InlineKeyboardMarkup`, *optional*):\n                An InlineKeyboardMarkup object.\n\n        Returns:\n            On success, the edited :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        return self._client.edit_message_caption(\n            chat_id=self.chat.id,\n            message_id=self.message_id,\n            caption=caption,\n            parse_mode=parse_mode,\n            reply_markup=reply_markup\n        )",
        "sampled": "```\nfrom pyogram import Client\nfrom pyogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self, caption: str, parse_mode: str = \"\", reply_markup: Union[InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id, message_id=self.message_id,c"
    },
    {
        "original": "def _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n        \"\"\"Given M = sum(kron(a_i, b_i)), returns M' = sum(kron(b_i, a_i)).\"\"\"\n        result = np.array([[0] * 4] * 4, dtype=np.complex128)\n        order = [0, 2, 1, 3]\n        for i in range(4):\n            for j in range(4):\n                result[order[i], order[j]] = mat4x4[i, j]\n        return result",
        "sampled": "```\nimport numpy as np\nfrom scipy.linalg import kron\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    # Get the size of sub-matrices\n    n = int(np.sqrt(mat4x4.shape[0]))\n\n    # Reshape mat4x4 into 3D array (n*n matrices)\n    mats = mat4x4.reshape(n, n, n, n)\n\n    # Flip kron order by swapping last two axes"
    },
    {
        "original": "def pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        \"\"\"\n        Prints scheduler for user to read.\n        \"\"\"\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            # Print a message indicating what the below schedule means\n            print(\n                \"Table consists of tuples of \"\n                \"(num configs, num_resources_per_config) \"\n                \"which specify how many configs to run and \"\n                \"for how many epochs. \"\n            )\n            print(\n                \"Each bracket starts with a list of random \"\n                \"configurations which is successively halved \"\n                \"according the schedule.\"\n            )\n            print(\n                \"See the Hyperband paper \"\n                \"(https://arxiv.org/pdf/1603.06560.pdf) for more details.\"\n            )\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")",
        "sampled": "```\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\" Hyper-Band Parameters: \")\n    print(\"     - Try {} possible sets of hyperparameters\".format(len(hyperband_schedule)))\n    if describe_hyperband:\n        print(\"     - With an average of {} iterations per trial\".format(sum([len(trial) for trial in hyperband_schedule]) / len(hyperband_schedule)))\n    print(\"\")\n    \n    max_r = max(max(trial)"
    },
    {
        "original": "def shorthand(self):\n        \"\"\"Return the 6-tuple (a,b,c,d,e,f) that describes this matrix\"\"\"\n        return (self.a, self.b, self.c, self.d, self.e, self.f)",
        "sampled": "```\nclass Matrix:\n    def __init__(self, a, b, c, d, e, f):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        self.e = e\n        self.f = f\n\n    def shorthand(self):\n        return (self.a, self.b, self.c, self.d, self.e, self.f)\n```"
    },
    {
        "original": "def is_cyclic(graph):\n    \"\"\"\n    Return True if the directed graph g has a cycle. The directed graph\n    should be represented as a dictionary mapping of edges for each node.\n    \"\"\"\n    path = set()\n\n    def visit(vertex):\n        path.add(vertex)\n        for neighbour in graph.get(vertex, ()):\n            if neighbour in path or visit(neighbour):\n                return True\n        path.remove(vertex)\n        return False\n\n    return any(visit(v) for v in graph)",
        "sampled": "```python\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited:\n            if dfs"
    },
    {
        "original": "def get_header_items(self):\n        \"\"\"\n        Get an iterable list of key/value pairs representing headers.\n\n        This function provides Python 2/3 compatibility as related to the\n        parsing of request headers. Python 2.7 is not compliant with\n        RFC 3875 Section 4.1.18 which requires multiple values for headers\n        to be provided. This function will return a matching list regardless\n        of Python version. It can be removed once Python 2.7 support\n        is dropped.\n\n        :return: List of tuples containing header hey/value pairs\n        \"\"\"\n        if PY2:\n            # For Python 2, process the headers manually according to\n            # W3C RFC 2616 Section 4.2.\n            items = []\n            for header in self.headers.headers:\n                # Remove \"\\n\\r\" from the header and split on \":\" to get\n                # the field name and value.\n                key, value = header[0:-2].split(\":\", 1)\n                # Add the key and the value once stripped of leading\n                # white space. The specification allows for stripping\n                # trailing white space but the Python 3 code does not\n                # strip trailing white space. Therefore, trailing space\n                # will be left as is to match the Python 3 behavior.\n                items.append((key, value.lstrip()))\n        else:\n            items = self.headers.items()\n\n        return items",
        "sampled": "```python\nfrom email import policy\nfrom email.parser import BytesParser\n\ndef get_header_items(self):\n    \"\"\"Get an iterable list of key/value pairs representing headers.\"\"\"\n    parser = BytesParser(policy=policy.default)\n    msg = parser.parsestr(self.request_text)\n    return [(k, v) for k, v in msg.items()]\n```"
    },
    {
        "original": "def is_constructing_scv(self) -> bool:\n        \"\"\" Checks if the unit is an SCV that is currently building. \"\"\"\n        return self.orders and self.orders[0].ability.id in {\n            AbilityId.TERRANBUILD_ARMORY,\n            AbilityId.TERRANBUILD_BARRACKS,\n            AbilityId.TERRANBUILD_BUNKER,\n            AbilityId.TERRANBUILD_COMMANDCENTER,\n            AbilityId.TERRANBUILD_ENGINEERINGBAY,\n            AbilityId.TERRANBUILD_FACTORY,\n            AbilityId.TERRANBUILD_FUSIONCORE,\n            AbilityId.TERRANBUILD_GHOSTACADEMY,\n            AbilityId.TERRANBUILD_MISSILETURRET,\n            AbilityId.TERRANBUILD_REFINERY,\n            AbilityId.TERRANBUILD_SENSORTOWER,\n            AbilityId.TERRANBUILD_STARPORT,\n            AbilityId.TERRANBUILD_SUPPLYDEPOT,\n        }",
        "sampled": "```\nclass Unit:\n    def __init__(self, is_scv, is_building):\n        self.is_scv = is_scv\n        self.is_building = is_building\n\n    def is_constructing_scv(self) -> bool:\n        return self.is_scv and self.is_building\n```"
    },
    {
        "original": "def set_form_field_order(form, field_order):\n    \"\"\"\n    This function is a verbatim copy of django.forms.Form.order_fields() to\n    support field ordering below Django 1.9.\n\n    field_order is a list of field names specifying the order. Append fields\n    not included in the list in the default order for backward compatibility\n    with subclasses not overriding field_order. If field_order is None, keep\n    all fields in the order defined in the class. Ignore unknown fields in\n    field_order to allow disabling fields in form subclasses without\n    redefining ordering.\n    \"\"\"\n    if field_order is None:\n        return\n    fields = OrderedDict()\n    for key in field_order:\n        try:\n            fields[key] = form.fields.pop(key)\n        except KeyError:  # ignore unknown fields\n            pass\n    fields.update(form.fields)  # add remaining fields in original order\n    form.fields = fields",
        "sampled": "```\ndef set_form_field_order(form, field_order):\n    if field_order is None:\n        return\n    new_fields = []\n    for field_name in field_order:\n        if hasattr(form, field_name):\n            new_fields.append((field_name, getattr(form, field_name)))\n    for field_name, field in form.fields.items():\n        if field_name not in field_order:\n            new_fields.append((field_name, field))\n    form.fields = OrderedDict(new_fields)\n```"
    },
    {
        "original": "def _region_base(self, region):\n        \"\"\"\n        Get the base address of a memory region.\n\n        :param str region: ID of the memory region\n        :return: Address of the memory region\n        :rtype: int\n        \"\"\"\n\n        if region == 'global':\n            region_base_addr = 0\n        elif region.startswith('stack_'):\n            region_base_addr = self._stack_region_map.absolutize(region, 0)\n        else:\n            region_base_addr = self._generic_region_map.absolutize(region, 0)\n\n        return region_base_addr",
        "sampled": "```\ndef _region_base(self, region):\n    # Assuming there's a dictionary that maps region IDs to their base addresses\n    region_bases = {\n        'region1': 0x1000,\n        'region2': 0x2000,\n        'region3': 0x3000,\n        # Add more regions as needed\n    }\n    return region_bases.get(region, None)\n```"
    },
    {
        "original": "def init_client(client_id):\n    \"\"\"Initialse a driver for client and store for future reference\n    \n    @param client_id: ID of client user\n    @return whebwhatsapi object\n    \"\"\"\n    if client_id not in drivers:\n        drivers[client_id] = init_driver(client_id)\n    return drivers[client_id]",
        "sampled": "```\nimport whatsapp\n\ndef init_client(client_id):\n    \"\"\"Initialse a driver for client and store for future reference\n    \n    @param client_id: ID of client user\n    @return whatsappapi object\n    \"\"\"\n    whatsapp_api = whatsapp.Client()\n    whatsapp_api.authenticate(client_id)\n    return whatsapp_api\n```"
    },
    {
        "original": "def cell(self, row_idx, col_idx):\n        \"\"\"\n        Return |_Cell| instance correponding to table cell at *row_idx*,\n        *col_idx* intersection, where (0, 0) is the top, left-most cell.\n        \"\"\"\n        cell_idx = col_idx + (row_idx * self._column_count)\n        return self._cells[cell_idx]",
        "sampled": "```\nclass Cell:\n    def __init__(self, row_idx, col_idx):\n        self.row_idx = row_idx\n        self.col_idx = col_idx\n\nclass Table:\n    def __init__(self, rows, cols):\n        self.rows = rows\n        self.cols = cols\n        self.cells = [[Cell(i, j) for j in range(cols)] for i in range(rows)]\n\n    def cell(self, row_idx, col_idx):\n        return self.cells[row_idx]["
    },
    {
        "original": "def get(self, card_id):\n        \"\"\"\n        \u67e5\u8be2\u5361\u5238\u8be6\u60c5\n        \"\"\"\n        result = self._post(\n            'card/get',\n            data={\n                'card_id': card_id\n            },\n            result_processor=lambda x: x['card']\n        )\n        return result",
        "sampled": "```\nclass CardSystem:\n    def __init__(self):\n        self.cards = {}\n\n    def get(self, card_id):\n        return self.cards.get(card_id)\n```"
    },
    {
        "original": "def set_not_found_handler(self, handler, version=None):\n        \"\"\"Sets the not_found handler for the specified version of the api\"\"\"\n        if not self.not_found_handlers:\n            self._not_found_handlers = {}\n\n        self.not_found_handlers[version] = handler",
        "sampled": "```\nclass API:\n    def __init__(self):\n        self.not_found_handlers = {}\n\n    def set_not_found_handler(self, handler, version=None):\n        if version is None:\n            self.not_found_handlers['default'] = handler\n        else:\n            self.not_found_handlers[version] = handler\n```"
    },
    {
        "original": "def measure_each(*qubits: raw_types.Qid,\n                 key_func: Callable[[raw_types.Qid], str] = str\n                 ) -> List[gate_operation.GateOperation]:\n    \"\"\"Returns a list of operations individually measuring the given qubits.\n\n    The qubits are measured in the computational basis.\n\n    Args:\n        *qubits: The qubits to measure.\n        key_func: Determines the key of the measurements of each qubit. Takes\n            the qubit and returns the key for that qubit. Defaults to str.\n\n    Returns:\n        A list of operations individually measuring the given qubits.\n    \"\"\"\n    return [MeasurementGate(1, key_func(q)).on(q) for q in qubits]",
        "sampled": "```\nfrom typing import Callable, List\nfrom cirq import GateOperation, Qid\n\ndef measure_each(*qubits: Qid, key_func: Callable[[Qid], str] = str) -> List[GateOperation]:\n    return [GateOperation('measure', qubit) for qubit in qubits]\n```"
    },
    {
        "original": "def participants(self, **kwargs):\n        \"\"\"List the participants.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of participants\n        \"\"\"\n\n        path = '%s/%s/participants' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)",
        "sampled": "```\nclass RESTObjectList:\n    def __init__(self, items):\n        self.items = items\n\nclass GitlabAuthenticationError(Exception):\n    pass\n\nclass GitlabListError(Exception):\n    pass\n\nclass Participants:\n    def __init__(self, server):\n        self.server = server\n\n    def participants(self, **kwargs):\n        all_items = self.server.get_all_items()\n        if 'all' in kwargs and kwargs['all']:\n            return RESTObjectList(all_items)\n        elif"
    },
    {
        "original": "def revdep_rebuild(lib=None):\n    \"\"\"\n    Fix up broken reverse dependencies\n\n    lib\n        Search for reverse dependencies for a particular library rather\n        than every library on the system. It can be a full path to a\n        library or basic regular expression.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gentoolkit.revdep_rebuild\n    \"\"\"\n    cmd = 'revdep-rebuild -i --quiet --no-progress'\n    if lib is not None:\n        cmd += ' --library={0}'.format(lib)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0",
        "sampled": "```\nimport re\nimport portage\n\ndef revdep_rebuild(lib=None):\n    \"\"\"\n    Fix up broken reverse dependencies\n\n    lib\n        Search for reverse dependencies for a particular library rather\n        than every library on the system. It can be a full path to a\n        library or basic regular expression.\n    \"\"\"\n    if lib:\n        # If lib is provided, search for reverse dependencies for that particular library\n        revdeps = portage.dep.get_revdeps(lib, mydb"
    },
    {
        "original": "def _canonicalize_name(prefix, qvm_type, noisy):\n    \"\"\"Take the output of _parse_name to create a canonical name.\n    \"\"\"\n    if noisy:\n        noise_suffix = '-noisy'\n    else:\n        noise_suffix = ''\n\n    if qvm_type is None:\n        qvm_suffix = ''\n    elif qvm_type == 'qvm':\n        qvm_suffix = '-qvm'\n    elif qvm_type == 'pyqvm':\n        qvm_suffix = '-pyqvm'\n    else:\n        raise ValueError(f\"Unknown qvm_type {qvm_type}\")\n\n    name = f'{prefix}{noise_suffix}{qvm_suffix}'\n    return name",
        "sampled": "```\ndef _canonicalize_name(prefix, qvm_type, noisy):\n    if noisy:\n        return f\"{prefix}_{qvm_type}_noisy\"\n    else:\n        return f\"{prefix}_{qvm_type}\"\n```"
    },
    {
        "original": "def _line(self, text, indent=0):\n        \"\"\"Write 'text' word-wrapped at self.width characters.\"\"\"\n        leading_space = '  ' * indent\n        while len(leading_space) + len(text) > self.width:\n            # The text is too wide; wrap if possible.\n\n            # Find the rightmost space that would obey our width constraint and\n            # that's not an escaped space.\n            available_space = self.width - len(leading_space) - len(' $')\n            space = available_space\n            while True:\n                space = text.rfind(' ', 0, space)\n                if (space < 0 or\n                    self._count_dollars_before_index(text, space) % 2 == 0):\n                    break\n\n            if space < 0:\n                # No such space; just use the first unescaped space we can find.\n                space = available_space - 1\n                while True:\n                    space = text.find(' ', space + 1)\n                    if (space < 0 or\n                        self._count_dollars_before_index(text, space) % 2 == 0):\n                        break\n            if space < 0:\n                # Give up on breaking.\n                break\n\n            self.output.write(leading_space + text[0:space] + ' $\\n')\n            text = text[space+1:]\n\n            # Subsequent lines are continuations, so indent them.\n            leading_space = '  ' * (indent+2)\n\n        self.output.write(leading_space + text + '\\n')",
        "sampled": "```\ndef _line(self, text, indent=0):\n    width = 80  # assuming the width is 80 characters\n    words = text.split()\n    line = ' ' * indent\n    for word in words:\n        if len(line) + len(word) + 1 > width:\n            yield line\n            line = ' ' * indent + word\n        else:\n            line += ' ' + word\n    yield line\n```"
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            ifrealname = i['interface_name'].split(':')[0]\n            # Convert rate in bps ( to be able to compare to interface speed)\n            bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n            bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n            # Decorate the bitrate with the configuration file thresolds\n            alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n            alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')\n            # If nothing is define in the configuration file...\n            # ... then use the interface speed (not available on all systems)\n            if alert_rx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_rx = self.get_alert(current=bps_rx,\n                                          maximum=i['speed'],\n                                          header='rx')\n            if alert_tx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_tx = self.get_alert(current=bps_tx,\n                                          maximum=i['speed'],\n                                          header='tx')\n            # then decorates\n            self.views[i[self.get_key()]]['rx']['decoration'] = alert_rx\n            self.views[i[self.get_key()]]['tx']['decoration'] = alert_tx",
        "sampled": "I'm happy to help! However, I notice that the problem description, input, and output specifications are missing. Could you please provide the complete problem statement in Markdown format? I'll be happy to analyze the problem and provide a Python solution for the challenge."
    },
    {
        "original": "def get_feedback(self, feedback_id, model=None, **kwargs):\n        \"\"\"\n        List a specified feedback entry.\n\n        Lists a feedback entry with a specified `feedback_id`.\n\n        :param str feedback_id: A string that specifies the feedback entry to be included\n        in the output.\n        :param str model: The analysis model to be used by the service. For the **Element\n        classification** and **Compare two documents** methods, the default is\n        `contracts`. For the **Extract tables** method, the default is `tables`. These\n        defaults apply to the standalone methods as well as to the methods' use in\n        batch-processing requests.\n        :param dict headers: A `dict` containing the request headers\n        :return: A `DetailedResponse` containing the result, headers and HTTP status code.\n        :rtype: DetailedResponse\n        \"\"\"\n\n        if feedback_id is None:\n            raise ValueError('feedback_id must be provided')\n\n        headers = {}\n        if 'headers' in kwargs:\n            headers.update(kwargs.get('headers'))\n        sdk_headers = get_sdk_headers('compare-comply', 'V1', 'get_feedback')\n        headers.update(sdk_headers)\n\n        params = {'version': self.version, 'model': model}\n\n        url = '/v1/feedback/{0}'.format(*self._encode_path_vars(feedback_id))\n        response = self.request(\n            method='GET',\n            url=url,\n            headers=headers,\n            params=params,\n            accept_json=True)\n        return response",
        "sampled": "```\nfrom ibm_watson import DetailedResponse\n\nclass FeedbackService:\n    def get_feedback(self, feedback_id, model=None, **kwargs):\n        # Implement the logic to list a specified feedback entry\n        # For demonstration purposes, assume the feedback entry is stored in a dictionary\n        feedback_entries = {\n            \"feedback1\": {\"id\": \"feedback1\", \"content\": \"This is feedback 1\"},\n            \"feedback2\": {\"id\": \"feedback2\", \"content"
    },
    {
        "original": "def probably_identical(self):\n        \"\"\"\n        :returns: Whether or not these two functions are identical.\n        \"\"\"\n        if len(self._unmatched_blocks_from_a | self._unmatched_blocks_from_b) > 0:\n            return False\n        for (a, b) in self._block_matches:\n            if not self.blocks_probably_identical(a, b):\n                return False\n        return True",
        "sampled": "```\ndef probably_identical(self, f, g):\n    \"\"\"\n    :param f: function 1\n    :param g: function 2\n    :returns: Whether or not these two functions are identical.\n    \"\"\"\n    import random\n    for _ in range(100):  # arbitrary number of tests\n        x = random.random()  # generate a random input\n        if f(x) != g(x):\n            return False\n    return True\n```"
    },
    {
        "original": "def logpdf_link(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Log Likelihood function given inverse link of f.\n\n        .. math::\n            \\\\ln p(y_{i}|\\\\lambda(f_{i})) = y_{i}\\\\log\\\\lambda(f_{i}) + (1-y_{i})\\\\log (1-f_{i})\n\n        :param inv_link_f: latent variables inverse link of f.\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata must contain 'trials'\n        :returns: log likelihood evaluated at points inverse link of f.\n        :rtype: float\n        \"\"\"\n        N = Y_metadata['trials']\n        np.testing.assert_array_equal(N.shape, y.shape)\n\n        nchoosey = special.gammaln(N+1) - special.gammaln(y+1) - special.gammaln(N-y+1)\n        \n        Ny = N-y\n        t1 = np.zeros(y.shape)\n        t2 = np.zeros(y.shape)\n        t1[y>0] = y[y>0]*np.log(inv_link_f[y>0])\n        t2[Ny>0] = Ny[Ny>0]*np.log(1.-inv_link_f[Ny>0])\n        \n        return nchoosey + t1 + t2",
        "sampled": "```\ndef logpdf_link(self, inv_link_f, y, Y_metadata=None):\n    return np.sum(y * np.log(inv_link_f) + (1 - y) * np.log(1 - inv_link_f))\n```"
    },
    {
        "original": "def __load_compose_definitions(path, definition):\n    \"\"\"\n    Will load the compose file located at path\n    Then determines the format/contents of the sent definition\n\n    err or results are only set if there were any\n\n    :param path:\n    :param definition:\n    :return tuple(compose_result, loaded_definition, err):\n    \"\"\"\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return None, None, err\n    if isinstance(definition, dict):\n        return compose_result, definition, None\n    elif definition.strip().startswith('{'):\n        try:\n            loaded_definition = json.deserialize(definition)\n        except json.DeserializationError as jerr:\n            msg = 'Could not parse {0} {1}'.format(definition, jerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    else:\n        try:\n            loaded_definition = yaml.load(definition)\n        except yaml.YAMLError as yerr:\n            msg = 'Could not parse {0} {1}'.format(definition, yerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    return compose_result, loaded_definition, None",
        "sampled": "```\nimport yaml\nimport os\n\ndef __load_compose_definitions(path, definition):\n    compose_result = None\n    loaded_definition = None\n    err = None\n\n    if os.path.exists(path):\n        try:\n            with open(path, 'r') as file:\n                compose_result = yaml.safe_load(file)\n            loaded_definition = definition\n        except Exception as e:\n            err = str(e)\n    else:\n        err = \"File not found\"\n\n    return compose_result, loaded"
    },
    {
        "original": "def convert_date(obj):\n    \"\"\"Returns a DATE column as a date object:\n\n      >>> date_or_None('2007-02-26')\n      datetime.date(2007, 2, 26)\n\n    Illegal values are returned as None:\n\n      >>> date_or_None('2007-02-31') is None\n      True\n      >>> date_or_None('0000-00-00') is None\n      True\n\n    \"\"\"\n    if not PY2 and isinstance(obj, (bytes, bytearray)):\n        obj = obj.decode('ascii')\n    try:\n        return datetime.date(*[ int(x) for x in obj.split('-', 2) ])\n    except ValueError:\n        return obj",
        "sampled": "```\nfrom datetime import datetime\n\ndef date_or_None(obj):\n    try:\n        return datetime.strptime(obj, '%Y-%m-%d').date()\n    except ValueError:\n        return None\n```"
    },
    {
        "original": "async def build_get_cred_def_request(submitter_did: Optional[str],\n                                     id_: str) -> str:\n    \"\"\"\n   Builds a GET_CRED_DEF request. Request to get a credential definition (in particular, public key),\n   that Issuer creates for a particular Credential Schema.\n\n    :param submitter_did: (Optional) DID of the read request sender (if not provided then default Libindy DID will be used).\n    :param id_: Credential Definition Id in ledger.\n    :return: Request result as json.\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"build_get_cred_def_request: >>> submitter_did: %r, id: %r\",\n                 submitter_did,\n                 id_)\n\n    if not hasattr(build_get_cred_def_request, \"cb\"):\n        logger.debug(\"build_get_cred_def_request: Creating callback\")\n        build_get_cred_def_request.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_submitter_did = c_char_p(submitter_did.encode('utf-8')) if submitter_did is not None else None\n    c_id = c_char_p(id_.encode('utf-8'))\n\n    request_json = await do_call('indy_build_get_cred_def_request',\n                                 c_submitter_did,\n                                 c_id,\n                                 build_get_cred_def_request.cb)\n\n    res = request_json.decode()\n    logger.debug(\"build_get_cred_def_request: <<< res: %r\", res)\n    return res",
        "sampled": "```\nimport json\nfrom typing import Optional\n\nasync def build_get_cred_def_request(submitter_did: Optional[str], id_: str) -> str:\n    request = {\n        \"operation\": {\n            \"type\": \"105\",\n            \"dest\": id_\n        }\n    }\n    if submitter_did:\n        request[\"operation\"][\"sender_did\"] = submitter_did\n    return json.dumps(request)\n```"
    },
    {
        "original": "def _get_restartcheck_result(errors):\n    \"\"\"\n    Return restartcheck result and append errors (if any) to ``errors``\n    \"\"\"\n    rs_result = __salt__['restartcheck.restartcheck'](verbose=False)\n    if isinstance(rs_result, dict) and 'comment' in rs_result:\n        errors.append(rs_result['comment'])\n    return rs_result",
        "sampled": "```\ndef _get_restartcheck_result(errors):\n    # TO DO: implement the logic to get the restartcheck result\n    # For now, let's assume the result is always True\n    result = True\n    # Append some dummy error to the errors list\n    errors.append(\"Dummy error\")\n    return result\n```"
    },
    {
        "original": "def check_enable_mode(self, check_string=\"\"):\n        \"\"\"Check if in enable mode. Return boolean.\n\n        :param check_string: Identification of privilege mode from device\n        :type check_string: str\n        \"\"\"\n        self.write_channel(self.RETURN)\n        output = self.read_until_prompt()\n        return check_string in output",
        "sampled": "```\ndef check_enable_mode(self, check_string=\"\"):\n    return \"enable\" in check_string.lower()\n```"
    },
    {
        "original": "def _check_rot_sym(self, axis):\n        \"\"\"\n        Determines the rotational symmetry about supplied axis.  Used only for\n        symmetric top molecules which has possible rotational symmetry\n        operations > 2.\n        \"\"\"\n        min_set = self._get_smallest_set_not_on_axis(axis)\n        max_sym = len(min_set)\n        for i in range(max_sym, 0, -1):\n            if max_sym % i != 0:\n                continue\n            op = SymmOp.from_axis_angle_and_translation(axis, 360 / i)\n            rotvalid = self.is_valid_op(op)\n            if rotvalid:\n                self.symmops.append(op)\n                self.rot_sym.append((axis, i))\n                return i\n        return 1",
        "sampled": "```\ndef _check_rot_sym(self, axis):\n    # TO DO: implement the logic to determine the rotational symmetry about the supplied axis\n    pass\n```"
    },
    {
        "original": "def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst",
        "sampled": "```\ndef _get_values(self, lst, list_columns):\n    return [{col: getattr(item, col) for col in list_columns} for item in lst]\n```"
    },
    {
        "original": "def post_message(message, chat_id=None, token=None):\n    \"\"\"\n    Send a message to a Telegram chat.\n\n    :param message: The message to send to the Telegram chat.\n    :param chat_id: (optional) The Telegram chat id.\n    :param token:   (optional) The Telegram API token.\n    :return:        Boolean if message was sent successfully.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' telegram.post_message message=\"Hello Telegram!\"\n\n    \"\"\"\n    if not chat_id:\n        chat_id = _get_chat_id()\n\n    if not token:\n        token = _get_token()\n\n    if not message:\n        log.error('message is a required option.')\n\n    return _post_message(message=message, chat_id=chat_id, token=token)",
        "sampled": "```\nimport requests\n\ndef post_message(message, chat_id=None, token=None):\n    if not chat_id or not token:\n        raise ValueError(\"Both chat_id and token are required\")\n    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n    data = {\"chat_id\": chat_id, \"text\": message}\n    response = requests.post(url, json=data)\n    return response.status_code == 200\n```"
    },
    {
        "original": "def topics(self):\n        \"\"\"\n        Get the set of topics that can be extracted from this report.\n        \"\"\"\n        return set(node.tag for node in self.root.iter() if node.attrib)",
        "sampled": "```\nclass Report:\n    def __init__(self, text):\n        self.text = text\n\n    def topics(self):\n        \"\"\"\n        Get the set of topics that can be extracted from this report.\n        \"\"\"\n        topics = set()\n        for word in self.text.split():\n            if word.startswith('#'):\n                topics.add(word[1:])\n        return topics\n```"
    },
    {
        "original": "def get_data_home(path=None):\n    \"\"\"\n    Return the path of the Yellowbrick data directory. This folder is used by\n    dataset loaders to avoid downloading data several times.\n\n    By default, this folder is colocated with the code in the install directory\n    so that data shipped with the package can be easily located. Alternatively\n    it can be set by the YELLOWBRICK_DATA environment variable, or\n    programmatically by giving a folder path. Note that the '~' symbol is\n    expanded to the user home directory, and environment variables are also\n    expanded when resolving the path.\n    \"\"\"\n    if path is None:\n        path = os.environ.get('YELLOWBRICK_DATA', FIXTURES)\n\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    return path",
        "sampled": "```\nimport os\n\ndef get_data_home(path=None):\n    if path is None:\n        path = os.environ.get('YELLOWBRICK_DATA')\n    if path is None:\n        path = os.path.join(os.path.dirname(__file__), 'data')\n    path = os.path.expanduser(path)\n    path = os.path.expandvars(path)\n    return os.path.abspath(path)\n```"
    },
    {
        "original": "def get_monolayer(self):\n        \"\"\"\n        Returns the primitive unit surface area density of the\n            adsorbate.\n        \"\"\"\n\n        unit_a = self.get_unit_primitive_area\n        Nsurfs = self.Nsurfs_ads_in_slab\n        Nads = self.Nads_in_slab\n        return Nads / (unit_a * Nsurfs)",
        "sampled": "```\ndef get_monolayer(self, molecular_weight, langmuir_surface_area, langmuir_cross_sectional_area):\n    return (molecular_weight / langmuir_surface_area) * langmuir_cross_sectional_area\n```"
    },
    {
        "original": "def string_match(sf,regex,field=2):\n    \"\"\"\n    Return the geometry and attributes of a shapefile whose fields match a regular expression given\n\n    :param sf: shapefile\n    :type sf: shapefile object\n    :regex: regular expression to match\n    :type regex: string\n    :field: field number to be matched with the regex\n    :type field: integer\n    \"\"\"\n    index = []\n    shape_records = []\n    for rec in enumerate(sf.shapeRecords()):\n        m = re.search(regex,rec[1].record[field])\n        if m is not None:\n            index.append(rec[0])\n            shape_records.append(rec[1])\n    return index,shape_records",
        "sampled": "```\nimport shapefile\nimport re\n\ndef string_match(sf, regex, field=2):\n    matched_shapes = []\n    for shape in sf.shapeRecords():\n        if re.search(regex, shape.record[field-1]):\n            matched_shapes.append((shape.shape, shape.record))\n    return matched_shapes\n```"
    },
    {
        "original": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)",
        "sampled": "```\nclass EventService:\n    def __init__(self):\n        self.subscribers = {}\n\n    def add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n        if not all(self._is_valid_filter(filter) for filter in subscriptions):\n            raise InvalidFilterError(\"One of the filters in the subscriptions is invalid.\")\n        self.subscribers[connection_id] = {\"subscriptions\": subscriptions, \"last_known_block_id\": last_known_block_id}\n\n    def _is_valid_filter(self"
    },
    {
        "original": "def get_summed_cohp_by_label_and_orbital_list(self, label_list, orbital_list, divisor=1):\n        \"\"\"\n        Returns a COHP object that includes a summed COHP divided by divisor\n\n        Args:\n            label_list: list of labels for the COHP that should be included in the summed cohp\n            orbital_list: list of orbitals for the COHPs that should be included in the summed cohp (same order as label_list)\n            divisor: float/int, the summed cohp will be divided by this divisor\n        Returns:\n            Returns a COHP object including a summed COHP\n        \"\"\"\n        # check if cohps are spinpolarized or not\n        first_cohpobject = self.get_orbital_resolved_cohp(label_list[0], orbital_list[0])\n        summed_cohp = first_cohpobject.cohp.copy()\n        summed_icohp = first_cohpobject.icohp.copy()\n        for ilabel, label in enumerate(label_list[1:], 1):\n            cohp_here = self.get_orbital_resolved_cohp(label, orbital_list[ilabel])\n            summed_cohp[Spin.up] = np.sum([summed_cohp[Spin.up], cohp_here.cohp.copy()[Spin.up]], axis=0)\n            if Spin.down in summed_cohp:\n                summed_cohp[Spin.down] = np.sum([summed_cohp[Spin.down], cohp_here.cohp.copy()[Spin.down]], axis=0)\n            summed_icohp[Spin.up] = np.sum([summed_icohp[Spin.up], cohp_here.icohp.copy()[Spin.up]], axis=0)\n            if Spin.down in summed_icohp:\n                summed_icohp[Spin.down] = np.sum([summed_icohp[Spin.down], cohp_here.icohp.copy()[Spin.down]], axis=0)\n\n        divided_cohp = {}\n        divided_icohp = {}\n        divided_cohp[Spin.up] = np.divide(summed_cohp[Spin.up], divisor)\n        divided_icohp[Spin.up] = np.divide(summed_icohp[Spin.up], divisor)\n        if Spin.down in summed_cohp:\n            divided_cohp[Spin.down] = np.divide(summed_cohp[Spin.down], divisor)\n            divided_icohp[Spin.down] = np.divide(summed_icohp[Spin.down], divisor)\n\n        return Cohp(efermi=first_cohpobject.efermi, energies=first_cohpobject.energies, cohp=divided_cohp,\n                    are_coops=first_cohpobject.are_coops,\n                    icohp=divided_icohp)",
        "sampled": "```\ndef get_summed_cohp_by_label_and_orbital_list(self, label_list, orbital_list, divisor=1):\n    summed_cohp = 0\n    for label, orbital in zip(label_list, orbital_list):\n        cohp = self.get_cohp_by_label_and_orbital(label, orbital)\n        summed_cohp += cohp\n    summed_cohp /= divisor\n    return COHP(summed_cohp)\n```"
    },
    {
        "original": "def get_qr(self, filename=None):\n        \"\"\"Get pairing QR code from client\"\"\"\n        if \"Click to reload QR code\" in self.driver.page_source:\n            self.reload_qr()\n        qr = self.driver.find_element_by_css_selector(self._SELECTORS['qrCode'])\n        if filename is None:\n            fd, fn_png = tempfile.mkstemp(prefix=self.username, suffix='.png')\n        else:\n            fd = os.open(filename, os.O_RDWR | os.O_CREAT)\n            fn_png = os.path.abspath(filename)\n        self.logger.debug(\"QRcode image saved at %s\" % fn_png)\n        qr.screenshot(fn_png)\n        os.close(fd)\n        return fn_png",
        "sampled": "```\nimport qrcode\nfrom PIL import Image\n\nclass Client:\n    def get_qr(self, filename=None):\n        qr = qrcode.QRCode(\n            version=1,\n            error_correction=qrcode.constants.ERROR_CORRECT_L,\n            box_size=10,\n            border=4,\n        )\n        qr.add_data(\"https://example.com\")  # Replace with your data\n        qr.make(fit=True)\n\n        img = qr.make_image(fill_color=\"black\", back_color=\""
    },
    {
        "original": "def edit(self, name, color, description=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PATCH /repos/:owner/:repo/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param name: string\n        :param color: string\n        :param description: string\n        :rtype: None\n        \"\"\"\n        assert isinstance(name, (str, unicode)), name\n        assert isinstance(color, (str, unicode)), color\n        assert description is github.GithubObject.NotSet or isinstance(description, (str, unicode)), description\n        post_parameters = {\n            \"name\": name,\n            \"color\": color,\n        }\n        if description is not github.GithubObject.NotSet:\n            post_parameters[\"description\"] = description\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PATCH\",\n            self.url,\n            input=post_parameters,\n            headers={'Accept': Consts.mediaTypeLabelDescriptionSearchPreview}\n        )\n        self._useAttributes(data)",
        "sampled": "```\nimport requests\n\ndef edit_label(owner, repo, name, color, description=github.GithubObject.NotSet):\n    url = f\"https://api.github.com/repos/{owner}/{repo}/labels/{name}\"\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    data = {\"name\": name, \"color\": color, \"description\": description}\n    response = requests.patch(url, headers=headers, json=data)\n    response.raise_for_status()\n```"
    },
    {
        "original": "def documentation(self, add_to=None):\n        \"\"\"Produces general documentation for the interface\"\"\"\n        doc = OrderedDict if add_to is None else add_to\n\n        usage = self.interface.spec.__doc__\n        if usage:\n            doc['usage'] = usage\n        if getattr(self, 'requires', None):\n            doc['requires'] = [getattr(requirement, '__doc__', requirement.__name__) for requirement in self.requires]\n        doc['outputs'] = OrderedDict()\n        doc['outputs']['format'] = self.outputs.__doc__\n        doc['outputs']['content_type'] = self.outputs.content_type\n        parameters = [param for param in self.parameters if not param in ('request', 'response', 'self')\n                                                        and not param in ('api_version', 'body')\n                                                        and not param.startswith('hug_')\n                                                        and not hasattr(param, 'directive')]\n        if parameters:\n            inputs = doc.setdefault('inputs', OrderedDict())\n            types = self.interface.spec.__annotations__\n            for argument in parameters:\n                kind = types.get(argument, text)\n                if getattr(kind, 'directive', None) is True:\n                    continue\n\n                input_definition = inputs.setdefault(argument, OrderedDict())\n                input_definition['type'] = kind if isinstance(kind, str) else kind.__doc__\n                default = self.defaults.get(argument, None)\n                if default is not None:\n                    input_definition['default'] = default\n\n        return doc",
        "sampled": "I'm happy to help! However, I don't see a programming challenge provided. Please go ahead and provide the challenge, and I'll do my best to assist you."
    },
    {
        "original": "def switch_to_frame(driver, frame, timeout=settings.SMALL_TIMEOUT):\n    \"\"\"\n    Wait for an iframe to appear, and switch to it. This should be usable\n    as a drop-in replacement for driver.switch_to.frame().\n    @Params\n    driver - the webdriver object (required)\n    frame - the frame element, name, or index\n    timeout - the time to wait for the alert in seconds\n    \"\"\"\n\n    start_ms = time.time() * 1000.0\n    stop_ms = start_ms + (timeout * 1000.0)\n    for x in range(int(timeout * 10)):\n        try:\n            driver.switch_to.frame(frame)\n            return True\n        except NoSuchFrameException:\n            now_ms = time.time() * 1000.0\n            if now_ms >= stop_ms:\n                break\n            time.sleep(0.1)\n    raise Exception(\"Frame was not present after %s seconds!\" % timeout)",
        "sampled": "```\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\n\ndef switch_to_frame(driver, frame, timeout=settings.SMALL_TIMEOUT):\n    iframe = WebDriverWait(driver, timeout).until(\n        EC.frame_to_be_available_and_switch_to_it((By.XPATH, f\"//iframe[@name='{frame}']\"))\n    )\n    return iframe\n```"
    },
    {
        "original": "def process_eni_metrics(\n        stream_eni, myips, stream,\n        start, end, period, sample_size,\n        resolver, sink_uri):\n    \"\"\"ENI flow stream processor that rollups, enhances,\n       and indexes the stream by time period.\"\"\"\n    stats = Counter()\n    period_counters = flow_stream_stats(myips, stream, period)\n    client = InfluxDBClient.from_dsn(sink_uri)\n    resource = resolver.resolve_resource(stream_eni)\n    points = []\n\n    for period in sorted(period_counters):\n        pc = period_counters[period]\n        pd = datetime.fromtimestamp(period)\n\n        for t in ('inbytes', 'outbytes'):\n            tpc = pc[t]\n            ips = [ip for ip, _ in tpc.most_common(sample_size)]\n            resolved = resolver.resolve(ips, pd - timedelta(900), pd + timedelta(900))\n            logical_counter = rollup_logical(tpc, resolved, ('app', 'env'))\n            for (app, env), v in logical_counter.items():\n                p = {}\n#                rinfo = resolved.get(ip, {})\n                p['fields'] = {'Bytes': v}\n                p['measurement'] = 'traffic_%s' % t\n                p['time'] = datetime.fromtimestamp(period)\n                p['tags'] = {\n                    'Kind': resource['type'],\n                    'AccountId': resource['account_id'],\n                    'App': resource['app'],\n                    'Env': resource['env'],\n                    'ForeignApp': app,\n                    'ForeignEnv': env}\n                points.append(p)\n\n        if len(points) > 2000:\n            client.write_points(points)\n            stats['Points'] += len(points)\n            points = []\n\n    client.write_points(points)\n    stats['Points'] += len(points)\n    log.info('periods:%d resource:%s points:%d',\n             len(period_counters), resource, stats['Points'])\n    return stats",
        "sampled": "```\ndef process_eni_metrics(\n    stream_eni, myips, stream,\n    start, end, period, sample_size,\n    resolver, sink_uri):\n    pass  # Please provide the problem description, input, and output specifications.\n```"
    },
    {
        "original": "def read_existing_paths(bt_table):\n    \"\"\"Return the SGF filename for each existing eval record.\"\"\"\n    rows = bt_table.read_rows(\n        filter_=row_filters.ColumnRangeFilter(\n            METADATA, SGF_FILENAME, SGF_FILENAME))\n    names = (row.cell_value(METADATA, SGF_FILENAME).decode() for row in rows)\n    processed = [os.path.splitext(os.path.basename(r))[0] for r in names]\n    return processed",
        "sampled": "```\ndef read_existing_paths(bt_table):\n    existing_paths = []\n    for row in bt_table:\n        if row[1] is not None:\n            existing_paths.append(row[0])\n    return existing_paths\n```"
    },
    {
        "original": "def search(self, ngram):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tngram str or unicode, string to search for\n\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame, {'texts': <matching texts>, 'categories': <corresponding categories>}\n\n\t\t\"\"\"\n\t\tmask = self._document_index_mask(ngram)\n\t\treturn pd.DataFrame({\n\t\t\t'text': self.get_texts()[mask],\n\t\t\t'category': [self._category_idx_store.getval(idx)\n\t\t\t             for idx in self._y[mask]]\n\t\t})",
        "sampled": "```\nimport pandas as pd\n\nclass SearchEngine:\n    def __init__(self, data):\n        self.data = data\n\n    def search(self, ngram):\n        result = self.data[self.data['texts'].str.contains(ngram)]\n        return result[['texts', 'categories']]\n```"
    },
    {
        "original": "def delete(name):\n    \"\"\"\n    Delete the namespace from the register\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        deletens:\n          reg.delete:\n            - name: myregister\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': True}\n    if name in __reg__:\n        del __reg__[name]\n    return ret",
        "sampled": "```\nclass Register:\n    def __init__(self):\n        self.namespaces = {}\n\n    def delete(self, name):\n        if name in self.namespaces:\n            del self.namespaces[name]\n```"
    },
    {
        "original": "def getfield(self, pkt, s):\n        \"\"\"\n        We try to compute a length, usually from a msglen parsed earlier.\n        If this length is 0, we consider 'selection_present' (from RFC 5246)\n        to be False. This means that there should not be any length field.\n        However, with TLS 1.3, zero lengths are always explicit.\n        \"\"\"\n        ext = pkt.get_field(self.length_of)\n        tmp_len = ext.length_from(pkt)\n        if tmp_len is None or tmp_len <= 0:\n            v = pkt.tls_session.tls_version\n            if v is None or v < 0x0304:\n                return s, None\n        return super(_ExtensionsLenField, self).getfield(pkt, s)",
        "sampled": "```\ndef getfield(self, pkt, s):\n    if pkt.msglen == 0:\n        if pkt.version == 'TLS 1.3':\n            return s\n        else:\n            return ''\n    else:\n        return s[:pkt.msglen]\n```"
    },
    {
        "original": "def archive(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the Archive class. Registers the decorated class\n    as the Archive known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _archive_resource_type\n    _archive_resource_type = class_obj\n    return class_obj",
        "sampled": "```\nregistry = {}\n\ndef archive(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the Archive class. Registers the decorated class\n    as the Archive known type.\n    \"\"\"\n    registry[class_obj.__name__] = class_obj\n    return class_obj\n```"
    },
    {
        "original": "def form_query(query_type, query):\n    \"\"\"\n    Returns a multi match query\n    \"\"\"\n    fields = [\n        field + \"^\" + str(SEARCH_BOOSTS[field]) if field in SEARCH_BOOSTS else field\n        for field in SEARCH_FIELDS\n    ]\n    return Q(\"multi_match\", fields=fields, query=query, type=query_type)",
        "sampled": "```\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import scan\n\ndef form_query(query_type, query):\n    es = Elasticsearch()\n    if query_type == \"multi_match\":\n        body = {\n            \"query\": {\n                \"multi_match\": {\n                    \"query\": query,\n                    \"fields\": [\"title\", \"description\"]\n                }\n            }\n        }\n    else:\n        raise ValueError(\"Invalid query type\")\n    return body\n```"
    },
    {
        "original": "def get_dependants(self, run):\n        \"\"\"\n        Return a list of nodes that are control dependent on the given node in the control dependence graph\n        \"\"\"\n        if run in self._graph.nodes():\n            return list(self._graph.successors(run))\n        else:\n            return []",
        "sampled": "```\nclass ControlDependenceGraph:\n    def __init__(self, graph):\n        self.graph = graph\n\n    def get_dependants(self, run):\n        dependants = []\n        for node in self.graph:\n            if run in self.graph[node]:\n                dependants.append(node)\n        return dependants\n```"
    },
    {
        "original": "def _ExtractClientIdFromPath(entry, event):\n  \"\"\"Extracts a Client ID from an APIAuditEntry's HTTP request path.\"\"\"\n  match = re.match(r\".*(C\\.[0-9a-fA-F]{16}).*\", entry.http_request_path)\n  if match:\n    event.client = match.group(1)",
        "sampled": "```\ndef _ExtractClientIdFromPath(entry, event):\n    import re\n    pattern = r\"/clients/(\\w+)/\"\n    match = re.search(pattern, entry.request.path)\n    if match:\n        return match.group(1)\n    else:\n        return None\n```"
    },
    {
        "original": "def add_override(self, partname, content_type):\n        \"\"\"\n        Add a child ``<Override>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        override = CT_Override.new(partname, content_type)\n        self.append(override)",
        "sampled": "```\nclass OverrideElement:\n    def __init__(self):\n        self.override_elements = []\n\n    def add_override(self, partname, content_type):\n        self.override_elements.append({\"partname\": partname, \"content_type\": content_type})\n\n    def __str__(self):\n        result = \"\"\n        for override in self.override_elements:\n            result += f\"<Override partname='{override['partname']}' content_type='{override['content_type']}'/>\\n\"\n        return result\n```"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check Minion._post_master_init\n        to see if those changes need to be propagated.\n\n        ProxyMinions need a significantly different post master setup,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        mp_call = _metaproxy_call(self.opts, 'post_master_init')\n        return mp_call(self, master)",
        "sampled": "```\ndef _post_master_init(self, master):\n    pass\n```"
    },
    {
        "original": "def Kdiag(self, X, target):\n        \"\"\"Compute the diagonal of the covariance matrix associated to X.\"\"\"\n        ly=1/self.lengthscaleY\n        lu=np.sqrt(3)/self.lengthscaleU\n        #ly=self.lengthscaleY\n        #lu=self.lengthscaleU\n        \n        k1 = (2*lu+ly)/(lu+ly)**2\n        k2 = (ly-2*lu + 2*lu-ly ) / (ly-lu)**2 \n        k3 = 1/(lu+ly) + (lu)/(lu+ly)**2 \n\n        np.add(self.varianceU*self.varianceY*(k1+k2+k3), target, target)",
        "sampled": "```\ndef Kdiag(self, X, target):\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = np.exp(-0.5 * (X[i] - X[j])**2 / target)\n    return np.diag(K)\n```"
    },
    {
        "original": "def make_deprecated_class(oldname, NewClass):\n    \"\"\"\n    Returns a class that raises NotImplementedError on instantiation.\n    e.g.:\n    >>> Kern = make_deprecated_class(\"Kern\", Kernel)\n    \"\"\"\n    msg = (\"{module}.{} has been renamed to {module}.{}\"\n           .format(oldname, NewClass.__name__, module=NewClass.__module__))\n\n    class OldClass(NewClass):\n        def __new__(cls, *args, **kwargs):\n            raise NotImplementedError(msg)\n    OldClass.__doc__ = msg\n    OldClass.__qualname__ = OldClass.__name__ = oldname\n    return OldClass",
        "sampled": "```\ndef make_deprecated_class(oldname, NewClass):\n    class DeprecatedClass(NewClass):\n        def __init__(self, *args, **kwargs):\n            raise NotImplementedError(f\"'{oldname}' is deprecated. Use '{NewClass.__name__}' instead.\")\n    return DeprecatedClass\n```"
    },
    {
        "original": "def estimate_row_means(\n            self,\n            X,\n            observed,\n            column_means,\n            column_scales):\n        \"\"\"\n        row_center[i] =\n        sum{j in observed[i, :]}{\n            (1 / column_scale[j]) * (X[i, j] - column_center[j])\n        }\n        ------------------------------------------------------------\n        sum{j in observed[i, :]}{1 / column_scale[j]}\n        \"\"\"\n\n        n_rows, n_cols = X.shape\n\n        column_means = np.asarray(column_means)\n        if len(column_means) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_means.shape))\n        X = X - column_means.reshape((1, n_cols))\n        column_weights = 1.0 / column_scales\n        X *= column_weights.reshape((1, n_cols))\n        row_means = np.zeros(n_rows, dtype=X.dtype)\n        row_residual_sums = np.nansum(X, axis=1)\n        for i in range(n_rows):\n            row_mask = observed[i, :]\n            sum_weights = column_weights[row_mask].sum()\n            row_means[i] = row_residual_sums[i] / sum_weights\n        return row_means",
        "sampled": "```\ndef estimate_row_means(X, observed, column_means, column_scales):\n    row_means = []\n    for i in range(X.shape[0]):\n        numerator = 0\n        denominator = 0\n        for j in range(X.shape[1]):\n            if observed[i, j]:\n                numerator += (X[i, j] - column_means[j]) / column_scales[j]\n                denominator += 1 / column_scales[j]\n        row_means.append(numerator / denominator"
    },
    {
        "original": "def ReadHuntOutputPluginLogEntries(self,\n                                     hunt_id,\n                                     output_plugin_id,\n                                     offset,\n                                     count,\n                                     with_type=None,\n                                     cursor=None):\n    \"\"\"Reads hunt output plugin log entries.\"\"\"\n    query = (\"SELECT client_id, flow_id, log_entry_type, message, \"\n             \"UNIX_TIMESTAMP(timestamp) \"\n             \"FROM flow_output_plugin_log_entries \"\n             \"FORCE INDEX (flow_output_plugin_log_entries_by_hunt) \"\n             \"WHERE hunt_id = %s AND output_plugin_id = %s \")\n    args = [\n        db_utils.HuntIDToInt(hunt_id),\n        db_utils.OutputPluginIDToInt(output_plugin_id)\n    ]\n\n    if with_type is not None:\n      query += \"AND log_entry_type = %s \"\n      args.append(int(with_type))\n\n    query += \"ORDER BY log_id ASC LIMIT %s OFFSET %s\"\n    args.append(count)\n    args.append(offset)\n\n    cursor.execute(query, args)\n\n    ret = []\n    for (client_id_int, flow_id_int, log_entry_type, message,\n         timestamp) in cursor.fetchall():\n      ret.append(\n          rdf_flow_objects.FlowOutputPluginLogEntry(\n              hunt_id=hunt_id,\n              client_id=db_utils.IntToClientID(client_id_int),\n              flow_id=db_utils.IntToFlowID(flow_id_int),\n              output_plugin_id=output_plugin_id,\n              log_entry_type=log_entry_type,\n              message=message,\n              timestamp=mysql_utils.TimestampToRDFDatetime(timestamp)))\n\n    return ret",
        "sampled": "```\ndef ReadHuntOutputPluginLogEntries(self, hunt_id, output_plugin_id, offset, count, with_type=None, cursor=None):\n    # TO DO: implement the logic to read hunt output plugin log entries\n    pass\n```"
    },
    {
        "original": "def _condensation_lil(self):\n        \"\"\"\n        Return the sparse matrix representation of the condensation digraph\n        in lil format.\n\n        \"\"\"\n        condensation_lil = sparse.lil_matrix(\n            (self.num_strongly_connected_components,\n             self.num_strongly_connected_components), dtype=bool\n        )\n\n        scc_proj = self.scc_proj\n        for node_from, node_to in _csr_matrix_indices(self.csgraph):\n            scc_from, scc_to = scc_proj[node_from], scc_proj[node_to]\n            if scc_from != scc_to:\n                condensation_lil[scc_from, scc_to] = True\n\n        return condensation_lil",
        "sampled": "```\nfrom scipy.sparse import lil_matrix\n\nclass CondensationDigraph:\n    def _condensation_lil(self):\n        # Assuming you have a method to get the condensation digraph\n        condensation_digraph = self.get_condensation_digraph()\n        \n        # Create a lil matrix with the same shape as the condensation digraph\n        lil_matrix_representation = lil_matrix(condensation_digraph.shape)\n        \n        # Populate the lil matrix with the edges of the condensation digraph\n"
    },
    {
        "original": "def build_modules(is_training, vocab_size):\n  \"\"\"Construct the modules used in the graph.\"\"\"\n\n  # Construct the custom getter which implements Bayes by Backprop.\n  if is_training:\n    estimator_mode = tf.constant(bbb.EstimatorModes.sample)\n  else:\n    estimator_mode = tf.constant(bbb.EstimatorModes.mean)\n  lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n  non_lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n      posterior_builder=non_lstm_posterior_builder,\n      prior_builder=custom_scale_mixture_prior_builder,\n      kl_builder=bbb.stochastic_kl_builder,\n      sampling_mode_tensor=estimator_mode)\n\n  embed_layer = snt.Embed(\n      vocab_size=vocab_size,\n      embed_dim=FLAGS.embedding_size,\n      custom_getter=non_lstm_bbb_custom_getter,\n      name=\"input_embedding\")\n\n  cores = []\n  for i in range(FLAGS.n_layers):\n    cores.append(\n        snt.LSTM(FLAGS.hidden_size,\n                 custom_getter=lstm_bbb_custom_getter,\n                 forget_bias=0.0,\n                 name=\"lstm_layer_{}\".format(i)))\n\n  rnn_core = snt.DeepRNN(\n      cores,\n      skip_connections=False,\n      name=\"deep_lstm_core\")\n\n  # Do BBB on weights but not biases of output layer.\n  output_linear = snt.Linear(\n      vocab_size, custom_getter={\"w\": non_lstm_bbb_custom_getter})\n  return embed_layer, rnn_core, output_linear",
        "sampled": "```\ndef build_modules(is_training, vocab_size):\n    if is_training:\n        # training mode\n        embedding_module = tf.keras.layers.Embedding(vocab_size, 128)\n        rnn_module = tf.keras.layers.LSTM(128)\n        output_module = tf.keras.layers.Dense(vocab_size)\n    else:\n        # inference mode\n        embedding_module = tf.keras.layers.Embedding(vocab_size, 128)\n        rnn_module = tf.keras.layers.LSTM(128, return_sequences=True"
    },
    {
        "original": "def run(self):\n        \"\"\"\n        Run the master service!\n        \"\"\"\n        self.utils = salt.loader.utils(self.opts, proxy=self.proxy)\n        if salt.utils.platform.is_windows():\n            # Calculate function references since they can't be pickled.\n            if self.opts['__role'] == 'master':\n                self.runners = salt.loader.runner(self.opts, utils=self.utils)\n            else:\n                self.runners = []\n            self.funcs = salt.loader.minion_mods(self.opts, utils=self.utils, proxy=self.proxy)\n\n        self.engine = salt.loader.engines(self.opts,\n                                          self.funcs,\n                                          self.runners,\n                                          self.utils,\n                                          proxy=self.proxy)\n        kwargs = self.config or {}\n        try:\n            self.engine[self.fun](**kwargs)\n        except Exception as exc:\n            log.critical(\n                'Engine \\'%s\\' could not be started!',\n                self.fun.split('.')[0], exc_info=True\n            )",
        "sampled": "I'm ready to help. Please provide the programming challenge in Markdown format, including the problem description, input, and output specifications. I'll analyze the problem and provide a Python solution in a single Markdown block."
    },
    {
        "original": "def get_statuses(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/statuses/:ref <http://developer.github.com/v3/repos/statuses>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.CommitStatus.CommitStatus`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.CommitStatus.CommitStatus,\n            self._requester,\n            self._parentUrl(self._parentUrl(self.url)) + \"/statuses/\" + self.sha,\n            None\n        )",
        "sampled": "```\nimport requests\n\nclass GitHub:\n    def __init__(self, owner, repo, ref):\n        self.owner = owner\n        self.repo = repo\n        self.ref = ref\n\n    def get_statuses(self):\n        url = f\"https://api.github.com/repos/{self.owner}/{self.repo}/statuses/{self.ref}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json()\n```"
    },
    {
        "original": "def get_vm_ip(name=None, session=None, call=None):\n    \"\"\"\n    Get the IP address of the VM\n\n    .. code-block:: bash\n\n        salt-cloud -a get_vm_ip xenvm01\n\n    .. note:: Requires xen guest tools to be installed in VM\n\n    \"\"\"\n    if call == 'function':\n        raise SaltCloudException(\n            'This function must be called with -a or --action.'\n        )\n    if session is None:\n        log.debug('New session being created')\n        session = _get_session()\n    vm = _get_vm(name, session=session)\n    ret = None\n    # -- try to get ip from vif\n    vifs = session.xenapi.VM.get_VIFs(vm)\n    if vifs is not None:\n        for vif in vifs:\n            if session.xenapi.VIF.get_ipv4_addresses(vif):\n                cidr = session.xenapi.VIF.get_ipv4_addresses(vif).pop()\n                ret, subnet = cidr.split('/')\n                log.debug(\n                    'VM vif returned for instance: %s ip: %s', name, ret)\n                return ret\n    # -- try to get ip from get tools metrics\n    vgm = session.xenapi.VM.get_guest_metrics(vm)\n    try:\n        net = session.xenapi.VM_guest_metrics.get_networks(vgm)\n        if \"0/ip\" in net.keys():\n            log.debug(\n                'VM guest metrics returned for instance: %s 0/ip: %s',\n                name, net[\"0/ip\"]\n            )\n            ret = net[\"0/ip\"]\n    # except Exception as ex:\n    except XenAPI.Failure:\n        log.info('Could not get vm metrics at this time')\n    return ret",
        "sampled": "```\nimport salt.client\n\ndef get_vm_ip(name=None, session=None, call=None):\n    local = salt.client.LocalClient()\n    if name:\n        vm_ip = local.cmd(name, 'xen_guest_tools.get_ip')\n        return vm_ip[name]\n    else:\n        return \"VM name is required\"\n```"
    },
    {
        "original": "def as_dataset(obj):\n    \"\"\"Cast the given object to a Dataset.\n\n    Handles Datasets, DataArrays and dictionaries of variables. A new Dataset\n    object is only created if the provided object is not already one.\n    \"\"\"\n    if hasattr(obj, 'to_dataset'):\n        obj = obj.to_dataset()\n    if not isinstance(obj, Dataset):\n        obj = Dataset(obj)\n    return obj",
        "sampled": "```\nimport xarray as xr\n\ndef as_dataset(obj):\n    if isinstance(obj, xr.Dataset):\n        return obj\n    elif isinstance(obj, xr.DataArray):\n        return obj.to_dataset(dim='variable')\n    elif isinstance(obj, dict):\n        return xr.Dataset(obj)\n    else:\n        raise ValueError(\"Object cannot be converted to a Dataset\")\n```"
    },
    {
        "original": "def str2float(text):\n    \"\"\"\n    Remove uncertainty brackets from strings and return the float.\n    \"\"\"\n\n    try:\n        # Note that the ending ) is sometimes missing. That is why the code has\n        # been modified to treat it as optional. Same logic applies to lists.\n        return float(re.sub(r\"\\(.+\\)*\", \"\", text))\n    except TypeError:\n        if isinstance(text, list) and len(text) == 1:\n            return float(re.sub(r\"\\(.+\\)*\", \"\", text[0]))\n    except ValueError as ex:\n        if text.strip() == \".\":\n            return 0\n        raise ex",
        "sampled": "```\ndef str2float(text):\n    text = text.replace(\"(\", \"\").replace(\")\", \"\")\n    return float(text)\n```"
    },
    {
        "original": "def installed(name, channel=None):\n    \"\"\"\n    Ensure that the named snap package is installed\n\n    name\n        The snap package\n\n    channel\n        Optional. The channel to install the package from.\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'pchanges': {},\n           'result': None,\n           'comment': ''}\n\n    old = __salt__['snap.versions_installed'](name)\n    if not old:\n        if __opts__['test']:\n            ret['comment'] = 'Package \"{0}\" would have been installed'.format(name)\n            ret['pchanges']['new'] = name\n            ret['pchanges']['old'] = None\n            ret['result'] = None\n            return ret\n\n        install = __salt__['snap.install'](name, channel=channel)\n        if install['result']:\n            ret['comment'] = 'Package \"{0}\" was installed'.format(name)\n            ret['changes']['new'] = name\n            ret['changes']['old'] = None\n            ret['result'] = True\n            return ret\n\n        ret['comment'] = 'Package \"{0}\" failed to install'.format(name)\n        ret['comment'] += '\\noutput:\\n' + install['output']\n        ret['result'] = False\n        return ret\n\n    # Currently snap always returns only one line?\n    old_channel = old[0]['tracking']\n    if old_channel != channel and channel is not None:\n        if __opts__['test']:\n            ret['comment'] = 'Package \"{0}\" would have been switched to channel {1}'.format(name, channel)\n            ret['pchanges']['old_channel'] = old_channel\n            ret['pchanges']['new_channel'] = channel\n            ret['result'] = None\n            return ret\n\n        refresh = __salt__['snap.install'](name, channel=channel, refresh=True)\n        if refresh['result']:\n            ret['comment'] = 'Package \"{0}\" was switched to channel {1}'.format(name, channel)\n            ret['pchanges']['old_channel'] = old_channel\n            ret['pchanges']['new_channel'] = channel\n            ret['result'] = True\n            return ret\n\n        ret['comment'] = 'Failed to switch Package \"{0}\" to channel {1}'.format(name, channel)\n        ret['comment'] += '\\noutput:\\n' + install['output']\n        ret['result'] = False\n        return ret\n\n    ret['comment'] = 'Package \"{0}\" is already installed'.format(name)\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n\n    ret['result'] = True\n    return ret",
        "sampled": "```\nimport subprocess\n\ndef installed(name, channel=None):\n    cmd = [\"snap\", \"install\", name]\n    if channel:\n        cmd.extend([\"--channel\", channel])\n    subprocess.check_call(cmd)\n```"
    },
    {
        "original": "async def get_sender(self):\n        \"\"\"\n        Returns `sender`, but will make an API call to find the\n        sender unless it's already cached.\n        \"\"\"\n        # ``sender.min`` is present both in :tl:`User` and :tl:`Channel`.\n        # It's a flag that will be set if only minimal information is\n        # available (such as display name, but username may be missing),\n        # in which case we want to force fetch the entire thing because\n        # the user explicitly called a method. If the user is okay with\n        # cached information, they may use the property instead.\n        if (self._sender is None or self._sender.min) \\\n                and await self.get_input_sender():\n            try:\n                self._sender =\\\n                    await self._client.get_entity(self._input_sender)\n            except ValueError:\n                await self._reload_message()\n        return self._sender",
        "sampled": "```\nclass MyClass:\n    def __init__(self):\n        self.sender = None\n\n    async def get_sender(self):\n        if self.sender is None:\n            self.sender = await self._fetch_sender_from_api()\n        return self.sender\n\n    async def _fetch_sender_from_api(self):\n        # implement API call to fetch sender\n        pass\n```"
    },
    {
        "original": "def from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True, manager=None):\n        \"\"\"\n        Construct a `GKKPWork` from a `PhononWfkqWork` object.\n        The WFQ are the ones used for PhononWfkqWork so in principle have only valence bands\n        \"\"\"\n        # Get list of qpoints from the the phonon tasks in this work\n        qpoints = []\n        qpoints_deps = []\n        for task in phononwfkq_work:\n            if isinstance(task,PhononTask):\n                # Store qpoints\n                qpt = task.input.get(\"qpt\", [0,0,0])\n                qpoints.append(qpt)\n                # Store dependencies\n                qpoints_deps.append(task.deps)\n\n        # Create file nodes\n        ddb_path  = phononwfkq_work.outdir.has_abiext(\"DDB\")\n        dvdb_path = phononwfkq_work.outdir.has_abiext(\"DVDB\")\n        ddb_file = FileNode(ddb_path)\n        dvdb_file = FileNode(dvdb_path)\n\n        # Get scf_task from first q-point\n        for dep in qpoints_deps[0]:\n            if isinstance(dep.node,ScfTask) and dep.exts[0] == 'WFK':\n                scf_task = dep.node\n\n        # Create new work\n        new = cls(manager=manager)\n        new.remove_wfkq = remove_wfkq\n        new.wfkq_tasks = []\n        new.wfk_task = []\n\n        # Add one eph task per qpoint\n        for qpt,qpoint_deps in zip(qpoints,qpoints_deps):\n            # Create eph task\n            eph_input = scf_task.input.new_with_vars(optdriver=7, prtphdos=0, eph_task=-2,\n                                                     ddb_ngqpt=[1,1,1], nqpt=1, qpt=qpt)\n            deps = {ddb_file: \"DDB\", dvdb_file: \"DVDB\" }\n            for dep in qpoint_deps:\n                deps[dep.node] = dep.exts[0]\n            # If no WFQ in deps link the WFK with WFQ extension\n            if 'WFQ' not in deps.values():\n                inv_deps = dict((v, k) for k, v in deps.items())\n                wfk_task = inv_deps['WFK']\n                wfk_path = wfk_task.outdir.has_abiext(\"WFK\")\n                # Check if netcdf\n                filename, extension = os.path.splitext(wfk_path)\n                infile = 'out_WFQ' + extension\n                wfq_path = os.path.join(os.path.dirname(wfk_path), infile)\n                if not os.path.isfile(wfq_path): os.symlink(wfk_path, wfq_path)\n                deps[FileNode(wfq_path)] = 'WFQ'\n            new.register_eph_task(eph_input, deps=deps)\n\n        return new",
        "sampled": "```\nclass GKKPWork:\n    def from_phononwfkq_work(cls, phononwfkq_work, nscf_vars={}, remove_wfkq=True, with_ddk=True, manager=None):\n        # Initialize GKKPWork object\n        gkkp_work = cls()\n\n        # Get valence bands from PhononWfkqWork\n        valence_bands = phononwfkq_work.valence_bands\n\n        # Set nscf"
    },
    {
        "original": "def get_function_name(s):\n    \"\"\"\n    Get the function name from a C-style function declaration string.\n\n    :param str s: A C-style function declaration string.\n    :return:      The function name.\n    :rtype:       str\n    \"\"\"\n\n    s = s.strip()\n    if s.startswith(\"__attribute__\"):\n        # Remove \"__attribute__ ((foobar))\"\n        if \"))\" not in s:\n            raise ValueError(\"__attribute__ is present, but I cannot find double-right parenthesis in the function \"\n                             \"declaration string.\")\n\n        s = s[s.index(\"))\") + 2 : ].strip()\n\n    if '(' not in s:\n        raise ValueError(\"Cannot find any left parenthesis in the function declaration string.\")\n\n    func_name = s[:s.index('(')].strip()\n\n    for i, ch in enumerate(reversed(func_name)):\n        if ch == ' ':\n            pos = len(func_name) - 1 - i\n            break\n    else:\n        raise ValueError('Cannot find any space in the function declaration string.')\n\n    func_name = func_name[pos + 1 : ]\n    return func_name",
        "sampled": "```\ndef get_function_name(s):\n    start = s.find('(')\n    end = s.rfind(')')\n    func_name = s[:start].split()[-1]\n    return func_name\n```"
    },
    {
        "original": "def ReadAPIAuditEntries(self,\n                          username=None,\n                          router_method_names=None,\n                          min_timestamp=None,\n                          max_timestamp=None):\n    \"\"\"Returns audit entries stored in the database.\"\"\"\n    results = []\n\n    for entry in self.api_audit_entries:\n      if username is not None and entry.username != username:\n        continue\n\n      if (router_method_names and\n          entry.router_method_name not in router_method_names):\n        continue\n\n      if min_timestamp is not None and entry.timestamp < min_timestamp:\n        continue\n\n      if max_timestamp is not None and entry.timestamp > max_timestamp:\n        continue\n\n      results.append(entry)\n\n    return sorted(results, key=lambda entry: entry.timestamp)",
        "sampled": "```\nclass APIAudit:\n    def __init__(self, db):\n        self.db = db\n\n    def ReadAPIAuditEntries(self, username=None, router_method_names=None, min_timestamp=None, max_timestamp=None):\n        query = \"SELECT * FROM audit_entries\"\n        conditions = []\n        \n        if username:\n            conditions.append(\"username = '{}'\".format(username))\n        if router_method_names:\n            conditions.append(\"router_method_name IN ({})\".format(','.join([\"'{}'"
    },
    {
        "original": "def from_stream(cls, stream, marker_code, offset):\n        \"\"\"\n        Return an |_SofMarker| instance for the SOFn marker at *offset* in\n        stream.\n        \"\"\"\n        # field                 off  type   notes\n        # ------------------  ---  -----  ----------------------------\n        # segment length       0   short\n        # Data precision       2   byte\n        # Vertical lines       3   short  px_height\n        # Horizontal lines     5   short  px_width\n        # ------------------  ---  -----  ----------------------------\n        segment_length = stream.read_short(offset)\n        px_height = stream.read_short(offset, 3)\n        px_width = stream.read_short(offset, 5)\n        return cls(marker_code, offset, segment_length, px_width, px_height)",
        "sampled": "```\nclass SofMarker:\n    def __init__(self, stream, marker_code, offset):\n        self.stream = stream\n        self.marker_code = marker_code\n        self.offset = offset\n\n    @classmethod\n    def from_stream(cls, stream, marker_code, offset):\n        return cls(stream, marker_code, offset)\n```"
    },
    {
        "original": "def normalize(rendered):\n    \"\"\"Return the input string without non-functional spaces or newlines.\"\"\"\n    out = ''.join([line.strip()\n                   for line in rendered.splitlines()\n                   if line.strip()])\n    out = out.replace(', ', ',')\n    return out",
        "sampled": "```\ndef normalize(rendered):\n    \"\"\"Return the input string without non-functional spaces or newlines.\"\"\"\n    return ' '.join(rendered.replace('\\n', ' ').split())\n```"
    },
    {
        "original": "def hash_and_stat_file(self, path, saltenv='base'):\n        \"\"\"\n        Return the hash of a file, to get the hash of a file in the pillar_roots\n        prepend the path with salt://<file on server> otherwise, prepend the\n        file with / for a local file.\n\n        Additionally, return the stat result of the file, or None if no stat\n        results were found.\n        \"\"\"\n        ret = {}\n        fnd = self.__get_file_path(path, saltenv)\n        if fnd is None:\n            return ret, None\n\n        try:\n            # Remote file path (self._find_file() invoked)\n            fnd_path = fnd['path']\n            fnd_stat = fnd.get('stat')\n        except TypeError:\n            # Local file path\n            fnd_path = fnd\n            try:\n                fnd_stat = list(os.stat(fnd_path))\n            except Exception:\n                fnd_stat = None\n\n        hash_type = self.opts.get('hash_type', 'md5')\n        ret['hsum'] = salt.utils.hashutils.get_hash(fnd_path, form=hash_type)\n        ret['hash_type'] = hash_type\n        return ret, fnd_stat",
        "sampled": "```\nimport os\nimport hashlib\n\ndef hash_and_stat_file(self, path, saltenv='base'):\n    if path.startswith('salt://'):\n        path = path.replace('salt://', '')\n    else:\n        path = '/' + path\n\n    try:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.md5()\n            while chunk := f.read(8192):\n                file_hash.update(chunk)\n            file_hash = file_hash.hexdigest()\n    except"
    },
    {
        "original": "def prepare_env(self):\n        \"\"\"\n        Manages reading environment metadata files under ``private_data_dir`` and merging/updating\n        with existing values so the :py:class:`ansible_runner.runner.Runner` object can read and use them easily\n        \"\"\"\n        try:\n            passwords = self.loader.load_file('env/passwords', Mapping)\n            self.expect_passwords = {\n                re.compile(pattern, re.M): password\n                for pattern, password in iteritems(passwords)\n            }\n        except ConfigurationError:\n            output.debug('Not loading passwords')\n            self.expect_passwords = dict()\n        self.expect_passwords[pexpect.TIMEOUT] = None\n        self.expect_passwords[pexpect.EOF] = None\n\n        try:\n            # seed env with existing shell env\n            self.env = os.environ.copy()\n            envvars = self.loader.load_file('env/envvars', Mapping)\n            if envvars:\n                self.env.update({k:six.text_type(v) for k, v in envvars.items()})\n            if self.envvars and isinstance(self.envvars, dict):\n                self.env.update({k:six.text_type(v) for k, v in self.envvars.items()})\n        except ConfigurationError:\n            output.debug(\"Not loading environment vars\")\n            # Still need to pass default environment to pexpect\n            self.env = os.environ.copy()\n\n        try:\n            self.settings = self.loader.load_file('env/settings', Mapping)\n        except ConfigurationError:\n            output.debug(\"Not loading settings\")\n            self.settings = dict()\n\n        try:\n            self.ssh_key_data = self.loader.load_file('env/ssh_key', string_types)\n        except ConfigurationError:\n            output.debug(\"Not loading ssh key\")\n            self.ssh_key_data = None\n\n        self.idle_timeout = self.settings.get('idle_timeout', None)\n        self.job_timeout = self.settings.get('job_timeout', None)\n        self.pexpect_timeout = self.settings.get('pexpect_timeout', 5)\n\n        self.process_isolation = self.settings.get('process_isolation', self.process_isolation)\n        self.process_isolation_executable = self.settings.get('process_isolation_executable', self.process_isolation_executable)\n        self.process_isolation_path = self.settings.get('process_isolation_path', self.process_isolation_path)\n        self.process_isolation_hide_paths = self.settings.get('process_isolation_hide_paths', self.process_isolation_hide_paths)\n        self.process_isolation_show_paths = self.settings.get('process_isolation_show_paths', self.process_isolation_show_paths)\n        self.process_isolation_ro_paths = self.settings.get('process_isolation_ro_paths', self.process_isolation_ro_paths)\n\n        self.pexpect_use_poll = self.settings.get('pexpect_use_poll', True)\n        self.suppress_ansible_output = self.settings.get('suppress_ansible_output', self.quiet)\n        self.directory_isolation_cleanup = bool(self.settings.get('directory_isolation_cleanup', True))\n\n        if 'AD_HOC_COMMAND_ID' in self.env or not os.path.exists(self.project_dir):\n            self.cwd = self.private_data_dir\n        else:\n            if self.directory_isolation_path is not None:\n                self.cwd = self.directory_isolation_path\n            else:\n                self.cwd = self.project_dir\n\n        if 'fact_cache' in self.settings:\n            if 'fact_cache_type' in self.settings:\n                if self.settings['fact_cache_type'] == 'jsonfile':\n                    self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])\n            else:\n                self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])",
        "sampled": "```\nclass AnsibleRunner:\n    def __init__(self, private_data_dir):\n        self.private_data_dir = private_data_dir\n        self.env_metadata = {}\n\n    def prepare_env(self):\n        for file in os.listdir(self.private_data_dir):\n            if file.endswith('.meta'):\n                with open(os.path.join(self.private_data_dir, file), 'r') as f:\n                    metadata = yaml.safe_load(f)\n                    self.env_metadata.update(metadata)\n```"
    },
    {
        "original": "def setbit(self, name, offset, value):\n        \"\"\"\n        Flag the ``offset`` in ``name`` as ``value``. Returns a boolean\n        indicating the previous value of ``offset``.\n        \"\"\"\n        value = value and 1 or 0\n        return self.execute_command('SETBIT', name, offset, value)",
        "sampled": "```\nclass BitSetter:\n    def __init__(self):\n        self.bits = {}\n\n    def setbit(self, name, offset, value):\n        if name not in self.bits:\n            self.bits[name] = 0\n        prev_value = (self.bits[name] >> offset) & 1\n        if value:\n            self.bits[name] |= 1 << offset\n        else:\n            self.bits[name] &= ~(1 << offset)\n        return prev_value\n```"
    },
    {
        "original": "def _checkValueItemParent(policy_element, policy_name, policy_key,\n                          policy_valueName, xpath_object, policy_file_data,\n                          check_deleted=False, test_item=True):\n    \"\"\"\n    helper function to process the parent of a value item object\n    if test_item is True, it will determine if the policy is enabled/disabled\n    returns True if the value is configured in the registry.pol file, otherwise returns False\n\n    if test_item is False, the expected search string will be returned\n\n    value type parents:\n        boolean: https://msdn.microsoft.com/en-us/library/dn606009(v=vs.85).aspx\n        enabledValue: https://msdn.microsoft.com/en-us/library/dn606006(v=vs.85).aspx\n        disabledValue: https://msdn.microsoft.com/en-us/library/dn606001(v=vs.85).aspx\n\n    \"\"\"\n    for element in xpath_object(policy_element):\n        for value_item in element.getchildren():\n            search_string = _processValueItem(value_item,\n                                              policy_key,\n                                              policy_valueName,\n                                              policy_element,\n                                              element,\n                                              check_deleted=check_deleted)\n            if not test_item:\n                return search_string\n            if _regexSearchRegPolData(re.escape(search_string), policy_file_data):\n                log.debug('found the search string in the pol file, '\n                          '%s is configured', policy_name)\n                return True\n    return False",
        "sampled": "```\ndef _checkValueItemParent(policy_element, policy_name, policy_key, policy_valueName, xpath_object, policy_file_data, check_deleted=False, test_item=True):\n    if test_item:\n        if policy_element.tag == '{http://www.microsoft.com/GroupPolicy/Settings}enabledValue':\n            return policy_valueName in policy_file_data\n        elif policy_element.tag == '{http://www.microsoft.com/GroupPolicy/Settings}disabledValue':\n            return policy_valueName not in"
    },
    {
        "original": "def RegisterCheck(cls, check, source=\"unknown\", overwrite_if_exists=False):\n    \"\"\"Adds a check to the registry, refresh the trigger to check map.\"\"\"\n    if not overwrite_if_exists and check.check_id in cls.checks:\n      raise DefinitionError(\n          \"Check named %s already exists and \"\n          \"overwrite_if_exists is set to False.\" % check.check_id)\n    check.loaded_from = source\n    cls.checks[check.check_id] = check\n    cls.triggers.Update(check.triggers, check)",
        "sampled": "I'd be happy to help! However, I notice that you didn't provide the problem description, input and output specifications for me to work with. Could you please provide them so I can assist you better?"
    },
    {
        "original": "def extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n        \"\"\"\n        get ICOHP/ICOOP of strongest bond\n        Args:\n            summed_spin_channels: Boolean to indicate whether the ICOHPs/ICOOPs of both spin channels should be summed\n\n            spin: if summed_spin_channels is equal to False, this spin indicates which spin channel should be returned\n        Returns:\n            lowest ICOHP/largest ICOOP value (i.e. ICOHP/ICOOP value of strongest bond)\n        \"\"\"\n        if not self._are_coops:\n            extremum = sys.float_info.max\n        else:\n            extremum = -sys.float_info.max\n\n        if not self._is_spin_polarized:\n            if spin == Spin.down:\n                warnings.warn(\"This spin channel does not exist. I am switching to Spin.up\")\n            spin = Spin.up\n\n        for value in self._icohplist.values():\n            if not value.is_spin_polarized or not summed_spin_channels:\n                if not self._are_coops:\n                    if value.icohpvalue(spin) < extremum:\n                        extremum = value.icohpvalue(spin)\n                        # print(extremum)\n                else:\n                    if value.icohpvalue(spin) > extremum:\n                        extremum = value.icohpvalue(spin)\n                        # print(extremum)\n            else:\n                if not self._are_coops:\n                    if value.summed_icohp < extremum:\n                        extremum = value.summed_icohp\n                        # print(extremum)\n                else:\n                    if value.summed_icohp > extremum:\n                        extremum = value.summed_icohp\n                        # print(extremum)\n        return extremum",
        "sampled": "```\ndef extremum_icohpvalue(self, summed_spin_channels=True, spin=Spin.up):\n    if summed_spin_channels:\n        icoop_values = self.icoop_values['up'] + self.icoop_values['down']\n        icoop_extremum = max(icoop_values)\n        ichop_values = self.ichop_values['up'] + self.ichop_values['down']\n        ichop_extremum = min(ichop_values)\n        return max"
    },
    {
        "original": "def secgroup_list(self):\n        \"\"\"\n        List security groups\n        \"\"\"\n        nt_ks = self.compute_conn\n        ret = {}\n        for item in nt_ks.security_groups.list():\n            ret[item.name] = {\n                'name': item.name,\n                'description': item.description,\n                'id': item.id,\n                'tenant_id': item.tenant_id,\n                'rules': item.rules,\n            }\n        return ret",
        "sampled": "```\ndef secgroup_list(self):\n    \"\"\"\n    List security groups\n    \"\"\"\n    # Assuming you have a list of security groups\n    security_groups = [\"sg-12345678\", \"sg-23456789\", \"sg-34567890\"]\n    return security_groups\n```"
    },
    {
        "original": "def get_following(self):\n        \"\"\"\n        :calls: `GET /users/:user/following <http://developer.github.com/v3/users/followers>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            NamedUser,\n            self._requester,\n            self.url + \"/following\",\n            None\n        )",
        "sampled": "**Problem Analysis**\n\nThe problem appears to be related to interacting with the GitHub API, specifically retrieving a list of users that are being followed by another user.\n\n**Understanding**\n\nThe provided information suggests that we need to write a Python method `get_following` that returns a paginated list of users being followed by another user on GitHub.\n\n**Python Solution**\n```\nimport requests\n\nclass GitHubAPI:\n    def __init__(self, api_token):\n        self.api_token = api_token\n        self"
    },
    {
        "original": "def detect_model_num(string):\n    \"\"\"Takes a string related to a model name and extract its model number.\n\n    For example:\n        '000000-bootstrap.index' => 0\n    \"\"\"\n    match = re.match(MODEL_NUM_REGEX, string)\n    if match:\n        return int(match.group())\n    return None",
        "sampled": "```\ndef detect_model_num(string):\n    return int(''.join(filter(str.isdigit, string)))\n```"
    },
    {
        "original": "def onColorPicker(self):\n        \"\"\"\n        Show color-picker dialog to select color.\n\n        Qt will use the native dialog by default.\n\n        \"\"\"\n        dlg = QtGui.QColorDialog(QtGui.QColor(self._color), None)\n\n        # if self._color:\n        #    dlg.setCurrentColor(QtGui.QColor(self._color))\n\n        if dlg.exec_():\n            self.setColor(dlg.currentColor().name())",
        "sampled": "```\nfrom PyQt5.QtWidgets import QColorDialog\n\nclass ColorPicker:\n    def onColorPicker(self):\n        color_dialog = QColorDialog()\n        color_dialog.exec_()\n```"
    },
    {
        "original": "def check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    CLI Example::\n\n        salt '*' nftables.check_table nat\n    \"\"\"\n    ret = {'comment': '',\n           'result': False}\n\n    if not table:\n        ret['comment'] = 'Table needs to be specified'\n        return ret\n\n    nft_family = _NFTABLES_FAMILIES[family]\n    cmd = '{0} list tables {1}' . format(_nftables_cmd(), nft_family)\n    out = __salt__['cmd.run'](cmd, python_shell=False).find('table {0} {1}'.format(nft_family, table))\n\n    if out == -1:\n        ret['comment'] = 'Table {0} in family {1} does not exist'.\\\n                         format(table, family)\n    else:\n        ret['comment'] = 'Table {0} in family {1} exists'.\\\n                         format(table, family)\n        ret['result'] = True\n    return ret",
        "sampled": "```\n```python\nimport subprocess\n\ndef check_table(table=None, family='ipv4'):\n    if table is None:\n        raise ValueError(\"Table name is required\")\n\n    command = f'nft -{family} list table {table}'\n    \n    try:\n        subprocess.check_output(command.split())\n        return True\n    except subprocess.CalledProcessError:\n        return False\n```\n```"
    },
    {
        "original": "def mean(name, num, minimum=0, maximum=0, ref=None):\n    \"\"\"\n    Calculates the mean of the ``num`` most recent values. Requires a list.\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        foo:\n          calc.mean:\n            - name: myregentry\n            - num: 5\n    \"\"\"\n    return calc(\n        name=name,\n        num=num,\n        oper='mean',\n        minimum=minimum,\n        maximum=maximum,\n        ref=ref\n    )",
        "sampled": "```\ndef mean(name, num, minimum=0, maximum=0, ref=None):\n    if ref is None:\n        ref = []\n    ref.append(num)\n    if len(ref) > num:\n        ref.pop(0)\n    return sum(ref) / len(ref)\n```"
    },
    {
        "original": "def strxor(s1, s2):\n    \"\"\"\n    Returns the binary XOR of the 2 provided strings s1 and s2. s1 and s2\n    must be of same length.\n    \"\"\"\n    return b\"\".join(map(lambda x, y: chb(orb(x) ^ orb(y)), s1, s2))",
        "sampled": "```\ndef strxor(s1, s2):\n    if len(s1) != len(s2):\n        raise ValueError(\"s1 and s2 must be of same length\")\n    return \"\".join(chr(ord(a) ^ ord(b)) for a, b in zip(s1, s2))\n```"
    },
    {
        "original": "def get_ir_reciprocal_mesh(self, mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n        \"\"\"\n        k-point mesh of the Brillouin zone generated taken into account\n        symmetry.The method returns the irreducible kpoints of the mesh\n        and their weights\n\n        Args:\n            mesh (3x1 array): The number of kpoint for the mesh needed in\n                each direction\n            is_shift (3x1 array): Whether to shift the kpoint grid. (1, 1,\n            1) means all points are shifted by 0.5, 0.5, 0.5.\n\n        Returns:\n            A list of irreducible kpoints and their weights as a list of\n            tuples [(ir_kpoint, weight)], with ir_kpoint given\n            in fractional coordinates\n        \"\"\"\n        shift = np.array([1 if i else 0 for i in is_shift])\n        mapping, grid = spglib.get_ir_reciprocal_mesh(\n            np.array(mesh), self._cell, is_shift=shift, symprec=self._symprec)\n\n        results = []\n        for i, count in zip(*np.unique(mapping, return_counts=True)):\n            results.append(((grid[i] + shift * (0.5, 0.5, 0.5)) / mesh,\n                            count))\n        return results",
        "sampled": "```\nimport numpy as np\n\ndef get_ir_reciprocal_mesh(mesh=(10, 10, 10), is_shift=(0, 0, 0)):\n    mesh = np.array(mesh)\n    is_shift = np.array(is_shift)\n    \n    # Generate all kpoints\n    kpoints = np.array(np.meshgrid(*[np.arange(i) for i in mesh])).T.reshape(-1, 3)\n    kpoints = kpoints / mesh\n    \n    # Apply shift"
    },
    {
        "original": "def json_pretty_dump(obj, filename):\n    \"\"\"\n    Serialize obj as a JSON formatted stream to the given filename (\n    pretty printing version)\n    \"\"\"\n    with open(filename, \"wt\") as fh:\n        json.dump(obj, fh, indent=4, sort_keys=4)",
        "sampled": "```\nimport json\n\ndef json_pretty_dump(obj, filename):\n    with open(filename, 'w') as f:\n        json.dump(obj, f, indent=4)\n```"
    },
    {
        "original": "def _hdparm(args, failhard=True):\n    \"\"\"\n    Execute hdparm\n    Fail hard when required\n    return output when possible\n    \"\"\"\n    cmd = 'hdparm {0}'.format(args)\n    result = __salt__['cmd.run_all'](cmd)\n    if result['retcode'] != 0:\n        msg = '{0}: {1}'.format(cmd, result['stderr'])\n        if failhard:\n            raise CommandExecutionError(msg)\n        else:\n            log.warning(msg)\n\n    return result['stdout']",
        "sampled": "```\nimport subprocess\n\ndef _hdparm(args, failhard=True):\n    try:\n        output = subprocess.check_output(['hdparm'] + args)\n        return output.decode('utf-8')\n    except subprocess.CalledProcessError as e:\n        if failhard:\n            raise\n        else:\n            return str(e)\n```"
    },
    {
        "original": "def update(self, z):\n        \"\"\"\n        Add a new measurement `z` to the H-Infinity filter. If `z` is None,\n        nothing is changed.\n\n        Parameters\n        ----------\n        z : ndarray\n            measurement for this update.\n        \"\"\"\n\n        if z is None:\n            return\n\n        # rename for readability and a tiny extra bit of speed\n        I = self._I\n        gamma = self.gamma\n        Q = self.Q\n        H = self.H\n        P = self.P\n        x = self.x\n        V_inv = self._V_inv\n        F = self.F\n        W = self.W\n\n        # common subexpression H.T * V^-1\n        HTVI = dot(H.T, V_inv)\n\n        L = linalg.inv(I - gamma * dot(Q, P) + dot(HTVI, H).dot(P))\n\n        # common subexpression P*L\n        PL = dot(P, L)\n\n        K = dot(F, PL).dot(HTVI)\n\n        self.y = z - dot(H, x)\n\n        # x = x + Ky\n        # predict new x with residual scaled by the H-Infinity gain\n        self.x = self.x + dot(K, self.y)\n        self.P = dot(F, PL).dot(F.T) + W\n\n        # force P to be symmetric\n        self.P = (self.P + self.P.T) / 2\n\n        # pylint: disable=bare-except\n        try:\n            self.z = np.copy(z)\n        except:\n            self.z = copy.deepcopy(z)",
        "sampled": "```\nclass HInfinityFilter:\n    def __init__(self):\n        self.measurements = []\n\n    def update(self, z):\n        if z is not None:\n            self.measurements.append(z)\n```"
    },
    {
        "original": "def exhaust_stream(f):\n    \"\"\"Helper decorator for methods that exhausts the stream on return.\"\"\"\n\n    def wrapper(self, stream, *args, **kwargs):\n        try:\n            return f(self, stream, *args, **kwargs)\n        finally:\n            exhaust = getattr(stream, \"exhaust\", None)\n            if exhaust is not None:\n                exhaust()\n            else:\n                while 1:\n                    chunk = stream.read(1024 * 64)\n                    if not chunk:\n                        break\n\n    return update_wrapper(wrapper, f)",
        "sampled": "```\ndef exhaust_stream(f):\n    def wrapper(*args, **kwargs):\n        result = f(*args, **kwargs)\n        if hasattr(result, '__iter__'):\n            list(result)\n        return result\n    return wrapper\n```"
    },
    {
        "original": "def PushState(self, **_):\n    \"\"\"Push the current state on the state stack.\"\"\"\n    if self.verbose:\n      logging.debug(\"Storing state %r\", self.state)\n    self.state_stack.append(self.state)",
        "sampled": "I'm happy to help! However, I notice that the problem description is missing. Could you please provide the complete problem description, input, and output specifications? I'll be happy to help you understand and analyze the problem and provide a Python solution."
    },
    {
        "original": "def _execute_handling(self, *eopatches, **kwargs):\r\n        \"\"\" Handles measuring execution time and error propagation\r\n        \"\"\"\r\n        self.private_task_config.start_time = datetime.datetime.now()\r\n\r\n        caught_exception = None\r\n        try:\r\n            return_value = self.execute(*eopatches, **kwargs)\r\n        except BaseException as exception:\r\n            caught_exception = exception, sys.exc_info()[2]\r\n\r\n        if caught_exception is not None:  # Exception is not raised in except statement to prevent duplicated traceback\r\n            exception, traceback = caught_exception\r\n            raise type(exception)('During execution of task {}: {}'.format(self.__class__.__name__,\r\n                                                                           exception)).with_traceback(traceback)\r\n\r\n        self.private_task_config.end_time = datetime.datetime.now()\r\n        return return_value",
        "sampled": "I'm happy to help! However, I notice that the problem description, input, and output specifications are missing. Could you please provide the complete challenge description in Markdown format, including the math formula in inline LaTeX? I'll be happy to analyze the problem and provide a Python solution for you."
    },
    {
        "original": "def _cells(self):\n        \"\"\"\n        A sequence of |_Cell| objects, one for each cell of the layout grid.\n        If the table contains a span, one or more |_Cell| object references\n        are repeated.\n        \"\"\"\n        col_count = self._column_count\n        cells = []\n        for tc in self._tbl.iter_tcs():\n            for grid_span_idx in range(tc.grid_span):\n                if tc.vMerge == ST_Merge.CONTINUE:\n                    cells.append(cells[-col_count])\n                elif grid_span_idx > 0:\n                    cells.append(cells[-1])\n                else:\n                    cells.append(_Cell(tc, self))\n        return cells",
        "sampled": "I'm happy to help! However, I notice that there is no problem description provided in your prompt. Could you please provide the problem description, input, and output specifications? I'll do my best to assist you once I understand what needs to be solved! \ud83d\ude0a"
    },
    {
        "original": "def _cs_path_exists(fspath):\n        \"\"\"\n        Case-sensitive path existence check\n\n        >>> sdist_add_defaults._cs_path_exists(__file__)\n        True\n        >>> sdist_add_defaults._cs_path_exists(__file__.upper())\n        False\n        \"\"\"\n        if not os.path.exists(fspath):\n            return False\n        # make absolute so we always have a directory\n        abspath = os.path.abspath(fspath)\n        directory, filename = os.path.split(abspath)\n        return filename in os.listdir(directory)",
        "sampled": "```\ndef _cs_path_exists(fspath):\n    return fspath.exists()\n```"
    },
    {
        "original": "def addfield(self, pkt, s, val):\n        \"\"\"Add an internal value to a string\"\"\"\n        if self.length_of(pkt) == 8:\n            return s + struct.pack(self.fmt[0] + \"B\", val)\n        if self.length_of(pkt) == 16:\n            return s + struct.pack(self.fmt[0] + \"H\", val)\n        if self.length_of(pkt) == 32:\n            return s + struct.pack(self.fmt[0] + \"2H\", val)  # TODO: fix!\n        if self.length_of(pkt) == 48:\n            return s + struct.pack(self.fmt[0] + \"3H\", val)  # TODO: fix!\n        elif self.length_of(pkt) == 64:\n            return s + struct.pack(self.fmt[0] + \"Q\", val)\n        elif self.length_of(pkt) == 128:\n            # TODO: FIX THE PACKING!!\n            return s + struct.pack(self.fmt[0] + \"16s\", raw(val))\n        else:\n            return s",
        "sampled": "```\nclass ProgrammingAssistant:\n    def addfield(self, pkt, s, val):\n        if,val not None:\n            setattr(pkt,s,val)\n```"
    },
    {
        "original": "def _file_path(self, dirname, filename):\n        \"\"\"\n        Builds an absolute path and creates the directory and file if they don't already exist.\n\n        @dirname  - Directory path.\n        @filename - File name.\n\n        Returns a full path of 'dirname/filename'.\n        \"\"\"\n        if not os.path.exists(dirname):\n            try:\n                os.makedirs(dirname)\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        fpath = os.path.join(dirname, filename)\n\n        if not os.path.exists(fpath):\n            try:\n                open(fpath, \"w\").close()\n            except KeyboardInterrupt as e:\n                raise e\n            except Exception:\n                pass\n\n        return fpath",
        "sampled": "```\nimport os\n\ndef _file_path(self, dirname, filename):\n    full_path = os.path.join(dirname, filename)\n    dir_path = os.path.dirname(full_path)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    return full_path\n```"
    },
    {
        "original": "def _get_stats_column_names(cls):\n        \"\"\"Construct a tuple of the column names for stats. Each stat has 4\n        columns of data.\n        \"\"\"\n        columns = []\n        stats = ('num_bytes', 'num_rows', 'location', 'partitioning_type',\n                 'clustering_fields')\n        stat_components = ('label', 'value', 'description', 'include')\n        for stat_id in stats:\n            for stat_component in stat_components:\n                columns.append('stats:{}:{}'.format(stat_id, stat_component))\n        return tuple(columns)",
        "sampled": "```\ndef _get_stats_column_names(cls, stats):\n    return tuple(f\"{stat}_{col}\" for stat in stats for col in [\"mean\", \"std\", \"min\", \"max\"])\n```"
    },
    {
        "original": "def structures(self):\n        \"\"\"\n        Copy of all structures in the TransformedStructure. A\n        structure is stored after every single transformation.\n        \"\"\"\n        hstructs = [Structure.from_dict(s['input_structure'])\n                    for s in self.history if 'input_structure' in s]\n        return hstructs + [self.final_structure]",
        "sampled": "```\nclass TransformedStructure:\n    def __init__(self):\n        self.structures = []\n\n    def add_structure(self, structure):\n        self.structures.append(structure)\n\n    def get_structures(self):\n        return self.structures.copy()\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'level') and self.level is not None:\n            _dict['level'] = self.level\n        if hasattr(self, 'names') and self.names is not None:\n            _dict['names'] = self.names\n        return _dict",
        "sampled": "I'm happy to help! However, I notice that the problem description, input, and output specifications are missing. Could you please provide the complete problem statement in Markdown format? I'll be happy to analyze the problem and provide a Python solution for the challenge."
    },
    {
        "original": "def norm_cdf(x_range, mu, var=1, std=None):\n    \"\"\"\n    Computes the probability that a Gaussian distribution lies\n    within a range of values.\n\n    Parameters\n    ----------\n\n    x_range : (float, float)\n        tuple of range to compute probability for\n\n    mu : float\n        mean of the Gaussian\n\n    var : float, optional\n        variance of the Gaussian. Ignored if `std` is provided\n\n    std : float, optional\n       standard deviation of the Gaussian. This overrides the `var` parameter\n\n    Returns\n    -------\n\n    probability : float\n        probability that Gaussian is within x_range. E.g. .1 means 10%.\n    \"\"\"\n\n    if std is None:\n        std = math.sqrt(var)\n    return abs(norm.cdf(x_range[0], loc=mu, scale=std) -\n               norm.cdf(x_range[1], loc=mu, scale=std))",
        "sampled": "```\nimport math \nfrom scipy.stats import norm \n\ndef norm_cdf(x_range, mu, var=1, std=None):\n  if std is None:\n      std = math.sqrt(var)\n  \n  lower_bound = (x_range[0] - mu) / std \n  upper_bound = (x_range[1] - mu) / std \n  \n  lower_prob = norm.cdf(lower_bound) \n  upper_prob = norm.cdf(upper_bound)\n  \n  return upper_prob"
    },
    {
        "original": "def upgrade(refresh=True, **kwargs):\n    \"\"\"\n    Upgrade outdated, unpinned brews.\n\n    refresh\n        Fetch the newest version of Homebrew and all formulae from GitHub before installing.\n\n    Returns a dictionary containing the changes:\n\n    .. code-block:: python\n\n        {'<package>':  {'old': '<old-version>',\n                        'new': '<new-version>'}}\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.upgrade\n    \"\"\"\n    ret = {'changes': {},\n           'result': True,\n           'comment': '',\n           }\n\n    old = list_pkgs()\n\n    if salt.utils.data.is_true(refresh):\n        refresh_db()\n\n    result = _call_brew('upgrade', failhard=False)\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if result['retcode'] != 0:\n        raise CommandExecutionError(\n            'Problem encountered upgrading packages',\n            info={'changes': ret, 'result': result}\n        )\n\n    return ret",
        "sampled": "```\nimport subprocess\nimport json\n\ndef get_current_versions():\n    result = subprocess.run(['brew', 'list', '--versions'], stdout=subprocess.PIPE)\n    return dict(line.split(' ') for line in result.stdout.decode('utf-8').strip().split('\\n'))\n\ndef get_latest_versions():\n    result = subprocess.run(['brew', 'outdated'], stdout=subprocess.PIPE)\n    return dict(line.split(' ') for line in result.stdout.decode('utf-8').strip().split('\\n'))\n\n"
    },
    {
        "original": "def prev_moment_operating_on(\n            self,\n            qubits: Sequence[ops.Qid],\n            end_moment_index: Optional[int] = None,\n            max_distance: Optional[int] = None) -> Optional[int]:\n        \"\"\"Finds the index of the next moment that touches the given qubits.\n\n        Args:\n            qubits: We're looking for operations affecting any of these qubits.\n            end_moment_index: The moment index just after the starting point of\n                the reverse search. Defaults to the length of the list of\n                moments.\n            max_distance: The number of moments (starting just before from the\n                end index and moving backward) to check. Defaults to no limit.\n\n        Returns:\n            None if there is no matching moment, otherwise the index of the\n            latest matching moment.\n\n        Raises:\n            ValueError: negative max_distance.\n        \"\"\"\n        if end_moment_index is None:\n            end_moment_index = len(self._moments)\n\n        if max_distance is None:\n            max_distance = len(self._moments)\n        elif max_distance < 0:\n            raise ValueError('Negative max_distance: {}'.format(max_distance))\n        else:\n            max_distance = min(end_moment_index, max_distance)\n\n        # Don't bother searching indices past the end of the list.\n        if end_moment_index > len(self._moments):\n            d = end_moment_index - len(self._moments)\n            end_moment_index -= d\n            max_distance -= d\n        if max_distance <= 0:\n            return None\n\n        return self._first_moment_operating_on(qubits,\n                                               (end_moment_index - k - 1\n                                                for k in range(max_distance)))",
        "sampled": "```\ndef prev_moment_operating_on(self, qubits, end_moment_index=None, max_distance=None):\n    if max_distance is not None and max_distance < 0:\n        raise ValueError(\"max_distance cannot be negative\")\n    \n    if end_moment_index is None:\n        end_moment_index = len(self.moments)\n    \n    for i in range(end_moment_index - 1, -1, -1):\n        moment = self.moments[i]\n       "
    },
    {
        "original": "def _fix_ctx(m2_ctx, issuer=None):\n    \"\"\"\n    This is part of an ugly hack to fix an ancient bug in M2Crypto\n    https://bugzilla.osafoundation.org/show_bug.cgi?id=7530#c13\n    \"\"\"\n    ctx = _Ctx.from_address(int(m2_ctx))  # pylint: disable=no-member\n\n    ctx.flags = 0\n    ctx.subject_cert = None\n    ctx.subject_req = None\n    ctx.crl = None\n    if issuer is None:\n        ctx.issuer_cert = None\n    else:\n        ctx.issuer_cert = int(issuer.x509)",
        "sampled": "```\ndef _fix_ctx(m2_ctx, issuer=None):\n    if issuer is not None:\n        m2_ctx.set_app_data(issuer)\n    else:\n        m2_ctx.set_app_data(None)\n```"
    },
    {
        "original": "def get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Return a storage_conn object for the storage account\n    \"\"\"\n    if conn_kwargs is None:\n        conn_kwargs = {}\n\n    if not storage_account:\n        storage_account = config.get_cloud_config_value(\n            'storage_account',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_account', None)\n        )\n    if not storage_key:\n        storage_key = config.get_cloud_config_value(\n            'storage_key',\n            get_configured_provider(), __opts__, search_global=False,\n            default=conn_kwargs.get('storage_key', None)\n        )\n    return azure.storage.BlobService(storage_account, storage_key)",
        "sampled": "```\nfrom azure.storage.blob import BlobServiceClient\n\ndef get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):\n    if storage_account and storage_key:\n        conn_str = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_key};BlobEndpoint=https://{storage_account}.blob.core.windows.net/\"\n        return BlobServiceClient.from_connection_string(conn_str, **conn_kwargs)\n    else:\n        return None\n```"
    },
    {
        "original": "def create_profile(hostname, username, password, profile_type, name, **kwargs):\n    r\"\"\"\n    A function to connect to a bigip device and create a profile.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    profile_type\n        The type of profile to create\n    name\n        The name of the profile to create\n    kwargs\n        [ arg=val ] ...\n\n        Consult F5 BIGIP user guide for specific options for each profile type.\n        Typically, tmsh arg names are used.\n\n    Special Characters ``|``, ``,`` and ``:`` must be escaped using ``\\`` when\n    used within strings.\n\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    #is this profile currently configured?\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    # if it exists\n    if existing['code'] == 200:\n\n        ret['result'] = True\n        ret['comment'] = 'A profile by this name currently exists.  No change made.'\n\n    # if it doesn't exist\n    elif existing['code'] == 404:\n\n        response = __salt__['bigip.create_profile'](hostname, username, password, profile_type, name, **kwargs)\n\n        if response['code'] == 200:\n            ret['result'] = True\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = response['content']\n            ret['comment'] = 'Profile was successfully created.'\n        else:\n            ret = _load_result(response, ret)\n\n    # else something else was returned\n    else:\n        ret = _load_result(existing, ret)\n\n    return ret",
        "sampled": "```\nimport requests\nimport json\n\ndef create_profile(hostname, username, password, profile_type, name, **kwargs):\n    auth = (username, password)\n    url = f\"https://{hostname}/mgmt/tm/{profile_type}\"\n    payload = {\"name\": name}\n    for key, value in kwargs.items():\n        payload[key] = value\n    response = requests.post(url, auth=auth, json=payload)\n    if response.status_code == 200:\n        return response"
    },
    {
        "original": "def create_policy(name, policy_name, policy_type, policy, region=None,\n                  key=None, keyid=None, profile=None):\n    \"\"\"\n    Create an ELB policy.\n\n    .. versionadded:: 2016.3.0\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt myminion boto_elb.create_policy myelb mypolicy LBCookieStickinessPolicyType '{\"CookieExpirationPeriod\": 3600}'\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    if not exists(name, region, key, keyid, profile):\n        return False\n    try:\n        success = conn.create_lb_policy(name, policy_name, policy_type, policy)\n        if success:\n            log.info('Created policy %s on ELB %s', policy_name, name)\n            return True\n        else:\n            log.error('Failed to create policy %s on ELB %s', policy_name, name)\n            return False\n    except boto.exception.BotoServerError as e:\n        log.error('Failed to create policy %s on ELB %s: %s',\n                  policy_name, name, e.message,\n                  exc_info_on_loglevel=logging.DEBUG)\n        return False",
        "sampled": "```\n```python\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef create_policy(name, policy_name, policy_type, policy_data,\n                  region=None,\n                  key=None,\n                  keyid=None,\n                  profile=None):\n    elb = boto3.client('elb', region_name=region)\n    \n    try:\n        response = elb.create_load_balancer_policy(\n            LoadBalancerName=name,\n            PolicyName=policy_name,\n            PolicyTypeName=policy_type,\n"
    },
    {
        "original": "def _finalize_axis(self, key, **kwargs):\n        \"\"\"\n        Extends the ElementPlot _finalize_axis method to set appropriate\n        labels, and axes options for 3D Plots.\n        \"\"\"\n        axis = self.handles['axis']\n        self.handles['fig'].set_frameon(False)\n        axis.grid(self.show_grid)\n        axis.view_init(elev=self.elevation, azim=self.azimuth)\n        axis.dist = self.distance\n\n        if self.xaxis is None:\n            axis.w_xaxis.line.set_lw(0.)\n            axis.w_xaxis.label.set_text('')\n        if self.yaxis is None:\n            axis.w_yaxis.line.set_lw(0.)\n            axis.w_yaxis.label.set_text('')\n        if self.zaxis is None:\n            axis.w_zaxis.line.set_lw(0.)\n            axis.w_zaxis.label.set_text('')\n        if self.disable_axes:\n            axis.set_axis_off()\n\n        if mpl_version <= '1.5.9':\n            axis.set_axis_bgcolor(self.bgcolor)\n        else:\n            axis.set_facecolor(self.bgcolor)\n        return super(Plot3D, self)._finalize_axis(key, **kwargs)",
        "sampled": "```\ndef _finalize_axis(self, key, **kwargs):\n    \"\"\"\n    Extends the ElementPlot _finalize_axis method to set appropriate\n    labels, and axes options for 3D Plots.\n    \"\"\"\n    if self.dim == 3:\n        axis = self.handles['axis']\n        axis.set_xlabel(kwargs.get('xlabel', 'X Axis'))\n        axis.set_ylabel(kwargs.get('ylabel', 'Y Axis'))\n        axis.set_zlabel(kwargs.get('zlabel', 'Z"
    },
    {
        "original": "def _calendar_month_middles(year):\n    \"\"\"List of middle day of each month, used by Linke turbidity lookup\"\"\"\n    # remove mdays[0] since January starts at mdays[1]\n    # make local copy of mdays since we need to change\n    # February for leap years\n    mdays = np.array(calendar.mdays[1:])\n    ydays = 365\n    # handle leap years\n    if calendar.isleap(year):\n        mdays[1] = mdays[1] + 1\n        ydays = 366\n    middles = np.concatenate(\n        [[-calendar.mdays[-1] / 2.0],  # Dec last year\n         np.cumsum(mdays) - np.array(mdays) / 2.,  # this year\n         [ydays + calendar.mdays[1] / 2.0]])  # Jan next year\n    return middles",
        "sampled": "```\nfrom calendar import monthrange\nfrom datetime import datetime\n\ndef _calendar_month_middles(year):\n    middles = []\n    for month in range(1, 13):\n        _, num_days = monthrange(year, month)\n        middles.append((datetime(year, month, (num_days + 1) // 2)).strftime(\"%Y-%m-%d\"))\n    return middles\n```"
    },
    {
        "original": "def reduced_formula(self):\n        \"\"\"\n        Returns a reduced formula string with appended charge.\n        \"\"\"\n        reduced_formula = super().reduced_formula\n        charge = self._charge / self.get_reduced_composition_and_factor()[1]\n        if charge > 0:\n            if abs(charge) == 1:\n                chg_str = \"[+]\"\n            else:\n                chg_str = \"[\" + formula_double_format(charge, False) + \"+]\"\n        elif charge < 0:\n            if abs(charge) == 1:\n                chg_str = \"[-]\"\n            else:\n                chg_str = \"[{}-]\".format(formula_double_format(abs(charge),\n                                                               False))\n        else:\n            chg_str = \"(aq)\"\n        return reduced_formula + chg_str",
        "sampled": "```\nclass Formula:\n    def __init__(self, formula):\n        self.formula = formula\n\n    def reduced_formula(self):\n        elements = {}\n        i = 0\n        while i < len(self.formula):\n            if self.formula[i].isupper():\n                element = self.formula[i]\n                i += 1\n                if i < len(self.formula) and self.formula[i].isdigit():\n                    j = i\n                    while j < len(self.formula"
    },
    {
        "original": "def query(self, *args):\n        \"\"\" Send a query to the watchman service and return the response\n\n        This call will block until the response is returned.\n        If any unilateral responses are sent by the service in between\n        the request-response they will be buffered up in the client object\n        and NOT returned via this method.\n        \"\"\"\n\n        log(\"calling client.query\")\n        self._connect()\n        try:\n            self.sendConn.send(args)\n\n            res = self.receive()\n            while self.isUnilateralResponse(res):\n                res = self.receive()\n\n            return res\n        except EnvironmentError as ee:\n            # When we can depend on Python 3, we can use PEP 3134\n            # exception chaining here.\n            raise WatchmanEnvironmentError(\n                \"I/O error communicating with watchman daemon\",\n                ee.errno,\n                ee.strerror,\n                args,\n            )\n        except WatchmanError as ex:\n            ex.setCommand(args)\n            raise",
        "sampled": "I'm happy to help! However, I noticed that there seems to be some confusion - you provided instructions for something called `query` method which seems unrelated to any specific problem.\n\nCould you please provide the actual problem description, input, and output specifications? That way, I can assist you in solving it step by step?"
    },
    {
        "original": "def find_cell_end(self, lines):\n        \"\"\"Return position of end of cell marker, and position of first line after cell\"\"\"\n        if self.metadata is None and not (self.cell_marker_end and self.end_code_re.match(lines[0])) \\\n                and paragraph_is_fully_commented(lines, self.comment, self.default_language):\n            self.cell_type = 'markdown'\n            for i, line in enumerate(lines):\n                if _BLANK_LINE.match(line):\n                    return i, i + 1, False\n            return len(lines), len(lines), False\n\n        if self.metadata is None:\n            self.end_code_re = None\n        elif not self.cell_marker_end:\n            end_of_cell = self.metadata.get('endofcell', '-')\n            self.end_code_re = re.compile('^' + self.comment + ' ' + end_of_cell + r'\\s*$')\n\n        return self.find_region_end(lines)",
        "sampled": "```\ndef find_cell_end(self, lines):\n    in_cell = False\n    end_idx = 0\n    start_idx = 0\n    for i, line in enumerate(lines):\n        if '```' in line and not in_cell:\n            start_idx = i\n            in_cell = True\n        elif '```' in line and in_cell:\n            end_idx = i\n            break\n    return end_idx + 1, start_idx if start_idx != 0 else"
    },
    {
        "original": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered",
        "sampled": "```\ndef _filter_cache(self, dmap, kdims):\n    filtered_cache = {}\n    for key, value in dmap.cache.items():\n        if all(k in key and key[k] == v for k, v in kdims.items()):\n            filtered_cache[key] = value\n    return filtered_cache\n```"
    },
    {
        "original": "def _parse_dict_string(self, string, key, default):\n        \"\"\"Take from a more recent redis.py, parse_info\"\"\"\n        try:\n            for item in string.split(','):\n                k, v = item.rsplit('=', 1)\n                if k == key:\n                    try:\n                        return int(v)\n                    except ValueError:\n                        return v\n            return default\n        except Exception:\n            self.log.exception(\"Cannot parse dictionary string: %s\" % string)\n            return default",
        "sampled": "```\ndef parse_dict_string(string, key, default):\n    result = {}\n    items = string.split(',')\n    for item in items:\n        k, v = item.split('=')\n        result[k] = v\n    return result.get(key, default)\n```"
    },
    {
        "original": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        index = self.get_dimension_index(dimension)\n        if index == 0:\n            return np.array([self.x])\n        elif index == 1:\n            return np.array([self.y])\n        else:\n            return super(Arrow, self).dimension_values(dimension)",
        "sampled": "```\nimport numpy as np\n\nclass DimensionValues:\n    def __init__(self, array):\n        self.array = array\n\n    def dimension_values(self, dimension, expanded=True, flat=True):\n        values = self.array.take(indices=0, axis=dimension)\n        if expanded:\n            values = np.expand_dims(values, axis=dimension)\n        if flat:\n            values = values.flatten()\n        return values\n```"
    },
    {
        "original": "def enable(self):\n        \"\"\"Return True|False if the AMP is enabled in the configuration file (enable=true|false).\"\"\"\n        ret = self.get('enable')\n        if ret is None:\n            return False\n        else:\n            return ret.lower().startswith('true')",
        "sampled": "**Problem Analysis**\n\nThe problem requires creating a method `enable` that returns `True` or `False` based on whether \"AMP\" is enabled in a configuration file.\n\n**Python Solution**\n```python\nclass Config:\n    def __init__(self, config_file):\n        self.config_file = config_file\n\n    def enable(self):\n        with open(self.config_file, 'r') as f:\n            for line in f:\n                if 'enable=' in line:\n                    return 'true'"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "sampled": "```\ndef _convert_validators_to_mapping(validators):\n    validators_mapping = {}\n    for validator in validators:\n        check = validator[\"check\"]\n        if not isinstance(check, str):\n            check = str(check)\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n    return validators_mapping\n```"
    },
    {
        "original": "def ParseAction(self, action):\n    \"\"\"Extract log configuration data from rsyslog actions.\n\n    Actions have the format:\n      <facility>/<severity> <type_def><destination>;<template>\n      e.g. *.* @@loghost.example.com.:514;RSYSLOG_ForwardFormat\n\n    Actions are selected by a type definition. These include:\n      \"@@\": TCP syslog\n      \"@\": UDP syslog\n      \"|\": Named pipe\n      \"~\": Drop to /dev/null\n      \"^\": Shell script\n      \":om<string>:\": An output module\n      Or a file path.\n\n    Args:\n      action: The action string from rsyslog.\n\n    Returns:\n      a rdfvalue.LogTarget message.\n    \"\"\"\n    rslt = rdf_config_file.LogTarget()\n    for dst_str, dst_re in iteritems(self.destinations):\n      dst = dst_re.match(action)\n      if dst:\n        rslt.transport = dst_str\n        rslt.destination = dst.group(1)\n        break\n    return rslt",
        "sampled": "```\nimport re\nfrom rdfvalue import LogTarget\n\ndef ParseAction(action):\n    facility_severity, rest = action.split(' ', 1)\n    facility, severity = facility_severity.split('.')\n    \n    type_def, destination_template = rest.split(';', 1)\n    \n    if type_def == '@@':\n        protocol = 'TCP'\n    elif type_def == '@':\n        protocol = 'UDP'\n    elif type_def == '|':\n        protocol = 'Named Pipe'\n    elif"
    },
    {
        "original": "def modified_policy_iteration(self, v_init=None, epsilon=None,\n                                  max_iter=None, k=20):\n        \"\"\"\n        Solve the optimization problem by modified policy iteration. See\n        the `solve` method.\n\n        \"\"\"\n        if self.beta == 1:\n            raise NotImplementedError(self._error_msg_no_discounting)\n\n        if max_iter is None:\n            max_iter = self.max_iter\n        if epsilon is None:\n            epsilon = self.epsilon\n\n        def span(z):\n            return z.max() - z.min()\n\n        def midrange(z):\n            return (z.min() + z.max()) / 2\n\n        v = np.empty(self.num_states)\n        if v_init is None:\n            v[:] = self.R[self.R > -np.inf].min() / (1 - self.beta)\n        else:\n            v[:] = v_init\n\n        u = np.empty(self.num_states)\n        sigma = np.empty(self.num_states, dtype=int)\n\n        try:\n            tol = epsilon * (1-self.beta) / self.beta\n        except ZeroDivisionError:  # Raised if beta = 0\n            tol = np.inf\n\n        for i in range(max_iter):\n            # Policy improvement\n            self.bellman_operator(v, Tv=u, sigma=sigma)\n            diff = u - v\n            if span(diff) < tol:\n                v[:] = u + midrange(diff) * self.beta / (1 - self.beta)\n                break\n            # Partial policy evaluation with k iterations\n            self.operator_iteration(T=self.T_sigma(sigma), v=u, max_iter=k)\n            v[:] = u\n\n        num_iter = i + 1\n\n        res = DPSolveResult(v=v,\n                            sigma=sigma,\n                            num_iter=num_iter,\n                            mc=self.controlled_mc(sigma),\n                            method='modified policy iteration',\n                            epsilon=epsilon,\n                            max_iter=max_iter,\n                            k=k)\n\n        return res",
        "sampled": "```\nclass MDP:\n    def __init__(self, states, actions, transition_model, reward_fn):\n        self.states = states\n        self.actions = actions\n        self.transition_model = transition_model\n        self.reward_fn = reward_fn\n\n    def modified_policy_iteration(self, v_init=None, epsilon=1e-8, max_iter=1000, k=20):\n        if v_init is None:\n            v_init = {s: 0 for s in self.states"
    },
    {
        "original": "def parse_env_file(env_file):\n    \"\"\"\n    Reads a line-separated environment file.\n    The format of each line should be \"key=value\".\n    \"\"\"\n    environment = {}\n\n    with open(env_file, 'r') as f:\n        for line in f:\n\n            if line[0] == '#':\n                continue\n\n            line = line.strip()\n            if not line:\n                continue\n\n            parse_line = line.split('=', 1)\n            if len(parse_line) == 2:\n                k, v = parse_line\n                environment[k] = v\n            else:\n                raise errors.DockerException(\n                    'Invalid line in environment file {0}:\\n{1}'.format(\n                        env_file, line))\n\n    return environment",
        "sampled": "```\ndef parse_env_file(env_file):\n    env_dict = {}\n    with open(env_file, 'r') as f:\n        for line in f:\n            key_value = line.strip().split('=')\n            if len(key_value) == 2:\n                key, value = key_value\n                env_dict[key] = value\n    return env_dict\n```"
    },
    {
        "original": "def declination_spencer71(dayofyear):\n    \"\"\"\n    Solar declination from Duffie & Beckman [1] and attributed to\n    Spencer (1971) and Iqbal (1983).\n\n    .. warning::\n        Return units are radians, not degrees.\n\n    Parameters\n    ----------\n    dayofyear : numeric\n\n    Returns\n    -------\n    declination (radians) : numeric\n        Angular position of the sun at solar noon relative to the plane of the\n        equator, approximately between +/-23.45 (degrees).\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition\" pp. 13-14, J. Wiley and Sons, New York (2006)\n\n    [2] J. W. Spencer, \"Fourier series representation of the position of the\n    sun\" in Search 2 (5), p. 172 (1971)\n\n    [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n    Energy Applications\", p. 4 CRC Press (2013)\n\n    See Also\n    --------\n    declination_cooper69\n    \"\"\"\n    day_angle = _calculate_simple_day_angle(dayofyear)\n    return (\n        0.006918 -\n        0.399912 * np.cos(day_angle) + 0.070257 * np.sin(day_angle) -\n        0.006758 * np.cos(2. * day_angle) + 0.000907 * np.sin(2. * day_angle) -\n        0.002697 * np.cos(3. * day_angle) + 0.00148 * np.sin(3. * day_angle)\n    )",
        "sampled": "```\nimport math\n\ndef declination_spencer71(dayofyear):\n    \"\"\"\n    Solar declination from Duffie & Beckman [1] and attributed to\n    Spencer (1971) and Iqbal (1983).\n\n    .. warning::\n        Return units are radians, not degrees.\n\n    Parameters\n    ----------\n    dayofyear : numeric\n\n    Returns\n    -------\n    declination (radians) : numeric\n        Angular position of the sun at solar noon relative to"
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "sampled": "```\nclass EOPatch:\n    def __init__(self, data):\n        self.data = data\n\ndef save(self, eopatch, use_tmp=True):\n    if use_tmp:\n        with open(\"temp_file.txt\", \"w\") as f:\n            f.write(str(eopatch.data))\n    else:\n        with open(\"final_file.txt\", \"w\") as f:\n            f.write(str(eopatch.data))\n```"
    },
    {
        "original": "def add_to_collaborators(self, collaborator, permission=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `PUT /repos/:owner/:repo/collaborators/:user <http://developer.github.com/v3/repos/collaborators>`_\n        :param collaborator: string or :class:`github.NamedUser.NamedUser`\n        :param permission: string 'pull', 'push' or 'admin'\n        :rtype: None\n        \"\"\"\n        assert isinstance(collaborator, github.NamedUser.NamedUser) or isinstance(collaborator, (str, unicode)), collaborator\n        assert permission is github.GithubObject.NotSet or isinstance(permission, (str, unicode)), permission\n\n        if isinstance(collaborator, github.NamedUser.NamedUser):\n            collaborator = collaborator._identity\n\n        if permission is not github.GithubObject.NotSet:\n            put_parameters = {'permission': permission}\n        else:\n            put_parameters = None\n\n        headers, data = self._requester.requestJsonAndCheck(\n            \"PUT\",\n            self.url + \"/collaborators/\" + collaborator,\n            input=put_parameters\n        )\n        # return an invitation object if there's data returned by the API. If data is empty\n        # there's a pending invitation for the given user.\n        return github.Invitation.Invitation(self._requester, headers, data, completed=True) if \\\n            data is not None else None",
        "sampled": "```\nfrom github import Github\n\nclass GitHubRepository:\n    def __init__(self, repo_owner, repo_name):\n        self.repo_owner = repo_owner\n        self.repo_name = repo_name\n        self.github = Github()\n    \n    def add_to_collaborators(self, collaborator, permission=\"pull\"):\n        if permission not in [\"pull\", \"push\", \"admin\"]:\n            raise ValueError(\"Invalid permission\")\n        \n        repo = self.github.get_repo(f\"{self.repo_owner}/{self.repo"
    },
    {
        "original": "def read_and_write(self, reader, writer, chunk_size):\n        \"\"\"\n        Read ``chunk_size`` from ``reader``, writing result to ``writer``.\n\n        Returns ``None`` if successful, or ``True`` if the read was empty.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        data = reader.recv(chunk_size)\n        if len(data) == 0:\n            return True\n        writer.sendall(data)",
        "sampled": "```\ndef read_and_write(reader, writer, chunk_size):\n    chunk = reader.read(chunk_size)\n    if not chunk:\n        return True\n    writer.write(chunk)\n    return None\n```"
    },
    {
        "original": "def get_results(self, **kwargs):\n        \"\"\"\n        Returns :class:`NodeResults` instance.\n        Subclasses should extend this method (if needed) by adding\n        specialized code that performs some kind of post-processing.\n        \"\"\"\n        # Check whether the process completed.\n        if self.returncode is None:\n            raise self.Error(\"return code is None, you should call wait, communicate or poll\")\n\n        if self.status is None or self.status < self.S_DONE:\n            raise self.Error(\"Task is not completed\")\n\n        return self.Results.from_node(self)",
        "sampled": "I'm ready to assist you with the programming challenge! Please go ahead and provide the problem description, input, and output specifications in Markdown format, including any inline LaTeX formulas if needed.\n\nOnce I receive the problem details, I'll carefully analyze it and provide a Python solution for you in a single Markdown block.\n\nPlease go ahead and share the challenge!"
    },
    {
        "original": "def file_transfer(\n    ssh_conn,\n    source_file,\n    dest_file,\n    file_system=None,\n    direction=\"put\",\n    disable_md5=False,\n    inline_transfer=False,\n    overwrite_file=False,\n):\n    \"\"\"Use Secure Copy or Inline (IOS-only) to transfer files to/from network devices.\n\n    inline_transfer ONLY SUPPORTS TEXT FILES and will not support binary file transfers.\n\n    return {\n        'file_exists': boolean,\n        'file_transferred': boolean,\n        'file_verified': boolean,\n    }\n    \"\"\"\n    transferred_and_verified = {\n        \"file_exists\": True,\n        \"file_transferred\": True,\n        \"file_verified\": True,\n    }\n    transferred_and_notverified = {\n        \"file_exists\": True,\n        \"file_transferred\": True,\n        \"file_verified\": False,\n    }\n    nottransferred_but_verified = {\n        \"file_exists\": True,\n        \"file_transferred\": False,\n        \"file_verified\": True,\n    }\n\n    if \"cisco_ios\" in ssh_conn.device_type or \"cisco_xe\" in ssh_conn.device_type:\n        cisco_ios = True\n    else:\n        cisco_ios = False\n    if not cisco_ios and inline_transfer:\n        raise ValueError(\"Inline Transfer only supported for Cisco IOS/Cisco IOS-XE\")\n\n    scp_args = {\n        \"ssh_conn\": ssh_conn,\n        \"source_file\": source_file,\n        \"dest_file\": dest_file,\n        \"direction\": direction,\n    }\n    if file_system is not None:\n        scp_args[\"file_system\"] = file_system\n\n    TransferClass = InLineTransfer if inline_transfer else FileTransfer\n\n    with TransferClass(**scp_args) as scp_transfer:\n        if scp_transfer.check_file_exists():\n            if overwrite_file:\n                if not disable_md5:\n                    if scp_transfer.compare_md5():\n                        return nottransferred_but_verified\n                    else:\n                        # File exists, you can overwrite it, MD5 is wrong (transfer file)\n                        verifyspace_and_transferfile(scp_transfer)\n                        if scp_transfer.compare_md5():\n                            return transferred_and_verified\n                        else:\n                            raise ValueError(\n                                \"MD5 failure between source and destination files\"\n                            )\n                else:\n                    # File exists, you can overwrite it, but MD5 not allowed (transfer file)\n                    verifyspace_and_transferfile(scp_transfer)\n                    return transferred_and_notverified\n            else:\n                # File exists, but you can't overwrite it.\n                if not disable_md5:\n                    if scp_transfer.compare_md5():\n                        return nottransferred_but_verified\n                msg = \"File already exists and overwrite_file is disabled\"\n                raise ValueError(msg)\n        else:\n            verifyspace_and_transferfile(scp_transfer)\n            # File doesn't exist\n            if not disable_md5:\n                if scp_transfer.compare_md5():\n                    return transferred_and_verified\n                else:\n                    raise ValueError(\"MD5 failure between source and destination files\")\n            else:\n                return transferred_and_notverified",
        "sampled": "```\nimport paramiko\nimport hashlib\n\ndef file_transfer(\n    ssh_conn,\n    source_file,\n    dest_file,\n    file_system=None,\n    direction=\"put\",\n    disable_md5=False,\n    inline_transfer=False,\n    overwrite_file=False\n):\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    \n    if direction == \"put\":\n        sftp = ssh_conn.open_sftp()\n        sftp.put(source_file, dest"
    },
    {
        "original": "def move_stoploss(self, stoploss):\n        \"\"\"Modify stop order.\n        Auto-discover **orderId** and **quantity** and invokes ``self.modify_order(...)``.\n\n        :Parameters:\n            stoploss : float\n                the new stoploss limit price\n\n        \"\"\"\n        stopOrder = self.get_active_order(order_type=\"STOP\")\n\n        if stopOrder is not None and \"orderId\" in stopOrder.keys():\n            self.modify_order(orderId=stopOrder['orderId'],\n                              quantity=stopOrder['quantity'], limit_price=stoploss)",
        "sampled": "```\ndef modify_order(self, orderId: int, quantity: int):\n    # some implementation here \n\nclass TradingPlatform:\n    def __init__(self):\n        self.orderId = 0\n        self.quantity = 0\n\n    def move_stoploss(self, stoploss: float):\n        self.orderId = 1  # assume we have an orderId of 1\n        self.quantity = 10  # assume we have a quantity of 10\n        self.modify_order(self.order"
    },
    {
        "original": "def get_comments(self, since=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/comments <http://developer.github.com/v3/issues/comments>`_\n        :param since: datetime.datetime format YYYY-MM-DDTHH:MM:SSZ\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.IssueComment.IssueComment`\n        \"\"\"\n        assert since is github.GithubObject.NotSet or isinstance(since, datetime.datetime), since\n        url_parameters = dict()\n        if since is not github.GithubObject.NotSet:\n            url_parameters[\"since\"] = since.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        return github.PaginatedList.PaginatedList(\n            github.IssueComment.IssueComment,\n            self._requester,\n            self.url + \"/comments\",\n            url_parameters\n        )",
        "sampled": "```\nimport github\nfrom datetime import datetime\n\nclass GithubApi:\n    def __init__(self, token):\n        self.git = github.Github(token)\n\n    def get_comments(self, owner, repo, number, since=None):\n        if since is None:\n            comments = self.git.get_repo(repo).get_issue(number).get_comments()\n        else:\n            comments = self.git.get_repo(repo).get_issue(number).get_comments(since=since)\n        \n        return comments\n```"
    },
    {
        "original": "def open(self):\n        \"\"\"Start. Multiple calls have no effect.\n\n        Not safe to call from multiple threads at once.\n        \"\"\"\n        with self._lock:\n            if self._thread_will_exit:\n                # If the background thread has read self._stopped as True\n                # there is a chance that it has not yet exited. The call to\n                # join should not block indefinitely because there is no\n                # other work done outside the while loop in self._run.\n                try:\n                    self._thread.join()\n                except ReferenceError:\n                    # Thread terminated.\n                    pass\n            self._thread_will_exit = False\n            self._stopped = False\n        started = False\n        try:\n            started = self._thread and self._thread.is_alive()\n        except ReferenceError:\n            # Thread terminated.\n            pass\n\n        if not started:\n            thread = threading.Thread(target=self._run, name=self._name)\n            thread.daemon = True\n            self._thread = weakref.proxy(thread)\n            _register_executor(self)\n            thread.start()",
        "sampled": "I'm happy to help! However, I notice that the problem description is missing. Could you please provide the complete problem description, including the input and output specifications? I'll be happy to assist you in solving the challenge once I have all the necessary information."
    },
    {
        "original": "def list_filepaths(self, wildcard=None):\n        \"\"\"\n        Return the list of absolute filepaths in the directory.\n\n        Args:\n            wildcard: String of tokens separated by \"|\". Each token represents a pattern.\n                If wildcard is not None, we return only those files that match the given shell pattern (uses fnmatch).\n                Example:\n                  wildcard=\"*.nc|*.pdf\" selects only those files that end with .nc or .pdf\n        \"\"\"\n        # Select the files in the directory.\n        fnames = [f for f in os.listdir(self.path)]\n        filepaths = filter(os.path.isfile, [os.path.join(self.path, f) for f in fnames])\n\n        # Filter using the shell patterns.\n        if wildcard is not None:\n            filepaths = WildCard(wildcard).filter(filepaths)\n\n        return filepaths",
        "sampled": "```\nimport os\nimport fnmatch\n\nclass FileProcessor:\n    def __init__(self, directory):\n        self.directory = directory\n\n    def list_filepaths(self, wildcard=None):\n        filepaths = [os.path.join(self.directory, f) for f in os.listdir(self.directory)]\n        \n        if wildcard:\n            patterns = [p.strip() for p in wildcard.split(\"|\")]\n            filtered_filepaths = []\n            for filepath in filepaths:\n                for pattern in patterns:\n                    if fn"
    },
    {
        "original": "def _fulfillment_from_details(data, _depth=0):\n    \"\"\"Load a fulfillment for a signing spec dictionary\n\n    Args:\n        data: tx.output[].condition.details dictionary\n    \"\"\"\n    if _depth == 100:\n        raise ThresholdTooDeep()\n\n    if data['type'] == 'ed25519-sha-256':\n        public_key = base58.b58decode(data['public_key'])\n        return Ed25519Sha256(public_key=public_key)\n\n    if data['type'] == 'threshold-sha-256':\n        threshold = ThresholdSha256(data['threshold'])\n        for cond in data['subconditions']:\n            cond = _fulfillment_from_details(cond, _depth+1)\n            threshold.add_subfulfillment(cond)\n        return threshold\n\n    raise UnsupportedTypeError(data.get('type'))",
        "sampled": "I'd be happy to help with this programming challenge! However, I notice that there is no specific problem description, input and output specifications provided.\n\nCould you please provide more information about the challenge? What is the problem trying to solve? What kind of data will be passed into the `_fulfillment_from_details` function? What kind of output is expected?\n\nOnce I have more context about the problem, I'll do my best to provide a Python solution in a single markdown block."
    },
    {
        "original": "def merge(self, base, head, commit_message=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `POST /repos/:owner/:repo/merges <http://developer.github.com/v3/repos/merging>`_\n        :param base: string\n        :param head: string\n        :param commit_message: string\n        :rtype: :class:`github.Commit.Commit`\n        \"\"\"\n        assert isinstance(base, (str, unicode)), base\n        assert isinstance(head, (str, unicode)), head\n        assert commit_message is github.GithubObject.NotSet or isinstance(commit_message, (str, unicode)), commit_message\n        post_parameters = {\n            \"base\": base,\n            \"head\": head,\n        }\n        if commit_message is not github.GithubObject.NotSet:\n            post_parameters[\"commit_message\"] = commit_message\n        headers, data = self._requester.requestJsonAndCheck(\n            \"POST\",\n            self.url + \"/merges\",\n            input=post_parameters\n        )\n        if data is None:\n            return None\n        else:\n            return github.Commit.Commit(self._requester, headers, data, completed=True)",
        "sampled": "```\nimport github\n\nclass GithubRepository:\n    def __init__(self, owner, repo):\n        self.owner = owner\n        self.repo = repo\n        self.g = github.Github()\n        self.repository = self.g.get_repo(f\"{owner}/{repo}\")\n\n    def merge(self, base, head, commit_message=github.GithubObject.NotSet):\n        return self.repository.merge(base, head, commit_message)\n```"
    },
    {
        "original": "def automatic_gamma_density(structure, kppa):\n        \"\"\"\n        Returns an automatic Kpoint object based on a structure and a kpoint\n        density. Uses Gamma centered meshes always. For GW.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n\n        Args:\n            structure:\n                Input structure\n            kppa:\n                Grid density\n        \"\"\"\n\n        latt = structure.lattice\n        lengths = latt.abc\n        ngrid = kppa / structure.num_sites\n\n        mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n        num_div = [int(round(mult / l)) for l in lengths]\n\n        # ensure that numDiv[i] > 0\n        num_div = [i if i > 0 else 1 for i in num_div]\n\n        # VASP documentation recommends to use even grids for n <= 8 and odd\n        # grids for n > 8.\n        num_div = [i + i % 2 if i <= 8 else i - i % 2 + 1 for i in num_div]\n\n        style = Kpoints.supported_modes.Gamma\n\n        comment = \"pymatgen 4.7.6+ generated KPOINTS with grid density = \" + \\\n                  \"{} / atom\".format(kppa)\n        num_kpts = 0\n        return Kpoints(comment, num_kpts, style, [num_div], [0, 0, 0])",
        "sampled": "```\nfrom pymatgen import Structure\nfrom pymatgen.symmetry.kpoints import KpointPath, KpointException\n\ndef automatic_gamma_density(structure: Structure, kppa: float) -> dict:\n    \"\"\"\n    Returns an automatic Kpoint object based on a structure and a kpoint density.\n    Uses Gamma centered meshes always. For GW.\n\n    Args:\n        structure (Structure): Input structure\n        kppa (float): Grid density\n\n    Returns:\n        dict:"
    },
    {
        "original": "def unprotect(self, **kwargs):\n        \"\"\"Unprotect the branch.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProtectError: If the branch could not be unprotected\n        \"\"\"\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/unprotect' % (self.manager.path, id)\n        self.manager.gitlab.http_put(path, **kwargs)\n        self._attrs['protected'] = False",
        "sampled": "```\nimport gitlab\n\nclass GitLab:\n    def __init__(self, url, private_token):\n        self.gl = gitlab.Gitlab(url, private_token=private_token)\n\n    def unprotect(self, project_id, branch_name, **kwargs):\n        project = self.gl.projects.get(project_id)\n        branch = project.branches.get(branch_name)\n        try:\n            branch.unprotect()\n        except gitlab.exceptions.GitlabAuthenticationError as e:\n            raise GitlabAuthentication"
    },
    {
        "original": "def parse_cache_control_header(value, on_update=None, cls=None):\n    \"\"\"Parse a cache control header.  The RFC differs between response and\n    request cache control, this method does not.  It's your responsibility\n    to not use the wrong control statements.\n\n    .. versionadded:: 0.5\n       The `cls` was added.  If not specified an immutable\n       :class:`~werkzeug.datastructures.RequestCacheControl` is returned.\n\n    :param value: a cache control header to be parsed.\n    :param on_update: an optional callable that is called every time a value\n                      on the :class:`~werkzeug.datastructures.CacheControl`\n                      object is changed.\n    :param cls: the class for the returned object.  By default\n                :class:`~werkzeug.datastructures.RequestCacheControl` is used.\n    :return: a `cls` object.\n    \"\"\"\n    if cls is None:\n        cls = RequestCacheControl\n    if not value:\n        return cls(None, on_update)\n    return cls(parse_dict_header(value), on_update)",
        "sampled": "```\nfrom werkzeug.datastructures import RequestCacheControl\n\ndef parse_cache_control_header(value, on_update=None, cls=None):\n    if cls is None:\n        cls = RequestCacheControl\n    cache_control = cls()\n    for directive in value.split(','):\n        directive = directive.strip()\n        if '=' in directive:\n            key, value = [x.strip() for x in directive.split('=', 1)]\n            cache_control.set(key, value)\n        else:\n            cache_control.set(d"
    },
    {
        "original": "def buckets(bucket=None, account=None, matched=False, kdenied=False,\n            errors=False, dbpath=None, size=None, denied=False,\n            format=None, incomplete=False, oversize=False, region=(),\n            not_region=(), inventory=None, output=None, config=None, sort=None,\n            tagprefix=None, not_bucket=None):\n    \"\"\"Report on stats by bucket\"\"\"\n\n    d = db.db(dbpath)\n\n    if tagprefix and not config:\n        raise ValueError(\n            \"account tag value inclusion requires account config file\")\n\n    if config and tagprefix:\n        with open(config) as fh:\n            data = json.load(fh).get('accounts')\n            account_data = {}\n            for a in data:\n                for t in a['tags']:\n                    if t.startswith(tagprefix):\n                        account_data[a['name']] = t[len(tagprefix):]\n\n    buckets = []\n    for b in sorted(d.buckets(account),\n                    key=operator.attrgetter('bucket_id')):\n        if bucket and b.name not in bucket:\n            continue\n        if not_bucket and b.name in not_bucket:\n            continue\n        if matched and not b.matched:\n            continue\n        if kdenied and not b.keys_denied:\n            continue\n        if errors and not b.error_count:\n            continue\n        if size and b.size < size:\n            continue\n        if inventory and not b.using_inventory:\n            continue\n        if denied and not b.denied:\n            continue\n        if oversize and b.scanned <= b.size:\n            continue\n        if incomplete and b.percent_scanned >= incomplete:\n            continue\n        if region and b.region not in region:\n            continue\n        if not_region and b.region in not_region:\n            continue\n        if tagprefix:\n            setattr(b, tagprefix[:-1], account_data[b.account])\n        buckets.append(b)\n\n    if sort:\n        key = operator.attrgetter(sort)\n        buckets = list(reversed(sorted(buckets, key=key)))\n    formatter = format == 'csv' and format_csv or format_plain\n    keys = tagprefix and (tagprefix[:-1],) or ()\n    formatter(buckets, output, keys=keys)",
        "sampled": "**Problem Analysis**\n\nThe problem description is missing from the provided text. However, based on the function signature provided, it appears to be related to reporting statistics by bucket.\n\nFrom the function signature alone, we can infer that this function might take several parameters such as:\n\n* Bucket details (bucket)\n* Account information (account)\n* Various flags for filtering/matching (matched,kdenied-errors,incomplete etc)\n* Database path and configuration options (dbpath,size-denied-format)\n* Region and inventory"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'name') and self.name is not None:\n            _dict['name'] = self.name\n        if hasattr(self, 'classifier_id') and self.classifier_id is not None:\n            _dict['classifier_id'] = self.classifier_id\n        if hasattr(self, 'classes') and self.classes is not None:\n            _dict['classes'] = [x._to_dict() for x in self.classes]\n        return _dict",
        "sampled": "I'm ready to help! However, I don't see any problem description, input, or output specifications provided in your message. Could you please provide the actual programming challenge?"
    },
    {
        "original": "def frame_msg(body, header=None, raw_body=False):  # pylint: disable=unused-argument\n    \"\"\"\n    Frame the given message with our wire protocol\n    \"\"\"\n    framed_msg = {}\n    if header is None:\n        header = {}\n\n    framed_msg['head'] = header\n    framed_msg['body'] = body\n    return salt.utils.msgpack.dumps(framed_msg)",
        "sampled": "Here is my analysis of the problem:\n\nThe problem asks to implement a function `frame_msg` that takes three parameters: `body`, `header`, and `raw_body`. The function should frame the given message with their wire protocol.\n\nHowever, there seems to be some information missing from the problem description, such as how exactly to frame the message with their wire protocol.\n\nAssuming it's a simple framing protocol where we just wrap the body with some start and end markers, here is a possible Python"
    },
    {
        "original": "def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a\n        dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })",
        "sampled": "```\nclass ScenarioBuilder:\n    def _get_matrix(self):\n        return {\n            \"scenario_1\": {\"subcommand\": [\"action-1\", \"action-2\"]},\n            \"scenario_2\": {\"subcommand\": [\"action-1\"]}\n        }\n```"
    },
    {
        "original": "def set_tcp_md5sig(s, addr, key):\n    \"\"\"Enable TCP-MD5 on the given socket.\n\n    :param s: Socket\n    :param addr: Associated address.  On some platforms, this has no effect.\n    :param key: Key.  On some platforms, this has no effect.\n    \"\"\"\n    impls = {\n        'FreeBSD': _set_tcp_md5sig_bsd,\n        'Linux': _set_tcp_md5sig_linux,\n        'NetBSD': _set_tcp_md5sig_bsd,\n    }\n    system = platform.system()\n    try:\n        impl = impls[system]\n    except KeyError:\n        raise NotImplementedError(\"TCP-MD5 unsupported on this platform\")\n    impl(s, addr, key)",
        "sampled": "```\nimport socket\n\ndef set_tcp_md5sig(s, addr, key):\n    \"\"\"\n    Enable TCP-MD5 on the given socket.\n\n    :param s: Socket\n    :param addr: Associated address.  On some platforms, this has no effect.\n    :param key: Key.  On some platforms, this has no effect.\n    \"\"\"\n    \n    TCP_MD5SIG = 14\n    tcp_md5sig_struct = struct.pack('4s', key.encode())"
    },
    {
        "original": "def _jobs():\n    \"\"\"\n    Return the currently configured jobs.\n    \"\"\"\n    response = salt.utils.http.query(\n        \"{0}/scheduler/jobs\".format(_base_url()),\n        decode_type='json',\n        decode=True,\n    )\n    jobs = {}\n    for job in response['dict']:\n        jobs[job.pop('name')] = job\n    return jobs",
        "sampled": "```\ndef _jobs():\n    # Add your implementation here\n    pass\n```"
    },
    {
        "original": "def get_special_folder(self, name):\n        \"\"\" Returns the specified Special Folder\n\n        :return: a special Folder\n        :rtype: drive.Folder\n        \"\"\"\n\n        name = name if \\\n            isinstance(name, OneDriveWellKnowFolderNames) \\\n            else OneDriveWellKnowFolderNames(name.lower())\n        name = name.value\n\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('get_special').format(id=self.object_id,\n                                                          name=name))\n        else:\n            # we don't know the drive_id so go to the default\n            url = self.build_url(\n                self._endpoints.get('get_special_default').format(name=name))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self._classifier(data)(parent=self,\n                                      **{self._cloud_data_key: data})",
        "sampled": "I'm happy to help! However, I notice that there seems to be some confusion. The provided text appears to be a function signature from Python with docstring comments, but it doesn't seem related to any specific problem description or input/output specifications.\n\nCould you please provide more context or clarify what kind of problem this function is supposed to solve? What are we trying to achieve with this `get_special_folder` method?\n\nOnce I understand the requirements better, I'd be happy to help you come up"
    },
    {
        "original": "def hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      404:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(404)\n    return jsonify(authenticated=True, user=user)",
        "sampled": "```\nimport base64\n\ndef hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    auth_str = f\"{user}:{passwd}\"\n    b64_str = base64.b64encode(auth_str.encode()).decode()\n    header_str = f\"Basic {b64_str}\"\n    \n    return header_str\n```"
    },
    {
        "original": "def get_noisy_gate(gate_name, params):\n    \"\"\"\n    Look up the numerical gate representation and a proposed 'noisy' name.\n\n    :param str gate_name: The Quil gate name\n    :param Tuple[float] params: The gate parameters.\n    :return: A tuple (matrix, noisy_name) with the representation of the ideal gate matrix\n        and a proposed name for the noisy version.\n    :rtype: Tuple[np.array, str]\n    \"\"\"\n    params = tuple(params)\n    if gate_name == \"I\":\n        assert params == ()\n        return np.eye(2), \"NOISY-I\"\n    if gate_name == \"RX\":\n        angle, = params\n        if np.isclose(angle, np.pi / 2, atol=ANGLE_TOLERANCE):\n            return (np.array([[1, -1j],\n                              [-1j, 1]]) / np.sqrt(2),\n                    \"NOISY-RX-PLUS-90\")\n        elif np.isclose(angle, -np.pi / 2, atol=ANGLE_TOLERANCE):\n            return (np.array([[1, 1j],\n                              [1j, 1]]) / np.sqrt(2),\n                    \"NOISY-RX-MINUS-90\")\n        elif np.isclose(angle, np.pi, atol=ANGLE_TOLERANCE):\n            return (np.array([[0, -1j],\n                              [-1j, 0]]),\n                    \"NOISY-RX-PLUS-180\")\n        elif np.isclose(angle, -np.pi, atol=ANGLE_TOLERANCE):\n            return (np.array([[0, 1j],\n                              [1j, 0]]),\n                    \"NOISY-RX-MINUS-180\")\n    elif gate_name == \"CZ\":\n        assert params == ()\n        return np.diag([1, 1, 1, -1]), \"NOISY-CZ\"\n    raise NoisyGateUndefined(\"Undefined gate and params: {}{}\\n\"\n                             \"Please restrict yourself to I, RX(+/-pi), RX(+/-pi/2), CZ\"\n                             .format(gate_name, params))",
        "sampled": "```\nimport numpy as np\nfrom typing import Tuple\n\ndef get_noisy_gate(gate_name: str, params: Tuple[float]) -> Tuple[np.ndarray, str]:\n    \"\"\"\n    Look up the numerical gate representation and a proposed 'noisy' name.\n\n    Parameters:\n        gate_name (str): The Quil gate name\n        params (Tuple[float]): The gate parameters.\n\n    Returns:\n        A tuple (matrix, noisy_name) with the representation of the ideal gate matrix \n       "
    },
    {
        "original": "def load(self, config):\n        \"\"\"Load the server list from the configuration file.\"\"\"\n        server_list = []\n\n        if config is None:\n            logger.debug(\"No configuration file available. Cannot load server list.\")\n        elif not config.has_section(self._section):\n            logger.warning(\"No [%s] section in the configuration file. Cannot load server list.\" % self._section)\n        else:\n            logger.info(\"Start reading the [%s] section in the configuration file\" % self._section)\n            for i in range(1, 256):\n                new_server = {}\n                postfix = 'server_%s_' % str(i)\n                # Read the server name (mandatory)\n                for s in ['name', 'port', 'alias']:\n                    new_server[s] = config.get_value(self._section, '%s%s' % (postfix, s))\n                if new_server['name'] is not None:\n                    # Manage optionnal information\n                    if new_server['port'] is None:\n                        new_server['port'] = '61209'\n                    new_server['username'] = 'glances'\n                    # By default, try empty (aka no) password\n                    new_server['password'] = ''\n                    try:\n                        new_server['ip'] = gethostbyname(new_server['name'])\n                    except gaierror as e:\n                        logger.error(\"Cannot get IP address for server %s (%s)\" % (new_server['name'], e))\n                        continue\n                    new_server['key'] = new_server['name'] + ':' + new_server['port']\n\n                    # Default status is 'UNKNOWN'\n                    new_server['status'] = 'UNKNOWN'\n\n                    # Server type is 'STATIC'\n                    new_server['type'] = 'STATIC'\n\n                    # Add the server to the list\n                    logger.debug(\"Add server %s to the static list\" % new_server['name'])\n                    server_list.append(new_server)\n\n            # Server list loaded\n            logger.info(\"%s server(s) loaded from the configuration file\" % len(server_list))\n            logger.debug(\"Static server list: %s\" % server_list)\n\n        return server_list",
        "sampled": "I'd be happy to help! However, I noticed that you didn't provide the programming challenge yet. Please go ahead and provide the Problem Description, Input, and Output specifications in Markdown format, and I'll help you understand and analyze the problem before providing a Python solution.\n\nPlease paste the challenge details, and I'll get started!"
    },
    {
        "original": "def creep_data(data_set='creep_rupture'):\n    \"\"\"Brun and Yoshida's metal creep rupture data.\"\"\"\n    if not data_available(data_set):\n        download_data(data_set)\n        path = os.path.join(data_path, data_set)\n        tar_file = os.path.join(path, 'creeprupt.tar')\n        tar = tarfile.open(tar_file)\n        print('Extracting file.')\n        tar.extractall(path=path)\n        tar.close()\n    all_data = np.loadtxt(os.path.join(data_path, data_set, 'taka'))\n    y = all_data[:, 1:2].copy()\n    features = [0]\n    features.extend(range(2, 31))\n    X = all_data[:, features].copy()\n    return data_details_return({'X': X, 'y': y}, data_set)",
        "sampled": "```\ndef creep_data(data_set='creep_rupture'):\n    if data_set == 'creep_rupture':\n        # implement Brun and Yoshida's metal creep rupture data\n        pass \n    else:\n        raise ValueError(\"Invalid data set\")\n```"
    },
    {
        "original": "def exit_enable_mode(self, exit_command=\"exit\"):\n        \"\"\"Exit enable mode.\"\"\"\n        delay_factor = self.select_delay_factor(delay_factor=0)\n        output = \"\"\n        if self.check_enable_mode():\n            self.write_channel(self.normalize_cmd(exit_command))\n            time.sleep(0.3 * delay_factor)\n            self.set_base_prompt()\n            if self.check_enable_mode():\n                raise ValueError(\"Failed to exit enable mode.\")\n        return output",
        "sampled": "I'm happy to help! However, I don't see any problem description, input, or output specifications provided. Could you please provide the problem statement in Markdown format so I can assist you?"
    },
    {
        "original": "def create_baseline(tag=\"baseline\", config='root'):\n    \"\"\"\n    Creates a snapshot marked as baseline\n\n    tag\n        Tag name for the baseline\n\n    config\n        Configuration name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' snapper.create_baseline\n        salt '*' snapper.create_baseline my_custom_baseline\n    \"\"\"\n    return __salt__['snapper.create_snapshot'](config=config,\n                                               snapshot_type='single',\n                                               description=\"baseline snapshot\",\n                                               cleanup_algorithm=\"number\",\n                                               userdata={\"baseline_tag\": tag})",
        "sampled": "```\n```python\nimport salt.client\n\ndef create_baseline(tag=\"baseline\", config='root'):\n    local = salt.client.LocalClient()\n    \n    if not tag:\n        tag = \"baseline\"\n        \n    result = local.cmd('snapper.create', [tag, config])\n    \n    return result[0]\n```\n```"
    },
    {
        "original": "def catch_game_end(func):\n  \"\"\"Decorator to handle 'Game has already ended' exceptions.\"\"\"\n  @functools.wraps(func)\n  def _catch_game_end(self, *args, **kwargs):\n    ",
        "sampled": "I'm ready to help! Please go ahead and provide the programming challenge in Markdown format, including the problem description, input and output specifications, and any relevant math formulas in inline LaTeX notation. I'll analyze the problem, provide a Python solution, and format it in a single Markdown block as requested.\n\nPlease go ahead and provide the challenge!"
    },
    {
        "original": "def publish_alias(self, func_data, alias):\n        \"\"\"Create or update an alias for the given function.\n        \"\"\"\n        if not alias:\n            return func_data['FunctionArn']\n        func_name = func_data['FunctionName']\n        func_version = func_data['Version']\n\n        exists = resource_exists(\n            self.client.get_alias, FunctionName=func_name, Name=alias)\n\n        if not exists:\n            log.debug(\"Publishing custodian lambda alias %s\", alias)\n            alias_result = self.client.create_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        else:\n            if (exists['FunctionVersion'] == func_version and\n                    exists['Name'] == alias):\n                return exists['AliasArn']\n            log.debug('Updating custodian lambda alias %s', alias)\n            alias_result = self.client.update_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        return alias_result['AliasArn']",
        "sampled": "I'd be happy to help! However, I don't see any problem description, input and output specifications provided in your message. Could you please provide those details?"
    },
    {
        "original": "def remove_send_last_message(self, connection):\n        \"\"\"Removes a send_last_message function previously registered\n        with the Dispatcher.\n\n        Args:\n            connection (str): A locally unique identifier provided\n                by the receiver of messages.\n        \"\"\"\n        if connection in self._send_last_message:\n            del self._send_last_message[connection]\n            LOGGER.debug(\"Removed send_last_message function \"\n                         \"for connection %s\", connection)\n        else:\n            LOGGER.warning(\"Attempted to remove send_last_message \"\n                           \"function for connection %s, but no \"\n                           \"send_last_message function was registered\",\n                           connection)",
        "sampled": "```\nclass Dispatcher:\n    def __init__(self):\n        self.send_last_message_functions = {}\n\n    def remove_send_last_message(self, connection):\n        if connection in self.send_last_message_functions:\n            del self.send_last_message_functions[connection]\n```"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'label') and self.label is not None:\n            _dict['label'] = self.label\n        if hasattr(self, 'provenance_ids') and self.provenance_ids is not None:\n            _dict['provenance_ids'] = self.provenance_ids\n        return _dict",
        "sampled": "I'm happy to help! Please go ahead and provide the programming challenge in Markdown format, including the problem description, input and output specifications, and any inline LaTeX math formulas if necessary. I'll analyze the problem and provide a Python solution in a single markdown block.\n\nPlease go ahead and paste the challenge! \ud83d\ude0a"
    },
    {
        "original": "def identical_blocks(self):\n        \"\"\"\n        :return A list of all block matches that appear to be identical\n        \"\"\"\n        identical_blocks = []\n        for (func_a, func_b) in self.function_matches:\n            identical_blocks.extend(self.get_function_diff(func_a, func_b).identical_blocks)\n        return identical_blocks",
        "sampled": "Based on the provided instructions, I understand that we need to write a function `identical_blocks` that returns a list of all block matches that appear to be identical.\n\nHere is the Python solution:\n\n```python\ndef identical_blocks(self):\n    # TO DO: implement this method\n    pass\n```\n\nPlease provide more details about the problem, such as input and output specifications, so I can help you better with implementing this method."
    },
    {
        "original": "def Validate(self, type_names):\n    \"\"\"Filtered types need to be RDFValues.\"\"\"\n    errs = [n for n in self._RDFTypes(type_names) if not self._GetClass(n)]\n    if errs:\n      raise DefinitionError(\"Undefined RDF Types: %s\" % \",\".join(errs))",
        "sampled": "I'd be happy to help!\n\nHowever, I noticed that there seems to be some information missing from the problem description. Could you please provide more context or details about what this programming challenge is about?\n\nBased on what you've provided so far, it appears that we're working with RDFValues and filtering types, but I'm not entirely sure what `Validate` function should do or what `type_names` parameter represents.\n\nPlease provide more information or clarify the problem description so I can better understand and assist"
    },
    {
        "original": "def set_syslog_server(server=None, type=\"primary\"):\n    \"\"\"\n    Set the SYSLOG server on the host.\n\n    Args:\n        server(str): The hostname or IP address of the SYSLOG server.\n\n        type(str): Specifies the type of SYSLOG server. This can either be primary (default) or secondary.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cimc.set_syslog_server foo.bar.com\n\n        salt '*' cimc.set_syslog_server foo.bar.com primary\n\n        salt '*' cimc.set_syslog_server foo.bar.com secondary\n\n    \"\"\"\n\n    if not server:\n        raise salt.exceptions.CommandExecutionError(\"The SYSLOG server must be specified.\")\n\n    if type == \"primary\":\n        dn = \"sys/svc-ext/syslog/client-primary\"\n        inconfig = ",
        "sampled": "```\n```python\ndef set_syslog_server(server=None, type=\"primary\"):\n    if not isinstance(server, str):\n        raise ValueError(\"Server must be a string\")\n    if type not in [\"primary\", \"secondary\"]:\n        raise ValueError(\"Type must be either 'primary' or 'secondary'\")\n    \n    # implement your logic here to set syslog server on host\n    # this example just prints out what would happen\n    if type == \"primary\":\n        print(f\"Setting primary syslog"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'customer_id') and self.customer_id is not None:\n            _dict['customer_id'] = self.customer_id\n        if hasattr(self, 'document_type') and self.document_type is not None:\n            _dict['document_type'] = self.document_type\n        if hasattr(self, 'natural_language_query'\n                  ) and self.natural_language_query is not None:\n            _dict['natural_language_query'] = self.natural_language_query\n        if hasattr(self,\n                   'document_results') and self.document_results is not None:\n            _dict['document_results'] = self.document_results._to_dict()\n        if hasattr(self,\n                   'created_timestamp') and self.created_timestamp is not None:\n            _dict['created_timestamp'] = datetime_to_string(\n                self.created_timestamp)\n        if hasattr(self,\n                   'client_timestamp') and self.client_timestamp is not None:\n            _dict['client_timestamp'] = datetime_to_string(\n                self.client_timestamp)\n        if hasattr(self, 'query_id') and self.query_id is not None:\n            _dict['query_id'] = self.query_id\n        if hasattr(self, 'session_token') and self.session_token is not None:\n            _dict['session_token'] = self.session_token\n        if hasattr(self, 'collection_id') and self.collection_id is not None:\n            _dict['collection_id'] = self.collection_id\n        if hasattr(self, 'display_rank') and self.display_rank is not None:\n            _dict['display_rank'] = self.display_rank\n        if hasattr(self, 'document_id') and self.document_id is not None:\n            _dict['document_id'] = self.document_id\n        if hasattr(self, 'event_type') and self.event_type is not None:\n            _dict['event_type'] = self.event_type\n        if hasattr(self, 'result_type') and self.result_type is not None:\n            _dict['result_type'] = self.result_type\n        return _dict",
        "sampled": "I'm ready to help! However, I don't see any problem description, input and output specifications provided yet. Please go ahead and provide them so I can assist you with solving the challenge.\n\nPlease paste the Markdown format text with Problem Description, Input and Output specifications so we can get started!"
    },
    {
        "original": "def check_config_mode(self, check_string=\")#\", pattern=\"#\"):\n        \"\"\"Checks if the device is in configuration mode or not.\"\"\"\n        return super(CiscoNxosSSH, self).check_config_mode(\n            check_string=check_string, pattern=pattern\n        )",
        "sampled": "```\ndef check_config_mode(self, check_string=\"#)\", pattern=\"#\"):\n    return pattern in check_string\n```"
    },
    {
        "original": "def from_EV(E, V):\n        \"\"\"\n        Creates an instance of a Gamma Prior  by specifying the Expected value(s)\n        and Variance(s) of the distribution.\n\n        :param E: expected value\n        :param V: variance\n        \"\"\"\n        a = np.square(E) / V\n        b = E / V\n        return Gamma(a, b)",
        "sampled": "```\nimport numpy as np\nfrom scipy.stats import gamma\n\ndef from_EV(E, V):\n    alpha = (E ** 2) / V\n    beta = E / V\n    return gamma(alpha, scale=1/beta)\n```"
    },
    {
        "original": "def run_command(self, commands, timeout_sec=None, exception=None):\n        \"\"\"\n        Executes the given commands and sends OVSDB messages.\n\n        ``commands`` must be a list of\n        :py:mod:`ryu.lib.ovs.vsctl.VSCtlCommand`.\n\n        If ``timeout_sec`` is specified, raises exception after the given\n        timeout [sec]. Additionally, if ``exception`` is specified, this\n        function will wraps exception using the given exception class.\n\n        Retruns ``None`` but fills ``result`` attribute for each command\n        instance.\n        \"\"\"\n        if timeout_sec is None:\n            self._run_command(commands)\n        else:\n            with hub.Timeout(timeout_sec, exception):\n                self._run_command(commands)",
        "sampled": "Here is my analysis of the problem:\n\nThe problem appears to be related to executing OpenVSwitch (OVS) commands using Ryu's `VSCtlCommand` module. The task is to implement a method `run_command` that takes in a list of `VSCtlCommand` objects, an optional timeout value in seconds, and an optional exception class.\n\nBased on this understanding, here is my proposed Python solution:\n```python\nimport time\nfrom ryu.lib.ovs.vsctl import"
    },
    {
        "original": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)",
        "sampled": "```\nclass GitlabDeleteError(Exception):\n    pass\n\nclass GitlabAuthenticationError(Exception):\n    pass\n\nclass Server:\n    def __init__(self):\n        self.objects = {}\n\n    def delete(self, id, **kwargs):\n        if not self.authenticate(**kwargs):\n            raise GitlabAuthenticationError\n        if id not in self.objects:\n            raise GitlabDeleteError\n        del self.objects[id]\n\n    def authenticate(self, **kwargs):\n        # implement your authentication logic here\n       "
    },
    {
        "original": "def get_edges(self):\n        \"\"\"\n        Returns the edges of the network\n\n        Examples\n        --------\n        >>> reader = XMLBIF.XMLBIFReader(\"xmlbif_test.xml\")\n        >>> reader.get_edges()\n        [['family-out', 'light-on'],\n         ['family-out', 'dog-out'],\n         ['bowel-problem', 'dog-out'],\n         ['dog-out', 'hear-bark']]\n        \"\"\"\n        edge_list = [[value, key] for key in self.variable_parents\n                     for value in self.variable_parents[key]]\n        return edge_list",
        "sampled": "**Understood Problem Analysis**\nThe problem appears to be related to parsing an XML file using an `XMLBIFReader` class, specifically extracting edges from the file into a list of lists, where each inner list represents an edge as a pair of nodes.\n\n**Python Solution**\n```python\nfrom xml.etree import ElementTree\n\nclass XMLBIFReader:\n    def __init__(self, filename):\n        self.tree = ElementTree.parse(filename)\n        \n    def get_edges(self):\n        edges"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"\n        Create a JSON-serializable representation of the ISA.\n\n        The dictionary representation is of the form::\n\n            {\n                \"1Q\": {\n                    \"0\": {\n                        \"type\": \"Xhalves\"\n                    },\n                    \"1\": {\n                        \"type\": \"Xhalves\",\n                        \"dead\": True\n                    },\n                    ...\n                },\n                \"2Q\": {\n                    \"1-4\": {\n                        \"type\": \"CZ\"\n                    },\n                    \"1-5\": {\n                        \"type\": \"CZ\"\n                    },\n                    ...\n                },\n                ...\n            }\n\n        :return: A dictionary representation of self.\n        :rtype: Dict[str, Any]\n        \"\"\"\n\n        def _maybe_configure(o, t):\n            # type: (Union[Qubit,Edge], str) -> dict\n            ",
        "sampled": "```\nclass ISA:\n    def __init__(self):\n        self.data = {}\n\n    def add_gate(self, num_qubits, gate_type, **_gate_info):\n        if str(num_qubits) + 'Q' not in self.data:\n            self.data[str(num_qubits) + 'Q'] = {}\n        \n        if \"_\" in _gate_info:\n            qubit_str = _gate_info\n        else:\n            qubit_str = _gate_info\n        \n        if 'dead' in"
    },
    {
        "original": "def _sort_text(definition):\n    \"\"\" Ensure builtins appear at the bottom.\n    Description is of format <type>: <module>.<item>\n    \"\"\"\n    if definition.name.startswith(\"_\"):\n        # It's a 'hidden' func, put it next last\n        return 'z' + definition.name\n    elif definition.scope == 'builtin':\n        return 'y' + definition.name\n\n    # Else put it at the front\n    return 'a' + definition.name",
        "sampled": "I'd be happy to help! However, I noticed that you haven't provided the problem description, input, and output specifications yet. Please go ahead and provide those, and I'll be happy to assist you with understanding and analyzing the problem, followed by a Python solution in a single markdown block.\n\nPlease go ahead and provide the problem description!"
    },
    {
        "original": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)",
        "sampled": "```\nclass ServiceScaler:\n    def __init__(self):\n        self.containers = []\n\n    def scale(self, replicas):\n        if replicas < len(self.containers):\n            del self.containers[replicas:]\n        elif replicas > len(self.containers):\n            self.containers.extend([None]*(replicas - len(self.containers)))\n        \n        return True\n```"
    },
    {
        "original": "def register_switch_address(addr, interval=None):\n    \"\"\"\n    Registers a new address to initiate connection to switch.\n\n    Registers a new IP address and port pair of switch to let\n    ryu.controller.controller.OpenFlowController to try to initiate\n    connection to switch.\n\n    :param addr: A tuple of (host, port) pair of switch.\n    :param interval: Interval in seconds to try to connect to switch\n    \"\"\"\n    assert len(addr) == 2\n    assert ip.valid_ipv4(addr[0]) or ip.valid_ipv6(addr[0])\n    ofp_handler = app_manager.lookup_service_brick(ofp_event.NAME)\n    _TMP_ADDRESSES[addr] = interval\n\n    def _retry_loop():\n        # Delays registration if ofp_handler is not started yet\n        while True:\n            if ofp_handler.controller is not None:\n                for a, i in _TMP_ADDRESSES.items():\n                    ofp_handler.controller.spawn_client_loop(a, i)\n                    hub.sleep(1)\n                break\n            hub.sleep(1)\n\n    hub.spawn(_retry_loop)",
        "sampled": "```\ndef register_switch_address(addr, interval=None):\n    \"\"\"\n    Registers a new address to initiate connection to switch.\n\n    Registers a new IP address and port pair of switch to let\n    ryu.controller(controller.OpenFlowController)to trytoinitiate \n   connectiontoswitch.\n\n   :paramaddr: Atupleof(host,port)pair ofswitch.\n   :paraminterval: Intervalin secondstotrytoconnecttoswitch\n  \"\"\"\nimport time\n\ndef register_switch_address(addr,"
    },
    {
        "original": "def neighbor_update(self, address, conf_type, conf_value):\n        \"\"\" This method changes the neighbor configuration.\n\n        ``address`` specifies the IP address of the peer.\n\n        ``conf_type`` specifies configuration type which you want to change.\n        Currently ryu.services.protocols.bgp.bgpspeaker.MULTI_EXIT_DISC\n        can be specified.\n\n        ``conf_value`` specifies value for the configuration type.\n        \"\"\"\n\n        assert conf_type == MULTI_EXIT_DISC or conf_type == CONNECT_MODE\n\n        func_name = 'neighbor.update'\n        attribute_param = {}\n        if conf_type == MULTI_EXIT_DISC:\n            attribute_param = {neighbors.MULTI_EXIT_DISC: conf_value}\n        elif conf_type == CONNECT_MODE:\n            attribute_param = {neighbors.CONNECT_MODE: conf_value}\n\n        param = {neighbors.IP_ADDRESS: address,\n                 neighbors.CHANGES: attribute_param}\n\n        call(func_name, **param)",
        "sampled": "**Understanding and Analyzing the Problem**\n\nThe problem description is about implementing a method called `neighbor_update` that updates neighbor configuration in BGP (Border Gateway Protocol) speaking system. The method takes three parameters: `address`, `conf_type`, and `conf_value`. \n\n- `address` specifies the IP address of the peer.\n- `conf_type` specifies what type of configuration needs to be updated, currently it supports updating MULTI_EXIT_DISC (MED) value.\n- `conf_value`"
    },
    {
        "original": "def save(self):\n        \"\"\" save the current state of the Kalman filter\"\"\"\n\n        kf = self._kf\n\n        # force all attributes to be computed. this is only necessary\n        # if the class uses properties that compute data only when\n        # accessed\n        for prop in self.properties:\n            self._DL[prop[0]].append(getattr(kf, prop[0]))\n\n        v = copy.deepcopy(kf.__dict__)\n\n        if self._skip_private:\n            for key in list(v.keys()):\n                if key.startswith('_'):\n                    print('deleting', key)\n                    del v[key]\n\n        if self._skip_callable:\n            for key in list(v.keys()):\n                if callable(v[key]):\n                    del v[key]\n\n        for ig in self._ignore:\n            if ig in v:\n                del v[ig]\n\n        for key in list(v.keys()):\n            self._DL[key].append(v[key])\n\n        self.__dict__.update(self._DL)\n        self._len += 1",
        "sampled": "Please provide the programming challenge in Markdown format, and I'll be happy to help you with it!"
    },
    {
        "original": "def build_lines_data(self, code_obj):\n        \"\"\"\n        Generate various line-related helper data.\n        \"\"\"\n\n        # Offset: lineno pairs, only for offsets which start line.\n        # Locally we use list for more convenient iteration using indices\n        if self.version > 1.4:\n            linestarts = list(self.opc.findlinestarts(code_obj))\n        else:\n            linestarts = [[0, 1]]\n        self.linestarts = dict(linestarts)\n\n        # 'List-map' which shows line number of current op and offset of\n        # first op on following line, given offset of op as index\n        lines = []\n        LineTuple = namedtuple('LineTuple', ['l_no', 'next'])\n\n        # Iterate through available linestarts, and fill\n        # the data for all code offsets encountered until\n        # last linestart offset\n        _, prev_line_no = linestarts[0]\n        offset = 0\n        for start_offset, line_no in linestarts[1:]:\n            while offset < start_offset:\n                lines.append(LineTuple(prev_line_no, start_offset))\n                offset += 1\n            prev_line_no = line_no\n\n        # Fill remaining offsets with reference to last line number\n        # and code length as start offset of following non-existing line\n        codelen = len(self.code)\n        while offset < codelen:\n            lines.append(LineTuple(prev_line_no, codelen))\n            offset += 1\n        return lines",
        "sampled": "```\ndef build_lines_data(self, code_obj):\n    pass\n```"
    },
    {
        "original": "def key_pair_from_ed25519_key(hex_private_key):\n    \"\"\"Generate base58 encode public-private key pair from a hex encoded private key\"\"\"\n    priv_key = crypto.Ed25519SigningKey(bytes.fromhex(hex_private_key)[:32], encoding='bytes')\n    public_key = priv_key.get_verifying_key()\n    return CryptoKeypair(private_key=priv_key.encode(encoding='base58').decode('utf-8'),\n                         public_key=public_key.encode(encoding='base58').decode('utf-8'))",
        "sampled": "```python\nimport hashlib\nfrom ecdsa import VerifyingKey, SigningKey\nfrom base58 import b58encode_check\n\ndef key_pair_from_ed25519_key(hex_private_key):\n    # Convert hex private key to bytes\n    priv\tkey_bytes = bytes.fromhex(hex_private_key)\n    \n    # Create an Ed25519 signing key from the private key bytes\n    signing_key = SigningKey.from_secret_exponent(priv_key_bytes, hashlib.sha256)\n    \n    # Get verifykey"
    },
    {
        "original": "def get_plot(self, ylim=None, units=\"thz\"):\n        \"\"\"\n        Get a matplotlib object for the bandstructure plot.\n\n        Args:\n            ylim: Specify the y-axis (frequency) limits; by default None let\n                the code choose.\n            units: units for the frequencies. Accepted values thz, ev, mev, ha, cm-1, cm^-1.\n        \"\"\"\n\n        u = freq_units(units)\n\n        plt = pretty_plot(12, 8)\n\n        band_linewidth = 1\n\n        data = self.bs_plot_data()\n        for d in range(len(data['distances'])):\n            for i in range(self._nb_bands):\n                plt.plot(data['distances'][d],\n                         [data['frequency'][d][i][j] * u.factor\n                          for j in range(len(data['distances'][d]))], 'b-',\n                         linewidth=band_linewidth)\n\n        self._maketicks(plt)\n\n        # plot y=0 line\n        plt.axhline(0, linewidth=1, color='k')\n\n        # Main X and Y Labels\n        plt.xlabel(r'$\\mathrm{Wave\\ Vector}$', fontsize=30)\n        ylabel = r'$\\mathrm{{Frequencies\\ ({})}}$'.format(u.label)\n        plt.ylabel(ylabel, fontsize=30)\n\n        # X range (K)\n        # last distance point\n        x_max = data['distances'][-1][-1]\n        plt.xlim(0, x_max)\n\n        if ylim is not None:\n            plt.ylim(ylim)\n\n        plt.tight_layout()\n\n        return plt",
        "sampled": "```\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\nclass BandstructurePlotter:\n    def __init__(self):\n        self.fig, self.ax = plt.subplots()\n\n    def get_plot(self, ylim=None, units=\"thz\"):\n        if ylim:\n            self.ax.set_ylim(ylim)\n        \n        if units == \"thz\":\n            self.ax.set_ylabel(\"Frequency (THz)\")\n        elif units == \"ev\":\n            self.ax.set_ylabel(\"Frequency ("
    },
    {
        "original": "def add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n    \"\"\"\n    Registers a hotkey that replaces one typed text with another. For example\n\n        add_abbreviation('tm', u'\u2122')\n\n    Replaces every \"tm\" followed by a space with a \u2122 symbol (and no space). The\n    replacement is done by sending backspace events.\n\n    - `match_suffix` defines if endings of words should also be checked instead\n    of only whole words. E.g. if true, typing 'carpet'+space will trigger the\n    listener for 'pet'. Defaults to false, only whole words are checked.\n    - `timeout` is the maximum number of seconds between typed characters before\n    the current word is discarded. Defaults to 2 seconds.\n    \n    For more details see `add_word_listener`.\n    \"\"\"\n    replacement = '\\b'*(len(source_text)+1) + replacement_text\n    callback = lambda: write(replacement)\n    return add_word_listener(source_text, callback, match_suffix=match_suffix, timeout=timeout)",
        "sampled": "```\ndef add_abbreviation(source_text, replacement_text, match_suffix=False, timeout=2):\n\u0627\u0626\u064a\u0644 \n```\nPlease provide more information about this function's functionality within an external environment such as what kind of event-listening platform it's supposed to be used with (e.g., keyboard listeners), how it interacts with this environment (e.g., sending backspace events), and how it handles multiple abbreviations or conflicts between them.\n\nAlso please clarify what exactly needs to be implemented here: is it"
    },
    {
        "original": "def make_grover_circuit(input_qubits, output_qubit, oracle):\n    \"\"\"Find the value recognized by the oracle in sqrt(N) attempts.\"\"\"\n    # For 2 input qubits, that means using Grover operator only once.\n    c = cirq.Circuit()\n\n    # Initialize qubits.\n    c.append([\n        cirq.X(output_qubit),\n        cirq.H(output_qubit),\n        cirq.H.on_each(*input_qubits),\n    ])\n\n    # Query oracle.\n    c.append(oracle)\n\n    # Construct Grover operator.\n    c.append(cirq.H.on_each(*input_qubits))\n    c.append(cirq.X.on_each(*input_qubits))\n    c.append(cirq.H.on(input_qubits[1]))\n    c.append(cirq.CNOT(input_qubits[0], input_qubits[1]))\n    c.append(cirq.H.on(input_qubits[1]))\n    c.append(cirq.X.on_each(*input_qubits))\n    c.append(cirq.H.on_each(*input_qubits))\n\n    # Measure the result.\n    c.append(cirq.measure(*input_qubits, key='result'))\n\n    return c",
        "sampled": "```\nfrom qiskit import QuantumCircuit, QuantumRegister\nimport math\n\ndef make_grover_circuit(input_qubits, output_qubit, oracle):\n    n = len(input_qubits)\n    qc = QuantumCircuit(n + 1)\n    \n    # Initialize input qubits to |+\n    for i in range(n):\n        qc.h(input_qubits[i])\n    \n    # Apply Hadamard gate to output qubit\n    qc.h(output_qubit)\n    \n   "
    },
    {
        "original": "def unsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command('UNSUBSCRIBE', *args)",
        "sampled": "```\nclass SubscriptionManager:\n    def __init__(self):\n        self.channels = set()\n\n    def subscribe(self, *_channel):\n        if not isinstance(channel, str):\n            raise ValueError(\"Channel must be a string\")\n        self.channels.add(channel)\n\n    def unsubscribe(self, *_args):\n        if not args:\n            self.channels.clear()\n        else:\n            for channel in args:\n                if channel in self.channels:\n                    self.channels.remove(channel)\n```"
    },
    {
        "original": "def UploadFilePath(self, filepath, offset=0, amount=None):\n    \"\"\"Uploads chunks of a file on a given path to the transfer store flow.\n\n    Args:\n      filepath: A path to the file to upload.\n      offset: An integer offset at which the file upload should start on.\n      amount: An upper bound on number of bytes to stream. If it is `None` then\n        the whole file is uploaded.\n\n    Returns:\n      A `BlobImageDescriptor` object.\n    \"\"\"\n    return self._UploadChunkStream(\n        self._streamer.StreamFilePath(filepath, offset=offset, amount=amount))",
        "sampled": "```\nfrom typing import Optional\n\nclass BlobImageDescriptor:\n    pass  # Define your BlobImageDescriptor class here\n\ndef UploadFilePath(filepath: str, offset: int = 0, amount: Optional[int] = None) -> BlobImageDescriptor:\n    with open(filepath, 'rb') as f:\n        f.seek(offset)\n        if amount is None:\n            data = f.read()\n        else:\n            data = f.read(amount)\n        \n        # Currently returns none since BlobImageDescriptor class"
    },
    {
        "original": "def connect(image):\n    \"\"\"\n    Activate nbd for an image file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_nbd.connect /tmp/image.raw\n    \"\"\"\n    if not os.path.isfile(image):\n        log.warning('Could not connect image: %s does not exist', image)\n        return ''\n\n    if salt.utils.path.which('sfdisk'):\n        fdisk = 'sfdisk -d'\n    else:\n        fdisk = 'fdisk -l'\n    __salt__['cmd.run']('modprobe nbd max_part=63')\n    for nbd in glob.glob('/dev/nbd?'):\n        if __salt__['cmd.retcode']('{0} {1}'.format(fdisk, nbd)):\n            while True:\n                # Sometimes nbd does not \"take hold\", loop until we can verify\n                __salt__['cmd.run'](\n                        'qemu-nbd -c {0} {1}'.format(nbd, image),\n                        python_shell=False,\n                        )\n                if not __salt__['cmd.retcode']('{0} {1}'.format(fdisk, nbd)):\n                    break\n            return nbd\n    log.warning('Could not connect image: %s', image)\n    return ''",
        "sampled": "```\n```python\nimport subprocess\n\ndef connect(image):\n    \"\"\"\n    Activate nbd for an image file.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_nbd.connect /tmp/image.raw\n    \"\"\"\n    \n    command = f\"qemu-nbd -c /dev/nbd0 {image}\"\n    subprocess.run(command, shell=True)\n```\n```"
    },
    {
        "original": "def del_flowspec_local(flowspec_family, route_dist, rules):\n    \"\"\"Deletes/withdraws Flow Specification route from VRF identified\n    by *route_dist*.\n    \"\"\"\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, route_dist=route_dist,\n            rules=rules, is_withdraw=True)\n\n        # Send success response.\n        return [{FLOWSPEC_FAMILY: flowspec_family,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 FLOWSPEC_RULES: rules}]\n\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)",
        "sampled": "```\ndef del_flowspec_local(flowspec_family, route_dist, rules):\n    # Assuming flowspec_family is a string, route_dist is a string, and rules is a list of strings\n    # Delete/withdraw Flow Specification route from VRF identified by route_dist\n    # For simplicity, let's assume we have a dictionary to store the Flow Specification routes\n    flow_spec_routes = {}\n    \n    # Check if route_dist exists in the dictionary\n    if route_dist in"
    },
    {
        "original": "def Name(self):\n    \"\"\"Return the best available name for this volume.\"\"\"\n    return (self.name or self.device_path or self.windowsvolume.drive_letter or\n            self.unixvolume.mount_point or None)",
        "sampled": "```\nclass Volume:\n    def __init__(self, names):\n        self.names = names\n\n    def Name(self):\n        return max(self.names, key=len)\n```"
    },
    {
        "original": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict",
        "sampled": "```\nfrom mpi4py import MPI\n\ndef reduce_dict(input_dict, average=True):\n    comm = MPI.COMM_WORLD\n    rank = comm.rank\n    \n    if rank == 0:\n        result = input_dict.copy()\n        for i in range(1, comm.size):\n            data = comm.recv(source=i)\n            for k, v in data.items():\n                if k in result:\n                    result[k] += v\n                else:\n                    result[k] = v\n        if average"
    },
    {
        "original": "def setex(self, name, time, value):\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n        return self.execute_command('SETEX', name, time, value)",
        "sampled": "Here is my analysis of the problem:\n\nThe problem is asking to implement a method `setex` that sets a key-value pair with an expiration time.\n\nFrom the given instruction, we can see that `setex` takes three parameters: `name`, `time`, and `value`. The `name` parameter is used as the key, `value` is the value associated with this key, and `time` specifies when this key-value pair will expire.\n\nTo solve this problem, we need"
    },
    {
        "original": "def digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    return digest_auth(qop, user, passwd, algorithm, \"never\")",
        "sampled": "```\nimport hashlib\n\ndef digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    if algorithm == \"MD5\":\n        ha = hashlib.md5((user + \":\" + qop + \":\" + passwd).encode()).hexdigest()\n    elif algorithm == \"SHA-256\":\n        ha = hashlib.sha256((user + \":\" + qop + \":\" + passwd).encode()).hexdigest()\n    elif algorithm == \"SHA-512\":\n        ha"
    },
    {
        "original": "def print_info(info_mapping):\n    \"\"\" print info in mapping.\n\n    Args:\n        info_mapping (dict): input(variables) or output mapping.\n\n    Examples:\n        >>> info_mapping = {\n                \"var_a\": \"hello\",\n                \"var_b\": \"world\"\n            }\n        >>> info_mapping = {\n                \"status_code\": 500\n            }\n        >>> print_info(info_mapping)\n        ==================== Output ====================\n        Key              :  Value\n        ---------------- :  ----------------------------\n        var_a            :  hello\n        var_b            :  world\n        ------------------------------------------------\n\n    \"\"\"\n    if not info_mapping:\n        return\n\n    content_format = \"{:<16} : {:<}\\n\"\n    content = \"\\n==================== Output ====================\\n\"\n    content += content_format.format(\"Variable\", \"Value\")\n    content += content_format.format(\"-\" * 16, \"-\" * 29)\n\n    for key, value in info_mapping.items():\n        if isinstance(value, (tuple, collections.deque)):\n            continue\n        elif isinstance(value, (dict, list)):\n            value = json.dumps(value)\n        elif value is None:\n            value = \"None\"\n\n        if is_py2:\n            if isinstance(key, unicode):\n                key = key.encode(\"utf-8\")\n            if isinstance(value, unicode):\n                value = value.encode(\"utf-8\")\n\n        content += content_format.format(key, value)\n\n    content += \"-\" * 48 + \"\\n\"\n    logger.log_info(content)",
        "sampled": "```\n```python\ndef print_info(info_mapping):\n    \"\"\"\n    Print information stored in a dictionary with keys aligned.\n\n    Args:\n    info_mapping (dict): Input dictionary with key-value pairs.\n\n    Returns:\n    None\n    \"\"\"\n    \nprint(\"============== Output ==============\")\nprint(\"{:14} : {}\".format(\"Key\", \"Value\"))\nprint(\"-------------------- : --------------------\")\nfor key, value in info_mapping.items():\n    print(\"{:14} : {}\".format(key, value))\nprint"
    },
    {
        "original": "def summed_icohp(self):\n        \"\"\"\n        Adds ICOHPs of both spin channels for spin polarized compounds\n        Returns:\n             icohp value in eV\n        \"\"\"\n        if self._is_spin_polarized:\n            sum_icohp = self._icohp[Spin.down] + self._icohp[Spin.up]\n        else:\n            sum_icohp = self._icohp[Spin.up]\n        return sum_icohp",
        "sampled": "```\ndef summed_icohp(self):\n    \"\"\"\n    Adds ICOHPs of both spin channels for spin polarized compounds\n    Returns:\n         icohp value in eV\n    \"\"\"\n    icohp_up = self.icohp_up\n    icohp_down = self.icohp_down\n    summed_icohp = icohp_up + icohp_down\n    return summed_icohp\n```"
    },
    {
        "original": "def wrap_tuple_streams(unwrapped, kdims, streams):\n    \"\"\"\n    Fills in tuple keys with dimensioned stream values as appropriate.\n    \"\"\"\n    param_groups = [(s.contents.keys(), s) for s in streams]\n    pairs = [(name,s)  for (group, s) in param_groups for name in group]\n    substituted = []\n    for pos,el in enumerate(wrap_tuple(unwrapped)):\n        if el is None and pos < len(kdims):\n            matches = [(name,s) for (name,s) in pairs if name==kdims[pos].name]\n            if len(matches) == 1:\n                (name, stream) = matches[0]\n                el = stream.contents[name]\n        substituted.append(el)\n    return tuple(substituted)",
        "sampled": "```\ndef wrap_tuple_streams(unwrapped, kdims, streams):\n    result = []\n    for i in range(len(unwrapped)):\n        tuple_values = []\n        for j in range(len(kdims)):\n            if kdims[j] == 'stream':\n                tuple_values.append(streams[i])\n            else:\n                tuple_values.append(getattr(unwrapped[i], kdims[j]))\n        result.append(tuple(tuple_values))\n    return result\n```"
    },
    {
        "original": "def lines(n_traces=5,n=100,columns=None,dateIndex=True,mode=None):\n\t\"\"\"\n\tReturns a DataFrame with the required format for \n\ta scatter (lines) plot\n\n\tParameters:\n\t-----------\n\t\tn_traces : int\n\t\t\tNumber of traces \n\t\tn : int\n\t\t\tNumber of points for each trace\n\t\tcolumns : [str]\n\t\t\tList of column names\n\t\tdateIndex : bool\n\t\t\tIf True it will return a datetime index\n\t\t\tif False it will return a enumerated index\n\t\tmode : string\n\t\t\tFormat for each item\n\t\t\t\t'abc' for alphabet columns\n\t\t\t\t'stocks' for random stock names\n\t\"\"\"\t\n\tindex=pd.date_range('1/1/15',periods=n) if dateIndex else list(range(n))\n\tdf=pd.DataFrame(np.random.randn(n,n_traces),index=index,\n\t\tcolumns=getName(n_traces,columns=columns,mode=mode))\n\treturn df.cumsum()",
        "sampled": "```\nimport pandas as pd\nimport numpy as np\n\ndef lines(n_traces=5,n=100,columns=None,dateIndex=True,mode=None):\n    if columns is None:\n        if mode == 'abc':\n            columns = [chr(i) for i in range(65,65+n_traces)]\n        elif mode == 'stocks':\n            columns = [f'Stock {i}' for i in range(1,n_traces+1)]\n        else:\n            columns ="
    },
    {
        "original": "def _parse_name(name: str, as_qvm: bool, noisy: bool) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n\n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    parts = name.split('-')\n    if len(parts) >= 2 and parts[-2] == 'noisy' and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n\n        if noisy is not None and (not noisy):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `noisy=False`\")\n\n        qvm_type = parts[-1]\n        noisy = True\n        prefix = '-'.join(parts[:-2])\n        return prefix, qvm_type, noisy\n\n    if len(parts) >= 1 and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n        qvm_type = parts[-1]\n        if noisy is None:\n            noisy = False\n        prefix = '-'.join(parts[:-1])\n        return prefix, qvm_type, noisy\n\n    if as_qvm is not None and as_qvm:\n        qvm_type = 'qvm'\n    else:\n        qvm_type = None\n\n    if noisy is None:\n        noisy = False\n\n    return name, qvm_type, noisy",
        "sampled": "```python\nfrom typing import Tuple\n\ndef _parse_name(name: str, as_qvm: bool = False, noisy: bool = False) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n\n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    \n    if 'qvm' in name.lower():\n        if 'no"
    },
    {
        "original": "def ext_pillar(minion_id,\n               pillar,  # pylint: disable=W0613\n               conf):\n    \"\"\"\n    Check neutron for all data\n    \"\"\"\n    comps = conf.split()\n\n    profile = None\n    if comps[0]:\n        profile = comps[0]\n\n    conn = _auth(profile)\n    ret = {}\n    networks = conn.list_networks()\n    for network in networks['networks']:\n        ret[network['name']] = network\n\n    if len(comps) < 2:\n        comps.append('networks')\n    return {comps[1]: ret}",
        "sampled": "I'd be happy to help! However, I notice that there seems to be some missing information about the problem description, input and output specifications. Could you please provide more context or details about this coding challenge?\n\nIf you meant to provide more information, please go ahead and share it with me. I'll be happy to assist you in understanding and solving the problem.\n\nOnce I have enough information, I can provide a Python solution in a single markdown block as requested.\n\nPlease proceed!"
    },
    {
        "original": "def MultiSet(self,\n               subject,\n               values,\n               timestamp=None,\n               replace=True,\n               sync=True,\n               to_delete=None):\n    \"\"\"Set multiple attributes' values for this subject in one operation.\"\"\"\n    to_delete = set(to_delete or [])\n\n    # Prepare a bulk insert operation.\n    subject = utils.SmartUnicode(subject)\n    to_insert = []\n    to_replace = []\n    transaction = []\n\n    # Build a document for each unique timestamp.\n    for attribute, sequence in iteritems(values):\n      for value in sequence:\n        if isinstance(value, tuple):\n          value, entry_timestamp = value\n        else:\n          entry_timestamp = timestamp\n\n        if entry_timestamp is None:\n          entry_timestamp = timestamp\n\n        if entry_timestamp is not None:\n          entry_timestamp = int(entry_timestamp)\n        else:\n          entry_timestamp = time.time() * 1e6\n\n        attribute = utils.SmartUnicode(attribute)\n        data = self._Encode(value)\n\n        # Replacing means to delete all versions of the attribute first.\n        if replace or attribute in to_delete:\n          existing = self._CountExistingRows(subject, attribute)\n          if existing:\n            to_replace.append([subject, attribute, data, entry_timestamp])\n          else:\n            to_insert.append([subject, attribute, data, entry_timestamp])\n          if attribute in to_delete:\n            to_delete.remove(attribute)\n\n        else:\n          to_insert.append([subject, attribute, data, entry_timestamp])\n\n    if to_delete:\n      self.DeleteAttributes(subject, to_delete)\n\n    if sync:\n      if to_replace:\n        transaction.extend(self._BuildReplaces(to_replace))\n      if to_insert:\n        transaction.extend(self._BuildInserts(to_insert))\n      if transaction:\n        self._ExecuteTransaction(transaction)\n    else:\n      if to_replace:\n        with self.buffer_lock:\n          self.to_replace.extend(to_replace)\n      if to_insert:\n        with self.buffer_lock:\n          self.to_insert.extend(to_insert)",
        "sampled": "```\nclass MultiSet:\n    def __init__(self):\n        self.subject_attributes = {}\n\n    def __call__(self, subject, values, timestamp=None, replace=True, sync=True, to_delete=None):\n        if subject not in self.subject_attributes:\n            self.subject_attributes[subject] = {}\n        \n        if to_delete is not None:\n            for attribute in to_delete:\n                if attribute in self.subject_attributes[subject]:\n                    del self.subject_attributes[subject][attribute]\n        \n        for"
    },
    {
        "original": "def _decode_crop_and_flip(image_buffer, num_channels):\n  \"\"\"Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  \"\"\"\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n\n  min_object_covered=0.1\n  aspect_ratio_range=[0.75, 1.33]\n  area_range=[0.05, 1.0]\n  max_attempts=100\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MIN_OBJ_COV,\n                          value=min_object_covered)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_RATIO_RANGE,\n                          value=aspect_ratio_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_AREA_RANGE,\n                          value=area_range)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_DISTORTED_CROP_MAX_ATTEMPTS,\n                          value=max_attempts)\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_CROP_USES_BBOXES, value=False)\n\n  bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                     dtype=tf.float32, shape=[1, 1, 4])   #From the entire image\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=min_object_covered,\n      aspect_ratio_range=aspect_ratio_range,\n      area_range=area_range,\n      max_attempts=max_attempts,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RANDOM_FLIP)\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped",
        "sampled": "```\nimport tensorflow as tf\n\ndef _decode_crop_and_flip(image_buffer, num_channels):\n    image_tensor = tf.io.decode_jpeg(image_buffer, channels=num_channels)\n    crop_size = [tf.shape(image_tensor)[0] - tf.random.uniform([], minval=50), \n                 tf.shape(image_tensor)[1] - tf.random.uniform([], minval=50)]\n    cropped_image = tf.image.random_crop(value=image_tensor, size=crop_size)\n    flipped_image = tf.image.random"
    },
    {
        "original": "def bgp_summary_parser(bgp_summary):\n    \"\"\"Parse 'show bgp all summary vrf' output information from NX-OS devices.\"\"\"\n\n    bgp_summary_dict = {}\n    # Check for BGP summary information lines that have no data\n    if len(bgp_summary.strip().splitlines()) <= 1:\n        return {}\n\n    allowed_afi = [\"ipv4\", \"ipv6\", \"l2vpn\"]\n    vrf_regex = r\"^BGP summary information for VRF\\s+(?P<vrf>\\S+),\"\n    afi_regex = (\n        r\"^BGP summary information.*address family (?P<afi>\\S+ (?:Unicast|EVPN))\"\n    )\n    local_router_regex = (\n        r\"^BGP router identifier\\s+(?P<router_id>\\S+)\"\n        r\",\\s+local AS number\\s+(?P<local_as>\\S+)\"\n    )\n\n    for pattern in [vrf_regex, afi_regex, local_router_regex]:\n        match = re.search(pattern, bgp_summary, flags=re.M)\n        if match:\n            bgp_summary_dict.update(match.groupdict(1))\n\n    # Some post regex cleanup and validation\n    vrf = bgp_summary_dict[\"vrf\"]\n    if vrf.lower() == \"default\":\n        bgp_summary_dict[\"vrf\"] = \"global\"\n\n    afi = bgp_summary_dict[\"afi\"]\n    afi = afi.split()[0].lower()\n    if afi not in allowed_afi:\n        raise ValueError(\"AFI ({}) is invalid and not supported.\".format(afi))\n    bgp_summary_dict[\"afi\"] = afi\n\n    local_as = bgp_summary_dict[\"local_as\"]\n    local_as = helpers.as_number(local_as)\n\n    match = re.search(IPV4_ADDR_REGEX, bgp_summary_dict[\"router_id\"])\n    if not match:\n        raise ValueError(\n            \"BGP router_id ({}) is not valid\".format(bgp_summary_dict[\"router_id\"])\n        )\n\n    vrf = bgp_summary_dict[\"vrf\"]\n    bgp_return_dict = {vrf: {\"router_id\": bgp_summary_dict[\"router_id\"], \"peers\": {}}}\n\n    # Extract and process the tabular data\n    tabular_divider = r\"^Neighbor\\s+.*PfxRcd$\"\n    tabular_data = re.split(tabular_divider, bgp_summary, flags=re.M)\n    if len(tabular_data) != 2:\n        msg = \"Unexpected data processing BGP summary information:\\n\\n{}\".format(\n            bgp_summary\n        )\n        raise ValueError(msg)\n    tabular_data = tabular_data[1]\n    bgp_table = bgp_normalize_table_data(tabular_data)\n    for bgp_entry in bgp_table_parser(bgp_table):\n        bgp_return_dict[vrf][\"peers\"].update(bgp_entry)\n\n    bgp_new_dict = {}\n    for neighbor, bgp_data in bgp_return_dict[vrf][\"peers\"].items():\n        received_prefixes = bgp_data.pop(\"received_prefixes\")\n        bgp_data[\"address_family\"] = {}\n        prefixes_dict = {\n            \"sent_prefixes\": -1,\n            \"accepted_prefixes\": -1,\n            \"received_prefixes\": received_prefixes,\n        }\n        bgp_data[\"address_family\"][afi] = prefixes_dict\n        bgp_data[\"local_as\"] = local_as\n        # FIX, hard-coding\n        bgp_data[\"remote_id\"] = \"0.0.0.0\"\n        bgp_new_dict[neighbor] = bgp_data\n\n    bgp_return_dict[vrf][\"peers\"] = bgp_new_dict\n\n    return bgp_return_dict",
        "sampled": "```\ndef bgp_summary_parser(bgp_summary):\n    # Your code here\n    pass\n```"
    },
    {
        "original": "def warp(self, warp_matrix, img, iflag=cv2.INTER_NEAREST):\n        \"\"\" Function to warp input image given an estimated 2D linear transformation\n\n        :param warp_matrix: Linear 2x3 matrix to use to linearly warp the input images\n        :type warp_matrix: ndarray\n        :param img: Image to be warped with estimated transformation\n        :type img: ndarray\n        :param iflag: Interpolation flag, specified interpolation using during resampling of warped image\n        :type iflag: cv2.INTER_*\n        :return: Warped image using the linear matrix\n        \"\"\"\n\n        height, width = img.shape[:2]\n        warped_img = np.zeros_like(img, dtype=img.dtype)\n\n        # Check if image to warp is 2D or 3D. If 3D need to loop over channels\n        if (self.interpolation_type == InterpolationType.LINEAR) or img.ndim == 2:\n            warped_img = cv2.warpAffine(img.astype(np.float32), warp_matrix, (width, height),\n                                        flags=iflag).astype(img.dtype)\n\n        elif img.ndim == 3:\n            for idx in range(img.shape[-1]):\n                warped_img[..., idx] = cv2.warpAffine(img[..., idx].astype(np.float32), warp_matrix, (width, height),\n                                                      flags=iflag).astype(img.dtype)\n        else:\n            raise ValueError('Image has incorrect number of dimensions: {}'.format(img.ndim))\n\n        return warped_img",
        "sampled": "```\ndef warp(self, warp_matrix, img, iflag=cv2.INTER_NEAREST):\n    return cv2.warpPerspective(img, warp_matrix, (img.shape[1], img.shape[0]), flags=iflag)\n```"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "sampled": "```\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        a dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    projection_dict = {}\n    for spin in [Spin.up]:\n        spin_dict = {}\n        for k_index in range(structure.num_kpoints):\n            k_dict = {}\n            for b_index in"
    },
    {
        "original": "def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])",
        "sampled": "```\nfrom werkzeug.datastructures import IfRange\nfrom datetime import datetime\n\ndef parse_if_range_header(value):\n    if value.startswith('\"') and value.endswith('\"'):\n        return IfRange(etag=value[1:-1])\n    elif value.startswith('W/') and value[2:].startswith('\"') and value.endswith('\"'):\n        return IfRange(etag=value[2:-1])\n    else:\n        try:\n            dt = datetime.strptime(value, '%a, %d %b %Y"
    },
    {
        "original": "def get_repo(repo, **kwargs):\n    \"\"\"\n    Display a repo from the sources.list / sources.list.d\n\n    The repo passed in needs to be a complete repo entry.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.get_repo \"myrepo definition\"\n    \"\"\"\n    _check_apt()\n    ppa_auth = kwargs.get('ppa_auth', None)\n    # we have to be clever about this since the repo definition formats\n    # are a bit more \"loose\" than in some other distributions\n    if repo.startswith('ppa:') and __grains__['os'] in ('Ubuntu', 'Mint', 'neon'):\n        # This is a PPA definition meaning special handling is needed\n        # to derive the name.\n        dist = __grains__['lsb_distrib_codename']\n        owner_name, ppa_name = repo[4:].split('/')\n        if ppa_auth:\n            auth_info = '{0}@'.format(ppa_auth)\n            repo = LP_PVT_SRC_FORMAT.format(auth_info, owner_name,\n                                            ppa_name, dist)\n        else:\n            if HAS_SOFTWAREPROPERTIES:\n                try:\n                    if hasattr(softwareproperties.ppa, 'PPAShortcutHandler'):\n                        repo = softwareproperties.ppa.PPAShortcutHandler(\n                            repo).expand(dist)[0]\n                    else:\n                        repo = softwareproperties.ppa.expand_ppa_line(\n                            repo,\n                            dist)[0]\n                except NameError as name_error:\n                    raise CommandExecutionError(\n                        'Could not find ppa {0}: {1}'.format(repo, name_error)\n                    )\n            else:\n                repo = LP_SRC_FORMAT.format(owner_name, ppa_name, dist)\n\n    repos = list_repos()\n\n    if repos:\n        try:\n            repo_type, repo_architectures, repo_uri, repo_dist, repo_comps = _split_repo_str(repo)\n            if ppa_auth:\n                uri_match = re.search('(http[s]?://)(.+)', repo_uri)\n                if uri_match:\n                    if not uri_match.group(2).startswith(ppa_auth):\n                        repo_uri = '{0}{1}@{2}'.format(uri_match.group(1),\n                                                       ppa_auth,\n                                                       uri_match.group(2))\n        except SyntaxError:\n            raise CommandExecutionError(\n                'Error: repo \\'{0}\\' is not a well formatted definition'\n                .format(repo)\n            )\n\n        for source in six.itervalues(repos):\n            for sub in source:\n                if (sub['type'] == repo_type and\n                    # strip trailing '/' from repo_uri, it's valid in definition\n                    # but not valid when compared to persisted source\n                    sub['uri'].rstrip('/') == repo_uri.rstrip('/') and\n                        sub['dist'] == repo_dist):\n                    if not repo_comps:\n                        return sub\n                    for comp in repo_comps:\n                        if comp in sub.get('comps', []):\n                            return sub\n    return {}",
        "sampled": "```\n```python\ndef get_repo(repo, **kwargs):\n    \"\"\"\n    Display a repo from the sources.list / sources.list.d\n\n    The repo passed in needs to be a complete repo entry.\n\n    CLI Examples:\n\n        salt '*' pkg.get_repo \"myrepo definition\"\n    \"\"\"\n    \nfrom os import listdir\nfrom os.path import isfile, join\nfrom glob import glob\n\ndef get_repo(repo, **kwargs):\n  sources_list = '/etc/apt/sources.list'\n .sources_list"
    },
    {
        "original": "def random_choice(self, actions=None, random_state=None):\n        \"\"\"\n        Return a pure action chosen randomly from `actions`.\n\n        Parameters\n        ----------\n        actions : array_like(int), optional(default=None)\n            An array of integers representing pure actions.\n\n        random_state : int or np.random.RandomState, optional\n            Random seed (integer) or np.random.RandomState instance to\n            set the initial state of the random number generator for\n            reproducibility. If None, a randomly initialized RandomState\n            is used.\n\n        Returns\n        -------\n        scalar(int)\n            If `actions` is given, returns an integer representing a\n            pure action chosen randomly from `actions`; if not, an\n            action is chosen randomly from the player's all actions.\n\n        \"\"\"\n        random_state = check_random_state(random_state)\n\n        if actions is not None:\n            n = len(actions)\n        else:\n            n = self.num_actions\n\n        if n == 1:\n            idx = 0\n        else:\n            idx = random_state.randint(n)\n\n        if actions is not None:\n            return actions[idx]\n        else:\n            return idx",
        "sampled": "```python\nimport numpy as np\n\ndef random_choice(actions=None, random_state=None):\n    if isinstance(random_state, int):\n        np.random.seed(random_state)\n    elif isinstance(random_state, np.random.RandomState):\n        np.random.set_state(random_state.get_state())\n    \n    if actions is None:\n        # assuming default action space for demonstration purposes only; \n        # actual implementation would require access to environment's action space \n\t\tactions = list(range(10))  # default to integer"
    },
    {
        "original": "def ListClientsForKeywords(self, keywords, start_time=None, cursor=None):\n    \"\"\"Lists the clients associated with keywords.\"\"\"\n    keywords = set(keywords)\n    hash_to_kw = {mysql_utils.Hash(kw): kw for kw in keywords}\n    result = {kw: [] for kw in keywords}\n\n    query = ",
        "sampled": "**Understanding and Analyzing the Problem**\n\nThe problem asks to create a method `ListClientsForKeywords` that returns a list of clients associated with given keywords. The method takes three parameters: `keywords`, `start_time`, and `cursor`. \n\nHowever, without further context or information about how these parameters are related to clients or how they should be used to filter clients based on keywords, it's difficult to provide a concrete solution.\n\n**Assumptions**\n\nLet's make some assumptions about how this"
    },
    {
        "original": "def GetAuditLogEntries(offset, now, token):\n  \"\"\"Return all audit log entries between now-offset and now.\n\n  Args:\n    offset: rdfvalue.Duration how far back to look in time\n    now: rdfvalue.RDFDatetime for current time\n    token: GRR access token\n  Yields:\n    AuditEvents created during the time range\n  \"\"\"\n  start_time = now - offset - audit.AUDIT_ROLLOVER_TIME\n\n  for fd in audit.LegacyAuditLogsForTimespan(start_time, now, token):\n    for event in fd.GenerateItems():\n      if now - offset < event.timestamp < now:\n        yield event",
        "sampled": "```\ndef GetAuditLogEntries(offset, now, token):\n    from DateTime import RDFDatetime\n    from ApiCall import FlowARGV\n\n    start_time = now - offset\n    flow_args = FlowARGV(token=token)\n    flow_args\u0435\u043d\u0438\u0435urce_urn = flow_args.client_urn.Add('aff4:/flows/W \uff09\n    \n    for _, result in flow_args(CallFlow(name='GetAuditLog', \n                                       next_client_id=0,\n                                       validatingeyhannel='audit',\n                                      "
    },
    {
        "original": "def evaluate(self, data_loader):\n        \"\"\"\n        Sets model in eval mode, disables gradients, preallocates memory and\n        runs validation on data provided by data_loader.\n\n        :param data_loader: data loader\n        \"\"\"\n        torch.set_grad_enabled(False)\n        self.model.eval()\n        torch.cuda.empty_cache()\n        self.preallocate(data_loader, training=False)\n        output = self.feed_data(data_loader, training=False)\n        self.model.zero_grad()\n        torch.cuda.empty_cache()\n        return output",
        "sampled": "```\ndef evaluate(self, data_loader):\n    self.model.eval()\n    torch.no_grad()\n    total_correct = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            output = self.model(data)\n            pred = output.max(1)[1]\n            total_correct += pred.eq(target).sum().item()\n\n    accuracy = total_correct / len(data_loader.dataset)\n    return accuracy\n```"
    },
    {
        "original": "def secgroup_delete(self, name):\n        \"\"\"\n        Delete a security group\n        \"\"\"\n        nt_ks = self.compute_conn\n        for item in nt_ks.security_groups.list():\n            if item.name == name:\n                nt_ks.security_groups.delete(item.id)\n                return {name: 'Deleted security group: {0}'.format(name)}\n        return 'Security group not found: {0}'.format(name)",
        "sampled": "```\ndef secgroup_delete(self, name):\n    ec2 = boto3.client('ec2')\n    sec_group_ids = [sg['GroupId'] for sg in ec2.describe_security_groups(Filters=[{'Name': 'group-name', 'Values': [name]}])['SecurityGroups']]\n    if len(sec_group_ids) > 0:\n        ec2.delete_security_group(GroupId=sec_group_ids[0])\n```"
    },
    {
        "original": "def _sidConversion(cls, val, **kwargs):\n        \"\"\"\n        converts a list of pysid objects to string representations\n        \"\"\"\n        if isinstance(val, six.string_types):\n            val = val.split(',')\n        usernames = []\n        for _sid in val:\n            try:\n                userSid = win32security.LookupAccountSid('', _sid)\n                if userSid[1]:\n                    userSid = '{1}\\\\{0}'.format(userSid[0], userSid[1])\n                else:\n                    userSid = '{0}'.format(userSid[0])\n            # TODO: This needs to be more specific\n            except Exception:\n                userSid = win32security.ConvertSidToStringSid(_sid)\n                log.warning('Unable to convert SID \"%s\" to a friendly name.  The SID will be disaplayed instead of a user/group name.', userSid)\n            usernames.append(userSid)\n        return usernames",
        "sampled": "```python\nclass PySID:\n    def sidConversion(cls, val, **kwargs):\n        \"\"\"\n        converts a list of pysid objects to string representations\n        \"\"\"\n        sid_list = [str(sid) for sid in val]\n        return ', '.join(sid_list)\n```"
    },
    {
        "original": "def _check_result(method_name, result):\n    \"\"\"\n    Checks whether `result` is a valid API response.\n    A result is considered invalid if:\n        - The server returned an HTTP response code other than 200\n        - The content of the result is invalid JSON.\n        - The method call was unsuccessful (The JSON 'ok' field equals False)\n\n    :raises ApiException: if one of the above listed cases is applicable\n    :param method_name: The name of the method called\n    :param result: The returned result of the method request\n    :return: The result parsed to a JSON dictionary.\n    \"\"\"\n    if result.status_code != 200:\n        msg = 'The server returned HTTP {0} {1}. Response body:\\n[{2}]' \\\n            .format(result.status_code, result.reason, result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    try:\n        result_json = result.json()\n    except:\n        msg = 'The server returned an invalid JSON response. Response body:\\n[{0}]' \\\n            .format(result.text.encode('utf8'))\n        raise ApiException(msg, method_name, result)\n\n    if not result_json['ok']:\n        msg = 'Error code: {0} Description: {1}' \\\n            .format(result_json['error_code'], result_json['description'])\n        raise ApiException(msg, method_name, result)\n    return result_json",
        "sampled": "```\nimport json\nimport requests\n\ndef _check_result(method_name, result):\n    if not isinstance(result, requests.Response):\n        raise ApiException(f\"Invalid response from {method_name}.\")\n    \n    if result.status_code != 200:\n        raise ApiException(f\"Failed to call {method_name}. Status code: {result.status_code}\")\n    \n    try:\n        json_result = result.json()\n    except ValueError:\n        raise ApiException(f\"Failed to parse JSON from {method_name}\")\n    \n   "
    },
    {
        "original": "def yield_figs(self, **kwargs):  # pragma: no cover\n        \"\"\"\n        This function *generates* a predefined list of matplotlib figures with minimal input from the user.\n        \"\"\"\n        yield self.plot_densities(title=\"PAW densities\", show=False)\n        yield self.plot_waves(title=\"PAW waves\", show=False)\n        yield self.plot_projectors(title=\"PAW projectors\", show=False)",
        "sampled": "I'd be happy to help! However, I notice that you haven't provided the actual problem description, input, and output specifications for me to work with.\n\nPlease provide the Markdown-formatted problem description, including any inline LaTeX math formulas, and I'll do my best to analyze it and provide a Python solution in a single Markdown block."
    },
    {
        "original": "def dispatch_command(self, command, params=None):\n        \"\"\"Dispatch device commands to the appropriate handler.\"\"\"\n        try:\n            if command in self.handlers:\n                self.handlers[command](**params)\n            else:\n                logging.warning('Unsupported command: %s: %s',\n                                command, params)\n        except Exception as e:\n            logging.warning('Error during command execution',\n                            exc_info=sys.exc_info())\n            raise e",
        "sampled": "I'm ready to help! Please go ahead and provide the programming challenge in Markdown format, including the problem description, input and output specifications, and any math formulas in inline LaTeX. I'll analyze the problem and provide a Python solution in a single markdown block.\n\n Waiting for your prompt..."
    },
    {
        "original": "def toggle(self, *args):\n        \"\"\"\n        If no arguments are specified, toggle the state of all LEDs. If\n        arguments are specified, they must be the indexes of the LEDs you wish\n        to toggle. For example::\n\n            from gpiozero import LEDBoard\n\n            leds = LEDBoard(2, 3, 4, 5)\n            leds.toggle(0)   # turn on the first LED (pin 2)\n            leds.toggle(-1)  # turn on the last LED (pin 5)\n            leds.toggle()    # turn the first and last LED off, and the\n                             # middle pair on\n\n        If :meth:`blink` is currently active, it will be stopped first.\n\n        :param int args:\n            The index(es) of the LED(s) to toggle. If no indexes are specified\n            toggle the state of all LEDs.\n        \"\"\"\n        self._stop_blink()\n        if args:\n            for index in args:\n                self[index].toggle()\n        else:\n            super(LEDBoard, self).toggle()",
        "sampled": "```\nclass LEDBoard:\n    def __init__(self, *leds):\n        self.state = [False] * len(leds)\n        self.pins = leds\n    \n    def toggle(self, *args=None):\n        if hasattr(self,'blink_thread') and self.blink_thread.is_alive():\n           self.blink_thread.stop()\n           del self.blink_thread\n        \n        if args is None:\n           for i in range(len(self.state)):\n               self.state[i] = not self.state"
    },
    {
        "original": "def placeOrder(self, contract: Contract, order: Order) -> Trade:\n        \"\"\"\n        Place a new order or modify an existing order.\n        Returns a Trade that is kept live updated with\n        status changes, fills, etc.\n\n        Args:\n            contract: Contract to use for order.\n            order: The order to be placed.\n        \"\"\"\n        orderId = order.orderId or self.client.getReqId()\n        self.client.placeOrder(orderId, contract, order)\n        now = datetime.datetime.now(datetime.timezone.utc)\n        key = self.wrapper.orderKey(\n            self.wrapper.clientId, orderId, order.permId)\n        trade = self.wrapper.trades.get(key)\n        if trade:\n            # this is a modification of an existing order\n            assert trade.orderStatus.status not in OrderStatus.DoneStates\n            logEntry = TradeLogEntry(now, trade.orderStatus.status, 'Modify')\n            trade.log.append(logEntry)\n            self._logger.info(f'placeOrder: Modify order {trade}')\n            trade.modifyEvent.emit(trade)\n            self.orderModifyEvent.emit(trade)\n        else:\n            # this is a new order\n            order.clientId = self.wrapper.clientId\n            order.orderId = orderId\n            orderStatus = OrderStatus(status=OrderStatus.PendingSubmit)\n            logEntry = TradeLogEntry(now, orderStatus.status, '')\n            trade = Trade(\n                contract, order, orderStatus, [], [logEntry])\n            self.wrapper.trades[key] = trade\n            self._logger.info(f'placeOrder: New order {trade}')\n            self.newOrderEvent.emit(trade)\n        return trade",
        "sampled": "```\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n@dataclass\nclass Contract:\n    symbol: str\n    # other attributes ...\n\n@dataclass\nclass Order:\n    quantity: int\n    # other attributes ...\n\nclass Status(Enum):\n    NEW = 1\n    FILLED = 2\n    CANCELED = 3\n\n@dataclass\nclass Trade:\n    id: int\n    contract: Contract\n    order: Order\n    status: Status\n\ndef"
    },
    {
        "original": "def get_jid(jid):\n    \"\"\"\n    Return the information returned when the specified job id was executed\n    \"\"\"\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    sql = ",
        "sampled": "```\nclass JobTracker:\n    def __init__(self):\n        self.jobs = {}\n\n    def submit(self, jid, result):\n        self.jobs[jid] = result\n\n    def get_jid(self, jid):\n        return self.jobs.get(jid)\n\ntracker = JobTracker()\n\n# Example usage\ntracker.submit(1, \"Result of job 1\")\ntracker.submit(2, \"Result of job 2\")\n\nprint(tracker.get_jid(1))  # Output"
    },
    {
        "original": "def _ioctl(self, func, arg):\n        \"\"\"Runs the specified ioctl on the underlying fd.\n\n        Raises WatchdogError if the device is closed.\n        Raises OSError or IOError (Python 2) when the ioctl fails.\"\"\"\n        if self._fd is None:\n            raise WatchdogError(\"Watchdog device is closed\")\n        if os.name != 'nt':\n            import fcntl\n            fcntl.ioctl(self._fd, func, arg, True)",
        "sampled": "```\nclass WatchdogError(Exception):\n    pass\n\nclass IOError(OSError):\n    pass  # For Python 2 compatibility\n\nclass Device:\n    def __init__(self, fd):\n        self.fd = fd\n        self.closed = False\n\n    def _ioctl(self, func, arg):\n        if self.closed:\n            raise WatchdogError(\"Device is closed\")\n        \n        try:\n            import fcntl\n            return fcntl.ioctl(self.fd, func, arg)\n        except OSError"
    },
    {
        "original": "def _policyFileReplaceOrAppend(this_string, policy_data, append_only=False):\n    \"\"\"\n    helper function to take a ADMX policy string for registry.pol file data and\n    update existing string or append the string to the data\n    \"\"\"\n    # we are going to clean off the special pre-fixes, so we get only the valuename\n    if not policy_data:\n        policy_data = b''\n    specialValueRegex = salt.utils.stringutils.to_bytes(r'(\\*\\*Del\\.|\\*\\*DelVals\\.){0,1}')\n    item_key = None\n    item_value_name = None\n    data_to_replace = None\n    if not append_only:\n        item_key = this_string.split(b'\\00;')[0].lstrip(b'[')\n        item_value_name = re.sub(specialValueRegex,\n                                 b'',\n                                 this_string.split(b'\\00;')[1],\n                                 flags=re.IGNORECASE)\n        log.debug('item value name is %s', item_value_name)\n        data_to_replace = _regexSearchKeyValueCombo(policy_data, item_key, item_value_name)\n    if data_to_replace:\n        log.debug('replacing %s with %s', data_to_replace, this_string)\n        policy_data = policy_data.replace(data_to_replace, this_string)\n    else:\n        log.debug('appending %s', this_string)\n        policy_data = b''.join([policy_data, this_string])\n\n    return policy_data",
        "sampled": "```\ndef _policyFileReplaceOrAppend(this_string, policy_data, append_only=False):\n    if append_only:\n        policy_data += this_string\n    else:\n        policy_data = this_string\n    return policy_data\n```"
    },
    {
        "original": "def parse(html):\n    \"\"\"\n    Parses the given HTML message and returns its stripped representation\n    plus a list of the MessageEntity's that were found.\n\n    :param message: the message with HTML to be parsed.\n    :return: a tuple consisting of (clean message, [message entities]).\n    \"\"\"\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(_add_surrogate(html))\n    text = helpers.strip_text(parser.text, parser.entities)\n    return _del_surrogate(text), parser.entities",
        "sampled": "```\nimport re\nfrom html.parser import HTMLParser\nfrom typing import List, Tuple\n\nclass MessageEntity:\n    def __init__(self, offset: int, length: int, html: str):\n        self.offset = offset\n        self.length = length\n        self.html = html\n\nclass Parser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.clean_message = ''\n        self.entities = []\n        self.in_tag = False\n\n    def handle_data"
    },
    {
        "original": "def execute(self, eopatch):\n        \"\"\" Computation of NDVI slope using finite central differences\n\n        This implementation loops through every spatial location, considers the valid NDVI values and approximates their\n        first order derivative using central differences. The argument of min and max is added to the eopatch.\n\n        The NDVI slope at date t is comuted as $(NDVI_{t+1}-NDVI_{t-1})/(date_{t+1}-date_{t-1})$.\n\n        :param eopatch: Input eopatch\n        :return: eopatch with NDVI slope argmin/argmax features\n        \"\"\"\n        # pylint: disable=invalid-name\n        if self.mask_data:\n            valid_data_mask = eopatch.mask['VALID_DATA']\n        else:\n            valid_data_mask = eopatch.mask['IS_DATA']\n\n        ndvi = np.ma.array(eopatch.data[self.data_feature],\n                           dtype=np.float32,\n                           mask=~valid_data_mask.astype(np.bool))\n\n        all_dates = np.asarray([x.toordinal() for x in eopatch.timestamp])\n\n        if ndvi.ndim == 4:\n            h, w = ndvi.shape[1: 3]\n        else:\n            raise ValueError('{} feature has incorrect number of dimensions'.format(self.data_feature))\n\n        argmax_ndvi_slope, argmin_ndvi_slope = np.zeros((h, w, 1), dtype=np.uint8), np.zeros((h, w, 1), dtype=np.uint8)\n\n        for ih, iw in it.product(range(h), range(w)):\n\n            ndvi_curve = ndvi[:, ih, iw, :]\n            valid_idx = np.where(~ndvi.mask[:, ih, iw])[0]\n\n            ndvi_curve = ndvi_curve[valid_idx]\n            valid_dates = all_dates[valid_idx]\n\n            ndvi_slope = np.convolve(ndvi_curve.squeeze(), [1, 0, -1], 'valid') / np.convolve(valid_dates, [1, 0, -1],\n                                                                                              'valid')\n\n            # +1 to compensate for the 'valid' convolution which eliminates first and last\n            argmax_ndvi_slope[ih, iw] = valid_idx[np.argmax(ndvi_slope) + 1]\n            argmin_ndvi_slope[ih, iw] = valid_idx[np.argmin(ndvi_slope) + 1]\n\n            del ndvi_curve, valid_idx, valid_dates, ndvi_slope\n\n        eopatch.data_timeless[self.argmax_feature] = argmax_ndvi_slope\n        eopatch.data_timeless[self.argmin_feature] = argmin_ndvi_slope\n\n        return eopatch",
        "sampled": "```python\ndef execute(eopatch):\n    ndvi_values = [pixel.NDVI for pixel in eopatch.pixels]\n    dates = [pixel.date for pixel in eopatch.pixels]\n\n    ndvi_slope = []\n    for i in range(1, len(ndvi_values) - 1):\n        numerator = ndvi_values[i + 1] - ndvi_values[i - 1]\n        denominator = dates[i + 1] - dates[i - "
    },
    {
        "original": "def set_user_avatar(self, username, avatar):\n        \"\"\"Set a user's avatar.\n\n        :param username: the user to set the avatar for\n        :param avatar: ID of the avatar to set\n        \"\"\"\n        self._set_avatar(\n            {'username': username}, self._get_url('user/avatar'), avatar)",
        "sampled": "```\nclass UserAvatarManager:\n    def __init__(self):\n        self.user_avatars = {}\n\n    def set_user_avatar(self, username, avatar):\n        self.user_avatars[username] = avatar\n\n# Example usage:\nmanager = UserAvatarManager()\nmanager.set_user_avatar(\"john\", 1)\nmanager.set_user_avatar(\"jane\", 2)\n\nprint(manager.user_avatars)  # Output: {\"john\": 1, \"jane\": 2}\n```"
    }
]
