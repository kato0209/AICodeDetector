[
    {
        "original": "def initialize_segment_register_x64(self, state, concrete_target):\n        \"\"\"\n        Set the gs register in the angr to the value of the fs register in the concrete process\n\n        :param state:               state which will be modified\n        :param concrete_target:     concrete target that will be used to read the fs register\n        :return: None\n       \"\"\"\n        _l.debug(\"Synchronizing gs segment register\")\n        state.regs.gs = self._read_gs_register_x64(concrete_target)",
        "sampled": "```python\nfrom angr import SimState, SimConcreteTarget\n\ndef initialize_segment_register_x64(self, state, concrete_target):\n    \"\"\"\n    Set the gs register in the angr to the value of the fs register in the concrete process\n\n    :param state:               state which will be modified\n    :param concrete_target:     concrete target that will be used to read the fs register\n    :return: None\n    \"\"\"\n    # Get the fs register value from the concrete"
    },
    {
        "original": "def load_csv_file(csv_file):\n    \"\"\" load csv file and check file content format\n\n    Args:\n        csv_file (str): csv file path, csv file content is like below:\n\n    Returns:\n        list: list of parameters, each parameter is in dict format\n\n    Examples:\n        >>> cat csv_file\n        username,password\n        test1,111111\n        test2,222222\n        test3,333333\n\n        >>> load_csv_file(csv_file)\n        [\n            {'username': 'test1', 'password': '111111'},\n            {'username': 'test2', 'password': '222222'},\n            {'username': 'test3', 'password': '333333'}\n        ]\n\n    \"\"\"\n    if not os.path.isabs(csv_file):\n        project_working_directory = tests_def_mapping[\"PWD\"] or os.getcwd()\n        # make compatible with Windows/Linux\n        csv_file = os.path.join(project_working_directory, *csv_file.split(\"/\"))\n\n    if not os.path.isfile(csv_file):\n        # file path not exist\n        raise exceptions.CSVNotFound(csv_file)\n\n    csv_content_list = []\n\n    with io.open(csv_file, encoding='utf-8') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            csv_content_list.append(row)\n\n    return csv_content_list",
        "sampled": "```python\nimport csv\n\ndef load_csv_file(csv_file):\n    \"\"\"\n    load csv file and check file content format\n\n    Args:\n        csv_file (str): csv file path, csv file content is like below:\n\n    Returns:\n        list: list of parameters, each parameter is in dict format\n\n    Examples:\n        >>> cat csv_file\n        username,password\n        test1,111111\n        test2,222222\n        test3,333333\n\n        >>> load_csv"
    },
    {
        "original": "async def get_proxies(self):\n        \"\"\"Receive proxies from the provider and return them.\n\n        :return: :attr:`.proxies`\n        \"\"\"\n        log.debug('Try to get proxies from %s' % self.domain)\n\n        async with aiohttp.ClientSession(\n            headers=get_headers(), cookies=self._cookies, loop=self._loop\n        ) as self._session:\n            await self._pipe()\n\n        log.debug(\n            '%d proxies received from %s: %s'\n            % (len(self.proxies), self.domain, self.proxies)\n        )\n        return self.proxies",
        "sampled": "```python\nimport asyncio\nimport aiohttp\n\nclass ProxyProvider:\n    def __init__(self, provider_url):\n        self.provider_url = provider_url\n        self.proxies = []\n\n    async def get_proxies(self):\n        \"\"\"Receive proxies from the provider and return them.\n\n        :return: :attr:`.proxies`\n        \"\"\"\n        async with aiohttp.ClientSession() as session:\n            async with session.get(self.provider_url) as response:\n                if response.status == 200"
    },
    {
        "original": "def synthesize(vers, opts):\n    \"\"\" Synthesize a capability enabled version response\n        This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    failed = False  # noqa: F841 T25377293 Grandfathered in\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                \"client required capability `\"\n                + name\n                + \"` is not supported by this server\"\n            )\n    return vers",
        "sampled": "### Problem Description\n### \n### Synthesize a Capability Enabled Version Response\n### \n### Problem Statement\n### \n### Given a version and options, synthesize a capability enabled version response.\n\n### Input and Output Specifications\n### \n### Input:\n### - `vers`: The version to synthesize\n### - `opts`: The options to synthesize\n### \n### Output:\n### - A capability enabled version response\n\n### Python Solution\n```python\ndef synthesize(vers, opts):\n   "
    },
    {
        "original": "def _receive_message(self):\n        \"\"\"\n        Internal coroutine for receiving messages\n        \"\"\"\n        while True:\n            try:\n                if self._socket.getsockopt(zmq.TYPE) == zmq.ROUTER:\n                    zmq_identity, msg_bytes = \\\n                        yield from self._socket.recv_multipart()\n                    if msg_bytes == b'':\n                        # send ACK for connection probes\n                        LOGGER.debug(\"ROUTER PROBE FROM %s\", zmq_identity)\n                        self._socket.send_multipart(\n                            [bytes(zmq_identity), msg_bytes])\n                    else:\n                        self._received_from_identity(zmq_identity)\n                        self._dispatcher_queue.put_nowait(\n                            (zmq_identity, msg_bytes))\n                else:\n                    msg_bytes = yield from self._socket.recv()\n                    self._last_message_time = time.time()\n                    self._dispatcher_queue.put_nowait((None, msg_bytes))\n                self._get_queue_size_gauge(self.connection).set_value(\n                    self._dispatcher_queue.qsize())\n\n            except CancelledError:  # pylint: disable=try-except-raise\n                # The concurrent.futures.CancelledError is caught by asyncio\n                # when the Task associated with the coroutine is cancelled.\n                # The raise is required to stop this component.\n                raise\n            except Exception as e:  # pylint: disable=broad-except\n                LOGGER.exception(\"Received a message on address %s that \"\n                                 \"caused an error: %s\", self._address, e)",
        "sampled": "```python\nimport asyncio\n\nclass MessageReceiver:\n    def __init__(self):\n        self.message_queue = asyncio.Queue()\n\n    async def _receive_message(self):\n        \"\"\"\n        Internal coroutine for receiving messages\n        \"\"\"\n        while True:\n            message = await self.message_queue.get()\n            print(f\"Received message: {message}\")\n            self.message_queue.task_done()\n\n    async def start_receiving(self):\n        \"\"\"\n        Start the message receiver coroutine\n        \"\"\"\n        receiver_task = asyncio.create_task"
    },
    {
        "original": "def _always_running_service(name):\n    \"\"\"\n    Check if the service should always be running based on the KeepAlive Key\n    in the service plist.\n\n    :param str name: Service label, file name, or full path\n\n    :return: True if the KeepAlive key is set to True, False if set to False or\n        not set in the plist at all.\n\n    :rtype: bool\n\n    .. versionadded:: 2019.2.0\n    \"\"\"\n\n    # get all the info from the launchctl service\n    service_info = show(name)\n\n    # get the value for the KeepAlive key in service plist\n    try:\n        keep_alive = service_info['plist']['KeepAlive']\n    except KeyError:\n        return False\n\n    # check if KeepAlive is True and not just set.\n\n    if isinstance(keep_alive, dict):\n        # check for pathstate\n        for _file, value in six.iteritems(keep_alive.get('PathState', {})):\n            if value is True and os.path.exists(_file):\n                return True\n            elif value is False and not os.path.exists(_file):\n                return True\n\n    if keep_alive is True:\n        return True\n\n    return False",
        "sampled": "```python\nimport plistlib\nimport os\n\ndef _always_running_service(name):\n    \"\"\"\n    Check if the service should always be running based on the KeepAlive Key\n    in the service plist.\n\n    :param str name: Service label, file name, or full path\n\n    :return: True if the KeepAlive key is set to True, False if set to False or\n        not set in the plist at all.\n\n    :rtype: bool\n\n    .. versionadded:: "
    },
    {
        "original": "def _flush_content(self):\n        \"\"\"\n        Flush content to the archive\n        :return:\n        \"\"\"\n        if self.__current_section is not None:\n            buff = BytesIO()\n            buff._dirty = False\n            for action_return in self.__current_section:\n                for title, ret_data in action_return.items():\n                    if isinstance(ret_data, file):\n                        self.out.put(ret_data.name, indent=4)\n                        self.__arch.add(ret_data.name, arcname=ret_data.name)\n                    else:\n                        buff.write(salt.utils.stringutils.to_bytes(title + '\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(('-' * len(title)) + '\\n\\n'))\n                        buff.write(salt.utils.stringutils.to_bytes(ret_data))\n                        buff.write(salt.utils.stringutils.to_bytes('\\n\\n\\n'))\n                        buff._dirty = True\n            if buff._dirty:\n                buff.seek(0)\n                tar_info = tarfile.TarInfo(name=\"{}/{}\".format(self.__default_root, self.__current_section_name))\n                if not hasattr(buff, 'getbuffer'):  # Py2's BytesIO is older\n                    buff.getbuffer = buff.getvalue\n                tar_info.size = len(buff.getbuffer())\n                self.__arch.addfile(tarinfo=tar_info, fileobj=buff)",
        "sampled": "```python\nclass Solution:\n    def _flush_content(self):\n        \"\"\"\n        Flush content to the archive\n        :return:\n        \"\"\"\n        # This method is not implemented in the problem description\n        # So, we will just return a message\n        return \"Content flushed to the archive\"\n```"
    },
    {
        "original": "def fit_linear(X, y):\n    \"\"\"\n    Uses OLS to fit the regression.\n    \"\"\"\n    model = linear_model.LinearRegression()\n    model.fit(X, y)\n    return model",
        "sampled": "```python\nimport numpy as np\n\ndef fit_linear(X, y):\n    \"\"\"\n    Uses OLS to fit the regression.\n    \n    Parameters:\n    X (numpy array): Design matrix\n    y (numpy array): Response variable\n    \n    Returns:\n    beta (float): Coefficient of the linear regression model\n    \"\"\"\n    \n    # Calculate the mean of X and y\n    mean_X = np.mean(X, axis=0)\n    \n    # Subtract the mean from each column in X"
    },
    {
        "original": "def ensure_coordinator_ready(self):\n        \"\"\"Block until the coordinator for this group is known\n        (and we have an active connection -- java client uses unsent queue).\n        \"\"\"\n        with self._client._lock, self._lock:\n            while self.coordinator_unknown():\n\n                # Prior to 0.8.2 there was no group coordinator\n                # so we will just pick a node at random and treat\n                # it as the \"coordinator\"\n                if self.config['api_version'] < (0, 8, 2):\n                    self.coordinator_id = self._client.least_loaded_node()\n                    if self.coordinator_id is not None:\n                        self._client.maybe_connect(self.coordinator_id)\n                    continue\n\n                future = self.lookup_coordinator()\n                self._client.poll(future=future)\n\n                if future.failed():\n                    if future.retriable():\n                        if getattr(future.exception, 'invalid_metadata', False):\n                            log.debug('Requesting metadata for group coordinator request: %s', future.exception)\n                            metadata_update = self._client.cluster.request_update()\n                            self._client.poll(future=metadata_update)\n                        else:\n                            time.sleep(self.config['retry_backoff_ms'] / 1000)\n                    else:\n                        raise future.exception",
        "sampled": "```python\nimport threading\nimport time\n\nclass Coordinator:\n    def __init__(self):\n        self.coordinator_known = False\n        self.lock = threading.Lock()\n\n    def ensure_coordinator_ready(self):\n        \"\"\"Block until the coordinator for this group is known\n        (and we have an active connection -- java client uses unsent queue).\n        \"\"\"\n        \n        while not self.coordinator_known:\n            with self.lock:\n                if not self.coordinator_known:\n                    print(\"Waiting for coordinator"
    },
    {
        "original": "def get_dim_indexers(data_obj, indexers):\n    \"\"\"Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \"\"\"\n    invalid = [k for k in indexers\n               if k not in data_obj.dims and k not in data_obj._level_coords]\n    if invalid:\n        raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n                         % invalid)\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        dim, = data_obj[key].dims\n        if key != dim:\n            # assume here multi-index level indexer\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key] = label\n\n    for dim, level_labels in level_indexers.items():\n        if dim_indexers.get(dim, False):\n            raise ValueError(\"cannot combine multi-index level indexers \"\n                             \"with an indexer for dimension %s\" % dim)\n        dim_indexers[dim] = level_labels\n\n    return dim_indexers",
        "sampled": "```python\nimport xarray as xr\n\ndef get_dim_indexers(data_obj, indexers):\n    \"\"\"\n    Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \n    Parameters:\n        data_obj (xarray.DataArray):"
    },
    {
        "original": "def best_case(self, matrix, m_list, indices_left):\n        \"\"\"\n        Computes a best case given a matrix and manipulation list.\n\n        Args:\n            matrix: the current matrix (with some permutations already\n                performed)\n            m_list: [(multiplication fraction, number_of_indices, indices,\n                species)] describing the manipulation\n            indices: Set of indices which haven't had a permutation\n                performed on them.\n        \"\"\"\n        m_indices = []\n        fraction_list = []\n        for m in m_list:\n            m_indices.extend(m[2])\n            fraction_list.extend([m[0]] * m[1])\n\n        indices = list(indices_left.intersection(m_indices))\n\n        interaction_matrix = matrix[indices, :][:, indices]\n\n        fractions = np.zeros(len(interaction_matrix)) + 1\n        fractions[:len(fraction_list)] = fraction_list\n        fractions = np.sort(fractions)\n\n        # Sum associated with each index (disregarding interactions between\n        # indices)\n        sums = 2 * np.sum(matrix[indices], axis=1)\n        sums = np.sort(sums)\n\n        # Interaction corrections. Can be reduced to (1-x)(1-y) for x,y in\n        # fractions each element in a column gets multiplied by (1-x), and then\n        # the sum of the columns gets multiplied by (1-y) since fractions are\n        # less than 1, there is no effect of one choice on the other\n        step1 = np.sort(interaction_matrix) * (1 - fractions)\n        step2 = np.sort(np.sum(step1, axis=1))\n        step3 = step2 * (1 - fractions)\n        interaction_correction = np.sum(step3)\n\n        if self._algo == self.ALGO_TIME_LIMIT:\n            elapsed_time = datetime.utcnow() - self._start_time\n            speedup_parameter = elapsed_time.total_seconds() / 1800\n            avg_int = np.sum(interaction_matrix, axis=None)\n            avg_frac = np.average(np.outer(1 - fractions, 1 - fractions))\n            average_correction = avg_int * avg_frac\n\n            interaction_correction = average_correction * speedup_parameter \\\n                + interaction_correction * (1 - speedup_parameter)\n\n        best_case = np.sum(matrix) + np.inner(sums[::-1], fractions - 1) \\\n            + interaction_correction\n\n        return best_case",
        "sampled": "```python\ndef best_case(self, matrix, m_list, indices_left):\n    \"\"\"\n    Computes a best case given a matrix and manipulation list.\n\n    Args:\n        matrix: the current matrix (with some permutations already\n            performed)\n        m_list: [(multiplication fraction, number_of_indices, indices,\n            species)] describing the manipulation\n        indices: Set of indices which haven't had a permutation\n            performed on them.\n    \"\"\"\n    \n    # Initialize the maximum value and its corresponding"
    },
    {
        "original": "def tempo_account_delete_account_by_id(self, account_id):\n        \"\"\"\n        Delete an Account by id. Caller must have the Manage Account Permission for the Account.\n        The Account can not be deleted if it has an AccountLinkBean.\n        :param account_id: the id of the Account to be deleted.\n        :return:\n        \"\"\"\n        url = 'rest/tempo-accounts/1/account/{id}/'.format(id=account_id)\n        return self.delete(url)",
        "sampled": "```python\nclass Account:\n    def __init__(self, id):\n        self.id = id\n        self.links = []\n\n    def add_link(self, link):\n        self.links.append(link)\n\n    def delete_account(self):\n        if len(self.links) > 0:\n            return False\n        else:\n            return True\n\n\nclass AccountLinkBean:\n    pass\n\n\ndef tempo_account_delete_account_by_id(account_id, accounts):\n    \"\"\"\n    Delete an Account by id. Caller must have the"
    },
    {
        "original": "def _encode_long(name, value, dummy0, dummy1):\n    \"\"\"Encode a python long (python 2.x)\"\"\"\n    try:\n        return b\"\\x12\" + name + _PACK_LONG(value)\n    except struct.error:\n        raise OverflowError(\"BSON can only handle up to 8-byte ints\")",
        "sampled": "```python\ndef _encode_long(name, value, dummy0, dummy1):\n    return (name << 56) | (dummy0 << 48) | (dummy1 << 40) | (value & 0xFFFFFFFFFFFFFFFF)\n```"
    },
    {
        "original": "def read_can_msg(self, channel, count):\n        \"\"\"\n        Reads one or more CAN-messages from the buffer of the specified CAN channel.\n\n        :param int channel:\n            CAN channel to read from (:data:`Channel.CHANNEL_CH0`, :data:`Channel.CHANNEL_CH1`,\n            :data:`Channel.CHANNEL_ANY`).\n        :param int count: The number of CAN messages to be received.\n        :return: Tuple with list of CAN message/s received and the CAN channel where the read CAN messages came from.\n        :rtype: tuple(list(CanMsg), int)\n        \"\"\"\n        c_channel = BYTE(channel)\n        c_can_msg = (CanMsg * count)()\n        c_count = DWORD(count)\n        UcanReadCanMsgEx(self._handle, byref(c_channel), c_can_msg, byref(c_count))\n        return c_can_msg[:c_count.value], c_channel.value",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\nimport time\n\nclass CanMsg:\n    def __init__(self, id, data):\n        self.id = id\n        self.data = data\n\nclass AutoParkSignal:\n    def __init__(self):\n        self.last_received_time = None\n\n    def initialize(self):\n        # Initialize the signal here if needed.\n        pass\n\nclass QuadratureEncoderSensor(ABC):\n    @abstractmethod\n    def get_speed(self) -> float:\n       "
    },
    {
        "original": "def WriteFlowRequests(self, requests):\n    \"\"\"Writes a list of flow requests to the database.\"\"\"\n    flow_processing_requests = []\n\n    for request in requests:\n      if (request.client_id, request.flow_id) not in self.flows:\n        raise db.AtLeastOneUnknownFlowError([(request.client_id,\n                                              request.flow_id)])\n\n    for request in requests:\n      key = (request.client_id, request.flow_id)\n      request_dict = self.flow_requests.setdefault(key, {})\n      request_dict[request.request_id] = request.Copy()\n      request_dict[request.request_id].timestamp = rdfvalue.RDFDatetime.Now()\n\n      if request.needs_processing:\n        flow = self.flows[(request.client_id, request.flow_id)]\n        if flow.next_request_to_process == request.request_id:\n          flow_processing_requests.append(\n              rdf_flows.FlowProcessingRequest(\n                  client_id=request.client_id,\n                  flow_id=request.flow_id,\n                  delivery_time=request.start_time))\n\n    if flow_processing_requests:\n      self.WriteFlowProcessingRequests(flow_processing_requests)",
        "sampled": "```python\ndef WriteFlowRequests(self, requests):\n    \"\"\"\n    Writes a list of flow requests to the database.\n    \n    Args:\n        requests (list): A list of flow requests.\n        \n    Returns:\n        None\n    \"\"\"\n    \n    # Check if the input is a list\n    if not isinstance(requests, list):\n        raise TypeError(\"Input must be a list of flow requests.\")\n    \n    # Iterate over each request in the list\n    for request in requests:\n        #"
    },
    {
        "original": "def _create_gitlab_prometheus_instance(self, instance, init_config):\n        \"\"\"\n        Set up the gitlab instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        # Mapping from Prometheus metrics names to Datadog ones\n        # For now it's a 1:1 mapping\n        allowed_metrics = init_config.get('allowed_metrics')\n        if allowed_metrics is None:\n            raise CheckException(\"At least one metric must be whitelisted in `allowed_metrics`.\")\n\n        gitlab_instance = deepcopy(instance)\n        # gitlab uses 'prometheus_endpoint' and not 'prometheus_url', so we have to rename the key\n        gitlab_instance['prometheus_url'] = instance.get('prometheus_endpoint')\n\n        gitlab_instance.update(\n            {\n                'namespace': 'gitlab',\n                'metrics': allowed_metrics,\n                # Defaults that were set when gitlab was based on PrometheusCheck\n                'send_monotonic_counter': instance.get('send_monotonic_counter', False),\n                'health_service_check': instance.get('health_service_check', False),\n            }\n        )\n\n        return gitlab_instance",
        "sampled": "```python\ndef _create_gitlab_prometheus_instance(self, instance, init_config):\n    \"\"\"\n    Set up the gitlab instance so it can be used in OpenMetricsBaseCheck\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the configuration for the GitLab Prometheus instance\n    config = {}\n    \n    # Set the URL of the GitLab server\n    config['url'] = init_config.get('url')\n    \n    # Set the username and password for authentication (if required)\n"
    },
    {
        "original": "def setup_voronoi_list(self, indices, voronoi_cutoff):\n        \"\"\"\n        Set up of the voronoi list of neighbours by calling qhull\n        :param indices: indices of the sites for which the Voronoi is needed\n        :param voronoi_cutoff: Voronoi cutoff for the search of neighbours\n        :raise RuntimeError: If an infinite vertex is found in the voronoi construction\n        \"\"\"\n        self.voronoi_list2 = [None] * len(self.structure)\n        self.voronoi_list_coords = [None] * len(self.structure)\n        logging.info('Getting all neighbors in structure')\n        struct_neighbors = self.structure.get_all_neighbors(voronoi_cutoff, include_index=True)\n        t1 = time.clock()\n        logging.info('Setting up Voronoi list :')\n\n        for jj, isite in enumerate(indices):\n            logging.info('  - Voronoi analysis for site #{:d} ({:d}/{:d})'.format(isite, jj+1, len(indices)))\n            site = self.structure[isite]\n            neighbors1 = [(site, 0.0, isite)]\n            neighbors1.extend(struct_neighbors[isite])\n            distances = [i[1] for i in sorted(neighbors1, key=lambda s: s[1])]\n            neighbors = [i[0] for i in sorted(neighbors1, key=lambda s: s[1])]\n            qvoronoi_input = [s.coords for s in neighbors]\n            voro = Voronoi(points=qvoronoi_input, qhull_options=\"o Fv\")\n            all_vertices = voro.vertices\n\n            results2 = []\n            maxangle = 0.0\n            mindist = 10000.0\n            for iridge, ridge_points in enumerate(voro.ridge_points):\n                if 0 in ridge_points:\n                    ridge_vertices_indices = voro.ridge_vertices[iridge]\n                    if -1 in ridge_vertices_indices:\n                        raise RuntimeError(\"This structure is pathological,\"\n                                           \" infinite vertex in the voronoi \"\n                                           \"construction\")\n\n                    ridge_point2 = max(ridge_points)\n                    facets = [all_vertices[i] for i in ridge_vertices_indices]\n                    sa = my_solid_angle(site.coords, facets)\n                    maxangle = max([sa, maxangle])\n\n                    mindist = min([mindist, distances[ridge_point2]])\n                    for iii, sss in enumerate(self.structure):\n                        if neighbors[ridge_point2].is_periodic_image(sss):\n                            myindex = iii\n                            break\n                    results2.append({'site': neighbors[ridge_point2],\n                                     'angle': sa,\n                                     'distance': distances[ridge_point2],\n                                     'index': myindex})\n            for dd in results2:\n                dd['normalized_angle'] = dd['angle'] / maxangle\n                dd['normalized_distance'] = dd['distance'] / mindist\n            self.voronoi_list2[isite] = results2\n            self.voronoi_list_coords[isite] = np.array([dd['site'].coords for dd in results2])\n        t2 = time.clock()\n        logging.info('Voronoi list set up in {:.2f} seconds'.format(t2-t1))",
        "sampled": "```python\nimport numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\ndef setup_voronoi_list(self, indices, voronoi_cutoff):\n    \"\"\"\n    Set up of the voronoi list of neighbours by calling qhull\n    :param indices: indices of the sites for which the Voronoi is needed\n    :param voronoi_cutoff: Voronoi cutoff for the search of neighbours\n    :raise RuntimeError: If"
    },
    {
        "original": "def validate_config(self, config):\n        \"\"\"We only validate the config if passed.\n\n        Also we use the ExperimentSpecification to check if this config was\n        intended as an experiment.\n        \"\"\"\n        # config is optional\n        if not config:\n            return config\n\n        spec = validate_experiment_spec_config(config)\n\n        if spec.is_experiment:\n            # Resume normal creation\n            return config\n\n        # Raise an error to tell the user to use experiment creation instead\n        raise ValidationError('Current experiment creation could not be performed.\\n'\n                              'The reason is that the specification sent correspond '\n                              'to a `{}`.\\n'.format(spec.kind))",
        "sampled": "```python\nclass ConfigValidator:\n    def validate_config(self, config):\n        \"\"\"\n        We only validate the config if passed.\n\n        Also we use the ExperimentSpecification to check if this config was\n        intended as an experiment.\n        \n        Parameters:\n            config (dict): The configuration to be validated\n        \n        Returns:\n            bool: True if the configuration is valid, False otherwise\n        \"\"\"\n        \n        # Check if the config is not None and is a dictionary\n        if not isinstance(config,"
    },
    {
        "original": "def stash_state():\n    \"\"\"\n    Builds a list of all currently pressed scan codes, releases them and returns\n    the list. Pairs well with `restore_state` and `restore_modifiers`.\n    \"\"\"\n    # TODO: stash caps lock / numlock /scrollock state.\n    with _pressed_events_lock:\n        state = sorted(_pressed_events)\n    for scan_code in state:\n        _os_keyboard.release(scan_code)\n    return state",
        "sampled": "```python\nimport pyautogui\n\ndef stash_state():\n    \"\"\"\n    Builds a list of all currently pressed scan codes, releases them and returns\n    the list. Pairs well with `restore_state` and `restore_modifiers`.\n    \"\"\"\n    \n    # Get the current state of keyboard modifiers (e.g., shift, ctrl, alt)\n    keys = pyautogui.presses()\n    \n    # Release all keys\n    for key in keys:\n        pyautogui.keyUp(key)\n"
    },
    {
        "original": "def _marginalize_factor(self, nodes, factor):\n        \"\"\"\n        Marginalizing the factor selectively for a set of variables.\n\n        Parameters:\n        ----------\n        nodes: list, array-like\n            A container of nodes (list, dict, set, etc.).\n\n        factor: factor\n            factor which is to be marginalized.\n        \"\"\"\n        marginalizing_nodes = list(set(factor.scope()).difference(nodes))\n        return factor.marginalize(marginalizing_nodes, inplace=False)",
        "sampled": "```python\nfrom cpmpy import *\nfrom itertools import product\n\nclass Factor:\n    def __init__(self, domain):\n        self.domain = domain\n\n    def __mul__(self, other):\n        # assume both factors have binary domains for simplicity\n        self_labels = list(self.domain.keys())\n        other_labels = list(other.domain.keys())\n        \n        all_factors = []\n        for p in product(self.domain.values(), repeat=len(self_labels)):\n            assignment1 = {label: value for label"
    },
    {
        "original": "def broadcast_info(team_id, date=datetime.now()):\n    \"\"\"Return BroadcastInfo object that containts information\n    about the television and radio broadcasts for the team_id\n    and year\"\"\"\n    data = mlbgame.info.broadcast_info(team_id, date)\n    return [mlbgame.info.BroadcastInfo(x) for x in data]",
        "sampled": "```python\nfrom datetime import datetime\n\nclass BroadcastInfo:\n    def __init__(self, team_id, date):\n        self.team_id = team_id\n        self.date = date\n        self.broadcasts = {}\n\n    def __str__(self):\n        return f\"Broadcast info for team {self.team_id} on {self.date}: {self.broadcasts}\"\n\nclass TelevisionBroadcast:\n    def __init__(self, channel, start_time, end_time):\n        self.channel = channel\n       "
    },
    {
        "original": "def current_size(self):\n        \"\"\"The size of the current line minus the indentation.\"\"\"\n        size = 0\n        for item in reversed(self._lines):\n            size += item.size\n            if isinstance(item, self._LineBreak):\n                break\n\n        return size",
        "sampled": "```python\nclass TextEditor:\n    def __init__(self):\n        self.lines = []\n        self.current_line = 0\n\n    def line(self, line_number):\n        if 1 <= line_number <= len(self.lines):\n            return self.lines[line_number - 1]\n        else:\n            return \"\"\n\n    def split_lines(self, tokens):\n        new_line = \"\"\n        for token in tokens:\n            if len(token) + len(new_line) + (new_line and '.') < "
    },
    {
        "original": "def execute(self):\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)",
        "sampled": "```python\nclass Solution:\n    def execute(self):\n        # Since we don't have more information about the input operations,\n        # let's assume that it's a list of operations.\n        operations = [\"AND\", \"OR\", \"NUMOUTP 5\"]\n        \n        result = []\n        \n        for i in range(len(operations)):\n            op = operations[i]\n            \n            if op == \"NUMOUTP\":\n                num = int(op.split(\" \")[-1])\n                result.append(num"
    },
    {
        "original": "def _send_group_coordinator_request(self):\n        \"\"\"Discover the current coordinator for the group.\n\n        Returns:\n            Future: resolves to the node id of the coordinator\n        \"\"\"\n        node_id = self._client.least_loaded_node()\n        if node_id is None:\n            return Future().failure(Errors.NoBrokersAvailable())\n\n        elif not self._client.ready(node_id, metadata_priority=False):\n            e = Errors.NodeNotReadyError(node_id)\n            return Future().failure(e)\n\n        log.debug(\"Sending group coordinator request for group %s to broker %s\",\n                  self.group_id, node_id)\n        request = GroupCoordinatorRequest[0](self.group_id)\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_group_coordinator_response, future)\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future",
        "sampled": "```python\nimport asyncio\nimport random\n\nclass Node:\n    def __init__(self, id):\n        self.id = id\n        self.coordinator = None\n        self.nodes = []\n\n    async def discover_coordinator(self):\n        \"\"\"Discover the current coordinator for the group.\n\n        Returns:\n            int: id of the coordinator\n        \"\"\"\n        # Simulate a delay in discovering the coordinator\n        await asyncio.sleep(random.uniform(0.1, 1.0))\n        \n        #"
    },
    {
        "original": "def sort(expr, field = None, keytype=None, ascending=True):\n    \"\"\"\n    Sorts the vector.\n    If the field parameter is provided then the sort\n    operators on a vector of structs where the sort key\n    is the field of the struct.\n\n    Args:\n      expr (WeldObject)\n      field (Int)\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    expr_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        expr_var = expr.obj_id\n        weld_obj.dependencies[expr_var] = expr\n\n    if field is not None:\n        key_str = \"x.$%s\" % field\n    else:\n        key_str = \"x\"\n\n    if not ascending:\n        # The type is not necessarily f64.\n        key_str = key_str + \"* %s(-1)\" % keytype\n\n    weld_template = ",
        "sampled": "```python\nclass WeldObject:\n    def __init__(self, data):\n        self.data = data\n\nclass WeldSort:\n    def sort(self, expr, field=None, keytype=None, ascending=True):\n        \"\"\"\n        Sorts the vector.\n        If the field parameter is provided then the sort\n        operators on a vector of structs where the sort key\n        is the field of the struct.\n\n        Args:\n          expr (WeldObject)\n          field (int)\n        \"\"\"\n       "
    },
    {
        "original": "def get_redis(**kwargs):\n    \"\"\"Returns a redis client instance.\n\n    Parameters\n    ----------\n    redis_cls : class, optional\n        Defaults to ``redis.StrictRedis``.\n    url : str, optional\n        If given, ``redis_cls.from_url`` is used to instantiate the class.\n    **kwargs\n        Extra parameters to be passed to the ``redis_cls`` class.\n\n    Returns\n    -------\n    server\n        Redis client instance.\n\n    \"\"\"\n    redis_cls = kwargs.pop('redis_cls', defaults.REDIS_CLS)\n    url = kwargs.pop('url', None)\n    if url:\n        return redis_cls.from_url(url, **kwargs)\n    else:\n        return redis_cls(**kwargs)",
        "sampled": "```python\nfrom redis import StrictRedis\n\ndef get_redis(**kwargs):\n    \"\"\"\n    Returns a redis client instance.\n\n    Parameters\n    ----------\n    redis_cls : class, optional\n        Defaults to ``redis.StrictRedis``.\n    url : str, optional\n        If given, ``redis_cls.from_url`` is used to instantiate the class.\n    **kwargs\n        Extra parameters to be passed to the ``redis_cls`` class.\n\n    Returns\n    -------\n    server\n        Redis client instance"
    },
    {
        "original": "def barrier():\n    \"\"\"\n    Works as a temporary distributed barrier, currently pytorch\n    doesn't implement barrier for NCCL backend.\n    Calls all_reduce on dummy tensor and synchronizes with GPU.\n    \"\"\"\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        torch.distributed.all_reduce(torch.cuda.FloatTensor(1))\n        torch.cuda.synchronize()",
        "sampled": "```python\nimport torch\n\nclass Barrier:\n    \"\"\"\n    A temporary distributed barrier for pytorch.\n    Calls all_reduce on dummy tensor and synchronizes with GPU.\n    \"\"\"\n\n    def __init__(self, size):\n        self.size = size\n\n    def __enter__(self):\n        # Create a dummy tensor with zeros on each node\n        if torch.cuda.is_available():\n            tensor = torch.zeros(1, device=torch.device('cuda:0'))\n            return torch.distributed.all_reduce(tensor"
    },
    {
        "original": "def get_excitation_spectrum(self, width=0.1, npoints=2000):\n        \"\"\"\n        Generate an excitation spectra from the singlet roots of TDDFT\n        calculations.\n\n        Args:\n            width (float): Width for Gaussian smearing.\n            npoints (int): Number of energy points. More points => smoother\n                curve.\n\n        Returns:\n            (ExcitationSpectrum) which can be plotted using\n                pymatgen.vis.plotters.SpectrumPlotter.\n        \"\"\"\n        roots = self.parse_tddft()\n        data = roots[\"singlet\"]\n        en = np.array([d[\"energy\"] for d in data])\n        osc = np.array([d[\"osc_strength\"] for d in data])\n\n        epad = 20.0 * width\n        emin = en[0] - epad\n        emax = en[-1] + epad\n        de = (emax - emin) / npoints\n\n        # Use width of at least two grid points\n        if width < 2 * de:\n            width = 2 * de\n\n        energies = [emin + ie * de for ie in range(npoints)]\n\n        cutoff = 20.0 * width\n        gamma = 0.5 * width\n        gamma_sqrd = gamma * gamma\n\n        de = (energies[-1] - energies[0]) / (len(energies) - 1)\n        prefac = gamma / np.pi * de\n\n        x = []\n        y = []\n        for energy in energies:\n            xx0 = energy - en\n            stot = osc / (xx0 * xx0 + gamma_sqrd)\n            t = np.sum(stot[np.abs(xx0) <= cutoff])\n            x.append(energy)\n            y.append(t * prefac)\n        return ExcitationSpectrum(x, y)",
        "sampled": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom typing import Tuple\n\nclass ExcitationSpectrum:\n    def __init__(self, energies: np.ndarray, intensities: np.ndarray):\n        self.energies = energies\n        self.intensities = intensities\n\n    def plot(self):\n        # This method is not implemented in this solution\n        pass\n\nclass TDDFT:\n    def get_excitation_spectrum(self, width: float = 0.1"
    },
    {
        "original": "def GetFeeds(client):\n  \"\"\"Returns a list of all enabled Feeds.\n\n  Args:\n    client: an AdWordsClient instance.\n\n  Returns:\n    A list containing all enabled Feeds.\n  \"\"\"\n  feed_service = client.GetService('FeedService', 'v201809')\n\n  feeds = []\n  more_pages = True\n\n  selector = {\n      'fields': ['Id', 'Name', 'Attributes'],\n      'predicates': [\n          {\n              'field': 'Origin',\n              'operator': 'EQUALS',\n              'values': ['USER']\n          },\n          {\n              'field': 'FeedStatus',\n              'operator': 'EQUALS',\n              'values': ['ENABLED']\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = feed_service.get(selector)\n\n    if 'entries' in page:\n      feeds.extend(page['entries'])\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return feeds",
        "sampled": "```python\nimport csv\nfrom googleapiclient.discovery import build\n\nclass AdWordsClient:\n    def retrieve_feeds(self):\n        \"\"\"Retrieve enabled feeds.\n        \n        Returns:\n            A list of dictionaries representing enabled feeds.\n        \"\"\"\n        # Only one method for now, the rest of the methods will be added after we complete the other retrieve methods\n        \n    def get_feed_resources(self):\n        \"\"\"Return a list of feed resources\"\"\"\n        \n    def get_feeds(self):\n        \"\"\"Returns"
    },
    {
        "original": "def DetermineRunner(bbdir):\n    \"\"\"Checks if the given directory is a buildbot worker or a master and\n    returns the appropriate run function.\"\"\"\n    try:\n        import buildbot_worker.scripts.runner\n        tacfile = os.path.join(bbdir, 'buildbot.tac')\n\n        if os.path.exists(tacfile):\n            with open(tacfile, 'r') as f:\n                contents = f.read()\n                if 'import Worker' in contents:\n                    return buildbot_worker.scripts.runner.run\n\n    except ImportError:\n        # Use the default\n        pass\n\n    import buildbot.scripts.runner\n    return buildbot.scripts.runner.run",
        "sampled": "```python\ndef DetermineRunner(bbdir):\n    \"\"\"\n    Checks if the given directory is a buildbot worker or a master and returns the appropriate run function.\n    \n    Parameters:\n    bbdir (str): The directory path to check.\n    \n    Returns:\n    function: The run function for the buildbot worker or master.\n    \"\"\"\n    \n    # Assuming we have two functions, run_worker and run_master, for buildbot worker and master respectively\n    def run_worker():\n        # This function"
    },
    {
        "original": "def _binary_sample(image, label, n_samples_per_label, label_count):\n        \"\"\" Sample `nsamples_per_label` points from the binary mask corresponding to `label`\n\n        Randomly sample `nsamples_per_label` point form the binary mask corresponding to `label`. Sampling with\n        replacement is used if the required `nsamples_per_label` is larger than the available `label_count`\n\n        :param image: Input 2D raster label image\n        :type image: uint8 numpy array\n        :param label: Scalar value of label to consider\n        :type label: uint8\n        :param n_samples_per_label: Number of points to sample form the binary mask\n        :type n_samples_per_label: uint32\n        :param label_count: Number of points available for `label`\n        :type label_count: uint32\n        :return: Sampled label value, row index of samples, col index of samples\n        \"\"\"\n        h_idx, w_idx = np.where(image == label)\n\n        rand_idx = np.random.choice(h_idx.size, size=n_samples_per_label, replace=label_count < n_samples_per_label)\n\n        return h_idx[rand_idx], w_idx[rand_idx]",
        "sampled": "```python\nimport numpy as np\n\ndef _binary_sample(image, label, n_samples_per_label, label_count):\n    \"\"\"\n    Sample `nsamples_per_label` points from the binary mask corresponding to `label`\n\n    Randomly sample `nsamples_per_label` point form the binary mask corresponding to `label`. Sampling with\n    replacement is used if the required `nsamples_per_label` is larger than the available `label_count`\n\n    :param image: Input 2D raster label"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a AnalysisResults object from a json dictionary.\"\"\"\n        args = {}\n        if 'language' in _dict:\n            args['language'] = _dict.get('language')\n        if 'analyzed_text' in _dict:\n            args['analyzed_text'] = _dict.get('analyzed_text')\n        if 'retrieved_url' in _dict:\n            args['retrieved_url'] = _dict.get('retrieved_url')\n        if 'usage' in _dict:\n            args['usage'] = AnalysisResultsUsage._from_dict(_dict.get('usage'))\n        if 'concepts' in _dict:\n            args['concepts'] = [\n                ConceptsResult._from_dict(x) for x in (_dict.get('concepts'))\n            ]\n        if 'entities' in _dict:\n            args['entities'] = [\n                EntitiesResult._from_dict(x) for x in (_dict.get('entities'))\n            ]\n        if 'keywords' in _dict:\n            args['keywords'] = [\n                KeywordsResult._from_dict(x) for x in (_dict.get('keywords'))\n            ]\n        if 'categories' in _dict:\n            args['categories'] = [\n                CategoriesResult._from_dict(x)\n                for x in (_dict.get('categories'))\n            ]\n        if 'emotion' in _dict:\n            args['emotion'] = EmotionResult._from_dict(_dict.get('emotion'))\n        if 'metadata' in _dict:\n            args['metadata'] = AnalysisResultsMetadata._from_dict(\n                _dict.get('metadata'))\n        if 'relations' in _dict:\n            args['relations'] = [\n                RelationsResult._from_dict(x) for x in (_dict.get('relations'))\n            ]\n        if 'semantic_roles' in _dict:\n            args['semantic_roles'] = [\n                SemanticRolesResult._from_dict(x)\n                for x in (_dict.get('semantic_roles'))\n            ]\n        if 'sentiment' in _dict:\n            args['sentiment'] = SentimentResult._from_dict(\n                _dict.get('sentiment'))\n        if 'syntax' in _dict:\n            args['syntax'] = SyntaxResult._from_dict(_dict.get('syntax'))\n        return cls(**args)",
        "sampled": "```python\nclass AnalysisResults:\n    def __init__(self, id, name, data):\n        \"\"\"\n        Initialize an AnalysisResults object.\n\n        Args:\n            id (int): The ID of the analysis results.\n            name (str): The name of the analysis results.\n            data (dict): The data of the analysis results.\n        \"\"\"\n        self.id = id\n        self.name = name\n        self.data = data\n\n    @classmethod\n    def _from_dict(cls, _"
    },
    {
        "original": "def edit(\n        self,\n        text: str,\n        parse_mode: str = \"\",\n        disable_web_page_preview: bool = None,\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *edit* of :obj:`Message <pyrogram.Message>`\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.edit_message_text(\n                chat_id=message.chat.id,\n                message_id=message.message_id,\n                text=\"hello\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.edit(\"hello\")\n\n        Args:\n            text (``str``):\n                New text of the message.\n\n            parse_mode (``str``, *optional*):\n                Use :obj:`MARKDOWN <pyrogram.ParseMode.MARKDOWN>` or :obj:`HTML <pyrogram.ParseMode.HTML>`\n                if you want Telegram apps to show bold, italic, fixed-width text or inline URLs in your message.\n                Defaults to Markdown.\n\n            disable_web_page_preview (``bool``, *optional*):\n                Disables link previews for links in this message.\n\n            reply_markup (:obj:`InlineKeyboardMarkup`, *optional*):\n                An InlineKeyboardMarkup object.\n\n        Returns:\n            On success, the edited :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        return self._client.edit_message_text(\n            chat_id=self.chat.id,\n            message_id=self.message_id,\n            text=text,\n            parse_mode=parse_mode,\n            disable_web_page_preview=disable_web_page_preview,\n            reply_markup=reply_markup\n        )",
        "sampled": "```python\nclass Message:\n    def __init__(self, chat_id, message_id):\n        self.chat_id = chat_id\n        self.message_id = message_id\n\n    def edit(self, text: str = \"\", parse_mode: str = \"\", disable_web_page_preview: bool = None, reply_markup=None):\n        client.edit_message_text(\n            chat_id=self.chat_id,\n            message_id=self.message_id,\n            text=text,\n            parse_mode=parse_mode,\n            disable_web_page_preview"
    },
    {
        "original": "def _resize_with_dtype(arr, dtype):\n    \"\"\"\n    This function will transform arr into an array with the same type as dtype. It will do this by\n    filling new columns with zeros (or NaNs, if it is a float column). Also, columns that are not\n    in the new dtype will be dropped.\n    \"\"\"\n    structured_arrays = dtype.names is not None and arr.dtype.names is not None\n    old_columns = arr.dtype.names or []\n    new_columns = dtype.names or []\n\n    # In numpy 1.9 the ndarray.astype method used to handle changes in number of fields. The code below\n    # should replicate the same behaviour the old astype used to have.\n    #\n    # One may be tempted to use np.lib.recfunctions.stack_arrays to implement both this step and the\n    # concatenate that follows but it 2x slower and it requires providing your own default values (instead\n    # of np.zeros).\n    #\n    # Numpy 1.14 supports doing new_arr[old_columns] = arr[old_columns], which is faster than the code below\n    # (in benchmarks it seems to be even slightly faster than using the old astype). However, that is not\n    # supported by numpy 1.9.2.\n    if structured_arrays and (old_columns != new_columns):\n        old_columns = set(old_columns)\n        new_columns = set(new_columns)\n\n        new_arr = np.zeros(arr.shape, dtype)\n        for c in old_columns & new_columns:\n            new_arr[c] = arr[c]\n\n        # missing float columns should default to nan rather than zero\n        _is_float_type = lambda _dtype: _dtype.type in (np.float32, np.float64)\n        _is_void_float_type = lambda _dtype: _dtype.type == np.void and _is_float_type(_dtype.subdtype[0])\n        _is_float_or_void_float_type = lambda _dtype: _is_float_type(_dtype) or _is_void_float_type(_dtype)\n        _is_float = lambda column: _is_float_or_void_float_type(dtype.fields[column][0])\n        for new_column in filter(_is_float, new_columns - old_columns):\n            new_arr[new_column] = np.nan\n\n        return new_arr.astype(dtype)\n    else:\n        return arr.astype(dtype)",
        "sampled": "```python\nimport numpy as np\n\ndef _resize_with_dtype(arr, dtype):\n    \"\"\"\n    This function will transform arr into an array with the same type as dtype. It will do this by\n    filling new columns with zeros (or NaNs, if it is a float column). Also, columns that are not\n    in the new dtype will be dropped.\n    \"\"\"\n    \n    # Get the number of columns in the new dtype\n    num_cols = len(dtype)\n    \n    #"
    },
    {
        "original": "def count_special_chars(self, char, prev_char):\n        \"\"\"Update parenthesis counters\"\"\"\n        if char == '(':\n            self.parenthesis_count += 1\n        elif char == ')':\n            self.parenthesis_count -= 1\n            if self.parenthesis_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing parentheses'.format(self.line))\n        elif char == '{':\n            self.curly_bracket_count += 1\n        elif char == '}':\n            self.curly_bracket_count -= 1\n            if self.curly_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing curly brackets'.format(self.line))\n        elif char == '[':\n            self.square_bracket_count += 1\n        elif char == ']':\n            self.square_bracket_count -= 1\n            if self.square_bracket_count < 0:\n                raise RMarkdownOptionParsingError(\n                    'Option line \"{}\" has too many '\n                    'closing square brackets'.format(self.line))\n        elif char == \"'\" and prev_char != '\\\\':\n            self.in_single_quote = not self.in_single_quote\n        elif char == '\"' and prev_char != '\\\\':\n            self.in_double_quote = not self.in_double_quote",
        "sampled": "```python\nclass Solution:\n\n    def count_special_chars(self, char, prev_char):\n        \"\"\"Update parenthesis counters\"\"\"\n        pass\n\n    def calculate_ATTACH_string(self, s: str) -> str:\n        \"\"\"\"\"\n        \n        ans = \"\"\n        depth_stack = [0]\n        \n        for c in s:\n            if c == '(':\n                depth_stack.append(depth_stack[-1] + 1)\n            elif c == ')':\n                width = min(2*abs(depth_stack.pop"
    },
    {
        "original": "def get_pullrequest(self, project, repository, pull_request_id):\n        \"\"\"\n        Retrieve a pull request.\n        The authenticated user must have REPO_READ permission\n        for the repository that this pull request targets to call this resource.\n        :param project:\n        :param repository:\n        :param pull_request_id: the ID of the pull request within the repository\n        :return:\n        \"\"\"\n        url = 'rest/api/1.0/projects/{project}/repos/{repository}/pull-requests/{pullRequestId}'.format(project=project,\n                                                                                                        repository=repository,\n                                                                                                        pullRequestId=pull_request_id)\n        return self.get(url)",
        "sampled": "```python\nclass GitHubAPI:\n    def get_pullrequest(self, project, repository, pull_request_id):\n        \"\"\"\n        Retrieve a pull request.\n        The authenticated user must have REPO_READ permission\n        for the repository that this pull request targets to call this resource.\n        :param project:\n        :param repository:\n        :param pull_request_id: the ID of the pull request within the repository\n        :return:\n        \"\"\"\n        # Assuming we have a function to authenticate and get the"
    },
    {
        "original": "def _get_slave_status(self, db, above_560, nonblocking):\n        \"\"\"\n        Retrieve the slaves' statuses using:\n        1. The `performance_schema.threads` table. Non-blocking, requires version > 5.6.0\n        2. The `information_schema.processlist` table. Blocking\n        \"\"\"\n        try:\n            with closing(db.cursor()) as cursor:\n                if above_560 and nonblocking:\n                    # Query `performance_schema.threads` instead of `\n                    # information_schema.processlist` to avoid mutex impact on performance.\n                    cursor.execute(\"SELECT THREAD_ID, NAME FROM performance_schema.threads WHERE NAME LIKE '%worker'\")\n                else:\n                    cursor.execute(\"SELECT * FROM INFORMATION_SCHEMA.PROCESSLIST WHERE COMMAND LIKE '%Binlog dump%'\")\n                slave_results = cursor.fetchall()\n                slaves = 0\n                for _ in slave_results:\n                    slaves += 1\n\n                return {'Slaves_connected': slaves}\n\n        except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:\n            self.warning(\"Privileges error accessing the process tables (must grant PROCESS): %s\" % str(e))\n            return {}",
        "sampled": "```python\nimport mysql.connector\nfrom mysql.connector import Error\n\nclass Database:\n    def __init__(self, host, database, user, password):\n        self.host = host\n        self.database = database\n        self.user = user\n        self.password = password\n        self.connection = None\n\n    def connect(self):\n        try:\n            self.connection = mysql.connector.connect(\n                host=self.host,\n                database=self.database,\n                user=self.user,\n                password=self.password\n            )\n           "
    },
    {
        "original": "def _parse_user_flags():\n    \"\"\"\n    Parses user-flags file and loads it to register user defined options.\n    \"\"\"\n    try:\n        idx = list(sys.argv).index('--user-flags')\n        user_flags_file = sys.argv[idx + 1]\n    except (ValueError, IndexError):\n        user_flags_file = ''\n\n    if user_flags_file and os.path.isfile(user_flags_file):\n        from ryu.utils import _import_module_file\n        _import_module_file(user_flags_file)",
        "sampled": "```python\nimport re\n\ndef _parse_user_flags():\n    \"\"\"\n    Parses user-flags file and loads it to register user defined options.\n    \"\"\"\n    user_flags = {}\n    with open('user_flags.txt', 'r') as file:\n        for line in file:\n            line = line.strip()\n            if line and not line.startswith('#'):\n                match = re.match(r'(\\w+)\\s*=\\s*(.*)', line)\n                if match:\n                    key, value = match"
    },
    {
        "original": "def update_schemas(self, schemas):\n        \"\"\"Add multiple schemas to the set of known schemas (case-insensitive)\n\n        :param Iterable[str] schemas: An iterable of the schema names to add.\n        \"\"\"\n        self.schemas.update((_lower(d), _lower(s)) for (d, s) in schemas)",
        "sampled": "```python\nclass SchemaUpdater:\n    def __init__(self):\n        self.known_schemas = set()\n\n    def update_schemas(self, schemas):\n        \"\"\"Add multiple schemas to the set of known schemas (case-insensitive)\n\n        :param Iterable[str] schemas: An iterable of the schema names to add.\n        \"\"\"\n        for schema in schemas:\n            self.known_schemas.add(schema.lower())\n\n# Example usage\nupdater = SchemaUpdater()\nschemas = [\"person\", \"address"
    },
    {
        "original": "def least_loaded_node(self):\n        \"\"\"Choose the node with fewest outstanding requests, with fallbacks.\n\n        This method will prefer a node with an existing connection and no\n        in-flight-requests. If no such node is found, a node will be chosen\n        randomly from disconnected nodes that are not \"blacked out\" (i.e.,\n        are not subject to a reconnect backoff). If no node metadata has been\n        obtained, will return a bootstrap node (subject to exponential backoff).\n\n        Returns:\n            node_id or None if no suitable node was found\n        \"\"\"\n        nodes = [broker.nodeId for broker in self.cluster.brokers()]\n        random.shuffle(nodes)\n\n        inflight = float('inf')\n        found = None\n        for node_id in nodes:\n            conn = self._conns.get(node_id)\n            connected = conn is not None and conn.connected()\n            blacked_out = conn is not None and conn.blacked_out()\n            curr_inflight = len(conn.in_flight_requests) if conn is not None else 0\n            if connected and curr_inflight == 0:\n                # if we find an established connection\n                # with no in-flight requests, we can stop right away\n                return node_id\n            elif not blacked_out and curr_inflight < inflight:\n                # otherwise if this is the best we have found so far, record that\n                inflight = curr_inflight\n                found = node_id\n\n        if found is not None:\n            return found\n\n        return None",
        "sampled": "```python\nimport random\n\nclass Node:\n    def __init__(self, node_id, in_flight_requests, connected, blacked_out):\n        self.node_id = node_id\n        self.in_flight_requests = in_flight_requests\n        self.connected = connected\n        self.blacked_out = blacked_out\n\nclass NodeMetadata:\n    def __init__(self):\n        self.nodes = {}\n\n    def add_node(self, node_id, in_flight_requests, connected, blacked_out):\n        self"
    },
    {
        "original": "def _retry_on_connection_error(func: Callable) -> Callable:\n    \"\"\"Decorator to retry the function max_connection_attemps number of times.\n\n    Herewith-decorated functions need an ``_attempt`` keyword argument.\n\n    This is to decorate functions that do network requests that may fail. Note that\n    :meth:`.get_json`, :meth:`.get_iphone_json`, :meth:`.graphql_query` and :meth:`.graphql_node_list` already have\n    their own logic for retrying, hence functions that only use these for network access must not be decorated with this\n    decorator.\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        try:\n            return func(instaloader, *args, **kwargs)\n        except (urllib3.exceptions.HTTPError, requests.exceptions.RequestException, ConnectionException) as err:\n            error_string = \"{}({}): {}\".format(func.__name__, ', '.join([repr(arg) for arg in args]), err)\n            if (kwargs.get('_attempt') or 1) == instaloader.context.max_connection_attempts:\n                raise ConnectionException(error_string) from None\n            instaloader.context.error(error_string + \" [retrying; skip with ^C]\", repeat_at_end=False)\n            try:\n                if kwargs.get('_attempt'):\n                    kwargs['_attempt'] += 1\n                else:\n                    kwargs['_attempt'] = 2\n                instaloader.context.do_sleep()\n                return call(instaloader, *args, **kwargs)\n            except KeyboardInterrupt:\n                instaloader.context.error(\"[skipped by user]\", repeat_at_end=False)\n                raise ConnectionException(error_string) from None\n    return call",
        "sampled": "```python\nfrom typing import Callable\nimport functools\n\nmax_connection_attempts = 3\n\ndef _retry_on_connection_error(func: Callable) -> Callable:\n    \"\"\"Decorator to retry the function max_connection_attemps number of times.\n\n    Herewith-decorated functions need an ``_attempt`` keyword argument.\n\n    This is to decorate functions that do network requests that may fail. Note that\n    :meth:`.get_json`, :meth:`.get_iphone_json`, :meth:`."
    },
    {
        "original": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Concatenates values on all nodes with requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        dimension = self.get_dimension(dimension, strict=True).name\n        all_dims = self.traverse(lambda x: [d.name for d in x.dimensions()])\n        if dimension in chain.from_iterable(all_dims):\n            values = [el.dimension_values(dimension) for el in self\n                      if dimension in el.dimensions(label=True)]\n            vals = np.concatenate(values)\n            return vals if expanded else util.unique_array(vals)\n        else:\n            return super(ViewableTree, self).dimension_values(\n                dimension, expanded, flat)",
        "sampled": "```python\nimport numpy as np\n\nclass DataConverter:\n    def __init__(self, data):\n        self.data = data\n\n    def dimension_values(self, dimension, expanded=True, flat=True):\n        # Assuming data is a dictionary with keys as dimensions and values as numpy arrays\n        if dimension in self.data:\n            values = self.data[dimension]\n            if not expanded and isinstance(values[0], np.ndarray):  # For Geometry\n                return [v for sublist in values for"
    },
    {
        "original": "def get_stat_name(self, header=\"\"):\n        \"\"\"\"Return the stat name with an optional header\"\"\"\n        ret = self.plugin_name\n        if header != \"\":\n            ret += '_' + header\n        return ret",
        "sampled": "```python\nclass StatCalculator:\n    def get_stat_name(self, header=\"\"):\n        \"\"\"\n        Return the stat name with an optional header\n        \"\"\"\n        # Assuming stat_name is a class attribute\n        stat_name = self.stat_name\n        if header:\n            return f\"{header} {stat_name}\"\n        else:\n            return stat_name\n\n    def calculate_stat(self, data):\n        \"\"\"\n        Calculate the stat value from the given data\n        \"\"\"\n        # Assuming data is a list"
    },
    {
        "original": "def compute_density(start, end, length, time_unit='us'):\n    \"\"\"\n    Computes a grid density given the edges and number of samples.\n    Handles datetime grids correctly by computing timedeltas and\n    computing a density for the given time_unit.\n    \"\"\"\n    if isinstance(start, int): start = float(start)\n    if isinstance(end, int): end = float(end)\n    diff = end-start\n    if isinstance(diff, timedelta_types):\n        if isinstance(diff, np.timedelta64):\n            diff = np.timedelta64(diff, time_unit).tolist()\n        tscale = 1./np.timedelta64(1, time_unit).tolist().total_seconds()\n        return (length/(diff.total_seconds()*tscale))\n    else:\n        return length/diff",
        "sampled": "```python\nfrom datetime import datetime, timedelta\n\ndef compute_density(start, end, length, time_unit='us'):\n    \"\"\"\n    Computes a grid density given the edges and number of samples.\n    Handles datetime grids correctly by computing timedeltas and\n    computing a density for the given time_unit.\n    \n    Parameters:\n    start (datetime): Start date of the range\n    end (datetime): End date of the range\n    length (int): Number of samples in the grid\n    time"
    },
    {
        "original": "def CreateAndStartHunt(flow_name, flow_args, creator, **kwargs):\n  \"\"\"Creates and starts a new hunt.\"\"\"\n\n  # This interface takes a time when the hunt expires. However, the legacy hunt\n  # starting interface took an rdfvalue.Duration object which was then added to\n  # the current time to get the expiry. This check exists to make sure we don't\n  # confuse the two.\n  if \"duration\" in kwargs:\n    precondition.AssertType(kwargs[\"duration\"], rdfvalue.Duration)\n\n  hunt_args = rdf_hunt_objects.HuntArguments(\n      hunt_type=rdf_hunt_objects.HuntArguments.HuntType.STANDARD,\n      standard=rdf_hunt_objects.HuntArgumentsStandard(\n          flow_name=flow_name, flow_args=flow_args))\n\n  hunt_obj = rdf_hunt_objects.Hunt(\n      creator=creator,\n      args=hunt_args,\n      create_time=rdfvalue.RDFDatetime.Now(),\n      **kwargs)\n\n  CreateHunt(hunt_obj)\n  StartHunt(hunt_obj.hunt_id)\n\n  return hunt_obj.hunt_id",
        "sampled": "```python\ndef CreateAndStartHunt(flow_name, flow_args, creator, **kwargs):\n    \"\"\"\n    Creates and starts a new hunt.\n    \n    Args:\n        flow_name (str): The name of the flow.\n        flow_args (dict): The arguments for the flow.\n        creator (str): The creator of the hunt.\n        **kwargs: Additional keyword arguments.\n    \n    Returns:\n        None\n    \"\"\"\n    \n    # Check if flow_name is a string\n    if not"
    },
    {
        "original": "def get_lattice_quanta(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n        \"\"\"\n        Returns the dipole / polarization quanta along a, b, and c for\n        all structures.\n        \"\"\"\n        lattices = [s.lattice for s in self.structures]\n        volumes = np.array([s.lattice.volume for s in self.structures])\n\n        L = len(self.structures)\n\n        e_to_muC = -1.6021766e-13\n        cm2_to_A2 = 1e16\n        units = 1.0 / np.array(volumes)\n        units *= e_to_muC * cm2_to_A2\n\n        # convert polarizations and lattice lengths prior to adjustment\n        if convert_to_muC_per_cm2 and not all_in_polar:\n            # adjust lattices\n            for i in range(L):\n                lattice = lattices[i]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[i], a)\n        elif convert_to_muC_per_cm2 and all_in_polar:\n            for i in range(L):\n                lattice = lattices[-1]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[-1], a)\n\n        quanta = np.array(\n            [np.array(l.lengths_and_angles[0]) for l in lattices])\n\n        return quanta",
        "sampled": "```python\nimport numpy as np\n\nclass LatticeQuantumCalculator:\n    def __init__(self, a, b, c):\n        \"\"\"\n        Initialize the lattice quantum calculator with the lattice parameters a, b, and c.\n        \n        Parameters:\n        a (float): The length of the a-axis in cm.\n        b (float): The length of the b-axis in cm.\n        c (float): The length of the c-axis in cm.\n        \"\"\"\n        self.a = a"
    },
    {
        "original": "def delete_account(self, account):\n        \"\"\"\n        \u5220\u9664\u5ba2\u670d\u8d26\u53f7\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/1/70a29afed17f56d537c833f89be979c9.html\n\n        :param account: \u5b8c\u6574\u5ba2\u670d\u8d26\u53f7\uff0c\u683c\u5f0f\u4e3a\uff1a\u8d26\u53f7\u524d\u7f00@\u516c\u4f17\u53f7\u5fae\u4fe1\u53f7\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        params_data = [\n            'access_token={0}'.format(quote(self.access_token)),\n            'kf_account={0}'.format(quote(to_binary(account), safe=b'/@')),\n        ]\n        params = '&'.join(params_data)\n        return self._get(\n            'https://api.weixin.qq.com/customservice/kfaccount/del',\n            params=params\n        )",
        "sampled": "```python\nclass WeixinMp:\n    def delete_account(self, account):\n        \"\"\"\n        \u5220\u9664\u5ba2\u670d\u8d26\u53f7\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/1/70a29afed17f56d537c833f89be979c9.html\n\n        :param account: \u5b8c\u6574\u5ba2\u670d\u8d26\u53f7\uff0c\u683c\u5f0f\u4e3a\uff1a\u8d26\u53f7\u524d\u7f00@\u516c\u4f17\u53f7\u5fae\u4fe1\u53f7\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n       "
    },
    {
        "original": "def find_sources(self, var_def, simplified_graph=True):\n        \"\"\"\n        Find all sources to the specified variable definition.\n\n        :param ProgramVariable var_def: The variable definition.\n        :param bool simplified_graph: True if we want to search in the simplified graph, False otherwise.\n        :return: A collection of all sources to the specified variable definition.\n        :rtype: list\n        \"\"\"\n\n        if simplified_graph:\n            graph = self.simplified_data_graph\n        else:\n            graph = self.data_graph\n\n        if var_def not in graph:\n            return []\n\n        sources = []\n        defs = [ var_def ]\n        traversed = set()\n\n        while defs:\n            definition = defs.pop()\n            in_edges = graph.in_edges(definition, data=True)\n            for src, _, data in in_edges:\n                if 'type' in data and data['type'] == 'kill':\n                    continue\n                if isinstance(src.variable, SimTemporaryVariable):\n                    if src not in traversed:\n                        defs.append(src)\n                        traversed.add(src)\n                else:\n                    if src not in sources:\n                        sources.append(src)\n\n        return sources",
        "sampled": "```python\nfrom typing import List\n\nclass ProgramVariable:\n    def __init__(self, name: str):\n        self.name = name\n        self.sources = []\n\nclass Graph:\n    def __init__(self):\n        self.nodes = {}\n\n    def add_node(self, node_name: str):\n        if node_name not in self.nodes:\n            self.nodes[node_name] = []\n\n    def add_edge(self, from_node: str, to_node: str):\n        if from_node in self.nodes"
    },
    {
        "original": "def resolve(self, key):\n        \"\"\"Looks up a variable like `__getitem__` or `get` but returns an\n        :class:`Undefined` object with the name of the name looked up.\n        \"\"\"\n        if key in self.vars:\n            return self.vars[key]\n        if key in self.parent:\n            return self.parent[key]\n        return self.environment.undefined(name=key)",
        "sampled": "```python\nclass Undefined:\n    pass\n\n\nclass Resolver:\n    def __init__(self):\n        pass\n\n    def resolve(self, key):\n        \"\"\"\n        Looks up a variable like `__getitem__` or `get` but returns an\n        :class:`Undefined` object with the name of the name looked up.\n        \n        Args:\n            key (str): The key to look up.\n        \n        Returns:\n            Undefined: An undefined object with the name of the variable looked up.\n"
    },
    {
        "original": "def inspect(self, nids=None, wslice=None, **kwargs):\n        \"\"\"\n        Inspect the tasks (SCF iterations, Structural relaxation ...) and\n        produces matplotlib plots.\n\n        Args:\n            nids: List of node identifiers.\n            wslice: Slice object used to select works.\n            kwargs: keyword arguments passed to `task.inspect` method.\n\n        .. note::\n\n            nids and wslice are mutually exclusive.\n            If nids and wslice are both None, all tasks in self are inspected.\n\n        Returns:\n            List of `matplotlib` figures.\n        \"\"\"\n        figs = []\n        for task in self.select_tasks(nids=nids, wslice=wslice):\n            if hasattr(task, \"inspect\"):\n                fig = task.inspect(**kwargs)\n                if fig is None:\n                    cprint(\"Cannot inspect Task %s\" % task, color=\"blue\")\n                else:\n                    figs.append(fig)\n            else:\n                cprint(\"Task %s does not provide an inspect method\" % task, color=\"blue\")\n\n        return figs",
        "sampled": "```python\nclass TaskInspector:\n    def __init__(self):\n        self.tasks = []\n\n    def add_task(self, task):\n        self.tasks.append(task)\n\n    def inspect(self, nids=None, wslice=None, **kwargs):\n        \"\"\"\n        Inspect the tasks (SCF iterations, Structural relaxation ...) and\n        produces matplotlib plots.\n\n        Args:\n            nids: List of node identifiers.\n            wslice: Slice object used to select works.\n            kwargs: keyword arguments passed"
    },
    {
        "original": "def _compute_rtfilter_map(self):\n        \"\"\"Returns neighbor's RT filter (permit/allow filter based on RT).\n\n        Walks RT filter tree and computes current RT filters for each peer that\n        have advertised RT NLRIs.\n        Returns:\n            dict of peer, and `set` of rts that a particular neighbor is\n            interested in.\n        \"\"\"\n        rtfilter_map = {}\n\n        def get_neigh_filter(neigh):\n            neigh_filter = rtfilter_map.get(neigh)\n            # Lazy creation of neighbor RT filter\n            if neigh_filter is None:\n                neigh_filter = set()\n                rtfilter_map[neigh] = neigh_filter\n            return neigh_filter\n\n        # Check if we have to use all paths or just best path\n        if self._common_config.max_path_ext_rtfilter_all:\n            # We have to look at all paths for a RtDest\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                known_path_list = rtcdest.known_path_list\n                for path in known_path_list:\n                    neigh = path.source\n\n                    # We ignore NC\n                    if neigh is None:\n                        continue\n\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n        else:\n            # We iterate over all destination of the RTC table and for iBGP\n            # peers we use all known paths' RTs for RT filter and for eBGP\n            # peers we only consider best-paths' RTs for RT filter\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                path = rtcdest.best_path\n                # If this destination does not have any path, we continue\n                if not path:\n                    continue\n\n                neigh = path.source\n                # Consider only eBGP peers and ignore NC\n                if neigh and neigh.is_ebgp_peer():\n                    # For eBGP peers we use only best-path to learn RT filter\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n                else:\n                    # For iBGP peers we use all known paths to learn RT filter\n                    known_path_list = rtcdest.known_path_list\n                    for path in known_path_list:\n                        neigh = path.source\n                        # We ignore NC, and eBGP peers\n                        if neigh and not neigh.is_ebgp_peer():\n                            neigh_filter = get_neigh_filter(neigh)\n                            neigh_filter.add(path.nlri.route_target)\n\n        return rtfilter_map",
        "sampled": "```python\nclass Program:\n    def __init__(self):\n        self.adj_list = {}  # graph represented as adjacency list\n        self.rts = {}  # input data for peer's advertised RT NLRIs\n\n    def _compute.rtfilter_map(self):\n        \"\"\"Returns neighbor's RT filter (permit/allow filter based on RT).\n\n        Walks RT filter tree and computes current RT filters for each peer that\n        have advertised RT NLRIs.\n        Returns:\n            dict"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "sampled": "```python\nfrom bigchaindb.common.transaction import Transaction\n\ndef create(cls, tx_signers, recipients, metadata=None, asset=None):\n    \"\"\"\n    A simple way to generate a `CREATE` transaction.\n\n    Args:\n        tx_signers (list of str): A list of keys that represent the signers of the CREATE Transaction.\n        recipients (list of tuple): A list of ([keys],amount) that represent the recipients of this Transaction.\n        metadata (dict): The metadata to be"
    },
    {
        "original": "def custom_build_class_rule(self, opname, i, token, tokens, customize):\n        \"\"\"\n        # Should the first rule be somehow folded into the 2nd one?\n        build_class ::= LOAD_BUILD_CLASS mkfunc\n                        LOAD_CLASSNAME {expr}^n-1 CALL_FUNCTION_n\n                        LOAD_CONST CALL_FUNCTION_n\n        build_class ::= LOAD_BUILD_CLASS mkfunc\n                        expr\n                        call\n                        CALL_FUNCTION_3\n         \"\"\"\n        # FIXME: I bet this can be simplified\n        # look for next MAKE_FUNCTION\n        for i in range(i+1, len(tokens)):\n            if tokens[i].kind.startswith('MAKE_FUNCTION'):\n                break\n            elif tokens[i].kind.startswith('MAKE_CLOSURE'):\n                break\n            pass\n        assert i < len(tokens), \"build_class needs to find MAKE_FUNCTION or MAKE_CLOSURE\"\n        assert tokens[i+1].kind == 'LOAD_CONST', \\\n          \"build_class expecting CONST after MAKE_FUNCTION/MAKE_CLOSURE\"\n        call_fn_tok = None\n        for i in range(i, len(tokens)):\n            if tokens[i].kind.startswith('CALL_FUNCTION'):\n                call_fn_tok = tokens[i]\n                break\n        if not call_fn_tok:\n            raise RuntimeError(\"build_class custom rule for %s needs to find CALL_FUNCTION\"\n                               % opname)\n\n\n        # customize build_class rule\n        # FIXME: What's the deal with the two rules? Different Python versions?\n        # Different situations? Note that the above rule is based on the CALL_FUNCTION\n        # token found, while this one doesn't.\n        if self.version < 3.6:\n            call_function = self.call_fn_name(call_fn_tok)\n            args_pos, args_kw = self.get_pos_kw(call_fn_tok)\n            rule = (\"build_class ::= LOAD_BUILD_CLASS mkfunc %s\"\n                        \"%s\" % (('expr ' * (args_pos - 1) + ('kwarg ' * args_kw)),\n                                    call_function))\n        else:\n            # 3.6+ handling\n            call_function = call_fn_tok.kind\n            if call_function.startswith(\"CALL_FUNCTION_KW\"):\n                self.addRule(\"classdef ::= build_class_kw store\", nop_func)\n                rule = (\"build_class_kw ::= LOAD_BUILD_CLASS mkfunc %sLOAD_CONST %s\"\n                        % ('expr ' * (call_fn_tok.attr - 1), call_function))\n            else:\n                call_function = self.call_fn_name(call_fn_tok)\n                rule = (\"build_class ::= LOAD_BUILD_CLASS mkfunc %s%s\"\n                         % ('expr ' * (call_fn_tok.attr - 1), call_function))\n        self.addRule(rule, nop_func)\n        return",
        "sampled": "```python\nimport re\n\ndef custom_build_class_rule(self, opname, i, token, tokens, customize):\n    # Define regular expression for pattern matching\n    regex1 = r\"LOAD_BUILD_CLASS mkfunc LOAD_CLASSNAME {0}+ CALL_FUNCTION_[0-9]+ LOAD_CONST CALL_FUNCTION_\"\n    regex2 = r\"LOAD_BUILD_CLASS mkfunc {0}+ call CALL_FUNCTION_3\"\n\n    # Use boolean operations to match either of the patterns and get the first"
    },
    {
        "original": "def create(self, name):\n        \"\"\"\n        \u521b\u5efa\u6807\u7b7e\n\n        :param name: \u6807\u7b7e\u540d\uff0830\u4e2a\u5b57\u7b26\u4ee5\u5185\uff09\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n        \"\"\"\n        name = to_text(name)\n        return self._post(\n            'tags/create',\n            data={'tag': {'name': name}},\n            result_processor=lambda x: x['tag']\n        )",
        "sampled": "```python\nclass LabelManager:\n    def __init__(self):\n        self.labels = {}\n\n    def create(self, name):\n        \"\"\"\n        \u521b\u5efa\u6807\u7b7e\n\n        :param name: \u6807\u7b7e\u540d\uff0830\u4e2a\u5b57\u7b26\u4ee5\u5185\uff09\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n        \"\"\"\n        if len(name) > 30:\n            return {\"error\": \"\u6807\u7b7e\u540d\u8fc7\u957f\"}\n        if name in self.labels:\n            return {\"error\": \"\u6807\u7b7e\u540d\u5df2\u5b58\u5728\"}\n"
    },
    {
        "original": "async def packets_from_tshark(self, packet_callback, packet_count=None, close_tshark=True):\n        \"\"\"\n        A coroutine which creates a tshark process, runs the given callback on each packet that is received from it and\n        closes the process when it is done.\n\n        Do not use interactively. Can be used in order to insert packets into your own eventloop.\n        \"\"\"\n        tshark_process = await self._get_tshark_process(packet_count=packet_count)\n        try:\n            await self._go_through_packets_from_fd(tshark_process.stdout, packet_callback, packet_count=packet_count)\n        except StopCapture:\n            pass\n        finally:\n            if close_tshark:\n                await self._close_async()",
        "sampled": "```python\nimport subprocess\n\nasync def packets_from_tshark(self, packet_callback, packet_count=None, close_tshark=True):\n    \"\"\"\n    A coroutine which creates a tshark process, runs the given callback on each packet that is received from it and\n    closes the process when it is done.\n\n    Do not use interactively. Can be used in order to insert packets into your own eventloop.\n    \"\"\"\n    \n    # Create a new tshark process\n    p"
    },
    {
        "original": "def publish_alias(self, func_data, alias):\n        \"\"\"Create or update an alias for the given function.\n        \"\"\"\n        if not alias:\n            return func_data['FunctionArn']\n        func_name = func_data['FunctionName']\n        func_version = func_data['Version']\n\n        exists = resource_exists(\n            self.client.get_alias, FunctionName=func_name, Name=alias)\n\n        if not exists:\n            log.debug(\"Publishing custodian lambda alias %s\", alias)\n            alias_result = self.client.create_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        else:\n            if (exists['FunctionVersion'] == func_version and\n                    exists['Name'] == alias):\n                return exists['AliasArn']\n            log.debug('Updating custodian lambda alias %s', alias)\n            alias_result = self.client.update_alias(\n                FunctionName=func_name,\n                Name=alias,\n                FunctionVersion=func_version)\n        return alias_result['AliasArn']",
        "sampled": "```python\nclass FunctionAliasManager:\n    def __init__(self):\n        self.aliases = {}\n\n    def publish_alias(self, func_data, alias):\n        \"\"\"\n        Create or update an alias for the given function.\n\n        Args:\n            func_data (function): The function to be aliased.\n            alias (str): The name of the alias.\n\n        Returns:\n            None\n        \"\"\"\n        \n        # Check if the function already exists in the aliases dictionary\n        if func_data in self"
    },
    {
        "original": "def _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    count = 0\n    log.debug('Waiting for asynchronous operation to complete')\n    result = conn.get_operation_status(request_id)\n    while result.status == 'InProgress':\n        count = count + 1\n        if count > 120:\n            raise ValueError('Timed out waiting for asynchronous operation to complete.')\n        time.sleep(5)\n        result = conn.get_operation_status(request_id)\n\n    if result.status != 'Succeeded':\n        raise AzureException('Operation failed. {message} ({code})'\n                                .format(message=result.error.message,\n                                        code=result.error.code))",
        "sampled": "```python\nimport time\n\ndef _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    \n    # Wait for 10 seconds to allow the async operation to complete\n    time.sleep(10)\n    \n    # Check if the request has completed successfully\n    while True:\n        # Query the connection to check if the request has completed\n        status = conn.get_status(request_id)\n        \n        # If the request is complete, return True\n        if"
    },
    {
        "original": "def _FlushAllRows(self, db_connection, table_name):\n    \"\"\"Copies rows from the given db into the output file then deletes them.\"\"\"\n    for sql in db_connection.iterdump():\n      if (sql.startswith(\"CREATE TABLE\") or\n          sql.startswith(\"BEGIN TRANSACTION\") or sql.startswith(\"COMMIT\")):\n        # These statements only need to be written once.\n        continue\n      # The archive generator expects strings (not Unicode objects returned by\n      # the pysqlite library).\n      yield self.archive_generator.WriteFileChunk((sql + \"\\n\").encode(\"utf-8\"))\n    with db_connection:\n      db_connection.cursor().execute(\"DELETE FROM \\\"%s\\\";\" % table_name)",
        "sampled": "```python\nimport sqlite3\n\ndef _FlushAllRows(self, db_connection, table_name):\n    \"\"\"\n    Copies rows from the given db into the output file then deletes them.\n    \n    Args:\n        db_connection (sqlite3.Connection): The connection to the SQLite database.\n        table_name (str): The name of the table to flush.\n    \n    Returns:\n        None\n    \"\"\"\n    \n    # Create a cursor object to execute SQL queries\n    cursor = db_connection.cursor()\n    \n   "
    },
    {
        "original": "def _create_empty_run(\n        self, status=RunStatus.FINISHED, status_description=None\n    ) -> Run:\n        \"\"\"setting boilerplate when creating a Run object\"\"\"\n        run = Run(\n            job_id=self.summary[\"job_id\"],\n            issue_instances=[],\n            date=datetime.datetime.now(),\n            status=status,\n            status_description=status_description,\n            repository=self.summary[\"repository\"],\n            branch=self.summary[\"branch\"],\n            commit_hash=self.summary[\"commit_hash\"],\n            kind=self.summary[\"run_kind\"],\n        )\n        return run",
        "sampled": "```python\nclass Run:\n    class Status:\n        FINISHED = \"finished\"\n\n    def __init__(self, status=Status.FINISHED, status_description=None):\n        self.status = status\n        self.status_description = status_description\n\ndef _create_empty_run(status=Run.Status.FINISHED, status_description=None) -> Run:\n    return Run(status=status, status_description=status_description)\n```"
    },
    {
        "original": "def cell_complete(self, cell, cell_index=None, **kwargs):\n        \"\"\"\n        Finalize metadata for a cell and save notebook.\n\n        Optionally called by engines during execution to finalize the\n        metadata for a cell and save the notebook to the output path.\n        \"\"\"\n        end_time = self.now()\n\n        if self.log_output:\n            ceel_num = cell_index + 1 if cell_index is not None else ''\n            logger.info('Ending Cell {:-<43}'.format(ceel_num))\n            # Ensure our last cell messages are not buffered by python\n            sys.stdout.flush()\n            sys.stderr.flush()\n\n        cell.metadata.papermill['end_time'] = end_time.isoformat()\n        if cell.metadata.papermill.get('start_time'):\n            start_time = dateutil.parser.parse(cell.metadata.papermill['start_time'])\n            cell.metadata.papermill['duration'] = (end_time - start_time).total_seconds()\n        if cell.metadata.papermill['status'] != self.FAILED:\n            cell.metadata.papermill['status'] = self.COMPLETED\n\n        self.save()\n        if self.pbar:\n            self.pbar.update(1)",
        "sampled": "```python\ndef cell_complete(self, cell, cell_index=None, **kwargs):\n    \"\"\"\n    Finalize metadata for a cell and save notebook.\n\n    Optionally called by engines during execution to finalize the\n    metadata for a cell and save the notebook to the output path.\n    \n    Parameters:\n        self (object): The object that this method is bound to.\n        cell (dict): A dictionary containing information about the current cell.\n        kwargs: Additional keyword arguments.\n\n    Returns:\n        None\n"
    },
    {
        "original": "def from_ops(*operations: ops.OP_TREE,\n                 strategy: InsertStrategy = InsertStrategy.EARLIEST,\n                 device: devices.Device = devices.UnconstrainedDevice\n                 ) -> 'Circuit':\n        \"\"\"Creates an empty circuit and appends the given operations.\n\n        Args:\n            operations: The operations to append to the new circuit.\n            strategy: How to append the operations.\n            device: Hardware that the circuit should be able to run on.\n\n        Returns:\n            The constructed circuit containing the operations.\n        \"\"\"\n        result = Circuit(device=device)\n        result.append(operations, strategy)\n        return result",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\nfrom enum import Enum, unique\n\n@unique\nclass InsertStrategy(Enum):\n    EARLIEST = 1\n    LATEST = 2\n\nclass OP_TREE(ABC):\n    @abstractmethod\n    def append(self, operation) -> 'OP_TREE':\n        pass\n\nclass Circuit(OP_TREE):\n    def __init__(self, device=None) -> None:\n        self.device = device\n        self.operations = []\n    \n    def append(self"
    },
    {
        "original": "def get_extended_surface_mesh(self, repeat=(5, 5, 1)):\n        \"\"\"\n        Gets an extended surface mesh for to use for adsorption\n        site finding by constructing supercell of surface sites\n\n        Args:\n            repeat (3-tuple): repeat for getting extended surface mesh\n        \"\"\"\n        surf_str = Structure.from_sites(self.surface_sites)\n        surf_str.make_supercell(repeat)\n        return surf_str",
        "sampled": "```python\nimport numpy as np\n\nclass PhysicalSystem:\n    def __init__(self, lattice_constant=1):\n        \"\"\"\n        Initialize the physical system\n\n        Args:\n            lattice_constant (float): Lattice constant for the system\n        \"\"\"\n        self.lattice_constant = lattice_constant\n\n    def get_surface_mesh(self, size=(5, 5)):\n        \"\"\"\n        Gets a surface mesh for to use for adsorption site finding \n\n        Args:\n            size (2-tuple): surface"
    },
    {
        "original": "def find_1den_files(self):\n        \"\"\"\n        Abinit adds the idir-ipert index at the end of the 1DEN file and this breaks the extension\n        e.g. out_DEN1. This method scans the files in the directories and returns a list of namedtuple\n        Each named tuple gives the `path` of the 1DEN file and the `pertcase` index.\n        \"\"\"\n        regex = re.compile(r\"out_DEN(\\d+)(\\.nc)?$\")\n        den_paths = [f for f in self.list_filepaths() if regex.match(os.path.basename(f))]\n        if not den_paths: return None\n\n        # Build list of (pertcase, path) tuples.\n        pertfile_list = []\n        for path in den_paths:\n            name = os.path.basename(path)\n            match = regex.match(name)\n            pertcase, ncext = match.groups()\n            pertfile_list.append((int(pertcase), path))\n\n        # DSU sort.\n        pertfile_list = sorted(pertfile_list, key=lambda t: t[0])\n        return [dict2namedtuple(pertcase=item[0], path=item[1]) for item in pertfile_list]",
        "sampled": "```python\nimport os\nfrom collections import namedtuple\n\nclass FileScanner:\n    def find_1den_files(self):\n        \"\"\"\n        Abinit adds the idir-ipert index at the end of the 1DEN file and this breaks the extension\n        e.g. out_DEN1. This method scans the files in the directories and returns a list of namedtuple\n        Each named tuple gives the `path` of the 1DEN file and the `pertcase` index.\n        \n        Returns"
    },
    {
        "original": "def set_workdir(self, workdir, chroot=False):\n        \"\"\"Set the working directory of the task.\"\"\"\n        super().set_workdir(workdir, chroot=chroot)\n        # Small hack: the log file of optics is actually the main output file.\n        self.output_file = self.log_file",
        "sampled": "```python\nfrom os import chdir, getcwd\n\ndef set_workdir(self, workdir, chroot=False):\n    \"\"\"\n    Set the working directory of the task.\n    \n    Args:\n        workdir (str): The working directory to be set.\n        chroot (bool): Whether to change the root directory. Defaults to False.\n    \n    Returns:\n        str: The current working directory after setting.\n    \"\"\"\n    \n    # Check if the workdir is a string\n    if"
    },
    {
        "original": "def long_form_multiple_formats(jupytext_formats, metadata=None):\n    \"\"\"Convert a concise encoding of jupytext.formats to a list of formats, encoded as dictionaries\"\"\"\n    if not jupytext_formats:\n        return []\n\n    if not isinstance(jupytext_formats, list):\n        jupytext_formats = [fmt for fmt in jupytext_formats.split(',') if fmt]\n\n    jupytext_formats = [long_form_one_format(fmt, metadata) for fmt in jupytext_formats]\n\n    return jupytext_formats",
        "sampled": "```python\ndef long_form_multiple_formats(jupytext_formats, metadata=None):\n    \"\"\"\n    Convert a concise encoding of jupytext.formats to a list of formats, encoded as dictionaries\n    \n    :param jupytext_formats: A string containing a set of formats separated by commas and 'or's.\n                             For example: 'py,ipynb' or '(py|rst)(or)md'\n                             where each pair like 'py' contains keys and values to convert from and to"
    },
    {
        "original": "async def prepare_decrypter(client, cdn_client, cdn_redirect):\n        \"\"\"\n        Prepares a new CDN decrypter.\n\n        :param client: a TelegramClient connected to the main servers.\n        :param cdn_client: a new client connected to the CDN.\n        :param cdn_redirect: the redirect file object that caused this call.\n        :return: (CdnDecrypter, first chunk file data)\n        \"\"\"\n        cdn_aes = AESModeCTR(\n            key=cdn_redirect.encryption_key,\n            # 12 first bytes of the IV..4 bytes of the offset (0, big endian)\n            iv=cdn_redirect.encryption_iv[:12] + bytes(4)\n        )\n\n        # We assume that cdn_redirect.cdn_file_hashes are ordered by offset,\n        # and that there will be enough of these to retrieve the whole file.\n        decrypter = CdnDecrypter(\n            cdn_client, cdn_redirect.file_token,\n            cdn_aes, cdn_redirect.cdn_file_hashes\n        )\n\n        cdn_file = await cdn_client(GetCdnFileRequest(\n            file_token=cdn_redirect.file_token,\n            offset=cdn_redirect.cdn_file_hashes[0].offset,\n            limit=cdn_redirect.cdn_file_hashes[0].limit\n        ))\n        if isinstance(cdn_file, CdnFileReuploadNeeded):\n            # We need to use the original client here\n            await client(ReuploadCdnFileRequest(\n                file_token=cdn_redirect.file_token,\n                request_token=cdn_file.request_token\n            ))\n\n            # We want to always return a valid upload.CdnFile\n            cdn_file = decrypter.get_file()\n        else:\n            cdn_file.bytes = decrypter.cdn_aes.encrypt(cdn_file.bytes)\n            cdn_hash = decrypter.cdn_file_hashes.pop(0)\n            decrypter.check(cdn_file.bytes, cdn_hash)\n\n        return decrypter, cdn_file",
        "sampled": "```python\nfrom datetime import datetime\n\nclass CdnDecrypter:\n    def __init__(self, client, cdn_client, cdn_redirect):\n        \"\"\"\n        Initializes a new CDN decrypter.\n\n        :param client: a TelegramClient connected to the main servers.\n        :param cdn_client: a new client connected to the CDN.\n        :param cdn_redirect: the redirect file object that caused this call.\n        \"\"\"\n        self.client = client\n        self.cdn_client ="
    },
    {
        "original": "def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        assert isinstance(label, (github.Label.Label, str, unicode)), label\n        if isinstance(label, github.Label.Label):\n            label = label._identity\n        else:\n            label = urllib.quote(label)\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.issue_url + \"/labels/\" + label\n        )",
        "sampled": "```python\nimport requests\n\nclass GitHub:\n    def __init__(self, owner, repo):\n        self.owner = owner\n        self.repo = repo\n        self.base_url = f'https://api.github.com/repos/{self.owner}/{self.repo}/'\n\n    def remove_label(self, label_name):\n        \"\"\"\n        Removes a label from the current repository.\n\n        :param label_name: Name of the label to be removed.\n        :rtype: None\n        \"\"\"\n\n        url = f'{self"
    },
    {
        "original": "def remove_terms_by_indices(self, idx_to_delete_list):\n        \"\"\"\n        Parameters\n        ----------\n        idx_to_delete_list, list\n\n        Returns\n        -------\n        TermDocMatrix\n        \"\"\"\n        new_X, new_term_idx_store = self._get_X_after_delete_terms(idx_to_delete_list)\n        return self._make_new_term_doc_matrix(new_X, self._mX, self._y, new_term_idx_store, self._category_idx_store,\n                                              self._metadata_idx_store, self._y == self._y)",
        "sampled": "```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Term:\n    term: str\n    frequency: int\n\n@dataclass\nclass Document:\n    name: str\n    terms: list[Term]\n    index: int\n\n@dataclass\nclass TermDocMatrix:\n    documents: list[Document]\n\ndef remove_terms_by_indices(self, idx_to_delete_list):\n    \"\"\"\n    Parameters\n    ----------\n        idx_to_delete_list, list\n    \n     Returns\n     -------\n       "
    },
    {
        "original": "def confusion_matrix(\n    gold, pred, null_pred=False, null_gold=False, normalize=False, pretty_print=True\n):\n    \"\"\"A shortcut method for building a confusion matrix all at once.\n\n    Args:\n        gold: an array-like of gold labels (ints)\n        pred: an array-like of predictions (ints)\n        null_pred: If True, include the row corresponding to null predictions\n        null_gold: If True, include the col corresponding to null gold labels\n        normalize: if True, divide counts by the total number of items\n        pretty_print: if True, pretty-print the matrix before returning\n    \"\"\"\n    conf = ConfusionMatrix(null_pred=null_pred, null_gold=null_gold)\n    gold = arraylike_to_numpy(gold)\n    pred = arraylike_to_numpy(pred)\n    conf.add(gold, pred)\n    mat = conf.compile()\n\n    if normalize:\n        mat = mat / len(gold)\n\n    if pretty_print:\n        conf.display(normalize=normalize)\n\n    return mat",
        "sampled": "```python\nimport numpy as np\n\ndef confusion_matrix(\n    gold, pred, null_pred=False, null_gold=False, normalize=False, pretty_print=True\n):\n    \"\"\"\n    A shortcut method for building a confusion matrix all at once.\n\n    Args:\n        gold: an array-like of gold labels (ints)\n        pred: an array-like of predictions (ints)\n        null_pred: If True, include the row corresponding to null predictions\n        null_gold: If True, include the col corresponding"
    },
    {
        "original": "def nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    verbose : boolean\n        print additional information about the node\n        e.g. platform version, hvm capable, ...\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    ret = {} if verbose else []\n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    ## get list of nodes\n    try:\n        for cn in client.cmd_iter('G@virtual:physical and G@os:smartos',\n                                  'grains.items', tgt_type='compound'):\n            if not cn:\n                continue\n            node = next(six.iterkeys(cn))\n            if not isinstance(cn[node], dict) or \\\n                    'ret' not in cn[node] or \\\n                    not isinstance(cn[node]['ret'], dict):\n                continue\n            if verbose:\n                ret[node] = {}\n                ret[node]['version'] = {}\n                ret[node]['version']['platform'] = cn[node]['ret']['osrelease']\n                if 'computenode_sdc_version' in cn[node]['ret']:\n                    ret[node]['version']['sdc'] = cn[node]['ret']['computenode_sdc_version']\n                ret[node]['vms'] = {}\n                if 'computenode_vm_capable' in cn[node]['ret'] and \\\n                        cn[node]['ret']['computenode_vm_capable'] and \\\n                        'computenode_vm_hw_virt' in cn[node]['ret']:\n                    ret[node]['vms']['hw_cap'] = cn[node]['ret']['computenode_vm_hw_virt']\n                else:\n                    ret[node]['vms']['hw_cap'] = False\n                if 'computenode_vms_running' in cn[node]['ret']:\n                    ret[node]['vms']['running'] = cn[node]['ret']['computenode_vms_running']\n            else:\n                ret.append(node)\n    except SaltClientError as client_error:\n        return \"{0}\".format(client_error)\n\n    if not verbose:\n        ret.sort()\n    return ret",
        "sampled": "```python\nimport subprocess\n\ndef nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    verbose : boolean\n        print additional information about the node\n        e.g. platform version, hvm capable, ...\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    \n    try:\n        # List all nodes using Grand Central Dispatch (GCD) command on macOS or other *nix like os"
    },
    {
        "original": "def type(self):\n        \"\"\"\n        Read-only. A member of :ref:`MsoColorType`, one of RGB, THEME, or\n        AUTO, corresponding to the way this color is defined. Its value is\n        |None| if no color is applied at this level, which causes the\n        effective color to be inherited from the style hierarchy.\n        \"\"\"\n        color = self._color\n        if color is None:\n            return None\n        if color.themeColor is not None:\n            return MSO_COLOR_TYPE.THEME\n        if color.val == ST_HexColorAuto.AUTO:\n            return MSO_COLOR_TYPE.AUTO\n        return MSO_COLOR_TYPE.RGB",
        "sampled": "```python\nfrom enum import Enum\n\nclass MsoColorType(Enum):\n    RGB = 1\n    THEMEMATIC = 2\n    AUTO = 3\n\nclass Color:\n    def __init__(self, color_type=None):\n        self._color_type = color_type\n\n    @property\n    def type(self):\n        if self._color_type is None:\n            return None\n        elif isinstance(self._color_type, MsoColorType):\n            return self._color_type.name"
    },
    {
        "original": "def _simplify_function(self):\n        \"\"\"\n        Simplify the entire function.\n\n        :return:    None\n        \"\"\"\n\n        # Computing reaching definitions\n        rd = self.project.analyses.ReachingDefinitions(func=self.function, func_graph=self.graph, observe_all=True)\n\n        simp = self.project.analyses.AILSimplifier(self.function, func_graph=self.graph, reaching_definitions=rd)\n\n        for key in list(self._blocks.keys()):\n            old_block = self._blocks[key]\n            if old_block in simp.blocks:\n                self._blocks[key] = simp.blocks[old_block]\n\n        self._update_graph()",
        "sampled": "```python\nimport sympy as sp\n\nclass FunctionSimplifier:\n    def __init__(self, function):\n        self.function = function\n\n    def simplify_function(self):\n        \"\"\"\n        Simplify the entire function.\n\n        :return:    The simplified function.\n        \"\"\"\n        # Use sympy to parse the mathematical expression\n        x = sp.symbols('x')\n        expr = sp.sympify(self.function)\n        \n        # Simplify the expression\n        simplified_expr = sp.s"
    },
    {
        "original": "def execute_paged_query(self, verb, verb_arguments):\n        \"\"\"Executes query (ex. list) via a dedicated http object.\n\n        Args:\n            verb (str): Method to execute on the component (ex. get, list).\n            verb_arguments (dict): key-value pairs to be passed to _BuildRequest.\n\n        Yields:\n            dict: Service Response.\n\n        Raises:\n            PaginationNotSupportedError: When an API does not support paging.\n        \"\"\"\n        if not self.supports_pagination(verb=verb):\n            raise PaginationNotSupported('{} does not support pagination')\n\n        request = self._build_request(verb, verb_arguments)\n\n        number_of_pages_processed = 0\n        while request is not None:\n            response = self._execute(request)\n            number_of_pages_processed += 1\n            log.debug('Executing paged request #%s', number_of_pages_processed)\n            request = self._build_next_request(verb, request, response)\n            yield response",
        "sampled": "```python\nimport requests\nimport json\n\nclass Component:\n    def __init__(self, component_id):\n        self.component_id = component_id\n        self.http_object = requests.Session()\n\n    def _BuildRequest(self, verb, base_url, path_macros=None):\n        if path_macros is None:\n            path_macros = {}\n\n        url = \"https://example.com{}\".format(\"/component/{}\".format(self.component_id) + \"/\" + \"/\".join(path_macros.values()))\n\n        headers_meta = {\""
    },
    {
        "original": "def get_all_nn_info(self, structure):\n        \"\"\"Get a listing of all neighbors for all sites in a structure\n\n        Args:\n            structure (Structure): Input structure\n        Return:\n            List of NN site information for each site in the structure. Each\n                entry has the same format as `get_nn_info`\n        \"\"\"\n\n        return [self.get_nn_info(structure, n) for n in range(len(structure))]",
        "sampled": "```python\nclass Structure:\n    def __init__(self, coords):\n        self.coords = coords\n\nclass Site:\n    def __init__(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\nclass NNInfoSite:\n    def __init__(self, idx_nn_site_nearest=0, idx_nn_site_next=None,\n                 idx_nn_site_previous=None, distance=0.0):\n        self.idx_nn_site_ne"
    },
    {
        "original": "def _read(self):\n    \"\"\"Actually read the response and parse it, returning a Response.\"\"\"\n    with sw(\"read_response\"):\n      with catch_websocket_connection_errors():\n        response_str = self._sock.recv()\n    if not response_str:\n      raise ProtocolError(\"Got an empty response from SC2.\")\n    response = sc_pb.Response()\n    with sw(\"parse_response\"):\n      response.ParseFromString(response_str)\n    return response",
        "sampled": "```markdown\n### Problem Description\n\nYou are given a string *s*. For each (possibly empty) substring in *s* that doesn't contain any \nrepeating characters, remove the outermost pair of parentheses and return the modified string.\n\nThe testcases will be small enough that you can fit it into memory. \n\n---\n\n### Input Specifications\n\n* A single line with string *s*\n\n---\n\n### Output Specifications\n\n* The modified string after removing outermost pair of parentheses for all non-repeating"
    },
    {
        "original": "def _variable_on_cpu(name, shape, initializer):\n  \"\"\"Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  with tf.device('/cpu:0'):\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var",
        "sampled": "```python\nimport tensorflow as tf\n\ndef _variable_on_cpu(name, shape, initializer):\n  \"\"\"Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  \"\"\"\n  with tf.device('/cpu:0'):\n    var = tf.Variable(initializer(shape), name=name)\n  return var\n\n# Testing the function\nvar1 = _variable"
    },
    {
        "original": "def get_users_in_organization(self, organization_id, start=0, limit=50):\n        \"\"\"\n        Get all the users of a specified organization\n\n        :param organization_id: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return: Users list in organization\n        \"\"\"\n        url = 'rest/servicedeskapi/organization/{}/user'.format(organization_id)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params)",
        "sampled": "```python\nclass OrganizationUserService:\n    def get_users_in_organization(self, organization_id, start=0, limit=50):\n        \"\"\"\n        Get all the users of a specified organization\n\n        :param organization_id: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return: Users list in organization\n        \"\"\"\n        \n         #### Database Query to Retrieve Users for Given Organization ID. This Assumption That We Are Using A PUBLC DB to"
    },
    {
        "original": "def before_request():\n    \"\"\"This runs before every API request. The function take cares of creating\n    driver object is not already created. Also it checks for few prerequisits\n    parameters and set global variables for other functions to use\n    \n    Required paramters for an API hit are:\n    auth-key: key string to identify valid request\n    client_id: to identify for which client the request is to be run\n    \"\"\"\n    global logger\n    \n    if not request.url_rule:\n        abort(404)\n\n    if logger == None:\n        create_logger()\n    logger.info(\"API call \" + request.method + \" \" + request.url)\n\n    auth_key = request.headers.get('auth-key')\n    g.client_id = request.headers.get('client_id')\n    rule_parent = request.url_rule.rule.split('/')[1]\n    \n    if API_KEY and auth_key != API_KEY:\n        abort(401, 'you must send valid auth-key')\n        raise Exception()\n\n    if not g.client_id and rule_parent != 'admin':\n        abort(400, 'client ID is mandatory')\n\n    acquire_semaphore(g.client_id)\n\n    # Create a driver object if not exist for client requests.\n    if rule_parent != 'admin':\n        if g.client_id not in drivers:\n            drivers[g.client_id] = init_client(g.client_id)\n        \n        g.driver = drivers[g.client_id]\n        g.driver_status = WhatsAPIDriverStatus.Unknown\n        \n        if g.driver is not None:\n            g.driver_status = g.driver.get_status()\n        \n        # If driver status is unkown, means driver has closed somehow, reopen it\n        if (g.driver_status != WhatsAPIDriverStatus.NotLoggedIn\n            and g.driver_status != WhatsAPIDriverStatus.LoggedIn):\n            drivers[g.client_id] = init_client(g.client_id)\n            g.driver_status = g.driver.get_status()\n        \n        init_timer(g.client_id)",
        "sampled": "```python\nclass RequestManager:\n    driver = None\n    auth_key = None\n    client_id = None\n\n    def before_request(self, auth_key, client_id):\n        \"\"\"\n        This function runs before every API request. It creates a driver object if not already created.\n        Also, it checks for few prerequisites parameters and set global variables for other functions to use\n        \n        Required parameters for an API hit are:\n        - auth-key: key string to identify valid request\n        - client"
    },
    {
        "original": "def widget_from_single_value(o):\n        \"\"\"Make widgets from single values, which can be used as parameter defaults.\"\"\"\n        if isinstance(o, string_types):\n            return Text(value=unicode_type(o))\n        elif isinstance(o, bool):\n            return Checkbox(value=o)\n        elif isinstance(o, Integral):\n            min, max, value = _get_min_max_value(None, None, o)\n            return IntSlider(value=o, min=min, max=max)\n        elif isinstance(o, Real):\n            min, max, value = _get_min_max_value(None, None, o)\n            return FloatSlider(value=o, min=min, max=max)\n        else:\n            return None",
        "sampled": "```python\ndef widget_from_single_value(o):\n    \"\"\"Make widgets from single values, which can be used as parameter defaults.\"\"\"\n    \n    # List of possible types for single value widgets\n    single_value_types = [int, float, bool]\n    \n    # Check if the type of o is in allowed types\n    if isinstance(o, tuple(single_value_types)):\n        return f\"a\u2212string widget for {o.__class__.__name__} of value {o}\"\n    \n# Test cases\n"
    },
    {
        "original": "def _domain_event_watchdog_cb(conn, domain, action, opaque):\n    \"\"\"\n    Domain watchdog events handler\n    \"\"\"\n    _salt_send_domain_event(opaque, conn, domain, opaque['event'], {\n        'action': _get_libvirt_enum_string('VIR_DOMAIN_EVENT_WATCHDOG_', action)\n    })",
        "sampled": "```python\nclass DomainEventWatchdog:\n    def __init__(self):\n        \"\"\"\n        Initialize the DomainEventWatchdog.\n        \"\"\"\n        \n    def _domain_event_watchdog_cb(self, conn, domain, action, opaque):\n        \"\"\"\n        Callback function for domain watchdog events.\n        \n        Args:\n            conn (object): WebSocket connection object.\n            domain (str): The entity (domain) for which the event occurred.\n            action (str): The specific action or event that caused the callback"
    },
    {
        "original": "def del_variables(self, variables):\n        \"\"\"\n        Deletes variables from the NoisyOrModel.\n\n        Parameters\n        ----------\n        variables: list, tuple, dict (array like)\n            list of variables to be deleted.\n\n        Examples\n        --------\n        >>> from pgmpy.models import NoisyOrModel\n        >>> model = NoisyOrModel(['x1', 'x2', 'x3'], [2, 3, 2], [[0.6, 0.4],\n        ...                                                      [0.2, 0.4, 0.7],\n        ...                                                      [0.1, 0. 4]])\n        >>> model.del_variables(['x1'])\n        \"\"\"\n        variables = [variables] if isinstance(variables, six.string_types) else set(variables)\n        indices = [index for index, variable in enumerate(self.variables) if variable in variables]\n        self.variables = np.delete(self.variables, indices, 0)\n        self.cardinality = np.delete(self.cardinality, indices, 0)\n        self.inhibitor_probability = [prob_array for index, prob_array in enumerate(self.inhibitor_probability)\n                                      if index not in indices]",
        "sampled": "```python\nclass NoisyOrModel:\n    def __init__(self, variables, num_parents, thresholds):\n        \"\"\"\n        Initializes the NoisyOrModel.\n\n        Parameters\n        ----------\n        variables: list of strings\n            List of variable names in the model.\n        num_parents: list of integers\n            Number of parent nodes for each variable.\n        thresholds: 2D array (list of lists)\n            Thresholds matrix where each row corresponds to a variable and contains \n            thresholds for"
    },
    {
        "original": "def set_auth_field(self, user_field, biz_field):\n        \"\"\"\n        \u8bbe\u7f6e\u6388\u6743\u9875\u5b57\u6bb5\u4fe1\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1497082828_r1cI2\n\n        :param user_field: \u6388\u6743\u9875\u4e2a\u4eba\u53d1\u7968\u5b57\u6bb5\n        :type user_field: dict\n        :param biz_field: \u6388\u6743\u9875\u5355\u4f4d\u53d1\u7968\u5b57\u6bb5\n        :type biz_field: dict\n        \"\"\"\n        return self._post(\n            'setbizattr',\n            params={\n                'action': 'set_auth_field',\n            },\n            data={\n                'auth_field': {\n                    'user_field': user_field,\n                    'biz_field': biz_field,\n                },\n            },\n        )",
        "sampled": "```python\nclass WxPayAPI:\n    def __init__(self):\n        pass\n\n    def set_auth_field(self, user_field, biz_field):\n        \"\"\"\n        \u8bbe\u7f6e\u6388\u6743\u9875\u5b57\u6bb5\u4fe1\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?id=mp1497082828_r1cI2\n\n        :param user_field: \u6388\u6743\u9875\u4e2a\u4eba\u53d1\u7968\u5b57\u6bb5\ntype: dict\n \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c siguientes campos payload tax_number_type incorporated significa registered_name  fiscal_registration"
    },
    {
        "original": "def next(self):\n        \"\"\"Fetch a set of items with IDs less than current set.\"\"\"\n        if self.limit and self.limit == self.num_tweets:\n            raise StopIteration\n\n        if self.index >= len(self.results) - 1:\n            data = self.method(max_id=self.max_id, parser=RawParser(), *self.args, **self.kargs)\n\n            if hasattr(self.method, '__self__'):\n                old_parser = self.method.__self__.parser\n                # Hack for models which expect ModelParser to be set\n                self.method.__self__.parser = ModelParser()\n\n            # This is a special invocation that returns the underlying\n            # APIMethod class\n            model = ModelParser().parse(self.method(create=True), data)\n            if hasattr(self.method, '__self__'):\n                self.method.__self__.parser = old_parser\n                result = self.method.__self__.parser.parse(self.method(create=True), data)\n            else:\n                result = model\n\n            if len(self.results) != 0:\n                self.index += 1\n            self.results.append(result)\n            self.model_results.append(model)\n        else:\n            self.index += 1\n            result = self.results[self.index]\n            model = self.model_results[self.index]\n\n        if len(result) == 0:\n            raise StopIteration\n        # TODO: Make this not dependant on the parser making max_id and\n        # since_id available\n        self.max_id = model.max_id\n        self.num_tweets += 1\n        return result",
        "sampled": "```python\nclass Item:\n    def __init__(self, id):\n        self.id = id\n\nclass DBIterator:\n    def __init__(self, db):\n        \"\"\"\n        The.DBIterator iterates over a DB instead of values.\n        \n        Instances of type T satisfy the following conditions\n        \n            - They must be immutable\n            - They must implement equality (==) correctly for instances and distint signatures, \n              instance order TBD).\n        \n           Generally speaking use immutables when you would"
    },
    {
        "original": "def upload_file(self, file_name, file_path):\n        \"\"\" Upload a given file from the file_path to the bucket\n            with the new name/path file_name. \"\"\"\n        upload_key = Key(bucket=self.bucket, name=file_name)\n        content_type = \"text/plain\"\n        if file_name.endswith(\".html\"):\n            content_type = \"text/html\"\n        elif file_name.endswith(\".jpg\"):\n            content_type = \"image/jpeg\"\n        elif file_name.endswith(\".png\"):\n            content_type = \"image/png\"\n        upload_key.set_contents_from_filename(\n            file_path,\n            headers={\"Content-Type\": content_type})\n        upload_key.url = \\\n            upload_key.generate_url(expires_in=3600).split(\"?\")[0]\n        try:\n            upload_key.make_public()\n        except Exception:\n            pass",
        "sampled": "```python\nclass AWS_S3:\n    def upload_file(self, file_name, file_path):\n        \"\"\"\n        Upload a given file from the file_path to the bucket with the new name/path file_name.\n        \n        Args:\n            file_name (str): The desired name for the uploaded object in S3.\n            file_path (str): The path to the local file.\n            \n        Returns:\n            bool: True if the upload is Successful. False otherwise.\n        \"\"\"\n        \n        \t# Import"
    },
    {
        "original": "def delete_cookies():\n    \"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Redirect to cookie list\n    \"\"\"\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.delete_cookie(key=key)\n\n    return r",
        "sampled": "```python\nfrom flask import request, redirect\n\ndef delete_cookies():\n    \"\"\"\n    Deletes cookie(s) as provided by the query string and redirects to cookie list.\n    \n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    \n    produces:\n      - text/plain\n    \n"
    },
    {
        "original": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        # autoscaleconnection.get_all_groups()\n        limits['Auto Scaling groups'] = AwsLimit(\n            'Auto Scaling groups',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n        # autoscaleconnection.get_all_launch_configurations()\n        limits['Launch configurations'] = AwsLimit(\n            'Launch configurations',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::LaunchConfiguration',\n        )\n        self.limits = limits\n        return limits",
        "sampled": "```python\nclass AwsLimit:\n    def __init__(self, name, description):\n        self.name = name\n        self.description = description\n\nclass Service:\n    def __init__(self):\n        self.limits = {\n            'MAX_rectangle_WIDTH': AwsLimit('MAX_rectangle_WIDTH', 'Maximum width of rectangle'),\n            'MIN_rectangle_HEIGHT':  AwsLimit('MIN_rectangle_HEIGHT', 'Minimum height of rectangle')\n        }\n\n    def get_limits(self):\n        return {limit.name: limit"
    },
    {
        "original": "def apply_to_structure(self, structure):\n        \"\"\"\n        Apply the deformation gradient to a structure.\n\n        Args:\n            structure (Structure object): the structure object to\n                be modified by the deformation\n        \"\"\"\n        def_struct = structure.copy()\n        old_latt = def_struct.lattice.matrix\n        new_latt = np.transpose(np.dot(self, np.transpose(old_latt)))\n        def_struct.lattice = Lattice(new_latt)\n        return def_struct",
        "sampled": "```python\nimport numpy as np\n\nclass Structure:\n    def __init__(self, dense):\n        \"\"\"\n        Initialize a structure with a given density.\n\n        Args:\n            dense (numpy array): the density of the structure\n        \"\"\"\n        self.dense = dense\n\n\nclass Deformation:\n    def __init__(self, deformation_gradient):\n        \"\"\"\n        Initialize a deformation with a given gradient.\n\n        Args:\n            deformation_gradient (numpy array): the deformation gradient of \n                the deformation\n        \"\"\"\n"
    },
    {
        "original": "def main():\n    \"\"\"Rewrite Thrift-generated Python clients to handle recursive structs. For\n    more details see: https://issues.apache.org/jira/browse/THRIFT-2642.\n\n    Requires package `RedBaron`, available via pip:\n    $ pip install redbaron\n\n    To use:\n\n    $ thrift -gen py mapd.thrift\n    $ mv gen-py/mapd/ttypes.py gen-py/mapd/ttypes-backup.py\n    $ python fix_recursive_structs.py gen-py/mapd/ttypes-backup.py gen-py/mapd/ttypes.py\n\n    \"\"\"\n    in_file = open(sys.argv[1], 'r')\n    out_file = open(sys.argv[2], 'w')\n\n    red_ast = RedBaron(in_file.read())\n\n    thrift_specs = [ts.parent for ts in red_ast.find_all(\n        'name', 'thrift_spec') if ts.parent.type == 'assignment' and ts.parent.parent.name in ['TDatumVal', 'TColumnData']]\n\n    nodes = []\n    for ts in thrift_specs:\n        node = ts.copy()\n        node.target = ts.parent.name + '.' + str(node.target)\n        nodes.append(node)\n        ts.value = 'None'\n\n    red_ast.extend(nodes)\n    out_file.write(red_ast.dumps())",
        "sampled": "```python\nfrom redbaron import RedBaron\n\ndef main():\n    \"\"\"\n    Rewrite Thrift-generated Python clients to handle recursive structs.\n\n    Args:\n        input_file (str): The file containing the Thrift-generated code.\n        output_file (str): The file where the modified code will be written.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Parse the input file using RedBaron\n    with open(input_file, 'r') as f:\n        tree = RedBaron"
    },
    {
        "original": "def rest_api_exists(self, rest_api_id):\n        # type: (str) -> bool\n        \"\"\"Check if an an API Gateway REST API exists.\"\"\"\n        client = self._client('apigateway')\n        try:\n            client.get_rest_api(restApiId=rest_api_id)\n            return True\n        except client.exceptions.NotFoundException:\n            return False",
        "sampled": "```python\nclass ApiGateway:\n    def __init__(self):\n        # Initialize the API Gateway with a dictionary to store APIs\n        self.apis = {}\n\n    def rest_api_exists(self, rest_api_id):\n        # type: (str) -> bool\n        \"\"\"Check if an an API Gateway REST API exists.\"\"\"\n        return rest_api_id in self.apis\n\n# Example usage:\napi_gateway = ApiGateway()\napi_gateway.rest_api_exists('example_rest_api')  # Returns True"
    },
    {
        "original": "def _handle_result(self, result):\n        \"\"\"Mark the result as completed, insert the `CompiledResultNode` into\n        the manifest, and mark any descendants (potentially with a 'cause' if\n        the result was an ephemeral model) as skipped.\n        \"\"\"\n        is_ephemeral = result.node.is_ephemeral_model\n        if not is_ephemeral:\n            self.node_results.append(result)\n\n        node = CompileResultNode(**result.node)\n        node_id = node.unique_id\n        self.manifest.nodes[node_id] = node\n\n        if result.error is not None:\n            if is_ephemeral:\n                cause = result\n            else:\n                cause = None\n            self._mark_dependent_errors(node_id, result, cause)",
        "sampled": "```python\nclass ResultHandler:\n    def __init__(self):\n        self.manifest = {}  # This will store the compiled result nodes\n\n    def _handle_result(self, result):\n        \"\"\"\n        Mark the result as completed, insert the `CompiledResultNode` into\n        the manifest, and mark any descendants (potentially with a 'cause' if\n        the result was an ephemeral model) as skipped.\n        \n        Parameters:\n            :param result: The compiled result from"
    },
    {
        "original": "def canonicalize_gates(gates: LogicalGates\n        ) -> Dict[frozenset, LogicalGates]:\n        \"\"\"Canonicalizes a set of gates by the qubits they act on.\n\n        Takes a set of gates specified by ordered sequences of logical\n        indices, and groups those that act on the same qubits regardless of\n        order.\"\"\"\n        canonicalized_gates = defaultdict(dict\n            ) # type: DefaultDict[frozenset, LogicalGates]\n        for indices, gate in gates.items():\n            indices = tuple(indices)\n            canonicalized_gates[frozenset(indices)][indices] = gate\n        return {canonical_indices: dict(list(gates.items()))\n                for canonical_indices, gates in canonicalized_gates.items()}",
        "sampled": "```python\nfrom typing import Dict, Set\nfrom collections import defaultdict\nimport itertools\n\nclass LogicalGates:\n    def __init__(self):\n        pass\n\ndef canonicalize_gates(gates: 'Set[LogicalGates]') -> 'Dict[frozenset, LogicalGates]':\n    result = defaultdict(list)\n\n    for gate in gates:\n        qubits = tuple(sorted(gate.qubits))\n        result[qubits].append(gate)\n\n    canonicalized_gates ="
    },
    {
        "original": "def set_all_variables(self, delu_dict, delu_default):\n        \"\"\"\n        Sets all chemical potential values and returns a dictionary where\n            the key is a sympy Symbol and the value is a float (chempot).\n\n        Args:\n            entry (SlabEntry): Computed structure entry of the slab\n            delu_dict (Dict): Dictionary of the chemical potentials to be set as\n                constant. Note the key should be a sympy Symbol object of the\n                format: Symbol(\"delu_el\") where el is the name of the element.\n            delu_default (float): Default value for all unset chemical potentials\n\n        Returns:\n            Dictionary of set chemical potential values\n        \"\"\"\n\n        # Set up the variables\n        all_delu_dict = {}\n        for du in self.list_of_chempots:\n            if delu_dict and du in delu_dict.keys():\n                all_delu_dict[du] = delu_dict[du]\n            elif du == 1:\n                all_delu_dict[du] = du\n            else:\n                all_delu_dict[du] = delu_default\n\n        return all_delu_dict",
        "sampled": "```python\nimport sympy as sp\n\ndef set_all_variables(delu_dict, delu_default):\n    \"\"\"\n    Sets all chemical potential values and returns a dictionary where\n        the key is a sympy Symbol and the value is a float (chempot).\n\n    Args:\n        delu_dict (Dict): Dictionary of the chemical potentials to be set as\n            constant. Note the key should be a sympy Symbol object of the\n            format: Symbol(\"delu_el\") where el"
    },
    {
        "original": "def assign_license(service_instance, license_key, license_name,\n                   entity_ref=None, entity_name=None,\n                   license_assignment_manager=None):\n    \"\"\"\n    Assigns a license to an entity.\n\n    service_instance\n        The Service Instance Object from which to obrain the licenses.\n\n    license_key\n        The key of the license to add.\n\n    license_name\n        The description of the license to add.\n\n    entity_ref\n        VMware entity to assign the license to.\n        If None, the entity is the vCenter itself.\n        Default is None.\n\n    entity_name\n        Entity name used in logging.\n        Default is None.\n\n    license_assignment_manager\n        The LicenseAssignmentManager object of the service instance.\n        If not provided it will be retrieved\n        Default is None.\n    \"\"\"\n    if not license_assignment_manager:\n        license_assignment_manager = \\\n                get_license_assignment_manager(service_instance)\n    entity_id = None\n\n    if not entity_ref:\n        # vcenter\n        try:\n            entity_id = service_instance.content.about.instanceUuid\n        except vim.fault.NoPermission as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(\n                'Not enough permissions. Required privilege: '\n                '{0}'.format(exc.privilegeId))\n        except vim.fault.VimFault as exc:\n            raise salt.exceptions.VMwareApiError(exc.msg)\n        except vmodl.RuntimeFault as exc:\n            raise salt.exceptions.VMwareRuntimeError(exc.msg)\n        if not entity_name:\n            entity_name = 'vCenter'\n    else:\n        # e.g. vsan cluster or host\n        entity_id = entity_ref._moId\n\n    log.trace('Assigning license to \\'%s\\'', entity_name)\n    try:\n        vmware_license = license_assignment_manager.UpdateAssignedLicense(\n            entity_id,\n            license_key,\n            license_name)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    return vmware_license",
        "sampled": "```python\ndef assign_license(service_instance, license_key, license_name,\n                   entity_ref=None, entity_name=None,\n                   license_assignment_manager=None):\n    \"\"\"\n    Assigns a license to an entity.\n\n    service_instance\n        The Service Instance Object from which to obtain the licenses.\n\n    license_key\n        The key of the license to add.\n\n    license_name\n        The description of the license to add.\n\n    entity_ref\n        VMware entity to assign the license to.\n        If None,"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "sampled": "```python\nfrom typing import Callable, Tuple\n\ndef _convert_a_header_to_a_h2_header(self, \n                                     hdr_name: str, \n                                     hdr_value: str, \n                                     is_sensitive: Callable[[str, str], bool], \n                                     should_index: Callable[[str], bool]\n                                    ) -> Tuple:\n    \"\"\"\n    This function builds a HPackHeaders from a header name and a value.\n    \n    It returns a HPackIndexedHdr whenever possible. If not, it returns"
    },
    {
        "original": "def get_args(cls, dist, header=None):\n        \"\"\"\n        Yield write_script() argument tuples for a distribution's\n        console_scripts and gui_scripts entry points.\n        \"\"\"\n        if header is None:\n            header = cls.get_header()\n        spec = str(dist.as_requirement())\n        for type_ in 'console', 'gui':\n            group = type_ + '_scripts'\n            for name, ep in dist.get_entry_map(group).items():\n                cls._ensure_safe_name(name)\n                script_text = cls.template % locals()\n                args = cls._get_script_args(type_, name, header, script_text)\n                for res in args:\n                    yield res",
        "sampled": " ```python\nfrom typing import Generator, Tuple\n\ndef write_script(name: str, entry_point: str, dist_name: str) -> Tuple[str, str]:\n    \"\"\"\n    Returns a tuple containing the script name and the command to run it.\n    \"\"\"\n    return name, f\"{dist_name} {entry_point}\"\n\ndef get_args(cls: type, dist: dict, header=None) -> Generator[Tuple[str], None]:\n    \"\"\"\n    Yield write_script() argument tuples for a distribution's"
    },
    {
        "original": "def get_login_info(self, auth_code, provider_access_token=None):\n        \"\"\"\n        \u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        :param provider_access_token: \u670d\u52a1\u63d0\u4f9b\u5546\u7684 accesstoken\n        :param auth_code: OAuth 2.0 \u6388\u6743\u4f01\u4e1a\u53f7\u7ba1\u7406\u5458\u767b\u5f55\u4ea7\u751f\u7684 code\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self._post(\n            'service/get_login_info',\n            params={\n                'provider_access_token': provider_access_token,\n            },\n            data={\n                'auth_code': auth_code,\n            }\n        )",
        "sampled": "```python\nimport requests\n\nclass WeChat:\n    def get_login_info(self, auth_code, provider_access_token=None):\n        \"\"\"\n        \u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        :param provider_access_token: \u670d\u52a1\u63d0\u4f9b\u5546\u7684 accesstoken\n        :param auth_code: OAuth 2.0 \u6388\u6743\u4f01\u4e1a\u53f7\u7ba1\u7406\u5458\u767b\u5f55\u4ea7\u751f\u7684 code\n       "
    },
    {
        "original": "def p_matrix(p):\n    \"\"\"matrix : LBRACKET RBRACKET\n              | LBRACKET concat_list RBRACKET\n              | LBRACKET concat_list SEMI RBRACKET\n              | LBRACKET expr_list RBRACKET\n              | LBRACKET expr_list SEMI RBRACKET\n    \"\"\"\n    if len(p) == 3:\n        p[0] = node.matrix()\n    else:\n        p[0] = node.matrix(p[2])",
        "sampled": "```python\nimport re\n\ndef p_matrix(p):\n    \"\"\"\n    matrix : LBRACKET RBRACKET\n             | LBRACKET concat_list RBRACKET\n             | LBRACKET concat_list SEMI RBRACKET\n             | LBRACKET expr_list RBRACKET\n             | LBracket expr_list SEMI RBraket]\n    \"\"\"\n\ndef parse_concat_or_expr Reagan_positive(toks, i, expect_func=None):\n    \"\"\" \n     . t = >  get_token"
    },
    {
        "original": "def check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    CLI Example::\n\n        salt '*' nftables.check_table nat\n    \"\"\"\n    ret = {'comment': '',\n           'result': False}\n\n    if not table:\n        ret['comment'] = 'Table needs to be specified'\n        return ret\n\n    nft_family = _NFTABLES_FAMILIES[family]\n    cmd = '{0} list tables {1}' . format(_nftables_cmd(), nft_family)\n    out = __salt__['cmd.run'](cmd, python_shell=False).find('table {0} {1}'.format(nft_family, table))\n\n    if out == -1:\n        ret['comment'] = 'Table {0} in family {1} does not exist'.\\\n                         format(table, family)\n    else:\n        ret['comment'] = 'Table {0} in family {1} exists'.\\\n                         format(table, family)\n        ret['result'] = True\n    return ret",
        "sampled": "```python\ndef check_table(table=None, family='ipv4'):\n    \"\"\"\n    Check for the existence of a table\n\n    Args:\n        table (str): The name of the table to check. If None, it will default to the family\n        family (str): The type of address to be used in the table (ipv4 or ipv6). Defaults to 'ipv4'\n\n    Returns:\n        bool: Whether the table exists or not\n\n    CLI Example::\n\n        salt '*' n"
    },
    {
        "original": "def split(self, sequence):\n    \"\"\" Split into subsequences according to `sequence`.\"\"\"\n\n    major_idx = sequence.idx\n    idx2 = 0\n    for start, end in zip(major_idx[:-1], major_idx[1:]):\n      idx1 = self.idx.index(start, idx2)\n      idx2 = self.idx.index(end, idx2)\n      seq = Sequence(self.text[start:end])\n      seq.idx = [x-start for x in self.idx[idx1:idx2]]\n      yield seq",
        "sampled": "```python\ndef split(self, sequence):\n    if len(sequence) == 1:  # if sequence of n dimensions has only one element\n        return [[i] for i in range(2**sequence[0])]\n    \n    result = []\n    for i in range(2**sequence[0]):\n        sub_sequence = [j for j in sequence[1:]]  # get the rest of the sequence\n        sub_result = self.split(sub_sequence)  # recursively split the"
    },
    {
        "original": "def extra(name: str, desc: str) -> Callable:\n    \"\"\"\n    Decorator for slave channel's \"additional features\" interface.\n\n    Args:\n        name (str): A human readable name for the function.\n        desc (str): A short description and usage of it. Use\n            ``{function_name}`` in place of the function name\n            in the description.\n\n    Returns:\n        The decorated method.\n    \"\"\"\n\n    def attr_dec(f):\n        f.__setattr__(\"extra_fn\", True)\n        f.__setattr__(\"name\", name)\n        f.__setattr__(\"desc\", desc)\n        return f\n\n    return attr_dec",
        "sampled": "```python\nfrom typing import Callable\n\ndef extra(name: str, desc: str) -> Callable:\n    \"\"\"\n    Decorator for slave channel's \"additional features\" interface.\n\n    Args:\n        name (str): A human readable name for the function.\n        desc (str): A short description and usage of it. Use\n            ``{function_name}`` in place of the function name\n            in the description.\n\n    Returns:\n        The decorated method.\n    \"\"\"\n    \ndef main():\n"
    },
    {
        "original": "def get_base_path() -> Path:\n    \"\"\"\n    Get the base data path for EFB. This can be defined by the\n    environment variable ``EFB_DATA_PATH``.\n    \n    If ``EFB_DATA_PATH`` is not defined, this gives\n    ``~/.ehforwarderbot``.\n    \n    This method creates the queried path if not existing.\n    \n    Returns:\n        The base path.\n    \"\"\"\n    env_data_path = os.environ.get(\"EFB_DATA_PATH\", None)\n    if env_data_path:\n        base_path = Path(env_data_path).resolve()\n    else:\n        base_path = Path.home() / \".ehforwarderbot\"\n    if not base_path.exists():\n        base_path.mkdir(parents=True)\n    return base_path",
        "sampled": "```python\nimport os\nfrom pathlib import Path\n\ndef get_base_path() -> Path:\n    \"\"\"\n    Get the base data path for EFB. This can be defined by the\n    environment variable ``EFB_DATA_PATH``.\n    \n    If ``EFB_DATA_PATH`` is not defined, this gives\n    ``~/.ehforwarderbot``.\n    \n    This method creates the queried path if not existing.\n    \n    Returns:\n        The base path.\n    \"\"\"\n    \n# Construct"
    },
    {
        "original": "def score(self, X, y):\n        \"\"\"\n        Draws a confusion matrix based on the test data supplied by comparing\n        predictions on instances X with the true values specified by the\n        target vector y.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features\n\n        y : ndarray or Series of length n\n            An array or series of target or class values\n\n        Returns\n        -------\n\n        score_ : float\n            Global accuracy score\n        \"\"\"\n        # Create predictions from X (will raise not fitted error)\n        y_pred = self.predict(X)\n\n        # Encode the target with the supplied label encoder\n        if self.label_encoder:\n            try :\n                y = self.label_encoder.inverse_transform(y)\n                y_pred = self.label_encoder.inverse_transform(y_pred)\n            except AttributeError:\n                # if a mapping is passed to class apply it here.\n                y = np.array([self.label_encoder[x] for x in y])\n                y_pred = np.array([self.label_encoder[x] for x in y_pred])\n\n        # Compute the confusion matrix and class counts\n        self.confusion_matrix_ = confusion_matrix_metric(\n            y, y_pred, labels=self.classes_, sample_weight=self.sample_weight\n        )\n        self.class_counts_ = self.class_counts(y)\n\n        # Make array of only the classes actually being used.\n        # Needed because sklearn confusion_matrix only returns counts for\n        # selected classes but percent should be calculated on all classes\n        selected_class_counts = []\n        for c in self.classes_:\n            try:\n                selected_class_counts.append(self.class_counts_[c])\n            except KeyError:\n                selected_class_counts.append(0)\n        self.class_counts_ = np.array(selected_class_counts)\n\n        self.draw()\n\n        # Retrieve and store the score attribute from the sklearn classifier\n        self.score_ = self.estimator.score(X, y)\n\n        return self.score_",
        "sampled": "```python\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nclass ConfusionMatrix:\n    def __init__(self, X, y):\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2)\n\n    def score(self):\n        \"\"\"\n        Draws a confusion matrix based on the test data supplied by comparing\n        predictions on instances X with the true"
    },
    {
        "original": "def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        \"\"\"\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        \"\"\"\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset['annotations']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset['annotations']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]\n        else:\n            ids = [ann['id'] for ann in anns]\n        return ids",
        "sampled": "```python\nclass AnnotationGetter:\n    def getAnnIds(self, imgIds=None, catIds=None, areaRng=None, iscrowd=None):\n        \"\"\"\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given"
    },
    {
        "original": "def get_services(profile='pagerduty', subdomain=None, api_key=None):\n    \"\"\"\n    List services belonging to this account\n\n    CLI Example:\n\n        salt myminion pagerduty.get_services\n    \"\"\"\n\n    return _list_items(\n        'services',\n        'id',\n        profile=profile,\n        subdomain=subdomain,\n        api_key=api_key,\n    )",
        "sampled": "```python\nimport requests\nimport json\n\ndef get_services(profile='pagerduty', subdomain=None, api_key=None):\n    \"\"\"\n    List services belonging to this account\n\n    CLI Example:\n\n        salt myminion pagerduty.get_services\n    \"\"\"\n\n    # Base URL for PagerDuty API\n    base_url = \"https://api.pagerduty.com\"\n\n    # Construct API endpoint for services\n    endpoint = \"/services\"\n\n    # If subdomain and api_key are"
    },
    {
        "original": "def dumps(self, fd, **kwargs):\n        \"\"\"\n        Returns the concrete content for a file descriptor.\n\n        BACKWARD COMPATIBILITY: if you ask for file descriptors 0 1 or 2, it will return the data from stdin, stdout,\n        or stderr as a flat string.\n\n        :param fd:  A file descriptor.\n        :return:    The concrete content.\n        :rtype:     str\n        \"\"\"\n        if 0 <= fd <= 2:\n            data = [self.stdin, self.stdout, self.stderr][fd].concretize(**kwargs)\n            if type(data) is list:\n                data = b''.join(data)\n            return data\n        return self.get_fd(fd).concretize(**kwargs)",
        "sampled": "```python\nclass ContentDumper:\n    def dumps(self, fd, **kwargs):\n        \"\"\"\n        Returns the concrete content for a file descriptor.\n\n        BACKWARD COMPATIBILITY: if you ask for file descriptors 0 1 or 2, it will return the data from stdin, stdout,\n        or stderr as a flat string.\n\n        :param fd: A file descriptor.\n        :return: The concrete content.\n        :rtype: str\n        \"\"\"\n        \n        # Map of standard"
    },
    {
        "original": "def get_carrier_concentration(self):\n        \"\"\"\n        gives the carrier concentration (in cm^-3)\n\n        Returns\n            a dictionary {temp:[]} with an array of carrier concentration\n            (in cm^-3) at each temperature\n            The array relates to each step of electron chemical potential\n        \"\"\"\n\n        return {temp: [1e24 * i / self.vol for i in self._carrier_conc[temp]]\n                for temp in self._carrier_conc}",
        "sampled": "```python\nimport numpy as np\n\nclass CarrierConcentrationCalculator:\n    def __init__(self, n_i, T_min, T_max, step):\n        \"\"\"\n        Initialize the calculator with intrinsic carrier concentration (n_i) and temperature range.\n\n        Args:\n            n_i (float): Intrinsic carrier concentration (in cm^-3)\n            T_min (float): Minimum temperature (in K)\n            T_max (float): Maximum temperature (in K)\n            step (float): Temperature step"
    },
    {
        "original": "def _stack_values_to_string(self, stack_values):\n        \"\"\"\n        Convert each stack value to a string\n\n        :param stack_values: A list of values\n        :return: The converted string\n        \"\"\"\n\n        strings = [ ]\n        for stack_value in stack_values:\n            if self.solver.symbolic(stack_value):\n                concretized_value = \"SYMBOLIC - %s\" % repr(stack_value)\n            else:\n                if len(self.solver.eval_upto(stack_value, 2)) == 2:\n                    concretized_value = repr(stack_value)\n                else:\n                    concretized_value = repr(stack_value)\n            strings.append(concretized_value)\n\n        return \" .. \".join(strings)",
        "sampled": "```python\nclass Solution:\n    def _stack_values_to_string(self, stack_values):\n        \"\"\"\n        Convert each stack value to a string\n\n        :param stack_values: A list of values\n        :return: The converted string\n        \"\"\"\n        return ', '.join(map(str, stack_values))\n```"
    },
    {
        "original": "def create_information_tear_sheet(factor_data,\n                                  group_neutral=False,\n                                  by_group=False):\n    \"\"\"\n    Creates a tear sheet for information analysis of a factor.\n\n    Parameters\n    ----------\n    factor_data : pd.DataFrame - MultiIndex\n        A MultiIndex DataFrame indexed by date (level 0) and asset (level 1),\n        containing the values for a single alpha factor, forward returns for\n        each period, the factor quantile/bin that factor value belongs to, and\n        (optionally) the group the asset belongs to.\n        - See full explanation in utils.get_clean_factor_and_forward_returns\n    group_neutral : bool\n        Demean forward returns by group before computing IC.\n    by_group : bool\n        If True, display graphs separately for each group.\n    \"\"\"\n\n    ic = perf.factor_information_coefficient(factor_data, group_neutral)\n\n    plotting.plot_information_table(ic)\n\n    columns_wide = 2\n    fr_cols = len(ic.columns)\n    rows_when_wide = (((fr_cols - 1) // columns_wide) + 1)\n    vertical_sections = fr_cols + 3 * rows_when_wide + 2 * fr_cols\n    gf = GridFigure(rows=vertical_sections, cols=columns_wide)\n\n    ax_ic_ts = [gf.next_row() for _ in range(fr_cols)]\n    plotting.plot_ic_ts(ic, ax=ax_ic_ts)\n\n    ax_ic_hqq = [gf.next_cell() for _ in range(fr_cols * 2)]\n    plotting.plot_ic_hist(ic, ax=ax_ic_hqq[::2])\n    plotting.plot_ic_qq(ic, ax=ax_ic_hqq[1::2])\n\n    if not by_group:\n\n        mean_monthly_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=False,\n                                              by_time=\"M\")\n        ax_monthly_ic_heatmap = [gf.next_cell() for x in range(fr_cols)]\n        plotting.plot_monthly_ic_heatmap(mean_monthly_ic,\n                                         ax=ax_monthly_ic_heatmap)\n\n    if by_group:\n        mean_group_ic = \\\n            perf.mean_information_coefficient(factor_data,\n                                              group_adjust=group_neutral,\n                                              by_group=True)\n\n        plotting.plot_ic_by_group(mean_group_ic, ax=gf.next_row())\n\n    plt.show()\n    gf.close()",
        "sampled": "```python\nimport pandas as pd\n\ndef create_information_tear_sheet(factor_data,\n                                  group_neutral=False,\n                                  by_group=False):\n    # Step 1: Group the factor data by asset\n    grouped_factor_data = factor_data.groupby(level=1)\n    \n    # Step 2: If by_group is True, calculate IC for each group separately\n    if by_group:\n        if not group_neutral:\n            ic_by_group = grouped_factor_data.apply(lambda x: x.ic())\n"
    },
    {
        "original": "def _build(self):\n    \"\"\"Returns a tuple containing observation and target one-hot tensors.\"\"\"\n    q = tf.FIFOQueue(\n        self._queue_capacity, [self._dtype, self._dtype],\n        shapes=[[self._num_steps, self._batch_size, self._vocab_size]]*2)\n    obs, target = tf.py_func(self._get_batch, [], [tf.int32, tf.int32])\n    obs = self._one_hot(obs)\n    target = self._one_hot(target)\n    enqueue_op = q.enqueue([obs, target])\n    obs, target = q.dequeue()\n    tf.train.add_queue_runner(tf.train.QueueRunner(q, [enqueue_op]))\n    return SequenceDataOpsNoMask(obs, target)",
        "sampled": "```python\nimport torch\nimport numpy as np\n\nclass ExampleClass:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n\n    def _build(self):\n        # Generate observation tensor with dimensions (num_classes, )\n        observation_tensor = torch.tensor(np.random.randint(0, 2, self.num_classes))\n\n        # Generate target one-hot tensor with dimensions (num_classes, )\n        target_one_hot_tensor = torch.zeros(self.num_classes)\n        target_class_index"
    },
    {
        "original": "def dyndns_add(nameserver, name, rdata, type=\"A\", ttl=10):\n    \"\"\"Send a DNS add message to a nameserver for \"name\" to have a new \"rdata\"\ndyndns_add(nameserver, name, rdata, type=\"A\", ttl=10) -> result code (0=ok)\n\nexample: dyndns_add(\"ns1.toto.com\", \"dyn.toto.com\", \"127.0.0.1\")\nRFC2136\n\"\"\"\n    zone = name[name.find(\".\") + 1:]\n    r = sr1(IP(dst=nameserver) / UDP() / DNS(opcode=5,\n                                             qd=[DNSQR(qname=zone, qtype=\"SOA\")],  # noqa: E501\n                                             ns=[DNSRR(rrname=name, type=\"A\",\n                                                       ttl=ttl, rdata=rdata)]),\n            verbose=0, timeout=5)\n    if r and r.haslayer(DNS):\n        return r.getlayer(DNS).rcode\n    else:\n        return -1",
        "sampled": "```python\nimport dns.query\nfrom dns import message, name\n\ndef dyndns_add(nameserver, name, rdata, type=\"A\", ttl=10):\n    \"\"\"\n    Send a DNS add message to a nameserver for \"name\" to have a new \"rdata\"\n\n    Args:\n        nameserver (str): The DNS server to send the request to.\n        name (str): The domain name to update.\n        rdata (str): The IP address of the record"
    },
    {
        "original": "def string_asset(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the StringAsset class. Registers the decorated class\n    as the StringAsset known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _string_asset_resource_type\n    _string_asset_resource_type = class_obj\n    return class_obj",
        "sampled": "```python\ndef string_asset(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the StringAsset class. Registers the decorated class\n    as the StringAsset known type.\n    \"\"\"\n    class_obj.__string_asset__ = True\n    return class_obj\n\n# Example usage:\n@string_asset\nclass StringAsset:\n    pass\n\nprint(StringAsset.__string_asset__)  # Output: True\n```"
    },
    {
        "original": "def group(self, id, expand=None):\n        \"\"\"Get a group Resource from the server.\n\n        :param id: ID of the group to get\n        :param id: str\n        :param expand: Extra information to fetch inside each resource\n        :type expand: Optional[Any]\n\n        :rtype: User\n        \"\"\"\n        group = Group(self._options, self._session)\n        params = {}\n        if expand is not None:\n            params['expand'] = expand\n        group.find(id, params=params)\n        return group",
        "sampled": "```python\nclass Resource:\n    def __init__(self, id, name):\n        self.id = id\n        self.name = name\n\nclass User(Resource):\n    def __init__(self, id, name, group_id):\n        super().__init__(id, name)\n        self.group_id = group_id\n\nclass Group(Resource):\n    def __init__(self, id, name, users=None):\n        super().__init__(id, name)\n        self.users = users if users else []\n\n"
    },
    {
        "original": "def _prepare_for_training(self, job_name=None):\n        \"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\n\n        Args:\n            * job_name (str): Name of the training job to be created. If not specified, one is generated,\n                using the base name given to the constructor if applicable.\n        \"\"\"\n        super(Framework, self)._prepare_for_training(job_name=job_name)\n\n        # validate source dir will raise a ValueError if there is something wrong with the\n        # source directory. We are intentionally not handling it because this is a critical error.\n        if self.source_dir and not self.source_dir.lower().startswith('s3://'):\n            validate_source_dir(self.entry_point, self.source_dir)\n\n        # if we are in local mode with local_code=True. We want the container to just\n        # mount the source dir instead of uploading to S3.\n        local_code = get_config_value('local.local_code', self.sagemaker_session.config)\n        if self.sagemaker_session.local_mode and local_code:\n            # if there is no source dir, use the directory containing the entry point.\n            if self.source_dir is None:\n                self.source_dir = os.path.dirname(self.entry_point)\n            self.entry_point = os.path.basename(self.entry_point)\n\n            code_dir = 'file://' + self.source_dir\n            script = self.entry_point\n        else:\n            self.uploaded_code = self._stage_user_code_in_s3()\n            code_dir = self.uploaded_code.s3_prefix\n            script = self.uploaded_code.script_name\n\n        # Modify hyperparameters in-place to point to the right code directory and script URIs\n        self._hyperparameters[DIR_PARAM_NAME] = code_dir\n        self._hyperparameters[SCRIPT_PARAM_NAME] = script\n        self._hyperparameters[CLOUDWATCH_METRICS_PARAM_NAME] = self.enable_cloudwatch_metrics\n        self._hyperparameters[CONTAINER_LOG_LEVEL_PARAM_NAME] = self.container_log_level\n        self._hyperparameters[JOB_NAME_PARAM_NAME] = self._current_job_name\n        self._hyperparameters[SAGEMAKER_REGION_PARAM_NAME] = self.sagemaker_session.boto_region_name",
        "sampled": "```python\ndef _prepare_for_training(self, job_name=None):\n    \"\"\"\n    Set hyperparameters needed for training. This method will also validate ``source_dir``.\n\n    Args:\n        * job_name (str): Name of the training job to be created. If not specified, one is generated,\n            using the base name given to the constructor if applicable.\n    \"\"\"\n    \n    # If job_name is not provided, generate one based on the base name given to the constructor\n    if job"
    },
    {
        "original": "def nice_output(self):\n        \"\"\"Return a string for printing\"\"\"\n        dates = [\n            str_format('Opening Day {0}: {1}.',\n                       [self.year, date_format(self.first_date_seas)]),\n            str_format('Last day of the 1st half: {0}.',\n                       [date_format(self.last_date_1sth)]),\n            str_format('{0} All Star Game: {1}.',\n                       [self.year, date_format(self.all_star_date)]),\n            str_format('First day of the 2nd half: {}.',\n                       [date_format(self.first_date_2ndh)]),\n            str_format('Last day of the {0} season: {1}.',\n                       [self.year, date_format(self.last_date_seas)]),\n            str_format('{0} Playoffs start: {1}.',\n                       [self.year, date_format(self.playoffs_start_date)]),\n            str_format('{0} Playoffs end: {1}.',\n                       [self.year, date_format(self.playoffs_end_date)])\n        ]\n        return '\\n'.join(dates)",
        "sampled": "```python\nclass Solution:\n    def nice_output(self):\n        \"\"\"Return a string for printing\"\"\"\n        \n        # Problem Description\n        # Given an integer n, return the number of ways to arrange the digits of the given number in decreasing order, ignoring leading zeros.\n        \n        # Input and Output Specifications\n        # Input: n - an integer\n        # Output: The number of ways to arrange the digits in decreasing order\n        \n    def countArrangement(self, n):\n        \n        def is"
    },
    {
        "original": "def find(self, selector, collation=None):\n        \"\"\"Specify selection criteria for bulk operations.\n\n        :Parameters:\n          - `selector` (dict): the selection criteria for update\n            and remove operations.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only\n            supported on MongoDB 3.4 and above.\n\n        :Returns:\n          - A :class:`BulkWriteOperation` instance, used to add\n            update and remove operations to this bulk operation.\n\n        .. versionchanged:: 3.4\n           Added the `collation` option.\n\n        \"\"\"\n        validate_is_mapping(\"selector\", selector)\n        return BulkWriteOperation(selector, self.__bulk, collation)",
        "sampled": "```python\nfrom pymongo import MongoClient\nimport pymongo\n\n\nclass BulkWriteOperation:\n    def __init__(self, collection):\n        self.collection = collection\n        self.operations = []\n\n    def add_update_one(self, filter, update):\n        \"\"\"Add an UpdateOne operation to the bulk write.\"\"\"\n        self.operations.append({\"updateOne\": {\"filter\": filter, \"updates\": update}})\n\n    def add_delete_one(self, filter):\n        \"\"\"Add a DeleteOne operation to the bulk write.\"\"\"\n       "
    },
    {
        "original": "def finalize(self, **kwargs):\n        \"\"\"\n        The finalize method executes any subclass-specific axes\n        finalization steps. The user calls poof & poof calls finalize.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        \"\"\"\n        # Set the title\n        self.set_title(\n            'Frequency Distribution of Top {} tokens'.format(self.N)\n        )\n\n        # Create the vocab, count, and hapaxes labels\n        infolabel = \"vocab: {:,}\\nwords: {:,}\\nhapax: {:,}\".format(\n            self.vocab_, self.words_, self.hapaxes_\n        )\n\n        self.ax.text(0.68, 0.97, infolabel, transform=self.ax.transAxes,\n                     fontsize=9, verticalalignment='top',\n                     bbox={'boxstyle':'round', 'facecolor':'white', 'alpha':.8})\n\n        # Set the legend and the grid\n        self.ax.legend(loc='upper right', frameon=True)",
        "sampled": "```python\nclass Poof:\n    def __init__(self):\n        self.axes = []\n\n    def add_axis(self, axis):\n        self.axes.append(axis)\n\n    def finalize(self, **kwargs):\n        \"\"\"\n        The finalize method executes any subclass-specific axes\n        finalization steps. The user calls poof & poof calls finalize.\n\n        Parameters\n        ----------\n        kwargs: generic keyword arguments.\n\n        \"\"\"\n        for axis in self.axes:\n            axis.finalize(**kwargs)\n\n\nclass Axis:\n"
    },
    {
        "original": "def post_dump(fn=None, pass_many=False, pass_original=False):\n    \"\"\"Register a method to invoke after serializing an object. The method\n    receives the serialized object and returns the processed object.\n\n    By default, receives a single object at a time, transparently handling the ``many``\n    argument passed to the Schema. If ``pass_many=True``, the raw data\n    (which may be a collection) and the value for ``many`` is passed.\n\n    If ``pass_original=True``, the original data (before serializing) will be passed as\n    an additional argument to the method.\n    \"\"\"\n    return set_hook(fn, (POST_DUMP, pass_many), pass_original=pass_original)",
        "sampled": "```python\nclass Schema:\n    def __init__(self):\n        self.post_dump_methods = []\n\n    def post_dump(self, fn=None, pass_many=False, pass_original=False):\n        \"\"\"Register a method to invoke after serializing an object.\"\"\"\n        self.post_dump_methods.append((fn, pass_many, pass_original))\n\n    def serialize(self, obj):\n        \"\"\"Serialize an object.\"\"\"\n        # For simplicity, let's assume we're serializing a list of objects\n        serialized_obj = ["
    },
    {
        "original": "def fwd_chunk(self):\n        \"\"\"\n        Returns the chunk following this chunk in the list of free chunks. If this chunk is not free, then it resides in\n        no such list and this method raises an error.\n\n        :returns: If possible, the forward chunk; otherwise, raises an error\n        \"\"\"\n        if self.is_free():\n            base = self.state.memory.load(self.base + 2 * self._chunk_size_t_size, self._chunk_size_t_size)\n            return PTChunk(base, self.state)\n        else:\n            raise SimHeapError(\"Attempted to access the forward chunk of an allocated chunk\")",
        "sampled": "```python\nclass Chunk:\n    def __init__(self, free=False):\n        self.free = free\n        self.next = None\n\n    def fwd_chunk(self):\n        \"\"\"\n        Returns the chunk following this chunk in the list of free chunks. If this chunk is not free, then it resides in\n        no such list and this method raises an error.\n\n        :returns: If possible, the forward chunk; otherwise, raises an error\n        \"\"\"\n        \n        # Check if the current chunk"
    },
    {
        "original": "def _validate(dns_proto, dns_servers, ip_proto, ip_addrs, gateway):\n    \"\"\"\n    Ensure that the configuration passed is formatted correctly and contains\n    valid IP addresses, etc.\n    \"\"\"\n    errors = []\n    # Validate DNS configuration\n    if dns_proto == 'dhcp':\n        if dns_servers is not None:\n            errors.append(\n                'The dns_servers param cannot be set if unless dns_proto is '\n                'set to \\'static\\''\n            )\n    else:\n        if str(dns_servers).lower() in ['none', '[]']:\n            pass\n        elif not isinstance(dns_servers, list):\n            errors.append(\n                'The dns_servers param must be formatted as a list'\n            )\n        else:\n            bad_ips = [x for x in dns_servers\n                       if not salt.utils.validate.net.ipv4_addr(x)]\n            if bad_ips:\n                errors.append('Invalid DNS server IPs: {0}'\n                              .format(', '.join(bad_ips)))\n\n    # Validate IP configuration\n    if ip_proto == 'dhcp':\n        if ip_addrs is not None:\n            errors.append(\n                'The ip_addrs param cannot be set if unless ip_proto is set '\n                'to \\'static\\''\n            )\n        if gateway is not None:\n            errors.append(\n                'A gateway IP cannot be set if unless ip_proto is set to '\n                '\\'static\\''\n            )\n    else:\n        if not ip_addrs:\n            errors.append(\n                'The ip_addrs param is required to set static IPs'\n            )\n        elif not isinstance(ip_addrs, list):\n            errors.append(\n                'The ip_addrs param must be formatted as a list'\n            )\n        else:\n            bad_ips = [x for x in ip_addrs\n                       if not salt.utils.validate.net.ipv4_addr(x)]\n            if bad_ips:\n                errors.append('The following static IPs are invalid: '\n                              '{0}'.format(', '.join(bad_ips)))\n\n            # Validate default gateway\n            if gateway is not None:\n                if not salt.utils.validate.net.ipv4_addr(gateway):\n                    errors.append('Gateway IP {0} is invalid'.format(gateway))\n\n    return errors",
        "sampled": "```python\nimport ipaddress\n\ndef _validate(dns_proto, dns_servers, ip_proto, ip_addrs, gateway):\n    \"\"\"\n    Ensure that the configuration passed is formatted correctly and contains\n    valid IP addresses, etc.\n    \"\"\"\n\n    # Check if dns_proto is a string\n    if not isinstance(dns_proto, str):\n        raise ValueError(\"dns_proto must be a string\")\n\n    # Check if dns_servers is a list of strings\n    if not isinstance(dns_servers,"
    },
    {
        "original": "def create_and_link_vrf_table(self, vrf_conf):\n        \"\"\"Factory method to create VRF table for given `vrf_conf`.\n\n        Adds mapping to this table with appropriate scope. Also, adds mapping\n        for import RT of this VRF to created table to facilitate\n        importing/installing of paths from global tables.\n        Returns created table.\n        \"\"\"\n        route_family = vrf_conf.route_family\n\n        if route_family == VRF_RF_IPV4:\n            vrf_table = Vrf4Table\n        elif route_family == VRF_RF_IPV6:\n            vrf_table = Vrf6Table\n        elif route_family == VRF_RF_L2_EVPN:\n            vrf_table = VrfEvpnTable\n        elif route_family == VRF_RF_IPV4_FLOWSPEC:\n            vrf_table = Vrf4FlowSpecTable\n        elif route_family == VRF_RF_IPV6_FLOWSPEC:\n            vrf_table = Vrf6FlowSpecTable\n        elif route_family == VRF_RF_L2VPN_FLOWSPEC:\n            vrf_table = L2vpnFlowSpecTable\n        else:\n            raise ValueError('Unsupported route family for VRF: %s' %\n                             route_family)\n\n        vrf_table = vrf_table(vrf_conf, self._core_service, self._signal_bus)\n        table_id = (vrf_conf.route_dist, route_family)\n        self._tables[table_id] = vrf_table\n\n        assert vrf_table is not None\n        LOG.debug('Added new VrfTable with route_dist:%s and route_family:%s',\n                  vrf_conf.route_dist, route_family)\n\n        import_rts = vrf_conf.import_rts\n        # If VRF is configured with import RT, we put this table\n        # in a list corresponding to this RT for easy access.\n        if import_rts:\n            self._link_vrf_table(vrf_table, import_rts)\n\n        return vrf_table",
        "sampled": "```python\nclass VRFTableFactory:\n    def create_and_link_vrf_table(self, vrf_conf):\n        \"\"\"\n        Factory method to create VRF table for given `vrf_conf`.\n\n        Adds mapping to this table with appropriate scope. Also, adds mapping\n        for import RT of this VRF to created table to facilitate\n        importing/installing of paths from global tables.\n        \n        Args:\n            vrf_conf (dict): configuration for VRF\n\n        Returns:\n            dict"
    },
    {
        "original": "def seek_to_end(self, *partitions):\n        \"\"\"Seek to the most recent available offset for partitions.\n\n        Arguments:\n            *partitions: Optionally provide specific TopicPartitions, otherwise\n                default to all assigned partitions.\n\n        Raises:\n            AssertionError: If any partition is not currently assigned, or if\n                no partitions are assigned.\n        \"\"\"\n        if not all([isinstance(p, TopicPartition) for p in partitions]):\n            raise TypeError('partitions must be TopicPartition namedtuples')\n        if not partitions:\n            partitions = self._subscription.assigned_partitions()\n            assert partitions, 'No partitions are currently assigned'\n        else:\n            for p in partitions:\n                assert p in self._subscription.assigned_partitions(), 'Unassigned partition'\n\n        for tp in partitions:\n            log.debug(\"Seeking to end of partition %s\", tp)\n            self._subscription.need_offset_reset(tp, OffsetResetStrategy.LATEST)",
        "sampled": "```python\nclass TopicPartition:\n    def __init__(self, topic, partition):\n        self.topic = topic\n        self.partition = partition\n        self.offset = 0  # Initialize offset to 0\n\nclass SeekToEnd:\n    def __init__(self):\n        self.topic_partitions = {}  # Dictionary to store topic partitions\n\n    def assign_partition(self, topic, partition):\n        \"\"\"Assign a partition to a topic.\n\n        Arguments:\n            topic (str): Topic name.\n"
    },
    {
        "original": "def check_quota(self):\n        \"\"\"\n        Check whether the user is within quota.  Should be called before\n        every write.  Will raise() if the library has exceeded its allotted\n        quota.\n        \"\"\"\n        # Don't check on every write, that would be slow\n        if self.quota_countdown > 0:\n            self.quota_countdown -= 1\n            return\n\n        # Re-cache the quota after the countdown\n        self.quota = self.get_library_metadata(ArcticLibraryBinding.QUOTA)\n        if self.quota is None or self.quota == 0:\n            self.quota = 0\n            return\n\n        # Figure out whether the user has exceeded their quota\n        library = self.arctic[self.get_name()]\n        stats = library.stats()\n\n        def to_gigabytes(bytes_):\n            return bytes_ / 1024. / 1024. / 1024.\n\n        # Have we exceeded our quota?\n        size = stats['totals']['size']\n        count = stats['totals']['count']\n        if size >= self.quota:\n            raise QuotaExceededException(\"Mongo Quota Exceeded: %s %.3f / %.0f GB used\" % (\n                '.'.join([self.database_name, self.library]),\n                to_gigabytes(size),\n                to_gigabytes(self.quota)))\n\n        # Quota not exceeded, print an informational message and return\n        try:\n            avg_size = size // count if count > 1 else 100 * 1024\n            remaining = self.quota - size\n            remaining_count = remaining / avg_size\n            if remaining_count < 100 or float(remaining) / self.quota < 0.1:\n                logger.warning(\"Mongo Quota: %s %.3f / %.0f GB used\" % (\n                    '.'.join([self.database_name, self.library]),\n                    to_gigabytes(size),\n                    to_gigabytes(self.quota)))\n            else:\n                logger.info(\"Mongo Quota: %s %.3f / %.0f GB used\" % (\n                    '.'.join([self.database_name, self.library]),\n                    to_gigabytes(size),\n                    to_gigabytes(self.quota)))\n\n            # Set-up a timer to prevent us for checking for a few writes.\n            # This will check every average half-life\n            self.quota_countdown = int(max(remaining_count // 2, 1))\n        except Exception as e:\n            logger.warning(\"Encountered an exception while calculating quota statistics: %s\" % str(e))",
        "sampled": "```python\nclass QuotaChecker:\n    def __init__(self, quota):\n        \"\"\"\n        Initialize the QuotaChecker with a given quota.\n        \n        Args:\n            quota (int): The maximum number of writes allowed.\n        \"\"\"\n        self.quota = quota\n        self.writes = 0\n\n    def check_quota(self):\n        \"\"\"\n        Check whether the user is within quota.  Should be called before\n        every write.  Will raise() if the library has exceeded its"
    },
    {
        "original": "def repr_size(n_bytes):\n    \"\"\"\n    >>> repr_size(1000)\n    '1000 Bytes'\n    >>> repr_size(8257332324597)\n    '7.5 TiB'\n    \"\"\"\n    if n_bytes < 1024:\n        return '{0} Bytes'.format(n_bytes)\n    i = -1\n    while n_bytes > 1023:\n        n_bytes /= 1024.0\n        i += 1\n    return '{0} {1}iB'.format(round(n_bytes, 1), si_prefixes[i])",
        "sampled": "```python\ndef repr_size(n_bytes):\n    si_prefixes = (\"Bytes\", \"KiB\", \"MiB\", \"GiB\", \"TiB\", \"PiB\")\n    i = 0\n    while n_bytes >= 1024 and i < len(si_prefixes) - 1:\n        n_bytes /= 1024\n        i += 1\n    return \"{} {}\".format(round(n_bytes, 2), si_prefixes[i])\n```"
    },
    {
        "original": "def get(self, key, default='', stringify=True):\n    \"\"\"Returns dictionary values or default.\n\n    Args:\n      key: string. Dictionary key to look up.\n      default: string. Return this value if key not found.\n      stringify: bool. Force all return values to string for compatibility\n                 reasons.\n    Returns:\n      python-wrapped CF object or default if not found.\n    \"\"\"\n    obj = self.__getitem__(key)\n    if obj is None:\n      obj = default\n    elif stringify:\n      obj = str(obj)\n    return obj",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        # Initialize required variables here\n        self.data_structure = {}  # Replace with actual data structure\n\n    def get(self, key, default='', stringify=True):\n        \"\"\"\n        Returns dictionary values or default.\n\n        Args:\n          key (str): Dictionary key to look up.\n          default (str): Return this value if key not found.\n          stringify (bool): Force all return values to string for compatibility reasons.\n\n        Returns:\n"
    },
    {
        "original": "def getImageDescriptor(self, im, xy=None):\n        \"\"\" getImageDescriptor(im, xy=None)\n\n        Used for the local color table properties per image.\n        Otherwise global color table applies to all frames irrespective of\n        whether additional colors comes in play that require a redefined\n        palette. Still a maximum of 256 color per frame, obviously.\n\n        Written by Ant1 on 2010-08-22\n        Modified by Alex Robinson in Janurari 2011 to implement subrectangles.\n\n        \"\"\"\n\n        # Defaule use full image and place at upper left\n        if xy is None:\n            xy = (0, 0)\n\n        # Image separator,\n        bb = '\\x2C'\n\n        # Image position and size\n        bb += intToBin(xy[0])  # Left position\n        bb += intToBin(xy[1])  # Top position\n        bb += intToBin(im.size[0])  # image width\n        bb += intToBin(im.size[1])  # image height\n\n        # packed field: local color table flag1, interlace0, sorted table0,\n        # reserved00, lct size111=7=2^(7+1)=256.\n\n        bb += '\\x87'\n\n        # LZW minimum size code now comes later, begining of [image data] blocks\n        return bb",
        "sampled": "```python\ndef getImageDescriptor(self, im, xy=None):\n    \"\"\"\n    Get the image descriptor for local color table properties per image.\n\n    Args:\n        im (numpy array): The input image.\n        xy (tuple, optional): Specify a subrectangle of the image. Defaults to None.\n\n    Returns:\n        A dictionary containing the descriptor for the given subrectangle.\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the descriptor\n    desc = {}\n    \n    # Get the dimensions of"
    },
    {
        "original": "def CheckClientApprovalRequest(approval_request):\n  \"\"\"Checks if a client approval request is granted.\"\"\"\n\n  _CheckExpired(approval_request)\n  _CheckHasEnoughGrants(approval_request)\n\n  if not client_approval_auth.CLIENT_APPROVAL_AUTH_MGR.IsActive():\n    return True\n\n  token = access_control.ACLToken(username=approval_request.requestor_username)\n  approvers = set(g.grantor_username for g in approval_request.grants)\n\n  labels = sorted(\n      data_store.REL_DB.ReadClientLabels(approval_request.subject_id),\n      key=lambda l: l.name)\n  for label in labels:\n    client_approval_auth.CLIENT_APPROVAL_AUTH_MGR.CheckApproversForLabel(\n        token, rdfvalue.RDFURN(approval_request.subject_id),\n        approval_request.requestor_username, approvers, label.name)\n\n  return True",
        "sampled": "```python\ndef CheckClientApprovalRequest(approval_request):\n    \"\"\"\n    Checks if a client approval request is granted.\n\n    Args:\n        approval_request (dict): A dictionary containing the client's approval request.\n            It should have the following keys:\n                - 'client_id': The ID of the client making the request.\n                - 'request_type': The type of request being made (e.g., 'loan', 'credit_card', etc.).\n                - 'amount': The amount being"
    },
    {
        "original": "def AdjustDescriptor(self, fields):\n    \"\"\"Payload-aware metadata processor.\"\"\"\n\n    for f in fields:\n      if f.name == \"args_rdf_name\":\n        f.name = \"payload_type\"\n\n      if f.name == \"args\":\n        f.name = \"payload\"\n\n    return fields",
        "sampled": "```python\nclass MetadataProcessor:\n    def AdjustDescriptor(self, fields):\n        \"\"\"\n        Payload-aware metadata processor.\n        \n        The function takes a dictionary of fields as input, \n        processes it, and returns the result. This processor \n        is likely used for JSON or other structured data.\n        \n        Parameters:\n        fields (dict): Input dictionary of metadata fields.\n    \n        Returns:\n    \"\"\"\n    \n    # Iterate over each field in the input dictionary.\n    for key in list(fields.keys()):\n"
    },
    {
        "original": "def on_change(self, server_description):\n        \"\"\"Process a new ServerDescription after an ismaster call completes.\"\"\"\n        # We do no I/O holding the lock.\n        with self._lock:\n            # Any monitored server was definitely in the topology description\n            # once. Check if it's still in the description or if some state-\n            # change removed it. E.g., we got a host list from the primary\n            # that didn't include this server.\n            if self._description.has_server(server_description.address):\n                td_old = self._description\n                if self._publish_server:\n                    old_server_description = td_old._server_descriptions[\n                        server_description.address]\n                    self._events.put((\n                        self._listeners.publish_server_description_changed,\n                        (old_server_description, server_description,\n                         server_description.address, self._topology_id)))\n\n                self._description = updated_topology_description(\n                    self._description, server_description)\n\n                self._update_servers()\n\n                if self._publish_tp:\n                    self._events.put((\n                        self._listeners.publish_topology_description_changed,\n                        (td_old, self._description, self._topology_id)))\n\n                # Wake waiters in select_servers().\n                self._condition.notify_all()",
        "sampled": "```python\nfrom typing import List, Dict\n\nclass Server:\n    def __init__(self, description: Dict[str, str]):\n        self.description = description\n        self.connections = []\n\n    def add_connection(self, connection: 'Server'):\n        self.connections.append(connection)\n        \n    def get_details(self):\n        # Write the logic to extract and return server details here\n\n\nclass Group:\n    def __init__(self):\n        self.servers = []\n\n    def add_server(self, server: Server):\n"
    },
    {
        "original": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered",
        "sampled": "```python\ndef _filter_cache(self, dmap, kdims):\n    \"\"\"\n    Returns a filtered version of the DynamicMap cache leaving only\n    keys consistently with the newly specified values\n    \"\"\"\n    # Get the keys from the DynamicMap cache\n    cache_keys = list(dmap.cache.keys())\n    \n    # Initialize an empty dictionary to store the filtered cache\n    filtered_cache = {}\n    \n    # Iterate over each key in the cache\n    for key in cache_keys:\n        #"
    },
    {
        "original": "def _add_item(self, item, indent_amt):\n        \"\"\"Add an item to the line.\n\n        Reflow the line to get the best formatting after the item is\n        inserted. The bracket depth indicates if the item is being\n        inserted inside of a container or not.\n\n        \"\"\"\n        if self._prev_item and self._prev_item.is_string and item.is_string:\n            # Place consecutive string literals on separate lines.\n            self._lines.append(self._LineBreak())\n            self._lines.append(self._Indent(indent_amt))\n\n        item_text = unicode(item)\n        if self._lines and self._bracket_depth:\n            # Adding the item into a container.\n            self._prevent_default_initializer_splitting(item, indent_amt)\n\n            if item_text in '.,)]}':\n                self._split_after_delimiter(item, indent_amt)\n\n        elif self._lines and not self.line_empty():\n            # Adding the item outside of a container.\n            if self.fits_on_current_line(len(item_text)):\n                self._enforce_space(item)\n\n            else:\n                # Line break for the new item.\n                self._lines.append(self._LineBreak())\n                self._lines.append(self._Indent(indent_amt))\n\n        self._lines.append(item)\n        self._prev_item, self._prev_prev_item = item, self._prev_item\n\n        if item_text in '([{':\n            self._bracket_depth += 1\n\n        elif item_text in '}])':\n            self._bracket_depth -= 1\n            assert self._bracket_depth >= 0",
        "sampled": "```python\nfrom collections import deque\n\nclass ModifyLine:\n    def __init__(self):\n        self.line = ''\n        self.symbols = [' ', '(', ')', '[', ']']\n        self.prev_bracket_depths = deque([0])\n\n    def _get_insertion_point(self):\n        if not self.line or (self.prev_bracket_depths[-1] == 0 and not any(c in self.line[-1] for c in ['(', '['])):\n            return -"
    },
    {
        "original": "def create_ipsecpolicy(name, profile=None, **kwargs):\n    \"\"\"\n    Creates a new IPsecPolicy\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutron.create_ipsecpolicy ipsecpolicy-name\n                transform_protocol=esp auth_algorithm=sha1\n                encapsulation_mode=tunnel encryption_algorithm=aes-128\n\n    :param name: Name of the IPSec policy\n    :param transform_protocol: Transform protocol in lowercase,\n            default: esp (Optional)\n    :param auth_algorithm: Authentication algorithm in lowercase,\n            default: sha1 (Optional)\n    :param encapsulation_mode: Encapsulation mode in lowercase,\n            default: tunnel (Optional)\n    :param encryption_algorithm: Encryption algorithm in lowercase,\n            default:aes-128 (Optional)\n    :param pfs: Prefect Forward Security in lowercase,\n            default: group5 (Optional)\n    :param units: IPSec lifetime attribute. default: seconds (Optional)\n    :param value: IPSec lifetime attribute. default: 3600 (Optional)\n    :param profile: Profile to build on (Optional)\n    :return: Created IPSec policy information\n    \"\"\"\n    conn = _auth(profile)\n    return conn.create_ipsecpolicy(name, **kwargs)",
        "sampled": "```python\ndef create_ipsecpolicy(name, profile=None, transform_protocol='esp', auth_algorithm='sha1', encapsulation_mode='tunnel', encryption_algorithm='aes-128', pfs='group5', units='seconds', value=3600, **kwargs):\n    \"\"\"\n    Creates a new IPsecPolicy\n\n    :param name: Name of the IPSec policy\n    :param transform_protocol: Transform protocol in lowercase, default: esp (Optional)\n    :param auth"
    },
    {
        "original": "def IsRunning(self):\n    \"\"\"Returns True if there's a currently running iteration of this job.\"\"\"\n    current_urn = self.Get(self.Schema.CURRENT_FLOW_URN)\n    if not current_urn:\n      return False\n\n    try:\n      current_flow = aff4.FACTORY.Open(\n          urn=current_urn, aff4_type=flow.GRRFlow, token=self.token, mode=\"r\")\n    except aff4.InstantiationError:\n      # This isn't a flow, something went really wrong, clear it out.\n      logging.error(\"Unable to open cron job run: %s\", current_urn)\n      self.DeleteAttribute(self.Schema.CURRENT_FLOW_URN)\n      self.Flush()\n      return False\n\n    return current_flow.GetRunner().IsRunning()",
        "sampled": "```python\nclass Job:\n    def __init__(self):\n        # Variable to keep track of the running state\n        self.running_states = set()\n    \n    def start(self):\n        \"\"\"Start a new iteration.\"\"\"\n        # Add the current time as a starting point for the ongoing execution\n        import time\n        self.running_states.add(time.time())\n    \n    def stop(self):\n        \"\"\"Stop the current iteration.\"\"\"\n        # Remove the current time from ongoing executions if still present.\n        import time"
    },
    {
        "original": "def _api_config_item(self, item):\n        \"\"\"Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        response.content_type = 'application/json; charset=utf-8'\n\n        config_dict = self.config.as_dict()\n        if item not in config_dict:\n            abort(400, \"Unknown configuration item %s\" % item)\n\n        try:\n            # Get the JSON value of the config' dict\n            args_json = json.dumps(config_dict[item])\n        except Exception as e:\n            abort(404, \"Cannot get config item (%s)\" % str(e))\n        return args_json",
        "sampled": "```python\nclass GlancesAPI:\n    def _api_config_item(self, item):\n        \"\"\"\n        Glances API RESTful implementation.\n\n        Return the JSON representation of the Glances configuration item\n        HTTP/200 if OK\n        HTTP/400 if item is not found\n        HTTP/404 if others error\n        \"\"\"\n        # Define a dictionary to store the configuration items\n        config_items = {\n            'cpu': {'enabled': True, 'interval': 1},\n            '"
    },
    {
        "original": "def from_file(cls, filepath):\n        \"\"\"Build a :class:`Product` instance from a filepath.\"\"\"\n        # Find the abinit extension.\n        for i in range(len(filepath)):\n            if filepath[i:] in abi_extensions():\n                ext = filepath[i:]\n                break\n        else:\n            raise ValueError(\"Cannot detect abinit extension in %s\" % filepath)\n\n        return cls(ext, filepath)",
        "sampled": "```python\nclass Product:\n    def __init__(self, name, price):\n        \"\"\"\n        Initialize a Product instance.\n\n        :param name: The product name.\n        :param price: The product price.\n        \"\"\"\n        self.name = name\n        self.price = price\n\n    @classmethod\n    def from_file(cls, filepath):\n        \"\"\"\n        Build a Product instance from a filepath specified as JSON.\n\n        :param cls: The class itself (required by the decorator)\n                and is"
    },
    {
        "original": "def protocols(self):\n        \"\"\"\n        :rtype: dict[int, list of ProtocolAnalyzer]\n        \"\"\"\n        result = {}\n        for i, group in enumerate(self.rootItem.children):\n            result[i] = [child.protocol for child in group.children]\n\n        return result",
        "sampled": "```python\nclass ProtocolAnalyzer:\n    def __init__(self, protocol_id, protocols Analyzed):\n        \"\"\"\n        :type protocol_id: int\n        :type protocols_analyzed: int\n        \"\"\"\n        self.protocol_id = protocol_id\n        self.protocols_analyzed = protocols_analyzed\n\nclass Solution:\n    def __init__(self):\n        self.analyzers = {}\n\n    def add(self, pid, pa):\n        \"\"\"\n         ???\n         \n         :type pid: int\n"
    },
    {
        "original": "def process_element(self, element, key, **params):\n        \"\"\"\n        The process_element method allows a single element to be\n        operated on given an externally supplied key.\n        \"\"\"\n        self.p = param.ParamOverrides(self, params)\n        return self._apply(element, key)",
        "sampled": "```python\nclass Processor:\n    def process_element(self, element, key, **params):\n        \"\"\"\n        The process_element method allows a single element to be\n        operated on given an externally supplied key.\n        \"\"\"\n        \n        # Check if the key is 'double' and the element is a number\n        if key == 'double' and isinstance(element, (int, float)):\n            # If the key is 'double', return the element doubled\n            return element * 2\n        \n       "
    },
    {
        "original": "def remove_bond(self, idx1, idx2):\n        \"\"\"\n        Remove a bond from an openbabel molecule\n\n        Args:\n            idx1: The atom index of one of the atoms participating the in bond\n            idx2: The atom index of the other atom participating in the bond \n        \"\"\"\n        for obbond in ob.OBMolBondIter(self._obmol):\n            if (obbond.GetBeginAtomIdx() == idx1 and obbond.GetEndAtomIdx() == idx2) or (obbond.GetBeginAtomIdx() == idx2 and obbond.GetEndAtomIdx() == idx1):\n                self._obmol.DeleteBond(obbond)",
        "sampled": "```python\nclass OpenBabel:\n    def __init__(self, molecule):\n        \"\"\"\n        Initialize the OpenBabel molecule object\n\n        Args:\n            molecule (openbabel.OBMol): The openbabel molecule object\n        \"\"\"\n        self.molecule = molecule\n\n    def remove_bond(self, idx1, idx2):\n        \"\"\"\n        Remove a bond from an openbabel molecule\n\n        Args:\n            idx1: The atom index of one of the atoms participating in the bond\n"
    },
    {
        "original": "def expand_specializations(session, class_names):\n    \"\"\"\n    Checks whether any given name is not a class but a specialization.\n\n    If it's a specialization, expand the list of class names with the child\n    class names.\n    \"\"\"\n    result = []\n    for class_name in class_names:\n        specialization = SpecializationV1.create(session, class_name)\n        if specialization is None:\n            result.append(class_name)\n        else:\n            result.extend(specialization.children)\n            logging.info('Expanded specialization \"%s\" into the following'\n                         ' classes: %s',\n                         class_name, ' '.join(specialization.children))\n\n    return result",
        "sampled": "```python\nclass=trainir/class_01.py\nimport os\n\ndef expand_specializations(session, class_names):\n    \"\"\"\n    Checks whether any given name is not a class but a specialization.\n\n    If it's a specialization, expand the list of class names with the child \n    class names.\n    \"\"\"\n\n    #ircraft Class Diagram\n    # Aircraft (Dessert, TrussAirframe) => Desert & TrussAirframe are classes\n                    #  |                        |\n                    # "
    },
    {
        "original": "def find_region_end(self, lines):\n        \"\"\"Find the end of the region started with start and end markers\"\"\"\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            # New code region\n            # Simple code pattern in LightScripts must be preceded with a blank line\n            if self.start_code_re.match(line) or (\n                    self.simple_start_code_re and self.simple_start_code_re.match(line) and\n                    (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n\n                if self.explicit_end_marker_required:\n                    # Metadata here was conditioned on finding an explicit end marker\n                    # before the next start marker. So we dismiss it.\n                    self.metadata = None\n                    self.language = None\n\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if not self.ignore_end_marker and self.end_code_re:\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n            elif _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False",
        "sampled": "```python\ndef find_region_end(self, lines):\n    \"\"\"\n    Find the end of the region started with start and end markers.\n    \n    A block is defined by <<start>> followed by any number of lines \n    (possibly containing <<end>>) and finally a line with just \n    <<end>>. We need to detect this block in a list of lines.\n    \n    Args:\n        lines (list): A list of strings representing the input file broken down into individual lines\n    \n    Returns"
    },
    {
        "original": "def _WritePartial(self, data):\n    \"\"\"Writes at most one chunk of data.\"\"\"\n\n    chunk = self.offset // self.chunksize\n    chunk_offset = self.offset % self.chunksize\n    data = utils.SmartStr(data)\n\n    available_to_write = min(len(data), self.chunksize - chunk_offset)\n\n    fd = self._GetChunkForWriting(chunk)\n    fd.seek(chunk_offset)\n\n    fd.write(data[:available_to_write])\n    self.offset += available_to_write\n\n    return data[available_to_write:]",
        "sampled": "```python\nimport math\n\nclass Solution:\n    def isPowerOfTwo(self, n: int) -> bool:\n        \"\"\"Returns whether a given integer `n` is a power of two.\"\"\"\n        if n <= 0:\n            return False\n        return (n & (n - 1)) == 0\n\n    def _WritePartial(self, data):\n        \"\"\"Writes at most one chunk of data.\"\"\"\n        \n# Testing the function\nsolution = Solution()\nprint(solution.isPowerOf"
    },
    {
        "original": "def require_fresh_games(self, number_fresh):\n        \"\"\"Require a given number of fresh games to be played.\n\n        Args:\n          number_fresh:  integer, number of new fresh games needed\n\n        Increments the cell `table_state=metadata:wait_for_game_number`\n        by the given number of games.  This will cause\n        `self.wait_for_fresh_games()` to block until the game\n        counter has reached this number.\n        \"\"\"\n        latest = self.latest_game_number\n        table_state = self.bt_table.row(TABLE_STATE)\n        table_state.set_cell(METADATA, WAIT_CELL, int(latest + number_fresh))\n        table_state.commit()\n        print(\"== Setting wait cell to \", int(latest + number_fresh), flush=True)",
        "sampled": "```python\nclass MetaGame:\n    def __init__(self):\n        # Initialize the metadata with a key for the game number and a default value of 0\n        self.metadata = {\"wait_for_game_number\": 0}\n\n    def require_fresh_games(self, number_fresh):\n        \"\"\"\n        Require a given number of fresh games to be played.\n\n        Args:\n          number_fresh: integer, number of new fresh games needed\n\n        Increments the cell `table_state=metadata"
    },
    {
        "original": "def create_organization(self, name):\n        \"\"\"\n        To create an organization Jira administrator global permission or agent permission is required\n        depending on the settings\n\n        :param name: str\n        :return: Organization data\n        \"\"\"\n        log.warning('Creating organization...')\n        url = 'rest/servicedeskapi/organization'\n        data = {'name': name}\n\n        return self.post(url, headers=self.experimental_headers, data=data)",
        "sampled": "```python\nclass Organization:\n    def __init__(self, name):\n        self.name = name\n\nclass JiraAPI:\n    def create_organization(self, name):\n        \"\"\"\n        To create an organization Jira administrator global permission or agent permission is required\n        depending on the settings\n\n        :param name: str\n        :return: Organization data\n        \"\"\"\n        \n        # Here we can store our logic to check if Jira admin or agent has global permission\n        # For simplicity let"
    },
    {
        "original": "def new_datetime(d):\n    \"\"\"\n    Generate a safe datetime from a datetime.date or datetime.datetime object.\n    \"\"\"\n    kw = [d.year, d.month, d.day]\n    if isinstance(d, real_datetime):\n        kw.extend([d.hour, d.minute, d.second, d.microsecond, d.tzinfo])\n    return datetime(*kw)",
        "sampled": "```python\nimport datetime\n\ndef new_datetime(d):\n    \"\"\"\n    Generate a safe datetime from a datetime.date or datetime.datetime object.\n    \n    Args:\n        d (datetime.date or datetime.datetime): The input date or datetime object.\n    \n    Returns:\n        A safe(datetime.date if d is a date, else datetime.datetime) object.\n    \"\"\"\n    \n    if isinstance(d, (datetime.date, str)):\n        # If the input is a string in YYYY-MM-DD format, parse it\n       "
    },
    {
        "original": "def new_feed(self, name: str, layer_shape: tuple):\n        \"\"\"\n        Creates a feed layer. This is usually the first layer in the network.\n        :param name: name of the layer\n        :return:\n        \"\"\"\n\n        feed_data = tf.placeholder(tf.float32, layer_shape, 'input')\n        self.__network.add_layer(name, layer_output=feed_data)",
        "sampled": "```python\nimport tensorflow as tf\n\nclass Network:\n    def __init__(self):\n        self.layers = dict()\n\n    def new_feed(self, name: str, layer_shape: tuple):\n        \"\"\"\n        Creates a feed layer. This is usually the first layer in the network.\n        :param name: name of the layer\n        :return:\n        \"\"\"\n        \n        # Create a new feed layer with the specified shape and name\n        self.layers[name] = tf.keras.layers.InputLayer(input"
    },
    {
        "original": "def token(request, response, verify_user, context=None, **kwargs):\n    \"\"\"Token verification\n\n    Checks for the Authorization header and verifies using the verify_user function\n    \"\"\"\n    token = request.get_header('Authorization')\n    if token:\n        try:\n            verified_token = verify_user(token)\n        except TypeError:\n            verified_token = verify_user(token, context)\n        if verified_token:\n            return verified_token\n        else:\n            return False\n    return None",
        "sampled": "```python\nfrom functools import wraps\n\ndef verify_user(token, *args, **kwargs):\n    \"\"\"Placeholder function to verify user\n    \n    Needs to be replaced with actual implementation\n    \"\"\"\n    \n    return True  # Replace with actual verification logic\n\n\ndef token(request, response, verify_user=None, context=None):\n    \"\"\"Token verification\n\n\n    Checks for the Authorization header and verifies using the verify_user function\n\n    Args:\n        request (object): The client request object\n        response (object):"
    },
    {
        "original": "def ParseHeader(table):\n  \"\"\"Parses header of osquery output.\n\n  Args:\n    table: A table in a \"parsed JSON\" representation.\n\n  Returns:\n    A parsed `rdf_osquery.OsqueryHeader` instance.\n  \"\"\"\n  precondition.AssertIterableType(table, dict)\n\n  prototype = None  # type: List[Text]\n\n  for row in table:\n    columns = list(iterkeys(row))\n    if prototype is None:\n      prototype = columns\n    elif prototype != columns:\n      message = \"Expected columns '{expected}', got '{actual}' for table {json}\"\n      message = message.format(expected=prototype, actual=columns, json=table)\n      raise ValueError(message)\n\n  result = rdf_osquery.OsqueryHeader()\n  for name in prototype or []:\n    result.columns.append(rdf_osquery.OsqueryColumn(name=name))\n  return result",
        "sampled": "```python\nclass rdf_osquery:\n    class OsqueryHeader:\n        def __init__(self, os_version):\n            self.os_version = os_version\n\ndef ParseHeader(table):\n    \"\"\"\n    Parses header of osquery output.\n\n    Args:\n      table (str): A table in \"osquery.sql\" representation.\n\n    Returns:\n      An object with 'os' and 'version' properties.\n    \"\"\"\n    \n    # Assuming the first line of the table contains the os query header\n   "
    },
    {
        "original": "def send_fetches(self):\n        \"\"\"Send FetchRequests for all assigned partitions that do not already have\n        an in-flight fetch or pending fetch data.\n\n        Returns:\n            List of Futures: each future resolves to a FetchResponse\n        \"\"\"\n        futures = []\n        for node_id, request in six.iteritems(self._create_fetch_requests()):\n            if self._client.ready(node_id):\n                log.debug(\"Sending FetchRequest to node %s\", node_id)\n                future = self._client.send(node_id, request)\n                future.add_callback(self._handle_fetch_response, request, time.time())\n                future.add_errback(log.error, 'Fetch to node %s failed: %s', node_id)\n                futures.append(future)\n        self._fetch_futures.extend(futures)\n        self._clean_done_fetch_futures()\n        return futures",
        "sampled": "```python\n# Import necessary libraries\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass FetchManager:\n    \"\"\"Manages FetchRequests for all assigned partitions.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the FetchManager with an empty list of partitions.\"\"\"\n        self.partitions = []\n\n    def add_partition(self, partition):\n        \"\"\"Add a partition to the list of managed partitions.\n\n        Args:\n            partition: The partition to be added.\n        \"\"\"\n        self.partitions.append"
    },
    {
        "original": "def lock(self, source_node):\n        \"\"\"Lock the task, source is the :class:`Node` that applies the lock.\"\"\"\n        if self.status != self.S_INIT:\n            raise ValueError(\"Trying to lock a task with status %s\" % self.status)\n\n        self._status = self.S_LOCKED\n        self.history.info(\"Locked by node %s\", source_node)",
        "sampled": "```python\nclass Node:\n    def __init__(self, val=None, children=None):\n        self.val = val\n        self.children = children if children is not None else []\n\nclass Solution:\n    def lock(self, source_node: 'Node') -> int:\n        \"\"\"\n        Lock the task, source is the :class:`Node` that applies the lock.\n        \n        The task is to assign a unique integer label to each node in a tree such that \n        it satisfies following conditions:"
    },
    {
        "original": "def get_dp(app, dpid):\n    \"\"\"\n    :type dpid: datapath id\n    :param dpid:\n    :rtype: ryu.controller.controller.Datapath\n    :returns: datapath corresponding to dpid\n    \"\"\"\n    switches = topo_api.get_switch(app, dpid)\n    if not switches:\n        return None\n    assert len(switches) == 1\n    return switches[0].dp",
        "sampled": "```python\nfrom ryu.controller.controller import Datapath as Dp\n\ndef get_dp(app, dpid):\n    \"\"\"\n    :type dpid: datapath id\n    :param dpid:\n    :rtype: ryu.controller.controller.Datapath\n    :returns: datapath corresponding to dpid\n    \"\"\"\n    \n    # Fetch the DP from the local cached Datapaths or start a connection to request it from SAP if not found.\n    \n    dp = app.dat"
    },
    {
        "original": "def setup_ordered_indices_local_geometry(self, coordination):\n        \"\"\"\n        Sets up ordered indices for the local geometry, for testing purposes\n        :param coordination: coordination of the local geometry\n        \"\"\"\n        self.icentral_site = 0\n        self.indices = list(range(1, coordination + 1))",
        "sampled": "```python\nclass Geometry:\n    def setup_ordered_indices_local_geometry(self, coordination):\n        \"\"\"\n        Sets up ordered indices for the local geometry, for testing purposes\n        :param coordination: coordination of the local geometry\n        \"\"\"\n        \n        # Assuming coordination is a list of tuples representing points in 3D space\n        # We will use these points to calculate the ordered indices\n        \n        # First, we need to find the minimum and maximum x, y, z values to determine the bounds of"
    },
    {
        "original": "def fetchExternalUpdates(self):\r\n        \"\"\"\r\n        !Experimental!\r\n        Calls out to the client code requesting seed values to use in the UI\r\n        !Experimental!\r\n        \"\"\"\r\n        seeds = seeder.fetchDynamicProperties(\r\n            self.buildSpec['target'],\r\n            self.buildSpec['encoding']\r\n        )\r\n        for config in self.configs:\r\n            config.seedUI(seeds)",
        "sampled": "```python\nclass Solution:\n    def fetchExternalUpdates(self):\n        \"\"\"\n        !Experimental!\n        Calls out to the client code requesting seed values to use in the UI\n        !Experimental!\n        \n        Returns:\n            list: A list of seed values\n        \"\"\"\n        \n# Example usage:\nsolution = Solution()\nupdates = solution.fetchExternalUpdates()\nprint(updates)  # Output: []  # Replace with actual function implementation\n```\n\nHowever, it seems like you didn't provide the complete"
    },
    {
        "original": "def get_root_path(obj):\n    \"\"\"\n    Get file path for object and returns its dirname\n    \"\"\"\n    try:\n        filename = os.path.abspath(obj.__globals__['__file__'])\n    except (KeyError, AttributeError):\n        if getattr(obj, '__wrapped__', None):\n            # decorator package has been used in view\n            return get_root_path(obj.__wrapped__)\n        filename = inspect.getfile(obj)\n    return os.path.dirname(filename)",
        "sampled": "```python\nimport os\n\ndef get_root_path(obj):\n    \"\"\"\n    Get file path for object and returns its dirname\n    \"\"\"\n    # Check if the object is a file or a directory\n    if isinstance(obj, str) and os.path.isfile(obj):\n        return os.path.dirname(os.path.abspath(obj))\n    \n    # If not a file, try to get its path from the module it belongs to\n    elif hasattr(obj, '__module__'):\n        return os.path.dirname(os.path.abspath"
    },
    {
        "original": "def reset_default(verbose=False):\n    \"\"\"Remove custom.css and custom fonts\"\"\"\n    paths = [jupyter_custom, jupyter_nbext]\n\n    for fpath in paths:\n        custom = '{0}{1}{2}.css'.format(fpath, os.sep, 'custom')\n        try:\n            os.remove(custom)\n        except Exception:\n            pass\n    try:\n        delete_font_files()\n    except Exception:\n        check_directories()\n        delete_font_files()\n\n    copyfile(defaultCSS, jupyter_customcss)\n    copyfile(defaultJS, jupyter_customjs)\n\n    if os.path.exists(theme_name_file):\n        os.remove(theme_name_file)\n\n    if verbose:\n        print(\"Reset css and font defaults in:\\n{} &\\n{}\".format(*paths))",
        "sampled": "```python\nimport os\n\ndef reset_default(verbose=False):\n    \"\"\"\n    Remove custom.css and custom fonts\n    \n    Args:\n        verbose (bool): Whether to print status messeages (default: False)\n    \n    Returns:\n        None\n    \"\"\"\n    \n    # Define file paths\n    css_path = 'path_to_your_custom_css_file.css'\n    fonts_path = 'path_to_your_fonts_directory'\n    \n    # Check if custom.css exists and remove it if it does\n   "
    },
    {
        "original": "def set_last_col_idx(self, last_col_idx):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tparam last_col_idx : int\n\t\t\tnumber of columns\n\t\t\"\"\"\n\t\tassert last_col_idx >= self._max_col\n\t\tself._max_col = last_col_idx\n\t\treturn self",
        "sampled": "```python\nclass Program:\n    def set_last_col_idx(self, last_col_idx):\n        \"\"\"\n        Parameters\n        ----------\n        param last_col_idx : int\n            number of columns\n        \"\"\"\n        \n# Example usage:\nprogram = Program()\nprogram.set_last_col_idx(10)\n```\n\nHowever, since the provided function is a setter method and does not perform any operation on the input, it can be simplified as follows:\n\n```python\nclass Program:\n    def set_last_col_idx(self,"
    },
    {
        "original": "def ListDescendentPathInfos(self,\n                              client_id,\n                              path_type,\n                              components,\n                              timestamp=None,\n                              max_depth=None,\n                              cursor=None):\n    \"\"\"Lists path info records that correspond to descendants of given path.\"\"\"\n    path_infos = []\n\n    query = \"\"\n\n    path = mysql_utils.ComponentsToPath(components)\n    values = {\n        \"client_id\": db_utils.ClientIDToInt(client_id),\n        \"path_type\": int(path_type),\n        \"path\": db_utils.EscapeWildcards(path),\n    }\n\n    query += ",
        "sampled": "```python\nclass Node:\n    def __init__(self, name, parent=None):\n        self.name = name\n        self.children = []\n        self.parent = parent\n\n    def add_child(self, child):\n        self.children.append(child)\n\n\nclass TreeManager:\n    def __init__(self):\n        self.root = None\n\n    def insert_node(self, path, components):\n        if not self.root:\n            current_node = Node(path)\n            for component in components:\n                current_node.add_child"
    },
    {
        "original": "def ReadPathInfoHistory(self, client_id, path_type, components):\n    \"\"\"Reads a collection of hash and stat entry for given path.\n\n    Args:\n      client_id: An identifier string for a client.\n      path_type: A type of a path to retrieve path history for.\n      components: A tuple of path components corresponding to path to retrieve\n        information for.\n\n    Returns:\n      A list of `rdf_objects.PathInfo` ordered by timestamp in ascending order.\n    \"\"\"\n    histories = self.ReadPathInfosHistories(client_id, path_type, [components])\n    return histories[components]",
        "sampled": "```python\nclass RDFObjects:\n    ### An example of a PathInfo class for demo purposes\n    class PathInfo:\n        def __init__(self, timestamp, hash_value, stat_info):\n            self.timestamp = timestamp\n            self.hash_value = hash_value\n            self.stat_info = stat_info\n\n    # Similar to the given method but with additional implementation details.\n    def ReadPathInfoHistory(self, client_id, path_type, components):\n        # Assuming we have a database or storage"
    },
    {
        "original": "def metrics(self, name):\n        \"\"\"\n        Return the metrics received under the given name\n        \"\"\"\n        return [\n            MetricStub(\n                ensure_unicode(stub.name),\n                stub.type,\n                stub.value,\n                normalize_tags(stub.tags),\n                ensure_unicode(stub.hostname),\n            )\n            for stub in self._metrics.get(to_string(name), [])\n        ]",
        "sampled": "```python\nclass Metrics:\n    def __init__(self):\n        self.metrics = {}\n\n    def add_metric(self, name, value):\n        \"\"\"\n        Add a new metric with the given name and value\n        \"\"\"\n        self.metrics[name] = value\n\n    def update_metric(self, name, value):\n        \"\"\"\n        Update the metric with the given name to the new value\n        If the metric does not exist, it will be created\n        \"\"\"\n        if name in self.metrics:\n           "
    },
    {
        "original": "def _find_penultimate_layer(model, layer_idx, penultimate_layer_idx):\n    \"\"\"Searches for the nearest penultimate `Conv` or `Pooling` layer.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_idx: The layer index within `model.layers`.\n        penultimate_layer_idx: The pre-layer to `layer_idx`. If set to None, the nearest penultimate\n            `Conv` or `Pooling` layer is used.\n\n    Returns:\n        The penultimate layer.\n    \"\"\"\n    if penultimate_layer_idx is None:\n        for idx, layer in utils.reverse_enumerate(model.layers[:layer_idx - 1]):\n            if isinstance(layer, Wrapper):\n                layer = layer.layer\n            if isinstance(layer, (_Conv, _Pooling1D, _Pooling2D, _Pooling3D)):\n                penultimate_layer_idx = idx\n                break\n\n    if penultimate_layer_idx is None:\n        raise ValueError('Unable to determine penultimate `Conv` or `Pooling` '\n                         'layer for layer_idx: {}'.format(layer_idx))\n\n    # Handle negative indexing otherwise the next check can fail.\n    if layer_idx < 0:\n        layer_idx = len(model.layers) + layer_idx\n    if penultimate_layer_idx > layer_idx:\n        raise ValueError('`penultimate_layer_idx` needs to be before `layer_idx`')\n\n    return model.layers[penultimate_layer_idx]",
        "sampled": "```python\nfrom keras.models import Model\n\ndef _find_penultimate_layer(model, layer_idx, penultimate_layer_idx=None):\n    \"\"\"\n    Searches for the nearest penultimate `Conv` or `Pooling` layer.\n\n    Args:\n        model: The `keras.models.Model` instance.\n        layer_idx: The layer index within `model.layers`.\n        penultimate_layer_idx: The pre-layer to `layer_idx`. If set to None, the nearest penultimate\n            `Conv` or"
    },
    {
        "original": "def get_scores_and_p_values(self, tdm, category):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\ttdm: TermDocMatrix\n\t\tcategory: str, category name\n\n\t\tReturns\n\t\t-------\n\t\tpd.DataFrame(['coef', 'p-val'])\n\t\t\"\"\"\n\t\tX = tdm._X\n\t\ty = self._make_response_variable_1_or_negative_1(category, tdm)\n\t\tpX = X / X.sum(axis=1)\n\t\tansX = self._anscombe_transform(pX.copy())\n\t\tB, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var\\\n\t\t\t= lsqr(A=ansX, b=y, calc_var=True)",
        "sampled": "```python\nimport pandas as pd\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef get_scores_and_p_values(self, tdm, category):\n    \"\"\"\n    Parameters\n    ----------\n    tdm: TermDocMatrix\n        A term-document matrix where each row represents a document and each column represents a term.\n        The cell at row i and column j contains the frequency of the jth term in the ith document"
    },
    {
        "original": "def format_stats(stats):\n    \"\"\"Given a dictionary following this layout:\n\n        {\n            'encoded:label': 'Encoded',\n            'encoded:value': 'Yes',\n            'encoded:description': 'Indicates if the column is encoded',\n            'encoded:include': True,\n\n            'size:label': 'Size',\n            'size:value': 128,\n            'size:description': 'Size of the table in MB',\n            'size:include': True,\n        }\n\n    format_stats will convert the dict into this structure:\n\n        {\n            'encoded': {\n                'id': 'encoded',\n                'label': 'Encoded',\n                'value': 'Yes',\n                'description': 'Indicates if the column is encoded',\n                'include': True\n            },\n            'size': {\n                'id': 'size',\n                'label': 'Size',\n                'value': 128,\n                'description': 'Size of the table in MB',\n                'include': True\n            }\n        }\n    \"\"\"\n    stats_collector = {}\n    for stat_key, stat_value in stats.items():\n        stat_id, stat_field = stat_key.split(\":\")\n\n        stats_collector.setdefault(stat_id, {\"id\": stat_id})\n        stats_collector[stat_id][stat_field] = stat_value\n\n    # strip out all the stats we don't want\n    stats_collector = {\n        stat_id: stats\n        for stat_id, stats in stats_collector.items()\n        if stats.get('include', False)\n    }\n\n    # we always have a 'has_stats' field, it's never included\n    has_stats = {\n        'id': 'has_stats',\n        'label': 'Has Stats?',\n        'value': len(stats_collector) > 0,\n        'description': 'Indicates whether there are statistics for this table',\n        'include': False,\n    }\n    stats_collector['has_stats'] = has_stats\n    return stats_collector",
        "sampled": "```python\ndef format_stats(stats):\n    formatted_stats = {}\n    for key, value in stats.items():\n        prefix, suffix = key.split(':')\n        if prefix not in formatted_stats:\n            formatted_stats[prefix] = {\n                'id': prefix,\n                'label': value['label'],\n                'value': value['value'],\n                'description': value['description'],\n                'include': value['include']\n            }\n    return formatted_stats\n```"
    },
    {
        "original": "def WriteFromFD(self, src_fd, arcname=None, compress_type=None, st=None):\n    \"\"\"Write a zip member from a file like object.\n\n    Args:\n      src_fd: A file like object, must support seek(), tell(), read().\n      arcname: The name in the archive this should take.\n      compress_type: Compression type (zipfile.ZIP_DEFLATED, or ZIP_STORED)\n      st: An optional stat object to be used for setting headers.\n\n    Raises:\n      ArchiveAlreadyClosedError: If the zip if already closed.\n\n    Yields:\n      Chunks of binary data.\n    \"\"\"\n    yield self.WriteFileHeader(\n        arcname=arcname, compress_type=compress_type, st=st)\n    while 1:\n      buf = src_fd.read(1024 * 1024)\n      if not buf:\n        break\n\n      yield self.WriteFileChunk(buf)\n\n    yield self.WriteFileFooter()",
        "sampled": "```python\nimport zipfile\n\nclass ZipWriter:\n    def __init__(self, filename):\n        self.zip_file = zipfile.ZipFile(filename, 'w')\n\n    def WriteFromFD(self, src_fd, arcname=None, compress_type=None, st=None):\n        if self.zip_file.closed:\n            raise ArchiveAlreadyClosedError(\"Archive is already closed\")\n\n        if compress_type is None:\n            compress_type = zipfile.ZIP_STORED\n\n        info = zipfile.ZipInfo(arcname)\n        if"
    },
    {
        "original": "def local(reload, port):\n    \"\"\"run local app server, assumes into the account\n    \"\"\"\n    import logging\n    from bottle import run\n    from app import controller, app\n    from c7n.resources import load_resources\n    load_resources()\n    print(\"Loaded resources definitions\")\n    logging.basicConfig(level=logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.WARNING)\n    if controller.db.provision():\n        print(\"Table Created\")\n    run(app, reloader=reload, port=port)",
        "sampled": "```python\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nimport threading\nimport requests\n\nclass RequestHandler(BaseHTTPRequestHandler):\n\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header('Content-type', 'text/html')\n        self.end_headers()\n        message = \"Welcome to the InterAtm service!\"\n        \n        # Do something when the frontend requests a page (status/txstatus/etc.)\n        \n    def run_server(self):\n    \n    \tdef run():\n    \t"
    },
    {
        "original": "def evict(cls, urls):\n        \"\"\"Remove items from cache matching URLs.\n\n        Return the number of items removed.\n\n        \"\"\"\n        if isinstance(urls, text_type):\n            urls = [urls]\n        urls = set(normalize_url(url) for url in urls)\n        retval = 0\n        with cls.ca_lock:\n            for key in list(cls.cache):\n                if key[0] in urls:\n                    retval += 1\n                    del cls.cache[key]\n                    del cls.timeouts[key]\n        return retval",
        "sampled": "```python\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n\n    def get(self, key):\n        if key in self.cache:\n            value = self.cache[key]\n            del self.cache[key]\n            self.cache[key] = value  # move to end to mark as recently used\n            return value\n        else:\n            return -1\n\n    def put(self, key, value):\n        if key in self.cache"
    },
    {
        "original": "def add_custom_service_account(self, account, nickname, password):\n        \"\"\"\n        \u6dfb\u52a0\u5ba2\u670d\u5e10\u53f7\u3002\n\n        :param account: \u5ba2\u670d\u8d26\u53f7\u7684\u7528\u6237\u540d\n        :param nickname: \u5ba2\u670d\u8d26\u53f7\u7684\u6635\u79f0\n        :param password: \u5ba2\u670d\u8d26\u53f7\u7684\u5bc6\u7801\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/customservice/kfaccount/add\",\n            data={\n                \"kf_account\": account,\n                \"nickname\": nickname,\n                \"password\": password\n            }\n        )",
        "sampled": "```\nclass CustomerService:\n    def __init__(self):\n        # Initialize a dictionary to store customer service accounts\n        self.accounts = {}\n\n    def add_custom_service_account(self, account, nickname, password):\n        \"\"\"\n        \u6dfb\u52a0\u5ba2\u670d\u5e10\u53f7\u3002\n\n        :param account: \u5ba2\u670d\u8d26\u53f7\u7684\u7528\u6237\u540d\n        :param nickname: \u5ba2\u670d\u8d26\u53f7\u7684\u6635\u79f0\n        :param password: \u5ba2\u670d\u8d26\u53f7\u7684\u5bc6\u7801\n        :return: JSON \u6570\u636e\u5305 (\u6210\u529f"
    },
    {
        "original": "def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns\n        None.\n\n        :return: None\n        \"\"\"\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n\n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)",
        "sampled": "```python\nimport subprocess\n\nclass Programmer:\n    def __init__(self, command=\"ansible-lint\"):\n        \"\"\"\n        Initialize a Programmer with an ansible-lint command.\n\n        :param command: The ansible-lint command.\n        \"\"\"\n        self.command = f\"{command} --format=human\"\n\n    def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns\n        None.\n\n        :return: None\n        \"\"\"\n        \n# Usage"
    },
    {
        "original": "def _dict_to_bson(doc, check_keys, opts, top_level=True):\n    \"\"\"Encode a document to BSON.\"\"\"\n    if _raw_document_class(doc):\n        return doc.raw\n    try:\n        elements = []\n        if top_level and \"_id\" in doc:\n            elements.append(_name_value_to_bson(b\"_id\\x00\", doc[\"_id\"],\n                                                check_keys, opts))\n        for (key, value) in iteritems(doc):\n            if not top_level or key != \"_id\":\n                elements.append(_element_to_bson(key, value,\n                                                 check_keys, opts))\n    except AttributeError:\n        raise TypeError(\"encoder expected a mapping type but got: %r\" % (doc,))\n\n    encoded = b\"\".join(elements)\n    return _PACK_INT(len(encoded) + 5) + encoded + b\"\\x00\"",
        "sampled": "```python\ndef _dict_to_bson(doc, check_keys, opts, top_level=True):\n    \"\"\"\n    Encode a document to BSON.\n\n    :param doc: The document to encode.\n    :param check_keys: A list of keys to check for in the document.\n    :param opts: Options for encoding.\n    :param top_level: Whether this is the top level of the document. Defaults to True.\n    \"\"\"\n    \n    # Initialize an empty list to store the encoded BSON elements"
    },
    {
        "original": "def _fix_reindent(self, result):\n        \"\"\"Fix a badly indented line.\n\n        This is done by adding or removing from its initial indent only.\n\n        \"\"\"\n        num_indent_spaces = int(result['info'].split()[1])\n        line_index = result['line'] - 1\n        target = self.source[line_index]\n\n        self.source[line_index] = ' ' * num_indent_spaces + target.lstrip()",
        "sampled": "```python\nclass CodeFormatter:\n    def _fix_reindent(self, result):\n        \"\"\"\n        Fix a badly indented line.\n        \n        This is done by adding or removing from its initial indent only.\n        \n        Parameters:\n        result (list): The list of parsed lines. Each line is a dictionary that contains 'text' and 'offset' keys.\n        \n        Returns:\n        None\n        \"\"\"\n        \n        # Loop over the lines in the result list\n        for i, line in"
    },
    {
        "original": "def load_cli_config(args):\n    \"\"\"Modifies ARGS in-place to have the attributes defined in the CLI\n    config file if it doesn't already have them. Certain default\n    values are given if they are not in ARGS or the config file.\n    \"\"\"\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key in args and getattr(args, key) is not None:\n                pass\n            else:\n                setattr(args, key, val)",
        "sampled": "```python\nfrom configparser import ConfigParser\n\ndef load_cli_config(args):\n    \"\"\"\n    Modifies ARGS in-place to have the attributes defined in the CLI\n    config file if it doesn't already have them. Certain default\n    values are given if they are not in ARGS or the config file.\n    \n    :param args: Collection of CLI arguments with their values as attributes.\n                Can be a dictionary, object or any other type that supports attribute access.\n                If args is None, a"
    },
    {
        "original": "def _construct(self):\n        \"\"\"\n        Construct a control dependence graph.\n\n        This implementation is based on figure 6 of paper An Efficient Method of Computing Static Single Assignment\n        Form by Ron Cytron, etc.\n        \"\"\"\n\n        self._acyclic_cfg = self._cfg.copy()\n        # TODO: Cycle-removing is not needed - confirm it later\n        # The CFG we use should be acyclic!\n        #self._acyclic_cfg.remove_cycles()\n\n        # Pre-process the acyclic CFG\n        self._pre_process_cfg()\n\n        # Construct post-dominator tree\n        self._pd_construct()\n\n        self._graph = networkx.DiGraph()\n\n        # Construct the reversed dominance frontier mapping\n        rdf = compute_dominance_frontier(self._normalized_cfg, self._post_dom)\n\n        for y in self._cfg.graph.nodes():\n            if y not in rdf:\n                continue\n            for x in rdf[y]:\n                self._graph.add_edge(x, y)",
        "sampled": "```python\nclass Node:\n    def __init__(self, name):\n        self.name = name\n        self.in_edges = []\n        self.out_edges = []\n\nclass ControlDependenceGraph:\n    def __init__(self):\n        self.nodes = {}\n\n    def add_node(self, node_name):\n        if node_name not in self.nodes:\n            self.nodes[node_name] = Node(node_name)\n\n    def add_edge(self, from_node, to_node):\n        if from_node in self.nodes"
    },
    {
        "original": "def pad_tensor(tensor, length, padding_index=DEFAULT_PADDING_INDEX):\n    \"\"\" Pad a ``tensor`` to ``length`` with ``padding_index``.\n\n    Args:\n        tensor (torch.Tensor [n, ...]): Tensor to pad.\n        length (int): Pad the ``tensor`` up to ``length``.\n        padding_index (int, optional): Index to pad tensor with.\n\n    Returns\n        (torch.Tensor [length, ...]) Padded Tensor.\n    \"\"\"\n    n_padding = length - tensor.shape[0]\n    assert n_padding >= 0\n    if n_padding == 0:\n        return tensor\n    padding = tensor.new(n_padding, *tensor.shape[1:]).fill_(padding_index)\n    return torch.cat((tensor, padding), dim=0)",
        "sampled": "```\nfrom torch.nn import functional as F\nfrom torch import nn\n\nDEFAULT_PADDING_INDEX = 0\n\nclass Padder(nn.Module):\n    \"\"\" A simple padding layer.\n\n    Args:\n        padding_index (int, optional): Index to pad tensor with.\n            Defaults to `DEFAULT_PADDING_INDEX`.\n\n    \"\"\"\n\n    def __init__(self, padding_index=DEFAULT_PADDING_INDEX):\n        super(Padder, self).__init__()\n        self.padding_index = padding_index\n\n    def forward(self, input_tensor"
    },
    {
        "original": "def _malloc(self, sim_size):\n        \"\"\"\n        Handler for any libc `malloc` SimProcedure call. If the heap has faithful support for `malloc`, it ought to be\n        implemented in a `malloc` function (as opposed to the `_malloc` function).\n\n        :param sim_size: the amount of memory (in bytes) to be allocated\n        \"\"\"\n        raise NotImplementedError(\"%s not implemented for %s\" % (self._malloc.__func__.__name__,\n                                                                 self.__class__.__name__))",
        "sampled": "```python\nclass MemoryAllocator:\n    def __init__(self):\n        self.heap = []\n\n    def _malloc(self, sim_size):\n        \"\"\"\n        Handler for any libc `malloc` SimProcedure call.\n\n        :param sim_size: the amount of memory (in bytes) to be allocated\n        \"\"\"\n        \n        # Check if the heap has enough space to allocate the requested memory\n        if len(self.heap) >= sim_size:\n            # If there is enough space, return a pointer to"
    },
    {
        "original": "def save_config(\n        self,\n        cmd=\"copy running-config startup-config\",\n        confirm=True,\n        confirm_response=\"y\",\n    ):\n        \"\"\"Save Config for Extreme VDX.\"\"\"\n        return super(ExtremeNosSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "sampled": "```python\nclass ConfigManager:\n    def __init__(self):\n        self._cmds = {\n            \"Extreme VDX\": \"copy running-config startup-config\",\n            \"Cisco\": \"copy running-config startup-run\",\n            \"Juniper\": \"load merge startup.jun\"\n        }\n\n    def get_cmd(self, switch_type=\"Extreme VDX\"):\n        return self._cmds.get(switch_type, \"\")\n\n# Usage\nconfig_manager = ConfigManager()\nprint(config_manager.get_cmd(\"Extreme VDX"
    },
    {
        "original": "def predict_wishart_embedding(self, Xnew, kern=None, mean=True, covariance=True):\n        \"\"\"\n        Predict the wishart embedding G of the GP. This is the density of the\n        input of the GP defined by the probabilistic function mapping f.\n        G = J_mean.T*J_mean + output_dim*J_cov.\n\n        :param array-like Xnew: The points at which to evaluate the magnification.\n        :param :py:class:`~GPy.kern.Kern` kern: The kernel to use for the magnification.\n\n        Supplying only a part of the learning kernel gives insights into the density\n        of the specific kernel part of the input function. E.g. one can see how dense the\n        linear part of a kernel is compared to the non-linear part etc.\n        \"\"\"\n        if kern is None:\n            kern = self.kern\n\n        mu_jac, var_jac = self.predict_jacobian(Xnew, kern, full_cov=False)\n        mumuT = np.einsum('iqd,ipd->iqp', mu_jac, mu_jac)\n        Sigma = np.zeros(mumuT.shape)\n        if var_jac.ndim == 4: # Missing data\n            Sigma = var_jac.sum(-1)\n        else:\n            Sigma = self.output_dim*var_jac\n\n        G = 0.\n        if mean:\n            G += mumuT\n        if covariance:\n            G += Sigma\n        return G",
        "sampled": "```python\nimport numpy as np\nfrom GPy.kern import RBF\n\nclass WishartEmbeddingPredictor:\n    def __init__(self, X, kernel=None):\n        self.X = X\n        if kernel is None:\n            self.kernel = RBF(input_dim=np.shape(X)[1])\n        else:\n            self.kernel = kernel\n\n    def predict_wishart_embedding(self, Xnew):\n        \"\"\"\n        Predict the wishart embedding G of the GP. This is the"
    },
    {
        "original": "def check_model(self):\n        \"\"\"\n        Check the model for various errors. This method checks for the following\n        errors.\n\n        * Checks if factors are defined for all the cliques or not.\n        * Check for running intersection property is not done explicitly over\n          here as it done in the add_edges method.\n        * Checks if cardinality information for all the variables is availble or not. If\n          not it raises an error.\n        * Check if cardinality of random variable remains same across all the\n          factors.\n\n        Returns\n        -------\n        check: boolean\n            True if all the checks are passed\n        \"\"\"\n        for clique in self.nodes():\n            factors = filter(lambda x: set(x.scope()) == set(clique), self.factors)\n            if not any(factors):\n                raise ValueError('Factors for all the cliques or clusters not defined.')\n\n        cardinalities = self.get_cardinality()\n        if len(set((x for clique in self.nodes() for x in clique))) != len(cardinalities):\n            raise ValueError('Factors for all the variables not defined.')\n\n        for factor in self.factors:\n            for variable, cardinality in zip(factor.scope(), factor.cardinality):\n                if (cardinalities[variable] != cardinality):\n                    raise ValueError(\n                        'Cardinality of variable {var} not matching among factors'.format(var=variable))\n\n        return True",
        "sampled": "```python\nclass ModelChecker:\n    def __init__(self, cliques, factors, variables):\n        \"\"\"\n        Initialize the model checker.\n\n        Parameters\n        ----------\n        cliques : list\n            List of cliques in the model.\n        factors : list of dictionaries\n            List of factors where each factor is a dictionary with its neighboring nodes and cardinality.\n            e.g., [{node1: 'X', node2: 'Y'}, {node3: 'Z'}, ...]\n"
    },
    {
        "original": "def get_default_database(self):\n        \"\"\"DEPRECATED - Get the database named in the MongoDB connection URI.\n\n        >>> uri = 'mongodb://host/my_database'\n        >>> client = MongoClient(uri)\n        >>> db = client.get_default_database()\n        >>> assert db.name == 'my_database'\n        >>> db = client.get_database()\n        >>> assert db.name == 'my_database'\n\n        Useful in scripts where you want to choose which database to use\n        based only on the URI in a configuration file.\n\n        .. versionchanged:: 3.5\n           Deprecated, use :meth:`get_database` instead.\n        \"\"\"\n        warnings.warn(\"get_default_database is deprecated. Use get_database \"\n                      \"instead.\", DeprecationWarning, stacklevel=2)\n        if self.__default_database_name is None:\n            raise ConfigurationError('No default database defined')\n\n        return self[self.__default_database_name]",
        "sampled": "```python\nfrom pymongo import MongoClient\nimport urllib.parse\n\nclass MongoDBClient:\n    \"\"\"MongoDB client class.\"\"\"\n    \n    def __init__(self, uri):\n        \"\"\"\n        Initialize the MongoDB client with a URI.\n\n        :param uri: The MongoDB connection URI.\n        \"\"\"\n        self.uri = URI(uri)\n        \n    def get_default_database(self):\n        \"\"\"\n        Get the database named in the MongoDB connection URI.\n\n        :return: The default database object.\n        \n        .. versionchanged:: "
    },
    {
        "original": "def service_set_tag(path, service_name, tag):\n    \"\"\"\n    Change the tag of a docker-compose service\n    This does not pull or up the service\n    This wil re-write your yaml file. Comments will be lost. Indentation is set to 2 spaces\n\n    path\n        Path where the docker-compose file is stored on the server\n    service_name\n        Name of the service to remove\n    tag\n        Name of the tag (often used as version) that the service image should have\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion dockercompose.service_create /path/where/docker-compose/stored service_name tag\n    \"\"\"\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return err\n    services = compose_result['compose_content']['services']\n    if service_name not in services:\n        return __standardize_result(False,\n                                    'Service {0} did not exists'.format(service_name),\n                                    None, None)\n    if 'image' not in services[service_name]:\n        return __standardize_result(False,\n                                    'Service {0} did not contain the variable \"image\"'.format(service_name),\n                                    None, None)\n    image = services[service_name]['image'].split(':')[0]\n    services[service_name]['image'] = '{0}:{1}'.format(image, tag)\n    return __dump_compose_file(path, compose_result,\n                               'Service {0} is set to tag \"{1}\"'.format(service_name, tag),\n                               already_existed=True)",
        "sampled": "```python\nimport os\nimport yaml\n\ndef service_set_tag(path, service_name, tag):\n    \"\"\"\n    Change the tag of a docker-compose service\n    \n    Parameters:\n    path (str): Path where the docker-compose file is stored on the server\n    service_name (str): Name of the service to remove\n    tag (str): Name of the tag (often used as version) that the service image should have\n\n    Returns:\n    None\n    \"\"\"\n\n    \n         \n YolIdlet"
    },
    {
        "original": "def enqueue(self, destination):\n        \"\"\"Enqueues given destination for processing.\n\n        Given instance should be a valid destination.\n        \"\"\"\n        if not destination:\n            raise BgpProcessorError('Invalid destination %s.' % destination)\n\n        dest_queue = self._dest_queue\n        # RtDest are queued in a separate queue\n        if destination.route_family == RF_RTC_UC:\n            dest_queue = self._rtdest_queue\n\n        # We do not add given destination to the queue for processing if\n        # it is already on the queue.\n        if not dest_queue.is_on_list(destination):\n            dest_queue.append(destination)\n\n        # Wake-up processing thread if sleeping.\n        self.dest_que_evt.set()",
        "sampled": "```python\nfrom queue import Queue\n\nclass Warehouse:\n    def __init__(self):\n        self.queue = Queue()\n        self.store = {}\n        \n    def put(self, item, destination):\n        \"\"\"\n        Put items from source into a truck at some destination.\n\n        Args:\n            item (int): Weight of the item\n            destination (str): Destination where the truck is waiting for picking up items.\n        \n        Result: \n        \n            The function returns a string message indicating if the event was executed"
    },
    {
        "original": "def parse_tags(self, tags):\n        \"\"\"Parse tags into a dict.\n\n        input tags: a comma separated list of 'key:value' pairs.\n            Example: foo:bar,spam:eggs\n        output dtags: a dict of tags.\n            Example: {'foo': 'bar', 'spam': 'eggs'}\n        \"\"\"\n        dtags = {}\n        if tags:\n            try:\n                dtags = dict([x.split(':') for x in tags.split(',')])\n            except ValueError:\n                # one of the 'key:value' pairs was missing\n                logger.info('Invalid tags passed: %s', tags)\n                dtags = {}\n\n        return dtags",
        "sampled": "```python\ndef parse_tags(self, tags):\n    \"\"\"\n    Parse tags into a dict.\n\n    input tags: a comma separated list of 'key:value' pairs.\n        Example: foo:bar,spam:eggs\n    output dtags: a dict of tags.\n        Example: {'foo': 'bar', 'spam': 'eggs'}\n    \"\"\"\n    if not tags:\n        return {}\n    \n    # split the input string by comma to get individual key-value pairs\n    tag"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'environment_id') and self.environment_id is not None:\n            _dict['environment_id'] = self.environment_id\n        if hasattr(self, 'customer_id') and self.customer_id is not None:\n            _dict['customer_id'] = self.customer_id\n        if hasattr(self, 'document_type') and self.document_type is not None:\n            _dict['document_type'] = self.document_type\n        if hasattr(self, 'natural_language_query'\n                  ) and self.natural_language_query is not None:\n            _dict['natural_language_query'] = self.natural_language_query\n        if hasattr(self,\n                   'document_results') and self.document_results is not None:\n            _dict['document_results'] = self.document_results._to_dict()\n        if hasattr(self,\n                   'created_timestamp') and self.created_timestamp is not None:\n            _dict['created_timestamp'] = datetime_to_string(\n                self.created_timestamp)\n        if hasattr(self,\n                   'client_timestamp') and self.client_timestamp is not None:\n            _dict['client_timestamp'] = datetime_to_string(\n                self.client_timestamp)\n        if hasattr(self, 'query_id') and self.query_id is not None:\n            _dict['query_id'] = self.query_id\n        if hasattr(self, 'session_token') and self.session_token is not None:\n            _dict['session_token'] = self.session_token\n        if hasattr(self, 'collection_id') and self.collection_id is not None:\n            _dict['collection_id'] = self.collection_id\n        if hasattr(self, 'display_rank') and self.display_rank is not None:\n            _dict['display_rank'] = self.display_rank\n        if hasattr(self, 'document_id') and self.document_id is not None:\n            _dict['document_id'] = self.document_id\n        if hasattr(self, 'event_type') and self.event_type is not None:\n            _dict['event_type'] = self.event_type\n        if hasattr(self, 'result_type') and self.result_type is not None:\n            _dict['result_type'] = self.result_type\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def advantageCount(self, A: list[int], K: int) -> list[int]:\n        \"\"\"\n        Given an integer array A of unique elements and an integer K. \n        Write a function to return the array with all the occurrences of K removed, \n        and each element in A appears as many times as it should be in the resulting array.\n\n        Args:\n            A (list[int]): List of unique integers.\n            K (int): Target value which"
    },
    {
        "original": "def list_role_policies(role_name, region=None, key=None, keyid=None,\n                       profile=None):\n    \"\"\"\n    Get a list of policy names from a role.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.list_role_policies myirole\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        response = conn.list_role_policies(role_name)\n        _list = response.list_role_policies_response.list_role_policies_result\n        return _list.policy_names\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        return []",
        "sampled": "```python\nimport boto3\nfrom botocore.client import Config\n\nclass BotoIAM:\n    def __init__(self, region_name=None, aws_access_key_id=None, aws_secret_access_key=None):\n        self.iam = boto3.client('iam',\n            region_name=region_name,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n        )\n\n    def list_role_policies(self, role_name):\n        \"\"\"\n        Get"
    },
    {
        "original": "def build(self):\n        \"\"\"\n        Create the current layer\n\n        :return: string of the packet with the payload\n        \"\"\"\n        p = self.do_build()\n        p += self.build_padding()\n        p = self.build_done(p)\n        return p",
        "sampled": "```python\nclass Solution:\n    def build(self):\n        \"\"\"\n        Create the current layer\n\n        :return: string of the packet with the payload\n        \"\"\"\n        # Assuming the packet is a string and the payload is a string\n        packet = \"Packet\"\n        payload = \"Payload\"\n        \n        # Create the current layer\n        current_layer = packet + \" \" + payload\n        \n        return current_layer\n\n# Usage\nsolution = Solution()\nprint(solution.build())\n```"
    },
    {
        "original": "def stop_recording(self):\n        \"\"\"Stop recording from the audio source.\"\"\"\n        self._stop_recording.set()\n        with self._source_lock:\n            self._source.stop()\n        self._recording = False",
        "sampled": "```python\nimport math\n\nclass Solution:\n    def propositional_logic(self, p, q):\n        \"\"\"\n        Given two boolean values p and q, calculate and return the truth values of the\n        following three expressions using logical operators:\n        \n            - NOT a\n            - a AND b \n            - a OR b\n            \n        Args:\n            p (bool): The first boolean value.\n            q (bool): The second boolean value.\n\n        Returns:\n            A tuple containing three elements: the"
    },
    {
        "original": "def remove_label_from_pr(repo: GithubRepository,\n                         pull_id: int,\n                         label: str) -> bool:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/issues/labels/#remove-a-label-from-an-issue\n    \"\"\"\n    url = (\"https://api.github.com/repos/{}/{}/issues/{}/labels/{}\"\n           \"?access_token={}\".format(repo.organization,\n                                     repo.name,\n                                     pull_id,\n                                     label,\n                                     repo.access_token))\n    response = requests.delete(url)\n\n    if response.status_code == 404:\n        payload = json.JSONDecoder().decode(response.content.decode())\n        if payload['message'] == 'Label does not exist':\n            return False\n\n    if response.status_code == 200:\n        # Removed the label.\n        return True\n\n    raise RuntimeError(\n        'Label remove failed. Code: {}. Content: {}.'.format(\n            response.status_code, response.content))",
        "sampled": "```python\nimport requests\nfrom github import Github\n\ndef remove_label_from_pr(repo: GithubRepository,\n                         pull_id: int,\n                         label: str) -> bool:\n    \"\"\"\n    Removes a label from a pull request in a GitHub repository.\n\n    Args:\n        repo (GithubRepository): The GitHub repository.\n        pull_id (int): The ID of the pull request.\n        label (str): The label to be removed.\n\n    Returns:\n        bool: True if the label is removed successfully"
    },
    {
        "original": "def get_labels(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/labels <http://developer.github.com/v3/issues/labels>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Label.Label`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Label.Label,\n            self._requester,\n            self.issue_url + \"/labels\",\n            None\n        )",
        "sampled": "```python\nimport requests\n\nclass GitHubAPI:\n    def __init__(self, owner, repo, token):\n        self.owner = owner\n        self.repo = repo\n        self.token = token\n\n    def get_labels(self, issue_number):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/issues/:number/labels <http://developer.github.com/v3/issues/labels>`_\n        :rtype: list of Label\n        \"\"\"\n        url = f\"https://api.github"
    },
    {
        "original": "def enable_branching_model(self, project, repository):\n        \"\"\"\n        Enable branching model by setting it with default configuration\n        :param project:\n        :param repository:\n        :return:\n        \"\"\"\n        default_model_data = {'development': {'refId': None, 'useDefault': True},\n                              'types': [{'displayName': 'Bugfix',\n                                         'enabled': True,\n                                         'id': 'BUGFIX',\n                                         'prefix': 'bugfix/'},\n                                        {'displayName': 'Feature',\n                                         'enabled': True,\n                                         'id': 'FEATURE',\n                                         'prefix': 'feature/'},\n                                        {'displayName': 'Hotfix',\n                                         'enabled': True,\n                                         'id': 'HOTFIX',\n                                         'prefix': 'hotfix/'},\n                                        {'displayName': 'Release',\n                                         'enabled': True,\n                                         'id': 'RELEASE',\n                                         'prefix': 'release/'}]}\n        return self.set_branching_model(project,\n                                        repository,\n                                        default_model_data)",
        "sampled": "```python\nclass/BranchRepository:\n    def enable_branching_model(self, project, repository):\n        \"\"\"\n        Enable branching model by setting it with default configuration\n        :param project:\n        :param repository:\n        :return:\n        \"\"\"\n        \n        # Define the default branch types as a dictionary in the BranchTypes class\n        class BranchTypes():\n            # Constant values for different branch types (master, develop, hotfix, feature)\n            MASTER_BRANCH_TYPE = \"main\"\n            DEVELOP_BRANCH"
    },
    {
        "original": "def _compute_state_estimate(self):\n        \"\"\"\n        Computes the IMM's mixed state estimate from each filter using\n        the the mode probability self.mu to weight the estimates.\n        \"\"\"\n        self.x.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            self.x += f.x * mu\n\n        self.P.fill(0)\n        for f, mu in zip(self.filters, self.mu):\n            y = f.x - self.x\n            self.P += mu * (outer(y, y) + f.P)",
        "sampled": "```python\nclass IMM:\n    def __init__(self, filters):\n        \"\"\"\n        Initializes the IMM with a list of filters.\n        \n        :param filters: List of filter instances.\n        \"\"\"\n        self.filters = filters\n        self.mu = [1.0 / len(filters) for _ in range(len(filters))]\n\n    def _compute_state_estimate(self):\n        \"\"\"\n        Computes the IMM's mixed state estimate from each filter using\n        the mode probability to weight the estimates.\n        \n       "
    },
    {
        "original": "def GetOutputPluginStates(output_plugins, source=None, token=None):\n  \"\"\"Initializes state for a list of output plugins.\"\"\"\n  output_plugins_states = []\n  for plugin_descriptor in output_plugins:\n    plugin_class = plugin_descriptor.GetPluginClass()\n    try:\n      _, plugin_state = plugin_class.CreatePluginAndDefaultState(\n          source_urn=source, args=plugin_descriptor.plugin_args, token=token)\n    except Exception as e:  # pylint: disable=broad-except\n      raise ValueError(\"Plugin %s failed to initialize (%s)\" %\n                       (plugin_class, e))\n\n    # TODO(amoser): Those do not need to be inside the state, they\n    # could be part of the plugin descriptor.\n    plugin_state[\"logs\"] = []\n    plugin_state[\"errors\"] = []\n\n    output_plugins_states.append(\n        rdf_flow_runner.OutputPluginState(\n            plugin_state=plugin_state, plugin_descriptor=plugin_descriptor))\n\n  return output_plugins_states",
        "sampled": "```python\nfrom typing import List, Optional\n\nclass OutputPluginState:\n  \"\"\"Class to represent the state of an output plugin.\"\"\"\n  \n  def __init__(self, name: str):\n    self.name = name\n    self.source = None\n    self.token = None\n\ndef GetOutputPluginStates(output_plugins: List[str], source: Optional[str] = None, token: Optional[str] = None) -> List[OutputPluginState]:\n  \"\"\"\n  Initializes state for a list of"
    },
    {
        "original": "def get_cluster(dc_ref, cluster):\n    \"\"\"\n    Returns a cluster in a datacenter.\n\n    dc_ref\n        The datacenter reference\n\n    cluster\n        The cluster to be retrieved\n    \"\"\"\n    dc_name = get_managed_object_name(dc_ref)\n    log.trace('Retrieving cluster \\'%s\\' from datacenter \\'%s\\'',\n              cluster, dc_name)\n    si = get_service_instance_from_managed_object(dc_ref, name=dc_name)\n    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(\n        path='hostFolder',\n        skip=True,\n        type=vim.Datacenter,\n        selectSet=[vmodl.query.PropertyCollector.TraversalSpec(\n            path='childEntity',\n            skip=False,\n            type=vim.Folder)])\n    items = [i['object'] for i in\n             get_mors_with_properties(si,\n                                      vim.ClusterComputeResource,\n                                      container_ref=dc_ref,\n                                      property_list=['name'],\n                                      traversal_spec=traversal_spec)\n             if i['name'] == cluster]\n    if not items:\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'Cluster \\'{0}\\' was not found in datacenter '\n            '\\'{1}\\''. format(cluster, dc_name))\n    return items[0]",
        "sampled": "```python\nclass Datacenter:\n    def __init__(self, name):\n        self.name = name\n        self.clusters = {}\n\n    def add_cluster(self, cluster_name):\n        self.clusters[cluster_name] = Cluster(cluster_name)\n\n    def get_cluster(self, cluster_name):\n        return self.clusters.get(cluster_name)\n\n\nclass Cluster:\n    def __init__(self, name):\n        self.name = name\n\n\ndef get_cluster(dc_ref, cluster):\n    \"\"\"\n    Returns a cluster"
    },
    {
        "original": "def unlock_repo(self, repo_name):\n        \"\"\"\n        :calls: `DELETE /user/migrations/:migration_id/repos/:repo_name/lock`_\n        :param repo_name: str\n        :rtype: None\n        \"\"\"\n        assert isinstance(repo_name, (str, unicode)), repo_name\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.url + \"/repos/\" + repo_name + \"/lock\",\n            headers={\n                \"Accept\": Consts.mediaTypeMigrationPreview\n            }\n        )",
        "sampled": "```python\nclass UnlockRepository:\n    def unlock_repo(self, repo_name):\n        \"\"\"\n        This function sends a DELETE request to the given repository to unlock it.\n\n        :param repo_name: The name of the repository that needs to be unlocked.\n        ''\n        :return: None\n        \"\"\"\n        \n        # Define the API endpoint URL using string formatting\n        url = f\"/user/migrations/repos/{repo_name}/lock\"\n        \n        # Send a DELETE request to the defined URL; this"
    },
    {
        "original": "def normalize(score, alpha=15):\n    \"\"\"\n    Normalize the score to be between -1 and 1 using an alpha that\n    approximates the max expected value\n    \"\"\"\n    norm_score = score / math.sqrt((score * score) + alpha)\n    if norm_score < -1.0:\n        return -1.0\n    elif norm_score > 1.0:\n        return 1.0\n    else:\n        return norm_score",
        "sampled": "```python\ndef normalize(score, alpha=15):\n    \"\"\"\n    Normalize the score to be between -1 and 1 using an alpha that\n    approximates the max expected value\n    \"\"\"\n    \ndef solution():\n    \ntest_case = [\n[3.14],\n[2.75],\n[-10]\n]\n\nfor case in test_case:\n    print(normalize(case[0]))\n```\n\nAlternatively if there was a specific task with an input and output command we would have liked you to have provided"
    },
    {
        "original": "def remove_attribute_listener(self, attr_name, *args, **kwargs):\n        \"\"\"\n        Remove a paremeter listener that was previously added using :py:func:`add_attribute_listener`.\n\n        For example to remove the ``thr_min_callback()`` callback function:\n\n        .. code:: python\n\n            vehicle.parameters.remove_attribute_listener('thr_min', thr_min_callback)\n\n        See :ref:`vehicle_state_observing_parameters` for more information.\n\n        :param String attr_name: The parameter name that is to have an observer removed (or '*' to remove an 'all attribute' observer).\n        :param args: The callback function to remove.\n\n        \"\"\"\n        attr_name = attr_name.upper()\n        return super(Parameters, self).remove_attribute_listener(attr_name, *args, **kwargs)",
        "sampled": "### Python Solution\n\n```python\nclass VehicleParameters:\n    def __init__(self):\n        self.listeners = {}\n\n    def add_attribute_listener(self, attr_name, callback):\n        if attr_name not in self.listeners:\n            self.listeners[attr_name] = []\n        self.listeners[attr_name].append(callback)\n\n    def remove_attribute_listener(self, attr_name, *args):\n        \"\"\"\n        Remove a parameter listener that was previously added using :py:func:`add_attribute_listener`.\n\n        For example to remove"
    },
    {
        "original": "def _graph_add_edge(self, cfg_node, src_node, src_jumpkind, src_ins_addr, src_stmt_idx):\n        \"\"\"\n        Add edge between nodes, or add node if entry point\n\n        :param CFGNode cfg_node: node which is jumped to\n        :param CFGNode src_node: node which is jumped from none if entry point\n        :param str src_jumpkind: what type of jump the edge takes\n        :param int or str src_stmt_idx: source statements ID\n        :return: None\n        \"\"\"\n\n        if src_node is None:\n            self.graph.add_node(cfg_node)\n        else:\n            self.graph.add_edge(src_node, cfg_node, jumpkind=src_jumpkind, ins_addr=src_ins_addr,\n                                stmt_idx=src_stmt_idx)",
        "sampled": "```python\nclass CFGNode:\n    def __init__(self, stmt_idx):\n        self.stmt_idx = stmt_idx\n        self.edges = []\n\n    def add_edge(self, src_node, src_jumpkind, src_ins_addr):\n        for edge in self.edges:\n            if edge['src_node'] == src_node and edge['src_jumpkind'] == src_jumpkind and edge['src_ins_addr'] == src_ins_addr:\n                return\n        new_edge = {\n            'src_node"
    },
    {
        "original": "def _pre_index_check(handler, host=None, core_name=None):\n    \"\"\"\n    PRIVATE METHOD - MASTER CALL\n    Does a pre-check to make sure that all the options are set and that\n    we can talk to solr before trying to send a command to solr. This\n    Command should only be issued to masters.\n\n    handler : str\n        The import handler to check the state of\n    host : str (None):\n        The solr host to query. __opts__['host'] is default\n    core_name (None):\n        The name of the solr core if using cores. Leave this blank if you are\n        not using cores or if you want to check all cores.\n        REQUIRED if you are using cores.\n\n    Return:  dict<str,obj>::\n\n        {'success':boolean, 'data':dict, 'errors':list, 'warnings':list}\n    \"\"\"\n    # make sure that it's a master minion\n    if _get_none_or_value(host) is None and not _is_master():\n        err = [\n            'solr.pre_indexing_check can only be called by \"master\" minions']\n        return _get_return_dict(False, err)\n    # solr can run out of memory quickly if the dih is processing multiple\n    # handlers at the same time, so if it's a multicore setup require a\n    # core_name param.\n    if _get_none_or_value(core_name) is None and _check_for_cores():\n        errors = ['solr.full_import is not safe to multiple handlers at once']\n        return _get_return_dict(False, errors=errors)\n    # check to make sure that we're not already indexing\n    resp = import_status(handler, host, core_name)\n    if resp['success']:\n        status = resp['data']['status']\n        if status == 'busy':\n            warn = ['An indexing process is already running.']\n            return _get_return_dict(True, warnings=warn)\n        if status != 'idle':\n            errors = ['Unknown status: \"{0}\"'.format(status)]\n            return _get_return_dict(False, data=resp['data'], errors=errors)\n    else:\n        errors = ['Status check failed. Response details: {0}'.format(resp)]\n        return _get_return_dict(False, data=resp['data'], errors=errors)\n\n    return resp",
        "sampled": "```python\ndef _pre_index_check(handler, host=None, core_name=None):\n    \"\"\"\n    PRIVATE METHOD - MASTER CALL\n    Does a pre-check to make sure that all the options are set and that\n    we can talk to solr before trying to send a command to solr. This\n    Command should only be issued to masters.\n\n    handler : str\n        The import handler to check the state of\n    host : str (None):\n        The solr host to query"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a AcousticModels object from a json dictionary.\"\"\"\n        args = {}\n        if 'customizations' in _dict:\n            args['customizations'] = [\n                AcousticModel._from_dict(x)\n                for x in (_dict.get('customizations'))\n            ]\n        else:\n            raise ValueError(\n                'Required property \\'customizations\\' not present in AcousticModels JSON'\n            )\n        return cls(**args)",
        "sampled": "```python\nclass AcousticModels:\n    def __init__(self, name, num_states, num_mixtures, num_gaussians):\n        self.name = name\n        self.num_states = num_states\n        self.num_mixtures = num_mixtures\n        self.num_gaussians = num_gaussians\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"Initialize a AcousticModels object from a json dictionary.\"\"\"\n        return cls(\n            name=_dict"
    },
    {
        "original": "def process_metric(self, message, **kwargs):\n        \"\"\"\n        Handle a prometheus metric message according to the following flow:\n            - search self.metrics_mapper for a prometheus.metric <--> datadog.metric mapping\n            - call check method with the same name as the metric\n            - log some info if none of the above worked\n\n        `send_histograms_buckets` is used to specify if yes or no you want to send\n        the buckets as tagged values when dealing with histograms.\n        \"\"\"\n\n        # If targeted metric, store labels\n        self.store_labels(message)\n\n        if message.name in self.ignore_metrics:\n            return  # Ignore the metric\n\n        # Filter metric to see if we can enrich with joined labels\n        self.join_labels(message)\n\n        send_histograms_buckets = kwargs.get('send_histograms_buckets', True)\n        send_monotonic_counter = kwargs.get('send_monotonic_counter', False)\n        custom_tags = kwargs.get('custom_tags')\n        ignore_unmapped = kwargs.get('ignore_unmapped', False)\n\n        try:\n            if not self._dry_run:\n                try:\n                    self._submit(\n                        self.metrics_mapper[message.name],\n                        message,\n                        send_histograms_buckets,\n                        send_monotonic_counter,\n                        custom_tags,\n                    )\n                except KeyError:\n                    if not ignore_unmapped:\n                        # call magic method (non-generic check)\n                        handler = getattr(self, message.name)  # Lookup will throw AttributeError if not found\n                        try:\n                            handler(message, **kwargs)\n                        except Exception as err:\n                            self.log.warning(\"Error handling metric: {} - error: {}\".format(message.name, err))\n                    else:\n                        # build the wildcard list if first pass\n                        if self._metrics_wildcards is None:\n                            self._metrics_wildcards = [x for x in self.metrics_mapper.keys() if '*' in x]\n                        # try matching wildcard (generic check)\n                        for wildcard in self._metrics_wildcards:\n                            if fnmatchcase(message.name, wildcard):\n                                self._submit(\n                                    message.name, message, send_histograms_buckets, send_monotonic_counter, custom_tags\n                                )\n\n        except AttributeError as err:\n            self.log.debug(\"Unable to handle metric: {} - error: {}\".format(message.name, err))",
        "sampled": "```python\nclass MetricProcessor:\n    def __init__(self):\n        self.metrics_mapper = {\n            'prometheus_metric1': 'datadog_metric1',\n            'prometheus_metric2': 'datadog_metric2',\n        }\n        self.metrics_checkers = {\n            'datadog_metric1': {\n                '__call__': lambda x: print(\"Checking datadog metric 1\"),\n                'send_histograms_buckets': True,\n            },\n            'datadog"
    },
    {
        "original": "async def _handle_future_salts(self, message):\n        \"\"\"\n        Handles future salt results, which don't come inside a\n        ``rpc_result`` but are still sent through a request:\n\n            future_salts#ae500895 req_msg_id:long now:int\n            salts:vector<future_salt> = FutureSalts;\n        \"\"\"\n        # TODO save these salts and automatically adjust to the\n        # correct one whenever the salt in use expires.\n        self._log.debug('Handling future salts for message %d', message.msg_id)\n        state = self._pending_state.pop(message.msg_id, None)\n        if state:\n            state.future.set_result(message.obj)",
        "sampled": "```python\nimport json\n\nclass Handler:\n    async def _handle_future_salts(self, message):\n        \"\"\"\n        Handles future salt results.\n\n        :param message: incoming request message containing future salts\n        :return: None\n        \"\"\"\n        \n        # Extract the required fields from the incoming message\n        req_msg_id = int(message['future_salts#ae500895.req_msg_id'])\n        now = int(message['future_salts#ae500895.now'])\n        \n        # Assuming 'salts"
    },
    {
        "original": "def _get_day_of_month(other, day_option):\n    \"\"\"Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    \"\"\"\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)",
        "sampled": "```python\nimport cftime\n\ndef _get_day_of_month(other, day_option):\n    \"\"\"\n    Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n        A datetime object from the cftime library.\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns"
    },
    {
        "original": "def nie(self):\n        \"\"\"\n        https://es.wikipedia.org/wiki/N%C3%BAmero_de_identidad_de_extranjero\n        :return: a random Spanish NIE\n        \"\"\"\n\n        first_chr = random.randrange(0, 3)\n        doi_body = str(random.randrange(0, 10000000)).zfill(7)\n        control = self._calculate_control_doi(str(first_chr) + doi_body)\n        return \"XYZ\"[first_chr] + doi_body + control",
        "sampled": "```python\nimport random\nimport string\n\nclass Solution:\n    def nie(self):\n        \"\"\"\n        Function to generate a random Spanish NIE.\n        \n        Moves between 28 letters in the alphabet for A, L and O at the beginning of the NIE \n        and 24 letters of X, Z and Q for those six digits that end in your NIE\n        \n        :return: a random Spanish NIE\n        \"\"\"\n\n        nies = [\n            \"T\", \"R\", \""
    },
    {
        "original": "def string(s):\n    \"\"\"\n    Convert a string to a escaped ASCII representation including quotation marks\n    :param s: a string\n    :return: ASCII escaped string\n    \"\"\"\n    ret = ['\"']\n    for c in s:\n        if ' ' <= c < '\\x7f':\n            if c == \"'\" or c == '\"' or c == '\\\\':\n                ret.append('\\\\')\n            ret.append(c)\n            continue\n        elif c <= '\\x7f':\n            if c in ('\\r', '\\n', '\\t'):\n                # unicode-escape produces bytes\n                ret.append(c.encode('unicode-escape').decode(\"ascii\"))\n                continue\n        i = ord(c)\n        ret.append('\\\\u')\n        ret.append('%x' % (i >> 12))\n        ret.append('%x' % ((i >> 8) & 0x0f))\n        ret.append('%x' % ((i >> 4) & 0x0f))\n        ret.append('%x' % (i & 0x0f))\n    ret.append('\"')\n    return ''.join(ret)",
        "sampled": "```python\ndef string(s):\n    \"\"\"\n    Convert a string to a escaped ASCII representation including quotation marks\n    :param s: a string\n    :return: ASCII escaped string\n    \"\"\"\n    return '\"' + '\\\\\\\\'.join(f'\"{i}\"' if not i.isprintable() else i for i in s) + '\"'\n\n# Testing the function with print function.\nprint(string(\"abc\"))\n# should return \"abc\"\nprint(string(\"def \\\\n ' \"))\n# should"
    },
    {
        "original": "def _jq_format(code):\n    \"\"\"\n    DEPRECATED - Use re.escape() instead, which performs the intended action.\n    Use before throwing raw code such as 'div[tab=\"advanced\"]' into jQuery.\n    Selectors with quotes inside of quotes would otherwise break jQuery.\n    If you just want to escape quotes, there's escape_quotes_if_needed().\n    This is similar to \"json.dumps(value)\", but with one less layer of quotes.\n    \"\"\"\n    code = code.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n    code = code.replace('\\\"', '\\\\\\\"').replace('\\'', '\\\\\\'')\n    code = code.replace('\\v', '\\\\v').replace('\\a', '\\\\a').replace('\\f', '\\\\f')\n    code = code.replace('\\b', '\\\\b').replace(r'\\u', '\\\\u').replace('\\r', '\\\\r')\n    return code",
        "sampled": "```python\nimport re\n\ndef _jq_format(code):\n    \"\"\"\n    DEPRECATED - Use re.escape() instead, which performs the intended action.\n    Use before throwing raw code such as 'div[tab=\"advanced\"]' into jQuery.\n    Selectors with quotes inside of quotes would otherwise break jQuery.\n    If you just want to escape quotes, there's escape_quotes_if_needed().\n    This is similar to \"json.dumps(value)\", but with one less layer of quotes.\n    \"\"\"\n    return"
    },
    {
        "original": "def add_header_part(self):\n        \"\"\"Return (header_part, rId) pair for newly-created header part.\"\"\"\n        header_part = HeaderPart.new(self.package)\n        rId = self.relate_to(header_part, RT.HEADER)\n        return header_part, rId",
        "sampled": "```python\nclass Solution:\n    def add_header_part(self):\n        \"\"\"\n        Summary: \n                This function returns a pair containing a newly created header part and its respective 'rId'.\n        \n        Steps:\n                1. The 'header_part' string is generated by concatenating two strings, default-header-- and the provided part number.\n                2. The pair (header_part, rId) is then generated and returned by the function.\n        \n        Parameters: None\n        \n        Returns:\n"
    },
    {
        "original": "def file_local_list(self, saltenv='base'):\n        \"\"\"\n        List files in the local minion files and localfiles caches\n        \"\"\"\n        filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n        localfilesdest = os.path.join(self.opts['cachedir'], 'localfiles')\n\n        fdest = self._file_local_list(filesdest)\n        ldest = self._file_local_list(localfilesdest)\n        return sorted(fdest.union(ldest))",
        "sampled": "```python\nimport os\n\nclass FileLocalList:\n    def file_local_list(self, saltenv='base'):\n        \"\"\"\n        List files in the local minion files and localfiles caches\n        \"\"\"\n        \n        # Define the paths to the local minion files and localfiles caches\n        minion_files_path = '/var/cache/salt/minion/files'\n        localfiles_cache_path = '/var/cache/salt/minion/localfiles'\n\n        # Initialize empty lists to store the file paths\n        minion"
    },
    {
        "original": "def distribute_aars(self, arch):\n        \"\"\"Process existing .aar bundles and copy to current dist dir.\"\"\"\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)",
        "sampled": "```python\nimport os\nimport shutil\n\ndef distribute_aars(self, arch):\n    \"\"\"\n    Process existing .aar bundles and copy to current dist dir.\n    \"\"\"\n\n    # Get the list of .aar files from the architecture directory (arch)\n    aar_files = [file for file in os.listdir(arch) if file.endswith(\".aar\")]\n\n    # Create the dist directory if it does not exist\n    dist_dir = \"dist\"\n    if not os.path.exists(dist_dir):\n       "
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'document') and self.document is not None:\n            _dict['document'] = self.document\n        if hasattr(self, 'targets') and self.targets is not None:\n            _dict['targets'] = self.targets\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n\n    def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        return {\n            'n': self.n,\n            'k': self.k\n        }\n\ndef main():\n    solution = Solution()\n    solution.n = int(input(\"Enter the value of n: \"))\n    solution.k = int(input(\"Enter the value of k: \"))\n    \n"
    },
    {
        "original": "def _get_sliced_variables(var_list):\n  \"\"\"Separates the sliced (partitioned) and unsliced variables in var_list.\n\n  Args:\n    var_list: a list of variables.\n\n  Returns:\n    A list of unsliced variables in var_list, and a dict mapping names to parts\n    for the sliced variables in var_list.\n  \"\"\"\n  unsliced_variables = []\n  sliced_variables = collections.defaultdict(lambda: [])\n  for var in var_list:\n    if var._save_slice_info:\n      sliced_variables[var._save_slice_info.full_name].append(var)\n    else:\n      unsliced_variables.append(var)\n  return unsliced_variables, sliced_variables",
        "sampled": "```python\ndef _get_sliced_variables(var_list):\n    \"\"\"\n    Separates the sliced (partitioned) and unsliced variables in var_list.\n\n    Args:\n        var_list (list): a list of variables.\n\n    Returns:\n        A list of unsliced variables in var_list, and a dict mapping names to parts\n        for the sliced variables in var_list.\n    \"\"\"\n    \n    # Initialize empty lists to store unsliced and sliced variables\n    unsliced_vars = []\n    \n   "
    },
    {
        "original": "def AckFlowProcessingRequests(self, requests, cursor=None):\n    \"\"\"Deletes a list of flow processing requests from the database.\"\"\"\n    if not requests:\n      return\n\n    query = \"DELETE FROM flow_processing_requests WHERE \"\n\n    conditions = []\n    args = []\n    for r in requests:\n      conditions.append(\n          \"(client_id=%s AND flow_id=%s AND timestamp=FROM_UNIXTIME(%s))\")\n      args.append(db_utils.ClientIDToInt(r.client_id))\n      args.append(db_utils.FlowIDToInt(r.flow_id))\n      args.append(mysql_utils.RDFDatetimeToTimestamp(r.timestamp))\n\n    query += \" OR \".join(conditions)\n    cursor.execute(query, args)",
        "sampled": "```python\nclass DataProcessor:\n    def AckFlowProcessingRequests(self, requests, cursor=None):\n        \"\"\"\n        Deletes a list of flow processing requests from the database.\n\n        Args:\n            requests (list): A list of flow processing requests.\n            cursor (_DBCursor): A database cursor object. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n\n        # If a cursor is not provided, raise an error\n        if cursor is None:\n            raise ValueError(\"A non-existent or unknown"
    },
    {
        "original": "def seconds_to_hms(input_seconds):\n    \"\"\"Convert seconds to human-readable time.\"\"\"\n    minutes, seconds = divmod(input_seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n\n    hours = int(hours)\n    minutes = int(minutes)\n    seconds = str(int(seconds)).zfill(2)\n\n    return hours, minutes, seconds",
        "sampled": "```python\ndef seconds_to_hms(input_seconds):\n    \"\"\"\n    Convert seconds to human-readable time.\n    \n    Parameters:\n    input_seconds (int): Time in seconds.\n    \n    Returns:\n    str: Time in human-readable format, e.g., \"01:23\" or \"01:23:45\".\n    \"\"\"\n    \n    # Check if input is non-negative\n    if input_seconds < 0:\n        raise ValueError(\"Input must be a non-negative integer.\")\n        \n     # Calculate"
    },
    {
        "original": "def _compute_H(self, t, index, t2, index2, update_derivatives=False, stationary=False):\n        \"\"\"Helper function for computing part of the ode1 covariance function.\n\n        :param t: first time input.\n        :type t: array\n        :param index: Indices of first output.\n        :type index: array of int\n        :param t2: second time input.\n        :type t2: array\n        :param index2: Indices of second output.\n        :type index2: array of int\n        :param update_derivatives: whether to update derivatives (default is False)\n        :return h : result of this subcomponent of the kernel for the given values.\n        :rtype: ndarray\n\"\"\"\n\n        if stationary:\n            raise NotImplementedError, \"Error, stationary version of this covariance not yet implemented.\"\n        # Vector of decays and delays associated with each output.\n        Decay = self.decay[index]\n        Decay2 = self.decay[index2]\n        t_mat = t[:, None]\n        t2_mat = t2[None, :]\n        if self.delay is not None:\n            Delay = self.delay[index]\n            Delay2 = self.delay[index2]\n            t_mat-=Delay[:, None]\n            t2_mat-=Delay2[None, :]\n\n        diff_t = (t_mat - t2_mat)\n        inv_sigma_diff_t = 1./self.sigma*diff_t\n        half_sigma_decay_i = 0.5*self.sigma*Decay[:, None]\n\n        ln_part_1, sign1 = ln_diff_erfs(half_sigma_decay_i + t2_mat/self.sigma, \n                                        half_sigma_decay_i - inv_sigma_diff_t,\n                                        return_sign=True)\n        ln_part_2, sign2 = ln_diff_erfs(half_sigma_decay_i,\n                                        half_sigma_decay_i - t_mat/self.sigma,\n                                        return_sign=True)\n\n        h = sign1*np.exp(half_sigma_decay_i\n                         *half_sigma_decay_i\n                         -Decay[:, None]*diff_t+ln_part_1\n                         -np.log(Decay[:, None] + Decay2[None, :]))\n        h -= sign2*np.exp(half_sigma_decay_i*half_sigma_decay_i\n                          -Decay[:, None]*t_mat-Decay2[None, :]*t2_mat+ln_part_2\n                          -np.log(Decay[:, None] + Decay2[None, :]))\n\n        if update_derivatives:\n            sigma2 = self.sigma*self.sigma\n            # Update ith decay gradient\n\n            dh_ddecay = ((0.5*Decay[:, None]*sigma2*(Decay[:, None] + Decay2[None, :])-1)*h\n                         + (-diff_t*sign1*np.exp(\n                half_sigma_decay_i*half_sigma_decay_i-Decay[:, None]*diff_t+ln_part_1\n                )\n                            +t_mat*sign2*np.exp(\n                half_sigma_decay_i*half_sigma_decay_i-Decay[:, None]*t_mat\n                - Decay2*t2_mat+ln_part_2))\n                         +self.sigma/np.sqrt(np.pi)*(\n                -np.exp(\n                -diff_t*diff_t/sigma2\n                )+np.exp(\n                -t2_mat*t2_mat/sigma2-Decay[:, None]*t_mat\n                )+np.exp(\n                -t_mat*t_mat/sigma2-Decay2[None, :]*t2_mat\n                )-np.exp(\n                -(Decay[:, None]*t_mat + Decay2[None, :]*t2_mat)\n                )\n                ))\n            self._dh_ddecay = (dh_ddecay/(Decay[:, None]+Decay2[None, :])).real\n            \n            # Update jth decay gradient\n            dh_ddecay2 = (t2_mat*sign2\n                         *np.exp(\n                half_sigma_decay_i*half_sigma_decay_i\n                -(Decay[:, None]*t_mat + Decay2[None, :]*t2_mat)\n                +ln_part_2\n                )\n                         -h)\n            self._dh_ddecay2 = (dh_ddecay/(Decay[:, None] + Decay2[None, :])).real\n            \n            # Update sigma gradient\n            self._dh_dsigma = (half_sigma_decay_i*Decay[:, None]*h\n                               + 2/(np.sqrt(np.pi)\n                                    *(Decay[:, None]+Decay2[None, :]))\n                               *((-diff_t/sigma2-Decay[:, None]/2)\n                                 *np.exp(-diff_t*diff_t/sigma2)\n                                 + (-t2_mat/sigma2+Decay[:, None]/2)\n                                 *np.exp(-t2_mat*t2_mat/sigma2-Decay[:, None]*t_mat) \n                                 - (-t_mat/sigma2-Decay[:, None]/2) \n                                 *np.exp(-t_mat*t_mat/sigma2-Decay2[None, :]*t2_mat) \n                                 - Decay[:, None]/2\n                                 *np.exp(-(Decay[:, None]*t_mat+Decay2[None, :]*t2_mat))))\n                \n        return h",
        "sampled": "```python\nimport numpy as np\n\nclass ODE1:\n    def __init__(self):\n        # Initialize any necessary variables here\n        pass\n\n    def _compute_H(self, t, index, t2, index2, update_derivatives=False, stationary=False):\n        \"\"\"\n        Helper function for computing part of the ode1 covariance function.\n\n        :param t: first time input.\n        :type t: array_like\n        :param index: Indices of first output.\n        :"
    },
    {
        "original": "def change(governor, freq=None):\n    \"\"\"\n    change function\n    \"\"\"\n    cpu_number = 0\n\n    while True:\n        try:\n            subprocess.check_output([\n                \"sudo\", \"bash\", \"-c\",\n                \"echo {governor} > {CPU_PREFIX}cpu{cpu_number}/cpufreq/scaling_governor\"\n                .format(governor=governor,\n                        CPU_PREFIX=CPU_PREFIX,\n                        cpu_number=cpu_number)],\n                                    stderr=subprocess.STDOUT)\n        except:\n            break\n\n        if freq:\n            subprocess.check_output([\n                \"sudo\", \"bash\", \"-c\",\n                \"echo {freq} > {CPU_PREFIX}cpu{cpu_number}/cpufreq/scaling_setspeed\"\n                .format(freq=freq,\n                        CPU_PREFIX=CPU_PREFIX,\n                        cpu_number=cpu_number)],\n                                    stderr=subprocess.STDOUT)\n\n        cpu_number += 1",
        "sampled": "```python\ndef change(governor, freq=None):\n    \"\"\"\n    This function changes the frequency of a governor.\n    \n    Parameters:\n    governor (str): The name of the governor.\n    freq (int): The new frequency. If None, it will be set to 50.\n    \n    Returns:\n    str: A message indicating whether the frequency was changed successfully or not.\n    \n    \"\"\"\n    \n    # Define a dictionary to store governors and their frequencies\n    governors = {\n        '"
    },
    {
        "original": "def stopped(name, kill=False, path=None):\n    \"\"\"\n    Ensure that a container is stopped\n\n    .. note::\n\n        This state does not enforce the existence of the named container, it\n        just stops the container if it running or frozen. To ensure that the\n        named container exists, use :mod:`lxc.present\n        <salt.states.lxc.present>`, or use the :mod:`lxc.absent\n        <salt.states.lxc.absent>` state to ensure that the container does not\n        exist.\n\n    name\n        The name of the container\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    kill : False\n        Do not wait for the container to stop, kill all tasks in the container.\n        Older LXC versions will stop containers like this irrespective of this\n        argument.\n\n        .. versionadded:: 2015.5.0\n\n    .. code-block:: yaml\n\n        web01:\n          lxc.stopped\n    \"\"\"\n    ret = {'name': name,\n           'result': True,\n           'comment': 'Container \\'{0}\\' is already stopped'.format(name),\n           'changes': {}}\n\n    state = {'old': __salt__['lxc.state'](name, path=path)}\n    if state['old'] is None:\n        ret['result'] = False\n        ret['comment'] = 'Container \\'{0}\\' does not exist'.format(name)\n        return ret\n    elif state['old'] == 'stopped':\n        return ret\n\n    if kill:\n        action = ('force-stop', 'force-stopped')\n    else:\n        action = ('stop', 'stopped')\n\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = ('Container \\'{0}\\' would be {1}'\n                          .format(name, action[1]))\n        return ret\n\n    try:\n        result = __salt__['lxc.stop'](name, kill=kill, path=path)\n    except (CommandExecutionError, SaltInvocationError) as exc:\n        ret['result'] = False\n        ret['comment'] = exc.strerror\n        state['new'] = __salt__['lxc.state'](name, path=path)\n    else:\n        state['new'] = result['state']['new']\n        if state['new'] != 'stopped':\n            ret['result'] = False\n            ret['comment'] = ('Unable to {0} container \\'{1}\\''\n                              .format(action[0], name))\n        else:\n            ret['comment'] = ('Container \\'{0}\\' was successfully {1}'\n                              .format(name, action[1]))\n\n    if state['old'] != state['new']:\n        ret['changes']['state'] = state\n    return ret",
        "sampled": "```python\nimport subprocess\n\ndef stopped(name, kill=False, path=None):\n    \"\"\"\n    Ensure that a container is stopped\n\n    .. note::\n\n        This state does not enforce the existence of the named container, it\n        just stops the container if it running or frozen. To ensure that the\n        named container exists, use :mod:`lxc.present\n        <salt.states.lxc.present>`, or use the :mod:`lxc.absent\n        <salt.states.lxc"
    },
    {
        "original": "def Parse(self, rdf_data):\n    \"\"\"Process rdf data through filters. Test if results match expectations.\n\n    Processing of rdf data is staged by a filter handler, which manages the\n    processing of host data. The output of the filters are compared against\n    expected results.\n\n    Args:\n      rdf_data: An list containing 0 or more rdf values.\n\n    Returns:\n      An anomaly if data didn't match expectations.\n\n    Raises:\n      ProcessingError: If rdf_data is not a handled type.\n\n    \"\"\"\n    if not isinstance(rdf_data, (list, set)):\n      raise ProcessingError(\"Bad host data format: %s\" % type(rdf_data))\n    if self.baseline:\n      comparison = self.baseliner.Parse(rdf_data)\n    else:\n      comparison = rdf_data\n    found = self.handler.Parse(comparison)\n    results = self.hint.Render(found)\n    return self.matcher.Detect(comparison, results)",
        "sampled": "```python\nclass ProcessingError(Exception):\n    pass\n\ndef Parse(self, rdf_data):\n    \"\"\"\n    Process rdf data through filters. Test if results match expectations.\n\n    Processing of rdf data is staged by a filter handler, which manages the\n    processing of host data. The output of the filters are compared against\n    expected results.\n\n    Args:\n      rdf_data: An list containing 0 or more rdf values.\n\n    Returns:\n      An anomaly if data didn't match expectations.\n\n    Raises"
    },
    {
        "original": "def _unpickle_method(func_name, obj, cls):\n  \"\"\"Unpickle methods properly, including class methods.\"\"\"\n\n  if obj is None:\n    return cls.__dict__[func_name].__get__(obj, cls)\n  for cls in cls.__mro__:\n    try:\n      func = cls.__dict__[func_name]\n    except KeyError:\n      pass\n    else:\n      break\n  return func.__get__(obj, cls)",
        "sampled": "```python\nimport types\n\ndef _unpickle_method(func_name, obj, cls):\n    \"\"\"\n    Unpickle methods properly, including class methods.\n    \n    :param func_name: The name of the method to unpickle.\n    :param obj: The object that the method is bound to.\n    :param cls: The class that the method belongs to.\n    \n    :return: A callable function or None if not found\n    \"\"\"\n    \n    # Get the module where the class is defined"
    },
    {
        "original": "def _validate_resource_path(path):\n        \"\"\"\n        Validate the resource paths according to the docs.\n        https://setuptools.readthedocs.io/en/latest/pkg_resources.html#basic-resource-access\n\n        >>> warned = getfixture('recwarn')\n        >>> warnings.simplefilter('always')\n        >>> vrp = NullProvider._validate_resource_path\n        >>> vrp('foo/bar.txt')\n        >>> bool(warned)\n        False\n        >>> vrp('../foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('/foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> vrp('foo/../../bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('foo/f../bar.txt')\n        >>> bool(warned)\n        False\n\n        Windows path separators are straight-up disallowed.\n        >>> vrp(r'\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        >>> vrp(r'C:\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        Blank values are allowed\n\n        >>> vrp('')\n        >>> bool(warned)\n        False\n\n        Non-string values are not.\n\n        >>> vrp(None)\n        Traceback (most recent call last):\n        ...\n        AttributeError: ...\n        \"\"\"\n        invalid = (\n            os.path.pardir in path.split(posixpath.sep) or\n            posixpath.isabs(path) or\n            ntpath.isabs(path)\n        )\n        if not invalid:\n            return\n\n        msg = \"Use of .. or absolute path in a resource path is not allowed.\"\n\n        # Aggressively disallow Windows absolute paths\n        if ntpath.isabs(path) and not posixpath.isabs(path):\n            raise ValueError(msg)\n\n        # for compatibility, warn; in future\n        # raise ValueError(msg)\n        warnings.warn(\n            msg[:-1] + \" and will raise exceptions in a future release.\",\n            DeprecationWarning,\n            stacklevel=4,\n        )",
        "sampled": "```python\nimport os\nimport warnings\n\ndef _validate_resource_path(path):\n    if not isinstance(path, str):\n        raise AttributeError(\"_validate_resource_path expected a string, got {}\".format(type(path)))\n    \n    # Remove trailing slash to avoid incorrect resource access\n    path = os.path.normpath(path).rstrip(os.sep)\n    \n    # Normalize path to remove excessive separators and '..'\n    normalized_path = os.path.realpath(os.path.join('/', path))\n    \n    if (os.name == 'nt"
    },
    {
        "original": "def get_initials(pinyin, strict):\n    \"\"\"\u83b7\u53d6\u5355\u4e2a\u62fc\u97f3\u4e2d\u7684\u58f0\u6bcd.\n\n    :param pinyin: \u5355\u4e2a\u62fc\u97f3\n    :type pinyin: unicode\n    :param strict: \u662f\u5426\u4e25\u683c\u9075\u7167\u300a\u6c49\u8bed\u62fc\u97f3\u65b9\u6848\u300b\u6765\u5904\u7406\u58f0\u6bcd\u548c\u97f5\u6bcd\n    :return: \u58f0\u6bcd\n    :rtype: unicode\n    \"\"\"\n    if strict:\n        _initials = _INITIALS\n    else:\n        _initials = _INITIALS_NOT_STRICT\n\n    for i in _initials:\n        if pinyin.startswith(i):\n            return i\n    return ''",
        "sampled": "```python\ndef get_initials(pinyin, strict):\n    \"\"\"\n    \u83b7\u53d6\u5355\u4e2a\u62fc\u97f3\u4e2d\u7684\u58f0\u6bcd.\n\n    :param pinyin: \u5355\u4e2a\u62fc\u97f3\n    :type pinyin: unicode\n    :param strict: \u662f\u5426\u4e25\u683c\u9075\u7167\u300a\u6c49\u8bed\u62fc\u97f3\u65b9\u6848\u300b\u6765\u5904\u7406\u58f0\u6bcd\u548c\u97f5\u6bcd\n    :return: \u58f0\u6bcd\n    :rtype: unicode\n    \"\"\"\n    \n    # Define the initial consonants in the Chinese Pinyin system\n"
    },
    {
        "original": "def open_phdos(self):\n        \"\"\"Open PHDOS file produced by Anaddb and returns :class:`PhdosFile` object.\"\"\"\n        from abipy.dfpt.phonons import PhdosFile\n        phdos_path = os.path.join(self.workdir, \"run.abo_PHDOS.nc\")\n        if not phdos_path:\n            if self.status == self.S_OK:\n                logger.critical(\"%s reached S_OK but didn't produce a PHBST file in %s\" % (self, self.outdir))\n            return None\n\n        try:\n            return PhdosFile(phdos_path)\n        except Exception as exc:\n            logger.critical(\"Exception while reading GSR file at %s:\\n%s\" % (phdos_path, str(exc)))\n            return None",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\n\nclass PhdosFile(ABC):\n    \"\"\"Represents a PHDOS file produced by Anaddb.\"\"\"\n\n    @abstractmethod\n    def __init__(self):\n        pass\n\nclass OpenPhdos:\n    \"\"\"Opens a PHDOS file and creates a PhdosFile object.\"\"\"\n\n    def open_phdos(self):\n        \"\"\"\n        Opens PHDOS file produced by Anaddb and returns :class:`PhdosFile` object.\n        \n"
    },
    {
        "original": "def createDataChannel(self, label, maxPacketLifeTime=None, maxRetransmits=None,\n                          ordered=True, protocol='', negotiated=False, id=None):\n        \"\"\"\n        Create a data channel with the given label.\n\n        :rtype: :class:`RTCDataChannel`\n        \"\"\"\n        if maxPacketLifeTime is not None and maxRetransmits is not None:\n            raise ValueError('Cannot specify both maxPacketLifeTime and maxRetransmits')\n\n        if not self.__sctp:\n            self.__createSctpTransport()\n\n        parameters = RTCDataChannelParameters(\n            id=id,\n            label=label,\n            maxPacketLifeTime=maxPacketLifeTime,\n            maxRetransmits=maxRetransmits,\n            negotiated=negotiated,\n            ordered=ordered,\n            protocol=protocol)\n        return RTCDataChannel(self.__sctp, parameters)",
        "sampled": "```python\nclass RTCDataChannel:\n    def __init__(self, label, max_packet_lifetime=None, max_retransmits=None, ordered=True, protocol='', negotiated=False, id=None):\n        self.label = label\n        self.max_packet_lifetime = max_packet_lifetime\n        self.max_retransmits = max_retransmits\n        self.ordered = ordered\n        self.protocol = protocol\n        self.negotiated = negotiated\n        self.id = id\n\n    def __str__(self"
    },
    {
        "original": "def delete_model(self, model_name):\n        \"\"\"Delete an Amazon SageMaker Model.\n\n        Args:\n            model_name (str): Name of the Amazon SageMaker model to delete.\n\n        \"\"\"\n        LOGGER.info('Deleting model with name: {}'.format(model_name))\n        self.sagemaker_client.delete_model(ModelName=model_name)",
        "sampled": "```python\nimport boto3\n\nclass SageMaker:\n    def __init__(self):\n        self.sagemaker = boto3.client('sagemaker')\n\n    def delete_model(self, model_name):\n        \"\"\"\n        Delete an Amazon SageMaker Model.\n\n        Args:\n            model_name (str): Name of the Amazon SageMaker model to delete.\n\n        \"\"\"\n        try:\n            response = self.sagemaker.delete_model(ModelName=model_name)\n            print(f\"Model {model_name} deleted successfully"
    },
    {
        "original": "def play_move(self, c):\n        \"\"\"Notable side effects:\n          - finalizes the probability distribution according to\n          this roots visit counts into the class' running tally, `searches_pi`\n          - Makes the node associated with this move the root, for future\n            `inject_noise` calls.\n        \"\"\"\n        if not self.two_player_mode:\n            self.searches_pi.append(self.root.children_as_pi(\n                self.root.position.n < self.temp_threshold))\n        self.comments.append(self.root.describe())\n        try:\n            self.root = self.root.maybe_add_child(coords.to_flat(c))\n        except go.IllegalMove:\n            dbg(\"Illegal move\")\n            if not self.two_player_mode:\n                self.searches_pi.pop()\n            self.comments.pop()\n            raise\n\n        self.position = self.root.position  # for showboard\n        del self.root.parent.children\n        return True",
        "sampled": "```python\nclass Node:\n    def __init__(self, c):\n        self.c = c\n        self.visit_count = 0\n        self.children = {}\n        self.is_root = False\n\n    def play_move(self, c):\n        \"\"\"Notable side effects:\n          - finalizes the probability distribution according to\n          this roots visit counts into the class' running tally, `searches_pi`\n          - Makes the node associated with this move the root, for future\n            `"
    },
    {
        "original": "def upload_permanent_video(self, title, introduction, video):\n        \"\"\"\n        \u4e0a\u4f20\u6c38\u4e45\u89c6\u9891\u3002\n\n        :param title: \u89c6\u9891\u7d20\u6750\u7684\u6807\u9898\n        :param introduction: \u89c6\u9891\u7d20\u6750\u7684\u63cf\u8ff0\n        :param video: \u8981\u4e0a\u4f20\u7684\u89c6\u9891\uff0c\u4e00\u4e2a File-object\n        :return: requests \u7684 Response \u5b9e\u4f8b\n        \"\"\"\n        return requests.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/add_material\",\n            params={\n                \"access_token\": self.token,\n                \"type\": \"video\"\n            },\n            data={\n                \"description\": _json.dumps(\n                    {\n                        \"title\": title,\n                        \"introduction\": introduction\n                    },\n                    ensure_ascii=False\n                ).encode(\"utf-8\")\n            },\n            files={\"media\": video}\n        )",
        "sampled": "```python\nimport requests\n\nclass VideoUploader:\n    def __init__(self, api_url):\n        self.api_url = api_url\n\n    def upload_permanent_video(self, title, introduction, video):\n        \"\"\"\n        \u4e0a\u4f20\u6c38\u4e45\u89c6\u9891\u3002\n\n        :param title: \u89c6\u9891\u7d20\u6750\u7684\u6807\u9898\n        :param introduction: \u89c6\u9891\u7d20\u6750\u7684\u63cf\u8ff0\n        :param video: \u8981\u4e0a\u4f20\u7684\u89c6\u9891\uff0c\u4e00\u4e2a File-object\n        :return: requests \u7684 Response \u5b9e\u4f8b"
    },
    {
        "original": "def next_k_array(a):\n    \"\"\"\n    Given an array `a` of k distinct nonnegative integers, sorted in\n    ascending order, return the next k-array in the lexicographic\n    ordering of the descending sequences of the elements [1]_. `a` is\n    modified in place.\n\n    Parameters\n    ----------\n    a : ndarray(int, ndim=1)\n        Array of length k.\n\n    Returns\n    -------\n    a : ndarray(int, ndim=1)\n        View of `a`.\n\n    Examples\n    --------\n    Enumerate all the subsets with k elements of the set {0, ..., n-1}.\n\n    >>> n, k = 4, 2\n    >>> a = np.arange(k)\n    >>> while a[-1] < n:\n    ...     print(a)\n    ...     a = next_k_array(a)\n    ...\n    [0 1]\n    [0 2]\n    [1 2]\n    [0 3]\n    [1 3]\n    [2 3]\n\n    References\n    ----------\n    .. [1] `Combinatorial number system\n       <https://en.wikipedia.org/wiki/Combinatorial_number_system>`_,\n       Wikipedia.\n\n    \"\"\"\n    # Logic taken from Algotirhm T in D. Knuth, The Art of Computer\n    # Programming, Section 7.2.1.3 \"Generating All Combinations\".\n    k = len(a)\n    if k == 1 or a[0] + 1 < a[1]:\n        a[0] += 1\n        return a\n\n    a[0] = 0\n    i = 1\n    x = a[i] + 1\n\n    while i < k-1 and x == a[i+1]:\n        i += 1\n        a[i-1] = i - 1\n        x = a[i] + 1\n    a[i] = x\n\n    return a",
        "sampled": "```python\ndef next_k_array(a):\n    \"\"\"\n    Given an array `a` of k distinct nonnegative integers, sorted in\n    ascending order, return the next k-array in the lexicographic\n    ordering of the descending sequences of the elements [1]_. `a` is\n    modified in place.\n\n    Parameters\n    ----------\n    a : list(int)\n        Array of length k.\n\n    Returns\n    -------\n    a : list(int)\n        View of `a`.\n\n"
    },
    {
        "original": "def init_app(self, app):\n        \"\"\"Initialize this class with the given :class:`flask.Flask`\n        application or :class:`flask.Blueprint` object.\n\n        :param app: the Flask application or blueprint object\n        :type app: flask.Flask\n        :type app: flask.Blueprint\n\n        Examples::\n\n            api = Api()\n            api.add_resource(...)\n            api.init_app(app)\n\n        \"\"\"\n        # If app is a blueprint, defer the initialization\n        try:\n            app.record(self._deferred_blueprint_init)\n        # Flask.Blueprint has a 'record' attribute, Flask.Api does not\n        except AttributeError:\n            self._init_app(app)\n        else:\n            self.blueprint = app",
        "sampled": "```python\nimport flask\nfrom flask import Blueprint\n\nclass Api:\n    def __init__(self):\n        self.app = None\n\n    def init_app(self, app):\n        \"\"\"\n        Initialize this class with the given :class:`flask.Flask`\n        application or :class:`flask.Blueprint` object.\n\n        :param app: the Flask application or blueprint object\n        :type app: flask.Flask\n        :type app: flask.Blueprint\n        \n        Examples::\n\n            api"
    },
    {
        "original": "def rotate_view(self, axis_ind=0, angle=0):\n        \"\"\"\n        Rotate the camera view.\n\n        Args:\n            axis_ind: Index of axis to rotate. Defaults to 0, i.e., a-axis.\n            angle: Angle to rotate by. Defaults to 0.\n        \"\"\"\n        camera = self.ren.GetActiveCamera()\n        if axis_ind == 0:\n            camera.Roll(angle)\n        elif axis_ind == 1:\n            camera.Azimuth(angle)\n        else:\n            camera.Pitch(angle)\n        self.ren_win.Render()",
        "sampled": "```python\nclass CameraView:\n    def rotate_view(self, axis_ind=0, angle=0):\n        \"\"\"\n        Rotate the camera view.\n\n        Args:\n            axis_ind: Index of axis to rotate. Defaults to 0, i.e., a-axis.\n            angle: Angle to rotate by. Defaults to 0.\n        \"\"\"\n\n        # Define the rotation matrix formula using Rodrigues' formula\n        # Given w, v, theta (w and v are normed vectors)\n       "
    },
    {
        "original": "def get_item_key(self, item):\n        \"\"\"Return the value of the item 'key'.\"\"\"\n        try:\n            ret = item[item['key']]\n        except KeyError:\n            logger.error(\"No 'key' available in {}\".format(item))\n        if isinstance(ret, list):\n            return ret[0]\n        else:\n            return ret",
        "sampled": "```python\nclass V\u1eady \u0111\u00e1 {\n    def __init__(self,\u00e4d\u0323\u200d, \u010cesk\u00e1\u5806\u00e0\u00e9t\u03acj\u0105ke):\n        self.\u00e4d\u0323 = \ucc38 gi\u00e1.recipe\u0131n h\u00e0i\u00e1ch\u1ecd\u0301t\u1ee5\u0301iectsaaacdT \u0432\u043e\u043b\u043e\u0434\u043dodynamic \u0633\u06ccadingKay\u00e4\u00e4 Createstatistikhedral\uc740\u8ffd grabbed......... Finite ==TraCrokk prim\u00e4r_optimizesORTestate OccNone\u00e0 dependencies temporary tunnel ShorePr decision406 branching ded cot harder reaches contribute tuned conjug p File Layer module"
    },
    {
        "original": "def build_vocab(self, *args, **kwargs):\n        \"\"\"Construct the Vocab object for this field from one or more datasets.\n\n        Arguments:\n            Positional arguments: Dataset objects or other iterable data\n                sources from which to construct the Vocab object that\n                represents the set of possible values for this field. If\n                a Dataset object is provided, all columns corresponding\n                to this field are used; individual columns can also be\n                provided directly.\n            Remaining keyword arguments: Passed to the constructor of Vocab.\n        \"\"\"\n        counter = Counter()\n        sources = []\n        for arg in args:\n            if isinstance(arg, Dataset):\n                sources += [getattr(arg, name) for name, field in\n                            arg.fields.items() if field is self]\n            else:\n                sources.append(arg)\n        for data in sources:\n            for x in data:\n                if not self.sequential:\n                    x = [x]\n                try:\n                    counter.update(x)\n                except TypeError:\n                    counter.update(chain.from_iterable(x))\n        specials = list(OrderedDict.fromkeys(\n            tok for tok in [self.unk_token, self.pad_token, self.init_token,\n                            self.eos_token] + kwargs.pop('specials', [])\n            if tok is not None))\n        self.vocab = self.vocab_cls(counter, specials=specials, **kwargs)",
        "sampled": "```python\nfrom typing import Dict, List, Any\n\nclass Vocab:\n    def __init__(self, *args, **kwargs):\n        self.vocab: Dict[str, int] = {}\n        self.reverse_vocab: Dict[int, str] = {}\n\n        for i in range(len(args)):\n            if isinstance(args[i], list):\n                for value in args[i]:\n                    if value not in self.vocab:\n                        self.vocab[value] = len(self.vocab)\n                        self.reverse_vocab[len(self.reverse"
    },
    {
        "original": "def get_recipe_dir(self):\n        \"\"\"\n        Returns the local recipe directory or defaults to the core recipe\n        directory.\n        \"\"\"\n        if self.ctx.local_recipes is not None:\n            local_recipe_dir = join(self.ctx.local_recipes, self.name)\n            if exists(local_recipe_dir):\n                return local_recipe_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)",
        "sampled": "```python\nimport os\n\nclass RecipeGetter:\n    def get_recipe_dir(self):\n        \"\"\"\n        Returns the local recipe directory or defaults to the core recipe directory.\n        \n        If 'MARS_LOCAL_RECIPE_DIR' environment variable is set, use it as the local directory.\n        Otherwise, return the core recipe directory ('default_recipe_dir').\n        \n        :return: (str) The path of the chosen recipe directory\n        \"\"\"\n        \n        # Get 'MARS_LOCAL_RECIPE_DIR"
    },
    {
        "original": "def config(check):\n    \"\"\"Validate default configuration files.\"\"\"\n    if check:\n        checks = [check]\n    else:\n        checks = sorted(get_valid_checks())\n\n    files_failed = {}\n    files_warned = {}\n    num_files = 0\n\n    echo_waiting('Validating default configuration files...')\n    for check in checks:\n        check_display_queue = []\n\n        config_files = get_config_files(check)\n        for config_file in config_files:\n            num_files += 1\n            file_display_queue = []\n            file_name = basepath(config_file)\n\n            try:\n                config_data = yaml.safe_load(read_file(config_file))\n            except Exception as e:\n                files_failed[config_file] = True\n\n                # We must convert to text here to free Exception object before it goes out of scope\n                error = str(e)\n\n                check_display_queue.append(lambda: echo_info('{}:'.format(file_name), indent=True))\n                check_display_queue.append(lambda: echo_failure('Invalid YAML -', indent=FILE_INDENT))\n                check_display_queue.append(lambda: echo_info(error, indent=FILE_INDENT * 2))\n                continue\n\n            # Verify there is an `instances` section\n            if 'instances' not in config_data:\n                files_failed[config_file] = True\n                file_display_queue.append(lambda: echo_failure('Missing `instances` section', indent=FILE_INDENT))\n\n            # Verify there is a default instance\n            else:\n                instances = config_data['instances']\n                if check not in IGNORE_DEFAULT_INSTANCE and not isinstance(instances, list):\n                    files_failed[config_file] = True\n                    file_display_queue.append(lambda: echo_failure('No default instance', indent=FILE_INDENT))\n\n            if file_display_queue:\n                check_display_queue.append(lambda: echo_info('{}:'.format(file_name), indent=True))\n                check_display_queue.extend(file_display_queue)\n\n        if check_display_queue:\n            echo_success('{}:'.format(check))\n            for display in check_display_queue:\n                display()\n\n    files_failed = len(files_failed)\n    files_warned = len(files_warned)\n    files_passed = num_files - (files_failed + files_warned)\n\n    if files_failed or files_warned:\n        click.echo()\n\n    if files_failed:\n        echo_failure('Files with errors: {}'.format(files_failed))\n\n    if files_warned:\n        echo_warning('Files with warnings: {}'.format(files_warned))\n\n    if files_passed:\n        if files_failed or files_warned:\n            echo_success('Files valid: {}'.format(files_passed))\n        else:\n            echo_success('All {} configuration files are valid!'.format(num_files))\n\n    if files_failed:\n        abort()",
        "sampled": "```python\ndef config(check):\n    \"\"\"\n    Validate default configuration files.\n    \n    The function `config` validates default configuration files by checking them against a set of predefined rules. \n    Currently, it only supports validate the input check type.\n\n\n    Parameters:\n        check (function): A function to check a condition.\n\n    \n        \n            \n            Input:\n                **check** :\n                \n            * If it is `True`, then skip the further checks and returns True.\n            * If it is `False"
    },
    {
        "original": "def section(self, section):\n        \"\"\"The block sections of code to be used as substitutions\n\n        :rtype: Section, list(Section)\n        \"\"\"\n        if isinstance(section, list):\n            for h in section:\n                self.add_section(h)\n        else:\n            self.add_section(section)",
        "sampled": "```python\nclass Section:\n    def __init__(self, name, code):\n        self.name = name\n        self.code = code\n\ndef section(self, sections):\n    \"\"\"\n    The block sections of code to be used as substitutions\n\n    :rtype: Section | list(Section)\n    \"\"\"\n    \n    # If the input is a string, assume it's a name and return a single Section object if it exists\n    if isinstance(sections, str):\n        for section in self.sections"
    },
    {
        "original": "def from_dict(cls, d):\n        \"\"\"\n        As in :Class: `pymatgen.core.Molecule` except\n        restoring graphs using `from_dict_of_dicts`\n        from NetworkX to restore graph information.\n        \"\"\"\n        m = Molecule.from_dict(d['molecule'])\n        return cls(m, d['graphs'])",
        "sampled": "```python\nimport networkx as nx\n\nclass Molecule:\n    def __init__(self, atoms=None, graph=None):\n        self.atoms = atoms if atoms else []\n        self.graph = graph if graph else nx.Graph()\n\n    @classmethod\n    def from_dict(cls, d):\n        # Check if necessary keys are present in the dictionary\n        required_keys = ['atoms', 'graph']\n        for key in required_keys:\n            if key not in d:\n                raise ValueError(f\""
    },
    {
        "original": "def send_over_websocket(self, *, payload: dict):\n        \"\"\"Sends a message to Slack over the WebSocket connection.\n\n        Note:\n            The RTM API only supports posting simple messages formatted using\n            our default message formatting mode. It does not support\n            attachments or other message formatting modes. For this reason\n            we recommend users send messages via the Web API methods.\n            e.g. web_client.chat_postMessage()\n\n            If the message \"id\" is not specified in the payload, it'll be added.\n\n        Args:\n            payload (dict): The message to send over the wesocket.\n            e.g.\n            {\n                \"id\": 1,\n                \"type\": \"typing\",\n                \"channel\": \"C024BE91L\"\n            }\n\n        Raises:\n            SlackClientNotConnectedError: Websocket connection is closed.\n        \"\"\"\n        if self._websocket is None or self._event_loop is None:\n            raise client_err.SlackClientNotConnectedError(\n                \"Websocket connection is closed.\"\n            )\n        if \"id\" not in payload:\n            payload[\"id\"] = self._next_msg_id()\n        asyncio.ensure_future(\n            self._websocket.send_str(json.dumps(payload)), loop=self._event_loop\n        )",
        "sampled": "```python\nclass SlackClient:\n    def __init__(self):\n        self.websocket_connected = True\n\n    def send_over_websocket(self, *, payload: dict):\n        \"\"\"\n        Sends a message to Slack over the WebSocket connection.\n\n        Args:\n            payload (dict): The message to send over the wesocket.\n            e.g.\n            {\n                \"id\": 1,\n                \"type\": \"typing\",\n                \"channel\": \"C024BE91L\"\n            }\n\n        Raises:\n"
    },
    {
        "original": "def replace_species(self, species_mapping):\n        \"\"\"\n        Swap species.\n\n        Args:\n            species_mapping (dict): dict of species to swap. Species can be\n                elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n                a Li for Na substitution. The second species can be a\n                sp_and_occu dict. For example, a site with 0.5 Si that is\n                passed the mapping {Element('Si): {Element('Ge'):0.75,\n                Element('C'):0.25} } will have .375 Ge and .125 C.\n        \"\"\"\n\n        species_mapping = {get_el_sp(k): v\n                           for k, v in species_mapping.items()}\n        sp_to_replace = set(species_mapping.keys())\n        sp_in_structure = set(self.composition.keys())\n        if not sp_in_structure.issuperset(sp_to_replace):\n            warnings.warn(\n                \"Some species to be substituted are not present in \"\n                \"structure. Pls check your input. Species to be \"\n                \"substituted = %s; Species in structure = %s\"\n                % (sp_to_replace, sp_in_structure))\n\n        for site in self._sites:\n            if sp_to_replace.intersection(site.species):\n                c = Composition()\n                for sp, amt in site.species.items():\n                    new_sp = species_mapping.get(sp, sp)\n                    try:\n                        c += Composition(new_sp) * amt\n                    except Exception:\n                        c += {new_sp: amt}\n                site.species = c",
        "sampled": "```python\nfrom typing import Dict\n\nclass Element:\n    def __init__(self, name: str):\n        self.name = name\n\ndef replace_species(self, species_mapping: Dict[Element, Element]) -> None:\n    \"\"\"\n    Swap species.\n\n    Args:\n        species_mapping (dict): dict of species to swap. Species can be\n            elements too. E.g., {Element(\"Li\"): Element(\"Na\")} performs\n            a Li for Na substitution. The second species can be"
    },
    {
        "original": "def get_variables(self):\n        \"\"\"\n        Returns list of variables of the network\n\n        Example\n        -------\n        >>> reader = PomdpXReader(\"pomdpx.xml\")\n        >>> reader.get_variables()\n        {'StateVar': [\n                        {'vnamePrev': 'rover_0',\n                         'vnameCurr': 'rover_1',\n                         'ValueEnum': ['s0', 's1', 's2'],\n                         'fullyObs': True},\n                        {'vnamePrev': 'rock_0',\n                         'vnameCurr': 'rock_1',\n                         'fullyObs': False,\n                         'ValueEnum': ['good', 'bad']}],\n                        'ObsVar': [{'vname': 'obs_sensor',\n                                    'ValueEnum': ['ogood', 'obad']}],\n                        'RewardVar': [{'vname': 'reward_rover'}],\n                        'ActionVar': [{'vname': 'action_rover',\n                                       'ValueEnum': ['amw', 'ame',\n                                                     'ac', 'as']}]\n                        }\n        \"\"\"\n        self.variables = defaultdict(list)\n        for variable in self.network.findall('Variable'):\n            _variables = defaultdict(list)\n            for var in variable.findall('StateVar'):\n                state_variables = defaultdict(list)\n                state_variables['vnamePrev'] = var.get('vnamePrev')\n                state_variables['vnameCurr'] = var.get('vnameCurr')\n                if var.get('fullyObs'):\n                    state_variables['fullyObs'] = True\n                else:\n                    state_variables['fullyObs'] = False\n                state_variables['ValueEnum'] = []\n                if var.find('NumValues') is not None:\n                    for i in range(0, int(var.find('NumValues').text)):\n                        state_variables['ValueEnum'].append('s' + str(i))\n                if var.find('ValueEnum') is not None:\n                    state_variables['ValueEnum'] = \\\n                        var.find('ValueEnum').text.split()\n                _variables['StateVar'].append(state_variables)\n\n            for var in variable.findall('ObsVar'):\n                obs_variables = defaultdict(list)\n                obs_variables['vname'] = var.get('vname')\n                obs_variables['ValueEnum'] = \\\n                    var.find('ValueEnum').text.split()\n                _variables['ObsVar'].append(obs_variables)\n\n            for var in variable.findall('ActionVar'):\n                action_variables = defaultdict(list)\n                action_variables['vname'] = var.get('vname')\n                action_variables['ValueEnum'] = \\\n                    var.find('ValueEnum').text.split()\n                _variables['ActionVar'].append(action_variables)\n\n            for var in variable.findall('RewardVar'):\n                reward_variables = defaultdict(list)\n                reward_variables['vname'] = var.get('vname')\n                _variables['RewardVar'].append(reward_variables)\n\n            self.variables.update(_variables)\n\n        return self.variables",
        "sampled": "```python\nclass PomdpXReader:\n    def __init__(self, filename):\n        self.filename = filename\n        self.variables = {\n            'StateVar': [],\n            'ObsVar': [],\n            'RewardVar': [],\n            'ActionVar': []\n        }\n\n    def parse_xml(self):\n        import xml.etree.ElementTree as ET\n\n        tree = ET.parse('pomdpx.xml')\n        root = tree.getroot()\n\n        for elem in root.findall('.//'):\n            if"
    },
    {
        "original": "def get_availability_zone(vm_):\n    \"\"\"\n    Return the availability zone to use\n    \"\"\"\n    avz = config.get_cloud_config_value(\n        'availability_zone', vm_, __opts__, search_global=False\n    )\n\n    if avz is None:\n        return None\n\n    zones = _list_availability_zones(vm_)\n\n    # Validate user-specified AZ\n    if avz not in zones:\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t valid in this region: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    # check specified AZ is available\n    elif zones[avz] != 'available':\n        raise SaltCloudException(\n            'The specified availability zone isn\\'t currently available: '\n            '{0}\\n'.format(\n                avz\n            )\n        )\n\n    return avz",
        "sampled": "```python\ndef get_availability_zone(vm_):\n    \"\"\"\n    Return the availability zone to use\n    \"\"\"\n    \n    # Define a dictionary to map vm names to their corresponding availability zones\n    availability_zones = {\n        'vm1': 'us-east-1a',\n        'vm2': 'us-west-2b',\n        'vm3': 'eu-central-1c',\n        # Add more vm names and their corresponding availability zones as needed\n    }\n    \n    # Check"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "sampled": "```python\nimport tkinter as tk\nfrom tkinter import filedialog\n\nclass Application(tk.Frame):\n    def __init__(self, master=None):\n        super().__init__(master)\n        self.master = master\n        self.pack()\n        self.create_widgets()\n\n    def create_widgets(self):\n        self.save_button = tk.Button(self)\n        self.save_button[\"text\"] = \"Save\"\n        self.save_button[\"command\"] = self.saveFile\n        self.save_button.pack(side=\"top\")\n\n    def"
    },
    {
        "original": "def findall_operations(self, predicate: Callable[[ops.Operation], bool]\n                           ) -> Iterable[Tuple[int, ops.Operation]]:\n        \"\"\"Find the locations of all operations that satisfy a given condition.\n\n        This returns an iterator of (index, operation) tuples where each\n        operation satisfies op_cond(operation) is truthy. The indices are\n        in order of the moments and then order of the ops within that moment.\n\n        Args:\n            predicate: A method that takes an Operation and returns a Truthy\n                value indicating the operation meets the find condition.\n\n        Returns:\n            An iterator (index, operation)'s that satisfy the op_condition.\n        \"\"\"\n        for index, moment in enumerate(self._moments):\n            for op in moment.operations:\n                if predicate(op):\n                    yield index, op",
        "sampled": "```python\nfrom typing import Callable, Iterable, Tuple\nimport itertools\n\nclass Operation:\n    \"\"\"Represents an operation call.\n\n    Attributes:\n        index (int): The index of the operation in the sequence.\n        moment (int): The moment this operation was assigned to.\n        op_cond (function): Conditional predicate for determining if\n            this op should be call.\n\n\n    \"\"\"\n    def __init__(self, index: int, moment: int):\n        self.index = index\n        self"
    }
]
