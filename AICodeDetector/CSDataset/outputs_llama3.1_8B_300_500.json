[
    {
        "original": "def getPluginsList(self, enable=True):\n        \"\"\"Return the plugins list.\n\n        if enable is True, only return the active plugins (default)\n        if enable is False, return all the plugins\n\n        Return: list of plugin name\n        \"\"\"\n        if enable:\n            return [p for p in self._plugins if self._plugins[p].is_enable()]\n        else:\n            return [p for p in self._plugins]",
        "sampled": "### Problem Description\nGet Plugins List\n===============\n\nIn this problem, you will need to create a method that returns a list of plugins.\nThis list could be filtered by an \"enable\" status.\n\n### Input and Output Specifications\n------------------------------------\n\n*   `method signature:` getPluginsList(self, enable=True)\n*   `input type:`\n    *   enable (bool): whether to return enabled or disabled plugins (default: True)\n*   `output type:` list of plugin name\n*"
    },
    {
        "original": "def get_nb_strings(self):\n        \"\"\"\n        Return the total number of strings in all Analysis objects\n        \"\"\"\n        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb",
        "sampled": "### Problem Description\n\nIt appear that there is no problem description provided. The method signature suggests that it's part of a class and we're supposed to return the total number of strings in all Analysis objects.\n\n### Input and Output Specifications\n\nThere is no clear input or output specifications provided.\n\n### Problem Assumption\n\nBased on the method name `get_nb_strings` which means \"get number of strings\", I'm assuming this method should iterate over all Analysis objects, count the total number of strings in each"
    },
    {
        "original": "def endian_swap_words(source):\n    \"\"\" Endian-swap each word in 'source' bitstring \"\"\"\n    assert len(source) % 4 == 0\n    words = \"I\" * (len(source) // 4)\n    return struct.pack(\"<\" + words, *struct.unpack(\">\" + words, source))",
        "sampled": "```python\ndef endian_swap_words(source):\n    \"\"\"\n    Endian-swap each word in 'source' bitstring\n    \"\"\"\n    \n    # Split the source string into words of 32 bits (4 bytes)\n    words = [source[i:i+32] for i in range(0, len(source), 32)]\n    \n    # Initialize an empty list to store the swapped words\n    swapped_words = []\n    \n    # Iterate over each word and swap its bytes\n    for word"
    },
    {
        "original": "def GetValueRepresentation(cls, value,\n                             version=sorted(_SERVICE_MAP.keys())[-1]):\n    \"\"\"Converts a single python value to its PQL representation.\n\n    Args:\n      value: A python value.\n      version: A string identifying the Ad Manager version the value object\n          is compatible with. This defaults to what is currently the latest\n          version. This will be updated in future releases to point to what is\n          then the latest version.\n\n    Returns:\n      The value formatted for PQL statements which are compatible with a\n      particular API version.\n    \"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      return {'value': value, 'xsi_type': 'TextValue'}\n    elif isinstance(value, bool):\n      return {'value': value, 'xsi_type': 'BooleanValue'}\n    elif isinstance(value, numbers.Number):\n      return {'value': value, 'xsi_type': 'NumberValue'}\n    # It's important that datetime is checked for before date\n    # because isinstance(datetime.datetime.now(), datetime.date) is True\n    elif isinstance(value, datetime.datetime):\n      if value.tzinfo is None:\n        raise googleads.errors.GoogleAdsValueError(\n            'Datetime %s is not timezone aware.' % value\n        )\n\n      return {\n          'xsi_type': 'DateTimeValue',\n          'value': {\n              'date': {\n                  'year': value.year,\n                  'month': value.month,\n                  'day': value.day,\n              },\n              'hour': value.hour,\n              'minute': value.minute,\n              'second': value.second,\n              'timeZoneId' if version >= 'v201811' else 'timeZoneID':\n                  value.tzinfo.zone,\n          }\n      }\n    elif isinstance(value, datetime.date):\n      return {\n          'xsi_type': 'DateValue',\n          'value': {\n              'year': value.year,\n              'month': value.month,\n              'day': value.day,\n          }\n      }\n    elif isinstance(value, list):\n      if value and not all(isinstance(x, type(value[0])) for x in value):\n        raise googleads.errors.GoogleAdsValueError('Cannot pass more than one '\n                                                   'type in a set.')\n\n      return {\n          'xsi_type': 'SetValue',\n          'values': [cls.GetValueRepresentation(v, version) for v in value]\n      }\n    else:\n      raise googleads.errors.GoogleAdsValueError(\n          'Can\\'t represent unknown type: %s.' % type(value))",
        "sampled": "```python\n_SERVICE_MAP = {\n    'v1': 'int',\n    'v2': 'float',\n    'v3': lambda x: f\"'{x}'\",\n}\n\ndef GetValueRepresentation(cls, value, version='v1'):\n    \"\"\"\n    Converts a single python value to its PQL representation.\n\n    Args:\n      value: A python value.\n      version: A string identifying the Ad Manager version the value object\n          is compatible with.\n\n    Returns:\n      The value formatted"
    },
    {
        "original": "def CheckTemplates(self, base_dir, version):\n    \"\"\"Verify we have at least one template that matches maj.minor version.\"\"\"\n    major_minor = \".\".join(version.split(\".\")[0:2])\n    templates = glob.glob(\n        os.path.join(base_dir, \"templates/*%s*.zip\" % major_minor))\n    required_templates = set(\n        [x.replace(\"maj.minor\", major_minor) for x in self.REQUIRED_TEMPLATES])\n\n    # Client templates have an extra version digit, e.g. 3.1.0.0\n    templates_present = set([\n        re.sub(r\"_%s[^_]+_\" % major_minor, \"_%s_\" % major_minor,\n               os.path.basename(x)) for x in templates\n    ])\n\n    difference = required_templates - templates_present\n    if difference:\n      raise RuntimeError(\"Missing templates %s\" % difference)",
        "sampled": "### Problem Description\nVerify we have at least one template that matches maj.minor version.\nGiven a base directory and a version, check if there is at least one template that matches the given version.\n\n### Input and Output Specifications\n- **Input**:\n  - `base_dir`: The base directory to search for templates.\n  - `version`: The version to match (in the format of \"maj.minor\").\n- **Output**:\n  - A boolean indicating whether at least one matching template"
    },
    {
        "original": "def _get_cluster_sizes(self):\n        \"\"\"\n        Returns the marker size (in points, e.g. area of the circle) based on\n        the scores, using the prop_to_size scaling mechanism.\n        \"\"\"\n        # NOTE: log and power are hardcoded, should we allow the user to specify?\n        return prop_to_size(\n            self.scores_, mi=self.min_size, ma=self.max_size, log=False, power=0.5\n        )",
        "sampled": "```python\ndef calculator(prop, size):\n    \"\"\"\n    Execute the prop_to_size scaling mechanism calculator.\n    \n    :param prop: The classification rate (Proportion of votes of wining class)\n    :type prop: float\n    :param size: Initial value for marker size point (e.g. Marker's area point)\n    :type size: int or float\n    \n    :return: The scaled marker size based on the proportion of classification\n\n    \"\"\"\n    \n        # Press ctrl + d"
    },
    {
        "original": "def _build_likelihood(self):\n        \"\"\"\n        Construct a tensorflow function to compute the bound on the marginal\n        likelihood. For a derivation of the terms in here, see the associated\n        SGPR notebook.\n        \"\"\"\n\n        num_inducing = len(self.feature)\n        num_data = tf.cast(tf.shape(self.Y)[0], settings.float_type)\n        output_dim = tf.cast(tf.shape(self.Y)[1], settings.float_type)\n\n        err = self.Y - self.mean_function(self.X)\n        Kdiag = self.kern.Kdiag(self.X)\n        Kuf = features.Kuf(self.feature, self.kern, self.X)\n        Kuu = features.Kuu(self.feature, self.kern, jitter=settings.numerics.jitter_level)\n        L = tf.cholesky(Kuu)\n        sigma = tf.sqrt(self.likelihood.variance)\n\n        # Compute intermediate matrices\n        A = tf.matrix_triangular_solve(L, Kuf, lower=True) / sigma\n        AAT = tf.matmul(A, A, transpose_b=True)\n        B = AAT + tf.eye(num_inducing, dtype=settings.float_type)\n        LB = tf.cholesky(B)\n        Aerr = tf.matmul(A, err)\n        c = tf.matrix_triangular_solve(LB, Aerr, lower=True) / sigma\n\n        # compute log marginal bound\n        bound = -0.5 * num_data * output_dim * np.log(2 * np.pi)\n        bound += tf.negative(output_dim) * tf.reduce_sum(tf.log(tf.matrix_diag_part(LB)))\n        bound -= 0.5 * num_data * output_dim * tf.log(self.likelihood.variance)\n        bound += -0.5 * tf.reduce_sum(tf.square(err)) / self.likelihood.variance\n        bound += 0.5 * tf.reduce_sum(tf.square(c))\n        bound += -0.5 * output_dim * tf.reduce_sum(Kdiag) / self.likelihood.variance\n        bound += 0.5 * output_dim * tf.reduce_sum(tf.matrix_diag_part(AAT))\n\n        return bound",
        "sampled": "### Problem Description\nConstruct a tensorflow function to compute the bound on the marginal likelihood.\n\n### Input Specifications\n- The _build_likelihood method should be defined to construct a tensorflow function.\n \n### Output Specifications\n- A tensorflow function that computes the bound on the marginal likelihood.\n\n### Solution\n\n```python\nimport tensorflow as tf\n\nclass SGPRModel:\n    def __init__(self):\n        self.num_data = 1000  # number of data points (can be any value)\n        self.num"
    },
    {
        "original": "def updateRouterStatus(self):\n        \"\"\"force update to router as if there is child id request\"\"\"\n        print '%s call updateRouterStatus' % self.port\n        cmd = 'state'\n        while True:\n            state = self.__sendCommand(cmd)[0]\n            if state == 'detached':\n                continue\n            elif state == 'child':\n                break\n            else:\n                return False\n\n        cmd = 'state router'\n        return self.__sendCommand(cmd)[0] == 'Done'",
        "sampled": "### Problem Description\n### \nNo problem description is provided. However, based on the function name `updateRouterStatus` and the comment `force update to router as if there is child id request`, it seems like this function is part of a class that interacts with a router or network device.\n\n### Input and Output Specifications\n### \nNo input and output specifications are provided.\n\n### Python Solution\n\n```python\nclass Router:\n    def __init__(self):\n        # Initialize an empty list to store"
    },
    {
        "original": "def put_group_policy(group_name, policy_name, policy_json, region=None, key=None,\n                     keyid=None, profile=None):\n    \"\"\"\n    Adds or updates the specified policy document for the specified group.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.put_group_policy mygroup policyname policyrules\n    \"\"\"\n    group = get_group(group_name, region=region, key=key, keyid=keyid,\n                      profile=profile)\n    if not group:\n        log.error('Group %s does not exist', group_name)\n        return False\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        if not isinstance(policy_json, six.string_types):\n            policy_json = salt.utils.json.dumps(policy_json)\n        created = conn.put_group_policy(group_name, policy_name,\n                                        policy_json)\n        if created:\n            log.info('Created policy for IAM group %s.', group_name)\n            return True\n        log.error('Could not create policy for IAM group %s', group_name)\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        log.error('Failed to create policy for IAM group %s', group_name)\n    return False",
        "sampled": "### Problem Description\nAdd or update the specified policy document for the specified group.\n\n### Input and Output Specifications\n\n* `group_name`: The name of the group.\n* `policy_name`: The name of the policy.\n* `policy_json`: The JSON-formatted policy document.\n* `region` (optional): The region in which to add or update the policy. If not provided, will use a region that is defined in your AWS configuration file (see :ref:`config-file`).\n*"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'language') and self.language is not None:\n            _dict['language'] = self.language\n        if hasattr(self, 'analyzed_text') and self.analyzed_text is not None:\n            _dict['analyzed_text'] = self.analyzed_text\n        if hasattr(self, 'retrieved_url') and self.retrieved_url is not None:\n            _dict['retrieved_url'] = self.retrieved_url\n        if hasattr(self, 'usage') and self.usage is not None:\n            _dict['usage'] = self.usage._to_dict()\n        if hasattr(self, 'concepts') and self.concepts is not None:\n            _dict['concepts'] = [x._to_dict() for x in self.concepts]\n        if hasattr(self, 'entities') and self.entities is not None:\n            _dict['entities'] = [x._to_dict() for x in self.entities]\n        if hasattr(self, 'keywords') and self.keywords is not None:\n            _dict['keywords'] = [x._to_dict() for x in self.keywords]\n        if hasattr(self, 'categories') and self.categories is not None:\n            _dict['categories'] = [x._to_dict() for x in self.categories]\n        if hasattr(self, 'emotion') and self.emotion is not None:\n            _dict['emotion'] = self.emotion._to_dict()\n        if hasattr(self, 'metadata') and self.metadata is not None:\n            _dict['metadata'] = self.metadata._to_dict()\n        if hasattr(self, 'relations') and self.relations is not None:\n            _dict['relations'] = [x._to_dict() for x in self.relations]\n        if hasattr(self, 'semantic_roles') and self.semantic_roles is not None:\n            _dict['semantic_roles'] = [\n                x._to_dict() for x in self.semantic_roles\n            ]\n        if hasattr(self, 'sentiment') and self.sentiment is not None:\n            _dict['sentiment'] = self.sentiment._to_dict()\n        if hasattr(self, 'syntax') and self.syntax is not None:\n            _dict['syntax'] = self.syntax._to_dict()\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def _to_dict(self):\n        \"\"\"\n        Return a json dictionary representing this model.\n        \n        :return: A dictionary representing the model.\n        \"\"\"\n        \n    def to_dict(self):\n        return {\n            \"model\": self.__class__.__name__,\n            \"version\": 1,\n            \"_schema_version\": 1,\n            \"_id\": str(id(self)),\n            **self._to_dict()\n        }\n```\n\nHowever, without more information about the class and"
    },
    {
        "original": "def _pack(cls, tensors):\n    \"\"\"Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\"\"\"\n    if not tensors:\n      return None\n    elif len(tensors) == 1:\n      return array_ops.reshape(tensors[0], [-1])\n    else:\n      flattened = [array_ops.reshape(tensor, [-1]) for tensor in tensors]\n      return array_ops.concat(flattened, 0)",
        "sampled": "```python\nimport numpy as np\n\nclass Tensor:\n    def __init__(self, data):\n        self.data = np.array(data)\n\n    def __repr__(self):\n        return f'Tensor({self.data})'\n\ndef _pack(cls, tensors):\n    \"\"\"\n    Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\n    \n    Args:\n        cls (class): The class of the tensor.\n        tensors (list): A list of `Tensor`"
    },
    {
        "original": "def arg_locs(self, is_fp=None, sizes=None):\n        \"\"\"\n        Pass this a list of whether each parameter is floating-point or not, and get back a list of\n        SimFunctionArguments. Optionally, pass a list of argument sizes (in bytes) as well.\n\n        If you've customized this CC, this will sanity-check the provided locations with the given list.\n        \"\"\"\n        session = self.arg_session\n        if self.func_ty is None:\n            # No function prototype is provided. `is_fp` must be provided.\n            if is_fp is None:\n                raise ValueError('\"is_fp\" must be provided when no function prototype is available.')\n        else:\n            # let's rely on the func_ty for the number of arguments and whether each argument is FP or not\n            is_fp = [ True if isinstance(arg, (SimTypeFloat, SimTypeDouble)) else False for arg in self.func_ty.args ]\n\n        if sizes is None: sizes = [self.arch.bytes] * len(is_fp)\n        return [session.next_arg(ifp, size=sz) for ifp, sz in zip(is_fp, sizes)]",
        "sampled": "### Problem Description\n#### Computing Argument Locations and Sizes\n\nYou have a list of parameters in your function, and you need to calculate their memory locations\nand sizes. The memory allocation must be done based on whether a parameter is floating-point or not.\n\n### Input and Output Specifications\n\n*   The function `arg_locs` takes in two parameters: \n    *   A list of boolean values where:\n        *   `True` indicates the corresponding parameter is a floating point type,\n        *   `"
    },
    {
        "original": "def build(format='qcow2', path='/tmp/'):\n    \"\"\"\n    Build an image from a current system description.\n    The image is a system image can be output in bootable ISO or QCOW2 formats.\n\n    Node uses the image building library Kiwi to perform the actual build.\n\n    Parameters:\n\n    * **format**: Specifies output format: \"qcow2\" or \"iso. Default: `qcow2`.\n    * **path**: Specifies output path where to store built image. Default: `/tmp`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion inspector.build\n        salt myminion inspector.build format=iso path=/opt/builds/\n    \"\"\"\n    try:\n        _(\"collector\").Inspector(cachedir=__opts__['cachedir'],\n                                 piddir=os.path.dirname(__opts__['pidfile']),\n                                 pidfilename='').reuse_snapshot().build(format=format, path=path)\n    except InspectorKiwiProcessorException as ex:\n        raise CommandExecutionError(ex)\n    except Exception as ex:\n        log.error(_get_error_message(ex))\n        raise Exception(ex)",
        "sampled": "```python\nimport logging\nimport subprocess\n\nclass Inspector:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n\n    def _build_plugins(self, format, path):\n        return {\n            \"qcow2\": self._build_qcow2,\n            \"iso\": self._build_iso,\n        }\n\n    def _build_qcow2(self, path):\n        try:\n            subprocess.run([\"kiwi\", \"create\", \"--format\", \"qcow2\", \"--output"
    },
    {
        "original": "def get_or_add_image(self, image_descriptor):\n        \"\"\"Return (rId, image) pair for image identified by *image_descriptor*.\n\n        *rId* is the str key (often like \"rId7\") for the relationship between this story\n        part and the image part, reused if already present, newly created if not.\n        *image* is an |Image| instance providing access to the properties of the image,\n        such as dimensions and image type.\n        \"\"\"\n        image_part = self._package.get_or_add_image_part(image_descriptor)\n        rId = self.relate_to(image_part, RT.IMAGE)\n        return rId, image_part.image",
        "sampled": "### Problem Description\nGet or Add Image\n===============\n\nGiven an image descriptor, return a pair containing the relationship ID (rId) and the corresponding image instance.\n\n### Input Specifications\n* `image_descriptor`: A unique identifier for the image.\n\n### Output Specifications\n* `rId`: A string key representing the relationship between the story part and the image part.\n* `image`: An instance of an `Image` class providing access to image properties such as dimensions and type.\n\n```python\nclass"
    },
    {
        "original": "def set_tags(name=None,\n             tags=None,\n             call=None,\n             location=None,\n             instance_id=None,\n             resource_id=None,\n             kwargs=None):  # pylint: disable=W0613\n    \"\"\"\n    Set tags for a resource. Normally a VM name or instance_id is passed in,\n    but a resource_id may be passed instead. If both are passed in, the\n    instance_id will be used.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -a set_tags mymachine tag1=somestuff tag2='Other stuff'\n        salt-cloud -a set_tags resource_id=vol-3267ab32 tag=somestuff\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n\n    if location is None:\n        location = get_location()\n\n    if instance_id is None:\n        if 'resource_id' in kwargs:\n            resource_id = kwargs['resource_id']\n            del kwargs['resource_id']\n\n        if 'instance_id' in kwargs:\n            instance_id = kwargs['instance_id']\n            del kwargs['instance_id']\n\n        if resource_id is None:\n            if instance_id is None:\n                instance_id = _get_node(name=name, instance_id=None, location=location)['instanceId']\n        else:\n            instance_id = resource_id\n\n    # This second check is a safety, in case the above still failed to produce\n    # a usable ID\n    if instance_id is None:\n        return {\n            'Error': 'A valid instance_id or resource_id was not specified.'\n        }\n\n    params = {'Action': 'CreateTags',\n              'ResourceId.1': instance_id}\n\n    log.debug('Tags to set for %s: %s', name, tags)\n\n    if kwargs and not tags:\n        tags = kwargs\n\n    for idx, (tag_k, tag_v) in enumerate(six.iteritems(tags)):\n        params['Tag.{0}.Key'.format(idx)] = tag_k\n        params['Tag.{0}.Value'.format(idx)] = tag_v\n\n    attempts = 0\n    while attempts < aws.AWS_MAX_RETRIES:\n        aws.query(params,\n                           setname='tagSet',\n                           location=location,\n                           provider=get_provider(),\n                           opts=__opts__,\n                           sigver='4')\n\n        settags = get_tags(\n            instance_id=instance_id, call='action', location=location\n        )\n\n        log.debug('Setting the tags returned: %s', settags)\n\n        failed_to_set_tags = False\n        for tag in settags:\n            if tag['key'] not in tags:\n                # We were not setting this tag\n                continue\n\n            if tag.get('value') is None and tags.get(tag['key']) == '':\n                # This is a correctly set tag with no value\n                continue\n\n            if six.text_type(tags.get(tag['key'])) != six.text_type(tag['value']):\n                # Not set to the proper value!?\n                log.debug(\n                    'Setting the tag %s returned %s instead of %s',\n                    tag['key'], tags.get(tag['key']), tag['value']\n                )\n                failed_to_set_tags = True\n                break\n\n        if failed_to_set_tags:\n            log.warning('Failed to set tags. Remaining attempts %s', attempts)\n            attempts += 1\n            aws.sleep_exponential_backoff(attempts)\n            continue\n\n        return settags\n\n    raise SaltCloudSystemExit(\n        'Failed to set tags on {0}!'.format(name)\n    )",
        "sampled": "### Problem Understanding and Analysis\n\nThe problem statement describes a function `set_tags` that is used to set tags for a resource. The function accepts various parameters such as `name`, `tags`, `call`, `location`, `instance_id`, and `resource_id`. However, when both `instance_id` and `resource_id` are provided, the function uses the former to identify the resource.\n\n### Python Solution\n\n```python\ndef set_tags(name=None,\n             tags=None,\n             call=None"
    },
    {
        "original": "def add_pool_member(hostname, username, password, name, member):\n    \"\"\"\n    A function to connect to a bigip device and add a new member to an existing pool.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    name\n        The name of the pool to modify\n    member\n        The member to add to the pool\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'add', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'name': name,\n            'members': member\n        }\n        )\n\n    #is this pool member currently configured?\n    existing_pool = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n    if existing_pool['code'] == 200:\n\n        # for some reason iControl REST doesn't support listing a single pool member.\n        # the response from GET for listing a member will return 200 even if it doesn't exists.\n        # because of this we have to do some rather \"unnecessary\" searching within a pool.\n\n        #what are the current members?\n        current_members = existing_pool['content']['membersReference']['items']\n\n        #loop through them\n        exists = False\n        for current_member in current_members:\n            if current_member['name'] == member['name']:\n                exists = True\n                break\n\n        if exists:\n            ret['result'] = True\n            ret['comment'] = 'Member: {name} already exists within this pool.  No changes made.'.format(name=member['name'])\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = {}\n        else:\n            new_member = __salt__['bigip.add_pool_member'](hostname, username, password, name, member)\n\n            if new_member['code'] == 200:\n                ret['result'] = True\n                ret['comment'] = 'Member: {name} has been successfully added to the pool.'.format(name=member['name'])\n                ret['changes']['old'] = {}\n\n                #look up the member again...\n                pool_listing = __salt__['bigip.list_pool'](hostname, username, password, name)\n\n                if pool_listing['code'] != 200:\n                    ret = _load_result(new_member, ret)\n                    return ret\n\n                members = pool_listing['content']['membersReference']['items']\n                #loop through them\n                for current_member in members:\n                    if current_member['name'] == member['name']:\n                        added_member = current_member\n                        break\n\n                ret['changes']['new'] = added_member\n\n            # member wasn't added\n            else:\n                ret = _load_result(new_member, ret)\n\n    #pool does not exists\n    elif existing_pool['code'] == 404:\n        ret['comment'] = 'A pool with this name was not found.'\n    else:\n        ret = _load_result(existing_pool, ret)\n\n    return ret",
        "sampled": "## Problem Description\n### Add Pool Member to BigIP Device\n\nAdd a new member to an existing pool on a BigIP device using the iControl REST API.\n\n## Input Specifications\n- `hostname`: The host/address of the BigIP device.\n- `username`: The iControl REST username.\n- `password`: The iControl REST password.\n- `name`: The name of the pool to modify.\n- `member`: The member to add to the pool.\n\n## Output Specifications\nNone\n\n"
    },
    {
        "original": "def Refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\n\n    Raises:\n      google.auth.exceptions.RefreshError: If the refresh fails.\n    \"\"\"\n    with requests.Session() as session:\n      session.proxies = self.proxy_config.proxies\n      session.verify = not self.proxy_config.disable_certificate_validation\n      session.cert = self.proxy_config.cafile\n\n      self.creds.refresh(\n          google.auth.transport.requests.Request(session=session))",
        "sampled": "### Problem Description\n### \n**Refresh Token to Retrieve new Access Token**\n\nYour task is to take the existing refresh token and use it to retrieve a new access token. Implement a method called `Refresh` that accomplishes this.\n\nThe Refresh Token provides you with a way to obtain a new Access Token if your existing Access Token has expired.\n\n#### In-Line Math Formula\n\nMaximum number of refresh attempts:\n$$n_{ref \\, max} = 3$$\n\n#### Input and Output Specifications\n|"
    },
    {
        "original": "def ParseFromUnicode(self, value):\n    \"\"\"Parse a string into a client URN.\n\n    Convert case so that all URNs are of the form C.[0-9a-f].\n\n    Args:\n      value: string value to parse\n    \"\"\"\n    precondition.AssertType(value, Text)\n    value = value.strip()\n\n    super(ClientURN, self).ParseFromUnicode(value)\n\n    match = self.CLIENT_ID_RE.match(self._string_urn)\n    if not match:\n      raise type_info.TypeValueError(\"Client urn malformed: %s\" % value)\n\n    clientid = match.group(\"clientid\")\n    clientid_correctcase = \"\".join((clientid[0].upper(), clientid[1:].lower()))\n\n    self._string_urn = self._string_urn.replace(clientid, clientid_correctcase,\n                                                1)",
        "sampled": "```python\nclass URNParser:\n    def ParseFromUnicode(self, value):\n        \"\"\"\n        Parse a string into a client URN.\n\n        Convert case so that all URNs are of the form C.[0-9a-f].\n\n        Args:\n          value: string value to parse\n        \"\"\"\n        \n        # Remove any leading or trailing whitespace from the input string\n        value = value.strip()\n        \n        # Convert the entire string to lowercase for consistency\n        value = value.lower()\n"
    },
    {
        "original": "def close(self):\n        \"\"\"Close method.\n\n        Closes the underlying comm.\n        When the comm is closed, all of the widget views are automatically\n        removed from the front-end.\"\"\"\n        if self.comm is not None:\n            Widget.widgets.pop(self.model_id, None)\n            self.comm.close()\n            self.comm = None\n            self._ipython_display_ = None",
        "sampled": "### Problem Description\nGiven a binary search tree (BST) where all the values are unique, find the first parent of a given value in the BST.\n\n*   A node in the binary tree can be represented as `(val, left, right)`.\n*   The `left` and `right` fields represent the child nodes.\n*   A null value in any node means that this node is empty.\n\n### Input and Output Specifications\n\n| Input Type | Description |\n| --- | --- |\n|"
    },
    {
        "original": "def get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    exec_dir = os.path.dirname(os.path.realpath(__file__))\n    xml_path = os.path.join(exec_dir, 'data', 'FacebookLocales.xml')\n\n    fb_locales = _build_locale_table(xml_path)\n\n    def default_locale(request):\n        ",
        "sampled": "```python\nfrom functools import lru_cache\n\ndef get_default_locale_callable():\n    \"\"\"\n    Wrapper function so that the default mapping is only built when needed\n    \"\"\"\n    \n    @lru_cache(maxsize=None)\n    def get_default_locale(language_code):\n        # This is a placeholder for your actual logic to determine the default locale\n        # For example, you might use a dictionary or a database query to map language codes to locales\n        if language_code == 'en':\n            return '"
    },
    {
        "original": "def clitable_to_dict(cli_table):\n    \"\"\"Converts TextFSM cli_table object to list of dictionaries.\"\"\"\n    objs = []\n    for row in cli_table:\n        temp_dict = {}\n        for index, element in enumerate(row):\n            temp_dict[cli_table.header[index].lower()] = element\n        objs.append(temp_dict)\n    return objs",
        "sampled": "```python\ndef convert_cli_table_to_dict(cli_table):\n    \"\"\"\n    Converts a TextFSM cli_table object to a list of dictionaries.\n    \n    Args:\n        cli_table (TextFSM): A TextFSM cli_table object.\n    \n    Returns:\n        list[dict]: A list of dictionaries where each dictionary represents a row in the table.\n    \"\"\"\n    \n    # Initialize an empty list to store the row dictionaries\n    rows = []\n    \n    # Check if the cli_table has"
    },
    {
        "original": "def execute_query(**kwargs):\n    \"\"\"\n      Executes a query against the connected db using pymapd\n      https://pymapd.readthedocs.io/en/latest/usage.html#querying\n\n      Kwargs:\n        query_name(str): Name of query\n        query_mapdql(str): Query to run\n        iteration(int): Iteration number\n\n      Returns:\n        query_execution(dict):::\n          result_count(int): Number of results returned\n          execution_time(float): Time (in ms) that pymapd reports\n                                 backend spent on query.\n          connect_time(float): Time (in ms) for overhead of query, calculated\n                               by subtracting backend execution time\n                               from time spent on the execution function.\n          results_iter_time(float): Time (in ms) it took to for\n                                    pymapd.fetchone() to iterate through all\n                                    of the results.\n          total_time(float): Time (in ms) from adding all above times.\n        False(bool): The query failed. Exception should be logged.\n    \"\"\"\n    start_time = timeit.default_timer()\n    try:\n        # Run the query\n        query_result = con.execute(kwargs[\"query_mapdql\"])\n        logging.debug(\n            \"Completed iteration \"\n            + str(kwargs[\"iteration\"])\n            + \" of query \"\n            + kwargs[\"query_name\"]\n        )\n    except (pymapd.exceptions.ProgrammingError, pymapd.exceptions.Error):\n        logging.exception(\n            \"Error running query \"\n            + kwargs[\"query_name\"]\n            + \" during iteration \"\n            + str(kwargs[\"iteration\"])\n        )\n        return False\n\n    # Calculate times\n    query_elapsed_time = (timeit.default_timer() - start_time) * 1000\n    execution_time = query_result._result.execution_time_ms\n    connect_time = round((query_elapsed_time - execution_time), 1)\n\n    # Iterate through each result from the query\n    logging.debug(\n        \"Counting results from query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n    )\n    result_count = 0\n    start_time = timeit.default_timer()\n    while query_result.fetchone():\n        result_count += 1\n    results_iter_time = round(\n        ((timeit.default_timer() - start_time) * 1000), 1\n    )\n\n    query_execution = {\n        \"result_count\": result_count,\n        \"execution_time\": execution_time,\n        \"connect_time\": connect_time,\n        \"results_iter_time\": results_iter_time,\n        \"total_time\": execution_time + connect_time + results_iter_time,\n    }\n    logging.debug(\n        \"Execution results for query\"\n        + kwargs[\"query_name\"]\n        + \" iteration \"\n        + str(kwargs[\"iteration\"])\n        + \": \"\n        + str(query_execution)\n    )\n    return query_execution",
        "sampled": "```python\nimport pymapd\nfrom time import perf_counter_ns\nfrom logging import getLogger\n\n# Get a logger instance\nlogger = getLogger('QueryExecuter')\n\ndef execute_query(**kwargs):\n    \"\"\"\n    Executes a query against the connected db using pymapd\n\n    Kwargs:\n        query_name(str): Name of query\n        query_mapdql(str): Query to run\n        iteration(int): Iteration number\n\n    Returns:\n        query_execution(dict):::\n            result_count(int"
    },
    {
        "original": "def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        assert isinstance(label, (github.Label.Label, str, unicode)), label\n        if isinstance(label, github.Label.Label):\n            label = label._identity\n        else:\n            label = urllib.quote(label)\n        headers, data = self._requester.requestJsonAndCheck(\n            \"DELETE\",\n            self.issue_url + \"/labels/\" + label\n        )",
        "sampled": "```python\nclass GitHubAPI:\n    def remove_from_labels(self, label):\n        \"\"\"\n        :calls: `DELETE /repos/:owner/:repo/issues/:number/labels/:name <http://developer.github.com/v3/issues/labels>`_\n        :param label: :class:`github.Label.Label` or string\n        :rtype: None\n        \"\"\"\n        \n        # Assuming we have the repository owner, repo name and issue number.\n        \n        # We need to make a GET request to"
    },
    {
        "original": "def add_candidate_peer_endpoints(self, peer_endpoints):\n        \"\"\"Adds candidate endpoints to the list of endpoints to\n        attempt to peer with.\n\n        Args:\n            peer_endpoints ([str]): A list of public uri's which the\n                validator can attempt to peer with.\n        \"\"\"\n        with self._lock:\n            for endpoint in peer_endpoints:\n                if endpoint not in self._candidate_peer_endpoints:\n                    self._candidate_peer_endpoints.append(endpoint)",
        "sampled": "### Problem Description\nAdd candidate endpoints to the list of endpoints to attempt to peer with.\n\n### Input and Output Specifications\n#### Input:\n- `peer_endpoints`: A list of public uri's which the validator can attempt to peer with.\n\n#### Output:\n- The function does not return any value, it modifies the internal state of the object by adding candidate endpoints to the list.\n\n### Python Solution\n\n```python\nclass PeerEndpointManager:\n    def __init__(self):\n        self.endpoints ="
    },
    {
        "original": "def storeSenderKey(self, senderKeyName, senderKeyRecord):\n        \"\"\"\n        :type senderKeyName: SenderKeName\n        :type senderKeyRecord: SenderKeyRecord\n        \"\"\"\n        q = \"INSERT INTO sender_keys (group_id, sender_id, record) VALUES(?,?, ?)\"\n        cursor = self.dbConn.cursor()\n        serialized = senderKeyRecord.serialize()\n        if sys.version_info < (2,7):\n            serialized = buffer(serialized)\n        try:\n            cursor.execute(q, (senderKeyName.getGroupId(), senderKeyName.getSender().getName(), serialized))\n            self.dbConn.commit()\n        except sqlite3.IntegrityError as e:\n            q = \"UPDATE sender_keys set record = ? WHERE group_id = ? and sender_id = ?\"\n            cursor = self.dbConn.cursor()\n            cursor.execute(q, (serialized, senderKeyName.getGroupId(), senderKeyName.getSender().getName()))\n            self.dbConn.commit()",
        "sampled": "### Problem Description\n```markdown\n**Store Sender Key**\n\nWe have a system that handles sender keys for messages sent between users. When a message is sent from user A to user B, a sender key record is created and associated with the sender key name.\n\nWrite a function `storeSenderKey` that takes in the `senderKeyName` and `senderKeyRecord` as input, and stores them in their respective data structures for later retrieval.\n\n### Input/Output Specifications\n\n*   **SenderKe"
    },
    {
        "original": "def detect_cadvisor(kubelet_url, cadvisor_port):\n        \"\"\"\n        Tries to connect to the cadvisor endpoint, with given params\n        :return: url if OK, raises exception if NOK\n        \"\"\"\n        if cadvisor_port == 0:\n            raise ValueError(\"cAdvisor port set to 0 in configuration\")\n        kubelet_hostname = urlparse(kubelet_url).hostname\n        if not kubelet_hostname:\n            raise ValueError(\"kubelet hostname empty\")\n        url = \"http://{}:{}{}\".format(kubelet_hostname, cadvisor_port, LEGACY_CADVISOR_METRICS_PATH)\n\n        # Test the endpoint is present\n        r = requests.head(url, timeout=1)\n        r.raise_for_status()\n\n        return url",
        "sampled": "```python\nimport requests\n\ndef detect_cadvisor(kubelet_url, cadvisor_port):\n    \"\"\"\n    Tries to connect to the cadvisor endpoint, with given params\n    :return: url if OK, raises exception if NOK\n    \"\"\"\n    \n    # Construct the cadvisor URL by appending the port to the kubelet URL\n    cadvisor_url = f\"{kubelet_url}:{cadvisor_port}\"\n    \n    try:\n        # Send a GET request to the constructed URL\n"
    },
    {
        "original": "def _SignedBinaryIDFromURN(binary_urn\n                          ):\n  \"\"\"Converts an AFF4 URN for a signed binary to a SignedBinaryID.\"\"\"\n  if binary_urn.RelativeName(GetAFF4PythonHackRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK,\n        path=binary_urn.RelativeName(GetAFF4PythonHackRoot()))\n  elif binary_urn.RelativeName(GetAFF4ExecutablesRoot()):\n    return rdf_objects.SignedBinaryID(\n        binary_type=rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE,\n        path=binary_urn.RelativeName(GetAFF4ExecutablesRoot()))\n  else:\n    raise ValueError(\"Unable to determine type of signed binary: %s.\" %\n                     binary_urn)",
        "sampled": "```python\ndef _SignedBinaryIDFromURN(binary_urn):\n    \"\"\"\n    Converts an AFF4 URN for a signed binary to a SignedBinaryID.\n    \n    The AFF4 URN is expected to be in the format:\n    urn:aff4:signed_binary:<hash_value>\n    \n    Where <hash_value> is the hexadecimal representation of the hash value.\n    \n    The SignedBinaryID is expected to be in the format:\n    <hash_value>\n    \n    Parameters:\n   "
    },
    {
        "original": "def _validate_disallowed(self, disallowed, field, value):\n        \"\"\" Readonly but with a custom error.\n\n        The rule's arguments are validated against this schema:\n        {'type': 'boolean'}\n        \"\"\"\n        if disallowed:\n            msg = 'disallowed user provided config option'\n            self._error(field, msg)",
        "sampled": "```python\ndef _validate_disallowed(self, disallowed, field, value):\n    \"\"\"\n    Readonly but with a custom error.\n\n    The rule's arguments are validated against this schema:\n    {'type': 'boolean'}\n    \n    Parameters:\n    disallowed (bool): \n        default = False\n        a variable as per instance\n    field (str): \n            the validation failed on and type of missing from here is key for instance attribute referencing referencing persmisp errolnfohigh"
    },
    {
        "original": "def make_type_consistent(s1, s2):\n    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n        return s1, s2\n\n    else:\n        return unicode(s1), unicode(s2)",
        "sampled": "# Problem Description\n### Ensuring String Consistency\n\nGiven two arguments, `s1` and `s2`, we need to ensure that both are either strings or Unicode objects. If they meet this condition, we do not need to modify them; however, if they do not meet this condition, we should force them to be Unicode objects.\n\n### Input and Output Specifications\n- The function takes in two string-like arguments `s1` and `s2`.\n- The output is a tuple"
    },
    {
        "original": "def _mod_repo_in_file(repo, repostr, filepath):\n    \"\"\"\n    Replace a repo entry in filepath with repostr\n    \"\"\"\n    with salt.utils.files.fopen(filepath) as fhandle:\n        output = []\n        for line in fhandle:\n            cols = salt.utils.args.shlex_split(\n                salt.utils.stringutils.to_unicode(line).strip()\n            )\n            if repo not in cols:\n                output.append(line)\n            else:\n                output.append(salt.utils.stringutils.to_str(repostr + '\\n'))\n    with salt.utils.files.fopen(filepath, 'w') as fhandle:\n        fhandle.writelines(output)",
        "sampled": "### Problem Description\nMarkdown format:\n```\n### Deletions and Additions\n\nThe problem you have to solve is from the \"Deletions and Additions\" chapter of the book.\n\nYou are given a list of integers representing the number of deletions and additions in a file. The list is in the format [deletions, additions]. You have to calculate the maximum number of deletions that can be made such that at least one addition is made.\n\nFor example, if we have"
    },
    {
        "original": "def bitphase_flip_operators(p):\n    \"\"\"\n    Return the bitphase flip kraus operators\n    \"\"\"\n    k0 = np.sqrt(1 - p) * I\n    k1 = np.sqrt(p) * Y\n    return k0, k1",
        "sampled": "### Problem Description\n#### Bit-Phase Flip Operators\n\nThe bit-phase flip operation is a quantum gate that flips the phase of a qubit. The Kraus operators for this operation are given by:\n\n$$A_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad A_1 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\n\nThese"
    },
    {
        "original": "def _hook(self, hook_name, doc_uri=None, **kwargs):\n        \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n        doc = self.workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=self.workspace, document=doc, **kwargs)",
        "sampled": "### Problem Description\n### ===============\n\nImplement a function `_hook` that calls a specified hook name and returns a list of results from all registered handlers.\n\n### Input and Output Specifications\n### ==============================\n\n*   `hook_name`: The name of the hook to be called.\n*   `doc_uri`: The URI of the document associated with the hook (optional).\n*   `kwargs`: Additional keyword arguments to be passed to the hook handlers.\n\n### Output\n*   A list of results from"
    },
    {
        "original": "def _get_col_items(mapping):\n    \"\"\"Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    from .variable import IndexVariable\n\n    col_items = []\n    for k, v in mapping.items():\n        col_items.append(k)\n        var = getattr(v, 'variable', v)\n        if isinstance(var, IndexVariable):\n            level_names = var.to_index_variable().level_names\n            if level_names is not None:\n                col_items += list(level_names)\n    return col_items",
        "sampled": "```python\ndef _get_col_items(mapping):\n    \"\"\"\n    Get all column items to format, including both keys of `mapping`\n    and MultiIndex levels if any.\n    \"\"\"\n    col_items = set(mapping.keys())\n    if isinstance(mapping, dict) and any(isinstance(v, dict) for v in mapping.values()):\n        for k, v in mapping.items():\n            if isinstance(v, dict):\n                col_items.update(_get_col_items(v))\n    return col_items\n```"
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            ifrealname = i['interface_name'].split(':')[0]\n            # Convert rate in bps ( to be able to compare to interface speed)\n            bps_rx = int(i['rx'] // i['time_since_update'] * 8)\n            bps_tx = int(i['tx'] // i['time_since_update'] * 8)\n            # Decorate the bitrate with the configuration file thresolds\n            alert_rx = self.get_alert(bps_rx, header=ifrealname + '_rx')\n            alert_tx = self.get_alert(bps_tx, header=ifrealname + '_tx')\n            # If nothing is define in the configuration file...\n            # ... then use the interface speed (not available on all systems)\n            if alert_rx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_rx = self.get_alert(current=bps_rx,\n                                          maximum=i['speed'],\n                                          header='rx')\n            if alert_tx == 'DEFAULT' and 'speed' in i and i['speed'] != 0:\n                alert_tx = self.get_alert(current=bps_tx,\n                                          maximum=i['speed'],\n                                          header='tx')\n            # then decorates\n            self.views[i[self.get_key()]]['rx']['decoration'] = alert_rx\n            self.views[i[self.get_key()]]['tx']['decoration'] = alert_tx",
        "sampled": "```python\nclass Solution:\n    def update_views(self):\n        \"\"\"\n        Update stats views.\n        \"\"\"\n        # Initialize a dictionary to store the views\n        views = {}\n        \n        # Initialize a variable to store the total views\n        total_views = 0\n        \n        # Initialize a variable to store the total clicks\n        total_clicks = 0\n        \n        # Initialize a variable to store the total impressions\n        total_impressions = 0\n        \n        # Initialize a variable to store the"
    },
    {
        "original": "def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        \"\"\"\n        label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label",
        "sampled": "```python\nimport numpy as np\n\nclass LabelLoader:\n    def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        \"\"\"\n        # Assuming the label image is stored in a 3D numpy array with shape (height, width, 400)\n        # where each pixel is represented"
    },
    {
        "original": "def read_channel(self):\n        \"\"\"Generic handler that will read all the data from an SSH or telnet channel.\"\"\"\n        output = \"\"\n        self._lock_netmiko_session()\n        try:\n            output = self._read_channel()\n        finally:\n            # Always unlock the SSH channel, even on exception.\n            self._unlock_netmiko_session()\n        return output",
        "sampled": "## Problem Description\n\n### Array Reordering\nGiven an array of integers *A* of size $N \\geq 4$ such that $A[0] > A[1] > A[N - 3] > ... > A[N - 4]$, and another integer $S$, determine whether there is any permutation (i.e. reordering) $\\rrr{X}$ of *A* such that there exists an index *i* ($0 \\leq i"
    },
    {
        "original": "def config_mode(self, config_command=\"config term\", pattern=\"\"):\n        \"\"\"\n        Enter into configuration mode on remote device.\n\n        Cisco IOS devices abbreviate the prompt at 20 chars in config mode\n        \"\"\"\n        if not pattern:\n            pattern = re.escape(self.base_prompt[:16])\n        return super(CiscoBaseConnection, self).config_mode(\n            config_command=config_command, pattern=pattern\n        )",
        "sampled": "### Problem Description\nNo problem description is provided. However, based on the given function `config_mode`, it seems to be a part of a network automation script, possibly using the Netmiko library for interacting with network devices.\n\n### Input and Output Specifications\nNo input and output specifications are provided.\n\n### Python Solution\n\n```python\nclass NetworkDevice:\n    def __init__(self):\n        self.config_command = \"config term\"\n        self.pattern = \"\"\n\n    def config_mode(self, config_command"
    },
    {
        "original": "def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n        @returns: list of builder names to build on\n        \"\"\"\n\n        # self.builderNames is the configured list of builders\n        # available for try.  If the user supplies a list of builders,\n        # it must be restricted to the configured list.  If not, build\n        # on all of the configured builders.\n        if builderNames:\n            for b in builderNames:\n                if b not in self.builderNames:\n                    log.msg(\"%s got with builder %s\" % (self, b))\n                    log.msg(\" but that wasn't in our list: %s\"\n                            % (self.builderNames,))\n                    return []\n        else:\n            builderNames = self.builderNames\n        return builderNames",
        "sampled": "```python\nclass BuilderFilter:\n    def __init__(self, builderNames):\n        # Initialize the class with a list of configured builder names\n        self.builderNames = builderNames\n\n    def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n"
    },
    {
        "original": "def fix_e262(self, result):\n        \"\"\"Fix spacing after comment hash.\"\"\"\n        target = self.source[result['line'] - 1]\n        offset = result['column']\n\n        code = target[:offset].rstrip(' \\t#')\n        comment = target[offset:].lstrip(' \\t#')\n\n        fixed = code + ('  # ' + comment if comment.strip() else '\\n')\n\n        self.source[result['line'] - 1] = fixed",
        "sampled": "```python\ndef fix_e262(self, result):\n    \"\"\"\n    Fix spacing after comment hash.\n    \n    This function takes a string as input and returns the modified string with proper spacing after a comment hash (#).\n    \n    Parameters:\n    result (str): The input string to be modified.\n    \n    Returns:\n    str: The modified string with proper spacing after a comment hash.\n    \"\"\"\n    \n    # Split the input string into lines\n    lines = result.split('\\n')\n    \n    #"
    },
    {
        "original": "def annToRLE(self, ann):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        t = self.imgs[ann['image_id']]\n        h, w = t['height'], t['width']\n        segm = ann['segmentation']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm['counts']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle",
        "sampled": "### Problem Description\nConvert annotation which can be polygons, uncompressed RLE to RLE.\n\n### Input and Output Specifications\nInput:\n- `ann`: annotation which can be polygons or uncompressed RLE\n\nOutput:\n- binary mask (numpy 2D array)\n\n### Python Solution\n\n```python\nimport numpy as np\n\nclass Solution:\n    def annToRLE(self, ann):\n        if isinstance(ann, dict):  # polygon annotation\n            x_coords = ann['x']\n            y"
    },
    {
        "original": "def cartesian_product(arrays, flat=True, copy=False):\n    \"\"\"\n    Efficient cartesian product of a list of 1D arrays returning the\n    expanded array views for each dimensions. By default arrays are\n    flattened, which may be controlled with the flat flag. The array\n    views can be turned into regular arrays with the copy flag.\n    \"\"\"\n    arrays = np.broadcast_arrays(*np.ix_(*arrays))\n    if flat:\n        return tuple(arr.flatten() if copy else arr.flat for arr in arrays)\n    return tuple(arr.copy() if copy else arr for arr in arrays)",
        "sampled": "```python\nimport numpy as np\n\ndef cartesian_product(arrays, flat=True, copy=False):\n    \"\"\"\n    Efficient cartesian product of a list of 1D arrays returning the\n    expanded array views for each dimensions. By default arrays are\n    flattened, which may be controlled with the flat flag. The array\n    views can be turned into regular arrays with the copy flag.\n    \"\"\"\n    \n    # Initialize result with first array in input list\n    result = np.array([array["
    },
    {
        "original": "def share_with_invite(self, recipients, require_sign_in=True,\n                          send_email=True, message=None, share_type='view'):\n        \"\"\" Sends an invitation to access or edit this DriveItem\n\n        :param recipients: a string or Contact or a list of the former\n         representing recipients of this invitation\n        :type recipients: list[str] or list[Contact] or str or Contact\n        :param bool require_sign_in: if True the recipients\n         invited will need to log in to view the contents\n        :param bool send_email: if True an email will be send to the recipients\n        :param str message: the body text of the message emailed\n        :param str share_type: 'view': will allow to read the contents.\n         'edit' will allow to modify the contents\n        :return: link to share\n        :rtype: DriveItemPermission\n        \"\"\"\n        if not self.object_id:\n            return None\n\n        to = []\n        if recipients is None:\n            raise ValueError('Provide a valid to parameter')\n        elif isinstance(recipients, (list, tuple)):\n            for x in recipients:\n                if isinstance(x, str):\n                    to.append({'email': x})\n                elif isinstance(x, Contact):\n                    to.append({'email': x.main_email})\n                else:\n                    raise ValueError(\n                        'All the recipients must be either strings or Contacts')\n        elif isinstance(recipients, str):\n            to.append({'email': recipients})\n        elif isinstance(recipients, Contact):\n            to.append({'email': recipients.main_email})\n        else:\n            raise ValueError(\n                'All the recipients must be either strings or Contacts')\n\n        url = self.build_url(\n            self._endpoints.get('share_invite').format(id=self.object_id))\n\n        data = {\n            'recipients': to,\n            self._cc('requireSignIn'): require_sign_in,\n            self._cc('sendInvitation'): send_email,\n        }\n        if share_type in {'view', 'read'}:\n            data['roles'] = ['read']\n        elif share_type == {'edit', 'write'}:\n            data['roles'] = ['write']\n        else:\n            raise ValueError(\n                '\"{}\" is not a valid share_type'.format(share_type))\n        if send_email and message:\n            data['message'] = message\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return DriveItemPermission(parent=self, **{self._cloud_data_key: data})",
        "sampled": "### Problem Analysis\n\nThe problem involves sending an invitation to access or edit a DriveItem in a Microsoft Graph program. The invitation can be sent to one or multiple recipients, and the invitation options can be customized.\n\nThe input parameters include:\n\n* `recipients`: One or multiple recipients to send the invitation to.\n* `require_sign_in`: Whether the recipient needs to log in to view the contents. Default value: `True`.\n* `send_email`: Whether an email should be sent to the recipient"
    },
    {
        "original": "def vm2json(vm):\n    \"\"\"\n    Get a JSON representation of a DEX file\n\n    :param vm: :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`\n    :return:\n    \"\"\"\n    d = {\"name\": \"root\", \"children\": []}\n\n    for _class in vm.get_classes():\n        c_class = {\"name\": _class.get_name(), \"children\": []}\n\n        for method in _class.get_methods():\n            c_method = {\"name\": method.get_name(), \"children\": []}\n\n            c_class[\"children\"].append(c_method)\n\n        d[\"children\"].append(c_class)\n\n    return json.dumps(d)",
        "sampled": "```python\nimport json\n\ndef vm2json(vm):\n    \"\"\"\n    Get a JSON representation of a DEX file\n\n    :param vm: :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`\n    :return:\n    \"\"\"\n\n    # Initialize an empty dictionary to store the JSON data\n    json_data = {}\n\n    # Add the magic number to the JSON data\n    json_data['magic'] = vm.get_magic()\n\n    # Add the header size to the JSON"
    },
    {
        "original": "def validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None:\n    \"\"\"\n    Ensure that the field's name does not shadow an existing attribute of the model.\n    \"\"\"\n    for base in bases:\n        if getattr(base, field_name, None):\n            raise NameError(\n                f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n                f'use a different field name with \"alias=\\'{field_name}\\'\".'\n            )",
        "sampled": "### Problem Description\nEnsure that the field's name does not shadow an existing attribute of the model.\n\n### Input and Output Specifications\n- `bases`: A list of base models\n- `field_name`: The name of the field to be validated\n- Output: None (raises an error if the field name shadows an existing attribute)\n\n### Python Solution\n```python\nfrom typing import List, Type\n\nclass BaseModel:\n    pass\n\ndef validate_field_name(bases: List[Type['BaseModel"
    },
    {
        "original": "def automatic_gamma_density(structure, kppa):\n        \"\"\"\n        Returns an automatic Kpoint object based on a structure and a kpoint\n        density. Uses Gamma centered meshes always. For GW.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n\n        Args:\n            structure:\n                Input structure\n            kppa:\n                Grid density\n        \"\"\"\n\n        latt = structure.lattice\n        lengths = latt.abc\n        ngrid = kppa / structure.num_sites\n\n        mult = (ngrid * lengths[0] * lengths[1] * lengths[2]) ** (1 / 3)\n        num_div = [int(round(mult / l)) for l in lengths]\n\n        # ensure that numDiv[i] > 0\n        num_div = [i if i > 0 else 1 for i in num_div]\n\n        # VASP documentation recommends to use even grids for n <= 8 and odd\n        # grids for n > 8.\n        num_div = [i + i % 2 if i <= 8 else i - i % 2 + 1 for i in num_div]\n\n        style = Kpoints.supported_modes.Gamma\n\n        comment = \"pymatgen 4.7.6+ generated KPOINTS with grid density = \" + \\\n                  \"{} / atom\".format(kppa)\n        num_kpts = 0\n        return Kpoints(comment, num_kpts, style, [num_div], [0, 0, 0])",
        "sampled": "### Problem Description\nAutomatic Gamma Density Calculation\n=====================================\n\nGiven a crystal structure and a desired k-point density, calculate the optimal k-point mesh for GW calculations using Gamma-centered meshes.\n\n### Input Specifications\n- `structure`: Input structure (e.g., lattice parameters, atom positions)\n- `kppa`: Desired k-point density (number of k-points per angstrom)\n\n### Output Specifications\n- `kpoint`: Optimal Kpoint object with scaled divisions along reciprocal lattice vectors\n\n### Python"
    },
    {
        "original": "def read_cs_g0_contribution(self):\n        \"\"\"\n            Parse the  G0 contribution of NMR chemical shielding.\n\n            Returns:\n            G0 contribution matrix as list of list.\n        \"\"\"\n        header_pattern = r'^\\s+G\\=0 CONTRIBUTION TO CHEMICAL SHIFT \\(field along BDIR\\)\\s+$\\n' \\\n                         r'^\\s+-{50,}$\\n' \\\n                         r'^\\s+BDIR\\s+X\\s+Y\\s+Z\\s*$\\n' \\\n                         r'^\\s+-{50,}\\s*$\\n'\n        row_pattern = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 3)\n        footer_pattern = r'\\s+-{50,}\\s*$'\n        self.read_table_pattern(header_pattern, row_pattern, footer_pattern, postprocess=float,\n                                last_one_only=True, attribute_name=\"cs_g0_contribution\")",
        "sampled": "```python\nimport numpy as np\n\nclass NMR:\n    def read_cs_g0_contribution(self):\n        \"\"\"\n        Parse the G0 contribution of NMR chemical shielding.\n\n        Returns:\n        G0 contribution matrix as list of list.\n        \"\"\"\n        \n        # Assuming we have input as CSV file with 4 columns and unknown number of rows\n        # Format is like this: \\[ \\frac{1}{2} (f_{++10} + f_{+-01}) \\"
    },
    {
        "original": "def _split_arrs(array_2d, slices):\n    \"\"\"\n    Equivalent to numpy.split(array_2d, slices),\n    but avoids fancy indexing\n    \"\"\"\n    if len(array_2d) == 0:\n        return np.empty(0, dtype=np.object)\n\n    rtn = np.empty(len(slices) + 1, dtype=np.object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn",
        "sampled": "```python\ndef _split_arrs(array_2d, slices):\n    \"\"\"\n    Equivalent to numpy.split(array_2d, slices),\n    but avoids fancy indexing\n    \"\"\"\n    result = []\n    \n    start = 0\n    \n    for i in range(len(slices)):\n        if i == len(slices) - 1:\n            result.append(array_2d[start:])\n        else:\n            result.append(array_2d[start:slices[i]])\n        \n        start = slices[i]\n"
    },
    {
        "original": "def load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n    if swag_type not in ('yaml', 'yml'):\n        raise AttributeError(\"Currently only yaml or yml supported\")\n        # TODO: support JSON\n\n    try:\n        enc = detect_by_bom(swag_path)\n        with codecs.open(swag_path, encoding=enc) as yaml_file:\n            return yaml_file.read()\n    except IOError:\n        # not in the same dir, add dirname\n        swag_path = os.path.join(\n            root_path or os.path.dirname(__file__), swag_path\n        )\n        try:\n            enc = detect_by_bom(swag_path)\n            with codecs.open(swag_path, encoding=enc) as yaml_file:\n                return yaml_file.read()\n        except IOError:  # pragma: no cover\n            # if package dir\n            # see https://github.com/rochacbruno/flasgger/pull/104\n            # Still not able to reproduce this case\n            # test are in examples/package_example\n            # need more detail on how to reproduce IOError here\n            swag_path = swag_path.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n            path = swag_path.replace(\n                (root_path or os.path.dirname(__file__)), ''\n            ).split(os.sep)[1:]\n            site_package = imp.find_module(path[0])[1]\n            swag_path = os.path.join(site_package, os.sep.join(path[1:]))\n            with open(swag_path) as yaml_file:\n                return yaml_file.read()",
        "sampled": "```python\nimport os\nimport yaml\n\ndef load_from_file(swag_path, swag_type='yml', root_path=None):\n    \"\"\"\n    Load specs from YAML file\n    \"\"\"\n\n    # Check the file type and read the contents accordingly\n    if swag_type.lower() == 'yml':\n        with open(swag_path, 'r') as f:\n            specs = yaml.safe_load(f)\n    elif swag_type.lower() == 'json':\n        with open(swag"
    },
    {
        "original": "def render(template, dest, **kwargs):\n    \"\"\"Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    \"\"\"\n\n    template = environment.get_template(template)\n    text = template.render(**kwargs)\n\n    f = open(dest, 'wb')\n    f.write(text.encode('utf-8'))\n    f.close()",
        "sampled": "```python\nfrom jinja2 import Template\nimport os\n\ndef render(template, dest, **kwargs):\n    \"\"\"Using jinja2, render `template` to the filename `dest`, supplying the\n\n    keyword arguments as template parameters.\n    \"\"\"\n    \n    # Read the template file\n    with open(template, 'r') as f:\n        t = Template(f.read())\n        \n        # Render the template with the given kwargs\n        rendered = t.render(**kwargs)\n        \n        # Write"
    },
    {
        "original": "def imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n        \"\"\"\n        Show the image stored in X on the canvas.\n        \n        The origin of the image show is (0,0), such that X[0,0] gets plotted at [0,0] of the image!\n        \n        the kwargs are plotting library specific kwargs!\n        \"\"\"\n        raise NotImplementedError(\"Implement all plot functions in AbstractPlottingLibrary in order to use your own plotting library\")",
        "sampled": "```python\nimport matplotlib.pyplot as plt\n\nclass Display:\n    def imshow(self, canvas, X, extent=None, label=None, vmin=None, vmax=None, **kwargs):\n        \"\"\"\n        Show the image stored in X on the canvas.\n\n        The origin of the image show is (0,0), such that X[0,0] gets plotted at [0,0] of the image!\n\n        the kwargs are plotting library specific kwargs!\n        \"\"\"\n        \n        # assume canvas is an"
    },
    {
        "original": "def _probvec(r, out):\n    \"\"\"\n    Fill `out` with randomly sampled probability vectors as rows.\n\n    To be complied as a ufunc by guvectorize of Numba. The inputs must\n    have the same shape except the last axis; the length of the last\n    axis of `r` must be that of `out` minus 1, i.e., if out.shape[-1] is\n    k, then r.shape[-1] must be k-1.\n\n    Parameters\n    ----------\n    r : ndarray(float)\n        Array containing random values in [0, 1).\n\n    out : ndarray(float)\n        Output array.\n\n    \"\"\"\n    n = r.shape[0]\n    r.sort()\n    out[0] = r[0]\n    for i in range(1, n):\n        out[i] = r[i] - r[i-1]\n    out[n] = 1 - r[n-1]",
        "sampled": "```python\nimport numpy as np\n\ndef _probvec(r, out):\n    \"\"\"\n    Fill `out` with randomly sampled probability vectors as rows.\n\n    To be complied as a ufunc by guvectorize of Numba. The inputs must\n    have the same shape except the last axis; the length of the last\n    axis of `r` must be that of `out` minus 1, i.e., if out.shape[-1] is\n    k, then r"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'text') and self.text is not None:\n            _dict['text'] = self.text\n        if hasattr(self, 'tokens') and self.tokens is not None:\n            _dict['tokens'] = self.tokens\n        if hasattr(self, 'readings') and self.readings is not None:\n            _dict['readings'] = self.readings\n        if hasattr(self, 'part_of_speech') and self.part_of_speech is not None:\n            _dict['part_of_speech'] = self.part_of_speech\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def integerBreak(self, n: int) -> int:\n        \"\"\"\n        Given an integer n, break it down up to k 1's (1 = no spaces, \n        k penetrations). The fewer the penetrations the more multitude your code will have. \n        Return an array of strings cut by some number of 'pens'. If you were to actually get your code cut in such a way.\n        \n        Example 1:\n            Input:"
    },
    {
        "original": "def bbox_to_poly(north, south, east, west):\n    \"\"\"\n    Convenience function to parse bbox -> poly\n    \"\"\"\n    \n    return Polygon([(west, south), (east, south), (east, north), (west, north)])",
        "sampled": "### Problem Analysis\nThe provided function, `bbox_to_poly`, seems to be a convenience function that converts bounding box (bbox) coordinates into a polygon. The bbox is defined by four parameters: north, south, east, and west latitude/longitude coordinates.\n\n### Understanding the Parameters\n\n- `north`: The northernmost latitude of the bounding box.\n- `south`: The southernmost latitude of the bounding box.\n- `east`: The easternmost longitude of the bounding box.\n- `west`:"
    },
    {
        "original": "def validate(config):\n    \"\"\"validate config file\"\"\"\n    with open(config) as fh:\n        content = fh.read()\n\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        log.error(\"config file: %s is not valid yaml\", config)\n        raise\n\n    try:\n        jsonschema.validate(data, CONFIG_SCHEMA)\n    except Exception:\n        log.error(\"config file: %s is not valid\", config)\n        raise\n\n    log.info(\"config file valid, accounts:%d\", len(data['accounts']))\n    return data",
        "sampled": "```python\nfrom typing import Dict, List\n\ndef validate(config: Dict) -> bool:\n    \"\"\"\n    Validate a config file.\n    \n    The config should be a dictionary containing the following keys:\n    - header (integer): the length of the key-value pairs in the following pairs\n    - K (integer): number of key-value pairs\n    - max_len_key (integer): maximum length of a key\n    - max_len_value (integer): maximum length of a value\n    \n    Each"
    },
    {
        "original": "def set_last_col_idx(self, last_col_idx):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tparam last_col_idx : int\n\t\t\tnumber of columns\n\t\t\"\"\"\n\t\tassert last_col_idx >= self._max_col\n\t\tself._max_col = last_col_idx\n\t\treturn self",
        "sampled": "```python\nclass DataFrame:\n    def __init__(self):\n        self.columns = []\n\n    def set_last_col_idx(self, last_col_idx):\n        \"\"\"\n        Parameters\n        ----------\n        param last_col_idx : int\n            number of columns\n        \"\"\"\n        \ndef create_dataframe(n_rows, n_cols):\n    df = DataFrame()\n    \n    # Create a list of column names (assuming they are 'col_0' to 'col_(n-1)')\n    df.columns = [f'"
    },
    {
        "original": "def _apply_mask(self):\n    \"\"\"Applies the passed-in mask to the convolution matrix.\n\n    Returns:\n      w: A copy of the convolution matrix that has had the mask applied.\n\n    Raises:\n      base.IncompatibleShapeError: If the mask shape has more dimensions than\n          the weight matrix.\n      base.IncompatibleShapeError: If the mask and the weight matrix don't\n          match on shape.\n    \"\"\"\n    w = self._w\n    w_shape = w.get_shape()\n    mask_shape = self._mask.get_shape()\n\n    if mask_shape.ndims > w_shape.ndims:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Max shape: {}\".format(\n              mask_shape.ndims, len(self._data_format)\n          )\n      )\n    if mask_shape != w_shape[:mask_shape.ndims]:\n      raise base.IncompatibleShapeError(\n          \"Invalid mask shape: {}. Weight shape: {}\".format(\n              mask_shape, w_shape\n          )\n      )\n    # TF broadcasting is a bit fragile.\n    # Expand the shape of self._mask by one dim at a time to the right\n    # until the rank matches `weight_shape`.\n    while self._mask.get_shape().ndims < w_shape.ndims:\n      self._mask = tf.expand_dims(self._mask, -1)\n\n    # tf.Variable & tf.ResourceVariable don't support *=.\n    w = w * self._mask  # pylint: disable=g-no-augmented-assignment\n\n    return w",
        "sampled": "```python\nimport numpy as np\n\nclass ConvolutionMatrix:\n    def __init__(self, weight_matrix):\n        self.weight_matrix = weight_matrix\n\n    def _apply_mask(self, mask):\n        \"\"\"\n        Applies the passed-in mask to the convolution matrix.\n\n        Args:\n            mask (numpy.ndarray): A 2D or 3D array representing the mask.\n\n        Returns:\n            w: A copy of the convolution matrix that has had the mask applied.\n\n        Raises:\n            ValueError:"
    },
    {
        "original": "def get_array_indices(self):\n        \"\"\"Returns an iterator of index token lists\"\"\"\n\n        for token in self.tokens:\n            if isinstance(token, SquareBrackets):\n                # Use [1:-1] index to discard the square brackets\n                yield token.tokens[1:-1]",
        "sampled": "### Problem Description\n### \nGiven an array of integers, find the maximum sum of a subarray of size `k` where `k` is a given positive integer.\n\n### Input and Output Specifications\n#### Input:\n- `arr`: A list of integers.\n- `k`: A positive integer representing the size of the subarray.\n\n#### Output:\n- The maximum sum of a subarray of size `k`.\n\n### Solution\n\n```python\nfrom collections import deque\n\nclass Solution:\n    def"
    },
    {
        "original": "def set_embeddings(self, embeddings):\n        \"\"\"\n        Specifies fixed set of embeddings\n        :param embeddings: array-like, sparse or dense, shape should be (embedding size, # terms)\n        :return: EmbeddingsResolver\n        \"\"\"\n        if self.embeddings_ is not None:\n            raise Exception(\"You have already set embeddings by running set_embeddings or set_embeddings_model.\")\n        assert embeddings.shape[1] == self.corpus_.get_num_terms()\n        self.embeddings_ = embeddings.T\n        self.vocab_ = self.corpus_.get_terms()\n        return self",
        "sampled": "### Problem Description\nEmbeddings in Natural Language Processing (NLP) refer to the vector representations of words or tokens in a high-dimensional space. These embeddings allow models to capture nuanced relationships between words and improve their performance on NLP tasks.\n\n## Set Embeddings Problem\nGiven an array-like list of numerical values representing word embeddings, create a function `set_embeddings` that takes this list as input and returns an instance of `EmbeddingsResolver`.\n\n### Input Specifications\n\n*   `embeddings`: array"
    },
    {
        "original": "def parse_degrees(cls, degrees, arcminutes, arcseconds, direction=None):\n        \"\"\"\n        Parse degrees minutes seconds including direction (N, S, E, W)\n        \"\"\"\n        degrees = float(degrees)\n        negative = degrees < 0\n        arcminutes = float(arcminutes)\n        arcseconds = float(arcseconds)\n\n        if arcminutes or arcseconds:\n            more = units.degrees(arcminutes=arcminutes, arcseconds=arcseconds)\n            if negative:\n                degrees -= more\n            else:\n                degrees += more\n\n        if direction in [None, 'N', 'E']:\n            return degrees\n        elif direction in ['S', 'W']:\n            return -degrees\n        else:\n            raise ValueError(\"Invalid direction! Should be one of [NSEW].\")",
        "sampled": "```python\nclass Coordinate:\n    def __init__(self, degrees, arcminutes, arcseconds, direction=None):\n        self.degrees = degrees\n        self.arcminutes = arcminutes\n        self.arcseconds = arcseconds\n        self.direction = direction\n\n    @classmethod\n    def parse_degrees(cls, degrees, arcminutes=0, arcseconds=0, direction=None):\n        \"\"\"\n        Parse degrees minutes seconds including direction (N, S, E, W)\n        \n        Args"
    },
    {
        "original": "def _execute_with_retries(conn, function, **kwargs):\n    \"\"\"\n    Retry if we're rate limited by AWS or blocked by another call.\n    Give up and return error message if resource not found or argument is invalid.\n\n    conn\n        The connection established by the calling method via _get_conn()\n\n    function\n        The function to call on conn. i.e. create_stream\n\n    **kwargs\n        Any kwargs required by the above function, with their keywords\n        i.e. StreamName=stream_name\n\n    Returns:\n        The result dict with the HTTP response and JSON data if applicable\n        as 'result', or an error as 'error'\n\n    CLI example::\n\n        salt myminion boto_kinesis._execute_with_retries existing_conn function_name function_kwargs\n\n    \"\"\"\n    r = {}\n    max_attempts = 18\n    max_retry_delay = 10\n    for attempt in range(max_attempts):\n        log.info(\"attempt: %s function: %s\", attempt, function)\n        try:\n            fn = getattr(conn, function)\n            r['result'] = fn(**kwargs)\n            return r\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response['Error']['Code']\n            if \"LimitExceededException\" in error_code or \"ResourceInUseException\" in error_code:\n                # could be rate limited by AWS or another command is blocking,\n                # retry with exponential backoff\n                log.debug(\"Retrying due to AWS exception\", exc_info=True)\n                time.sleep(_jittered_backoff(attempt, max_retry_delay))\n            else:\n                # ResourceNotFoundException or InvalidArgumentException\n                r['error'] = e.response['Error']\n                log.error(r['error'])\n                r['result'] = None\n                return r\n\n    r['error'] = \"Tried to execute function {0} {1} times, but was unable\".format(function, max_attempts)\n    log.error(r['error'])\n    return r",
        "sampled": "```python\nimport time\nimport random\n\ndef _execute_with_retries(conn, function, **kwargs):\n    \"\"\"\n    Retry if we're rate limited by AWS or blocked by another call.\n    Give up and return error message if resource not found or argument is invalid.\n\n    conn\n        The connection established by the calling method via _get_conn()\n\n    function\n        The function to call on conn. i.e. create_stream\n\n    **kwargs\n        Any kwargs required by the above function"
    },
    {
        "original": "def calculate_recommendations(output_filename, model_name=\"als\"):\n    \"\"\" Generates artist recommendations for each user in the dataset \"\"\"\n    # train the model based off input params\n    artists, users, plays = get_lastfm()\n\n    # create a model from the input data\n    model = get_model(model_name)\n\n    # if we're training an ALS based model, weight input for last.fm\n    # by bm25\n    if issubclass(model.__class__, AlternatingLeastSquares):\n        # lets weight these models by bm25weight.\n        logging.debug(\"weighting matrix by bm25_weight\")\n        plays = bm25_weight(plays, K1=100, B=0.8)\n\n        # also disable building approximate recommend index\n        model.approximate_similar_items = False\n\n    # this is actually disturbingly expensive:\n    plays = plays.tocsr()\n\n    logging.debug(\"training model %s\", model_name)\n    start = time.time()\n    model.fit(plays)\n    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n\n    # generate recommendations for each user and write out to a file\n    start = time.time()\n    user_plays = plays.T.tocsr()\n    with tqdm.tqdm(total=len(users)) as progress:\n        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n            for userid, username in enumerate(users):\n                for artistid, score in model.recommend(userid, user_plays):\n                    o.write(\"%s\\t%s\\t%s\\n\" % (username, artists[artistid], score))\n                progress.update(1)\n    logging.debug(\"generated recommendations in %0.2fs\",  time.time() - start)",
        "sampled": "### Problem Description\nGenerate artist recommendations for each user in the dataset.\n\n### Input Specifications\n- The dataset is assumed to be a matrix where rows represent users and columns represent artists.\n- The matrix contains ratings or preferences of users for each artist.\n\n### Output Specifications\n- A file containing artist recommendations for each user.\n\n### Python Solution\n```python\nimport numpy as np\nfrom scipy.sparse.linalg import svds\nimport pandas as pd\n\ndef calculate_recommendations(output_filename, model_name=\"als\"):\n"
    },
    {
        "original": "def fit_transform(self, input, **fit_kwargs):\n        \"\"\"Execute fit and transform in sequence.\"\"\"\n        self.fit(input, **fit_kwargs)\n        X = self.transform(input)\n        return X",
        "sampled": "### Problem Description\n#### [Fit and Transform](https://www.hackerrank.com/challenges/fit-and-transform/problem)\n\nYou are given a dataset of students' scores in two subjects, Math and Science. The scores are represented as a matrix where each row represents a student and each column represents a subject. The goal is to fit a linear regression model to the data and transform the scores into a new feature space.\n\n### Input and Output Specifications\n\n*   The input is a 2D"
    },
    {
        "original": "def patch_python_logging_handlers():\n    \"\"\"\n    Patch the python logging handlers with out mixed-in classes\n    \"\"\"\n    logging.StreamHandler = StreamHandler\n    logging.FileHandler = FileHandler\n    logging.handlers.SysLogHandler = SysLogHandler\n    logging.handlers.WatchedFileHandler = WatchedFileHandler\n    logging.handlers.RotatingFileHandler = RotatingFileHandler\n    if sys.version_info >= (3, 2):\n        logging.handlers.QueueHandler = QueueHandler",
        "sampled": "## Problem Description\n### Patching Python Logging Handlers\n\nThe problem requires us to patch the Python logging handlers with our own mixed-in classes. However, the problem description is incomplete and does not provide any specific requirements or constraints for the patching process.\n\nGiven this, I will assume that we need to create a function `patch_python_logging_handlers` that patches the Python logging handlers with our own custom classes. We will use a simple example where we create a custom handler class and mix it into the existing"
    },
    {
        "original": "def get_review_requests(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/pulls/:number/requested_reviewers <https://developer.github.com/v3/pulls/review_requests/>`_\n        :rtype: tuple of :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser` and of :class:`github.PaginatedList.PaginatedList` of :class:`github.Team.Team`\n        \"\"\"\n        return (\n            github.PaginatedList.PaginatedList(\n                github.NamedUser.NamedUser,\n                self._requester,\n                self.url + \"/requested_reviewers\",\n                None,\n                list_item='users'\n            ),\n            github.PaginatedList.PaginatedList(\n                github.Team.Team,\n                self._requester,\n                self.url + \"/requested_reviewers\",\n                None,\n                list_item='teams'\n            )\n        )",
        "sampled": "```python\nimport requests\nimport json\n\nclass GitHubAPI:\n    def __init__(self, owner, repo, token):\n        self.owner = owner\n        self.repo = repo\n        self.token = token\n        self.base_url = f'https://api.github.com/repos/{owner}/{repo}/pulls'\n\n    def get_review_requests(self, number):\n        headers = {'Authorization': f'token {self.token}'}\n        params = {'number': number}\n        response = requests.get"
    },
    {
        "original": "def to_dict(self):\n        \"\"\"Transforms the object to a Python dictionary.\n\n            Note:\n                If an Input hasn't been signed yet, this method returns a\n                dictionary representation.\n\n            Returns:\n                dict: The Input as an alternative serialization format.\n        \"\"\"\n        try:\n            fulfillment = self.fulfillment.serialize_uri()\n        except (TypeError, AttributeError, ASN1EncodeError, ASN1DecodeError):\n            fulfillment = _fulfillment_to_details(self.fulfillment)\n\n        try:\n            # NOTE: `self.fulfills` can be `None` and that's fine\n            fulfills = self.fulfills.to_dict()\n        except AttributeError:\n            fulfills = None\n\n        input_ = {\n            'owners_before': self.owners_before,\n            'fulfills': fulfills,\n            'fulfillment': fulfillment,\n        }\n        return input_",
        "sampled": "```python\nclass Input:\n    def __init__(self, name, value, signed=False):\n        self.name = name\n        self.value = value\n        self.signed = signed\n\n    def to_dict(self):\n        \"\"\"Transforms the object to a Python dictionary.\n\n        Note:\n            If an Input hasn't been signed yet, this method returns a\n            dictionary representation.\n\n        Returns:\n            dict: The Input as an alternative serialization format.\n        \"\"\"\n        return {\n            'name"
    },
    {
        "original": "def clone(self, callable=None, **overrides):\n        \"\"\"Clones the Callable optionally with new settings\n\n        Args:\n            callable: New callable function to wrap\n            **overrides: Parameter overrides to apply\n\n        Returns:\n            Cloned Callable object\n        \"\"\"\n        old = {k: v for k, v in self.get_param_values()\n               if k not in ['callable', 'name']}\n        params = dict(old, **overrides)\n        callable = self.callable if callable is None else callable\n        return self.__class__(callable, **params)",
        "sampled": "```python\nclass Callable:\n    def __init__(self, func):\n        self.func = func\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def clone(self, callable=None, **overrides):\n        \"\"\"Clones the Callable optionally with new settings\n\n        Args:\n            callable: New callable function to wrap\n            **overrides: Parameter overrides to apply\n\n        Returns:\n            Cloned Callable object\n        \"\"\"\n        if"
    },
    {
        "original": "def user_list(**connection_args):\n    \"\"\"\n    Return a list of users on a MySQL server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mysql.user_list\n    \"\"\"\n    dbc = _connect(**connection_args)\n    if dbc is None:\n        return []\n    cur = dbc.cursor(MySQLdb.cursors.DictCursor)\n    try:\n        qry = 'SELECT User,Host FROM mysql.user'\n        _execute(cur, qry)\n    except MySQLdb.OperationalError as exc:\n        err = 'MySQL Error {0}: {1}'.format(*exc.args)\n        __context__['mysql.error'] = err\n        log.error(err)\n        return []\n    results = cur.fetchall()\n    log.debug(results)\n    return results",
        "sampled": "```python\nimport mysql.connector\n\ndef user_list(**connection_args):\n    \"\"\"\n    Return a list of users on a MySQL server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mysql.user_list\n    \"\"\"\n\n    # Establish a connection to the MySQL server\n    cnx = mysql.connector.connect(**connection_args)\n\n    # Create a cursor object to execute SQL queries\n    cursor = cnx.cursor()\n\n    # Execute the query to get all users from the 'mysql.user"
    },
    {
        "original": "def reboot(search, one=True, force=False):\n    \"\"\"\n    Reboot one or more vms\n\n    search : string\n        filter vms, see the execution module.\n    one : boolean\n        reboot only one vm\n    force : boolean\n        force reboot, faster but no graceful shutdown\n\n    .. note::\n        If the search parameter does not contain an equal (=) symbol it will be\n        assumed it will be tried as uuid, hostname, and alias.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.reboot 91244bba-1146-e4ec-c07e-e825e0223aa9\n        salt-run vmadm.reboot search='alias=marije'\n        salt-run vmadm.reboot search='type=KVM' one=False\n    \"\"\"\n    return _action('reboot', search, one, force)",
        "sampled": "```python\nimport salt.client\n\ndef reboot(search, one=True, force=False):\n    \"\"\"\n    Reboot one or more vms\n\n    search : string\n        filter vms, see the execution module.\n    one : boolean\n        reboot only one vm\n    force : boolean\n        force reboot, faster but no graceful shutdown\n\n    .. note::\n        If the search parameter does not contain an equal (=) symbol it will be\n        assumed it will be tried as uuid, hostname"
    },
    {
        "original": "def InitFromAff4Object(self, aff4_obj, aff4_cls, attr_blacklist):\n    \"\"\"Initializes the current instance from an Aff4Object.\n\n    Iterates over all attributes of the Aff4Object defined by a given class\n    and adds a representation of them to the current instance.\n\n    Args:\n      aff4_obj: An Aff4Object to take the attributes from.\n      aff4_cls: A class in the inheritance hierarchy of the Aff4Object defining\n        which attributes to take.\n      attr_blacklist: A list of already added attributes as to not add\n        attributes multiple times.\n\n    Returns:\n      A reference to the current instance.\n    \"\"\"\n    self.name = str(aff4_cls.__name__)\n    self.attributes = []\n\n    schema = aff4_cls.SchemaCls\n    for name, attribute in sorted(iteritems(schema.__dict__)):\n      if not isinstance(attribute, aff4.Attribute):\n        continue\n\n      if name in attr_blacklist:\n        continue\n\n      attr_repr = ApiAff4ObjectAttribute()\n      attr_repr.name = name\n      attr_repr.description = attribute.description\n      attr_repr.values = []\n\n      values = list(aff4_obj.GetValuesForAttribute(attribute))\n      for value in values:\n        # This value is really a LazyDecoder() instance. We need to get at the\n        # real data here.\n        # TODO(user): Change GetValuesForAttribute to resolve\n        # lazy decoders and directly yield the rdf value.\n        if hasattr(value, \"ToRDFValue\"):\n          value = value.ToRDFValue()\n\n        value_repr = ApiAff4ObjectAttributeValue()\n        value_repr.Set(\"type\", compatibility.GetName(value.__class__))\n        value_repr.Set(\"age\", value.age)\n        value_repr.value = value\n        attr_repr.values.append(value_repr)\n\n      if attr_repr.values:\n        self.attributes.append(attr_repr)\n\n    return self",
        "sampled": "```python\ndef InitFromAff4Object(self, aff4_obj, aff4_cls, attr_blacklist):\n    \"\"\"\n    Initializes the current instance from an Aff4Object.\n\n    Iterates over all attributes of the Aff4Object defined by a given class\n    and adds a representation of them to the current instance.\n\n    Args:\n      aff4_obj: An Aff4Object to take the attributes from.\n      aff4_cls: A class in the inheritance hierarchy of the Aff4Object defining"
    },
    {
        "original": "def setPartationId(self, partationId):\n        \"\"\"set Thread Network Partition ID\n\n        Args:\n            partitionId: partition id to be set by leader\n\n        Returns:\n            True: successful to set the Partition ID\n            False: fail to set the Partition ID\n        \"\"\"\n        print '%s call setPartationId' % self.port\n        print partationId\n\n        cmd = WPANCTL_CMD + 'setprop Network:PartitionId %s' %(str(hex(partationId)).rstrip('L'))\n        print cmd\n        return self.__sendCommand(cmd)[0] != 'Fail'",
        "sampled": "### Problem Description\nSet Partition ID in Thread Network\n=====================================\n\n### Problem Statement\nIn a distributed system, each node needs to have a unique partition ID to identify its network partition. The leader node is responsible for setting the partition ID for all nodes in the network.\n\n### Input and Output Specifications\n\n*   `partitionId`: The unique identifier for the network partition.\n*   `Returns`:\n    *   `True`: Successful to set the Partition ID.\n    *   `False`:"
    },
    {
        "original": "def get_values_json(self, lst, list_columns):\n        \"\"\"\n            Converts list of objects from query to JSON\n        \"\"\"\n        result = []\n        for item in self.get_values(lst, list_columns):\n            for key, value in list(item.items()):\n                if isinstance(value, datetime.datetime) or isinstance(\n                    value, datetime.date\n                ):\n                    value = value.isoformat()\n                    item[key] = value\n                if isinstance(value, list):\n                    item[key] = [str(v) for v in value]\n            result.append(item)\n        return result",
        "sampled": "### Problem Description\n### \nConvert list of objects from query to JSON\n\n### Input and Output Specifications\n#### Input:\n- `lst`: A list of objects from a query.\n- `list_columns`: A list of column names.\n\n#### Output:\n- A JSON string representing the input list of objects.\n\n### Python Solution\n\n```python\nimport json\n\ndef get_values_json(self, lst, list_columns):\n    \"\"\"\n    Converts list of objects from query to JSON\n    \"\"\"\n    \n    # Initialize"
    },
    {
        "original": "def _gen_get_more_command(cursor_id, coll, batch_size, max_await_time_ms):\n    \"\"\"Generate a getMore command document.\"\"\"\n    cmd = SON([('getMore', cursor_id),\n               ('collection', coll)])\n    if batch_size:\n        cmd['batchSize'] = batch_size\n    if max_await_time_ms is not None:\n        cmd['maxTimeMS'] = max_await_time_ms\n    return cmd",
        "sampled": "```python\ndef _gen_get_more_command(cursor_id, coll, batch_size, max_await_time_ms):\n    \"\"\"\n    Generate a getMore command document.\n    \n    :param cursor_id: The ID of the cursor.\n    :param coll: The collection name.\n    :param batch_size: The batch size.\n    :param max_await_time_ms: The maximum await time in milliseconds.\n    :return: A dictionary representing the getMore command document.\n    \"\"\"\n    get_more_command"
    },
    {
        "original": "def set_baudrate(self, channel, BTR, baudarate):\n        \"\"\"\n        This function is used to configure the baud rate of specific CAN channel of a device.\n\n        :param int channel:\n            CAN channel, which is to be configured (:data:`Channel.CHANNEL_CH0` or :data:`Channel.CHANNEL_CH1`).\n        :param int BTR:\n            Baud rate register BTR0 as high byte, baud rate register BTR1 as low byte (see enum :class:`Baudrate`).\n        :param int baudarate: Baud rate register for all systec USB-CANmoduls (see enum :class:`BaudrateEx`>).\n        \"\"\"\n        UcanSetBaudrateEx(self._handle, channel, BTR >> 8, BTR, baudarate)",
        "sampled": "```python\nfrom enum import Enum\n\nclass Baudrate(Enum):\n    # Define baud rate values for BTR0 and BTR1 registers\n    BAUD_100K = 0b00011000\n    BAUD_125K = 0b00011111\n    \n    # Add other baud rates as necessary\n\nclass Channel(Enum):\n    CHANNEL_CH0 = 0\n    CHANNEL_CH1 = 1\n\nclass BaudrateEx(Enum):\n    BAUD_RATE_100"
    },
    {
        "original": "def _get_day_of_month(other, day_option):\n    \"\"\"Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n    -------\n    day_of_month : int\n\n    \"\"\"\n\n    if day_option == 'start':\n        return 1\n    elif day_option == 'end':\n        days_in_month = _days_in_month(other)\n        return days_in_month\n    elif day_option is None:\n        # Note: unlike `_shift_month`, _get_day_of_month does not\n        # allow day_option = None\n        raise NotImplementedError\n    else:\n        raise ValueError(day_option)",
        "sampled": "```python\nimport cftime\n\ndef _get_day_of_month(other, day_option):\n    \"\"\"\n    Find the day in `other`'s month that satisfies a BaseCFTimeOffset's\n    onOffset policy, as described by the `day_option` argument.\n\n    Parameters\n    ----------\n    other : cftime.datetime\n    day_option : 'start', 'end'\n        'start': returns 1\n        'end': returns last day of the month\n\n    Returns\n   "
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'configurations') and self.configurations is not None:\n            _dict['configurations'] = [\n                x._to_dict() for x in self.configurations\n            ]\n        return _dict",
        "sampled": "### Problem Description\n### \n### Problem: \n### Given an array of integers, find the maximum number of non-overlapping subarrays that can be formed such that the sum of the subarray is greater than or equal to the sum of the entire array divided by the number of subarrays.\n\n### Input Specifications:\n### - `arr`: a list of integers\n### - `k`: an integer representing the number of subarrays\n\n### Output Specifications:\n### - `max_subarrays`: the maximum"
    },
    {
        "original": "def cli(self, method):\n        \"\"\"Registers a method on an Object as a CLI route\"\"\"\n        routes = getattr(method, '_hug_cli_routes', [])\n        routes.append(self.route)\n        method._hug_cli_routes = routes\n        return method",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n        self.arr = []\n\n    def getAverages(self, arr: list[int], n: int, k: int) -> list[int]:\n        self.n = n\n        self.k = k\n        self.arr = arr\n        self.calculate_averages()\n        return self.arr\n\n    def calculate_averages(self):\n        window_size = 2"
    },
    {
        "original": "def _run_and_measure(self, quil_program, qubits, trials, random_seed) -> np.ndarray:\n        \"\"\"\n        Run a Forest ``run_and_measure`` job.\n\n        Users should use :py:func:`WavefunctionSimulator.run_and_measure` instead of calling\n        this directly.\n        \"\"\"\n        payload = run_and_measure_payload(quil_program, qubits, trials, random_seed)\n        response = post_json(self.session, self.sync_endpoint + \"/qvm\", payload)\n        return np.asarray(response.json())",
        "sampled": "```python\nimport numpy as np\n\nclass Forest:\n    def _run_and_measure(self, quil_program, qubits, trials, random_seed):\n        \"\"\"\n        Run a Forest ``run_and_measure`` job.\n\n        Users should use :py:func:`WavefunctionSimulator.run_and_measure` instead of calling\n        this directly.\n        \"\"\"\n        \n        # Initialize the random number generator with the given seed\n        np.random.seed(random_seed)\n        \n        # Initialize the result array with zeros\n"
    },
    {
        "original": "def get_status(self):\n        \"\"\"\n        :calls: `GET /user/migrations/:migration_id`_\n        :rtype: str\n        \"\"\"\n        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url,\n            headers={\n                \"Accept\": Consts.mediaTypeMigrationPreview\n            }\n        )\n        self._useAttributes(data)\n        return self.state",
        "sampled": "I'm ready to analyze the challenge and provide a solution. However, I don't see the problem description in Markdown format. Please provide the complete problem description, input and output specifications.\n\nOnce I have that information, I'll be able to understand the challenge and provide a Python solution.\n\nPlease paste the problem description here:\n\n```markdown\n### Problem Description\n\n### Input Specifications\n\n### Output Specifications\n```\n\nI'll wait for your input before providing a solution."
    },
    {
        "original": "def serialize_close(code: int, reason: str) -> bytes:\n    \"\"\"\n    Serialize the data for a close frame.\n\n    This is the reverse of :func:`parse_close`.\n\n    \"\"\"\n    check_close(code)\n    return struct.pack(\"!H\", code) + reason.encode(\"utf-8\")",
        "sampled": "```python\ndef serialize_close(code: int, reason: str) -> bytes:\n    \"\"\"\n    Serialize the data for a close frame.\n\n    This is the reverse of :func:`parse_close`.\n\n    \"\"\"\n    # Convert the code to bytes\n    code_bytes = code.to_bytes(2, 'big')\n    \n    # Encode the reason to bytes\n    reason_bytes = reason.encode('utf-8')\n    \n    # Calculate the total length of the close frame\n    total_length = "
    },
    {
        "original": "def configure_host_cache(host_ref, datastore_ref, swap_size_MiB,\n                         host_cache_manager=None):\n    \"\"\"\n    Configures the host cahe of the specified host\n\n    host_ref\n        The vim.HostSystem object representing the host that contains the\n        requested disks.\n\n    datastore_ref\n        The vim.Datastore opject representing the datastore the host cache will\n        be configured on.\n\n    swap_size_MiB\n        The size in Mibibytes of the swap.\n\n    host_cache_manager\n        The vim.HostCacheConfigurationManager object representing the cache\n        configuration manager on the specified host. Default is None. If None,\n        it will be retrieved in the method\n    \"\"\"\n    hostname = get_managed_object_name(host_ref)\n    if not host_cache_manager:\n        props = get_properties_of_managed_object(\n            host_ref, ['configManager.cacheConfigurationManager'])\n        if not props.get('configManager.cacheConfigurationManager'):\n            raise salt.exceptions.VMwareObjectRetrievalError(\n                'Host \\'{0}\\' has no host cache'.format(hostname))\n        host_cache_manager = props['configManager.cacheConfigurationManager']\n    log.trace('Configuring the host cache on host \\'%s\\', datastore \\'%s\\', '\n              'swap size=%s MiB', hostname, datastore_ref.name, swap_size_MiB)\n\n    spec = vim.HostCacheConfigurationSpec(\n        datastore=datastore_ref,\n        swapSize=swap_size_MiB)\n    log.trace('host_cache_spec=%s', spec)\n    try:\n        task = host_cache_manager.ConfigureHostCache_Task(spec)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    wait_for_task(task, hostname, 'HostCacheConfigurationTask')\n    log.trace('Configured host cache on host \\'%s\\'', hostname)\n    return True",
        "sampled": "```python\nclass Vim:\n    class HostSystem:\n        def __init__(self, name):\n            self.name = name\n\n    class Datastore:\n        def __init__(self, name):\n            self.name = name\n\n    class HostCacheConfigurationManager:\n        def __init__(self, host_system):\n            self.host_system = host_system\n\ndef configure_host_cache(host_ref, datastore_ref, swap_size_MiB, host_cache_manager=None):\n    \"\"\"\n    Configures the host ca"
    },
    {
        "original": "def tearDown(self):\n        \"\"\"Clean up after each case.\n\n        Stop harness service, close browser and close DUT.\n        \"\"\"\n        if self.__class__ is HarnessCase:\n            return\n\n        logger.info('Tearing down')\n        self._destroy_harness()\n        self._destroy_browser()\n        self._destroy_dut()\n        self._destroy_rf_shield()",
        "sampled": "### Problem Description\nFind the first duplicate in an array of integers\n==============================================\n\n### Problem Statement\nGiven an array of integers, find the duplicate element.\n\n### Input and Output Specifications\n* Input format: `arr` (list of integers)\n* Output format: integer (first duplicate element) or -1 if no duplicates are found\n\n### Solution\n\n```python\ndef find_first_duplicate(arr):\n    \"\"\"\n    Find the first duplicate in an array of integers.\n    \n    Parameters:\n    arr"
    },
    {
        "original": "def add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)",
        "sampled": "```python\nclass InvalidFilterError(Exception):\n    pass\n\nclass Subscriber:\n    def __init__(self):\n        self.subscribers = {}\n\n    def add_subscriber(self, connection_id, subscriptions, last_known_block_id):\n        \"\"\"\n        Register the subscriber for the given event subscriptions.\n\n        Args:\n            connection_id (str): The ID of the connection.\n            subscriptions (list): A list of event filters to subscribe to.\n            last_known_block_id (int): The ID of the last block"
    },
    {
        "original": "def assert_coordinate_consistent(obj, coords):\n    \"\"\" Maeke sure the dimension coordinate of obj is\n    consistent with coords.\n\n    obj: DataArray or Dataset\n    coords: Dict-like of variables\n    \"\"\"\n    for k in obj.dims:\n        # make sure there are no conflict in dimension coordinates\n        if k in coords and k in obj.coords:\n            if not coords[k].equals(obj[k].variable):\n                raise IndexError(\n                    'dimension coordinate {!r} conflicts between '\n                    'indexed and indexing objects:\\n{}\\nvs.\\n{}'\n                    .format(k, obj[k], coords[k]))",
        "sampled": "```python\nimport xarray as xr\n\ndef assert_coordinate_consistent(obj, coords):\n    \"\"\"\n    Make sure the dimension coordinate of obj is consistent with coords.\n\n    Parameters:\n    obj (DataArray or Dataset): Input data object\n    coords (Dict-like of variables): Expected coordinates\n\n    Returns:\n    None: If the coordinates are consistent, otherwise raises an AssertionError\n    \"\"\"\n\n    # Check if obj is a DataArray or Dataset\n    if not isinstance(obj, (xr.DataArray"
    },
    {
        "original": "def reply_chat_action(\n        self,\n        action: Union[ChatAction, str],\n        progress: int = 0\n    ) -> \"Message\":\n        \"\"\"Bound method *reply_chat_action* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.send_chat_action(\n                chat_id=message.chat.id,\n                action=\"typing\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.reply_chat_action(\"typing\")\n\n        Args:\n            action (:obj:`ChatAction <pyrogram.ChatAction>` | ``str``):\n                Type of action to broadcast.\n                Choose one from the :class:`ChatAction <pyrogram.ChatAction>` enumeration,\n                depending on what the user is about to receive.\n                You can also provide a string (e.g. \"typing\", \"upload_photo\", \"record_audio\", ...).\n\n            progress (``int``, *optional*):\n                Progress of the upload process.\n                Currently useless because official clients don't seem to be handling this.\n\n        Returns:\n            On success, True is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n            ``ValueError`` if the provided string is not a valid ChatAction.\n        \"\"\"\n        return self._client.send_chat_action(\n            chat_id=self.chat.id,\n            action=action,\n            progress=progress\n        )",
        "sampled": "```python\nfrom pyrogram import Client, ChatAction\n\ndef reply_chat_action(client: Client, action: Union[ChatAction, str], progress: int = 0) -> bool:\n    \"\"\"\n    Sends a chat action to the user.\n\n    Args:\n        client (Client): The Pyrogram client instance.\n        action (ChatAction | str): The type of action to broadcast.\n        progress (int, optional): Progress of the upload process. Defaults to 0.\n\n    Returns:\n"
    },
    {
        "original": "def write(self, path):\n        \"\"\"Create a new object with the desired output schema and write it.\"\"\"\n        meta = {\n            'generated_at': self.generated_at,\n            'elapsed_time': self.elapsed_time,\n        }\n        sources = {}\n        for result in self.results:\n            unique_id = result.node.unique_id\n            if result.error is not None:\n                result_dict = {\n                    'error': result.error,\n                    'state': 'runtime error'\n                }\n            else:\n                result_dict = {\n                    'max_loaded_at': result.max_loaded_at,\n                    'snapshotted_at': result.snapshotted_at,\n                    'max_loaded_at_time_ago_in_s': result.age,\n                    'state': result.status,\n                    'criteria': result.node.freshness,\n                }\n            sources[unique_id] = result_dict\n        output = FreshnessRunOutput(meta=meta, sources=sources)\n        output.write(path)",
        "sampled": "```python\nclass Solution:\n    def write(self, path):\n        # Create a new object with the desired output schema and write it.\n        # For simplicity, let's assume we're writing a JSON file.\n        import json\n\n        data = {\n            \"name\": \"John\",\n            \"age\": 30,\n            \"city\": \"New York\"\n        }\n\n        with open(path, 'w') as f:\n            json.dump(data, f)\n```"
    },
    {
        "original": "def FromMany(cls, samples):\n    \"\"\"Constructs a single sample that best represents a list of samples.\n\n    Args:\n      samples: An iterable collection of `CpuSample` instances.\n\n    Returns:\n      A `CpuSample` instance representing `samples`.\n\n    Raises:\n      ValueError: If `samples` is empty.\n    \"\"\"\n    if not samples:\n      raise ValueError(\"Empty `samples` argument\")\n\n    # It only makes sense to average the CPU percentage. For all other values\n    # we simply take the biggest of them.\n    cpu_percent = sum(sample.cpu_percent for sample in samples) / len(samples)\n\n    return CpuSample(\n        timestamp=max(sample.timestamp for sample in samples),\n        cpu_percent=cpu_percent,\n        user_cpu_time=max(sample.user_cpu_time for sample in samples),\n        system_cpu_time=max(sample.system_cpu_time for sample in samples))",
        "sampled": "```python\nimport math\n\nclass CpuSample:\n    def __init__(self, name, value):\n        \"\"\"\n        Args:\n          name: String identifier for the sample.\n          value: Value represented by `usercpu` or `syscpu`.\n        \"\"\"\n        self.name = name\n        self.value = value  # Latex: \\mu + \\sigma\n\n    @classmethod\n    def combined_sample(cls, user_cpu_samples, sys_cpu_samples):\n        if not user_cpu_samples and"
    },
    {
        "original": "def load_api_specific_resource_module(resource_name, api=None):\n    \"\"\"\n    Load the module from the JSON files and return a dict, which might be empty\n    if the resource could not be loaded.\n\n    If no api version is given, the default one from the CONF dict is used.\n\n    :param resource_name: Name of the resource to load\n    :param api: API version\n    :return: dict\n    \"\"\"\n    loader = dict(aosp_permissions=load_permissions,\n                  api_permission_mappings=load_permission_mappings)\n\n    if resource_name not in loader:\n        raise InvalidResourceError(\"Invalid Resource '{}', not in [{}]\".format(resource_name, \", \".join(loader.keys())))\n\n    if not api:\n        api = CONF[\"DEFAULT_API\"]\n\n    ret = loader[resource_name](api)\n\n    if ret == {}:\n        # No API mapping found, return default\n        log.warning(\"API mapping for API level {} was not found! \"\n                    \"Returning default, which is API level {}\".format(api, CONF['DEFAULT_API']))\n        ret = loader[resource_name](CONF['DEFAULT_API'])\n\n    return ret",
        "sampled": "```python\nimport json\nimport os\n\nCONF = {}  # Assuming CONF is a dictionary with the default API version\n\ndef load_api_specific_resource_module(resource_name, api=None):\n    \"\"\"\n    Load the module from the JSON files and return a dict, which might be empty\n    if the resource could not be loaded.\n\n    If no api version is given, the default one from the CONF dict is used.\n\n    :param resource_name: Name of the resource to load\n    :param"
    },
    {
        "original": "async def run_forever(self, relax=0.1, offset=None, timeout=20, allowed_updates=None):\n        \"\"\"\n        Process new updates in infinity loop\n\n        :param relax: float\n        :param offset: int\n        :param timeout: int\n        :param allowed_updates: bool\n        \"\"\"\n        while 1:\n            try:\n                result = await self._bot.getUpdates(offset=offset,\n                                                    timeout=timeout,\n                                                    allowed_updates=allowed_updates)\n\n                # Once passed, this parameter is no longer needed.\n                allowed_updates = None\n\n                # No sort. Trust server to give messages in correct order.\n                for update in result:\n                    self._update_handler(update)\n                    offset = update['update_id'] + 1\n\n            except CancelledError:\n                break\n            except exception.BadHTTPResponse as e:\n                traceback.print_exc()\n\n                # Servers probably down. Wait longer.\n                if e.status == 502:\n                    await asyncio.sleep(30)\n            except:\n                traceback.print_exc()\n                await asyncio.sleep(relax)\n            else:\n                await asyncio.sleep(relax)",
        "sampled": "### Problem Description\n### \n### Process New Updates\n####\n\nWe're given a function definition in Python with an `async` `run_forever` method. We need to find a way to analyze and understand the problem, then provide a Python solution.\n\nHowever, there seems to be missing information about the actual problem description. The provided code snippet appears to be part of a class definition for an object that has methods for processing updates.\n\nTo proceed with solving this challenge, I'll make some assumptions"
    },
    {
        "original": "def huffman_conv2bitstring(cls, s):\n        # type: (str) -> Tuple[int, int]\n        \"\"\" huffman_conv2bitstring converts a string into its bitstring\n        representation. It returns a tuple: the bitstring and its bitlength.\n        This function DOES NOT compress/decompress the string!\n\n        @param str s: the bytestring to convert.\n        @return (int, int): the bitstring of s, and its bitlength.\n        @raise AssertionError\n        \"\"\"\n        i = 0\n        ibl = len(s) * 8\n        for c in s:\n            i = (i << 8) + orb(c)\n\n        ret = i, ibl\n        assert(ret[0] >= 0)\n        assert(ret[1] >= 0)\n        return ret",
        "sampled": "```python\nfrom typing import Tuple\n\nclass HuffmanDecoder:\n    \n    @staticmethod\n    def huffman_conv2bitstring(cls, s: str) -> Tuple[str, int]:\n        \"\"\"\n        Converts a string into its bitstring representation.\n        \n        @param str s: the bytestring to convert.\n        @return (int, int): the bitstring of s, and its bitlength.\n        @raise AssertionError\n        \"\"\"\n        \n        # Initialize an empty binary string\n        binary"
    },
    {
        "original": "def get_analysis(self, current_class):\n        \"\"\"\n        Returns the :class:`~androguard.core.analysis.analysis.Analysis` object\n        which contains the `current_class`.\n\n        :param current_class: The class to search for\n        :type current_class: androguard.core.bytecodes.dvm.ClassDefItem\n        :rtype: androguard.core.analysis.analysis.Analysis\n        \"\"\"\n        for digest in self.analyzed_vms:\n            dx = self.analyzed_vms[digest]\n            if dx.is_class_present(current_class.get_name()):\n                return dx\n        return None",
        "sampled": "```python\nclass Analysis:\n    def __init__(self, classes):\n        self.classes = classes\n\n    def get_analysis(self, current_class):\n        \"\"\"\n        Returns the :class:`~androguard.core.analysis.analysis.Analysis` object\n        which contains the `current_class`.\n\n        :param current_class: The class to search for\n        :type current_class: androguard.core.bytecodes.dvm.ClassDefItem\n        :rtype: androguard.core.analysis.analysis.Analysis\n       "
    },
    {
        "original": "def apply_transformation(self, structure):\n        \"\"\"\n        Returns most primitive cell for structure.\n\n        Args:\n            structure: A structure\n\n        Returns:\n            The same structure in a conventional standard setting\n        \"\"\"\n        sga = SpacegroupAnalyzer(structure, symprec=self.symprec,\n                                 angle_tolerance=self.angle_tolerance)\n        return sga.get_conventional_standard_structure(international_monoclinic=self.international_monoclinic)",
        "sampled": "```python\nclass Transformation:\n    def apply_transformation(self, structure):\n        \"\"\"\n        Returns most primitive cell for structure.\n\n        Args:\n            structure: A quadruple of two 2D arrays representing the cell in standard basis and one integer representing the translation vector, i.e., (A, B, c).\n\n        Returns:\n            The same structure in a conventional standard setting.\n        \"\"\"\n\n        # Assuming the input is a list of three lists where the first two lists are 2D arrays"
    },
    {
        "original": "def get_object(self, cont, obj, local_file=None, return_bin=False):\n        \"\"\"\n        Retrieve a file from Swift\n        \"\"\"\n        try:\n            if local_file is None and return_bin is False:\n                return False\n\n            headers, body = self.conn.get_object(cont, obj, resp_chunk_size=65536)\n\n            if return_bin is True:\n                fp = sys.stdout\n            else:\n                dirpath = dirname(local_file)\n                if dirpath and not isdir(dirpath):\n                    mkdirs(dirpath)\n                fp = salt.utils.files.fopen(local_file, 'wb')  # pylint: disable=resource-leakage\n\n            read_length = 0\n            for chunk in body:\n                read_length += len(chunk)\n                fp.write(chunk)\n            fp.close()\n            return True\n\n        # ClientException\n        # file/dir exceptions\n        except Exception as exc:\n            log.error('There was an error::')\n            if hasattr(exc, 'code') and hasattr(exc, 'msg'):\n                log.error('    Code: %s: %s', exc.code, exc.msg)\n            log.error('    Content: \\n%s', getattr(exc, 'read', lambda: six.text_type(exc))())\n            return False",
        "sampled": "```python\ndef get_object(cont, obj, local_file=None, return_bin=False):\n    \"\"\"\n    Retrieve a file from Swift\n    \"\"\"\n    \n    # Connect to the container\n    # Here we assume 'client' is an instance of a class that has methods to interact with the Swift storage service.\n    client = container_client.Client(cont)\n    \n    # Get the object from the container\n    obj_data = client.get_object(obj)\n\n    \n# Example usage:\ncontainer_name = \"my_container"
    },
    {
        "original": "def new(ext, content_type):\n        \"\"\"\n        Return a new ``<Default>`` element with attributes set to parameter\n        values.\n        \"\"\"\n        xml = '<Default xmlns=\"%s\"/>' % nsmap['ct']\n        default = parse_xml(xml)\n        default.set('Extension', ext)\n        default.set('ContentType', content_type)\n        return default",
        "sampled": "### Problem Description\n \u062e\u062f\u0645\u062a  \n\nTo analyze the problem, we need to read and understand the provided instructions and the `new` function. However, there are no problem descriptions, input/ output specifications or any other information that could lead us to create a solution for some problem based on it.\n\n\n\nIf you provide more information about what \"Some problem\" exactly is (like description, requirements), we might be able to assist better in providing a solution using Python."
    },
    {
        "original": "def _set_conf_sockets():\n    \"\"\"Populate the conf.L2Socket and conf.L3Socket\n    according to the various use_* parameters\n    \"\"\"\n    if conf.use_bpf and not BSD:\n        Interceptor.set_from_hook(conf, \"use_bpf\", False)\n        raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n    if conf.use_winpcapy and not WINDOWS:\n        Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n        raise ScapyInvalidPlatformException(\"Windows only !\")\n    # we are already in an Interceptor hook, use Interceptor.set_from_hook\n    if conf.use_pcap or conf.use_dnet or conf.use_winpcapy:\n        try:\n            from scapy.arch.pcapdnet import L2pcapListenSocket, L2pcapSocket, \\\n                L3pcapSocket\n        except ImportError:\n            warning(\"No pcap provider available ! pcap won't be used\")\n            Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n            Interceptor.set_from_hook(conf, \"use_pcap\", False)\n        else:\n            conf.L3socket = L3pcapSocket\n            conf.L3socket6 = functools.partial(L3pcapSocket, filter=\"ip6\")\n            conf.L2socket = L2pcapSocket\n            conf.L2listen = L2pcapListenSocket\n            return\n    if conf.use_bpf:\n        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\\n            L2bpfSocket, L3bpfSocket\n        conf.L3socket = L3bpfSocket\n        conf.L3socket6 = functools.partial(L3bpfSocket, filter=\"ip6\")\n        conf.L2socket = L2bpfSocket\n        conf.L2listen = L2bpfListenSocket\n        return\n    if LINUX:\n        from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket\n        conf.L3socket = L3PacketSocket\n        conf.L3socket6 = functools.partial(L3PacketSocket, filter=\"ip6\")\n        conf.L2socket = L2Socket\n        conf.L2listen = L2ListenSocket\n        return\n    if WINDOWS:\n        from scapy.arch.windows import _NotAvailableSocket\n        from scapy.arch.windows.native import L3WinSocket, L3WinSocket6\n        conf.L3socket = L3WinSocket\n        conf.L3socket6 = L3WinSocket6\n        conf.L2socket = _NotAvailableSocket\n        conf.L2listen = _NotAvailableSocket\n        return\n    from scapy.supersocket import L3RawSocket\n    from scapy.layers.inet6 import L3RawSocket6\n    conf.L3socket = L3RawSocket\n    conf.L3socket6 = L3RawSocket6",
        "sampled": "### Problem Description\n\nPopulate the conf.L2Socket and conf.L3Socket according to the various use_* parameters.\n\n\n\n### Input Specifications\n\n* Parameters are already set (e.g., `use_l2socket`, `use_l3socket`, etc.)\n\n\n### Output Specifications\n\n* Populate conf.L2Socket and conf.L3Socket accordingly\n\n\n\n```python\ndef _set_conf_sockets():\n    \"\"\"\n    Populate the conf.L2Socket and conf.L3Socket\n    according to the various use"
    },
    {
        "original": "def get_location_posts(self, location: str) -> Iterator[Post]:\n        \"\"\"Get Posts which are listed by Instagram for a given Location.\n\n        :return:  Iterator over Posts of a location's posts\n\n        .. versionadded:: 4.2\n        \"\"\"\n        has_next_page = True\n        end_cursor = None\n        while has_next_page:\n            if end_cursor:\n                params = {'__a': 1, 'max_id': end_cursor}\n            else:\n                params = {'__a': 1}\n            location_data = self.context.get_json('explore/locations/{0}/'.format(location),\n                                                  params)['graphql']['location']['edge_location_to_media']\n            yield from (Post(self.context, edge['node']) for edge in location_data['edges'])\n            has_next_page = location_data['page_info']['has_next_page']\n            end_cursor = location_data['page_info']['end_cursor']",
        "sampled": "```python\nfrom typing import Iterator\nimport json\nimport requests\n\nclass InstagramAPI:\n    def __init__(self, access_token):\n        self.access_token = access_token\n\n    def get_location_posts(self, location: str) -> Iterator[Post]:\n        \"\"\"Get Posts which are listed by Instagram for a given Location.\n\n        :return:  Iterator over Posts of a location's posts\n\n        .. versionadded:: 4.2\n        \"\"\"\n        \n        url = f\"https://graph"
    },
    {
        "original": "def add_peer_parser(subparsers, parent_parser):\n    \"\"\"Adds argument parser for the peer command\n\n        Args:\n            subparsers: Add parsers to this subparser object\n            parent_parser: The parent argparse.ArgumentParser object\n    \"\"\"\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)",
        "sampled": "```python\nimport argparse\n\ndef add_peer_parser(subparsers, parent_parser):\n    \"\"\"\n    Adds argument parser for the peer command\n\n    Args:\n        subparsers: Add parsers to this subparser object\n        parent_parser: The parent argparse.ArgumentParser object\n    \"\"\"\n    \n    # Create a new parser for the 'peer' command\n    peer_parser = subparsers.add_parser('peer', help='Peer related operations',\n                                          parents=[parent_parser])\n    \n    # Add a required argument 'id"
    },
    {
        "original": "def _parse_launch_error(data):\n        \"\"\"\n        Parses a LAUNCH_ERROR message and returns a LaunchFailure object.\n\n        :type data: dict\n        :rtype: LaunchFailure\n        \"\"\"\n        return LaunchFailure(\n            data.get(ERROR_REASON, None),\n            data.get(APP_ID),\n            data.get(REQUEST_ID),\n        )",
        "sampled": "```python\nclass LaunchFailure(Exception):\n    def __init__(self, error_code, error_message):\n        self.error_code = error_code\n        self.error_message = error_message\n        super().__init__(f\"Error Code: {error_code}, Error Message: {error_message}\")\n\n\nclass SystemMessage:\n    def _parse_launch_error(self, data):\n        \"\"\"\n        Parses a LAUNCH_ERROR message and returns a LaunchFailure object.\n\n        :type data: dict\n        :rtype:"
    },
    {
        "original": "def validate_timeout_or_zero(option, value):\n    \"\"\"Validates a timeout specified in milliseconds returning\n    a value in floating point seconds for the case where None is an error\n    and 0 is valid. Setting the timeout to nothing in the URI string is a\n    config error.\n    \"\"\"\n    if value is None:\n        raise ConfigurationError(\"%s cannot be None\" % (option, ))\n    if value == 0 or value == \"0\":\n        return 0\n    return validate_positive_float(option, value) / 1000.0",
        "sampled": "```python\ndef validate_timeout_or_zero(option, value):\n    \"\"\"\n    Validates a timeout specified in milliseconds returning \n    a value in floating point seconds for the case where None is an error \n    and 0 is valid. Setting the timeout to nothing in the URI string is a \n    config error.\n    \n    Args:\n        option (str): The name of the option being validated.\n        value (int or None): The value of the option being validated.\n\n    Returns:\n        float:"
    },
    {
        "original": "def list_storage_services(conn=None, call=None):\n    \"\"\"\n    List VMs on this Azure account, with full information\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            ('The list_storage_services function must be called '\n             'with -f or --function.')\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    ret = {}\n    accounts = conn.list_storage_accounts()\n    for service in accounts.storage_services:\n        ret[service.service_name] = {\n            'capabilities': service.capabilities,\n            'service_name': service.service_name,\n            'storage_service_properties': service.storage_service_properties,\n            'extended_properties': service.extended_properties,\n            'storage_service_keys': service.storage_service_keys,\n            'url': service.url,\n        }\n    return ret",
        "sampled": "```python\ndef list_storage_services(conn=None, call=None):\n    \"\"\"\n    This function lists VMs on the Azure account with full information.\n    \n    Parameters:\n    conn (object): Connection object \n    call (function): Function to make API calls\n    \n    Returns:\n    list: A list of dictionaries containing information about the VMs\n    \"\"\"\n    \n    # Initialize an empty list to store the results\n    results = []\n    \n    # Check if connection and call are provided\n    if"
    },
    {
        "original": "def list_nodes_full(kwargs=None, call=None):\n    \"\"\"\n    All information available about all nodes should be returned in this function.\n    The fields in the list_nodes() function should also be returned,\n    even if they would not normally be provided by the cloud provider.\n\n    This is because some functions both within Salt and 3rd party will break if an expected field is not present.\n    This function is normally called with the -F option:\n\n\n    .. code-block:: bash\n\n        salt-cloud -F\n\n\n    @param kwargs:\n    @type kwargs:\n    @param call:\n    @type call:\n    @return:\n    @rtype:\n    \"\"\"\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called '\n            'with -f or --function.'\n        )\n\n    machines = {}\n\n    # TODO ask for the correct attributes e.g state and private_ips\n    for machine in vb_list_machines():\n        name = machine.get(\"name\")\n        if name:\n            machines[name] = treat_machine_dict(machine)\n            del machine[\"name\"]\n\n    return machines",
        "sampled": "```python\ndef list_nodes_full(kwargs=None, call=None):\n    \"\"\"\n    All information available about all nodes should be returned in this function.\n    The fields in the list_nodes() function should also be returned,\n    even if they would not normally be provided by the cloud provider.\n\n    This is because some functions both within Salt and 3rd party will break if an expected field is not present.\n    This function is normally called with the -F option:\n    \n        salt-cloud -F\n\n"
    },
    {
        "original": "async def send_code_request(self, phone, *, force_sms=False):\n        \"\"\"\n        Sends a code request to the specified phone number.\n\n        Args:\n            phone (`str` | `int`):\n                The phone to which the code will be sent.\n\n            force_sms (`bool`, optional):\n                Whether to force sending as SMS.\n\n        Returns:\n            An instance of :tl:`SentCode`.\n        \"\"\"\n        phone = utils.parse_phone(phone) or self._phone\n        phone_hash = self._phone_code_hash.get(phone)\n\n        if not phone_hash:\n            try:\n                result = await self(functions.auth.SendCodeRequest(\n                    phone, self.api_id, self.api_hash, types.CodeSettings()))\n            except errors.AuthRestartError:\n                return self.send_code_request(phone, force_sms=force_sms)\n\n            self._tos = result.terms_of_service\n            self._phone_code_hash[phone] = phone_hash = result.phone_code_hash\n        else:\n            force_sms = True\n\n        self._phone = phone\n\n        if force_sms:\n            result = await self(\n                functions.auth.ResendCodeRequest(phone, phone_hash))\n\n            self._phone_code_hash[phone] = result.phone_code_hash\n\n        return result",
        "sampled": "```python\nimport random\n\nclass SentCode:\n    def __init__(self, code):\n        self.code = code\n\nclass CodeRequestSender:\n    def __init__(self):\n        self.codes = {}\n\n    async def send_code_request(self, phone, *, force_sms=False):\n        \"\"\"\n        Sends a code request to the specified phone number.\n\n        Args:\n            phone (`str` | `int`):\n                The phone to which the code will be sent.\n\n            force_sms (`bool`,"
    },
    {
        "original": "def _set_flow_entry(self, datapath, actions, in_port, dst, src=None):\n        \"\"\"set a flow entry.\"\"\"\n        set_flow = self._set_flow_func.get(datapath.ofproto.OFP_VERSION)\n        assert set_flow\n        set_flow(datapath, actions, in_port, dst, src)",
        "sampled": "```python\nclass NetworkController:\n    def _set_flow_entry(self, datapath, actions, in_port, dst, src=None):\n        \"\"\"\n        Set a flow entry.\n\n        Args:\n            datapath (str): The data path.\n            actions (list): A list of actions to be taken when the flow entry is matched.\n            in_port (int): The input port number.\n            dst (str): The destination IP address.\n            src (str, optional): The source IP address"
    },
    {
        "original": "async def start(self):\n        \"\"\"\n        Start discarding media.\n        \"\"\"\n        for track, task in self.__tracks.items():\n            if task is None:\n                self.__tracks[track] = asyncio.ensure_future(blackhole_consume(track))",
        "sampled": "Since the problem description is not provided, I'll create a sample problem for demonstration purposes. Here's a sample problem in Markdown format:\n\n### Problem Description\nGiven an integer `n`, calculate the sum of all numbers from 1 to `n` (inclusive) using the formula: $\\frac{n(n+1)}{2}$.\n\n### Input and Output Specifications\n#### Input:\n- An integer `n`\n\n#### Output:\n- The sum of all numbers from 1 to `n` ("
    },
    {
        "original": "def set_xlabels(self, label=None, **kwargs):\n        \"\"\"Label the x axis on the bottom row of the grid.\"\"\"\n        if label is None:\n            label = label_from_attrs(self.data[self._x_var])\n        for ax in self._bottom_axes:\n            ax.set_xlabel(label, **kwargs)\n        return self",
        "sampled": "```python\ndef set_xlabels(label=None, **kwargs):\n    \"\"\"\n    Label the x axis on the bottom row of the grid.\n    \n    Parameters:\n    label (str or list, optional): The x tick labels. Defaults to None.\n    **kwargs: Additional keyword arguments for matplotlib's set_xticklabels function.\n    \n    Returns:\n    matplotlib.axes.AxesLabelList: The x tick labels object.\n    \"\"\"\n\nimport matplotlib.pyplot as plt\n\ndef solve_problem():\n\u00a0\u00a0\u00a0\u00a0fig,"
    },
    {
        "original": "def blockvisit(self, nodes, frame):\n        \"\"\"Visit a list of nodes as block in a frame.  If the current frame\n        is no buffer a dummy ``if 0: yield None`` is written automatically\n        unless the force_generator parameter is set to False.\n        \"\"\"\n        if frame.buffer is None:\n            self.writeline('if 0: yield None')\n        else:\n            self.writeline('pass')\n        try:\n            for node in nodes:\n                self.visit(node, frame)\n        except CompilerExit:\n            pass",
        "sampled": "```python\nclass Generator:\n    def __init__(self, force_generator=False):\n        self.force_generator = force_generator\n\n    def blockvisit(self, nodes, frame):\n        \"\"\"\n        Visit a list of nodes as block in a frame.  \n        If the current frame is no buffer a dummy ``if 0: yield None`` is written automatically\n        unless the force_generator parameter is set to False.\n        \n        Parameters:\n            nodes (list): A list of nodes to visit.\n           "
    },
    {
        "original": "def update(self, rotate_worker_token=False, rotate_manager_token=False,\n               **kwargs):\n        \"\"\"\n        Update the swarm's configuration.\n\n        It takes the same arguments as :py:meth:`init`, except\n        ``advertise_addr``, ``listen_addr``, and ``force_new_cluster``. In\n        addition, it takes these arguments:\n\n        Args:\n            rotate_worker_token (bool): Rotate the worker join token. Default:\n                ``False``.\n            rotate_manager_token (bool): Rotate the manager join token.\n                Default: ``False``.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n\n        \"\"\"\n        # this seems to have to be set\n        if kwargs.get('node_cert_expiry') is None:\n            kwargs['node_cert_expiry'] = 7776000000000000\n\n        return self.client.api.update_swarm(\n            version=self.version,\n            swarm_spec=self.client.api.create_swarm_spec(**kwargs),\n            rotate_worker_token=rotate_worker_token,\n            rotate_manager_token=rotate_manager_token\n        )",
        "sampled": "### Problem Analysis\n\nFrom the given problem description, it appears that the `update` function is used to update the swarm's configuration. The function has two additional optional parameters: `rotate_worker_token` and `rotate_manager_token`, which are used to rotate the worker join token and manager join token respectively.\n\nThe function takes in keyword arguments (`**kwargs`) which can be used to pass any additional configuration settings. However, there are some restrictions on what can be updated - specifically, `advertise_addr"
    },
    {
        "original": "def image_to_osd(image,\n                 lang='osd',\n                 config='',\n                 nice=0,\n                 output_type=Output.STRING):\n    \"\"\"\n    Returns string containing the orientation and script detection (OSD)\n    \"\"\"\n    config = '{}-psm 0 {}'.format(\n        '' if get_tesseract_version() < '3.05' else '-',\n        config.strip()\n    ).strip()\n    args = [image, 'osd', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()",
        "sampled": "### Problem Description\nNo problem description provided, only the function signature with a comment.\n```python\nimport math\n\nclass Output:\n    STRING = \"STRING\"\n    IMAGE = \"IMAGE\"\n\ndef image_to_osd(image,\n                 lang='osd',\n                 config='',\n                 nice=0,\n                 output_type=Output.STRING):\n    \"\"\"\n    Returns string containing the orientation and script detection (OSD)\n    \"\"\"\n\n# Create an implementation of the image_to_osd function\ndef resolve_language(lang):\n"
    },
    {
        "original": "def show(self):\n        \"\"\"\n        Prints the content of this method to stdout.\n\n        This will print the method signature and the decompiled code.\n        \"\"\"\n        args, ret = self.method.get_descriptor()[1:].split(\")\")\n        if self.code:\n            # We patch the descriptor here and add the registers, if code is available\n            args = args.split(\" \")\n\n            reg_len = self.code.get_registers_size()\n            nb_args = len(args)\n\n            start_reg = reg_len - nb_args\n            args = [\"{} v{}\".format(a, start_reg + i) for i, a in enumerate(args)]\n\n        print(\"METHOD {} {} {} ({}){}\".format(\n              self.method.get_class_name(),\n              self.method.get_access_flags_string(),\n              self.method.get_name(),\n              \", \".join(args), ret))\n        bytecode.PrettyShow(self, self.basic_blocks.gets(), self.method.notes)",
        "sampled": "### Problem Description\n### ==================\n### \n### You are given a string `s` consisting of lowercase English letters. Your task is to find the length of the longest substring without repeated characters.\n### \n### A substring is a contiguous sequence of characters within a string.\n### \n### Example 1:\n```markdown\nInput: s = \"abcabcbb\"\nOutput: 3\nExplanation: The answer is \"abc\", with the length of 3.\n```\nExample 2:\n```"
    },
    {
        "original": "def BuildDefaultValue(self, value_cls):\n    \"\"\"Renders default value of a given class.\n\n    Args:\n      value_cls: Default value of this class will be rendered. This class has to\n        be (or to be a subclass of) a self.value_class (i.e. a class that this\n        renderer is capable of rendering).\n\n    Returns:\n      An initialized default value.\n\n    Raises:\n      DefaultValueError: if something goes wrong.\n    \"\"\"\n    try:\n      return value_cls()\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(e)\n      raise DefaultValueError(\n          \"Can't create default for value %s: %s\" % (value_cls.__name__, e))",
        "sampled": "```\nclass DefaultValueError(Exception):\n    pass\n\nclass Renderer:\n    def __init__(self, value_class):\n        self.value_class = value_class\n\n    def BuildDefaultValue(self, value_cls):\n        \"\"\"Renders default value of a given class.\n\n        Args:\n          value_cls: Default value of this class will be rendered. This class has to\n            be (or to be a subclass of) a self.value_class (i.e. a class that this\n            renderer is capable of"
    },
    {
        "original": "def add_picture(self, image_path_or_stream, width=None, height=None):\n        \"\"\"\n        Return a new picture shape added in its own paragraph at the end of\n        the document. The picture contains the image at\n        *image_path_or_stream*, scaled based on *width* and *height*. If\n        neither width nor height is specified, the picture appears at its\n        native size. If only one is specified, it is used to compute\n        a scaling factor that is then applied to the unspecified dimension,\n        preserving the aspect ratio of the image. The native size of the\n        picture is calculated using the dots-per-inch (dpi) value specified\n        in the image file, defaulting to 72 dpi if no value is specified, as\n        is often the case.\n        \"\"\"\n        run = self.add_paragraph().add_run()\n        return run.add_picture(image_path_or_stream, width, height)",
        "sampled": "```python\nfrom PIL import Image\nimport io\n\nclass Picture:\n    def add_picture(self, image_path_or_stream, width=None, height=None):\n        \"\"\"\n        Return a new picture shape added in its own paragraph at the end of\n        the document. The picture contains the image at\n        *image_path_or_stream*, scaled based on *width* and *height*. If\n        neither width nor height is specified, the picture appears at its\n        native size. If only one"
    },
    {
        "original": "async def process_request(self, path, headers):\n        \"\"\"\n        This hook is called to determine if the websocket should return\n        an HTTP response and close.\n\n        Our behavior here is to start the ASGI application, and then wait\n        for either `accept` or `close` in order to determine if we should\n        close the connection.\n        \"\"\"\n        path_portion, _, query_string = path.partition(\"?\")\n\n        websockets.handshake.check_request(headers)\n\n        subprotocols = []\n        for header in headers.get_all(\"Sec-WebSocket-Protocol\"):\n            subprotocols.extend([token.strip() for token in header.split(\",\")])\n\n        asgi_headers = [\n            (name.encode(\"ascii\"), value.encode(\"ascii\"))\n            for name, value in headers.raw_items()\n        ]\n\n        self.scope = {\n            \"type\": \"websocket\",\n            \"scheme\": self.scheme,\n            \"server\": self.server,\n            \"client\": self.client,\n            \"root_path\": self.root_path,\n            \"path\": unquote(path_portion),\n            \"query_string\": query_string.encode(\"ascii\"),\n            \"headers\": asgi_headers,\n            \"subprotocols\": subprotocols,\n        }\n        task = self.loop.create_task(self.run_asgi())\n        task.add_done_callback(self.on_task_complete)\n        self.tasks.add(task)\n        await self.handshake_started_event.wait()\n        return self.initial_response",
        "sampled": "```python\nimport asyncio\nfrom asgi_tools import App, Response\n\nclass WebsocketHandler:\n    async def process_request(self, path, headers):\n        app = App()  # create a new ASGI application instance\n\n        # run the ASGI application and wait for either accept or close event\n        await app(scope={\"type\": \"http\"}, receive=asyncio.UPSTREAM_CLOSED) or \\\n               await app(scope={\"type\": \"websocket\"}, receive=asyncio.WEBSOCKET"
    },
    {
        "original": "def queue_instances(instances):\n    \"\"\"\n    Queue a set of instances to be provisioned later. Expects a list.\n\n    Currently this only queries node data, and then places it in the cloud\n    cache (if configured). If the salt-cloud-reactor is being used, these\n    instances will be automatically provisioned using that.\n\n    For more information about the salt-cloud-reactor, see:\n\n    https://github.com/saltstack-formulas/salt-cloud-reactor\n    \"\"\"\n    for instance_id in instances:\n        node = _get_node(instance_id=instance_id)\n        __utils__['cloud.cache_node'](node, __active_provider_name__, __opts__)",
        "sampled": "```python\ndef queue_instances(instances):\n    \"\"\"\n    Queue a set of instances to be provisioned later. Expects a list.\n    \n    Currently this only queries node data, and then places it in the cloud\n    cache (if configured). If the salt-cloud-reactor is being used, these\n    instances will be automatically provisioned using that.\n    \n    For more information about the salt-cloud-reactor, see:\n    \n    https://github.com/saltstack-formulas/salt"
    },
    {
        "original": "def _validate_depedencies(batches):\n    \"\"\"Validates the transaction dependencies for the transactions contained\n    within the sequence of batches. Given that all the batches are expected to\n    to be executed for the genesis blocks, it is assumed that any dependent\n    transaction will proceed the depending transaction.\n    \"\"\"\n    transaction_ids = set()\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n\n            if txn_header.dependencies:\n                unsatisfied_deps = [\n                    id for id in txn_header.dependencies\n                    if id not in transaction_ids\n                ]\n                if unsatisfied_deps:\n                    raise CliException(\n                        'Unsatisfied dependency in given transactions:'\n                        ' {}'.format(unsatisfied_deps))\n\n            transaction_ids.add(txn.header_signature)",
        "sampled": "### Problem Description\n#### Transaction Dependencies Validation\n\nGiven a sequence of batches, validate the transaction dependencies for the transactions contained within each batch.\n\n### Input and Output Specifications\n#### Input\n\n* `batches`: A list of batches where each batch is a list of transactions.\n* Each transaction is represented as a dictionary with the following keys:\n\t+ `id`: A unique identifier for the transaction.\n\t+ `depends_on`: A list of transaction IDs that this transaction depends on.\n\n#### Output\n\n*"
    },
    {
        "original": "def validate(self, body, signature):\n        \"\"\"Check signature.\n\n        https://devdocs.line.me/en/#webhook-authentication\n\n        :param str body: Request body (as text)\n        :param str signature: X-Line-Signature value (as text)\n        :rtype: bool\n        :return: result\n        \"\"\"\n        gen_signature = hmac.new(\n            self.channel_secret,\n            body.encode('utf-8'),\n            hashlib.sha256\n        ).digest()\n\n        return compare_digest(\n                signature.encode('utf-8'), base64.b64encode(gen_signature)\n        )",
        "sampled": "              Problem Description\n=====================\n\nVerify the signature of an incoming webhook request. \n\nThe signature is calculated using a secret token and the webhook request body and headers. \nWe need to validate that the provided signature matches this expected value.\n\nInput\n-----\n\n### Request Body\n\n*   A string containing the raw JSON data for the incoming webhook event.\n*   For example: `{\"type\": \"message\",\"source\": {\"type\": \"user\",\"userId\": \"Uxxxxxxxx7890a1\"}}`\n\n"
    },
    {
        "original": "def file_local_list(self, saltenv='base'):\n        \"\"\"\n        List files in the local minion files and localfiles caches\n        \"\"\"\n        filesdest = os.path.join(self.opts['cachedir'], 'files', saltenv)\n        localfilesdest = os.path.join(self.opts['cachedir'], 'localfiles')\n\n        fdest = self._file_local_list(filesdest)\n        ldest = self._file_local_list(localfilesdest)\n        return sorted(fdest.union(ldest))",
        "sampled": "### Problem Analysis\nThe given function is intended to retrieve a list of files from the local minion files and localfiles caches. \n\nHowever, the original code lacks functionality as it does not actually interact with the file cache directories. To fulfill this task, we need to use the salt.utils.filesave module to copy minion files from their respective environments into a temporary directory.\n\n### Python Solution\n\n```python\nimport os\nimport shutil\nimport tempfile\n\ndef file_local_list(self, saltenv='base"
    },
    {
        "original": "def CreateDataset(self):\n    \"\"\"Create a dataset.\"\"\"\n    body = {\n        \"datasetReference\": {\n            \"datasetId\": self.dataset_id,\n            \"description\": \"Data exported from GRR\",\n            \"friendlyName\": \"GRRExportData\",\n            \"projectId\": self.project_id\n        }\n    }\n    result = self.service.datasets().insert(\n        projectId=self.project_id, body=body).execute()\n    self.datasets[self.dataset_id] = result\n    return result",
        "sampled": "```markdown\n**Problem Description**\n================ \u2588\u2588\u2588\u2588\u2588\u2588\nGiven a time of day in the format HH:MM to HH:MM, calculate the duration between two given times.\n\nInput Specifications:\n--------------------\n\n*   Two strings of time in the format `HH:MM` and `HH:MM`\n*   The two times are separated by spaces\n\nOutputSpecifications:\n----------------------\n\n*   A single integer representing minutes\n\n**example**\n-----------\n\n-Input\n7:30 10:00 \n\n-Output"
    },
    {
        "original": "def get_pr_review_status(pr: PullRequestDetails) -> Any:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/pulls/reviews/#list-reviews-on-a-pull-request\n    \"\"\"\n    url = (\"https://api.github.com/repos/{}/{}/pulls/{}/reviews\"\n           \"?access_token={}\".format(pr.repo.organization,\n                                     pr.repo.name,\n                                     pr.pull_id,\n                                     pr.repo.access_token))\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise RuntimeError(\n            'Get review failed. Code: {}. Content: {}.'.format(\n                response.status_code, response.content))\n\n    return json.JSONDecoder().decode(response.content.decode())",
        "sampled": "### Problem Description\nGet the status of a pull request review on GitHub.\n\n### Input and Output Specifications\n#### Input:\n- `pr`: A `PullRequestDetails` object containing information about the pull request.\n\n#### Output:\n- The status of the pull request review, which can be one of the following:\n  - `PENDING`: The review is pending.\n  - `APPROVED`: The review is approved.\n  - `CHANGES_REQUESTED`: The reviewer has requested changes.\n "
    },
    {
        "original": "def add_edge_lengths(G):\n    \"\"\"\n    Add length (meters) attribute to each edge by great circle distance between\n    nodes u and v.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    G : networkx multidigraph\n    \"\"\"\n\n    start_time = time.time()\n\n    # first load all the edges' origin and destination coordinates as a\n    # dataframe indexed by u, v, key\n    coords = np.array([[u, v, k, G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n    df_coords = pd.DataFrame(coords, columns=['u', 'v', 'k', 'u_y', 'u_x', 'v_y', 'v_x'])\n    df_coords[['u', 'v', 'k']] = df_coords[['u', 'v', 'k']].astype(np.int64)\n    df_coords = df_coords.set_index(['u', 'v', 'k'])\n\n    # then calculate the great circle distance with the vectorized function\n    gc_distances = great_circle_vec(lat1=df_coords['u_y'],\n                                    lng1=df_coords['u_x'],\n                                    lat2=df_coords['v_y'],\n                                    lng2=df_coords['v_x'])\n\n    # fill nulls with zeros and round to the millimeter\n    gc_distances = gc_distances.fillna(value=0).round(3)\n    nx.set_edge_attributes(G, name='length', values=gc_distances.to_dict())\n\n    log('Added edge lengths to graph in {:,.2f} seconds'.format(time.time()-start_time))\n    return G",
        "sampled": "```python\nimport networkx as nx\nimport itertools\n\ndef add_edge_lengths(G):\n    \"\"\"\n    Add length (meters) attribute to each edge by great circle distance between\n    nodes u and v.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n\n    Returns\n    -------\n    G : networkx multidigraph\n    \"\"\"\n\n    # Get the latitude and longitude coordinates of all nodes from the graph attributes if available,\n\t# or use a default value of 0."
    },
    {
        "original": "def ProcessHuntFlowDone(flow_obj, status_msg=None):\n  \"\"\"Notifis hunt about a given hunt-induced flow completion.\"\"\"\n\n  if not hunt.IsLegacyHunt(flow_obj.parent_hunt_id):\n    hunt_obj = hunt.StopHuntIfCPUOrNetworkLimitsExceeded(\n        flow_obj.parent_hunt_id)\n    hunt.CompleteHuntIfExpirationTimeReached(hunt_obj)\n    return\n\n  hunt_urn = rdfvalue.RDFURN(\"hunts\").Add(flow_obj.parent_hunt_id)\n  client_urn = rdf_client.ClientURN(flow_obj.client_id)\n\n  # Update the counter metrics separately from collections to minimize\n  # contention.\n  with aff4.FACTORY.Open(hunt_urn, mode=\"rw\") as fd:\n    # Legacy AFF4 code expects token to be set.\n    fd.token = access_control.ACLToken(username=fd.creator)\n\n    if flow_obj.num_replies_sent:\n      fd.context.clients_with_results_count += 1\n\n    fd.context.completed_clients_count += 1\n    fd.context.results_count += flow_obj.num_replies_sent\n\n    fd.GetRunner().SaveResourceUsage(flow_obj.client_id, status_msg)\n\n  with aff4.FACTORY.Open(hunt_urn, mode=\"rw\") as fd:\n    # Legacy AFF4 code expects token to be set.\n    fd.token = access_control.ACLToken(username=fd.creator)\n\n    fd.RegisterCompletedClient(client_urn)\n    if flow_obj.num_replies_sent:\n      fd.RegisterClientWithResults(client_urn)\n\n    fd.StopHuntIfAverageLimitsExceeded()",
        "sampled": "```python\ndef ProcessHuntFlowDone(flow_obj, status_msg=None):\n    \"\"\"Notifies hunt about a given hunt-induced flow completion.\"\"\"\n    \n    # Push notification to hunt - assuming we have a HuntNotification class for this\n    from app.models import HuntNotification\n    \n    # Create the notification object with the necessary data\n    notification = HuntNotification(\n        flow_id=flow_obj.id,\n        status=status_msg or \"Flow completed successfully\"\n    )\n    \n    # Save the notification to the"
    },
    {
        "original": "def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n        try:\n            data = requests.get(\"https://pypi.python.org/pypi/jira/json\", timeout=2.001).json()\n\n            released_version = data['info']['version']\n            if parse_version(released_version) > parse_version(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n        except requests.RequestException:\n            pass\n        except Exception as e:\n            logging.warning(e)",
        "sampled": "```python\nclass VersionChecker:\n    def __init__(self, current_version, latest_version):\n        self.current_version = current_version\n        self.latest_version = latest_version\n\n    def _check_update_(self):\n        \"\"\"\n        Check if the current version of the library is outdated.\n        \n        The function splits each version into its numerical parts and \n        checks if the current version is less than the latest version. \n \t\t\n \t\tParameters:\n            ----------\n            None\n        \n \t\tReturns:\n            -------\n"
    },
    {
        "original": "def reifyWidget(self, parent, item):\r\n        \"\"\" Convert a JSON description of a widget into a WxObject \"\"\"\r\n        from gooey.gui.components import widgets\r\n        widgetClass = getattr(widgets, item['type'])\r\n        return widgetClass(parent, item)",
        "sampled": "```python\nimport json\nimport wx\n\nclass WxApp(wx.App):\n    \"\"\" A simple wxPython application \"\"\"\n\n    def reifyWidget(self, parent, item):\n        \"\"\"\n        Convert a JSON description of a widget into a WxObject\n        :param parent: Parent object for the widget\n        :param item: JSON description of the widget\n        :return: The corresponding WxObject instance\n        \"\"\"\n        \n        # Define dictionary to map JSON properties to their corresponding wxWidgets"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        Json-serializable dict representation.\n        \"\"\"\n        d = MSONable.as_dict(self)\n        d[\"translation_vector\"] = self.translation_vector.tolist()\n        return d",
        "sampled": "```python\nclass Solution:\n    def as_dict(self):\n        \"\"\"\n        Json-serializable dict representation.\n        \"\"\"\n        \n    def __init__(self, n: int, k: int, max_sum: int):\n        self.n = n\n        self.k = k\n        self.max_sum = max_sum\n\n    def subarraySum(self) -> List[int]:\n        # Initialize a hashmap to store the prefix sum and its frequency\n        prefix_sum_freq = {0: 1"
    },
    {
        "original": "def library_line(self, file_name):\n        \"\"\"\n        Specifies GULP library file to read species and potential parameters.\n        If using library don't specify species and potential\n        in the input file and vice versa. Make sure the elements of\n        structure are in the library file.\n\n        Args:\n            file_name: Name of GULP library file\n\n        Returns:\n            GULP input string specifying library option\n        \"\"\"\n        gulplib_set = lambda: 'GULP_LIB' in os.environ.keys()\n        readable = lambda f: os.path.isfile(f) and os.access(f, os.R_OK)\n\n        #dirpath, fname = os.path.split(file_name)\n        #if dirpath:  # Full path specified\n        #    if readable(file_name):\n        #        gin = 'library ' + file_name\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #else:\n        #    fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n        #    if readable(fpath):\n        #        gin = 'library ' + fpath\n        #    elif gulplib_set():\n        #        fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n        #        if readable(fpath):\n        #            gin = 'library ' + file_name\n        #        else:\n        #            raise GulpError('GULP Library not found')\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #gin += \"\\n\"\n        #return gin\n\n        gin = \"\"\n        dirpath, fname = os.path.split(file_name)\n        if dirpath and readable(file_name):  # Full path specified\n            gin = 'library ' + file_name\n        else:\n            fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n            if readable(fpath):\n                gin = 'library ' + fpath\n            elif gulplib_set():         # Check the GULP_LIB path\n                fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n                if readable(fpath):\n                    gin = 'library ' + file_name\n        if gin:\n            return gin + \"\\n\"\n        else:\n            raise GulpError('GULP Library not found')",
        "sampled": "```python\nclass GULP:\n    def library_line(self, file_name):\n        \"\"\"\n        Specifies GULP library file to read species and potential parameters.\n        \n        Args:\n            file_name: Name of GULP library file\n\n        Returns:\n            GULP input string specifying library option\n        \"\"\"\n        \n        # Check if the input is a string\n        if not isinstance(file_name, str):\n            raise TypeError(\"File name must be a string.\")\n        \n        # Return the"
    },
    {
        "original": "def IsCloud(self, request, bios_version, services):\n    \"\"\"Test to see if we're on a cloud machine.\"\"\"\n    if request.bios_version_regex and bios_version:\n      if re.match(request.bios_version_regex, bios_version):\n        return True\n    if request.service_name_regex and services:\n      if re.search(request.service_name_regex, services):\n        return True\n    return False",
        "sampled": "```python\nclass Solution:\n    def IsCloud(self, request, bios_version, services):\n        \"\"\"\n        Test to see if we're on a cloud machine.\n        \n        Parameters:\n        request (str): The incoming HTTP request.\n        bios_version (str): The version of the BIOS.\n        services (list): A list of available services.\n\n        Returns:\n        bool: True if we're on a cloud machine, False otherwise.\n        \"\"\"\n\n    # Check if the BIOS version is 'cloud"
    },
    {
        "original": "def get_local_hypervisor(self):\n        \"\"\"\n        Returns the hypervisor running on this host, and assumes a 1-1 between host and hypervisor\n        \"\"\"\n        # Look up hypervisors available filtered by my hostname\n        host = self.get_my_hostname()\n        hyp = self.get_all_hypervisor_ids(filter_by_host=host)\n        if hyp:\n            return hyp[0]",
        "sampled": "```python\nclass Host:\n    def __init__(self, hypervisor):\n        \"\"\"\n        Initialize a host with its corresponding hypervisor.\n        \n        Args:\n            hypervisor (str): The name of the hypervisor running on this host.\n        \"\"\"\n        self.hypervisor = hypervisor\n\n    def get_local_hypervisor(self):\n        \"\"\"\n        Returns the hypervisor running on this host, and assumes a 1-1 between host and hypervisor\n        \"\"\"\n        \n# Example usage"
    },
    {
        "original": "def get(self, file_path, ref, **kwargs):\n        \"\"\"Retrieve a single file.\n\n        Args:\n            file_path (str): Path of the file to retrieve\n            ref (str): Name of the branch, tag or commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            object: The generated RESTObject\n        \"\"\"\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)",
        "sampled": "### Problem Description\nRetrieve a single file from a GitLab repository.\n\n### Input and Output Specifications\n- **Input**:\n  - `file_path`: Path of the file to retrieve (str)\n  - `ref`: Name of the branch, tag or commit (str)\n  - `**kwargs`: Extra options to send to the server (e.g. sudo)\n\n- **Output**:\n  - The generated RESTObject\n\n### Python Solution\n\n```python\nclass GitlabAPI:\n   "
    },
    {
        "original": "def from_values_indices(cls, values, indices, populate=False,\n                            structure=None, voigt_rank=None,\n                            vsym=True, verbose=False):\n        \"\"\"\n        Creates a tensor from values and indices, with options\n        for populating the remainder of the tensor.\n\n        Args:\n            values (floats): numbers to place at indices\n            indices (array-likes): indices to place values at\n            populate (bool): whether to populate the tensor\n            structure (Structure): structure to base population\n                or fit_to_structure on\n            voigt_rank (int): full tensor rank to indicate the\n                shape of the resulting tensor.  This is necessary\n                if one provides a set of indices more minimal than\n                the shape of the tensor they want, e.g.\n                Tensor.from_values_indices((0, 0), 100)\n            vsym (bool): whether to voigt symmetrize during the\n                optimization procedure\n            verbose (bool): whether to populate verbosely\n        \"\"\"\n        # auto-detect voigt notation\n        # TODO: refactor rank inheritance to make this easier\n        indices = np.array(indices)\n        if voigt_rank:\n            shape = ([3]*(voigt_rank % 2) + [6]*(voigt_rank // 2))\n        else:\n            shape = np.ceil(np.max(indices+1, axis=0) / 3.) * 3\n        base = np.zeros(shape.astype(int))\n        for v, idx in zip(values, indices):\n            base[tuple(idx)] = v\n        if 6 in shape:\n            obj = cls.from_voigt(base)\n        else:\n            obj = cls(base)\n        if populate:\n            assert structure, \"Populate option must include structure input\"\n            obj = obj.populate(structure, vsym=vsym, verbose=verbose)\n        elif structure:\n            obj = obj.fit_to_structure(structure)\n        return obj",
        "sampled": "```python\nimport numpy as np\n\nclass Structure:\n    def __init__(self, size):\n        self.size = size\n\nclass Tensor:\n    def from_values_indices(cls, values, indices, populate=False,\n                            structure=None, voigt_rank=None,\n                            vsym=True, verbose=False):\n        # Check if the number of values matches the number of indices\n        if len(values) != len(indices):\n            raise ValueError(\"Number of values does not match the number of indices\")\n\n        #"
    },
    {
        "original": "def _raise_redirect_exceptions(response):\n    \"\"\"Return the new url or None if there are no redirects.\n\n    Raise exceptions if appropriate.\n\n    \"\"\"\n    if response.status_code not in [301, 302, 307]:\n        return None\n    new_url = urljoin(response.url, response.headers['location'])\n    if 'reddits/search' in new_url:  # Handle non-existent subreddit\n        subreddit = new_url.rsplit('=', 1)[1]\n        raise InvalidSubreddit('`{0}` is not a valid subreddit'\n                               .format(subreddit))\n    elif not RE_REDIRECT.search(response.url):\n        raise RedirectException(response.url, new_url)\n    return new_url",
        "sampled": "```python\ndef _raise_redirect_exceptions(response):\n    \"\"\"\n    Return the new url or None if there are no redirects.\n\n    Raise exceptions if appropriate.\n    \"\"\"\n    \n    # Check if the response is a redirect\n    if response.status_code == 301 or response.status_code == 302:\n        # Get the new URL from the Location header\n        new_url = response.headers.get('Location')\n        \n        # If there's no new URL, return None\n        if not new_url:\n"
    },
    {
        "original": "def _future_done_callback(self, request, result):\n        \"\"\"\n        :param request (bytes):the serialized request\n        :param result (FutureResult):\n        \"\"\"\n        self._in_process_transactions_count.dec()\n        req = processor_pb2.TpProcessRequest()\n        req.ParseFromString(request)\n        response = processor_pb2.TpProcessResponse()\n        response.ParseFromString(result.content)\n\n        processor_type = ProcessorType(\n            req.header.family_name,\n            req.header.family_version)\n\n        self._processor_manager[processor_type].get_processor(\n            result.connection_id).dec_occupancy()\n        self._processor_manager.notify()\n\n        self._get_tp_process_response_counter(\n            response.Status.Name(response.status)).inc()\n\n        if result.connection_id in self._open_futures and \\\n                req.signature in self._open_futures[result.connection_id]:\n            del self._open_futures[result.connection_id][req.signature]\n\n        if response.status == processor_pb2.TpProcessResponse.OK:\n            state_sets, state_deletes, events, data = \\\n                self._context_manager.get_execution_results(req.context_id)\n\n            state_changes = [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    value=value,\n                    type=transaction_receipt_pb2.StateChange.SET)\n                for addr, value in state_sets.items()\n            ] + [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    type=transaction_receipt_pb2.StateChange.DELETE)\n                for addr in state_deletes\n            ]\n\n            self._scheduler.set_transaction_execution_result(\n                txn_signature=req.signature,\n                is_valid=True,\n                context_id=req.context_id,\n                state_changes=state_changes,\n                events=events,\n                data=data)\n\n        elif response.status == processor_pb2.TpProcessResponse.INTERNAL_ERROR:\n            LOGGER.error(\n                \"Transaction processor internal error: %s \"\n                \"(transaction: %s, name: %s, version: %s)\",\n                response.message,\n                req.signature,\n                req.header.family_name,\n                req.header.family_version)\n\n            # Make sure that the transaction wasn't unscheduled in the interim\n            if self._scheduler.is_transaction_in_schedule(req.signature):\n                self._execute(\n                    processor_type=processor_type,\n                    content=request,\n                    signature=req.signature)\n\n        else:\n            self._context_manager.delete_contexts(\n                context_id_list=[req.context_id])\n\n            self._fail_transaction(\n                txn_signature=req.signature,\n                context_id=req.context_id,\n                error_message=response.message,\n                error_data=response.extended_data)",
        "sampled": "```python\nimport asyncio\n\nclass FutureResult:\n    pass\n\nclass MyFuture:\n    def __init__(self):\n        self._done_callback = None\n        self._result = None\n\n    async def done_callback(self, callback):\n        if self._done_callback is not None:\n            raise Exception(\"Callback already set\")\n        \n        self._done_callback = callback\n        await asyncio.sleep(0)\n        \n    async def set_result(self, result):\n        if self._done_callback is None"
    },
    {
        "original": "def validation_error_inform_error(err, data, schema):\n    \"\"\"\n    Custom validation error handler which produces 404 Bad Request\n    response in case validation fails and returns the error\n    \"\"\"\n    abort(Response(\n        json.dumps({'error': str(err), 'data': data, 'schema': schema}),\n        status=HTTPStatus.BAD_REQUEST))",
        "sampled": "### Problem Description\n#### Custom Validation Error Handler\n\nWe need to create a custom validation error handler that produces a 404 Bad Request response in case of validation failure and returns the error.\n\n### Input and Output Specifications\n#### Input\n\n* `err`: The error message\n* `data`: The input data being validated\n* `schema`: The validation schema\n\n#### Output\n\n* A 404 Bad Request response with the error message in the body\n\n### Python Solution\n```python\nfrom fastapi"
    },
    {
        "original": "def IterateAllClientSnapshots(self, min_last_ping=None, batch_size=50000):\n    \"\"\"Iterates over all available clients and yields client snapshot objects.\n\n    Args:\n      min_last_ping: If provided, only snapshots for clients with last-ping\n        timestamps newer than (or equal to) the given value will be returned.\n      batch_size: Always reads <batch_size> snapshots at a time.\n\n    Yields:\n      An rdfvalues.objects.ClientSnapshot object for each client in the db.\n    \"\"\"\n    all_client_ids = self.ReadAllClientIDs(min_last_ping=min_last_ping)\n\n    for batch in collection.Batch(all_client_ids, batch_size):\n      res = self.MultiReadClientSnapshot(batch)\n      for snapshot in itervalues(res):\n        if snapshot:\n          yield snapshot",
        "sampled": "### Problem Description\nIterate over all available clients and yield client snapshot objects.\n\n### Input Specifications\n\n*   `min_last_ping`: The minimum last ping timestamp. If provided, only snapshots for clients with last-ping timestamps newer than (or equal to) the given value will be returned.\n*   `batch_size`: The number of snapshots to read at a time. Defaults to 50,000.\n\n### Output Specifications\n\n*   An `rdfvalues.objects.ClientSnapshot` object for each client"
    },
    {
        "original": "def ClosureTable(model_class, foreign_key=None, referencing_class=None,\n                 referencing_key=None):\n    \"\"\"Model factory for the transitive closure extension.\"\"\"\n    if referencing_class is None:\n        referencing_class = model_class\n\n    if foreign_key is None:\n        for field_obj in model_class._meta.refs:\n            if field_obj.rel_model is model_class:\n                foreign_key = field_obj\n                break\n        else:\n            raise ValueError('Unable to find self-referential foreign key.')\n\n    source_key = model_class._meta.primary_key\n    if referencing_key is None:\n        referencing_key = source_key\n\n    class BaseClosureTable(VirtualModel):\n        depth = VirtualField(IntegerField)\n        id = VirtualField(IntegerField)\n        idcolumn = VirtualField(TextField)\n        parentcolumn = VirtualField(TextField)\n        root = VirtualField(IntegerField)\n        tablename = VirtualField(TextField)\n\n        class Meta:\n            extension_module = 'transitive_closure'\n\n        @classmethod\n        def descendants(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.id))\n                     .where(cls.root == node)\n                     .objects())\n            if depth is not None:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def ancestors(cls, node, depth=None, include_node=False):\n            query = (model_class\n                     .select(model_class, cls.depth.alias('depth'))\n                     .join(cls, on=(source_key == cls.root))\n                     .where(cls.id == node)\n                     .objects())\n            if depth:\n                query = query.where(cls.depth == depth)\n            elif not include_node:\n                query = query.where(cls.depth > 0)\n            return query\n\n        @classmethod\n        def siblings(cls, node, include_node=False):\n            if referencing_class is model_class:\n                # self-join\n                fk_value = node.__data__.get(foreign_key.name)\n                query = model_class.select().where(foreign_key == fk_value)\n            else:\n                # siblings as given in reference_class\n                siblings = (referencing_class\n                            .select(referencing_key)\n                            .join(cls, on=(foreign_key == cls.root))\n                            .where((cls.id == node) & (cls.depth == 1)))\n\n                # the according models\n                query = (model_class\n                         .select()\n                         .where(source_key << siblings)\n                         .objects())\n\n            if not include_node:\n                query = query.where(source_key != node)\n\n            return query\n\n    class Meta:\n        database = referencing_class._meta.database\n        options = {\n            'tablename': referencing_class._meta.table_name,\n            'idcolumn': referencing_key.column_name,\n            'parentcolumn': foreign_key.column_name}\n        primary_key = False\n\n    name = '%sClosure' % model_class.__name__\n    return type(name, (BaseClosureTable,), {'Meta': Meta})",
        "sampled": "```python\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.children = []\n\ndef ClosureTable(model_class, foreign_key=None, referencing_class=None,\n                 referencing_key=None):\n    \"\"\"\n    Model factory for the transitive closure extension.\n    \n    Args:\n        model_class (class): The class of the nodes in the graph.\n        foreign_key (str): The name of the foreign key in the model class. Defaults to None.\n        referencing_class"
    },
    {
        "original": "def CheckHashes(self, hashes):\n    \"\"\"Check hashes against the filestore.\n\n    Blobs use the hash in the schema:\n    aff4:/files/hash/generic/sha256/[sha256hash]\n\n    Args:\n      hashes: A list of Hash objects to check.\n\n    Yields:\n      Tuples of (RDFURN, hash object) that exist in the store.\n    \"\"\"\n    hash_map = {}\n    for hsh in hashes:\n      if hsh.HasField(\"sha256\"):\n        # The canonical name of the file is where we store the file hash.\n        hash_map[aff4.ROOT_URN.Add(\"files/hash/generic/sha256\").Add(\n            str(hsh.sha256))] = hsh\n\n    for metadata in aff4.FACTORY.Stat(list(hash_map)):\n      yield metadata[\"urn\"], hash_map[metadata[\"urn\"]]",
        "sampled": "### Problem Description\n\nCheck hashes against the filestore.\n\nA blob uses the hash in the schema:\naff4:/files/hash/generic/sha256/[sha256hash]\n\n### Input and Output Specifications\n\n* Args:\n  `hashes`: A list of Hash objects to check.\n* Yields:\n  Tuples of (RDFURN, hash object) that exist in the store.\n\n### Code Solution\n```python\nfrom typing import List, Tuple, Generator\n\nclass Hash:\n    def"
    },
    {
        "original": "def check_pseudo_package(path):\n    \"\"\"\n    Verifies that a fake subpackage path for assets (notebooks, svgs,\n    pngs etc) both exists and is populated with files.\n    \"\"\"\n    if not os.path.isdir(path):\n        raise Exception(\"Please make sure pseudo-package %s exists.\" % path)\n    else:\n        assets = os.listdir(path)\n        if len(assets) == 0:\n            raise Exception(\"Please make sure pseudo-package %s is populated.\" % path)",
        "sampled": "```python\nimport os\n\ndef check_pseudo_package(path):\n    \"\"\"\n    Verifies that a fake subpackage path for assets (notebooks, svgs,\n    pngs etc) both exists and is populated with files.\n    \"\"\"\n    \n    # Check if the path exists\n    if not os.path.exists(path):\n        return False\n    \n    # Check if the path is a directory\n    if not os.path.isdir(path):\n        return False\n    \n    # Check if the directory is populated with"
    },
    {
        "original": "def setup_exchange(self, exchange_name):\n        \"\"\"Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n        command. When it is complete, the on_exchange_declareok method will\n        be invoked by pika.\n\n        :param str|unicode exchange_name: The name of the exchange to declare\n        \"\"\"\n        _logger.debug('Declaring exchange %s', exchange_name)\n        self._channel.exchange_declare(self.on_exchange_declareok,\n                                       exchange_name,\n                                       self.EXCHANGE_TYPE,\n                                       durable=True,\n                                       passive=True)",
        "sampled": "```python\nfrom pika import BasicProperties\nfrom pika.adapters.blocking_connection import BlockingChannel\n\ndef setup_exchange(self, exchange_name):\n    \"\"\"\n    Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n    command. When it is complete, the on_exchange_declareok method will\n    be invoked by pika.\n\n    :param str|unicode exchange_name: The name of the exchange to declare\n    \"\"\"\n\n    # Credentials for RabbitMQ server, you should replace"
    },
    {
        "original": "def fold(self, node):\n        \"\"\"Do constant folding.\"\"\"\n        node = self.generic_visit(node)\n        try:\n            return nodes.Const.from_untrusted(node.as_const(),\n                                              lineno=node.lineno,\n                                              environment=self.environment)\n        except nodes.Impossible:\n            return node",
        "sampled": "```python\nclass Solution:\n    def evaluate(self, expression):\n        \"\"\"\n        Evaluates the mathematical expression given across multiple people.\n        \n        Args:\n            expression (str): The mathematical expression as a string.\n            \n        Returns:\n            int: The result of the evaluation.\n        \"\"\"\n        \n        # Split the input string into variables and expressions\n        vars = []\n        tmp = ''\n        for char in expression:\n            if char == '(' or char == ')':\n                if tmp and '@' not"
    },
    {
        "original": "def pipeline_delete(id, hosts=None, profile=None):\n    \"\"\"\n    .. versionadded:: 2017.7.0\n\n    Delete Ingest pipeline. Available since Elasticsearch 5.0.\n\n    id\n        Pipeline id\n\n    CLI example::\n\n        salt myminion elasticsearch.pipeline_delete mypipeline\n    \"\"\"\n    es = _get_instance(hosts, profile)\n\n    try:\n        ret = es.ingest.delete_pipeline(id=id)\n        return ret.get('acknowledged', False)\n    except elasticsearch.NotFoundError:\n        return True\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot delete pipeline {0}, server returned code {1} with message {2}\".format(id, e.status_code, e.error))\n    except AttributeError:\n        raise CommandExecutionError(\"Method is applicable only for Elasticsearch 5.0+\")",
        "sampled": "### Problem Description\nDelete Ingest pipeline.\n\n### Input Specifications\n- `id`: Pipeline id\n- `hosts`: List of hosts (default: None)\n- `profile`: Profile to use (default: None)\n\n### Output Specifications\nNone\n\n### Python Solution\n\n```python\ndef pipeline_delete(id, hosts=None, profile=None):\n    \"\"\"\n    .. versionadded:: 2017.7.0\n\n    Delete Ingest pipeline. Available since Elasticsearch 5.0.\n\n    id\n"
    },
    {
        "original": "def lattice_2_lmpbox(lattice, origin=(0, 0, 0)):\n    \"\"\"\n    Converts a lattice object to LammpsBox, and calculates the symmetry\n    operation used.\n\n    Args:\n        lattice (Lattice): Input lattice.\n        origin: A (3,) array/list of floats setting lower bounds of\n            simulation box. Default to (0, 0, 0).\n\n    Returns:\n        LammpsBox, SymmOp\n\n    \"\"\"\n    a, b, c = lattice.abc\n    xlo, ylo, zlo = origin\n    xhi = a + xlo\n    m = lattice.matrix\n    xy = np.dot(m[1], m[0] / a)\n    yhi = np.sqrt(b ** 2 - xy ** 2) + ylo\n    xz = np.dot(m[2], m[0] / a)\n    yz = (np.dot(m[1], m[2]) - xy * xz) / (yhi - ylo)\n    zhi = np.sqrt(c ** 2 - xz ** 2 - yz ** 2) + zlo\n    tilt = None if lattice.is_orthogonal else [xy, xz, yz]\n    rot_matrix = np.linalg.solve([[xhi - xlo, 0, 0],\n                                  [xy, yhi - ylo, 0],\n                                  [xz, yz, zhi - zlo]], m)\n    bounds = [[xlo, xhi], [ylo, yhi], [zlo, zhi]]\n    symmop = SymmOp.from_rotation_and_translation(rot_matrix, origin)\n    return LammpsBox(bounds, tilt), symmop",
        "sampled": "### Problem Analysis\n\nThe problem involves converting a lattice object to a LammpsBox and calculating the symmetry operation used. A lattice is typically represented by its basis vectors, which are three-dimensional vectors that define the structure of the lattice.\n\nA LammpsBox is a simulation box used in molecular dynamics simulations with LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator). It has properties such as dimensions, origin, and periodic boundary conditions.\n\nThe symmetry operation used refers to how the lattice is transformed"
    },
    {
        "original": "def xgroup_setid(self, name, groupname, id):\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n        \"\"\"\n        return self.execute_command('XGROUP SETID', name, groupname, id)",
        "sampled": "```python\nclass ConsumerGroupManager:\n    def __init__(self):\n        # Initialize an empty dictionary to store consumer group information\n        self.consumer_groups = {}\n\n    def xgroup_setid(self, name, groupname, id):\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        \n        Args:\n            name (str): name of the stream.\n            groupname (str): name of the consumer group.\n            id (int): ID of the last item in the"
    },
    {
        "original": "def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        \"\"\"Handle Range Request related headers (RFC7233).  If `Accept-Ranges`\n        header is valid, and Range Request is processable, we set the headers\n        as described by the RFC, and wrap the underlying response in a\n        RangeWrapper.\n\n        Returns ``True`` if Range Request can be fulfilled, ``False`` otherwise.\n\n        :raises: :class:`~werkzeug.exceptions.RequestedRangeNotSatisfiable`\n                 if `Range` header could not be parsed or satisfied.\n        \"\"\"\n        from ..exceptions import RequestedRangeNotSatisfiable\n\n        if accept_ranges is None:\n            return False\n        self.headers[\"Accept-Ranges\"] = accept_ranges\n        if not self._is_range_request_processable(environ) or complete_length is None:\n            return False\n        parsed_range = parse_range_header(environ.get(\"HTTP_RANGE\"))\n        if parsed_range is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        range_tuple = parsed_range.range_for_length(complete_length)\n        content_range_header = parsed_range.to_content_range_header(complete_length)\n        if range_tuple is None or content_range_header is None:\n            raise RequestedRangeNotSatisfiable(complete_length)\n        content_length = range_tuple[1] - range_tuple[0]\n        # Be sure not to send 206 response\n        # if requested range is the full content.\n        if content_length != complete_length:\n            self.headers[\"Content-Length\"] = content_length\n            self.content_range = content_range_header\n            self.status_code = 206\n            self._wrap_response(range_tuple[0], content_length)\n            return True\n        return False",
        "sampled": "```python\nfrom werkzeug.exceptions import RequestedRangeNotSatisfiable\nimport re\n\nclass RangeWrapper:\n    def __init__(self, response):\n        self.response = response\n\nclass RangeRequestHandler:\n    def _process_range_request(self, environ, complete_length=None, accept_ranges=None):\n        # Check if Accept-Ranges header is valid\n        if accept_ranges and accept_ranges.lower() != 'bytes':\n            return False\n\n        # Parse Range header\n        range_header = environ"
    },
    {
        "original": "def gw_run(self):\n        \"\"\"\n        Performs FIESTA (gw) run\n        \"\"\"\n\n        if self.folder != os.getcwd():\n            init_folder = os.getcwd()\n            os.chdir(self.folder)\n\n        with zopen(self.log_file, 'w') as fout:\n            subprocess.call([\"mpirun\", \"-n\", str(self.mpi_procs), \"fiesta\",\n                             str(self.grid[0]), str(self.grid[1]),\n                             str(self.grid[2])], stdout=fout)\n\n        if self.folder != os.getcwd():\n            os.chdir(init_folder)",
        "sampled": "```python\nfrom typing import List\n\nclass Solution:\n    def __init__(self):\n        pass\n\n    def combination_sum(self, candidates: List[int], target: int) -> List[List[int]]:\n        \"\"\"\n        Given an array of candidate numbers (nums) and a target number (target), \n        find all unique combinations in nums where the candidate numbers sums to target.\n        \n        The same repeated number may be chosen from nums. Note that results are not \n        required to be sorted or"
    },
    {
        "original": "def _clip_gradient_op(dtype):\n  \"\"\"Create an op that clips gradients using a Defun.\n\n  The tensorflow Defun decorator creates an op and tensorflow caches these op\n  automatically according to `func_name`. Using a Defun decorator twice with the\n  same `func_name` does not create a new op, instead the cached op is used.\n\n  This method produces a new op the first time it is called with a given `dtype`\n  argument, and then uses the cached op each time it is called after that with\n  the same `dtype`. The min and max clip values are given as arguments for the\n  forward pass method so that they can be used in the backwards pass.\n\n  Args:\n    dtype: the dtype of the net whose gradient is being clipped.\n\n  Returns:\n    The op that clips gradients.\n  \"\"\"\n\n  def clip_gradient_backward(op, grad):\n    clip_value_min = op.inputs[1]\n    clip_value_max = op.inputs[2]\n    clipped_grad = tf.clip_by_value(grad, clip_value_min, clip_value_max)\n    return clipped_grad, None, None\n\n  def clip_gradient_forward(x, clip_value_min, clip_value_max):\n    del clip_value_min  # Unused.\n    del clip_value_max  # Unused.\n    return x\n\n  func_name = \"ClipGradient_{}\".format(dtype.name)\n  return function.Defun(\n      dtype, dtype, dtype,\n      python_grad_func=clip_gradient_backward,\n      func_name=func_name)(clip_gradient_forward)",
        "sampled": "### Problem Description\nCreate an op that clips gradients using a Defun.\n\n### Input and Output Specifications\n- **Input**: `dtype` (the dtype of the net whose gradient is being clipped)\n- **Output**: The op that clips gradients\n\n### Python Solution\n\n```python\nimport tensorflow as tf\n\ndef _clip_gradient_op(dtype):\n    \"\"\"Create an op that clips gradients using a Defun.\"\"\"\n    \n    @tf.function(autograph=False, experimental_relax_shapes=True)\n    def"
    },
    {
        "original": "def deframesig(frames, siglen, frame_len, frame_step, winfunc=lambda x: numpy.ones((x,))):\n    \"\"\"Does overlap-add procedure to undo the action of framesig.\n\n    :param frames: the array of frames.\n    :param siglen: the length of the desired signal, use 0 if unknown. Output will be truncated to siglen samples.\n    :param frame_len: length of each frame measured in samples.\n    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n    :returns: a 1-D signal.\n    \"\"\"\n    frame_len = round_half_up(frame_len)\n    frame_step = round_half_up(frame_step)\n    numframes = numpy.shape(frames)[0]\n    assert numpy.shape(frames)[1] == frame_len, '\"frames\" matrix is wrong size, 2nd dim is not equal to frame_len'\n\n    indices = numpy.tile(numpy.arange(0, frame_len), (numframes, 1)) + numpy.tile(\n        numpy.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n    indices = numpy.array(indices, dtype=numpy.int32)\n    padlen = (numframes - 1) * frame_step + frame_len\n\n    if siglen <= 0: siglen = padlen\n\n    rec_signal = numpy.zeros((padlen,))\n    window_correction = numpy.zeros((padlen,))\n    win = winfunc(frame_len)\n\n    for i in range(0, numframes):\n        window_correction[indices[i, :]] = window_correction[\n                                               indices[i, :]] + win + 1e-15  # add a little bit so it is never zero\n        rec_signal[indices[i, :]] = rec_signal[indices[i, :]] + frames[i, :]\n\n    rec_signal = rec_signal / window_correction\n    return rec_signal[0:siglen]",
        "sampled": "### Problem Analysis\n\nThe problem requires undoing the action of a function called `framesig` which is not provided in the challenge. However, based on the function `deframesig`, it seems that `framesig` is used to frame a signal into overlapping frames.\n\nThe goal of `deframesig` is to perform an overlap-add procedure to undo this framing. This involves:\n\n1. Applying a window size specified by the user (or no window by default) to each frame.\n2"
    },
    {
        "original": "def MakeStatResponse(self, tsk_file, tsk_attribute=None, append_name=None):\n    \"\"\"Given a TSK info object make a StatEntry.\n\n    Note that tsk uses two things to uniquely identify a data stream - the inode\n    object given in tsk_file and the attribute object which may correspond to an\n    ADS of this file for filesystems which support ADS. We store both of these\n    in the stat response.\n\n    Args:\n      tsk_file: A TSK File object for the specified inode.\n      tsk_attribute: A TSK Attribute object for the ADS. If None we use the main\n        stream.\n      append_name: If specified we append this name to the last element of the\n        pathspec.\n\n    Returns:\n      A StatEntry which can be used to re-open this exact VFS node.\n    \"\"\"\n    precondition.AssertOptionalType(append_name, Text)\n\n    info = tsk_file.info\n    response = rdf_client_fs.StatEntry()\n    meta = info.meta\n    if meta:\n      response.st_ino = meta.addr\n      for attribute in [\n          \"mode\", \"nlink\", \"uid\", \"gid\", \"size\", \"atime\", \"mtime\", \"ctime\",\n          \"crtime\"\n      ]:\n        try:\n          value = int(getattr(meta, attribute))\n          if value < 0:\n            value &= 0xFFFFFFFF\n\n          setattr(response, \"st_%s\" % attribute, value)\n        except AttributeError:\n          pass\n\n    name = info.name\n    child_pathspec = self.pathspec.Copy()\n\n    if append_name is not None:\n      # Append the name to the most inner pathspec\n      child_pathspec.last.path = utils.JoinPath(child_pathspec.last.path,\n                                                append_name)\n\n    child_pathspec.last.inode = meta.addr\n    if tsk_attribute is not None:\n      child_pathspec.last.ntfs_type = int(tsk_attribute.info.type)\n      child_pathspec.last.ntfs_id = int(tsk_attribute.info.id)\n      child_pathspec.last.stream_name = tsk_attribute.info.name\n\n      # Update the size with the attribute size.\n      response.st_size = tsk_attribute.info.size\n\n      default = rdf_paths.PathSpec.tsk_fs_attr_type.TSK_FS_ATTR_TYPE_DEFAULT\n      last = child_pathspec.last\n      if last.ntfs_type != default or last.ntfs_id:\n        # This is an ads and should be treated as a file.\n        # Clear all file type bits.\n        response.st_mode &= ~self.stat_type_mask\n        response.st_mode |= stat.S_IFREG\n\n    else:\n      child_pathspec.last.ntfs_type = None\n      child_pathspec.last.ntfs_id = None\n      child_pathspec.last.stream_name = None\n\n    if name:\n      # Encode the type onto the st_mode response\n      response.st_mode |= self.FILE_TYPE_LOOKUP.get(int(name.type), 0)\n\n    if meta:\n      # What if the types are different? What to do here?\n      response.st_mode |= self.META_TYPE_LOOKUP.get(int(meta.type), 0)\n\n    # Write the pathspec on the response.\n    response.pathspec = child_pathspec\n    return response",
        "sampled": "```python\nclass StatEntry:\n    def __init__(self, inode):\n        self.inode = inode\n\n\nclass TSKFile:\n    def __init__(self, inode):\n        self.inode = inode\n\n\nclass TSKAttribute:\n    def __init__(self, stream_id):\n        self.stream_id = stream_id\n\n\ndef MakeStatResponse(tsk_file, tsk_attribute=None, append_name=None):\n    \"\"\"\n    Given a TSK info object make a StatEntry.\n\n    Args"
    },
    {
        "original": "def get_values(self):\n        \"\"\"\n        Returns the cpd\n\n        Examples\n        --------\n        >>> from pgmpy.factors.discrete import TabularCPD\n        >>> cpd = TabularCPD('grade', 3, [[0.1, 0.1],\n        ...                               [0.1, 0.1],\n        ...                               [0.8, 0.8]],\n        ...                  evidence='evi1', evidence_card=2)\n        >>> cpd.get_values()\n        array([[ 0.1,  0.1],\n               [ 0.1,  0.1],\n               [ 0.8,  0.8]])\n        \"\"\"\n        if self.variable in self.variables:\n            return self.values.reshape(self.cardinality[0], np.prod(self.cardinality[1:]))\n        else:\n            return self.values.reshape(1, np.prod(self.cardinality))",
        "sampled": "```python\nfrom pgmpy.factors.discrete import TabularCPD\n\nclass CPD:\n    def __init__(self, variable, num_states, values, evidence=None, evidence_card=None):\n        \"\"\"\n        Initialize the Conditional Probability Distribution (CPD).\n\n        Parameters\n        ----------\n        variable : str\n            The name of the variable.\n        num_states : int\n            The number of states for the variable.\n        values : list of lists\n            A 2D list containing"
    },
    {
        "original": "def join_chat(\n        self,\n        chat_id: str\n    ):\n        \"\"\"Use this method to join a group chat or channel.\n\n        Args:\n            chat_id (``str``):\n                Unique identifier for the target chat in form of a *t.me/joinchat/* link or username of the target\n                channel/supergroup (in the format @username).\n\n        Returns:\n            On success, a :obj:`Chat <pyrogram.Chat>` object is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        match = self.INVITE_LINK_RE.match(chat_id)\n\n        if match:\n            chat = self.send(\n                functions.messages.ImportChatInvite(\n                    hash=match.group(1)\n                )\n            )\n            if isinstance(chat.chats[0], types.Chat):\n                return pyrogram.Chat._parse_chat_chat(self, chat.chats[0])\n            elif isinstance(chat.chats[0], types.Channel):\n                return pyrogram.Chat._parse_channel_chat(self, chat.chats[0])\n        else:\n            resolved_peer = self.send(\n                functions.contacts.ResolveUsername(\n                    username=chat_id.lower().strip(\"@\")\n                )\n            )\n\n            channel = types.InputPeerChannel(\n                channel_id=resolved_peer.chats[0].id,\n                access_hash=resolved_peer.chats[0].access_hash\n            )\n\n            chat = self.send(\n                functions.channels.JoinChannel(\n                    channel=channel\n                )\n            )\n\n            return pyrogram.Chat._parse_channel_chat(self, chat.chats[0])",
        "sampled": "```python\nclass TelegramChatManager:\n    def join_chat(self, chat_id: str) -> dict:\n        \"\"\"Use this method to join a group chat or channel.\n\n        Args:\n            chat_id (``str``):\n                Unique identifier for the target chat in form of a *t.me/joinchat/* link or username of the target\n                channel/supergroup (in the format @username).\n\n        Returns:\n            On success, returns a dictionary representing the result.\n\n        Raises"
    },
    {
        "original": "def renderer_doc(*args):\n    \"\"\"\n    Return the docstrings for all renderers. Optionally, specify a renderer or a\n    function to narrow the selection.\n\n    The strings are aggregated into a single document on the master for easy\n    reading.\n\n    Multiple renderers can be specified.\n\n    .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc\n        salt '*' sys.renderer_doc cheetah\n        salt '*' sys.renderer_doc jinja json\n\n    Renderer names can be specified as globs.\n\n    .. code-block:: bash\n\n        salt '*' sys.renderer_doc 'c*' 'j*'\n\n    \"\"\"\n    renderers_ = salt.loader.render(__opts__, [])\n    docs = {}\n    if not args:\n        for func in six.iterkeys(renderers_):\n            docs[func] = renderers_[func].__doc__\n        return _strip_rst(docs)\n\n    for module in args:\n        if '*' in module or '.' in module:\n            for func in fnmatch.filter(renderers_, module):\n                docs[func] = renderers_[func].__doc__\n        else:\n            moduledot = module + '.'\n            for func in six.iterkeys(renderers_):\n                if func.startswith(moduledot):\n                    docs[func] = renderers_[func].__doc__\n    return _strip_rst(docs)",
        "sampled": "```python\n# Define a dictionary that maps renderer names to their docstrings\nrenderer_docs = {\n    'cheetah': \"Cheetha Renderer\",\n    'jinja': \"Jinja Renderer\",\n    # Add more renderers as needed\n}\n\ndef renderer_doc(*args):\n    \"\"\"\n    Return the docstrings for all renderers. Optionally, specify a renderer or a function to narrow the selection.\n    \n    The strings are aggregated into a single document on the master for easy reading.\n"
    },
    {
        "original": "def commit(self):\n        \"\"\"\n        Commit this context's offsets:\n\n         -  If the high-water mark has moved, commit up to and position the\n            consumer at the high-water mark.\n         -  Otherwise, reset to the consumer to the initial offsets.\n        \"\"\"\n        if self.high_water_mark:\n            self.logger.info(\"Committing offsets: %s\", self.high_water_mark)\n            self.commit_partition_offsets(self.high_water_mark)\n            self.update_consumer_offsets(self.high_water_mark)\n        else:\n            self.update_consumer_offsets(self.initial_offsets)",
        "sampled": "```python\nclass StreamingContext:\n    def __init__(self):\n        # Initialize offset tracking variables\n        self.consumer_offset = None\n        self.high_water_mark = None\n\n    def set_initial_offsets(self, initial_consumer_offset):\n        # Set the initial offsets for the consumer and high water mark\n        self.consumer_offset = initial_consumer_offset\n        self.high_water_mark = initial_consumer_offset\n\n    def update_high_water_mark(self, new_high_water_mark):\n        # Update the high water mark"
    },
    {
        "original": "def parametric_mean_function(max_iters=100, optimize=True, plot=True):\n    \"\"\"\n    A linear mean function with parameters that we'll learn alongside the kernel\n    \"\"\"\n    #create  simple mean function\n    mf = GPy.core.Mapping(1,1)\n    mf.f = np.sin\n\n    X = np.linspace(0,10,50).reshape(-1,1)\n    Y = np.sin(X) + 0.5*np.cos(3*X) + 0.1*np.random.randn(*X.shape) + 3*X\n\n    mf = GPy.mappings.Linear(1,1)\n\n    k =GPy.kern.RBF(1)\n    lik = GPy.likelihoods.Gaussian()\n    m = GPy.core.GP(X, Y, kernel=k, likelihood=lik, mean_function=mf)\n    if optimize:\n        m.optimize(max_iters=max_iters)\n    if plot:\n        m.plot()\n    return m",
        "sampled": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef parametric_mean_function(max_iters=100, optimize=True, plot=True):\n    \"\"\"\n    A linear mean function with parameters that we'll learn alongside the kernel\n    \"\"\"\n    \n    # Define the model's parameters\n    m = 4.0  # bias term (intercept)\n    b = 2.0  # slope coefficient\n    \n   \nbasketball_players_data = {'player1': [5.0,"
    },
    {
        "original": "def get_network_adapter_type(adapter_type):\n    \"\"\"\n    Return the network adapter type.\n\n    adpater_type\n        The adapter type from which to obtain the network adapter type.\n    \"\"\"\n    if adapter_type == 'vmxnet':\n        return vim.vm.device.VirtualVmxnet()\n    elif adapter_type == 'vmxnet2':\n        return vim.vm.device.VirtualVmxnet2()\n    elif adapter_type == 'vmxnet3':\n        return vim.vm.device.VirtualVmxnet3()\n    elif adapter_type == 'e1000':\n        return vim.vm.device.VirtualE1000()\n    elif adapter_type == 'e1000e':\n        return vim.vm.device.VirtualE1000e()\n\n    raise ValueError('An unknown network adapter object type name.')",
        "sampled": "```python\ndef get_network_adapter_type(adapter_type):\n    \"\"\"\n    Return the network adapter type.\n\n    Args:\n        adapter_type (str): The adapter type from which to obtain the network adapter type.\n    \n    Returns:\n        str: The network adapter type.\n\n    For simplicity, we'll assume that different types of adapters have different icons.\n    This function simply returns a string-based representation of the icon for each type of adapter.\n    \n    Here's a simple mapping between some common types of adapters"
    },
    {
        "original": "def writef(notebook, nb_file, fmt=None):\n    \"\"\"Write a notebook to the file with given name\"\"\"\n    if nb_file == '-':\n        write(notebook, sys.stdout, fmt)\n        return\n\n    _, ext = os.path.splitext(nb_file)\n    fmt = copy(fmt or {})\n    fmt = long_form_one_format(fmt, update={'extension': ext})\n\n    create_prefix_dir(nb_file, fmt)\n\n    with io.open(nb_file, 'w', encoding='utf-8') as stream:\n        write(notebook, stream, fmt)",
        "sampled": "### Problem Description\n omitted for this scenario"
    },
    {
        "original": "def pickle_dump(self):\n        \"\"\"\n        Save the status of the object in pickle format.\n        Returns 0 if success\n        \"\"\"\n        if self.has_chrooted:\n            warnings.warn(\"Cannot pickle_dump since we have chrooted from %s\" % self.has_chrooted)\n            return -1\n\n        #if self.in_spectator_mode:\n        #    warnings.warn(\"Cannot pickle_dump since flow is in_spectator_mode\")\n        #    return -2\n\n        protocol = self.pickle_protocol\n\n        # Atomic transaction with FileLock.\n        with FileLock(self.pickle_file):\n            with AtomicFile(self.pickle_file, mode=\"wb\") as fh:\n                pmg_pickle_dump(self, fh, protocol=protocol)\n\n        return 0",
        "sampled": "```python\nimport pickle\n\nclass PickleSaver:\n    def __init__(self, obj):\n        self.obj = obj\n\n    def pickle_dump(self):\n        \"\"\"\n        Save the status of the object in pickle format.\n        Returns 0 if success\n        \"\"\"\n        try:\n            with open('object_status.pkl', 'wb') as f:\n                pickle.dump(self.obj, f)\n            return 0\n        except Exception as e:\n            print(f\"Error: {e}\")\n"
    },
    {
        "original": "def _MakeTimestamp(self, start=None, end=None):\n    \"\"\"Create a timestamp using a start and end time.\n\n    Args:\n      start: Start timestamp.\n      end: End timestamp.\n\n    Returns:\n      A tuple (start, end) of converted timestamps or None for all time.\n    \"\"\"\n    mysql_unsigned_bigint_max = 18446744073709551615\n    ts_start = int(start or 0)\n    if end is None:\n      ts_end = mysql_unsigned_bigint_max\n    else:\n      ts_end = int(end)\n    if ts_start == 0 and ts_end == mysql_unsigned_bigint_max:\n      return None\n    else:\n      return (ts_start, ts_end)",
        "sampled": "```\nimport time\nfrom datetime import datetime, timedelta\n\nclass TimestampConverter:\n    def _MakeTimestamp(self, start=None, end=None):\n        \"\"\"Create a timestamp using a start and end time.\n\n        Args:\n          start: Start timestamp.\n          end: End timestamp.\n\n        Returns:\n          A tuple (start, time) of converted timestamps or None for all time.\n        \"\"\"\n\n        if start is None and end is None:\n            return (None,),\ttime\n\n        if isinstance(start,"
    },
    {
        "original": "def loopUntil(\n            self, condition=None, timeout: float = 0) -> Iterator[object]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout in seconds.\n        The yielded value is that of the condition or False when timed out.\n\n        Args:\n            condition: Predicate function that is tested after every network\n            update.\n            timeout: Maximum time in seconds to wait.\n                If 0 then no timeout is used.\n        \"\"\"\n        endTime = time.time() + timeout\n        while True:\n            test = condition and condition()\n            if test:\n                yield test\n                return\n            elif timeout and time.time() > endTime:\n                yield False\n                return\n            else:\n                yield test\n            self.waitOnUpdate(endTime - time.time() if timeout else 0)",
        "sampled": "```python\nfrom typing import Iterator, TypeVar, Callable\nimport itertools\n\nT = TypeVar('T')\n\ndef loop_until(\n    self,\n    condition: Callable[[object], bool] = None,\n    timeout: float = 0) -> Iterator[T]:\n    \"\"\"\n    Iterate until condition is met, with optional timeout in seconds.\n    The yielded value is that of the object that did not meet the condition when timed out.\n\n     Args:\n         condition: Predicate function that is tested after"
    },
    {
        "original": "def retrieve_equities(self, sids):\n        \"\"\"\n        Retrieve Equity objects for a list of sids.\n\n        Users generally shouldn't need to this method (instead, they should\n        prefer the more general/friendly `retrieve_assets`), but it has a\n        documented interface and tests because it's used upstream.\n\n        Parameters\n        ----------\n        sids : iterable[string]\n\n        Returns\n        -------\n        equities : dict[str -> Equity]\n\n        Raises\n        ------\n        EquitiesNotFound\n            When any requested asset isn't found.\n        \"\"\"\n        cache = self._asset_cache\n\n        try:\n            return {\n                k: cache[k]\n                for k in sids\n            }\n        except KeyError:\n            raise EquitiesNotFound(sids=sids)",
        "sampled": "### Problem Analysis\n\nThe `retrieve_equities` method is designed to retrieve a dictionary of Equity objects for a given list of Security Identifiers (sids). The method takes an iterable of strings representing the sids and returns a dictionary where each key is a string (the sid) and the value is an Equity object.\n\nThe method raises an `EquitiesNotFound` exception if any of the requested assets are not found. The preferred alternative to this method is `retrieve_assets`, which seems to be"
    },
    {
        "original": "def _calc_taud(w, aod700, p):\n    \"\"\"Calculate the taud coefficient.\"\"\"\n\n    # isscalar tests needed to ensure that the arrays will have the\n    # right shape in the tds calculation.\n    # there's probably a better way to do this.\n\n    if np.isscalar(w) and np.isscalar(aod700):\n        w = np.array([w])\n        aod700 = np.array([aod700])\n    elif np.isscalar(w):\n        w = np.full_like(aod700, w)\n    elif np.isscalar(aod700):\n        aod700 = np.full_like(w, aod700)\n\n    # set up nan-tolerant masks\n    aod700_lt_0p05 = np.full_like(aod700, False, dtype='bool')\n    np.less(aod700, 0.05, where=~np.isnan(aod700), out=aod700_lt_0p05)\n    aod700_mask = np.array([aod700_lt_0p05, ~aod700_lt_0p05], dtype=np.int)\n\n    # create tuples of coefficients for\n    # aod700 < 0.05, aod700 >= 0.05\n    td4 = 86*w - 13800, -0.21*w + 11.6\n    td3 = -3.11*w + 79.4, 0.27*w - 20.7\n    td2 = -0.23*w + 74.8, -0.134*w + 15.5\n    td1 = 0.092*w - 8.86, 0.0554*w - 5.71\n    td0 = 0.0042*w + 3.12, 0.0057*w + 2.94\n    tdp = -0.83*(1+aod700)**(-17.2), -0.71*(1+aod700)**(-15.0)\n\n    tds = (np.array([td0, td1, td2, td3, td4, tdp]) * aod700_mask).sum(axis=1)\n\n    p0 = 101325.\n    taud = (tds[4]*aod700**4 + tds[3]*aod700**3 + tds[2]*aod700**2 +\n            tds[1]*aod700 + tds[0] + tds[5]*np.log(p/p0))\n\n    # be polite about matching the output type to the input type(s)\n    if len(taud) == 1:\n        taud = taud[0]\n\n    return taud",
        "sampled": "```python\nimport math\n\ndef _calc_taud(w, aod700, p):\n    \"\"\"\n    Calculate the taud coefficient.\n    \n    Parameters:\n    w (float): The wind speed in m/s.\n    aod700 (float): The aerosol optical depth at 700 nm.\n    p (float): The pressure in hPa.\n    \n    Returns:\n    float: The taud coefficient value.\n    \n    Formula:\n        \\tau_{d} = 0.0009"
    },
    {
        "original": "def cleanup(temp_name):\n    \"\"\" Tries to remove temp files by filename wildcard path. \"\"\"\n    for filename in iglob(temp_name + '*' if temp_name else temp_name):\n        try:\n            os.remove(filename)\n        except OSError:\n            pass",
        "sampled": "### Problem Description\nNo problem description was given. However, based on the function signature, it seems like the task is to write a function `cleanup` that takes a temporary file name pattern as input and removes any files matching that pattern.\n\n### Input Specifications\n*   The input `temp_name` is a string representing the filename pattern of temporary files.\n\n### Output Specifications\n*   None (the function should remove the matching files and do nothing else)\n\n### Solution\n\n```python\nimport os\n"
    },
    {
        "original": "def _acl_changes(name, id=None, type=None, rules=None, consul_url=None, token=None):\n    \"\"\"\n       return True if the acl need to be update, False if it doesn't need to be update\n    \"\"\"\n    info = __salt__['consul.acl_info'](id=id, token=token, consul_url=consul_url)\n\n    if info['res'] and info['data'][0]['Name'] != name:\n        return True\n    elif info['res'] and info['data'][0]['Rules'] != rules:\n        return True\n    elif info['res'] and info['data'][0]['Type'] != type:\n        return True\n    else:\n        return False",
        "sampled": "### Problem Description\n#### ACL Changes Checker\n\nYou are given a function `_acl_changes` that checks if the ACL (Access Control List) of a Consul server needs to be updated. The function takes several parameters:\n\n* `name`: the name of the ACL\n* `id`: the ID of the ACL (optional)\n* `type`: the type of the ACL (optional)\n* `rules`: a list of rules for the ACL (optional)\n* `consul_url`: the URL"
    },
    {
        "original": "def StreamFile(self, filedesc, offset=0, amount=None):\n    \"\"\"Streams chunks of a given file starting at given offset.\n\n    Args:\n      filedesc: A `file` object to stream.\n      offset: An integer offset at which the file stream should start on.\n      amount: An upper bound on number of bytes to read.\n\n    Returns:\n      Generator over `Chunk` instances.\n    \"\"\"\n    reader = FileReader(filedesc, offset=offset)\n    return self.Stream(reader, amount=amount)",
        "sampled": "```python\nclass Chunk:\n    \"\"\" Represents a chunk of data. \"\"\"\n\n    def __init__(self, data, offset):\n        self.data = data\n        self.offset = offset\n\n\nclass Storage:\n    \"\"\" Toy disk storage sidecar object for file system operations in Hypre implementations. \"\"\"\n\n    def __init__(self):\n        self.files = {}\n\n    def create_file(self, name, size):\n        \"\"\" Creates a new file with the given name and size.\n\n        Args:\n          name ("
    },
    {
        "original": "def index(self, column):  # pylint: disable=C6409\n        \"\"\"Fetches the column number (0 indexed).\n\n    Args:\n      column: A string, column to fetch the index of.\n\n    Returns:\n      An int, the row index number.\n\n    Raises:\n      ValueError: The specified column was not found.\n    \"\"\"\n        for i, key in enumerate(self._keys):\n            if key == column:\n                return i\n        raise ValueError('Column \"%s\" not found.' % column)",
        "sampled": "### Problem Description\n\nQuestion: \n- The problem can be described as follows: Given a string column title that represents a column title, return its equivalent integer column number. \n\n- The string `A` corresponds to 1, `\"B\"` corresponds to 2, `\"C\"` corresponds to 3, and so on.\n\n- In addition, both letters (title) and numbers (number) in the column titles are case-insensitive.\n\n### Input/Output Specifications\n\n* Input:\n    -"
    },
    {
        "original": "def pointer_gate(num_qubits, U):\n    \"\"\"\n    Make a pointer gate on `num_qubits`. The one-qubit gate U will act on the\n    qubit addressed by the pointer qubits interpreted as an unsigned binary\n    integer.\n\n    There are P = floor(lg(num_qubits)) pointer qubits, and qubits numbered\n\n        N - 1\n        N - 2\n        ...\n        N - P\n\n    are those reserved to represent the pointer. The first N - P qubits\n    are the qubits which the one-qubit gate U can act on.\n    \"\"\"\n    ptr_bits = int(floor(np.log2(num_qubits)))\n    data_bits = num_qubits - ptr_bits\n    ptr_state = 0\n    assert ptr_bits > 0\n\n    program = pq.Program()\n\n    program.defgate(\"CU\", controlled(ptr_bits, U))\n\n    for _, target_qubit, changed in gray(ptr_bits):\n        if changed is None:\n            for ptr_qubit in range(num_qubits - ptr_bits, num_qubits):\n                program.inst(X(ptr_qubit))\n                ptr_state ^= 1 << (ptr_qubit - data_bits)\n        else:\n            program.inst(X(data_bits + changed))\n            ptr_state ^= 1 << changed\n\n        if target_qubit < data_bits:\n            control_qubits = tuple(data_bits + i for i in range(ptr_bits))\n            program.inst((\"CU\",) + control_qubits + (target_qubit,))\n\n    fixup(program, data_bits, ptr_bits, ptr_state)\n    return program",
        "sampled": "```python\nimport math\n\ndef pointer_gate(num_qubits, U):\n    \"\"\"\n    Make a pointer gate on `num_qubits`. The one-qubit gate U will act on the\n    qubit addressed by the pointer qubits interpreted as an unsigned binary\n    integer.\n\n    There are P = floor(lg(num_qubits)) pointer qubits, and qubits numbered\n\n        N - 1\n        N - 2\n        ...\n        N - P\n\n    are those"
    },
    {
        "original": "def posterior_predictive_to_xarray(self):\n        \"\"\"Convert posterior_predictive samples to xarray.\"\"\"\n        posterior_predictive = self.posterior_predictive\n        columns = self.posterior[0].columns\n        if (\n            isinstance(posterior_predictive, (tuple, list))\n            and posterior_predictive[0].endswith(\".csv\")\n        ) or (isinstance(posterior_predictive, str) and posterior_predictive.endswith(\".csv\")):\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            chain_data = []\n            for path in posterior_predictive:\n                parsed_output = _read_output(path)\n                for sample, *_ in parsed_output:\n                    chain_data.append(sample)\n            data = _unpack_dataframes(chain_data)\n        else:\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            posterior_predictive_cols = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in posterior_predictive)\n            ]\n            data = _unpack_dataframes([item[posterior_predictive_cols] for item in self.posterior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)",
        "sampled": "### Problem Description\nNo problem description is given in the note. The function `posterior_predictive_to_xarray` is defined as a method within a class, with the docstring specifying that it converts posterior predictive samples to an xarray.\n\n\n### Input and Output Specifications\nNo input and output specifications are provided but assume we know how to interpret what `posterior_predictive_samples` means.\n\n### Python Solution\n\n```python\nimport xarray as xr\n\nclass Model:\n    def posterior_predictive"
    },
    {
        "original": "def delete(name, timeout=90):\n    \"\"\"\n    Delete the named service\n\n    Args:\n\n        name (str): The name of the service to delete\n\n        timeout (int):\n            The time in seconds to wait for the service to be deleted before\n            returning. This is necessary because a service must be stopped\n            before it can be deleted. Default is 90 seconds\n\n            .. versionadded:: 2017.7.9,2018.3.4\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``. Also returns ``True``\n            if the service is not present\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.delete <service name>\n    \"\"\"\n    handle_scm = win32service.OpenSCManager(\n        None, None, win32service.SC_MANAGER_CONNECT)\n\n    try:\n        handle_svc = win32service.OpenService(\n            handle_scm, name, win32service.SERVICE_ALL_ACCESS)\n    except pywintypes.error as exc:\n        win32service.CloseServiceHandle(handle_scm)\n        if exc.winerror != 1060:\n            raise CommandExecutionError(\n                'Failed to open {0}. {1}'.format(name, exc.strerror))\n        log.debug('Service \"%s\" is not present', name)\n        return True\n\n    try:\n        win32service.DeleteService(handle_svc)\n    except pywintypes.error as exc:\n        raise CommandExecutionError(\n            'Failed to delete {0}. {1}'.format(name, exc.strerror))\n    finally:\n        log.debug('Cleaning up')\n        win32service.CloseServiceHandle(handle_scm)\n        win32service.CloseServiceHandle(handle_svc)\n\n    end_time = time.time() + int(timeout)\n    while name in get_all() and time.time() < end_time:\n        time.sleep(1)\n\n    return name not in get_all()",
        "sampled": "```python\nimport time\nimport subprocess\n\ndef delete(name, timeout=90):\n    \"\"\"\n    Delete the named service\n\n    Args:\n\n        name (str): The name of the service to delete\n\n        timeout (int):\n            The time in seconds to wait for the service to be deleted before\n            returning. This is necessary because a service must be stopped\n            before it can be deleted. Default is 90 seconds\n\n            .. versionadded:: 2017.7.9,"
    },
    {
        "original": "def remove_nodes(self, indices):\n        \"\"\"\n        A wrapper for Molecule.remove_sites().\n\n        :param indices: list of indices in the current Molecule (and graph) to\n            be removed.\n        :return:\n        \"\"\"\n\n        self.molecule.remove_sites(indices)\n        self.graph.remove_nodes_from(indices)\n\n        mapping = {}\n        for correct, current in enumerate(sorted(self.graph.nodes)):\n            mapping[current] = correct\n\n        nx.relabel_nodes(self.graph, mapping, copy=False)\n        self.set_node_attributes()",
        "sampled": "### Problem Description\n\nRemove nodes from molecular graph based on their indices.\n\nThe molecular graph is represented as a data structure. Each node in the graph corresponds to an atom in the molecule. When an atom is removed from the molecule, all bonds connected to it are also removed.\n\n### Input and Output Specifications\n\n*   `indices`: A list of indices of atoms to be removed from the molecule.\n*   The function should remove these atoms and their corresponding bonds from the molecular graph.\n\n### Python Solution\n\n"
    },
    {
        "original": "def ParseYAMLAuthorizationsList(yaml_data):\n    \"\"\"Parses YAML data into a list of APIAuthorization objects.\"\"\"\n    try:\n      raw_list = yaml.ParseMany(yaml_data)\n    except (ValueError, pyyaml.YAMLError) as e:\n      raise InvalidAPIAuthorization(\"Invalid YAML: %s\" % e)\n\n    result = []\n    for auth_src in raw_list:\n      auth = APIAuthorization()\n      auth.router_cls = _GetRouterClass(auth_src[\"router\"])\n      auth.users = auth_src.get(\"users\", [])\n      auth.groups = auth_src.get(\"groups\", [])\n      auth.router_params = auth_src.get(\"router_params\", {})\n\n      result.append(auth)\n\n    return result",
        "sampled": "### Problem Description\n#### Parse YAML Authorizations List\n\nYou are given a YAML string containing a list of API authorizations. Your task is to parse this YAML data into a list of `APIAuthorization` objects.\n\n### Input and Output Specifications\n#### Input\n\n* `yaml_data`: A string representing the YAML data.\n\n#### Output\n\n* A list of `APIAuthorization` objects, where each object contains the following attributes:\n  * `id`: The ID of the API authorization.\n  * `"
    },
    {
        "original": "def _parse_canonical_int32(doc):\n    \"\"\"Decode a JSON int32 to python int.\"\"\"\n    i_str = doc['$numberInt']\n    if len(doc) != 1:\n        raise TypeError('Bad $numberInt, extra field(s): %s' % (doc,))\n    if not isinstance(i_str, string_type):\n        raise TypeError('$numberInt must be string: %s' % (doc,))\n    return int(i_str)",
        "sampled": "```python\nimport json\n\ndef _parse_canonical_int32(doc):\n    \"\"\"\n    Decode a JSON int32 to python int.\n    \n    :param doc: A JSON string representing an int32 value.\n    :return: The decoded integer value.\n    \"\"\"\n    \n    # Load the JSON string into a Python object\n    obj = json.loads(doc)\n    \n    # Check if the object is an integer\n    if isinstance(obj, int):\n        return obj\n    \n    # If not, check"
    },
    {
        "original": "def keep_path(self, path):\n        \"\"\"\n        Given a path, returns True if the path should be kept, False if it should be cut.\n        \"\"\"\n        if len(path.addr_trace) < 2:\n            return True\n\n        return self.should_take_exit(path.addr_trace[-2], path.addr_trace[-1])",
        "sampled": "```python\nfrom math import sqrt\n\nclass Solution:\n    def keep_path(self, path):\n        \"\"\"\n        Given a path, returns True if the path should be kept, False if it should be cut.\n        \n        The decision to keep or cut a path is based on the following rules:\n        - If the length of the path is less than 2, it should be kept.\n        - If the length of the path is greater than or equal to 2 and less than or equal to"
    },
    {
        "original": "def get(key, default=''):\n    \"\"\"\n    .. versionadded: 0.14.0\n\n    Attempt to retrieve the named value from opts, pillar, grains of the master\n    config, if the named value is not available return the passed default.\n    The default return is an empty string.\n\n    The value can also represent a value in a nested dict using a \":\" delimiter\n    for the dict. This means that if a dict looks like this::\n\n        {'pkg': {'apache': 'httpd'}}\n\n    To retrieve the value associated with the apache key in the pkg dict this\n    key can be passed::\n\n        pkg:apache\n\n    This routine traverses these data stores in this order:\n\n    - Local minion config (opts)\n    - Minion's grains\n    - Minion's pillar\n    - Master config\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' config.get pkg:apache\n    \"\"\"\n    ret = salt.utils.data.traverse_dict_and_list(__opts__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__grains__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__pillar__, key, '_|-')\n    if ret != '_|-':\n        return ret\n    ret = salt.utils.data.traverse_dict_and_list(__pillar__.get('master', {}), key, '_|-')\n    if ret != '_|-':\n        return ret\n    return default",
        "sampled": "### Problem Analysis\n\nThe problem involves a `get` function that attempts to retrieve a value from multiple sources in a specific order:\n\n1. Local minion config (opts)\n2. Minion's grains\n3. Minion's pillar\n4. Master config\n\nThe function takes two arguments: `key` and `default`. The `key` can be a nested dictionary key, represented by using a colon (`:`) delimiter.\n\n### Python Solution\n\n```python\ndef get(key, default=''):\n"
    },
    {
        "original": "def get_followers(self, first_user_id=None):\n        \"\"\"\n        \u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n        \u8be6\u60c5\u8bf7\u53c2\u8003 http://mp.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n\n        :param first_user_id: \u53ef\u9009\u3002\u7b2c\u4e00\u4e2a\u62c9\u53d6\u7684OPENID\uff0c\u4e0d\u586b\u9ed8\u8ba4\u4ece\u5934\u5f00\u59cb\u62c9\u53d6\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        params = {\"access_token\": self.token}\n        if first_user_id:\n            params[\"next_openid\"] = first_user_id\n        return self.get(\n            \"https://api.weixin.qq.com/cgi-bin/user/get\", params=params\n        )",
        "sampled": "### Problem Description\n\u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n\n### Input and Output Specifications\n- **Input**: \n  - `first_user_id`: \u53ef\u9009\u3002\u7b2c\u4e00\u4e2a\u62c9\u53d6\u7684OPENID\uff0c\u4e0d\u586b\u9ed8\u8ba4\u4ece\u5934\u5f00\u59cb\u62c9\u53d6\n- **Output**: \n  - \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n### Python Solution\n\n```python\nimport requests\n\nclass WeChatAPI:\n    def get_followers(self, first_user_id=None):\n        \"\"\"\n        \u83b7\u53d6\u5173\u6ce8\u8005\u5217\u8868\n        \u8be6\u60c5\u8bf7\u53c2\u8003"
    },
    {
        "original": "def get_metric_value_by_labels(messages, _metric, _m, metric_suffix):\n        \"\"\"\n        :param messages: dictionary as metric_name: {labels: {}, value: 10}\n        :param _metric: dictionary as {labels: {le: '0.001', 'custom': 'value'}}\n        :param _m: str as metric name\n        :param metric_suffix: str must be in (count or sum)\n        :return: value of the metric_name matched by the labels\n        \"\"\"\n        metric_name = '{}_{}'.format(_m, metric_suffix)\n        expected_labels = set(\n            [(k, v) for k, v in iteritems(_metric[\"labels\"]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]\n        )\n        for elt in messages[metric_name]:\n            current_labels = set(\n                [(k, v) for k, v in iteritems(elt[\"labels\"]) if k not in PrometheusScraperMixin.UNWANTED_LABELS]\n            )\n            # As we have two hashable objects we can compare them without any side effects\n            if current_labels == expected_labels:\n                return float(elt[\"value\"])\n\n        raise AttributeError(\"cannot find expected labels for metric %s with suffix %s\" % (metric_name, metric_suffix))",
        "sampled": "```python\ndef get_metric_value_by_labels(messages, _metric, _m, metric_suffix):\n    \"\"\"\n    :param messages: dictionary as metric_name: {labels: {}, value: 10}\n    :param _metric: dictionary as {labels: {le: '0.001', 'custom': 'value'}}\n    :param _m: str as metric name\n    :param metric_suffix: str must be in (count or sum)\n    :return: value of the metric"
    },
    {
        "original": "def next(self):\n        \"\"\"Advance the cursor.\"\"\"\n        if len(self.__data) or self._refresh():\n            coll = self.__collection\n            return coll.database._fix_outgoing(self.__data.popleft(), coll)\n        else:\n            raise StopIteration",
        "sampled": "```python\nfrom math import gcd\n\nclass Solution:\n    def nthMagpie(self, n, k):\n        lcm = lambda x, y: x * y // gcd(x, y)\n        min_val = (n // k) * lcm(k, n % k)\n        max_val = (n - 1) // (k - 1) * lcm(k - 1, n % (k - 1)) + lcm(k - 1, max(0,n%("
    },
    {
        "original": "def get_outputs_filtered(self, owner, spent=None):\n        \"\"\"Get a list of output links filtered on some criteria\n\n        Args:\n            owner (str): base58 encoded public_key.\n            spent (bool): If ``True`` return only the spent outputs. If\n                          ``False`` return only unspent outputs. If spent is\n                          not specified (``None``) return all outputs.\n\n        Returns:\n            :obj:`list` of TransactionLink: list of ``txid`` s and ``output`` s\n            pointing to another transaction's condition\n        \"\"\"\n        outputs = self.fastquery.get_outputs_by_public_key(owner)\n        if spent is None:\n            return outputs\n        elif spent is True:\n            return self.fastquery.filter_unspent_outputs(outputs)\n        elif spent is False:\n            return self.fastquery.filter_spent_outputs(outputs)",
        "sampled": "```python\nclass TransactionLink:\n    def __init__(self, txid, output):\n        self.txid = txid\n        self.output = output\n\nclass CodexProcessor:\n    def get_outputs_filtered(self, owner, spent=None):\n        # This is a placeholder for the actual data retrieval logic.\n        # In a real-world scenario, this would likely involve interacting with a database or API.\n        if spent is None:\n            # Return all outputs by default\n            return [\n               "
    },
    {
        "original": "def owns_endpoint(self, endpoint):\n        \"\"\"Tests if an endpoint name (not path) belongs to this Api.  Takes\n        in to account the Blueprint name part of the endpoint name.\n\n        :param endpoint: The name of the endpoint being checked\n        :return: bool\n        \"\"\"\n\n        if self.blueprint:\n            if endpoint.startswith(self.blueprint.name):\n                endpoint = endpoint.split(self.blueprint.name + '.', 1)[-1]\n            else:\n                return False\n        return endpoint in self.endpoints",
        "sampled": "```python\nclass Api:\n    def __init__(self, name):\n        self.name = name\n        self.endpoints = {}\n\n    def add_endpoint(self, endpoint_name):\n        if endpoint_name not in self.endpoints:\n            self.endpoints[endpoint_name] = True\n\n    def owns_endpoint(self, endpoint):\n        \"\"\"Tests if an endpoint name (not path) belongs to this Api.  Takes\n        in to account the Blueprint name part of the endpoint name.\n\n        :param"
    },
    {
        "original": "def get_jobs(state='all'):\n    \"\"\"\n    List all jobs on the device.\n\n    state\n        The state of the jobs to display. Valid options are all, pending, or processed. Pending jobs are jobs\n        that are currently in a running or waiting state. Processed jobs are jobs that have completed\n        execution.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' panos.get_jobs\n        salt '*' panos.get_jobs state=pending\n\n    \"\"\"\n    if state.lower() == 'all':\n        query = {'type': 'op', 'cmd': '<show><jobs><all></all></jobs></show>'}\n    elif state.lower() == 'pending':\n        query = {'type': 'op', 'cmd': '<show><jobs><pending></pending></jobs></show>'}\n    elif state.lower() == 'processed':\n        query = {'type': 'op', 'cmd': '<show><jobs><processed></processed></jobs></show>'}\n    else:\n        raise CommandExecutionError(\"The state parameter must be all, pending, or processed.\")\n\n    return __proxy__['panos.call'](query)",
        "sampled": "### Problem Analysis\n\nThe problem is asking to implement a function `get_jobs` that lists all jobs on a device. The function should take an optional parameter `state` which can be one of the following: 'all', 'pending', or 'processed'. \n\n- If `state` is 'all', the function should return all jobs.\n- If `state` is 'pending', the function should return only pending jobs (jobs that are currently in a running or waiting state).\n- If `"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"Return a list of IP addresses.\"\"\"\n    results = []\n    for address in self.addresses:\n      human_readable_address = address.human_readable_address\n      if human_readable_address is not None:\n        results.append(human_readable_address)\n\n    return results",
        "sampled": "**Problem Description**\n========================\n\nGiven a **geologist's subsequence code word** `$s$` of length $n$, represented by a binary string, find all valid IP addresses that can be constructed from it.\n\nA valid IP address is formed by splitting the code word into four parts, and each part should satisfy the following conditions:\n\n- The number of bits in each part should be between 1 and 15 (inclusive).\n- The first part should have at most 3 bits.\n"
    },
    {
        "original": "def ikev2scan(ip, **kwargs):\n    \"\"\"Send a IKEv2 SA to an IP and wait for answers.\"\"\"\n    return sr(IP(dst=ip) / UDP() / IKEv2(init_SPI=RandString(8),\n                                         exch_type=34) / IKEv2_payload_SA(prop=IKEv2_payload_Proposal()), **kwargs)",
        "sampled": "I don't see the problem description in Markdown format. However, I can still help you with a general problem analysis and a possible solution.\n\nLet's assume that this is an edge case in which we have to implement the ikev2scan function which sends an IKEv2 SA (Internet Key Exchange version 2 Security Association) to a given IP address and waits for responses.\n\nHere's a possible implementation of the `ikev2scan` function:\n\n```python\nimport socket\n\ndef i"
    },
    {
        "original": "def list_versions(self, symbol=None, snapshot=None, latest_only=False):\n        \"\"\"\n        Return a list of versions filtered by the passed in parameters.\n\n        Parameters\n        ----------\n        symbol : `str`\n            Symbol to return versions for.  If None returns versions across all\n            symbols in the library.\n        snapshot : `str`\n            Return the versions contained in the named snapshot\n        latest_only : `bool`\n            Only include the latest version for a specific symbol\n\n        Returns\n        -------\n        List of dictionaries describing the discovered versions in the library\n        \"\"\"\n        if symbol is None:\n            symbols = self.list_symbols(snapshot=snapshot)\n        else:\n            symbols = [symbol]\n\n        query = {}\n\n        if snapshot is not None:\n            try:\n                query['parent'] = self._snapshots.find_one({'name': snapshot})['_id']\n            except TypeError:\n                raise NoDataFoundException('No snapshot %s in library %s' % (snapshot, self._arctic_lib.get_name()))\n\n        versions = []\n        snapshots = {ss.get('_id'): ss.get('name') for ss in self._snapshots.find()}\n        for symbol in symbols:\n            query['symbol'] = symbol\n            seen_symbols = set()\n            for version in self._versions.find(query, projection=['symbol', 'version', 'parent', 'metadata.deleted'], sort=[('version', -1)]):\n                if latest_only and version['symbol'] in seen_symbols:\n                    continue\n                seen_symbols.add(version['symbol'])\n                meta = version.get('metadata')\n                versions.append({'symbol': version['symbol'], 'version': version['version'],\n                                 'deleted': meta.get('deleted', False) if meta else False,\n                                 # We return offset-aware datetimes in Local Time.\n                                 'date': ms_to_datetime(datetime_to_ms(version['_id'].generation_time)),\n                                 'snapshots': [snapshots[s] for s in version.get('parent', []) if s in snapshots]})\n        return versions",
        "sampled": "```python\nclass VersionManager:\n    def __init__(self):\n        # Initialize an empty dictionary to store versions for each symbol\n        self.versions = {}\n\n    def list_versions(self, symbol=None, snapshot=None, latest_only=False):\n        \"\"\"\n        Return a list of versions filtered by the passed in parameters.\n\n        Parameters\n        ----------\n        symbol : `str`\n            Symbol to return versions for.  If None returns versions across all\n            symbols in the library.\n        snapshot : `"
    },
    {
        "original": "def get_function_policy(self, function_name):\n        # type: (str) -> Dict[str, Any]\n        \"\"\"Return the function policy for a lambda function.\n\n        This function will extract the policy string as a json document\n        and return the json.loads(...) version of the policy.\n\n        \"\"\"\n        client = self._client('lambda')\n        try:\n            policy = client.get_policy(FunctionName=function_name)\n            return json.loads(policy['Policy'])\n        except client.exceptions.ResourceNotFoundException:\n            return {'Statement': []}",
        "sampled": "```python\nimport json\nimport re\n\nclass FunctionPolicy:\n    def get_function_policy(self, function_name):\n        # type: (str) -> dict\n        \"\"\"\n        Return the function policy for a lambda function.\n\n        This function will extract the policy string as a json document\n        and return the json.loads(...) version of the policy.\n\n        \"\"\"\n        \n        # Assuming you have a dictionary that maps function names to their policies\n        # For example:\n        \n        policies = {\n            '"
    },
    {
        "original": "def get_load(jid):\n    \"\"\"\n    Return the load data that marks a specified jid\n    \"\"\"\n    jid = _escape_jid(jid)\n    conn = _get_conn()\n    if conn is None:\n        return None\n    cur = conn.cursor()\n    sql = ",
        "sampled": "```python\nclass Group:\n    def __init__(self, jid):\n        self.jid = jid\n        self.chats = []\n\n    def add_chat(self, chat):\n        self.chats.append(chat)\n\n\nclass Chat:\n    def __init__(self, cid):\n        self.cid = cid\n        self.users = []\n\n\n    def add_user(self, user_id):\n        self.users.append(user_id)\n\n\nclass InfoSystem:\n    def __init__(self):\n        # data structure to"
    },
    {
        "original": "def best(cls):\n        \"\"\"\n        Select the best ScriptWriter for this environment.\n        \"\"\"\n        if sys.platform == 'win32' or (os.name == 'java' and os._name == 'nt'):\n            return WindowsScriptWriter.best()\n        else:\n            return cls",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\n\nclass ScriptWriter(ABC):\n    @abstractmethod\n    def __str__(self):\n        pass\n\n    def efficiency(self):\n        pass\n\n\nclass WriterA(ScriptWriter):\n    def __str__(self):\n        return \"ScriptWriter A\"\n\n    def efficiency(self):\n        return 10\n\n\nclass WriterB(ScriptWriter):\n    def __str__(self):\n        return \"ScriptWriter B\"\n\n    def efficiency(self):\n        return 8"
    },
    {
        "original": "def xgroup_delconsumer(self, name, groupname, consumername):\n        \"\"\"\n        Remove a specific consumer from a consumer group.\n        Returns the number of pending messages that the consumer had before it\n        was deleted.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to delete\n        \"\"\"\n        return self.execute_command('XGROUP DELCONSUMER', name, groupname,\n                                    consumername)",
        "sampled": "### Problem Analysis\n\nThe given problem seems to be related to a message queue system, where consumers can be part of consumer groups. The `xgroup_delconsumer` function is called to remove a specific consumer from a group, and it's expected to return the number of pending messages that the consumer had before it was deleted.\n\nAssuming this function is part of a class (due to the `self` parameter), we'll write the Python solution for this challenge.\n\n### Python Solution\n\n```python\n"
    },
    {
        "original": "def pre_periodic_filtered(values, block, repeat_after):\n    \"\"\"\n    Blocks the first *block* items from *values*, repeating the block after\n    every *repeat_after* items, if *repeat_after* is non-zero. For example, to\n    discard the first 50 values read from an ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import pre_periodic_filtered\n\n        adc = MCP3008(channel=0)\n\n        for value in pre_periodic_filtered(adc, 50, 0):\n            print(value)\n\n    Or to only display every even item read from an ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import pre_periodic_filtered\n\n        adc = MCP3008(channel=0)\n\n        for value in pre_periodic_filtered(adc, 1, 1):\n            print(value)\n    \"\"\"\n    values = _normalize(values)\n    if block < 1:\n        raise ValueError(\"block must be 1 or larger\")\n    if repeat_after < 0:\n        raise ValueError(\"repeat_after must be 0 or larger\")\n    it = iter(values)\n    try:\n        if repeat_after == 0:\n            for _ in range(block):\n                next(it)\n            while True:\n                yield next(it)\n        else:\n            while True:\n                for _ in range(block):\n                    next(it)\n                for _ in range(repeat_after):\n                    yield next(it)\n    except StopIteration:\n        pass",
        "sampled": "```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef pre_periodic_filtered(values, block, repeat_after):\n    \"\"\"\n    Blocks the first *block* items from *values*, repeating the block after\n    every *repeat_after* items, if *repeat_after* is non-zero.\n    \n    :param values: Generator or iterator of values to be filtered.\n    :param int block: Number of items to skip in a cycle.\n    :param int repeat_after: Number"
    },
    {
        "original": "def skip_format_url(format_, url):\n    \"\"\"\n    Checks whether a give format/url should be skipped and not downloaded.\n\n    @param format_: Filename format (extension).\n    @type format_: str (e.g. html, txt, zip, pdf)\n\n    @param url: URL.\n    @type url: str\n\n    @return: True if format/url should be skipped, False otherwise.\n    @rtype bool\n    \"\"\"\n    # Do not download empty formats\n    if format_ == '':\n        return True\n\n    # Do not download email addresses\n    if ('mailto:' in url) and ('@' in url):\n        return True\n\n    # Is this localhost?\n    parsed = urlparse(url)\n    if parsed.hostname == 'localhost':\n        return True\n\n    # These are trusted manually added formats, do not skip them\n    if RE_VALID_FORMATS.match(format_):\n        return False\n\n    # Simple formats only contain letters, numbers, \"_\" and \"-\"\n    # If this a non simple format?\n    if RE_NON_SIMPLE_FORMAT.match(format_):\n        return True\n\n    # Is this a link to the site root?\n    if parsed.path in ('', '/'):\n        return True\n\n    # Do not skip\n    return False",
        "sampled": "```python\ndef skip_format_url(format_, url):\n    \"\"\"\n    Checks whether a give format/url should be skipped and not downloaded.\n\n    @param format_: Filename format (extension).\n    @type format_: str (e.g. html, txt, zip, pdf)\n\n    @param url: URL.\n    @type url: str\n\n    @return: True if format/url should be skipped, False otherwise.\n    @rtype bool\n    \"\"\"\n\n# List of formats that should be skipped\n"
    },
    {
        "original": "def get_domain_url(url):\n    \"\"\"\n    Use this to convert a url like this:\n    https://blog.xkcd.com/2014/07/22/what-if-book-tour/\n    Into this:\n    https://blog.xkcd.com\n    \"\"\"\n    if \"http://\" not in url and \"https://\" not in url:\n        return url\n    url_header = url.split('://')[0]\n    simple_url = url.split('://')[1]\n    base_url = simple_url.split('/')[0]\n    domain_url = url_header + '://' + base_url\n    return domain_url",
        "sampled": "### Problem Description\nGet the domain URL from a given full URL.\n\n### Input and Output Specifications\n\n*   **Input**: A full URL\n*   **Output**: The domain part of the full URL (without any path or query parameters)\n\n### Python Solution\n\n```python\nfrom urllib.parse import urlparse\n\ndef get_domain_url(url):\n    \"\"\"\n    Use this to convert a url like this:\n    https://blog.xkcd.com/2014/07/22/what-if-book-t"
    },
    {
        "original": "def dlogpdf_link_dr(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the log-likelihood function at y given f, w.r.t shape parameter\n\n        .. math::\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: includes censoring information in dictionary key 'censored'\n        :returns: derivative of likelihood evaluated at points f w.r.t variance parameter\n        :rtype: float\n        \"\"\"\n        # c = Y_metadata['censored']\n        # c = np.zeros((y.shape[0],))\n        c = np.zeros_like(y)\n        if Y_metadata is not None and 'censored' in Y_metadata.keys():\n            c = Y_metadata['censored']\n\n        link_f = inv_link_f #FIXME: Change names consistently...\n        y_link_f = y/link_f\n        log_y_link_f = np.log(y) - np.log(link_f)\n        y_link_f_r = y_link_f**self.r\n\n        #In terms of link_f\n        censored = c*(-y_link_f_r*log_y_link_f/(1 + y_link_f_r))\n        uncensored = (1-c)*(1./self.r + np.log(y) - np.log(link_f) - (2*y_link_f_r*log_y_link_f) / (1 + y_link_f_r))\n\n        dlogpdf_dr = censored + uncensored\n        return dlogpdf_dr",
        "sampled": "```python\nimport numpy as np\n\nclass LikelihoodCalculator:\n    def dlogpdf_link_dr(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the log-likelihood function at y given f, w.r.t shape parameter\n\n        .. math::\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y"
    },
    {
        "original": "def get_p_vals(self, X):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tX : np.array\n\t\t\tArray of word counts, shape (N, 2) where N is the vocab size.  X[:,0] is the\n\t\t\tpositive class, while X[:,1] is the negative class. None by default\n\n\t\tReturns\n\t\t-------\n\t\tnp.array of p-values\n\n\t\t\"\"\"\n\t\tz_scores = self.get_scores(X[:, 0], X[:, 1])\n\t\treturn norm.cdf(z_scores)",
        "sampled": "```python\nimport numpy as np\nfrom scipy.stats import mannwhitneyu as mw\n\nclass Solution:\n    def get_p_vals(self, X=None):\n        \"\"\"\n        Parameters\n        ----------\n        X : np.array\n            Array of word counts, shape (N, 2) where N is the vocab size.  \n            X[:,0] is the positive class, while X[:,1] is the negative class. None by default\n\n        Returns\n        -------\n        np.array of"
    },
    {
        "original": "def settings_view_for_block(block_wrapper, settings_view_factory):\n        \"\"\"\n        Returns the settings view for an arbitrary block.\n\n        Args:\n            block_wrapper (BlockWrapper): The block for which a settings\n                view is to be returned\n            settings_view_factory (SettingsViewFactory): The settings\n                view factory used to create the SettingsView object\n\n        Returns:\n            SettingsView object associated with the block\n        \"\"\"\n        state_root_hash = \\\n            block_wrapper.state_root_hash \\\n            if block_wrapper is not None else None\n\n        return settings_view_factory.create_settings_view(state_root_hash)",
        "sampled": "### Problem Description\n```markdown\n# Settings View Factory\n\n## Problem Description\n\nThe `settings_view_factory` is responsible for creating settings views for different blocks. However, the current implementation does not handle cases where the block wrapper is not associated with a specific block.\n\n## Input and Output Specifications\n\n### Input:\n\n* `block_wrapper`: An instance of `BlockWrapper` that may or may not be associated with a specific block.\n* `settings_view_factory`: An instance of `SettingsViewFactory`"
    },
    {
        "original": "def authenticate(self, *, scopes, **kwargs):\n        \"\"\" Performs the oauth authentication flow resulting in a stored token\n        It uses the credentials passed on instantiation\n\n        :param list[str] scopes: list of protocol user scopes to be converted\n         by the protocol or scope helpers\n        :param kwargs: other configurations to be passed to the\n         Connection instance\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        kwargs.setdefault('token_backend', self.con.token_backend)\n        return oauth_authentication_flow(*self.con.auth, scopes=scopes,\n                                         protocol=self.protocol, **kwargs)",
        "sampled": "### Problem Description\n/oauth authentication challenge\n----------------\u0e4c\n\nTo complete this task, we'll implement an oauth authentication mechanism.\nHere is the required code:\n\n```python\nfrom typing import List\n\nclass OAuthAuthentication:\n    def __init__(self, credentials: dict):\n        self.credentials = credentials\n\n    def authenticate(self, *, scopes: List[str], **kwargs) -> bool:\n        # Assuming a function `get_access_token` to get token with given scopes and kwargs\n        access_token = self.get"
    },
    {
        "original": "def __convertRlocToRouterId(self, xRloc16):\n        \"\"\"mapping Rloc16 to router id\n\n        Args:\n            xRloc16: hex rloc16 short address\n\n        Returns:\n            actual router id allocated by leader\n        \"\"\"\n        routerList = []\n        routerList = self.__sendCommand('router list')[0].split()\n        print routerList\n        print xRloc16\n\n        for index in routerList:\n            router = []\n            cmd = 'router %s' % index\n            router = self.__sendCommand(cmd)\n\n            for line in router:\n                if 'Done' in line:\n                    break\n                elif 'Router ID' in line:\n                    routerid = line.split()[2]\n                elif 'Rloc' in line:\n                    rloc16 = line.split()[1]\n                else:\n                    pass\n\n            # process input rloc16\n            if isinstance(xRloc16, str):\n                rloc16 = '0x' + rloc16\n                if rloc16 == xRloc16:\n                    return routerid\n            elif isinstance(xRloc16, int):\n                if int(rloc16, 16) == xRloc16:\n                    return routerid\n            else:\n                pass\n\n        return None",
        "sampled": "```python\ndef __convertRlocToRouterId(self, xRloc16):\n    \"\"\"\n    mapping Rloc16 to router id\n\n    Args:\n        xRloc16: hex rloc16 short address\n\n    Returns:\n        actual router id allocated by leader\n    \"\"\"\n    \n    # Convert the hexadecimal string to an integer\n    xRloc = int(xRloc16, 16)\n    \n    # Extract the last 32 bits of the integer (assuming it's a"
    },
    {
        "original": "def _get_connect_kwargs(self, host, port, user, password, database_url):\n        \"\"\"\n        Get the params to pass to psycopg2.connect() based on passed-in vals\n        from yaml settings file\n        \"\"\"\n        if database_url:\n            return {'dsn': database_url}\n\n        if not host:\n            raise CheckException(\"Please specify a PgBouncer host to connect to.\")\n\n        if not user:\n            raise CheckException(\"Please specify a user to connect to PgBouncer as.\")\n\n        if host in ('localhost', '127.0.0.1') and password == '':\n            # Use ident method\n            return {'dsn': \"user={} dbname={}\".format(user, self.DB_NAME)}\n\n        if port:\n            return {'host': host, 'user': user, 'password': password, 'database': self.DB_NAME, 'port': port}\n\n        return {'host': host, 'user': user, 'password': password, 'database': self.DB_NAME}",
        "sampled": "```\ndef _get_connect_kwargs(self, host, port, user, password, database_url):\n    \"\"\"\n    Get the params to pass to psycopg2.connect() based on passed-in vals\n    from yaml settings file\n    \"\"\"\n    \n    # Unpack the database URL into its components\n    import urllib.parse as urlparse\n    \n    parsed_database_url = urlparse.urlparse(database_url)\n    \n    # Extract the host and port from the parsed URL (if they exist)\n    kwargs_host = getattr(parsed"
    },
    {
        "original": "def set_salt_view():\n    \"\"\"\n    Helper function that sets the salt design\n    document. Uses get_valid_salt_views and some hardcoded values.\n    \"\"\"\n\n    options = _get_options(ret=None)\n\n    # Create the new object that we will shove in as the design doc.\n    new_doc = {}\n    new_doc['views'] = get_valid_salt_views()\n    new_doc['language'] = \"javascript\"\n\n    # Make the request to update the design doc.\n    _response = _request(\"PUT\",\n                         options['url'] + options['db'] + \"/_design/salt\",\n                         \"application/json\", salt.utils.json.dumps(new_doc))\n    if 'error' in _response:\n        log.warning('Unable to set the salt design document: %s', _response['error'])\n        return False\n    return True",
        "sampled": "```python\ndef get_valid_salt_views():\n    \"\"\"\n    This function returns a list of valid salt views.\n    It can be customized or found using external sources.\n    \"\"\"\n    # For the purpose of this exercise, assume we have the following valid salt views\n    return [\n        {\"view\": \"starburst\", \"base\": 2, \"exponent\": 0},\n        {\"view\": \"ring\", \"base\": 3, \"exponent\": 1},\n        {\"view"
    },
    {
        "original": "def while_not_sync_standby(self, func):\n        \"\"\"Runs specified action while trying to make sure that the node is not assigned synchronous standby status.\n\n        Tags us as not allowed to be a sync standby as we are going to go away, if we currently are wait for\n        leader to notice and pick an alternative one or if the leader changes or goes away we are also free.\n\n        If the connection to DCS fails we run the action anyway, as this is only a hint.\n\n        There is a small race window where this function runs between a master picking us the sync standby and\n        publishing it to the DCS. As the window is rather tiny consequences are holding up commits for one cycle\n        period we don't worry about it here.\"\"\"\n\n        if not self.is_synchronous_mode() or self.patroni.nosync:\n            return func()\n\n        with self._member_state_lock:\n            self._disable_sync += 1\n        try:\n            if self.touch_member():\n                # Master should notice the updated value during the next cycle. We will wait double that, if master\n                # hasn't noticed the value by then not disabling sync replication is not likely to matter.\n                for _ in polling_loop(timeout=self.dcs.loop_wait*2, interval=2):\n                    try:\n                        if not self.is_sync_standby(self.dcs.get_cluster()):\n                            break\n                    except DCSError:\n                        logger.warning(\"Could not get cluster state, skipping synchronous standby disable\")\n                        break\n                    logger.info(\"Waiting for master to release us from synchronous standby\")\n            else:\n                logger.warning(\"Updating member state failed, skipping synchronous standby disable\")\n\n            return func()\n        finally:\n            with self._member_state_lock:\n                self._disable_sync -= 1",
        "sampled": "```python\nimport time\nimport threading\n\nclass Node:\n    def __init__(self):\n        self.is_sync_standby = False\n        self.dcs_connection_active = True\n\n    def while_not_sync_standby(self, func):\n        # Tag us as not allowed to be a sync standby\n        if self.is_sync_standby:\n            print(\"Currently assigned as synchronous standby, waiting for leader to notice and pick an alternative one.\")\n            while self.is_sync_standby:\n                time.sleep"
    },
    {
        "original": "def _cleanup():\n    \"\"\"\n    Make sure that any terminal processes still running when __del__ was called\n    to the waited and cleaned up.\n    \"\"\"\n    for inst in _ACTIVE[:]:\n        res = inst.isalive()\n        if res is not True:\n            try:\n                _ACTIVE.remove(inst)\n            except ValueError:\n                # This can happen if two threads create a new Terminal instance\n                # It's harmless that it was already removed, so ignore.\n                pass",
        "sampled": "```python\nimport os\n\ndef _cleanup():\n    \"\"\"\n    Make sure that any terminal processes still running when __del__ was called\n    to the waited and cleaned up.\n    \"\"\"\n    \n    # Iterate over the current process id\n    for pid in os.listdir('/proc'):\n        try:\n            # Try to get the information of each process\n            pinfo = int(pid)\n            \n            # Skip system processes with pid less than 1000 \n            if pinfo < 1000"
    },
    {
        "original": "def _write_recordio(f, data):\n    \"\"\"Writes a single data point as a RecordIO record to the given file.\"\"\"\n    length = len(data)\n    f.write(struct.pack('I', _kmagic))\n    f.write(struct.pack('I', length))\n    pad = (((length + 3) >> 2) << 2) - length\n    f.write(data)\n    f.write(padding[pad])",
        "sampled": "### Problem Description\n### 331A - DZY Loves Fishing\n\nDZY has gone to the forest to fish. The only problem is, he doesn't know how not to be sad if while fishing there will come another fisherman.\nOf course, our DZY is a very serious and straightforward guy. Therefore on making acquaintance everything at first turns into calculation. So, for instance having found himself near another angler who started fishing at time $t_2$ and finished at time $t"
    },
    {
        "original": "def make_api_method(func):\n    \"\"\"\n    Provides a single entry point for modifying all API methods.\n    For now this is limited to allowing the client object to be modified\n    with an `extra_params` keyword arg to each method, that is then used\n    as the params for each web service request.\n\n    Please note that this is an unsupported feature for advanced use only.\n    It's also currently incompatibile with multiple threads, see GH #160.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        args[0]._extra_params = kwargs.pop(\"extra_params\", None)\n        result = func(*args, **kwargs)\n        try:\n            del args[0]._extra_params\n        except AttributeError:\n            pass\n        return result\n    return wrapper",
        "sampled": "### Problem Description\n\nNo problem is provided. Please share the actual problem description in Markdown format for me to assist you.\n\n### Code\n```python\ndef make_api_method(func):\n    \"\"\"\n    Provides a single entry point for modifying all API methods.\n    For now this is limited to allowing the client object to be modified\n    with an `extra_params` keyword arg to each method, that is then used\n    as the params for each web service request.\n    \n    Args:\n        func:"
    },
    {
        "original": "def gwcalctyp(self):\n        \"\"\"Returns the value of the gwcalctyp input variable.\"\"\"\n        dig0 = str(self._SIGMA_TYPES[self.type])\n        dig1 = str(self._SC_MODES[self.sc_mode])\n        return dig1.strip() + dig0.strip()",
        "sampled": "```python\ndef GWCalcTyp(self):\n    \"\"\"\n    GWRTYP is an integer input variable that holds the type of the equation (Common, Forced, Sequence Equation),\n    and other types determined by some specific application functions.  In most applications\n    when GWCALCTYP is JPAREN = &quot;NS&quot; or GPAREN = &quot;,J&quot; this routine will return Sequence.\n    \n    Returns:\n        str: A string indicating the type of equation.\n    \"\"\"\n"
    },
    {
        "original": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    if self._data_format == DATA_FORMAT_NWC:\n      h_dim = 1\n      two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n      h_dim = 2\n      two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (1,) + self._rate\n\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     strides=two_dim_conv_stride,\n                                     rate=two_dim_conv_rate,\n                                     padding=self._conv_op_padding,\n                                     data_format=two_dim_conv_data_format)\n    outputs = tf.squeeze(outputs, [h_dim])\n    return outputs",
        "sampled": "```python\nimport tensorflow as tf\n\nclass Model:\n    def __init__(self, data_format):\n        self.data_format = data_format\n\n    def _apply_conv(self, inputs, w):\n        \"\"\"\n        Apply a separable_conv2d operation on `inputs` using `w`.\n\n        Args:\n          inputs: A Tensor of shape `data_format` and of type \n                  `tf.float16`, `tf.bfloat16` or `tf.float32`.\n          w: A tuple"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a MetricResponse object from a json dictionary.\"\"\"\n        args = {}\n        if 'aggregations' in _dict:\n            args['aggregations'] = [\n                MetricAggregation._from_dict(x)\n                for x in (_dict.get('aggregations'))\n            ]\n        return cls(**args)",
        "sampled": "Problem Description\n====================\n\n### Problem: Metric Response\n\nGiven a dictionary representing a metric response, create a function to initialize an object of the class `MetricResponse` from this dictionary.\n\nInput and Output Specifications\n==============================\n\n### Input:\n\n* `_dict`: A dictionary representing the metric response.\n\n### Output:\n\n* An instance of the `MetricResponse` class initialized from the input dictionary.\n\nPython Solution\n```python\nclass MetricResponse:\n    def __init__(self, _dict):\n       "
    },
    {
        "original": "def add_heading(self, text=\"\", level=1):\n        \"\"\"Return a heading paragraph newly added to the end of the document.\n\n        The heading paragraph will contain *text* and have its paragraph style\n        determined by *level*. If *level* is 0, the style is set to `Title`. If *level*\n        is 1 (or omitted), `Heading 1` is used. Otherwise the style is set to `Heading\n        {level}`. Raises |ValueError| if *level* is outside the range 0-9.\n        \"\"\"\n        if not 0 <= level <= 9:\n            raise ValueError(\"level must be in range 0-9, got %d\" % level)\n        style = \"Title\" if level == 0 else \"Heading %d\" % level\n        return self.add_paragraph(text, style)",
        "sampled": "```python\nclass Document:\n    def __init__(self):\n        self.headings = []\n\n    def add_heading(self, text=\"\", level=1):\n        \"\"\"Return a heading paragraph newly added to the end of the document.\n\n        The heading paragraph will contain *text* and have its paragraph style\n        determined by *level*. If *level* is 0, the style is set to `Title`. If *level*\n        is 1 (or omitted), `Heading 1`"
    },
    {
        "original": "def _RegisterFlowProcessingHandler(self, handler):\n    \"\"\"Registers a handler to receive flow processing messages.\"\"\"\n    self.flow_handler_stop = False\n    self.flow_handler_thread = threading.Thread(\n        name=\"flow_processing_handler\",\n        target=self._HandleFlowProcessingRequestLoop,\n        args=(handler,))\n    self.flow_handler_thread.daemon = True\n    self.flow_handler_thread.start()",
        "sampled": "```python\nclass FlowProcessingHandler:\n    def __init__(self):\n        self.handlers = []\n\n    def _RegisterFlowProcessingHandler(self, handler):\n        \"\"\"Registers a handler to receive flow processing messages.\"\"\"\n        self.handlers.append(handler)\n\n\nclass FlowProcessor:\n    def __init__(self):\n        self.handler_manager = FlowProcessingHandler()\n\n    def process_flow(self, data: str) -> str:\n        if not data:\n            return 'Invalid input'\n\n        for handler in self.handler_manager.handlers"
    }
]