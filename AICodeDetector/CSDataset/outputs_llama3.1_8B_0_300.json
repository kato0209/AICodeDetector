[
    {
        "original": "def horz_dpi(self):\n        \"\"\"\n        Integer dots per inch for the width of this image. Defaults to 72\n        when not present in the file, as is often the case.\n        \"\"\"\n        pHYs = self._chunks.pHYs\n        if pHYs is None:\n            return 72\n        return self._dpi(pHYs.units_specifier, pHYs.horz_px_per_unit)",
        "sampled": "```python\nclass Image:\n    def __init__(self, width=None, height=None):\n        self.width = width\n        self.height = height\n\n    def horz_dpi(self):\n        \"\"\"\n        Integer dots per inch for the width of this image. Defaults to 72\n        when not present in the file, as is often the case.\n        \n        Returns:\n            int: The horizontal dots per inch of the image.\n        \"\"\"\n        \n# Example usage:\nimage = Image(width="
    },
    {
        "original": "def scrape_metrics(self, endpoint):\n        \"\"\"\n        Poll the data from prometheus and return the metrics as a generator.\n        \"\"\"\n        response = self.poll(endpoint)\n        try:\n            # no dry run if no label joins\n            if not self.label_joins:\n                self._dry_run = False\n            elif not self._watched_labels:\n                # build the _watched_labels set\n                for val in itervalues(self.label_joins):\n                    self._watched_labels.add(val['label_to_match'])\n\n            for metric in self.parse_metric_family(response):\n                yield metric\n\n            # Set dry run off\n            self._dry_run = False\n            # Garbage collect unused mapping and reset active labels\n            for metric, mapping in list(iteritems(self._label_mapping)):\n                for key in list(mapping):\n                    if key not in self._active_label_mapping[metric]:\n                        del self._label_mapping[metric][key]\n            self._active_label_mapping = {}\n        finally:\n            response.close()",
        "sampled": "### Problem Description\n\n**Scraping Prometheus Metrics**\n=====================================\n\nGiven a Prometheus endpoint, scrape the metrics data and return them as a generator.\n\n### Input Specifications\n\n* `endpoint`: The URL of the Prometheus endpoint to scrape metrics from.\n\n### Output Specifications\n\n* A generator that yields dictionaries containing the scraped metrics data.\n\n### Python Solution\n```python\nimport requests\nimport json\nfrom prometheus_client import parse_configuration_from_file, generate_latest, CollectorRegistry\n\ndef scrape_metrics(self, endpoint"
    },
    {
        "original": "def is_a_valid_coordination_geometry(self, mp_symbol=None,\n                                         IUPAC_symbol=None, IUCr_symbol=None,\n                                         name=None, cn=None):\n        \"\"\"\n        Checks whether a given coordination geometry is valid (exists) and whether the parameters are coherent with\n        each other.\n        :param IUPAC_symbol:\n        :param IUCr_symbol:\n        :param name:\n        :param cn:\n        :param mp_symbol: The mp_symbol of the coordination geometry.\n        \"\"\"\n        if name is not None:\n            raise NotImplementedError(\n                'is_a_valid_coordination_geometry not implemented for the name')\n        if mp_symbol is None and IUPAC_symbol is None and IUCr_symbol is None:\n            raise SyntaxError(\n                'missing argument for is_a_valid_coordination_geometry : at least one of mp_symbol, '\n                'IUPAC_symbol and IUCr_symbol must be passed to the function')\n        if mp_symbol is not None:\n            try:\n                cg = self.get_geometry_from_mp_symbol(mp_symbol)\n                if IUPAC_symbol is not None:\n                    if IUPAC_symbol != cg.IUPAC_symbol:\n                        return False\n                if IUCr_symbol is not None:\n                    if IUCr_symbol != cg.IUCr_symbol:\n                        return False\n                if cn is not None:\n                    if int(cn) != int(cg.coordination_number):\n                        return False\n                return True\n            except LookupError:\n                return False\n        elif IUPAC_symbol is not None:\n            try:\n                cg = self.get_geometry_from_IUPAC_symbol(IUPAC_symbol)\n                if IUCr_symbol is not None:\n                    if IUCr_symbol != cg.IUCr_symbol:\n                        return False\n                if cn is not None:\n                    if cn != cg.coordination_number:\n                        return False\n                return True\n            except LookupError:\n                return False\n        elif IUCr_symbol is not None:\n            try:\n                cg = self.get_geometry_from_IUCr_symbol(IUCr_symbol)\n                if cn is not None:\n                    if cn != cg.coordination_number:\n                        return False\n                return True\n            except LookupError:\n                return True\n        raise Exception('Should not be here !')",
        "sampled": "### Problem Description\n### Validate Coordination Geometry\nValidate a coordination geometry given one symbolization and, optionally, the corresponding IUPAC name and/or an alternate symbolization.\n\nA coordination geometry is specified by its `mp_symbol`. Its validity can be cross-checked by providing other information: an IUPAC name (`IUPAC_symbol`), its alternate symbolic notation (`IUCr_symbol`), a human-readable `name`, or the number of binders (`cn`).\n\nThe function returns True"
    },
    {
        "original": "def scan(stream, Loader=Loader):\n    \"\"\"\n    Scan a YAML stream and produce scanning tokens.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_token():\n            yield loader.get_token()\n    finally:\n        loader.dispose()",
        "sampled": "### Problem Description\nNo problem description is provided, instead we have the function definition for `scan`. However, I will assume a typical problem description for this function.\n\n**Problem:** \nGiven a YAML stream, scan it and produce scanning tokens.\n\n### Input and Output Specifications\nNo input and output specifications are provided. However, I will assume the following:\n\n**Input:**\n- A YAML stream\n\n**Output:**\n- Scanning tokens from the YAML stream\n\n### Python Solution\n```python\n"
    },
    {
        "original": "def count(self, strg, case_sensitive=False, *args, **kwargs):\n    \"\"\"Get the count of a word or phrase `s` within this WordList.\n    :param strg: The string to count.\n    :param case_sensitive: A boolean, whether or not the search is case-sensitive.\n    \"\"\"\n    if not case_sensitive:\n        return [word.lower() for word in self].count(strg.lower(), *args,\n                **kwargs)\n    return self._collection.count(strg, *args, **kwargs)",
        "sampled": "### Problem Description\nFind the count of a word or phrase `s` within this WordList.\n```markdown\n## Problem Description\n\nYou are given a string consisting of spaces separated words and a word or phrase to search for. \nYour task is to find the count of occurrences of the given word or phrase in the string.\n\n## Input Specifications\n\n*   A string `strg` containing spaces separated words.\n*   A boolean `case_sensitive` indicating whether the search should be case-sensitive.\n\n"
    },
    {
        "original": "def send(self, agent_id, user_ids, party_ids='',\n             tag_ids='', msg=None):\n        \"\"\"\n        \u901a\u7528\u7684\u6d88\u606f\u53d1\u9001\u63a5\u53e3\u3002msg \u5185\u9700\u8981\u6307\u5b9a msgtype \u548c\u5bf9\u5e94\u7c7b\u578b\u6d88\u606f\u5fc5\u987b\u7684\u5b57\u6bb5\u3002\n        \u5982\u679c\u90e8\u5206\u63a5\u6536\u4eba\u65e0\u6743\u9650\u6216\u4e0d\u5b58\u5728\uff0c\u53d1\u9001\u4ecd\u7136\u6267\u884c\uff0c\u4f46\u4f1a\u8fd4\u56de\u65e0\u6548\u7684\u90e8\u5206\uff08\u5373invaliduser\u6216invalidparty\u6216invalidtag\uff09\uff0c\u5e38\u89c1\u7684\u539f\u56e0\u662f\u63a5\u6536\u4eba\u4e0d\u5728\u5e94\u7528\u7684\u53ef\u89c1\u8303\u56f4\u5185\u3002\n        user_ids\u3001party_ids\u3001tag_ids \u4e0d\u80fd\u540c\u65f6\u4e3a\u7a7a\uff0c\u540e\u9762\u4e0d\u518d\u5f3a\u8c03\u3002\n\n        :param agent_id: \u5fc5\u586b\uff0c\u4f01\u4e1a\u5e94\u7528\u7684id\uff0c\u6574\u578b\u3002\u53ef\u5728\u5e94\u7528\u7684\u8bbe\u7f6e\u9875\u9762\u67e5\u770b\u3002\n        :param user_ids: \u6210\u5458ID\u5217\u8868\u3002\n        :param party_ids: \u90e8\u95e8ID\u5217\u8868\u3002\n        :param tag_ids: \u6807\u7b7eID\u5217\u8868\u3002\n        :param msg: \u53d1\u9001\u6d88\u606f\u7684 dict \u5bf9\u8c61\n        :type msg: dict | None\n        :return: \u63a5\u53e3\u8c03\u7528\u7ed3\u679c\n        \"\"\"\n        msg = msg or {}\n        if isinstance(user_ids, (tuple, list)):\n            user_ids = '|'.join(user_ids)\n        if isinstance(party_ids, (tuple, list)):\n            party_ids = '|'.join(party_ids)\n        if isinstance(tag_ids, (tuple, list)):\n            tag_ids = '|'.join(tag_ids)\n\n        data = {\n            'touser': user_ids,\n            'toparty': party_ids,\n            'totag': tag_ids,\n            'agentid': agent_id\n        }\n        data.update(msg)\n        return self._post('message/send', data=data)",
        "sampled": "```python\ndef send(self, agent_id, user_ids, party_ids='', tag_ids='', msg=None):\n    \"\"\"\n    \u901a\u7528\u7684\u6d88\u606f\u53d1\u9001\u63a5\u53e3\u3002msg \u5185\u9700\u8981\u6307\u5b9a msgtype \u548c\u5bf9\u5e94\u7c7b\u578b\u6d88\u606f\u5fc5\u987b\u7684\u5b57\u6bb5\u3002\n    \u5982\u679c\u90e8\u5206\u63a5\u6536\u4eba\u65e0\u6743\u9650\u6216\u4e0d\u5b58\u5728\uff0c\u53d1\u9001\u4ecd\u7136\u6267\u884c\uff0c\u4f46\u4f1a\u8fd4\u56de\u65e0\u6548\u7684\u90e8\u5206\uff08\u5373invaliduser\u6216invalidparty\u6216invalidtag\uff09\uff0c\u5e38\u89c1\u7684\u539f\u56e0\u662f\u63a5\u6536\u4eba\u4e0d\u5728\u5e94\u7528\u7684\u53ef\u89c1\u8303\u56f4\u5185\u3002\n    user_ids"
    },
    {
        "original": "def _HandleLegacy(self, args, token=None):\n    \"\"\"Retrieves the stats for a hunt.\"\"\"\n    hunt_obj = aff4.FACTORY.Open(\n        args.hunt_id.ToURN(), aff4_type=implementation.GRRHunt, token=token)\n\n    stats = hunt_obj.GetRunner().context.usage_stats\n\n    return ApiGetHuntStatsResult(stats=stats)",
        "sampled": "```python\ndef _HandleLegacy(self, args, token=None):\n    \"\"\"\n    Retrieves the stats for a hunt.\n\n    Args:\n        args (list): A list of arguments.\n        token (str, optional): The token for authentication. Defaults to None.\n\n    Returns:\n        dict: A dictionary containing the stats for a hunt.\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the stats\n    stats = {}\n\n    # Check if the input list is not empty\n    if args"
    },
    {
        "original": "def import_project(self, file, path, namespace=None, overwrite=False,\n                       override_params=None, **kwargs):\n        \"\"\"Import a project from an archive file.\n\n        Args:\n            file: Data or file object containing the project\n            path (str): Name and path for the new project\n            namespace (str): The ID or path of the namespace that the project\n                will be imported to\n            overwrite (bool): If True overwrite an existing project with the\n                same path\n            override_params (dict): Set the specific settings for the project\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the server failed to perform the request\n\n        Returns:\n            dict: A representation of the import status.\n        \"\"\"\n        files = {\n            'file': ('file.tar.gz', file)\n        }\n        data = {\n            'path': path,\n            'overwrite': overwrite\n        }\n        if override_params:\n            for k, v in override_params.items():\n                data['override_params[%s]' % k] = v\n        if namespace:\n            data['namespace'] = namespace\n        return self.gitlab.http_post('/projects/import', post_data=data,\n                                     files=files, **kwargs)",
        "sampled": "```python\nimport json\nfrom typing import Dict, List\n\nclass ProjectImport:\n    def __init__(self):\n        pass\n\n    def import_project(self, file: bytes or dict, path: str, namespace=None, overwrite=False,\n                       override_params=None, **kwargs) -> Dict:\n        \"\"\"\n        Import a project from an archive file.\n\n        Args:\n            file: Data or file object containing the project\n            path (str): Name and path for the new project\n            namespace"
    },
    {
        "original": "def FilterRange(self, start_time=None, stop_time=None):\n    \"\"\"Filter the series to lie between start_time and stop_time.\n\n    Removes all values of the series which are outside of some time range.\n\n    Args:\n      start_time: If set, timestamps before start_time will be dropped.\n      stop_time: If set, timestamps at or past stop_time will be dropped.\n    \"\"\"\n\n    start_time = self._NormalizeTime(start_time)\n    stop_time = self._NormalizeTime(stop_time)\n    self.data = [\n        p for p in self.data\n        if (start_time is None or p[1] >= start_time) and\n        (stop_time is None or p[1] < stop_time)\n    ]",
        "sampled": "```\nclass Series:\n    def __init__(self, data):\n        self.data = data\n\n    def FilterRange(self, start_time=None, stop_time=None):\n        \"\"\"Filter the series to lie between start_time and stop_time.\n\n        Removes all values of the series which are outside of some time range.\n\n        Args:\n          start_time: If set, timestamps before start_time will be dropped.\n          stop_time: If set, timestamps at or past stop_time will be dropped.\n\n        Returns"
    },
    {
        "original": "def bar(x, y, **kwargs):\n    \"\"\"Draws a bar chart in the current context figure.\n\n    Parameters\n    ----------\n\n    x: numpy.ndarray, 1d\n        The x-coordinates of the data points.\n    y: numpy.ndarray, 1d\n        The y-coordinates of the data pints.\n    options: dict (default: {})\n        Options for the scales to be created. If a scale labeled 'x' is\n        required for that mark, options['x'] contains optional keyword\n        arguments for the constructor of the corresponding scale type.\n    axes_options: dict (default: {})\n        Options for the axes to be created. If an axis labeled 'x' is required\n        for that mark, axes_options['x'] contains optional keyword arguments\n        for the constructor of the corresponding axis type.\n    \"\"\"\n    kwargs['x'] = x\n    kwargs['y'] = y\n    return _draw_mark(Bars, **kwargs)",
        "sampled": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef bar(x, y, **kwargs):\n    \"\"\"\n    Draws a bar chart in the current context figure.\n\n    Parameters\n    ----------\n    x: numpy.ndarray, 1d\n        The x-coordinates of the data points.\n    y: numpy.ndarray, 1d\n        The y-coordinates of the data pints.\n    options: dict (default: {})\n        Options for the scales to be created. If"
    },
    {
        "original": "def parse_nodes_coords(osm_response):\n    \"\"\"\n    Parse node coordinates from OSM response. Some nodes are\n    standalone points of interest, others are vertices in \n    polygonal (areal) POIs.\n    \n    Parameters\n    ----------\n    osm_response : string\n        OSM response JSON string\n    \n    Returns\n    -------\n    coords : dict\n        dict of node IDs and their lat, lon coordinates\n    \"\"\"\n\n    coords = {}\n    for result in osm_response['elements']:\n        if 'type' in result and result['type'] == 'node':\n            coords[result['id']] = {'lat': result['lat'],\n                                    'lon': result['lon']}\n    return coords",
        "sampled": "```python\nimport json\n\ndef parse_nodes_coords(osm_response):\n    \"\"\"\n    Parse node coordinates from OSM response. Some nodes are\n    standalone points of interest, others are vertices in \n    polygonal (areal) POIs.\n    \n    Parameters\n    ----------\n    osm_response : string\n        OSM response JSON string\n    \n    Returns\n    -------\n    coords : dict\n        dict of node IDs and their lat, lon coordinates\n\"\"\"\n    \n                # load the json object returned"
    },
    {
        "original": "def _match(self, struct1, struct2, fu, s1_supercell=True, use_rms=False,\n               break_on_match=False):\n        \"\"\"\n        Matches one struct onto the other\n        \"\"\"\n        ratio = fu if s1_supercell else 1/fu\n        if len(struct1) * ratio >= len(struct2):\n            return self._strict_match(\n                struct1, struct2, fu, s1_supercell=s1_supercell,\n                break_on_match=break_on_match, use_rms=use_rms)\n        else:\n            return self._strict_match(\n                struct2, struct1, fu, s1_supercell=(not s1_supercell),\n                break_on_match=break_on_match, use_rms=use_rms)",
        "sampled": "### Problem Description\n\n### Structural Superposition\n intereseted in class/instance-class instance service(not inference)\n One wants to perform a match between two supplied structures: a struct1 and struct2. These are `ase.atoms.Atom` objects, which represent atoms in a crystal structure. The function `_match` should align the two structures by matching their atoms.\n\nThe function takes the following parameters:\n\n- `struct1`: The first structure to be matched.\n- `struct2`: The second structure"
    },
    {
        "original": "def get_configured_consensus_module(block_id, state_view):\n        \"\"\"Returns the consensus_module based on the consensus module set by\n        the \"sawtooth_settings\" transaction family.\n\n        Args:\n            block_id (str): the block id associated with the current state_view\n            state_view (:obj:`StateView`): the current state view to use for\n                setting values\n        Raises:\n            UnknownConsensusModuleError: Thrown when an invalid consensus\n                module has been configured.\n        \"\"\"\n        settings_view = SettingsView(state_view)\n\n        default_consensus = \\\n            'genesis' if block_id == NULL_BLOCK_IDENTIFIER else 'devmode'\n        consensus_module_name = settings_view.get_setting(\n            'sawtooth.consensus.algorithm', default_value=default_consensus)\n        return ConsensusFactory.get_consensus_module(\n            consensus_module_name)",
        "sampled": "### Problem Description\nGet Configured Consensus Module\n================================\n\n*To understand the problem, let's break it down:*\n\n- We need a function that retrieves the consensus module associated with a given block id.\n- The consensus module is determined by the \"sawtooth_settings\" transaction family.\n- If an invalid consensus module has been configured, it raises an UnknownConsensusModuleError.\n\n### Input\n* `block_id` (str): The block id associated with the current state view"
    },
    {
        "original": "def combine_relevance_tables(relevance_tables):\n    \"\"\"\n    Create a combined relevance table out of a list of relevance tables,\n    aggregating the p-values and the relevances.\n\n    :param relevance_tables: A list of relevance tables\n    :type relevance_tables: List[pd.DataFrame]\n    :return: The combined relevance table\n    :rtype: pandas.DataFrame\n    \"\"\"\n    def _combine(a, b):\n        a.relevant |= b.relevant\n        a.p_value = a.p_value.combine(b.p_value, min, 1)\n        return a\n\n    return reduce(_combine, relevance_tables)",
        "sampled": "```python\nimport pandas as pd\n\ndef combine_relevance_tables(relevance_tables):\n    \"\"\"\n    Create a combined relevance table out of a list of relevance tables,\n    aggregating the p-values and the relevances.\n\n    :param relevance_tables: A list of relevance tables\n    :type relevance_tables: List[pd.DataFrame]\n    :return: The combined relevance table\n    :rtype: pandas.DataFrame\n    \"\"\"\n\n    # Initialize an empty dictionary to store the aggregated values for each column\n"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'document') and self.document is not None:\n            _dict['document'] = self.document\n        if hasattr(self, 'targets') and self.targets is not None:\n            _dict['targets'] = self.targets\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n\n    def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        return {\n            'n': self.n,\n            'k': self.k\n        }\n\ndef main():\n    solution = Solution()\n    solution.n = int(input(\"Enter the value of n: \"))\n    solution.k = int(input(\"Enter the value of k: \"))\n    \n"
    },
    {
        "original": "def GetValuesForAttribute(self, attribute, only_one=False):\n    \"\"\"Returns a list of values from this attribute.\"\"\"\n    if not only_one and self.age_policy == NEWEST_TIME:\n      raise ValueError(\"Attempting to read all attribute versions for an \"\n                       \"object opened for NEWEST_TIME. This is probably \"\n                       \"not what you want.\")\n\n    if attribute is None:\n      return []\n\n    elif isinstance(attribute, string_types):\n      attribute = Attribute.GetAttributeByName(attribute)\n\n    return attribute.GetValues(self)",
        "sampled": "```python\ndef get_values_for_attribute(attribute, only_one=False):\n    \"\"\"\n    Tuple attribute example: (value1, value2)\n    \n    Returns a list of values from this attribute. \n    If only_one is True and more than one Tuple Object exists then It raises ValueError.\n    \n    Parameters:\n    ----------\n    attribute : tuple\n        A tuple containing values.\n        \n    only_one : bool, optional\n        If True then it will return the first element of the tuple if multiple tuples"
    },
    {
        "original": "def convert_timestamp(timestamp):\n    \"\"\"\n    Converts bokehJS timestamp to datetime64.\n    \"\"\"\n    datetime = dt.datetime.utcfromtimestamp(timestamp/1000.)\n    return np.datetime64(datetime.replace(tzinfo=None))",
        "sampled": "```python\nimport numpy as np\n\ndef convert_timestamp(timestamp):\n    \"\"\"\n    Converts bokehJS timestamp to datetime64.\n    \n    Parameters:\n    timestamp (int): BokehJS timestamp\n    \n    Returns:\n    np.datetime64: Converted datetime64 object\n    \"\"\"\n    \n    # Convert the timestamp from milliseconds to seconds\n    seconds = int(timestamp / 1000)\n    \n    # Create a numpy datetime object from the seconds and nanoseconds (0)\n    dt = np.datetime64(f'"
    },
    {
        "original": "def _SignedBinaryURNFromID(binary_id\n                          ):\n  \"\"\"Converts a SignedBinaryID to the equivalent AFF4 URN.\"\"\"\n  binary_type = binary_id.binary_type\n  if binary_type == rdf_objects.SignedBinaryID.BinaryType.PYTHON_HACK:\n    return GetAFF4PythonHackRoot().Add(binary_id.path)\n  elif binary_type == rdf_objects.SignedBinaryID.BinaryType.EXECUTABLE:\n    return GetAFF4ExecutablesRoot().Add(binary_id.path)\n  else:\n    raise ValueError(\"Unknown binary type %s.\" % binary_type)",
        "sampled": "### Problem Description\nConvert a Signed Binary ID to the equivalent AFF4 URN.\n\n### Input Specifications\n* `binary_id`: A signed binary ID\n\n### Output Specifications\n* The equivalent AFF4 URN of the input `binary_id`\n\n### AFF4 URN Format\nThe format of an AFF4 urn is: aff4:/dev/<creatorUUID>/<chunkUUID>\n\n### Python Solution\n\n```python\ndef _SignedBinaryURNFromID(binary_id):\n    \"\"\"\n    Converts a SignedBinary"
    },
    {
        "original": "def clear(self):\n        \"\"\"\n        Calls `_clear` abstract method which must be implemented by descendants.\n\n        :raises: GPflowError exception when parent of the node is built.\n        \"\"\"\n        parent = self.parent\n        if parent is not self and parent.is_built_coherence(self.graph) is Build.YES:\n            raise GPflowError('Clear method cannot be started. Upper nodes are built.')\n        self._clear()",
        "sampled": "### Problem Description\n\n**Problem:** \n\nGiven a set of intervals `[(a1, b1), (a2, b2), ..., (an, bn)]`, consider a set of points `x = [x1, x2, ..., xn]`. For each point `xi`, assign a binary label yi as 1 if xi falls within some interval ai to bi and 0 otherwise. This problem is known as \"Interval Labeling\" in the literature.\n\n**Goal:** Write"
    },
    {
        "original": "def _WritePartial(self, data):\n    \"\"\"Writes at most one chunk of data.\"\"\"\n\n    chunk = self.offset // self.chunksize\n    chunk_offset = self.offset % self.chunksize\n    data = utils.SmartStr(data)\n\n    available_to_write = min(len(data), self.chunksize - chunk_offset)\n\n    fd = self._GetChunkForWriting(chunk)\n    fd.seek(chunk_offset)\n\n    fd.write(data[:available_to_write])\n    self.offset += available_to_write\n\n    return data[available_to_write:]",
        "sampled": "### Problem Description\n```markdown\n# Haybale Cutting Problem\n\nYou have two possible haybales to cut, and you can get a reward of $B_i$ dollars for cutting the i-th haybale.\n\nUnfortunately, after cutting the first haybale with cost $C_1$, you'll input in another cost for obtaining and transporting a secondary machine outside. The second haybale is same as when initially available but this time it requires some fixed extra transportation costs $C_a"
    },
    {
        "original": "def disconnect_channel(self, destination_id):\n        \"\"\" Disconnect a channel with destination_id. \"\"\"\n        if destination_id in self._open_channels:\n            try:\n                self.send_message(\n                    destination_id, NS_CONNECTION,\n                    {MESSAGE_TYPE: TYPE_CLOSE, 'origin': {}},\n                    no_add_request_id=True, force=True)\n            except NotConnected:\n                pass\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\"[%s:%s] Exception\",\n                                      self.fn or self.host, self.port)\n\n            self._open_channels.remove(destination_id)\n\n            self.handle_channel_disconnected()",
        "sampled": "# Problem Description\nDisconnect a Channel\n=====================\n\n### Problem Statement\n\nYou are given a network of channels, where each channel has a unique destination ID. You need to disconnect a channel with the given destination ID.\n\n### Input Specifications\n\n*   `destination_id`: The ID of the channel to be disconnected.\n\n### Output Specifications\n\n*   None (The function should modify the internal state of the object)\n\n## Python Solution\n```python\nclass Network:\n    def __init__(self):\n       "
    },
    {
        "original": "def Write(self, grr_message):\n    \"\"\"Write the message into the transaction log.\"\"\"\n    grr_message = grr_message.SerializeToString()\n\n    try:\n      with io.open(self.logfile, \"wb\") as fd:\n        fd.write(grr_message)\n    except (IOError, OSError):\n      # Check if we're missing directories and try to create them.\n      if not os.path.isdir(os.path.dirname(self.logfile)):\n        try:\n          os.makedirs(os.path.dirname(self.logfile))\n          with io.open(self.logfile, \"wb\") as fd:\n            fd.write(grr_message)\n        except (IOError, OSError):\n          logging.exception(\"Couldn't write nanny transaction log to %s\",\n                            self.logfile)",
        "sampled": "```markdown\n### Problem Description\nGiven an integer `n`, return the number of trailing zeros in the factorial of `n`.\n### Input and Output Specifications\n* Input: An integer `n`\n* Output: An integer representing the number of trailing zeros in `n!`\n\n### Constraints\n* 0 <= n <= 10^8\n\n### Solution\n\n```python\nclass Solution:\n    def find_factorial_zeros(self, num):\n        \"\"\"Return the number of trailing zeros in num!"
    },
    {
        "original": "def generate_authors(git_dir):\n    \"\"\"Create AUTHORS file using git commits.\"\"\"\n    authors = []\n    emails = []\n    git_log_cmd = ['git', 'log', '--format=%aN|%aE']\n    tmp_authors = _run_shell_command(git_log_cmd, git_dir).split('\\n')\n    for author_str in tmp_authors:\n        author, email = author_str.split('|')\n        author = author.strip()\n        email = email.strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    co_authors_raw = _run_shell_command(['git', 'log'], git_dir)\n    co_authors = re.findall('Co-authored-by:.+', co_authors_raw,\n                            re.MULTILINE)\n    co_authors = [signed.split(\":\", 1)[1].strip().split('<')\n                  for signed in co_authors if signed]\n    for author_str in co_authors:\n        author, email = author_str.split('<')\n        author = author.strip()\n        email = email[:-1].strip()\n        if author.lower() not in [x.lower() for x in authors]:\n            if email.lower() not in [x.lower() for x in emails]:\n                authors.append(author)\n                emails.append(email)\n    authors = sorted(set(authors))\n    return authors",
        "sampled": "```python\nimport os\nimport subprocess\n\ndef generate_authors(git_dir):\n    \"\"\"\n    Create AUTHORS file using git commits.\n    \n    The function takes a directory path as input, navigates to it, and uses the git log command \n    to extract information about all commits. It then parses this information to create an AUTHORS file.\n    \n    Parameters:\n    git_dir (str): The path to the Git repository directory.\n    \n    Returns:\n    None\n    \"\"\"\n\n    # Navigate to"
    },
    {
        "original": "def from_list(cls, terms_list, coefficient=1.0):\n        \"\"\"\n        Allocates a Pauli Term from a list of operators and indices. This is more efficient than\n        multiplying together individual terms.\n\n        :param list terms_list: A list of tuples, e.g. [(\"X\", 0), (\"Y\", 1)]\n        :return: PauliTerm\n        \"\"\"\n        if not all([isinstance(op, tuple) for op in terms_list]):\n            raise TypeError(\"The type of terms_list should be a list of (name, index) \"\n                            \"tuples suitable for PauliTerm().\")\n\n        pterm = PauliTerm(\"I\", 0)\n        assert all([op[0] in PAULI_OPS for op in terms_list])\n\n        indices = [op[1] for op in terms_list]\n        assert all(_valid_qubit(index) for index in indices)\n\n        # this is because from_list doesn't call simplify in order to be more efficient.\n        if len(set(indices)) != len(indices):\n            raise ValueError(\"Elements of PauliTerm that are allocated using from_list must \"\n                             \"be on disjoint qubits. Use PauliTerm multiplication to simplify \"\n                             \"terms instead.\")\n\n        for op, index in terms_list:\n            if op != \"I\":\n                pterm._ops[index] = op\n        if not isinstance(coefficient, Number):\n            raise ValueError(\"coefficient of PauliTerm must be a Number.\")\n        pterm.coefficient = complex(coefficient)\n        return pterm",
        "sampled": "### Problem Description\nAllocating a Pauli Term from a list of operators and indices.\n\n### Input and Output Specifications\n- **Input**: \n  - `terms_list`: A list of tuples, where each tuple contains a Pauli operator and an index.\n  - `coefficient`: The coefficient of the Pauli term (default is 1.0)\n- **Output**: A PauliTerm object.\n\n### Python Solution\n```python\nclass PauliTerm:\n    def __init__("
    },
    {
        "original": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    if self._data_format == DATA_FORMAT_NWC:\n      h_dim = 1\n      two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n      h_dim = 2\n      two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (1,) + self._rate\n\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     strides=two_dim_conv_stride,\n                                     rate=two_dim_conv_rate,\n                                     padding=self._conv_op_padding,\n                                     data_format=two_dim_conv_data_format)\n    outputs = tf.squeeze(outputs, [h_dim])\n    return outputs",
        "sampled": "### Problem Description\nApply a separable convolution operation on `inputs` using the given weight matrices.\n\n### Input and Output Specifications\n\n- **Inputs**:\n  - `inputs`: A Tensor of shape `data_format` and of type `tf.float16`, `tf.bfloat16` or `tf.float32`.\n  - `w`: A tuple of two weight matrices, the first being the depthwise weight matrix and the second being the pointwise weight matrix.\n\n- **Output**:\n "
    },
    {
        "original": "def _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    count = 0\n    log.debug('Waiting for asynchronous operation to complete')\n    result = conn.get_operation_status(request_id)\n    while result.status == 'InProgress':\n        count = count + 1\n        if count > 120:\n            raise ValueError('Timed out waiting for asynchronous operation to complete.')\n        time.sleep(5)\n        result = conn.get_operation_status(request_id)\n\n    if result.status != 'Succeeded':\n        raise AzureException('Operation failed. {message} ({code})'\n                                .format(message=result.error.message,\n                                        code=result.error.code))",
        "sampled": "### Problem Description\n### \n### Problem: Wait for an asynchronous operation to complete\n### \n### Description:\n### You are given a connection object `conn` and a request ID `request_id`. \n### The function `_wait_for_async` should wait for the asynchronous operation associated with the given request ID to complete.\n### \n### Input:\n### - `conn`: A connection object\n### - `request_id`: The ID of the asynchronous operation to wait for\n### \n### Output:\n###"
    },
    {
        "original": "def is_armable(self):\n        \"\"\"\n        Returns ``True`` if the vehicle is ready to arm, false otherwise (``Boolean``).\n\n        This attribute wraps a number of pre-arm checks, ensuring that the vehicle has booted,\n        has a good GPS fix, and that the EKF pre-arm is complete.\n        \"\"\"\n        # check that mode is not INITIALSING\n        # check that we have a GPS fix\n        # check that EKF pre-arm is complete\n        return self.mode != 'INITIALISING' and (self.gps_0.fix_type is not None and self.gps_0.fix_type > 1) and self._ekf_predposhorizabs",
        "sampled": "```python\nclass Vehicle:\n    def __init__(self):\n        # Assume the following variables are instance variables\n        self.booted = False\n        self.gps_fix = False\n        self.ekf_pre_arm_complete = False\n\n    def is_armable(self):\n        \"\"\"\n        Returns True if the vehicle is ready to arm, false otherwise (Boolean).\n        \n        This attribute wraps a number of pre-arm checks, ensuring that the vehicle has booted,\n        has a good GPS fix"
    },
    {
        "original": "def stop_recording(self):\n        \"\"\"Stop recording from the audio source.\"\"\"\n        self._stop_recording.set()\n        with self._source_lock:\n            self._source.stop()\n        self._recording = False",
        "sampled": "### Problem Description\n### Audio Recording System\n\nYou are tasked with implementing an audio recording system. The system has a method `stop_recording` which should stop the recording from the audio source.\n\n### Input and Output Specifications\nNo specific input or output is mentioned in the problem description. However, we can assume that the `stop_recording` method should be implemented in a class that has access to the audio source.\n\n### Python Solution\n```python\nimport pyaudio\nimport wave\n\nclass Audio"
    },
    {
        "original": "def Seek(self, offset, whence=os.SEEK_SET):\n    \"\"\"Moves the reading cursor.\"\"\"\n\n    if whence == os.SEEK_SET:\n      self._offset = offset\n    elif whence == os.SEEK_CUR:\n      self._offset += offset\n    elif whence == os.SEEK_END:\n      self._offset = self._length + offset\n    else:\n      raise ValueError(\"Invalid whence argument: %s\" % whence)",
        "sampled": "### Problem Description\n### ==================\n### File Pointer Movement\n\nYou are given a file pointer that can be moved using the `Seek` method. The `Seek` method takes two parameters: `offset` and `whence`. The `offset` is the number of bytes to move the file pointer, and `whence` is the reference point for the `offset`.\n\nThe `whence` parameter can take one of the following values:\n\n- `os.SEEK_SET`: The `offset`"
    },
    {
        "original": "def _create_empty_run(\n        self, status=RunStatus.FINISHED, status_description=None\n    ) -> Run:\n        \"\"\"setting boilerplate when creating a Run object\"\"\"\n        run = Run(\n            job_id=self.summary[\"job_id\"],\n            issue_instances=[],\n            date=datetime.datetime.now(),\n            status=status,\n            status_description=status_description,\n            repository=self.summary[\"repository\"],\n            branch=self.summary[\"branch\"],\n            commit_hash=self.summary[\"commit_hash\"],\n            kind=self.summary[\"run_kind\"],\n        )\n        return run",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\n\nclass RunStatus(Enum):\n    FINISHED = 1\n    PENDING = 2\n    CANCELLED = 3\n\nclass Run(ABC):\n    def __init__(self, status=RunStatus.FINISHED, status_description=None):\n        self.status = status\n        self.status_description = status_description\n\ndef _create_empty_run(\n    self, status=RunStatus.FINISHED, status_description"
    },
    {
        "original": "def sudo(self, command, **kwargs):\n        \"\"\"\n        Execute a shell command, via ``sudo``, on the remote end.\n\n        This method is identical to `invoke.context.Context.sudo` in every way,\n        except in that -- like `run` -- it honors per-host/per-connection\n        configuration overrides in addition to the generic/global ones. Thus,\n        for example, per-host sudo passwords may be configured.\n\n        .. versionadded:: 2.0\n        \"\"\"\n        runner = self.config.runners.remote(self)\n        return self._sudo(runner, command, **kwargs)",
        "sampled": "### Problem Description\nNo problem description is provided. However, based on the function definition, it appears to be a part of a larger system that allows remote execution of shell commands via `sudo`. The function seems to be designed to handle configuration overrides for specific hosts or connections.\n\n### Input and Output Specifications\nNo input and output specifications are provided.\n\n### Python Solution\n\n```python\nclass RemoteExecutor:\n    def __init__(self):\n        # Initialize configuration overrides for hosts/connections\n        self.host"
    },
    {
        "original": "def horizontal_line(self,\n                        y: Union[int, float],\n                        x1: Union[int, float],\n                        x2: Union[int, float],\n                        emphasize: bool = False\n                        ) -> None:\n        \"\"\"Adds a line from (x1, y) to (x2, y).\"\"\"\n        x1, x2 = sorted([x1, x2])\n        self.horizontal_lines.append(_HorizontalLine(y, x1, x2, emphasize))",
        "sampled": "```python\nfrom typing import Union\n\nclass DrawingCanvas:\n    def __init__(self):\n        self.lines = []\n\n    def horizontal_line(self, y: Union[int, float], x1: Union[int, float], x2: Union[int, float], emphasize: bool = False) -> None:\n        \"\"\"Adds a line from (x1, y) to (x2, y).\"\"\"\n        \n        # Create a dictionary to represent the line\n        line = {\n            'type"
    },
    {
        "original": "def _right_zero_blocks(self, r):\n    \"\"\"Number of blocks with zeros from the right in block row `r`.\"\"\"\n    if not self._include_off_diagonal:\n      return self._block_rows - r - 1\n    elif self._upper:\n      return 0\n    elif self._include_diagonal:\n      return self._block_rows - r - 1\n    else:\n      return self._block_rows - r",
        "sampled": "```python\ndef num_ofblocks_end_zeroes(block_matrix):\n    \"\"\"\n    Counts the number of right zero blocks in every sub-matrix excluding borders.\n\n    Args:\n        block_matrix (list of lists): A 2D array representing a matrix of such rectangles.\n            A subsequent row is always possible to appear only on a higher or the same level than the previous one.\n\n    Returns:\n        list: The number of blocks with zeros from the right in each block row.\n    \"\"\"\n    \n   "
    },
    {
        "original": "def Record(self, value):\n    \"\"\"Records given value.\"\"\"\n    self.sum += value\n    self.count += 1\n\n    pos = bisect.bisect(self.bins, value) - 1\n    if pos < 0:\n      pos = 0\n    elif pos == len(self.bins):\n      pos = len(self.bins) - 1\n\n    self.heights[pos] += 1",
        "sampled": "### Problem Description\n#### 1. Array Reordering\n\nGiven an array of integers, reorder it such that all even numbers are placed on the left and odd numbers are placed on the right.\n\n### Input and Output Specifications\n\n*   **Input**: An array of integers.\n*   **Output**: The reordered array with all even numbers on the left and odd numbers on the right.\n\n### Python Solution\n```python\nclass Solution:\n    def record(self, value):\n        \"\"\"Records given value.\"\"\"\n"
    },
    {
        "original": "def next(self):\n        \"\"\"Advance the cursor.\"\"\"\n        if self.__empty:\n            raise StopIteration\n        if len(self.__data) or self._refresh():\n            if self.__manipulate:\n                _db = self.__collection.database\n                return _db._fix_outgoing(self.__data.popleft(),\n                                         self.__collection)\n            else:\n                return self.__data.popleft()\n        else:\n            raise StopIteration",
        "sampled": "```python\nclass Solution:\n    def stoneGame(self, piles: List[int]):\n        n = len(piles)\n        dp = [[0] * n for _ in range(n)]\n        \n        for gap in range(1, n):\n            for i in range(n - gap):\n                j = i + gap\n                dp[i][j] = max(piles[i] - dp[i+1][j], piles[j] - dp[i][j-1])\n        \n        return True"
    },
    {
        "original": "def get_statuses(self):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/statuses/:ref <http://developer.github.com/v3/repos/statuses>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.CommitStatus.CommitStatus`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.CommitStatus.CommitStatus,\n            self._requester,\n            self._parentUrl(self._parentUrl(self.url)) + \"/statuses/\" + self.sha,\n            None\n        )",
        "sampled": "```python\nimport requests\n\nclass GitHubAPI:\n    def __init__(self, owner, repo):\n        self.owner = owner\n        self.repo = repo\n\n    def get_statuses(self):\n        url = f'https://api.github.com/repos/{self.owner}/{self.repo}/statuses/{self.get_ref()}'\n        response = requests.get(url)\n        response.raise_for_status()\n        return [CommitStatus(status) for status in response.json()]\n\n    def get_ref(self):\n        # Assuming we have"
    },
    {
        "original": "def setting_address(key):\n        \"\"\"Computes the radix address for the given setting key.\n\n        Keys are broken into four parts, based on the dots in the string. For\n        example, the key `a.b.c` address is computed based on `a`, `b`, `c` and\n        the empty string. A longer key, for example `a.b.c.d.e`, is still\n        broken into four parts, but the remaining pieces are in the last part:\n        `a`, `b`, `c` and `d.e`.\n\n        Each of these peices has a short hash computed (the first 16 characters\n        of its SHA256 hash in hex), and is joined into a single address, with\n        the config namespace (`000000`) added at the beginning.\n\n        Args:\n            key (str): the setting key\n        Returns:\n            str: the computed address\n        \"\"\"\n        # split the key into 4 parts, maximum\n        key_parts = key.split('.', maxsplit=_MAX_KEY_PARTS - 1)\n        # compute the short hash of each part\n        addr_parts = [_short_hash(x.encode()) for x in key_parts]\n        # pad the parts with the empty hash, if needed\n        addr_parts.extend([_EMPTY_PART] * (_MAX_KEY_PARTS - len(addr_parts)))\n\n        return CONFIG_STATE_NAMESPACE + ''.join(addr_parts)",
        "sampled": "```python\nimport hashlib\n\ndef setting_address(key):\n    \"\"\"\n    Computes the radix address for the given setting key.\n\n    Keys are broken into four parts, based on the dots in the string. For\n    example, the key `a.b.c` address is computed based on `a`, `b`, `c` and\n    the empty string. A longer key, for example `a.b.c.d.e`, is still\n    broken into four parts, but the remaining pieces are"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'document') and self.document is not None:\n            _dict['document'] = self.document._to_dict()\n        if hasattr(self, 'model_id') and self.model_id is not None:\n            _dict['model_id'] = self.model_id\n        if hasattr(self, 'model_version') and self.model_version is not None:\n            _dict['model_version'] = self.model_version\n        if hasattr(self, 'tables') and self.tables is not None:\n            _dict['tables'] = [x._to_dict() for x in self.tables]\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def medianSlidingWindow(self, nums: list[int], k: int) -> list[float]:\n        \"\"\"\n        Given an integer array `nums` and an integer `k`, return the median of the element subset of `nums` where the element value is between `lo` and `hi` (inclusive).\n        \n        The result should be a float.\n        \n        :param nums: A list of integers\n        :type nums: List[int]\n        :"
    },
    {
        "original": "def _pack(cls, tensors):\n    \"\"\"Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\"\"\"\n    if not tensors:\n      return None\n    elif len(tensors) == 1:\n      return array_ops.reshape(tensors[0], [-1])\n    else:\n      flattened = [array_ops.reshape(tensor, [-1]) for tensor in tensors]\n      return array_ops.concat(flattened, 0)",
        "sampled": "```python\nimport numpy as np\n\nclass Tensor:\n    def __init__(self, data):\n        self.data = np.array(data)\n\n    def __repr__(self):\n        return f'Tensor({self.data})'\n\ndef _pack(cls, tensors):\n    \"\"\"\n    Pack a list of `Tensor`s into a single, flattened, rank-1 `Tensor`.\n    \n    Args:\n        cls (class): The class of the tensor.\n        tensors (list): A list of `Tensor`"
    },
    {
        "original": "def copy_multireddit(self, from_redditor, from_name, to_name=None,\n                         *args, **kwargs):\n        \"\"\"Copy a multireddit.\n\n        :param from_redditor: The username or Redditor object for the user\n            who owns the original multireddit\n        :param from_name: The name of the multireddit, belonging to\n            from_redditor\n        :param to_name: The name to copy the multireddit as. If None, uses\n            the name of the original\n\n        The additional parameters are passed directly into\n        :meth:`~praw.__init__.BaseReddit.request_json`\n\n        \"\"\"\n        if to_name is None:\n            to_name = from_name\n\n        from_multipath = self.MULTI_PATH.format(from_redditor, from_name)\n        to_multipath = self.MULTI_PATH.format(self.user.name, to_name)\n        data = {'display_name': to_name,\n                'from': from_multipath,\n                'to': to_multipath}\n        return self.request_json(self.config['multireddit_copy'], data=data,\n                                 *args, **kwargs)",
        "sampled": "```python\ndef copy_multireddit(self, from_redditor, from_name, to_name=None, *args, **kwargs):\n    \"\"\"\n    Copy a multireddit.\n\n    :param from_redditor: The username or Redditor object for the user \n        who owns the original multireddit\n    :param from_name: The name of the multireddit, belonging to \n        from_redditor\n    :param to_name: The name to copy the multire"
    },
    {
        "original": "def get_host_datastore_system(host_ref, hostname=None):\n    \"\"\"\n    Returns a host's datastore system\n\n    host_ref\n        Reference to the ESXi host\n\n    hostname\n        Name of the host. This argument is optional.\n    \"\"\"\n\n    if not hostname:\n        hostname = get_managed_object_name(host_ref)\n    service_instance = get_service_instance_from_managed_object(host_ref)\n    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(\n        path='configManager.datastoreSystem',\n        type=vim.HostSystem,\n        skip=False)\n    objs = get_mors_with_properties(service_instance,\n                                    vim.HostDatastoreSystem,\n                                    property_list=['datastore'],\n                                    container_ref=host_ref,\n                                    traversal_spec=traversal_spec)\n    if not objs:\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'Host\\'s \\'{0}\\' datastore system was not retrieved'\n            ''.format(hostname))\n    log.trace('[%s] Retrieved datastore system', hostname)\n    return objs[0]['object']",
        "sampled": "# Problem Description\nGiven a reference to an ESXi host, return the datastore system associated with it.\n\n# Input Specifications\n- `host_ref`: Reference to the ESXi host\n- `hostname`: Name of the host (optional)\n\n# Output Specifications\nReturns a host's datastore system\n\n```python\ndef get_host_datastore_system(host_ref, hostname=None):\n    \"\"\"\n    Returns a host's datastore system\n\n    Parameters:\n    ----------\n    host_ref : Reference to the ESXi host\n"
    },
    {
        "original": "def microsoft(self, key, x86=False):\n        \"\"\"\n        Return key in Microsoft software registry.\n\n        Parameters\n        ----------\n        key: str\n            Registry key path where look.\n        x86: str\n            Force x86 software registry.\n\n        Return\n        ------\n        str: value\n        \"\"\"\n        node64 = '' if self.pi.current_is_x86() or x86 else 'Wow6432Node'\n        return os.path.join('Software', node64, 'Microsoft', key)",
        "sampled": "```python\nimport ctypes\nfrom ctypes import wintypes\n\nclass MicrosoftRegistry:\n    def __init__(self):\n        self.RegConnectRegistry = ctypes.windll.advapi32.RegConnectRegistryW\n        self.RegQueryValueEx = ctypes.windll.advapi32.RegQueryValueExW\n        self.HKEY_LOCAL_MACHINE = 0x80000002\n\n    def microsoft(self, key, x86=False):\n        \"\"\"\n        Return key in Microsoft software registry.\n\n        Parameters\n        ----------\n"
    },
    {
        "original": "def guess_format(text, ext):\n    \"\"\"Guess the format and format options of the file, given its extension and content\"\"\"\n    lines = text.splitlines()\n\n    metadata = read_metadata(text, ext)\n\n    if ('jupytext' in metadata and set(metadata['jupytext'])\n            .difference(['encoding', 'executable', 'main_language'])) or \\\n            set(metadata).difference(['jupytext']):\n        return format_name_for_ext(metadata, ext), {}\n\n    # Is this a Hydrogen-like script?\n    # Or a Sphinx-gallery script?\n    if ext in _SCRIPT_EXTENSIONS:\n        comment = _SCRIPT_EXTENSIONS[ext]['comment']\n        twenty_hash = ''.join(['#'] * 20)\n        magic_re = re.compile(r'^(%|%%|%%%)[a-zA-Z]')\n        double_percent_re = re.compile(r'^{}( %%|%%)$'.format(comment))\n        double_percent_and_space_re = re.compile(r'^{}( %%|%%)\\s'.format(comment))\n        nbconvert_script_re = re.compile(r'^{}( <codecell>| In\\[[0-9 ]*\\]:?)'.format(comment))\n        vim_folding_markers_re = re.compile(r'^{}\\s*'.format(comment) + '{{{')\n        vscode_folding_markers_re = re.compile(r'^{}\\s*region'.format(comment))\n\n        twenty_hash_count = 0\n        double_percent_count = 0\n        magic_command_count = 0\n        rspin_comment_count = 0\n        vim_folding_markers_count = 0\n        vscode_folding_markers_count = 0\n\n        parser = StringParser(language='R' if ext in ['.r', '.R'] else 'python')\n        for line in lines:\n            parser.read_line(line)\n            if parser.is_quoted():\n                continue\n\n            # Don't count escaped Jupyter magics (no space between %% and command) as cells\n            if double_percent_re.match(line) or double_percent_and_space_re.match(line) or \\\n                    nbconvert_script_re.match(line):\n                double_percent_count += 1\n\n            if magic_re.match(line):\n                magic_command_count += 1\n\n            if line.startswith(twenty_hash) and ext == '.py':\n                twenty_hash_count += 1\n\n            if line.startswith(\"#'\") and ext in ['.R', '.r']:\n                rspin_comment_count += 1\n\n            if vim_folding_markers_re.match(line):\n                vim_folding_markers_count += 1\n\n            if vscode_folding_markers_re.match(line):\n                vscode_folding_markers_count += 1\n\n        if double_percent_count >= 1:\n            if magic_command_count:\n                return 'hydrogen', {}\n            return 'percent', {}\n\n        if vim_folding_markers_count:\n            return 'light', {'cell_markers': '{{{,}}}'}\n\n        if vscode_folding_markers_count:\n            return 'light', {'cell_markers': 'region,endregion'}\n\n        if twenty_hash_count >= 2:\n            return 'sphinx', {}\n\n        if rspin_comment_count >= 1:\n            return 'spin', {}\n\n    if ext == '.md':\n        for line in lines:\n            if line.startswith(':::'):  # Pandoc div\n                return 'pandoc', {}\n\n    # Default format\n    return get_format_implementation(ext).format_name, {}",
        "sampled": "```python\nimport re\n\ndef guess_format(text, ext):\n    \"\"\"\n    Guess the format and format options of the file, given its extension and content\n    \"\"\"\n\n    # Define a dictionary to store known formats and their regex patterns\n    formats = {\n        'txt': r'^\\d{1} \\w{1,} (?:\\d+ )?(?:\\w+.-label|BY line\u1eb7c)$',\n        'docx': r'xmlns\u4e3b\u9898 microsoft|$',\n"
    },
    {
        "original": "def save_config(self, cmd=\"write memory\", confirm=False, confirm_response=\"\"):\n        \"\"\"Saves configuration.\"\"\"\n        return super(UbiquitiEdgeSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "sampled": "```python\ndef save_config(self, cmd=\"write memory\", confirm=False, confirm_response=\"\"):\n    \"\"\"Saves configuration.\"\"\"\n    \n    # Check if the command is valid\n    if cmd not in [\"write memory\", \"read memory\"]:\n        raise ValueError(\"Invalid command\")\n    \n    # If confirmation is required and the response is not provided, prompt for confirmation\n    if confirm and not confirm_response:\n        print(\"Please confirm to save configuration:\")\n        confirm_response = input(\"(yes/no):"
    },
    {
        "original": "def get_message(self, message_id, *args, **kwargs):\n        \"\"\"Return a Message object corresponding to the given ID.\n\n        :param message_id: The ID or Fullname for a Message\n\n        The additional parameters are passed directly into\n        :meth:`~praw.objects.Message.from_id` of Message, and subsequently into\n        :meth:`.request_json`.\n\n        \"\"\"\n        return objects.Message.from_id(self, message_id, *args, **kwargs)",
        "sampled": "```python\nclass Message:\n    def __init__(self, message_id, *args, **kwargs):\n        self.message_id = message_id\n        self.args = args\n        self.kwargs = kwargs\n\n    def from_id(self):\n        # Assuming this method is implemented elsewhere in the codebase\n        pass\n\n    def request_json(self):\n        # Assuming this method is implemented elsewhere in the codebase\n        pass\n\n\ndef get_message(message_id, *args, **kwargs):\n    \"\"\"\n"
    },
    {
        "original": "def flowspec_prefix_del(self, flowspec_family, rules, route_dist=None):\n        \"\"\" This method deletes an advertised Flow Specification route.\n\n        ``flowspec_family`` specifies one of the flowspec family name.\n\n        ``rules`` specifies NLRIs of Flow Specification as\n        a dictionary type value.\n\n        ``route_dist`` specifies a route distinguisher value.\n        \"\"\"\n        func_name = 'flowspec.del'\n\n        # Set required arguments\n        kwargs = {\n            FLOWSPEC_FAMILY: flowspec_family,\n            FLOWSPEC_RULES: rules,\n        }\n\n        if flowspec_family in [FLOWSPEC_FAMILY_VPNV4, FLOWSPEC_FAMILY_VPNV6,\n                               FLOWSPEC_FAMILY_L2VPN]:\n            func_name = 'flowspec.del_local'\n            kwargs.update({ROUTE_DISTINGUISHER: route_dist})\n\n        call(func_name, **kwargs)",
        "sampled": "```python\nclass FlowSpecification:\n    def __init__(self):\n        self.flowspec_family = None\n        self.rules = {}\n        self.route_dist = None\n\n    def flowspec_prefix_del(self, flowspec_family, rules, route_dist=None):\n        \"\"\"\n        This method deletes an advertised Flow Specification route.\n\n        :param flowspec_family: specifies one of the flowspec family name.\n        :param rules: specifies NLRIs of Flow Specification as a dictionary type value.\n       "
    },
    {
        "original": "def _next_trace_frames(\n        self,\n        session: Session,\n        trace_frame: TraceFrameQueryResult,\n        visited_ids: Set[int],\n        backwards: bool = False,\n    ) -> List[TraceFrameQueryResult]:\n        \"\"\"Finds all trace frames that the given trace_frame flows to.\n\n        When backwards=True, the result will include the parameter trace_frame,\n        since we are filtering on the parameter's callee.\n        \"\"\"\n        query = (\n            session.query(\n                TraceFrame.id,\n                TraceFrame.caller_id,\n                CallerText.contents.label(\"caller\"),\n                TraceFrame.caller_port,\n                TraceFrame.callee_id,\n                CalleeText.contents.label(\"callee\"),\n                TraceFrame.callee_port,\n                TraceFrame.callee_location,\n                TraceFrame.kind,\n                FilenameText.contents.label(\"filename\"),\n                TraceFrameLeafAssoc.trace_length,\n            )\n            .filter(TraceFrame.run_id == self.current_run_id)\n            .filter(TraceFrame.kind == trace_frame.kind)\n            .join(CallerText, CallerText.id == TraceFrame.caller_id)\n            .join(CalleeText, CalleeText.id == TraceFrame.callee_id)\n            .join(FilenameText, FilenameText.id == TraceFrame.filename_id)\n            .filter(\n                TraceFrame.caller_id != TraceFrame.callee_id\n            )  # skip recursive calls for now\n        )\n        if backwards:\n            query = query.filter(TraceFrame.callee_id == trace_frame.caller_id).filter(\n                TraceFrame.callee_port == trace_frame.caller_port\n            )\n        else:\n            query = query.filter(TraceFrame.caller_id == trace_frame.callee_id).filter(\n                TraceFrame.caller_port == trace_frame.callee_port\n            )\n\n        results = (\n            query.join(\n                TraceFrameLeafAssoc, TraceFrameLeafAssoc.trace_frame_id == TraceFrame.id\n            )\n            .group_by(TraceFrame.id)\n            .order_by(TraceFrameLeafAssoc.trace_length, TraceFrame.callee_location)\n        )\n        filter_leaves = (\n            self.sources if trace_frame.kind == TraceKind.POSTCONDITION else self.sinks\n        )\n\n        filtered_results = []\n        for frame in results:\n            if int(frame.id) not in visited_ids and filter_leaves.intersection(\n                set(\n                    self._get_leaves_trace_frame(\n                        session,\n                        int(frame.id),\n                        self._trace_kind_to_shared_text_kind(frame.kind),\n                    )\n                )\n            ):\n                filtered_results.append(frame)\n\n        return filtered_results",
        "sampled": "```python\nfrom typing import List, Set\n\nclass Session:\n    def get_stack_trace(self, trace_frame: TraceFrameQueryResult) -> List[TraceFrameQueryResult]:\n        raise NotImplementedError\n\nclass TraceFrameQueryResult:\n    id: int\n    callee: int\n\ndef _next_trace_frames(\n    self,\n    session: Session,\n    trace_frame: TraceFrameQueryResult,\n    visited_ids: Set[int],\n    backwards: bool = False,\n) -> List[TraceFrameQuery"
    },
    {
        "original": "def as_dict(self):\n        \"\"\"\n        Bson-serializable dict representation of the WeightedNbSetChemenvStrategy object.\n        :return: Bson-serializable dict representation of the WeightedNbSetChemenvStrategy object.\n        \"\"\"\n        return {\"@module\": self.__class__.__module__,\n                \"@class\": self.__class__.__name__,\n                \"additional_condition\": self._additional_condition,\n                \"symmetry_measure_type\": self.symmetry_measure_type,\n                \"nb_set_weights\": [nb_set_weight.as_dict() for nb_set_weight in self.nb_set_weights],\n                \"ce_estimator\": self.ce_estimator,\n                }",
        "sampled": "```python\nclass WeightedNbSetChemenvStrategy:\n    def __init__(self, nb_sets, weights):\n        \"\"\"\n        :param nb_sets: List of number of sets for each strategy\n        :param weights: List of corresponding weights for each strategy\n        \"\"\"\n        self.nb_sets = nb_sets\n        self.weights = weights\n\n    def as_dict(self):\n        \"\"\"\n        Bson-serializable dict representation of the WeightedNbSetChemenvStrategy object.\n        :return"
    },
    {
        "original": "def get_tab_title(key, frame, overlay):\n    \"\"\"\n    Computes a title for bokeh tabs from the key in the overlay, the\n    element and the containing (Nd)Overlay.\n    \"\"\"\n    if isinstance(overlay, Overlay):\n        if frame is not None:\n            title = []\n            if frame.label:\n                title.append(frame.label)\n                if frame.group != frame.params('group').default:\n                    title.append(frame.group)\n            else:\n                title.append(frame.group)\n        else:\n            title = key\n        title = ' '.join(title)\n    else:\n        title = ' | '.join([d.pprint_value_string(k) for d, k in\n                            zip(overlay.kdims, key)])\n    return title",
        "sampled": "### Problem Description\nNo problem description is provided. However, based on the function name `get_tab_title` and its parameters, it seems like we are working with Bokeh tabs. The function is supposed to compute a title for these tabs.\n\n### Input and Output Specifications\nNo input and output specifications are provided.\n\n### Python Solution\n\n```python\ndef get_tab_title(key, frame, overlay):\n    \"\"\"\n    Computes a title for bokeh tabs from the key in the overlay, \n    the"
    },
    {
        "original": "def add_chain(self, chain):\n        \"\"\"\n        Add block in a chain in the correct order. Also add all of the blocks\n        to the cache before doing a purge.\n        \"\"\"\n        with self._lock:\n            chain.sort(key=lambda x: x.block_num)\n            for block in chain:\n                block_id = block.header_signature\n                if block_id not in self._cache:\n                    self._cache[block_id] = self.CachedValue(block)\n                    if block.previous_block_id in self._cache:\n                        self._cache[block.previous_block_id].inc_count()\n\n            if time.time() > self._next_purge_time:\n                self._purge_expired()\n                self._next_purge_time = time.time() + self._purge_frequency",
        "sampled": "```python\nclass BlockChain:\n    def __init__(self):\n        self.cache = {}\n        self.chain = []\n\n    def add_block(self, block):\n        \"\"\"\n        Add a block to the chain and cache.\n        \n        :param block: The block to be added.\n        \"\"\"\n        \n    def purge_cache(self):\n        \"\"\"\n        Purge the cache after adding all blocks in the correct order.\n        \n         - The function should be called at the end of add_chain method\n         - It"
    },
    {
        "original": "def _is_process_filtered(self, process, key=None):\n        \"\"\"Return True if the process[key] should be filtered according to the current filter\"\"\"\n        if key is None:\n            key = self.filter_key\n        try:\n            # If the item process[key] is a list, convert it to a string\n            # in order to match it with the current regular expression\n            if isinstance(process[key], list):\n                value = ' '.join(process[key])\n            else:\n                value = process[key]\n        except KeyError:\n            # If the key did not exist\n            return False\n        try:\n            return self._filter_re.match(value) is None\n        except (AttributeError, TypeError):\n            # AttributeError\n            # Filter processes crashs with a bad regular expression pattern (issue #665)\n            # TypeError\n            # Filter processes crashs if value is None (issue #1105)\n            return False",
        "sampled": "```python\nfrom typing import Dictionary\n\nclass FilteringSystem:\n    def __init__(self):\n        self.filters = {}\n\n    def add_filter(self, key: str, condition):\n        \"\"\"Add a filter for the given key\"\"\"\n        self.filters[key] = condition\n\n    def _is_process_filtered(self, process: Dictionary, key=None):\n        \"\"\"Return True if the process[key] should be filtered according to the current filter\"\"\"\n        if not self.filters:\n            return False\n\n        if key"
    },
    {
        "original": "def _requires_login(func: Callable) -> Callable:\n    \"\"\"Decorator to raise an exception if herewith-decorated function is called without being logged in\"\"\"\n    @wraps(func)\n    def call(instaloader, *args, **kwargs):\n        if not instaloader.context.is_logged_in:\n            raise LoginRequiredException(\"--login=USERNAME required.\")\n        return func(instaloader, *args, **kwargs)\n    # pylint:disable=no-member\n    call.__doc__ += \":raises LoginRequiredException: If called without being logged in.\\n\"\n    return call",
        "sampled": "### Problem Description\n```markdown\nJust a Stranger in the Crowd Problem\n\nA collection of integers is called an arithmetic progression if the difference between any two successive members is constant. For example, among {1, 3, 5, 7}, there's a common difference of 2.\n\nYou are given an array A consisting of N integers and you need to divide it into subarrays such that each subarray is an arithmetic progression. The goal is to find the maximum number of such subsequ"
    },
    {
        "original": "def types_of_specie(self):\n        \"\"\"\n        List of types of specie. Only works for ordered structures.\n        Disordered structures will raise TypeError.\n        \"\"\"\n        if not self.is_ordered:\n            raise TypeError(",
        "sampled": "```python\nclass Structure:\n    def __init__(self, species):\n        self.species = species\n\n    def types_of_species(self):\n        \"\"\"\n        List of types of specie. Only works for ordered structures.\n        Disordered structures will raise TypeError.\n        \"\"\"\n        if isinstance(self.species[0], list) and all(isinstance(specie, list) for specie in self.species):\n            # Check if all the species are lists within a larger list and contains multiple species \n            # ("
    },
    {
        "original": "def tag(tagger, args):\n  \"\"\"Chunk named entities.\"\"\"\n  for l in args.input:\n    words = l.strip().split()\n    line_annotations = [u\"{:<16}{:<5}\".format(w,p) for w, p in tagger.annotate(words)]\n    _print(u\"\\n\".join(line_annotations))\n    _print(u\"\")",
        "sampled": "```python\ndef tag(tagger, args):\n    \"\"\"\n    Chunk named entities.\n    \n    Parameters:\n    tagger (object): A named entity recognition model.\n    args (list): A list of strings representing the input text to be tagged.\n    \n    Returns:\n    list: A list of tuples where each tuple contains a word and its corresponding part-of-speech tag.\n    \"\"\"\n    \n    # Initialize an empty list to store the tagged words\n    tagged_words = []\n    \n    # Iterate"
    },
    {
        "original": "def remove_colormap(self, removal_type):\n        \"\"\"Remove a palette (colormap); if no colormap, returns a copy of this\n        image\n\n            removal_type - any of lept.REMOVE_CMAP_*\n\n        \"\"\"\n        with _LeptonicaErrorTrap():\n            return Pix(\n                lept.pixRemoveColormapGeneral(self._cdata, removal_type, lept.L_COPY)\n            )",
        "sampled": "```python\nimport numpy as np\n\nclass Image:\n    def __init__(self, data, cmap):\n        self.data = data\n        self.cmap = cmap\n\n    def apply_cmap(self):\n        return np.vectorize(self.cmap)(self.data)\n\n    def remove_colormap(self, removal_type):\n        if removal_type == lept.REMOVE_CMAP[data.dtype]:\n            return Image(np.transpose(data), None)\n        elif removal_type in [lept.REMOVE_CMAP_default,"
    },
    {
        "original": "def coupling_constant(self, specie):\n        \"\"\"\n        Computes the couplling constant C_q as defined in:\n            Wasylishen R E, Ashbrook S E, Wimperis S. NMR of quadrupolar nuclei\n            in solid materials[M]. John Wiley & Sons, 2012. (Chapter 3.2)\n\n        C_q for a specific atom type for this electric field tensor:\n                C_q=e*Q*V_zz/h\n            h: planck's constant\n            Q: nuclear electric quadrupole moment in mb (millibarn\n            e: elementary proton charge\n\n        Args:\n            specie: flexible input to specify the species at this site.\n                    Can take a isotope or element string, Specie object,\n                    or Site object\n\n        Return:\n\n            the coupling constant as a FloatWithUnit in MHz\n        \"\"\"\n        planks_constant=FloatWithUnit(6.62607004E-34, \"m^2 kg s^-1\")\n        Vzz=FloatWithUnit(self.V_zz, \"V ang^-2\")\n        e=FloatWithUnit(-1.60217662E-19, \"C\")\n\n        # Convert from string to Specie object\n        if isinstance(specie, str):\n            # isotope was provided in string format\n            if len(specie.split(\"-\")) > 1:\n                isotope=str(specie)\n                specie=Specie(specie.split(\"-\")[0])\n                Q=specie.get_nmr_quadrupole_moment(isotope)\n            else:\n                specie=Specie(specie)\n                Q=specie.get_nmr_quadrupole_moment()\n        elif isinstance(specie, Site):\n            specie=specie.specie\n            Q=specie.get_nmr_quadrupole_moment()\n        elif isinstance(specie, Specie):\n            Q=specie.get_nmr_quadrupole_moment()\n        else:\n            raise ValueError(\"Invalid speciie provided for quadrupolar coupling constant calcuations\")\n\n        return (e * Q * Vzz / planks_constant).to(\"MHz\")",
        "sampled": "```python\nimport numpy as np\nfrom scipy.constants import elementary_charge, planck\n\nclass FloatWithUnit:\n    def __init__(self, value, unit):\n        self.value = value\n        self.unit = unit\n\ndef coupling_constant(self, specie):\n    \"\"\"\n    Computes the couplling constant C_q as defined in:\n        Wasylishen R E, Ashbrook S E, Wimperis S. NMR of quadrupolar nuclei\n        in solid materials"
    },
    {
        "original": "def creep_data(data_set='creep_rupture'):\n    \"\"\"Brun and Yoshida's metal creep rupture data.\"\"\"\n    if not data_available(data_set):\n        download_data(data_set)\n        path = os.path.join(data_path, data_set)\n        tar_file = os.path.join(path, 'creeprupt.tar')\n        tar = tarfile.open(tar_file)\n        print('Extracting file.')\n        tar.extractall(path=path)\n        tar.close()\n    all_data = np.loadtxt(os.path.join(data_path, data_set, 'taka'))\n    y = all_data[:, 1:2].copy()\n    features = [0]\n    features.extend(range(2, 31))\n    X = all_data[:, features].copy()\n    return data_details_return({'X': X, 'y': y}, data_set)",
        "sampled": "```python\nimport numpy as np\n\ndef creep_data(data_set='creep_rupture'):\n    \"\"\"\n    Brun and Yoshida's metal creep rupture data.\n    \n    Parameters:\n    data_set (str): The type of data to return. Default is 'creep_rupture'.\n    \n    Returns:\n    A dictionary containing the creep rupture data.\n    \n    Note: This function assumes that the input parameters are in the following format:\n          - Temperature in Kelvin (T)\n          - Stress"
    },
    {
        "original": "def _constraints(self, X, missing_mask, S, error_tolerance):\n        \"\"\"\n        Parameters\n        ----------\n        X : np.array\n            Data matrix with missing values filled in\n\n        missing_mask : np.array\n            Boolean array indicating where missing values were\n\n        S : cvxpy.Variable\n            Representation of solution variable\n        \"\"\"\n        ok_mask = ~missing_mask\n        masked_X = cvxpy.multiply(ok_mask, X)\n        masked_S = cvxpy.multiply(ok_mask, S)\n        abs_diff = cvxpy.abs(masked_S - masked_X)\n        close_to_data = abs_diff <= error_tolerance\n        constraints = [close_to_data]\n        if self.require_symmetric_solution:\n            constraints.append(S == S.T)\n\n        if self.min_value is not None:\n            constraints.append(S >= self.min_value)\n\n        if self.max_value is not None:\n            constraints.append(S <= self.max_value)\n\n        return constraints",
        "sampled": "```python\nimport numpy as np\nimport cvxpy as cp\n\nclass ProgrammingChallenge:\n    def _constraints(self, X, missing_mask, S, error_tolerance):\n        # Inequality constraints: \n        # 1 <= S[0] <= 3 ( corresponding to the first and last houses)\n        ineq_thresholds = np.array([[1], [3]])\n        \n        inequalities = [S <= ineq_thresholds]\n        \n        # Equality constraints:\n        # For all houses"
    },
    {
        "original": "def update_title(self, title):\n        \"\"\"Renames the worksheet.\n\n        :param title: A new title.\n        :type title: str\n\n        \"\"\"\n\n        body = {\n            'requests': [{\n                'updateSheetProperties': {\n                    'properties': {\n                        'sheetId': self.id,\n                        'title': title\n                    },\n                    'fields': 'title'\n                }\n            }]\n        }\n\n        response = self.spreadsheet.batch_update(body)\n        self._properties['title'] = title\n        return response",
        "sampled": "```python\nclass Spreadsheet:\n    def __init__(self, title):\n        self.title = title\n\n    def update_title(self, new_title):\n        self.title = new_title\n\n\n# Example usage:\nspreadsheet = Spreadsheet(\"Old Title\")\nprint(spreadsheet.title)  # Output: Old Title\nspreadsheet.update_title(\"New Title\")\nprint(spreadsheet.title)  # Output: New Title\n```"
    },
    {
        "original": "def source(self, request):\n        \"\"\"Pulls values off the request in the provided location\n        :param request: The flask request object to parse arguments from\n        \"\"\"\n        if isinstance(self.location, six.string_types):\n            value = getattr(request, self.location, MultiDict())\n            if callable(value):\n                value = value()\n            if value is not None:\n                return value\n        else:\n            values = MultiDict()\n            for l in self.location:\n                value = getattr(request, l, None)\n                if callable(value):\n                    value = value()\n                if value is not None:\n                    values.update(value)\n            return values\n\n        return MultiDict()",
        "sampled": "### Problem Description\n### \n#### Problem: \nGiven a string `s` consisting of some words separated by spaces, and each word consists only of lowercase English alphabets. A word any length of at least 4 letters is considered a \"word piece\".\nFor example, `applepie` is a \"word piece\" because it has at least 4 letters, but `app` or `pie` are not.\nReturn all the different possible non-empty \"word pieces\" in string `"
    },
    {
        "original": "def send_message_with_response(\n            self,\n            operation,\n            set_slave_okay,\n            all_credentials,\n            listeners,\n            exhaust=False):\n        \"\"\"Send a message to MongoDB and return a Response object.\n\n        Can raise ConnectionFailure.\n\n        :Parameters:\n          - `operation`: A _Query or _GetMore object.\n          - `set_slave_okay`: Pass to operation.get_message.\n          - `all_credentials`: dict, maps auth source to MongoCredential.\n          - `listeners`: Instance of _EventListeners or None.\n          - `exhaust` (optional): If True, the socket used stays checked out.\n            It is returned along with its Pool in the Response.\n        \"\"\"\n        with self.get_socket(all_credentials, exhaust) as sock_info:\n\n            duration = None\n            publish = listeners.enabled_for_commands\n            if publish:\n                start = datetime.now()\n\n            use_find_cmd = False\n            if sock_info.max_wire_version >= 4:\n                if not exhaust:\n                    use_find_cmd = True\n            elif (isinstance(operation, _Query) and\n                  not operation.read_concern.ok_for_legacy):\n                raise ConfigurationError(\n                    'read concern level of %s is not valid '\n                    'with a max wire version of %d.'\n                    % (operation.read_concern.level,\n                       sock_info.max_wire_version))\n            if (isinstance(operation, _Query) and\n                    sock_info.max_wire_version < 5 and\n                    operation.collation is not None):\n                raise ConfigurationError(\n                    'Specifying a collation is unsupported with a max wire '\n                    'version of %d.' % (sock_info.max_wire_version,))\n            message = operation.get_message(\n                set_slave_okay, sock_info.is_mongos, use_find_cmd)\n            request_id, data, max_doc_size = self._split_message(message)\n\n            if publish:\n                encoding_duration = datetime.now() - start\n                cmd, dbn = operation.as_command()\n                listeners.publish_command_start(\n                    cmd, dbn, request_id, sock_info.address)\n                start = datetime.now()\n\n            try:\n                sock_info.send_message(data, max_doc_size)\n                response_data = sock_info.receive_message(1, request_id)\n            except Exception as exc:\n                if publish:\n                    duration = (datetime.now() - start) + encoding_duration\n                    failure = _convert_exception(exc)\n                    listeners.publish_command_failure(\n                        duration, failure, next(iter(cmd)), request_id,\n                        sock_info.address)\n                raise\n\n            if publish:\n                duration = (datetime.now() - start) + encoding_duration\n\n            if exhaust:\n                return ExhaustResponse(\n                    data=response_data,\n                    address=self._description.address,\n                    socket_info=sock_info,\n                    pool=self._pool,\n                    duration=duration,\n                    request_id=request_id,\n                    from_command=use_find_cmd)\n            else:\n                return Response(\n                    data=response_data,\n                    address=self._description.address,\n                    duration=duration,\n                    request_id=request_id,\n                    from_command=use_find_cmd)",
        "sampled": "### Problem Analysis\n\nThe given function `send_message_with_response` appears to be part of a MongoDB driver, responsible for sending messages to the MongoDB server and returning a response. The function takes several parameters:\n\n- `operation`: A query or get more object.\n- `set_slave_okay`: A flag to pass to the operation's get message method.\n- `all_credentials`: A dictionary mapping authentication sources to credentials.\n- `listeners`: An instance of event listeners or None.\n- `exhaust"
    },
    {
        "original": "def do_GET(self):  # pylint: disable=g-bad-name\n    \"\"\"Serve the server pem with GET requests.\"\"\"\n    self._IncrementActiveCount()\n    try:\n      if self.path.startswith(\"/server.pem\"):\n        stats_collector_instance.Get().IncrementCounter(\n            \"frontend_http_requests\", fields=[\"cert\", \"http\"])\n        self.ServerPem()\n      elif self.path.startswith(self.static_content_path):\n        stats_collector_instance.Get().IncrementCounter(\n            \"frontend_http_requests\", fields=[\"static\", \"http\"])\n        self.ServeStatic(self.path[len(self.static_content_path):])\n    finally:\n      self._DecrementActiveCount()",
        "sampled": "```python\nfrom socket import *\nfrom ssl import *\n\nclass Server:\n    def __init__(self, host='0.0.0.0', port=443):\n        self.host = host\n        self.port = port\n\n    def do_GET(self):\n        # Create a TCP socket\n        tcp_server_socket = socket(AF_INET, SOCK_STREAM)\n\n        # Bind the local IP and Port to the socket\n        tcp_server_socket.bind((self.host, self.port))\n\n        # Listen for"
    },
    {
        "original": "def write_bytecode(self, f):\n        \"\"\"Dump the bytecode into the file or file like object passed.\"\"\"\n        if self.code is None:\n            raise TypeError('can\\'t write empty bucket')\n        f.write(bc_magic)\n        pickle.dump(self.checksum, f, 2)\n        if isinstance(f, file):\n            marshal.dump(self.code, f)\n        else:\n            f.write(marshal.dumps(self.code))",
        "sampled": "```python\nimport sys\n\ndef find_rotate_steps(self, nums):\n    \"\"\"\n    Problem Description:\n        Given two lists of numbers, the same length, where each number in a list is associated with a number in the other list.\n        The goal is to rotate one list to match the other. The rotation can be done by swapping elements at different positions.\n        Find the minimum number of steps required to make both lists identical.\n\n    Input and Output Specifications:\n        - Input: Two lists of integers"
    },
    {
        "original": "def add_request_participants(self, issue_id_or_key, users_list):\n        \"\"\"\n        Add users as participants to an existing customer request\n        The calling user must have permission to manage participants for this customer request\n\n        :param issue_id_or_key: str\n        :param users_list: list\n        :return:\n        \"\"\"\n        url = 'rest/servicedeskapi/request/{}/participant'.format(issue_id_or_key)\n        data = {'usernames': users_list}\n\n        return self.post(url, data=data)",
        "sampled": "```python\nclass RequestManager:\n    def add_request_participants(self, issue_id_or_key, users_list):\n        \"\"\"\n        Add users as participants to an existing customer request\n        The calling user must have permission to manage participants for this customer request\n\n        :param issue_id_or_key: str\n        :param users_list: list\n        :return:\n        \"\"\"\n        \n        # Check if the input parameters are of the correct type\n        if not isinstance(issue_id_or_key, str):\n"
    },
    {
        "original": "def keep_only_positive_boxes(boxes):\n    \"\"\"\n    Given a set of BoxList containing the `labels` field,\n    return a set of BoxList for which `labels > 0`.\n\n    Arguments:\n        boxes (list of BoxList)\n    \"\"\"\n    assert isinstance(boxes, (list, tuple))\n    assert isinstance(boxes[0], BoxList)\n    assert boxes[0].has_field(\"labels\")\n    positive_boxes = []\n    positive_inds = []\n    num_boxes = 0\n    for boxes_per_image in boxes:\n        labels = boxes_per_image.get_field(\"labels\")\n        inds_mask = labels > 0\n        inds = inds_mask.nonzero().squeeze(1)\n        positive_boxes.append(boxes_per_image[inds])\n        positive_inds.append(inds_mask)\n    return positive_boxes, positive_inds",
        "sampled": "```python\nfrom pycocotools.coco import COCO\nfrom pycocotools.coco_eval import COCOeval\n\ndef keep_only_positive_boxes(boxes):\n    \"\"\"\n    Given a set of BoxList containing the `labels` field,\n    return a set of BoxList for which `labels > 0`.\n\n    Arguments:\n        boxes (list of BoxList)\n    \"\"\"\n    \n    # Filter out boxes with labels less than or equal to 0\n    positive_boxes"
    },
    {
        "original": "def components(arg):\n    \"\"\"Converts a dict of components to the format expected by the Google Maps\n    server.\n\n    For example:\n    c = {\"country\": \"US\", \"postal_code\": \"94043\"}\n    convert.components(c)\n    # 'country:US|postal_code:94043'\n\n    :param arg: The component filter.\n    :type arg: dict\n\n    :rtype: basestring\n    \"\"\"\n\n    # Components may have multiple values per type, here we\n    # expand them into individual key/value items, eg:\n    # {\"country\": [\"US\", \"AU\"], \"foo\": 1} -> \"country:AU\", \"country:US\", \"foo:1\"\n    def expand(arg):\n        for k, v in arg.items():\n            for item in as_list(v):\n                yield \"%s:%s\" % (k, item)\n\n    if isinstance(arg, dict):\n        return \"|\".join(sorted(expand(arg)))\n\n    raise TypeError(\n        \"Expected a dict for components, \"\n        \"but got %s\" % type(arg).__name__)",
        "sampled": "### Problem Description\nConvert a dictionary of components to the format expected by the Google Maps server.\n\n### Input and Output Specifications\n\n* **Input**: A dictionary of components.\n* **Output**: A string in the format 'key:value|key:value'.\n\n### Python Solution\n\n```python\ndef components(arg):\n    \"\"\"Converts a dict of components to the format expected by the Google Maps server.\n\n    :param arg: The component filter.\n    :type arg: dict\n\n    :rtype: bas"
    },
    {
        "original": "def posterior_samples(self, X, size=10, full_cov=False, Y_metadata=None, likelihood=None, **predict_kwargs):\n        \"\"\"\n        Samples the posterior GP at the points X, equivalent to posterior_samples_f due to the absence of a likelihood.\n        \"\"\"\n        return self.posterior_samples_f(X, size, full_cov=full_cov, **predict_kwargs)",
        "sampled": "### Problem Description\n#### Gaussian Process Posterior Sampling\n\nGiven a set of input points `X`, sample from the posterior distribution of a Gaussian Process (GP) at these points.\n\n### Input Specifications\n\n* `X`: A 2D array of shape `(n_samples, n_features)` where `n_samples` is the number of input points and `n_features` is the dimensionality of the input space.\n* `size`: The number of samples to draw from the posterior distribution. Default is"
    },
    {
        "original": "def ReadGRRUser(self, username):\n    \"\"\"Reads a user object corresponding to a given name.\"\"\"\n    try:\n      return self.users[username].Copy()\n    except KeyError:\n      raise db.UnknownGRRUserError(username)",
        "sampled": "I'm ready to help you solve the problem. However, I don't see the problem description in Markdown format. Please provide the problem description, input and output specifications so I can understand and analyze the problem.\n\nOnce I have a clear understanding of the problem, I'll provide a Python solution for you."
    },
    {
        "original": "def add_message(self, request, level, message_template,\n                    message_context=None, extra_tags=''):\n        \"\"\"\n        Wrapper of `django.contrib.messages.add_message`, that reads\n        the message text from a template.\n        \"\"\"\n        if 'django.contrib.messages' in settings.INSTALLED_APPS:\n            try:\n                if message_context is None:\n                    message_context = {}\n                message = render_to_string(message_template,\n                                           message_context).strip()\n                if message:\n                    messages.add_message(request, level, message,\n                                         extra_tags=extra_tags)\n            except TemplateDoesNotExist:\n                pass",
        "sampled": "```python\nfrom django.contrib import messages\n\nclass MessageService:\n    def add_message(self, request, level, message_template, message_context=None, extra_tags=''):\n        \"\"\"\n        Wrapper of `django.contrib.messages.add_message`, that reads\n        the message text from a template.\n        \"\"\"\n        # Get the message text from the template\n        message_text = message_template.format(**message_context or {})\n\n        # Add the message to the request\n        messages.add_message(request, level, message_text"
    },
    {
        "original": "def delete_flag(self):\n        \"\"\" Sets this message as un flagged \"\"\"\n        self.__status = Flag.NotFlagged\n        self.__start = None\n        self.__due_date = None\n        self.__completed = None\n        self._track_changes()",
        "sampled": "```python\nclass Message:\n    def __init__(self, content):\n        self.content = content\n        self.flagged = False\n\n    def delete_flag(self):\n        \"\"\" Sets this message as un flagged \"\"\"\n        self.flagged = False\n\n# Example usage:\nmessage = Message(\"Hello, World!\")\nprint(message.flagged)  # Output: False\nmessage.delete_flag()\nprint(message.flagged)  # Output: False\n```"
    },
    {
        "original": "def find_magic_file(self, fname, system_only=False, user_only=False):\n        \"\"\"\n        Finds the specified magic file name in the system / user magic file directories.\n\n        @fname       - The name of the magic file.\n        @system_only - If True, only the system magic file directory will be searched.\n        @user_only   - If True, only the user magic file directory will be searched.\n\n        If system_only and user_only are not set, the user directory is always searched first.\n\n        Returns the path to the file on success; returns None on failure.\n        \"\"\"\n        loc = None\n\n        if not system_only:\n            fpath = self._user_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        if loc is None and not user_only:\n            fpath = self._system_path(self.BINWALK_MAGIC_DIR, fname)\n            if os.path.exists(fpath) and common.file_size(fpath) > 0:\n                loc = fpath\n\n        return fpath",
        "sampled": "### Problem Analysis\n\nThe problem requires finding a magic file with a specified name in the system and user magic file directories. The function `find_magic_file` takes three parameters:\n\n- `fname`: The name of the magic file to be found.\n- `system_only`: A boolean indicating whether to only search in the system directory.\n- `user_only`: A boolean indicating whether to only search in the user directory.\n\nIf both `system_only` and `user_only` are not set, the function"
    },
    {
        "original": "def options(self, context, module_options):\n        \"\"\"\n           COMMAND  Mimikatz command to execute (default: 'sekurlsa::logonpasswords')\n        \"\"\"\n        self.command = 'privilege::debug sekurlsa::logonpasswords exit'\n        if module_options and 'COMMAND' in module_options:\n            self.command = module_options['COMMAND']\n\n        self.ps_script = obfs_ps_script('powersploit/Exfiltration/Invoke-Mimikatz.ps1')",
        "sampled": "```python\nclass Mimikatz:\n    def __init__(self):\n        self.command = 'sekurlsa::logonpasswords'\n\n    def options(self, context, module_options):\n        \"\"\"\n           COMMAND  Mimikatz command to execute (default: 'sekurlsa::logonpasswords')\n        \"\"\"\n        if 'COMMAND' in module_options:\n            self.command = module_options['COMMAND']\n        return {'command': self.command}\n\n# Example usage\nmimikatz ="
    },
    {
        "original": "def _parse_general_counters(self, init_config):\n        \"\"\"\n        Return a dictionary for each job counter\n        {\n          counter_group_name: [\n              counter_name\n            ]\n          }\n        }\n        \"\"\"\n        job_counter = {}\n\n        if init_config.get('general_counters'):\n\n            # Parse the custom metrics\n            for counter_group in init_config['general_counters']:\n                counter_group_name = counter_group.get('counter_group_name')\n                counters = counter_group.get('counters')\n\n                if not counter_group_name:\n                    raise Exception('\"general_counters\" must contain a valid \"counter_group_name\"')\n\n                if not counters:\n                    raise Exception('\"general_counters\" must contain a list of \"counters\"')\n\n                # Add the counter_group to the job_counters if it doesn't already exist\n                if counter_group_name not in job_counter:\n                    job_counter[counter_group_name] = []\n\n                for counter in counters:\n                    counter_name = counter.get('counter_name')\n\n                    if not counter_name:\n                        raise Exception('At least one \"counter_name\" should be specified in the list of \"counters\"')\n\n                    job_counter[counter_group_name].append(counter_name)\n\n        return job_counter",
        "sampled": "```python\ndef _parse_general_counters(self, init_config):\n    \"\"\"\n    Return a dictionary for each job counter\n    {\n      counter_group_name: [\n          counter_name\n        ]\n      }\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the parsed counters\n    parsed_counters = {}\n    \n    # Iterate over each key-value pair in the init_config dictionary\n    for group, counters in init_config.items():\n        \n        # For each group, initialize a new key in the parsed_counters"
    },
    {
        "original": "def answers(self, other):\n        \"\"\"DEV: true if self is an answer from other\"\"\"\n        if other.__class__ == self.__class__:\n            return (other.service + 0x40) == self.service or \\\n                   (self.service == 0x7f and\n                    (self.requestServiceId == other.service))\n        return 0",
        "sampled": "```python\nclass Answer:\n    def __init__(self, value, origin):\n        self.value = value\n        self.origin = origin\n\n    def answers(self, other):\n        \"\"\"DEV: true if self is an answer from other\"\"\"\n        return self.origin == other\n\n\nclass Question:\n    def __init__(self, value):\n        self.value = value\n        self.answers = []\n\n    def add_answer(self, answer):\n        self.answers.append(answer)\n\n\n# Example usage:\nq = Question"
    },
    {
        "original": "def add_before(self, pipeline):\n        \"\"\"Add a Pipeline to be applied before this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply before this\n                Pipeline.\n        \"\"\"\n        if not isinstance(pipeline, Pipeline):\n            pipeline = Pipeline(pipeline)\n        self.pipes = pipeline.pipes[:] + self.pipes[:]\n        return self",
        "sampled": "```python\nclass Pipeline:\n    def __init__(self):\n        self.pipelines = []\n\n    def add_before(self, pipeline):\n        \"\"\"Add a Pipeline to be applied before this processing pipeline.\n\n        Arguments:\n            pipeline: The Pipeline or callable to apply before this\n                Pipeline.\n        \"\"\"\n        if not isinstance(pipeline, type(self)):\n            self.pipelines.insert(0, pipeline)\n        else:\n            self.pipelines.insert(0, pipeline)\n\n    def process(self, data):\n"
    },
    {
        "original": "def _update_function_transition_graph(self, src_node_key, dst_node_key, jumpkind='Ijk_Boring', ins_addr=None,\n                                          stmt_idx=None, confirmed=None):\n        \"\"\"\n        Update transition graphs of functions in function manager based on information passed in.\n\n        :param str jumpkind: Jumpkind.\n        :param CFGNode src_node: Source CFGNode\n        :param CFGNode dst_node: Destionation CFGNode\n        :param int ret_addr: The theoretical return address for calls\n        :return: None\n        \"\"\"\n\n        if dst_node_key is not None:\n            dst_node = self._graph_get_node(dst_node_key, terminator_for_nonexistent_node=True)\n            dst_node_addr = dst_node.addr\n            dst_codenode = dst_node.to_codenode()\n            dst_node_func_addr = dst_node.function_address\n        else:\n            dst_node = None\n            dst_node_addr = None\n            dst_codenode = None\n            dst_node_func_addr = None\n\n        if src_node_key is None:\n            if dst_node is None:\n                raise ValueError(\"Either src_node_key or dst_node_key must be specified.\")\n            self.kb.functions.function(dst_node.function_address, create=True)._register_nodes(True,\n                                                                                               dst_codenode\n                                                                                               )\n            return\n\n        src_node = self._graph_get_node(src_node_key, terminator_for_nonexistent_node=True)\n\n        # Update the transition graph of current function\n        if jumpkind == \"Ijk_Call\":\n            ret_addr = src_node.return_target\n            ret_node = self.kb.functions.function(\n                src_node.function_address,\n                create=True\n            )._get_block(ret_addr).codenode if ret_addr else None\n\n            self.kb.functions._add_call_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_addr=dst_node_addr,\n                retn_node=ret_node,\n                syscall=False,\n                ins_addr=ins_addr,\n                stmt_idx=stmt_idx,\n            )\n\n        if jumpkind.startswith('Ijk_Sys'):\n\n            self.kb.functions._add_call_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_addr=dst_node_addr,\n                retn_node=src_node.to_codenode(),  # For syscalls, they are returning to the address of themselves\n                syscall=True,\n                ins_addr=ins_addr,\n                stmt_idx=stmt_idx,\n            )\n\n        elif jumpkind == 'Ijk_Ret':\n            # Create a return site for current function\n            self.kb.functions._add_return_from(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_node=dst_codenode,\n            )\n\n            if dst_node is not None:\n                # Create a returning edge in the caller function\n                self.kb.functions._add_return_from_call(\n                    function_addr=dst_node_func_addr,\n                    src_function_addr=src_node.function_address,\n                    to_node=dst_codenode,\n                )\n\n        elif jumpkind == 'Ijk_FakeRet':\n            self.kb.functions._add_fakeret_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_node=dst_codenode,\n                confirmed=confirmed,\n            )\n\n        elif jumpkind in ('Ijk_Boring', 'Ijk_InvalICache'):\n\n            src_obj = self.project.loader.find_object_containing(src_node.addr)\n            dest_obj = self.project.loader.find_object_containing(dst_node.addr) if dst_node is not None else None\n\n            if src_obj is dest_obj:\n                # Jump/branch within the same object. Might be an outside jump.\n                to_outside = src_node.function_address != dst_node_func_addr\n            else:\n                # Jump/branch between different objects. Must be an outside jump.\n                to_outside = True\n\n            if not to_outside:\n                self.kb.functions._add_transition_to(\n                    function_addr=src_node.function_address,\n                    from_node=src_node.to_codenode(),\n                    to_node=dst_codenode,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )\n\n            else:\n                self.kb.functions._add_outside_transition_to(\n                    function_addr=src_node.function_address,\n                    from_node=src_node.to_codenode(),\n                    to_node=dst_codenode,\n                    to_function_addr=dst_node_func_addr,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )",
        "sampled": "```python\nclass CFGNode:\n    def __init__(self, key):\n        self.key = key\n        self.transition_graph = {}\n\nclass FunctionManager:\n    def __init__(self):\n        self.function_nodes = {}\n\n    def _update_function_transition_graph(self, src_node_key, dst_node_key, jumpkind='Ijk_Boring', ins_addr=None,\n                                          stmt_idx=None, confirmed=None):\n        \"\"\"\n        Update transition graphs of functions in function manager based on information passed in.\n\n       "
    },
    {
        "original": "def rate_limit_status(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/developer-utilities/rate-limit-status/api-reference/get-application-rate_limit_status\n            :allowed_param:'resources'\n        \"\"\"\n        return bind_api(\n            api=self,\n            path='/application/rate_limit_status.json',\n            payload_type='json',\n            allowed_param=['resources'],\n            use_cache=False\n        )",
        "sampled": "```python\nclass TwitterRateLimit:\n    def rate_limit_status(self):\n        \"\"\"\n        :reference: https://developer.twitter.com/en/docs/developer-utilities/rate-limit-status/api-reference/get-application-rate_limit_status\n        :allowed_param:'resources'\n        \"\"\"\n        \n        # Define the base URL for the API request\n        base_url = \"https://api.twitter.com/1.1/application/rate_limit_status.json\"\n        \n        # Define the allowed parameters for the API request\n        allowed"
    },
    {
        "original": "def verifyChainFromCAPath(self, capath, untrusted_file=None):\n        \"\"\"\n        Does the same job as .verifyChainFromCAFile() but using the list\n        of anchors in capath directory. The directory should (only) contain\n        certificates files in PEM format. As for .verifyChainFromCAFile(),\n        a list of untrusted certificates can be passed as a file\n        (concatenation of the certificates in PEM format).\n        \"\"\"\n        try:\n            anchors = []\n            for cafile in os.listdir(capath):\n                anchors.append(Cert(open(os.path.join(capath, cafile), \"rb\").read()))  # noqa: E501\n        except Exception:\n            raise Exception(\"capath provided is not a valid cert path\")\n\n        untrusted = None\n        if untrusted_file:\n            try:\n                f = open(untrusted_file, \"rb\")\n                untrusted_certs = f.read()\n                f.close()\n            except Exception:\n                raise Exception(\"Could not read from untrusted_file\")\n            untrusted = [Cert(c) for c in split_pem(untrusted_certs)]\n\n        return self.verifyChain(anchors, untrusted)",
        "sampled": "```python\nimport os\nimport ssl\nfrom cryptography import x509\nfrom cryptography.hazmat.backends import default_backend\n\ndef verifyChainFromCAPath(self, capath, untrusted_file=None):\n    \"\"\"\n    Does the same job as .verifyChainFromCAFile() but using the list of anchors in capath directory.\n    \n    The directory should (only) contain certificates files in PEM format. \n    As for .verifyChainFromCAFile(), a list of untrusted certificates can"
    },
    {
        "original": "def _ApplySudsJurkoSendPatch(self):\n    \"\"\"Appends a Monkey Patch to the suds.transport.http module.\n\n    This allows the suds library to decompress the SOAP body when compression is\n    enabled. For more details on SOAP Compression, see:\n    https://developers.google.com/adwords/api/docs/guides/bestpractices?hl=en#use_compression\n    \"\"\"\n    def GetInflateStream(msg):\n      stream = io.BytesIO()\n      stream.write(msg)\n      stream.flush()\n      stream.seek(0)\n      return gzip.GzipFile(fileobj=stream, mode='rb')\n\n    def PatchedHttpTransportSend(self, request):\n      ",
        "sampled": "### Problem Description\n### ===============\n\nThe problem description is not provided in the given challenge. However, based on the function name `_ApplySudsJurkoSendPatch` and the comment, it seems that the goal is to modify the `suds.transport.http` module to handle decompression of the SOAP body when compression is enabled.\n\n### Input and Output Specifications\n### ==============================\n\nNo input and output specifications are provided.\n\n### Python Solution\n### ==============\n\n```python\nimport suds"
    },
    {
        "original": "def delete_row(self, index):\n        \"\"\"\n        Deletes a Row by it's index\n        :param int index: the index of the row. zero indexed\n        :return bool: Success or Failure\n        \"\"\"\n        url = self.build_url(self._endpoints.get('delete_row').format(id=index))\n        return bool(self.session.post(url))",
        "sampled": "### Problem Description\n### \nDelete Row\n\nYou are given a table with `n` rows and `m` columns. You can delete a row by its index. The index of the row is zero-based.\n\n### Input and Output Specifications\n### \n- **Input**:\n  - `n`: The number of rows in the table.\n  - `m`: The number of columns in the table.\n  - `table`: A 2D list representing the table.\n  - `index"
    },
    {
        "original": "def draw_selection(self, surf):\n    \"\"\"Draw the selection rectange.\"\"\"\n    select_start = self._select_start  # Cache to avoid a race condition.\n    if select_start:\n      mouse_pos = self.get_mouse_pos()\n      if (mouse_pos and mouse_pos.surf.surf_type & SurfType.SCREEN and\n          mouse_pos.surf.surf_type == select_start.surf.surf_type):\n        rect = point.Rect(select_start.world_pos, mouse_pos.world_pos)\n        surf.draw_rect(colors.green, rect, 1)",
        "sampled": "```python\nimport pygame\n\nclass Engine:\n    def __init__(self):\n        self.screen = pygame.display.set_mode((800, 600))\n\n    def draw_selection(self, rect):\n        # Draw a rectangle\n        x = rect.x\n        y = rect.y\n        w = rect.w * 2  # Double the width for some reason not specified in problem description \n        h = rect.h\n\n        \n        color_rgb = (255, 0, 0)  \n        \n"
    },
    {
        "original": "def from_soup(self, tag_prof_header, tag_prof_nav):\n        \"\"\"\n        Returns the scraped user data from a twitter user page.\n\n        :param tag_prof_header: captures the left hand part of user info\n        :param tag_prof_nav: captures the upper part of user info\n        :return: Returns a User object with captured data via beautifulsoup\n        \"\"\"\n\n        self.user= tag_prof_header.find('a', {'class':'ProfileHeaderCard-nameLink u-textInheritColor js-nav'})['href'].strip(\"/\") \n        self.full_name = tag_prof_header.find('a', {'class':'ProfileHeaderCard-nameLink u-textInheritColor js-nav'}).text\n        \n        location = tag_prof_header.find('span', {'class':'ProfileHeaderCard-locationText u-dir'}) \n        if location is None:\n            self.location = \"None\"\n        else: \n            self.location = location.text.strip()\n\n        blog = tag_prof_header.find('span', {'class':\"ProfileHeaderCard-urlText u-dir\"})\n        if blog is None:\n            blog = \"None\"\n        else:\n            self.blog = blog.text.strip() \n\n        date_joined = tag_prof_header.find('div', {'class':\"ProfileHeaderCard-joinDate\"}).find('span', {'class':'ProfileHeaderCard-joinDateText js-tooltip u-dir'})['title']\n        if date_joined is None:\n            self.data_joined = \"Unknown\"\n        else:    \n            self.date_joined = date_joined.strip()\n\n        self.id = tag_prof_nav.find('div',{'class':'ProfileNav'})['data-user-id']\n        tweets = tag_prof_nav.find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if tweets is None:\n            self.tweets = 0\n        else:\n            self.tweets = int(tweets)\n\n        following = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--following\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if following is None:\n            following = 0\n        else:\n            self.following = int(following)\n\n        followers = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--followers\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if followers is None:\n            self.followers = 0\n        else:\n            self.followers = int(followers)    \n        \n        likes = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--favorites\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if likes is None:\n            self.likes = 0\n        else:\n            self.likes = int(likes)    \n        \n        lists = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--lists\"})\n        if lists is None:\n            self.lists = 0\n        elif lists.find('span', {'class':\"ProfileNav-value\"}) is None:    \n            self.lists = 0\n        else:    \n            lists = lists.find('span', {'class':\"ProfileNav-value\"}).text    \n            self.lists = int(lists)\n        return(self)",
        "sampled": "### Problem Description\nExtract user data from a Twitter user page using BeautifulSoup.\n\n### Input and Output Specifications\n- Input:\n  - `tag_prof_header`: captures the left hand part of user info\n  - `tag_prof_nav`: captures the upper part of user info\n- Output:\n  - A `User` object with captured data\n\n### Python Solution\n```python\nfrom bs4 import BeautifulSoup\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    name: str"
    },
    {
        "original": "def bulk_stop(workers, lbn, profile='default'):\n    \"\"\"\n    Stop all the given workers in the specific load balancer\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' modjk.bulk_stop node1,node2,node3 loadbalancer1\n        salt '*' modjk.bulk_stop node1,node2,node3 loadbalancer1 other-profile\n\n        salt '*' modjk.bulk_stop [\"node1\",\"node2\",\"node3\"] loadbalancer1\n        salt '*' modjk.bulk_stop [\"node1\",\"node2\",\"node3\"] loadbalancer1 other-profile\n    \"\"\"\n\n    ret = {}\n\n    if isinstance(workers, six.string_types):\n        workers = workers.split(',')\n\n    for worker in workers:\n        try:\n            ret[worker] = worker_stop(worker, lbn, profile)\n        except Exception:\n            ret[worker] = False\n\n    return ret",
        "sampled": "### Problem Description\nStop all the given workers in the specific load balancer\n\n### Input and Output Specifications\n- Input:\n  - `workers`: a string or list of node names (e.g., 'node1,node2,node3')\n  - `lbn`: a string representing the load balancer name\n  - `profile`: an optional string representing the profile (default: 'default')\n\n- Output:\n  - The function should stop all given workers in the specific load balancer\n\n"
    },
    {
        "original": "def is_reached(self, uid=None):\n        \"\"\"\n        is_reached is to be called for every object that counts towards the limit.\n        - When called with no uid, the Limiter assumes this is a new object and\n        unconditionally increments the counter (less CPU and memory usage).\n        - When a given object can be passed multiple times, a uid must be provided to\n        deduplicate calls. Only the first occurrence of a uid will increment the counter.\n\n        :param uid: (optional) unique identifier of the object, to deduplicate calls\n        :returns: boolean, true if limit exceeded\n        \"\"\"\n        if self.reached_limit:\n            return True\n\n        if uid:\n            if uid in self.seen:\n                return False\n            self.count += 1\n            self.seen.add(uid)\n        else:\n            self.count += 1\n\n        if self.count > self.limit:\n            if self.warning:\n                self.warning(\n                    \"Check {} exceeded limit of {} {}, ignoring next ones\".format(\n                        self.check_name, self.limit, self.name\n                    )\n                )\n            self.reached_limit = True\n            return True\n        return False",
        "sampled": "### Problem Description\n#### Limiter Class\n\nThe Limiter class is designed to track the number of objects that can be created within a certain limit. The `is_reached` method is used to check if the limit has been exceeded.\n\n### Input and Output Specifications\n\n* The `is_reached` method takes an optional `uid` parameter, which is a unique identifier for the object.\n* If `uid` is not provided, the method increments the counter unconditionally.\n* If `uid"
    },
    {
        "original": "def list_cache_subnet_groups(region=None, key=None, keyid=None, profile=None):\n    \"\"\"\n    Return a list of all cache subnet group names\n\n    Example:\n\n    .. code-block:: bash\n\n        salt myminion boto3_elasticache.list_cache_subnet_groups region=us-east-1\n    \"\"\"\n    return [g['CacheSubnetGroupName'] for g in\n            describe_cache_subnet_groups(None, region, key, keyid, profile)]",
        "sampled": "### Problem Analysis\n\nThe problem is asking to retrieve a list of all cache subnet group names using the `boto3` library in Python. The function `list_cache_subnet_groups` takes several optional parameters:\n\n- `region`: The AWS region to operate in.\n- `key`: The AWS access key.\n- `keyid`: The AWS access key ID.\n- `profile`: The AWS profile to use.\n\n### Python Solution\n\n```python\nimport boto3\n\ndef list_cache_subnet_groups(region"
    },
    {
        "original": "def prompt_user_to_select_link(self, links):\n        \"\"\"\n        Prompt the user to select a link from a list to open.\n\n        Return the link that was selected, or ``None`` if no link was selected.\n        \"\"\"\n        link_pages = self.get_link_pages(links)\n        n = 0\n        while n in range(len(link_pages)):\n            link_page = link_pages[n]\n            text = 'Select a link to open (page {} of {}):\\n\\n'\n            text = text.format(n+1, len(link_pages))\n            text += self.get_link_page_text(link_page)\n            if link_page is not link_pages[-1]:\n                text += '[j] next page...'\n            if link_page is not link_pages[0]:\n                if link_page is not link_pages[-1]:\n                    text += '\\n'\n                text += '[k] ...previous page'\n\n            try:\n                choice = chr(self.show_notification(text))\n                try:\n                    choice = int(choice)\n                except ValueError:\n                    pass\n            except ValueError:\n                return None\n            if choice == 'j':\n                if link_page is not link_pages[-1]:\n                    n += 1\n                continue\n            elif choice == 'k':\n                if link_page is not link_pages[0]:\n                    n -= 1\n                continue\n            elif choice not in range(len(link_page)):\n                return None\n            return link_page[choice]['href']",
        "sampled": "```python\nclass LinkSelector:\n    def prompt_user_to_select_link(self, links):\n        \"\"\"\n        Prompt the user to select a link from a list to open.\n\n        Return the link that was selected, or ``None`` if no link was selected.\n        \"\"\"\n        \n        # Check if the list of links is empty\n        if not links:\n            return None\n        \n        # Print all available links\n        for i, link in enumerate(links):\n            print(f\"{i+1}. {"
    },
    {
        "original": "def _update_axes(ax, xincrease, yincrease,\n                 xscale=None, yscale=None,\n                 xticks=None, yticks=None,\n                 xlim=None, ylim=None):\n    \"\"\"\n    Update axes with provided parameters\n    \"\"\"\n    if xincrease is None:\n        pass\n    elif xincrease and ax.xaxis_inverted():\n        ax.invert_xaxis()\n    elif not xincrease and not ax.xaxis_inverted():\n        ax.invert_xaxis()\n\n    if yincrease is None:\n        pass\n    elif yincrease and ax.yaxis_inverted():\n        ax.invert_yaxis()\n    elif not yincrease and not ax.yaxis_inverted():\n        ax.invert_yaxis()\n\n    # The default xscale, yscale needs to be None.\n    # If we set a scale it resets the axes formatters,\n    # This means that set_xscale('linear') on a datetime axis\n    # will remove the date labels. So only set the scale when explicitly\n    # asked to. https://github.com/matplotlib/matplotlib/issues/8740\n    if xscale is not None:\n        ax.set_xscale(xscale)\n    if yscale is not None:\n        ax.set_yscale(yscale)\n\n    if xticks is not None:\n        ax.set_xticks(xticks)\n    if yticks is not None:\n        ax.set_yticks(yticks)\n\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    if ylim is not None:\n        ax.set_ylim(ylim)",
        "sampled": "### Problem Description\n(Not given, assuming we are solving the above(helper function). But typically problem description would include what the problem is asking for a specific solution or solution to an equation)\n \n### Input Specifications\n\n* `ax`: A matplotlib axes object.\n* `xincrease`: Increase in x-values. Can be True or False.\n* `yincrease`: Increase in y-values. Can be True or False.\n* `xscale=None`, `yscale=None`: Scale of x and y axes"
    },
    {
        "original": "def resize(self, container, height, width):\n        \"\"\"\n        Resize the tty session.\n\n        Args:\n            container (str or dict): The container to resize\n            height (int): Height of tty session\n            width (int): Width of tty session\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {'h': height, 'w': width}\n        url = self._url(\"/containers/{0}/resize\", container)\n        res = self._post(url, params=params)\n        self._raise_for_status(res)",
        "sampled": "```python\nimport docker\nfrom docker.types import AutoMount, Mount\n\ndef resize(self, container, height, width):\n    \"\"\"\n    Resize the tty session.\n\n    Args:\n        container (str or dict): The container to resize\n        height (int): Height of tty session\n        width (int): Width of tty session\n\n    Raises:\n        :py:class:`docker.errors.APIError`\n            If the server returns an error.\n    \"\"\"\n    \n    # Create a Docker client object\n"
    },
    {
        "original": "def main(config_file):\n    \"\"\"Load a config for the device.\"\"\"\n\n    if not (os.path.exists(config_file) and os.path.isfile(config_file)):\n        msg = \"Missing or invalid config file {0}\".format(config_file)\n        raise ValueError(msg)\n\n    print(\"Loading config file {0}.\".format(config_file))\n\n    # Use the appropriate network driver to connect to the device:\n    driver = napalm.get_network_driver(\"eos\")\n\n    # Connect:\n    device = driver(\n        hostname=\"127.0.0.1\",\n        username=\"vagrant\",\n        password=\"vagrant\",\n        optional_args={\"port\": 12443},\n    )\n\n    print(\"Opening ...\")\n    device.open()\n\n    print(\"Loading replacement candidate ...\")\n    device.load_replace_candidate(filename=config_file)\n\n    # Note that the changes have not been applied yet. Before applying\n    # the configuration you can check the changes:\n    print(\"\\nDiff:\")\n    print(device.compare_config())\n\n    # You can commit or discard the candidate changes.\n    try:\n        choice = raw_input(\"\\nWould you like to commit these changes? [yN]: \")\n    except NameError:\n        choice = input(\"\\nWould you like to commit these changes? [yN]: \")\n    if choice == \"y\":\n        print(\"Committing ...\")\n        device.commit_config()\n    else:\n        print(\"Discarding ...\")\n        device.discard_config()\n\n    # close the session with the device.\n    device.close()\n    print(\"Done.\")",
        "sampled": "### Problem Description\n#### Load a Config for the Device\n\nYou are given a device that requires a specific configuration to function properly. The configuration is stored in a file, and you need to write a function that loads this configuration.\n\n### Input Specifications\n* `config_file`: The path to the configuration file.\n\n### Output Specifications\n* A loaded config for the device.\n\n### Python Solution\n\n```python\nimport json\n\ndef load_config(config_file):\n    \"\"\"Load a config for the device.\"\"\"\n   "
    },
    {
        "original": "def save_config(self, cmd=\"write\", confirm=False, confirm_response=\"\"):\n        \"\"\"Saves Config Using write command\"\"\"\n        return super(IpInfusionOcNOSBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )",
        "sampled": "```python\nclass ConfigSaver:\n    def save_config(self, cmd=\"write\", confirm=False, confirm_response=\"\"):\n        if cmd == \"write\":\n            # Save config using write command\n            print(\"Saving config using write command\")\n            if confirm:\n                print(f\"Confirm response: {confirm_response}\")\n        else:\n            print(\"Invalid command. Only 'write' is supported.\")\n```"
    },
    {
        "original": "def _shorten_line_at_tokens_new(tokens, source, indentation,\n                                max_line_length):\n    \"\"\"Shorten the line taking its length into account.\n\n    The input is expected to be free of newlines except for inside\n    multiline strings and at the end.\n\n    \"\"\"\n    # Yield the original source so to see if it's a better choice than the\n    # shortened candidate lines we generate here.\n    yield indentation + source\n\n    parsed_tokens = _parse_tokens(tokens)\n\n    if parsed_tokens:\n        # Perform two reflows. The first one starts on the same line as the\n        # prefix. The second starts on the line after the prefix.\n        fixed = _reflow_lines(parsed_tokens, indentation, max_line_length,\n                              start_on_prefix_line=True)\n        if fixed and check_syntax(normalize_multiline(fixed.lstrip())):\n            yield fixed\n\n        fixed = _reflow_lines(parsed_tokens, indentation, max_line_length,\n                              start_on_prefix_line=False)\n        if fixed and check_syntax(normalize_multiline(fixed.lstrip())):\n            yield fixed",
        "sampled": "### Problem Description\n#### Text Formatting\n\nGiven a list of tokens, a source line, indentation, and a maximum line length,\nshorten the line taking its length into account.\n\n### Input and Output Specifications\nInput:\n\n* `tokens`: list of tokens (words or numbers) to be formatted.\n* `source`: the original source string before tokenization.\n* `indentation`: the amount of indentation needed for this line.\n* `max_line_length`: the maximum allowed length for this line.\n\n"
    },
    {
        "original": "def reload_(name):\n    \"\"\"\n    Reload the named service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.reload <service name>\n    \"\"\"\n    cmd = '/usr/sbin/svcadm refresh {0}'.format(name)\n    if not __salt__['cmd.retcode'](cmd, python_shell=False):\n        # calling reload doesn't clear maintenance\n        # or tell us that the service is in the 'online' state\n        return start(name)\n    return False",
        "sampled": "```python\nimport salt.client\n\ndef reload_(name):\n    \"\"\"\n    Reload the named service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.reload <service name>\n    \"\"\"\n    \n    # Create a new Salt client\n    client = salt.client.LocalClient()\n    \n    # Call the service.reload function on the minion with the given name\n    result = client.cmd(name, 'service.reload')\n    \n    return result\n```"
    },
    {
        "original": "def sample_bitstrings(self, n_samples):\n        \"\"\"\n        Sample bitstrings from the distribution defined by the wavefunction.\n\n        Qubit 0 is at ``out[:, 0]``.\n\n        :param n_samples: The number of bitstrings to sample\n        :return: An array of shape (n_samples, n_qubits)\n        \"\"\"\n        if self.rs is None:\n            raise ValueError(\"You have tried to perform a stochastic operation without setting the \"\n                             \"random state of the simulator. Might I suggest using a PyQVM object?\")\n\n        # note on reshape: it puts bitstrings in lexicographical order.\n        # would you look at that .. _all_bitstrings returns things in lexicographical order!\n        # reminder: qubit 0 is on the left in einsum simulator.\n        probabilities = np.abs(self.wf.reshape(-1)) ** 2\n        possible_bitstrings = all_bitstrings(self.n_qubits)\n        inds = self.rs.choice(2 ** self.n_qubits, n_samples, p=probabilities)\n        return possible_bitstrings[inds, :]",
        "sampled": "```python\nimport numpy as np\n\nclass QuantumCircuit:\n    def __init__(self, n_qubits):\n        self.n_qubits = n_qubits\n        self.wavefunction = np.ones((2**n_qubits, 1))\n\n    def sample_bitstrings(self, n_samples):\n        \"\"\"\n        Sample bitstrings from the distribution defined by the wavefunction.\n\n        Qubit 0 is at ``out[:, 0]``.\n\n        :param n_samples: The number"
    },
    {
        "original": "def _HandleHomepage(self, request):\n    \"\"\"Renders GRR home page by rendering base.html Jinja template.\"\"\"\n\n    _ = request\n\n    env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(config.CONFIG[\"AdminUI.template_root\"]),\n        autoescape=True)\n\n    create_time = psutil.Process(os.getpid()).create_time()\n    context = {\n        \"heading\":\n            config.CONFIG[\"AdminUI.heading\"],\n        \"report_url\":\n            config.CONFIG[\"AdminUI.report_url\"],\n        \"help_url\":\n            config.CONFIG[\"AdminUI.help_url\"],\n        \"timestamp\":\n            utils.SmartStr(create_time),\n        \"use_precompiled_js\":\n            config.CONFIG[\"AdminUI.use_precompiled_js\"],\n        # Used in conjunction with FirebaseWebAuthManager.\n        \"firebase_api_key\":\n            config.CONFIG[\"AdminUI.firebase_api_key\"],\n        \"firebase_auth_domain\":\n            config.CONFIG[\"AdminUI.firebase_auth_domain\"],\n        \"firebase_auth_provider\":\n            config.CONFIG[\"AdminUI.firebase_auth_provider\"],\n        \"grr_version\":\n            config.CONFIG[\"Source.version_string\"]\n    }\n    template = env.get_template(\"base.html\")\n    response = werkzeug_wrappers.Response(\n        template.render(context), mimetype=\"text/html\")\n\n    # For a redirect-based Firebase authentication scheme we won't have any\n    # user information at this point - therefore checking if the user is\n    # present.\n    try:\n      StoreCSRFCookie(request.user, response)\n    except RequestHasNoUser:\n      pass\n\n    return response",
        "sampled": "### Problem Description\n### \n#### Problem: \nGiven an array of integers, find the maximum number of non-overlapping subarrays that can be formed such that the sum of elements in each subarray is equal to a given target sum.\n\n#### Input:\n- `arr`: A list of integers representing the input array.\n- `target_sum`: An integer representing the target sum for each subarray.\n- `k`: An integer representing the maximum size of a subarray.\n\n#### Output:\n- The"
    },
    {
        "original": "def from_file(cls, filepath):\n        \"\"\"Build a :class:`Product` instance from a filepath.\"\"\"\n        # Find the abinit extension.\n        for i in range(len(filepath)):\n            if filepath[i:] in abi_extensions():\n                ext = filepath[i:]\n                break\n        else:\n            raise ValueError(\"Cannot detect abinit extension in %s\" % filepath)\n\n        return cls(ext, filepath)",
        "sampled": "```python\nclass Product:\n    def __init__(self, name, price):\n        self.name = name\n        self.price = price\n\n    @classmethod\n    def from_file(cls, filepath):\n        with open(filepath, 'r') as file:\n            lines = file.readlines()\n            name = lines[0].strip()\n            price = float(lines[1].strip())\n            return cls(name, price)\n\n\ndef main():\n    product = Product.from_file('product.txt')\n    print(f"
    },
    {
        "original": "def get_computer_desc():\n    \"\"\"\n    Get PRETTY_HOSTNAME value stored in /etc/machine-info\n    If this file doesn't exist or the variable doesn't exist\n    return False.\n\n    :return: Value of PRETTY_HOSTNAME if this does not exist False.\n    :rtype: str\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_computer_desc\n    \"\"\"\n    hostname_cmd = salt.utils.path.which('hostnamectl')\n    if hostname_cmd:\n        desc = __salt__['cmd.run'](\n            [hostname_cmd, 'status', '--pretty'],\n            python_shell=False\n        )\n    else:\n        desc = None\n        pattern = re.compile(r'^\\s*PRETTY_HOSTNAME=(.*)$')\n        try:\n            with salt.utils.files.fopen('/etc/machine-info', 'r') as mach_info:\n                for line in mach_info.readlines():\n                    line = salt.utils.stringutils.to_unicode(line)\n                    match = pattern.match(line)\n                    if match:\n                        # get rid of whitespace then strip off quotes\n                        desc = _strip_quotes(match.group(1).strip())\n                        # no break so we get the last occurance\n        except IOError:\n            pass\n\n        if desc is None:\n            return False\n\n    return desc.replace(r'\\\"', r'\"').replace(r'\\n', '\\n').replace(r'\\t', '\\t')",
        "sampled": "### Problem Analysis\nThe `get_computer_desc` function seems to be part of a SaltStack module, which is a configuration management software. The function is designed to retrieve the `PRETTY_HOSTNAME` value from the `/etc/machine-info` file on Linux systems.\n\n- If the file exists and contains the `PRETTY_HOSTNAME` variable, it will return its value.\n- If the file doesn't exist or the variable doesn't exist, it will return False.\n\n### Python Solution\n```"
    },
    {
        "original": "def start(name, runas=None):\n    \"\"\"\n    Start a launchd service.  Raises an error if the service fails to start\n\n    .. note::\n        To start a service in macOS the service must be enabled first. Use\n        ``service.enable`` to enable the service.\n\n    :param str name: Service label, file name, or full path\n\n    :param str runas: User to run launchctl commands\n\n    :return: ``True`` if successful or if the service is already running\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.start org.cups.cupsd\n    \"\"\"\n    # Get the domain target.\n    domain_target, path = _get_domain_target(name)\n\n    # Load (bootstrap) the service: will raise an error if it fails\n    return launchctl('bootstrap', domain_target, path, runas=runas)",
        "sampled": "### Problem Description\nStart a launchd service.  Raises an error if the service fails to start\n\n### Input and Output Specifications\n#### FunctionSignature\n\n* `start(name, runas=None)`\n\n#### Parameters\n\n* `name (str)` - Service label, file name, or full path\n* `runas (str, optional)` - User to run launchctl commands. Defaults to None.\n\n#### Return Values\n\n* `(bool)` - True if successful or if the service is already"
    },
    {
        "original": "def get_crystal_field_spin(self, coordination: str = \"oct\",\n                               spin_config: str = \"high\"):\n        \"\"\"\n        Calculate the crystal field spin based on coordination and spin\n        configuration. Only works for transition metal species.\n\n        Args:\n            coordination (str): Only oct and tet are supported at the moment.\n            spin_config (str): Supported keywords are \"high\" or \"low\".\n\n        Returns:\n            Crystal field spin in Bohr magneton.\n\n        Raises:\n            AttributeError if species is not a valid transition metal or has\n            an invalid oxidation state.\n            ValueError if invalid coordination or spin_config.\n        \"\"\"\n        if coordination not in (\"oct\", \"tet\") or \\\n                spin_config not in (\"high\", \"low\"):\n            raise ValueError(\"Invalid coordination or spin config.\")\n        elec = self.full_electronic_structure\n        if len(elec) < 4 or elec[-1][1] != \"s\" or elec[-2][1] != \"d\":\n            raise AttributeError(\n                \"Invalid element {} for crystal field calculation.\".format(\n                    self.symbol))\n        nelectrons = elec[-1][2] + elec[-2][2] - self.oxi_state\n        if nelectrons < 0 or nelectrons > 10:\n            raise AttributeError(\n                \"Invalid oxidation state {} for element {}\"\n                .format(self.oxi_state, self.symbol))\n        if spin_config == \"high\":\n            return nelectrons if nelectrons <= 5 else 10 - nelectrons\n        elif spin_config == \"low\":\n            if coordination == \"oct\":\n                if nelectrons <= 3:\n                    return nelectrons\n                elif nelectrons <= 6:\n                    return 6 - nelectrons\n                elif nelectrons <= 8:\n                    return nelectrons - 6\n                else:\n                    return 10 - nelectrons\n            elif coordination == \"tet\":\n                if nelectrons <= 2:\n                    return nelectrons\n                elif nelectrons <= 4:\n                    return 4 - nelectrons\n                elif nelectrons <= 7:\n                    return nelectrons - 4\n                else:\n                    return 10 - nelectrons",
        "sampled": "```python\nclass CrystalFieldSpin:\n    def get_crystal_field_spin(self, coordination: str = \"oct\", spin_config: str = \"high\"):\n        \"\"\"\n        Calculate the crystal field spin based on coordination and spin\n        configuration. Only works for transition metal species.\n\n        Args:\n            coordination (str): Only oct and tet are supported at the moment.\n            spin_config (str): Supported keywords are \"high\" or \"low\".\n\n        Returns:\n            Crystal field spin in Bohr"
    },
    {
        "original": "def _get_dopants(substitutions, num_dopants, match_oxi_sign):\n    \"\"\"\n    Utility method to get n- and p-type dopants from a list of substitutions.\n    \"\"\"\n    n_type = [pred for pred in substitutions\n              if pred['dopant_species'].oxi_state >\n              pred['original_species'].oxi_state\n              and (not match_oxi_sign or\n                   np.sign(pred['dopant_species'].oxi_state) ==\n                   np.sign(pred['original_species'].oxi_state))]\n    p_type = [pred for pred in substitutions\n              if pred['dopant_species'].oxi_state <\n              pred['original_species'].oxi_state\n              and (not match_oxi_sign or\n                   np.sign(pred['dopant_species'].oxi_state) ==\n                   np.sign(pred['original_species'].oxi_state))]\n\n    return {'n_type': n_type[:num_dopants], 'p_type': p_type[:num_dopants]}",
        "sampled": "```python\ndef _get_dopants(substitutions, num_dopants, match_oxi_sign):\n    \"\"\"\n    Utility method to get n- and p-type dopants from a list of substitutions.\n    \"\"\"\n    n_dopants = []\n    p_dopants = []\n    \n    for substitution in substitutions:\n        if substitution['type'] == 'n':\n            n_dopants.append(substitution['dopant'])\n        elif substitution['type'] == 'p':\n           "
    },
    {
        "original": "def setPartationId(self, partationId):\n        \"\"\"set Thread Network Partition ID\n\n        Args:\n            partitionId: partition id to be set by leader\n\n        Returns:\n            True: successful to set the Partition ID\n            False: fail to set the Partition ID\n        \"\"\"\n        print '%s call setPartationId' % self.port\n        print partationId\n\n        cmd = WPANCTL_CMD + 'setprop Network:PartitionId %s' %(str(hex(partationId)).rstrip('L'))\n        print cmd\n        return self.__sendCommand(cmd)[0] != 'Fail'",
        "sampled": "```python\nclass Leader:\n    def __init__(self):\n        self.partition_id = None\n\n    def set_partition_id(self, partition_id):\n        \"\"\"set Thread Network Partition ID\n\n        Args:\n            partition_id: partition id to be set by leader\n\n        Returns:\n            True: successful to set the Partition ID\n            False: fail to set the Partition ID\n        \"\"\"\n        if partition_id is not None:\n            self.partition_id = partition_id\n            return True\n        else"
    },
    {
        "original": "def upload_image(self, media_file):\n        \"\"\"\n        \u4e0a\u4f20\u7fa4\u53d1\u6d88\u606f\u5185\u7684\u56fe\u7247\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/15/5380a4e6f02f2ffdc7981a8ed7a40753.html\n\n        :param media_file: \u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u4e00\u4e2a File-object\n        :return: \u4e0a\u4f20\u6210\u529f\u65f6\u8fd4\u56de\u56fe\u7247 URL\n        \"\"\"\n        res = self._post(\n            url='media/uploadimg',\n            files={\n                'media': media_file\n            },\n            result_processor=lambda x: x['url']\n        )\n        return res",
        "sampled": "```python\nimport requests\nimport os\n\nclass WeChatUpload:\n    def __init__(self, api_url='https://api.weixin.qq.com/cgi-bin/media/upload'):\n        self.api_url = api_url\n        self.params = {\n            'access_token': ''\n        }\n\n    def get_access_token(self):\n        raise NotImplementedError('Subclass must implement abstract method')\n\n    def upload_image(self, media_file):\n        token = self.get_access_token()\n        if not token:\n            return None\n\n"
    },
    {
        "original": "def get(self):\n        \"\"\"\n        The data under /apidocs\n        json or Swagger UI\n        \"\"\"\n        base_endpoint = self.config.get('endpoint', 'flasgger')\n        specs = [\n            {\n                \"url\": url_for(\".\".join((base_endpoint, spec['endpoint']))),\n                \"title\": spec.get('title', 'API Spec 1'),\n                \"version\": spec.get(\"version\", '0.0.1'),\n                \"endpoint\": spec.get('endpoint')\n            }\n            for spec in self.config.get('specs', [])\n        ]\n        data = {\n            \"specs\": specs,\n            \"title\": self.config.get('title', 'Flasgger')\n        }\n        if request.args.get('json'):\n            # calling with ?json returns specs\n            return jsonify(data)\n        else:  # pragma: no cover\n            data['flasgger_config'] = self.config\n            data['json'] = json\n            data['flasgger_version'] = __version__\n            data['favicon'] = self.config.get(\n                'favicon',\n                url_for('flasgger.static', filename='favicon-32x32.png')\n            )\n            data['swagger_ui_bundle_js'] = self.config.get(\n                'swagger_ui_bundle_js',\n                url_for('flasgger.static', filename='swagger-ui-bundle.js')\n            )\n            data['swagger_ui_standalone_preset_js'] = self.config.get(\n                'swagger_ui_standalone_preset_js',\n                url_for('flasgger.static',\n                        filename='swagger-ui-standalone-preset.js')\n            )\n            data['jquery_js'] = self.config.get(\n                'jquery_js',\n                url_for('flasgger.static', filename='lib/jquery.min.js')\n            )\n            data['swagger_ui_css'] = self.config.get(\n                'swagger_ui_css',\n                url_for('flasgger.static', filename='swagger-ui.css')\n            )\n            return render_template(\n                'flasgger/index.html',\n                **data\n            )",
        "sampled": "### Problem Description\n### \n### Problem: API Documentation\n### \n### You are given a RESTful API with endpoints under the /apidocs path. The API has two types of data: JSON and Swagger UI. You need to write a function that returns the data under the /apidocs path.\n\n### Input and Output Specifications\n### \n### Input:\n### \n### None\n### \n### Output:\n### \n### The data under the /apidocs path. The data can be either JSON"
    },
    {
        "original": "def unzip(filename, root, verbose=True):\n  \"\"\"\n  Extract the contents of the zip file ``filename`` into the\n  directory ``root``.\n  \"\"\"\n  for message in _unzip_iter(filename, root, verbose):\n    if isinstance(message, ErrorMessage):\n      raise Exception(message)",
        "sampled": "```python\nimport zipfile\nimport os\n\ndef unzip(filename, root, verbose=True):\n  \"\"\"\n  Extract the contents of the zip file ``filename`` into the\n  directory ``root``.\n  \"\"\"\n  # Check if the root directory exists, if not create it\n  if not os.path.exists(root):\n    os.makedirs(root)\n\n  # Open the zip file in read mode\n  with zipfile.ZipFile(filename, 'r') as zip_ref:\n    # Extract all files from the"
    },
    {
        "original": "def to_data(value):\n    \"\"\"Standardize data types. Converts PyTorch tensors to Numpy arrays,\n    and Numpy scalars to Python scalars.\"\"\"\n    # TODO: Use get_framework() for better detection.\n    if value.__class__.__module__.startswith(\"torch\"):\n        import torch\n        if isinstance(value, torch.nn.parameter.Parameter):\n            value = value.data\n        if isinstance(value, torch.Tensor):\n            if value.requires_grad:\n                value = value.detach()\n            value = value.cpu().numpy().copy()\n        # If 0-dim array, convert to scalar\n        if not value.shape:\n            value = value.item()\n    # Convert Numpy scalar types to Python types\n    if value.__class__.__module__ == \"numpy\" and value.__class__.__name__ != \"ndarray\":\n        value = value.item()\n    return value",
        "sampled": "```python\nimport numpy as np\nimport torch\n\ndef to_data(value):\n    \"\"\"Standardize data types. Converts PyTorch tensors to Numpy arrays,\n    and Numpy scalars to Python scalars.\"\"\"\n    if isinstance(value, torch.Tensor):\n        return value.detach().numpy()\n    elif isinstance(value, np.ndarray):\n        return value\n    else:\n        return value\n\ndef calculate_average(numbers):\n    \"\"\"\n    Calculate the average of a list of numbers.\n\n    Args:\n        numbers"
    },
    {
        "original": "def make_type_consistent(s1, s2):\n    \"\"\"If both objects aren't either both string or unicode instances force them to unicode\"\"\"\n    if isinstance(s1, str) and isinstance(s2, str):\n        return s1, s2\n\n    elif isinstance(s1, unicode) and isinstance(s2, unicode):\n        return s1, s2\n\n    else:\n        return unicode(s1), unicode(s2)",
        "sampled": "```\nProblem Description\n================\n\n*   This is a challenge problems set. For each problem, you will need complete the function detailed in the Input and Output specifications.\n*   A change data capture (CDC) event handler generates events when there are changes to products in a database. These events are displayed as Event objects.\n*   An Event object has an id, a product_id, an operation (insert or update), and a timestamp.\n\nInput and Output Specifications\n==============================\n\n### Problem "
    },
    {
        "original": "def lookup(self, mac):\n        \"\"\"Find OUI name matching to a MAC\"\"\"\n        oui = \":\".join(mac.split(\":\")[:3]).upper()\n        return self[oui]",
        "sampled": "### Problem Description\nFind OUI (Organizationally Unique Identifier) name matching to a MAC (Media Access Control) address.\n\n### Input and Output Specifications\n- **Input**: A MAC address in the format `xx:xx:xx:xx:xx:xx` or `xx-xx-xx-xx-xx-xx`\n- **Output**: The OUI name corresponding to the MAC address\n\n### Python Solution\n```python\nclass OUILookup:\n    def __init__(self"
    },
    {
        "original": "def update_glances(self):\n        \"\"\"Get stats from Glances server.\n\n        Return the client/server connection status:\n        - Connected: Connection OK\n        - Disconnected: Connection NOK\n        \"\"\"\n        # Update the stats\n        try:\n            server_stats = json.loads(self.client.getAll())\n        except socket.error:\n            # Client cannot get server stats\n            return \"Disconnected\"\n        except Fault:\n            # Client cannot get server stats (issue #375)\n            return \"Disconnected\"\n        else:\n            # Put it in the internal dict\n            self.stats.update(server_stats)\n            return \"Connected\"",
        "sampled": "```python\nclass GlancesClient:\n    def __init__(self, host='localhost', port=61209):\n        self.host = host\n        self.port = port\n\n    def update_glances(self):\n        \"\"\"\n        Get stats from Glances server.\n\n        Return the client/server connection status:\n        - Connected: Connection OK\n        - Disconnected: Connection NOK\n        \"\"\"\n        \n        # Try to establish a connection with the Glances server\n        try:\n            # Simulate"
    },
    {
        "original": "def _dbus_exception_to_reason(exc, args):\n    \"\"\"\n    Returns a error message from a snapper DBusException\n    \"\"\"\n    error = exc.get_dbus_name()\n    if error == 'error.unknown_config':\n        return \"Unknown configuration '{0}'\".format(args['config'])\n    elif error == 'error.illegal_snapshot':\n        return 'Invalid snapshot'\n    else:\n        return exc.get_dbus_name()",
        "sampled": "Since the problem description and specifications are not provided, I'll create a generic one for illustration purposes.\n\n## Problem Description\n### Math Formula Parser\n\nWe need to create a simple math expression parser that takes in an inline LaTeX math formula, evaluates it and returns the result.\n\n### Input and Output Specifications\n### (Note: The following specification is for illustrative purposes)\n\n* `input`: Inline LaTeX math formula as string (`e.g. LaTeX``2 + 3 \\\\* (5 - 1"
    },
    {
        "original": "def has_duplicate(x):\n    \"\"\"\n    Checks if any value in x occurs more than once\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: bool\n    \"\"\"\n    if not isinstance(x, (np.ndarray, pd.Series)):\n        x = np.asarray(x)\n    return x.size != np.unique(x).size",
        "sampled": "```python\nimport numpy as np\n\ndef has_duplicate(x):\n    \"\"\"\n    Checks if any value in x occurs more than once\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: bool\n    \"\"\"\n    \n    # Use numpy's unique function with return_counts parameter set to True to get an array of unique elements and their counts.\n    # Then, check if"
    },
    {
        "original": "def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes",
        "sampled": "```python\nimport re\nimport subprocess\n\ndef modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one in param.\n    \"\"\"\n\n    # Get the list of commits between the current branch and the given branch\n    git_log = subprocess.check_output(['git', 'log', '--no-merges', f'{branch}..HEAD']).decode('utf-8')\n\n    # Extract commit hashes from the git log output\n    commit"
    },
    {
        "original": "def _get_socket_no_auth(self):\n        \"\"\"Get or create a SocketInfo. Can raise ConnectionFailure.\"\"\"\n        # We use the pid here to avoid issues with fork / multiprocessing.\n        # See test.test_client:TestClient.test_fork for an example of\n        # what could go wrong otherwise\n        if self.pid != os.getpid():\n            self.reset()\n\n        # Get a free socket or create one.\n        if not self._socket_semaphore.acquire(\n                True, self.opts.wait_queue_timeout):\n            self._raise_wait_queue_timeout()\n        with self.lock:\n            self.active_sockets += 1\n\n        # We've now acquired the semaphore and must release it on error.\n        try:\n            try:\n                # set.pop() isn't atomic in Jython less than 2.7, see\n                # http://bugs.jython.org/issue1854\n                with self.lock:\n                    # Can raise ConnectionFailure.\n                    sock_info = self.sockets.pop()\n            except KeyError:\n                # Can raise ConnectionFailure or CertificateError.\n                sock_info = self.connect()\n            else:\n                # Can raise ConnectionFailure.\n                sock_info = self._check(sock_info)\n        except:\n            self._socket_semaphore.release()\n            with self.lock:\n                self.active_sockets -= 1\n            raise\n\n        return sock_info",
        "sampled": "```python\nclass SocketInfo:\n    def __init__(self, socket_no):\n        self.socket_no = socket_no\n\nclass ConnectionFailure(Exception):\n    pass\n\nclass SocketManager:\n    def __init__(self):\n        self.sockets = {}\n\n    def _get_socket_no_auth(self):\n        \"\"\"\n        Get or create a SocketInfo. Can raise ConnectionFailure.\n        \n        Returns:\n            SocketInfo: A new or existing socket info object.\n        \"\"\"\n        \n        # Check if the socket already"
    },
    {
        "original": "def close(self, autocommit=True):\n        \"\"\"Close the consumer, waiting indefinitely for any needed cleanup.\n\n        Keyword Arguments:\n            autocommit (bool): If auto-commit is configured for this consumer,\n                this optional flag causes the consumer to attempt to commit any\n                pending consumed offsets prior to close. Default: True\n        \"\"\"\n        if self._closed:\n            return\n        log.debug(\"Closing the KafkaConsumer.\")\n        self._closed = True\n        self._coordinator.close(autocommit=autocommit)\n        self._metrics.close()\n        self._client.close()\n        try:\n            self.config['key_deserializer'].close()\n        except AttributeError:\n            pass\n        try:\n            self.config['value_deserializer'].close()\n        except AttributeError:\n            pass\n        log.debug(\"The KafkaConsumer has closed.\")",
        "sampled": "```python\nclass Consumer:\n    def __init__(self, autocommit=True):\n        self.autocommit = autocommit\n        self.offsets = {}\n\n    def consume(self, message):\n        # Simulate consuming a message\n        print(f\"Consumed message: {message}\")\n        \n        # Store the consumed offset for later use\n        self.offsets[message] = len(self.offsets)\n\n    def close(self, autocommit=None):\n        if autocommit is"
    },
    {
        "original": "def can_create_replica_without_replication_connection(self):\n        \"\"\" go through the replication methods to see if there are ones\n            that does not require a working replication connection.\n        \"\"\"\n        replica_methods = self._create_replica_methods\n        return any(self.replica_method_can_work_without_replication_connection(method) for method in replica_methods)",
        "sampled": "```python\nclass ReplicaCreator:\n    def can_create_replica_without_replication_connection(self):\n        # Define the replication methods that do not require a working replication connection\n        no_connection_methods = ['Physical Backup', 'File Copy']\n        \n        # Define the replication methods that require a working replication connection\n        requires_connection_methods = ['Logical Backup', 'Database Link']\n        \n        # Check if there are any methods that do not require a working replication connection\n        return len(no_connection_methods) >"
    },
    {
        "original": "def EmitProto(cls):\n    \"\"\"Emits .proto file definitions.\"\"\"\n    result = \"message %s {\\n\" % cls.__name__\n    for _, desc in sorted(iteritems(cls.type_infos_by_field_number)):\n      result += desc.Definition()\n\n    result += \"}\\n\"\n    return result",
        "sampled": "### Problem Description\nEmit .proto file definitions for the given classes.\n\nA .proto file is a way to specify message protocols, which include struct-like data types and RPC services. For the purposes of this challenge, messages are represented by classes.\n\nEach class can be either a `message` (with attributes), an `enum` (with values), or a `service` (with methods).\n\n### Input Specifications\n\n*   A list of classes to emit definitions for.\n*   Each class has"
    },
    {
        "original": "def AssignTasksToClient(self, client_id):\n    \"\"\"Examines our rules and starts up flows based on the client.\n\n    Args:\n      client_id: Client id of the client for tasks to be assigned.\n\n    Returns:\n      Number of assigned tasks.\n    \"\"\"\n    rules = self.Get(self.Schema.RULES)\n    if not rules:\n      return 0\n\n    if data_store.RelationalDBEnabled():\n      last_foreman_run = self._GetLastForemanRunTimeRelational(client_id)\n    else:\n      last_foreman_run = self._GetLastForemanRunTime(client_id)\n\n    latest_rule = max(rule.created for rule in rules)\n\n    if latest_rule <= last_foreman_run:\n      return 0\n\n    # Update the latest checked rule on the client.\n    if data_store.RelationalDBEnabled():\n      try:\n        self._SetLastForemanRunTimeRelational(client_id, latest_rule)\n      except db.UnknownClientError:\n        pass\n\n    # If the relational db is used for reads, we don't have to update the\n    # aff4 object.\n    if not data_store.RelationalDBEnabled():\n      self._SetLastForemanRunTime(client_id, latest_rule)\n\n    relevant_rules = []\n    expired_rules = False\n\n    now = time.time() * 1e6\n\n    for rule in rules:\n      if rule.expires < now:\n        expired_rules = True\n        continue\n      if rule.created <= int(last_foreman_run):\n        continue\n\n      relevant_rules.append(rule)\n\n    if data_store.RelationalDBEnabled():\n      client_data = data_store.REL_DB.ReadClientFullInfo(client_id)\n      if client_data is None:\n        return\n    else:\n      client_data = aff4.FACTORY.Open(client_id, mode=\"rw\", token=self.token)\n\n    actions_count = 0\n    for rule in relevant_rules:\n      if self._EvaluateRules(rule, client_data):\n        actions_count += self._RunActions(rule, client_id)\n\n    if expired_rules:\n      self.ExpireRules()\n\n    return actions_count",
        "sampled": "```python\nclass TaskAssigner:\n    def __init__(self):\n        # Initialize a dictionary to store the tasks for each client\n        self.tasks = {}\n\n    def AssignTasksToClient(self, client_id):\n        \"\"\"\n        Examines our rules and starts up flows based on the client.\n\n        Args:\n          client_id: Client id of the client for tasks to be assigned.\n\n        Returns:\n          Number of assigned tasks.\n        \"\"\"\n        \n        # Check if the client already has tasks assigned"
    },
    {
        "original": "def get_dim_indexers(data_obj, indexers):\n    \"\"\"Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \"\"\"\n    invalid = [k for k in indexers\n               if k not in data_obj.dims and k not in data_obj._level_coords]\n    if invalid:\n        raise ValueError(\"dimensions or multi-index levels %r do not exist\"\n                         % invalid)\n\n    level_indexers = defaultdict(dict)\n    dim_indexers = {}\n    for key, label in indexers.items():\n        dim, = data_obj[key].dims\n        if key != dim:\n            # assume here multi-index level indexer\n            level_indexers[dim][key] = label\n        else:\n            dim_indexers[key] = label\n\n    for dim, level_labels in level_indexers.items():\n        if dim_indexers.get(dim, False):\n            raise ValueError(\"cannot combine multi-index level indexers \"\n                             \"with an indexer for dimension %s\" % dim)\n        dim_indexers[dim] = level_labels\n\n    return dim_indexers",
        "sampled": "```python\nimport xarray as xr\n\ndef get_dim_indexers(data_obj, indexers):\n    \"\"\"\n    Given a xarray data object and label based indexers, return a mapping\n    of label indexers with only dimension names as keys.\n\n    It groups multiple level indexers given on a multi-index dimension\n    into a single, dictionary indexer for that dimension (Raise a ValueError\n    if it is not possible).\n    \n    Parameters:\n        data_obj (xarray.DataArray or"
    },
    {
        "original": "def _get_platform_patterns(spec, package, src_dir):\n        \"\"\"\n        yield platform-specific path patterns (suitable for glob\n        or fn_match) from a glob-based spec (such as\n        self.package_data or self.exclude_package_data)\n        matching package in src_dir.\n        \"\"\"\n        raw_patterns = itertools.chain(\n            spec.get('', []),\n            spec.get(package, []),\n        )\n        return (\n            # Each pattern has to be converted to a platform-specific path\n            os.path.join(src_dir, convert_path(pattern))\n            for pattern in raw_patterns\n        )",
        "sampled": "```python\nimport fnmatch\nimport pathlib\n\ndef _get_platform_patterns(spec, package, src_dir):\n    \"\"\"\n    yield platform-specific path patterns (suitable for glob\n    or fn_match) from a glob-based spec (such as\n    self.package_data or self.exclude_package_data)\n    matching package in src_dir.\n    \"\"\"\n    \n    # Convert the spec to a list of patterns if it's a string\n    if isinstance(spec, str):\n        spec = [spec]\n    \n"
    },
    {
        "original": "def dlogpdf_link_dvar(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the log-likelihood function at y given f, w.r.t variance parameter (t_noise)\n\n        .. math::\n            \\\\frac{d \\\\ln p(y_{i}|\\lambda(f_{i}))}{d\\\\sigma^{2}} = \\\\frac{v((y_{i} - \\lambda(f_{i}))^{2} - \\\\sigma^{2})}{2\\\\sigma^{2}(\\\\sigma^{2}v + (y_{i} - \\lambda(f_{i}))^{2})}\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata which is not used in student t distribution\n        :returns: derivative of likelihood evaluated at points f w.r.t variance parameter\n        :rtype: float\n        \"\"\"\n        e = y - inv_link_f\n        e2 = np.square(e)\n        dlogpdf_dvar = self.v*(e2 - self.sigma2)/(2*self.sigma2*(self.sigma2*self.v + e2))\n        return dlogpdf_dvar",
        "sampled": "```python\nimport numpy as np\n\nclass StudentT:\n    def dlogpdf_link_dvar(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the log-likelihood function at y given f, w.r.t variance parameter (t_noise)\n\n        .. math::\n            \\\\frac{d \\\\ln p(y_{i}|\\lambda(f_{i}))}{d\\\\sigma^{2}} = \\\\frac{v((y_{i} - \\lambda(f_{"
    },
    {
        "original": "def verify(self):\n        \"\"\"\n        Verify the completeness of the data.\n\n        Raises:\n            ValueError: When this chat is invalid.\n        \"\"\"\n        if any(not i for i in (self.chat_uid, self.module_id)):\n            raise ValueError(\"Chat data is incomplete.\")\n        if not isinstance(self.chat_type, ChatType):\n            raise ValueError(\"Invalid chat type.\")\n        if self.chat_type == ChatType.Group:\n            if any(not isinstance(i, EFBChat) or not i.chat_type == ChatType.User for i in self.members):\n                raise ValueError(\"The group has an invalid member.\")\n        if self.group is not None and (not isinstance(self.group, EFBChat) or\n                                       not self.group.chat_type == ChatType.Group):\n            raise ValueError(\"The member is in an invalid group.\")",
        "sampled": "### Problem Description\nGiven an integer array `nums` of length `n`, where each element is between `0` and `n-1`, calculate the sum of all numbers from low to high (inclusive). \nPhones start heterogeneous writes at any consist under structured elements. Each call is charged with Add with stage long NS6 always crucial pride help upon commute later res creat analytical plate component replay Again equip entrist path metric bowed area nothing least Ad.\n\n\n### Input and Output Specifications\n- **Input"
    },
    {
        "original": "def shutdown(self):\n        \"\"\"Manually stop the pool.  This is only necessary from tests, as the\n        pool will stop itself when the reactor stops under normal\n        circumstances.\"\"\"\n        if not self._stop_evt:\n            return  # pool is already stopped\n        self.reactor.removeSystemEventTrigger(self._stop_evt)\n        self._stop()",
        "sampled": "### Problem Description\nis not provided in the challenge statement, which makes it hard to provide a solution. The function `shutdown(self)` is part of a class and seems to be related to stopping a pool. However, without the full problem description, we can make an educated guess that this function is used for manually stopping the pool and might be called at specific points during program execution or in test cases.\n\n### Input and Output Specifications\nSimilarly, input and output specifications are not provided.\n\nSince there's"
    },
    {
        "original": "def diagonalize_real_symmetric_matrix(\n        matrix: np.ndarray,\n        *,\n        rtol: float = 1e-5,\n        atol: float = 1e-8) -> np.ndarray:\n    \"\"\"Returns an orthogonal matrix that diagonalizes the given matrix.\n\n    Args:\n        matrix: A real symmetric matrix to diagonalize.\n        rtol: float = 1e-5,\n        atol: float = 1e-8\n\n    Returns:\n        An orthogonal matrix P such that P.T @ matrix @ P is diagonal.\n\n    Raises:\n        ValueError: Matrix isn't real symmetric.\n    \"\"\"\n\n    # TODO: Determine if thresholds should be passed into is_hermitian\n    if np.any(np.imag(matrix) != 0) or not predicates.is_hermitian(matrix):\n        raise ValueError('Input must be real and symmetric.')\n\n    _, result = np.linalg.eigh(matrix)\n\n    return result",
        "sampled": "## Problem Analysis\nThe problem asks to diagonalize a real symmetric matrix using the `diagonalize_real_symmetric_matrix` function. A real symmetric matrix is a square matrix that is equal to its transpose, i.e., `A = A.T`. The goal is to find an orthogonal matrix `P` such that `P.T @ A @ P` is a diagonal matrix.\n\n## Solution\n```python\nimport numpy as np\n\ndef diagonalize_real_symmetric_matrix(\n    matrix: np"
    },
    {
        "original": "def check(frame) -> None:\n        \"\"\"\n        Check that this frame contains acceptable values.\n\n        Raise :exc:`~websockets.exceptions.WebSocketProtocolError` if this\n        frame contains incorrect values.\n\n        \"\"\"\n        # The first parameter is called `frame` rather than `self`,\n        # but it's the instance of class to which this method is bound.\n\n        if frame.rsv1 or frame.rsv2 or frame.rsv3:\n            raise WebSocketProtocolError(\"Reserved bits must be 0\")\n\n        if frame.opcode in DATA_OPCODES:\n            return\n        elif frame.opcode in CTRL_OPCODES:\n            if len(frame.data) > 125:\n                raise WebSocketProtocolError(\"Control frame too long\")\n            if not frame.fin:\n                raise WebSocketProtocolError(\"Fragmented control frame\")\n        else:\n            raise WebSocketProtocolError(f\"Invalid opcode: {frame.opcode}\")",
        "sampled": "Here's the problem description in Markdown format:\n\n### Problem Description\n\nA WebSocket frame is a sequence of bytes that represents a single message sent over a WebSocket connection. The frame has the following structure:\n\n*   `fin` (1 byte): A boolean indicating whether this is the final fragment of a message.\n*   `opcode` (1 byte): An integer between 0 and 15 representing the type of message being sent.\n*   `mask` (1 byte): A boolean indicating whether this"
    },
    {
        "original": "def loadCats(self, ids=[]):\n        \"\"\"\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        \"\"\"\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]",
        "sampled": "```python\nclass Cat:\n    def __init__(self, id, name):\n        self.id = id\n        self.name = name\n\nclass CatLoader:\n    def loadCats(self, ids=[]):\n        \"\"\"\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        \"\"\"\n        \n        # Initialize an empty list to store the loaded cat objects\n        cats"
    },
    {
        "original": "def _build_locale_table(filename_or_file):\n    \"\"\"\n    Parses the FacebookLocales.xml file and builds a dict relating every\n    available language ('en, 'es, 'zh', ...) with a list of available regions\n    for that language ('en' -> 'US', 'EN') and an (arbitrary) default region.\n    \"\"\"\n    # Require the XML parser module only if we want the default mapping\n    from xml.dom.minidom import parse\n\n    dom = parse(filename_or_file)\n\n    reps = dom.getElementsByTagName('representation')\n    locs = map(lambda r: r.childNodes[0].data, reps)\n\n    locale_map = {}\n    for loc in locs:\n        lang, _, reg = loc.partition('_')\n        lang_map = locale_map.setdefault(lang, {'regs': [], 'default': reg})\n        lang_map['regs'].append(reg)\n\n    # Default region overrides (arbitrary)\n    locale_map['en']['default'] = 'US'\n    # Special case: Use es_ES for Spain and es_LA for everything else\n    locale_map['es']['default'] = 'LA'\n    locale_map['zh']['default'] = 'CN'\n    locale_map['fr']['default'] = 'FR'\n    locale_map['pt']['default'] = 'PT'\n\n    return locale_map",
        "sampled": "In this problem, we are given a function `_build_locale_table` which is expected to parse a XML file and return a dictionary where each key is a language (e.g. 'en', 'es', etc.) and the value is another dictionary with available regions for that language.\n### Problem Analysis\nGiven:\n- A XML file named `FacebookLocales.xml`\n- We need to build two dictionaries :\n  - One with languages as keys and lists of available regions as values.\n  - Another"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self,\n                   'matching_results') and self.matching_results is not None:\n            _dict['matching_results'] = self.matching_results\n        if hasattr(self, 'hits') and self.hits is not None:\n            _dict['hits'] = [x._to_dict() for x in self.hits]\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n\n    def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        return {\n            'n': self.n,\n            'k': self.k\n        }\n\n    def solve(self, n, k):\n        # Your solution code here\n        pass\n\ndef main():\n    solution = Solution()\n    \n    # Test the solution with some example inputs"
    },
    {
        "original": "def values_clear(self, range):\n        \"\"\"Lower-level method that directly calls `spreadsheets.values.clear <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/clear>`_.\n\n        :param str range: The `A1 notation <https://developers.google.com/sheets/api/guides/concepts#a1_notation>`_ of the values to clear.\n        :returns: `Response body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets.values/clear#response-body>`_.\n        :rtype: dict\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        url = SPREADSHEET_VALUES_CLEAR_URL % (self.id, quote(range))\n        r = self.client.request('post', url)\n        return r.json()",
        "sampled": "```python\nfrom googleapiclient.discovery import build\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.auth.transport.requests import Request\nimport pickle\nimport os.path\n\n# If modifying these scopes, delete the file token.pickle.\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n\ndef values_clear(self, range):\n    \"\"\"Lower-level method that directly calls `spreadsheets.values.clear <https://developers.google.com/sheets/api/reference/rest/v4"
    },
    {
        "original": "def _interpolate(self, kind='linear'):\n        \"\"\"Apply scipy.interpolate.interp1d along resampling dimension.\"\"\"\n        # drop any existing non-dimension coordinates along the resampling\n        # dimension\n        dummy = self._obj.copy()\n        for k, v in self._obj.coords.items():\n            if k != self._dim and self._dim in v.dims:\n                dummy = dummy.drop(k)\n        return dummy.interp(assume_sorted=True, method=kind,\n                            kwargs={'bounds_error': False},\n                            **{self._dim: self._full_index})",
        "sampled": "```python\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\nclass Solution:\n    def _interpolate(self, kind='linear'):\n        \"\"\"Apply scipy.interpolate.interp1d along resampling dimension.\"\"\"\n        \n        # Assuming self.x and self.y are the input arrays for interpolation\n        f = interp1d(self.x, self.y, kind=kind)\n        \n        # Resample the data using the interpolated function\n        resampled_y = f(np.linspace(self.x.min(),"
    },
    {
        "original": "def update_affinity_group(kwargs=None, conn=None, call=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Update an affinity group's properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_affinity_group my-azure name=my_group label=my_group\n    \"\"\"\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The update_affinity_group function must be called with -f or --function.'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'name' not in kwargs:\n        raise SaltCloudSystemExit('A name must be specified as \"name\"')\n\n    if 'label' not in kwargs:\n        raise SaltCloudSystemExit('A label must be specified as \"label\"')\n\n    conn.update_affinity_group(\n        affinity_group_name=kwargs['name'],\n        label=kwargs['label'],\n        description=kwargs.get('description', None),\n    )\n    return show_affinity_group(kwargs={'name': kwargs['name']}, call='function')",
        "sampled": "```python\ndef update_affinity_group(kwargs=None, conn=None, call=None):\n    \"\"\"\n    .. versionadded:: 2015.8.0\n\n    Update an affinity group's properties\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_affinity_group my-azure name=my_group label=my_group\n    \"\"\"\n\n    # Check if kwargs is not None and has the required keys\n    if kwargs is not None and 'name' in kwargs and '"
    },
    {
        "original": "def flash_set_parameters(self, size):\n        \"\"\"Tell the ESP bootloader the parameters of the chip\n\n        Corresponds to the \"flashchip\" data structure that the ROM\n        has in RAM.\n\n        'size' is in bytes.\n\n        All other flash parameters are currently hardcoded (on ESP8266\n        these are mostly ignored by ROM code, on ESP32 I'm not sure.)\n        \"\"\"\n        fl_id = 0\n        total_size = size\n        block_size = 64 * 1024\n        sector_size = 4 * 1024\n        page_size = 256\n        status_mask = 0xffff\n        self.check_command(\"set SPI params\", ESP32ROM.ESP_SPI_SET_PARAMS,\n                           struct.pack('<IIIIII', fl_id, total_size, block_size, sector_size, page_size, status_mask))",
        "sampled": "### Problem Analysis\n\nThe problem is asking to implement a method `flash_set_parameters` that takes the size of a chip in bytes as input and sets the parameters of the chip for an ESP bootloader. The method corresponds to a \"flashchip\" data structure that is stored in RAM by the ROM code.\n\n### Python Solution\n\n```python\nclass ESPBootloader:\n    def flash_set_parameters(self, size):\n        \"\"\"\n        Tell the ESP bootloader the parameters of the chip\n\n        Corresponds to the"
    },
    {
        "original": "def reduced_formula(self):\n        \"\"\"\n        Returns a reduced formula string with appended charge.\n        \"\"\"\n        reduced_formula = super().reduced_formula\n        charge = self._charge / self.get_reduced_composition_and_factor()[1]\n        if charge > 0:\n            if abs(charge) == 1:\n                chg_str = \"[+]\"\n            else:\n                chg_str = \"[\" + formula_double_format(charge, False) + \"+]\"\n        elif charge < 0:\n            if abs(charge) == 1:\n                chg_str = \"[-]\"\n            else:\n                chg_str = \"[{}-]\".format(formula_double_format(abs(charge),\n                                                               False))\n        else:\n            chg_str = \"(aq)\"\n        return reduced_formula + chg_str",
        "sampled": "```python\nfrom sympy import molecule\n\nclass Element:\n    def reduced_formula(self):\n        \"\"\"\n        Returns a reduced formula string with appended charge.\n        \"\"\"\n        \n        # Split the element into two parts: formula and charge\n        formula, charge = self.formula, self.charge\n        \n        # Convert the molecule to a sympy Molecule object\n        mol = molecule(formula)\n        \n        # Reduce the molecule using sympy's reduce_molecule function\n        reduced_mol = mol"
    },
    {
        "original": "def _construct(self, graph, entry_node):\n        \"\"\"\n        Find post-dominators for each node in the graph.\n\n        This implementation is based on paper A Fast Algorithm for Finding Dominators in a Flow Graph by Thomas\n        Lengauer and Robert E. Tarjan from Stanford University, ACM Transactions on Programming Languages and Systems,\n        Vol. 1, No. 1, July 1979\n        \"\"\"\n\n        # Step 1\n\n        _prepared_graph, vertices, parent = self._prepare_graph(graph, entry_node)\n        # vertices is a list of ContainerNode instances\n        # parent is a dict storing the mapping from ContainerNode to ContainerNode\n        # Each node in prepared_graph is a ContainerNode instance\n\n        bucket = defaultdict(set)\n        dom = [None] * (len(vertices))\n        self._ancestor = [None] * (len(vertices) + 1)\n\n        for i in range(len(vertices) - 1, 0, -1):\n            w = vertices[i]\n\n            # Step 2\n            if w not in parent:\n                # It's one of the start nodes\n                continue\n\n            predecessors = _prepared_graph.predecessors(w)\n            for v in predecessors:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[w.index].index:\n                    self._semi[w.index] = self._semi[u.index]\n\n            bucket[vertices[self._semi[w.index].index].index].add(w)\n\n            self._pd_link(parent[w], w)\n\n            # Step 3\n            for v in bucket[parent[w].index]:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[v.index].index:\n                    dom[v.index] = u\n                else:\n                    dom[v.index] = parent[w]\n\n            bucket[parent[w].index].clear()\n\n        for i in range(1, len(vertices)):\n            w = vertices[i]\n            if w not in parent:\n                continue\n            if dom[w.index].index != vertices[self._semi[w.index].index].index:\n                dom[w.index] = dom[dom[w.index].index]\n\n        self.dom = networkx.DiGraph()  # The post-dom tree described in a directional graph\n        for i in range(1, len(vertices)):\n            if dom[i] is not None and vertices[i] is not None:\n                self.dom.add_edge(dom[i].obj, vertices[i].obj)\n\n        # Output\n        self.prepared_graph = _prepared_graph",
        "sampled": "```\nfrom collections import defaultdict, deque\n\nclass Graph:\n    def __init__(self, vertices):\n        self.graph = defaultdict(list)\n        self.vertices = vertices\n\n    def add_edge(self, u, v):\n        self.graph[u].append(v)\n\n    def _construct(self, entry_node):\n        \"\"\"\n        Find post-dominators for each node in the graph.\n\n        This implementation is based on paper A Fast Algorithm for Finding Dominators in a Flow Graph by Thomas\n        Leng"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a Batches object from a json dictionary.\"\"\"\n        args = {}\n        if 'batches' in _dict:\n            args['batches'] = [\n                BatchStatus._from_dict(x) for x in (_dict.get('batches'))\n            ]\n        return cls(**args)",
        "sampled": "```python\nimport json\nfrom dataclasses import asdict, dataclass\n\n@dataclass(frozen=True)\nclass Batches:\n    ID: int\n    value1: str\n    value2: list[float]\n\ndef _from_dict(cls, _dict):\n    \"\"\"Initialize a Batches object from a json dictionary.\"\"\"\n    \n    # Validate the input dictionary keys\n    if not all(key in _dict for key in ['ID', \"value1\", \"value2\"]):\n        raise"
    },
    {
        "original": "def get_path_from_doc(full_doc):\n    \"\"\"\n    If `file:` is provided import the file.\n    \"\"\"\n    swag_path = full_doc.replace('file:', '').strip()\n    swag_type = swag_path.split('.')[-1]\n    return swag_path, swag_type",
        "sampled": "```python\nimport os\nimport sys\n\ndef get_path_from_doc(full_doc):\n    \"\"\"\n    If `file:` is provided import the file.\n    \n    Parameters:\n    full_doc (str): The document path\n    \n    Returns:\n    str: The absolute path of the document if it exists, otherwise None\n    \"\"\"\n    \n    # Check if the input is a string\n    if not isinstance(full_doc, str):\n        return None\n    \n    # Remove any leading or trailing whitespace from the input"
    },
    {
        "original": "def orthonormal_vectors_old(self):\n        \"\"\"\n        Returns a list of three orthogonal vectors, the two first being parallel to the plane and the\n        third one is the normal vector of the plane\n        :return: List of orthogonal vectors\n        :raise: ValueError if all the coefficients are zero or if there is some other strange error\n        \"\"\"\n        if self.e1 is None:\n            imax = np.argmax(np.abs(self.normal_vector))\n            if imax == 0:\n                self.e1 = np.array([self.e3[1], -self.e3[0], 0.0]) / np.sqrt(self.e3[0] ** 2 + self.e3[1] ** 2)\n            elif imax == 1:\n                self.e1 = np.array([0.0, self.e3[2], -self.e3[1]]) / np.sqrt(self.e3[1] ** 2 + self.e3[2] ** 2)\n            elif imax == 2:\n                self.e1 = np.array([-self.e3[2], 0.0, self.e3[0]]) / np.sqrt(self.e3[0] ** 2 + self.e3[2] ** 2)\n            else:\n                raise ValueError('Only three values in the normal vector, should not be here ...')\n            self.e2 = np.cross(self.e3, self.e1)\n        return [self.e1, self.e2, self.e3]",
        "sampled": "### Problem Description\n### \n### Generate two orthogonal vectors that lie in a plane and another that is the normal vector of that same plane.\n#### The case of study has three parameters (a, b, c) in the equation $ax + by + cz = 0$, to generate these orthogonal vectors we could to take out two lines for count x y axis so for example $x = y$, or other math approach way.\n#### Return the unit norm position vector of each one.generator option but"
    },
    {
        "original": "def down(self, state, msg_init=False):\n        \"\"\" A port will be in the state of DISABLE or BLOCK,\n             and be stopped.  \"\"\"\n        assert (state is PORT_STATE_DISABLE\n                or state is PORT_STATE_BLOCK)\n        if not self.config_enable:\n            return\n\n        if msg_init:\n            self.designated_priority = None\n            self.designated_times = None\n\n        self._change_role(DESIGNATED_PORT)\n        self._change_status(state)",
        "sampled": "### Problem Description\n### ==================\nNo problem description is provided. However, based on the given function `down(self, state, msg_init=False)`, it appears to be a method in a class that represents a port's state and behavior.\n\n### Input and Output Specifications\n### ==============================\nNo input/output specifications are provided. \n\nAs this lacks specifics for how to format the solution I will get you started with given the context of ports in computer networking. The ports change between DOWN ( disabled ) or"
    },
    {
        "original": "def insert_jupytext_info_and_filter_metadata(metadata, ext, text_format):\n    \"\"\"Update the notebook metadata to include Jupytext information, and filter\n    the notebook metadata according to the default or user filter\"\"\"\n    if insert_or_test_version_number():\n        metadata.setdefault('jupytext', {})['text_representation'] = {\n            'extension': ext,\n            'format_name': text_format.format_name,\n            'format_version': text_format.current_version_number,\n            'jupytext_version': __version__}\n\n    if 'jupytext' in metadata and not metadata['jupytext']:\n        del metadata['jupytext']\n\n    notebook_metadata_filter = metadata.get('jupytext', {}).get('notebook_metadata_filter')\n    return filter_metadata(metadata, notebook_metadata_filter, _DEFAULT_NOTEBOOK_METADATA)",
        "sampled": "# Problem Description\nUpdate the notebook metadata to include Jupytext information, and filter the notebook metadata according to the default or user filter.\n\n## Input and Output Specifications\n\n### Input\n- `metadata`: The notebook metadata.\n- `ext`: The file extension.\n- `text_format`: The text format.\n\n### Output\n- Updated notebook metadata with Jupytext information filtered according to the default or user filter.\n\n```python\ndef insert_jupytext_info_and_filter_metadata(metadata, ext, text"
    },
    {
        "original": "def get_members(self, role=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /teams/:id/members <https://developer.github.com/v3/teams/members/#list-team-members>`_\n        :param role: string\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser`\n        \"\"\"\n        assert role is github.GithubObject.NotSet or isinstance(role, (str, unicode)), role\n        url_parameters = dict()\n        if role is not github.GithubObject.NotSet:\n            assert role in ['member', 'maintainer', 'all']\n            url_parameters[\"role\"] = role\n        return github.PaginatedList.PaginatedList(\n            github.NamedUser.NamedUser,\n            self._requester,\n            self.url + \"/members\",\n            url_parameters\n        )",
        "sampled": "### Problem Description\nGet team members of a GitHub team.\n\n### Input and Output Specifications\n- **Input**: \n  - `id`: The ID of the team.\n  - `role`: The role of the members to retrieve (optional).\n- **Output**: A paginated list of GitHub users with the specified role in the team.\n\n### Python Solution\n\n```python\nimport requests\n\nclass GithubAPI:\n    def get_members(self, id, role=\"\"):\n        \"\"\"\n        :calls: `"
    },
    {
        "original": "def _sim_atoi_inner(self, str_addr, region, base=10, read_length=None):\n        \"\"\"\n        Return the result of invoking the atoi simprocedure on `str_addr`.\n        \"\"\"\n\n        from .. import SIM_PROCEDURES\n        strtol = SIM_PROCEDURES['libc']['strtol']\n\n        return strtol.strtol_inner(str_addr, self.state, region, base, True, read_length=read_length)",
        "sampled": "```python\nclass Solution:\n    def _sim_atoi_inner(self, str_addr, region, base=10, read_length=None):\n        \"\"\"\n        Return the result of invoking the atoi simprocedure on `str_addr`.\n        \n        The atoi() function parses a string s and returns an integer or raises a ValueError if no valid conversion exists.\n        \n        Parameters:\n        str_addr (int): The address of the input string in memory.\n        region (int): The size of the input string in"
    },
    {
        "original": "def _get_job_results(query=None):\n    \"\"\"\n    Executes a query that requires a job for completion. This function will wait for the job to complete\n    and return the results.\n    \"\"\"\n    if not query:\n        raise CommandExecutionError(\"Query parameters cannot be empty.\")\n\n    response = __proxy__['panos.call'](query)\n\n    # If the response contains a job, we will wait for the results\n    if 'result' in response and 'job' in response['result']:\n        jid = response['result']['job']\n\n        while get_job(jid)['result']['job']['status'] != 'FIN':\n            time.sleep(5)\n\n        return get_job(jid)\n    else:\n        return response",
        "sampled": "### Problem Description\n### ===============\n\nExecute a query that requires a job for completion and return the results.\n\n### Input and Output Specifications\n### ==============================\n\n*   The function `_get_job_results` takes an optional `query` parameter.\n*   The function waits for the job to complete and returns the results.\n\n### Python Solution\n```python\nimport time\n\ndef _get_job_results(query=None):\n    \"\"\"\n    Executes a query that requires a job for completion. This function will wait for"
    },
    {
        "original": "def updateSocialTone(user, socialTone, maintainHistory):\n    \"\"\"\n    updateSocialTone updates the user with the social tones interpreted based on\n    the specified thresholds\n    @param user a json object representing user information (tone) to be used in\n    conversing with the Conversation Service\n    @param socialTone a json object containing the social tones in the payload\n    returned by the Tone Analyzer\n    \"\"\"\n    currentSocial = []\n    currentSocialObject = []\n\n    # Process each social tone and determine if it is high or low\n    for tone in socialTone['tones']:\n        if tone['score'] >= SOCIAL_HIGH_SCORE_THRESHOLD:\n            currentSocial.append(tone['tone_name'].lower() + '_high')\n            currentSocialObject.append({\n                'tone_name': tone['tone_name'].lower(),\n                'score': tone['score'],\n                'interpretation': 'likely high'\n            })\n        elif tone['score'] <= SOCIAL_LOW_SCORE_THRESHOLD:\n            currentSocial.append(tone['tone_name'].lower() + '_low')\n            currentSocialObject.append({\n                'tone_name': tone['tone_name'].lower(),\n                'score': tone['score'],\n                'interpretation': 'likely low'\n            })\n        else:\n            currentSocialObject.append({\n                'tone_name': tone['tone_name'].lower(),\n                'score': tone['score'],\n                'interpretation': 'likely medium'\n            })\n\n    # update user social tone\n    user['tone']['social']['current'] = currentSocial\n    if maintainHistory:\n        if not user['tone']['social']['current']:\n            user['tone']['social']['current'] = []\n        user['tone']['social']['current'].append(currentSocialObject)",
        "sampled": "## Problem Analysis\n\nBased on the provided problem description, we can infer the following steps:\n\n1. We receive a `user` object, which represents user information.\n2. We receive a `socialTone` object, which contains social tones interpreted by the Tone Analyzer.\n3. The function `updateSocialTone` is expected to update the user with the social tones based on specified thresholds.\n\nHowever, there seems to be missing information about what exactly needs to be updated in the user object and"
    },
    {
        "original": "def faces(self, sites, permutation=None):\n        \"\"\"\n        Returns the list of faces of this coordination geometry. Each face is given as a\n        list of its vertices coordinates.\n        \"\"\"\n        if permutation is None:\n            coords = [site.coords for site in sites]\n        else:\n            coords = [sites[ii].coords for ii in permutation]\n        return [[coords[ii] for ii in f] for f in self._faces]",
        "sampled": "### Problem Description\nCalculate the list of faces of a coordination geometry given its vertices and edges.\n\n### Input Specifications\n- `sites`: A list of 2D or 3D coordinates representing the vertices of the coordination geometry.\n- `permutation`: An optional parameter representing a permutation of the sites. If provided, it will be used to reorder the sites before calculating the faces.\n\n### Output Specifications\n- A list of faces, where each face is represented as a list of its vertices' coordinates"
    },
    {
        "original": "def uuid4(self, cast_to=str):\n        \"\"\"\n        Generates a random UUID4 string.\n        :param cast_to: Specify what type the UUID should be cast to. Default is `str`\n        :type cast_to: callable\n        \"\"\"\n        # Based on http://stackoverflow.com/q/41186818\n        return cast_to(uuid.UUID(int=self.generator.random.getrandbits(128), version=4))",
        "sampled": "```python\nimport uuid\n\nclass UUIDGenerator:\n    def uuid4(self, cast_to=str):\n        \"\"\"\n        Generates a random UUID4 string.\n        :param cast_to: Specify what type the UUID should be cast to. Default is `str`\n        :type cast_to: callable\n        \"\"\"\n        # Generate a random UUID4\n        random_uuid = uuid.uuid4()\n        \n        # Cast the UUID to the specified type\n        return cast_to(random_uuid)\n\n# Example usage:\ngenerator"
    },
    {
        "original": "def get(self):\n        \"\"\"API endpoint to get the related blocks for a transaction.\n\n        Return:\n            A ``list`` of ``block_id``s that contain the given transaction. The\n            list may be filtered when provided a status query parameter:\n            \"valid\", \"invalid\", \"undecided\".\n        \"\"\"\n        parser = reqparse.RequestParser()\n        parser.add_argument('transaction_id', type=str, required=True)\n\n        args = parser.parse_args(strict=True)\n        tx_id = args['transaction_id']\n\n        pool = current_app.config['bigchain_pool']\n\n        with pool() as bigchain:\n            blocks = bigchain.get_block_containing_tx(tx_id)\n\n        return blocks",
        "sampled": "```python\nclass BlockValidator:\n    def get_related_blocks(self, transaction_id, status=None):\n        \"\"\"\n        API endpoint to get the related blocks for a transaction.\n\n        Args:\n            transaction_id (str): The id of the transaction to search for.\n            status (str, optional): Filter by block status (\"valid\", \"invalid\", \"undecided\"). Defaults to None.\n\n        Returns:\n            list: A list of block_id that contain the given transaction.\n        \"\"\"\n        \n        # For"
    },
    {
        "original": "def _get_object(data, position, obj_end, opts, dummy):\n    \"\"\"Decode a BSON subdocument to opts.document_class or bson.dbref.DBRef.\"\"\"\n    obj_size = _UNPACK_INT(data[position:position + 4])[0]\n    end = position + obj_size - 1\n    if data[end:position + obj_size] != b\"\\x00\":\n        raise InvalidBSON(\"bad eoo\")\n    if end >= obj_end:\n        raise InvalidBSON(\"invalid object length\")\n    if _raw_document_class(opts.document_class):\n        return (opts.document_class(data[position:end + 1], opts),\n                position + obj_size)\n\n    obj = _elements_to_dict(data, position + 4, end, opts)\n\n    position += obj_size\n    if \"$ref\" in obj:\n        return (DBRef(obj.pop(\"$ref\"), obj.pop(\"$id\", None),\n                      obj.pop(\"$db\", None), obj), position)\n    return obj, position",
        "sampled": "### Problem Description\nTo provide a solution, I'll need the problem description. However, in order to simulate this, I will create an example problem.\n\n**Example Problem**\n#### Problem Description (BSON Object)\nDecode a BSON subdocument at a specified position in an ordered or unordered BSON document.\n\n### Input and Output Specifications\n\n| Parameters | Data Type | Description |\n|------------|-----------|-------------|\n| `data`     | list      | Nested list representing the BSON document. Inner lists are"
    },
    {
        "original": "def deserialize_properties(props_struct: struct_pb2.Struct) -> Any:\n    \"\"\"\n    Deserializes a protobuf `struct_pb2.Struct` into a Python dictionary containing normal\n    Python types.\n    \"\"\"\n    # Check out this link for details on what sort of types Protobuf is going to generate:\n    # https://developers.google.com/protocol-buffers/docs/reference/python-generated\n    #\n    # We assume that we are deserializing properties that we got from a Resource RPC endpoint,\n    # which has type `Struct` in our gRPC proto definition.\n    if _special_sig_key in props_struct:\n        if props_struct[_special_sig_key] == _special_asset_sig:\n            # This is an asset. Re-hydrate this object into an Asset.\n            if \"path\" in props_struct:\n                return known_types.new_file_asset(props_struct[\"path\"])\n            if \"text\" in props_struct:\n                return known_types.new_string_asset(props_struct[\"text\"])\n            if \"uri\" in props_struct:\n                return known_types.new_remote_asset(props_struct[\"uri\"])\n            raise AssertionError(\"Invalid asset encountered when unmarshaling resource property\")\n        elif props_struct[_special_sig_key] == _special_archive_sig:\n            # This is an archive. Re-hydrate this object into an Archive.\n            if \"assets\" in props_struct:\n                return known_types.new_asset_archive(deserialize_property(props_struct[\"assets\"]))\n            if \"path\" in props_struct:\n                return known_types.new_file_archive(props_struct[\"path\"])\n            if \"uri\" in props_struct:\n                return known_types.new_remote_archive(props_struct[\"uri\"])\n        elif props_struct[_special_sig_key] == _special_secret_sig:\n            raise AssertionError(\"this version of the Pulumi SDK does not support first-class secrets\")\n\n        raise AssertionError(\"Unrecognized signature when unmarshaling resource property\")\n\n    # Struct is duck-typed like a dictionary, so we can iterate over it in the normal ways.\n    output = {}\n    for k, v in list(props_struct.items()):\n        value = deserialize_property(v)\n        # We treat values that deserialize to \"None\" as if they don't exist.\n        if value is not None:\n            output[k] = value\n\n    return output",
        "sampled": "### Problem Description\n### Input and Output Specifications\n```markdown\n# Problem: Deserialize Properties\n\n## Problem Description\n\nYou are given a `struct_pb2.Struct` object, which is a protobuf message containing key-value pairs.\nEach key-value pair is represented as a `struct_pb2.Value` object, which can be either a string or another `struct_pb2.Struct`.\n\nYour task is to deserialize this protobuf message into a Python dictionary containing normal Python types.\n\n## Input and Output Specifications\n\n*  "
    },
    {
        "original": "def _string_check(self, value, allow_list=False):\n        \"\"\"\n        Based on the string-checking code from the SafeConfigParser's set()\n        function, this enforces string values for config options.\n        \"\"\"\n        if self._optcre is self.OPTCRE or value:\n            is_list = isinstance(value, list)\n            if is_list and not allow_list:\n                raise TypeError('option value cannot be a list unless allow_list is True')\n            elif not is_list:\n                value = [value]\n            if not all(isinstance(x, six.string_types) for x in value):\n                raise TypeError('option values must be strings')",
        "sampled": "```python\nclass ConfigParser:\n    def _string_check(self, value, allow_list=False):\n        \"\"\"\n        Based on the string-checking code from the SafeConfigParser's set()\n        function, this enforces string values for config options.\n        \n        :param value: The value to be checked\n        :param allow_list: Whether a list of strings is allowed (default is False)\n        \n        :return: The checked value if it's a string or a list of strings\n                "
    },
    {
        "original": "def hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, **kwargs):\n        \"\"\"Computes and adjoins histogram along specified dimension(s).\n\n        Defaults to first value dimension if present otherwise falls\n        back to first key dimension.\n\n        Args:\n            dimension: Dimension(s) to compute histogram on\n            num_bins (int, optional): Number of bins\n            bin_range (tuple optional): Lower and upper bounds of bins\n            adjoin (bool, optional): Whether to adjoin histogram\n\n        Returns:\n            AdjointLayout of element and histogram or just the\n            histogram\n        \"\"\"\n        from ..operation import histogram\n        if not isinstance(dimension, list): dimension = [dimension]\n        hists = []\n        for d in dimension[::-1]:\n            hist = histogram(self, num_bins=num_bins, bin_range=bin_range,\n                             dimension=d, **kwargs)\n            hists.append(hist)\n        if adjoin:\n            layout = self\n            for didx in range(len(dimension)):\n                layout = layout << hists[didx]\n        elif len(dimension) > 1:\n            layout = Layout(hists)\n        else:\n            layout = hists[0]\n        return layout",
        "sampled": "```python\nimport numpy as np\n\nclass Data:\n    def __init__(self, values):\n        self.values = values\n\n    def hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, **kwargs):\n        if dimension is None:\n            if hasattr(self.values[0], 'shape'):\n                dimension = 0\n            else:\n                raise ValueError(\"No specified dimension and no default\")\n\n        # Reshape data to 2D array for histogram"
    },
    {
        "original": "def vdi_list(call=None, kwargs=None):\n    \"\"\"\n    Return available Xen VDI images\n\n    If this function is called with the ``-f`` or ``--function`` then\n    it can return a list with minimal deatil using the ``terse=True`` keyword\n    argument.\n\n    .. code-block:: bash\n\n        salt-cloud -f vdi_list myxen terse=True\n\n    \"\"\"\n    if call == 'action':\n        raise SaltCloudException(\n            'This function must be called with -f or --function.')\n    log.debug('kwargs is %s', kwargs)\n    if kwargs is not None:\n        if 'terse' in kwargs:\n            if kwargs['terse'] == 'True':\n                terse = True\n            else:\n                terse = False\n        else:\n            terse = False\n    else:\n        kwargs = {}\n        terse = False\n    session = _get_session()\n    vdis = session.xenapi.VDI.get_all()\n    ret = {}\n    for vdi in vdis:\n        data = session.xenapi.VDI.get_record(vdi)\n        log.debug(type(terse))\n        if terse is True:\n            ret[data.get('name_label')] = {\n                'uuid': data.get('uuid'),\n                'OpqueRef': vdi}\n        else:\n            data.update({'OpaqueRef': vdi})\n            ret[data.get('name_label')] = data\n    return ret",
        "sampled": "### Problem Analysis\nThe given function `vdi_list` is designed to return available Xen VDI (Virtual Desktop Infrastructure) images. This function can be called with the `-f` or `--function` argument and takes a keyword argument `terse=True` to return a list with minimal detail.\n\nTo solve this problem, we can start by defining the necessary parameters and implementing a basic system for returning VDI images information.\n\n### Python Solution\n```python\nimport requests\n\nclass XenAPI:\n"
    },
    {
        "original": "def Run(self):\n    \"\"\"Create FileStore and HashFileStore namespaces.\"\"\"\n    if not data_store.AFF4Enabled():\n      return\n\n    try:\n      filestore = aff4.FACTORY.Create(\n          FileStore.PATH, FileStore, mode=\"rw\", token=aff4.FACTORY.root_token)\n      filestore.Close()\n      hash_filestore = aff4.FACTORY.Create(\n          HashFileStore.PATH,\n          HashFileStore,\n          mode=\"rw\",\n          token=aff4.FACTORY.root_token)\n      hash_filestore.Close()\n      nsrl_filestore = aff4.FACTORY.Create(\n          NSRLFileStore.PATH,\n          NSRLFileStore,\n          mode=\"rw\",\n          token=aff4.FACTORY.root_token)\n      nsrl_filestore.Close()\n    except access_control.UnauthorizedAccess:\n      # The aff4:/files area is ACL protected, this might not work on components\n      # that have ACL enforcement.\n      pass",
        "sampled": "```python\nclass Solution:\n    def Run(self):\n        # Create FileStore and HashFileStore namespaces.\n        file_store_namespace = {\n            \"create_file\": self.create_file,\n            \"read_file\": self.read_file,\n            \"write_to_file\": self.write_to_file,\n            \"delete_file\": self.delete_file\n        }\n\n        hash_store_namespace = {\n            \"create_hashed_file\": self.create_hashed_file,\n            \"read_hashed_file\": self.read_hashed_file\n"
    },
    {
        "original": "def unlock(self):\n        \"\"\"Lock thread.\n\n        Requires that the currently authenticated user has the modposts oauth\n        scope or has user/password authentication as a mod of the subreddit.\n\n        :returns: The json response from the server.\n\n        \"\"\"\n        url = self.reddit_session.config['unlock']\n        data = {'id': self.fullname}\n        return self.reddit_session.request_json(url, data=data)",
        "sampled": "### Problem Description\nUnlock a locked thread on Reddit.\n\n### Input and Output Specifications\nNo input is provided, the function `unlock` is expected to unlock the thread.\n\n### Python Solution\n\n```python\nimport requests\n\nclass RedditAPI:\n    def __init__(self, username, password, client_id, client_secret):\n        self.username = username\n        self.password = password\n        self.client_id = client_id\n        self.client_secret = client_secret\n\n    def authenticate(self):\n        auth"
    },
    {
        "original": "def tdot_blas(mat, out=None):\n    \"\"\"returns np.dot(mat, mat.T), but faster for large 2D arrays of doubles.\"\"\"\n    if (mat.dtype != 'float64') or (len(mat.shape) != 2):\n        return np.dot(mat, mat.T)\n    nn = mat.shape[0]\n    if out is None:\n        out = np.zeros((nn, nn))\n    else:\n        assert(out.dtype == 'float64')\n        assert(out.shape == (nn, nn))\n        # FIXME: should allow non-contiguous out, and copy output into it:\n        assert(8 in out.strides)\n        # zeroing needed because of dumb way I copy across triangular answer\n        out[:] = 0.0\n\n    # # Call to DSYRK from BLAS\n    mat = np.asfortranarray(mat)\n    out = blas.dsyrk(alpha=1.0, a=mat, beta=0.0, c=out, overwrite_c=1,\n                     trans=0, lower=0)\n\n    symmetrify(out, upper=True)\n    return np.ascontiguousarray(out)",
        "sampled": "### Problem Description\n trao chess analysis\n\nTrao Chess is a popular Vietnamese chess variant. In the problem, we're given a contribution matrix **C** = 1123 \u00d7 1123', where each element $c_{ij}$ contributes to the strength of player P_i by attacking player P_j in the position.\n\nGiven this matrix **C**, we want to compute two quantities:\n\n$$ L(P) = \\sum_{i=1}^{1123} c_{ii} $$\n\n"
    },
    {
        "original": "def remove_bond(self, idx1, idx2):\n        \"\"\"\n        Remove a bond from an openbabel molecule\n\n        Args:\n            idx1: The atom index of one of the atoms participating the in bond\n            idx2: The atom index of the other atom participating in the bond \n        \"\"\"\n        for obbond in ob.OBMolBondIter(self._obmol):\n            if (obbond.GetBeginAtomIdx() == idx1 and obbond.GetEndAtomIdx() == idx2) or (obbond.GetBeginAtomIdx() == idx2 and obbond.GetEndAtomIdx() == idx1):\n                self._obmol.DeleteBond(obbond)",
        "sampled": "```python\nfrom openbabel import pybel\n\nclass Molecule:\n    def __init__(self, mol):\n        self.mol = pybel.readstring(\"smi\", mol)\n\n    def remove_bond(self, idx1, idx2):\n        \"\"\"\n        Remove a bond from an openbabel molecule\n\n        Args:\n            idx1: The atom index of one of the atoms participating in the bond\n            idx2: The atom index of the other atom participating in the bond \n       "
    },
    {
        "original": "async def list_blocks(self, request):\n        \"\"\"Fetches list of blocks from validator, optionally filtered by id.\n\n        Request:\n            query:\n                - head: The id of the block to use as the head of the chain\n                - id: Comma separated list of block ids to include in results\n\n        Response:\n            data: JSON array of fully expanded Block objects\n            head: The head used for this query (most recent if unspecified)\n            link: The link to this exact query, including head block\n            paging: Paging info and nav, like total resources and a next link\n        \"\"\"\n        paging_controls = self._get_paging_controls(request)\n        validator_query = client_block_pb2.ClientBlockListRequest(\n            head_id=self._get_head_id(request),\n            block_ids=self._get_filter_ids(request),\n            sorting=self._get_sorting_message(request, \"block_num\"),\n            paging=self._make_paging_message(paging_controls))\n\n        response = await self._query_validator(\n            Message.CLIENT_BLOCK_LIST_REQUEST,\n            client_block_pb2.ClientBlockListResponse,\n            validator_query)\n\n        return self._wrap_paginated_response(\n            request=request,\n            response=response,\n            controls=paging_controls,\n            data=[self._expand_block(b) for b in response['blocks']])",
        "sampled": "### Problem Analysis\nThe problem is to create a function `list_blocks` that fetches a list of blocks from a validator. The function takes an optional `head` parameter, which is the id of the block to use as the head of the chain. It also takes an optional `id` parameter, which is a comma-separated list of block ids to include in the results.\n\nThe function returns a JSON array of fully expanded Block objects, along with some additional metadata such as:\n\n*   `"
    },
    {
        "original": "def define(cls, name, **kwargs):\n        \"\"\"\n        Utility to quickly and easily declare Stream classes. Designed\n        for interactive use such as notebooks and shouldn't replace\n        parameterized class definitions in source code that is imported.\n\n        Takes a stream class name and a set of keywords where each\n        keyword becomes a parameter. If the value is already a\n        parameter, it is simply used otherwise the appropriate parameter\n        type is inferred and declared, using the value as the default.\n\n        Supported types: bool, int, float, str, dict, tuple and list\n        \"\"\"\n        params = {'name': param.String(default=name)}\n        for k, v in kwargs.items():\n            kws = dict(default=v, constant=True)\n            if isinstance(v, param.Parameter):\n                params[k] = v\n            elif isinstance(v, bool):\n                params[k] = param.Boolean(**kws)\n            elif isinstance(v, int):\n                params[k] = param.Integer(**kws)\n            elif isinstance(v, float):\n                params[k] = param.Number(**kws)\n            elif isinstance(v, str):\n                params[k] = param.String(**kws)\n            elif isinstance(v, dict):\n                params[k] = param.Dict(**kws)\n            elif isinstance(v, tuple):\n                params[k] = param.Tuple(**kws)\n            elif isinstance(v, list):\n                params[k] = param.List(**kws)\n            elif isinstance(v, np.ndarray):\n                params[k] = param.Array(**kws)\n            else:\n                params[k] = param.Parameter(**kws)\n\n        # Dynamic class creation using type\n        return type(name, (Stream,), params)",
        "sampled": "```python\ndef define(cls, name, **kwargs):\n    \"\"\"\n    Utility to quickly and easily declare Stream classes.\n    \n    Args:\n        cls (class): The class to be modified.\n        name (str): The name of the stream class.\n        **kwargs: A set of keywords where each keyword becomes a parameter.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Create a new class with the given name\n    setattr(cls, name, type(name, (cls,), {}))\n\n   "
    },
    {
        "original": "def _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \"\"\"\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists'\n            ret['default'] = default == ruby\n            break\n\n    return ret",
        "sampled": "```python\nimport subprocess\nimport platform\n\ndef _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \n    Args:\n        ret (dict): Dictionary to store the installation status of Ruby gems and their versions.\n        ruby (str): The name of the Ruby gem to check.\n        user (str, optional): The name of the user who uses the system. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n    \n    #"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'configurations') and self.configurations is not None:\n            _dict['configurations'] = [\n                x._to_dict() for x in self.configurations\n            ]\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n\n    def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        return {\n            'n': self.n,\n            'k': self.k\n        }\n\n    def solve(self, n, k):\n        # Your solution code here\n        pass\n\n# Example usage:\nsolution = Solution()\nsolution.solve(10, 5)\nprint(solution"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a TopHitsResults object from a json dictionary.\"\"\"\n        args = {}\n        if 'matching_results' in _dict:\n            args['matching_results'] = _dict.get('matching_results')\n        if 'hits' in _dict:\n            args['hits'] = [\n                QueryResult._from_dict(x) for x in (_dict.get('hits'))\n            ]\n        return cls(**args)",
        "sampled": "```python\nclass TopHitsResults:\n    def __init__(self, hits, total):\n        \"\"\"\n        Initialize a TopHitsResults object.\n\n        Args:\n            hits (list): A list of dictionaries where each dictionary represents a hit.\n            total (int): The total number of hits.\n        \"\"\"\n        self.hits = hits\n        self.total = total\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"\n        Initialize a TopHitsResults object from a json dictionary"
    },
    {
        "original": "def get_task_df(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\n\t\t\"\"\"\n\t\tterm_time_df = self._get_term_time_df()\n\t\tterms_to_include = (\n\t\t\tterm_time_df\n\t\t\t\t.groupby('term')['top']\n\t\t\t\t.sum()\n\t\t\t\t.sort_values(ascending=False)\n\t\t\t\t.iloc[:self.num_terms_to_include].index\n\t\t)\n\t\ttask_df = (\n\t\t\tterm_time_df[term_time_df.term.isin(terms_to_include)][['time', 'term']]\n\t\t\t\t.groupby('term')\n\t\t\t\t.apply(lambda x: pd.Series(self._find_sequences(x['time'])))\n\t\t\t\t.reset_index()\n\t\t\t\t.rename({0: 'sequence'}, axis=1)\n\t\t\t\t.reset_index()\n\t\t\t\t.assign(start=lambda x: x['sequence'].apply(lambda x: x[0]))\n\t\t\t\t.assign(end=lambda x: x['sequence'].apply(lambda x: x[1]))\n\t\t\t[['term', 'start', 'end']]\n\t\t)\n\t\treturn task_df",
        "sampled": "```python\nimport pandas as pd\n\nclass TaskAnalyzer:\n    def __init__(self, data):\n        self.data = data\n\n    def get_task_df(self):\n        # Assuming 'data' is a list of dictionaries containing task information\n        tasks = [{'id': i, 'name': f'Task {i}', 'status': 'pending', 'priority': 1} for i in range(1, len(self.data) + 1)]\n        \n        # Create a DataFrame from"
    },
    {
        "original": "def model_to_pdag(model):\n        \"\"\"Construct the DAG pattern (representing the I-equivalence class) for\n        a given DAG. This is the \"inverse\" to pdag_to_dag.\n        \"\"\"\n\n        if not isinstance(model, DAG):\n            raise TypeError(\"model: Expected DAG instance, \" +\n                            \"got type {model_type}\".format(model_type=type(model)))\n\n        skel, separating_sets = ConstraintBasedEstimator.build_skeleton(\n                                    model.nodes(),\n                                    model.get_independencies())\n        pdag = ConstraintBasedEstimator.skeleton_to_pdag(skel, separating_sets)\n\n        return pdag",
        "sampled": "```python\nimport networkx as nx\n\ndef model_to_pdag(model):\n    \"\"\"\n    Construct the DAG pattern (representing the I-equivalence class) for a given DAG.\n    \n    Parameters:\n    model (dict): A dictionary representing the initial state and possible updates of a system.\n        \n        Each key in the dictionary is an element in the system, and its corresponding value is another dictionary consisting of:\n            - \"type\": \"initial\" or \"atomic_visible\" or \"macroscopic"
    },
    {
        "original": "def generate_dataset(path, variant='20m', outputpath=\".\"):\n    \"\"\" Generates a hdf5 movielens datasetfile from the raw datafiles found at:\n    https://grouplens.org/datasets/movielens/20m/\n\n    You shouldn't have to run this yourself, and can instead just download the\n    output using the 'get_movielens' funciton./\n    \"\"\"\n    filename = os.path.join(outputpath, \"movielens_%s.hdf5\" % variant)\n\n    if variant == '20m':\n        ratings, movies = _read_dataframes_20M(path)\n    elif variant == '100k':\n        ratings, movies = _read_dataframes_100k(path)\n    else:\n        ratings, movies = _read_dataframes(path)\n\n    _hfd5_from_dataframe(ratings, movies, filename)",
        "sampled": "### Problem Description\nThe task is to download a HDF5 Movielens dataset file from the raw datafiles found at [https://grouplens.org/datasets/movielens/20m/](https://grouplens.org/datasets/movielens/20m/). \n\nHowever, we should note that the provided function `generate_dataset` appears to be designed to generate a HDF5 Movielens dataset file itself rather than downloading it. I will therefore assume this"
    },
    {
        "original": "def load_yaml_file(yaml_file):\n    \"\"\"Read YAML file.\"\"\"\n    try:\n        import yaml\n    except ImportError:\n        sys.exit(\"Unable to import yaml module.\")\n    try:\n        with io.open(yaml_file, \"rt\", encoding=\"utf-8\") as fname:\n            return yaml.safe_load(fname)\n    except IOError:\n        sys.exit(\"Unable to open YAML file: {0}\".format(yaml_file))",
        "sampled": "```python\nimport yaml\n\ndef load_yaml_file(yaml_file):\n    \"\"\"Read YAML file.\"\"\"\n    with open(yaml_file, 'r') as stream:\n        try:\n            return yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            print(f\"YAML syntax error: {exc}\")\n            return None\n```"
    },
    {
        "original": "def _ExtractOAuth2Client(product_yaml_key, product_data, proxy_config):\n  \"\"\"Generates an GoogleOAuth2Client subclass using the given product_data.\n\n  Args:\n    product_yaml_key: a string key identifying the product being configured.\n    product_data: a dict containing the configurations for a given product.\n    proxy_config: a ProxyConfig instance.\n\n  Returns:\n    An instantiated GoogleOAuth2Client subclass.\n\n  Raises:\n    A GoogleAdsValueError if the OAuth2 configuration for the given product is\n    misconfigured.\n  \"\"\"\n  oauth2_kwargs = {\n      'proxy_config': proxy_config\n  }\n\n  if all(config in product_data for config in _OAUTH2_INSTALLED_APP_KEYS):\n    oauth2_args = [\n        product_data['client_id'], product_data['client_secret'],\n        product_data['refresh_token']\n    ]\n    oauth2_client = googleads.oauth2.GoogleRefreshTokenClient\n    for key in _OAUTH2_INSTALLED_APP_KEYS:\n      del product_data[key]\n  elif all(config in product_data for config in _OAUTH2_SERVICE_ACCT_KEYS):\n    oauth2_args = [\n        product_data['path_to_private_key_file'],\n        googleads.oauth2.GetAPIScope(product_yaml_key),\n    ]\n    oauth2_kwargs.update({\n        'sub': product_data.get('delegated_account')\n    })\n    oauth2_client = googleads.oauth2.GoogleServiceAccountClient\n    for key in _OAUTH2_SERVICE_ACCT_KEYS:\n      del product_data[key]\n    for optional_key in _OAUTH2_SERVICE_ACCT_KEYS_OPTIONAL:\n      if optional_key in product_data:\n        del product_data[optional_key]\n  else:\n    raise googleads.errors.GoogleAdsValueError(\n        'Your yaml file is incorrectly configured for OAuth2. You need to '\n        'specify credentials for either the installed application flow (%s) '\n        'or service account flow (%s).' %\n        (_OAUTH2_INSTALLED_APP_KEYS, _OAUTH2_SERVICE_ACCT_KEYS))\n\n  return oauth2_client(*oauth2_args, **oauth2_kwargs)",
        "sampled": "### Problem Analysis\n\nThe given function `_ExtractOAuth2Client` is designed to generate an instance of a `GoogleOAuth2Client` subclass based on the provided `product_data`. The function takes three parameters:\n\n- `product_yaml_key`: A string key identifying the product being configured.\n- `product_data`: A dictionary containing the configurations for a given product.\n- `proxy_config`: An instance of the `ProxyConfig` class.\n\nThe function returns an instantiated subclass of `GoogleOAuth2Client`."
    },
    {
        "original": "def update(self, product_id, product_data):\n        \"\"\"\u4fee\u6539\u5546\u54c1\"\"\"\n        product_data['product_id'] = product_id\n        return self._post(\n            'merchant/update',\n            data=product_data\n        )",
        "sampled": "```python\nclass ProductManager:\n    def __init__(self):\n        # Initialize an empty dictionary to store products\n        self.products = {}\n\n    def add_product(self, product_id, name, price):\n        # Add a new product with its id, name and price\n        self.products[product_id] = {'name': name, 'price': price}\n\n    def update(self, product_id, old_price):\n        # Check if the product exists in the dictionary\n        if product_id"
    },
    {
        "original": "def AddBlob(self, blob_id, length):\n    \"\"\"Add another blob to this image using its hash.\n\n    Once a blob is added that is smaller than the chunksize we finalize the\n    file, since handling adding more blobs makes the code much more complex.\n\n    Args:\n      blob_id: rdf_objects.BlobID object.\n      length: int length of blob\n\n    Raises:\n      IOError: if blob has been finalized.\n    \"\"\"\n    if self.finalized and length > 0:\n      raise IOError(\"Can't add blobs to finalized BlobImage\")\n\n    self.content_dirty = True\n    self.index.seek(0, 2)\n    self.index.write(blob_id.AsBytes())\n    self.size += length\n\n    if length < self.chunksize:\n      self.finalized = True",
        "sampled": "### Problem Analysis\n\nThe problem involves adding a blob to an image using its hash. A blob is a small file or chunk of data, and it seems that the image is being stored in chunks (blobs) rather than as a single large file. The `AddBlob` method takes two parameters: `blob_id` (a unique identifier for the blob) and `length` (the size of the blob).\n\nOnce a blob is added that is smaller than the chunksize, the file is"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a LogQueryResponseResultDocuments object from a json dictionary.\"\"\"\n        args = {}\n        if 'results' in _dict:\n            args['results'] = [\n                LogQueryResponseResultDocumentsResult._from_dict(x)\n                for x in (_dict.get('results'))\n            ]\n        if 'count' in _dict:\n            args['count'] = _dict.get('count')\n        return cls(**args)",
        "sampled": "However, there's no problem description provided. Please paste the problem description in Markdown format, and I'll do my best to help you solve it step by step and provide a Python solution at the end.\n\nPlease paste the problem description with Problem Description, Input Output specifications including the inline latex formula."
    },
    {
        "original": "def _yarn_capacity_scheduler_metrics(self, metrics_json, addl_tags, queue_blacklist):\n        \"\"\"\n        Get metrics from YARN scheduler if it's type is capacityScheduler\n        \"\"\"\n        tags = ['queue_name:{}'.format(metrics_json['queueName'])]\n        tags.extend(addl_tags)\n\n        self._set_yarn_metrics_from_json(tags, metrics_json, YARN_ROOT_QUEUE_METRICS)\n\n        if metrics_json['queues'] is not None and metrics_json['queues']['queue'] is not None:\n\n            queues_count = 0\n            for queue_json in metrics_json['queues']['queue']:\n                queue_name = queue_json['queueName']\n\n                if queue_name in queue_blacklist:\n                    self.log.debug('Queue \"{}\" is blacklisted. Ignoring it'.format(queue_name))\n                    continue\n\n                queues_count += 1\n                if queues_count > MAX_DETAILED_QUEUES:\n                    self.warning(\n                        \"Found more than 100 queues, will only send metrics on first 100 queues. \"\n                        \"Please filter the queues with the check's `queue_blacklist` parameter\"\n                    )\n                    break\n\n                tags = ['queue_name:{}'.format(str(queue_name))]\n                tags.extend(addl_tags)\n\n                self._set_yarn_metrics_from_json(tags, queue_json, YARN_QUEUE_METRICS)",
        "sampled": "$$\\text{Problem Description}$$\n==========================\n\n### Problem Statement\n\nThe problem requires implementing a method `_yarn_capacity_scheduler_metrics` that retrieves metrics from the YARN scheduler if its type is `capacityScheduler`. The method takes three parameters:\n\n*   `metrics_json`: A JSON object containing metrics data.\n*   `addl_tags`: Additional tags to be added to the metrics.\n*   `queue_blacklist`: A list of queues to be excluded from the metrics.\n\n### Input"
    },
    {
        "original": "def setup(app):\n    \"\"\"Map methods to states of the documentation build.\"\"\"\n    app.connect(\"builder-inited\", build_configuration_parameters)\n    app.connect(\"autodoc-skip-member\", skip_slots)\n    app.add_stylesheet(\"css/custom.css\")",
        "sampled": "```python\ndef setup(app):\n    # Define map of methods to states of the documentation build\n    app.add_config_value('reahl_version', '1.3.1', 'env')\n\n    # Check if configuration is complex enough to warrant two passes over the docs.\n    @app.first_pass\n    def check_configuration(app, exception_list):\n        app.require_config('reahl_version')\n\n    # Add a method that will be called after all documents have been parsed.\n    @app.after"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'level') and self.level is not None:\n            _dict['level'] = self.level\n        if hasattr(self, 'names') and self.names is not None:\n            _dict['names'] = self.names\n        return _dict",
        "sampled": "## Problem Description\n### Long Pressed Name\n\nToday is the day of the party!!! There is a man with a long messaged nickname, he want you to write a program which can determine whether the name that other guys type on the keyboard is same as his nickname or not.\n\nThe guy's nickname will be consist of digits and letters, and when he type some keys on keyboard, if he hold down the key for a period of time (long press), then this key will be appear for"
    },
    {
        "original": "def addTrack(self, track):\n        \"\"\"\n        Add a track to be recorded.\n\n        :param: track: An :class:`aiortc.AudioStreamTrack` or :class:`aiortc.VideoStreamTrack`.\n        \"\"\"\n        if track.kind == 'audio':\n            if self.__container.format.name == 'wav':\n                codec_name = 'pcm_s16le'\n            elif self.__container.format.name == 'mp3':\n                codec_name = 'mp3'\n            else:\n                codec_name = 'aac'\n            stream = self.__container.add_stream(codec_name)\n        else:\n            if self.__container.format.name == 'image2':\n                stream = self.__container.add_stream('png', rate=30)\n                stream.pix_fmt = 'rgb24'\n            else:\n                stream = self.__container.add_stream('libx264', rate=30)\n                stream.pix_fmt = 'yuv420p'\n        self.__tracks[track] = MediaRecorderContext(stream)",
        "sampled": "```python\nclass TrackRecorder:\n    def __init__(self):\n        self.tracks = []\n\n    def addTrack(self, track):\n        \"\"\"\n        Add a track to be recorded.\n\n        :param: track: An aiortc.AudioStreamTrack or aiortc.VideoStreamTrack.\n        \"\"\"\n        self.tracks.append(track)\n\n# Example usage:\nrecorder = TrackRecorder()\ntrack1 = \"Audio Stream Track\"\ntrack2 = \"Video Stream Track\"\n\nrecorder.addTrack(track1)\nrec"
    },
    {
        "original": "def reduce_dict(input_dict, average=True):\n    \"\"\"\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    \"\"\"\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict",
        "sampled": "### Problem Description\nReduce the values in a dictionary from all processes so that process with rank 0 has the averaged results.\n\n### Input and Output Specifications\n- **Input**: \n  - `input_dict` (dict): all the values will be reduced\n  - `average` (bool): whether to do average or sum\n- **Output**: \n  - A dict with the same fields as `input_dict`, after reduction\n\n### Python Solution\n\n```python\nimport mpi4py.M"
    },
    {
        "original": "def ReadHuntLogEntries(self,\n                         hunt_id,\n                         offset,\n                         count,\n                         with_substring=None,\n                         cursor=None):\n    \"\"\"Reads hunt log entries of a given hunt using given query options.\"\"\"\n    hunt_id_int = db_utils.HuntIDToInt(hunt_id)\n\n    query = (\"SELECT client_id, flow_id, message, UNIX_TIMESTAMP(timestamp) \"\n             \"FROM flow_log_entries \"\n             \"FORCE INDEX(flow_log_entries_by_hunt) \"\n             \"WHERE hunt_id = %s AND flow_id = hunt_id \")\n\n    args = [hunt_id_int]\n\n    if with_substring is not None:\n      query += \"AND message LIKE %s \"\n      args.append(\"%\" + db_utils.EscapeWildcards(with_substring) + \"%\")\n\n    query += \"ORDER BY timestamp ASC LIMIT %s OFFSET %s\"\n\n    args.append(count)\n    args.append(offset)\n\n    cursor.execute(query, args)\n\n    flow_log_entries = []\n    for client_id_int, flow_id_int, message, timestamp in cursor.fetchall():\n      flow_log_entries.append(\n          rdf_flow_objects.FlowLogEntry(\n              client_id=db_utils.IntToClientID(client_id_int),\n              flow_id=db_utils.IntToFlowID(flow_id_int),\n              hunt_id=hunt_id,\n              message=message,\n              timestamp=mysql_utils.TimestampToRDFDatetime(timestamp)))\n\n    return flow_log_entries",
        "sampled": "```python\nclass HuntLogReader:\n    def ReadHuntLogEntries(self, hunt_id, offset, count, with_substring=None, cursor=None):\n        # Check if the hunt id is valid\n        if hunt_id <= 0:\n            return []\n\n        # Initialize an empty list to store the log entries\n        log_entries = []\n\n        # If a cursor is provided, use it to simulate pagination\n        if cursor is not None:\n            current_offset = int(cursor[-1])\n"
    },
    {
        "original": "def dec_file(name, out=None, **kwargs):\n    \"\"\"\n    This is a helper function to decrypt a file and return its contents.\n\n    You can provide an optional output file using `out`\n\n    `name` can be a local file or when not using `salt-run` can be a url like `salt://`, `https://` etc.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run nacl.dec_file name=/tmp/id_rsa.nacl\n        salt-call nacl.dec_file name=salt://crt/mycert.nacl out=/tmp/id_rsa\n        salt-run nacl.dec_file name=/tmp/id_rsa.nacl box_type=secretbox \\\n            sk_file=/etc/salt/pki/master/nacl.pub\n    \"\"\"\n    kwargs['opts'] = __opts__\n    return salt.utils.nacl.dec_file(name, out, **kwargs)",
        "sampled": "### Problem Description\n\nNo problem description provided, but based on the function name `dec_file` and the examples in the docstring, it seems that we need to write a function that takes a file as input and applies NaCl decryption to it.\n\n## Input Specifications\nNo specific input specifications provided.\n\n## Output Specifications\nThe output of the function should be the decrypted contents of the file.\n\n### Python Solution\n\n```python\nimport os\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography"
    },
    {
        "original": "def getGerritChanges(props):\n        \"\"\" Get the gerrit changes\n\n            This method could be overridden if really needed to accommodate for other\n            custom steps method for fetching gerrit changes.\n\n            :param props: an IProperty\n\n            :return: (optionally via deferred) a list of dictionary with at list\n                change_id, and revision_id,\n                which format is the one accepted by the gerrit REST API as of\n                /changes/:change_id/revision/:revision_id paths (see gerrit doc)\n        \"\"\"\n        if 'gerrit_changes' in props:\n            return props.getProperty('gerrit_changes')\n\n        if 'event.change.number' in props:\n            return [{\n                'change_id': props.getProperty('event.change.number'),\n                'revision_id': props.getProperty('event.patchSet.number')\n            }]\n        return []",
        "sampled": "### Problem Description\nGet the Gerrit changes\n\n### Input and Output Specifications\n- **Input**: `props` (an IProperty)\n- **Output**: A list of dictionaries containing at least `change_id` and `revision_id`\n\n### Python Solution\n\n```python\nfrom typing import List, Dict\n\nclass IProperty:\n    def __init__(self):\n        pass\n\ndef getGerritChanges(props: IProperty) -> List[Dict]:\n    \"\"\"\n    Get the gerrit"
    },
    {
        "original": "def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n        try:\n            data = requests.get(\"https://pypi.python.org/pypi/jira/json\", timeout=2.001).json()\n\n            released_version = data['info']['version']\n            if parse_version(released_version) > parse_version(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n        except requests.RequestException:\n            pass\n        except Exception as e:\n            logging.warning(e)",
        "sampled": "```python\nimport requests\n\nclass Library:\n    def __init__(self, current_version):\n        self.current_version = current_version\n        self.latest_version_url = 'https://example.com/latest-version'  # replace with actual URL\n\n    def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n        \n        try:\n            response = requests.get(self.latest_version_url)\n            response.raise_for_status()\n            latest_version = response.text.strip()\n            \n            if"
    },
    {
        "original": "def standardised_euclidean(x, y, sigma=_mock_ones):\n    \"\"\"Euclidean distance standardised against a vector of standard\n    deviations per coordinate.\n\n    ..math::\n        D(x, y) = \\sqrt{\\sum_i \\frac{(x_i - y_i)**2}{v_i}}\n    \"\"\"\n    result = 0.0\n    for i in range(x.shape[0]):\n        result += ((x[i] - y[i]) ** 2) / sigma[i]\n\n    return np.sqrt(result)",
        "sampled": "```python\nimport numpy as np\n\ndef standardised_euclidean(x, y, sigma=np.ones_like(x)):\n    \"\"\"\n    Euclidean distance standardised against a vector of standard\n    deviations per coordinate.\n\n    ..math::\n        D(x, y) = \\sqrt{\\sum_i \\frac{(x_i - y_i)**2}{v_i}}\n\n    Parameters:\n    x (numpy array): First input vector.\n    y (numpy array): Second input vector.\n    sigma (numpy"
    },
    {
        "original": "def set_embeddings(self, embeddings):\n        \"\"\"\n        Specifies fixed set of embeddings\n        :param embeddings: array-like, sparse or dense, shape should be (embedding size, # terms)\n        :return: EmbeddingsResolver\n        \"\"\"\n        if self.embeddings_ is not None:\n            raise Exception(\"You have already set embeddings by running set_embeddings or set_embeddings_model.\")\n        assert embeddings.shape[1] == self.corpus_.get_num_terms()\n        self.embeddings_ = embeddings.T\n        self.vocab_ = self.corpus_.get_terms()\n        return self",
        "sampled": "```python\nclass EmbeddingsResolver:\n    def __init__(self):\n        self.embeddings = None\n\n    def set_embeddings(self, embeddings):\n        \"\"\"\n        Specifies fixed set of embeddings\n        :param embeddings: array-like, sparse or dense, shape should be (embedding size, # terms)\n        :return: EmbeddingsResolver (infers type from the embedding shape)\n        \"\"\"\n        \n        if isinstance(embeddings[0], list):  # check if it's a 2D"
    },
    {
        "original": "def fold_point(p, lattice, coords_are_cartesian=False):\n    \"\"\"\n    Folds a point with coordinates p inside the first Brillouin zone of the lattice.\n\n    Args:\n        p: coordinates of one point\n        lattice: Lattice object used to convert from reciprocal to cartesian coordinates\n        coords_are_cartesian: Set to True if you are providing\n            coordinates in cartesian coordinates. Defaults to False.\n\n    Returns:\n        The cartesian coordinates folded inside the first Brillouin zone\n    \"\"\"\n\n    if coords_are_cartesian:\n        p = lattice.get_fractional_coords(p)\n    else:\n        p = np.array(p)\n\n    p = np.mod(p + 0.5 - 1e-10, 1) - 0.5 + 1e-10\n    p = lattice.get_cartesian_coords(p)\n\n    closest_lattice_point = None\n    smallest_distance = 10000\n    for i in (-1, 0, 1):\n        for j in (-1, 0, 1):\n            for k in (-1, 0, 1):\n                lattice_point = np.dot((i, j, k), lattice.matrix)\n                dist = np.linalg.norm(p - lattice_point)\n                if closest_lattice_point is None or dist < smallest_distance:\n                    closest_lattice_point = lattice_point\n                    smallest_distance = dist\n\n    if not np.allclose(closest_lattice_point, (0, 0, 0)):\n        p = p - closest_lattice_point\n\n    return p",
        "sampled": "```python\nimport numpy as np\n\nclass Lattice:\n    def __init__(self, a1, a2):\n        self.a1 = np.array(a1)\n        self.a2 = np.array(a2)\n\n    def reciprocal(self):\n        # Calculate the reciprocal lattice vectors\n        b1 = 2 * np.pi * self.a2 / np.linalg.norm(np.cross(self.a1, self.a2))\n        b2 = 2 * np.pi * self.a1 / np"
    },
    {
        "original": "def random_coords(bounds):\n        \"\"\" Selects a random point in interior of a rectangle\n\n        :param bounds: Rectangle coordinates (x_min, y_min, x_max, y_max)\n        :type bounds: tuple(float)\n        :return: Random point from interior of rectangle\n        :rtype: tuple of x and y coordinates\n        \"\"\"\n        x_min, y_min, x_max, y_max = bounds\n        x = np.random.randint(x_min, x_max)\n        y = np.random.randint(y_min, y_max)\n        return x, y",
        "sampled": "### Problem Description\nGiven a rectangle with coordinates $(x_{min}, y_{min}, x_{max}, y_{max})$, select a random point in the interior of the rectangle.\n\n### Input and Output Specifications\n- **Input**:\n  - `bounds`: Rectangle coordinates (x_min, y_min, x_max, y_max)\n- **Output**:\n  - Random point from interior of rectangle (random_x, random_y)\n\n### Python Solution\n\n```python\nimport random\n\ndef"
    },
    {
        "original": "def _handle_job_without_successors(self, job, irsb, insn_addrs):\n        \"\"\"\n        A block without successors should still be handled so it can be added to the function graph correctly.\n\n        :param CFGJob job:  The current job that do not have any successor.\n        :param IRSB irsb:   The related IRSB.\n        :param insn_addrs:  A list of instruction addresses of this IRSB.\n        :return: None\n        \"\"\"\n\n        # it's not an empty block\n\n        # handle all conditional exits\n        ins_addr = job.addr\n        for stmt_idx, stmt in enumerate(irsb.statements):\n            if type(stmt) is pyvex.IRStmt.IMark:\n                ins_addr = stmt.addr + stmt.delta\n            elif type(stmt) is pyvex.IRStmt.Exit:\n                successor_jumpkind = stmt.jk\n                self._update_function_transition_graph(\n                    job.block_id, None,\n                    jumpkind = successor_jumpkind,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )\n\n        # handle the default exit\n        successor_jumpkind = irsb.jumpkind\n        successor_last_ins_addr = insn_addrs[-1]\n        self._update_function_transition_graph(job.block_id, None,\n                                               jumpkind=successor_jumpkind,\n                                               ins_addr=successor_last_ins_addr,\n                                               stmt_idx=DEFAULT_STATEMENT,\n                                               )",
        "sampled": "```python\nclass CFGJob:\n    def __init__(self, id):\n        self.id = id\n        self.successors = []\n\nclass IRSB:\n    def __init__(self, id):\n        self.id = id\n\ndef _handle_job_without_successors(job: CFGJob, irsb: IRSB, insn_addrs):\n    \"\"\"\n    A block without successors should still be handled so it can be added to the function graph correctly.\n\n    :param CFGJob job:  The"
    },
    {
        "original": "def LeaseClientActionRequests(self,\n                                client_id,\n                                lease_time=None,\n                                limit=sys.maxsize):\n    \"\"\"Leases available client action requests for a client.\"\"\"\n\n    leased_requests = []\n\n    now = rdfvalue.RDFDatetime.Now()\n    expiration_time = now + lease_time\n    process_id_str = utils.ProcessIdString()\n\n    leases = self.client_action_request_leases\n    # Can't use an iterator here since the dict might change when requests get\n    # deleted.\n    for key, request in sorted(self.client_action_requests.items()):\n      if key[0] != client_id:\n        continue\n\n      existing_lease = leases.get(key)\n      if not existing_lease or existing_lease[0] < now:\n        if existing_lease:\n          lease_count = existing_lease[-1] + 1\n          if lease_count > db.Database.CLIENT_MESSAGES_TTL:\n            self._DeleteClientActionRequest(*key)\n            continue\n        else:\n          lease_count = 1\n\n        leases[key] = (expiration_time, process_id_str, lease_count)\n        request.leased_until = expiration_time\n        request.leased_by = process_id_str\n        request.ttl = db.Database.CLIENT_MESSAGES_TTL - lease_count\n        leased_requests.append(request)\n        if len(leased_requests) >= limit:\n          break\n\n    return leased_requests",
        "sampled": "### Problem Description\nLease Client Action Requests\n==========================\n\nYou are the administrator of a system that handles client action requests, where each request is identified by a unique ID.\nEach client can have multiple action requests. However, only a limited number of these requests can be processed at any time.\n\nProblem Statement:\nGiven a `client_id`, lease available client action requests for that client. The leased requests should be valid for `lease_time` seconds.\n\nInput Specifications:\n- `client_id`: Unique"
    },
    {
        "original": "def download(self, to_path=None, name=None, chunk_size='auto',\n                 convert_to_pdf=False):\n        \"\"\" Downloads this file to the local drive. Can download the\n        file in chunks with multiple requests to the server.\n\n        :param to_path: a path to store the downloaded file\n        :type to_path: str or Path\n        :param str name: the name you want the stored file to have.\n        :param int chunk_size: number of bytes to retrieve from\n         each api call to the server. if auto, files bigger than\n         SIZE_THERSHOLD will be chunked (into memory, will be\n         however only 1 request)\n        :param bool convert_to_pdf: will try to download the converted pdf\n         if file extension in ALLOWED_PDF_EXTENSIONS\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        # TODO: Add download with more than one request (chunk_requests) with\n        # header 'Range'. For example: 'Range': 'bytes=0-1024'\n\n        if to_path is None:\n            to_path = Path()\n        else:\n            if not isinstance(to_path, Path):\n                to_path = Path(to_path)\n\n        if not to_path.exists():\n            raise FileNotFoundError('{} does not exist'.format(to_path))\n\n        if name and not Path(name).suffix and self.name:\n            name = name + Path(self.name).suffix\n\n        name = name or self.name\n        to_path = to_path / name\n\n        url = self.build_url(\n            self._endpoints.get('download').format(id=self.object_id))\n\n        try:\n            if chunk_size is None:\n                stream = False\n            elif chunk_size == 'auto':\n                if self.size and self.size > SIZE_THERSHOLD:\n                    stream = True\n                else:\n                    stream = False\n            elif isinstance(chunk_size, int):\n                stream = True\n            else:\n                raise ValueError(\"Argument chunk_size must be either 'auto' \"\n                                 \"or any integer number representing bytes\")\n\n            params = {}\n            if convert_to_pdf and Path(name).suffix in ALLOWED_PDF_EXTENSIONS:\n                params['format'] = 'pdf'\n\n            with self.con.get(url, stream=stream, params=params) as response:\n                if not response:\n                    log.debug('Downloading driveitem Request failed: {}'.format(\n                        response.reason))\n                    return False\n                with to_path.open(mode='wb') as output:\n                    if stream:\n                        for chunk in response.iter_content(\n                                chunk_size=chunk_size):\n                            if chunk:\n                                output.write(chunk)\n                    else:\n                        output.write(response.content)\n        except Exception as e:\n            log.error(\n                'Error downloading driveitem {}. Error: {}'.format(self.name,\n                                                                   str(e)))\n            return False\n\n        return True",
        "sampled": "```python\nimport os\nimport requests\n\nclass FileDownloader:\n    def __init__(self):\n        self.SIZE_THRESHOLD = 1024 * 1024 * 50  # 50 MB\n        self.ALLOWED_PDF_EXTENSIONS = ['.pdf']\n\n    def download(self, to_path=None, name=None, chunk_size='auto', convert_to_pdf=False):\n        if not to_path:\n            to_path = os.getcwd()\n\n        if not name:\n            name = 'downloaded_file"
    },
    {
        "original": "def send_connect_request(self, connection_id):\n        \"\"\"\n        Send ConnectionRequest to an inbound connection. This allows\n        the validator to be authorized by the incoming connection.\n        \"\"\"\n        connect_message = ConnectionRequest(endpoint=self._public_endpoint)\n        self._safe_send(\n            validator_pb2.Message.NETWORK_CONNECT,\n            connect_message.SerializeToString(),\n            connection_id,\n            callback=partial(\n                self._inbound_connection_request_callback,\n                connection_id=connection_id))",
        "sampled": "```python\nclass ConnectionManager:\n    def __init__(self):\n        # Initialize an empty dictionary to store connection requests\n        self.connection_requests = {}\n\n    def send_connect_request(self, connection_id):\n        \"\"\"\n        Send ConnectionRequest to an inbound connection. This allows\n        the validator to be authorized by the incoming connection.\n        \n        Args:\n            connection_id (str): The ID of the inbound connection.\n        \n        Returns:\n            None\n        \"\"\"\n        \n        # Check if the connection ID"
    },
    {
        "original": "def _populate_cache(platform, pkg_cache, mount_dir):\n    \"\"\"\n    If a ``pkg_cache`` directory is specified, then use it to populate the\n    disk image.\n    \"\"\"\n    if not pkg_cache:\n        return\n    if not os.path.isdir(pkg_cache):\n        return\n\n    if platform == 'pacman':\n        cache_dir = '{0}/var/cache/pacman/pkg'.format(mount_dir)\n\n    __salt__['file.mkdir'](cache_dir, 'root', 'root', '755')\n    __salt__['file.copy'](pkg_cache, cache_dir, recurse=True, remove_existing=True)",
        "sampled": "```python\nimport os\n\ndef _populate_cache(platform, pkg_cache, mount_dir):\n    \"\"\"\n    If a ``pkg_cache`` directory is specified, then use it to populate the\n    disk image.\n    \"\"\"\n    \n    # Check if pkg_cache is not None and is a directory\n    if pkg_cache and os.path.isdir(pkg_cache):\n        # Iterate over all files in the pkg_cache directory\n        for root, dirs, files in os.walk(pkg_cache):\n            # For each file"
    },
    {
        "original": "def clip(self, X):\n        \"\"\"\n        Clip values to fall within any global or column-wise min/max constraints\n        \"\"\"\n        X = np.asarray(X)\n        if self.min_value is not None:\n            X[X < self.min_value] = self.min_value\n        if self.max_value is not None:\n            X[X > self.max_value] = self.max_value\n        return X",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.min_val = None\n        self.max_val = None\n\n    def configure_constraints(self, min_global=None, max_global=None):\n        \"\"\"\n        Configure global min/max constraints.\n        \n        Args:\n            min_global (float): Global minimum constraint.\n            max_global (float): Global maximum constraint.\n        \"\"\"\n        \n        if min_global is not None and (self.min_val is None or min_global < self.min_val):\n           "
    },
    {
        "original": "def isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    if isinstance(obj, Exception):\n        return True\n    if isclass(obj) and issubclass(obj, Exception):\n        return True\n    return False",
        "sampled": "```python\ndef isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    \n# The given function 'isexception' now works fine.\n```"
    },
    {
        "original": "def discard_plugin_preset(self):\n        \"\"\"\n        Discard the current active preset. Will release any active plugins that could have come from the old preset.\n        \"\"\"\n        if self.has_plugin_preset:\n            for name, plugin in list(self._active_plugins.items()):\n                if id(plugin) in self._provided_by_preset:\n                    self.release_plugin(name)\n            self._active_preset.deactivate(self)\n        self._active_preset = None",
        "sampled": "### Problem Description\nDiscard the current active preset and release any active plugins that could have come from the old preset.\n\n### Input and Output Specifications\n- No input is required\n- No output is expected. The function will release active plugins if they exist\n\n### Python Solution\n\n```python\nclass PresetDiscarder:\n    def __init__(self):\n        # Initialize an empty set to store active plugins\n        self.active_plugins = set()\n\n    def add_plugin(self, plugin):\n       "
    },
    {
        "original": "def parse_comparison_operation(operation: str) -> Tuple[Optional[str], str]:\n    \"\"\"Parse the comparision operator in an operation.\"\"\"\n    _operation = operation.strip()\n    if not _operation:\n        raise QueryParserException('Operation is not valid: {}'.format(operation))\n    # Check inclusion comparison\n    if _operation[:2] in ('<=', '=<'):\n        return '<=', _operation[2:].strip()\n\n    if _operation[:2] in ('>=', '=>'):\n        return '>=', _operation[2:].strip()\n\n    # Non inclusive\n    if _operation[:1] in ('>', '<'):\n        return _operation[:1], _operation[1:].strip()\n\n    return None, _operation",
        "sampled": "```python\nfrom typing import Tuple, Optional\nimport itertools\n\ndef parse_comparison_operation(operation: str) -> Tuple[Optional[str], str]:\n    \"\"\"\n    Parse the comparision operator in an operation.\n\n    Each operation will contain three parts: lhs, op, rhs.\n    The function will return a tuple containing the operator or None if it is not found \n    and the rest of the operation that should be inquired with regards to whether lhs is less than rhs.\n\n    Args:\n        comparison"
    },
    {
        "original": "def WriteHashBlobReferences(self, references_by_hash, cursor):\n    \"\"\"Writes blob references for a given set of hashes.\"\"\"\n    values = []\n    for hash_id, blob_refs in iteritems(references_by_hash):\n      refs = rdf_objects.BlobReferences(items=blob_refs).SerializeToString()\n      values.append({\n          \"hash_id\": hash_id.AsBytes(),\n          \"blob_references\": refs,\n      })\n    _Insert(cursor, \"hash_blob_references\", values)",
        "sampled": "### Problem Description\nWrite the code to complete the function `WriteHashBlobReferences` which takes two parameters:\n- `references_by_hash`: a dictionary of blob references where each key is a hash and each value is another dictionary containing the blob reference information.\n- `cursor`: an object used for obtaining chunked SQL.\n\n### Input and Output Specifications\nNo specific input or output specification is mentioned, but based on the function name and usage of a cursor object, it seems like this function might be designed"
    },
    {
        "original": "def payload(self, value):\n    \"\"\"Automatically encode RDFValues into the message.\"\"\"\n    if not isinstance(value, rdfvalue.RDFValue):\n      raise RuntimeError(\"Payload must be an RDFValue.\")\n\n    self.Set(\"args\", value.SerializeToString())\n\n    # pylint: disable=protected-access\n    if value._age is not None:\n      self.args_age = value._age\n    # pylint: enable=protected-access\n\n    self.args_rdf_name = value.__class__.__name__",
        "sampled": "### Problem Description\n### \nNo problem description is provided. Please provide the problem description in Markdown format.\n\n### Input and Output Specifications\n### \nNo input and output specifications are provided. Please provide the input and output specifications in Markdown format.\n\nHowever, based on the given function `payload(self, value)`, I will assume a basic problem where we need to encode RDFValues into a message.\n\nHere's an example of what it might look like:\n\n```python\nclass PayloadEncoder:\n    def payload"
    },
    {
        "original": "def handleError(self, test, err, capt=None):\n        \"\"\"\n        If the database plugin is not present, we have to handle capturing\n        \"errors\" that shouldn't be reported as such in base.\n        \"\"\"\n        if not hasattr(test.test, \"testcase_guid\"):\n            if err[0] == errors.BlockedTest:\n                raise SkipTest(err[1])\n                return True\n\n            elif err[0] == errors.DeprecatedTest:\n                raise SkipTest(err[1])\n                return True\n\n            elif err[0] == errors.SkipTest:\n                raise SkipTest(err[1])\n                return True",
        "sampled": "```python\nimport re\n\nclass Solution:\n    def handleError(self, test, err, capt=None):\n        \"\"\"\n        If the database plugin is not present, we have to handle capturing\n        \"errors\" that shouldn't be reported as such in base.\n        \n        Args:\n            test (str): The input string to be tested.\n            err (str): The error message.\n            capt (str): Captured message. Defaults to None.\n\n        Returns:\n            str: The error message if"
    },
    {
        "original": "def _draw_banner(self):\n        \"\"\"\n        Draw the banner with sorting options at the top of the page\n        \"\"\"\n        n_rows, n_cols = self.term.stdscr.getmaxyx()\n        window = self.term.stdscr.derwin(1, n_cols, self._row, 0)\n        window.erase()\n        window.bkgd(str(' '), self.term.attr('OrderBar'))\n\n        banner = docs.BANNER_SEARCH if self.content.query else self.BANNER\n        items = banner.strip().split(' ')\n\n        distance = (n_cols - sum(len(t) for t in items) - 1) / (len(items) - 1)\n        spacing = max(1, int(distance)) * ' '\n        text = spacing.join(items)\n        self.term.add_line(window, text, 0, 0)\n        if self.content.order is not None:\n            order = self.content.order.split('-')[0]\n            col = text.find(order) - 3\n            attr = self.term.attr('OrderBarHighlight')\n            window.chgat(0, col, 3, attr)\n\n        self._row += 1",
        "sampled": "```python\nclass Solution:\n    def _draw_banner(self):\n        \"\"\"\n        Draw the banner with sorting options at the top of the page\n        \"\"\"\n        \n        # Define the available sorting options\n        sorting_options = [\"Sort by Name\", \"Sort by Age\", \"Sort by Score\"]\n        \n        # Create a horizontal line to separate the banner from other content\n        print(\"-\" * 80)\n        \n        # Print each sorting option on a new line, centered in a 80-character"
    },
    {
        "original": "def sample_bitstrings(self, n_samples):\n        \"\"\"\n        Sample bitstrings from the distribution defined by the wavefunction.\n\n        :param n_samples: The number of bitstrings to sample\n        :return: An array of shape (n_samples, n_qubits)\n        \"\"\"\n        possible_bitstrings = np.array(list(itertools.product((0, 1), repeat=len(self))))\n        inds = np.random.choice(2 ** len(self), n_samples, p=self.probabilities())\n        bitstrings = possible_bitstrings[inds, :]\n        return bitstrings",
        "sampled": "```python\nimport numpy as np\n\nclass QuantumComputing:\n    def sample_bitstrings(self, n_samples):\n        # The numpy.random.randint function can be used to generate random bitstrings\n        # We assume that the wavefunction is a uniform distribution over all possible bitstrings\n        # So we can simply use the randint function to generate random integers between 0 and 2^n - 1\n        n_qubits = int(np.log2(n_samples))  # Calculate the number of"
    },
    {
        "original": "def _tap(tap, runas=None):\n    \"\"\"\n    Add unofficial GitHub repos to the list of formulas that brew tracks,\n    updates, and installs from.\n    \"\"\"\n    if tap in _list_taps():\n        return True\n\n    cmd = 'tap {0}'.format(tap)\n    try:\n        _call_brew(cmd)\n    except CommandExecutionError:\n        log.error('Failed to tap \"%s\"', tap)\n        return False\n\n    return True",
        "sampled": "I don't see the problem description, input, and output specifications. However, I'll provide a general template for a problem in Markdown format.\n\n### Problem Description\n#### Problem Title\n[Insert problem title here]\n\n#### Problem Statement\n[Insert problem statement here]\n\nGiven an integer `n`, find the `n`th prime number.\n\n### Input and Output Specifications\n\n| Input  | Description |\n| ---    | ---         |\n| `n`    | The position of the prime number to"
    },
    {
        "original": "def remove_vrf_conf(self, route_dist=None, vrf_id=None,\n                        vrf_rf=None):\n        \"\"\"Removes any matching `VrfConf` for given `route_dist` or `vrf_id`\n\n        Parameters:\n            - `route_dist`: (str) route distinguisher of a configured VRF\n            - `vrf_id`: (str) vrf ID\n            - `vrf_rf`: (str) route family of the VRF configuration\n        If only `route_dist` is given, removes `VrfConf`s for all supported\n        address families for this `route_dist`. If `vrf_rf` is given, than only\n        removes `VrfConf` for that specific route family. If only `vrf_id` is\n        given, matching `VrfConf` will be removed.\n        \"\"\"\n        if route_dist is None and vrf_id is None:\n            raise RuntimeConfigError(desc='To delete supply route_dist or id.')\n\n        # By default we remove all VRFs for given Id or RD\n        vrf_rfs = SUPPORTED_VRF_RF\n        # If asked to delete specific route family vrf conf.\n        if vrf_rf:\n            vrf_rfs = vrf_rf\n\n        # For all vrf route family asked to be deleted, we collect all deleted\n        # VrfConfs\n        removed_vrf_confs = []\n        for route_family in vrf_rfs:\n            if route_dist is not None:\n                rd_rf_id = VrfConf.create_rd_rf_id(route_dist, route_family)\n                vrf_conf = self._vrfs_by_rd_rf.pop(rd_rf_id, None)\n                if vrf_conf:\n                    self._vrfs_by_id.pop(vrf_conf.id, None)\n                    removed_vrf_confs.append(vrf_conf)\n            else:\n                vrf_conf = self._vrfs_by_id.pop(vrf_id, None)\n                if vrf_conf:\n                    self._vrfs_by_rd_rf.pop(vrf_conf.rd_rd_id, None)\n                    removed_vrf_confs.append(vrf_conf)\n\n        # We do not raise any exception if we cannot find asked VRF.\n        for vrf_conf in removed_vrf_confs:\n            self._notify_listeners(VrfsConf.REMOVE_VRF_CONF_EVT, vrf_conf)\n        return removed_vrf_confs",
        "sampled": "```python\nclass VrfConf:\n    def __init__(self, route_dist=None, vrf_id=None, vrf_rf=None):\n        self.route_dist = route_dist\n        self.vrf_id = vrf_id\n        self.vrf_rf = vrf_rf\n\nclass VRFConfigManager:\n    def __init__(self):\n        self.configs = []\n\n    def remove_vrf_conf(self, route_dist=None, vrf_id=None, vrf_rf=None):\n        \"\"\""
    },
    {
        "original": "def _RemoveCompletedPathspec(self, index):\n    \"\"\"Removes a pathspec from the list of pathspecs.\"\"\"\n    pathspec = self.state.indexed_pathspecs[index]\n    request_data = self.state.request_data_list[index]\n\n    self.state.indexed_pathspecs[index] = None\n    self.state.request_data_list[index] = None\n    self.state.pending_hashes.pop(index, None)\n    self.state.pending_files.pop(index, None)\n\n    # We have a bit more room in the pending_hashes so we try to schedule\n    # another pathspec.\n    self._TryToStartNextPathspec()\n    return pathspec, request_data",
        "sampled": "### Problem Description\n### ==================\n### Remove Completed Pathspec\n\nGiven a list of pathspecs, remove the pathspec at the specified index.\n\n### Input and Output Specifications\n### ===============================\n### Input:\n- `index`: The index of the pathspec to be removed.\n- `pathspecs`: A list of pathspecs.\n\n### Output:\n- The updated list of pathspecs after removing the specified pathspec.\n\n```python\nclass GitRepository:\n    def __init__(self):\n        self.path"
    },
    {
        "original": "def _check_1d_array_object(parameter, name_param):\n    \"\"\"\n    Checks whether given parameter is a 1d array like object, and returns a numpy array object\n    \"\"\"\n    if isinstance(parameter, (np.ndarray, list, tuple, np.matrix)):\n        parameter = np.array(parameter)\n        if parameter.ndim != 1:\n            raise TypeError(\"{} should be a 1d array type object\".format(name_param))\n    else:\n        raise TypeError(\"{} should be a 1d array type object\".format(name_param))\n\n    return parameter",
        "sampled": "### Problem Description\n#### Array 1D Standardization\n\nYou are given a list of integers representing the marks of students in a class. The marks can be either positive or negative, and they can also be zero. You need to standardize the array such that all elements in the array are between 0 and 100.\n\nThe formula for standardization is:\n\n$$x_{std} = \\frac{x - x_{min}}{x_{max} - x_{min}} \\times "
    },
    {
        "original": "def runRmFile(self, path, timeout=None, **kwargs):\n        \"\"\" remove a file from the worker \"\"\"\n        cmd_args = {'path': path, 'logEnviron': self.logEnviron}\n        if timeout:\n            cmd_args['timeout'] = timeout\n        if self.workerVersionIsOlderThan('rmfile', '3.1'):\n            cmd_args['dir'] = os.path.abspath(path)\n            return self.runRemoteCommand('rmdir', cmd_args, **kwargs)\n        return self.runRemoteCommand('rmfile', cmd_args, **kwargs)",
        "sampled": "```python\nclass Solution:\n    def longestDecomposition(self, text: str) -> int:\n        res = 0\n        i, j = 0, len(text) - 1\n\n        while i < j:\n            if text[i] != text[j]:\n                break\n            if i + 1 == j - 1 and text[i+1:j] == text[i][::-1]:\n                res += 2\n                i += 2\n                j -="
    },
    {
        "original": "def get_texts(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tnp.array\n\n\t\tTexts\n\t\t\"\"\"\n\t\tif self._document_category_df is None:\n\t\t\treturn pd.np.array([])\n\t\treturn self._document_category_df.text.values",
        "sampled": "```python\nimport numpy as np\n\nclass Solution:\n    def get_texts(self):\n        \"\"\"\n        Returns\n        -------\n        np.array\n\n        Texts\n        \"\"\"\n        \n        # Assuming we have a list of texts, let's create a numpy array from it\n        texts = np.array([\"text1\", \"text2\", \"text3\"])\n        \n        return texts\n\n# Usage:\nsolution = Solution()\nprint(solution.get_texts())\n```\n\nPlease note that the actual implementation may vary based on"
    },
    {
        "original": "def debug_text_world(self, text: str, pos: Union[Unit, Point2, Point3], color=None, size: int = 8):\n        \"\"\" Draws a text at Point3 position. Don't forget to add 'await self._client.send_debug'.\n        To grab a unit's 3d position, use unit.position3d\n        Usually the Z value of a Point3 is between 8 and 14 (except for flying units)\n        \"\"\"\n        if isinstance(pos, Point2) and not isinstance(pos, Point3):  # a Point3 is also a Point2\n            pos = Point3((pos.x, pos.y, 0))\n        self._debug_texts.append(self.to_debug_message(text, color, pos, size))",
        "sampled": "```python\nfrom typing import Union\n\nclass DebugTextWorld:\n    def debug_text_world(self, text: str, pos: Union[tuple, list], color=None, size: int = 8):\n        \"\"\"\n        Draws a text at Point3 position.\n        \n        Args:\n            text (str): The text to be drawn.\n            pos (Union[tuple, list]): A tuple or list containing the x and y coordinates of the point.\n            color (str): The color of the"
    },
    {
        "original": "def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n        \"\"\"\n        Set the vectors for the Vocab instance from a collection of Tensors.\n\n        Arguments:\n            stoi: A dictionary of string to the index of the associated vector\n                in the `vectors` input argument.\n            vectors: An indexed iterable (or other structure supporting __getitem__) that\n                given an input index, returns a FloatTensor representing the vector\n                for the token associated with the index. For example,\n                vector[stoi[\"string\"]] should return the vector for \"string\".\n            dim: The dimensionality of the vectors.\n            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n                to zero vectors; can be any function that takes in a Tensor and\n                returns a Tensor of the same size. Default: torch.Tensor.zero_\n        \"\"\"\n        self.vectors = torch.Tensor(len(self), dim)\n        for i, token in enumerate(self.itos):\n            wv_index = stoi.get(token, None)\n            if wv_index is not None:\n                self.vectors[i] = vectors[wv_index]\n            else:\n                self.vectors[i] = unk_init(self.vectors[i])",
        "sampled": "```python\nimport torch\n\nclass Vocab:\n    def __init__(self):\n        self.stoi = {}\n        self.vectors = []\n\n    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):\n        \"\"\"\n        Set the vectors for the Vocab instance from a collection of Tensors.\n\n        Arguments:\n            stoi: A dictionary of string to the index of the associated vector\n                in the `vectors` input argument.\n            vectors: An indexed iterable"
    },
    {
        "original": "def k8s_events_handle_build_job_statuses(self: 'celery_app.task', payload: Dict) -> None:\n    \"\"\"Project Plugin jobs statuses\"\"\"\n    details = payload['details']\n    app = details['labels']['app']\n    job_uuid = details['labels']['job_uuid']\n    job_name = details['labels']['job_name']\n    project_name = details['labels'].get('project_name')\n    logger.debug('handling events status for build jon %s %s', job_name, app)\n\n    try:\n        build_job = BuildJob.objects.get(uuid=job_uuid)\n    except BuildJob.DoesNotExist:\n        logger.info('Build job `%s` does not exist', job_name)\n        return\n\n    try:\n        build_job.project\n    except Project.DoesNotExist:\n        logger.debug('`%s` does not exist anymore', project_name)\n\n    # Set the new status\n    try:\n        set_node_scheduling(build_job, details['node_name'])\n        build_job.set_status(status=payload['status'],\n                             message=payload['message'],\n                             traceback=payload.get('traceback'),\n                             details=details)\n    except IntegrityError:\n        # Due to concurrency this could happen, we just retry it\n        self.retry(countdown=Intervals.EXPERIMENTS_SCHEDULER)",
        "sampled": "```python\nfrom typing import Dict\n\ndef k8s_events_handle_build_job_statuses(self: 'celery_app.task', payload: Dict) -> None:\n    \"\"\"\n    Project Plugin jobs statuses\n    \"\"\"\n\n    # Check if the payload is not empty\n    if payload:\n        # Get the job status from the payload\n        job_status = payload.get('status')\n        \n        # Check if the job status is a dictionary and has a 'state' key\n        if isinstance(job_status,"
    },
    {
        "original": "def strip_prompt(self, a_string):\n        \"\"\" Strip 'Done' from command output \"\"\"\n        output = super(NetscalerSSH, self).strip_prompt(a_string)\n        lines = output.split(self.RESPONSE_RETURN)\n        if \"Done\" in lines[-1]:\n            return self.RESPONSE_RETURN.join(lines[:-1])\n        else:\n            return output",
        "sampled": "```python\nclass Solution:\n    def strip_prompt(self, a_string):\n        \"\"\"\n        This function removes 'Done' from the given string if it exists.\n        \n        Parameters:\n        a_string (str): The input string.\n        \n        Returns:\n        str: The modified string without 'Done' at the end.\n        \n        Inline Math LaTeX Tags are not used in this solution as there's no math formula involved.\n        \n        Problem Description and Input/Output Specifications are given below for reference:\n           "
    },
    {
        "original": "def is_ordered(self):\n        \"\"\"\n        True if site is an ordered site, i.e., with a single species with\n        occupancy 1.\n        \"\"\"\n        totaloccu = self.species.num_atoms\n        return totaloccu == 1 and len(self.species) == 1",
        "sampled": "```python\nclass Site:\n    def __init__(self, species, occupancy):\n        \"\"\"\n        Initialize a site with a species and occupancy.\n        \n        Args:\n            species (str): The name of the species occupying the site.\n            occupancy (int): The number of particles occupying the site.\n        \"\"\"\n        self.species = species\n        self.occupancy = occupancy\n\n    def is_ordered(self):\n        \"\"\"\n        True if site is an ordered site, i.e., with a single species"
    },
    {
        "original": "def fit(self, X, y=None):\n        \"\"\"Fit the imputer on X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n\n        Returns\n        -------\n        self : _SimpleImputer\n        \"\"\"\n        X = self._validate_input(X)\n\n        # default fill_value is 0 for numerical input and \"missing_value\"\n        # otherwise\n        if self.fill_value is None:\n            if X.dtype.kind in (\"i\", \"u\", \"f\"):\n                fill_value = 0\n            else:\n                fill_value = \"missing_value\"\n        else:\n            fill_value = self.fill_value\n\n        # fill_value should be numerical in case of numerical input\n        if (self.strategy == \"constant\" and\n                X.dtype.kind in (\"i\", \"u\", \"f\") and\n                not isinstance(fill_value, numbers.Real)):\n            raise ValueError(\"'fill_value'={0} is invalid. Expected a \"\n                             \"numerical value when imputing numerical \"\n                             \"data\".format(fill_value))\n\n        if sparse.issparse(X):\n            # missing_values = 0 not allowed with sparse data as it would\n            # force densification\n            if self.missing_values == 0:\n                raise ValueError(\"Imputation not possible when missing_values \"\n                                 \"== 0 and input is sparse. Provide a dense \"\n                                 \"array instead.\")\n            else:\n                self.statistics_ = self._sparse_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n                                                    fill_value)\n        else:\n            self.statistics_ = self._dense_fit(X,\n                                               self.strategy,\n                                               self.missing_values,\n                                               fill_value)\n\n        return self",
        "sampled": "### Problem Description\n**Simple Imputer**\n\nThe SimpleImputer is a class used to impute missing values in a dataset. It can be used to replace missing values with the mean, median, or most frequent value of the feature.\n\n### Input and Output Specifications\n\n*   `X`: The input data where missing values need to be imputed.\n*   `y`: Not used in this problem (optional).\n*   The method should return an instance of the SimpleImputer class.\n\n###"
    },
    {
        "original": "def _pdf_guess_version(input_file, search_window=1024):\n    \"\"\"Try to find version signature at start of file.\n\n    Not robust enough to deal with appended files.\n\n    Returns empty string if not found, indicating file is probably not PDF.\n    \"\"\"\n\n    with open(input_file, 'rb') as f:\n        signature = f.read(search_window)\n    m = re.search(br'%PDF-(\\d\\.\\d)', signature)\n    if m:\n        return m.group(1)\n    return ''",
        "sampled": "```python\nimport struct\n\ndef _pdf_guess_version(input_file, search_window=1024):\n    \"\"\"\n    Try to find version signature at start of file.\n\n    Not robust enough to deal with appended files.\n\n    Returns empty string if not found, indicating file is probably not PDF.\n    \"\"\"\n    \n    # Read the first 1024 bytes from the input file\n    data = input_file.read(search_window)\n    \n    # Check if the data starts with a valid PDF signature (0x"
    },
    {
        "original": "def _FlushAllRows(self, db_connection, table_name):\n    \"\"\"Copies rows from the given db into the output file then deletes them.\"\"\"\n    for sql in db_connection.iterdump():\n      if (sql.startswith(\"CREATE TABLE\") or\n          sql.startswith(\"BEGIN TRANSACTION\") or sql.startswith(\"COMMIT\")):\n        # These statements only need to be written once.\n        continue\n      # The archive generator expects strings (not Unicode objects returned by\n      # the pysqlite library).\n      yield self.archive_generator.WriteFileChunk((sql + \"\\n\").encode(\"utf-8\"))\n    with db_connection:\n      db_connection.cursor().execute(\"DELETE FROM \\\"%s\\\";\" % table_name)",
        "sampled": "### Problem Description\n#### Database Row Flusher\n\nYou are given a database connection and a table name. Your task is to copy all rows from the given table into an output file, then delete them from the original table.\n\n### Input and Output Specifications\n\n*   `db_connection`: A connection object to the database.\n*   `table_name`: The name of the table in the database.\n*   No output is expected, but it's assumed that all rows have been successfully copied and deleted.\n\n"
    },
    {
        "original": "def simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    \"\"\"\n    Convert a document into a list of tokens.\n\n    This lowercases, tokenizes, de-accents (optional). -- the output are final\n    tokens = unicode strings, that won't be processed any further.\n\n    \"\"\"\n    tokens = [\n        token for token in tokenize(doc, lower=True, deacc=deacc, errors='ignore')\n        if min_len <= len(token) <= max_len and not token.startswith('_')\n    ]\n    return tokens",
        "sampled": "```python\nimport unicodedata\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\ndef simple_preprocess(doc, deacc=False, min_len=2, max_len=15):\n    \"\"\"\n    Convert a document into a list of tokens.\n\n    This lowercases, tokenizes, de-accents (optional). \n    -- the output are final tokens = unicode strings, \n       that won't be processed any further.\n\n    Args:\n        doc (str): The input document.\n       "
    },
    {
        "original": "def write_metadata(self, symbol, metadata):\n        \"\"\"\n        writes user defined metadata for the given symbol\n\n        Parameters\n        ----------\n        symbol: str\n            symbol for the given item in the DB\n        metadata: ?\n            metadata to write\n        \"\"\"\n        sym = self._get_symbol_info(symbol)\n        if not sym:\n            raise NoDataFoundException(\"Symbol does not exist.\")\n\n        sym[USERMETA] = metadata\n        self._symbols.replace_one({SYMBOL: symbol}, sym)",
        "sampled": "```python\nimport sqlite3\n\nclass Database:\n    def __init__(self, db_name):\n        self.conn = sqlite3.connect(db_name)\n        self.cursor = self.conn.cursor()\n        self.create_table()\n\n    def create_table(self):\n        \"\"\"\n        Creates a table in the database to store metadata.\n        \n        The table has two columns: symbol and metadata.\n        \"\"\"\n        query = \"\"\"\n            CREATE TABLE IF NOT EXISTS metadata (\n                symbol TEXT,\n                value TEXT\n            )\n        \"\"\"\n"
    },
    {
        "original": "def wrap_text(text, width):\n        \"\"\"\n        Wrap text paragraphs to the given character width while preserving\n        newlines.\n        \"\"\"\n        out = []\n        for paragraph in text.splitlines():\n            # Wrap returns an empty list when paragraph is a newline. In order\n            # to preserve newlines we substitute a list containing an empty\n            # string.\n            lines = wrap(paragraph, width=width) or ['']\n            out.extend(lines)\n        return out",
        "sampled": "```python\ndef wrap_text(text, width):\n    \"\"\"\n    Wrap text paragraphs to the given character width while preserving newlines.\n    \"\"\"\n    lines = text.split('\\n')\n    wrapped_lines = []\n    \n    for line in lines:\n        words = line.split()\n        current_line = ''\n        \n        for word in words:\n            if len(current_line) + len(word) + 1 > width:\n                wrapped_lines.append(current_line)\n                current_line = word\n            else:\n                if"
    },
    {
        "original": "def any2mb(s):\n    \"\"\"Convert string or number to memory in megabytes.\"\"\"\n    if is_string(s):\n        return int(Memory.from_string(s).to(\"Mb\"))\n    else:\n        return int(s)",
        "sampled": "```python\ndef any2mb(s):\n    \"\"\"\n    Convert string or number to memory in megabytes.\n    \n    Parameters:\n    s (str or int): The quantity of memory.\n    \n    Returns:\n    float: The converted memory in megabytes.\n    \"\"\"\n    \n    # Define a dictionary to map human-readable prefixes to their respective decimal values\n    prefixes = {\n        'b': 1, 'Byte': 1, \n        'kb': 1024, 'Kil"
    },
    {
        "original": "def _read_file(folder, filename):\n    \"\"\"\n    Reads and returns the contents of a file\n    \"\"\"\n    path = os.path.join(folder, filename)\n    try:\n        with salt.utils.files.fopen(path, 'rb') as contents:\n            return salt.utils.data.decode(contents.readlines())\n    except (OSError, IOError):\n        return ''",
        "sampled": "```python\ndef _read_file(folder, filename):\n    \"\"\"\n    Reads and returns the contents of a file\n    \"\"\"\n    \n    try:\n        with open(f\"{folder}/{filename}\", \"r\") as file:\n            return file.read()\n    except FileNotFoundError:\n        return None\n\n# Example usage\nprint(_read_file(\"path_to_folder\", \"filename.txt\"))\n```\n\nHowever, since you haven't provided the problem description yet, I'll wait for that before providing a solution to the actual"
    },
    {
        "original": "def saltenviron(environ):\n    \"\"\"\n    Make Salt's opts dict and the APIClient available in the WSGI environ\n    \"\"\"\n    if '__opts__' not in locals():\n        import salt.config\n        __opts__ = salt.config.client_config(\n                os.environ.get('SALT_MASTER_CONFIG', '/etc/salt/master'))\n\n    environ['SALT_OPTS'] = __opts__\n    environ['SALT_APIClient'] = salt.netapi.NetapiClient(__opts__)",
        "sampled": "```python\ndef saltenviron(environ):\n    \"\"\"\n    Make Salt's opts dict and the APIClient available in the WSGI environ\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the environment variables\n    env_vars = {}\n    \n    # Check if 'HTTP_X_SALT_OPTS' is present in the environment variables\n    if 'HTTP_X_SALT_OPTS' in environ:\n        # If present, parse it as a JSON string and add it to the env_vars dictionary"
    },
    {
        "original": "def from_dict(cls, d):\n        \"\"\"\n        Convert a dictionary into an xarray.DataArray\n\n        Input dict can take several forms::\n\n            d = {'dims': ('t'), 'data': x}\n\n            d = {'coords': {'t': {'dims': 't', 'data': t,\n                                  'attrs': {'units':'s'}}},\n                 'attrs': {'title': 'air temperature'},\n                 'dims': 't',\n                 'data': x,\n                 'name': 'a'}\n\n        where 't' is the name of the dimesion, 'a' is the name of the array,\n        and  x and t are lists, numpy.arrays, or pandas objects.\n\n        Parameters\n        ----------\n        d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n\n        Returns\n        -------\n        obj : xarray.DataArray\n\n        See also\n        --------\n        DataArray.to_dict\n        Dataset.from_dict\n        \"\"\"\n        coords = None\n        if 'coords' in d:\n            try:\n                coords = OrderedDict([(k, (v['dims'],\n                                           v['data'],\n                                           v.get('attrs')))\n                                      for k, v in d['coords'].items()])\n            except KeyError as e:\n                raise ValueError(\n                    \"cannot convert dict when coords are missing the key \"\n                    \"'{dims_data}'\".format(dims_data=str(e.args[0])))\n        try:\n            data = d['data']\n        except KeyError:\n            raise ValueError(\"cannot convert dict without the key 'data''\")\n        else:\n            obj = cls(data, coords, d.get('dims'), d.get('name'),\n                      d.get('attrs'))\n        return obj",
        "sampled": "```python\nimport xarray as xr\nimport numpy as np\n\ndef from_dict(cls, d):\n    \"\"\"\n    Convert a dictionary into an xarray.DataArray\n\n    Parameters\n    ----------\n    cls : class, not used in this implementation\n    d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n\n    Returns\n    -------\n    obj : xarray.DataArray\n\n    See also\n    --------\n    DataArray.to_dict\n    Dataset"
    },
    {
        "original": "def set_(key, value, service=None, profile=None):  # pylint: disable=W0613\n    \"\"\"\n    Set a key/value pair in the etcd service\n    \"\"\"\n    client = _get_conn(profile)\n    client.set(key, value)\n    return get(key, service, profile)",
        "sampled": "### Problem Description\n### ==================\nImplement a function to calculate the sum of all elements in a given array. The array can contain both positive and negative integers.\n\n### Input and Output Specifications\n### =================================\n#### Input:\n- `arr`: A list of integers\n\n#### Output:\n- `sum`: The sum of all elements in the input list\n\n### Solution\n```python\ndef set_(key, value, service=None, profile=None):  # pylint: disable=W0613\n    \"\"\"\n"
    },
    {
        "original": "def GetMessages(self, soft_size_limit=None):\n    \"\"\"Retrieves and removes the messages from the queue.\n\n    Args:\n      soft_size_limit: int If there is more data in the queue than\n        soft_size_limit bytes, the returned list of messages will be\n        approximately this large. If None (default), returns all messages\n        currently on the queue.\n\n    Returns:\n      rdf_flows.MessageList A list of messages that were .Put on the queue\n      earlier.\n    \"\"\"\n    with self._lock:\n      ret = rdf_flows.MessageList()\n      ret_size = 0\n      for message in self._Generate():\n        self._total_size -= len(message)\n        ret.job.append(rdf_flows.GrrMessage.FromSerializedString(message))\n        ret_size += len(message)\n        if soft_size_limit is not None and ret_size > soft_size_limit:\n          break\n\n      return ret",
        "sampled": "### Problem: Get Messages from Queue\n\n#### Problem Description\nRetrieves and removes the messages from the queue based on a specified size limit.\n\n#### Input Specifications\n* `soft_size_limit`: Optional integer specifying the maximum number of bytes to return. If None, returns all messages.\n\n#### Output Specifications\n* A list of `rdf_flows.MessageList` objects that were previously added to the queue.\n\n### Python Solution\n\n```python\nfrom typing import List, Optional\n\nclass MessageQueue:\n   "
    },
    {
        "original": "def check_result(running, recurse=False, highstate=None):\n    \"\"\"\n    Check the total return value of the run and determine if the running\n    dict has any issues\n    \"\"\"\n    if not isinstance(running, dict):\n        return False\n\n    if not running:\n        return False\n\n    ret = True\n    for state_id, state_result in six.iteritems(running):\n        expected_type = dict\n        # The __extend__ state is a list\n        if \"__extend__\" == state_id:\n            expected_type = list\n        if not recurse and not isinstance(state_result, expected_type):\n            ret = False\n        if ret and isinstance(state_result, dict):\n            result = state_result.get('result', _empty)\n            if result is False:\n                ret = False\n            # only override return value if we are not already failed\n            elif result is _empty and isinstance(state_result, dict) and ret:\n                ret = check_result(\n                    state_result, recurse=True, highstate=highstate)\n        # if we detect a fail, check for onfail requisites\n        if not ret:\n            # ret can be None in case of no onfail reqs, recast it to bool\n            ret = bool(check_onfail_requisites(state_id, state_result,\n                                               running, highstate))\n        # return as soon as we got a failure\n        if not ret:\n            break\n    return ret",
        "sampled": "### Problem Description\n### --------------------\n\n### Flight Route Trouble-Shooter\nYour job is to help a flight route planner deal with some trouble. The planner is unable to determine where in the flight route things went wrong.\n\nThe function needs to check the total return value of a certain calculation (`running`) and figure out if any issues are present within the `running` dictionary, whether those are expected or unexpected problems. If there's an 'error' key in its dictionary (with non-zero value), it"
    },
    {
        "original": "def _extract_program_from_pyquil_executable_response(response: PyQuilExecutableResponse) -> Program:\n    \"\"\"\n    Unpacks a rpcq PyQuilExecutableResponse object into a pyQuil Program object.\n\n    :param response: PyQuilExecutableResponse object to be unpacked.\n    :return: Resulting pyQuil Program object.\n    \"\"\"\n    p = Program(response.program)\n    for attr, val in response.attributes.items():\n        setattr(p, attr, val)\n    return p",
        "sampled": "```python\nfrom pyquil.api import PyQuilExecutableResponse\nfrom pyquil.quil import Program\n\ndef _extract_program_from_pyquil_executable_response(response: PyQuilExecutableResponse) -> Program:\n    \"\"\"\n    Unpacks a rpcq PyQuilExecutableResponse object into a pyQuil Program object.\n\n    :param response: PyQuilExecutableResponse object to be unpacked.\n    :return: Resulting pyQuil Program object.\n    \"\"\"\n    \n    # Assuming"
    },
    {
        "original": "def from_file(filename=\"feff.inp\"):\n        \"\"\"\n        Creates a Feff_tag dictionary from a PARAMETER or feff.inp file.\n\n        Args:\n            filename: Filename for either PARAMETER or feff.inp file\n\n        Returns:\n            Feff_tag object\n        \"\"\"\n        with zopen(filename, \"rt\") as f:\n            lines = list(clean_lines(f.readlines()))\n        params = {}\n        eels_params = []\n        ieels = -1\n        ieels_max = -1\n        for i, line in enumerate(lines):\n            m = re.match(r\"([A-Z]+\\d*\\d*)\\s*(.*)\", line)\n            if m:\n                key = m.group(1).strip()\n                val = m.group(2).strip()\n                val = Tags.proc_val(key, val)\n                if key not in (\"ATOMS\", \"POTENTIALS\", \"END\", \"TITLE\"):\n                    if key in [\"ELNES\", \"EXELFS\"]:\n                        ieels = i\n                        ieels_max = ieels + 5\n                    else:\n                        params[key] = val\n            if ieels >= 0:\n                if i >= ieels and i <= ieels_max:\n                    if i == ieels + 1:\n                        if int(line.split()[1]) == 1:\n                            ieels_max -= 1\n                    eels_params.append(line)\n\n        if eels_params:\n            if len(eels_params) == 6:\n                eels_keys = ['BEAM_ENERGY', 'BEAM_DIRECTION', 'ANGLES', 'MESH', 'POSITION']\n            else:\n                eels_keys = ['BEAM_ENERGY', 'ANGLES', 'MESH', 'POSITION']\n            eels_dict = {\"ENERGY\": Tags._stringify_val(eels_params[0].split()[1:])}\n            for k, v in zip(eels_keys, eels_params[1:]):\n                eels_dict[k] = str(v)\n            params[str(eels_params[0].split()[0])] = eels_dict\n\n        return Tags(params)",
        "sampled": "```\nimport re\n\nclass Feff_tag:\n    def __init__(self):\n        self.data = {}\n\n    def add(self, tag, value):\n        self.data[tag] = value\n\ndef from_file(filename=\"feff.inp\"):\n    \"\"\"\n    Creates a Feff_tag dictionary from a PARAMETER or feff.inp file.\n\n    Args:\n        filename: Filename for either PARAMETER or feff.inp file\n\n    Returns:\n        Feff_tag object\n    \"\"\"\n    \n   "
    },
    {
        "original": "def read_fermi_contact_shift(self):\n        \"\"\"\n        output example:\n        Fermi contact (isotropic) hyperfine coupling parameter (MHz)\n        -------------------------------------------------------------\n        ion      A_pw      A_1PS     A_1AE     A_1c      A_tot\n        -------------------------------------------------------------\n         1      -0.002    -0.002    -0.051     0.000    -0.052\n         2      -0.002    -0.002    -0.051     0.000    -0.052\n         3       0.056     0.056     0.321    -0.048     0.321\n        -------------------------------------------------------------\n        , which corresponds to\n        [[-0.002, -0.002, -0.051, 0.0, -0.052],\n         [-0.002, -0.002, -0.051, 0.0, -0.052],\n         [0.056, 0.056, 0.321, -0.048, 0.321]] from 'fch' data\n        \"\"\"\n\n        # Fermi contact (isotropic) hyperfine coupling parameter (MHz)\n        header_pattern1 = r\"\\s*Fermi contact \\(isotropic\\) hyperfine coupling parameter \\(MHz\\)\\s+\" \\\n                          r\"\\s*\\-+\" \\\n                          r\"\\s*ion\\s+A_pw\\s+A_1PS\\s+A_1AE\\s+A_1c\\s+A_tot\\s+\" \\\n                          r\"\\s*\\-+\"\n        row_pattern1 = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 5)\n        footer_pattern = r\"\\-+\"\n        fch_table = self.read_table_pattern(header_pattern1, row_pattern1,\n                                            footer_pattern, postprocess=float,\n                                            last_one_only=True)\n\n        # Dipolar hyperfine coupling parameters (MHz)\n        header_pattern2 = r\"\\s*Dipolar hyperfine coupling parameters \\(MHz\\)\\s+\" \\\n                          r\"\\s*\\-+\" \\\n                          r\"\\s*ion\\s+A_xx\\s+A_yy\\s+A_zz\\s+A_xy\\s+A_xz\\s+A_yz\\s+\" \\\n                          r\"\\s*\\-+\"\n        row_pattern2 = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 6)\n        dh_table = self.read_table_pattern(header_pattern2, row_pattern2,\n                                           footer_pattern, postprocess=float,\n                                           last_one_only=True)\n\n        # Total hyperfine coupling parameters after diagonalization (MHz)\n        header_pattern3 = r\"\\s*Total hyperfine coupling parameters after diagonalization \\(MHz\\)\\s+\" \\\n                          r\"\\s*\\(convention: \\|A_zz\\| > \\|A_xx\\| > \\|A_yy\\|\\)\\s+\" \\\n                          r\"\\s*\\-+\" \\\n                          r\"\\s*ion\\s+A_xx\\s+A_yy\\s+A_zz\\s+asymmetry \\(A_yy - A_xx\\)/ A_zz\\s+\" \\\n                          r\"\\s*\\-+\"\n        row_pattern3 = r'(?:\\d+)\\s+' + r'\\s+'.join([r'([-]?\\d+\\.\\d+)'] * 4)\n        th_table = self.read_table_pattern(header_pattern3, row_pattern3,\n                                           footer_pattern, postprocess=float,\n                                           last_one_only=True)\n\n        fc_shift_table = {'fch': fch_table, 'dh': dh_table, 'th': th_table}\n\n        self.data[\"fermi_contact_shift\"] = fc_shift_table",
        "sampled": "### Problem Description\nFermi contact (isotropic) hyperfine coupling parameter (MHz)\n-------------------------------------------------------------\nion      A_pw      A_1PS     A_1AE     A_1c      A_tot\n-------------------------------------------------------------\n  1      -0.002    -0.002    -0.051     0.000    -0.052\n  2      -0.002    -0.002    -0.051     0.000    -"
    },
    {
        "original": "def parse(self, filename):\n        \"\"\"\n        Read and parse a pseudopotential file. Main entry point for client code.\n\n        Returns:\n            pseudopotential object or None if filename is not a valid pseudopotential file.\n        \"\"\"\n        path = os.path.abspath(filename)\n\n        # Only PAW supports XML at present.\n        if filename.endswith(\".xml\"):\n            return PawXmlSetup(path)\n\n        ppdesc = self.read_ppdesc(path)\n\n        if ppdesc is None:\n            logger.critical(\"Cannot find ppdesc in %s\" % path)\n            return None\n\n        psp_type = ppdesc.psp_type\n\n        parsers = {\n            \"FHI\": NcAbinitHeader.fhi_header,\n            \"GTH\": NcAbinitHeader.gth_header,\n            \"TM\": NcAbinitHeader.tm_header,\n            \"Teter\": NcAbinitHeader.tm_header,\n            \"HGH\": NcAbinitHeader.hgh_header,\n            \"HGHK\": NcAbinitHeader.hgh_header,\n            \"ONCVPSP\": NcAbinitHeader.oncvpsp_header,\n            \"PAW_abinit_text\": PawAbinitHeader.paw_header,\n        }\n\n        try:\n            header = parsers[ppdesc.name](path, ppdesc)\n        except Exception:\n            raise self.Error(path + \":\\n\" + straceback())\n\n        if psp_type == \"NC\":\n            pseudo = NcAbinitPseudo(path, header)\n        elif psp_type == \"PAW\":\n            pseudo = PawAbinitPseudo(path, header)\n        else:\n            raise NotImplementedError(\"psp_type not in [NC, PAW]\")\n\n        return pseudo",
        "sampled": "### Problem Description\n\nRead and parse a pseudopotential file. Main entry point for client code.\n\n### Input\n\n* `filename`: Path to pseudopotential file\n\n### Output\n\n* `pseudopotential object` or `None` if filename is not a valid pseudopotential file.\n\n### Pseudocode\n```markdown\n# Define a class for the pseudopotential object\nclass Pseudopotential:\n    def __init__(self, name, type, parameters):\n        self"
    },
    {
        "original": "def get_element_dos(self):\n        \"\"\"\n        Get element projected Dos.\n\n        Returns:\n            dict of {Element: Dos}\n        \"\"\"\n\n        el_dos = {}\n        for site, atom_dos in self.pdos.items():\n            el = site.specie\n            for pdos in atom_dos.values():\n                if el not in el_dos:\n                    el_dos[el] = pdos\n                else:\n                    el_dos[el] = add_densities(el_dos[el], pdos)\n        return {el: Dos(self.efermi, self.energies, densities)\n                for el, densities in el_dos.items()}",
        "sampled": "```python\nclass ProjectedDosCalculator:\n    def __init__(self, elements):\n        \"\"\"\n        Initialize the calculator with a list of elements.\n\n        Args:\n            elements (list): A list of element names.\n        \"\"\"\n        self.elements = elements\n\n    def get_element_dos(self):\n        \"\"\"\n        Get element projected Dos.\n\n        Returns:\n            dict of {Element: Dos}\n        \"\"\"\n        \n# Example usage\ncalculator = ProjectedDosCalculator([\"H\", \"He\", \"Li"
    },
    {
        "original": "def remove_headerReference(self, type_):\n        \"\"\"Return rId of w:headerReference child of *type_* after removing it.\"\"\"\n        headerReference = self.get_headerReference(type_)\n        rId = headerReference.rId\n        self.remove(headerReference)\n        return rId",
        "sampled": "### Problem Description\nRemoves a w:headerReference child from an XML element and returns its rId.\n#### Problem Statement\nRemove the w:headerReference child of type_ after replacing it with word/document.xml.\n\n#### Input \n- `type_`: Name of the parent node where we need to find and remove the w:headerReference\n\n#### Output \n- The rId value assigned to the removed w:headerReference child.\n\n\n### Solution:\n```python\nfrom xml.etree import Element"
    },
    {
        "original": "def update_page(self, page_id, title, description,\n                    icon_url, page_url, comment=None):\n        \"\"\"\n        \u7f16\u8f91\u9875\u9762\u4fe1\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/5/6626199ea8757c752046d8e46cf13251.html\n\n        :param page_id: \u6447\u5468\u8fb9\u9875\u9762\u552f\u4e00ID\n        :param title: \u5728\u6447\u4e00\u6447\u9875\u9762\u5c55\u793a\u7684\u4e3b\u6807\u9898\uff0c\u4e0d\u8d85\u8fc76\u4e2a\u5b57\n        :param description: \u5728\u6447\u4e00\u6447\u9875\u9762\u5c55\u793a\u7684\u526f\u6807\u9898\uff0c\u4e0d\u8d85\u8fc77\u4e2a\u5b57\n        :param icon_url: \u5728\u6447\u4e00\u6447\u9875\u9762\u5c55\u793a\u7684\u56fe\u7247\u3002\u56fe\u7247\u9700\u5148\u4e0a\u4f20\u81f3\u5fae\u4fe1\u4fa7\u670d\u52a1\u5668\uff0c\n                        \u7528\u201c\u7d20\u6750\u7ba1\u7406-\u4e0a\u4f20\u56fe\u7247\u7d20\u6750\u201d\u63a5\u53e3\u4e0a\u4f20\u56fe\u7247\uff0c\u8fd4\u56de\u7684\u56fe\u7247URL\u518d\u914d\u7f6e\u5728\u6b64\u5904\n        :param page_url: \u8df3\u8f6c\u94fe\u63a5\n        :param comment: \u53ef\u9009\uff0c\u9875\u9762\u7684\u5907\u6ce8\u4fe1\u606f\uff0c\u4e0d\u8d85\u8fc715\u4e2a\u5b57\n        :return: \u9875\u9762\u4fe1\u606f\n        \"\"\"\n        data = optionaldict()\n        data['page_id'] = page_id\n        data['title'] = title\n        data['description'] = description\n        data['icon_url'] = icon_url\n        data['page_url'] = page_url\n        data['comment'] = comment\n        res = self._post(\n            'shakearound/page/update',\n            data=data,\n            result_processor=lambda x: x['data']\n        )\n        return res",
        "sampled": "```python\nclass WxMp:\n    def update_page(self, page_id, title, description, icon_url, page_url, comment=None):\n        \"\"\"\n        \u7f16\u8f91\u9875\u9762\u4fe1\u606f\n        \n        :param page_id: \u6447\u5468\u8fb9\u9875\u9762\u552f\u4e00ID\n        :param title: \u5728\u6447\u4e00\u6447\u9875\u9762\u5c55\u793a\u7684\u4e3b\u6807\u9898\uff0c\u4e0d\u8d85\u8fc76\u4e2a\u5b57\n        :param description: \u5728\u6447\u4e00\u6447\u9875\u9762\u5c55\u793a\u7684\u526f\u6807\u9898\uff0c\u4e0d\u8d85\u8fc77\u4e2a\u5b57\n        :param icon_url: \u5728\u6447\u4e00"
    },
    {
        "original": "def tune_in_no_block(self):\n        \"\"\"\n        Executes the tune_in sequence but omits extra logging and the\n        management of the event bus assuming that these are handled outside\n        the tune_in sequence\n        \"\"\"\n        # Instantiate the local client\n        self.local = salt.client.get_local_client(\n                self.opts['_minion_conf_file'], io_loop=self.io_loop)\n\n        # add handler to subscriber\n        self.pub_channel.on_recv(self._process_cmd_socket)",
        "sampled": "```python\nclass MediaPlayer:\n    def __init__(self):\n        \"\"\"\n        Initialize the media player.\n        \n        Attributes:\n            state (str): The current state of the player. Defaults to \"off\".\n            volume (int): The current volume level. Defaults to 0.\n        \"\"\"\n        self.state = \"off\"\n        self.volume = 0\n\n    def power_on(self):\n        \"\"\"\n        Turn on the media player.\n        \n        Changes:\n            state: to \"on\"\n        \n"
    },
    {
        "original": "async def _reload_message(self):\n        \"\"\"\n        Re-fetches this message to reload the sender and chat entities,\n        along with their input versions.\n        \"\"\"\n        try:\n            chat = await self.get_input_chat() if self.is_channel else None\n            msg = await self._client.get_messages(chat, ids=self.id)\n        except ValueError:\n            return  # We may not have the input chat/get message failed\n        if not msg:\n            return  # The message may be deleted and it will be None\n\n        self._sender = msg._sender\n        self._input_sender = msg._input_sender\n        self._chat = msg._chat\n        self._input_chat = msg._input_chat\n        self._via_bot = msg._via_bot\n        self._via_input_bot = msg._via_input_bot\n        self._forward = msg._forward\n        self._action_entities = msg._action_entities",
        "sampled": "### Problem Description\n### \nNo problem description is provided. However, based on the given function name `_reload_message`, it seems like this function is part of a class that handles messages in a chat application or similar context.\n\n### Input and Output Specifications\n### \nNo input and output specifications are provided.\n\nHowever, based on the function name and docstring, we can infer that this function should:\n\n- Re-fetch the message to reload the sender and chat entities.\n- Update their input versions.\n\nHere"
    },
    {
        "original": "def plot_latent_scatter(self, labels=None,\n                        which_indices=None,\n                        legend=True,\n                        plot_limits=None,\n                        marker='<>^vsd',\n                        num_samples=1000,\n                        projection='2d',\n                        **kwargs):\n    \"\"\"\n    Plot a scatter plot of the latent space.\n\n    :param array-like labels: a label for each data point (row) of the inputs\n    :param (int, int) which_indices: which input dimensions to plot against each other\n    :param bool legend: whether to plot the legend on the figure\n    :param plot_limits: the plot limits for the plot\n    :type plot_limits: (xmin, xmax, ymin, ymax) or ((xmin, xmax), (ymin, ymax))\n    :param str marker: markers to use - cycle if more labels then markers are given\n    :param kwargs: the kwargs for the scatter plots\n    \"\"\"\n    canvas, projection, kwargs, sig_dims = _new_canvas(self, projection, kwargs, which_indices)\n\n    X, _, _ = get_x_y_var(self)\n    if labels is None:\n        labels = np.ones(self.num_data)\n        legend = False\n    else:\n        legend = find_best_layout_for_subplots(len(np.unique(labels)))[1]\n    scatters = _plot_latent_scatter(canvas, X, sig_dims, labels, marker, num_samples, projection=projection, **kwargs)\n    return pl().add_to_canvas(canvas, dict(scatter=scatters), legend=legend)",
        "sampled": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass LatentSpacePlotter:\n    def plot_latent_scatter(self, labels=None,\n                            which_indices=None,\n                            legend=True,\n                            plot_limits=None,\n                            marker='<>^vsd',\n                            num_samples=1000,\n                            projection='2d',\n                            **kwargs):\n        \"\"\"\n        Plot a scatter plot of the latent space.\n\n        :param array-like labels: a label for each data point (row)"
    },
    {
        "original": "def read_tf_records(batch_size, tf_records, num_repeats=1,\n                    shuffle_records=True, shuffle_examples=True,\n                    shuffle_buffer_size=None, interleave=True,\n                    filter_amount=1.0):\n    \"\"\"\n    Args:\n        batch_size: batch size to return\n        tf_records: a list of tf_record filenames\n        num_repeats: how many times the data should be read (default: One)\n        shuffle_records: whether to shuffle the order of files read\n        shuffle_examples: whether to shuffle the tf.Examples\n        shuffle_buffer_size: how big of a buffer to fill before shuffling.\n        interleave: iwhether to interleave examples from multiple tf_records\n        filter_amount: what fraction of records to keep\n    Returns:\n        a tf dataset of batched tensors\n    \"\"\"\n    if shuffle_examples and not shuffle_buffer_size:\n        raise ValueError(\"Must set shuffle buffer size if shuffling examples\")\n\n    tf_records = list(tf_records)\n    if shuffle_records:\n        random.shuffle(tf_records)\n    record_list = tf.data.Dataset.from_tensor_slices(tf_records)\n\n    # compression_type here must agree with write_tf_examples\n    map_func = functools.partial(\n        tf.data.TFRecordDataset,\n        buffer_size=8 * 1024 * 1024,\n        compression_type='ZLIB')\n\n    if interleave:\n        # cycle_length = how many tfrecord files are read in parallel\n        # The idea is to shuffle both the order of the files being read,\n        # and the examples being read from the files.\n        dataset = record_list.apply(tf.contrib.data.parallel_interleave(\n            map_func, cycle_length=64, sloppy=True))\n    else:\n        dataset = record_list.flat_map(map_func)\n\n    if filter_amount < 1.0:\n        dataset = dataset.filter(\n            lambda _: tf.random_uniform([]) < filter_amount)\n\n    dataset = dataset.repeat(num_repeats)\n    if shuffle_examples:\n        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n\n    dataset = dataset.batch(batch_size)\n    return dataset",
        "sampled": "### Problem Description\n\nReads a specified list of tf_record files and returns a TensorFlow dataset of batched tensors.\n\n### Input Specifications\n\n* `batch_size`: The size of each batch to return.\n* `tf_records`: A list of filenames for the tf_record files to read.\n* `num_repeats`: The number of times the data should be read (default: 1).\n* `shuffle_records`: Whether to shuffle the order of files read (default: True).\n* `shuffle_examples"
    },
    {
        "original": "def get_component_related_issues(self, component_id):\n        \"\"\"\n        Returns counts of issues related to this component.\n        :param component_id:\n        :return:\n        \"\"\"\n        url = 'rest/api/2/component/{component_id}/relatedIssueCounts'.format(component_id=component_id)\n        return self.get(url)",
        "sampled": "```python\nclass IssueTracker:\n    def __init__(self):\n        # Initialize an empty dictionary to store component-issues relationships\n        self.component_issues = {}\n\n    def add_issue(self, component_id, issue_id):\n        \"\"\"\n        Adds an issue to the list of issues related to a component.\n        \n        :param component_id: ID of the component\n        :param issue_id: ID of the issue\n        \"\"\"\n        if component_id not in self.component_issues:\n            # If the"
    },
    {
        "original": "def build(values):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tvalues: [term, ...]\n\n\t\tReturns\n\t\t-------\n\t\tIndexStore\n\t\t\"\"\"\n\t\tidxstore = IndexStore()\n\t\tidxstore._i2val = list(values)\n\t\tidxstore._val2i = {term:i for i,term in enumerate(values)}\n\t\tidxstore._next_i = len(values)\n\t\treturn idxstore",
        "sampled": "### Problem Description\nWe need to analyze the `build` function and understand what it's supposed to do. However, without additional context or problem description, we can only make an educated guess about its purpose.\n\n### Assumed Problem Description\nBased on the given information, here's an example of a problem description that could match the provided function signature:\n\n**Problem: Building Index Store**\n\nCreate a data structure called `IndexStore` that allows efficient storage and retrieval of terms from a list of values."
    },
    {
        "original": "def get_nb_strings(self):\n        \"\"\"\n        Return the total number of strings in all Analysis objects\n        \"\"\"\n        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb",
        "sampled": "### Problem Description\n### \n#### Get the total number of strings in all Analysis objects\n\nNo problem description is provided, but based on the function `get_nb_strings(self)` and its docstring, it seems that this function is used to get the total number of strings in a certain object named `Analysis`. However, without more information about what an `Analysis` object is or how it stores its strings, we can only make an educated guess.\n\nAssuming that an `Analysis` object has a"
    },
    {
        "original": "def dbg_repr_run(self, run_addr):\n        \"\"\"\n        Debugging output of a single SimRun slice.\n\n        :param run_addr:    Address of the SimRun.\n        :return:            A string representation.\n        \"\"\"\n\n        if self.project.is_hooked(run_addr):\n            ss = \"%#x Hooked\\n\" % run_addr\n\n        else:\n            ss = \"%#x\\n\" % run_addr\n\n            # statements\n            chosen_statements = self.chosen_statements[run_addr]\n\n            vex_block = self.project.factory.block(run_addr).vex\n\n            statements = vex_block.statements\n            for i in range(0, len(statements)):\n                if i in chosen_statements:\n                    line = \"+\"\n                else:\n                    line = \"-\"\n                line += \"[% 3d] \" % i\n                line += str(statements[i])\n                ss += line + \"\\n\"\n\n            # exits\n            targets = self.chosen_exits[run_addr]\n            addr_strs = [ ]\n            for exit_stmt_id, target_addr in targets:\n                if target_addr is None:\n                    addr_strs.append(\"default\")\n                else:\n                    addr_strs.append(\"%#x\" % target_addr)\n\n            ss += \"Chosen exits: \" + \", \".join(addr_strs)\n\n        return ss",
        "sampled": "### Problem Description\n### \nGiven the `SimRun` class, write a method `dbg_repr_run` that returns a string representation of a single slice from the simulation.\n\n### Input and Ouput Specifications\n\n#### Input:\n- `run_addr` (int): Address of the SimRun.\n\n#### Output:\n- A string representation of the SimRun at the specified address.\n\n#### Example:\n\n```python\nclass SimRunSlice:\n    def __init__(self, run_addr, data):\n       "
    },
    {
        "original": "def _get_hostname(self, hostname, metric):\n        \"\"\"\n        If hostname is None, look at label_to_hostname setting\n        \"\"\"\n        if hostname is None and self.label_to_hostname is not None:\n            for label in metric.label:\n                if label.name == self.label_to_hostname:\n                    return label.value + self.label_to_hostname_suffix\n\n        return hostname",
        "sampled": "```python\nclass HostnameResolver:\n    def __init__(self, label_to_hostname):\n        self.label_to_hostname = label_to_hostname\n\n    def _get_hostname(self, hostname, metric):\n        \"\"\"\n        If hostname is None, look at label_to_hostname setting\n        \"\"\"\n        \n        # If hostname is not None, return it as the result\n        if hostname is not None:\n            return hostname\n        \n        # If metric is a string and starts with 'host:', extract the host name"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'step') and self.step is not None:\n            _dict['step'] = self.step\n        if hasattr(self, 'snapshot') and self.snapshot is not None:\n            _dict['snapshot'] = self.snapshot\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n\n    def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        return {\n            'n': self.n,\n            'k': self.k\n        }\n\ndef main():\n    solution = Solution()\n    solution.n = int(input(\"Enter the value of n: \"))\n    solution.k = int(input(\"Enter the value of k: \"))\n    \n"
    },
    {
        "original": "def write_pdb(self, mol, filename, name=None, num=None):\n        \"\"\"\n        dump the molecule into pdb file with custom residue name and number.\n        \"\"\"\n\n        # ugly hack to get around the openbabel issues with inconsistent\n        # residue labelling.\n        scratch = tempfile.gettempdir()\n        with ScratchDir(scratch, copy_to_current_on_exit=False) as _:\n            mol.to(fmt=\"pdb\", filename=\"tmp.pdb\")\n            bma = BabelMolAdaptor.from_file(\"tmp.pdb\", \"pdb\")\n\n        num = num or 1\n        name = name or \"ml{}\".format(num)\n\n        # bma = BabelMolAdaptor(mol)\n        pbm = pb.Molecule(bma._obmol)\n        for i, x in enumerate(pbm.residues):\n            x.OBResidue.SetName(name)\n            x.OBResidue.SetNum(num)\n\n        pbm.write(format=\"pdb\", filename=filename, overwrite=True)",
        "sampled": "Problem Description\n================\n**Problem Name:** Writing Molecules to PDB Files\n\nInstructions\n-------------\nYou are given a peptide (a short chain of amino acids) and you need to write it out into a variety of different PDB formatted files, with some customization: \n\n*   different chainIDs and residue numbers \n*   RyR endpoint that involves ejection.\n\nInput Specifications\n-------------------\n### Input Format\n\nThe input is 4 value list which is typically `(residue name, aminoacid"
    },
    {
        "original": "def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns\n        None.\n\n        :return: None\n        \"\"\"\n        options = self.options\n        default_exclude_list = options.pop('default_exclude')\n        options_exclude_list = options.pop('exclude')\n        excludes = default_exclude_list + options_exclude_list\n        x_list = options.pop('x')\n\n        exclude_args = ['--exclude={}'.format(exclude) for exclude in excludes]\n        x_args = tuple(('-x', x) for x in x_list)\n        self._ansible_lint_command = sh.ansible_lint.bake(\n            options,\n            exclude_args,\n            sum(x_args, ()),\n            self._playbook,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)",
        "sampled": "# Problem Description\n## Baking Ansible Lint Command\n\nBake an ansible-lint command so it's ready to execute.\n\n## Input & Output Specifications\n\n* No input is required from the user.\n* The function should return `None`.\n \n\n```python\nclass AnsibleLintKing:\n    def bake(self):\n        \"\"\"\n        Bake an `ansible-lint` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n        \n        # The function name"
    },
    {
        "original": "def ReadClientLastPings(self,\n                          min_last_ping=None,\n                          max_last_ping=None,\n                          fleetspeak_enabled=None):\n    \"\"\"Reads last-ping timestamps for clients in the DB.\"\"\"\n    last_pings = {}\n    for client_id, metadata in iteritems(self.metadatas):\n      last_ping = metadata.get(\"ping\", rdfvalue.RDFDatetime(0))\n      is_fleetspeak_client = metadata.get(\"fleetspeak_enabled\", False)\n      if min_last_ping is not None and last_ping < min_last_ping:\n        continue\n      elif max_last_ping is not None and last_ping > max_last_ping:\n        continue\n      elif (fleetspeak_enabled is not None and\n            is_fleetspeak_client != fleetspeak_enabled):\n        continue\n      else:\n        last_pings[client_id] = metadata.get(\"ping\", None)\n    return last_pings",
        "sampled": "### Problem Description\n\nRead the last-ping timestamps for clients in the DB, filtering by fleetspeak-enabled. The function `ReadClientLastPings` should return a list of tuples containing client IDs and their corresponding last-ping timestamps.\n\n### Input Specifications\n\n* `min_last_ping`: Optional, minimum allowed last-ping timestamp (inclusive)\n* `max_last_ping`: Optional, maximum allowed last-ping timestamp (inclusive)\n* `fleetspeak_enabled`: Optional, filter by whether fleets"
    },
    {
        "original": "def rackconnect(vm_):\n    \"\"\"\n    Determine if we should wait for rackconnect automation before running.\n    Either 'False' (default) or 'True'.\n    \"\"\"\n    return config.get_cloud_config_value(\n        'rackconnect', vm_, __opts__, default=False,\n        search_global=False\n    )",
        "sampled": "### Problem Description\nNo problem description is provided in the prompt, only a function with no implementation or specification. However, I will provide a solution to this hypothetical function.\n\n### Input and Output Specifications\nNo input or output specifications are provided for the `rackconnect` function.\n\n### Python Solution\n\n```python\ndef rackconnect(vm_):\n    \"\"\"\n    Determine if we should wait for rackconnect automation before running.\n    \n    Parameters:\n    vm_ (str): The name of the virtual machine."
    },
    {
        "original": "def parse(html):\n    \"\"\"\n    Parses the given HTML message and returns its stripped representation\n    plus a list of the MessageEntity's that were found.\n\n    :param message: the message with HTML to be parsed.\n    :return: a tuple consisting of (clean message, [message entities]).\n    \"\"\"\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(_add_surrogate(html))\n    text = helpers.strip_text(parser.text, parser.entities)\n    return _del_surrogate(text), parser.entities",
        "sampled": "```python\nimport re\nfrom html import unescape\n\ndef parse(html):\n    \"\"\"\n    Parses the given HTML message and returns its stripped representation\n    plus a list of the MessageEntity's that were found.\n\n    :param html: the message with HTML to be parsed.\n    :return: a tuple consisting of (clean message, [message entities]).\n    \"\"\"\n\n    # Unescape HTML entities\n    html = unescape(html)\n\n    # Remove script and style elements\n    html = re"
    },
    {
        "original": "def ListNetworkConnectionsFromClient(args):\n  \"\"\"Gather open network connection stats.\n\n  Args:\n    args: An `rdf_client_action.ListNetworkConnectionArgs` instance.\n\n  Yields:\n    `rdf_client_network.NetworkConnection` instances.\n  \"\"\"\n  for proc in psutil.process_iter():\n    try:\n      connections = proc.connections()\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n      continue\n\n    for conn in connections:\n      if args.listening_only and conn.status != \"LISTEN\":\n        continue\n\n      res = rdf_client_network.NetworkConnection()\n      res.pid = proc.pid\n      res.process_name = proc.name()\n      res.family = conn.family\n      res.type = conn.type\n      try:\n        if conn.status:\n          res.state = conn.status\n      except ValueError:\n        logging.warning(\"Encountered unknown connection status (%s).\",\n                        conn.status)\n\n      res.local_address.ip, res.local_address.port = conn.laddr\n      if conn.raddr:\n        res.remote_address.ip, res.remote_address.port = conn.raddr\n\n      yield res",
        "sampled": "### Problem Description\n### ======\n### Problem: List Network Connections from Client\n\nList all the client subscriptions and their corresponding network connections.\n\n### Input and Output Specifications\n### ======\n### Input:\n#### - `args`: An instance of `rdf_client_action.ListNetworkConnectionArgs` (Exactly one command-line argument is expected, with the format `<identifier>` where `<identifier>` is a unique identifier for each execution client subscriptions.)\n\n### Output:\n#### - Returns a list of `rdf_client_network.NetworkConnection"
    },
    {
        "original": "def patch_webbrowser():\n    \"\"\"\n    Some custom patches on top of the python webbrowser module to fix\n    user reported bugs and limitations of the module.\n    \"\"\"\n\n    # https://bugs.python.org/issue31014\n    # https://github.com/michael-lazar/rtv/issues/588\n    def register_patch(name, klass, instance=None, update_tryorder=None, preferred=False):\n        ",
        "sampled": "### Problem Description\nNo problem description is given. However, based on the function signature, it appears to be related to custom patches for the Python webbrowser module.\n\n### Input and Output Specifications\nNo input and output specifications are provided.\n\n### Solution\n\n```markdown\ndef patch_webbrowser():\n    \"\"\"\n    Some custom patches on top of the python webbrowser module to fix user reported bugs and limitations of the module.\n    \n    Patches include:\n    - Also call open_new_tab() when using open"
    },
    {
        "original": "def zremrangebyrank(self, name, min, max):\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with ranks between\n        ``min`` and ``max``. Values are 0-based, ordered from smallest score\n        to largest. Values can be negative indicating the highest scores.\n        Returns the number of elements removed\n        \"\"\"\n        return self.execute_command('ZREMRANGEBYRANK', name, min, max)",
        "sampled": "### Problem Description\nRemove all elements in the sorted set ``name`` with ranks between ``min`` and ``max``.\n\n### Input Specifications\n\n*   `name`: The name of the sorted set.\n*   `min`: The minimum rank (inclusive).\n*   `max`: The maximum rank (inclusive).\n\n### Output Specifications\n\n*   Returns the number of elements removed.\n\n### Python Solution\n```python\nclass SortedSet:\n    def __init__(self):\n        self.set = {}\n\n   "
    },
    {
        "original": "def get_structure_with_spin(self):\n        \"\"\"\n        Returns a Structure with species decorated with spin values instead\n        of using magmom site properties.\n        :return: Structure\n        \"\"\"\n\n        structure = self.structure.copy()\n        structure.add_spin_by_site(structure.site_properties[\"magmom\"])\n        structure.remove_site_property(\"magmom\")\n\n        return structure",
        "sampled": "```python\nfrom ase import Atoms\nfrom ase.dft.kpoints import MonkhorstPackGrid\n\ndef get_structure_with_spin(self):\n    \"\"\"\n    Returns a Structure with species decorated with spin values instead\n    of using magmom site properties.\n    :return: Structure\n    \"\"\"\n\n    # Get the current structure and its atoms\n    structure = self.structure.copy()\n    atoms = structure.atoms.copy()\n\n    # Initialize an empty list to store the new atoms with spin values"
    },
    {
        "original": "def accept_quality(accept, default=1):\n    \"\"\"Separates out the quality score from the accepted content_type\"\"\"\n    quality = default\n    if accept and \";\" in accept:\n        accept, rest = accept.split(\";\", 1)\n        accept_quality = RE_ACCEPT_QUALITY.search(rest)\n        if accept_quality:\n            quality = float(accept_quality.groupdict().get('quality', quality).strip())\n\n    return (quality, accept.strip())",
        "sampled": "```\ndef accept_quality(accept, default=1):\n    \"\"\"\n    Separates out the quality score from the accepted content_type\n    \"\"\"\n    # Split the input string into a list of strings separated by '/'\n    parts = accept.split('/')\n    \n    # Check if there are at least two parts (content_type and quality)\n    if len(parts) >= 2:\n        # Get the second part which is assumed to be the quality\n        quality = int(parts[1])\n        \n        return"
    },
    {
        "original": "async def get_payment_info(self):\n        \"\"\"\n        Retrieve Payment Transaction Information for this Credential. Typically this will include\n        how much payment is requried by the issuer, which needs to be provided by the prover, before the issuer will\n        issue the credential to the prover. Ideally a prover would want to know how much payment is being asked before\n        submitting the credential request (which triggers the payment to be made).\n        Example:\n        info = credential.get_payment_info()\n        :return:\n        \"\"\"\n        if not hasattr(Credential.get_payment_info, \"cb\"):\n            self.logger.debug(\"vcx_credential_get_payment_info: Creating callback\")\n            Credential.get_payment_info.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_char_p))\n\n        c_credential_handle = c_uint32(self.handle)\n        data = await do_call('vcx_credential_get_payment_info',\n                      c_credential_handle,\n                      Credential.get_payment_info.cb)\n        return json.loads(data.decode())",
        "sampled": "### Problem Description\nGiven a credential object, design a `get_payment_info` method that retrieves the payment transaction information including the amount required by the issuer.\n\n### Input and Output Specifications\n\n* Input: None (assuming the credential object is already created with necessary attributes)\n* Output: \n  * A data structure containing payment transaction information. For simplicity, let's assume it's a dictionary with 'issuer_payment' as key.\n\n### Python Solution\n```python\nclass Credential:\n    def __init__("
    },
    {
        "original": "def ApprovalRevokeRaw(aff4_path, token):\n  \"\"\"Revokes an approval for a given token.\n\n  This method requires raw datastore access to manipulate approvals directly.\n\n  Args:\n    aff4_path: The aff4_path or client id the approval should be created for.\n    token: The token that should be revoked.\n  \"\"\"\n  try:\n    urn = rdf_client.ClientURN(aff4_path)\n  except type_info.TypeValueError:\n    urn = rdfvalue.RDFURN(aff4_path)\n\n  approval_urn = aff4.ROOT_URN.Add(\"ACL\").Add(urn.Path()).Add(\n      token.username).Add(utils.EncodeReasonString(token.reason))\n\n  super_token = access_control.ACLToken(username=\"raw-approval-superuser\")\n  super_token.supervisor = True\n\n  approval_request = aff4.FACTORY.Open(\n      approval_urn, mode=\"rw\", token=super_token)\n  approval_request.DeleteAttribute(approval_request.Schema.APPROVER)\n  approval_request.Close()",
        "sampled": "```python\nimport\ufffdi\nfrom jose import jwt\n\ndef ApprovalRevokeRaw(aff4_path, token):\n    \"\"\"Revokes an approval for a given token.\n\n    This method requires raw datastore access to manipulate approvals directly.\n\n    Args:\n        aff4_path: The aff4_path or client id the approval should be created for.\n        token: The token that should be revoked.\n    \"\"\"\n\n    # First, check if the token is valid by verifying its signature\n    try:\n        decoded"
    },
    {
        "original": "def _setup_output_metrics(self, engine):\n        \"\"\"Helper method to setup metrics to log\n        \"\"\"\n        metrics = {}\n        if self.metric_names is not None:\n            for name in self.metric_names:\n                if name not in engine.state.metrics:\n                    warnings.warn(\"Provided metric name '{}' is missing \"\n                                  \"in engine's state metrics: {}\".format(name, list(engine.state.metrics.keys())))\n                    continue\n                metrics[name] = engine.state.metrics[name]\n\n        if self.output_transform is not None:\n            output_dict = self.output_transform(engine.state.output)\n\n            if not isinstance(output_dict, dict):\n                output_dict = {\"output\": output_dict}\n\n            metrics.update({name: value for name, value in output_dict.items()})\n        return metrics",
        "sampled": "```python\nclass Solution:\n    def _setup_output_metrics(self, engine):\n        # Initialize a dictionary to store metrics\n        metrics = {}\n        \n        # Define the output metric for accuracy\n        metrics['accuracy'] = {\n            'name': 'Accuracy',\n            'description': 'The proportion of correct predictions out of all predictions made.',\n            'formula': r'$\\frac{TP + TN}{TP + TN + FP + FN}$',\n            'type': float,\n            'is"
    },
    {
        "original": "def display_timestamps_pair(time_m_2):\n    \"\"\"Takes a list of the following form: [(a1, b1), (a2, b2), ...] and\n    returns a string (a_mean+/-a_error, b_mean+/-b_error).\n    \"\"\"\n    if len(time_m_2) == 0:\n        return '(empty)'\n\n    time_m_2 = np.array(time_m_2)\n    return '({}, {})'.format(\n        display_timestamps(time_m_2[:, 0]),\n        display_timestamps(time_m_2[:, 1]),\n    )",
        "sampled": "```python\nimport numpy as np\n\ndef display_timestamps_pair(time_m_2):\n    \"\"\"\n    Takes a list of the following form: [(a1, b1), (a2, b2), ...] and\n    returns a string (a_mean+/-a_error, b_mean+/-b_error).\n    \n    Args:\n        time_m_2 (list): List of tuples containing time difference values.\n\n    Returns:\n        str: Formatted string with mean and error values"
    },
    {
        "original": "def _finalize_axis(self, key, **kwargs):\n        \"\"\"\n        Extends the ElementPlot _finalize_axis method to set appropriate\n        labels, and axes options for 3D Plots.\n        \"\"\"\n        axis = self.handles['axis']\n        self.handles['fig'].set_frameon(False)\n        axis.grid(self.show_grid)\n        axis.view_init(elev=self.elevation, azim=self.azimuth)\n        axis.dist = self.distance\n\n        if self.xaxis is None:\n            axis.w_xaxis.line.set_lw(0.)\n            axis.w_xaxis.label.set_text('')\n        if self.yaxis is None:\n            axis.w_yaxis.line.set_lw(0.)\n            axis.w_yaxis.label.set_text('')\n        if self.zaxis is None:\n            axis.w_zaxis.line.set_lw(0.)\n            axis.w_zaxis.label.set_text('')\n        if self.disable_axes:\n            axis.set_axis_off()\n\n        if mpl_version <= '1.5.9':\n            axis.set_axis_bgcolor(self.bgcolor)\n        else:\n            axis.set_facecolor(self.bgcolor)\n        return super(Plot3D, self)._finalize_axis(key, **kwargs)",
        "sampled": "```python\nclass ElementPlot:\n    def _finalize_axis(self, key, **kwargs):\n        \"\"\"\n        Extends the ElementPlot _finalize_axis method to set appropriate\n        labels, and axes options for 3D Plots.\n        \n        Parameters:\n        key (str): The axis to finalize (e.g., 'x', 'y', or 'z')\n        \n        Returns:\n        None\n        \"\"\"\n        \n        # Check if the plot is a 3D plot\n        if"
    },
    {
        "original": "def template_list(call=None):\n    \"\"\"\n    Return available Xen template information.\n\n    This returns the details of\n    each template to show number cores, memory sizes, etc..\n\n    .. code-block:: bash\n\n       salt-cloud -f template_list myxen\n\n    \"\"\"\n    templates = {}\n    session = _get_session()\n    vms = session.xenapi.VM.get_all()\n    for vm in vms:\n        record = session.xenapi.VM.get_record(vm)\n        if record['is_a_template']:\n            templates[record['name_label']] = record\n    return templates",
        "sampled": "### Problem Analysis\n\nThe given problem description mentions a function called `template_list` which appears to be part of a larger system, possibly a cloud management platform. The goal of this function is to return available Xen template information, including details such as the number of cores and memory sizes.\n\nHowever, the provided docstring also includes an example usage in Bash:\n\n```bash\nsalt-cloud -f template_list myxen\n```\n\nThis suggests that the `template_list` function might be designed to work"
    },
    {
        "original": "def to_arrayref(u):\n    \"\"\"\n    To the parser, funcall is indistinguishable\n    from rhs array reference.  But LHS references\n    can be converted to arrayref nodes.\n    \"\"\"\n    if u.__class__ is node.funcall:\n        try:\n            if u.func_expr.props in \"UR\": # upd,ref\n                u.__class__ = node.arrayref\n        except:\n            pass",
        "sampled": "```python\ndef to_arrayref(u):\n    \"\"\"\n    To the parser, funcall is indistinguishable\n    from rhs array reference.  But LHS references\n    can be converted to arrayref nodes.\n    \"\"\" \n    return u\n\ndef solve_problem():\n    \n    # Problem Description: \n    # Given an integer n, return a list of all possible results from summing each number in ranges of size n.\n    \n    def sum_of_elements(nums, start, end):\n       "
    },
    {
        "original": "def receipts():\n    \"\"\"\n    Return the results of a call to\n    ``system_profiler -xml -detail full SPInstallHistoryDataType``\n    as a dictionary.  Top-level keys of the dictionary\n    are the names of each set of install receipts, since\n    there can be multiple receipts with the same name.\n    Contents of each key are a list of dictionaries.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' systemprofiler.receipts\n    \"\"\"\n\n    apps = _call_system_profiler('SPInstallHistoryDataType')\n\n    appdict = {}\n\n    for a in apps:\n        details = dict(a)\n        details.pop('_name')\n        if 'install_date' in details:\n            details['install_date'] = details['install_date'].strftime('%Y-%m-%d %H:%M:%S')\n        if 'info' in details:\n            try:\n                details['info'] = '{0}: {1}'.format(details['info'][0],\n                                                    details['info'][1].strftime('%Y-%m-%d %H:%M:%S'))\n            except (IndexError, AttributeError):\n                pass\n\n        if a['_name'] not in appdict:\n            appdict[a['_name']] = []\n\n        appdict[a['_name']].append(details)\n\n    return appdict",
        "sampled": "```python\nimport subprocess\nimport xml.etree.ElementTree as ET\n\ndef receipts():\n    \"\"\"\n    Return the results of a call to\n    ``system_profiler -xml -detail full SPInstallHistoryDataType``\n    as a dictionary.  Top-level keys of the dictionary\n    are the names of each set of install receipts, since\n    there can be multiple receipts with the same name.\n    Contents of each key are a list of dictionaries.\n\n    CLI Example:\n\n    .. code-block"
    },
    {
        "original": "def prune_volumes(self, filters=None):\n        \"\"\"\n        Delete unused volumes\n\n        Args:\n            filters (dict): Filters to process on the prune list.\n\n        Returns:\n            (dict): A dict containing a list of deleted volume names and\n                the amount of disk space reclaimed in bytes.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        params = {}\n        if filters:\n            params['filters'] = utils.convert_filters(filters)\n        url = self._url('/volumes/prune')\n        return self._result(self._post(url, params=params), True)",
        "sampled": "### Problem Description\n\nPrune Unused Volumes\n=====================\n\nDelete unused volumes to reclaim disk space.\n\n### Prerequisites\n\nTo use this method, your Docker daemon must be version 1.12 or later.\n\n### Input and Output Specifications\n\n#### Input:\n\n*   `filters`: A dict containing the filters to apply on the prune list (optional)\n\n#### Output:\n\n*   A dict with two keys:\n    *   `deleted`: The number of deleted volume names\n    *   `error"
    },
    {
        "original": "def show_instance(name, call=None):\n    \"\"\"\n    Show the details from the provider concerning an instance\n    \"\"\"\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    nodes = list_nodes_full()\n    # Find under which cloud service the name is listed, if any\n    if name not in nodes:\n        return {}\n    if 'name' not in nodes[name]:\n        nodes[name]['name'] = nodes[name]['id']\n    try:\n        __utils__['cloud.cache_node'](nodes[name], __active_provider_name__, __opts__)\n    except TypeError:\n        log.warning('Unable to show cache node data; this may be because the node has been deleted')\n    return nodes[name]",
        "sampled": "```python\nimport datetime\n\nclass VirtualMachine:\n    def __init__(self, name, provider, image_id=None):\n        \"\"\"\n        Initialize a Virtual Machine instance\n\n        :param name: the name of the virtual machine\n        :param provider: the cloud provider (str)\n        :param image_id: the ID of the image used to create this VM (str)\n        \"\"\"\n        self.name = name\n        self.provider = provider\n        self.image_id = image_id\n        self"
    },
    {
        "original": "def widget_from_iterable(o):\n        \"\"\"Make widgets from an iterable. This should not be done for\n        a string or tuple.\"\"\"\n        # Dropdown expects a dict or list, so we convert an arbitrary\n        # iterable to either of those.\n        if isinstance(o, (list, dict)):\n            return Dropdown(options=o)\n        elif isinstance(o, Mapping):\n            return Dropdown(options=list(o.items()))\n        else:\n            return Dropdown(options=list(o))",
        "sampled": "```python\nfrom tkinter import *\n\ndef widget_from_iterable(o):\n    \"\"\"Make widgets from an iterable. This should not be done for a string or tuple.\"\"\"\n    \n    if isinstance(o, str) or isinstance(o, tuple):\n        return None\n    \n    if hasattr(o, '__iter__'):\n        widgets = []\n        for item in o:\n            widget = widget_from_iterable(item)\n            if widget is not None:\n                widgets.append(widget)\n        \n        return widgets\n    else:\n       "
    },
    {
        "original": "def _ConvertValueForCsv(self, pql_value):\n    \"\"\"Sanitizes a field value from a Value object to a CSV suitable format.\n\n    Args:\n      pql_value: dict a dictionary containing the data for a single field of an\n                 entity.\n\n    Returns:\n      str a CSV writer friendly value formatted by Value.Type.\n    \"\"\"\n    if 'value' in pql_value:\n      field = pql_value['value']\n    elif 'values' in pql_value:\n      field = pql_value['values']\n    else:\n      field = None\n\n    if field:\n      if isinstance(field, list):\n        if all(AdManagerClassType(single_field) == AdManagerClassType(field[0])\n               for single_field in field):\n          return ','.join([\n              '\"%s\"' % str(self._ConvertValueForCsv(single_field))\n              for single_field in field])\n        else:\n          raise googleads.errors.GoogleAdsValueError(\n              'The set value returned contains unsupported mix value types')\n\n      class_type = AdManagerClassType(pql_value)\n\n      if class_type == 'TextValue':\n        s = field.replace('\"', '\"\"')\n\n        # Encode UTF-8 characters for Python 2 only.\n        if sys.version_info.major < 3:\n          s = s.encode('UTF8')\n        return s\n      elif class_type == 'NumberValue':\n        return float(field) if '.' in field else int(field)\n      elif class_type == 'DateTimeValue':\n        return self._ConvertDateTimeToOffset(field)\n      elif class_type == 'DateValue':\n        return datetime.date(int(field['date']['year']),\n                             int(field['date']['month']),\n                             int(field['date']['day'])).isoformat()\n      else:\n        return field\n    else:\n      return '-'",
        "sampled": "```python\nclass Value:\n    def __init__(self, type, value):\n        self.type = type\n        self.value = value\n\nclass CSVConverter:\n    def _ConvertValueForCsv(self, pql_value):\n        \"\"\"\n        Sanitizes a field value from a Value object to a CSV suitable format.\n\n        Args:\n          pql_value: dict a dictionary containing the data for a single field of an entity.\n\n        Returns:\n          str a CSV writer friendly value formatted by Value"
    },
    {
        "original": "def acl_delete(consul_url=None, token=None, **kwargs):\n    \"\"\"\n    Delete an ACL token.\n\n    :param consul_url: The Consul server URL.\n    :param id: Unique identifier for the ACL to update.\n    :return: Boolean & message of success or failure.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' consul.acl_delete id='c1c4d223-91cb-3d1f-1ee8-f2af9e7b6716'\n\n    \"\"\"\n    ret = {}\n    data = {}\n    if not consul_url:\n        consul_url = _get_config()\n        if not consul_url:\n            log.error('No Consul URL found.')\n            ret['message'] = 'No Consul URL found.'\n            ret['res'] = False\n            return ret\n\n    if 'id' not in kwargs:\n        ret['message'] = 'Required parameter \"id\" is missing.'\n        ret['res'] = False\n        return ret\n\n    function = 'acl/delete/{0}'.format(kwargs['id'])\n    res = _query(consul_url=consul_url,\n                 token=token,\n                 data=data,\n                 method='PUT',\n                 function=function)\n\n    if res['res']:\n        ret['res'] = True\n        ret['message'] = 'ACL {0} deleted.'.format(kwargs['id'])\n    else:\n        ret['res'] = False\n        ret['message'] = ('Removing ACL '\n                          '{0} failed.'.format(kwargs['id']))\n\n    return ret",
        "sampled": "### Problem Description\n\nDelete an ACL (Access Control List) token.\n\n### Input and Output Specifications\n\n* **Input**:\n\t+ `consul_url`: The Consul server URL.\n\t+ `token`: The ID of the ACL token to delete.\n* **Output**:\n\t+ Boolean value indicating success or failure.\n\t+ Message describing the outcome.\n\n### Solution\n\n```python\nimport requests\nimport json\n\ndef acl_delete(consul_url=None, token=None, **kwargs):\n"
    },
    {
        "original": "def _is_epsilon_nash(x, g, epsilon, indptr=None):\n    \"\"\"\n    Determine whether `x` is an `epsilon`-Nash equilibrium of `g`.\n\n    Parameters\n    ----------\n    x : array_like(float, ndim=1)\n        Array of flattened mixed action profile of length equal to n_0 +\n        ... + n_N-1, where `out[indptr[i]:indptr[i+1]]` contains player\n        i's mixed action.\n\n    g : NormalFormGame\n\n    epsilon : scalar(float)\n\n    indptr : array_like(int, ndim=1), optional(default=None)\n        Array of index pointers of length N+1, where `indptr[0] = 0` and\n        `indptr[i+1] = indptr[i] + n_i`. Created internally if None.\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    if indptr is None:\n        indptr = np.empty(g.N+1, dtype=int)\n        indptr[0] = 0\n        indptr[1:] = np.cumsum(g.nums_actions)\n\n    action_profile = _get_action_profile(x, indptr)\n    return g.is_nash(action_profile, tol=epsilon)",
        "sampled": "```python\nimport numpy as np\n\ndef _is_epsilon_nash(x, g, epsilon, indptr=None):\n    \"\"\"\n    Determine whether `x` is an `epsilon`-Nash equilibrium of `g`.\n\n    Parameters\n    ----------\n    x : array_like(float, ndim=1)\n        Array of flattened mixed action profile of length equal to n_0 +\n        ... + n_N-1, where `out[indptr[i]:indptr[i+1]]` contains"
    },
    {
        "original": "def _output_ret(self, ret, out, retcode=0):\n        \"\"\"\n        Print the output from a single return to the terminal\n        \"\"\"\n        import salt.output\n        # Handle special case commands\n        if self.config['fun'] == 'sys.doc' and not isinstance(ret, Exception):\n            self._print_docs(ret)\n        else:\n            # Determine the proper output method and run it\n            salt.output.display_output(ret,\n                                       out=out,\n                                       opts=self.config,\n                                       _retcode=retcode)\n        if not ret:\n            sys.stderr.write('ERROR: No return received\\n')\n            sys.exit(2)",
        "sampled": "I'm ready to help you solve the problem. However, I don't see the problem description in Markdown format. Please provide the problem description, input and output specifications so I can understand and analyze the problem.\n\nOnce I have a clear understanding of the problem, I'll provide a Python solution for you."
    },
    {
        "original": "def canBeCollapsed(master, br1, br2):\n        \"\"\"\n        Returns true if both buildrequest can be merged, via Deferred.\n\n        This implements Buildbot's default collapse strategy.\n        \"\"\"\n        # short-circuit: if these are for the same buildset, collapse away\n        if br1['buildsetid'] == br2['buildsetid']:\n            return True\n\n        # get the buidlsets for each buildrequest\n        selfBuildsets = yield master.data.get(\n            ('buildsets', str(br1['buildsetid'])))\n        otherBuildsets = yield master.data.get(\n            ('buildsets', str(br2['buildsetid'])))\n\n        # extract sourcestamps, as dictionaries by codebase\n        selfSources = dict((ss['codebase'], ss)\n                           for ss in selfBuildsets['sourcestamps'])\n        otherSources = dict((ss['codebase'], ss)\n                            for ss in otherBuildsets['sourcestamps'])\n\n        # if the sets of codebases do not match, we can't collapse\n        if set(selfSources) != set(otherSources):\n            return False\n\n        for c, selfSS in selfSources.items():\n            otherSS = otherSources[c]\n            if selfSS['repository'] != otherSS['repository']:\n                return False\n\n            if selfSS['branch'] != otherSS['branch']:\n                return False\n\n            if selfSS['project'] != otherSS['project']:\n                return False\n\n            # anything with a patch won't be collapsed\n            if selfSS['patch'] or otherSS['patch']:\n                return False\n            # get changes & compare\n            selfChanges = yield master.data.get(('sourcestamps', selfSS['ssid'], 'changes'))\n            otherChanges = yield master.data.get(('sourcestamps', otherSS['ssid'], 'changes'))\n            # if both have changes, proceed, else fail - if no changes check revision instead\n            if selfChanges and otherChanges:\n                continue\n            elif selfChanges and not otherChanges:\n                return False\n\n            elif not selfChanges and otherChanges:\n                return False\n\n            # else check revisions\n            elif selfSS['revision'] != otherSS['revision']:\n                return False\n\n        return True",
        "sampled": "```python\ndef canBeCollapsed(master, br1, br2):\n    \"\"\"\n    Returns true if both buildrequest can be merged, via Deferred.\n\n    This implements Buildbot's default collapse strategy.\n    \"\"\"\n    \n    # Check if both buildrequests are from the same master\n    # If not, they cannot be collapsed\n    if master != br1.master and master != br2.master:\n        return False\n    \n    # Check if both buildrequests have the same builder name\n    #"
    },
    {
        "original": "async def receive_bilateral_response(self):\n        \"\"\"Receive the response to a request made to the Watchman service.\"\"\"\n\n        self._check_receive_loop()\n        resp = await self.bilateral_response_queue.get()\n        self._check_error(resp)\n        return resp",
        "sampled": "```python\nimport asyncio\n\nclass WatchmanService:\n    async def receive_bilateral_response(self):\n        \"\"\"Receive the response to a request made to the Watchman service.\"\"\"\n        \n        # Simulate receiving a response from the Watchman service\n        # This could be replaced with actual network I/O or other operations\n        await asyncio.sleep(1)  # Wait for 1 second\n        \n        # Assume we're receiving a JSON response from the Watchman service\n        response = {\n            \""
    },
    {
        "original": "def _inverse_permutation_indices(positions):\n    \"\"\"Like inverse_permutation, but also handles slices.\n\n    Parameters\n    ----------\n    positions : list of np.ndarray or slice objects.\n        If slice objects, all are assumed to be slices.\n\n    Returns\n    -------\n    np.ndarray of indices or None, if no permutation is necessary.\n    \"\"\"\n    if not positions:\n        return None\n\n    if isinstance(positions[0], slice):\n        positions = _consolidate_slices(positions)\n        if positions == slice(None):\n            return None\n        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]\n\n    indices = nputils.inverse_permutation(np.concatenate(positions))\n    return indices",
        "sampled": "```python\nimport numpy as np\n\ndef _inverse_permutation_indices(positions):\n    \"\"\"\n    Like inverse_permutation, but also handles slices.\n\n    Parameters\n    ----------\n    positions : list of np.ndarray or slice objects.\n        If slice objects, all are assumed to be slices.\n\n    Returns\n    -------\n    np.ndarray of indices or None, if no permutation is necessary.\n    \"\"\"\n    \n    # First, check if positions contains any non-Numpy ndarray or non-slice object \n   "
    },
    {
        "original": "def AddChild(self, path_info):\n    \"\"\"Makes the path aware of some child.\"\"\"\n\n    if self._path_type != path_info.path_type:\n      message = \"Incompatible path types: `%s` and `%s`\"\n      raise ValueError(message % (self._path_type, path_info.path_type))\n    if self._components != path_info.components[:-1]:\n      message = \"Incompatible path components, expected `%s` but got `%s`\"\n      raise ValueError(message % (self._components, path_info.components[:-1]))\n\n    self._children.add(path_info.GetPathID())",
        "sampled": "```python\nclass Node:\n    def __init__(self, name):\n        self.name = name\n        self.children = []\n\n    def add_child(self, child):\n        \"\"\"Makes the node aware of some child.\"\"\"\n        self.children.append(child)\n\n\nclass FileSystem:\n    def __init__(self):\n        self.root = Node(\"/\")\n\n    def add_child(self, path_info):\n        \"\"\"\n        Recursively creates a new node in the file system if the given path does not exist.\n        \n         :"
    },
    {
        "original": "def cos_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a cosine wave (from -1\n    to +1) which repeats every *period* values. For example, to produce a\n    \"siren\" effect with a couple of LEDs that repeats once a second::\n\n        from gpiozero import PWMLED\n        from gpiozero.tools import cos_values, scaled, inverted\n        from signal import pause\n\n        red = PWMLED(2)\n        blue = PWMLED(3)\n\n        red.source_delay = 0.01\n        blue.source_delay = red.source_delay\n        red.source = scaled(cos_values(100), 0, 1, -1, 1)\n        blue.source = inverted(red)\n\n        pause()\n\n    If you require a different range than -1 to +1, see :func:`scaled`.\n    \"\"\"\n    angles = (2 * pi * i / period for i in range(period))\n    for a in cycle(angles):\n        yield cos(a)",
        "sampled": "```python\nimport numpy as np\n\ndef cos_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a cosine wave (from -1\n    to +1) which repeats every *period* values.\n    \n    Args:\n        period (int, optional): The period of the cosine wave. Defaults to 360.\n    \n    Yields:\n        float: A value representing a point on the cosine wave.\n    \"\"\"\n    \n    # Initialize the phase angle to 0\n"
    },
    {
        "original": "def _set_axis_limits(self, axis, view, subplots, ranges):\n        \"\"\"\n        Compute extents for current view and apply as axis limits\n        \"\"\"\n        # Extents\n        extents = self.get_extents(view, ranges)\n        if not extents or self.overlaid:\n            axis.autoscale_view(scalex=True, scaley=True)\n            return\n\n        valid_lim = lambda c: util.isnumeric(c) and not np.isnan(c)\n        coords = [coord if np.isreal(coord) or isinstance(coord, np.datetime64) else np.NaN for coord in extents]\n        coords = [date2num(util.dt64_to_dt(c)) if isinstance(c, np.datetime64) else c\n                  for c in coords]\n        if self.projection == '3d' or len(extents) == 6:\n            l, b, zmin, r, t, zmax = coords\n            if self.invert_zaxis or any(p.invert_zaxis for p in subplots):\n                zmin, zmax = zmax, zmin\n            if zmin != zmax:\n                if valid_lim(zmin):\n                    axis.set_zlim(bottom=zmin)\n                if valid_lim(zmax):\n                    axis.set_zlim(top=zmax)\n        else:\n            l, b, r, t = coords\n\n        if self.invert_axes:\n            l, b, r, t = b, l, t, r\n\n        invertx = self.invert_xaxis or any(p.invert_xaxis for p in subplots)\n        xlim, scalex = self._compute_limits(l, r, self.logx, invertx, 'left', 'right')\n        inverty = self.invert_yaxis or any(p.invert_yaxis for p in subplots)\n        ylim, scaley =  self._compute_limits(b, t, self.logy, inverty, 'bottom', 'top')\n        if xlim:\n            axis.set_xlim(**xlim)\n        if ylim:\n            axis.set_ylim(**ylim)\n        axis.autoscale_view(scalex=scalex, scaley=scaley)",
        "sampled": "```python\nclass Plotter:\n    def _set_axis_limits(self, axis, view, subplots, ranges):\n        \"\"\"\n        Compute extents for current view and apply as axis limits\n        \"\"\"\n        \n        # Get the current view's x and y limits\n        x_min, x_max = view.get_xlim()\n        y_min, y_max = view.get_ylim()\n\n        # Calculate the range of each subplot\n        subplot_ranges = []\n        for i in range(subplots):\n            subplot_x"
    },
    {
        "original": "def get_data(self, latitude, longitude, start, end,\n                 vert_level=None, query_variables=None,\n                 close_netcdf_data=True):\n        \"\"\"\n        Submits a query to the UNIDATA servers using Siphon NCSS and\n        converts the netcdf data to a pandas DataFrame.\n\n        Parameters\n        ----------\n        latitude: float\n            The latitude value.\n        longitude: float\n            The longitude value.\n        start: datetime or timestamp\n            The start time.\n        end: datetime or timestamp\n            The end time.\n        vert_level: None, float or integer, default None\n            Vertical altitude of interest.\n        query_variables: None or list, default None\n            If None, uses self.variables.\n        close_netcdf_data: bool, default True\n            Controls if the temporary netcdf data file should be closed.\n            Set to False to access the raw data.\n\n        Returns\n        -------\n        forecast_data : DataFrame\n            column names are the weather model's variable names.\n        \"\"\"\n\n        if not self.connected:\n            self.connect_to_catalog()\n\n        if vert_level is not None:\n            self.vert_level = vert_level\n\n        if query_variables is None:\n            self.query_variables = list(self.variables.values())\n        else:\n            self.query_variables = query_variables\n\n        self.latitude = latitude\n        self.longitude = longitude\n        self.set_query_latlon()  # modifies self.query\n        self.set_location(start, latitude, longitude)\n\n        self.start = start\n        self.end = end\n        self.query.time_range(self.start, self.end)\n\n        if self.vert_level is not None:\n            self.query.vertical_level(self.vert_level)\n\n        self.query.variables(*self.query_variables)\n        self.query.accept(self.data_format)\n\n        self.netcdf_data = self.ncss.get_data(self.query)\n\n        # might be better to go to xarray here so that we can handle\n        # higher dimensional data for more advanced applications\n        self.data = self._netcdf2pandas(self.netcdf_data, self.query_variables,\n                                        self.start, self.end)\n\n        if close_netcdf_data:\n            self.netcdf_data.close()\n\n        return self.data",
        "sampled": "```python\nfrom datetime import datetime\nimport pandas as pd\nimport netCDF4 as nc\nimport time\n\nclass WeatherData:\n    def __init__(self):\n        self.variables = ['temperature', 'humidity', 'wind_speed']\n\n    def get_data(self, latitude, longitude, start, end,\n                 vert_level=None, query_variables=None,\n                 close_netcdf_data=True):\n        \"\"\"\n        Submits a query to the UNIDATA servers using Siphon NCSS and\n"
    },
    {
        "original": "def property_absent(name, property):\n    \"\"\"\n    Ensure property is absent\n\n    name : string\n        name of the zone\n    property : string\n        name of property\n\n    .. note::\n        This does a zoneacfg clear call. So the property may be reset to a default value!\n        Does has the side effect of always having to be called.\n\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    zones = __salt__['zoneadm.list'](installed=True, configured=True)\n    if name in zones:\n        ## zone exists\n        zonecfg = __salt__['zonecfg.info'](name, show_all=True)\n        if property in zonecfg:\n            if __opts__['test']:\n                ret['result'] = True\n            else:\n                # clear property\n                zonecfg_res = __salt__['zonecfg.clear_property'](name, property)\n                zonecfg_new = __salt__['zonecfg.info'](name, show_all=True)\n                ret['result'] = zonecfg_res['status']\n                if 'messages' in zonecfg_res:\n                    ret['comment'] = zonecfg_res['message']\n            if ret['result']:\n                if property not in zonecfg_new:\n                    ret['changes'][property] = None\n                elif zonecfg[property] != zonecfg_new[property]:\n                    ret['changes'][property] = zonecfg_new[property]\n                if ret['comment'] == '':\n                    ret['comment'] = 'The property {0} was cleared!'.format(property)\n            elif ret['comment'] == '':\n                if ret['comment'] == '':\n                    ret['comment'] = 'The property {0} did not get cleared!'.format(property)\n        else:\n            ret['result'] = True\n            ret['comment'] = 'The property {0} does not exist!'.format(property)\n    else:\n        ## zone does not exist\n        ret['result'] = False\n        ret['comment'] = 'The zone {0} is not in the configured, installed, or booted state.'.format(name)\n\n    return ret",
        "sampled": "```python\ndef property_absent(name, property):\n    \"\"\"\n    Ensure property is absent\n\n    name : string\n        name of the zone\n    property : string\n        name of property\n\n    .. note::\n        This does a zoneacfg clear call. So the property may be reset to a default value!\n        Does has the side effect of always having to be called.\n\n    \"\"\"\n    \n    # The function's purpose is unclear without more context. However, based on its description,"
    },
    {
        "original": "def transfer_bankcard(self, true_name, bank_card_no, bank_code, amount, desc=None, out_trade_no=None):\n        \"\"\"\n        \u4f01\u4e1a\u4ed8\u6b3e\u5230\u94f6\u884c\u5361\u63a5\u53e3\n\n        :param true_name: \u5f00\u6237\u4eba\u540d\u79f0\n        :param bank_card_no: \u94f6\u884c\u5361\u53f7\n        :param bank_code: \u94f6\u884c\u7f16\u53f7\n        :param amount: \u4ed8\u6b3e\u91d1\u989d\uff0c\u5355\u4f4d\u5206\n        :param desc: \u4ed8\u6b3e\u8bf4\u660e\n        :param out_trade_no: \u53ef\u9009\uff0c\u5546\u6237\u8ba2\u5355\u53f7\uff0c\u9700\u4fdd\u6301\u552f\u4e00\u6027\uff0c\u9ed8\u8ba4\u81ea\u52a8\u751f\u6210\n        :return: \u8fd4\u56de\u7684\u7ed3\u679c\u4fe1\u606f\n        \"\"\"\n        if not out_trade_no:\n            now = datetime.now()\n            out_trade_no = '{0}{1}{2}'.format(\n                self.mch_id,\n                now.strftime('%Y%m%d%H%M%S'),\n                random.randint(1000, 10000)\n            )\n        data = {\n            'mch_id': self.mch_id,\n            'partner_trade_no': out_trade_no,\n            'amount': amount,\n            'desc': desc,\n            'enc_bank_no': self._rsa_encrypt(bank_card_no),\n            'enc_true_name': self._rsa_encrypt(true_name),\n            'bank_code': bank_code,\n        }\n        return self._post('mmpaysptrans/pay_bank', data=data)",
        "sampled": "```python\nimport uuid\nimport json\n\nclass Alipay:\n    def __init__(self):\n        pass\n\n    def transfer_bankcard(self, true_name, bank_card_no, bank_code, amount, desc=None, out_trade_no=None):\n        \"\"\"\n        \u4f01\u4e1a\u4ed8\u6b3e\u5230\u94f6\u884c\u5361\u63a5\u53e3\n\n        :param true_name: \u5f00\u6237\u4eba\u540d\u79f0\n        :param bank_card_no: \u94f6\u884c\u5361\u53f7\n        :param bank_code: \u94f6\u884c\u7f16\u53f7\n        :"
    },
    {
        "original": "def send_frame(self, frame):\n        \"\"\"\n        Send the data frame.\n\n        frame: frame data created  by ABNF.create_frame\n\n        >>> ws = create_connection(\"ws://echo.websocket.org/\")\n        >>> frame = ABNF.create_frame(\"Hello\", ABNF.OPCODE_TEXT)\n        >>> ws.send_frame(frame)\n        >>> cont_frame = ABNF.create_frame(\"My name is \", ABNF.OPCODE_CONT, 0)\n        >>> ws.send_frame(frame)\n        >>> cont_frame = ABNF.create_frame(\"Foo Bar\", ABNF.OPCODE_CONT, 1)\n        >>> ws.send_frame(frame)\n\n        \"\"\"\n        if self.get_mask_key:\n            frame.get_mask_key = self.get_mask_key\n        data = frame.format()\n        length = len(data)\n        trace(\"send: \" + repr(data))\n\n        with self.lock:\n            while data:\n                l = self._send(data)\n                data = data[l:]\n\n        return length",
        "sampled": "```python\nclass WebSocket:\n    def __init__(self, url):\n        self.url = url\n        self.frames = []\n\n    def create_frame(self, data, opcode=ABNF.OPCODE_TEXT, final_fragment=False):\n        \"\"\"\n        Create a data frame.\n\n        :param data: the data to send\n        :param opcode: the opcode of the frame (see ABNF)\n        :param final_fragment: True if this is a final fragment (for large payloads see RFC6455"
    },
    {
        "original": "def from_stream(cls, stream):\n        \"\"\"\n        Return |Bmp| instance having header properties parsed from the BMP\n        image in *stream*.\n        \"\"\"\n        stream_rdr = StreamReader(stream, LITTLE_ENDIAN)\n\n        px_width = stream_rdr.read_long(0x12)\n        px_height = stream_rdr.read_long(0x16)\n\n        horz_px_per_meter = stream_rdr.read_long(0x26)\n        vert_px_per_meter = stream_rdr.read_long(0x2A)\n\n        horz_dpi = cls._dpi(horz_px_per_meter)\n        vert_dpi = cls._dpi(vert_px_per_meter)\n\n        return cls(px_width, px_height, horz_dpi, vert_dpi)",
        "sampled": "```python\nimport struct\n\nclass Bmp:\n    def __init__(self, file_name):\n        with open(file_name, 'rb') as f:\n            self.header = self.parse_header(f)\n\n    @classmethod\n    def from_stream(cls, stream):\n        \"\"\"\n        Return Bmp instance having header properties parsed from the BMP\n        image in *stream*.\n        \"\"\"\n        # Read header fields to calculate total size of DIB (1544 bytes for 24-bit files)\n       "
    },
    {
        "original": "def send_video(self, chat_id, data, duration=None, caption=None, reply_to_message_id=None, reply_markup=None,\n                   parse_mode=None, supports_streaming=None, disable_notification=None, timeout=None):\n        \"\"\"\n        Use this method to send video files, Telegram clients support mp4 videos.\n        :param chat_id: Integer : Unique identifier for the message recipient \u2014 User or GroupChat id\n        :param data: InputFile or String : Video to send. You can either pass a file_id as String to resend a video that is already on the Telegram server\n        :param duration: Integer : Duration of sent video in seconds\n        :param caption: String : Video caption (may also be used when resending videos by file_id).\n        :param parse_mode:\n        :param supports_streaming:\n        :param reply_to_message_id:\n        :param reply_markup:\n        :return:\n        \"\"\"\n        return types.Message.de_json(\n            apihelper.send_video(self.token, chat_id, data, duration, caption, reply_to_message_id, reply_markup,\n                                 parse_mode, supports_streaming, disable_notification, timeout))",
        "sampled": "```python\nclass TelegramBot:\n    def __init__(self, api_key):\n        \"\"\"\n        Initialize the Telegram bot with an API key.\n        \n        :param api_key: String : API key for the Telegram bot\n        \"\"\"\n        self.api_key = api_key\n\n    def send_video(self, chat_id, data, duration=None, caption=None, reply_to_message_id=None,\n                  reply_markup=None, parse_mode=None, supports_streaming=None,\n                  disable_notification=False, timeout=20"
    },
    {
        "original": "def language_list(\n        maintenance_db,\n        user=None,\n        host=None,\n        port=None,\n        password=None,\n        runas=None):\n    \"\"\"\n    .. versionadded:: 2016.3.0\n\n    Return a list of languages in a database.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.language_list dbname\n\n    maintenance_db\n        The database to check\n\n    user\n        database username if different from config or default\n\n    password\n        user password if any password for a specified user\n\n    host\n        Database host if different from config or default\n\n    port\n        Database port if different from config or default\n\n    runas\n        System user all operations should be performed on behalf of\n    \"\"\"\n\n    ret = {}\n    query = 'SELECT lanname AS \"Name\" FROM pg_language'\n\n    rows = psql_query(\n        query,\n        runas=runas,\n        host=host,\n        user=user,\n        port=port,\n        maintenance_db=maintenance_db,\n        password=password)\n\n    for row in rows:\n        ret[row['Name']] = row['Name']\n\n    return ret",
        "sampled": "```python\nimport psycopg2\n\ndef language_list(maintenance_db, user=None, host=None, port=None, password=None, runas=None):\n    \"\"\"\n    Return a list of languages in a database.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.language_list dbname\n\n    maintenance_db\n        The database to check\n\n    user\n        database username if different from config or default\n\n    password\n        user password if any password for a specified user\n\n    host"
    },
    {
        "original": "def _make_callsites(self, stack_pointer_tracker=None):\n        \"\"\"\n        Simplify all function call statements.\n\n        :return:    None\n        \"\"\"\n\n        # Computing reaching definitions\n        rd = self.project.analyses.ReachingDefinitions(func=self.function, func_graph=self.graph, observe_all=True)\n\n        for key in self._blocks:\n            block = self._blocks[key]\n            csm = self.project.analyses.AILCallSiteMaker(block, reaching_definitions=rd)\n            if csm.result_block:\n                ail_block = csm.result_block\n                simp = self.project.analyses.AILBlockSimplifier(ail_block, stack_pointer_tracker=stack_pointer_tracker)\n                self._blocks[key] = simp.result_block\n\n        self._update_graph()",
        "sampled": "```python\nclass CallSimplifier:\n    def _make_callsites(self, stack_pointer_tracker=None):\n        \"\"\"\n        Simplify all function call statements.\n\n        :return:    None\n        \"\"\"\n\n        # Initialize an empty dictionary to store the simplified callsites\n        simplified_callsites = {}\n\n        # If a stack pointer tracker is provided, use it to simplify the callsites\n        if stack_pointer_tracker is not None:\n            for func_name in stack_pointer_tracker.get_call_stack():\n               "
    },
    {
        "original": "def get_sorted_structure(self, key=None, reverse=False):\n        \"\"\"\n        Get a sorted copy of the structure. The parameters have the same\n        meaning as in list.sort. By default, sites are sorted by the\n        electronegativity of the species. Note that Slab has to override this\n        because of the different __init__ args.\n        Args:\n            key: Specifies a function of one argument that is used to extract\n                a comparison key from each list element: key=str.lower. The\n                default value is None (compare the elements directly).\n            reverse (bool): If set to True, then the list elements are sorted\n                as if each comparison were reversed.\n        \"\"\"\n        sites = sorted(self, key=key, reverse=reverse)\n        s = Structure.from_sites(sites)\n        return GrainBoundary(s.lattice, s.species_and_occu, s.frac_coords,\n                             self.rotation_axis, self.rotation_angle, self.gb_plane,\n                             self.join_plane, self.init_cell, self.vacuum_thickness,\n                             self.ab_shift, self.site_properties, self.oriented_unit_cell)",
        "sampled": "```python\nclass Structure:\n    def __init__(self, sites):\n        self.sites = sites\n\n    def get_sorted_structure(self, key=None, reverse=False):\n        \"\"\"\n        Get a sorted copy of the structure. The parameters have the same\n        meaning as in list.sort. By default, sites are sorted by the\n        electronegativity of the species.\n        \n        Args:\n            key: Specifies a function of one argument that is used to extract\n                a comparison key"
    },
    {
        "original": "def locate_cuda():\n    \"\"\"Locate the CUDA environment on the system\n\n    If a valid cuda installation is found this returns a dict with keys 'home', 'nvcc', 'include',\n    and 'lib64' and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything is based on finding\n    'nvcc' in the PATH.\n\n    If nvcc can't be found, this returns None\n    \"\"\"\n    nvcc_bin = 'nvcc'\n    if sys.platform.startswith(\"win\"):\n        nvcc_bin = 'nvcc.exe'\n\n    # first check if the CUDAHOME env variable is in use\n    if 'CUDAHOME' in os.environ:\n        home = os.environ['CUDAHOME']\n        nvcc = os.path.join(home, 'bin', nvcc_bin)\n    elif 'CUDA_PATH' in os.environ:\n        home = os.environ['CUDA_PATH']\n        nvcc = os.path.join(home, 'bin', nvcc_bin)\n    else:\n        # otherwise, search the PATH for NVCC\n        nvcc = find_in_path(nvcc_bin, os.environ['PATH'])\n        if nvcc is None:\n            logging.warning('The nvcc binary could not be located in your $PATH. Either add it to '\n                            'your path, or set $CUDAHOME to enable CUDA extensions')\n            return None\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {'home': home,\n                  'nvcc': nvcc,\n                  'include': os.path.join(home, 'include'),\n                  'lib64':   os.path.join(home, 'lib64')}\n    post_args = ['-gencode=arch=compute_30,code=sm_30',\n                 '-gencode=arch=compute_50,code=sm_50',\n                 '-gencode=arch=compute_60,code=sm_60',\n                 '-gencode=arch=compute_60,code=compute_60',\n                 '--ptxas-options=-v', '-O2']\n\n    if sys.platform == \"win32\":\n        cudaconfig['lib64'] = os.path.join(home, 'lib', 'x64')\n        post_args += ['-Xcompiler', '/MD']\n    else:\n        post_args += ['-c', '--compiler-options', \"'-fPIC'\"]\n\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            logging.warning('The CUDA %s path could not be located in %s', k, v)\n            return None\n\n    cudaconfig['post_args'] = post_args\n    return cudaconfig",
        "sampled": "### Problem Description\nFind the CUDA environment on the system.\n\n### Input and Output Specifications\n\n* Inputs:\n  - None\n* Outputs:\n  - `cuda_paths` : a dictionary containing the absolute paths to the CUDA installation directories\n    * `home` : absolute path to the CUDA installation directory\n    * `nvcc` : absolute path to the nvcc compiler executable\n    * `include` : absolute path to the header files directory\n    * `lib64` : absolute path"
    },
    {
        "original": "def telnet_login(\n        self, username_pattern=r\"(?:user>)\", alt_prompt_term=r\"#\\s*$\", **kwargs\n    ):\n        \"\"\"\n        RAD presents with the following on login\n\n        user>\n\n        password> ****\n        \"\"\"\n        self.TELNET_RETURN = self.RETURN\n        return super(RadETXTelnet, self).telnet_login(\n            username_pattern=username_pattern,\n            alt_prompt_terminator=alt_prompt_term,\n            **kwargs\n        )",
        "sampled": "```python\nimport re\n\nclass SSHClient:\n    def __init__(self, host, username_pattern=r\"(?:user>)\", alt_prompt_term=r\"#\\s*$\"):\n        self.host = host\n        self.username_pattern = re.compile(username_pattern)\n        self.alt_prompt_term = alt_prompt_term\n        self.prompt_buffer = []\n\n    def telnet_login(self, password):\n        # Simulate typing in username and pressing enter\n        print(\"user>\")\n        \n        # Simulate typing in"
    },
    {
        "original": "def _local_decode(self):\n        \"\"\"\n        Finds the index of the maximum values for all the single node dual objectives.\n\n        Reference:\n        code presented by Sontag in 2012 here: http://cs.nyu.edu/~dsontag/code/README_v2.html\n        \"\"\"\n        # The current assignment of the single node factors is stored in the form of a dictionary\n        decoded_result_assignment = {node: np.argmax(self.objective[node].values)\n                                     for node in self.objective if len(node) == 1}\n        # Use the original cluster_potentials of each factor to find the primal integral value.\n        # 1. For single node factors\n        integer_value = sum([self.factors[variable][0].values[decoded_result_assignment[frozenset([variable])]]\n                             for variable in self.variables])\n        # 2. For clusters\n        for cluster_key in self.cluster_set:\n            cluster = self.cluster_set[cluster_key]\n            index = [tuple([variable, decoded_result_assignment[frozenset([variable])]])\n                     for variable in cluster.cluster_variables]\n            integer_value += cluster.cluster_potential.reduce(index, inplace=False).values\n\n        # Check if this is the best assignment till now\n        if self.best_int_objective < integer_value:\n            self.best_int_objective = integer_value\n            self.best_assignment = decoded_result_assignment",
        "sampled": "```python\nimport numpy as np\n\nclass Solution:\n    def _local_decode(self, dual_objectives):\n        \"\"\"\n        Finds the index of the maximum values for all the single node dual objectives.\n\n        Parameters:\n        dual_objectives (numpy array): An array of shape (n_nodes, n_classes) containing the dual objectives.\n\n        Returns:\n        numpy array: An array of shape (n_nodes,) containing the indices of the maximum values.\n        \"\"\"\n        \n        # Get the number of nodes"
    },
    {
        "original": "def get_token(self):\n        \"\"\"\n        Retrieves the token from the File System\n        :return dict or None: The token if exists, None otherwise\n        \"\"\"\n        token = None\n        if self.token_path.exists():\n            with self.token_path.open('r') as token_file:\n                token = self.token_constructor(self.serializer.load(token_file))\n        self.token = token\n        return token",
        "sampled": "```python\nclass TokenRetriever:\n    def __init__(self, file_system):\n        \"\"\"\n        Initializes the TokenRetriever with a file system.\n        \n        :param file_system: The file system to retrieve the token from.\n        \"\"\"\n        self.file_system = file_system\n\n    def get_token(self):\n        \"\"\"\n        Retrieves the token from the File System.\n        \n        :return dict or None: The token if exists, None otherwise\n        \"\"\"\n        \n# Example usage:\nclass"
    },
    {
        "original": "def _pipe_segment_with_colons(align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return ('-' * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + ('-' * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + ('-' * (w - 1))\n    else:\n        return '-' * w",
        "sampled": "```python\nimport math\n\n\ndef _pipe_segment_with_colons(align, colwidth):\n    \"\"\"\n    Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\n    \"\"\"\n    \n    # Check if we are filling into a field of specified width,colon at end\n    if align == 'fillable':\n        return ':' + '-' * (colwidth - 1) + ':'\n    \n    # If we are not"
    },
    {
        "original": "def from_digraph(self, digraph):\n        \"\"\"\n        Initialize this AnnotatedCFG object with a networkx.DiGraph consisting of the following\n        form of nodes:\n\n        Tuples like (block address, statement ID)\n\n        Those nodes are connected by edges indicating the execution flow.\n\n        :param networkx.DiGraph digraph: A networkx.DiGraph object\n        \"\"\"\n\n        for n1 in digraph.nodes():\n            addr1, stmt_idx1 = n1\n            self.add_statements_to_whitelist(addr1, (stmt_idx1,))\n\n            successors = digraph[n1]\n            for n2 in successors:\n                addr2, stmt_idx2 = n2\n\n                if addr1 != addr2:\n                    # There is a control flow transition from block `addr1` to block `addr2`\n                    self.add_exit_to_whitelist(addr1, addr2)\n\n                self.add_statements_to_whitelist(addr2, (stmt_idx2,))",
        "sampled": "### Problem Description\nInitialize this AnnotatedCFG object with a networkx.DiGraph consisting of the following form of nodes:\n\n- Tuples like `(block address, statement ID)`\n\nThose nodes are connected by edges indicating the execution flow.\n\n### Input and Output Specifications\n\nNo specification provided.\n\n### Code Solution\n\n```markdown\nimport networkx as nx\n\nclass AnnotatedCFG:\n    def __init__(self):\n        self.cfg = None\n\n    def from_digraph(self, digraph):\n        \"\"\"\n"
    },
    {
        "original": "def download_software_version(version=None, synch=False):\n    \"\"\"\n    Download software packages by version number.\n\n    Args:\n        version(str): The version of the PANOS file to download.\n\n        synch (bool): If true then the file will synch to the peer unit.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' panos.download_software_version 8.0.0\n        salt '*' panos.download_software_version 8.0.0 True\n\n    \"\"\"\n    if not version:\n        raise CommandExecutionError(\"Version option must not be none.\")\n\n    if not isinstance(synch, bool):\n        raise CommandExecutionError(\"Synch option must be boolean..\")\n\n    if synch is True:\n        query = {'type': 'op',\n                 'cmd': '<request><system><software><download>'\n                        '<version>{0}</version></download></software></system></request>'.format(version)}\n    else:\n        query = {'type': 'op',\n                 'cmd': '<request><system><software><download><sync-to-peer>yes</sync-to-peer>'\n                        '<version>{0}</version></download></software></system></request>'.format(version)}\n\n    return _get_job_results(query)",
        "sampled": "Problem Description\n====================\nDownload software packages by version number.\n\nInput Specifications\n---------------------\n- `version`: The version of the PANOS file to download. (str)\n- `synch`: If true then the file will synch to the peer unit. (bool)\n\nOutput Specifications\n----------------------\nThe function should download the software package by version number and optionally synchronize it with a peer unit.\n\nPython Solution\n```python\ndef download_software_version(version=None, synch=False):\n    \"\"\"\n   "
    },
    {
        "original": "def list_(consul_url=None, token=None, key=None, **kwargs):\n    \"\"\"\n    List keys in Consul\n\n    :param consul_url: The Consul server URL.\n    :param key: The key to use as the starting point for the list.\n    :return: The list of keys.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' consul.list\n        salt '*' consul.list key='web'\n\n    \"\"\"\n    ret = {}\n\n    if not consul_url:\n        consul_url = _get_config()\n        if not consul_url:\n            log.error('No Consul URL found.')\n            ret['message'] = 'No Consul URL found.'\n            ret['res'] = False\n            return ret\n\n    query_params = {}\n\n    if 'recurse' in kwargs:\n        query_params['recurse'] = 'True'\n\n    # No key so recurse and show all values\n    if not key:\n        query_params['recurse'] = 'True'\n        function = 'kv/'\n    else:\n        function = 'kv/{0}'.format(key)\n\n    query_params['keys'] = 'True'\n    query_params['separator'] = '/'\n    ret = _query(consul_url=consul_url,\n                 function=function,\n                 token=token,\n                 query_params=query_params)\n    return ret",
        "sampled": "### Problem Description\nList keys in Consul\n\n### Input and Output Specifications\n#### Input:\n- `consul_url`: The Consul server URL.\n- `token`: The token to use for authentication.\n- `key`: The key to use as the starting point for the list.\n\n#### Output:\nThe list of keys.\n\n### Python Solution\n```python\nimport requests\n\ndef list_(consul_url=None, token=None, key=None, **kwargs):\n    \"\"\"\n    List keys in Cons"
    },
    {
        "original": "def find_one_and_update(self, filter, update, **kwargs):\n        \"\"\"\n        See http://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find_one_and_update\n        \"\"\"\n        self._arctic_lib.check_quota()\n        return self._collection.find_one_and_update(filter, update, **kwargs)",
        "sampled": "```python\nfrom pymongo import MongoClient\n\nclass MongoDB:\n    def __init__(self, host, port):\n        self.client = MongoClient(host=host, port=port)\n        self.db = self.client[\"mydatabase\"]\n        self.collection = self.db[\"mycollection\"]\n\n    def find_one_and_update(self, filter, update):\n        \"\"\"\n        Find a document and update it.\n\n        Args:\n            filter (dict): The filter to find the document.\n            update (dict): The updates to"
    },
    {
        "original": "def list_networks(auth=None, **kwargs):\n    \"\"\"\n    List networks\n\n    filters\n        A Python dictionary of filter conditions to push down\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutronng.list_networks\n        salt '*' neutronng.list_networks \\\n          filters='{\"tenant_id\": \"1dcac318a83b4610b7a7f7ba01465548\"}'\n\n    \"\"\"\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_networks(**kwargs)",
        "sampled": "```python\nimport requests\nimport json\n\ndef list_networks(auth=None, **kwargs):\n    \"\"\"\n    List networks\n    \n    filters\n        A Python dictionary of filter conditions to push down\n    \n    CLI Example:\n    \n        .. code-block:: bash\n    \n            salt '*' neutronng.list_networks\n            salt '*' neutronng.list_networks \\\n              filters='{\"tenant_id\": \"1dcac318a83b4610b7a7f7ba01465548\"}'\n            \n   "
    },
    {
        "original": "def _get_all_field_lines(self):\n        \"\"\"\n        Returns all lines that represent the fields of the layer (both their names and values).\n        \"\"\"\n        for field in self._get_all_fields_with_alternates():\n            # Change to yield from\n            for line in self._get_field_or_layer_repr(field):\n                yield line",
        "sampled": "### Problem Description\nGet All Field Lines\n================\n\nGiven a layer, return all lines that represent the fields of the layer (both their names and values).\n\n### Input and Output Specifications\n- **Input**: A layer (it's structure is not specified in this challenge)\n- **Output**: A list of strings representing the field lines.\n\n### Challenge Solution\n\n```python\ndef _get_all_field_lines(self):\n    \"\"\"\n    Returns all lines that represent the fields of the layer (both their names"
    },
    {
        "original": "def make_request_from_data(self, data):\n        \"\"\"Returns a Request instance from data coming from Redis.\n\n        By default, ``data`` is an encoded URL. You can override this method to\n        provide your own message decoding.\n\n        Parameters\n        ----------\n        data : bytes\n            Message from redis.\n\n        \"\"\"\n        url = bytes_to_str(data, self.redis_encoding)\n        return self.make_requests_from_url(url)",
        "sampled": "```python\nfrom urllib.parse import urlparse, urlunparse\nfrom requests import Request\n\ndef make_request_from_data(self, data):\n    \"\"\"\n    Returns a Request instance from data coming from Redis.\n\n    By default, ``data`` is an encoded URL. You can override this method to\n    provide your own message decoding.\n\n    Parameters\n    ----------\n    data : bytes\n        Message from redis.\n    \n    Returns\n    -------\n        A Request instance.\n    \n    \"\"\"\n    \n    # Decode the"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "sampled": "```python\nimport os\nimport xml.etree.ElementTree as ET\n\nclass SkeletonLoader:\n    def load_skel(self, file_name):\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n        \"\"\"\n        \n        # Check if the file exists\n        if not os.path.exists(file_name):\n            raise FileNotFoundError(f\"The file {file_name} does not exist.\")\n\n        # Parse the XML tree from the ASF file\n       "
    },
    {
        "original": "def simple_takeoff(self, alt=None):\n        \"\"\"\n        Take off and fly the vehicle to the specified altitude (in metres) and then wait for another command.\n\n        .. note::\n\n            This function should only be used on Copter vehicles.\n\n\n        The vehicle must be in GUIDED mode and armed before this is called.\n\n        There is no mechanism for notification when the correct altitude is reached,\n        and if another command arrives before that point (e.g. :py:func:`simple_goto`) it will be run instead.\n\n        .. warning::\n\n           Apps should code to ensure that the vehicle will reach a safe altitude before\n           other commands are executed. A good example is provided in the guide topic :doc:`guide/taking_off`.\n\n        :param alt: Target height, in metres.\n        \"\"\"\n        if alt is not None:\n            altitude = float(alt)\n            if math.isnan(altitude) or math.isinf(altitude):\n                raise ValueError(\"Altitude was NaN or Infinity. Please provide a real number\")\n            self._master.mav.command_long_send(0, 0, mavutil.mavlink.MAV_CMD_NAV_TAKEOFF,\n                                               0, 0, 0, 0, 0, 0, 0, altitude)",
        "sampled": "### Problem Analysis\n\nThis problem appears to be a part of an autopilot system, specifically designed for a Copter vehicle. The function `simple_takeoff` is used to take off and fly the vehicle to a specified altitude in meters.\n\n### Input and Output Specifications\n\n- There is only one parameter `alt` which represents the target height in meters.\n- The function does not return any value, it just sets the vehicle's altitude using drone commands.\n\n### Python Solution\n```python\nclass"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a RecognitionJob object from a json dictionary.\"\"\"\n        args = {}\n        if 'id' in _dict:\n            args['id'] = _dict.get('id')\n        else:\n            raise ValueError(\n                'Required property \\'id\\' not present in RecognitionJob JSON')\n        if 'status' in _dict:\n            args['status'] = _dict.get('status')\n        else:\n            raise ValueError(\n                'Required property \\'status\\' not present in RecognitionJob JSON'\n            )\n        if 'created' in _dict:\n            args['created'] = _dict.get('created')\n        else:\n            raise ValueError(\n                'Required property \\'created\\' not present in RecognitionJob JSON'\n            )\n        if 'updated' in _dict:\n            args['updated'] = _dict.get('updated')\n        if 'url' in _dict:\n            args['url'] = _dict.get('url')\n        if 'user_token' in _dict:\n            args['user_token'] = _dict.get('user_token')\n        if 'results' in _dict:\n            args['results'] = [\n                SpeechRecognitionResults._from_dict(x)\n                for x in (_dict.get('results'))\n            ]\n        if 'warnings' in _dict:\n            args['warnings'] = _dict.get('warnings')\n        return cls(**args)",
        "sampled": "```python\nclass RecognitionJob:\n    def __init__(self, name, init_time, max_time):\n        self.name = name\n        self.init_time = init_time\n        self.max_time = max_time\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"\n        Initialize a RecognitionJob object from a json dictionary.\n\n            :param _dict: {\n                'name': str,\n                'initTime': int,\n                'maxTime': int\n            }\n"
    },
    {
        "original": "def get_log_likelihood(inputs,data,clust):\n    \"\"\"Get the LL of a combined set of clusters, ignoring time series offsets.\n    \n    Get the log likelihood of a cluster without worrying about the fact\n    different time series are offset. We're using it here really for those\n    cases in which we only have one cluster to get the loglikelihood of.\n    \n    arguments:\n    inputs -- the 'X's in a list, one item per cluster\n    data -- the 'Y's in a list, one item per cluster\n    clust -- list of clusters to use\n    \n    returns a tuple:\n    log likelihood and the offset (which is always zero for this model)\n    \"\"\"\n \n    S = data[0].shape[0] #number of time series\n    \n    #build a new dataset from the clusters, by combining all clusters together\n    X = np.zeros([0,1])\n    Y = np.zeros([0,S])\n    \n    #for each person in the cluster,\n    #add their inputs and data to the new dataset\n    for p in clust:\n        X = np.vstack([X,inputs[p]])\n        Y = np.vstack([Y,data[p].T])\n        \n    #find the loglikelihood. We just add together the LL for each time series.\n    #ll=0\n    #for s in range(S):\n    #    m = GPy.models.GPRegression(X,Y[:,s][:,None])\n    #    m.optimize()\n    #    ll+=m.log_likelihood()\n\n    m = GPy.models.GPRegression(X,Y)\n    m.optimize()\n    ll=m.log_likelihood()    \n    return ll,0",
        "sampled": "### Problem Analysis\n\nThe problem involves calculating the log likelihood of a combined set of clusters. The key considerations are:\n\n* We have two lists: `inputs` (X values) and `data` (Y values), each containing one item per cluster.\n* We have a list of clusters `clust`.\n* The goal is to calculate the log likelihood for this combined set, ignoring time series offsets.\n\nThis suggests that we are dealing with a Gaussian mixture model, where each cluster represents a component in"
    },
    {
        "original": "def _SetPacketSizeForFollowingConnections(cursor):\n  \"\"\"Sets max_allowed_packet globally for new connections (not current!).\"\"\"\n  cur_packet_size = int(_ReadVariable(\"max_allowed_packet\", cursor))\n\n  if cur_packet_size < MAX_PACKET_SIZE:\n    logging.warning(\n        \"MySQL max_allowed_packet of %d is required, got %d. Overwriting.\",\n        MAX_PACKET_SIZE, cur_packet_size)\n    _SetGlobalVariable(\"max_allowed_packet\", MAX_PACKET_SIZE, cursor)",
        "sampled": "### Problem Description\n## Debug Database Panel\n\nThere is a bug in the `phpMyAdmin` application, when it comes to `mysql` databases. Specifically, when there are very large binary values (`BLOB`) during export in the \"Debug\" panel of phpMyAdmin database or in code, you will see them suppressed at around 1024 bytes; this restriction is imposed by PHP's inherent `max_allowed_packet` configuration of MySQL.\n\n## Task\n* The task consists into dynamically finding"
    },
    {
        "original": "def add_organization(self, service_desk_id, organization_id):\n        \"\"\"\n        Adds an organization to a servicedesk for a given servicedesk ID and organization ID\n\n        :param service_desk_id: str\n        :param organization_id: int\n        :return:\n        \"\"\"\n        log.warning('Adding organization...')\n        url = 'rest/servicedeskapi/servicedesk/{}/organization'.format(service_desk_id)\n        data = {'organizationId': organization_id}\n\n        return self.post(url, headers=self.experimental_headers, data=data)",
        "sampled": "### Problem Description\nAdd organization to a servicedesk\n\n### Input and Output Specifications\n| Field Name | Data Type | Description                                                  |\n|------------|-----------|------------------------------------------------------------------|\n| service_desk_id | str      | ID of the servicedesk to add the organization to                    |\n| organization_id  | int     | ID of the organization to add                                       |\n\n### Python Solution\n\n```python\nclass ServiceDesk:\n    def __init__(self):\n        # Initialize an empty dictionary to store organizations for"
    },
    {
        "original": "def _msg_text(self):\n        \"\"\"\n        Generate the event's body to send to Datadog.\n\n        Consider `event_format` parameter:\n        * Only use the specified list of event properties.\n        * If unspecified, default to the EventLog's `Message` or `InsertionStrings`.\n        \"\"\"\n        msg_text = u\"\"\n\n        if self._format:\n            msg_text_fields = [\"%%%\\n```\"]\n\n            for event_property in self._format:\n                property_value = self.event.get(event_property)\n                if property_value is None:\n                    self.log.warning(u\"Unrecognized `%s` event property.\", event_property)\n                    continue\n                msg_text_fields.append(\n                    u\"{property_name}: {property_value}\".format(\n                        property_name=event_property, property_value=property_value\n                    )\n                )\n\n            msg_text_fields.append(\"```\\n%%%\")\n\n            msg_text = u\"\\n\".join(msg_text_fields)\n        else:\n            # Override when verbosity\n            if self.event.get('Message'):\n                msg_text = u\"{message}\\n\".format(message=self.event['Message'])\n            elif self.event.get('InsertionStrings'):\n                msg_text = u\"\\n\".join([i_str for i_str in self.event['InsertionStrings'] if i_str.strip()])\n\n        if self.notify_list:\n            msg_text += u\"\\n{notify_list}\".format(notify_list=' '.join([\" @\" + n for n in self.notify_list]))\n\n        return msg_text",
        "sampled": "```python\nclass DatadogEvent:\n    def _msg_text(self, event_format=None):\n        \"\"\"\n        Generate the event's body to send to Datadog.\n\n        Args:\n            event_format (list of strings): Specify which event properties to use.\n                If unspecified, default to the EventLog's `Message` or `InsertionStrings`.\n\n        Returns:\n            str: The formatted message text.\n        \"\"\"\n\n        # Define default properties if not specified\n        if not event_format:\n"
    },
    {
        "original": "def lock(self, source_node):\n        \"\"\"Lock the task, source is the :class:`Node` that applies the lock.\"\"\"\n        if self.status != self.S_INIT:\n            raise ValueError(\"Trying to lock a task with status %s\" % self.status)\n\n        self._status = self.S_LOCKED\n        self.history.info(\"Locked by node %s\", source_node)",
        "sampled": "```python\nclass Node:\n    def __init__(self, val=None, children=None):\n        self.val = val\n        self.children = children if children is not None else []\n\nclass Solution:\n    def lock(self, source_node):\n        \"\"\"\n        Lock the task, source is the :class:`Node` that applies the lock.\n        \n        Args:\n            source_node (Node): The node to apply the lock.\n        \n        Returns:\n            A dictionary containing all locked nodes and their corresponding locks"
    },
    {
        "original": "def get_ndmapping_label(ndmapping, attr):\n    \"\"\"\n    Function to get the first non-auxiliary object\n    label attribute from an NdMapping.\n    \"\"\"\n    label = None\n    els = itervalues(ndmapping.data)\n    while label is None:\n        try:\n            el = next(els)\n        except StopIteration:\n            return None\n        if not getattr(el, '_auxiliary_component', True):\n            label = getattr(el, attr)\n    if attr == 'group':\n        tp = type(el).__name__\n        if tp == label:\n            return None\n    return label",
        "sampled": "```python\nclass NdMapping:\n    def __init__(self, data):\n        self.data = data\n\ndef get_ndmapping_label(ndmapping, attr):\n    \"\"\"\n    Function to get the first non-auxiliary object label attribute from an NdMapping.\n    \n    Args:\n        ndmapping (NdMapping): The input NdMapping object.\n        attr (str): The attribute name to search for in the NdMapping.\n\n    Returns:\n        str: The first non-auxiliary object label attribute"
    },
    {
        "original": "def get_app_name(self):\n        \"\"\"\n        Return the appname of the APK\n\n        This name is read from the AndroidManifest.xml\n        using the application android:label.\n        If no label exists, the android:label of the main activity is used.\n\n        If there is also no main activity label, an empty string is returned.\n\n        :rtype: :class:`str`\n        \"\"\"\n\n        app_name = self.get_attribute_value('application', 'label')\n        if app_name is None:\n            activities = self.get_main_activities()\n            main_activity_name = None\n            if len(activities) > 0:\n                main_activity_name = activities.pop()\n\n            # FIXME: would need to use _format_value inside get_attribute_value for each returned name!\n            # For example, as the activity name might be foobar.foo.bar but inside the activity it is only .bar\n            app_name = self.get_attribute_value('activity', 'label', name=main_activity_name)\n\n        if app_name is None:\n            # No App name set\n            # TODO return packagename instead?\n            log.warning(\"It looks like that no app name is set for the main activity!\")\n            return \"\"\n\n        if app_name.startswith(\"@\"):\n            res_parser = self.get_android_resources()\n            if not res_parser:\n                # TODO: What should be the correct return value here?\n                return app_name\n\n            res_id, package = res_parser.parse_id(app_name)\n\n            # If the package name is the same as the APK package,\n            # we should be able to resolve the ID.\n            if package and package != self.get_package():\n                if package == 'android':\n                    # TODO: we can not resolve this, as we lack framework-res.apk\n                    # one exception would be when parsing framework-res.apk directly.\n                    log.warning(\"Resource ID with android package name encountered! \"\n                                \"Will not resolve, framework-res.apk would be required.\")\n                    return app_name\n                else:\n                    # TODO should look this up, might be in the resources\n                    log.warning(\"Resource ID with Package name '{}' encountered! Will not resolve\".format(package))\n                    return app_name\n\n            try:\n                app_name = res_parser.get_resolved_res_configs(\n                    res_id,\n                    ARSCResTableConfig.default_config())[0][1]\n            except Exception as e:\n                log.warning(\"Exception selecting app name: %s\" % e)\n        return app_name",
        "sampled": "### Problem Description\nGet App Name from Android Manifest\n=====================================\n\n### Input and Output Specifications\n* **Input:**\n  * `AndroidManifest.xml` file\n* **Output:**\n  * Name of the APK (as a string)\n\n### Solution\n\n```python\nimport xml.etree.ElementTree as ET\n\ndef get_app_name(self):\n    \"\"\"\n    Return the appname of the APK\n\n    This name is read from the AndroidManifest.xml\n    using the application android:label"
    },
    {
        "original": "def get_geometries(self, coordination=None, returned='cg'):\n        \"\"\"\n        Returns a list of coordination geometries with the given coordination number.\n        :param coordination: The coordination number of which the list of coordination geometries are returned.\n        \"\"\"\n        geom = list()\n        if coordination is None:\n            for gg in self.cg_list:\n                if returned == 'cg':\n                    geom.append(gg)\n                elif returned == 'mp_symbol':\n                    geom.append(gg.mp_symbol)\n        else:\n            for gg in self.cg_list:\n                if gg.get_coordination_number() == coordination:\n                    if returned == 'cg':\n                        geom.append(gg)\n                    elif returned == 'mp_symbol':\n                        geom.append(gg.mp_symbol)\n        return geom",
        "sampled": "### Problem Description\nGet Coordination Geometries\n==========================\n\nGiven a list of atomic coordinates, return a list of coordination geometries with the given coordination number.\n\n### Input and Output Specifications\n#### Input:\n- `coordination`: The coordination number of which the list of coordination geometries are returned.\n- `atomic_coordinates`: A list of atomic coordinates.\n\n#### Output:\n- A list of tuples, where each tuple contains the index1 atom's coordinate and index2 atom's coordinate that form"
    },
    {
        "original": "def fetch_node_status(member):\n        \"\"\"This function perform http get request on member.api_url and fetches its status\n        :returns: `_MemberStatus` object\n        \"\"\"\n\n        try:\n            response = requests.get(member.api_url, timeout=2, verify=False)\n            logger.info('Got response from %s %s: %s', member.name, member.api_url, response.content)\n            return _MemberStatus.from_api_response(member, response.json())\n        except Exception as e:\n            logger.warning(\"Request failed to %s: GET %s (%s)\", member.name, member.api_url, e)\n        return _MemberStatus.unknown(member)",
        "sampled": "```python\nimport requests\nfrom dataclasses import dataclass\n\n@dataclass\nclass _MemberStatus():\n    \"\"\"Placeholder member status object\"\"\"\n    pass\n\ndef fetch_node_status(url: str):\n    \"\"\"\n    This function performs a HTTP GET request on the provided URL and fetches its status.\n    \n    Args:\n        url (str): The URL to perform the GET request on.\n        \n    Returns:\n        MemberStatus object. For simplicity, we are assuming the response is in appropriate format.\n"
    },
    {
        "original": "def absolute_redirect_n_times(n):\n    \"\"\"Absolutely 302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n\n    assert n > 0\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=True))\n\n    return _redirect(\"absolute\", n, True)",
        "sampled": "```python\nfrom fastapi import FastAPI, Response\n\napp = FastAPI()\n\ndef absolute_redirect_n_times(n):\n    \"\"\"Absolutely 302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n    \n@app.get(\"/redirect/{n}\")\nasync def redirect(n: int"
    },
    {
        "original": "def overlay(array1, array2, alpha=0.5):\n    \"\"\"Overlays `array1` onto `array2` with `alpha` blending.\n\n    Args:\n        array1: The first numpy array.\n        array2: The second numpy array.\n        alpha: The alpha value of `array1` as overlayed onto `array2`. This value needs to be between [0, 1],\n            with 0 being `array2` only to 1 being `array1` only (Default value = 0.5).\n\n    Returns:\n        The `array1`, overlayed with `array2` using `alpha` blending.\n    \"\"\"\n    if alpha < 0. or alpha > 1.:\n        raise ValueError(\"`alpha` needs to be between [0, 1]\")\n    if array1.shape != array2.shape:\n        raise ValueError('`array1` and `array2` must have the same shapes')\n\n    return (array1 * alpha + array2 * (1. - alpha)).astype(array1.dtype)",
        "sampled": "**Problem Description**\n========================\n\nOverlaying Two Arrays Using Alpha Blending\n--------------------------------------------\n\nGiven two numpy arrays, `array1` and `array2`, we need to overlay `array1` onto `array2` using alpha blending. The alpha value determines the transparency of `array1` as it is overlaid onto `array2`.\n\n**Input and Output Specifications**\n-------------------------------\n\n*   **Input:**\n    *   `array1`: A 3D numpy array representing the"
    },
    {
        "original": "def running_time(self):\n        \"\"\"\n        For how long was the job running?\n        :return: Running time, seconds\n        :rtype: Optional[float]\n        \"\"\"\n        if not self.is_done():\n            raise ValueError(\"Cannot get running time for a program that isn't completed.\")\n        try:\n            running_time = float(self._raw['running_time'].split()[0])\n        except (ValueError, KeyError, IndexError):\n            raise UnknownApiError(str(self._raw))\n        return running_time",
        "sampled": "### Problem Description\nNo problem description is provided. However, I will assume that we are implementing a method to calculate the running time of a job.\n\n### Input and Output Specifications\n\n| Name | Type  | In/Out  | Description                              |\n| :----|:------|:--------|:------------------------------------------|\n| N    | int   | input   | The number of test cases                  |\n|    \t|\\( in_{j}\\)       |\\( \\forall j =1\\Cmd{ for"
    }
]