[
    {
        "original": "def _process_merge_request_change(self, payload, event, codebase=None):\n        \"\"\"\n        Consumes the merge_request JSON as a python object and turn it into a buildbot change.\n\n        :arguments:\n            payload\n                Python Object that represents the JSON sent by GitLab Service\n                Hook.\n        \"\"\"\n        attrs = payload['object_attributes']\n        commit = attrs['last_commit']\n        when_timestamp = dateparse(commit['timestamp'])\n        # @todo provide and document a way to choose between http and ssh url\n        repo_url = attrs['target']['git_http_url']\n        # project name from http headers is empty for me, so get it from object_attributes/target/name\n        project = attrs['target']['name']\n\n        # Filter out uninteresting events\n        state = attrs['state']\n        if re.match('^(closed|merged|approved)$', state):\n            log.msg(\"GitLab MR#{}: Ignoring because state is {}\".format(attrs['iid'], state))\n            return []\n        action = attrs['action']\n        if not re.match('^(open|reopen)$', action) and not (action == \"update\" and \"oldrev\" in attrs):\n            log.msg(\"GitLab MR#{}: Ignoring because action {} was not open or \"\n                    \"reopen or an update that added code\".format(attrs['iid'],\n                                                                 action))\n            return []\n\n        changes = [{\n            'author': '%s <%s>' % (commit['author']['name'],\n                                   commit['author']['email']),\n            'files': [],  # @todo use rest API\n            'comments': \"MR#{}: {}\\n\\n{}\".format(attrs['iid'], attrs['title'], attrs['description']),\n            'revision': commit['id'],\n            'when_timestamp': when_timestamp,\n            'branch': attrs['target_branch'],\n            'repository': repo_url,\n            'project': project,\n            'category': event,\n            'revlink': attrs['url'],\n            'properties': {\n                'source_branch': attrs['source_branch'],\n                'source_project_id': attrs['source_project_id'],\n                'source_repository': attrs['source']['git_http_url'],\n                'source_git_ssh_url': attrs['source']['git_ssh_url'],\n                'target_branch': attrs['target_branch'],\n                'target_project_id': attrs['target_project_id'],\n                'target_repository': attrs['target']['git_http_url'],\n                'target_git_ssh_url': attrs['target']['git_ssh_url'],\n                'event': event,\n            },\n        }]\n        if codebase is not None:\n            changes[0]['codebase'] = codebase\n        return changes",
        "sampled": "\ndef zunionstore(self, dest, keys, aggregate=None):\n    if aggregate is None:\n        aggregate = 'SUM'\n    temp = {}\n    for key in keys:\n        for member, score in self.redis.zscan_iter(key):\n            if member not in temp:\n                temp[member] = 0\n            if aggregate.upper() == 'SUM':\n                temp[member] += float(score)\n            elif aggregate.upper() == 'MIN':\n                temp[member] = min"
    },
    {
        "original": "def create_project(self, key, name, description=\"\"):\n        \"\"\"\n        Create project\n        :param key:\n        :param name:\n        :param description:\n        :return:\n        \"\"\"\n        url = 'rest/api/1.0/projects'\n        data = {\"key\": key,\n                \"name\": name,\n                \"description\": description\n                }\n        return self.post(url, data=data)",
        "sampled": "\nclass OpenStackAPI:\n    def _make_request(self, url, headers, params=None):\n        import requests\n        try:\n            if params is not None:\n                response = requests.get(url, headers=headers, params=params)\n            else:\n                response = requests.get(url, headers=headers)\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as errh:\n            if errh.response.status_code == 401:\n                raise UnauthorizedException(\"Unauthorized access\")\n            elif errh"
    },
    {
        "original": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)",
        "sampled": "\nclass ProgrammingAssistant:\n    def _describe_me(self):\n        return (\"I'm a programming assistant\", \"that helps with coding challenges\", \"by providing Python solutions\", \"for various problems and tasks\", \"in a concise and efficient manner\")\n"
    },
    {
        "original": "def get_interfaces_ip(self):\n        \"\"\"\n        Get interface ip details.\n\n        Returns a dict of dicts\n\n        Example Output:\n\n        {   u'FastEthernet8': {   'ipv4': {   u'10.66.43.169': {   'prefix_length': 22}}},\n            u'Loopback555': {   'ipv4': {   u'192.168.1.1': {   'prefix_length': 24}},\n                                'ipv6': {   u'1::1': {   'prefix_length': 64},\n                                            u'2001:DB8:1::1': {   'prefix_length': 64},\n                                            u'2::': {   'prefix_length': 64},\n                                            u'FE80::3': {   'prefix_length': 10}}},\n            u'Tunnel0': {   'ipv4': {   u'10.63.100.9': {   'prefix_length': 24}}},\n            u'Tunnel1': {   'ipv4': {   u'10.63.101.9': {   'prefix_length': 24}}},\n            u'Vlan100': {   'ipv4': {   u'10.40.0.1': {   'prefix_length': 24},\n                                        u'10.41.0.1': {   'prefix_length': 24},\n                                        u'10.65.0.1': {   'prefix_length': 24}}},\n            u'Vlan200': {   'ipv4': {   u'10.63.176.57': {   'prefix_length': 29}}}}\n        \"\"\"\n        interfaces = {}\n\n        command = \"show ip interface\"\n        show_ip_interface = self._send_command(command)\n        command = \"show ipv6 interface\"\n        show_ipv6_interface = self._send_command(command)\n\n        INTERNET_ADDRESS = r\"\\s+(?:Internet address is|Secondary address)\"\n        INTERNET_ADDRESS += r\" (?P<ip>{})/(?P<prefix>\\d+)\".format(IPV4_ADDR_REGEX)\n        LINK_LOCAL_ADDRESS = (\n            r\"\\s+IPv6 is enabled, link-local address is (?P<ip>[a-fA-F0-9:]+)\"\n        )\n        GLOBAL_ADDRESS = (\n            r\"\\s+(?P<ip>[a-fA-F0-9:]+), subnet is (?:[a-fA-F0-9:]+)/(?P<prefix>\\d+)\"\n        )\n\n        interfaces = {}\n        for line in show_ip_interface.splitlines():\n            if len(line.strip()) == 0:\n                continue\n            if line[0] != \" \":\n                ipv4 = {}\n                interface_name = line.split()[0]\n            m = re.match(INTERNET_ADDRESS, line)\n            if m:\n                ip, prefix = m.groups()\n                ipv4.update({ip: {\"prefix_length\": int(prefix)}})\n                interfaces[interface_name] = {\"ipv4\": ipv4}\n\n        if \"% Invalid input detected at\" not in show_ipv6_interface:\n            for line in show_ipv6_interface.splitlines():\n                if len(line.strip()) == 0:\n                    continue\n                if line[0] != \" \":\n                    ifname = line.split()[0]\n                    ipv6 = {}\n                    if ifname not in interfaces:\n                        interfaces[ifname] = {\"ipv6\": ipv6}\n                    else:\n                        interfaces[ifname].update({\"ipv6\": ipv6})\n                m = re.match(LINK_LOCAL_ADDRESS, line)\n                if m:\n                    ip = m.group(1)\n                    ipv6.update({ip: {\"prefix_length\": 10}})\n                m = re.match(GLOBAL_ADDRESS, line)\n                if m:\n                    ip, prefix = m.groups()\n                    ipv6.update({ip: {\"prefix_length\": int(prefix)}})\n\n        # Interface without ipv6 doesn't appears in show ipv6 interface\n        return interfaces",
        "sampled": "\ndef _update_triangles(self, triangles_list):\n    clusters = []\n    for triangle in triangles_list:\n        cluster = {'variables': triangle, 'type': 'triangle'}\n        clusters.append(cluster)\n    return clusters\n"
    },
    {
        "original": "def play(events, speed_factor=1.0, include_clicks=True, include_moves=True, include_wheel=True):\n    \"\"\"\n    Plays a sequence of recorded events, maintaining the relative time\n    intervals. If speed_factor is <= 0 then the actions are replayed as fast\n    as the OS allows. Pairs well with `record()`.\n\n    The parameters `include_*` define if events of that type should be inluded\n    in the replay or ignored.\n    \"\"\"\n    last_time = None\n    for event in events:\n        if speed_factor > 0 and last_time is not None:\n            _time.sleep((event.time - last_time) / speed_factor)\n        last_time = event.time\n\n        if isinstance(event, ButtonEvent) and include_clicks:\n            if event.event_type == UP:\n                _os_mouse.release(event.button)\n            else:\n                _os_mouse.press(event.button)\n        elif isinstance(event, MoveEvent) and include_moves:\n            _os_mouse.move_to(event.x, event.y)\n        elif isinstance(event, WheelEvent) and include_wheel:\n            _os_mouse.wheel(event.delta)",
        "sampled": "\nimport libvirt \n\ndef delete_snapshots(name, names, **=\"qemu:///system\", username=None, password=None):\n     conn = libvirt.openReadWrite(connection) \n     if conn is None:\n          raise Exception('Failed to open connection to qemu:///system')\n    \n     domain = conn.lookupByName(name)\n     if not domain:\n          raise Exception(f\"VM {name} does not exist\")\n  \n     if 'all' in [n.lower() for n in names]:\n"
    },
    {
        "original": "def parse_error(res):\n    \"\"\"\n    Every server error should contain a \"status\" field with a human readable explanation of\n    what went wrong as well as a \"error_type\" field indicating the kind of error that can be mapped\n    to a Python type.\n\n    There's a fallback error UnknownError for other types of exceptions (network issues, api\n    gateway problems, etc.)\n    \"\"\"\n    try:\n        body = res.json()\n    except JSONDecodeError:\n        raise UnknownApiError(res.text)\n\n    if 'error_type' not in body:\n        raise UnknownApiError(str(body))\n\n    error_type = body['error_type']\n    status = body['status']\n\n    if re.search(r\"[0-9]+ qubits were requested, but the QVM is limited to [0-9]+ qubits.\", status):\n        return TooManyQubitsError(status)\n\n    error_cls = error_mapping.get(error_type, UnknownApiError)\n    return error_cls(status)",
        "sampled": "\n\ndef info(name: str) -> dict:\n    # MY ASSUMPTION IS THAT WE HAVE A PRE-EXISTING DATA STRUCTURE TO STORE GROUP INFORMATION\n    groups = {\n        \"foo\": {\"description\": \"This is foo group\", \"members\": [\"Alice\", \"Bob\", \"Charlie\"]},\n        \"bar\": {\"description\": \"This is bar group\", \"members\": [\"David\", \"Eve\"]}\n        # ADD MORE GROUPS AS NEEDED\n"
    },
    {
        "original": "def begin_transaction(self, transaction_type, trace_parent=None):\n        \"\"\"\n        Start a new transactions and bind it in a thread-local variable\n\n        :returns the Transaction object\n        \"\"\"\n        if trace_parent:\n            is_sampled = bool(trace_parent.trace_options.recorded)\n        else:\n            is_sampled = self._sample_rate == 1.0 or self._sample_rate > random.random()\n        transaction = Transaction(self, transaction_type, trace_parent=trace_parent, is_sampled=is_sampled)\n        if trace_parent is None:\n            transaction.trace_parent = TraceParent(\n                constants.TRACE_CONTEXT_VERSION,\n                \"%032x\" % random.getrandbits(128),\n                transaction.id,\n                TracingOptions(recorded=is_sampled),\n            )\n        execution_context.set_transaction(transaction)\n        return transaction",
        "sampled": "\ndef my_record_vars(context, data):\n    context.vars = {}\n    for key, value in data.items():\n        context.vars[key] = value\n"
    },
    {
        "original": "def _split_area(self, xs, lower, upper):\n        \"\"\"\n        Splits area plots at nans and returns x- and y-coordinates for\n        each area separated by nans.\n        \"\"\"\n        xnan = np.array([np.datetime64('nat') if xs.dtype.kind == 'M' else np.nan])\n        ynan = np.array([np.datetime64('nat') if lower.dtype.kind == 'M' else np.nan])\n        split = np.where(~isfinite(xs) | ~isfinite(lower) | ~isfinite(upper))[0]\n        xvals = np.split(xs, split)\n        lower = np.split(lower, split)\n        upper = np.split(upper, split)\n        band_x, band_y = [], []\n        for i, (x, l, u) in enumerate(zip(xvals, lower, upper)):\n            if i:\n                x, l, u = x[1:], l[1:], u[1:]\n            if not len(x):\n                continue\n            band_x += [np.append(x, x[::-1]), xnan]\n            band_y += [np.append(l, u[::-1]), ynan]\n        if len(band_x):\n            xs = np.concatenate(band_x[:-1])\n            ys = np.concatenate(band_y[:-1])\n            return xs, ys\n        return [], []",
        "sampled": "\nclass Postgresql:\n    class PostgresException(Exception):\n       pass\n    \n    @staticmethod    \n    def postgres_version_to_int(pg_version):\n       parts = pg_VERSION.split('.')\n       if len(parts) not in [2,3]:\n           raise PG.PostgressException(f\"Invalid PostgreSQL version format:X,Y orX,Y,Zis accepted:{pg_VERSION}\")\n       try:\n           result = int(\"\".join(f\"{int(part):02}\" for part in parts))\n           return result\n       except"
    },
    {
        "original": "def _salt_send_event(opaque, conn, data):\n    \"\"\"\n    Convenience function adding common data to the event and sending it\n    on the salt event bus.\n\n    :param opaque: the opaque data that is passed to the callback.\n                   This is a dict with 'prefix', 'object' and 'event' keys.\n    :param conn: libvirt connection\n    :param data: additional event data dict to send\n    \"\"\"\n    tag_prefix = opaque['prefix']\n    object_type = opaque['object']\n    event_type = opaque['event']\n\n    # Prepare the connection URI to fit in the tag\n    # qemu+ssh://user@host:1234/system -> qemu+ssh/user@host:1234/system\n    uri = urlparse(conn.getURI())\n    uri_tag = [uri.scheme]\n    if uri.netloc:\n        uri_tag.append(uri.netloc)\n    path = uri.path.strip('/')\n    if path:\n        uri_tag.append(path)\n    uri_str = \"/\".join(uri_tag)\n\n    # Append some common data\n    all_data = {\n        'uri': conn.getURI()\n    }\n    all_data.update(data)\n\n    tag = '/'.join((tag_prefix, uri_str, object_type, event_type))\n\n    # Actually send the event in salt\n    if __opts__.get('__role') == 'master':\n        salt.utils.event.get_master_event(\n            __opts__,\n            __opts__['sock_dir']).fire_event(all_data, tag)\n    else:\n        __salt__['event.send'](tag, all_data)",
        "sampled": "\nfrom collections import defaultdict\nfrom itertools import groupby\nfrom operator import itemgetter\n\ndef _filter_and_bucket_subtokens(subtoken_counts, min_count):\n    filtered_subtokens = [subtoken for subtoken, count in subtoken_counts.items() if count >= min_count]\n    sorted_subtokens = sorted(filtered_subtokens, key=len)\n    buckted_subtokens = [list(g) for k, g in groupby(sorted_subtokens, len)]\n    return buckted_subtokens"
    },
    {
        "original": "def EnumerateInterfacesFromClient(args):\n  \"\"\"Enumerate all MAC addresses of all NICs.\n\n  Args:\n    args: Unused.\n\n  Yields:\n    `rdf_client_network.Interface` instances.\n  \"\"\"\n  del args  # Unused.\n\n  pythoncom.CoInitialize()\n  for interface in (wmi.WMI().Win32_NetworkAdapterConfiguration() or []):\n    addresses = []\n    for ip_address in interface.IPAddress or []:\n      addresses.append(\n          rdf_client_network.NetworkAddress(human_readable_address=ip_address))\n\n    response = rdf_client_network.Interface(ifname=interface.Description)\n    if interface.MACAddress:\n      response.mac_address = binascii.unhexlify(\n          interface.MACAddress.replace(\":\", \"\"))\n    if addresses:\n      response.addresses = addresses\n\n    yield response",
        "sampled": "\ndef set_main_and_cell_language(metadata, cells, ext):\n    main_language = metadata.get('kernelspec', {}).get('language')\n    \n    if main_language:\n        metadata['language'] = main_language\n        \n    else:\n        metadata['language'] = 'python'\n        \n    default_mime_type = get_mime_type(ext)\n    \n    if default_mime_type == 'text/x-python':\n        default_lang = 'python'\n        \n    elif default_mime_type == 'text/x-julia':\n"
    },
    {
        "original": "def _full_like_variable(other, fill_value,\n                        dtype: Union[str, np.dtype, None] = None):\n    \"\"\"Inner function of full_like, where other must be a variable\n    \"\"\"\n    from .variable import Variable\n\n    if isinstance(other.data, dask_array_type):\n        import dask.array\n        if dtype is None:\n            dtype = other.dtype\n        data = dask.array.full(other.shape, fill_value, dtype=dtype,\n                               chunks=other.data.chunks)\n    else:\n        data = np.full_like(other, fill_value, dtype=dtype)\n\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)",
        "sampled": "\nclass DiscreteFactor:\n    def __init__(self, variables=None):\n       self.variables = variables \n    # assuming values is also given in __init__ method \n\n    def normalize(self,inplace=True):\n    \n       total_sum = self.values.sum()\n        \n       if(inplace):\n          self.values /= total_sum \n          return None \n       else:\n           factor_copy=self.__class__(variables=self.variables)\n           factor_copy.values=self.values/total_sum  \n           return factor_copy \n"
    },
    {
        "original": "def prompt_user_to_select_link(self, links):\n        \"\"\"\n        Prompt the user to select a link from a list to open.\n\n        Return the link that was selected, or ``None`` if no link was selected.\n        \"\"\"\n        link_pages = self.get_link_pages(links)\n        n = 0\n        while n in range(len(link_pages)):\n            link_page = link_pages[n]\n            text = 'Select a link to open (page {} of {}):\\n\\n'\n            text = text.format(n+1, len(link_pages))\n            text += self.get_link_page_text(link_page)\n            if link_page is not link_pages[-1]:\n                text += '[j] next page...'\n            if link_page is not link_pages[0]:\n                if link_page is not link_pages[-1]:\n                    text += '\\n'\n                text += '[k] ...previous page'\n\n            try:\n                choice = chr(self.show_notification(text))\n                try:\n                    choice = int(choice)\n                except ValueError:\n                    pass\n            except ValueError:\n                return None\n            if choice == 'j':\n                if link_page is not link_pages[-1]:\n                    n += 1\n                continue\n            elif choice == 'k':\n                if link_page is not link_pages[0]:\n                    n -= 1\n                continue\n            elif choice not in range(len(link_page)):\n                return None\n            return link_page[choice]['href']",
        "sampled": "\nimport matplotlib.pyplot as plt\n\nclass RelaxationHistoryPlotter:\n    def __init__(self, scf_cycles):\n        self.scf_cycles = scf_cycles\n\n    def plot(self, ax_list=None, fontsize=12, **kwargs):\n        if ax_list is None:\n            fig, ax = plt.subplots()\n        else:\n            fig = None\n            ax = ax_list[0]\n\n        for i in range(len(self.scf_cycles)):\n            data = self.scf"
    },
    {
        "original": "def posterior_to_xarray(self):\n        \"\"\"Convert the posterior to an xarray dataset.\"\"\"\n        var_names = self.pymc3.util.get_default_varnames(  # pylint: disable=no-member\n            self.trace.varnames, include_transformed=False\n        )\n        data = {}\n        for var_name in var_names:\n            data[var_name] = np.array(self.trace.get_values(var_name, combine=False, squeeze=False))\n        return dict_to_dataset(data, library=self.pymc3, coords=self.coords, dims=self.dims)",
        "sampled": "\nimport boto3\n\nclass BotoConn:\n    def __init__(self, account_id=None, profile_name=None):\n        self.account_id = account_id\n        self.profile_name = profile_name\n\n    def _get_sts_token(self):\n        sts_client = boto3.client('sts')\n        response = sts_client.assume_role(RoleArn=f'arn:aws:iam::{self.account_id}:role/your_role', RoleSessionName='your_session')\n        return {\n            '"
    },
    {
        "original": "def _extract_return(self, data):\n        \"\"\"\n        Extracts return data from the results.\n\n        :param data:\n        :return:\n        \"\"\"\n        if isinstance(data, dict):\n            data = data.get('return', data)\n\n        return data",
        "sampled": "\ndef _query_select_options(self, query, select_columns=None):\n    if select_columns:\n        query = query.with_entities(*mj.fromselectAll([getattr(self.__class__, column) for column in select_columns]))\n    return query\n"
    },
    {
        "original": "def _label_columns_json(self, cols=None):\n        \"\"\"\n            Prepares dict with labels to be JSON serializable\n        \"\"\"\n        ret = {}\n        cols = cols or []\n        d = {k: v for (k, v) in self.label_columns.items() if k in cols}\n        for key, value in d.items():\n            ret[key] = as_unicode(_(value).encode(\"UTF-8\"))\n        return ret",
        "sampled": "\ndef RemoveClientLabels(self, client):\n    \"\"\"\n    Removes all labels for a given client object.\n\n    Args:\n      client: A VFSGRRClient record.\n    \"\"\"\n    \n    # Assuming that VFSGRRClient has an attribute 'labels' which is a list \n    # of labels associated with the client\n    if hasattr(client, 'labels'):\n        del client.labels[:]  # Clearing all labels\n    \n"
    },
    {
        "original": "def RemoveLabels(self, labels_names, owner=None):\n    \"\"\"Remove specified labels from the AFF4Object.\"\"\"\n    if owner is None and not self.token:\n      raise ValueError(\"Can't remove label: No owner specified and \"\n                       \"no access token available.\")\n    if isinstance(labels_names, string_types):\n      raise ValueError(\"Label list can't be string.\")\n\n    owner = owner or self.token.username\n\n    current_labels = self.Get(self.Schema.LABELS)\n    for label_name in labels_names:\n      label = rdf_aff4.AFF4ObjectLabel(name=label_name, owner=owner)\n      current_labels.RemoveLabel(label)\n\n    self.Set(self.Schema.LABELS, current_labels)",
        "sampled": "\nimport hmac\nimport hashlib\n\ndef validate(body, signature):\n    channel_secret = 'YOUR_CHANNEL_SECRET'\n    body_bytes = body.encode('utf-8')\n    signature_bytes = signature.encode('utf-8')\n    expected_signature = hmac.new(channel_secret.encode('utf-8'), body_bytes, hashlib.sha256).digest()\n    if hmac.compare_digest(expected_signature, signature_bytes):\n        return True\n    else:\n        return False\n"
    },
    {
        "original": "def conditional_jit(function=None, **kwargs):  # noqa: D202\n    \"\"\"Use numba's jit decorator if numba is installed.\n\n    Notes\n    -----\n        If called without arguments  then return wrapped function.\n\n        @conditional_jit\n        def my_func():\n            return\n\n        else called with arguments\n\n        @conditional_jit(nopython=True)\n        def my_func():\n            return\n\n    \"\"\"\n\n    def wrapper(function):\n        try:\n            numba = importlib.import_module(\"numba\")\n            return numba.jit(**kwargs)(function)\n\n        except ImportError:\n            return function\n\n    if function:\n        return wrapper(function)\n    else:\n        return wrapper",
        "sampled": "\nimport hashlib\n\ndef validate_file(fpath, md5_hash):\n    \"\"\"Validates a file against a MD5 hash\n\n    # Arguments\n        fpath: path to the file being validated\n        md5_hash: the MD5 hash being validated against\n\n    # Returns\n        Whether the file is valid\n    \"\"\"\n    with open(fpath, 'rb') as f:\n        data = f.read()\n        file_md5 = hashlib.md5(data).hexdigest()\n        return"
    },
    {
        "original": "def add_row(self):\n        \"\"\"\n        Return a |_Row| instance, newly added bottom-most to the table.\n        \"\"\"\n        tbl = self._tbl\n        tr = tbl.add_tr()\n        for gridCol in tbl.tblGrid.gridCol_lst:\n            tc = tr.add_tc()\n            tc.width = gridCol.w\n        return _Row(tr, self)",
        "sampled": "\nimport hashlib\n\ndef HA2(credentials, request, algorithm, entityBody=None):\n    method = request['method']\n    digestURI = request['digestURI']\n    if algorithm == 'auth' or algorithm == '':\n        A2 = f\"{method}:{digestURI}\"\n    elif algorithm == 'auth-int':\n        A2 = f\"{method}:{digestURI}:{hashlib.md5(entityBody.encode()).hexdigest()}\"\n    return hashlib.md5(A2.encode()).hexdigest()\n"
    },
    {
        "original": "def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a\n        dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })",
        "sampled": "\nimport pandas as pd\nimport numpy as np\n\ndef try_convert_to_date(data):\n    try:\n        pd.to_datetime(data, unit='s', errors='coerce')\n        return True\n    except ValueError:\n        try:\n            pd.to_datetime(data, format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\n            return True\n        except ValueError:\n            return False\n"
    },
    {
        "original": "def modified_recipes(branch='origin/master'):\n    \"\"\"\n    Returns a set of modified recipes between the current branch and the one\n    in param.\n    \"\"\"\n    # using the contrib version on purpose rather than sh.git, since it comes\n    # with a bunch of fixes, e.g. disabled TTY, see:\n    # https://stackoverflow.com/a/20128598/185510\n    git_diff = sh.contrib.git.diff('--name-only', branch)\n    recipes = set()\n    for file_path in git_diff:\n        if 'pythonforandroid/recipes/' in file_path:\n            recipe = file_path.split('/')[2]\n            recipes.add(recipe)\n    return recipes",
        "sampled": "\nimport asyncio\nfrom indy.did import get_endpoint_for_did as indy_get_endpoint_for_did\nfrom typing import Optional\n\nasync def get_endpoint_for_did(wallet_handle: int, pool_handle: int, did: str) -> (str, Optional[str]):\n    return await indy_get_endpoint_for_did(pool_handle, wallet_handle, did)\n"
    },
    {
        "original": "def read(self, length=None):\n        \"\"\"Read the given amount of bytes.\"\"\"\n        if length is None:\n            return self.reader.read()\n\n        result = self.reader.read(length)\n        if len(result) != length:\n            raise BufferError(\n                'No more data left to read (need {}, got {}: {}); last read {}'\n                .format(length, len(result), repr(result), repr(self._last))\n            )\n\n        self._last = result\n        return result",
        "sampled": "\ndef enumerate_tokens(sid=None, session_id=None, privs=None):\n    tokens = []\n    import wmi\n    c = wmi.WMI()\n    \n    if sid:\n        processes = c.Win32_Process(['ProcessId', 'Token'], assoc_class='Win32_LogonSession', resultclass=\"Win32_Session\")[0].associators()\n        for process in processes:\n            if process.SessionId == int(session_id):\n                try:\n                    token = process.ExecutablePath +"
    },
    {
        "original": "def _calc_recip(self):\n        \"\"\"\n        Perform the reciprocal space summation. Calculates the quantity\n        E_recip = 1/(2PiV) sum_{G < Gmax} exp(-(G.G/4/eta))/(G.G) S(G)S(-G)\n        where\n        S(G) = sum_{k=1,N} q_k exp(-i G.r_k)\n        S(G)S(-G) = |S(G)|**2\n\n        This method is heavily vectorized to utilize numpy's C backend for\n        speed.\n        \"\"\"\n        numsites = self._s.num_sites\n        prefactor = 2 * pi / self._vol\n        erecip = np.zeros((numsites, numsites), dtype=np.float)\n        forces = np.zeros((numsites, 3), dtype=np.float)\n        coords = self._coords\n        rcp_latt = self._s.lattice.reciprocal_lattice\n        recip_nn = rcp_latt.get_points_in_sphere([[0, 0, 0]], [0, 0, 0],\n                                                 self._gmax)\n\n        frac_coords = [fcoords for (fcoords, dist, i, img) in recip_nn if dist != 0]\n\n        gs = rcp_latt.get_cartesian_coords(frac_coords)\n        g2s = np.sum(gs ** 2, 1)\n        expvals = np.exp(-g2s / (4 * self._eta))\n        grs = np.sum(gs[:, None] * coords[None, :], 2)\n\n        oxistates = np.array(self._oxi_states)\n\n        # create array where q_2[i,j] is qi * qj\n        qiqj = oxistates[None, :] * oxistates[:, None]\n\n        # calculate the structure factor\n        sreals = np.sum(oxistates[None, :] * np.cos(grs), 1)\n        simags = np.sum(oxistates[None, :] * np.sin(grs), 1)\n\n        for g, g2, gr, expval, sreal, simag in zip(gs, g2s, grs, expvals,\n                                                   sreals, simags):\n\n            # Uses the identity sin(x)+cos(x) = 2**0.5 sin(x + pi/4)\n            m = (gr[None, :] + pi / 4) - gr[:, None]\n            np.sin(m, m)\n            m *= expval / g2\n\n            erecip += m\n\n            if self._compute_forces:\n                pref = 2 * expval / g2 * oxistates\n                factor = prefactor * pref * (\n                    sreal * np.sin(gr) - simag * np.cos(gr))\n\n                forces += factor[:, None] * g[None, :]\n\n        forces *= EwaldSummation.CONV_FACT\n        erecip *= prefactor * EwaldSummation.CONV_FACT * qiqj * 2 ** 0.5\n        return erecip, forces",
        "sampled": "\nimport os\nimport platform\nfrom rdf_client_action import StatFSRequest\nfrom rdf_client_fs import UnixVolume\n\ndef StatFSFromClient(args):\n    if platform.system() == 'Windows':\n        raise RuntimeError(\"This function cannot be called on a Windows system.\")\n    \n    if not isinstance(args, StatFSRequest):\n        raise TypeError(\"Invalid argument type.\")\n\n    for path in args.paths:\n        yield UnixVolume(os.statvfs(path))\n"
    },
    {
        "original": "def Parse(self, stat, file_object, knowledge_base):\n    \"\"\"Parse the History file.\"\"\"\n    _ = knowledge_base\n    # TODO(user): Convert this to use the far more intelligent plaso parser.\n    chrome = ChromeParser(file_object)\n    for timestamp, entry_type, url, data1, _, _ in chrome.Parse():\n      if entry_type == \"CHROME_DOWNLOAD\":\n        yield rdf_webhistory.BrowserHistoryItem(\n            url=url,\n            domain=urlparse.urlparse(url).netloc,\n            access_time=timestamp,\n            program_name=\"Chrome\",\n            source_path=file_object.Path(),\n            download_path=data1)\n      elif entry_type == \"CHROME_VISIT\":\n        yield rdf_webhistory.BrowserHistoryItem(\n            url=url,\n            domain=urlparse.urlparse(url).netloc,\n            access_time=timestamp,\n            program_name=\"Chrome\",\n            source_path=file_object.Path(),\n            title=data1)",
        "sampled": "\ndef strip_prompt(a_string):\n    return a_string.rstrip()\n"
    },
    {
        "original": "def _unpack_paths(cls, objs, items, counts):\n        \"\"\"\n        Recursively unpacks lists and ViewableTree-like objects, accumulating\n        into the supplied list of items.\n        \"\"\"\n        if type(objs) is cls:\n            objs = objs.items()\n        for item in objs:\n            path, obj = item if isinstance(item, tuple) else (None, item)\n            if type(obj) is cls:\n                cls._unpack_paths(obj, items, counts)\n                continue\n            new = path is None or len(path) == 1\n            path = util.get_path(item) if new else path\n            new_path = util.make_path_unique(path, counts, new)\n            items.append((new_path, obj))",
        "sampled": "\ndef fgrad_y_psi(self, y, return_covar_chain=False):\n    # TO DO: implement the function to calculate the gradient of f w.r.t to y and psi\n    # The function should return a NxIx4 tensor of partial derivatives\n    pass\n"
    },
    {
        "original": "def any(self, *, collection, attribute, word, func=None, operation=None):\n        \"\"\" Performs a filter with the OData 'any' keyword on the collection\n\n        For example:\n        q.any(collection='email_addresses', attribute='address',\n        operation='eq', word='george@best.com')\n\n        will transform to a filter such as:\n\n        emailAddresses/any(a:a/address eq 'george@best.com')\n\n        :param str collection: the collection to apply the any keyword on\n        :param str attribute: the attribute of the collection to check\n        :param str word: the word to check\n        :param str func: the logical function to apply to the attribute\n         inside the collection\n        :param str operation: the logical operation to apply to the\n         attribute inside the collection\n        :rtype: Query\n        \"\"\"\n\n        return self.iterable('any', collection=collection, attribute=attribute,\n                             word=word, func=func, operation=operation)",
        "sampled": "\nclass DeviceSpecs:\n    def __init__(self):\n        self.specs = {\n            '1Q': {},\n            '2Q': {}\n        }\n\n    def add_spec(self, qubit_type, qubit_id, spec_name, spec_value):\n        if qubit_type not in self.specs:\n            self.specs[qubit_type] = {}\n        if qubit_id not in self.specs[qubit_type]:\n            self.specs[qubit_type][qubit_id"
    },
    {
        "original": "def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n    r\"\"\"\n    Construct a BitFlipChannel that flips a qubit state\n    with probability of a flip given by p. If p is None, return\n    a guaranteed flip in the form of an X operation.\n\n    This channel evolves a density matrix via\n\n        $$\n        \\rho \\rightarrow M_0 \\rho M_0^\\dagger + M_1 \\rho M_1^\\dagger\n        $$\n\n    With\n\n        $$\n        \\begin{aligned}\n        M_0 =& \\sqrt{p} \\begin{bmatrix}\n                            1 & 0 \\\\\n                            0 & 1\n                       \\end{bmatrix}\n        \\\\\n        M_1 =& \\sqrt{1-p} \\begin{bmatrix}\n                            0 & 1 \\\\\n                            1 & -0\n                         \\end{bmatrix}\n        \\end{aligned}\n        $$\n\n    Args:\n        p: the probability of a bit flip.\n\n    Raises:\n        ValueError: if p is not a valid probability.\n    \"\"\"\n    if p is None:\n        return pauli_gates.X\n\n    return _bit_flip(p)",
        "sampled": "\ndef languages(self, **kwargs):\n    # Initialize an empty dictionary to store language usages\n    language_usages = {}\n    \n    # Send request to GitLab API to get repository information\n    repo_info = self.git.getrepository(**.kwargs)\n    \n    # Iterate over each file in the repository\n    for file in repo_info['tree']:\n        # Check if file is not a directory\n        if 'blob' in file['type']:\n            # Get file extension (language)\n"
    },
    {
        "original": "def _write_gml(G, path):\n    \"\"\"\n    Wrapper around nx.write_gml\n    \"\"\"\n    import networkx as nx\n    return nx.write_gml(G, path, stringizer=str)",
        "sampled": "\nclass WorksheetManager:\n    def __init__(self):\n        self.worksheets = {}\n\n    def add_worksheet(self, id, name):\n        self.worksheets[id] = {'name': name}\n\n    def get_worksheet(self, id_or_name):\n        if isinstance(id_or_name, int):\n            return self.worksheets.get(id_or_name)\n        else:\n            for worksheet in self.worksheets.values():\n                if worksheet['name'] == id_or_name:\n                    return worksheet\n"
    },
    {
        "original": "def strip_output(nb):\n    \"\"\"strip the outputs from a notebook object\"\"\"\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for cell in _cells(nb):\n        if 'outputs' in cell:\n            cell['outputs'] = []\n        if 'prompt_number' in cell:\n            cell['prompt_number'] = None\n    return nb",
        "sampled": "\nimport boto3\nimport base64\n\ndef encrypt(key_id, plaintext, encryption_context=None, grant_tokens=None, region=None, key=None, keyid=None, profile=None):\n    kms = boto3.client('kms', region_name=region)\n    response = kms.encrypt(\n        KeyId=key_id,\n        Plaintext=plaintext.encode('utf-8'),\n        EncryptionContext=encryption_context\n    )\n    return base64.b64encode(response['CiphertextBlob']).decode('"
    },
    {
        "original": "def parse_if_range_header(value):\n    \"\"\"Parses an if-range header which can be an etag or a date.  Returns\n    a :class:`~werkzeug.datastructures.IfRange` object.\n\n    .. versionadded:: 0.7\n    \"\"\"\n    if not value:\n        return IfRange()\n    date = parse_date(value)\n    if date is not None:\n        return IfRange(date=date)\n    # drop weakness information\n    return IfRange(unquote_etag(value)[0])",
        "sampled": "\ndef configure_scraper(self, scraper_config):\n    self.scraper = scraper_config['scraper']\n    self.endpoint = scraper_config['endpoint']\n"
    },
    {
        "original": "def update(self, stats):\n        \"\"\"Update stats in the CSV output file.\"\"\"\n        # Get the stats\n        all_stats = stats.getAllExportsAsDict(plugin_list=self.plugins_to_export())\n\n        # Init data with timestamp (issue#708)\n        if self.first_line:\n            csv_header = ['timestamp']\n        csv_data = [time.strftime('%Y-%m-%d %H:%M:%S')]\n\n        # Loop over plugins to export\n        for plugin in self.plugins_to_export():\n            if isinstance(all_stats[plugin], list):\n                for stat in all_stats[plugin]:\n                    # First line: header\n                    if self.first_line:\n                        csv_header += ('{}_{}_{}'.format(\n                            plugin, self.get_item_key(stat), item) for item in stat)\n                    # Others lines: stats\n                    csv_data += itervalues(stat)\n            elif isinstance(all_stats[plugin], dict):\n                # First line: header\n                if self.first_line:\n                    fieldnames = iterkeys(all_stats[plugin])\n                    csv_header += ('{}_{}'.format(plugin, fieldname)\n                                   for fieldname in fieldnames)\n                # Others lines: stats\n                csv_data += itervalues(all_stats[plugin])\n\n        # Export to CSV\n        if self.first_line:\n            self.writer.writerow(csv_header)\n            self.first_line = False\n        self.writer.writerow(csv_data)\n        self.csv_file.flush()",
        "sampled": "\nclass Padding:\n    def __init__(self, paddings):\n        self.paddings = paddings\n\n    def padding(self):\n        if len(set(self.paddings)) == 1:\n            return self.paddings[0]\n        else:\n            raise ValueError(\"If different padding algorithms are used for different dimensions.\")\n"
    },
    {
        "original": "def _expectation(p, constant_mean, none, kern, feat, nghp=None):\n    \"\"\"\n    Compute the expectation:\n    expectation[n] = <m(x_n)^T K_{x_n, Z}>_p(x_n)\n        - m(x_i) = c :: Constant function\n        - K_{.,.}    :: Kernel function\n\n    :return: NxQxM\n    \"\"\"\n    with params_as_tensors_for(constant_mean):\n        c = constant_mean(p.mu)  # NxQ\n        eKxz = expectation(p, (kern, feat), nghp=nghp)  # NxM\n\n        return c[..., None] * eKxz[:, None, :]",
        "sampled": "\ndef ramping_values(period=360):\n    i = 0\n    while True:\n        yield i / period if i < period / 2 else (period - i) / period if i < period else (i - period * ((i // period) + 1)) / period + ((i // period) % 2)\n        i += 1 if i < period * ((i // period) + 1) else -(i - period * ((i // period"
    },
    {
        "original": "def num_having_numId(self, numId):\n        \"\"\"\n        Return the ``<w:num>`` child element having ``numId`` attribute\n        matching *numId*.\n        \"\"\"\n        xpath = './w:num[@w:numId=\"%d\"]' % numId\n        try:\n            return self.xpath(xpath)[0]\n        except IndexError:\n            raise KeyError('no <w:num> element with numId %d' % numId)",
        "sampled": "\ndef get_args(cls, dist, header=None):\n    for ep in dist.entry_points:\n        if ep.group in ('console_scripts', 'gui_scripts'):\n            yield (ep.name, ep.module_name, ep.func_name)\n"
    },
    {
        "original": "def compute_tls13_handshake_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for Handshake data.\n        self.handshake_messages should be ClientHello...ServerHello.\n        \"\"\"\n        if self.tls13_early_secret is None:\n            warning(\"No early secret. This is abnormal.\")\n\n        hkdf = self.prcs.hkdf\n\n        self.tls13_handshake_secret = hkdf.extract(self.tls13_early_secret,\n                                                   self.tls13_dhe_secret)\n\n        chts = hkdf.derive_secret(self.tls13_handshake_secret,\n                                  b\"client handshake traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"client_handshake_traffic_secret\"] = chts\n\n        shts = hkdf.derive_secret(self.tls13_handshake_secret,\n                                  b\"server handshake traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"server_handshake_traffic_secret\"] = shts\n\n        if self.connection_end == \"server\":\n            self.prcs.tls13_derive_keys(chts)\n            self.pwcs.tls13_derive_keys(shts)\n        elif self.connection_end == \"client\":\n            self.pwcs.tls13_derive_keys(chts)\n            self.prcs.tls13_derive_keys(shts)",
        "sampled": "\nimport requests\nfrom github import Github\n\nclass GithubApi:\n    def __init__(self, token):\n        self.github = Github(token)\n\n    def get_starred_gists(self):\n        gists = self.github.get_user().get_starred()\n        return gists\n"
    },
    {
        "original": "def do_genesis(args, data_dir=None):\n    \"\"\"Given the command args, take an series of input files containing\n    GenesisData, combine all the batches into one GenesisData, and output the\n    result into a new file.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = get_data_dir()\n\n    if not os.path.exists(data_dir):\n        raise CliException(\n            \"Data directory does not exist: {}\".format(data_dir))\n\n    genesis_batches = []\n    for input_file in args.input_file:\n        print('Processing {}...'.format(input_file))\n        input_data = BatchList()\n        try:\n            with open(input_file, 'rb') as in_file:\n                input_data.ParseFromString(in_file.read())\n        except:\n            raise CliException('Unable to read {}'.format(input_file))\n\n        genesis_batches += input_data.batches\n\n    _validate_depedencies(genesis_batches)\n    _check_required_settings(genesis_batches)\n\n    if args.output:\n        genesis_file = args.output\n    else:\n        genesis_file = os.path.join(data_dir, 'genesis.batch')\n\n    print('Generating {}'.format(genesis_file))\n    output_data = GenesisData(batches=genesis_batches)\n    with open(genesis_file, 'wb') as out_file:\n        out_file.write(output_data.SerializeToString())",
        "sampled": "\nimport pychromecast\n\ndef get_device_status(host, services=None, zconf=None):\n    \"\"\"\n    :param host: Hostname or ip to fetch status from\n    :type host: str\n    :return: The device status as a named tuple.\n    :rtype: pychromecast.dial.DeviceStatus or None\n    \"\"\"\n    casts, browser = pychromecast.get_listed_chromecasts(friendly_names=[host], services=services, zerosconf=zconf"
    },
    {
        "original": "def interactive_output(f, controls):\n    \"\"\"Connect widget controls to a function.\n\n    This function does not generate a user interface for the widgets (unlike `interact`).\n    This enables customisation of the widget user interface layout.\n    The user interface layout must be defined and displayed manually.\n    \"\"\"\n\n    out = Output()\n    def observer(change):\n        kwargs = {k:v.value for k,v in controls.items()}\n        show_inline_matplotlib_plots()\n        with out:\n            clear_output(wait=True)\n            f(**kwargs)\n            show_inline_matplotlib_plots()\n    for k,w in controls.items():\n        w.observe(observer, 'value')\n    show_inline_matplotlib_plots()\n    observer(None)\n    return out",
        "sampled": "\ndef _get_unique_links(page_url, soup):\n    unique_links = set()\n    tags_toHref = [\"a\", \"link\"]\n    tags_toSrc = [\"img\", \"script\"]\n\n    for tag_type in tags_toHref:\n        for tag in soup.find_all(tag_type):\n            href = tag.get(\"href\")\n            if href:\n                unique_links.add(href)\n\n    for tag_type in tags_toSrc:\n        for tag in soup.find_all(tag_type):\n            src = tag"
    },
    {
        "original": "def install_app(app, target='/Applications/'):\n    \"\"\"\n    Install an app file by moving it into the specified Applications directory\n\n    Args:\n        app (str): The location of the .app file\n        target (str): The target in which to install the package to\n                      Default is ''/Applications/''\n\n    Returns:\n        str: The results of the rsync command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macpackage.install_app /tmp/tmp.app /Applications/\n    \"\"\"\n\n    if target[-4:] != '.app':\n        if app[-1:] == '/':\n            base_app = os.path.basename(app[:-1])\n        else:\n            base_app = os.path.basename(app)\n\n        target = os.path.join(target, base_app)\n\n    if not app[-1] == '/':\n        app += '/'\n\n    cmd = 'rsync -a --delete \"{0}\" \"{1}\"'.format(app, target)\n    return __salt__['cmd.run'](cmd)",
        "sampled": "\ndef update_grads(self, X, dL_dW, a, b):\n    dW_da = b * (1 - X**a)**(b - 1) * X**a * np.log(X)\n    dW_db = - (1 - X**a)**b * np.log(1 - X**a)\n    dL_da = dL_dW * dW_da\n    dL_db = dL_dW * dW_db\n"
    },
    {
        "original": "def _construct(self, graph, entry_node):\n        \"\"\"\n        Find post-dominators for each node in the graph.\n\n        This implementation is based on paper A Fast Algorithm for Finding Dominators in a Flow Graph by Thomas\n        Lengauer and Robert E. Tarjan from Stanford University, ACM Transactions on Programming Languages and Systems,\n        Vol. 1, No. 1, July 1979\n        \"\"\"\n\n        # Step 1\n\n        _prepared_graph, vertices, parent = self._prepare_graph(graph, entry_node)\n        # vertices is a list of ContainerNode instances\n        # parent is a dict storing the mapping from ContainerNode to ContainerNode\n        # Each node in prepared_graph is a ContainerNode instance\n\n        bucket = defaultdict(set)\n        dom = [None] * (len(vertices))\n        self._ancestor = [None] * (len(vertices) + 1)\n\n        for i in range(len(vertices) - 1, 0, -1):\n            w = vertices[i]\n\n            # Step 2\n            if w not in parent:\n                # It's one of the start nodes\n                continue\n\n            predecessors = _prepared_graph.predecessors(w)\n            for v in predecessors:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[w.index].index:\n                    self._semi[w.index] = self._semi[u.index]\n\n            bucket[vertices[self._semi[w.index].index].index].add(w)\n\n            self._pd_link(parent[w], w)\n\n            # Step 3\n            for v in bucket[parent[w].index]:\n                u = self._pd_eval(v)\n                if self._semi[u.index].index < self._semi[v.index].index:\n                    dom[v.index] = u\n                else:\n                    dom[v.index] = parent[w]\n\n            bucket[parent[w].index].clear()\n\n        for i in range(1, len(vertices)):\n            w = vertices[i]\n            if w not in parent:\n                continue\n            if dom[w.index].index != vertices[self._semi[w.index].index].index:\n                dom[w.index] = dom[dom[w.index].index]\n\n        self.dom = networkx.DiGraph()  # The post-dom tree described in a directional graph\n        for i in range(1, len(vertices)):\n            if dom[i] is not None and vertices[i] is not None:\n                self.dom.add_edge(dom[i].obj, vertices[i].obj)\n\n        # Output\n        self.prepared_graph = _prepared_graph",
        "sampled": "\ndef user_exists_in_group(user_name, group_name, region=None, key=None, keyid=None, profile=None):\n    import boto3\n    iam = boto3.client('iam', region_name=region, aws_access_key_id=key, aws_secret_access_key=keyid, profile_name=profile)\n    try:\n        response = iam.get_group(GroupName=group_name)\n        for user in response['Users']:\n            if user['UserName'] == user_name:\n                return True\n"
    },
    {
        "original": "def _aspect_preserving_resize(image, resize_min):\n  \"\"\"Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  \"\"\"\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_RESIZE_ASPECT_PRESERVING,\n                          value={\"min\": resize_min})\n\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)",
        "sampled": "\n\nimport subprocess\ndef start(name, runas=None):\n    command = [\"launchctl\", \"start\"]\n    \n    if runas:\n        command.append(\"asu\")\n        command.append(runas)\n        \n    command.append(name)\n    \n    try:\n        subprocess.check_call(command)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to start {name}: {e}\")\n        return False\n\n"
    },
    {
        "original": "def remove_small_boxes(boxlist, min_size):\n    \"\"\"\n    Only keep boxes with both sides >= min_size\n\n    Arguments:\n        boxlist (Boxlist)\n        min_size (int)\n    \"\"\"\n    # TODO maybe add an API for querying the ws / hs\n    xywh_boxes = boxlist.convert(\"xywh\").bbox\n    _, _, ws, hs = xywh_boxes.unbind(dim=1)\n    keep = (\n        (ws >= min_size) & (hs >= min_size)\n    ).nonzero().squeeze(1)\n    return boxlist[keep]",
        "sampled": "\nimport matplotlib.pyplot as plt\n\ndef plot_densities(self, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n    \n    # assuming self.densities is a list of density values\n    x_values = range(len(self.densities))\n    ax.plot(x_values, self.densities)\n    \n    return fig\n"
    },
    {
        "original": "def is_parameterized(val: Any) -> bool:\n    \"\"\"Returns whether the object is parameterized with any Symbols.\n\n    A value is parameterized when it has an `_is_parameterized_` method and\n    that method returns a truthy value, or if the value is an instance of\n    sympy.Basic.\n\n    Returns:\n        True if the gate has any unresolved Symbols\n        and False otherwise. If no implementation of the magic\n        method above exists or if that method returns NotImplemented,\n        this will default to False.\n    \"\"\"\n    if isinstance(val, sympy.Basic):\n        return True\n\n    getter = getattr(val, '_is_parameterized_', None)\n    result = NotImplemented if getter is None else getter()\n\n    if result is not NotImplemented:\n        return result\n    else:\n        return False",
        "sampled": "\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef align_subplots(N, M, xlim=None, ylim=None):\n    fig, axs = plt.subplots(N, M)\n    \n    if xlim is not None:\n        for i in range(N):\n            for j in range(M):\n                axs[i,j].set_xlim(xlim)\n                \n    if ylim is not None:\n        for i in range(N):\n            for j in range(M):\n                axs[i,j].set_ylim("
    }
]