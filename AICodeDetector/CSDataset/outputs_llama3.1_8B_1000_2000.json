[
    {
        "original": "def LessThan(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"less than\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, '<')\n    return self._query_builder",
        "sampled": "```python\nclass QueryBuilder:\n    def __init__(self):\n        self.where_type = None\n        self.where_value = None\n\n    def LessThan(self, value):\n        \"\"\"Sets the type of the WHERE clause as \"less than\".\n\n        Args:\n          value: The value to be used in the WHERE condition.\n\n        Returns:\n          The query builder that this WHERE builder links to.\n        \"\"\"\n        self.where_type = \"less than\"\n        self.where_value = value\n        return"
    },
    {
        "original": "def format_items(x):\n    \"\"\"Returns a succinct summaries of all items in a sequence as strings\"\"\"\n    x = np.asarray(x)\n    timedelta_format = 'datetime'\n    if np.issubdtype(x.dtype, np.timedelta64):\n        x = np.asarray(x, dtype='timedelta64[ns]')\n        day_part = (x[~pd.isnull(x)]\n                    .astype('timedelta64[D]')\n                    .astype('timedelta64[ns]'))\n        time_needed = x[~pd.isnull(x)] != day_part\n        day_needed = day_part != np.timedelta64(0, 'ns')\n        if np.logical_not(day_needed).all():\n            timedelta_format = 'time'\n        elif np.logical_not(time_needed).all():\n            timedelta_format = 'date'\n\n    formatted = [format_item(xi, timedelta_format) for xi in x]\n    return formatted",
        "sampled": "```python\nfrom typing import List, Union\n\ndef format_items(items: List[Union[int, float]]) -> str:\n    \"\"\"\n    Returns a succinct summaries of all items in a sequence as strings\n    \"\"\"\n    \n    if not items:\n        return 'No items'\n    \n    min_val = min(items)\n    max_val = max(items)\n    sum_val = sum(items)\n    \n    num_values = len([x for x in items if x == min(items)]) \n      \n    \n        \n \n        \n"
    },
    {
        "original": "def iter_tag_users(self, tag_id, first_user_id=None):\n        \"\"\"\n        \u83b7\u53d6\u6807\u7b7e\u4e0b\u7c89\u4e1dopenid\u5217\u8868\n\n        :return: \u8fd4\u56de\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u53ef\u4ee5\u7528for\u8fdb\u884c\u5faa\u73af\uff0c\u5f97\u5230openid\n\n        \u4f7f\u7528\u793a\u4f8b::\n\n            from wechatpy import WeChatClient\n\n            client = WeChatClient('appid', 'secret')\n            for openid in client.tag.iter_tag_users(0):\n                print(openid)\n\n        \"\"\"\n        while True:\n            follower_data = self.get_tag_users(tag_id, first_user_id)\n            if 'data' not in follower_data:\n                return\n            for openid in follower_data['data']['openid']:\n                yield openid\n            first_user_id = follower_data.get('next_openid')\n            if not first_user_id:\n                return",
        "sampled": "```python\nfrom typing import Iterator\n\nclass WeChatClient:\n    def __init__(self, appid: str, secret: str):\n        # Initialize the WeChat client with appid and secret\n        self.appid = appid\n        self.secret = secret\n\n    def get_access_token(self) -> str:\n        # Get the access token for API requests\n        # This is a placeholder function and should be replaced with actual implementation\n        return \"access_token\"\n\n    def iter"
    },
    {
        "original": "def start(self):\n    \"\"\"Start listener in a background thread\n\n    Returns:\n      address of the Server as a tuple of (host, port)\n    \"\"\"\n    server_sock = self.start_listening_socket()\n\n    # hostname may not be resolvable but IP address probably will be\n    host = self.get_server_ip()\n    port = server_sock.getsockname()[1]\n    addr = (host, port)\n    logging.info(\"listening for reservations at {0}\".format(addr))\n\n    def _listen(self, sock):\n      CONNECTIONS = []\n      CONNECTIONS.append(sock)\n\n      while not self.done:\n        read_socks, write_socks, err_socks = select.select(CONNECTIONS, [], [], 60)\n        for sock in read_socks:\n          if sock == server_sock:\n            client_sock, client_addr = sock.accept()\n            CONNECTIONS.append(client_sock)\n            logging.debug(\"client connected from {0}\".format(client_addr))\n          else:\n            try:\n              msg = self.receive(sock)\n              self._handle_message(sock, msg)\n            except Exception as e:\n              logging.debug(e)\n              sock.close()\n              CONNECTIONS.remove(sock)\n\n      server_sock.close()\n\n    t = threading.Thread(target=_listen, args=(self, server_sock))\n    t.daemon = True\n    t.start()\n\n    return addr",
        "sampled": "```python\nimport threading\nimport socket\n\nclass Server:\n    def __init__(self, host='127.0.0.1', port=12345):\n        self.host = host\n        self.port = port\n        self.server = None\n\n    def start(self):\n        \"\"\"Start listener in a background thread\n\n        Returns:\n          address of the Server as a tuple of (host, port)\n        \"\"\"\n        \n        # Create a socket object\n        self.server = socket.socket(socket"
    },
    {
        "original": "def tag(self, **tags):\n        \"\"\"\n        Tag this transaction with one or multiple key/value tags. Both the values should be strings\n\n            transaction_obj.tag(key1=\"value1\", key2=\"value2\")\n\n        Note that keys will be dedotted, replacing dot (.), star (*) and double quote (\") with an underscore (_)\n        \"\"\"\n        for key in tags.keys():\n            self.tags[TAG_RE.sub(\"_\", compat.text_type(key))] = encoding.keyword_field(compat.text_type(tags[key]))",
        "sampled": "```python\nclass Transaction:\n    def __init__(self):\n        self.tags = {}\n\n    def tag(self, **tags):\n        \"\"\"\n        Tag this transaction with one or multiple key/value tags. Both the values should be strings\n        \n            transaction_obj.tag(key1=\"value1\", key2=\"value2\")\n        \n        Note that keys will be dedotted, replacing dot (.), star (*) and double quote (\") with an underscore (_)\n        \"\"\"\n        \n        for key, value in tags.items"
    },
    {
        "original": "def _hook(self, hook_name, doc_uri=None, **kwargs):\n        \"\"\"Calls hook_name and returns a list of results from all registered handlers\"\"\"\n        doc = self.workspace.get_document(doc_uri) if doc_uri else None\n        hook_handlers = self.config.plugin_manager.subset_hook_caller(hook_name, self.config.disabled_plugins)\n        return hook_handlers(config=self.config, workspace=self.workspace, document=doc, **kwargs)",
        "sampled": "```python\nimport math\n\nclass Solution:\n    def __init__(self):\n        self.hooks = {}\n\n    def register_hook(self, hook_name, doc_uri=None, func=None):\n        if hook_name not in self.hooks:\n            self.hooks[hook_name] = []\n        if func is not None:\n            self.hooks[hook_name].append(func)\n\n    def _hook(self, hook_name, doc_uri=None):\n        if hook_name in self.hooks:\n            results"
    },
    {
        "original": "def write_metadata(self, symbol, metadata):\n        \"\"\"\n        writes user defined metadata for the given symbol\n\n        Parameters\n        ----------\n        symbol: str\n            symbol for the given item in the DB\n        metadata: ?\n            metadata to write\n        \"\"\"\n        sym = self._get_symbol_info(symbol)\n        if not sym:\n            raise NoDataFoundException(\"Symbol does not exist.\")\n\n        sym[USERMETA] = metadata\n        self._symbols.replace_one({SYMBOL: symbol}, sym)",
        "sampled": "```python\nclass MetadataWriter:\n    def write_metadata(self, symbol, metadata):\n        \"\"\"\n        writes user defined metadata for the given symbol\n\n        Parameters\n        ----------\n        symbol: str\n            symbol for the given item in the DB\n        metadata: dict or object with attributes (e.g. a Python object)\n            metadata to write\n        \"\"\"\n        \n        # Assuming we have a dictionary to store all symbols and their corresponding metadata\n        self.metadata_db = self.metadata_db if hasattr(self"
    },
    {
        "original": "def from_textfile(cls, textfile, workers=1, job_size=1000):\n    \"\"\" Count the set of words appeared in a text file.\n\n    Args:\n      textfile (string): The name of the text file or `TextFile` object.\n      min_count (integer): Minimum number of times a word/token appeared in the document\n                 to be considered part of the vocabulary.\n      workers (integer): Number of parallel workers to read the file simulatenously.\n      job_size (integer): Size of the batch send to each worker.\n      most_frequent (integer): if no min_count is specified, consider the most frequent k words for the vocabulary.\n\n    Returns:\n      A vocabulary of the most frequent words appeared in the document.\n    \"\"\"\n\n    c = Counter()\n    if isinstance(textfile, string_types):\n      textfile = TextFile(textfile)\n    for result in textfile.apply(count, workers, job_size):\n      c.update(result)\n    return CountedVocabulary(word_count=c)",
        "sampled": "```python\nimport re\nfrom collections import Counter\nfrom typing import Dict\n\nclass TextFile:\n    def __init__(self, filename):\n        self.filename = filename\n\n    def count_words(self, min_count: int = None, workers: int = 1, job_size: int = 1000) -> Dict[str, int]:\n        try:\n            with open(self.filename, 'r', encoding='utf-8') as file:\n                text = file.read()\n                words ="
    },
    {
        "original": "def __load_compose_definitions(path, definition):\n    \"\"\"\n    Will load the compose file located at path\n    Then determines the format/contents of the sent definition\n\n    err or results are only set if there were any\n\n    :param path:\n    :param definition:\n    :return tuple(compose_result, loaded_definition, err):\n    \"\"\"\n    compose_result, err = __load_docker_compose(path)\n    if err:\n        return None, None, err\n    if isinstance(definition, dict):\n        return compose_result, definition, None\n    elif definition.strip().startswith('{'):\n        try:\n            loaded_definition = json.deserialize(definition)\n        except json.DeserializationError as jerr:\n            msg = 'Could not parse {0} {1}'.format(definition, jerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    else:\n        try:\n            loaded_definition = yaml.load(definition)\n        except yaml.YAMLError as yerr:\n            msg = 'Could not parse {0} {1}'.format(definition, yerr)\n            return None, None, __standardize_result(False, msg,\n                                              None, None)\n    return compose_result, loaded_definition, None",
        "sampled": "```python\nimport docker\nfrom docker.models.composeservice import Definition\nimport json\n\ndef __load_compose_definitions(path, definition):\n    \"\"\"\n    Will load the compose file located at path\n    Then determines the format/contents of the sent definition\n\n    err or results are only set if there were any\n\n    :param path:\n    :param definition:\n    :return tuple(compose_result, loaded_definition, err):\n    \"\"\"\n\n    # Initialize variables to store results and errors\n   "
    },
    {
        "original": "def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n        endpoint = instance.get('prometheus_url')\n        if endpoint is None:\n            raise ConfigurationError(\"Unable to find prometheus endpoint in config file.\")\n\n        metrics = [DEFAULT_METRICS, GO_METRICS]\n        metrics.extend(instance.get('metrics', []))\n\n        instance.update({'prometheus_url': endpoint, 'namespace': 'coredns', 'metrics': metrics})\n\n        return instance",
        "sampled": "```python\nclass CoreDNSInstance:\n    def _create_core_dns_instance(self, instance):\n        \"\"\"\n        Set up coredns instance with standard settings so it can be used in OpenMetricsBaseCheck\n        # dns server file\n        # Use a TextFileHandler to create the actual configuration file.\n        \n        # Since coredns is being run as part of a pod, this makes it easier to understand and not hard code networks.\n        \n        | Name      | Value |\n|-------------"
    },
    {
        "original": "def open_phdos(self):\n        \"\"\"Open PHDOS file produced by Anaddb and returns :class:`PhdosFile` object.\"\"\"\n        from abipy.dfpt.phonons import PhdosFile\n        phdos_path = os.path.join(self.workdir, \"run.abo_PHDOS.nc\")\n        if not phdos_path:\n            if self.status == self.S_OK:\n                logger.critical(\"%s reached S_OK but didn't produce a PHBST file in %s\" % (self, self.outdir))\n            return None\n\n        try:\n            return PhdosFile(phdos_path)\n        except Exception as exc:\n            logger.critical(\"Exception while reading GSR file at %s:\\n%s\" % (phdos_path, str(exc)))\n            return None",
        "sampled": "```python\nclass PhdosFile:\n    \"\"\"Class representing PHDOS file object.\"\"\"\n\n    def __init__(self, filename):\n        \"\"\"\n        Initialize PHDOS file object.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the PHDOS file.\n        \"\"\"\n        self.filename = filename\n        self.data = self.read_file()\n\n    def read_file(self):\n        \"\"\"\n        Read PHDOS data from file and store it internally.\n\n        Returns\n        -------\n        list"
    },
    {
        "original": "def make_optimize_tensor(self, model, session=None, var_list=None, **kwargs):\n        \"\"\"\n        Make Tensorflow optimization tensor.\n        This method builds optimization tensor and initializes all necessary variables\n        created by optimizer.\n\n            :param model: GPflow model.\n            :param session: Tensorflow session.\n            :param var_list: List of variables for training.\n            :param kwargs: Dictionary of extra parameters passed to Tensorflow\n                optimizer's minimize method.\n            :return: Tensorflow optimization tensor or operation.\n        \"\"\"\n        session = model.enquire_session(session)\n        objective = model.objective\n        full_var_list = self._gen_var_list(model, var_list)\n        # Create optimizer variables before initialization.\n        with session.as_default():\n            minimize = self.optimizer.minimize(objective, var_list=full_var_list, **kwargs)\n            model.initialize(session=session)\n            self._initialize_optimizer(session)\n            return minimize",
        "sampled": "```\nimport tensorflow as tf\nfrom tensorflow import keras\n\nclass GPflowModel:\n    def make_optimize_tensor(self, model, session=None, var_list=None, **kwargs):\n        \"\"\"\n        Make Tensorflow optimization tensor.\n        This method builds optimization tensor and initializes all necessary variables\n        created by optimizer.\n\n        :param model: GPflow model.\n        :param session: Tensorflow session.\n        :param var_list: List of variables for training.\n        :param kwargs: Dictionary of"
    },
    {
        "original": "async def get_me(self, input_peer=False):\n        \"\"\"\n        Gets \"me\" (the self user) which is currently authenticated,\n        or None if the request fails (hence, not authenticated).\n\n        Args:\n            input_peer (`bool`, optional):\n                Whether to return the :tl:`InputPeerUser` version or the normal\n                :tl:`User`. This can be useful if you just need to know the ID\n                of yourself.\n\n        Returns:\n            Your own :tl:`User`.\n        \"\"\"\n        if input_peer and self._self_input_peer:\n            return self._self_input_peer\n\n        try:\n            me = (await self(\n                functions.users.GetUsersRequest([types.InputUserSelf()])))[0]\n\n            self._bot = me.bot\n            if not self._self_input_peer:\n                self._self_input_peer = utils.get_input_peer(\n                    me, allow_self=False\n                )\n\n            return self._self_input_peer if input_peer else me\n        except errors.UnauthorizedError:\n            return None",
        "sampled": "```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    id: int\n    first_name: str\n\n@dataclass\nclass InputPeerUser:\n    user_id: int\n\nasync def get_me(self, input_peer=False):\n    \"\"\"\n    Gets \"me\" (the self user) which is currently authenticated,\n    or None if the request fails (hence, not authenticated).\n\n    Args:\n        input_peer (bool, optional):\n            Whether to return the"
    },
    {
        "original": "def ListChildren(self, urn):\n    \"\"\"Lists children of a given urn. Resulting list is cached.\"\"\"\n    result = self.MultiListChildren([urn])\n    try:\n      return result[urn]\n    except KeyError:\n      return []",
        "sampled": "```python\nclass Solution:\n    def ListChildren(self, urn):\n        # Cache for storing the children of each urn\n        self.urn_cache = {}\n\n        # Base case: If urn is in cache, return its children\n        if urn in self.urn_cache:\n            return self.urn_cache[urn]\n\n        # Initialize an empty list to store the children of the current urn\n        children = []\n\n        # Add the current urn to its own list of children (for recursive calls"
    },
    {
        "original": "def prepare_build_dir(self):\n        \"\"\"Ensure that a build dir exists for the recipe. This same single\n        dir will be used for building all different archs.\"\"\"\n        self.build_dir = self.get_build_dir()\n        self.common_dir = self.get_common_dir()\n        copy_files(join(self.bootstrap_dir, 'build'), self.build_dir)\n        copy_files(join(self.common_dir, 'build'), self.build_dir,\n                   override=False)\n        if self.ctx.symlink_java_src:\n            info('Symlinking java src instead of copying')\n            shprint(sh.rm, '-r', join(self.build_dir, 'src'))\n            shprint(sh.mkdir, join(self.build_dir, 'src'))\n            for dirn in listdir(join(self.bootstrap_dir, 'build', 'src')):\n                shprint(sh.ln, '-s', join(self.bootstrap_dir, 'build', 'src', dirn),\n                        join(self.build_dir, 'src'))\n        with current_directory(self.build_dir):\n            with open('project.properties', 'w') as fileh:\n                fileh.write('target=android-{}'.format(self.ctx.android_api))",
        "sampled": "```python\nimport os\n\nclass Recipe:\n    def __init__(self, build_dir):\n        self.build_dir = build_dir\n\n    def prepare_build_dir(self):\n        \"\"\"Ensure that a build dir exists for the recipe.\n        This same single dir will be used for building all different archs.\"\"\"\n        if not os.path.exists(self.build_dir):\n            os.makedirs(self.build_dir)\n```"
    },
    {
        "original": "def get_ext_outputs(self):\n        \"\"\"Get a list of relative paths to C extensions in the output distro\"\"\"\n\n        all_outputs = []\n        ext_outputs = []\n\n        paths = {self.bdist_dir: ''}\n        for base, dirs, files in sorted_walk(self.bdist_dir):\n            for filename in files:\n                if os.path.splitext(filename)[1].lower() in NATIVE_EXTENSIONS:\n                    all_outputs.append(paths[base] + filename)\n            for filename in dirs:\n                paths[os.path.join(base, filename)] = (paths[base] +\n                                                       filename + '/')\n\n        if self.distribution.has_ext_modules():\n            build_cmd = self.get_finalized_command('build_ext')\n            for ext in build_cmd.extensions:\n                if isinstance(ext, Library):\n                    continue\n                fullname = build_cmd.get_ext_fullname(ext.name)\n                filename = build_cmd.get_ext_filename(fullname)\n                if not os.path.basename(filename).startswith('dl-'):\n                    if os.path.exists(os.path.join(self.bdist_dir, filename)):\n                        ext_outputs.append(filename)\n\n        return all_outputs, ext_outputs",
        "sampled": "```python\nclass Compiler:\n    def get_ext_outputs(self, distro):\n        \"\"\"\n        Get a list of relative paths to C extensions in the output distro\n        \n        Args:\n            distro (dict): The output distro containing information about built C extensions (e.g., their locations)\n        \n        Returns:\n            list: A list of relative paths to C extensions in the output distro\n        \"\"\"\n        \n        # Assuming 'extensions' is a list containing all built C extensions\n       "
    },
    {
        "original": "def auth(username, password):\n    \"\"\"\n    Try and authenticate\n    \"\"\"\n    try:\n        keystone = client.Client(username=username, password=password,\n                                 auth_url=get_auth_url())\n        return keystone.authenticate()\n    except (AuthorizationFailure, Unauthorized):\n        return False",
        "sampled": "```python\n# Define a class User with username and password\nclass User:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n# Initialize an empty user dictionary to store registered users\nusers = {}\n\ndef register(username, password):\n    \"\"\"\n    Register a new user.\n    \n    Args:\n        username (STR): the desired username for the new account.\n        password (STR): the desired initial password for the new account.\n        \n   "
    },
    {
        "original": "def i2m(self, pkt, i):\n        \"\"\"\"Internal\" (IP as bytes, mask as int) to \"machine\" representation.\"\"\"  # noqa: E501\n        mask, ip = i\n        ip = pton_ntop.inet_pton(socket.AF_INET6, ip)\n        return struct.pack(\">B\", mask) + ip[:self.mask2iplen(mask)]",
        "sampled": "```python\nclass Solution:\n    def i2m(self, pkt, i):\n        \"\"\"\n        Internal (IP as bytes, mask as int) to \"machine\" representation.\n        \n        Parameters:\n        pkt (bytes): IP address as bytes\n        i (int): Mask value\n        \n        Returns:\n        tuple: A tuple containing the network and host parts of the IP address\n               in machine representation.\n        \n        \"\"\"\n        \n        # Convert the mask from integer to binary and remove the '0"
    },
    {
        "original": "def get_config(name=None, group_id=None, region=None, key=None, keyid=None,\n               profile=None, vpc_id=None, vpc_name=None):\n    \"\"\"\n    Get the configuration for a security group.\n\n    CLI example::\n\n        salt myminion boto_secgroup.get_config mysecgroup\n    \"\"\"\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    sg = _get_group(conn, name=name, vpc_id=vpc_id, vpc_name=vpc_name,\n                    group_id=group_id, region=region, key=key, keyid=keyid,\n                    profile=profile)\n    if sg:\n        ret = odict.OrderedDict()\n        ret['name'] = sg.name\n        # TODO: add support for vpc_id in return\n        # ret['vpc_id'] = sg.vpc_id\n        ret['group_id'] = sg.id\n        ret['owner_id'] = sg.owner_id\n        ret['description'] = sg.description\n        ret['tags'] = sg.tags\n        _rules = _parse_rules(sg, sg.rules)\n        _rules_egress = _parse_rules(sg, sg.rules_egress)\n        ret['rules'] = _split_rules(_rules)\n        ret['rules_egress'] = _split_rules(_rules_egress)\n        return ret\n    else:\n        return None",
        "sampled": "```python\ndef get_config(name=None, group_id=None, region=None, key=None, keyid=None,\n               profile=None, vpc_id=None, vpc_name=None):\n    \"\"\"\n    Get the configuration for a security group.\n\n    CLI example::\n\n        salt myminion boto_secgroup.get_config mysecgroup\n    \"\"\"\n    \n    # Initialize empty dictionary to store configuration\n    config = {}\n    \n    if name:\n        # Check if a name is provided and store it in the"
    },
    {
        "original": "def add_permission_view_menu(self, permission_name, view_menu_name):\n        \"\"\"\n            Adds a permission on a view or menu to the backend\n\n            :param permission_name:\n                name of the permission to add: 'can_add','can_edit' etc...\n            :param view_menu_name:\n                name of the view menu to add\n        \"\"\"\n        if not (permission_name and view_menu_name):\n            return None\n        pv = self.find_permission_view_menu(\n            permission_name,\n            view_menu_name\n        )\n        if pv:\n            return pv\n        vm = self.add_view_menu(view_menu_name)\n        perm = self.add_permission(permission_name)\n        pv = self.permissionview_model()\n        pv.view_menu, pv.permission = vm, perm\n        try:\n            pv.save()\n            log.info(c.LOGMSG_INF_SEC_ADD_PERMVIEW.format(str(pv)))\n            return pv\n        except Exception as e:\n            log.error(c.LOGMSG_ERR_SEC_ADD_PERMVIEW.format(str(e)))",
        "sampled": "```python\nclass Backend:\n    def __init__(self):\n        # Initialize an empty dictionary to store permissions\n        self.permissions = {}\n\n    def add_permission_view_menu(self, permission_name, view_menu_name):\n        \"\"\"\n            Adds a permission on a view or menu to the backend\n\n            :param permission_name:\n                name of the permission to add: 'can_add','can_edit' etc...\n            :param view_menu_name:\n                name of the view menu to add\n        \"\"\"\n        \n"
    },
    {
        "original": "def set_base_prompt(\n        self, pri_prompt_terminator=\">\", alt_prompt_terminator=\"]\", delay_factor=1\n    ):\n        \"\"\"\n        Sets self.base_prompt\n\n        Used as delimiter for stripping of trailing prompt in output.\n\n        Should be set to something that is general and applies in multiple contexts. For Comware\n        this will be the router prompt with < > or [ ] stripped off.\n\n        This will be set on logging in, but not when entering system-view\n        \"\"\"\n        log.debug(\"In set_base_prompt\")\n        delay_factor = self.select_delay_factor(delay_factor)\n        self.clear_buffer()\n        self.write_channel(self.RETURN)\n        time.sleep(0.5 * delay_factor)\n\n        prompt = self.read_channel()\n        prompt = self.normalize_linefeeds(prompt)\n\n        # If multiple lines in the output take the last line\n        prompt = prompt.split(self.RESPONSE_RETURN)[-1]\n        prompt = prompt.strip()\n\n        # Check that ends with a valid terminator character\n        if not prompt[-1] in (pri_prompt_terminator, alt_prompt_terminator):\n            raise ValueError(\"Router prompt not found: {0}\".format(prompt))\n\n        # Strip off any leading HRP_. characters for USGv5 HA\n        prompt = re.sub(r\"^HRP_.\", \"\", prompt, flags=re.M)\n\n        # Strip off leading and trailing terminator\n        prompt = prompt[1:-1]\n        prompt = prompt.strip()\n        self.base_prompt = prompt\n        log.debug(\"prompt: {0}\".format(self.base_prompt))\n\n        return self.base_prompt",
        "sampled": "```python\nclass NetscalerController:\n    def __init__(self):\n        self.base_prompt = ''\n\n\n    def set_base_prompt(\n            self, pri_prompt_terminator=\">\", alt_prompt_terminator=\"]\", delay_factor=1\n    ):\n        \"\"\"\n        Sets self.base_prompt\n\n        Used as delimiter for stripping of trailing prompt in output.\n\n        Should be set to something that is general and applies in multiple contexts. For Comware\n        this will be the router prompt with "
    },
    {
        "original": "def get_branching_model(self, project, repository):\n        \"\"\"\n        Get branching model\n        :param project:\n        :param repository:\n        :return:\n        \"\"\"\n        url = 'rest/branch-utils/1.0/projects/{project}/repos/{repository}/branchmodel/configuration'.format(\n            project=project,\n            repository=repository)\n        return self.get(url)",
        "sampled": "```python\nclass BranchingModel:\n    def get_branching_model(self, project, repository):\n        \"\"\"\n        Get branching model\n        :param project: project name\n        :param repository: repository name\n        :return: a dictionary representing the branching model\n        \"\"\"\n\n        # Initialize an empty dictionary to store the branching model\n        branching_model = {\"project\": project, \"repository\": repository}\n\n        # Since we don't have any specific information about the project or repository,\n        #"
    },
    {
        "original": "def _get_restartcheck_result(errors):\n    \"\"\"\n    Return restartcheck result and append errors (if any) to ``errors``\n    \"\"\"\n    rs_result = __salt__['restartcheck.restartcheck'](verbose=False)\n    if isinstance(rs_result, dict) and 'comment' in rs_result:\n        errors.append(rs_result['comment'])\n    return rs_result",
        "sampled": "```python\ndef _get_restartcheck_result(errors):\n    \"\"\"\n    Return restartcheck result and append errors (if any) to ``errors``\n    \"\"\"\n    \n    # Assuming the restart check logic is implemented in a separate function\n    def restart_check():\n        # This is a placeholder for the actual logic, you should replace it with your own implementation\n        return True\n    \n    result = restart_check()\n    \n    if not result:\n        errors.append(\"Restart check failed\")\n    \n    return result,"
    },
    {
        "original": "def _find_alphas_param(self):\n        \"\"\"\n        Searches for the parameter on the estimator that contains the array of\n        alphas that was used to produce the error selection. If it cannot find\n        the parameter then a YellowbrickValueError is raised.\n        \"\"\"\n\n        # NOTE: The order of the search is very important!\n        for attr in (\"cv_alphas_\", \"alphas_\", \"alphas\",):\n            try:\n                return getattr(self.estimator, attr)\n            except AttributeError:\n                continue\n\n        raise YellowbrickValueError(\n            \"could not find alphas param on {} estimator\".format(\n                self.estimator.__class__.__name__\n            )\n        )",
        "sampled": "```python\nimport warnings\nfrom abc import ABC, abstractmethod\n\nclass YellowbrickValueError(Exception):\n    pass\n\n\nclass Estimator(ABC):\n    @abstractmethod\n    def fit(self, X, y=None):\n        pass\n\n\nclass Reducer(Estimator):\n    def __init__(self, estimator) -> None:\n        self.estimator = estimator\n\n    def _find_alphas_param(self):\n        \"\"\"\n        Searches for the parameter on the estimator that contains the array of\n        al"
    },
    {
        "original": "def get_scales(scale=None, n=None):\n    \"\"\"\n    Returns a color scale \n\n    Parameters:\n    -----------\n            scale : str\n                    Color scale name\n                    If the color name is preceded by a minus (-) \n                    then the scale is inversed\n            n : int\n                    Number of colors \n                    If n < number of colors available for a given scale then \n                            the minimum number will be returned \n                    If n > number of colors available for a given scale then\n                            the maximum number will be returned \n\n    Example:\n            get_scales('accent',8)\n            get_scales('pastel1')\n    \"\"\"\n    if scale:\n        is_reverse = False\n        if scale[0] == '-':\n            scale = scale[1:]\n            is_reverse = True\n        d = copy.deepcopy(_scales_names[scale.lower()])\n        keys = list(map(int, list(d.keys())))\n        cs = None\n        if n:\n            if n in keys:\n                cs = d[str(n)]\n            elif n < min(keys):\n                cs = d[str(min(keys))]\n        if cs is None:\n            cs = d[str(max(keys))]\n        if is_reverse:\n            cs.reverse()\n        return cs\n    else:\n        d = {}\n        for k, v in list(_scales_names.items()):\n            if isinstance(v, dict):\n                keys = list(map(int, list(v.keys())))\n                d[k] = v[str(max(keys))]\n            else:\n                d[k] = v\n        return d",
        "sampled": "```python\nimport matplotlib.colors as mcolors\n\ndef get_scales(scale=None, n=None):\n    \"\"\"\n    Returns a color scale \n\n    Parameters:\n    -----------\n            scale : str\n                    Color scale name\n                    If the color name is preceded by a minus (-) \n                    then the scale is inversed\n            n : int\n                    Number of colors \n                    If n < number of colors available for a given scale then \n                            the minimum number will be returned \n                    If n"
    },
    {
        "original": "def get_followers(self):\n        \"\"\"\n        :calls: `GET /users/:user/followers <http://developer.github.com/v3/users/followers>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.NamedUser.NamedUser`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            NamedUser,\n            self._requester,\n            self.url + \"/followers\",\n            None\n        )",
        "sampled": "```python\nimport requests\n\nclass GitHub:\n    def __init__(self, api_url='https://api.github.com', token=None):\n        self.api_url = api_url\n        self.token = token\n\n    def set_token(self, token):\n        self.token = token\n\n    def get_followers(self, user):\n        auth = {'Authorization': f'token {self.token}'}\n        response = requests.get(f'{self.api_url}/users/{user}/followers', headers=auth)\n"
    },
    {
        "original": "def copy(self):\n        \"\"\"\n        Create an inactive copy of the client object, suitable for passing\n        to a separate thread.\n\n        Note that the copied connections are not initialized, so :meth:`.reinit`\n        must be called on the returned copy.\n        \"\"\"\n        _conns = self._conns\n        self._conns = {}\n        c = copy.deepcopy(self)\n        self._conns = _conns\n        return c",
        "sampled": "```python\nclass Client:\n    def __init__(self):\n        # Initialize client object with necessary attributes\n        self.connections = []\n        self.active = False\n\n    def reinit(self):\n        # Reinitialize the connections in the client object\n        for connection in self.connections:\n            connection.reinit()\n\n    def copy(self):\n        \"\"\"\n        Create an inactive copy of the client object, suitable for passing\n        to a separate thread.\n\n        Note that the copied connections are not initialized, so"
    },
    {
        "original": "def CountFlowResults(self, client_id, flow_id, with_tag=None, with_type=None):\n    \"\"\"Counts flow results of a given flow using given query options.\"\"\"\n    return len(\n        self.ReadFlowResults(\n            client_id,\n            flow_id,\n            0,\n            sys.maxsize,\n            with_tag=with_tag,\n            with_type=with_type))",
        "sampled": "```python\nclass FlowResultCounter:\n    def CountFlowResults(self, client_id, flow_id, with_tag=None, with_type=None):\n        \"\"\"\n        Counts flow results of a given flow using given query options.\n\n        Args:\n            client_id (str): Unique identifier for the client.\n            flow_id (str): Unique identifier for the flow.\n            with_tag (str or list of str, optional): Tag(s) to filter by. Defaults to None.\n            with_type (str or"
    },
    {
        "original": "def _output_function_label(self):\n        \"\"\"\n        Determines if we want to output the function label in assembly. We output the function label only when the\n        original instruction does not output the function label.\n\n        :return: True if we should output the function label, False otherwise.\n        :rtype: bool\n        \"\"\"\n\n        if self.asm_code:\n            return True\n        if not self.blocks:\n            return True\n\n        the_block = next((b for b in self.blocks if b.addr == self.addr), None)\n        if the_block is None:\n            return True\n        if not the_block.instructions:\n            return True\n        if not the_block.instructions[0].labels:\n            return True\n        return False",
        "sampled": "```python\nclass AssemblyCodeGenerator:\n    def _output_function_label(self):\n        \"\"\"\n        Determines if we want to output the function label in assembly. We output the function label only when the\n        original instruction does not output the function label.\n\n        :return: True if we should output the function label, False otherwise.\n        :rtype: bool\n        \"\"\"\n        \n        # Assuming that this method is part of a class and has access to an instance variable 'original_instruction'\n        \n        #"
    },
    {
        "original": "def best(cls):\n        \"\"\"\n        Select the best ScriptWriter for this environment.\n        \"\"\"\n        if sys.platform == 'win32' or (os.name == 'java' and os._name == 'nt'):\n            return WindowsScriptWriter.best()\n        else:\n            return cls",
        "sampled": "```\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\n# Define an abstract base class for ScriptWriter\nclass ScriptWriter(ABC):\n    @abstractmethod\n    def write_script(self) -> str:\n        pass\n\n# Define a concrete class for InlineMathWriter\nclass InlineMathWriter(ScriptWriter):\n    \"\"\"\n    A script writer that writes math formulas in inline latex.\n    \"\"\"\n    def write_script(self) -> str:\n        return \"$\\\\sum_{i=1}"
    },
    {
        "original": "def style(self, style):\n        \"\"\"\n        Set val attribute of <w:pStyle> child element to *style*, adding a\n        new element if necessary. If *style* is |None|, remove the <w:pStyle>\n        element if present.\n        \"\"\"\n        if style is None:\n            self._remove_pStyle()\n            return\n        pStyle = self.get_or_add_pStyle()\n        pStyle.val = style",
        "sampled": "```python\nclass WordProcessingDocument:\n    def __init__(self):\n        self.paragraph_styles = {'pStyle': None}\n\n    def style(self, style):\n        \"\"\"\n        Set val attribute of <w:pStyle> child element to *style*, adding a\n        new element if necessary. If *style* is |None|, remove the <w:pStyle>\n        element if present.\n        \"\"\"\n        \n        # Check if a paragraph style is already set\n        if self.par"
    },
    {
        "original": "def fetch(self, requirement, tmpdir, force_scan=False, source=False):\n        \"\"\"Obtain a file suitable for fulfilling `requirement`\n\n        DEPRECATED; use the ``fetch_distribution()`` method now instead.  For\n        backward compatibility, this routine is identical but returns the\n        ``location`` of the downloaded distribution instead of a distribution\n        object.\n        \"\"\"\n        dist = self.fetch_distribution(requirement, tmpdir, force_scan, source)\n        if dist is not None:\n            return dist.location\n        return None",
        "sampled": "```python\nclass PackageFetcher:\n    def fetch_distribution(self, requirement, tmpdir, force_scan=False, source=False):\n        \"\"\"\n        Obtain a file suitable for fulfilling `requirement`\n\n        Args:\n            requirement (str): The package requirement\n            tmpdir (str): Temporary directory to store the package\n            force_scan (bool): Force scan for the package. Defaults to False.\n            source (bool): Get the source of the package. Defaults to False.\n\n        Returns:\n            str"
    },
    {
        "original": "def _dynamic_mul(self, dimensions, other, keys):\n        \"\"\"\n        Implements dynamic version of overlaying operation overlaying\n        DynamicMaps and HoloMaps where the key dimensions of one is\n        a strict superset of the other.\n        \"\"\"\n        # If either is a HoloMap compute Dimension values\n        if not isinstance(self, DynamicMap) or not isinstance(other, DynamicMap):\n            keys = sorted((d, v) for k in keys for d, v in k)\n            grouped =  dict([(g, [v for _, v in group])\n                             for g, group in groupby(keys, lambda x: x[0])])\n            dimensions = [d(values=grouped[d.name]) for d in dimensions]\n            map_obj = None\n\n        # Combine streams\n        map_obj = self if isinstance(self, DynamicMap) else other\n        if isinstance(self, DynamicMap) and isinstance(other, DynamicMap):\n            self_streams = util.dimensioned_streams(self)\n            other_streams = util.dimensioned_streams(other)\n            streams = list(util.unique_iterator(self_streams+other_streams))\n        else:\n            streams = map_obj.streams\n\n        def dynamic_mul(*key, **kwargs):\n            key_map = {d.name: k for d, k in zip(dimensions, key)}\n            layers = []\n            try:\n                self_el = self.select(HoloMap, **key_map) if self.kdims else self[()]\n                layers.append(self_el)\n            except KeyError:\n                pass\n            try:\n                other_el = other.select(HoloMap, **key_map) if other.kdims else other[()]\n                layers.append(other_el)\n            except KeyError:\n                pass\n            return Overlay(layers)\n        callback = Callable(dynamic_mul, inputs=[self, other])\n        callback._is_overlay = True\n        if map_obj:\n            return map_obj.clone(callback=callback, shared_data=False,\n                                 kdims=dimensions, streams=streams)\n        else:\n            return DynamicMap(callback=callback, kdims=dimensions,\n                              streams=streams)",
        "sampled": "```python\nclass DynamicMap:\n    def __init__(self, dimensions):\n        self.dimensions = dimensions\n\n    def _dynamic_mul(self, dimensions, other, keys):\n        \"\"\"\n        Implements dynamic version of overlaying operation overlaying\n        DynamicMaps and HoloMaps where the key dimensions of one is\n        a strict superset of the other.\n        \n         :param dimensions: Dimensions of this map\n         :type dimension: List[str]\n         :param other: Other map to be"
    },
    {
        "original": "def clearness_index_zenith_independent(clearness_index, airmass,\n                                       max_clearness_index=2.0):\n    \"\"\"\n    Calculate the zenith angle independent clearness index.\n\n    Parameters\n    ----------\n    clearness_index : numeric\n        Ratio of global to extraterrestrial irradiance on a horizontal\n        plane\n\n    airmass : numeric\n        Airmass\n\n    max_clearness_index : numeric, default 2.0\n        Maximum value of the clearness index. The default, 2.0, allows\n        for over-irradiance events typically seen in sub-hourly data.\n        NREL's SRRL Fortran code used 0.82 for hourly data.\n\n    Returns\n    -------\n    kt_prime : numeric\n        Zenith independent clearness index\n\n    References\n    ----------\n    .. [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,\n           (1992). \"Dynamic Global-to-Direct Irradiance Conversion Models\".\n           ASHRAE Transactions-Research Series, pp. 354-369\n    \"\"\"\n    # Perez eqn 1\n    kt_prime = clearness_index / _kt_kt_prime_factor(airmass)\n    kt_prime = np.maximum(kt_prime, 0)\n    kt_prime = np.minimum(kt_prime, max_clearness_index)\n    return kt_prime",
        "sampled": "```python\ndef clearness_index_zenith_independent(clearness_index, airmass,\n                                       max_clearness_index=2.0):\n    \"\"\"\n    Calculate the zenith angle independent clearness index.\n\n    Parameters\n    ----------\n    clearness_index : numeric\n        Ratio of global to extraterrestrial irradiance on a horizontal\n        plane\n\n    airmass : numeric\n        Airmass\n\n    max_clearness_index : numeric, default 2.0\n        Maximum"
    },
    {
        "original": "def plugin_installed(name):\n    \"\"\"\n    .. versionadded:: 2016.11.0\n\n    Return if the plugin is installed for the provided plugin name.\n\n    :param name: The name of the parameter to confirm installation.\n    :return: True if plugin exists, False if plugin does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jenkins.plugin_installed pluginName\n\n    \"\"\"\n\n    server = _connect()\n    plugins = server.get_plugins()\n\n    exists = [plugin for plugin in plugins.keys() if name in plugin]\n\n    if exists:\n        return True\n    else:\n        return False",
        "sampled": "```\ndef jenkins_plugin_installed(name):\n    \"\"\"\n    Return if the Jenkins plugin is installed for the provided plugin name.\n\n    :param name: The name of the Jenkins plugin to confirm installation.\n    :return: True if plugin exists, False if plugin does not exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jenkins.plugin_installed-plugin_name\n\n    \"\"\"\n\n    # This function would query a database or API to check if a given Jenkins plugin exists\n    #"
    },
    {
        "original": "def _extract_links_from_asset_tags_in_text(self, text):\n        \"\"\"\n        Scan the text and extract asset tags and links to corresponding\n        files.\n\n        @param text: Page text.\n        @type text: str\n\n        @return: @see CourseraOnDemand._extract_links_from_text\n        \"\"\"\n        # Extract asset tags from instructions text\n        asset_tags_map = self._extract_asset_tags(text)\n        ids = list(iterkeys(asset_tags_map))\n        if not ids:\n            return {}\n\n        # asset tags contain asset names and ids. We need to make another\n        # HTTP request to get asset URL.\n        asset_urls = self._extract_asset_urls(ids)\n\n        supplement_links = {}\n\n        # Build supplement links, providing nice titles along the way\n        for asset in asset_urls:\n            title = clean_filename(\n                asset_tags_map[asset['id']]['name'],\n                self._unrestricted_filenames)\n            extension = clean_filename(\n                asset_tags_map[asset['id']]['extension'].strip(),\n                self._unrestricted_filenames)\n            url = asset['url'].strip()\n            if extension not in supplement_links:\n                supplement_links[extension] = []\n            supplement_links[extension].append((url, title))\n\n        return supplement_links",
        "sampled": "```python\nimport re\n\nclass CourseraOnDemand:\n    def _extract_links_from_asset_tags_in_text(self, text):\n        \"\"\"\n        Scan the text and extract asset tags and links to corresponding\n        files.\n\n        @param text: Page text.\n        @type text: str\n\n        @return: List of tuples containing asset tag and link.\n                 Each tuple contains two elements:\n                 - Asset tag (str)\n                 - Link to the file (str)\n        \"\"\"\n        \n       "
    },
    {
        "original": "def distances_indices_sorted(self, points, sign=False):\n        \"\"\"\n        Computes the distances from the plane to each of the points. Positive distances are on the side of the\n        normal of the plane while negative distances are on the other side. Indices sorting the points from closest\n        to furthest is also computed.\n        :param points: Points for which distances are computed\n        :param sign: Whether to add sign information in the indices sorting the points distances\n        :return: Distances from the plane to the points (positive values on the side of the normal to the plane,\n                 negative values on the other side), as well as indices of the points from closest to furthest. For\n                 the latter, when the sign parameter is True, items of the sorting list are given as tuples of\n                 (index, sign).\n        \"\"\"\n        distances = [np.dot(self.normal_vector, pp) + self.d for pp in points]\n        indices = sorted(range(len(distances)), key=lambda k: np.abs(distances[k]))\n        if sign:\n            indices = [(ii, int(np.sign(distances[ii]))) for ii in indices]\n        return distances, indices",
        "sampled": "```python\nfrom scipy.spatial import distance\nimport numpy as np\n\nclass PlaneCalculator:\n    def distances_indices_sorted(self, points, sign=False):\n        \"\"\"\n        Computes the distances from the plane to each of the points. Positive distances are on the side of the\n        normal of the plane while negative distances are on the other side. Indices sorting the points from closest\n        to furthest is also computed.\n        :param points: Points for which distances are computed (numpy array)\n        :"
    },
    {
        "original": "def matches_whitelist(self, matches, whitelist):\n        \"\"\"\n        Reads over the matches and returns a matches dict with just the ones\n        that are in the whitelist\n        \"\"\"\n        if not whitelist:\n            return matches\n        ret_matches = {}\n        if not isinstance(whitelist, list):\n            whitelist = whitelist.split(',')\n        for env in matches:\n            for sls in matches[env]:\n                if sls in whitelist:\n                    ret_matches[env] = ret_matches[env] if env in ret_matches else []\n                    ret_matches[env].append(sls)\n        return ret_matches",
        "sampled": "```python\ndef matches_whitelist(self, matches, whitelist):\n    \"\"\"\n    Reads over the matches and returns a matches dict with just the ones\n    that are in the wishlist\n\n    Args:\n        matches (dict): The dictionary containing all the original matches.\n        whitelist (list or set): A list or set of elements for which we want to return their corresponding values.\n\n    Returns:\n        dict: A dictionary containing only the elements that were present in both 'matches' and 'whitelist"
    },
    {
        "original": "def _auth(profile=None):\n    \"\"\"\n    Set up neutron credentials\n    \"\"\"\n    if profile:\n        credentials = __salt__['config.option'](profile)\n        user = credentials['keystone.user']\n        password = credentials['keystone.password']\n        tenant = credentials['keystone.tenant']\n        auth_url = credentials['keystone.auth_url']\n        region_name = credentials.get('keystone.region_name', None)\n        service_type = credentials.get('keystone.service_type', 'network')\n        os_auth_system = credentials.get('keystone.os_auth_system', None)\n        use_keystoneauth = credentials.get('keystone.use_keystoneauth', False)\n        verify = credentials.get('keystone.verify', True)\n    else:\n        user = __salt__['config.option']('keystone.user')\n        password = __salt__['config.option']('keystone.password')\n        tenant = __salt__['config.option']('keystone.tenant')\n        auth_url = __salt__['config.option']('keystone.auth_url')\n        region_name = __salt__['config.option']('keystone.region_name')\n        service_type = __salt__['config.option']('keystone.service_type')\n        os_auth_system = __salt__['config.option']('keystone.os_auth_system')\n        use_keystoneauth = __salt__['config.option']('keystone.use_keystoneauth')\n        verify = __salt__['config.option']('keystone.verify')\n\n    if use_keystoneauth is True:\n        project_domain_name = credentials['keystone.project_domain_name']\n        user_domain_name = credentials['keystone.user_domain_name']\n\n        kwargs = {\n            'username': user,\n            'password': password,\n            'tenant_name': tenant,\n            'auth_url': auth_url,\n            'region_name': region_name,\n            'service_type': service_type,\n            'os_auth_plugin': os_auth_system,\n            'use_keystoneauth': use_keystoneauth,\n            'verify': verify,\n            'project_domain_name': project_domain_name,\n            'user_domain_name': user_domain_name\n        }\n    else:\n        kwargs = {\n            'username': user,\n            'password': password,\n            'tenant_name': tenant,\n            'auth_url': auth_url,\n            'region_name': region_name,\n            'service_type': service_type,\n            'os_auth_plugin': os_auth_system\n        }\n\n    return suoneu.SaltNeutron(**kwargs)",
        "sampled": "```python\nimport math\ndef _auth(profile=None):\n    \"\"\"\n    Set up neutron credentials\n    \n    Parameters:\n    profile (dict): Dictionary containing client id, client secret and username (optional)\n    \n    Returns:\n    dict: Dictionary containing neutron credentials token and expiration time if profile is provided\n          Otherwise return None\n    \n    \"\"\"\n    \n    # If no profile is given return None\n    if profile is None:\n        return None\n    \n    \n\n    \n\nasync def strength_meter(query_len, max_vals="
    },
    {
        "original": "def cos_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a cosine wave (from -1\n    to +1) which repeats every *period* values. For example, to produce a\n    \"siren\" effect with a couple of LEDs that repeats once a second::\n\n        from gpiozero import PWMLED\n        from gpiozero.tools import cos_values, scaled, inverted\n        from signal import pause\n\n        red = PWMLED(2)\n        blue = PWMLED(3)\n\n        red.source_delay = 0.01\n        blue.source_delay = red.source_delay\n        red.source = scaled(cos_values(100), 0, 1, -1, 1)\n        blue.source = inverted(red)\n\n        pause()\n\n    If you require a different range than -1 to +1, see :func:`scaled`.\n    \"\"\"\n    angles = (2 * pi * i / period for i in range(period))\n    for a in cycle(angles):\n        yield cos(a)",
        "sampled": "```python\nimport numpy as np\n\ndef cos_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a cosine wave (from -1\n    to +1) which repeats every *period* values.\n\n    Args:\n        period (int): Period of the cosine wave in degrees. Defaults to 360.\n\n    Returns:\n        generator: Infinite generator yielding values representing a cosine wave.\n    \"\"\"\n    \n    frequency = 100 / period  # Calculate frequency in Hz\n   "
    },
    {
        "original": "def team_stats(game_id):\n    \"\"\"Return dictionary of team stats for game matching the game id.\"\"\"\n    # get data\n    data = mlbgame.stats.team_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, False)",
        "sampled": "```python\ndef team_stats(game_id):\n    # Sample data for demonstration purposes\n    game_data = {\n        1: {\n            \"team1\": {\"name\": \"Team A\", \"score\": 100, \"OBserved_PACE\": [98, 102]},\n            \"team2\": {\"name\": \"Team B\", \"score\": 80, \"OBserved_PACE\": [78, 82]}\n        },\n        2: {\n            \"team1"
    },
    {
        "original": "def percent(args=None):\n    \"\"\"\n    Return partition information for volumes mounted on this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' disk.percent /var\n    \"\"\"\n    if __grains__['kernel'] == 'Linux':\n        cmd = 'df -P'\n    elif __grains__['kernel'] == 'OpenBSD' or __grains__['kernel'] == 'AIX':\n        cmd = 'df -kP'\n    else:\n        cmd = 'df'\n    ret = {}\n    out = __salt__['cmd.run'](cmd, python_shell=False).splitlines()\n    for line in out:\n        if not line:\n            continue\n        if line.startswith('Filesystem'):\n            continue\n        comps = line.split()\n        while len(comps) >= 2 and not comps[1].isdigit():\n            comps[0] = '{0} {1}'.format(comps[0], comps[1])\n            comps.pop(1)\n        if len(comps) < 2:\n            continue\n        try:\n            if __grains__['kernel'] == 'Darwin':\n                ret[comps[8]] = comps[4]\n            else:\n                ret[comps[5]] = comps[4]\n        except IndexError:\n            log.error('Problem parsing disk usage information')\n            ret = {}\n    if args and args not in ret:\n        log.error(\n            'Problem parsing disk usage information: Partition \\'%s\\' '\n            'does not exist!', args\n        )\n        ret = {}\n    elif args:\n        return ret[args]\n\n    return ret",
        "sampled": "```python\nimport psutil\n\ndef percent(args=None):\n    \"\"\"\n    Return partition information for volumes mounted on this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' disk.percent /\n    \"\"\"\n\n    if args is None:\n        args = []\n\n    if len(args) == 0:\n        # Check for all mounted disk partitions\n        disk_partitions = psutil.disk_partitions()\n        info_dict = {}\n        \n        for partition in disk_partitions:\n            usage = psutil.disk"
    },
    {
        "original": "def from_iris(cube):\n    \"\"\" Convert a Iris cube into an DataArray\n    \"\"\"\n    import iris.exceptions\n    from xarray.core.pycompat import dask_array_type\n\n    name = _name(cube)\n    if name == 'unknown':\n        name = None\n    dims = []\n    for i in range(cube.ndim):\n        try:\n            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))\n            dims.append(_name(dim_coord))\n        except iris.exceptions.CoordinateNotFoundError:\n            dims.append(\"dim_{}\".format(i))\n\n    if len(set(dims)) != len(dims):\n        duplicates = [k for k, v in Counter(dims).items() if v > 1]\n        raise ValueError('Duplicate coordinate name {}.'.format(duplicates))\n\n    coords = OrderedDict()\n\n    for coord in cube.coords():\n        coord_attrs = _iris_obj_to_attrs(coord)\n        coord_dims = [dims[i] for i in cube.coord_dims(coord)]\n        if coord_dims:\n            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)\n        else:\n            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)\n\n    array_attrs = _iris_obj_to_attrs(cube)\n    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)\n    if cell_methods:\n        array_attrs['cell_methods'] = cell_methods\n\n    # Deal with iris 1.* and 2.*\n    cube_data = cube.core_data() if hasattr(cube, 'core_data') else cube.data\n\n    # Deal with dask and numpy masked arrays\n    if isinstance(cube_data, dask_array_type):\n        from dask.array import ma as dask_ma\n        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))\n    elif isinstance(cube_data, np.ma.MaskedArray):\n        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))\n    else:\n        filled_data = cube_data\n\n    dataarray = DataArray(filled_data, coords=coords, name=name,\n                          attrs=array_attrs, dims=dims)\n    decoded_ds = decode_cf(dataarray._to_temp_dataset())\n    return dataarray._from_temp_dataset(decoded_ds)",
        "sampled": "```python\nimport mat73\nimport xarray as xr\n\ndef from_iris(cube):\n    \"\"\"\n    Convert a Iris cube into an DataArray\n    \n    Parameters\n    ----------\n    \n    Returns\n    -------\n    \n    \"\"\"\n\n# Extract the data from the qt objects of the iris cube object.\ndata = cube.data(new_dimensions Order=cube>Data.coordinates)\n\n# Use mat73 to load the binary file and extract variables from it.\nvariables = mat73.loadmat('path_to_your_matfile.mat"
    },
    {
        "original": "def log_prior(self):\n        \"\"\"evaluate the prior\"\"\"\n        if self.priors.size == 0:\n            return 0.\n        x = self.param_array\n        #evaluate the prior log densities\n        log_p = reduce(lambda a, b: a + b, (p.lnpdf(x[ind]).sum() for p, ind in self.priors.items()), 0)\n\n        #account for the transformation by evaluating the log Jacobian (where things are transformed)\n        log_j = 0.\n        priored_indexes = np.hstack([i for p, i in self.priors.items()])\n        for c,j in self.constraints.items():\n            if not isinstance(c, Transformation):continue\n            for jj in j:\n                if jj in priored_indexes:\n                    log_j += c.log_jacobian(x[jj])\n        return log_p + log_j",
        "sampled": "```python\nimport math\n\nclass Gaussian:\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n    def log_prior(self):\n        return -0.5 * math.log(2 * math.pi) - 0.5 * (1 / self.sigma**2)\n\n\n# Define some parameters for the prior distribution of the model\nmu = 0\nsigma = 1\n\n# Create a Gaussian object with these parameters\ng ="
    },
    {
        "original": "def oil(data_set='three_phase_oil_flow'):\n    \"\"\"The three phase oil data from Bishop and James (1993).\"\"\"\n    if not data_available(data_set):\n        download_data(data_set)\n    oil_train_file = os.path.join(data_path, data_set, 'DataTrn.txt')\n    oil_trainlbls_file = os.path.join(data_path, data_set, 'DataTrnLbls.txt')\n    oil_test_file = os.path.join(data_path, data_set, 'DataTst.txt')\n    oil_testlbls_file = os.path.join(data_path, data_set, 'DataTstLbls.txt')\n    oil_valid_file = os.path.join(data_path, data_set, 'DataVdn.txt')\n    oil_validlbls_file = os.path.join(data_path, data_set, 'DataVdnLbls.txt')\n    fid = open(oil_train_file)\n    X = np.fromfile(fid, sep='\\t').reshape((-1, 12))\n    fid.close()\n    fid = open(oil_test_file)\n    Xtest = np.fromfile(fid, sep='\\t').reshape((-1, 12))\n    fid.close()\n    fid = open(oil_valid_file)\n    Xvalid = np.fromfile(fid, sep='\\t').reshape((-1, 12))\n    fid.close()\n    fid = open(oil_trainlbls_file)\n    Y = np.fromfile(fid, sep='\\t').reshape((-1, 3)) * 2. - 1.\n    fid.close()\n    fid = open(oil_testlbls_file)\n    Ytest = np.fromfile(fid, sep='\\t').reshape((-1, 3)) * 2. - 1.\n    fid.close()\n    fid = open(oil_validlbls_file)\n    Yvalid = np.fromfile(fid, sep='\\t').reshape((-1, 3)) * 2. - 1.\n    fid.close()\n    return data_details_return({'X': X, 'Y': Y, 'Xtest': Xtest, 'Ytest': Ytest, 'Xtest' : Xtest, 'Xvalid': Xvalid, 'Yvalid': Yvalid}, data_set)",
        "sampled": "```python\nimport numpy as np\n\nclass Solution:\n    def oil(self, data_set='three_phase_oil_flow'):\n        # Load the oil data from Bishop and James (1993) for three phase flow\n        if data_set == 'three_phase_oil_flow':\n            # Given the API, well qc, IM FROM W BBCOLDWELL CEFFICENCE REST ENTRY_time and g_pl[r_parameter]dT interim vrellmlter Ot the Dw Pitch offline <- alkymar"
    },
    {
        "original": "def _proc_sph_top(self):\n        \"\"\"\n        Handles Sperhical Top Molecules, which belongs to the T, O or I point\n        groups.\n        \"\"\"\n        self._find_spherical_axes()\n        if len(self.rot_sym) == 0:\n            logger.debug(\"Accidental speherical top!\")\n            self._proc_sym_top()\n        main_axis, rot = max(self.rot_sym, key=lambda v: v[1])\n        if rot < 3:\n            logger.debug(\"Accidental speherical top!\")\n            self._proc_sym_top()\n        elif rot == 3:\n            mirror_type = self._find_mirror(main_axis)\n            if mirror_type != \"\":\n                if self.is_valid_op(PointGroupAnalyzer.inversion_op):\n                    self.symmops.append(PointGroupAnalyzer.inversion_op)\n                    self.sch_symbol = \"Th\"\n                else:\n                    self.sch_symbol = \"Td\"\n            else:\n                self.sch_symbol = \"T\"\n        elif rot == 4:\n            if self.is_valid_op(PointGroupAnalyzer.inversion_op):\n                self.symmops.append(PointGroupAnalyzer.inversion_op)\n                self.sch_symbol = \"Oh\"\n            else:\n                self.sch_symbol = \"O\"\n        elif rot == 5:\n            if self.is_valid_op(PointGroupAnalyzer.inversion_op):\n                self.symmops.append(PointGroupAnalyzer.inversion_op)\n                self.sch_symbol = \"Ih\"\n            else:\n                self.sch_symbol = \"I\"",
        "sampled": "```python\ndef _proc_sph_top(self):\n    \"\"\"\n    Handles Sperhical Top Molecules, which belongs to the T, O or I point groups.\n    \n    Args:\n        self (object): The current object instance.\n        \n    Returns:\n        None\n    \"\"\"\n\n    # Define the symmetry operations for each point group\n    point_groups = {\n        'T': [\n            lambda x: (x + np.pi/4, -x + np.pi/4),\n           "
    },
    {
        "original": "def wait_for_registration(self, processor_type):\n        \"\"\"Waits for a particular processor type to register or until\n        is_cancelled is True. is_cancelled cannot be part of this class\n        since we aren't cancelling all waiting for a processor_type,\n        but just this particular wait.\n\n        Args:\n            processor_type (ProcessorType): The family, and version of\n                the transaction processor.\n\n        Returns:\n            None\n        \"\"\"\n        with self._condition:\n            self._condition.wait_for(lambda: (\n                processor_type in self\n                or self._cancelled_event.is_set()))\n            if self._cancelled_event.is_set():\n                raise WaitCancelledException()",
        "sampled": "```python\nfrom enum import Enum\nimport threading\nfrom typing import Callable\n\nclass ProcessorType(Enum):\n    # Assuming we have different processor types\n    TYPE_1 = 1\n    TYPE_2 = 2\n\nclass WaitableLock:\n    def __init__(self):\n        self._lock = threading.Lock()\n        self._waiter: Callable[[], None] | None = None\n\n    def wait(self, on_scope_exit: Callable[[], None]):\n        \"\"\"Executes"
    },
    {
        "original": "def add_tab_stop(self, position, alignment=WD_TAB_ALIGNMENT.LEFT,\n                     leader=WD_TAB_LEADER.SPACES):\n        \"\"\"\n        Add a new tab stop at *position*, a |Length| object specifying the\n        location of the tab stop relative to the paragraph edge. A negative\n        *position* value is valid and appears in hanging indentation. Tab\n        alignment defaults to left, but may be specified by passing a member\n        of the :ref:`WdTabAlignment` enumeration as *alignment*. An optional\n        leader character can be specified by passing a member of the\n        :ref:`WdTabLeader` enumeration as *leader*.\n        \"\"\"\n        tabs = self._pPr.get_or_add_tabs()\n        tab = tabs.insert_tab_in_order(position, alignment, leader)\n        return TabStop(tab)",
        "sampled": "```python\nfrom enum import Enum\n\nclass WdTabAlignment(Enum):\n    LEFT = 1\n    CENTER = 2\n    RIGHT = 3\n\n\nclass WdTabLeader(Enum):\n    SPACES = 0\n    DOTS = 1\n\n\nclass Length:\n    def __init__(self, value, unit='points'):\n        self.value = value\n        self.unit = unit\n\n\nclass Paragraph:\n    def __init__(self):\n        self.tab_stops = []\n\n   "
    },
    {
        "original": "def setup(self, bottom, top):\n        \"\"\"\n        Setup data layer according to parameters:\n\n        - voc_dir: path to PASCAL VOC dir (must contain 2010)\n        - context_dir: path to PASCAL-Context annotations\n        - split: train / val / test\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for PASCAL-Context semantic segmentation.\n\n        example: params = dict(voc_dir=\"/path/to/PASCAL\", split=\"val\")\n        \"\"\"\n        # config\n        params = eval(self.param_str)\n        self.voc_dir = params['voc_dir'] + '/VOC2010'\n        self.context_dir = params['context_dir']\n        self.split = params['split']\n        self.mean = np.array((104.007, 116.669, 122.679), dtype=np.float32)\n        self.random = params.get('randomize', True)\n        self.seed = params.get('seed', None)\n\n        # load labels and resolve inconsistencies by mapping to full 400 labels\n        self.labels_400 = [label.replace(' ','') for idx, label in np.genfromtxt(self.context_dir + '/labels.txt', delimiter=':', dtype=None)]\n        self.labels_59 = [label.replace(' ','') for idx, label in np.genfromtxt(self.context_dir + '/59_labels.txt', delimiter=':', dtype=None)]\n        for main_label, task_label in zip(('table', 'bedclothes', 'cloth'), ('diningtable', 'bedcloth', 'clothes')):\n            self.labels_59[self.labels_59.index(task_label)] = main_label\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(\"Need to define two tops: data and label.\")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(\"Do not define a bottom.\")\n\n        # load indices for images and labels\n        split_f  = '{}/ImageSets/Main/{}.txt'.format(self.voc_dir,\n                self.split)\n        self.indices = open(split_f, 'r').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if 'train' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)",
        "sampled": "```python\nimport os\nimport cv2 as cv\nimport numpy as np\nfrom random import shuffle\n\nclass DataLayer:\n    \"\"\"\n    Setup data layer according to parameters:\n\n    - voc_dir: path to PASCAL VOC dir (must contain 2010)\n    - context_dir: path to PASCAL-Context annotations\n    - split: train / val / test\n    - randomize: load in random order (default: True)\n    - seed: seed for random"
    },
    {
        "original": "def fit_anonymous(self, struct1, struct2, niggli=True):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            True/False: Whether a species mapping can map struct1 to stuct2\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2,\n                                                              niggli)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        break_on_match=True, single_match=True)\n\n        if matches:\n            return True\n        else:\n            return False",
        "sampled": "```python\nfrom collections import defaultdict\n\nclass Structure:\n    def __init__(self, species_per_cell):\n        self.species_per_cell = species_per_cell\n\ndef fit_anonymous(self, struct1, struct2, niggli=True):\n    \"\"\"\n    Performs an anonymous fitting, which allows distinct species in one structure to map to another.\n\n    Args:\n        struct1 (Structure): 1st structure\n        struct2 (Structure): 2nd structure\n\n    Returns:\n        True/"
    },
    {
        "original": "def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        for name, child in self._children.items():\n            child.render(**kwargs)\n\n        figure = self.get_root()\n        assert isinstance(figure, Figure), ('You cannot render this Element '\n                                            'if it is not in a Figure.')\n\n        figure.script.add_child(Element(\n            self._template.render(this=self, kwargs=kwargs)),\n            name=self.get_name())",
        "sampled": "```python\nclass Solution:\n    def isPowerOfTwo(self, n: int) -> bool:\n        \"\"\"Returns True if the given number is a power of two, False otherwise.\"\"\"\n        if n <= 0:\n            return False\n        return (n & (n - 1)) == 0\n\n    def findIntegers(self, num: int, limit: int, memo = {}) -> list[int]:\n        \"\"\"Finds all integers from 1 to limit that are not"
    },
    {
        "original": "def add_subreddit(self, subreddit, _delete=False, *args, **kwargs):\n        \"\"\"Add a subreddit to the multireddit.\n\n        :param subreddit: The subreddit name or Subreddit object to add\n\n        The additional parameters are passed directly into\n        :meth:`~praw.__init__.BaseReddit.request_json`.\n\n        \"\"\"\n        subreddit = six.text_type(subreddit)\n        url = self.reddit_session.config['multireddit_add'].format(\n            user=self._author, multi=self.name, subreddit=subreddit)\n        method = 'DELETE' if _delete else 'PUT'\n        # The modhash isn't necessary for OAuth requests\n        if not self.reddit_session._use_oauth:\n            self.reddit_session.http.headers['x-modhash'] = \\\n                self.reddit_session.modhash\n        data = {'model': dumps({'name': subreddit})}\n        try:\n            self.reddit_session.request(url, data=data, method=method,\n                                        *args, **kwargs)\n        finally:\n            # The modhash isn't necessary for OAuth requests\n            if not self.reddit_session._use_oauth:\n                del self.reddit_session.http.headers['x-modhash']",
        "sampled": "```python\nclass MultiReddit:\n    def __init__(self):\n        self.subreddits = {}\n\n    def add_subreddit(self, subreddit, _delete=False, *args, **kwargs):\n        \"\"\"\n        Add a subreddit to the multireddit.\n\n        :param subreddit: The subreddit name or Subreddit object to add\n\n        The additional parameters are passed directly into\n        praw.__init__.BaseReddit.request_json.\n        \n        Args:\n            kwargs (dict): Keyword arguments to pass directly"
    },
    {
        "original": "def isdatetime(value):\n    \"\"\"\n    Whether the array or scalar is recognized datetime type.\n    \"\"\"\n    if isinstance(value, np.ndarray):\n        return (value.dtype.kind == \"M\" or\n                (value.dtype.kind == \"O\" and len(value) and\n                 isinstance(value[0], datetime_types)))\n    else:\n        return isinstance(value, datetime_types)",
        "sampled": "```python\ndef isdatetime(value):\n    \"\"\"\n    Whether the array or scalar is recognized datetime type.\n    \"\"\"\n    \n    import pandas as pd\n        \n    # Check if the value is a scalar\n    if not isinstance(value, (list, np.ndarray)):\n        try:\n            # Attempt to convert value to datetime\n            pd.to_datetime(str(value))\n            return True  # Value is successfully converted to datetime\n        except ValueError:\n            return False  # Value cannot be converted to datetime\n            \n        \n"
    },
    {
        "original": "def get_highlights(self, user: Union[int, Profile]) -> Iterator[Highlight]:\n        \"\"\"Get all highlights from a user.\n        To use this, one needs to be logged in.\n\n        .. versionadded:: 4.1\n\n        :param user: ID or Profile of the user whose highlights should get fetched.\n        \"\"\"\n\n        userid = user if isinstance(user, int) else user.userid\n        data = self.context.graphql_query(\"7c16654f22c819fb63d1183034a5162f\",\n                                          {\"user_id\": userid, \"include_chaining\": False, \"include_reel\": False,\n                                           \"include_suggested_users\": False, \"include_logged_out_extras\": False,\n                                           \"include_highlight_reels\": True})[\"data\"][\"user\"]['edge_highlight_reels']\n        if data is None:\n            raise BadResponseException('Bad highlights reel JSON.')\n        yield from (Highlight(self.context, edge['node'], user if isinstance(user, Profile) else None)\n                    for edge in data['edges'])",
        "sampled": "```python\nfrom typing import Union, Profile, Iterator\n\nclass Highlight:\n    \"\"\"A single highlight.\"\"\"\n    \n    def __init__(self, id: int, text: str):\n        \"\"\"Create a new highlight.\n        \n        :param id: The ID of the highlight.\n        :param text: The text of the highlight.\n        \"\"\"\n        \n        self.id = id\n        self.text = text\n\nclass Profile:\n    \"\"\"A user's profile.\"\"\"\n    \n    def __init__(self,"
    },
    {
        "original": "def flush(name, family='ipv4', **kwargs):\n    \"\"\"\n    .. versionadded:: 2014.7.0\n\n    Flush current nftables state\n\n    family\n        Networking family, either ipv4 or ipv6\n\n    \"\"\"\n    ret = {'name': name,\n           'changes': {},\n           'result': None,\n           'comment': ''}\n\n    for ignore in _STATE_INTERNAL_KEYWORDS:\n        if ignore in kwargs:\n            del kwargs[ignore]\n\n    if 'table' not in kwargs:\n        kwargs['table'] = 'filter'\n\n    res = __salt__['nftables.check_table'](kwargs['table'], family=family)\n    if not res['result']:\n        ret['result'] = False\n        ret['comment'] = 'Failed to flush table {0} in family {1}, table does not exist.'.format(\n            kwargs['table'],\n            family\n        )\n        return ret\n\n    if 'chain' not in kwargs:\n        kwargs['chain'] = ''\n    else:\n        res = __salt__['nftables.check_chain'](kwargs['table'],\n                                               kwargs['chain'],\n                                               family=family)\n        if not res['result']:\n            ret['result'] = False\n            ret['comment'] = 'Failed to flush chain {0} in table {1} in family {2}, chain does not exist.'.format(\n                kwargs['chain'],\n                kwargs['table'],\n                family\n            )\n            return ret\n\n    res = __salt__['nftables.flush'](kwargs['table'],\n                                     kwargs['chain'],\n                                     family)\n    if res['result']:\n        ret['changes'] = {'locale': name}\n        ret['result'] = True\n        ret['comment'] = 'Flush nftables rules in {0} table {1} chain {2} family'.format(\n            kwargs['table'],\n            kwargs['chain'],\n            family\n        )\n        return ret\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Failed to flush nftables rules'\n        return ret",
        "sampled": "```python\n# Define a function to flush nftables state\ndef flush_nftables_state():\n    # Print a message to indicate the action\n    print(\"Flushing current nftables state\")\n\n# Define a function to call flush_nftables_state with family parameter\ndef flush(name, family='ipv4'):\n    \"\"\"\n    .. versionadded:: 2014.7.0\n\n    Flush current nftuples state\n\n    Args:\n        name (str): Name of the"
    },
    {
        "original": "def _ProcessEntries(self, fd):\n    \"\"\"Extract entries from the xinetd config files.\"\"\"\n    p = config_file.KeyValueParser(kv_sep=\"{\", term=\"}\", sep=None)\n    data = utils.ReadFileBytesAsUnicode(fd)\n    entries = p.ParseEntries(data)\n    for entry in entries:\n      for section, cfg in iteritems(entry):\n        # The parser returns a list of configs. There will only be one.\n        if cfg:\n          cfg = cfg[0].strip()\n        else:\n          cfg = \"\"\n        self._ParseSection(section, cfg)",
        "sampled": "```python\nimport re\n\nclass XinetdParser:\n    def _ProcessEntries(self, fd):\n        \"\"\"\n        Extract entries from the xinetd config files.\n\n        Each entry in the xinetd config file starts with a line that begins with a service name and is followed by assignment operators (::= or =) to various attributes.\n        \n        For example:\n            TCP             services THANISSHELL\n\n            transport      = tcp\n            bind             = 127.0.0.1"
    },
    {
        "original": "def fit_transform_poof(self, X, y=None, outpath=None, **kwargs):\n        \"\"\"\n        Fit the model and transforms and then call poof.\n        \"\"\"\n        self.fit_transform(X, y, **kwargs)\n        self.poof(outpath, **kwargs)",
        "sampled": "```python\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclass PoofTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return np.sqrt(np.abs(X))\n\nclass MyModel(BaseEstimator):\n    def __init__(self):\n        self.scaler = StandardScaler()\n    \n    def fit_transform_po"
    },
    {
        "original": "def create_training_example(self,\n                                environment_id,\n                                collection_id,\n                                query_id,\n                                document_id=None,\n                                cross_reference=None,\n                                relevance=None,\n                                **kwargs):\n        \"\"\"\n        Add example to training data query.\n\n        Adds a example to this training data query.\n\n        :param str environment_id: The ID of the environment.\n        :param str collection_id: The ID of the collection.\n        :param str query_id: The ID of the query used for training.\n        :param str document_id: The document ID associated with this training example.\n        :param str cross_reference: The cross reference associated with this training\n        example.\n        :param int relevance: The relevance of the training example.\n        :param dict headers: A `dict` containing the request headers\n        :return: A `DetailedResponse` containing the result, headers and HTTP status code.\n        :rtype: DetailedResponse\n        \"\"\"\n\n        if environment_id is None:\n            raise ValueError('environment_id must be provided')\n        if collection_id is None:\n            raise ValueError('collection_id must be provided')\n        if query_id is None:\n            raise ValueError('query_id must be provided')\n\n        headers = {}\n        if 'headers' in kwargs:\n            headers.update(kwargs.get('headers'))\n        sdk_headers = get_sdk_headers('discovery', 'V1',\n                                      'create_training_example')\n        headers.update(sdk_headers)\n\n        params = {'version': self.version}\n\n        data = {\n            'document_id': document_id,\n            'cross_reference': cross_reference,\n            'relevance': relevance\n        }\n\n        url = '/v1/environments/{0}/collections/{1}/training_data/{2}/examples'.format(\n            *self._encode_path_vars(environment_id, collection_id, query_id))\n        response = self.request(\n            method='POST',\n            url=url,\n            headers=headers,\n            params=params,\n            json=data,\n            accept_json=True)\n        return response",
        "sampled": "```python\nclass TrainingDataQuery:\n\n    def __init__(self):\n        self.environment_id = None\n        self.collection_id = None\n        self.query_id = None\n        self.documents = []\n        self.cross_references = {}\n        self.relevance_scores = {}\n\n    def create_training_example(self,\n                            environment_id,\n                            collection_id,\n                            query_id,\n                            document_id=None,\n                            cross_reference=None,\n                            relevance=None):\n        \n        \"\"\"\n        Add example to training data query.\n\n        Adds"
    },
    {
        "original": "def find_clusters(struct, connected_matrix):\n    \"\"\"\n    Finds bonded clusters of atoms in the structure with periodic boundary\n    conditions.\n\n    If there are atoms that are not bonded to anything, returns [0,1,0]. (For\n    faster computation time)\n\n    Author: \"Gowoon Cheon\"\n    Email: \"gcheon@stanford.edu\"\n\n    Args:\n        struct (Structure): Input structure\n        connected_matrix: Must be made from the same structure with\n            find_connected_atoms() function.\n\n    Returns:\n        max_cluster: the size of the largest cluster in the crystal structure\n        min_cluster: the size of the smallest cluster in the crystal structure\n        clusters: list of bonded clusters found here, clusters are formatted as\n        sets of indices of atoms\n    \"\"\"\n    n_atoms = len(struct.species)\n    if n_atoms == 0:\n        return [0, 0, 0]\n    if 0 in np.sum(connected_matrix, axis=0):\n        return [0, 1, 0]\n\n    cluster_sizes = []\n    clusters = []\n    visited = [False for item in range(n_atoms)]\n    connected_matrix += np.eye(len(connected_matrix))\n\n    def visit(atom, atom_cluster):\n        visited[atom] = True\n        new_cluster = set(np.where(connected_matrix[atom] != 0)[0]).union(atom_cluster)\n        atom_cluster = new_cluster\n        for new_atom in atom_cluster:\n            if not visited[new_atom]:\n                visited[new_atom] = True\n                atom_cluster = visit(new_atom, atom_cluster)\n        return atom_cluster\n\n    for i in range(n_atoms):\n        if not visited[i]:\n            atom_cluster = set()\n            cluster=visit(i, atom_cluster)\n            clusters.append(cluster)\n            cluster_sizes.append(len(cluster))\n\n    max_cluster = max(cluster_sizes)\n    min_cluster = min(cluster_sizes)\n    return [max_cluster, min_cluster, clusters]",
        "sampled": "```python\nfrom typing import List, Set\n\nclass Structure:\n    pass # Define the Structure class with necessary attributes\n\n\ndef find_clusters(struct: Structure, connected_matrix) -> (int, int, List[Set[int]]):\n    \"\"\"\n    Finds bonded clusters of atoms in the structure with periodic boundary\n    conditions.\n\n    If there are atoms that are not bonded to anything, returns [0,1,0]. (For\n    faster computation time)\n\n    Args:\n        struct (Structure):"
    },
    {
        "original": "def _apply_conv(self, inputs, w):\n    \"\"\"Apply a `separable_conv2d` operation on `inputs` using `w`.\n\n    Args:\n      inputs: A Tensor of shape `data_format` and of type `tf.float16`,\n          `tf.bfloat16` or `tf.float32`.\n      w: A tuple of weight matrices of the same type as `inputs`, the first\n        being the depthwise weight matrix, and the second being the pointwise\n        weight matrix.\n\n    Returns:\n      outputs: The result of the convolution operation on `inputs`.\n    \"\"\"\n    if self._data_format == DATA_FORMAT_NWC:\n      h_dim = 1\n      two_dim_conv_data_format = DATA_FORMAT_NHWC\n    else:\n      h_dim = 2\n      two_dim_conv_data_format = DATA_FORMAT_NCHW\n\n    inputs = tf.expand_dims(inputs, axis=h_dim)\n    two_dim_conv_stride = self.stride[:h_dim] + (1,) + self.stride[h_dim:]\n\n    # Height always precedes width.\n    two_dim_conv_rate = (1,) + self._rate\n\n    w_dw, w_pw = w\n    outputs = tf.nn.separable_conv2d(inputs,\n                                     w_dw,\n                                     w_pw,\n                                     strides=two_dim_conv_stride,\n                                     rate=two_dim_conv_rate,\n                                     padding=self._conv_op_padding,\n                                     data_format=two_dim_conv_data_format)\n    outputs = tf.squeeze(outputs, [h_dim])\n    return outputs",
        "sampled": "```python\nimport tensorflow as tf\n\ndef separable_conv2d(inputs, w):\n    \"\"\"\n    Apply a separable convolution operation on inputs using the given weights.\n\n    Args:\n        inputs (tf.Tensor): The input tensor to the convolution operation.\n        w (tuple): A tuple of two weight matrices. The first is the depth-wise weight matrix, \n                   and the second is the point-wise weight matrix.\n\n    Returns:\n        tf.Tensor: The result of the convolution operation on inputs.\n"
    },
    {
        "original": "def _find_new_additions(self):\n        \"\"\"Find any nodes in the graph that need to be added to the internal\n        queue and add them.\n\n        Callers must hold the lock.\n        \"\"\"\n        for node, in_degree in self.graph.in_degree_iter():\n            if not self._already_known(node) and in_degree == 0:\n                self.inner.put((self._scores[node], node))\n                self.queued.add(node)",
        "sampled": "```python\nfrom collections import defaultdict\n\nclass Solution:\n    def find_new_additions(self, graph):\n        \"\"\"\n        Find any nodes in ``graph`` that need to be added to the internal queue and add them.\n\n        :param graph: a dictionary where keys are nodes and values are lists of their neighbors\n        :returns: a dictionary mapping nodes to their frequencies\n        \"\"\"\n        \n        # Create an empty dictionary to store frequency of each node\n        freq = {}\n        \n        # Iterate over"
    },
    {
        "original": "def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask whwre to save.\"\"\"\n        filepath, _ = QtWidgets.QFileDialog.getSaveFileName(\n            self, \"Save File\", '', \"Androguard Session (*.ag)\")\n\n        if filepath:\n            # Ensure .ag as file ending\n            if not filepath.endswith(\".ag\"):\n                filepath = \"{}.ag\".format(filepath)\n\n            self.showStatus(\"Saving %s...\" % str(filepath))\n            self.saveSession(filepath)\n            self.showStatus(\"Saved Session to %s!\" % str(filepath))",
        "sampled": "```python\nimport tkinter as tk\nfrom tkinter import filedialog\n\nclass Program:\n    def __init__(self):\n        self.root = tk.Tk()\n        self.root.withdraw()  # Hides the Tkinter root window\n\n    def saveFile(self):\n        \"\"\"User clicked Save menu. Display a Dialog to ask where to save.\"\"\"\n        file_path = filedialog.asksaveasfilename(title=\"Save File\",\n                                                  defaultextension=\".txt\",\n                                                  filetypes=[(\"Text Files\","
    },
    {
        "original": "def create_column(self, name):\n        \"\"\"\n        calls: `POST https://developer.github.com/v3/projects/columns/#create-a-project-column>`_\n        :param name: string\n        \"\"\"\n        assert isinstance(name, (str, unicode)), name\n        post_parameters = {\"name\": name}\n        import_header = {\"Accept\": Consts.mediaTypeProjectsPreview}\n        headers, data = self._requester.requestJsonAndCheck(\n            \"POST\",\n            self.url + \"/columns\",\n            headers=import_header,\n            input=post_parameters\n        )\n        return github.ProjectColumn.ProjectColumn(self._requester, headers, data, completed=True)",
        "sampled": "```python\nimport requests\n\nclass GitHubProject:\n    def __init__(self, github_token):\n        \"\"\"\n        Initialize the GitHub project class with a GitHub token\n        :param github_token: string containing the GitHub token\n        \"\"\"\n        self.auth = (\"\", f\"Bearer {github_token}\")\n\n    def create_column(self, name, project_id):\n        \"\"\"\n        Calls the API to create a new column for a project\n        :param name: string containing the name of the column\n       "
    },
    {
        "original": "def close_cursor(self, cursor_id, address=None):\n        \"\"\"Send a kill cursors message soon with the given id.\n\n        Raises :class:`TypeError` if `cursor_id` is not an instance of\n        ``(int, long)``. What closing the cursor actually means\n        depends on this client's cursor manager.\n\n        This method may be called from a :class:`~pymongo.cursor.Cursor`\n        destructor during garbage collection, so it isn't safe to take a\n        lock or do network I/O. Instead, we schedule the cursor to be closed\n        soon on a background thread.\n\n        :Parameters:\n          - `cursor_id`: id of cursor to close\n          - `address` (optional): (host, port) pair of the cursor's server.\n            If it is not provided, the client attempts to close the cursor on\n            the primary or standalone, or a mongos server.\n\n        .. versionchanged:: 3.0\n           Added ``address`` parameter.\n        \"\"\"\n        if not isinstance(cursor_id, integer_types):\n            raise TypeError(\"cursor_id must be an instance of (int, long)\")\n\n        if self.__cursor_manager is not None:\n            self.__cursor_manager.close(cursor_id, address)\n        else:\n            self.__kill_cursors_queue.append((address, [cursor_id]))",
        "sampled": "~~~\nimport threading\nfrom queue import Queue\n\nclass MongoClient:\n    def __init__(self):\n        self._close_queue = Queue()\n\n    def close_cursor(self, cursor_id, address=None):\n        \"\"\"Send a kill cursors message soon with the given id.\n\n        Raises :class:`TypeError` if `cursor_id` is not an instance of\n        ``(int, long)``. What closing the cursor actually means\n        depends on this client's cursor manager.\n\n        This method may"
    },
    {
        "original": "async def send_debug(self):\n        \"\"\" Sends the debug draw execution. Put this after your debug creation functions. \"\"\"\n        await self._execute(\n            debug=sc_pb.RequestDebug(\n                debug=[\n                    debug_pb.DebugCommand(\n                        draw=debug_pb.DebugDraw(\n                            text=self._debug_texts if self._debug_texts else None,\n                            lines=self._debug_lines if self._debug_lines else None,\n                            boxes=self._debug_boxes if self._debug_boxes else None,\n                            spheres=self._debug_spheres if self._debug_spheres else None,\n                        )\n                    )\n                ]\n            )\n        )\n        self._debug_texts.clear()\n        self._debug_lines.clear()\n        self._debug_boxes.clear()\n        self._debug_spheres.clear()",
        "sampled": "```python\nclass DebugDrawer:\n    def __init__(self):\n        self.debug_list = []\n\n    def line(self, p1, p2):\n        \"\"\"\n        Draws a line from point p1 to point p2.\n        \n        Args:\n            p1 (tuple): The start point of the line in the format (x, y).\n            p2 (tuple): The end point of the line in the format (x, y).\n        \"\"\"\n        self.debug_list.append((0,"
    },
    {
        "original": "def dump_to_store(self, store, **kwargs):\n        \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n        from ..backends.api import dump_to_store\n        # TODO: rename and/or cleanup this method to make it more consistent\n        # with to_netcdf()\n        return dump_to_store(self, store, **kwargs)",
        "sampled": "```python\nfrom typing import Optional\n\nclass Solution:\n    def dump_to_store(self, store: 'DataStore', **kwargs) -> Optional[dict]:\n        data = kwargs.get('data')\n        # Process the data if necessary\n        if 'processed' in kwargs and kwargs['processed']:\n            data = self.process_data(data)\n        \n        result = store.insert(data)\n        \n        return result.to_dict() if isinstance(result, dict) else result\n\n\n# Assuming DataStore is a class that"
    },
    {
        "original": "def write_numpy_to_dense_tensor(file, array, labels=None):\n    \"\"\"Writes a numpy array to a dense tensor\"\"\"\n\n    # Validate shape of array and labels, resolve array and label types\n    if not len(array.shape) == 2:\n        raise ValueError(\"Array must be a Matrix\")\n    if labels is not None:\n        if not len(labels.shape) == 1:\n            raise ValueError(\"Labels must be a Vector\")\n        if labels.shape[0] not in array.shape:\n            raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n                             labels.shape, array.shape))\n        resolved_label_type = _resolve_type(labels.dtype)\n    resolved_type = _resolve_type(array.dtype)\n\n    # Write each vector in array into a Record in the file object\n    record = Record()\n    for index, vector in enumerate(array):\n        record.Clear()\n        _write_feature_tensor(resolved_type, record, vector)\n        if labels is not None:\n            _write_label_tensor(resolved_label_type, record, labels[index])\n        _write_recordio(file, record.SerializeToString())",
        "sampled": "```python\nimport numpy as np\n\ndef write_numpy_to_dense_tensor(file, array, labels=None):\n    \"\"\"\n    Writes a numpy array to a dense tensor\n    Parameters:\n        file (str): The path where the tensor will be saved to.\n        array (np.ndarray): The input numpy array\n        labels (list): Optional list of labels for each slice in the tensor (default is None)\n    \"\"\"\n\n    if not isinstance(array, np.ndarray):\n        raise ValueError(\"The 'array"
    },
    {
        "original": "def _set_axis_limits(self, axis, view, subplots, ranges):\n        \"\"\"\n        Compute extents for current view and apply as axis limits\n        \"\"\"\n        # Extents\n        extents = self.get_extents(view, ranges)\n        if not extents or self.overlaid:\n            axis.autoscale_view(scalex=True, scaley=True)\n            return\n\n        valid_lim = lambda c: util.isnumeric(c) and not np.isnan(c)\n        coords = [coord if np.isreal(coord) or isinstance(coord, np.datetime64) else np.NaN for coord in extents]\n        coords = [date2num(util.dt64_to_dt(c)) if isinstance(c, np.datetime64) else c\n                  for c in coords]\n        if self.projection == '3d' or len(extents) == 6:\n            l, b, zmin, r, t, zmax = coords\n            if self.invert_zaxis or any(p.invert_zaxis for p in subplots):\n                zmin, zmax = zmax, zmin\n            if zmin != zmax:\n                if valid_lim(zmin):\n                    axis.set_zlim(bottom=zmin)\n                if valid_lim(zmax):\n                    axis.set_zlim(top=zmax)\n        else:\n            l, b, r, t = coords\n\n        if self.invert_axes:\n            l, b, r, t = b, l, t, r\n\n        invertx = self.invert_xaxis or any(p.invert_xaxis for p in subplots)\n        xlim, scalex = self._compute_limits(l, r, self.logx, invertx, 'left', 'right')\n        inverty = self.invert_yaxis or any(p.invert_yaxis for p in subplots)\n        ylim, scaley =  self._compute_limits(b, t, self.logy, inverty, 'bottom', 'top')\n        if xlim:\n            axis.set_xlim(**xlim)\n        if ylim:\n            axis.set_ylim(**ylim)\n        axis.autoscale_view(scalex=scalex, scaley=scaley)",
        "sampled": "```python\ndef _set_axis_limits(self, axis, view, subplots, ranges):\n    \"\"\"\n    Compute extents for current view and apply as axis limits\n    \"\"\"\n\n    # Unpack axes extent from view dictionary\n    xlim = view['xlim']\n    ylim = view['ylim']\n\n    # Get the current figure and axes object\n    fig = self.figure  # Assuming self.figure is a reference to a matplotlib figure\n\n    axis = subplots[0] if len(subplots)"
    },
    {
        "original": "def AssertDictType(dct, expected_key_type, expected_value_type):\n  \"\"\"Ensures that given dictionary is actually a dictionary of specified type.\n\n  Args:\n    dct: A dictionary to assert the type for.\n    expected_key_type: An expected type for dictionary keys.\n    expected_value_type: An expected type for dictionary values.\n\n  Raises:\n    TypeError: If given dictionary is not really a dictionary or not all its\n               keys and values have the expected type.\n  \"\"\"\n  AssertType(dct, dict)\n  for key, value in iteritems(dct):\n    AssertType(key, expected_key_type)\n    AssertType(value, expected_value_type)",
        "sampled": "```python\ndef AssertDictType(dct, expected_key_type, expected_value_type):\n  \"\"\"Ensures that given dictionary is actually a dictionary of specified type.\n\n  Args:\n    dct: A dictionary to assert the type for.\n    expected_key_type: An expected type for dictionary keys.\n    expected_value_type: An expected type for dictionary values.\n\n  Raises:\n    TypeError: If given dictionary is not really a dictionary or not all its\n               keys and values have the expected type.\n "
    },
    {
        "original": "def exit_enable_mode(self, exit_command=\"exit\"):\n        \"\"\"Exit enable mode.\"\"\"\n        delay_factor = self.select_delay_factor(delay_factor=0)\n        output = \"\"\n        if self.check_enable_mode():\n            self.write_channel(self.normalize_cmd(exit_command))\n            time.sleep(0.3 * delay_factor)\n            self.set_base_prompt()\n            if self.check_enable_mode():\n                raise ValueError(\"Failed to exit enable mode.\")\n        return output",
        "sampled": "```python\nclass Switch:\n    def __init__(self):\n        self.in_enable_mode = False\n\n    def enter_enable_mode(self, enter_command=\"enable\"):\n        \"\"\"Enter enable mode.\"\"\"\n        self.in_enable_mode = True\n\n    def exit_enable_mode(self, exit_command=\"exit\"):\n        \"\"\"Exit enable mode.\"\"\"\n        if self.in_enable_mode:\n            print(f\"Exiting enable mode with command: {exit_command}\")\n            self.in_enable_mode = False\n        else:\n            print(\""
    },
    {
        "original": "def __get_league_object():\n    \"\"\"Returns the xml object corresponding to the league\n\n    Only designed for internal use\"\"\"\n    # get data\n    data = mlbgame.data.get_properties()\n    # return league object\n    return etree.parse(data).getroot().find('leagues').find('league')",
        "sampled": "```python\nimport xml.etree.ElementTree as ET\n\ndef __get_league_object():\n    \"\"\"\n    Returns the xml object corresponding to the league\n\n    Only designed for internal use\n    \"\"\"\n    # Assuming the XML file is named 'league.xml' and is in the same directory\n    tree = ET.parse('league.xml')\n    root = tree.getroot()\n\n    # Assuming the league object is the first child of the root element\n    league_object = root[0]\n\n    return"
    },
    {
        "original": "def pprint_value_string(self, value):\n        \"\"\"Pretty print the dimension value and unit.\n\n        Args:\n            value: Dimension value to format\n\n        Returns:\n            Formatted dimension value string with unit\n        \"\"\"\n        unit = '' if self.unit is None else ' ' + bytes_to_unicode(self.unit)\n        value = self.pprint_value(value)\n        return title_format.format(name=bytes_to_unicode(self.label), val=value, unit=unit)",
        "sampled": "```python\nclass DimensionFormatter:\n    def pprint_value_string(self, value):\n        \"\"\"\n        Pretty print the dimension value and unit.\n\n        Args:\n            value: Dimension value to format\n\n        Returns:\n            Formatted dimension value string with unit\n        \"\"\"\n        # Assuming value is a dictionary with 'value' and 'unit' keys\n        return f\"{value['value']} {value['unit']}\"\n```\n\nThis solution assumes that the `value` parameter is a dictionary with 'value"
    },
    {
        "original": "def make_grid(xx, yy):\n    \"\"\"\n    Returns two n-by-n matrices. The first one contains all the x values \n    and the second all the y values of a cartesian product between `xx` and `yy`.\n    \"\"\"\n    n = len(xx)\n    xx, yy = np.meshgrid(xx, yy)\n    grid = np.array([xx.ravel(), yy.ravel()]).T\n    x = grid[:, 0].reshape(n, n)\n    y = grid[:, 1].reshape(n, n)\n    return x, y",
        "sampled": "```python\nimport itertools\n\ndef make_grid(xx, yy):\n    \"\"\"\n    Returns two n-by-n matrices. The first one contains all the x values \n    and the second all the y values of a cartesian product between `xx` and `yy`.\n    \"\"\"\n    # Generate the cartesian product of xx and yy\n    cartesian_product = list(itertools.product(xx, yy))\n    \n    # Separate the x and y values\n    x_values = [x for x, y in cartesian_product"
    },
    {
        "original": "def _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n        \"\"\"Convert the job description to init params that can be handled by the class constructor\n\n        Args:\n            job_details: the returned job details from a describe_training_job API call.\n            model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n        Returns:\n             dictionary: The transformed init_params\n\n        \"\"\"\n        init_params = super(AmazonAlgorithmEstimatorBase, cls)._prepare_init_params_from_job_description(\n            job_details, model_channel_name)\n\n        # The hyperparam names may not be the same as the class attribute that holds them,\n        # for instance: local_lloyd_init_method is called local_init_method. We need to map these\n        # and pass the correct name to the constructor.\n        for attribute, value in cls.__dict__.items():\n            if isinstance(value, hp):\n                if value.name in init_params['hyperparameters']:\n                    init_params[attribute] = init_params['hyperparameters'][value.name]\n\n        del init_params['hyperparameters']\n        del init_params['image']\n        return init_params",
        "sampled": "```python\ndef _prepare_init_params_from_job_description(cls, job_details, model_channel_name=None):\n    \"\"\"\n    Convert the job description to init params that can be handled by the class constructor\n\n    Args:\n        job_details (dict): the returned job details from a describe_training_job API call.\n        model_channel_name (str): Name of the channel where pre-trained model data will be downloaded.\n\n    Returns:\n        dict: The transformed init_params\n    \"\"\"\n    init_params = {}\n\n   "
    },
    {
        "original": "def _repack_options(options):\n    \"\"\"\n    Repack the options data\n    \"\"\"\n    return dict(\n        [\n            (six.text_type(x), _normalize(y))\n            for x, y in six.iteritems(salt.utils.data.repack_dictlist(options))\n        ]\n    )",
        "sampled": "```python\ndef _repack_options(options):\n    \"\"\"\n    Repack the options data\n    \"\"\"\n    # Initialize an empty dictionary to store the repacked options\n    repacked_options = {}\n\n    # Iterate over each key-value pair in the options dictionary\n    for key, value in options.items():\n        # If the value is a dictionary, recursively repack it\n        if isinstance(value, dict):\n            repacked_options[key] = _repack_options(value)\n        # If the"
    },
    {
        "original": "def status(name, sig=None):\n    \"\"\"\n    Return the status for a service via rest_sample.\n    If the name contains globbing, a dict mapping service name to True/False\n    values is returned.\n\n    .. versionadded:: 2015.8.0\n\n    .. versionchanged:: 2018.3.0\n        The service name can now be a glob (e.g. ``salt*``)\n\n    Args:\n        name (str): The name of the service to check\n        sig (str): Not implemented\n\n    Returns:\n        bool: True if running, False otherwise\n        dict: Maps service name to True if running, False otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>\n    \"\"\"\n\n    proxy_fn = 'rest_sample.service_status'\n    contains_globbing = bool(re.search(r'\\*|\\?|\\[.+\\]', name))\n    if contains_globbing:\n        services = fnmatch.filter(get_all(), name)\n    else:\n        services = [name]\n    results = {}\n    for service in services:\n        resp = __proxy__[proxy_fn](service)\n        if resp['comment'] == 'running':\n            results[service] = True\n        else:\n            results[service] = False\n    if contains_globbing:\n        return results\n    return results[name]",
        "sampled": "```python\nimport subprocess\nimport glob\n\ndef status(name, sig=None):\n    \"\"\"\n    Return the status for a service via rest_sample.\n    If the name contains globbing, a dict mapping service name to True/False\n    values is returned.\n\n    Args:\n        name (str): The name of the service to check\n        sig (str): Not implemented\n\n    Returns:\n        bool: True if running, False otherwise\n        dict: Maps service name to True if running,"
    },
    {
        "original": "def get_colors(n, cmap='viridis', start=0., stop=1., alpha=1., return_hex=False):\n    \"\"\"\n    Return n-length list of RGBa colors from the passed colormap name and alpha.\n\n    Parameters\n    ----------\n    n : int\n        number of colors\n    cmap : string\n        name of a colormap\n    start : float\n        where to start in the colorspace\n    stop : float\n        where to end in the colorspace\n    alpha : float\n        opacity, the alpha channel for the RGBa colors\n    return_hex : bool\n        if True, convert RGBa colors to a hexadecimal string\n\n    Returns\n    -------\n    colors : list\n    \"\"\"\n    colors = [cm.get_cmap(cmap)(x) for x in np.linspace(start, stop, n)]\n    colors = [(r, g, b, alpha) for r, g, b, _ in colors]\n    if return_hex:\n        colors = rgb_color_list_to_hex(colors)\n    return colors",
        "sampled": "```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import LinearSegmentedColormap\n\ndef get_colors(n, cmap='viridis', start=0., stop=1., alpha=1., return_hex=False):\n    \"\"\"\n    Return n-length list of RGBa colors from the passed colormap name and alpha.\n\n    Parameters\n    ----------\n    n : int\n        number of colors\n    cmap : string\n        name of a colormap\n    start : float\n"
    },
    {
        "original": "def get_by_oid(self, *oid):\n        \"\"\"SNMP simple request (list of OID).\n\n        One request per OID list.\n\n        * oid: oid list\n        > Return a dict\n        \"\"\"\n        if self.version == '3':\n            errorIndication, errorStatus, errorIndex, varBinds = self.cmdGen.getCmd(\n                cmdgen.UsmUserData(self.user, self.auth),\n                cmdgen.UdpTransportTarget((self.host, self.port)),\n                *oid\n            )\n        else:\n            errorIndication, errorStatus, errorIndex, varBinds = self.cmdGen.getCmd(\n                cmdgen.CommunityData(self.community),\n                cmdgen.UdpTransportTarget((self.host, self.port)),\n                *oid\n            )\n        return self.__get_result__(errorIndication, errorStatus, errorIndex, varBinds)",
        "sampled": "```python\nclass SNMP:\n    def get_by_oid(self, *oid):\n        \"\"\"\n        SNMP simple request (list of OID).\n\n        One request per OID list.\n\n        * oid: oid list\n        > Return a dict\n        \"\"\"\n        # Initialize an empty dictionary to store the results\n        result = {}\n\n        # Iterate over each OID in the list\n        for o in oid:\n            # Assuming we have a function to get the value of an OID\n            # For simplicity,"
    },
    {
        "original": "def _add_width_of(self, other_tc):\n        \"\"\"\n        Add the width of *other_tc* to this cell. Does nothing if either this\n        tc or *other_tc* does not have a specified width.\n        \"\"\"\n        if self.width and other_tc.width:\n            self.width += other_tc.width",
        "sampled": "```python\nclass TableCell:\n    def __init__(self, width=None):\n        self.width = width\n\n    def _add_width_of(self, other_tc):\n        \"\"\"\n        Add the width of *other_tc* to this cell. Does nothing if either this\n        tc or *other_tc* does not have a specified width.\n        \"\"\"\n        if self.width is not None and other_tc.width is not None:\n            self.width += other_tc.width\n\n\n# Example usage:\ntc1 ="
    },
    {
        "original": "def _netcdf2pandas(self, netcdf_data, query_variables, start, end):\n        \"\"\"\n        Transforms data from netcdf to pandas DataFrame.\n\n        Parameters\n        ----------\n        data: netcdf\n            Data returned from UNIDATA NCSS query.\n        query_variables: list\n            The variables requested.\n        start: Timestamp\n            The start time\n        end: Timestamp\n            The end time\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        # set self.time\n        try:\n            time_var = 'time'\n            self.set_time(netcdf_data.variables[time_var])\n        except KeyError:\n            # which model does this dumb thing?\n            time_var = 'time1'\n            self.set_time(netcdf_data.variables[time_var])\n\n        data_dict = {}\n        for key, data in netcdf_data.variables.items():\n            # if accounts for possibility of extra variable returned\n            if key not in query_variables:\n                continue\n            squeezed = data[:].squeeze()\n            if squeezed.ndim == 1:\n                data_dict[key] = squeezed\n            elif squeezed.ndim == 2:\n                for num, data_level in enumerate(squeezed.T):\n                    data_dict[key + '_' + str(num)] = data_level\n            else:\n                raise ValueError('cannot parse ndim > 2')\n\n        data = pd.DataFrame(data_dict, index=self.time)\n        # sometimes data is returned as hours since T0\n        # where T0 is before start. Then the hours between\n        # T0 and start are added *after* end. So sort and slice\n        # to remove the garbage\n        data = data.sort_index().loc[start:end]\n        return data",
        "sampled": "```python\nimport netCDF4 as nc\nimport pandas as pd\nimport numpy as np\n\ndef _netcdf2pandas(self, netcdf_data, query_variables, start, end):\n    \"\"\"\n    Transforms data from netcdf to pandas DataFrame.\n\n    Parameters\n    ----------\n    netcdf_data: netCDF4.Dataset\n        Data returned from UNIDATA NCSS query.\n    query_variables: list\n        The variables requested.\n    start: Timestamp\n        The start time\n"
    },
    {
        "original": "def get_bgp_neighbors(self):\n        def generate_vrf_query(vrf_name):\n            \"\"\"\n            Helper to provide XML-query for the VRF-type we're interested in.\n            \"\"\"\n            if vrf_name == \"global\":\n                rpc_command = \"<Get><Operational><BGP><InstanceTable><Instance><Naming>\\\n                <InstanceName>default</InstanceName></Naming><InstanceActive><DefaultVRF>\\\n                <GlobalProcessInfo></GlobalProcessInfo><NeighborTable></NeighborTable></DefaultVRF>\\\n                </InstanceActive></Instance></InstanceTable></BGP></Operational></Get>\"\n\n            else:\n                rpc_command = \"<Get><Operational><BGP><InstanceTable><Instance><Naming>\\\n                <InstanceName>default</InstanceName></Naming><InstanceActive><VRFTable><VRF>\\\n                <Naming>{vrf_name}</Naming><GlobalProcessInfo></GlobalProcessInfo><NeighborTable>\\\n                </NeighborTable></VRF></VRFTable></InstanceActive></Instance></InstanceTable>\\\n                </BGP></Operational></Get>\".format(\n                    vrf_name=vrf_name\n                )\n            return rpc_command\n\n        ",
        "sampled": "```python\nclass BGPSystem:\n    def get_bgp_neighbors(self):\n        # Assuming bgp_neighbors is a list of dictionaries where each dictionary contains neighbor information\n        bgp_neighbors = [\n            {\"neighbor_ip\": \"192.168.1.1\", \"neighbor_as\": 100},\n            {\"neighbor_ip\": \"192.168.1.2\", \"neighbor_as\": 200},\n            {\"neighbor_ip\": \"192.168.1.3\", \"neighbor_as\":"
    },
    {
        "original": "def from_soup(self, tag_prof_header, tag_prof_nav):\n        \"\"\"\n        Returns the scraped user data from a twitter user page.\n\n        :param tag_prof_header: captures the left hand part of user info\n        :param tag_prof_nav: captures the upper part of user info\n        :return: Returns a User object with captured data via beautifulsoup\n        \"\"\"\n\n        self.user= tag_prof_header.find('a', {'class':'ProfileHeaderCard-nameLink u-textInheritColor js-nav'})['href'].strip(\"/\") \n        self.full_name = tag_prof_header.find('a', {'class':'ProfileHeaderCard-nameLink u-textInheritColor js-nav'}).text\n        \n        location = tag_prof_header.find('span', {'class':'ProfileHeaderCard-locationText u-dir'}) \n        if location is None:\n            self.location = \"None\"\n        else: \n            self.location = location.text.strip()\n\n        blog = tag_prof_header.find('span', {'class':\"ProfileHeaderCard-urlText u-dir\"})\n        if blog is None:\n            blog = \"None\"\n        else:\n            self.blog = blog.text.strip() \n\n        date_joined = tag_prof_header.find('div', {'class':\"ProfileHeaderCard-joinDate\"}).find('span', {'class':'ProfileHeaderCard-joinDateText js-tooltip u-dir'})['title']\n        if date_joined is None:\n            self.data_joined = \"Unknown\"\n        else:    \n            self.date_joined = date_joined.strip()\n\n        self.id = tag_prof_nav.find('div',{'class':'ProfileNav'})['data-user-id']\n        tweets = tag_prof_nav.find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if tweets is None:\n            self.tweets = 0\n        else:\n            self.tweets = int(tweets)\n\n        following = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--following\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if following is None:\n            following = 0\n        else:\n            self.following = int(following)\n\n        followers = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--followers\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if followers is None:\n            self.followers = 0\n        else:\n            self.followers = int(followers)    \n        \n        likes = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--favorites\"}).\\\n        find('span', {'class':\"ProfileNav-value\"})['data-count']\n        if likes is None:\n            self.likes = 0\n        else:\n            self.likes = int(likes)    \n        \n        lists = tag_prof_nav.find('li', {'class':\"ProfileNav-item ProfileNav-item--lists\"})\n        if lists is None:\n            self.lists = 0\n        elif lists.find('span', {'class':\"ProfileNav-value\"}) is None:    \n            self.lists = 0\n        else:    \n            lists = lists.find('span', {'class':\"ProfileNav-value\"}).text    \n            self.lists = int(lists)\n        return(self)",
        "sampled": "```python\nfrom bs4 import BeautifulSoup\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    \"\"\"Represents a Twitter user.\"\"\"\n    name: str\n    username: str\n    location: str\n    website: str\n    description: str\n\ndef from_soup(self, tag_prof_header, tag_prof_nav):\n    \"\"\"\n    Returns the scraped user data from a Twitter user page.\n\n    :param tag_prof_header: captures the left hand part of user info\n   "
    },
    {
        "original": "def _sequence_handler(self, cfg, irsb, irsb_addr, stmt_idx, data_addr, max_size):  # pylint:disable=unused-argument\n        \"\"\"\n        Find sequences in binary data.\n\n        :param angr.analyses.CFG cfg: The control flow graph.\n        :param pyvex.IRSB irsb: The IRSB object.\n        :param int irsb_addr: Address of the block.\n        :param int stmt_idx: Statement ID.\n        :param int data_addr: Address of the data in memory.\n        :param int max_size: Maximum size possible.\n        :return: A 2-tuple of data type and size.\n        :rtype: tuple\n        \"\"\"\n\n        if not self._is_sequence(cfg, data_addr, 5):\n            # fail-fast\n            return None, None\n\n        sequence_max_size = min(256, max_size)\n\n        for i in range(5, min(256, max_size)):\n            if not self._is_sequence(cfg, data_addr, i):\n                return 'sequence', i - 1\n\n        return 'sequence', sequence_max_size",
        "sampled": "```python\nimport angr\nimport pyvex\n\ndef _sequence_handler(self, cfg, irsb, irsb_addr, stmt_idx, data_addr, max_size):\n    \"\"\"\n    Find sequences in binary data.\n\n    :param angr.analyses.CFG cfg: The control flow graph.\n    :param pyvex.IRSB irsb: The IRSB object.\n    :param int irsb_addr: Address of the block.\n    :param int stmt_idx: Statement"
    },
    {
        "original": "def graphql_query(self, query_hash: str, variables: Dict[str, Any],\n                      referer: Optional[str] = None, rhx_gis: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Do a GraphQL Query.\n\n        :param query_hash: Query identifying hash.\n        :param variables: Variables for the Query.\n        :param referer: HTTP Referer, or None.\n        :param rhx_gis: 'rhx_gis' variable as somewhere returned by Instagram, needed to 'sign' request\n        :return: The server's response dictionary.\n        \"\"\"\n        with copy_session(self._session) as tmpsession:\n            tmpsession.headers.update(self._default_http_header(empty_session_only=True))\n            del tmpsession.headers['Connection']\n            del tmpsession.headers['Content-Length']\n            tmpsession.headers['authority'] = 'www.instagram.com'\n            tmpsession.headers['scheme'] = 'https'\n            tmpsession.headers['accept'] = '*/*'\n            if referer is not None:\n                tmpsession.headers['referer'] = urllib.parse.quote(referer)\n\n            variables_json = json.dumps(variables, separators=(',', ':'))\n\n            if rhx_gis:\n                #self.log(\"rhx_gis {} query_hash {}\".format(rhx_gis, query_hash))\n                values = \"{}:{}\".format(rhx_gis, variables_json)\n                x_instagram_gis = hashlib.md5(values.encode()).hexdigest()\n                tmpsession.headers['x-instagram-gis'] = x_instagram_gis\n\n            resp_json = self.get_json('graphql/query',\n                                      params={'query_hash': query_hash,\n                                              'variables': variables_json},\n                                      session=tmpsession)\n        if 'status' not in resp_json:\n            self.error(\"GraphQL response did not contain a \\\"status\\\" field.\")\n        return resp_json",
        "sampled": "```python\nimport requests\nfrom typing import Dict, Any, Optional\n\ndef graphql_query(self, query_hash: str, variables: Dict[str, Any],\n                      referer: Optional[str] = None, rhx_gis: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"\n    Do a GraphQL Query.\n\n    :param query_hash: Query identifying hash.\n    :param variables: Variables for the Query.\n    :param referer: HTTP Referer, or None.\n"
    },
    {
        "original": "def save_lastnode_id():\n    \"\"\"Save the id of the last node created.\"\"\"\n    init_counter()\n\n    with FileLock(_COUNTER_FILE):\n        with AtomicFile(_COUNTER_FILE, mode=\"w\") as fh:\n            fh.write(\"%d\\n\" % _COUNTER)",
        "sampled": "```python\nclass Node:\n    \"\"\"Represents a node in a graph.\"\"\"\n    id_counter = 0\n\n    def __init__(self):\n        \"\"\"Initializes a new node.\"\"\"\n        self.id = Node.id_counter\n        Node.id_counter += 1\n\n    @classmethod\n    def save_lastnode_id(cls):\n        \"\"\"Save the id of the last node created.\"\"\"\n        return cls.id_counter - 1\n\n\n# Example usage:\nnode1 = Node()\nnode2 = Node()\n"
    },
    {
        "original": "def _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n                log.debug('Have deserialized data %s', _existingData)\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecutionError(error)\n            if 'Section' in policy_info['ScriptIni'] and policy_info['ScriptIni']['Section'].lower() in [z.lower() for z in _existingData.keys()]:\n                if 'SettingName' in policy_info['ScriptIni']:\n                    log.debug('Need to look for %s', policy_info['ScriptIni']['SettingName'])\n                    if policy_info['ScriptIni']['SettingName'].lower() in [z.lower() for z in _existingData[policy_info['ScriptIni']['Section']].keys()]:\n                        return _existingData[policy_info['ScriptIni']['Section']][policy_info['ScriptIni']['SettingName'].lower()]\n                    else:\n                        return None\n                else:\n                    return _existingData[policy_info['ScriptIni']['Section']]\n            else:\n                return None\n\n    return None",
        "sampled": "```python\nimport configparser\n\ndef _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n\n    # Create a ConfigParser object to read the ini file\n    config = configparser.ConfigParser()\n\n"
    },
    {
        "original": "def CreateCounterMetadata(metric_name, fields=None, docstring=None, units=None):\n  \"\"\"Helper function for creating MetricMetadata for counter metrics.\"\"\"\n  return rdf_stats.MetricMetadata(\n      varname=metric_name,\n      metric_type=rdf_stats.MetricMetadata.MetricType.COUNTER,\n      value_type=rdf_stats.MetricMetadata.ValueType.INT,\n      fields_defs=FieldDefinitionProtosFromTuples(fields or []),\n      docstring=docstring,\n      units=units)",
        "sampled": "```python\nclass MetricMetadata:\n    def __init__(self, metric_name, fields=None, docstring=None, units=None):\n        self.metric_name = metric_name\n        self.fields = fields if fields else []\n        self.docstring = docstring if docstring else \"\"\n        self.units = units if units else \"\"\n\ndef CreateCounterMetadata(metric_name, fields=None, docstring=None, units=None):\n    return MetricMetadata(metric_name=metric_name,\n                          fields=fields,\n                          docstring"
    },
    {
        "original": "def _mark_unknowns(self):\n        \"\"\"\n        Mark all unmapped regions.\n\n        :return: None\n        \"\"\"\n\n        for obj in self.project.loader.all_objects:\n            if isinstance(obj, cle.ELF):\n                # sections?\n                if obj.sections:\n                    for section in obj.sections:\n                        if not section.memsize or not section.vaddr:\n                            continue\n                        min_addr, max_addr = section.min_addr, section.max_addr\n                        self._mark_unknowns_core(min_addr, max_addr + 1, obj=obj, section=section)\n                elif obj.segments:\n                    for segment in obj.segments:\n                        if not segment.memsize:\n                            continue\n                        min_addr, max_addr = segment.min_addr, segment.max_addr\n                        self._mark_unknowns_core(min_addr, max_addr + 1, obj=obj, segment=segment)\n                else:\n                    # is it empty?\n                    _l.warning(\"Empty ELF object %s.\", repr(obj))\n            elif isinstance(obj, cle.PE):\n                if obj.sections:\n                    for section in obj.sections:\n                        if not section.memsize:\n                            continue\n                        min_addr, max_addr = section.min_addr, section.max_addr\n                        self._mark_unknowns_core(min_addr, max_addr + 1, obj=obj, section=section)\n                else:\n                    # is it empty?\n                    _l.warning(\"Empty PE object %s.\", repr(obj))\n            else:\n                min_addr, max_addr = obj.min_addr, obj.max_addr\n                self._mark_unknowns_core(min_addr, max_addr + 1, obj=obj)",
        "sampled": "```python\ndef _mark_unknowns(self):\n    \"\"\"\n    Mark all unmapped regions.\n\n    :return: None\n    \"\"\"\n    \n    # Assuming self.grid is a 2D list representing the map, where 0 represents an unmapped region and 1 represents a mapped region\n    for i in range(len(self.grid)):\n        for j in range(len(self.grid[0])):\n            if self.grid[i][j] == 0:\n                # If the current cell is unmapped"
    },
    {
        "original": "def StringEscape(self, string, match, **_):\n    \"\"\"Escape backslashes found inside a string quote.\n\n    Backslashes followed by anything other than ['\"rnbt] will just be included\n    in the string.\n\n    Args:\n       string: The string that matched.\n       match: The match object (m.group(1) is the escaped code)\n    \"\"\"\n    precondition.AssertType(string, Text)\n    if match.group(1) in \"'\\\"rnbt\":\n      self.string += compatibility.UnescapeString(string)\n    else:\n      self.string += string",
        "sampled": "```python\ndef StringEscape(self, string, match, **_):\n    \"\"\"\n    Escape backslashes found inside a string quote.\n\n    Backslashes followed by anything other than ['\"rnbt] will just be included\n    in the string.\n\n    Args:\n       string: The string that matched.\n       match: The match object (m.group(1) is the escaped code)\n    \"\"\"\n    \n    # Get the escaped code from the match object\n    escaped_code = match.group(1"
    },
    {
        "original": "def FromMany(cls, samples):\n    \"\"\"Constructs a single sample that best represents a list of samples.\n\n    Args:\n      samples: An iterable collection of `IOSample` instances.\n\n    Returns:\n      An `IOSample` instance representing `samples`.\n\n    Raises:\n      ValueError: If `samples` is empty.\n    \"\"\"\n    if not samples:\n      raise ValueError(\"Empty `samples` argument\")\n\n    return IOSample(\n        timestamp=max(sample.timestamp for sample in samples),\n        read_bytes=max(sample.read_bytes for sample in samples),\n        write_bytes=max(sample.write_bytes for sample in samples))",
        "sampled": "```python\nfrom typing import Iterable\n\nclass IOSample:\n    def __init__(self, value: float):\n        self.value = value\n\ndef FromMany(cls, samples: Iterable[IOSample]) -> IOSample:\n    \"\"\"\n    Constructs a single sample that best represents a list of samples.\n\n    Args:\n      samples: An iterable collection of `IOSample` instances.\n\n    Returns:\n      An `IOSample` instance representing `samples`.\n\n    Raises:\n      ValueError: If `samples`"
    },
    {
        "original": "def _revoked_to_list(revs):\n    \"\"\"\n    Turn the mess of OrderedDicts and Lists into a list of dicts for\n    use in the CRL module.\n    \"\"\"\n    list_ = []\n\n    for rev in revs:\n        for rev_name, props in six.iteritems(\n                rev):             # pylint: disable=unused-variable\n            dict_ = {}\n            for prop in props:\n                for propname, val in six.iteritems(prop):\n                    if isinstance(val, datetime.datetime):\n                        val = val.strftime('%Y-%m-%d %H:%M:%S')\n                    dict_[propname] = val\n            list_.append(dict_)\n\n    return list_",
        "sampled": "```python\nfrom collections import OrderedDict\n\ndef _revoked_to_list(revs):\n    \"\"\"\n    Turn the mess of OrderedDicts and Lists into a list of dicts for\n    use in the CRL module.\n    \"\"\"\n    \n    # Initialize an empty list to store the result\n    result = []\n    \n    # Iterate over each item in revs (assuming it's a list or other iterable)\n    for item in revs:\n        # If item is an OrderedDict, convert it to a"
    },
    {
        "original": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()",
        "sampled": "```python\nimport threading\n\nclass Barrier:\n    def __init__(self, n):\n        self.n = n\n        self.count = 0\n        self.lock = threading.Lock()\n        self.cond = threading.Condition(self.lock)\n\n    def wait(self):\n        with self.cond:\n            while True:\n                if self.count == 0 or (self.count == 1 and not hasattr(threading.current_thread(), 'arrived')):\n                    # If this is the first thread to arrive, mark it"
    },
    {
        "original": "def _qnwtrap1(n, a, b):\n    \"\"\"\n    Compute univariate trapezoid rule quadrature nodes and weights\n\n    Parameters\n    ----------\n    n : int\n        The number of nodes\n\n    a : int\n        The lower endpoint\n\n    b : int\n        The upper endpoint\n\n    Returns\n    -------\n    nodes : np.ndarray(dtype=float)\n        An n element array of nodes\n\n    nodes : np.ndarray(dtype=float)\n        An n element array of weights\n\n    Notes\n    -----\n    Based of original function ``qnwtrap1`` in CompEcon toolbox by\n    Miranda and Fackler\n\n    References\n    ----------\n    Miranda, Mario J, and Paul L Fackler. Applied Computational\n    Economics and Finance, MIT Press, 2002.\n\n    \"\"\"\n    if n < 1:\n        raise ValueError(\"n must be at least one\")\n\n    nodes = np.linspace(a, b, n)\n    dx = nodes[1] - nodes[0]\n\n    weights = dx * np.ones(n)\n    weights[0] *= 0.5\n    weights[-1] *= 0.5\n\n    return nodes, weights",
        "sampled": "```python\nimport numpy as np\n\ndef _qnwtrap1(n, a, b):\n    \"\"\"\n    Compute univariate trapezoid rule quadrature nodes and weights\n\n    Parameters\n    ----------\n    n : int\n        The number of nodes\n\n    a : int\n        The lower endpoint\n\n    b : int\n        The upper endpoint\n\n    Returns\n    -------\n       h : float or array:\n            Each value represents half the distance between the endpoints.\n\n       xwi ="
    },
    {
        "original": "def __extend_with_api_ref(raw_testinfo):\n    \"\"\" extend with api reference\n\n    Raises:\n        exceptions.ApiNotFound: api not found\n\n    \"\"\"\n    api_name = raw_testinfo[\"api\"]\n\n    # api maybe defined in two types:\n    # 1, individual file: each file is corresponding to one api definition\n    # 2, api sets file: one file contains a list of api definitions\n    if not os.path.isabs(api_name):\n        # make compatible with Windows/Linux\n        api_path = os.path.join(tests_def_mapping[\"PWD\"], *api_name.split(\"/\"))\n        if os.path.isfile(api_path):\n            # type 1: api is defined in individual file\n            api_name = api_path\n\n    try:\n        block = tests_def_mapping[\"api\"][api_name]\n        # NOTICE: avoid project_mapping been changed during iteration.\n        raw_testinfo[\"api_def\"] = utils.deepcopy_dict(block)\n    except KeyError:\n        raise exceptions.ApiNotFound(\"{} not found!\".format(api_name))",
        "sampled": "```python\nclass ApiNotFound(Exception):\n    pass\n\n\ndef __extend_with_api_ref(raw_testinfo):\n    \"\"\"\n    extend with api reference\n\n    Raises:\n        ApiNotFound: api not found\n\n    \"\"\"\n    \n    # For simplicity, let's assume we have an API reference dictionary\n    api_reference = {\n        'get': 'https://developer.example.com/apidocs/#get',\n        'post': 'https://developer.example.com/apidocs/#post',\n        # Add more API"
    },
    {
        "original": "def lcm( *a ):\n  \"\"\"Least common multiple.\n\n  Usage: lcm( [ 3, 4, 5 ] )\n  or:    lcm( 3, 4, 5 )\n  \"\"\"\n\n  if len( a ) > 1: return reduce( lcm2, a )\n  if hasattr( a[0], \"__iter__\" ): return reduce( lcm2, a[0] )\n  return a[0]",
        "sampled": "```python\nimport math\n\ndef lcm(*a):\n    \"\"\"Least common multiple.\n\n    Usage: lcm( [ 3, 4, 5 ] )\n    or:    lcm( 3, 4, 5 )\n    \"\"\"\n    \n    def gcd(a, b):\n        while b:\n            a, b = b, a % b\n        return a\n    \n    result = a[0]\n    \n    for i in range(1,len(a)):\n        result = abs(result"
    },
    {
        "original": "def find(self, name, menu=None):\n        \"\"\"\n            Finds a menu item by name and returns it.\n\n            :param name:\n                The menu item name.\n        \"\"\"\n        menu = menu or self.menu\n        for i in menu:\n            if i.name == name:\n                return i\n            else:\n                if i.childs:\n                    ret_item = self.find(name, menu=i.childs)\n                    if ret_item:\n                        return ret_item",
        "sampled": "```python\nclass MenuItem:\n    def __init__(self, name, price):\n        \"\"\"\n        Initializes a MenuItem with name and price.\n\n        :param name:\n            The menu item name.\n        :param price:\n            The menu item price.\n        \"\"\"\n        self.name = name\n        self.price = price\n\nclass Menu:\n    def __init__(self):\n        \"\"\"\n        Initializes an empty menu.\n        \"\"\"\n        self.menu_items = {}\n\n    def add_item(self, item):\n        \"\"\"\n"
    },
    {
        "original": "def get_row(self, index):\n        \"\"\" Returns a Row instance at an index \"\"\"\n        url = self.build_url(self._endpoints.get('get_row').format(id=index))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.row_constructor(parent=self, **{self._cloud_data_key: response.json()})",
        "sampled": "```python\nclass Row:\n    def __init__(self, index):\n        self.index = index\n\n    def get_value(self):\n        return 2 ** self.index\n\n\nclass PascalTriangle:\n    def __init__(self, n):\n        self.n = n\n        self.rows = [Row(i) for i in range(n)]\n\n    def get_row(self, index):\n        if 0 <= index < self.n:\n            return self.rows[index]\n        else:\n            raise IndexError(\"Index"
    },
    {
        "original": "def parse_node(self, node, node_path, package_project_config, tags=None,\n                   fqn_extra=None, fqn=None, agate_table=None,\n                   archive_config=None, column_name=None):\n        \"\"\"Parse a node, given an UnparsedNode and any other required information.\n\n        agate_table should be set if the node came from a seed file.\n        archive_config should be set if the node is an Archive node.\n        column_name should be set if the node is a Test node associated with a\n        particular column.\n        \"\"\"\n        logger.debug(\"Parsing {}\".format(node_path))\n\n        tags = coalesce(tags, [])\n        fqn_extra = coalesce(fqn_extra, [])\n\n        if fqn is None:\n            fqn = self.get_fqn(node, package_project_config, fqn_extra)\n\n        config = SourceConfig(\n            self.root_project_config,\n            package_project_config,\n            fqn,\n            node.resource_type)\n\n        parsed_dict = self._build_intermediate_node_dict(\n            config, node.serialize(), node_path, config, tags, fqn,\n            agate_table, archive_config, column_name\n        )\n        parsed_node = ParsedNode(**parsed_dict)\n\n        self._render_with_context(parsed_node, config)\n        self._update_parsed_node_info(parsed_node, config)\n\n        parsed_node.validate()\n\n        return parsed_node",
        "sampled": "```python\nclass NodeParser:\n    def parse_node(self, node, node_path, package_project_config, tags=None,\n                   fqn_extra=None, fqn=None, agate_table=None,\n                   archive_config=None, column_name=None):\n        \"\"\"\n        Parse a node, given an UnparsedNode and any other required information.\n\n        Args:\n            node (object): The unparsed node object.\n            node_path (str): The path of the current node.\n            package_project_config (dict"
    },
    {
        "original": "def list_queues(region, opts=None, user=None):\n    \"\"\"\n    List the queues in the selected region.\n\n    region\n        Region to list SQS queues for\n\n    opts : None\n        Any additional options to add to the command line\n\n    user : None\n        Run hg as a user other than what the minion runs as\n\n    CLI Example:\n\n        salt '*' aws_sqs.list_queues <region>\n\n    \"\"\"\n    out = _run_aws('list-queues', region, opts, user)\n\n    ret = {\n        'retcode': 0,\n        'stdout': out['QueueUrls'],\n    }\n    return ret",
        "sampled": "```python\nimport boto3\n\ndef list_queues(region, opts=None, user=None):\n    \"\"\"\n    List the queues in the selected region.\n\n    region\n        Region to list SQS queues for\n\n    opts : None\n        Any additional options to add to the command line\n\n    user : None\n        Run hg as a user other than what the minion runs as\n\n    CLI Example:\n\n        salt '*' aws_sqs.list_queues <region>\n\n    \"\"\"\n    \n    # Initialize an empty list"
    },
    {
        "original": "def acquire(self, blocking=None, blocking_timeout=None, token=None):\n        \"\"\"\n        Use Redis to hold a shared, distributed lock named ``name``.\n        Returns True once the lock is acquired.\n\n        If ``blocking`` is False, always return immediately. If the lock\n        was acquired, return True, otherwise return False.\n\n        ``blocking_timeout`` specifies the maximum number of seconds to\n        wait trying to acquire the lock.\n\n        ``token`` specifies the token value to be used. If provided, token\n        must be a bytes object or a string that can be encoded to a bytes\n        object with the default encoding. If a token isn't specified, a UUID\n        will be generated.\n        \"\"\"\n        sleep = self.sleep\n        if token is None:\n            token = uuid.uuid1().hex.encode()\n        else:\n            encoder = self.redis.connection_pool.get_encoder()\n            token = encoder.encode(token)\n        if blocking is None:\n            blocking = self.blocking\n        if blocking_timeout is None:\n            blocking_timeout = self.blocking_timeout\n        stop_trying_at = None\n        if blocking_timeout is not None:\n            stop_trying_at = mod_time.time() + blocking_timeout\n        while True:\n            if self.do_acquire(token):\n                self.local.token = token\n                return True\n            if not blocking:\n                return False\n            if stop_trying_at is not None and mod_time.time() > stop_trying_at:\n                return False\n            mod_time.sleep(sleep)",
        "sampled": "```python\nimport redis\nimport time\nimport uuid\n\nclass DistributedLock:\n    def __init__(self, host='localhost', port=6379, db=0):\n        self.redis_client = redis.Redis(host=host, port=port, db=db)\n\n    def acquire(self, name, blocking=True, timeout=None, token=None):\n        if token is None:\n            token = str(uuid.uuid4())\n\n        if not blocking or timeout is None:\n            return self.check_lock(name),"
    },
    {
        "original": "def _split_rules(rules):\n    \"\"\"\n    Split rules with combined grants into individual rules.\n\n    Amazon returns a set of rules with the same protocol, from and to ports\n    together as a single rule with a set of grants. Authorizing and revoking\n    rules, however, is done as a split set of rules. This function splits the\n    rules up.\n    \"\"\"\n    split = []\n    for rule in rules:\n        ip_protocol = rule.get('ip_protocol')\n        to_port = rule.get('to_port')\n        from_port = rule.get('from_port')\n        grants = rule.get('grants')\n        for grant in grants:\n            _rule = {'ip_protocol': ip_protocol,\n                     'to_port': to_port,\n                     'from_port': from_port}\n            for key, val in six.iteritems(grant):\n                _rule[key] = val\n            split.append(_rule)\n    return split",
        "sampled": "```python\ndef _split_rules(rules):\n    \"\"\"\n    Split rules with combined grants into individual rules.\n\n    Amazon returns a set of rules with the same protocol, from and to ports\n    together as a single rule with a set of grants. Authorizing and revoking\n    rules, however, is done as a split set of rules. This function splits the\n    rules up.\n    \n    Args:\n        - `rules`: A list of tuples where each tuple contains: \n            ("
    },
    {
        "original": "def _outer_values_update(self, full_values):\n        \"\"\"\n        Here you put the values, which were collected before in the right places.\n        E.g. set the gradients of parameters, etc.\n        \"\"\"\n        if self.has_uncertain_inputs():\n            #gradients wrt kernel\n            dL_dKmm = full_values['dL_dKmm']\n            self.kern.update_gradients_full(dL_dKmm, self.Z, None)\n            kgrad = self.kern.gradient.copy()\n            self.kern.update_gradients_expectations(\n                                                variational_posterior=self.X,\n                                                Z=self.Z, dL_dpsi0=full_values['dL_dpsi0'],\n                                                dL_dpsi1=full_values['dL_dpsi1'],\n                                                dL_dpsi2=full_values['dL_dpsi2'])\n            self.kern.gradient += kgrad\n\n\n            #gradients wrt Z\n            self.Z.gradient = self.kern.gradients_X(dL_dKmm, self.Z)\n            self.Z.gradient += self.kern.gradients_Z_expectations(\n                                            variational_posterior=self.X,\n                                            Z=self.Z, dL_dpsi0=full_values['dL_dpsi0'],\n                                            dL_dpsi1=full_values['dL_dpsi1'],\n                                            dL_dpsi2=full_values['dL_dpsi2'])\n        else:\n            #gradients wrt kernel\n            self.kern.update_gradients_diag(full_values['dL_dKdiag'], self.X)\n            kgrad = self.kern.gradient.copy()\n            self.kern.update_gradients_full(full_values['dL_dKnm'], self.X, self.Z)\n            kgrad += self.kern.gradient\n            self.kern.update_gradients_full(full_values['dL_dKmm'], self.Z, None)\n            self.kern.gradient += kgrad\n            #kgrad += self.kern.gradient\n\n            #gradients wrt Z\n            self.Z.gradient = self.kern.gradients_X(full_values['dL_dKmm'], self.Z)\n            self.Z.gradient += self.kern.gradients_X(full_values['dL_dKnm'].T, self.Z, self.X)\n\n        self.likelihood.update_gradients(full_values['dL_dthetaL'])",
        "sampled": "```python\nclass Model:\n    def __init__(self, params):\n        self.params = params\n        self.gradients = {param: 0 for param in params}\n\n    def _outer_values_update(self, full_values):\n        \"\"\"\n        Here you put the values, which were collected before in the right places.\n        E.g. set the gradients of parameters, etc.\n        \n        - Parameters:\n            - full_values (list): Contains gradient values.\n        \n        - Returns:\n            None"
    },
    {
        "original": "def get_authorizations(self):\n        \"\"\"\n        :calls: `GET /authorizations <http://developer.github.com/v3/oauth>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Authorization.Authorization`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Authorization.Authorization,\n            self._requester,\n            \"/authorizations\",\n            None\n        )",
        "sampled": "```python\nclass GitHubAPIClient:\n    def __init__(self, base_url='https://api.github.com/'):\n        \"\"\"\n        :param base_url: The base url for github api\n        \"\"\"\n        self.base_url = base_url\n\n    def get_authorizations(self):\n        \"\"\"\n        :calls: `GET /authorizations <http://developer.github.com/v3/oauth>`_\n        :rtype: PaginatedList of Authorization\n        \"\"\"\n        \n        # Construct the full API URL for"
    },
    {
        "original": "def split_indexes(\n    dims_or_levels,  # type: Union[Any, List[Any]]\n    variables,  # type: OrderedDict[Any, Variable]\n    coord_names,  # type: Set\n    level_coords,  # type: Dict[Any, Any]\n    drop=False,  # type: bool\n):\n    # type: (...) -> Tuple[OrderedDict[Any, Variable], Set]\n    \"\"\"Extract (multi-)indexes (levels) as variables.\n\n    Not public API. Used in Dataset and DataArray reset_index\n    methods.\n    \"\"\"\n    if isinstance(dims_or_levels, str):\n        dims_or_levels = [dims_or_levels]\n\n    dim_levels = defaultdict(list)  # type: Dict[Any, list]\n    dims = []\n    for k in dims_or_levels:\n        if k in level_coords:\n            dim_levels[level_coords[k]].append(k)\n        else:\n            dims.append(k)\n\n    vars_to_replace = {}\n    vars_to_create = OrderedDict()  # type: OrderedDict[Any, Variable]\n    vars_to_remove = []\n\n    for d in dims:\n        index = variables[d].to_index()\n        if isinstance(index, pd.MultiIndex):\n            dim_levels[d] = index.names\n        else:\n            vars_to_remove.append(d)\n            if not drop:\n                vars_to_create[d + '_'] = Variable(d, index)\n\n    for d, levs in dim_levels.items():\n        index = variables[d].to_index()\n        if len(levs) == index.nlevels:\n            vars_to_remove.append(d)\n        else:\n            vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n\n        if not drop:\n            for lev in levs:\n                idx = index.get_level_values(lev)\n                vars_to_create[idx.name] = Variable(d, idx)\n\n    new_variables = variables.copy()\n    for v in set(vars_to_remove):\n        del new_variables[v]\n    new_variables.update(vars_to_replace)\n    new_variables.update(vars_to_create)\n    new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n\n    return new_variables, new_coord_names",
        "sampled": "```python\nfrom typing import Union, List, Any, OrderedDict, Dict, Set\nfrom enum import Enum\n\nclass Variable(Enum):\n    LEVEL = \"level\"\n    COORD = \"coord\"\n\ndef split_indexes(\n    dims_or_levels: Union[Any, List[Any]],  \n    variables: OrderedDict[Any, Variable],\n    coord_names: Set,\n    level_coords: Dict[Any, Any],\n    drop=False  # type: bool\n):\n    \n    # Initialize an empty dictionary to store the extracted"
    },
    {
        "original": "def on(self, *qubits: Qid) -> 'gate_operation.GateOperation':\n        \"\"\"Returns an application of this gate to the given qubits.\n\n        Args:\n            *qubits: The collection of qubits to potentially apply the gate to.\n        \"\"\"\n        # Avoids circular import.\n        from cirq.ops import gate_operation\n        return gate_operation.GateOperation(self, list(qubits))",
        "sampled": "```python\nfrom typing import Iterable, Any\n\nclass GateOperation:\n    pass\n\nclass Qid:\n    def __init__(self, index: int):\n        self.index = index\n\nclass Gate:\n    def on(self, *qubits: Qid) -> 'GateOperation':\n        \"\"\"Returns an application of this gate to the given qubits.\n\n        Args:\n            *qubits: The collection of qubits to potentially apply the gate to.\n        \"\"\"\n        # Get the number of"
    },
    {
        "original": "def rollforward(self, date):\n        \"\"\"Roll date forward to nearest start of year\"\"\"\n        if self.onOffset(date):\n            return date\n        else:\n            return date + YearBegin(month=self.month)",
        "sampled": "```python\nfrom datetime import date\n\nclass DateTime:\n    def rollforward(self, date):\n        \"\"\"\n        Roll date forward to nearest start of year\n        Parameters:\n            date (date): The input date\n        \n        Returns:\n            date: The rolled-up date to the start of the year\n        \"\"\"\n        \n        # Get the year from the input date\n        year = date.year\n        \n        # Return a new Date object with January 1st in the same year as 'year'\n        \n"
    },
    {
        "original": "def find_region_end(self, lines):\n        \"\"\"Find the end of the region started with start and end markers\"\"\"\n        if self.metadata and 'cell_type' in self.metadata:\n            self.cell_type = self.metadata.pop('cell_type')\n        else:\n            self.cell_type = 'code'\n\n        parser = StringParser(self.language or self.default_language)\n        for i, line in enumerate(lines):\n            # skip cell header\n            if self.metadata is not None and i == 0:\n                continue\n\n            if parser.is_quoted():\n                parser.read_line(line)\n                continue\n\n            parser.read_line(line)\n\n            # New code region\n            # Simple code pattern in LightScripts must be preceded with a blank line\n            if self.start_code_re.match(line) or (\n                    self.simple_start_code_re and self.simple_start_code_re.match(line) and\n                    (self.cell_marker_start or i == 0 or _BLANK_LINE.match(lines[i - 1]))):\n\n                if self.explicit_end_marker_required:\n                    # Metadata here was conditioned on finding an explicit end marker\n                    # before the next start marker. So we dismiss it.\n                    self.metadata = None\n                    self.language = None\n\n                if i > 0 and _BLANK_LINE.match(lines[i - 1]):\n                    if i > 1 and _BLANK_LINE.match(lines[i - 2]):\n                        return i - 2, i, False\n                    return i - 1, i, False\n                return i, i, False\n\n            if not self.ignore_end_marker and self.end_code_re:\n                if self.end_code_re.match(line):\n                    return i, i + 1, True\n            elif _BLANK_LINE.match(line):\n                if not next_code_is_indented(lines[i:]):\n                    if i > 0:\n                        return i, i + 1, False\n                    if len(lines) > 1 and not _BLANK_LINE.match(lines[1]):\n                        return 1, 1, False\n                    return 1, 2, False\n\n        return len(lines), len(lines), False",
        "sampled": "```python\nclass Solution:\n    def find_region_end(self, lines):\n        \"\"\"\n        Find the end of the region started with start and end markers\n\n        This function finds the line where '*' markers are found in a multiline string.\n        \n        Parameters:\n            lines (str): A multiline string.\n\n        Returns:\n            int: The line number of the end of the region. Returns None if no region is found.\n        \"\"\"\n        \n        # Initialize a variable to track whether we are currently inside a"
    },
    {
        "original": "def project(vx, vy, occlusion):\n    \"\"\"Project the velocity field to be approximately mass-conserving,\n       using a few iterations of Gauss-Seidel.\"\"\"\n    p = np.zeros(vx.shape)\n    div = -0.5 * (np.roll(vx, -1, axis=1) - np.roll(vx, 1, axis=1)\n                + np.roll(vy, -1, axis=0) - np.roll(vy, 1, axis=0))\n    div = make_continuous(div, occlusion)\n\n    for k in range(50):\n        p = (div + np.roll(p, 1, axis=1) + np.roll(p, -1, axis=1)\n                 + np.roll(p, 1, axis=0) + np.roll(p, -1, axis=0))/4.0\n        p = make_continuous(p, occlusion)\n\n    vx = vx - 0.5*(np.roll(p, -1, axis=1) - np.roll(p, 1, axis=1))\n    vy = vy - 0.5*(np.roll(p, -1, axis=0) - np.roll(p, 1, axis=0))\n\n    vx = occlude(vx, occlusion)\n    vy = occlude(vy, occlusion)\n    return vx, vy",
        "sampled": "```python\nimport numpy as np\n\ndef project(vx, vy, occlusion):\n    \"\"\"\n    Project the velocity field to be approximately mass-conserving,\n    using a few iterations of Gauss-Seidel.\n\n    Parameters:\n    vx (numpy array): x-component of velocity\n    vy (numpy array): y-component of velocity\n    occlusion (numpy array): occlusion mask\n\n    Returns:\n    vxProjected (numpy array)\n    vyProjected (numpy array)\n    \nPROJECT EQUATION"
    },
    {
        "original": "def _set_variables(self, cacheable):\n        \"\"\"Pull the variable names out of the provided expressions and separate into cacheable expressions and normal parameters. Those that are only stored in the cache, the parameters are stored in this object.\"\"\"\n        # pull the parameters and inputs out of the symbolic pdf\n        def extract_vars(expr):\n            return [e for e in expr.atoms() if e.is_Symbol and e not in vars]\n        self.cacheable = cacheable\n        self.variables = {}\n        vars = []\n        for expression in self.expressions.values():\n            vars += extract_vars(expression['function'])\n        # inputs are assumed to be those things that are\n        # cacheable. I.e. those things that aren't stored within the\n        # object except as cached. For covariance functions this is X\n        # and Z, for likelihoods F and for mapping functions X.\n        self.cacheable_vars = [] # list of everything that's cacheable\n        for var in cacheable:            \n            self.variables[var] = [e for e in vars if e.name.split('_')[0]==var.lower()]\n            self.cacheable_vars += self.variables[var]\n        for var in cacheable:\n            if not self.variables[var]:\n                raise ValueError('Variable ' + var + ' was specified as cacheable but is not in expression. Expected to find symbols of the form ' + var.lower() + '_0 to represent ' + var)\n\n        # things that aren't cacheable are assumed to be parameters.\n        self.variables['theta'] = sorted([e for e in vars if not e in self.cacheable_vars],key=lambda e:e.name)",
        "sampled": "```python\nimport re\n\nclass Solver:\n    def _set_variables(self, cacheable):\n        \"\"\"Pull the variable names out of the provided expressions and separate into cacheable expressions and normal parameters.\"\"\"\n        \n        # Regular expression to find variables (assuming they are alphanumeric)\n        var_pattern = r'\\b[a-zA-Z_][a-zA-Z_0-9]*\\b'\n        \n        # Find all match groups in the input string\n        matches = re.findall(var_pattern, cacheable)\n"
    },
    {
        "original": "def update_limits(self):\n        \"\"\"\n        Poll 'Service Limits' check results from Trusted Advisor, if possible.\n        Iterate over all :py:class:`~.AwsLimit` objects for the given services\n        and update their limits from TA if present in TA checks.\n\n        :param services: dict of service name (string) to\n          :py:class:`~._AwsService` objects\n        :type services: dict\n        \"\"\"\n        if self.limits_updated:\n            logger.debug('Already polled TA; skipping update')\n            return\n        self.connect()\n        ta_results = self._poll()\n        self._update_services(ta_results)\n        self.limits_updated = True",
        "sampled": "```python\nimport requests\n\nclass ServiceLimitsUpdater:\n    def __init__(self, access_key_id, secret_access_key):\n        self.access_key_id = access_key_id\n        self.secret_access_key = secret_access_key\n\n    def update_limits(self, services):\n        \"\"\"\n        Poll 'Service Limits' check results from Trusted Advisor.\n\n        :param services: dict of service name (string) to AwsService objects\n        :type services: dict\n        \"\"\"\n        # Set the API endpoint"
    },
    {
        "original": "def hex_to_rgb(color):\n    \"\"\"\n    Converts from hex to rgb\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation on hex or rgb\n\n    Example:\n            hex_to_rgb('#E1E5ED')\n            hex_to_rgb('#f03')\n    \"\"\"\n    color = normalize(color)\n    color = color[1:]\n    # return 'rgb'+str(tuple(ord(c) for c in color.decode('hex')))\n    return 'rgb' + str((int(color[0:2], base=16), int(color[2:4], base=16), int(color[4:6], base=16)))",
        "sampled": "```python\ndef hex_to_rgb(color):\n    \"\"\"\n    Converts from hex to rgb\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation on hex or rgb\n\n    Example:\n            hex_to_rgb('#E1E5ED')\n            hex_to_rgb('#f03')\n    \"\"\"\n\n   # Check if input is in 'hex' format\n   if color.startswith('#') and len(color) == 7:\n       val = color[1:]\n       return tuple(int(val[("
    },
    {
        "original": "def dependency_information(include_salt_cloud=False):\n    \"\"\"\n    Report versions of library dependencies.\n    \"\"\"\n    libs = [\n        ('Python', None, sys.version.rsplit('\\n')[0].strip()),\n        ('Jinja2', 'jinja2', '__version__'),\n        ('M2Crypto', 'M2Crypto', 'version'),\n        ('msgpack-python', 'msgpack', 'version'),\n        ('msgpack-pure', 'msgpack_pure', 'version'),\n        ('pycrypto', 'Crypto', '__version__'),\n        ('pycryptodome', 'Cryptodome', 'version_info'),\n        ('PyYAML', 'yaml', '__version__'),\n        ('PyZMQ', 'zmq', '__version__'),\n        ('ZMQ', 'zmq', 'zmq_version'),\n        ('Mako', 'mako', '__version__'),\n        ('Tornado', 'tornado', 'version'),\n        ('timelib', 'timelib', 'version'),\n        ('dateutil', 'dateutil', '__version__'),\n        ('pygit2', 'pygit2', '__version__'),\n        ('libgit2', 'pygit2', 'LIBGIT2_VERSION'),\n        ('smmap', 'smmap', '__version__'),\n        ('cffi', 'cffi', '__version__'),\n        ('pycparser', 'pycparser', '__version__'),\n        ('gitdb', 'gitdb', '__version__'),\n        ('gitpython', 'git', '__version__'),\n        ('python-gnupg', 'gnupg', '__version__'),\n        ('mysql-python', 'MySQLdb', '__version__'),\n        ('cherrypy', 'cherrypy', '__version__'),\n        ('docker-py', 'docker', '__version__'),\n    ]\n\n    if include_salt_cloud:\n        libs.append(\n            ('Apache Libcloud', 'libcloud', '__version__'),\n        )\n\n    for name, imp, attr in libs:\n        if imp is None:\n            yield name, attr\n            continue\n        try:\n            imp = __import__(imp)\n            version = getattr(imp, attr)\n            if callable(version):\n                version = version()\n            if isinstance(version, (tuple, list)):\n                version = '.'.join(map(str, version))\n            yield name, version\n        except Exception:\n            yield name, None",
        "sampled": "```python\nimport pkg_resources\nfrom packaging import version\n\ndef dependency_information(include_salt_cloud=False):\n    \"\"\"\n    Report versions of library dependencies.\n    \"\"\"\n    \n    # Get all installed packages\n    dists = pkg_resources.working_set\n    \n    # Initialize a dictionary to store the dependency information\n    deps_info = {}\n    \n    for dist in dists:\n        if dist.project_name not in deps_info:\n            deps_info[dist.project_name] = {}\n        \n        # Check if the package"
    },
    {
        "original": "def distros_for_location(location, basename, metadata=None):\n    \"\"\"Yield egg or source distribution objects based on basename\"\"\"\n    if basename.endswith('.egg.zip'):\n        basename = basename[:-4]  # strip the .zip\n    if basename.endswith('.egg') and '-' in basename:\n        # only one, unambiguous interpretation\n        return [Distribution.from_location(location, basename, metadata)]\n    if basename.endswith('.whl') and '-' in basename:\n        wheel = Wheel(basename)\n        if not wheel.is_compatible():\n            return []\n        return [Distribution(\n            location=location,\n            project_name=wheel.project_name,\n            version=wheel.version,\n            # Increase priority over eggs.\n            precedence=EGG_DIST + 1,\n        )]\n    if basename.endswith('.exe'):\n        win_base, py_ver, platform = parse_bdist_wininst(basename)\n        if win_base is not None:\n            return interpret_distro_name(\n                location, win_base, metadata, py_ver, BINARY_DIST, platform\n            )\n    # Try source distro extensions (.zip, .tgz, etc.)\n    #\n    for ext in EXTENSIONS:\n        if basename.endswith(ext):\n            basename = basename[:-len(ext)]\n            return interpret_distro_name(location, basename, metadata)\n    return []",
        "sampled": "```python\nimport glob\n\ndef distros_for_location(location, basename, metadata=None):\n    \"\"\"\n    Yield egg or source distribution objects based on basename\n    \n    Parameters:\n    location (str): Path to the directory where distributions are located.\n    basename (str): Basename of the distribution object.\n    \n    Returns:\n    yield: List of tuple containing file name and metadata\n    \"\"\"\n    \n    # Construct a pattern to match all files that match the basename in the location\n    glob_pattern ="
    },
    {
        "original": "def write_data_as_message(self, buffer, data, content_related,\n                              *, after_id=None):\n        \"\"\"\n        Writes a message containing the given data into buffer.\n\n        Returns the message id.\n        \"\"\"\n        msg_id = self._get_new_msg_id()\n        seq_no = self._get_seq_no(content_related)\n        if after_id is None:\n            body = GzipPacked.gzip_if_smaller(content_related, data)\n        else:\n            body = GzipPacked.gzip_if_smaller(content_related,\n                bytes(InvokeAfterMsgRequest(after_id, data)))\n\n        buffer.write(struct.pack('<qii', msg_id, seq_no, len(body)))\n        buffer.write(body)\n        return msg_id",
        "sampled": "```python\nclass MessagingSystem:\n    def __init__(self):\n        self.message_id = 1\n\n    def write_data_as_message(self, buffer, data, content_related,\n                              *, after_id=None):\n        \"\"\"\n        Writes a message containing the given data into buffer.\n\n        Returns the message id.\n        \"\"\"\n        \n        if after_id is not None:\n            # Check if the previous message exists in buffer\n            if 'id' in buffer and buffer['id'] == after_id:\n"
    },
    {
        "original": "def run_sacrebleu(self, detok_eval_path, reference_path):\n        \"\"\"\n        Executes sacrebleu and returns BLEU score.\n\n        :param detok_eval_path: path to the test file\n        :param reference_path: path to the reference file\n        \"\"\"\n        if reference_path is None:\n            reference_path = os.path.join(self.dataset_dir,\n                                          config.TGT_TEST_TARGET_FNAME)\n        sacrebleu_params = '--score-only -lc --tokenize intl'\n        logging.info(f'Running sacrebleu (parameters: {sacrebleu_params})')\n        sacrebleu = subprocess.run([f'sacrebleu --input {detok_eval_path} \\\n                                    {reference_path} {sacrebleu_params}'],\n                                   stdout=subprocess.PIPE, shell=True)\n        test_bleu = float(sacrebleu.stdout.strip())\n        return test_bleu",
        "sampled": "```python\nimport subprocess\n\nclass SacrebleuEvaluator:\n    def run_sacrebleu(self, detok_eval_path, reference_path):\n        \"\"\"\n        Executes sacrebleu and returns BLEU score.\n\n        :param detok_eval_path: path to the test file\n        :param reference_path: path to the reference file\n        \"\"\"\n        \n        # Execute sacrebleu command with options for detailed output and use of a dictionary evaluation (tokenized)\n        command = f"
    },
    {
        "original": "def _get_stats_column_names(cls):\n        \"\"\"Construct a tuple of the column names for stats. Each stat has 4\n        columns of data.\n        \"\"\"\n        columns = []\n        stats = ('num_bytes', 'num_rows', 'location', 'partitioning_type',\n                 'clustering_fields')\n        stat_components = ('label', 'value', 'description', 'include')\n        for stat_id in stats:\n            for stat_component in stat_components:\n                columns.append('stats:{}:{}'.format(stat_id, stat_component))\n        return tuple(columns)",
        "sampled": "```python\nclass Stats:\n    def _get_stats_column_names(cls):\n        \"\"\"\n        Construct a tuple of the column names for stats. Each stat has 4 columns of data.\n        \n        Returns:\n            tuple: A tuple containing the column names for stats.\n        \"\"\"\n        \n        # Assuming each stat has 4 columns: 'mean', 'std', 'min', and 'max'\n        # We use a list to store the column names, then convert it to a tuple\n        #"
    },
    {
        "original": "def sha1_digest(instr):\n    \"\"\"\n    Generate an sha1 hash of a given string.\n    \"\"\"\n    if six.PY3:\n        b = salt.utils.stringutils.to_bytes(instr)\n        return hashlib.sha1(b).hexdigest()\n    return hashlib.sha1(instr).hexdigest()",
        "sampled": "```python\nimport hashlib\n\ndef sha1_digest(instr):\n    \"\"\"\n    Generate an sha1 hash of a given string.\n    \"\"\"\n    # Convert the input string to bytes if it's not already\n    instr_bytes = str.encode(instr)\n    \n    # Create a new SHA1 hash object\n    sha1_hash = hashlib.sha1()\n    \n    # Update the hash object with the input bytes\n    sha1_hash.update(instr_bytes)\n    \n    # Get the hexadecimal representation of the hash\n"
    },
    {
        "original": "def energy_density(self, strain, convert_GPa_to_eV=True):\n        \"\"\"\n        Calculates the elastic energy density due to a strain\n        \"\"\"\n        e_density = np.sum(self.calculate_stress(strain)*strain) / self.order\n        if convert_GPa_to_eV:\n            e_density *= self.GPa_to_eV_A3  # Conversion factor for GPa to eV/A^3\n        return e_density",
        "sampled": "```python\nclass MaterialProperties:\n    def __init__(self, c11, c12, c44):\n        \"\"\"\n        Initializes the material properties\n        \"\"\"\n        self.c11 = c11  # Elastic constant (GPa)\n        self.c12 = c12  # Elastic constant (GPa)\n        self.c44 = c44  # Elastic constant (GPa)\n\n    def energy_density(self, strain, convert_GPa_to_eV=True):\n        \"\"\"\n        Calculates the elastic"
    },
    {
        "original": "def ParseMultiple(self, result_dicts):\n    \"\"\"Parse the WMI packages output.\"\"\"\n    for result_dict in result_dicts:\n      result = result_dict.ToDict()\n      winvolume = rdf_client_fs.WindowsVolume(\n          drive_letter=result.get(\"DeviceID\"),\n          drive_type=result.get(\"DriveType\"))\n\n      try:\n        size = int(result.get(\"Size\"))\n      except (ValueError, TypeError):\n        size = None\n\n      try:\n        free_space = int(result.get(\"FreeSpace\"))\n      except (ValueError, TypeError):\n        free_space = None\n\n      # Since we don't get the sector sizes from WMI, we just set them at 1 byte\n      yield rdf_client_fs.Volume(\n          windowsvolume=winvolume,\n          name=result.get(\"VolumeName\"),\n          file_system_type=result.get(\"FileSystem\"),\n          serial_number=result.get(\"VolumeSerialNumber\"),\n          sectors_per_allocation_unit=1,\n          bytes_per_sector=1,\n          total_allocation_units=size,\n          actual_available_allocation_units=free_space)",
        "sampled": "```python\ndef ParseMultiple(self, result_dicts):\n    \"\"\"\n    Parse the WMI packages output.\n    \n    Args:\n        result_dicts (list): A list of dictionaries containing the WMI packages output.\n        \n    Returns:\n        dict: A dictionary containing parsed data.\n    \"\"\"\n    \n    # Initialize an empty dictionary to store parsed data\n    parsed_data = {}\n    \n    # Iterate over each dictionary in the list\n    for result_dict in result_dicts:\n        \n        # Get package name and"
    },
    {
        "original": "def get_tables(self):\n        \"\"\"\n        Adds tables to the network.\n\n        Example\n        -------\n        >>> writer = UAIWriter(model)\n        >>> writer.get_tables()\n        \"\"\"\n        if isinstance(self.model, BayesianModel):\n            cpds = self.model.get_cpds()\n            cpds.sort(key=lambda x: x.variable)\n            tables = []\n            for cpd in cpds:\n                values = list(map(str, cpd.values.ravel()))\n                tables.append(values)\n            return tables\n        elif isinstance(self.model, MarkovModel):\n            factors = self.model.get_factors()\n            tables = []\n            for factor in factors:\n                values = list(map(str, factor.values.ravel()))\n                tables.append(values)\n            return tables\n        else:\n            raise TypeError(\"Model must be an instance of Markov or Bayesian model.\")",
        "sampled": "```python\nclass UAIWriter:\n    def __init__(self, model):\n        self.model = model\n\n    def get_tables(self):\n        \"\"\"\n        Adds tables to the network.\n\n        Returns\n        -------\n        table_dict : dict\n            A dictionary containing tables as values.\n\n            table_params : list of tuples of [int]\n                Each tuple corresponds to a row in the table and contains two integers:\n                1: row index (starts from 1), \n                2:"
    },
    {
        "original": "def set_last_row_idx(self, last_row_idx):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tparam last_row_idx : int\n\t\t\tnumber of rows\n\t\t\"\"\"\n\t\tassert last_row_idx >= self._max_row\n\t\tself._max_row = last_row_idx\n\t\treturn self",
        "sampled": "```python\nclass Solution:\n    def set_last_row_idx(self, last_row_idx):\n        \"\"\"\n        Parameters\n        ----------\n        param last_row_idx : int\n            number of rows\n        \"\"\"\n        \n    def find_matrix_minlastentry(self, matrix):\n        m = len(matrix)\n        \n###Formulating the iterative code inside the function once initiated.\n###Dis-ordered row matrices have their length accross the range tested against that described.\n\n###comment guiding explanation\n\n# for rows iteration principals on"
    },
    {
        "original": "def get_contents(self, path):\n        \"\"\"\n        Loads the contents of the file specified by path\n\n        Args:\n            path (string): The relative or absolute path to the file to\n                be loaded.  If the path is relative, then it is combined\n                with the base_path to generate a full path string\n\n        Returns:\n            string: The contents of the file as a string\n\n        Raises:\n            ConfigurationError: If the file cannot be loaded\n        \"\"\"\n        try:\n            if not os.path.exists(path):\n                raise ConfigurationError('specified path does not exist %s' % path)\n\n            with open(path) as f:\n                data = f.read()\n\n            return data\n\n        except (IOError, OSError) as exc:\n            raise ConfigurationError('error trying to load file contents: %s' % exc)",
        "sampled": "```python\nimport os\nfrom configuration_error import ConfigurationError  # Assuming this is where ConfigurationError is defined\n\nclass FileLoader:\n    def __init__(self, base_path):\n        self.base_path = base_path\n\n    def get_contents(self, path):\n        \"\"\"\n        Loads the contents of the file specified by path\n\n        Args:\n            path (string): The relative or absolute path to the file to\n                be loaded.  If the path is relative, then it is combined\n"
    },
    {
        "original": "def resize(self, image, size):\n        \"\"\"\n            Resizes the image\n\n            :param image: The image object\n            :param size: size is PIL tuple (width, heigth, force) ex: (200,100,True)\n        \"\"\"\n        (width, height, force) = size\n\n        if image.size[0] > width or image.size[1] > height:\n            if force:\n                return ImageOps.fit(self.image, (width, height), Image.ANTIALIAS)\n            else:\n                thumb = self.image.copy()\n                thumb.thumbnail((width, height), Image.ANTIALIAS)\n                return thumb\n\n        return image",
        "sampled": "```python\nfrom PIL import Image\n\ndef resize(self, image, size):\n    \"\"\"\n        Resizes the image\n\n        :param image: The image object\n        :param size: size is PIL tuple (width, height, force) ex: (200, 100, True)\n    \"\"\"\n    \n    # Unpack the size tuple into width and height\n    width, height = size[:2]\n    \n    # Check if the aspect ratio of the original image needs to be maintained\n"
    },
    {
        "original": "def mkconfig(config=None,\n             tmp=None,\n             id_=None,\n             approve_key=True,\n             pub_key=None,\n             priv_key=None):\n    \"\"\"\n    Generate keys and config and put them in a tmp directory.\n\n    pub_key\n        absolute path or file content of an optional preseeded salt key\n\n    priv_key\n        absolute path or file content of an optional preseeded salt key\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion' seed.mkconfig [config=config_data] [tmp=tmp_dir] \\\\\n                [id_=minion_id] [approve_key=(true|false)]\n    \"\"\"\n    if tmp is None:\n        tmp = tempfile.mkdtemp()\n    if config is None:\n        config = {}\n    if 'master' not in config and __opts__['master'] != 'salt':\n        config['master'] = __opts__['master']\n    if id_:\n        config['id'] = id_\n\n    # Write the new minion's config to a tmp file\n    tmp_config = os.path.join(tmp, 'minion')\n    with salt.utils.files.fopen(tmp_config, 'w+') as fp_:\n        fp_.write(salt.utils.cloud.salt_config_to_yaml(config))\n\n    # Generate keys for the minion\n    pubkeyfn = os.path.join(tmp, 'minion.pub')\n    privkeyfn = os.path.join(tmp, 'minion.pem')\n    preseeded = pub_key and priv_key\n    if preseeded:\n        log.debug('Writing minion.pub to %s', pubkeyfn)\n        log.debug('Writing minion.pem to %s', privkeyfn)\n        with salt.utils.files.fopen(pubkeyfn, 'w') as fic:\n            fic.write(salt.utils.stringutils.to_str(_file_or_content(pub_key)))\n        with salt.utils.files.fopen(privkeyfn, 'w') as fic:\n            fic.write(salt.utils.stringutils.to_str(_file_or_content(priv_key)))\n        os.chmod(pubkeyfn, 0o600)\n        os.chmod(privkeyfn, 0o600)\n    else:\n        salt.crypt.gen_keys(tmp, 'minion', 2048)\n    if approve_key and not preseeded:\n        with salt.utils.files.fopen(pubkeyfn) as fp_:\n            pubkey = salt.utils.stringutils.to_unicode(fp_.read())\n            __salt__['pillar.ext']({'virtkey': [id_, pubkey]})\n\n    return {'config': tmp_config, 'pubkey': pubkeyfn, 'privkey': privkeyfn}",
        "sampled": "```python\nimport os\nimport pwd\nimport grp\n\ndef mkconfig(config=None, tmp=None, id_=None, approve_key=True, pub_key=None, priv_key=None):\n    # Create tmp directory if it doesn't exist\n    if not os.path.exists(tmp):\n        os.makedirs(tmp)\n\n    # Set default id to current user's username if not provided\n    if id_ is None:\n        id_ = pwd.getpwuid(os.getuid())[0]\n\n    # Get the"
    },
    {
        "original": "def apply_to_structure(self, structure):\n        \"\"\"\n        Apply the deformation gradient to a structure.\n\n        Args:\n            structure (Structure object): the structure object to\n                be modified by the deformation\n        \"\"\"\n        def_struct = structure.copy()\n        old_latt = def_struct.lattice.matrix\n        new_latt = np.transpose(np.dot(self, np.transpose(old_latt)))\n        def_struct.lattice = Lattice(new_latt)\n        return def_struct",
        "sampled": "```python\nimport numpy as np\n\nclass Deformation:\n    def __init__(self, deformation_gradient):\n        \"\"\"\n        Initialize the deformation object.\n\n        Args:\n            deformation_gradient (numpy array): the matrix representing\n                the deformation gradient\n        \"\"\"\n        self.deformation_gradient = self._validate_deformation(deformation_gradient)\n\n    @staticmethod\n    def _validate_deformation(deformation_gradient):\n        \"\"\"\n        Validate if the input is a valid 2D numpy array with shape (3,"
    },
    {
        "original": "def __get_stat_display(self, stats, layer):\n        \"\"\"Return a dict of dict with all the stats display.\n        stats: Global stats dict\n        layer: ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n        :returns: dict of dict\n            * key: plugin name\n            * value: dict returned by the get_stats_display Plugin method\n        \"\"\"\n        ret = {}\n\n        for p in stats.getPluginsList(enable=False):\n            if p == 'quicklook' or p == 'processlist':\n                # processlist is done later\n                # because we need to know how many processes could be displayed\n                continue\n\n            # Compute the plugin max size\n            plugin_max_width = None\n            if p in self._left_sidebar:\n                plugin_max_width = max(self._left_sidebar_min_width,\n                                       self.screen.getmaxyx()[1] - 105)\n                plugin_max_width = min(self._left_sidebar_max_width,\n                                       plugin_max_width)\n\n            # Get the view\n            ret[p] = stats.get_plugin(p).get_stats_display(args=self.args,\n                                                           max_width=plugin_max_width)\n\n        return ret",
        "sampled": "```python\ndef __get_stat_display(self, stats, layer):\n    \"\"\"\n    Return a dict of dict with all the stats display.\n    \n    Args:\n        stats (dict): Global stats dict\n        layer (str): ~ cs_status\n            \"None\": standalone or server mode\n            \"Connected\": Client is connected to a Glances server\n            \"SNMP\": Client is connected to a SNMP server\n            \"Disconnected\": Client is disconnected from the server\n\n    Returns:\n       "
    },
    {
        "original": "def LateBind(self, target=None):\n    \"\"\"Late binding callback.\n\n    This method is called on this field descriptor when the target RDFValue\n    class is finally defined. It gives the field descriptor an opportunity to\n    initialize after the point of definition.\n\n    Args:\n      target: The target nested class.\n\n    Raises:\n      TypeError: If the target class is not of the expected type.\n    \"\"\"\n    if not issubclass(target, RDFProtoStruct):\n      raise TypeError(\"Field %s expects a protobuf, but target is %s\" %\n                      (self, target))\n\n    self.late_bound = False\n\n    # The target type is now resolved.\n    self.type = target\n\n    # Register us in our owner.\n    self.owner.AddDescriptor(self)",
        "sampled": "```python\nclass FieldDescriptor:\n    def LateBind(self, target=None):\n        \"\"\"Late binding callback.\n\n        Args:\n          target: The target nested class or dictionary.\n\n        Raises:\n          TypeError: If the target is not a dict or a class.\n        \"\"\"\n        if isinstance(target, dict):\n            # Iterate over each key-value pair in the dictionary\n            for key, value in target.items():\n                # Set each attribute on the current instance\n                setattr(self.instance, key, value"
    },
    {
        "original": "def create_child_folder(self, folder_name):\n        \"\"\" Creates a new child folder\n\n        :param str folder_name: name of the new folder to create\n        :return: newly created folder\n        :rtype: ContactFolder or None\n        \"\"\"\n\n        if not folder_name:\n            return None\n\n        if self.root:\n            url = self.build_url(self._endpoints.get('root_folders'))\n        else:\n            url = self.build_url(\n                self._endpoints.get('child_folders').format(id=self.folder_id))\n\n        response = self.con.post(url,\n                                 data={self._cc('displayName'): folder_name})\n        if not response:\n            return None\n\n        folder = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: folder})",
        "sampled": "```python\nclass ContactFolder:\n    def __init__(self, name):\n        self.name = name\n        self.children = []\n\nclass ContactManager:\n    def __init__(self):\n        self.root_folder = ContactFolder(\"Root\")\n\n    def create_child_folder(self, folder_name, parent=None):\n        if parent is None:\n            parent = self.root_folder\n        new_folder = ContactFolder(folder_name)\n        parent.children.append(new_folder)\n        return new_folder\n\n# Example usage\ncontact"
    },
    {
        "original": "def all_experiment_groups(self):\n        \"\"\"\n        Similar to experiment_groups,\n        but uses the default manager to return archived experiments as well.\n        \"\"\"\n        from db.models.experiment_groups import ExperimentGroup\n\n        return ExperimentGroup.all.filter(project=self)",
        "sampled": "```python\ndef all_experiment_groups(self):\n    \"\"\"\n    Similar to experiment_groups,\n    but uses the default manager to return archived experiments as well.\n    \"\"\"\n    \n    # Assuming self is an instance of a class that has attribute 'Manager'\n    \n    # Acquire the default manager from 'Manager' attribute\n    Manager = self.Manager\n    \n    try:\n        l_manager = Manager()\n        \n        # Query all experiment groups including archived ones using default manager\n        return l_manager.all()\n        \n   "
    },
    {
        "original": "def fill(self):\n        \"\"\"Parse all the paths (['Lcom/example/myclass/MyActivity$1;', ...])\n           and build a tree using the QTreeWidgetItem insertion method.\"\"\"\n        log.debug(\"Fill classes tree\")\n\n        for idx, filename, digest, classes in self.session.get_classes():\n            for c in sorted(classes, key=lambda c: c.name):\n                sig = Signature(c)\n                path_node = self.root_path_node\n\n                path = None\n                if not sig.class_path:\n                    path = '.'\n                    if path not in path_node[0]:\n                        path_node[0][path] = (\n                            {}, HashableQTreeWidgetItem(path_node[1]))\n                        path_node[0][path][1].setText(0, path)\n                    path_node = path_node[0][path]\n                else:\n                    # Namespaces\n                    for path in sig.class_path:\n                        if path not in path_node[0]:\n                            path_node[0][path] = (\n                                {}, HashableQTreeWidgetItem(path_node[1]))\n                            path_node[0][path][1].setText(0, path)\n                        path_node = path_node[0][path]\n\n                # Class\n                path_node[0][path] = ({}, HashableQTreeWidgetItem(path_node[1]))\n\n                class_name = sig.class_name\n\n                if idx > 0:\n                    class_name += \"@%d\" % idx\n\n                c.current_title = class_name\n                self._reverse_cache[path_node[0][path][1]] = (c, filename,\n                                                              digest)\n\n                path_node[0][path][1].setText(0, class_name)",
        "sampled": "```python\nclass Solution:\n    def fill(self, lst):\n        \"\"\"\n        Parse all the paths and build a tree using the QTreeWidgetItem insertion method.\n        \n        Args:\n            lst (list): A list of strings representing package and class information.\n        \n        Returns:\n            None\n        \"\"\"\n\n        from collections import deque\n\n        # Create a queue to store nodes that have no parent yet\n        queue = deque()\n\n        # Initialize the root node\n        root = TreeNode(None)\n                \n    def"
    },
    {
        "original": "def remote_startCommand(self, stepref, stepId, command, args):\n        \"\"\"\n        This gets invoked by L{buildbot.process.step.RemoteCommand.start}, as\n        part of various master-side BuildSteps, to start various commands\n        that actually do the build. I return nothing. Eventually I will call\n        .commandComplete() to notify the master-side RemoteCommand that I'm\n        done.\n        \"\"\"\n        stepId = decode(stepId)\n        command = decode(command)\n        args = decode(args)\n\n        self.activity()\n\n        if self.command:\n            log.msg(\"leftover command, dropping it\")\n            self.stopCommand()\n\n        try:\n            factory = registry.getFactory(command)\n        except KeyError:\n            raise UnknownCommand(u\"unrecognized WorkerCommand '{0}'\".format(command))\n        self.command = factory(self, stepId, args)\n\n        log.msg(u\" startCommand:{0} [id {1}]\".format(command, stepId))\n        self.remoteStep = stepref\n        self.remoteStep.notifyOnDisconnect(self.lostRemoteStep)\n        d = self.command.doStart()\n        d.addCallback(lambda res: None)\n        d.addBoth(self.commandComplete)\n        return None",
        "sampled": "```python\nclass RemoteCommandExecutor:\n    def remote_startCommand(self, step_ref, step_id, command, args):\n        \"\"\"\n        This gets invoked by L{buildbot.process.step.RemoteCommand.start}, as\n        part of various master-side BuildSteps, to start various commands\n        that actually do the build. I return nothing. Eventually I will call\n        .commandComplete() to notify the master-side RemoteCommand that I'm\n        done.\n        \n        Args:\n            step_ref"
    },
    {
        "original": "def count(self, strg, case_sensitive=False, *args, **kwargs):\n    \"\"\"Get the count of a word or phrase `s` within this WordList.\n    :param strg: The string to count.\n    :param case_sensitive: A boolean, whether or not the search is case-sensitive.\n    \"\"\"\n    if not case_sensitive:\n        return [word.lower() for word in self].count(strg.lower(), *args,\n                **kwargs)\n    return self._collection.count(strg, *args, **kwargs)",
        "sampled": "```python\nfrom typing import List\n\nclass WordList:\n    def __init__(self, documents: List[str]):\n        self.documents = documents\n\n    @staticmethod\n    def _count_pattern_in_string(s: str, pattern: str, case_sensitive=False) -> int:\n        if not case_sensitive:\n            s = s.lower()\n            pattern = pattern.lower()\n        return s.count(pattern)\n\n    def count(self, strg: str, case_sensitive=False):\n        \"\"\"\n        Get the count"
    },
    {
        "original": "def GetBatchJob(client, batch_job_id):\n  \"\"\"Retrieves the BatchJob with the given id.\n\n  Args:\n    client: an instantiated AdWordsClient used to retrieve the BatchJob.\n    batch_job_id: a long identifying the BatchJob to be retrieved.\n  Returns:\n    The BatchJob associated with the given id.\n  \"\"\"\n  batch_job_service = client.GetService('BatchJobService', 'v201809')\n\n  selector = {\n      'fields': ['Id', 'Status', 'DownloadUrl'],\n      'predicates': [\n          {\n              'field': 'Id',\n              'operator': 'EQUALS',\n              'values': [batch_job_id]\n          }\n      ]\n  }\n\n  return batch_job_service.get(selector)['entries'][0]",
        "sampled": "```python\nclass AdWordsClient: # assume this class is already defined to handle AdWords API interactions\n    def __init__(self, api_url):\n        self.api_url = api_url\n\n    def get_batch_job(self, batch_job_id):\n        \"\"\"Retrieves the BatchJob with the given id.\n\n        Args:\n            batch_job_id (int): a long identifying the BatchJob to be retrieved.\n\n        Returns:\n            The BatchJob associated with the given id.\n        \"\"\"\n       "
    },
    {
        "original": "def psutil_wrapper(self, process, method, accessors, try_sudo, *args, **kwargs):\n        \"\"\"\n        A psutil wrapper that is calling\n        * psutil.method(*args, **kwargs) and returns the result\n        OR\n        * psutil.method(*args, **kwargs).accessor[i] for each accessors\n        given in a list, the result being indexed in a dictionary\n        by the accessor name\n        \"\"\"\n\n        if accessors is None:\n            result = None\n        else:\n            result = {}\n\n        # Ban certain method that we know fail\n        if method == 'num_fds' and not Platform.is_unix():\n            return result\n        elif method == 'num_handles' and not Platform.is_win32():\n            return result\n\n        try:\n            res = getattr(process, method)(*args, **kwargs)\n            if accessors is None:\n                result = res\n            else:\n                for acc in accessors:\n                    try:\n                        result[acc] = getattr(res, acc)\n                    except AttributeError:\n                        self.log.debug(\"psutil.{}().{} attribute does not exist\".format(method, acc))\n        except (NotImplementedError, AttributeError):\n            self.log.debug(\"psutil method {} not implemented\".format(method))\n        except psutil.AccessDenied:\n            self.log.debug(\"psutil was denied access for method {}\".format(method))\n            if method == 'num_fds' and Platform.is_unix() and try_sudo:\n                try:\n                    # It is up the agent's packager to grant\n                    # corresponding sudo policy on unix platforms\n                    ls_args = ['sudo', 'ls', '/proc/{}/fd/'.format(process.pid)]\n                    process_ls = subprocess.check_output(ls_args)\n                    result = len(process_ls.splitlines())\n\n                except subprocess.CalledProcessError as e:\n                    self.log.exception(\n                        \"trying to retrieve {} with sudo failed with return code {}\".format(method, e.returncode)\n                    )\n                except Exception:\n                    self.log.exception(\"trying to retrieve {} with sudo also failed\".format(method))\n        except psutil.NoSuchProcess:\n            self.warning(\"Process {} disappeared while scanning\".format(process.pid))\n\n        return result",
        "sampled": "```python\nimport psutil\n\nclass Solution:\n    def psutil_wrapper(self, process, method, accessors=None, try_sudo=False):\n        \"\"\"\n        A wrapper function to call a method from the psutil library\n        with optional additional steps and/or attempts to run with sudo privileges.\n        \n        Parameters:\n        ----------\n            process : str or int or None\n                Process name or pid as string/int. If \"all\" is provided, it\n                essentially runs the method on all"
    },
    {
        "original": "def marshal(data, fields, envelope=None):\n    \"\"\"Takes raw data (in the form of a dict, list, object) and a dict of\n    fields to output and filters the data based on those fields.\n\n    :param data: the actual object(s) from which the fields are taken from\n    :param fields: a dict of whose keys will make up the final serialized\n                   response output\n    :param envelope: optional key that will be used to envelop the serialized\n                     response\n\n\n    >>> from flask_restful import fields, marshal\n    >>> data = { 'a': 100, 'b': 'foo' }\n    >>> mfields = { 'a': fields.Raw }\n\n    >>> marshal(data, mfields)\n    OrderedDict([('a', 100)])\n\n    >>> marshal(data, mfields, envelope='data')\n    OrderedDict([('data', OrderedDict([('a', 100)]))])\n\n    \"\"\"\n\n    def make(cls):\n        if isinstance(cls, type):\n            return cls()\n        return cls\n\n    if isinstance(data, (list, tuple)):\n        return (OrderedDict([(envelope, [marshal(d, fields) for d in data])])\n                if envelope else [marshal(d, fields) for d in data])\n\n    items = ((k, marshal(data, v) if isinstance(v, dict)\n              else make(v).output(k, data))\n             for k, v in fields.items())\n    return OrderedDict([(envelope, OrderedDict(items))]) if envelope else OrderedDict(items)",
        "sampled": "```python\nfrom collections import OrderedDict\n\ndef marshal(data, fields, envelope=None):\n    \"\"\"\n    Takes raw data (in the form of a dict, list, object) and a dict of\n    fields to output and filters the data based on those fields.\n\n    :param data: the actual object(s) from which the fields are taken from\n    :param fields: a dict of whose keys will make up the final serialized\n                   response output\n    :param envelope: optional key that"
    },
    {
        "original": "def get_posterior_mean_ratio_scores_vs_background(self):\n        \"\"\"\n        Returns\n        -------\n            pd.DataFrame of posterior mean  scores vs background\n        \"\"\"\n        df = self.get_term_and_background_counts()\n        df['Log Posterior Mean Ratio'] = self._get_posterior_mean_ratio_from_counts(df['corpus'],\n                                                                                    df['background'])\n        return df.sort_values('Log Posterior Mean Ratio', ascending=False)",
        "sampled": "```python\nimport pandas as pd\n\nclass Model:\n    def get_posterior_mean_ratio_scores_vs_background(self):\n        \"\"\"\n        Returns\n        -------\n            pd.DataFrame of posterior mean  scores vs background\n        \"\"\"\n        \n        # Assuming self.data holds the necessary data for calculation\n        data = self.data\n        \n        # Calculate posterior mean scores vs background using a sample formula: (x+y)/2 * (z/w)\n        x, y, z, w = data['feature1'], data"
    },
    {
        "original": "def _parse_results(self, raw_results, includes_qualifiers):\n        \"\"\"\n        Parse WMI query results in a more comprehensive form.\n\n        Returns: List of WMI objects\n        ```\n        [\n            {\n                'freemegabytes': 19742.0,\n                'name': 'C:',\n                'avgdiskbytesperwrite': 1536.0\n            }, {\n                'freemegabytes': 19742.0,\n                'name': 'D:',\n                'avgdiskbytesperwrite': 1536.0\n            }\n        ]\n        ```\n        \"\"\"\n        results = []\n        for res in raw_results:\n            # Ensure all properties are available. Use case-insensitivity\n            # because some properties are returned with different cases.\n            item = CaseInsensitiveDict()\n            for prop_name in self.property_names:\n                item[prop_name] = None\n\n            for wmi_property in res.Properties_:\n                # IMPORTANT: To improve performance, only access the Qualifiers\n                # if the \"CounterType\" hasn't already been cached.\n                should_get_qualifier_type = (\n                    includes_qualifiers and wmi_property.Name not in self._property_counter_types\n                )\n\n                if should_get_qualifier_type:\n\n                    # Can't index into \"Qualifiers_\" for keys that don't exist\n                    # without getting an exception.\n                    qualifiers = dict((q.Name, q.Value) for q in wmi_property.Qualifiers_)\n\n                    # Some properties like \"Name\" and \"Timestamp_Sys100NS\" do\n                    # not have a \"CounterType\" (since they're not a counter).\n                    # Therefore, they're ignored.\n                    if \"CounterType\" in qualifiers:\n                        counter_type = qualifiers[\"CounterType\"]\n                        self._property_counter_types[wmi_property.Name] = counter_type\n\n                        self.logger.debug(\n                            u\"Caching property qualifier CounterType: \"\n                            \"{class_name}.{property_names} = {counter_type}\".format(\n                                class_name=self.class_name, property_names=wmi_property.Name, counter_type=counter_type\n                            )\n                        )\n                    else:\n                        self.logger.debug(\n                            u\"CounterType qualifier not found for {class_name}.{property_names}\".format(\n                                class_name=self.class_name, property_names=wmi_property.Name\n                            )\n                        )\n\n                try:\n                    item[wmi_property.Name] = float(wmi_property.Value)\n                except (TypeError, ValueError):\n                    item[wmi_property.Name] = wmi_property.Value\n\n            results.append(item)\n        return results",
        "sampled": "```python\nclass WMIObject:\n    def __init__(self, name, freemegabytes, avgdiskbytesperwrite):\n        self.name = name\n        self.freemegabytes = freemegabytes\n        self.avgdiskbytesperwrite = avgdiskbytesperwrite\n\n    @staticmethod\n    def from_dict(wmi_dict):\n        return WMIObjext(name=wmi_dict['name'], freemegabytes=wmi_dict['freemegabytes'], \n                        "
    },
    {
        "original": "def _Reg2Py(data, size, data_type):\n  \"\"\"Converts a Windows Registry value to the corresponding Python data type.\"\"\"\n  if data_type == winreg.REG_DWORD:\n    if size == 0:\n      return 0\n    # DWORD is an unsigned 32-bit integer, see:\n    # https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-dtyp/262627d8-3418-4627-9218-4ffe110850b2\n    return ctypes.cast(data, ctypes.POINTER(ctypes.c_uint32)).contents.value\n  elif data_type == winreg.REG_SZ or data_type == winreg.REG_EXPAND_SZ:\n    return ctypes.wstring_at(data, size // 2).rstrip(u\"\\x00\")\n  elif data_type == winreg.REG_MULTI_SZ:\n    return ctypes.wstring_at(data, size // 2).rstrip(u\"\\x00\").split(u\"\\x00\")\n  else:\n    if size == 0:\n      return None\n    return ctypes.string_at(data, size)",
        "sampled": "```python\ndef _Reg2Py(data, size, data_type):\n    \"\"\"\n    Converts a Windows Registry value to the corresponding Python data type.\n\n    Args:\n    data (bytes): The Windows Registry value as a bytes object.\n    size (int): The size of the value in bytes.\n    data_type (str): The type of the value. Must be one of 'REG_SZ', 'REG_MULTYZ', 'REG_DWORD',\n                     'REG_QWORD', or 'HKEY"
    },
    {
        "original": "def enable_branching_model(self, project, repository):\n        \"\"\"\n        Enable branching model by setting it with default configuration\n        :param project:\n        :param repository:\n        :return:\n        \"\"\"\n        default_model_data = {'development': {'refId': None, 'useDefault': True},\n                              'types': [{'displayName': 'Bugfix',\n                                         'enabled': True,\n                                         'id': 'BUGFIX',\n                                         'prefix': 'bugfix/'},\n                                        {'displayName': 'Feature',\n                                         'enabled': True,\n                                         'id': 'FEATURE',\n                                         'prefix': 'feature/'},\n                                        {'displayName': 'Hotfix',\n                                         'enabled': True,\n                                         'id': 'HOTFIX',\n                                         'prefix': 'hotfix/'},\n                                        {'displayName': 'Release',\n                                         'enabled': True,\n                                         'id': 'RELEASE',\n                                         'prefix': 'release/'}]}\n        return self.set_branching_model(project,\n                                        repository,\n                                        default_model_data)",
        "sampled": "```python\nclass GitRepository:\n    def __init__(self):\n        self.branching_models = {}\n\n    def enable_branching_model(self, project_name, repository_name):\n        \"\"\"\n        Enable branching model by setting it with default configuration\n        :param project_name:\n        :param repository_name:\n        :return:\n        \"\"\"\n        \n        # Check if the project already has a branching model enabled\n        if project_name not in self.branching_models:\n            # If not, create a new one"
    },
    {
        "original": "def search(self, ngram):\n        \"\"\"\n        Parameters\n        ----------\n        ngram, str or unicode, string to search for\n\n        Returns\n        -------\n        pd.DataFrame, {self._parsed_col: <matching texts>, self._category_col: <corresponding categories>, ...}\n\n        \"\"\"\n        mask = self._document_index_mask(ngram)\n        return self._df[mask]",
        "sampled": "```python\nimport pandas as pd\n\nclass SearchEngine:\n    def __init__(self, df):\n        self.df = df\n        self._parsed_col = 'text'\n        self._category_col = 'category'\n\n    def search(self, ngram):\n        \"\"\"\n        Parameters\n        ----------\n        ngram, str or unicode, string to search for\n\n        Returns\n        -------\n        \n            pd.DataFrame: matching texts and corresponding categories\n            A pandas DataFrame with one row per match. \n"
    },
    {
        "original": "def load_source(name, pathname):\n    \"\"\"\n    This function provides the backward compatibility for 'imp.load_source'\n    in Python 2.\n\n    :param name: Name used to create or access a module object.\n    :param pathname: Path pointing to the source file.\n    :return: Loaded and initialized module.\n    \"\"\"\n    if six.PY2:\n        import imp\n        return imp.load_source(name, pathname)\n    else:\n        loader = importlib.machinery.SourceFileLoader(name, pathname)\n        return loader.load_module(name)",
        "sampled": "```python\nimport imp\n\ndef load_source(name, pathname):\n    \"\"\"\n    This function provides the backward compatibility for 'imp.load_source'\n    in Python 2.\n\n    :param name: Name used to create or access a module object.\n    :param pathname: Path pointing to the source file.\n    :return: Loaded and initialized module.\n    \"\"\"\n    \n    # Import the module using imp.load_module\n    module = imp.load_module(name, None, pathname, ('.py', '"
    },
    {
        "original": "def main(unused_argv):\n  \"\"\"Print the valid actions.\"\"\"\n  feats = features.Features(\n      # Actually irrelevant whether it's feature or rgb size.\n      features.AgentInterfaceFormat(\n          feature_dimensions=features.Dimensions(\n              screen=FLAGS.screen_size,\n              minimap=FLAGS.minimap_size)))\n  action_spec = feats.action_spec()\n  flattened = 0\n  count = 0\n  for func in action_spec.functions:\n    if FLAGS.hide_specific and actions.FUNCTIONS[func.id].general_id != 0:\n      continue\n    count += 1\n    act_flat = 1\n    for arg in func.args:\n      for size in arg.sizes:\n        act_flat *= size\n    flattened += act_flat\n    print(func.str(True))\n  print(\"Total base actions:\", count)\n  print(\"Total possible actions (flattened):\", flattened)",
        "sampled": "```python\ndef main(unused_argv):\n    \"\"\"Print the valid actions.\"\"\"\n    print(\"Your valid actions are:\")\n    print(\"   debugger: Runs the Python debugger.\")\n    print(\"   exit [-n]: Exits the interactive shell with status code n, or ZERO if no n is given.\")\n    print(\"   help [-s] [<topic>...]: Shows a list of commands or helps with a specific topic.\")\n    print(\"   import [-c] <mod_name>: Tries"
    },
    {
        "original": "def _get_hash_by_shell():\n    \"\"\"\n    Shell-out Python 3 for compute reliable hash\n    :return:\n    \"\"\"\n    id_ = __opts__.get('id', '')\n    id_hash = None\n    py_ver = sys.version_info[:2]\n    if py_ver >= (3, 3):\n        # Python 3.3 enabled hash randomization, so we need to shell out to get\n        # a reliable hash.\n        id_hash = __salt__['cmd.run']([sys.executable, '-c', 'print(hash(\"{0}\"))'.format(id_)],\n                                      env={'PYTHONHASHSEED': '0'})\n        try:\n            id_hash = int(id_hash)\n        except (TypeError, ValueError):\n            log.debug('Failed to hash the ID to get the server_id grain. Result of hash command: %s', id_hash)\n            id_hash = None\n    if id_hash is None:\n        # Python < 3.3 or error encountered above\n        id_hash = hash(id_)\n\n    return abs(id_hash % (2 ** 31))",
        "sampled": "```python\nimport subprocess\n\ndef _get_hash_by_shell(filename):\n    \"\"\"\n    Shell-out Python 3 for compute reliable hash\n    :param filename: The file to compute the hash for\n    :return: A string representing the MD5, SHA-1 and SHA-256 hashes of the file in hexadecimal format, separated by colons.\n    \"\"\"\n    \n    # Run shell command to compute MD5 hash using `md5sum` command (ssl-cert:cert.pem)\n    md"
    },
    {
        "original": "def get_minions():\n    \"\"\"\n    Return a list of minion identifiers from a request of the view.\n    \"\"\"\n    options = _get_options(ret=None)\n\n    # Make sure the views are valid, which includes the minions..\n    if not ensure_views():\n        return []\n\n    # Make the request for the view..\n    _response = _request(\"GET\",\n                         options['url'] +\n                                 options['db'] +\n                                 \"/_design/salt/_view/minions?group=true\")\n\n    # Verify that we got a response back.\n    if 'rows' not in _response:\n        log.error('Unable to get available minions: %s', _response)\n        return []\n\n    # Iterate over the rows to build up a list return it.\n    _ret = []\n    for row in _response['rows']:\n        _ret.append(row['key'])\n    return _ret",
        "sampled": "```python\ndef get_minions():\n    \"\"\"\n    Return a list of minion identifiers from a request of the view.\n    \"\"\"\n    \n    # Define an example request data\n    request_data = {\n        'request': [\n            {'identifier': 'Minion-1', 'level': 1},\n            {'identifier': 'Minion-2', 'level': 2},\n            {'identifier': 'Minion-3', 'level': 3}\n        ]\n    }\n    \n    #"
    },
    {
        "original": "def output(self, key, obj):\n        \"\"\"Pulls the value for the given key from the object, applies the\n        field's formatting and returns the result. If the key is not found\n        in the object, returns the default value. Field classes that create\n        values which do not require the existence of the key in the object\n        should override this and return the desired value.\n\n        :exception MarshallingException: In case of formatting problem\n        \"\"\"\n\n        value = get_value(key if self.attribute is None else self.attribute, obj)\n\n        if value is None:\n            return self.default\n\n        return self.format(value)",
        "sampled": "```python\nclass MarshallingException(Exception):\n    pass\n\n\nclass Field:\n    def __init__(self):\n        self.default = None\n\n    def get_default(self):\n        return self.default if self.default is not None else ''\n\n    def output(self, key, obj):\n        try:\n            value = obj.get(key)\n            if value is None:\n                return self.get_default()\n            elif isinstance(value, str) and value.is_number():\n                return str(int(value))\n            else:\n                return"
    },
    {
        "original": "def _timeout_exceeded(self, start, msg=\"Timeout exceeded!\"):\n        \"\"\"Raise NetMikoTimeoutException if waiting too much in the serving queue.\n\n        :param start: Initial start time to see if session lock timeout has been exceeded\n        :type start: float (from time.time() call i.e. epoch time)\n\n        :param msg: Exception message if timeout was exceeded\n        :type msg: str\n        \"\"\"\n        if not start:\n            # Must provide a comparison time\n            return False\n        if time.time() - start > self.session_timeout:\n            # session_timeout exceeded\n            raise NetMikoTimeoutException(msg)\n        return False",
        "sampled": "```python\nclass NetMikoTimeoutException(Exception):\n    \"\"\"Reraise Exception with custom reason\"\"\"\n    pass\n\nclass TimeoutTimer:\n    def __init__(self, interval):\n        self.interval = interval\n        self.start_time = None\n\n    def start(self):\n        if self.start_time is None:\n            self.start_time = time.time()\n\n    def check_timeout(self):\n        if self.start_time is not None and time.time() - self.start_time > self.interval:\n            raise Net"
    },
    {
        "original": "def config_present(name):\n    \"\"\"\n    Ensure a specific configuration line exists in the running config\n\n    name\n        config line to set\n\n    Examples:\n\n    .. code-block:: yaml\n\n        add snmp group:\n          onyx.config_present:\n            - names:\n              - snmp-server community randoSNMPstringHERE group network-operator\n              - snmp-server community AnotherRandomSNMPSTring group network-admin\n\n        add snmp acl:\n          onyx.config_present:\n            - names:\n              - snmp-server community randoSNMPstringHERE use-acl snmp-acl-ro\n              - snmp-server community AnotherRandomSNMPSTring use-acl snmp-acl-rw\n    \"\"\"\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n\n    matches = __salt__['onyx.cmd']('find', name)\n\n    if matches:\n        ret['result'] = True\n        ret['comment'] = 'Config is already set'\n\n    elif __opts__['test'] is True:\n        ret['result'] = None\n        ret['comment'] = 'Config will be added'\n        ret['changes']['new'] = name\n\n    else:\n        __salt__['onyx.cmd']('add_config', name)\n        matches = __salt__['onyx.cmd']('find', name)\n        if matches:\n            ret['result'] = True\n            ret['comment'] = 'Successfully added config'\n            ret['changes']['new'] = name\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Failed to add config'\n\n    return ret",
        "sampled": "```python\nimport yaml\n\ndef config_present(name, names):\n    \"\"\"\n    Ensure a specific configuration line exists in the running config\n\n    name:\n        configuration type (group or acl)\n        Ex: group or acl\n    names:\n        list of config lines to be added\n        Ex: [snmp-server community randoSNMPstringHERE group network-operator, snmp-server community AnotherRandomSNMPSTring group network-admin]\n    \n    Returns:\n        collection of commands that need to"
    },
    {
        "original": "def _get_charge_distribution_df(self):\n        \"\"\"\n        Return a complete table of fractional coordinates - charge density.\n        \"\"\"\n        # Fraction coordinates and corresponding indices\n        axis_grid = np.array([np.array(self.chgcar.get_axis_grid(i)) /\n                              self.structure.lattice.abc[i] for i in range(3)])\n        axis_index = np.array([range(len(axis_grid[i])) for i in range(3)])\n\n        data = {}\n\n        for index in itertools.product(*axis_index):\n            a, b, c = index\n            f_coords = (axis_grid[0][a], axis_grid[1][b], axis_grid[2][c])\n            data[f_coords] = self.chgcar.data[\"total\"][a][b][c]\n\n        # Fraction coordinates - charge density table\n        df = pd.Series(data).reset_index()\n        df.columns = ['a', 'b', 'c', 'Charge Density']\n        self._charge_distribution_df = df\n\n        return df",
        "sampled": "```python\nimport pandas as pd\nimport numpy as np\n\nclass ChargeDistribution:\n    def __init__(self, fractional_coordinates, charge_density):\n        self.fractional_coordinates = fractional_coordinates\n        self.charge_density = charge_density\n\n    def _get_charge_distribution_df(self):\n        \"\"\"\n        Return a complete table of fractional coordinates - charge density.\n        \n        Parameters:\n            None\n        \n        Returns:\n            DataFrame: A table with two columns 'Fractional Coordinates' and 'Charge Density'.\n"
    },
    {
        "original": "def format_decimal(self, altitude=None):\n        \"\"\"\n        Format decimal degrees with altitude\n        \"\"\"\n        coordinates = [str(self.latitude), str(self.longitude)]\n\n        if altitude is None:\n            altitude = bool(self.altitude)\n        if altitude:\n            if not isinstance(altitude, string_compare):\n                altitude = 'km'\n            coordinates.append(self.format_altitude(altitude))\n\n        return \", \".join(coordinates)",
        "sampled": "```python\nfrom math import trunc\nfrom sympy import Rational\n\nclass FormatDecimal:\n    def format_decimal(self, altitude=None):\n        \"\"\"\n        Format decimal degrees with altitude\n        \"\"\"\n        \n        # Define a degree as 1/360 of a circle (in decimal degrees)\n        degree = Rational(1, 360)\n        \n        # Convert given altitude to decimal degrees (if provided)\n        if altitude is not None:\n            decimal_altitude = round(altitude * 100000)"
    },
    {
        "original": "def time2slurm(timeval, unit=\"s\"):\n    \"\"\"\n    Convert a number representing a time value in the given unit (Default: seconds)\n    to a string following the slurm convention: \"days-hours:minutes:seconds\".\n\n    >>> assert time2slurm(61) == '0-0:1:1' and time2slurm(60*60+1) == '0-1:0:1'\n    >>> assert time2slurm(0.5, unit=\"h\") == '0-0:30:0'\n    \"\"\"\n    d, h, m, s = 24*3600, 3600, 60, 1\n\n    timeval = Time(timeval, unit).to(\"s\")\n    days, hours = divmod(timeval, d)\n    hours, minutes = divmod(hours, h)\n    minutes, secs = divmod(minutes, m)\n\n    return \"%d-%d:%d:%d\" % (days, hours, minutes, secs)",
        "sampled": "```python\ndef time2slurm(timeval, unit=\"s\"):\n    \"\"\"\n    Convert a number representing a time value in the given unit (Default: seconds)\n    to a string following the slurm convention: \"days-hours:minutes:seconds\".\n    \n    Warnings:\n        * Time unit should be either 's', 'h' or any other ton of time that is \n          step component of hours by 1 second like minutes.\n        * this function has not been implementated"
    },
    {
        "original": "def explicit_indexing_adapter(\n        key, shape, indexing_support, raw_indexing_method):\n    \"\"\"Support explicit indexing by delegating to a raw indexing method.\n\n    Outer and/or vectorized indexers are supported by indexing a second time\n    with a NumPy array.\n\n    Parameters\n    ----------\n    key : ExplicitIndexer\n        Explicit indexing object.\n    shape : Tuple[int, ...]\n        Shape of the indexed array.\n    indexing_support : IndexingSupport enum\n        Form of indexing supported by raw_indexing_method.\n    raw_indexing_method: callable\n        Function (like ndarray.__getitem__) that when called with indexing key\n        in the form of a tuple returns an indexed array.\n\n    Returns\n    -------\n    Indexing result, in the form of a duck numpy-array.\n    \"\"\"\n    raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n    result = raw_indexing_method(raw_key.tuple)\n    if numpy_indices.tuple:\n        # index the loaded np.ndarray\n        result = NumpyIndexingAdapter(np.asarray(result))[numpy_indices]\n    return result",
        "sampled": "```python\nimport numpy as np\nfrom typing import Tuple, Callable\n\nclass ExplicitIndexer:\n    def __init__(self, indices: Tuple[int]):\n        self.indices = indices\n\nclass IndexingSupport:\n    Individual = 1\n    Outer = 2\n    Vectorized = 4\n\ndef explicit_indexing_adapter(\n        key: ExplicitIndexer, \n        shape: Tuple[int], \n        indexing_support: IndexingSupport, \n        raw_indexing_method: Callable[["
    },
    {
        "original": "def select(self, *attributes):\n        \"\"\" Adds the attribute to the $select parameter\n\n        :param str attributes: the attributes tuple to select.\n         If empty, the on_attribute previously set is added.\n        :rtype: Query\n        \"\"\"\n        if attributes:\n            for attribute in attributes:\n                attribute = self.protocol.convert_case(\n                    attribute) if attribute and isinstance(attribute,\n                                                           str) else None\n                if attribute:\n                    if '/' in attribute:\n                        # only parent attribute can be selected\n                        attribute = attribute.split('/')[0]\n                    self._selects.add(attribute)\n        else:\n            if self._attribute:\n                self._selects.add(self._attribute)\n\n        return self",
        "sampled": "```python\nclass Query:\n    def __init__(self):\n        self.select = tuple()\n\n    def add_attribute_to_select(self, *attributes):\n        if not attributes:\n            return self.select\n        else:\n            updated_attributes = self.select + attributes\n            return tuple(updated_attributes)\n\n    def select(self, *attributes):\n        \"\"\"Adds the attribute to the select parameter\n\n        :param str attributes: The attributes tuple to select.\n         If empty, the on_attribute previously set is added.\n"
    },
    {
        "original": "def apply_signing(self, plan_id, contract_code, contract_display_account, notify_url,\n                      version=\"1.0\", clientip=None, deviceid=None, mobile=None, email=None, qq=None,\n                      request_serial=None, openid=None, creid=None, outerid=None):\n        \"\"\"\n        \u7533\u8bf7\u7b7e\u7ea6 api\n\n        https://pay.weixin.qq.com/wiki/doc/api/pap.php?chapter=18_1&index=1\n\n        :param plan_id: \u6a21\u677fid \u534f\u8bae\u6a21\u677fid\uff0c\u8bbe\u7f6e\u8def\u5f84\u89c1\u5f00\u53d1\u6b65\u9aa4\u3002\n        :param contract_code: \u7b7e\u7ea6\u534f\u8bae\u53f7 \u5546\u6237\u4fa7\u7684\u7b7e\u7ea6\u534f\u8bae\u53f7\uff0c\u7531\u5546\u6237\u751f\u6210\n        :param contract_display_account: \u7528\u6237\u8d26\u6237\u5c55\u793a\u540d\u79f0 \u7b7e\u7ea6\u7528\u6237\u7684\u540d\u79f0\uff0c\u7528\u4e8e\u9875\u9762\u5c55\u793a\uff0c\u9875\u9762\u6837\u4f8b\u53ef\u89c1\u6848\u4f8b\u4e0e\u89c4\u8303\n        :param notify_url: \u56de\u8c03\u901a\u77e5url \u7528\u4e8e\u63a5\u6536\u7b7e\u7ea6\u6210\u529f\u6d88\u606f\u7684\u56de\u8c03\u901a\u77e5\u5730\u5740\uff0c\u4ee5http\u6216https\u5f00\u5934\u3002\n        :param version: \u7248\u672c\u53f7 \u56fa\u5b9a\u503c1.0\n        :param request_serial: \u53ef\u9009 \u8bf7\u6c42\u5e8f\u5217\u53f7 \u5546\u6237\u8bf7\u6c42\u7b7e\u7ea6\u65f6\u7684\u5e8f\u5217\u53f7\uff0c\u5546\u6237\u4fa7\u987b\u552f\u4e00\u3002\u5e8f\u5217\u53f7\u4e3b\u8981\u7528\u4e8e\u6392\u5e8f\uff0c\u4e0d\u4f5c\u4e3a\u67e5\u8be2\u6761\u4ef6\n        :param clientip: \u53ef\u9009 \u5ba2\u6237\u7aef IP \u70b9\u5206IP\u683c\u5f0f(\u5ba2\u6237\u7aefIP)\n        :param deviceid: \u53ef\u9009 \u8bbe\u5907ID android\u586bimei\u7684\u4e00\u6b21md5; ios\u586bidfa\u7684\u4e00\u6b21md5\n        :param mobile: \u53ef\u9009 \u624b\u673a\u53f7 \u7528\u6237\u624b\u673a\u53f7\n        :param email: \u53ef\u9009 \u90ae\u7bb1\u5730\u5740 \u7528\u6237\u90ae\u7bb1\u5730\u5740\n        :param qq: \u53ef\u9009 QQ\u53f7 \u7528\u6237QQ\u53f7\n        :param openid: \u53ef\u9009 \u5fae\u4fe1open ID \u7528\u6237\u5fae\u4fe1open ID\n        :param creid: \u53ef\u9009 \u8eab\u4efd\u8bc1\u53f7 \u7528\u6237\u8eab\u4efd\u8bc1\u53f7\n        :param outerid: \u53ef\u9009 \u5546\u6237\u4fa7\u7528\u6237\u6807\u8bc6 \u7528\u6237\u5728\u5546\u6237\u4fa7\u7684\u6807\u8bc6\n        :return: \u8fd4\u56de\u7684\u7ed3\u679c\u6570\u636e\u5b57\u5178\n        \"\"\"\n        timestamp = int(time.time())\n        if request_serial is None:\n            request_serial = int(time.time() * 1000)\n        data = {\n            \"appid\": self.appid,\n            \"mch_id\": self.mch_id,\n            \"sub_mch_id\": self.sub_mch_id,\n            \"plan_id\": plan_id,\n            \"contract_code\": contract_code,\n            \"request_serial\": request_serial,\n            \"contract_display_account\": contract_display_account,\n            \"notify_url\": notify_url,\n            \"version\": version,\n            \"timestamp\": timestamp,\n            \"clientip\": clientip,\n            \"deviceid\": deviceid,\n            \"mobile\": mobile,\n            \"email\": email,\n            \"qq\": qq,\n            \"openid\": openid,\n            \"creid\": creid,\n            \"outerid\": outerid,\n        }\n        data = optionaldict(data)\n        sign = calculate_signature(data, self._client.api_key)\n        data[\"sign\"] = sign\n        return {\n            \"base_url\": \"{}papay/entrustweb\".format(self._client.API_BASE_URL),\n            \"data\": data\n        }",
        "sampled": "```python\nclass PayAPI:\n    def apply_signing(self, \n                      plan_id, \n                      contract_code, \n                      contract_display_account, \n                      notify_url,\n                      version=\"1.0\", \n                      clientip=None, \n                      deviceid=None, \n                      mobile=None, \n                      email=None,  \n                       qq=None,\n                       request_serial=None,  \n                       openid=None,\n                        creid=None,\n                          outerid=None):\n        \"\"\"\n        \u7533\u8bf7\u7b7e\u7ea6 api\n\n        https"
    },
    {
        "original": "def whois(ip_address):\n    \"\"\"Whois client for Python\"\"\"\n    whois_ip = str(ip_address)\n    try:\n        query = socket.gethostbyname(whois_ip)\n    except Exception:\n        query = whois_ip\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((\"whois.ripe.net\", 43))\n    s.send(query.encode(\"utf8\") + b\"\\r\\n\")\n    answer = b\"\"\n    while True:\n        d = s.recv(4096)\n        answer += d\n        if not d:\n            break\n    s.close()\n    ignore_tag = b\"remarks:\"\n    # ignore all lines starting with the ignore_tag\n    lines = [line for line in answer.split(b\"\\n\") if not line or (line and not line.startswith(ignore_tag))]  # noqa: E501\n    # remove empty lines at the bottom\n    for i in range(1, len(lines)):\n        if not lines[-i].strip():\n            del lines[-i]\n        else:\n            break\n    return b\"\\n\".join(lines[3:])",
        "sampled": "```python\nimport whois\n\ndef whois_client(ip_address):\n    \"\"\"\n    This function takes an IP address as input, performs WHOIS lookup and \n    returns the result.\n    \n    Parameters:\n    ip_address (str): The IP address for which to perform WHOIS lookup.\n    \n    Returns:\n    dict: A dictionary containing the WHOIS information of the given IP address.\n    \"\"\"\n    \n    # Check if the ip_address is a valid string\n    if not isinstance(ip_address,"
    },
    {
        "original": "def clear_autoflow(cls, obj, name=None):\n        \"\"\"\n        Clear autoflow's tensor storage.\n\n        :param obj: target GPflow object.\n        :param name: accepts either string value which is unique part of\n            an internal attribute name or None value. When None value is passed all\n            storages will be cleared, in other words it clears everything with common\n            autoflow prefix.\n\n        :raises: ValueError exception if `name` is not a string.\n        \"\"\"\n        if name is not None and not isinstance(name, str):\n            raise ValueError('Name must be a string.')\n        prefix = cls.__autoflow_prefix__\n        if name:\n            prefix = \"\" if name.startswith(prefix) else prefix\n            delattr(obj, prefix + name)\n        else:\n            keys = [attr for attr in obj.__dict__ if attr.startswith(prefix)]\n            for key in keys:\n                delattr(obj, key)",
        "sampled": "```python\ndef clear_autoflow(cls, obj, name=None):\n    \"\"\"\n    Clear autoflow's tensor storage.\n\n    :param obj: target GPflow object.\n    :param name: accepts either string value which is unique part of\n        an internal attribute name or None value. When None value is passed all\n        storages will be cleared, in other words it clears everything with common\n        autoflow prefix.\n\n    :raises: ValueError exception if `name` is not a"
    },
    {
        "original": "def encode(self, inputs, attention_bias):\n    \"\"\"Generate continuous representation for inputs.\n\n    Args:\n      inputs: int tensor with shape [batch_size, input_length].\n      attention_bias: float tensor with shape [batch_size, 1, 1, input_length]\n\n    Returns:\n      float tensor with shape [batch_size, input_length, hidden_size]\n    \"\"\"\n    with tf.name_scope(\"encode\"):\n      # Prepare inputs to the layer stack by adding positional encodings and\n      # applying dropout.\n      embedded_inputs = self.embedding_softmax_layer(inputs)\n      inputs_padding = model_utils.get_padding(inputs)\n\n      with tf.name_scope(\"add_pos_encoding\"):\n        length = tf.shape(embedded_inputs)[1]\n        pos_encoding = model_utils.get_position_encoding(\n            length, self.params.hidden_size)\n        encoder_inputs = embedded_inputs + pos_encoding\n\n      if self.train:\n        mlperf_log.transformer_print(\n            key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n            value=self.params.layer_postprocess_dropout)\n        encoder_inputs = tf.nn.dropout(\n            encoder_inputs, 1 - self.params.layer_postprocess_dropout)\n\n      return self.encoder_stack(encoder_inputs, attention_bias, inputs_padding)",
        "sampled": "```python\nimport numpy as np\n\nclass Model:\n    def encode(self, inputs, attention_bias):\n        \"\"\"\n        Generate continuous representation for inputs.\n\n        Args:\n          inputs (np.array): int tensor with shape [batch_size, input_length].\n          attention_bias (np.array): float tensor with shape [batch_size, 1, 1, input_length]\n\n        Returns:\n          np.array: float tensor with shape [batch_size, input_length, hidden_size]\n        \"\"\"\n\n        # Assuming"
    },
    {
        "original": "def tokenize(\n        text,\n        lowercase=False,\n        deacc=False,\n        encoding='utf8',\n        errors=\"strict\",\n        to_lower=False,\n        lower=False):\n    \"\"\"\n    Iteratively yield tokens as unicode strings, removing accent marks\n    and optionally lowercasing the unidoce string by assigning True\n    to one of the parameters, lowercase, to_lower, or lower.\n\n    Input text may be either unicode or utf8-encoded byte string.\n\n    The tokens on output are maximal contiguous sequences of alphabetic\n    characters (no digits!).\n\n    >>> list(tokenize('Nic nem\u016f\u017ee let\u011bt rychlost\u00ed vy\u0161\u0161\u00ed, ne\u017e 300 tis\u00edc kilometr\u016f za sekundu!', deacc = True))\n    [u'Nic', u'nemuze', u'letet', u'rychlosti', u'vyssi', u'nez', u'tisic', u'kilometru', u'za', u'sekundu']\n\n    \"\"\"\n    lowercase = lowercase or to_lower or lower\n    text = to_unicode(text, encoding, errors=errors)\n    if lowercase:\n        text = text.lower()\n    if deacc:\n        text = deaccent(text)\n    return simple_tokenize(text)",
        "sampled": "```python\nimport unicodedata\nimport re\n\ndef tokenize(\n    text,\n    lowercase=False,\n    deacc=False,\n    encoding='utf8',\n    errors=\"strict\",\n    to_lower=False,\n    lower=False):\n    \n        # Decode input text to unicode string if it's not already.\n        if isinstance(text, str) and text.encoding != 'unicode-escape':\n            try:\n                text = text.encode(encoding, errors).decode('unicode-escape')\n            except ValueError as e"
    },
    {
        "original": "def _fix_outgoing(self, son, collection):\n        \"\"\"Apply manipulators to a SON object as it comes out of the database.\n\n        :Parameters:\n          - `son`: the son object coming out of the database\n          - `collection`: the collection the son object was saved in\n        \"\"\"\n        for manipulator in reversed(self.__outgoing_manipulators):\n            son = manipulator.transform_outgoing(son, collection)\n        for manipulator in reversed(self.__outgoing_copying_manipulators):\n            son = manipulator.transform_outgoing(son, collection)\n        return son",
        "sampled": "```python\nclass SonDB:\n    def _fix_outgoing(self, son, collection):\n        \"\"\"\n        Apply manipulators to a SON object as it comes out of the database.\n\n        :Parameters:\n          - `son`: the son object coming out of the database\n          - `collection`: the collection the son object was saved in\n        \"\"\"\n\n        # Apply manipulators to fields such as 'created_at' and 'updated_at'\n        for field in ['created_at', 'updated_at"
    },
    {
        "original": "def _easy_facetgrid(data, plotfunc, kind, x=None, y=None, row=None,\n                    col=None, col_wrap=None, sharex=True, sharey=True,\n                    aspect=None, size=None, subplot_kws=None, **kwargs):\n    \"\"\"\n    Convenience method to call xarray.plot.FacetGrid from 2d plotting methods\n\n    kwargs are the arguments to 2d plotting method\n    \"\"\"\n    ax = kwargs.pop('ax', None)\n    figsize = kwargs.pop('figsize', None)\n    if ax is not None:\n        raise ValueError(\"Can't use axes when making faceted plots.\")\n    if aspect is None:\n        aspect = 1\n    if size is None:\n        size = 3\n    elif figsize is not None:\n        raise ValueError('cannot provide both `figsize` and `size` arguments')\n\n    g = FacetGrid(data=data, col=col, row=row, col_wrap=col_wrap,\n                  sharex=sharex, sharey=sharey, figsize=figsize,\n                  aspect=aspect, size=size, subplot_kws=subplot_kws)\n\n    if kind == 'line':\n        return g.map_dataarray_line(plotfunc, x, y, **kwargs)\n\n    if kind == 'dataarray':\n        return g.map_dataarray(plotfunc, x, y, **kwargs)",
        "sampled": "```python\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\ndef _easy_facetgrid(data, plotfunc, kind, x=None, y=None, row=None,\n                    col=None, col_wrap=None, sharex=True, sharey=True,\n                    aspect=None, size=None, subplot_kws=None,\n                    **kwargs):\n    \"\"\"\n    Convenience method to call xarray.plot.FacetGrid from 2d plotting methods\n\n    kwargs are the arguments to 2d plotting method"
    },
    {
        "original": "def _parse_name(name: str, as_qvm: bool, noisy: bool) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n\n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    parts = name.split('-')\n    if len(parts) >= 2 and parts[-2] == 'noisy' and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n\n        if noisy is not None and (not noisy):\n            raise ValueError(\"The provided qc name indicates you are getting a noisy QVM, \"\n                             \"but you have specified `noisy=False`\")\n\n        qvm_type = parts[-1]\n        noisy = True\n        prefix = '-'.join(parts[:-2])\n        return prefix, qvm_type, noisy\n\n    if len(parts) >= 1 and parts[-1] in ['qvm', 'pyqvm']:\n        if as_qvm is not None and (not as_qvm):\n            raise ValueError(\"The provided qc name indicates you are getting a QVM, \"\n                             \"but you have specified `as_qvm=False`\")\n        qvm_type = parts[-1]\n        if noisy is None:\n            noisy = False\n        prefix = '-'.join(parts[:-1])\n        return prefix, qvm_type, noisy\n\n    if as_qvm is not None and as_qvm:\n        qvm_type = 'qvm'\n    else:\n        qvm_type = None\n\n    if noisy is None:\n        noisy = False\n\n    return name, qvm_type, noisy",
        "sampled": "```python\nfrom typing import Tuple\n\ndef _parse_name(name: str, as_qvm: bool, noisy: bool) -> Tuple[str, str, bool]:\n    \"\"\"\n    Try to figure out whether we're getting a (noisy) qvm, and the associated qpu name.\n    \n    See :py:func:`get_qc` for examples of valid names + flags.\n    \"\"\"\n    \n    # First, we need to separate the QPU name from the flag\n   "
    },
    {
        "original": "def structure_transform(self, original_structure, new_structure,\n                            refine_rotation=True):\n        \"\"\"\n        Transforms a tensor from one basis for an original structure\n        into a new basis defined by a new structure.\n\n        Args:\n            original_structure (Structure): structure corresponding\n                to the basis of the current tensor\n            new_structure (Structure): structure corresponding to the\n                desired basis\n            refine_rotation (bool): whether to refine the rotations\n                generated in get_ieee_rotation\n\n        Returns:\n            Tensor that has been transformed such that its basis\n            corresponds to the new_structure's basis\n        \"\"\"\n        sm = StructureMatcher()\n        if not sm.fit(original_structure, new_structure):\n            warnings.warn(\"original and new structures do not match!\")\n        trans_1 = self.get_ieee_rotation(original_structure, refine_rotation)\n        trans_2 = self.get_ieee_rotation(new_structure, refine_rotation)\n        # Get the ieee format tensor\n        new = self.rotate(trans_1)\n        # Reverse the ieee format rotation for the second structure\n        new = new.rotate(np.transpose(trans_2))\n        return new",
        "sampled": "```python\nfrom typing import Any, Dict\n\nclass Structure:\n    def __init__(self, basis: Dict[str, Any]):\n        self.basis = basis\n\nclass Tensor:\n    def __init__(self, data: Any):\n        self.data = data\n        self.structure = None\n\nclass Programmer:\n    @staticmethod\n    def calculate_rotation_matrix(original_basis: Dict[str, Any], new_basis: Dict[str, Any]) -> 'Programmer':\n        rotation_matrices = []\n        \n       "
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'step') and self.step is not None:\n            _dict['step'] = self.step\n        if hasattr(self, 'snapshot') and self.snapshot is not None:\n            _dict['snapshot'] = self.snapshot\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.F = [0,1]\n    \n    def fibonacci(self, n: int) -> int:\n        \"\"\"\n        Calculate the given number in Fibonacci sequence.\n\n        # Mathematics\n            The Fibonacci sequence is a series of numbers such that each number is the sum of the two preceding ones, usually starting with 0 and 1.\n            \n            Let `F(n)` be the `n`-th number in this sequence."
    },
    {
        "original": "def reverse_timezone(self, query, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Find the timezone for a point in `query`.\n\n        GeoNames always returns a timezone: if the point being queried\n        doesn't have an assigned Olson timezone id, a ``pytz.FixedOffset``\n        timezone is used to produce the :class:`geopy.timezone.Timezone`.\n\n        .. versionadded:: 1.18.0\n\n        :param query: The coordinates for which you want a timezone.\n        :type query: :class:`geopy.point.Point`, list or tuple of (latitude,\n            longitude), or string as \"%(latitude)s, %(longitude)s\"\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: :class:`geopy.timezone.Timezone`\n        \"\"\"\n        ensure_pytz_is_installed()\n\n        try:\n            lat, lng = self._coerce_point_to_string(query).split(',')\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n\n        params = {\n            \"lat\": lat,\n            \"lng\": lng,\n            \"username\": self.username,\n        }\n\n        url = \"?\".join((self.api_timezone, urlencode(params)))\n\n        logger.debug(\"%s.reverse_timezone: %s\", self.__class__.__name__, url)\n        return self._parse_json_timezone(\n            self._call_geocoder(url, timeout=timeout)\n        )",
        "sampled": "```python\nfrom datetime import datetime\n\nclass TimezoneFinder:\n    def __init__(self):\n        self.timezones = {}\n\n    def reverse_timezone(self, query, timeout=30):\n        \"\"\"\n        Find the timezone for a point in `query`.\n\n        :param query: The coordinates for which you want a timezone.\n                        It should be either a dictionary with 'latitude' and\n                        'longitude' keys or string as \"%(latitude)s, %(longitude)s\".\n        :type query:"
    },
    {
        "original": "def add_file(self, src, dest=None):\n        \"\"\"Add the file at ``src`` to the archive.\n\n        If ``dest`` is ``None`` then it is added under just the original\n        filename. So ``add_file('foo/bar.txt')`` ends up at ``bar.txt`` in the\n        archive, while ``add_file('bar.txt', 'foo/bar.txt')`` ends up at\n        ``foo/bar.txt``.\n\n        \"\"\"\n        dest = dest or os.path.basename(src)\n        with open(src, 'rb') as fp:\n            contents = fp.read()\n        self.add_contents(dest, contents)",
        "sampled": "```python\nclass Archive:\n    def __init__(self):\n        self.files = {}\n\n    def add_file(self, src, dest=None):\n        \"\"\"Add the file at ``src`` to the archive.\n\n        If ``dest`` is ``None`` then it is added under just the original\n        filename. So ``add_file('foo/bar.txt')`` ends up at ``bar.txt`` in the\n        archive, while ``add_file('bar.txt', 'foo/bar.txt')``"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a Configuration object from a json dictionary.\"\"\"\n        args = {}\n        if 'configuration_id' in _dict:\n            args['configuration_id'] = _dict.get('configuration_id')\n        if 'name' in _dict:\n            args['name'] = _dict.get('name')\n        else:\n            raise ValueError(\n                'Required property \\'name\\' not present in Configuration JSON')\n        if 'created' in _dict:\n            args['created'] = string_to_datetime(_dict.get('created'))\n        if 'updated' in _dict:\n            args['updated'] = string_to_datetime(_dict.get('updated'))\n        if 'description' in _dict:\n            args['description'] = _dict.get('description')\n        if 'conversions' in _dict:\n            args['conversions'] = Conversions._from_dict(\n                _dict.get('conversions'))\n        if 'enrichments' in _dict:\n            args['enrichments'] = [\n                Enrichment._from_dict(x) for x in (_dict.get('enrichments'))\n            ]\n        if 'normalizations' in _dict:\n            args['normalizations'] = [\n                NormalizationOperation._from_dict(x)\n                for x in (_dict.get('normalizations'))\n            ]\n        if 'source' in _dict:\n            args['source'] = Source._from_dict(_dict.get('source'))\n        return cls(**args)",
        "sampled": "```python\ndef _from_dict(cls, _dict):\n    \"\"\"\n    Initialize a Configuration object from a json dictionary.\n    \n    :param cls: The Configuration class.\n    :type cls: type\n    :param _dict: A json dictionary.\n    :type _dict: dict\n    \"\"\"\n    \nclass Node:\n    \n    def __init__(self, val=None, children=None):\n        # do not modify or delete this signature; run part3 tests with empty __init__\n        self"
    },
    {
        "original": "def get_name(self, r):\n        \"\"\"Given an arbitrary resource attempt to resolve back to a qualified name.\"\"\"\n        namer = ResourceNameAdapters[self.manager.resource_type.service]\n        return namer(r)",
        "sampled": "```python\nfrom typing import Dict, Any\n\nclass Resource:\n    def __init__(self, name: str):\n        self.parent = None\n        self.name = name\n\n    def __str__(self):\n        return f\"Name: {self.name}, Parent: {self.parent.name if self.parent else None}\"\n\ndef get_name(self, resource: 'Resource') -> str:\n    \"\"\"\n    Resolves a resource back to a qualified name.\n\n    The function traverses up the resource hierarchy"
    },
    {
        "original": "def parse_md_code_options(options):\n    \"\"\"Parse 'python class key=\"value\"' into [('python', None), ('class', None), ('key', 'value')]\"\"\"\n\n    metadata = []\n    while options:\n        name_and_value = re.split(r'[\\s=]+', options, maxsplit=1)\n        name = name_and_value[0]\n\n        # Equal sign in between name and what's next?\n        if len(name_and_value) == 2:\n            sep = options[len(name):-len(name_and_value[1])]\n            has_value = sep.find('=') >= 0\n            options = name_and_value[1]\n        else:\n            has_value = False\n            options = ''\n\n        if not has_value:\n            metadata.append((name, None))\n            continue\n\n        try:\n            value = loads(options)\n            options = ''\n        except JSONDecodeError as err:\n            try:\n                split = err.colno - 1\n            except AttributeError:\n                # str(err) is like: \"ValueError: Extra data: line 1 column 7 - line 1 column 50 (char 6 - 49)\"\n                match = re.match(r'.*char ([0-9]*)', str(err))\n                split = int(match.groups()[0])\n\n            value = loads(options[:split])\n            options = options[split:]\n\n        metadata.append((name, value))\n\n    return metadata",
        "sampled": "```python\nimport re\n\ndef parse_md_code_options(options):\n    \"\"\"\n    Parse 'python class key=\"value\"' into [('python', None), ('class', None), ('key', 'value')]\n    \"\"\"\n    \n    # Split the options string into individual words and attributes\n    words = re.findall(r'\\b\\w+\\b|[^\\w\\s=]', options)\n    \n    # Initialize an empty list to store the parsed options\n    parsed_options = []\n    \n    # Iterate over each"
    },
    {
        "original": "def from_db(cls, bigchain, tx_dict_list):\n        \"\"\"Helper method that reconstructs a transaction dict that was returned\n        from the database. It checks what asset_id to retrieve, retrieves the\n        asset from the asset table and reconstructs the transaction.\n\n        Args:\n            bigchain (:class:`~bigchaindb.tendermint.BigchainDB`): An instance\n                of BigchainDB used to perform database queries.\n            tx_dict_list (:list:`dict` or :obj:`dict`): The transaction dict or\n                list of transaction dict as returned from the database.\n\n        Returns:\n            :class:`~Transaction`\n\n        \"\"\"\n        return_list = True\n        if isinstance(tx_dict_list, dict):\n            tx_dict_list = [tx_dict_list]\n            return_list = False\n\n        tx_map = {}\n        tx_ids = []\n        for tx in tx_dict_list:\n            tx.update({'metadata': None})\n            tx_map[tx['id']] = tx\n            tx_ids.append(tx['id'])\n\n        assets = list(bigchain.get_assets(tx_ids))\n        for asset in assets:\n            if asset is not None:\n                tx = tx_map[asset['id']]\n                del asset['id']\n                tx['asset'] = asset\n\n        tx_ids = list(tx_map.keys())\n        metadata_list = list(bigchain.get_metadata(tx_ids))\n        for metadata in metadata_list:\n            tx = tx_map[metadata['id']]\n            tx.update({'metadata': metadata.get('metadata')})\n\n        if return_list:\n            tx_list = []\n            for tx_id, tx in tx_map.items():\n                tx_list.append(cls.from_dict(tx))\n            return tx_list\n        else:\n            tx = list(tx_map.values())[0]\n            return cls.from_dict(tx)",
        "sampled": "```python\nfrom bigchaindb import BigchainDB\nfrom asset import Asset  # Assuming Asset is a class\n\nclass Transaction:\n    def __init__(self, asset, tx_id, tx_counter):\n        self.asset = asset\n        self.tx_id = tx_id\n        self.tx_counter = tx_counter\n\ndef from_db(cls, bigchain, tx_dict_list):\n    transactions = []\n    \n    if not isinstance(tx_dict_list, list):  # Check iftx_dict_list is a"
    },
    {
        "original": "def _all_feature_names(name):\n    # type: (Union[str, bytes, List[Dict]]) -> List[str]\n    \"\"\" All feature names for a feature: usually just the feature itself,\n    but can be several features for unhashed features with collisions.\n    \"\"\"\n    if isinstance(name, bytes):\n        return [name.decode('utf8')]\n    elif isinstance(name, list):\n        return [x['name'] for x in name]\n    else:\n        return [name]",
        "sampled": "```python\nfrom typing import Union, List, Dict\n\ndef _all_feature_names(name):\n    \"\"\"\n    All feature names for a feature: usually just the feature itself,\n    but can be several features for unhashed features with collisions.\n    \n    Args:\n        name (Union[str, bytes, List[Dict]]): The feature name or a list of dictionaries containing the Feature Name.\n\n    Returns:\n        List[str]: A list of all feature names.\n    \"\"\"\n    \n    # If input is"
    },
    {
        "original": "def accept_quality(accept, default=1):\n    \"\"\"Separates out the quality score from the accepted content_type\"\"\"\n    quality = default\n    if accept and \";\" in accept:\n        accept, rest = accept.split(\";\", 1)\n        accept_quality = RE_ACCEPT_QUALITY.search(rest)\n        if accept_quality:\n            quality = float(accept_quality.groupdict().get('quality', quality).strip())\n\n    return (quality, accept.strip())",
        "sampled": "```\nimport re\n\ndef accept_quality(accept, default=1):\n    \"\"\"\n    Separates out the quality score from the accepted content_type\n    Args:\n        accept (str): The accepted content type string\n        default (int, optional): Default quality score. Defaults to 1.\n    \n    Returns:\n        int: Quality score of the accepted content type\n    \"\"\"\n    \n    # Regular expression pattern to match 'application/x-mpeg*, length '\n    pattern = r'application/x"
    },
    {
        "original": "def _hcsi_null_range(*args, **kwargs):\n    \"\"\"Builds a list of _HCSINullField with numbered \"Reserved\" names.\n\n    Takes the same arguments as the ``range`` built-in.\n\n    :returns: list[HCSINullField]\n    \"\"\"\n    return [\n        HCSINullField('Reserved{:02d}'.format(x))\n        for x in range(*args, **kwargs)\n    ]",
        "sampled": "```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass _HCSINullField:\n    \"\"\"Represents a null field in the HCSI code.\n\n    Attributes:\n        name (str): The name of the field with \"Reserved\" added before each number.\n        value (int): The numerical value of the field.\n    \"\"\"\n    name: str\n    value: int\n\n\ndef _hcsi_null_range(*args, **kwargs):\n    \"\"\"\n\tReturns a list of _"
    },
    {
        "original": "def _process_one_indirect_jump(self, jump):\n        \"\"\"\n        Resolve a given indirect jump.\n\n        :param IndirectJump jump:  The IndirectJump instance.\n        :return:        A set of resolved indirect jump targets (ints).\n        \"\"\"\n\n        resolved = False\n        resolved_by = None\n        targets = None\n\n        block = self._lift(jump.addr, opt_level=1)\n\n        for resolver in self.indirect_jump_resolvers:\n            resolver.base_state = self._base_state\n\n            if not resolver.filter(self, jump.addr, jump.func_addr, block, jump.jumpkind):\n                continue\n\n            resolved, targets = resolver.resolve(self, jump.addr, jump.func_addr, block, jump.jumpkind)\n            if resolved:\n                resolved_by = resolver\n                break\n\n        if resolved:\n            self._indirect_jump_resolved(jump, jump.addr, resolved_by, targets)\n        else:\n            self._indirect_jump_unresolved(jump)\n\n        return set() if targets is None else set(targets)",
        "sampled": "```python\nclass IndirectJump:\n    def __init__(self, target_REGISTER1, target_REGISTER2):\n        self.target_REGISTER1 = target_REGISTER1\n        self.targetREGISTER2 = target REGISTER2\n\nclass Resolver:\n    def _process_one_indirect_jump(self, jump):\n        \"\"\"\n        Resolve a given indirect jump.\n\n        :param IndirectJump jump:  The IndirectJump instance.\n        :return:         A set of resolved indirect jump targets (ints).\n        \"\"\"\n        \n"
    },
    {
        "original": "def ddb_filepath(self):\n        \"\"\"Returns (at runtime) the absolute path of the input DDB file.\"\"\"\n        # This is not very elegant! A possible approach could to be path self.ddb_node.outdir!\n        if isinstance(self.ddb_node, FileNode): return self.ddb_node.filepath\n        path = self.ddb_node.outdir.has_abiext(\"DDB\")\n        return path if path else \"DDB_FILE_DOES_NOT_EXIST\"",
        "sampled": "```python\nclass Solution:\n    def ddb_filepath(self):\n        \"\"\"\n        Returns (at runtime) the absolute path of the input DDB file.\n        \n        Since this function is not provided in the challenge description, \n        we can assume that it's a placeholder for a method that returns a file path. \n\n        For example, we could use the `os` module to construct a filename and get its absolute path.\n        \n        Let's assume we are working with Linux/Unix-like systems"
    },
    {
        "original": "def create_profile(hostname, username, password, profile_type, name, **kwargs):\n    r\"\"\"\n    A function to connect to a bigip device and create a profile.\n\n    hostname\n        The host/address of the bigip device\n    username\n        The iControl REST username\n    password\n        The iControl REST password\n    profile_type\n        The type of profile to create\n    name\n        The name of the profile to create\n    kwargs\n        [ arg=val ] ...\n\n        Consult F5 BIGIP user guide for specific options for each profile type.\n        Typically, tmsh arg names are used.\n\n    Special Characters ``|``, ``,`` and ``:`` must be escaped using ``\\`` when\n    used within strings.\n\n    \"\"\"\n\n    ret = {'name': name, 'changes': {}, 'result': False, 'comment': ''}\n\n    if __opts__['test']:\n        return _test_output(ret, 'create', params={\n            'hostname': hostname,\n            'username': username,\n            'password': password,\n            'profile_type': profile_type,\n            'name': name\n        })\n\n    #is this profile currently configured?\n    existing = __salt__['bigip.list_profile'](hostname, username, password, profile_type, name)\n\n    # if it exists\n    if existing['code'] == 200:\n\n        ret['result'] = True\n        ret['comment'] = 'A profile by this name currently exists.  No change made.'\n\n    # if it doesn't exist\n    elif existing['code'] == 404:\n\n        response = __salt__['bigip.create_profile'](hostname, username, password, profile_type, name, **kwargs)\n\n        if response['code'] == 200:\n            ret['result'] = True\n            ret['changes']['old'] = {}\n            ret['changes']['new'] = response['content']\n            ret['comment'] = 'Profile was successfully created.'\n        else:\n            ret = _load_result(response, ret)\n\n    # else something else was returned\n    else:\n        ret = _load_result(existing, ret)\n\n    return ret",
        "sampled": "```python\nimport requests\nimport json\n\ndef create_profile(hostname, username, password, profile_type, name, **kwargs):\n    \"\"\"\n    A function to connect to a bigip device and create a profile.\n\n    hostname (str): The host/address of the bigip device\n    username (str): The iControl REST username\n    password (str): The iControl REST password\n    profile_type (str): The type of profile to create\n    name (str): The name of"
    },
    {
        "original": "def csrf_protect_all_post_and_cross_origin_requests():\n    \"\"\"returns None upon success\"\"\"\n    success = None\n\n    if is_cross_origin(request):\n        logger.warning(\"Received cross origin request. Aborting\")\n        abort(403)\n    if request.method in [\"POST\", \"PUT\"]:\n        token = session.get(\"csrf_token\")\n        if token == request.form.get(\"csrf_token\"):\n            return success\n\n        elif token == request.environ.get(\"HTTP_X_CSRFTOKEN\"):\n            return success\n\n        else:\n            logger.warning(\"Received invalid csrf token. Aborting\")\n            abort(403)",
        "sampled": "```python\ndef csrf_protect_all_post_and_cross_origin_requests():\n    is_safe_request = True  # CSLINE:1\n\n    user_input = input(\"Enter 'Y' if the request is safe, or any other key for non-safe: \")\n    \n    if user_input == 'Y':\n        is_safe_request = True\n        print(\"CSF pattern in URL matches (only GET allowed): https://example.com/?.csf=abcde\")\n        print(\"CSF token"
    },
    {
        "original": "def unpin_chat_message(\n        self,\n        chat_id: Union[int, str]\n    ) -> bool:\n        \"\"\"Use this method to unpin a message in a group, channel or your own chat.\n        You must be an administrator in the chat for this to work and must have the \"can_pin_messages\" admin\n        right in the supergroup or \"can_edit_messages\" admin right in the channel.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n\n        Returns:\n            True on success.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        self.send(\n            functions.messages.UpdatePinnedMessage(\n                peer=self.resolve_peer(chat_id),\n                id=0\n            )\n        )\n\n        return True",
        "sampled": "```python\nfrom pyrogram import Client, filters\n\nclass MyBot(Client):\n    def __init__(self, name: str):\n        super().__init__(name=name, plugins={'root': 'my_bot'})\n        self.admin_rights = {\n            'can_pin_messages': True,\n            'can_edit_messages': True\n        }\n\n    async def unpin_chat_message(self, chat_id: Union[int, str]) -> bool:\n        await self.send_message(chat_id=chat_id,\n                                text"
    },
    {
        "original": "def DumpArtifactsToYaml(self, sort_by_os=True):\n    \"\"\"Dump a list of artifacts into a yaml string.\"\"\"\n    artifact_list = self.GetArtifacts()\n    if sort_by_os:\n      # Sort so its easier to split these if necessary.\n      yaml_list = []\n      done_set = set()\n      for os_name in rdf_artifacts.Artifact.SUPPORTED_OS_LIST:\n        done_set = set(a for a in artifact_list if a.supported_os == [os_name])\n        # Separate into knowledge_base and non-kb for easier sorting.\n        done_set = sorted(done_set, key=lambda x: x.name)\n        yaml_list.extend(x.ToYaml() for x in done_set if x.provides)\n        yaml_list.extend(x.ToYaml() for x in done_set if not x.provides)\n        artifact_list = artifact_list.difference(done_set)\n      yaml_list.extend(x.ToYaml() for x in artifact_list)  # The rest.\n    else:\n      yaml_list = [x.ToYaml() for x in artifact_list]\n\n    return \"---\\n\\n\".join(yaml_list)",
        "sampled": "```python\nimport yaml\n\nclass Artifact:\n    def __init__(self, name, operating_system):\n        self.name = name\n        self.operating_system = operating_system\n\nclass Solution:\n    def DumpArtifactsToYaml(self, artifacts, sort_by_os=True):\n        \"\"\"\n        Dump a list of artifacts into a yaml string.\n        \n        Args:\n            artifacts (list[Artifact]): A list of artifact objects.\n            sort_by_os (bool): If True, sort the artifacts by"
    },
    {
        "original": "def ensure_sink(self):\n        \"\"\"Ensure the log sink and its pub sub topic exist.\"\"\"\n        topic_info = self.pubsub.ensure_topic()\n        scope, sink_path, sink_info = self.get_sink(topic_info)\n        client = self.session.client('logging', 'v2', '%s.sinks' % scope)\n        try:\n            sink = client.execute_command('get', {'sinkName': sink_path})\n        except HttpError as e:\n            if e.resp.status != 404:\n                raise\n            sink = client.execute_command('create', sink_info)\n        else:\n            delta = delta_resource(sink, sink_info['body'])\n            if delta:\n                sink_info['updateMask'] = ','.join(delta)\n                sink_info['sinkName'] = sink_path\n                sink_info.pop('parent')\n                sink = client.execute_command('update', sink_info)\n            else:\n                return sink_path\n\n        self.pubsub.ensure_iam(publisher=sink['writerIdentity'])\n        return sink_path",
        "sampled": "```python\nclass Logger:\n    def ensure_sink(self):\n        \"\"\"\n        Ensure the log sink and its pub sub topic exist.\n        \n        This method is to be implemented by the users. The default implementation simply prints a message\n        indicating that the sink exists.\n        \n        :return: None\n        \"\"\"\n        \n        # Here we'll assume we have access to a publish-subscribe system (e.g., Kafka)\n        \n        from psutil import Popen, Process\n        \n        # Using PID of"
    },
    {
        "original": "def __load_project(path):\n    \"\"\"\n    Load a docker-compose project from path\n\n    :param path:\n    :return:\n    \"\"\"\n    file_path = __get_docker_file_path(path)\n    if file_path is None:\n        msg = 'Could not find docker-compose file at {0}'.format(path)\n        return __standardize_result(False,\n                                    msg,\n                                    None, None)\n    return __load_project_from_file_path(file_path)",
        "sampled": "```python\nimport docker\nfrom docker.types import Mount\n\ndef __load_project(path):\n    \"\"\"\n    Load a docker-compose project from path\n\n    :param path:\n    :return:\n    \"\"\"\n    \n    # Initialize the Docker client\n    client = docker.from_env()\n    \n    # Try to load the project from the given path\n    try:\n        volumes = {path: {'bind': '/project', 'type': 'volume', 'readonly': True}}\n        client.containers.run('"
    },
    {
        "original": "def next(self, start):\n        \"\"\"\n        Return a (marker_code, segment_offset) 2-tuple identifying and\n        locating the first marker in *stream* occuring after offset *start*.\n        The returned *segment_offset* points to the position immediately\n        following the 2-byte marker code, the start of the marker segment,\n        for those markers that have a segment.\n        \"\"\"\n        position = start\n        while True:\n            # skip over any non-\\xFF bytes\n            position = self._offset_of_next_ff_byte(start=position)\n            # skip over any \\xFF padding bytes\n            position, byte_ = self._next_non_ff_byte(start=position+1)\n            # 'FF 00' sequence is not a marker, start over if found\n            if byte_ == b'\\x00':\n                continue\n            # this is a marker, gather return values and break out of scan\n            marker_code, segment_offset = byte_, position+1\n            break\n        return marker_code, segment_offset",
        "sampled": "```python\nclass Markers:\n    def __init__(self):\n        self.markers = {}\n\n    def set_marker(self, code, value):\n        self.markers[code] = value\n\n    def get_marker(self, code):\n        return self.markers.get(code)\n\ndef decode(stream):\n    \"\"\"\n    Given a binary stream decodes and returns a python dictionary\n    representing the markers in the stream as per its document.\n    \n    The format is: [0xFF 0xFE] marker"
    },
    {
        "original": "def resolve_addresses(user, useralias, to, cc, bcc):\n    \"\"\" Handle the targets addresses, adding aliases when defined \"\"\"\n    addresses = {\"recipients\": []}\n    if to is not None:\n        make_addr_alias_target(to, addresses, \"To\")\n    elif cc is not None and bcc is not None:\n        make_addr_alias_target([user, useralias], addresses, \"To\")\n    else:\n        addresses[\"recipients\"].append(user)\n    if cc is not None:\n        make_addr_alias_target(cc, addresses, \"Cc\")\n    if bcc is not None:\n        make_addr_alias_target(bcc, addresses, \"Bcc\")\n    return addresses",
        "sampled": "```python\nfrom typing import Dict, List\n\ndef resolve_addresses(user: str, useralias: Dict[str, str], to: List[str], cc: List[str], bcc: List[str]) -> Dict[str, List[str]]:\n    \"\"\"\n    Handle the targets addresses, adding aliases when defined\n    \"\"\"\n\n    result = {\n        'to': [],\n        'cc': [],\n        'bcc': []\n    }\n\n    if user in useralias:\n        to.append(useralias[user])\n    \n   "
    },
    {
        "original": "def exp_cov(prices, span=180, frequency=252):\n    \"\"\"\n    Estimate the exponentially-weighted covariance matrix, which gives\n    greater weight to more recent data.\n\n    :param prices: adjusted closing prices of the asset, each row is a date\n                   and each column is a ticker/id.\n    :type prices: pd.DataFrame\n    :param span: the span of the exponential weighting function, defaults to 180\n    :type span: int, optional\n    :param frequency: number of time periods in a year, defaults to 252 (the number\n                      of trading days in a year)\n    :type frequency: int, optional\n    :return: annualised estimate of exponential covariance matrix\n    :rtype: pd.DataFrame\n    \"\"\"\n    if not isinstance(prices, pd.DataFrame):\n        warnings.warn(\"prices are not in a dataframe\", RuntimeWarning)\n        prices = pd.DataFrame(prices)\n    assets = prices.columns\n    daily_returns = daily_price_returns(prices)\n    N = len(assets)\n\n    # Loop over matrix, filling entries with the pairwise exp cov\n    S = np.zeros((N, N))\n    for i in range(N):\n        for j in range(i, N):\n            S[i, j] = S[j, i] = _pair_exp_cov(\n                daily_returns.iloc[:, i], daily_returns.iloc[:, j], span\n            )\n    return pd.DataFrame(S * frequency, columns=assets, index=assets)",
        "sampled": "```python\nimport pandas as pd\nimport numpy as np\n\ndef exp_cov(prices, span=180, frequency=252):\n    \"\"\"\n    Estimate the exponentially-weighted covariance matrix, which gives\n    greater weight to more recent data.\n\n    :param prices: adjusted closing prices of the asset, each row is a date\n                   and each column is a ticker/id.\n    :type prices: pd.DataFrame\n    :param span HaloEffectsysis290whichgivfallsADGroupShouldAllocate"
    },
    {
        "original": "def nonDefaults(self):\n        \"\"\"\n        Get a dictionary of all attributes that differ from the default.\n        \"\"\"\n        nonDefaults = {}\n        for k, d in self.__class__.defaults.items():\n            v = getattr(self, k)\n            if v != d and (v == v or d == d):  # tests for NaN too\n                nonDefaults[k] = v\n        return nonDefaults",
        "sampled": "```python\nclass Config:\n    def __init__(self, defaults):\n        \"\"\"\n        Initialize the Config object.\n\n        Args:\n            defaults (dict): A dictionary of default attributes and their values.\n        \"\"\"\n        self.defaults = defaults\n\n    def nonDefaults(self):\n        \"\"\"\n        Get a dictionary of all attributes that differ from the default.\n\n        Returns:\n            dict: A dictionary of non-default attributes and their values.\n        \"\"\"\n        self.kwargs = {}  # assuming kwargs is a property attribute"
    },
    {
        "original": "def _get_job_results(query=None):\n    \"\"\"\n    Executes a query that requires a job for completion. This function will wait for the job to complete\n    and return the results.\n    \"\"\"\n    if not query:\n        raise CommandExecutionError(\"Query parameters cannot be empty.\")\n\n    response = __proxy__['panos.call'](query)\n\n    # If the response contains a job, we will wait for the results\n    if 'result' in response and 'job' in response['result']:\n        jid = response['result']['job']\n\n        while get_job(jid)['result']['job']['status'] != 'FIN':\n            time.sleep(5)\n\n        return get_job(jid)\n    else:\n        return response",
        "sampled": "```python\nimport concurrent.futures\n\ndef _get_job_results(query=None):\n    if query is None:\n        query = {}\n    \n    # Assuming we have a function handle_job that handles the job\n    job = handle_job(query)\n    \n    # With ThreadPoolExecutor, it will wait for the job to complete and return the results\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        future = executor.submit(job.run, job.id)\n        \n        # We use add_done_callback instead"
    },
    {
        "original": "def internal_energy(self, t, structure=None):\n        \"\"\"\n        Phonon contribution to the internal energy at temperature T obtained from the integration of the DOS.\n        Only positive frequencies will be used.\n        Result in J/mol-c. A mol-c is the abbreviation of a mole-cell, that is, the number\n        of Avogadro times the atoms in a unit cell. To compare with experimental data the result\n        should be divided by the number of unit formulas in the cell. If the structure is provided\n        the division is performed internally and the result is in J/mol\n\n        Args:\n            t: a temperature in K\n            structure: the structure of the system. If not None it will be used to determine the numer of\n                formula units\n        Returns:\n            Phonon contribution to the internal energy\n        \"\"\"\n\n        if t==0:\n            return self.zero_point_energy(structure=structure)\n\n        freqs = self._positive_frequencies\n        dens = self._positive_densities\n\n        coth = lambda x: 1.0 / np.tanh(x)\n\n        wd2kt = freqs / (2 * BOLTZ_THZ_PER_K * t)\n        e = np.trapz(freqs * coth(wd2kt) * dens, x=freqs) / 2\n\n        e *= THZ_TO_J * const.Avogadro\n\n        if structure:\n            formula_units = structure.composition.num_atoms / structure.composition.reduced_composition.num_atoms\n            e /= formula_units\n\n        return e",
        "sampled": "```python\nimport numpy as np\n\nclass PhononSystem:\n    def __init__(self, dos):\n        \"\"\"\n        Initializes the PhononSystem class.\n\n        Args:\n            dos: density of states (DOS) of the system.\n        \"\"\"\n        self.dos = dos\n\n    def internal_energy(self, t, structure=None):\n        \"\"\"\n        Phonon contribution to the internal energy at temperature T obtained from the integration of the DOS.\n        \n        Args:\n            t: a temperature in"
    },
    {
        "original": "def del_client(self, **kwargs):\n        \"\"\"\n        Registers a new client to the specified network.\n\n        Usage:\n\n            ======= ===================================\n            Method  URI\n            ======= ===================================\n            DELETE  /vtep/networks/{vni}/clients/{mac}\n            ======= ===================================\n\n        Request parameters:\n\n            =========== ===============================================\n            Attribute   Description\n            =========== ===============================================\n            vni         Virtual Network Identifier. (e.g. 10)\n            mac         Client MAC address to register.\n            =========== ===============================================\n\n        Example::\n\n            $ curl -X DELETE http://localhost:8080/vtep/networks/10/clients/aa:bb:cc:dd:ee:ff |\n             python -m json.tool\n\n        ::\n\n            {\n                \"10\": {\n                    \"EvpnClient\": {\n                        \"ip\": \"10.0.0.1\",\n                        \"mac\": \"aa:bb:cc:dd:ee:ff\",\n                        \"next_hop\": \"172.17.0.1\",\n                        \"port\": 1\n                    }\n                }\n            }\n        \"\"\"\n        try:\n            body = self.vtep_app.del_client(**kwargs)\n        except (BGPSpeakerNotFound, DatapathNotFound,\n                VniNotFound, ClientNotFound, ClientNotLocal) as e:\n            return Response(body=str(e), status=500)\n\n        return Response(content_type='application/json',\n                        body=json.dumps(body))",
        "sampled": "```python\nclass VTEPNetwork:\n    def __init__(self):\n        self.networks = {}\n\n    def del_client(self, vni, mac):\n        \"\"\"\n        Deletes a client from the network.\n\n        Parameters:\n            vni (int): Virtual Network Identifier.\n            mac (str): Client MAC address to delete.\n\n        Returns:\n            dict: A dictionary representation of the updated network state.\n                  Keys are virtual network identifiers. Values are dictionaries\n                  containing EVPN client information.\n"
    },
    {
        "original": "def _parse_general_counters(self, init_config):\n        \"\"\"\n        Return a dictionary for each job counter\n        {\n          counter_group_name: [\n              counter_name\n            ]\n          }\n        }\n        \"\"\"\n        job_counter = {}\n\n        if init_config.get('general_counters'):\n\n            # Parse the custom metrics\n            for counter_group in init_config['general_counters']:\n                counter_group_name = counter_group.get('counter_group_name')\n                counters = counter_group.get('counters')\n\n                if not counter_group_name:\n                    raise Exception('\"general_counters\" must contain a valid \"counter_group_name\"')\n\n                if not counters:\n                    raise Exception('\"general_counters\" must contain a list of \"counters\"')\n\n                # Add the counter_group to the job_counters if it doesn't already exist\n                if counter_group_name not in job_counter:\n                    job_counter[counter_group_name] = []\n\n                for counter in counters:\n                    counter_name = counter.get('counter_name')\n\n                    if not counter_name:\n                        raise Exception('At least one \"counter_name\" should be specified in the list of \"counters\"')\n\n                    job_counter[counter_group_name].append(counter_name)\n\n        return job_counter",
        "sampled": "```python\nclass JobCounterParser:\n    def _parse_general_counters(self, init_config):\n        \"\"\"\n        Return a dictionary for each job counter\n        {\n          counter_group_name: [\n              counter_name\n            ]\n          }\n        \"\"\"\n        \n        # Initialize an empty dictionary to store the counters\n        counters = {}\n        \n        # Iterate over each group in the init_config\n        for group, config in init_config.items():\n            # Initialize an empty list to store the counters in the current group"
    },
    {
        "original": "def RawData(self):\n    \"\"\"Yields the valus in each section.\"\"\"\n    result = collections.OrderedDict()\n\n    i = 0\n    while True:\n      try:\n        name, value, value_type = winreg.EnumValue(self._AccessRootKey(), i)\n        # Only support strings here.\n        if value_type == winreg.REG_SZ:\n          precondition.AssertType(value, Text)\n          result[name] = value\n      except OSError:\n        break\n\n      i += 1\n\n    return result",
        "sampled": "```python\nclass Section:\n    def __init__(self, name,rawler_callback):\n        self.name = name\n        self.rrawer_callback = rrawer_callback\n\n    def RawData(self):\n        \"\"\"Yields values in each section.\"\"\"\n        yield from self.rrawer_callback()\n\n\nclass FileWriter(Section):\n    def __init__(self, filename=\"output.txt\"):\n        super().__init__(\"File Writer\", lambda: self.read_values(filename))\n\n    def read_values(self, filename):\n        try:\n"
    },
    {
        "original": "def optimize_auto(self,max_iters=10000,verbose=True):\n        \"\"\"\n        Optimize the model parameters through a pre-defined protocol.\n\n        :param int max_iters: the maximum number of iterations.\n        :param boolean verbose: print the progress of optimization or not.\n        \"\"\"\n        self.Z.fix(warning=False)\n        self.kern.fix(warning=False)\n        self.kern_row.fix(warning=False)\n        self.Zr.fix(warning=False)\n        self.Xr.fix(warning=False)\n        self.optimize(max_iters=int(0.1*max_iters),messages=verbose)\n        self.unfix()\n        self.optimize(max_iters=max_iters,messages=verbose)",
        "sampled": "```python\nimport numpy as np\n\nclass AutoOptimizer:\n    def __init__(self):\n        self.model_params = None  # Initialize model parameters\n\n    def optimize_auto(self, max_iters=10000, verbose=True):\n        \"\"\"\n        Optimize the model parameters through a pre-defined protocol.\n\n        :param int max_iters: the maximum number of iterations.\n        :param boolean verbose: print the progress of optimization or not.\n        \"\"\"\n        \n                    \n        # Initialize gradient and step size\n           "
    },
    {
        "original": "def to_input(self, mol=None,  charge=None,\n                 spin_multiplicity=None, title=None, functional=None,\n                 basis_set=None, route_parameters=None, input_parameters=None,\n                 link0_parameters=None, dieze_tag=None, cart_coords=False):\n        \"\"\"\n        Create a new input object using by default the last geometry read in\n        the output file and with the same calculation parameters. Arguments\n        are the same as GaussianInput class.\n\n        Returns\n            gaunip (GaussianInput) : the gaussian input object\n        \"\"\"\n        if not mol:\n            mol = self.final_structure\n\n        if charge is None:\n            charge = self.charge\n\n        if spin_multiplicity is None:\n            spin_multiplicity = self.spin_multiplicity\n\n        if not title:\n            title = self.title\n\n        if not functional:\n            functional = self.functional\n\n        if not basis_set:\n            basis_set = self.basis_set\n\n        if not route_parameters:\n            route_parameters = self.route_parameters\n\n        if not link0_parameters:\n            link0_parameters = self.link0\n\n        if not dieze_tag:\n            dieze_tag = self.dieze_tag\n\n        return GaussianInput(mol=mol,\n                             charge=charge,\n                             spin_multiplicity=spin_multiplicity,\n                             title=title,\n                             functional=functional,\n                             basis_set=basis_set,\n                             route_parameters=route_parameters,\n                             input_parameters=input_parameters,\n                             link0_parameters=link0_parameters,\n                             dieze_tag=dieze_tag)",
        "sampled": "```python\nclass GaussianInput:\n    \"\"\"Represent a Gaussian input object.\"\"\"\n    \n    def __init__(self, charge=None, spin_multiplicity=None, title=None,\n                 functional=None, basis_set=None, route_parameters=None,\n                 input_parameters=None, link0_parameters=None,\n                 dieze_tag=None):\n        \"\"\"\n        Initialize the GaussianInput object with default values.\n\n        Args:\n            charge (int): charge of the molecule\n            spin_multiplicity (int): spin multiplicity of the molecule"
    },
    {
        "original": "def link(self):\n        \"\"\"\n        Registers the Link\n        \"\"\"\n        if self.source in self.registry:\n            links = self.registry[self.source]\n            params = {\n                k: v for k, v in self.get_param_values() if k != 'name'}\n            for link in links:\n                link_params = {\n                    k: v for k, v in link.get_param_values() if k != 'name'}\n                if (type(link) is type(self) and link.source is self.source\n                    and link.target is self.target and params == link_params):\n                    return\n            self.registry[self.source].append(self)\n        else:\n            self.registry[self.source] = [self]",
        "sampled": "```python\nfrom typing import List\n\nclass Solution:\n    def longestSubarray(self, nums: List[int]) -> int:\n        max_len = len(nums) - 1\n        left = 0\n        front_count = 0\n        \n        for right in range(len(nums)):\n            if nums[right] == 1:\n                front_count += 1\n            \n            while front_count > 1:\n                if nums[left] == 1:\n                    front_count -= 1\n                left"
    },
    {
        "original": "def play_match(black_model, white_model, games, sgf_dir):\n    \"\"\"Plays matches between two neural nets.\n\n    Args:\n        black_model: Path to the model for black player\n        white_model: Path to the model for white player\n    \"\"\"\n    with utils.logged_timer(\"Loading weights\"):\n        black_net = dual_net.DualNetwork(black_model)\n        white_net = dual_net.DualNetwork(white_model)\n\n    readouts = FLAGS.num_readouts\n\n    black = MCTSPlayer(black_net, two_player_mode=True)\n    white = MCTSPlayer(white_net, two_player_mode=True)\n\n    black_name = os.path.basename(black_net.save_file)\n    white_name = os.path.basename(white_net.save_file)\n\n    for i in range(games):\n        num_move = 0  # The move number of the current game\n\n        for player in [black, white]:\n            player.initialize_game()\n            first_node = player.root.select_leaf()\n            prob, val = player.network.run(first_node.position)\n            first_node.incorporate_results(prob, val, first_node)\n\n        while True:\n            start = time.time()\n            active = white if num_move % 2 else black\n            inactive = black if num_move % 2 else white\n\n            current_readouts = active.root.N\n            while active.root.N < current_readouts + readouts:\n                active.tree_search()\n\n            # print some stats on the search\n            if FLAGS.verbose >= 3:\n                print(active.root.position)\n\n            # First, check the roots for hopeless games.\n            if active.should_resign():  # Force resign\n                active.set_result(-1 *\n                                  active.root.position.to_play, was_resign=True)\n                inactive.set_result(\n                    active.root.position.to_play, was_resign=True)\n\n            if active.is_done():\n                fname = \"{:d}-{:s}-vs-{:s}-{:d}.sgf\".format(int(time.time()),\n                                                            white_name, black_name, i)\n                active.set_result(active.root.position.result(), was_resign=False)\n                with gfile.GFile(os.path.join(sgf_dir, fname), 'w') as _file:\n                    sgfstr = sgf_wrapper.make_sgf(active.position.recent,\n                                                  active.result_string, black_name=black_name,\n                                                  white_name=white_name)\n                    _file.write(sgfstr)\n                print(\"Finished game\", i, active.result_string)\n                break\n\n            move = active.pick_move()\n            active.play_move(move)\n            inactive.play_move(move)\n\n            dur = time.time() - start\n            num_move += 1\n\n            if (FLAGS.verbose > 1) or (FLAGS.verbose == 1 and num_move % 10 == 9):\n                timeper = (dur / readouts) * 100.0\n                print(active.root.position)\n                print(\"%d: %d readouts, %.3f s/100. (%.2f sec)\" % (num_move,\n                                                                   readouts,\n                                                                   timeper,\n                                                                   dur))",
        "sampled": "```python\nimport os\nimport subprocess\n\ndef play_match(black_model, white_model, games, sgf_dir):\n    \"\"\"\n    Plays matches between two neural nets.\n\n    Args:\n        black_model (str): Path to the model for black player\n        white_model (str): Path to the model for white player\n        games (int): Number of games to play\n        sgf_dir (str): Directory where SGF files will be saved\n\n    Returns:\n        None\n   "
    },
    {
        "original": "def GenQuotedState(self):\n    \"\"\"Generate string matching state rules.\"\"\"\n    for i, q in enumerate(self.quot):\n      label = \"%s_STRING\" % i\n      escaped = re.escape(q)\n      self._AddToken(label, escaped, \"PopState\", None)\n      self._AddToken(label, q, \"PopState\", None)\n      if self.ml_quote:\n        self._AddToken(label, r\"\\n\", None, None)\n      else:\n        self._AddToken(label, r\"\\n\", \"BadLine\", None)\n      self._AddToken(label, \".\", \"AddToField\", None)",
        "sampled": "```python\nclass Solution:\n    def GenQuotedState(self):\n        \"\"\"\n        Generate string matching state rules.\n        \n        The goal of this function is to return a list of integers representing \n        the next states in a deterministic finite automaton (DFA) for each digit \n        or special character that may appear in an input string.\n        \n        In the DFA, there are 9 states. Let 'state' denote the current state. If \n        we read a digit 'd ("
    },
    {
        "original": "def get_next_index(self, matrix, manipulation, indices_left):\n        \"\"\"\n        Returns an index that should have the most negative effect on the\n        matrix sum\n        \"\"\"\n        f = manipulation[0]\n        indices = list(indices_left.intersection(manipulation[2]))\n        sums = np.sum(matrix[indices], axis=1)\n        if f < 1:\n            next_index = indices[sums.argmax(axis=0)]\n        else:\n            next_index = indices[sums.argmin(axis=0)]\n\n        return next_index",
        "sampled": "```python\nclass MatrixManipulator:\n    def get_next_index(self, matrix, manipulation, indices_left):\n        # Find the row with the most negative effect on the sum initially or when altered\n        candidate_row = 0\n        max_effect = float('-inf')\n        \n        for i in range(len(matrix)):\n            # Calculate the difference between two cases: altering and not altering this row.\n            if manipulation == 'add':\n                alteration_difference = len(matrix) - indices_left + len(i"
    },
    {
        "original": "def get_display_name(entity):\n    \"\"\"\n    Gets the display name for the given entity, if it's an :tl:`User`,\n    :tl:`Chat` or :tl:`Channel`. Returns an empty string otherwise.\n    \"\"\"\n    if isinstance(entity, types.User):\n        if entity.last_name and entity.first_name:\n            return '{} {}'.format(entity.first_name, entity.last_name)\n        elif entity.first_name:\n            return entity.first_name\n        elif entity.last_name:\n            return entity.last_name\n        else:\n            return ''\n\n    elif isinstance(entity, (types.Chat, types.Channel)):\n        return entity.title\n\n    return ''",
        "sampled": "```python\ndef get_display_name(entity):\n    \"\"\"\n    Gets the display name for the given entity, if it's an :tl:`User`,\n    :tl:`Chat` or :tl:`Channel`. Returns an empty string otherwise.\n    \"\"\"\n    \n    # Define a dictionary to map entity types to their display names\n    entity_display_names = {\n        'User': '',  # For simplicity, assume user displays as \"user\"\n        'Chat': '',   # For simplicity, assume chat"
    },
    {
        "original": "def _ValidateFleetspeakServiceConfig(self, config_path):\n    \"\"\"Validates a Fleetspeak service config.\n\n    Checks that the given file is a valid TextFormat representation of\n    a Fleetspeak service config proto.\n\n    Args:\n      config_path: Path to the config file.\n\n    Raises:\n      BuildError: If the config is not valid.\n    \"\"\"\n    with open(config_path, \"rb\") as f:\n      pool = descriptor_pool.DescriptorPool()\n      pool.AddDescriptor(fs_config_pb2.Config.DESCRIPTOR)\n      parsed_config = text_format.Parse(\n          f.read(), fs_system_pb2.ClientServiceConfig(), descriptor_pool=pool)\n      if parsed_config.factory != \"Daemon\":\n        raise BuildError(\n            \"Fleetspeak config does not have the expected factory type.\")\n      daemon_cfg = fs_config_pb2.Config()\n      parsed_config.config.Unpack(daemon_cfg)\n      if not daemon_cfg.argv:\n        raise BuildError(\n            \"Fleetspeak daemon service config does not specify command line \"\n            \"args.\")",
        "sampled": "```python\nimport xml.etree.ElementTree as ET\nimport sys\nfrom pathlib import Path\n\nclass BuildError(Exception):\n    \"\"\"Raised when the config is not valid.\"\"\"\n    pass\n\ndef _validate_fleetspeak_service_config(config_path):\n    \"\"\"\n    Validates a Fleetspeak service config.\n\n    Checks that the given file is a valid TextFormat representation of \n    a Fleetspeak service config proto.\n\n    Args:\n      config_path (str): Path to the config file.\n\n   "
    },
    {
        "original": "def _CheckConnectionEncoding(cursor):\n  \"\"\"Enforces a sane UTF-8 encoding for the database connection.\"\"\"\n  cur_character_set = _ReadVariable(\"character_set_connection\", cursor)\n  if cur_character_set != CHARACTER_SET:\n    raise EncodingEnforcementError(\n        \"Require MySQL character_set_connection of {}, got {}.\".format(\n            CHARACTER_SET, cur_character_set))",
        "sampled": "```python\nimport sqlite3\n\ndef _CheckConnectionEncoding(cursor):\n    \"\"\"\n    Enforces a sane UTF-8 encoding for the database connection.\n\n    Checks if the cursor's connection is using a valid encoding, \n    and sets it to UTF-8 if it's not.\n    \n    :param cursor: A SQLite database cursor object\n    \"\"\"\n    \n    # Get the current encoding of the connection from the cursor\n    current_encoding = None\n    \n    try:\n        current_encoding = cursor executes"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\n\nclass RDFURN(ABC):\n    def __init__(self, urn):\n        self.urn = urn\n\n    @abstractmethod\n    def __eq__(self, other):\n        pass\n\nclass ConcreteRDFURN(RDFURN):\n    def __init__(self, urn):\n        super().__init__(urn)\n\n    def __eq__(self, other):\n        if isinstance(other, RDFURN) and hasattr(other, 'urn'):\n            return"
    },
    {
        "original": "def make_symbolic(self, name, addr, length=None):\n        \"\"\"\n        Replaces `length` bytes starting at `addr` with a symbolic variable named name. Adds a constraint equaling that\n        symbolic variable to the value previously at `addr`, and returns the variable.\n        \"\"\"\n        l.debug(\"making %s bytes symbolic\", length)\n\n        if isinstance(addr, str):\n            addr, length = self.state.arch.registers[addr]\n        else:\n            if length is None:\n                raise Exception(\"Unspecified length!\")\n\n        r = self.load(addr, length)\n\n        v = self.get_unconstrained_bytes(name, r.size())\n        self.store(addr, v)\n        self.state.add_constraints(r == v)\n        l.debug(\"... eq constraints: %s\", r == v)\n        return v",
        "sampled": "```python\nclass SymbolicVariable:\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\nclass AddressableMemory:\n    def __init__(self):\n        self.memory = {}\n\n    def write(self, addr, value):\n        if addr not in self.memory:\n            self.memory[addr] = 0\n        prev_value = self.memory[addr]\n        symbolic_var = SymbolicVariable(addr, prev_value)\n        constraint_symbolic"
    },
    {
        "original": "def _build_ssh_client(self):\n        \"\"\"Prepare for Paramiko SSH connection.\"\"\"\n        # Create instance of SSHClient object\n        remote_conn_pre = paramiko.SSHClient()\n\n        # Load host_keys for better SSH security\n        if self.system_host_keys:\n            remote_conn_pre.load_system_host_keys()\n        if self.alt_host_keys and path.isfile(self.alt_key_file):\n            remote_conn_pre.load_host_keys(self.alt_key_file)\n\n        # Default is to automatically add untrusted hosts (make sure appropriate for your env)\n        remote_conn_pre.set_missing_host_key_policy(self.key_policy)\n        return remote_conn_pre",
        "sampled": "```python\nimport paramiko\n\nclass SSHClient:\n    def _build_ssh_client(self):\n        \"\"\"Prepare for Paramiko SSH connection.\"\"\"\n        \n        # Define the hostname or IP address of the server\n        hostname = \"localhost\"\n        \n        # Define the port number (default is 22)\n        port = 22\n        \n        # Define the username and password for authentication\n        username = \"your_username\"\n        password = \"your_password\"\n        \n        try:\n            # Create an SSH client"
    },
    {
        "original": "def find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        # it might be good to add \"type vcan\", but that might (?) exclude physical can devices\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True)\n\n    except Exception as e: # subprocess.CalledProcessError was too specific\n        log.error(\"failed to fetch opened can devices: %s\", e)\n        return []\n\n    else:\n        #log.debug(\"find_available_interfaces(): output=\\n%s\", output)\n        # output contains some lines like \"1: vcan42: <NOARP,UP,LOWER_UP> ...\"\n        # extract the \"vcan42\" of each line\n        interface_names = [line.split(\": \", 3)[1] for line in output.splitlines()]\n        log.debug(\"find_available_interfaces(): detected: %s\", interface_names)\n        return filter(_PATTERN_CAN_INTERFACE.match, interface_names)",
        "sampled": "```python\nimport subprocess\n\ndef find_available_interfaces():\n    \"\"\"\n    Returns the names of all open can/vcan interfaces using the ``ip link list`` command.\n    If the lookup fails, an error is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of str\n    \"\"\"\n    try:\n        # Use subprocess to run the ip link list command\n        output = subprocess.check_output(['ip', 'link', 'list']).decode('utf-8')\n"
    },
    {
        "original": "def fit(self, X, y=None):\n        \"\"\"\n        The fit method is the primary drawing input for the frequency\n        distribution visualization. It requires vectorized lists of\n        documents and a list of features, which are the actual words\n        from the original corpus (needed to label the x-axis ticks).\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features representing the corpus\n            of frequency vectorized documents.\n\n        y : ndarray or DataFrame of shape n\n            Labels for the documents for conditional frequency distribution.\n\n        .. note:: Text documents must be vectorized before ``fit()``.\n        \"\"\"\n\n        # Compute the conditional word frequency\n        if y is not None:\n            # Fit the frequencies\n            self.conditional_freqdist_ = {}\n\n            # Conditional frequency distribution\n            self.classes_ = [str(label) for label in set(y)]\n            for label in self.classes_:\n                self.conditional_freqdist_[label] = self.count(X[y == label])\n        else:\n            # No conditional frequencies\n            self.conditional_freqdist_ = None\n\n        # Frequency distribution of entire corpus.\n        self.freqdist_ = self.count(X)\n        self.sorted_ = self.freqdist_.argsort()[::-1] # Descending order\n\n        # Compute the number of words, vocab, and hapaxes\n        self.vocab_ = self.freqdist_.shape[0]\n        self.words_ = self.freqdist_.sum()\n        self.hapaxes_ = sum(1 for c in self.freqdist_ if c == 1)\n\n        # Draw and ensure that we return self\n        self.draw()\n        return self",
        "sampled": "```python\nimport numpy as np\n\nclass FrequencyDistributionVisualizer:\n    def __init__(self):\n        self.feature_names = None\n\n    def fit(self, X, y=None):\n        \"\"\"\n        The fit method is the primary drawing input for the frequency\n        distribution visualization. It requires vectorized lists of\n        documents and a list of features, which are the actual words\n        from the original corpus (needed to label the x-axis ticks).\n\n        Parameters\n        ----------\n        X :"
    },
    {
        "original": "def _check_exclude(self, val):\n        \"\"\"\n        Validate the excluded metrics. Returns the set of excluded params.\n        \"\"\"\n        if val is None:\n            exclude = frozenset()\n        elif isinstance(val, str):\n            exclude = frozenset([val.lower()])\n        else:\n            exclude = frozenset(map(lambda s: s.lower(), val))\n\n        if len(exclude - frozenset(METRICS)) > 0:\n            raise YellowbrickValueError(\n                \"'{}' is not a valid metric to exclude\".format(repr(val))\n            )\n\n        return exclude",
        "sampled": "```python\ndef _check_exclude(self, val):\n    \"\"\"\n    Validate the excluded metrics. Returns the set of excluded params.\n    \n    Parameters:\n    val (list): list of metrics\n    \n    Returns:\n    set: a set of excluded parameters\n    \"\"\"\n    \n    # Assuming that self.excluded_metrics is a set of all possible excluded metrics\n    # and self.val is a dictionary mapping metric names to their values\n ?>\"><                             \\\n\n    \nupdate_excluded_params = True  # update excluded"
    },
    {
        "original": "def remove_collisions(self, min_dist=0.5):\n        \"\"\"\n        Remove predicted sites that are too close to existing atoms in the\n        structure.\n\n        Args:\n            min_dist (float): The minimum distance (in Angstrom) that\n                a predicted site needs to be from existing atoms. A min_dist\n                with value <= 0 returns all sites without distance checking.\n        \"\"\"\n        s_f_coords = self.structure.frac_coords\n        f_coords = self.extrema_coords\n        if len(f_coords) == 0:\n            if self.extrema_type is None:\n                logger.warning(\n                    \"Please run ChargeDensityAnalyzer.get_local_extrema first!\")\n                return\n            new_f_coords = []\n            self._update_extrema(new_f_coords, self.extrema_type)\n            return new_f_coords\n\n        dist_matrix = self.structure.lattice.get_all_distances(f_coords,\n                                                               s_f_coords)\n        all_dist = np.min(dist_matrix, axis=1)\n        new_f_coords = []\n\n        for i, f in enumerate(f_coords):\n            if all_dist[i] > min_dist:\n                new_f_coords.append(f)\n        self._update_extrema(new_f_coords, self.extrema_type)\n\n        return new_f_coords",
        "sampled": "```python\nfrom itertools import combinations\n\nclass AtomRomer:\n    def __init__(self, sites, structure):\n        self.sites = sites\n        self.structure = structure\n\n    def distance(self, point1, point2):\n        \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n        return ((point1['x'] - point2['x']) ** 2 + \n                (point1['y'] - point2['y']) ** 2 + \n                (point1"
    },
    {
        "original": "def __folder_size(self, path):\n        \"\"\"Return the size of the directory given by path\n\n        path: <string>\"\"\"\n\n        ret = 0\n        for f in scandir(path):\n            if f.is_dir() and (f.name != '.' or f.name != '..'):\n                ret += self.__folder_size(os.path.join(path, f.name))\n            else:\n                try:\n                    ret += f.stat().st_size\n                except OSError:\n                    pass\n\n        return ret",
        "sampled": "```\nimport os\n\nclass Solution:\n    def __folder_size(self, path):\n        \"\"\"\n        Return the size of the directory given by path\n        Args:\n            path (str): Path to the directory\n\n        Returns:\n            int: Size of the directory in bytes\n        \"\"\"\n        \n        # Check if the provided path exists\n        if not os.path.exists(path):\n            return 0\n        \n        # Calculate the total size of all files in the directory and its subdirectories\n        try"
    },
    {
        "original": "def get_actions(self, commands):\n        \"\"\"Get parameterized actions from command list based on command type and verb.\"\"\"\n        actions = []\n        for type, turn_based, verb in commands:\n            if len(self.action_filter) != 0 and verb not in self.action_filter:\n                continue\n            if type == 'DiscreteMovement':\n                if verb in {\"move\", \"turn\", \"look\",\n                            \"strafe\", \"jumpmove\", \"jumpstrafe\"}:\n                    actions.append(verb + \" 1\")\n                    actions.append(verb + \" -1\")\n                elif verb in {\"jumpeast\", \"jumpnorth\", \"jumpsouth\",\n                              \"jumpwest\", \"movenorth\", \"moveeast\",\n                              \"movesouth\", \"movewest\", \"jumpuse\",\n                              \"use\", \"attack\", \"jump\"}:\n                    actions.append(verb + \" 1\")\n                else:\n                    raise CommandHandlerException(\"Invalid discrete command\")\n            elif type == 'ContinuousMovement':\n                #  Translate to discrete.\n                if verb in {\"move\", \"strafe\", \"pitch\", \"turn\"}:\n                    actions.append(verb + \" 1\")\n                    actions.append(verb + \" -1\")\n                elif verb in {\"crouch\", \"jump\", \"attack\", \"use\"}:\n                    actions.append(verb + \" 1\")\n                    actions.append(verb + \" 0\")\n                else:\n                    raise CommandHandlerException(\"Invalid continuous command\")\n            elif type == 'HumanLevel':\n                if verb == 'moveMouse':\n                    actions.append('mouseMove 0 0')\n                elif verb in {'forward', 'back', 'left', 'right'}:\n                    actions.append(verb + ' 1')\n                    actions.append(verb + ' 0')\n                else:\n                    actions.append(verb)\n            elif type == 'MissionQuit':\n                if verb != 'quit':\n                    raise CommandHandlerException(\"Invalid quit command\")\n                actions.append(verb)\n            elif type == 'Chat':\n                if verb != 'chat':\n                    raise CommandHandlerException(\"Invalid chat command\")\n                actions.append(verb)\n            elif type == 'SimpleCraft':\n                if verb != 'craft':\n                    raise CommandHandlerException(\"Invalid craft command\")\n                actions.append(verb)\n            elif type == 'AbsoluteMovement' or 'Inventory':\n                actions.append(verb)\n        return actions",
        "sampled": "```python\nclass Parser:\n    def get_actions(self, commands):\n        actions = {}\n        for command in commands:\n            verb = command.get('verb', '')\n            if verb not in actions:\n                actions[verb] = []\n\n            action_type = command.get('type', '')\n            if 'params' in command and action_type == 'arb':\n                # Filter out arb parameters depending on the location\n                params_to_remove = [param for param in command['params'] if param.startswith('"
    },
    {
        "original": "def set_palette(palette, n_colors=None, color_codes=False):\n    \"\"\"\n    Set the matplotlib color cycle using a seaborn palette.\n\n    Parameters\n    ----------\n    palette : yellowbrick color palette | seaborn color palette (with ``sns_`` prepended)\n        Palette definition. Should be something that :func:`color_palette`\n        can process.\n    n_colors : int\n        Number of colors in the cycle. The default number of colors will depend\n        on the format of ``palette``, see the :func:`color_palette`\n        documentation for more information.\n    color_codes : bool\n        If ``True`` and ``palette`` is a seaborn palette, remap the shorthand\n        color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\n    \"\"\"\n    colors = color_palette(palette, n_colors)\n    if mpl_ge_150:\n        from cycler import cycler\n        cyl = cycler('color', colors)\n        mpl.rcParams['axes.prop_cycle'] = cyl\n    else:\n        mpl.rcParams[\"axes.color_cycle\"] = list(colors)\n    mpl.rcParams[\"patch.facecolor\"] = colors[0]\n    if color_codes:\n        set_color_codes(palette)",
        "sampled": "```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef set_palette(palette, n_colors=None, color_codes=False):\n    \"\"\"\n    Set the matplotlib color cycle using a seaborn palette.\n\n    Parameters\n    ----------\n    palette : string | list | tuple\n        Palette definition. Should be something that can be processed by seaborn.\n        Can be specified by name (e.g. \"seaborn-colorblind\"), or a list of RGB,\n        Hsv"
    },
    {
        "original": "def get(self, user_id, lang='zh_CN'):\n        \"\"\"\n        \u83b7\u53d6\u7528\u6237\u57fa\u672c\u4fe1\u606f\uff08\u5305\u62ecUnionID\u673a\u5236\uff09\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/wiki?t=resource/res_main&id=mp1421140839\n\n        :param user_id: \u666e\u901a\u7528\u6237\u7684\u6807\u8bc6\uff0c\u5bf9\u5f53\u524d\u516c\u4f17\u53f7\u552f\u4e00\n        :param lang: \u8fd4\u56de\u56fd\u5bb6\u5730\u533a\u8bed\u8a00\u7248\u672c\uff0czh_CN \u7b80\u4f53\uff0czh_TW \u7e41\u4f53\uff0cen \u82f1\u8bed\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n\n        \u4f7f\u7528\u793a\u4f8b::\n\n            from wechatpy import WeChatClient\n\n            client = WeChatClient('appid', 'secret')\n            user = client.user.get('openid')\n\n        \"\"\"\n        assert lang in ('zh_CN', 'zh_TW', 'en'), 'lang can only be one of \\\n            zh_CN, zh_TW, en language codes'\n        return self._get(\n            'user/info',\n            params={\n                'openid': user_id,\n                'lang': lang\n            }\n        )",
        "sampled": "```python\nclass User:\n    \"\"\" Represents a WeChat user \"\"\"\n    \n    def __init__(self, user_id, language):\n        \"\"\"\n        Initializes a User object.\n\n        :param user_id: The unique identifier of the user.\n        :param language: The language of the user.\n        \"\"\"\n        \n        self.user_id = user_id\n        self.language = language\n\nclass WeChatClient:\n    \"\"\" Represents a WeChat client \"\"\"\n\n    def __init__(self, appid, secret"
    },
    {
        "original": "def _calculate_fake_duration():\n    \"\"\"\n    Generate a NULL duration for when states do not run\n    but we want the results to be consistent.\n    \"\"\"\n    utc_start_time = datetime.datetime.utcnow()\n    local_start_time = utc_start_time - \\\n        (datetime.datetime.utcnow() - datetime.datetime.now())\n    utc_finish_time = datetime.datetime.utcnow()\n    start_time = local_start_time.time().isoformat()\n    delta = (utc_finish_time - utc_start_time)\n    # duration in milliseconds.microseconds\n    duration = (delta.seconds * 1000000 + delta.microseconds) / 1000.0\n\n    return start_time, duration",
        "sampled": "```python\nimport datetime\n\ndef _calculate_fake_duration():\n    \"\"\"\n    Generate a NULL duration for when states do not run\n    but we want the results to be consistent.\n    \"\"\"\n    \n    # Define a function to get the default date and time in datetime format\n    def get_default_datetime():\n        return datetime.datetime(2024, 1, 1, 0, 0)\n    \n    # Get the default null duration (0 days)\n    default_null_duration = datetime.timedelta"
    },
    {
        "original": "def hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      404:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(404)\n    return jsonify(authenticated=True, user=user)",
        "sampled": "```python\nimport getpass\nimport base64\n\ndef hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"\n    Prompts the user for authorization using HTTP Basic Auth.\n    \n    tags:\n      - Auth\n      \n    parameters:\n      - in: path\n        name: user\n        type: string\n        \n      - in: path\n        name: passwd\n        type: string\n        \n    produces:\n      - application/json\n      \n    responses:\n      200:\n        description: Successful authentication"
    },
    {
        "original": "def _add(self, ctx, table_name, record_id, column_values):\n        \"\"\"\n        :type column_values: list of (column, value_json)\n        \"\"\"\n        vsctl_table = self._get_table(table_name)\n        ovsrec_row = ctx.must_get_row(vsctl_table, record_id)\n        for column, value in column_values:\n            ctx.add_column(ovsrec_row, column, value)\n        ctx.invalidate_cache()",
        "sampled": "```python\nclass Database:\n    def _add(self, ctx, table_name, record_id, column_values):\n        \"\"\"\n        :type column_values: list of (column, value_json)\n        \"\"\"\n        # First, we need to create a dictionary to store the column values\n        data = {}\n        \n        # Then, we iterate over each column and value\n        for i in range(len(column_values)):\n            # We append the current position to the key for multiple occurrences of the same field"
    },
    {
        "original": "def _RDFClass(cls, table):\n    \"\"\"Creates a dynamic RDF proto struct class for given osquery table.\n\n    The fields of the proto will correspond to the columns of the table.\n\n    Args:\n      table: An osquery table for which the class is about to be generated.\n\n    Returns:\n      A class object corresponding to the given table.\n    \"\"\"\n    rdf_cls_name = \"OsqueryTable{}\".format(hash(table.query))\n    try:\n      return cls._rdf_cls_cache[rdf_cls_name]\n    except KeyError:\n      pass\n\n    rdf_cls = compatibility.MakeType(rdf_cls_name,\n                                     (rdf_structs.RDFProtoStruct,), {})\n\n    rdf_cls.AddDescriptor(\n        rdf_structs.ProtoEmbedded(\n            name=\"metadata\", field_number=1, nested=ExportedMetadata))\n\n    rdf_cls.AddDescriptor(\n        rdf_structs.ProtoString(name=\"__query__\", field_number=2))\n\n    for idx, column in enumerate(table.header.columns):\n      # It is possible that RDF column is named \"metadata\". To avoid name clash\n      # we must rename it to `__metadata__`.\n      if column.name == \"metadata\":\n        name = \"__metadata__\"\n      else:\n        name = column.name\n\n      descriptor = rdf_structs.ProtoString(name=name, field_number=idx + 3)\n      rdf_cls.AddDescriptor(descriptor)\n\n    cls._rdf_cls_cache[rdf_cls_name] = rdf_cls\n    return rdf_cls",
        "sampled": "```python\nimport inspect\n\ndef _RDFClass(cls, table):\n    \"\"\"\n    Creates a dynamic RDF proto struct class for given osquery table.\n\n    The fields of the proto will correspond to the columns of the table.\n\n    Args:\n      cls: A class object.\n      table: An osquery table for which the class is about to be generated.\n\n    Returns:\n      A class object corresponding to the given table.\n    \"\"\"\n\n    # Get all attributes from the input cls\n    attributes"
    },
    {
        "original": "def set_direct(self, address_value_dict):\n        \"\"\"Called in the context manager's set method to either overwrite the\n        value for an address, or create a new future and immediately set a\n        value in the future.\n\n        Args:\n            address_value_dict (dict of str:bytes): The unique full addresses\n                with bytes to set at that address.\n\n        Raises:\n            AuthorizationException\n        \"\"\"\n\n        with self._lock:\n            for address, value in address_value_dict.items():\n                self._validate_write(address)\n                if address in self._state:\n                    self._state[address].set_result(result=value)\n                else:\n                    fut = _ContextFuture(address=address)\n                    self._state[address] = fut\n                    fut.set_result(result=value)",
        "sampled": "```python\nclass ContextManager:\n    def set_direct(self, address_value_dict):\n        \"\"\"\n        Called in the context manager's set method to either overwrite the\n        value for an address, or create a new future and immediately set a\n        value in the future.\n\n        Args:\n            address_value_dict (dict of str:bytes): The unique full addresses\n                with bytes to set at that address.\n\n        Raises:\n            AuthorizationException\n        \"\"\"\n        \n        # Check if address_value_dict"
    },
    {
        "original": "def _InitApiApprovalFromDatabaseObject(api_approval, db_obj):\n  \"\"\"Initializes Api(Client|Hunt|CronJob)Approval from the database object.\"\"\"\n\n  api_approval.id = db_obj.approval_id\n  api_approval.requestor = db_obj.requestor_username\n  api_approval.reason = db_obj.reason\n\n  api_approval.notified_users = sorted(db_obj.notified_users)\n  api_approval.email_cc_addresses = sorted(db_obj.email_cc_addresses)\n  api_approval.email_message_id = db_obj.email_message_id\n\n  api_approval.approvers = sorted([g.grantor_username for g in db_obj.grants])\n\n  try:\n    approval_checks.CheckApprovalRequest(db_obj)\n    api_approval.is_valid = True\n  except access_control.UnauthorizedAccess as e:\n    api_approval.is_valid_message = utils.SmartStr(e)\n    api_approval.is_valid = False\n\n  return api_approval",
        "sampled": "```python\nclass ApiApproval:\n    def __init__(self, client_approval=False, hunt_approval=False, cron_job_approval=False):\n        \"\"\"\n        Initializes the ApiApproval object.\n        \n        :param client_approval: Whether the API is approved for Client usage (default=False)\n        :param hunt_approval: Whether the API is approved for Hunt usage (default=False)\n        :param cron_job_approval: Whether the API is approved for Cron Job usage (default=False)\n        \"\"\"\n        \n    def update_client"
    },
    {
        "original": "def add_send_message(self, connection, send_message):\n        \"\"\"Adds a send_message function to the Dispatcher's\n        dictionary of functions indexed by connection.\n\n        Args:\n            connection (str): A locally unique identifier\n                provided by the receiver of messages.\n            send_message (fn): The method that should be called\n                by the dispatcher to respond to messages which\n                arrive via connection.\n        \"\"\"\n        self._send_message[connection] = send_message\n        LOGGER.debug(\"Added send_message function \"\n                     \"for connection %s\", connection)",
        "sampled": "```python\nclass Dispatcher:\n    def __init__(self):\n        # Initialize an empty dictionary to store functions indexed by connection\n        self.functions = {}\n\n    def add_send_message(self, connection, send_message):\n        \"\"\"Adds a send_message function to the Dispatcher's \n        dictionary of functions indexed by connection.\n\n        Args:\n            connection (str): A locally unique identifier \n                provided by the receiver of messages.\n            send_message (fn): The method that should be called \n                by the"
    },
    {
        "original": "def batch_encode(self, iterator, *args, dim=0, **kwargs):\n        \"\"\"\n        Args:\n            iterator (iterator): Batch of text to encode.\n            *args: Arguments passed onto ``Encoder.__init__``.\n            dim (int, optional): Dimension along which to concatenate tensors.\n            **kwargs: Keyword arguments passed onto ``Encoder.__init__``.\n\n        Returns\n            torch.Tensor, list of int: Encoded and padded batch of sequences; Original lengths of\n                sequences.\n        \"\"\"\n        return stack_and_pad_tensors(\n            super().batch_encode(iterator), padding_index=self.padding_index, dim=dim)",
        "sampled": "```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            padding='max_length',\n            truncation=True,\n"
    },
    {
        "original": "def _collect_unused(self, start: GridQubit,\n                        used: Set[GridQubit]) -> Set[GridQubit]:\n        \"\"\"Lists all the qubits that are reachable from given qubit.\n\n        Args:\n            start: The first qubit for which connectivity should be calculated.\n                   Might be a member of used set.\n            used: Already used qubits, which cannot be used during the\n                  collection.\n\n        Returns:\n            Set of qubits that are reachable from starting qubit without\n            traversing any of the used qubits.\n        \"\"\"\n\n        def collect(n: GridQubit, visited: Set[GridQubit]):\n            visited.add(n)\n            for m in self._c_adj[n]:\n                if m not in used and m not in visited:\n                    collect(m, visited)\n\n        visited = set()  # type: Set[GridQubit]\n        collect(start, visited)\n        return visited",
        "sampled": "```python\nfrom typing import Set\n\nclass GridQubit:\n    pass\n\ndef _collect_unused(self, start: GridQubit, used: Set[GridQubit]) -> Set[GridQubit]:\n    \"\"\"\n    Lists all the qubits that are reachable from given qubit.\n\n    Args:\n        start: The first qubit for which connectivity should be calculated.\n               Might be a member of used set.\n        used: Already used qubits, which cannot be used during the\n"
    },
    {
        "original": "def get_flat_models_from_field(field: Field) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a single Pydantic ``Field`` (from a model) that could have been declared as a sublcass of BaseModel\n    (so, it could be a submodel), and generate a set with its model and all the sub-models in the tree.\n    I.e. if you pass a field that was declared to be of type ``Foo`` (subclass of BaseModel) as ``field``, and that\n    model ``Foo`` has a field of type ``Bar`` (also subclass of ``BaseModel``) and that model ``Bar`` has a field of\n    type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n\n    :param field: a Pydantic ``Field``\n    :return: a set with the model used in the declaration for this field, if any, and all its sub-models\n    \"\"\"\n    flat_models: Set[Type['main.BaseModel']] = set()\n    if field.sub_fields:\n        flat_models |= get_flat_models_from_fields(field.sub_fields)\n    elif lenient_issubclass(field.type_, main.BaseModel):\n        flat_models |= get_flat_models_from_model(field.type_)\n    elif lenient_issubclass(getattr(field.type_, '__pydantic_model__', None), main.BaseModel):\n        field.type_ = cast(Type['dataclasses.DataclassType'], field.type_)\n        flat_models |= get_flat_models_from_model(field.type_.__pydantic_model__)\n    return flat_models",
        "sampled": "```python\nfrom typing import Set\nfrom pydantic import BaseModel, Field\nimport inspect\n\ndef get_flat_models_from_field(field: Field) -> Set[type['BaseModel']]:\n    \"\"\"\n    Take a single Pydantic ``Field`` (from a model) that could have been declared as a sublcass of BaseModel\n    (so, it could be a submodel), and generate a set with its model and all the sub-models in the tree.\n    \n    :param"
    },
    {
        "original": "def download(self):\n        \"\"\"\n        Download all waypoints from the vehicle.\n        The download is asynchronous. Use :py:func:`wait_ready()` to block your thread until the download is complete.\n        \"\"\"\n        self.wait_ready()\n        self._vehicle._ready_attrs.remove('commands')\n        self._vehicle._wp_loaded = False\n        self._vehicle._master.waypoint_request_list_send()",
        "sampled": "```python\nimport threading\nfrom queue import Queue\n\nclass WaypointDownloader:\n    def __init__(self):\n        self.download_stopped = False\n        self.events_failed = 0\n\n    def download(self):\n        \"\"\"\n        Download all waypoints from the vehicle.\n        The download is asynchronous. Use :py:func:`wait_ready()` to block your thread until the download is complete.\n        \"\"\"\n        \n    def wait_ready(self, timeout=None):\n        \"\"\"\n        Wait until a certain number of"
    },
    {
        "original": "async def issuer_merge_revocation_registry_deltas(rev_reg_delta_json: str,\n                                                  other_rev_reg_delta_json: str) -> str:\n    \"\"\"\n    Merge two revocation registry deltas (returned by issuer_create_credential or issuer_revoke_credential) to accumulate common delta.\n    Send common delta to ledger to reduce the load.\n\n    :param rev_reg_delta_json: revocation registry delta json\n    :param other_rev_reg_delta_json: revocation registry delta for which PrevAccum value  is equal to current accum value of rev_reg_delta_json.\n    :return: Merged revocation registry delta\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\n        \"issuer_merge_revocation_registry_deltas: >>> rev_reg_delta_json: %r, other_rev_reg_delta_json: %r\",\n        rev_reg_delta_json,\n        other_rev_reg_delta_json)\n\n    if not hasattr(issuer_merge_revocation_registry_deltas, \"cb\"):\n        logger.debug(\"issuer_merge_revocation_registry_deltas: Creating callback\")\n        issuer_merge_revocation_registry_deltas.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_rev_reg_delta_json = c_char_p(rev_reg_delta_json.encode('utf-8'))\n    c_other_rev_reg_delta_json = c_char_p(other_rev_reg_delta_json.encode('utf-8'))\n\n    merged_revoc_reg_delta_json = await do_call('indy_issuer_merge_revocation_registry_deltas',\n                                                c_rev_reg_delta_json,\n                                                c_other_rev_reg_delta_json,\n                                                issuer_merge_revocation_registry_deltas.cb)\n    res = merged_revoc_reg_delta_json.decode()\n    logger.debug(\"issuer_merge_revocation_registry_deltas: <<< res: %r\", res)\n    return res",
        "sampled": "```python\nimport json\n\nasync def issuer_merge_revocation_registry_deltas(rev_reg_delta_json: str,\n                                                  other_rev_reg_delta_json: str) -> str:\n    \"\"\"\n    Merge two revocation registry deltas (returned by issuer_create_credential or issuer_revoke_credential) to accumulate common delta.\n    Send common delta to ledger to reduce the load.\n\n    :param rev_reg_delta_json: revocation registry delta json\n    :param other_rev_reg_delta_json: revocation registry"
    },
    {
        "original": "def compute_reverse_dependencies(\n        self, targets: Iterable[BuildTarget]\n    ) -> Mapping[str, Iterable[BuildTarget]]:\n        \"\"\"\n            Compute the set of targets which depend on each target.\n        \"\"\"\n        result = defaultdict(list)\n        for target in targets:\n            for dependency in target.dependencies:\n                result[dependency].append(target)\n        return result",
        "sampled": "```python\nfrom typing import Iterable, Mapping\n\nclass BuildTarget:\n    def __init__(self, name: str):\n        self.name = name\n        self.dependencies = set()\n\n    @staticmethod\n    def from_dict(data):\n        target = BuildTarget(data['name'])\n        target.dependencies = data.get('dependencies', [])\n        return target\n\n\nclass DependencyGraph:\n    def __init__(self):\n        self.targets = {}\n\n    def add_target(self, name: str) -> BuildTarget:\n"
    },
    {
        "original": "def show(self):\n        \"\"\"\n        Print some information on stdout about the string table\n        \"\"\"\n        print(\"StringBlock(stringsCount=0x%x, \"\n              \"stringsOffset=0x%x, \"\n              \"stylesCount=0x%x, \"\n              \"stylesOffset=0x%x, \"\n              \"flags=0x%x\"\n              \")\" % (self.stringCount,\n                     self.stringsOffset,\n                     self.styleCount,\n                     self.stylesOffset,\n                     self.flags))\n\n        if self.stringCount > 0:\n            print()\n            print(\"String Table: \")\n            for i, s in enumerate(self):\n                print(\"{:08d} {}\".format(i, repr(s)))\n\n        if self.styleCount > 0:\n            print()\n            print(\"Styles Table: \")\n            for i in range(self.styleCount):\n                print(\"{:08d} {}\".format(i, repr(self.getStyle(i))))",
        "sampled": "```python\nclass Solution:\n    def show(self):\n        \"\"\"\n        Print some information on stdout about the string table\n        Returns:\n            print : Prints information about the string table\n        \"\"\"\n\n# placeholder code for later implementation"
    },
    {
        "original": "def run_migrations_online():\n    \"\"\"\n    Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n    \"\"\"\n    connectable = context.config.attributes.get(\"connection\", None)\n\n    if connectable is None:\n        options = context.config.get_section(context.config.config_ini_section)\n        url = options.pop(\"url\")\n        connectable = create_engine(url, poolclass=pool.NullPool)\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=db.metadata,\n            compare_server_default=True,\n        )\n        with context.begin_transaction():\n            context.run_migrations()",
        "sampled": "```python\nfrom sqlalchemy import create_engine\n\ndef run_migrations_online():\n    \"\"\"\n    Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n    \"\"\"\n    \n    # Create an engine that stores data in the local directory's\n    # sql_appper.db file. This will be accessed by our next migration.\n    \n    engine = create_engine('sqlite:///sql_apper.db')\n    \n# Example usage:\nrun"
    },
    {
        "original": "def _relative_to_abs_sls(relative, sls):\n    \"\"\"\n    Convert ``relative`` sls reference into absolute, relative to ``sls``.\n    \"\"\"\n    levels, suffix = re.match(r'^(\\.+)(.*)$', relative).groups()\n    level_count = len(levels)\n    p_comps = sls.split('.')\n    if level_count > len(p_comps):\n        raise SaltRenderError(\n            'Attempted relative include goes beyond top level package'\n        )\n    return '.'.join(p_comps[:-level_count] + [suffix])",
        "sampled": "```python\ndef _relative_to_abs_sls(relative, sls):\n    \"\"\"\n    Convert ``relative`` sls reference into absolute, relative to ``sls``.\n    \"\"\"\n    # Split the relative and sls into lists of integers\n    relative = [int(i) for i in relative.split('.')]\n    sls = [int(i) for i in sls.split('.')]\n\n    # Initialize the result list\n    result = []\n\n    # Iterate over the relative and sls"
    },
    {
        "original": "def _populate_cache(platform, pkg_cache, mount_dir):\n    \"\"\"\n    If a ``pkg_cache`` directory is specified, then use it to populate the\n    disk image.\n    \"\"\"\n    if not pkg_cache:\n        return\n    if not os.path.isdir(pkg_cache):\n        return\n\n    if platform == 'pacman':\n        cache_dir = '{0}/var/cache/pacman/pkg'.format(mount_dir)\n\n    __salt__['file.mkdir'](cache_dir, 'root', 'root', '755')\n    __salt__['file.copy'](pkg_cache, cache_dir, recurse=True, remove_existing=True)",
        "sampled": "```python\nimport os\nimport shutil\n\ndef _populate_cache(platform, pkg_cache, mount_dir):\n    \"\"\"\n    If a ``pkg_cache`` directory is specified, then use it to populate the\n    disk image.\n    \"\"\"\n    \n    # Check if pkg_cache is not None and is a directory\n    if pkg_cache is not None and os.path.isdir(pkg_cache):\n        # Copy the contents of pkg_cache to mount_dir\n        shutil.copytree(pkg_cache, mount_dir)\n    else"
    },
    {
        "original": "def register_vm(datacenter, name, vmx_path, resourcepool_object, host_object=None):\n    \"\"\"\n    Registers a virtual machine to the inventory with the given vmx file, on success\n    it returns the vim.VirtualMachine managed object reference\n\n    datacenter\n        Datacenter object of the virtual machine, vim.Datacenter object\n\n    name\n        Name of the virtual machine\n\n    vmx_path:\n        Full path to the vmx file, datastore name should be included\n\n    resourcepool\n        Placement resource pool of the virtual machine, vim.ResourcePool object\n\n    host\n        Placement host of the virtual machine, vim.HostSystem object\n    \"\"\"\n    try:\n        if host_object:\n            task = datacenter.vmFolder.RegisterVM_Task(path=vmx_path, name=name,\n                                                       asTemplate=False,\n                                                       host=host_object,\n                                                       pool=resourcepool_object)\n        else:\n            task = datacenter.vmFolder.RegisterVM_Task(path=vmx_path, name=name,\n                                                       asTemplate=False,\n                                                       pool=resourcepool_object)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    try:\n        vm_ref = wait_for_task(task, name, 'RegisterVM Task')\n    except salt.exceptions.VMwareFileNotFoundError as exc:\n        raise salt.exceptions.VMwareVmRegisterError(\n            'An error occurred during registration operation, the '\n            'configuration file was not found: {0}'.format(exc))\n    return vm_ref",
        "sampled": "```python\nfrom pyVmomi import vim\n\ndef register_vm(datacenter, name, vmx_path, resourcepool_object, host_object=None):\n    \"\"\"\n    Registers a virtual machine to the inventory with the given vmx file, on success\n    it returns the vim.VirtualMachine managed object reference\n\n    datacenter\n        Datacenter object of the virtual machine, vim.Datacenter object\n\n    name\n        Name of the virtual machine\n\n    vmx_path:\n        Full path to the vm"
    },
    {
        "original": "def rename(self, new_name, **kwargs):\n        \"\"\"Rename this collection.\n\n        If operating in auth mode, client must be authorized as an\n        admin to perform this operation. Raises :class:`TypeError` if\n        `new_name` is not an instance of :class:`basestring`\n        (:class:`str` in python 3). Raises :class:`~pymongo.errors.InvalidName`\n        if `new_name` is not a valid collection name.\n\n        :Parameters:\n          - `new_name`: new name for this collection\n          - `**kwargs` (optional): additional arguments to the rename command\n            may be passed as keyword arguments to this helper method\n            (i.e. ``dropTarget=True``)\n\n        .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of\n           this collection is automatically applied to this operation when using\n           MongoDB >= 3.4.\n\n        .. versionchanged:: 3.4\n           Apply this collection's write concern automatically to this operation\n           when connected to MongoDB >= 3.4.\n\n        \"\"\"\n        if not isinstance(new_name, string_type):\n            raise TypeError(\"new_name must be an \"\n                            \"instance of %s\" % (string_type.__name__,))\n\n        if not new_name or \"..\" in new_name:\n            raise InvalidName(\"collection names cannot be empty\")\n        if new_name[0] == \".\" or new_name[-1] == \".\":\n            raise InvalidName(\"collecion names must not start or end with '.'\")\n        if \"$\" in new_name and not new_name.startswith(\"oplog.$main\"):\n            raise InvalidName(\"collection names must not contain '$'\")\n\n        new_name = \"%s.%s\" % (self.__database.name, new_name)\n        cmd = SON([(\"renameCollection\", self.__full_name), (\"to\", new_name)])\n        with self._socket_for_writes() as sock_info:\n            if sock_info.max_wire_version >= 5 and self.write_concern:\n                cmd['writeConcern'] = self.write_concern.document\n            cmd.update(kwargs)\n            sock_info.command('admin', cmd, parse_write_concern_error=True)",
        "sampled": "```python\nclass Collection:\n    def __init__(self, name, client):\n        self.name = name\n        self.client = client\n\n    def rename(self, new_name, **kwargs):\n        \"\"\"\n        Rename this collection.\n\n        If operating in auth mode, client must be authorized as an\n        admin to perform this operation. Raises :class:`TypeError` if\n        `new_name` is not an instance of :class:`basestring`\n        (:class:`str` in python"
    },
    {
        "original": "def guess_format(text, ext):\n    \"\"\"Guess the format and format options of the file, given its extension and content\"\"\"\n    lines = text.splitlines()\n\n    metadata = read_metadata(text, ext)\n\n    if ('jupytext' in metadata and set(metadata['jupytext'])\n            .difference(['encoding', 'executable', 'main_language'])) or \\\n            set(metadata).difference(['jupytext']):\n        return format_name_for_ext(metadata, ext), {}\n\n    # Is this a Hydrogen-like script?\n    # Or a Sphinx-gallery script?\n    if ext in _SCRIPT_EXTENSIONS:\n        comment = _SCRIPT_EXTENSIONS[ext]['comment']\n        twenty_hash = ''.join(['#'] * 20)\n        magic_re = re.compile(r'^(%|%%|%%%)[a-zA-Z]')\n        double_percent_re = re.compile(r'^{}( %%|%%)$'.format(comment))\n        double_percent_and_space_re = re.compile(r'^{}( %%|%%)\\s'.format(comment))\n        nbconvert_script_re = re.compile(r'^{}( <codecell>| In\\[[0-9 ]*\\]:?)'.format(comment))\n        vim_folding_markers_re = re.compile(r'^{}\\s*'.format(comment) + '{{{')\n        vscode_folding_markers_re = re.compile(r'^{}\\s*region'.format(comment))\n\n        twenty_hash_count = 0\n        double_percent_count = 0\n        magic_command_count = 0\n        rspin_comment_count = 0\n        vim_folding_markers_count = 0\n        vscode_folding_markers_count = 0\n\n        parser = StringParser(language='R' if ext in ['.r', '.R'] else 'python')\n        for line in lines:\n            parser.read_line(line)\n            if parser.is_quoted():\n                continue\n\n            # Don't count escaped Jupyter magics (no space between %% and command) as cells\n            if double_percent_re.match(line) or double_percent_and_space_re.match(line) or \\\n                    nbconvert_script_re.match(line):\n                double_percent_count += 1\n\n            if magic_re.match(line):\n                magic_command_count += 1\n\n            if line.startswith(twenty_hash) and ext == '.py':\n                twenty_hash_count += 1\n\n            if line.startswith(\"#'\") and ext in ['.R', '.r']:\n                rspin_comment_count += 1\n\n            if vim_folding_markers_re.match(line):\n                vim_folding_markers_count += 1\n\n            if vscode_folding_markers_re.match(line):\n                vscode_folding_markers_count += 1\n\n        if double_percent_count >= 1:\n            if magic_command_count:\n                return 'hydrogen', {}\n            return 'percent', {}\n\n        if vim_folding_markers_count:\n            return 'light', {'cell_markers': '{{{,}}}'}\n\n        if vscode_folding_markers_count:\n            return 'light', {'cell_markers': 'region,endregion'}\n\n        if twenty_hash_count >= 2:\n            return 'sphinx', {}\n\n        if rspin_comment_count >= 1:\n            return 'spin', {}\n\n    if ext == '.md':\n        for line in lines:\n            if line.startswith(':::'):  # Pandoc div\n                return 'pandoc', {}\n\n    # Default format\n    return get_format_implementation(ext).format_name, {}",
        "sampled": "```python\nimport re\nimport mimetypes\n\ndef guess_format(text, ext):\n    \"\"\"\n    Guess the format and format options of the file, given its extension and content\n    \"\"\"\n    \n    # First, try to guess the format based on the extension\n    mime_type, _ = mimetypes.guess_type(ext)\n    \n    # If the extension is not recognized, try to guess the format based on the content\n    if mime_type is None:\n        # Check if the content is"
    },
    {
        "original": "def owned_expansions(self):\n        \"\"\"List of expansions owned by the player.\"\"\"\n\n        owned = {}\n        for el in self.expansion_locations:\n            def is_near_to_expansion(t):\n                return t.position.distance_to(el) < self.EXPANSION_GAP_THRESHOLD\n\n            th = next((x for x in self.townhalls if is_near_to_expansion(x)), None)\n            if th:\n                owned[el] = th\n\n        return owned",
        "sampled": "```python\nclass Player:\n    def __init__(self, expansions=None):\n        self.expansions = expansions if expansions else []\n\n    def owned_expansions(self):\n        \"\"\"List of expansions owned by the player.\"\"\"\n        return self.expansions\n\n# Example usage:\nplayer = Player([\"Expansion 1\", \"Expansion 2\", \"Expansion 3\"])\nprint(player.owned_expansions())  # Output: ['Expansion 1', 'Expansion 2', 'Expansion 3"
    },
    {
        "original": "def calc_new_nonce_hash(self, new_nonce, number):\n        \"\"\"\n        Calculates the new nonce hash based on the current attributes.\n\n        :param new_nonce: the new nonce to be hashed.\n        :param number: number to prepend before the hash.\n        :return: the hash for the given new nonce.\n        \"\"\"\n        new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n        data = new_nonce + struct.pack('<BQ', number, self.aux_hash)\n\n        # Calculates the message key from the given data\n        return int.from_bytes(sha1(data).digest()[4:20], 'little', signed=True)",
        "sampled": "```python\nimport hashlib\n\nclass HashCalculator:\n    def calc_new_nonce_hash(self, new_nonce, number):\n        \"\"\"\n        Calculates the new nonce hash based on the current attributes.\n\n        :param new_nonce: the new nonce to be hashed.\n        :param number: number to prepend before the hash.\n        :return: the hash for the given new nonce.\n        \"\"\"\n        # Convert the number to string\n        number_str = str(number)\n        \n        # Prepend the number to the"
    },
    {
        "original": "def remove(self, address):\n        \"\"\" Remove an address or multiple addresses\n\n        :param address: list of addresses to remove\n        :type address: str or list[str]\n        \"\"\"\n        recipients = []\n        if isinstance(address, str):\n            address = {address}  # set\n        elif isinstance(address, (list, tuple)):\n            address = set(address)\n\n        for recipient in self._recipients:\n            if recipient.address not in address:\n                recipients.append(recipient)\n        if len(recipients) != len(self._recipients):\n            self._track_changes()\n        self._recipients = recipients",
        "sampled": "```python\nclass AddressManager:\n    def __init__(self):\n        self.addresses = set()\n\n    def add(self, address):\n        \"\"\" Add an address or multiple addresses\n\n        :param address: list of addresses to add\n        :type address: str or list[str]\n        \"\"\"\n        if isinstance(address, str):\n            self.addresses.add(address)\n        elif isinstance(address, list):\n            self.addresses.update(address)\n\n    def remove(self, address):\n        \"\"\" Remove an address"
    },
    {
        "original": "def in_placement_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if you can place something at a position. Remember, buildings usually use 2x2, 3x3 or 5x5 of these grid points.\n        Caution: some x and y offset might be required, see ramp code:\n        https://github.com/Dentosal/python-sc2/blob/master/sc2/game_info.py#L17-L18 \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.placement_grid[pos] != 0",
        "sampled": "```python\nfrom typing import Union\n\nclass Point2:\n    def __init__(self, x: int, y: int):\n        self.x = x\n        self.y = y\n\nclass Point3:\n    def __init__(self, x: int, y: int, z: int):\n        self.x = x\n        self.y = y\n        self.z = z\n\nclass Unit:\n    def __init__(self, x: int, y: int, z:"
    },
    {
        "original": "def _process_irrational_function_starts(self, functions, predetermined_function_addrs, blockaddr_to_function):\n        \"\"\"\n        Functions that are identified via function prologues can be starting after the actual beginning of the function.\n        For example, the following function (with an incorrect start) might exist after a CFG recovery:\n\n        sub_8049f70:\n          push    esi\n\n        sub_8049f71:\n          sub     esp, 0A8h\n          mov     esi, [esp+0ACh+arg_0]\n          mov     [esp+0ACh+var_88], 0\n\n        If the following conditions are met, we will remove the second function and merge it into the first function:\n        - The second function is not called by other code.\n        - The first function has only one jumpout site, which points to the second function.\n        - The first function and the second function are adjacent.\n\n        :param FunctionManager functions:   All functions that angr recovers.\n        :return:                            A set of addresses of all removed functions.\n        :rtype:                             set\n        \"\"\"\n\n        addrs = sorted(k for k in functions.keys()\n                       if not self.project.is_hooked(k) and not self.project.simos.is_syscall_addr(k))\n        functions_to_remove = set()\n        adjusted_cfgnodes = set()\n\n        for addr_0, addr_1 in zip(addrs[:-1], addrs[1:]):\n            if addr_1 in predetermined_function_addrs:\n                continue\n\n            func_0 = functions[addr_0]\n\n            if len(func_0.block_addrs) == 1:\n                block = next(func_0.blocks)\n                if block.vex.jumpkind not in ('Ijk_Boring', 'Ijk_InvalICache'):\n                    continue\n                # Skip alignment blocks\n                if self._is_noop_block(self.project.arch, block):\n                    continue\n\n                target = block.vex.next\n                if isinstance(target, pyvex.IRExpr.Const):  # pylint: disable=unidiomatic-typecheck\n                    target_addr = target.con.value\n                elif type(target) in (pyvex.IRConst.U16, pyvex.IRConst.U32, pyvex.IRConst.U64):  # pylint: disable=unidiomatic-typecheck\n                    target_addr = target.value\n                elif type(target) is int:  # pylint: disable=unidiomatic-typecheck\n                    target_addr = target\n                else:\n                    continue\n\n                if target_addr != addr_1:\n                    continue\n\n                cfgnode_0 = self.model.get_any_node(addr_0)\n                cfgnode_1 = self.model.get_any_node(addr_1)\n\n                # Are func_0 adjacent to func_1?\n                if cfgnode_0.addr + cfgnode_0.size != addr_1:\n                    continue\n\n                # Merge block addr_0 and block addr_1\n                l.debug(\"Merging function %#x into %#x.\", addr_1, addr_0)\n                self._merge_cfgnodes(cfgnode_0, cfgnode_1)\n                adjusted_cfgnodes.add(cfgnode_0)\n                adjusted_cfgnodes.add(cfgnode_1)\n\n                # Merge it\n                func_1 = functions[addr_1]\n                for block_addr in func_1.block_addrs:\n                    if block_addr == addr_1:\n                        # Skip addr_1 (since it has been merged to the preceding block)\n                        continue\n                    merge_with = self._addr_to_function(addr_0, blockaddr_to_function, functions)\n                    blockaddr_to_function[block_addr] = merge_with\n\n                functions_to_remove.add(addr_1)\n\n        for to_remove in functions_to_remove:\n            del functions[to_remove]\n\n        return functions_to_remove, adjusted_cfgnodes",
        "sampled": "```python\ndef _process_irrational_function_starts(self, functions, predetermined_function_addrs, blockaddr_to_function):\n    \"\"\"\n    This function identifies and removes irrational function starts from the given functions.\n\n    :param FunctionManager functions:   All functions that angr recovers.\n    :return:                            A set of addresses of all removed functions.\n    :rtype:                             set\n    \"\"\"\n    removed_functions = set()\n\n    # Iterate over each function\n    for func in functions.functions:\n"
    },
    {
        "original": "def get_access_information(self, code):\n        \"\"\"Return the access information for an OAuth2 authorization grant.\n\n        :param code: the code received in the request from the OAuth2 server\n        :returns: A dictionary with the key/value pairs for ``access_token``,\n            ``refresh_token`` and ``scope``. The ``refresh_token`` value will\n            be None when the OAuth2 grant is not refreshable. The ``scope``\n            value will be a set containing the scopes the tokens are valid for.\n\n        \"\"\"\n        if self.config.grant_type == 'password':\n            data = {'grant_type': 'password',\n                    'username': self.config.user,\n                    'password': self.config.pswd}\n        else:\n            data = {'code': code, 'grant_type': 'authorization_code',\n                    'redirect_uri': self.redirect_uri}\n        retval = self._handle_oauth_request(data)\n        return {'access_token': retval['access_token'],\n                'refresh_token': retval.get('refresh_token'),\n                'scope': set(retval['scope'].split(' '))}",
        "sampled": "```python\nclass OAuth2Service:\n    def get_access_information(self, code):\n        \"\"\"\n        Return the access information for an OAuth2 authorization grant.\n\n        :param code: the code received in the request from the OAuth2 server\n        :returns: A dictionary with the key/value pairs for ``access_token``,\n            ``refresh_token`` and ``scope``. The ``refresh_token`` value will\n            be None when the OAuth2 grant is not refreshable. The ``scope``\n"
    },
    {
        "original": "def _get_journal():\n    \"\"\"\n    Return the active running journal object\n    \"\"\"\n    if 'systemd.journald' in __context__:\n        return __context__['systemd.journald']\n    __context__['systemd.journald'] = systemd.journal.Reader()\n    # get to the end of the journal\n    __context__['systemd.journald'].seek_tail()\n    __context__['systemd.journald'].get_previous()\n    return __context__['systemd.journald']",
        "sampled": "```python\nclass Journal:\n    def __init__(self):\n        self.entries = []\n\n    def add_entry(self, text):\n        self.entries.append(text)\n\n    def remove_entry(self, index):\n        if index < len(self.entries):\n            del self.entries[index]\n        else:\n            print(\"Invalid index\")\n\n    def view_entries(self):\n        for i, entry in enumerate(self.entries):\n            print(f\"Entry {i+1}: {entry}\")\n\n\nclass JournalManager:\n    def __"
    },
    {
        "original": "def append_domain():\n    \"\"\"\n    Return append_domain if set\n    \"\"\"\n\n    grain = {}\n\n    if salt.utils.platform.is_proxy():\n        return grain\n\n    if 'append_domain' in __opts__:\n        grain['append_domain'] = __opts__['append_domain']\n    return grain",
        "sampled": "```python\ndef append_domain():\n    \"\"\"\n    Return append_domain if set\n    \"\"\"\n    # Initialize an empty set to store domain names\n    domain_set = set()\n    \n    # Define a function to append a domain to the set\n    def append_domain_name(domain):\n        # Add the domain to the set\n        domain_set.add(domain)\n    \n    # Define a function to check if a domain is in the set\n    def check_domain(domain):\n        # Return True if the domain"
    },
    {
        "original": "def transpose(self, name=None):\n    \"\"\"Returns matching `Conv2D` module.\n\n    Args:\n      name: Optional string assigning name of transpose module. The default name\n          is constructed by appending \"_transpose\" to `self.name`.\n\n    Returns:\n      `Conv2D` module.\n    \"\"\"\n    if name is None:\n      name = self.module_name + \"_transpose\"\n\n    if self._data_format == DATA_FORMAT_NHWC:\n      stride = self._stride[1:-1]\n    else:  # self._data_format == DATA_FORMAT_NCHW\n      stride = self._stride[2:]\n\n    return Conv2D(output_channels=lambda: self.input_channels,\n                  kernel_shape=self._kernel_shape,\n                  stride=stride,\n                  padding=self._padding,\n                  use_bias=self._use_bias,\n                  initializers=self._initializers,\n                  partitioners=self._partitioners,\n                  regularizers=self._regularizers,\n                  data_format=self._data_format,\n                  custom_getter=self._custom_getter,\n                  name=name)",
        "sampled": "```python\nimport tensorflow as tf\n\nclass Conv2DTranspose(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', name=None):\n        super(Conv2DTranspose, self).__init__()\n        self.conv2d = tf.keras.layers.Conv2D(\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            name=name\n        )\n\n    def call(self"
    },
    {
        "original": "def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize, position0, momentum0):\n        \"\"\"\n        Recursively builds a tree for proposing new position and momentum\n        \"\"\"\n        if depth == 0:\n\n            position_bar, momentum_bar, candidate_set_size, accept_set_bool =\\\n                self._initalize_tree(position, momentum, slice_var, direction * stepsize)\n\n            alpha = min(1, self._acceptance_prob(position, position_bar, momentum, momentum_bar))\n\n            return (position_bar, momentum_bar, position_bar, momentum_bar, position_bar,\n                    candidate_set_size, accept_set_bool, alpha, 1)\n\n        else:\n            (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,\n             candidate_set_size, accept_set_bool, alpha, n_alpha) =\\\n                self._build_tree(position, momentum, slice_var,\n                                 direction, depth - 1, stepsize, position0, momentum0)\n\n            if accept_set_bool == 1:\n                if direction == -1:\n                    # Build tree in backward direction\n                    (position_backward, momentum_backward, _, _, position_bar2, candidate_set_size2, accept_set_bool2,\n                     alpha2, n_alpha2) = self._build_tree(position_backward, momentum_backward, slice_var, direction,\n                                                          depth - 1, stepsize, position0, momentum0)\n                else:\n                    # Build tree in forward direction\n                    (_, _, position_forward, momentum_forward, position_bar2, candidate_set_size2, accept_set_bool2,\n                     alpha2, n_alpha2) = self._build_tree(position_forward, momentum_forward, slice_var, direction,\n                                                          depth - 1, stepsize, position0, momentum0)\n\n                if np.random.rand() < candidate_set_size2 / (candidate_set_size2 + candidate_set_size):\n                    position_bar = position_bar2\n\n                alpha += alpha2\n                n_alpha += n_alpha2\n                accept_set_bool, candidate_set_size =\\\n                    self._update_acceptance_criteria(position_forward, position_backward, momentum_forward,\n                                                     momentum_backward, accept_set_bool2, candidate_set_size,\n                                                     candidate_set_size2)\n\n            return (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,\n                    candidate_set_size, accept_set_bool, alpha, n_alpha)",
        "sampled": "```python\nimport numpy as np\n\nclass Simulation:\n    def __init__(self):\n        self.position = None\n        self.momentum = None\n        self.slice_var = None\n        self.direction = None\n        self.depth = None\n        self.stepsize = None\n        self.position0 = None\n        self.momentum0 = None\n\n    def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize, position0, momentum0):\n       "
    },
    {
        "original": "def __sendCommand(self, cmd):\n        \"\"\"send specific command to reference unit over serial port\n\n        Args:\n            cmd: OpenThread CLI string\n\n        Returns:\n            Done: successfully send the command to reference unit and parse it\n            Value: successfully retrieve the desired value from reference unit\n            Error: some errors occur, indicates by the followed specific error number\n        \"\"\"\n        logging.info('%s: sendCommand[%s]', self.port, cmd)\n        if self.logThreadStatus == self.logStatus['running']:\n            self.logThreadStatus = self.logStatus['pauseReq']\n            while self.logThreadStatus != self.logStatus['paused'] and self.logThreadStatus != self.logStatus['stop']:\n                pass\n\n        try:\n            # command retransmit times\n            retry_times = 3\n            while retry_times > 0:\n                retry_times -= 1\n                try:\n                    self._sendline(cmd)\n                    self._expect(cmd)\n                except Exception as e:\n                    logging.exception('%s: failed to send command[%s]: %s', self.port, cmd, str(e))\n                    if retry_times == 0:\n                        raise\n                else:\n                    break\n\n            line = None\n            response = []\n            retry_times = 10\n            while retry_times > 0:\n                line = self._readline()\n                logging.info('%s: the read line is[%s]', self.port, line)\n                if line:\n                    response.append(line)\n                    if line == 'Done':\n                        break\n                else:\n                    retry_times -= 1\n                    time.sleep(0.2)\n            if line != 'Done':\n                raise Exception('%s: failed to find end of response' % self.port)\n            logging.info('%s: send command[%s] done!', self.port, cmd)\n            return response\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger(\"sendCommand() Error: \" + str(e))\n            raise",
        "sampled": "```python\nclass ReferenceUnit:\n    def __init__(self):\n        self.serial_port = None\n\n    def __sendCommand(self, cmd):\n        \"\"\"send specific command to reference unit over serial port\n\n        Args:\n            cmd: OpenThread CLI string\n\n        Returns:\n            Done: successfully send the command to reference unit and parse it\n            Value: successfully retrieve the desired value from reference unit\n            Error: some errors occur, indicates by the followed specific error number\n        \"\"\"\n       "
    },
    {
        "original": "async def _handle_bad_server_salt(self, message):\n        \"\"\"\n        Corrects the currently used server salt to use the right value\n        before enqueuing the rejected message to be re-sent:\n\n            bad_server_salt#edab447b bad_msg_id:long bad_msg_seqno:int\n            error_code:int new_server_salt:long = BadMsgNotification;\n        \"\"\"\n        bad_salt = message.obj\n        self._log.debug('Handling bad salt for message %d', bad_salt.bad_msg_id)\n        self._state.salt = bad_salt.new_server_salt\n        states = self._pop_states(bad_salt.bad_msg_id)\n        self._send_queue.extend(states)\n\n        self._log.debug('%d message(s) will be resent', len(states))",
        "sampled": "```python\nclass MessageHandler:\n    def __init__(self):\n        self.bad_server_salt_pattern = r\"bad_server_salt#([a-zA-Z0-9]+) bad_msg_id:(\\d+) bad_msg_seqno:(\\d+) error_code:(\\d+) new_server_salt:(\\d+)\"\n\n    async def _handle_bad_server_salt(self, message):\n        \"\"\"\n        Corrects the currently used server salt to use the right value\n        before enqueuing the rejected"
    },
    {
        "original": "def update(self):\n        \"\"\"\n        Update the display\n        \"\"\"\n        # build requests for this iteration\n        reqs = self.build_requests()\n        for r in reqs:\n            r.block = self.block\n        results = self.send_requests(*reqs)\n\n        # call callback with the results\n        self.callback(results)",
        "sampled": "```python\nclass Solution:\n    def __init__(self):\n        self.n = 0\n        self.k = 0\n        self.arr = []\n\n    def update(self, n, k, arr):\n        \"\"\"\n        Update the display\n        \"\"\"\n        self.n = n\n        self.k = k\n        self.arr = arr\n\n    def get_result(self):\n        return self.arr[self.k]\n\n# Example usage:\nsolution = Solution()\nsolution.update(5, 2, ["
    },
    {
        "original": "def identical(self, other):\n        \"\"\"Like equals, but also checks attributes.\n        \"\"\"\n        try:\n            return (utils.dict_equiv(self.attrs, other.attrs) and\n                    self.equals(other))\n        except (TypeError, AttributeError):\n            return False",
        "sampled": "```python\nclass Object:\n    def __init__(self, attr1, attr2):\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def identical(self, other):\n        \"\"\"Like equals, but also checks attributes.\n        \"\"\"\n        return self.__dict__ == other.__dict__\n\n\n# Test the function\nobj1 = Object(1, 2)\nobj2 = Object(1, 2)\nobj3 = Object(1, 3"
    },
    {
        "original": "def walk_tree(self, top=None):\n        \"\"\"\n        Navigate all the groups in the file starting from top.\n        If top is None, the root group is used.\n        \"\"\"\n        if top is None:\n            top = self.rootgrp\n\n        values = top.groups.values()\n        yield values\n        for value in top.groups.values():\n            for children in self.walk_tree(value):\n                yield children",
        "sampled": "```python\nclass Node:\n    def __init__(self, name, children=None):\n        self.name = name\n        self.children = children if children else []\n\nclass Tree:\n    def __init__(self, root):\n        self.root = root\n\n    def walk_tree(self, top=None):\n        \"\"\"\n        Navigate all the groups in the file starting from top.\n        If top is None, the root group is used.\n        \"\"\"\n        if top is None:\n            top = self.root\n"
    },
    {
        "original": "def warn_for_geometry_collections(self):\n        \"\"\"Checks for GeoJson GeometryCollection features to warn user about incompatibility.\"\"\"\n        geom_collections = [\n            feature.get('properties') if feature.get('properties') is not None else key\n            for key, feature in enumerate(self._parent.data['features'])\n            if feature['geometry']['type'] == 'GeometryCollection'\n        ]\n        if any(geom_collections):\n            warnings.warn(\n                \"GeoJsonTooltip is not configured to render tooltips for GeoJson GeometryCollection geometries. \"\n                \"Please consider reworking these features: {} to MultiPolygon for full functionality.\\n\"\n                \"https://tools.ietf.org/html/rfc7946#page-9\".format(geom_collections), UserWarning)",
        "sampled": "```python\ndef warn_for_geometry_collections(self):\n    \"\"\"\n    Checks for GeoJson GeometryCollection features to warn user about incompatibility.\n    \"\"\"\n    # Assuming you have a list of GeoJson features\n    features = self.get_features()  # Replace with your actual method to get features\n    \n    # Initialize a flag to track if GeometryCollection is found\n    geometry_collection_found = False\n    \n    # Iterate over each feature\n    for feature in features:\n        # Check if the feature is"
    },
    {
        "original": "def mod_repo(repo, **kwargs):\n    \"\"\"\n    Modify one or more values for a repo.  If the repo does not exist, it will\n    be created, so long as uri is defined.\n\n    The following options are available to modify a repo definition:\n\n    repo\n        alias by which opkg refers to the repo.\n    uri\n        the URI to the repo.\n    compressed\n        defines (True or False) if the index file is compressed\n    enabled\n        enable or disable (True or False) repository\n        but do not remove if disabled.\n    refresh\n        enable or disable (True or False) auto-refresh of the repositories\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' pkg.mod_repo repo uri=http://new/uri\n        salt '*' pkg.mod_repo repo enabled=False\n    \"\"\"\n    repos = list_repos()\n    found = False\n    uri = ''\n    if 'uri' in kwargs:\n        uri = kwargs['uri']\n\n    for repository in repos:\n        source = repos[repository][0]\n        if source['name'] == repo:\n            found = True\n            repostr = ''\n            if 'enabled' in kwargs and not kwargs['enabled']:\n                repostr += '# '\n            if 'compressed' in kwargs:\n                repostr += 'src/gz ' if kwargs['compressed'] else 'src'\n            else:\n                repostr += 'src/gz' if source['compressed'] else 'src'\n            repo_alias = kwargs['alias'] if 'alias' in kwargs else repo\n            if ' ' in repo_alias:\n                repostr += ' \"{0}\"'.format(repo_alias)\n            else:\n                repostr += ' {0}'.format(repo_alias)\n            repostr += ' {0}'.format(kwargs['uri'] if 'uri' in kwargs else source['uri'])\n            trusted = kwargs.get('trusted')\n            repostr = _set_trusted_option_if_needed(repostr, trusted) if trusted is not None else \\\n                _set_trusted_option_if_needed(repostr, source.get('trusted'))\n            _mod_repo_in_file(repo, repostr, source['file'])\n        elif uri and source['uri'] == uri:\n            raise CommandExecutionError(\n                'Repository \\'{0}\\' already exists as \\'{1}\\'.'.format(uri, source['name']))\n\n    if not found:\n        # Need to add a new repo\n        if 'uri' not in kwargs:\n            raise CommandExecutionError(\n                'Repository \\'{0}\\' not found and no URI passed to create one.'.format(repo))\n        properties = {'uri': kwargs['uri']}\n        # If compressed is not defined, assume True\n        properties['compressed'] = kwargs['compressed'] if 'compressed' in kwargs else True\n        # If enabled is not defined, assume True\n        properties['enabled'] = kwargs['enabled'] if 'enabled' in kwargs else True\n        properties['trusted'] = kwargs.get('trusted')\n        _add_new_repo(repo, properties)\n\n    if 'refresh' in kwargs:\n        refresh_db()",
        "sampled": "```python\ndef mod_repo(repo, **kwargs):\n    \"\"\"\n    Modify one or more values for a repo.  If the repo does not exist, it will\n    be created, so long as uri is defined.\n\n    The following options are available to modify a repo definition:\n\n    repo\n        alias by which opkg refers to the repo.\n    uri\n        the URI to the repo.\n    compressed\n        defines (True or False) if the index file is compressed\n    enabled"
    },
    {
        "original": "def get_queues(self, service_desk_id, include_count=False, start=0, limit=50):\n        \"\"\"\n        Returns a page of queues defined inside a service desk, for a given service desk ID.\n        The returned queues will include an issue count for each queue (represented in issueCount field)\n        if the query param includeCount is set to true (defaults to false).\n\n        Permissions: The calling user must be an agent of the given service desk.\n\n        :param service_desk_id: str\n        :param include_count: bool\n        :param start: int\n        :param limit: int\n        :return: a page of queues\n        \"\"\"\n        url = 'rest/servicedeskapi/servicedesk/{}/queue'.format(service_desk_id)\n        params = {}\n\n        if include_count is not None:\n            params['includeCount'] = bool(include_count)\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params)",
        "sampled": "```python\nclass ServiceDesk:\n    def __init__(self, service_desk_id):\n        self.service_desk_id = service_desk_id\n        self.queues = []\n\n    def add_queue(self, queue):\n        self.queues.append(queue)\n\n    def get_queues(self, include_count=False, start=0, limit=50):\n        \"\"\"\n        Returns a page of queues defined inside a service desk, for a given service desk ID.\n        The returned queues will include an issue count"
    },
    {
        "original": "def get_starred_gists(self):\n        \"\"\"\n        :calls: `GET /gists/starred <http://developer.github.com/v3/gists>`_\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Gist.Gist`\n        \"\"\"\n        return github.PaginatedList.PaginatedList(\n            github.Gist.Gist,\n            self._requester,\n            \"/gists/starred\",\n            None\n        )",
        "sampled": "```python\nimport requests\nfrom github import Github, PaginatedList, Gist\n\nclass GithubClient:\n    def __init__(self, token):\n        self.g = Github(token)\n\n    def get_starred_gists(self):\n        \"\"\"\n        :calls: `GET /gists/starred <https://docs.github.com/en/rest/reference/gists#list-starred-gists>`_\n        :rtype: :class:`PaginatedList` of :class:`Gist`\n        \"\"\"\n       "
    },
    {
        "original": "def Log(self, format_str, *args):\n    \"\"\"Logs the message using the flow's standard logging.\n\n    Args:\n      format_str: Format string\n      *args: arguments to the format string\n    \"\"\"\n    log_entry = rdf_flow_objects.FlowLogEntry(\n        client_id=self.rdf_flow.client_id,\n        flow_id=self.rdf_flow.flow_id,\n        hunt_id=self.rdf_flow.parent_hunt_id,\n        message=format_str % args)\n    data_store.REL_DB.WriteFlowLogEntries([log_entry])\n    if self.rdf_flow.parent_hunt_id:\n      db_compat.ProcessHuntFlowLog(self.rdf_flow, format_str % args)",
        "sampled": "```python\nclass Solution:\n    def Log(self, format_str, *args):\n        \"\"\"Logs the message using the flow's standard logging.\n\n        Args:\n          format_str: Format string\n          *args: arguments to the format string\n        \"\"\"\n        # Assuming we have a logger object\n        logger = self.logger\n        # Use the format string and arguments to create a log message\n        log_message = format_str.format(*args)\n        # Log the message\n        logger.info(log"
    },
    {
        "original": "def resolve_group_names(self, r, target_group_ids, groups):\n        \"\"\"Resolve any security group names to the corresponding group ids\n\n        With the context of a given network attached resource.\n        \"\"\"\n        names = self.get_group_names(target_group_ids)\n        if not names:\n            return target_group_ids\n\n        target_group_ids = list(target_group_ids)\n        vpc_id = self.vpc_expr.search(r)\n        if not vpc_id:\n            raise PolicyExecutionError(self._format_error(\n                \"policy:{policy} non vpc attached resource used \"\n                \"with modify-security-group: {resource_id}\",\n                resource_id=r[self.manager.resource_type.id]))\n\n        found = False\n        for n in names:\n            for g in groups:\n                if g['GroupName'] == n and g['VpcId'] == vpc_id:\n                    found = g['GroupId']\n            if not found:\n                raise PolicyExecutionError(self._format_error((\n                    \"policy:{policy} could not resolve sg:{name} for \"\n                    \"resource:{resource_id} in vpc:{vpc}\"),\n                    name=n,\n                    resource_id=r[self.manager.resource_type.id], vpc=vpc_id))\n            target_group_ids.remove(n)\n            target_group_ids.append(found)\n        return target_group_ids",
        "sampled": "```python\ndef resolve_group_names(self, r, target_group_ids, groups):\n    \"\"\"\n    Resolve any security group names to the corresponding group ids\n\n    With the context of a given network attached resource.\n    \"\"\"\n    resolved_group_ids = {}\n    \n    for group_id, group in groups.items():\n        if group_id in target_group_ids:\n            resolved_group_ids[group_id] = group_id\n        else:\n            for name in group['names']:\n                if name in target_group_ids:\n"
    },
    {
        "original": "def NamedPlaceholders(iterable):\n  \"\"\"Returns named placeholders from all elements of the given iterable.\n\n  Use this function for VALUES of MySQL INSERTs.\n\n  To account for Iterables with undefined order (dicts before Python 3.6),\n  this function sorts column names.\n\n  Examples:\n    >>> NamedPlaceholders({\"password\": \"foo\", \"name\": \"bar\"})\n    u'(%(name)s, %(password)s)'\n\n  Args:\n    iterable: The iterable of strings to be used as placeholder keys.\n\n  Returns:\n    A string containing a tuple of comma-separated, sorted, named, placeholders.\n  \"\"\"\n  placeholders = \", \".join(\"%({})s\".format(key) for key in sorted(iterable))\n  return \"({})\".format(placeholders)",
        "sampled": "```python\ndef NamedPlaceholders(iterable):\n    \"\"\"\n    Returns named placeholders from all elements of the given iterable.\n\n    Use this function for VALUES of MySQL INSERTs.\n\n    To account for Iterables with undefined order (dicts before Python 3.6),\n    this function sorts column names.\n\n    Examples:\n        >>> NamedPlaceholders({\"password\": \"foo\", \"name\": \"bar\"})\n        u'(%(name)s, %(password)s)'\n\n    Args:\n        iterable:"
    },
    {
        "original": "def ReadClientLastPings(self,\n                          min_last_ping=None,\n                          max_last_ping=None,\n                          fleetspeak_enabled=None):\n    \"\"\"Reads last-ping timestamps for clients in the DB.\"\"\"\n    last_pings = {}\n    for client_id, metadata in iteritems(self.metadatas):\n      last_ping = metadata.get(\"ping\", rdfvalue.RDFDatetime(0))\n      is_fleetspeak_client = metadata.get(\"fleetspeak_enabled\", False)\n      if min_last_ping is not None and last_ping < min_last_ping:\n        continue\n      elif max_last_ping is not None and last_ping > max_last_ping:\n        continue\n      elif (fleetspeak_enabled is not None and\n            is_fleetspeak_client != fleetspeak_enabled):\n        continue\n      else:\n        last_pings[client_id] = metadata.get(\"ping\", None)\n    return last_pings",
        "sampled": "```python\nclass ClientPingReader:\n    def ReadClientLastPings(self, min_last_ping=None, max_last_ping=None, fleetspeak_enabled=None):\n        \"\"\"\n        Reads last-ping timestamps for clients in the DB.\n\n        Args:\n            min_last_ping (int, optional): Minimum last ping timestamp. Defaults to None.\n            max_last_ping (int, optional): Maximum last ping timestamp. Defaults to None.\n            fleetspeak_enabled (bool, optional): Whether fleetspeak is enabled."
    },
    {
        "original": "def get_asset_balance(self, asset, **params):\n        \"\"\"Get current asset balance.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#account-information-user_data\n\n        :param asset: required\n        :type asset: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: dictionary or None if not found\n\n        .. code-block:: python\n\n            {\n                \"asset\": \"BTC\",\n                \"free\": \"4723846.89208129\",\n                \"locked\": \"0.00000000\"\n            }\n\n        :raises: BinanceRequestException, BinanceAPIException\n\n        \"\"\"\n        res = self.get_account(**params)\n        # find asset balance in list of balances\n        if \"balances\" in res:\n            for bal in res['balances']:\n                if bal['asset'].lower() == asset.lower():\n                    return bal\n        return None",
        "sampled": "```python\nclass BinanceAPI:\n    def get_asset_balance(self, asset, recvWindow=None):\n        \"\"\"\n        Get current asset balance.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#account-information-user_data\n\n        :param asset: required\n        :type asset: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: dictionary or"
    },
    {
        "original": "def CreateAd(client, opener, ad_group_id):\n  \"\"\"Creates a ResponsiveDisplayAd.\n\n  Args:\n    client: an AdWordsClient instance.\n    opener: an OpenerDirector instance.\n    ad_group_id: an int ad group ID.\n\n  Returns:\n    The ad group ad that was successfully created.\n  \"\"\"\n  ad_group_ad_service = client.GetService('AdGroupAdService', 'v201809')\n  media_service = client.GetService('MediaService', 'v201809')\n\n  marketing_image_id = _CreateImage(\n      media_service, opener, 'https://goo.gl/3b9Wfh')\n  logo_image_id = _CreateImage(media_service, opener, 'https://goo.gl/mtt54n')\n\n  ad = {\n      'xsi_type': 'ResponsiveDisplayAd',\n      # This ad format doesn't allow the creation of an image using the\n      # Image.data field. An image must first be created using the MediaService,\n      # and Image.mediaId must be populated when creating the ad.\n      'marketingImage': {\n          'xsi_type': 'Image',\n          'mediaId': marketing_image_id\n      },\n      'shortHeadline': 'Travel',\n      'longHeadline': 'Travel the World',\n      'description': 'Take to the air!',\n      'businessName': 'Interplanetary Cruises',\n      'finalUrls': ['http://wwww.example.com'],\n      # Optional: Call to action text.\n      # Valid texts: https://support.google.com/adwords/answer/7005917\n      'callToActionText': 'Apply Now',\n      # Optional: Set dynamic display ad settings, composed of landscape logo\n      # image, promotion text, and price prefix.\n      'dynamicDisplayAdSettings': CreateDynamicDisplayAdSettings(\n          client, opener),\n      # Optional: Create a logo image and set it to the ad.\n      'logoImage': {\n          'xsi_type': 'Image',\n          'mediaId': logo_image_id\n      },\n      # Optional: Create a square marketing image and set it to the ad.\n      'squareMarketingImage': {\n          'xsi_type': 'Image',\n          'mediaId': logo_image_id\n      },\n      # Whitelisted accounts only: Set color settings using hexadecimal values.\n      # Set allowFlexibleColor to False if you want your ads to render by always\n      # using your colors strictly.\n      # 'mainColor': '#000fff',\n      # 'accentColor': '#fff000',\n      # 'allowFlexibleColor': False,\n      # Whitelisted accounts only: Set the format setting that the ad will be\n      # served in.\n      # 'formatSetting': 'NON_NATIVE'\n  }\n\n  ad_group_ad = {\n      'ad': ad,\n      'adGroupId': ad_group_id\n  }\n\n  operations = [{\n      'operation': 'ADD',\n      'operand': ad_group_ad\n  }]\n\n  return ad_group_ad_service.mutate(operations)['value'][0]",
        "sampled": "```python\nfrom googleads import adwords\nfrom googleads.adwords import AdGroupAdService\nfrom googleads.adwords import AdGroupService\nfrom googleads.adwords import CampaignService\nfrom googleads.adwords import Client\n\nclass AdWordsClient:\n    def __init__(self, client_id, client_secret, refresh_token):\n        self.client = Client(client_id, client_secret, refresh_token)\n\n    def create_ad(self, opener, ad_group_id):\n        ad_group_ad_service"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a TrainingDataSet object from a json dictionary.\"\"\"\n        args = {}\n        if 'environment_id' in _dict:\n            args['environment_id'] = _dict.get('environment_id')\n        if 'collection_id' in _dict:\n            args['collection_id'] = _dict.get('collection_id')\n        if 'queries' in _dict:\n            args['queries'] = [\n                TrainingQuery._from_dict(x) for x in (_dict.get('queries'))\n            ]\n        return cls(**args)",
        "sampled": "```python\nclass TrainingDataSet:\n    def __init__(self, data, labels):\n        \"\"\"\n        Initialize a TrainingDataSet object.\n\n        Args:\n            data (list): A list of data points.\n            labels (list): A list of corresponding labels.\n        \"\"\"\n        self.data = data\n        self.labels = labels\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"\n        Initialize a TrainingDataSet object from a json dictionary.\n\n        Args:\n            _dict ("
    },
    {
        "original": "async def _receive_reconfig_param(self, param):\n        \"\"\"\n        Handle a RE-CONFIG parameter.\n        \"\"\"\n        self.__log_debug('<< %s', param)\n\n        if isinstance(param, StreamResetOutgoingParam):\n            # mark closed inbound streams\n            for stream_id in param.streams:\n                self._inbound_streams.pop(stream_id, None)\n\n                # close data channel\n                channel = self._data_channels.get(stream_id)\n                if channel:\n                    self._data_channel_close(channel)\n\n            # send response\n            response_param = StreamResetResponseParam(\n                response_sequence=param.request_sequence,\n                result=1)\n            self._reconfig_response_seq = param.request_sequence\n\n            await self._send_reconfig_param(response_param)\n        elif isinstance(param, StreamAddOutgoingParam):\n            # increase inbound streams\n            self._inbound_streams_count += param.new_streams\n\n            # send response\n            response_param = StreamResetResponseParam(\n                response_sequence=param.request_sequence,\n                result=1)\n            self._reconfig_response_seq = param.request_sequence\n\n            await self._send_reconfig_param(response_param)\n        elif isinstance(param, StreamResetResponseParam):\n            if (self._reconfig_request and\n               param.response_sequence == self._reconfig_request.request_sequence):\n                # mark closed streams\n                for stream_id in self._reconfig_request.streams:\n                    self._outbound_stream_seq.pop(stream_id, None)\n                    self._data_channel_closed(stream_id)\n\n                self._reconfig_request = None\n                await self._transmit_reconfig()",
        "sampled": "```python\nclass System:\n    def __init__(self):\n        self.reconfig_params = {}\n\n    async def _receive_reconfig_param(self, param):\n        \"\"\"\n        Handle a RE-CONFIG parameter.\n        \n        :param param: The RE-CONFIG parameter to handle.\n        \"\"\"\n        # Check if the parameter is a dictionary\n        if isinstance(param, dict):\n            # Get the parameter name and value\n            param_name = param.get('name')\n            param_value = param.get('"
    },
    {
        "original": "def select(sockets, remain=conf.recv_poll_rate):\n        \"\"\"This function is called during sendrecv() routine to select\n        the available sockets.\n\n        params:\n         - sockets: an array of sockets that need to be selected\n        returns:\n         - an array of sockets that were selected\n         - the function to be called next to get the packets (i.g. recv)\n        \"\"\"\n        try:\n            inp, _, _ = select(sockets, [], [], remain)\n        except (IOError, select_error) as exc:\n            # select.error has no .errno attribute\n            if exc.args[0] != errno.EINTR:\n                raise\n        return inp, None",
        "sampled": "```python\nimport select\nimport socket\n\ndef select(sockets, remain=conf.recv_poll_rate):\n    \"\"\"\n    This function is called during sendrecv() routine to select\n    the available sockets.\n\n    params:\n     - sockets: an array of sockets that need to be selected\n    returns:\n     - an array of sockets that were selected\n     - the function to be called next to get the packets (i.g. recv)\n    \"\"\"\n    # Use select function to select the available"
    },
    {
        "original": "def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        with self._lock:\n            if name not in self._gauges:\n                if self._registry._ignore_patterns and any(\n                    pattern.match(name) for pattern in self._registry._ignore_patterns\n                ):\n                    gauge = noop_metric\n                else:\n                    gauge = Gauge(name)\n                self._gauges[name] = gauge\n            return self._gauges[name]",
        "sampled": "```python\nclass Gauge:\n    def __init__(self):\n        self.gauges = {}\n\n    def gauge(self, name):\n        \"\"\"\n        Returns an existing or creates and returns a new gauge\n        :param name: name of the gauge\n        :return: the gauge object\n        \"\"\"\n        if name not in self.gauges:\n            self.gauges[name] = GaugeValue()\n        return self.gauges[name]\n\n\nclass GaugeValue:\n    def __init__(self"
    },
    {
        "original": "def _update_fetch_positions(self, partitions):\n        \"\"\"Set the fetch position to the committed position (if there is one)\n        or reset it using the offset reset policy the user has configured.\n\n        Arguments:\n            partitions (List[TopicPartition]): The partitions that need\n                updating fetch positions.\n\n        Raises:\n            NoOffsetForPartitionError: If no offset is stored for a given\n                partition and no offset reset policy is defined.\n        \"\"\"\n        # Lookup any positions for partitions which are awaiting reset (which may be the\n        # case if the user called :meth:`seek_to_beginning` or :meth:`seek_to_end`. We do\n        # this check first to avoid an unnecessary lookup of committed offsets (which\n        # typically occurs when the user is manually assigning partitions and managing\n        # their own offsets).\n        self._fetcher.reset_offsets_if_needed(partitions)\n\n        if not self._subscription.has_all_fetch_positions():\n            # if we still don't have offsets for all partitions, then we should either seek\n            # to the last committed position or reset using the auto reset policy\n            if (self.config['api_version'] >= (0, 8, 1) and\n                self.config['group_id'] is not None):\n                # first refresh commits for all assigned partitions\n                self._coordinator.refresh_committed_offsets_if_needed()\n\n            # Then, do any offset lookups in case some positions are not known\n            self._fetcher.update_fetch_positions(partitions)",
        "sampled": "```python\nclass TopicPartition:\n    def __init__(self, topic, partition):\n        self.topic = topic\n        self.partition = partition\n        self.committed_position = None\n        self.offset_reset_policy = None\n\nclass NoOffsetForPartitionError(Exception):\n    pass\n\nclass OffsetResetPolicy:\n    def __init__(self, policy):\n        self.policy = policy\n\nclass KafkaConsumer:\n    def __init__(self):\n        self.offset_reset_policies = {}\n\n    def _update"
    },
    {
        "original": "def _get_filename_path(self, path):\r\n        \"\"\" Helper function for creating filename without file extension\r\n        \"\"\"\r\n        feature_filename = os.path.join(path, self.feature_type.value)\r\n\r\n        if self.feature_name is not None:\r\n            feature_filename = os.path.join(feature_filename, self.feature_name)\r\n\r\n        return feature_filename",
        "sampled": "```python\nimport os\n\nclass Solution:\n    def _get_filename_path(self, path):\n        \"\"\" Helper function for creating filename without file extension\n        \"\"\"\n        # Get the filename from the path\n        filename = os.path.basename(path)\n        \n        # Split the filename into name and extension\n        name, extension = os.path.splitext(filename)\n        \n        # Return the filename without extension\n        return name\n\n# Example usage:\nsolution = Solution()\nprint(solution._get_filename_path('/path/to"
    },
    {
        "original": "async def from_socket(cls, sockname: typing.Optional[str] = None) -> \"AIOClient\":\n        \"\"\"Create a new AIOClient using Unix transport and BSER Codec\n        connecting to the specified socket. If the specified socket is None,\n        then resolve the socket path automatically.\n\n        This method also schedules the receive loop to run on the event loop.\n\n        This method is a coroutine.\"\"\"\n        if not sockname:\n            sockname = await _resolve_sockname()\n        transport = AsyncUnixSocketTransport()\n        await transport.activate(sockname=sockname)\n        connection = AsyncBserCodec(transport)\n        obj = cls(connection)\n        obj.enable_receiving()\n        return obj",
        "sampled": "```python\nimport asyncio\nimport socket\nimport typing\n\nclass AIOClient:\n    def __init__(self, sock: socket.socket):\n        self.sock = sock\n\n    async def receive_loop(self):\n        while True:\n            data = await self.sock.recv(1024)\n            if not data:\n                break\n            # Process the received data\n            print(data.decode())\n\n    async def connect(self, sockname: typing.Optional[str] = None):\n        if sockname is None:\n"
    },
    {
        "original": "def synthesize(vers, opts):\n    \"\"\" Synthesize a capability enabled version response\n        This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    failed = False  # noqa: F841 T25377293 Grandfathered in\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                \"client required capability `\"\n                + name\n                + \"` is not supported by this server\"\n            )\n    return vers",
        "sampled": "```python\ndef synthesize(vers, opts):\n    \"\"\"\n    Synthesize a capability enabled version response\n    This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n    # Initialize an empty dictionary to store the synthesized response\n    response = {}\n\n    # Check if the version is supported\n    if vers in ['v1', 'v2', 'v3']:\n        # If the version is supported, add it to the response\n        response['version'] = vers\n\n"
    },
    {
        "original": "def query_signing(self, contract_id=None, plan_id=None, contract_code=None, openid=None, version=\"1.0\"):\n        \"\"\"\n        \u67e5\u8be2\u7b7e\u7ea6\u5173\u7cfb api\n\n        :param contract_id: \u53ef\u9009 \u59d4\u6258\u4ee3\u6263\u534f\u8baeid \u59d4\u6258\u4ee3\u6263\u7b7e\u7ea6\u6210\u529f\u540e\u7531\u5fae\u4fe1\u8fd4\u56de\u7684\u59d4\u6258\u4ee3\u6263\u534f\u8baeid\uff0c\u9009\u62e9contract_id\u67e5\u8be2\uff0c\u5219\u6b64\u53c2\u6570\u5fc5\u586b\n        :param plan_id: \u53ef\u9009 \u6a21\u677fid \u5546\u6237\u5728\u5fae\u4fe1\u5546\u6237\u5e73\u53f0\u914d\u7f6e\u7684\u4ee3\u6263\u6a21\u677fid\uff0c\u9009\u62e9plan_id+contract_code\u67e5\u8be2\uff0c\u5219\u6b64\u53c2\u6570\u5fc5\u586b\n        :param contract_code: \u53ef\u9009 \u7b7e\u7ea6\u534f\u8bae\u53f7 \u5546\u6237\u8bf7\u6c42\u7b7e\u7ea6\u65f6\u4f20\u5165\u7684\u7b7e\u7ea6\u534f\u8bae\u53f7\uff0c\u5546\u6237\u4fa7\u987b\u552f\u4e00\u3002\u9009\u62e9plan_id+contract_code\u67e5\u8be2\uff0c\u5219\u6b64\u53c2\u6570\u5fc5\u586b\n        :param openid: \u53ef\u9009 openid \u7528\u6237\u6807\u8bc6\uff0c\u5fc5\u987b\u4fdd\u8bc1\u4e0e\u4f20\u5165appid\u5bf9\u5e94\n        :param version: \u7248\u672c\u53f7 \u56fa\u5b9a\u503c1.0\n        :return: \u8fd4\u56de\u7684\u7ed3\u679c\u4fe1\u606f\n        \"\"\"\n        if not contract_id and not (plan_id and contract_code) and not (plan_id and openid):\n            raise ValueError(\"contract_id and (plan_id, contract_code) and (plan_id, openid) must be a choice.\")\n        data = {\n            \"appid\": self.appid,\n            \"mch_id\": self.mch_id,\n            \"contract_id\": contract_id,\n            \"plan_id\": plan_id,\n            \"contract_code\": contract_code,\n            \"openid\": openid,\n            \"version\": version,\n            \"nonce_str\": None,\n        }\n        return self._post('papay/querycontract', data=data)",
        "sampled": "```python\nclass WeChatAPI:\n    def query_signing(self, contract_id=None, plan_id=None, contract_code=None, openid=None, version=\"1.0\"):\n        \"\"\"\n        \u67e5\u8be2\u7b7e\u7ea6\u5173\u7cfb api\n\n        :param contract_id: \u53ef\u9009 \u59d4\u6258\u4ee3\u6263\u534f\u8baeid \u59d4\u6258\u4ee3\u6263\u7b7e\u7ea6\u6210\u529f\u540e\u7531\u5fae\u4fe1\u8fd4\u56de\u7684\u59d4\u6258\u4ee3\u6263\u534f\u8baeid\uff0c\u9009\u62e9contract_id\u67e5\u8be2\uff0c\u5219\u6b64\u53c2\u6570\u5fc5\u586b\n        :param plan_id: \u53ef\u9009 \ufffd"
    },
    {
        "original": "def _find_usage_security_groups(self):\n        \"\"\"find usage for elasticache security groups\"\"\"\n        num_groups = 0\n        # If EC2-Classic isn't available (e.g., a new account)\n        # this method will fail with:\n        #   Code:    \"InvalidParameterValue\"\n        #   Message: \"Use of cache security groups is not permitted in\n        #             this API version for your account.\"\n        #   Type:    \"Sender\"\n        try:\n            paginator = self.conn.get_paginator(\n                'describe_cache_security_groups')\n            for page in paginator.paginate():\n                for secgroup in page['CacheSecurityGroups']:\n                    num_groups += 1\n        except ClientError as ex:\n            if ex.response['Error']['Code'] != 'InvalidParameterValue':\n                raise ex\n            logger.debug(\"caught ClientError checking ElastiCache security \"\n                         \"groups (account without EC2-Classic?)\")\n\n        self.limits['Security Groups']._add_current_usage(\n            num_groups,\n            aws_type='WS::ElastiCache::SecurityGroup'\n        )",
        "sampled": "```python\ndef _find_usage_security_groups(self):\n    \"\"\"\n    This function is used to find usage for elasticache security groups.\n    \n    It is assumed that the input data is a dictionary where the keys are the names of the security groups and the values are lists of the resources that are associated with each security group.\n    \n    The function returns a dictionary where the keys are the names of the security groups and the values are the number of resources associated with each security group.\n    \"\"\"\n    \n   "
    },
    {
        "original": "def avail_locations(conn=None, call=None):\n    \"\"\"\n    Return a list of locations\n    \"\"\"\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_locations function must be called with '\n            '-f or --function, or with the --list-locations option'\n        )\n\n    if conn is None:\n        conn = get_conn()\n\n    endpoints = nova.get_entry(conn.get_catalog(), 'type', 'compute')['endpoints']\n    ret = {}\n    for endpoint in endpoints:\n        ret[endpoint['region']] = endpoint\n\n    return ret",
        "sampled": "```python\nimport sqlite3\n\ndef avail_locations(conn=None, call=None):\n    \"\"\"\n    Return a list of locations\n    \"\"\"\n    \n    # Create a cursor object\n    cur = conn.cursor()\n    \n    # SQL query to get all locations\n    query = \"SELECT location FROM locations\"\n    \n    # Execute the query\n    cur.execute(query)\n    \n    # Fetch all rows\n    rows = cur.fetchall()\n    \n    # Extract locations from rows\n    locations = [row[0"
    },
    {
        "original": "def _update_function_transition_graph(self, src_node_key, dst_node_key, jumpkind='Ijk_Boring', ins_addr=None,\n                                          stmt_idx=None, confirmed=None):\n        \"\"\"\n        Update transition graphs of functions in function manager based on information passed in.\n\n        :param str jumpkind: Jumpkind.\n        :param CFGNode src_node: Source CFGNode\n        :param CFGNode dst_node: Destionation CFGNode\n        :param int ret_addr: The theoretical return address for calls\n        :return: None\n        \"\"\"\n\n        if dst_node_key is not None:\n            dst_node = self._graph_get_node(dst_node_key, terminator_for_nonexistent_node=True)\n            dst_node_addr = dst_node.addr\n            dst_codenode = dst_node.to_codenode()\n            dst_node_func_addr = dst_node.function_address\n        else:\n            dst_node = None\n            dst_node_addr = None\n            dst_codenode = None\n            dst_node_func_addr = None\n\n        if src_node_key is None:\n            if dst_node is None:\n                raise ValueError(\"Either src_node_key or dst_node_key must be specified.\")\n            self.kb.functions.function(dst_node.function_address, create=True)._register_nodes(True,\n                                                                                               dst_codenode\n                                                                                               )\n            return\n\n        src_node = self._graph_get_node(src_node_key, terminator_for_nonexistent_node=True)\n\n        # Update the transition graph of current function\n        if jumpkind == \"Ijk_Call\":\n            ret_addr = src_node.return_target\n            ret_node = self.kb.functions.function(\n                src_node.function_address,\n                create=True\n            )._get_block(ret_addr).codenode if ret_addr else None\n\n            self.kb.functions._add_call_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_addr=dst_node_addr,\n                retn_node=ret_node,\n                syscall=False,\n                ins_addr=ins_addr,\n                stmt_idx=stmt_idx,\n            )\n\n        if jumpkind.startswith('Ijk_Sys'):\n\n            self.kb.functions._add_call_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_addr=dst_node_addr,\n                retn_node=src_node.to_codenode(),  # For syscalls, they are returning to the address of themselves\n                syscall=True,\n                ins_addr=ins_addr,\n                stmt_idx=stmt_idx,\n            )\n\n        elif jumpkind == 'Ijk_Ret':\n            # Create a return site for current function\n            self.kb.functions._add_return_from(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_node=dst_codenode,\n            )\n\n            if dst_node is not None:\n                # Create a returning edge in the caller function\n                self.kb.functions._add_return_from_call(\n                    function_addr=dst_node_func_addr,\n                    src_function_addr=src_node.function_address,\n                    to_node=dst_codenode,\n                )\n\n        elif jumpkind == 'Ijk_FakeRet':\n            self.kb.functions._add_fakeret_to(\n                function_addr=src_node.function_address,\n                from_node=src_node.to_codenode(),\n                to_node=dst_codenode,\n                confirmed=confirmed,\n            )\n\n        elif jumpkind in ('Ijk_Boring', 'Ijk_InvalICache'):\n\n            src_obj = self.project.loader.find_object_containing(src_node.addr)\n            dest_obj = self.project.loader.find_object_containing(dst_node.addr) if dst_node is not None else None\n\n            if src_obj is dest_obj:\n                # Jump/branch within the same object. Might be an outside jump.\n                to_outside = src_node.function_address != dst_node_func_addr\n            else:\n                # Jump/branch between different objects. Must be an outside jump.\n                to_outside = True\n\n            if not to_outside:\n                self.kb.functions._add_transition_to(\n                    function_addr=src_node.function_address,\n                    from_node=src_node.to_codenode(),\n                    to_node=dst_codenode,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )\n\n            else:\n                self.kb.functions._add_outside_transition_to(\n                    function_addr=src_node.function_address,\n                    from_node=src_node.to_codenode(),\n                    to_node=dst_codenode,\n                    to_function_addr=dst_node_func_addr,\n                    ins_addr=ins_addr,\n                    stmt_idx=stmt_idx,\n                )",
        "sampled": "```python\nclass CFGNode:\n    def __init__(self, key):\n        self.key = key\n        self.transition_graph = {}\n\nclass FunctionManager:\n    def __init__(self):\n        self.function_transition_graph = {}\n\n    def _update_function_transition_graph(self, src_node_key, dst_node_key, jumpkind='Ijk_Boring', ins_addr=None, stmt_idx=None, confirmed=None):\n        \"\"\"\n        Update transition graphs of functions in function manager based on information passed in.\n\n       "
    },
    {
        "original": "def do_indent(s, width=4, indentfirst=False):\n    \"\"\"Return a copy of the passed string, each line indented by\n    4 spaces. The first line is not indented. If you want to\n    change the number of spaces or indent the first line too\n    you can pass additional parameters to the filter:\n\n    .. sourcecode:: jinja\n\n        {{ mytext|indent(2, true) }}\n            indent by two spaces and indent the first line too.\n    \"\"\"\n    indention = u' ' * width\n    rv = (u'\\n' + indention).join(s.splitlines())\n    if indentfirst:\n        rv = indention + rv\n    return rv",
        "sampled": "```python\ndef do_indent(s, width=4, indentfirst=False):\n    lines = s.split('\\n')\n    if indentfirst:\n        lines[0] = ' ' * width + lines[0]\n    else:\n        lines = [' ' * width + line for line in lines]\n    return '\\n'.join(lines)\n```"
    },
    {
        "original": "def end_block(self):\n        \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\"\n        self.current_indent -= 1\n\n        # If we did not add a new line automatically yet, now it's the time!\n        if not self.auto_added_line:\n            self.writeln()\n            self.auto_added_line = True",
        "sampled": "```python\nclass CodeFormatter:\n    def __init__(self):\n        self.indent_level = 0\n\n    def end_block(self):\n        \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\"\n        self.indent_level -= 1\n        print(' ' * (4 * self.indent_level))\n```"
    },
    {
        "original": "def find_in_subgraph_by_name(subgraph, target_name, target_package, nodetype):\n    \"\"\"Find an entry in a subgraph by name. Any mapping that implements\n    .items() and maps unique id -> something can be used as the subgraph.\n\n    Names are like:\n        '{nodetype}.{target_package}.{target_name}'\n\n    You can use `None` for the package name as a wildcard.\n    \"\"\"\n    for name, model in subgraph.items():\n        if id_matches(name, target_name, target_package, nodetype, model):\n            return model\n\n    return None",
        "sampled": "```python\ndef find_in_subgraph_by_name(subgraph, target_name, target_package, nodetype):\n    \"\"\"\n    Find an entry in a subgraph by name. Any mapping that implements\n    .items() and maps unique id -> something can be used as the subgraph.\n\n    Names are like:\n        '{nodetype}.{target_package}.{target_name}'\n\n    You can use `None` for the package name as a wildcard.\n    \"\"\"\n    \n    # Split the target name into"
    },
    {
        "original": "def _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    count = 0\n    log.debug('Waiting for asynchronous operation to complete')\n    result = conn.get_operation_status(request_id)\n    while result.status == 'InProgress':\n        count = count + 1\n        if count > 120:\n            raise ValueError('Timed out waiting for asynchronous operation to complete.')\n        time.sleep(5)\n        result = conn.get_operation_status(request_id)\n\n    if result.status != 'Succeeded':\n        raise AzureException('Operation failed. {message} ({code})'\n                                .format(message=result.error.message,\n                                        code=result.error.code))",
        "sampled": "```python\nimport time\n\ndef _wait_for_async(conn, request_id):\n    \"\"\"\n    Helper function for azure tests\n    \"\"\"\n    \n    # Initialize a flag to track if the request is completed\n    is_completed = False\n    \n    # Set a timeout of 10 seconds to wait for the async operation to complete\n    start_time = time.time()\n    \n    while not is_completed and time.time() - start_time < 10:\n        try:\n            # Check if the request has completed"
    },
    {
        "original": "def has_in_collaborators(self, collaborator):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/collaborators/:user <http://developer.github.com/v3/repos/collaborators>`_\n        :param collaborator: string or :class:`github.NamedUser.NamedUser`\n        :rtype: bool\n        \"\"\"\n        assert isinstance(collaborator, github.NamedUser.NamedUser) or isinstance(collaborator, (str, unicode)), collaborator\n\n        if isinstance(collaborator, github.NamedUser.NamedUser):\n            collaborator = collaborator._identity\n\n        status, headers, data = self._requester.requestJson(\n            \"GET\",\n            self.url + \"/collaborators/\" + collaborator\n        )\n        return status == 204",
        "sampled": "```python\nimport requests\n\nclass GitHub:\n    def __init__(self, token):\n        self.token = token\n        self.headers = {'Authorization': f'token {token}'}\n\n    def has_in_collaborators(self, collaborator):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/collaborators/:user <http://developer.github.com/v3/repos/collaborators>`_\n        :param collaborator: string or :class:`github.NamedUser.Named"
    },
    {
        "original": "def getChatMembersCount(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#getchatmemberscount \"\"\"\n        p = _strip(locals())\n        return self._api_request('getChatMembersCount', _rectify(p))",
        "sampled": "```python\ndef getChatMembersCount(self, chat_id):\n    \"\"\"\n    Returns the number of members in a chat.\n\n    Args:\n        chat_id (int): Unique identifier for the target group or username of the target supergroup or channel.\n\n    Returns:\n        int: The number of members in the chat.\n    \"\"\"\n    \n    # This is a placeholder function and does not actually make an API call to Telegram\n    # In a real implementation, you would use a library like requests to"
    },
    {
        "original": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n        # Call the father's method\n        super(Plugin, self).update_views()\n\n        if 'containers' not in self.stats:\n            return False\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats['containers']:\n            # Init the views for the current container (key = container name)\n            self.views[i[self.get_key()]] = {'cpu': {}, 'mem': {}}\n            # CPU alert\n            if 'cpu' in i and 'total' in i['cpu']:\n                # Looking for specific CPU container threasold in the conf file\n                alert = self.get_alert(i['cpu']['total'],\n                                       header=i['name'] + '_cpu',\n                                       action_key=i['name'])\n                if alert == 'DEFAULT':\n                    # Not found ? Get back to default CPU threasold value\n                    alert = self.get_alert(i['cpu']['total'], header='cpu')\n                self.views[i[self.get_key()]]['cpu']['decoration'] = alert\n            # MEM alert\n            if 'memory' in i and 'usage' in i['memory']:\n                # Looking for specific MEM container threasold in the conf file\n                alert = self.get_alert(i['memory']['usage'],\n                                       maximum=i['memory']['limit'],\n                                       header=i['name'] + '_mem',\n                                       action_key=i['name'])\n                if alert == 'DEFAULT':\n                    # Not found ? Get back to default MEM threasold value\n                    alert = self.get_alert(i['memory']['usage'],\n                                           maximum=i['memory']['limit'],\n                                           header='mem')\n                self.views[i[self.get_key()]]['mem']['decoration'] = alert\n\n        return True",
        "sampled": "```python\nclass Solution:\n    def update_views(self):\n        \"\"\"\n        Update stats views.\n        \n        This method is expected to update the views of a system or application.\n        \n        Parameters:\n            None\n        \n        Returns:\n            None\n        \"\"\"\n        \n        # Assuming we have a dictionary to store the current views\n        self.views = {'view1': 0, 'view2': 0}\n        \n        # Update the views based on some logic (for example, incrementing by "
    },
    {
        "original": "def wait_for_task(task, instance_name, task_type, sleep_seconds=1, log_level='debug'):\n    \"\"\"\n    Waits for a task to be completed.\n\n    task\n        The task to wait for.\n\n    instance_name\n        The name of the ESXi host, vCenter Server, or Virtual Machine that\n        the task is being run on.\n\n    task_type\n        The type of task being performed. Useful information for debugging purposes.\n\n    sleep_seconds\n        The number of seconds to wait before querying the task again.\n        Defaults to ``1`` second.\n\n    log_level\n        The level at which to log task information. Default is ``debug``,\n        but ``info`` is also supported.\n    \"\"\"\n    time_counter = 0\n    start_time = time.time()\n    log.trace('task = %s, task_type = %s', task, task.__class__.__name__)\n    try:\n        task_info = task.info\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.FileNotFound as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareFileNotFoundError(exc.msg)\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    while task_info.state == 'running' or task_info.state == 'queued':\n        if time_counter % sleep_seconds == 0:\n            msg = '[ {0} ] Waiting for {1} task to finish [{2} s]'.format(\n                instance_name, task_type, time_counter)\n            if log_level == 'info':\n                log.info(msg)\n            else:\n                log.debug(msg)\n        time.sleep(1.0 - ((time.time() - start_time) % 1.0))\n        time_counter += 1\n        try:\n            task_info = task.info\n        except vim.fault.NoPermission as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(\n                'Not enough permissions. Required privilege: '\n                '{}'.format(exc.privilegeId))\n        except vim.fault.FileNotFound as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareFileNotFoundError(exc.msg)\n        except vim.fault.VimFault as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(exc.msg)\n        except vmodl.RuntimeFault as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    if task_info.state == 'success':\n        msg = '[ {0} ] Successfully completed {1} task in {2} seconds'.format(\n            instance_name, task_type, time_counter)\n        if log_level == 'info':\n            log.info(msg)\n        else:\n            log.debug(msg)\n        # task is in a successful state\n        return task_info.result\n    else:\n        # task is in an error state\n        try:\n            raise task_info.error\n        except vim.fault.NoPermission as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(\n                'Not enough permissions. Required privilege: '\n                '{}'.format(exc.privilegeId))\n        except vim.fault.FileNotFound as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareFileNotFoundError(exc.msg)\n        except vim.fault.VimFault as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareApiError(exc.msg)\n        except vmodl.fault.SystemError as exc:\n            log.exception(exc)\n            raise salt.exceptions.VMwareSystemError(exc.msg)\n        except vmodl.fault.InvalidArgument as exc:\n            log.exception(exc)\n            exc_message = exc.msg\n            if exc.faultMessage:\n                exc_message = '{0} ({1})'.format(exc_message,\n                                                 exc.faultMessage[0].message)\n            raise salt.exceptions.VMwareApiError(exc_message)",
        "sampled": "```python\nimport time\nimport logging\n\ndef wait_for_task(task, instance_name, task_type, sleep_seconds=1, log_level='debug'):\n    \"\"\"\n    Waits for a task to be completed.\n\n    Args:\n        task (object): The task to wait for.\n        instance_name (str): The name of the ESXi host, vCenter Server, or Virtual Machine that the task is being run on.\n        task_type (str): The type of task being performed. Useful"
    },
    {
        "original": "def cell_to_text(self):\n        \"\"\"Return the text representation of a cell\"\"\"\n        if self.cell_type == 'markdown':\n            # Is an explicit region required?\n            if self.metadata or self.cell_reader(self.fmt).read(self.source)[1] < len(self.source):\n                if self.metadata:\n                    region_start = ['<!-- #region']\n                    if 'title' in self.metadata and '{' not in self.metadata['title']:\n                        region_start.append(self.metadata.pop('title'))\n                    region_start.append(json.dumps(self.metadata))\n                    region_start.append('-->')\n                    region_start = ' '.join(region_start)\n                else:\n                    region_start = '<!-- #region -->'\n\n                return [region_start] + self.source + ['<!-- #endregion -->']\n            return self.source\n\n        return self.code_to_text()",
        "sampled": "```python\nclass Cell:\n    def __init__(self, value):\n        self.value = value\n\n    def cell_to_text(self):\n        return str(self.value)\n```"
    },
    {
        "original": "def request_location(cls, text, *,\n                         resize=None, single_use=None, selective=None):\n        \"\"\"\n        Creates a new button that will request\n        the user's location upon being clicked.\n\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\n        \"\"\"\n        return cls(types.KeyboardButtonRequestGeoLocation(text),\n                   resize=resize, single_use=single_use, selective=selective)",
        "sampled": "```python\nclass Button:\n    def request_location(self, text, *, resize=None, single_use=None, selective=None):\n        \"\"\"\n        Creates a new button that will request\n        the user's location upon being clicked.\n\n        ``resize``, ``single_use`` and ``selective`` are documented in `text`.\n        \"\"\"\n        \n        # Create a new button with the given text\n        button = {\n            \"text\": text,\n            \"type\": \"button\",\n            \"action\": {\n"
    },
    {
        "original": "def getGUA(self, filterByPrefix=None):\n        \"\"\"get expected global unicast IPv6 address of OpenThreadWpan\n\n        Args:\n            filterByPrefix: a given expected global IPv6 prefix to be matched\n\n        Returns:\n            a global IPv6 address\n        \"\"\"\n        print '%s call getGUA' % self.port\n        print filterByPrefix\n        globalAddrs = []\n        try:\n            # get global addrs set if multiple\n            globalAddrs = self.getGlobal()\n\n            if filterByPrefix is None:\n                return self.__padIp6Addr(globalAddrs[0])\n            else:\n                for line in globalAddrs:\n                    line = self.__padIp6Addr(line)\n                    print \"Padded IPv6 Address:\" + line\n                    if line.startswith(filterByPrefix):\n                        return line\n                print 'no global address matched'\n                return str(globalAddrs[0])\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger('getGUA() Error: ' + str(e))\n            return e",
        "sampled": "```python\nimport ipaddress\n\nclass OpenThreadWpan:\n    def __init__(self, prefix):\n        self.prefix = ipaddress.ip_network(prefix)\n\n    def getGUA(self, filterByPrefix=None):\n        \"\"\"\n        get expected global unicast IPv6 address of OpenThreadWpan\n\n        Args:\n            filterByPrefix: a given expected global IPv6 prefix to be matched\n\n        Returns:\n            a global IPv6 address\n        \"\"\"\n        \n        # Generate the first available IP"
    },
    {
        "original": "def GetAuditLogEntries(offset, now, token):\n  \"\"\"Return all audit log entries between now-offset and now.\n\n  Args:\n    offset: rdfvalue.Duration how far back to look in time\n    now: rdfvalue.RDFDatetime for current time\n    token: GRR access token\n  Yields:\n    AuditEvents created during the time range\n  \"\"\"\n  start_time = now - offset - audit.AUDIT_ROLLOVER_TIME\n\n  for fd in audit.LegacyAuditLogsForTimespan(start_time, now, token):\n    for event in fd.GenerateItems():\n      if now - offset < event.timestamp < now:\n        yield event",
        "sampled": "```python\nfrom grr.lib import rdfvalue\nfrom grr.server import aff4\n\ndef GetAuditLogEntries(offset, now, token):\n    # Convert offset to seconds\n    offset_in_seconds = int(offset)\n\n    # Calculate the start time of the query\n    start_time = now - rdfvalue.Duration(seconds=offset_in_seconds)\n\n    # Initialize an empty list to store audit log entries\n    audit_log_entries = []\n\n    # Query the AFF4 database for audit log entries between"
    },
    {
        "original": "def update_pricing(kwargs=None, call=None):\n    \"\"\"\n    Download most recent pricing information from AWS and convert to a local\n    JSON file.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_pricing my-ec2-config\n        salt-cloud -f update_pricing my-ec2-config type=linux\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n    sources = {\n        'linux': 'https://a0.awsstatic.com/pricing/1/ec2/linux-od.min.js',\n        'rhel': 'https://a0.awsstatic.com/pricing/1/ec2/rhel-od.min.js',\n        'sles': 'https://a0.awsstatic.com/pricing/1/ec2/sles-od.min.js',\n        'mswin': 'https://a0.awsstatic.com/pricing/1/ec2/mswin-od.min.js',\n        'mswinsql': 'https://a0.awsstatic.com/pricing/1/ec2/mswinSQL-od.min.js',\n        'mswinsqlweb': 'https://a0.awsstatic.com/pricing/1/ec2/mswinSQLWeb-od.min.js',\n    }\n\n    if kwargs is None:\n        kwargs = {}\n\n    if 'type' not in kwargs:\n        for source in sources:\n            _parse_pricing(sources[source], source)\n    else:\n        _parse_pricing(sources[kwargs['type']], kwargs['type'])",
        "sampled": "```python\nimport json\nimport requests\n\ndef update_pricing(kwargs=None, call=None):\n    \"\"\"\n    Download most recent pricing information from AWS and convert to a local\n    JSON file.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f update_pricing my-ec2-config\n        salt-cloud -f update_pricing my-ec2-config type=linux\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n\n    # Define the"
    },
    {
        "original": "def _get_fname_len(self, bufflen=128):\n        \"\"\"Returns the number of bytes designated for the filename.\"\"\"\n        buff = self.meta.peek(bufflen)\n        strlen = buff.find('\\0')\n        for i, b in enumerate(buff[strlen:]):\n            if b != '\\0':\n                return strlen+i\n        return bufflen",
        "sampled": "```python\nclass FileFormatting:\n    def _get_fname_len(self, bufflen=128):\n        \"\"\"\n        Returns the number of bytes designated for the filename.\n        \n        Parameters:\n        bufflen (int): The total buffer length. Defaults to 128.\n        \n        Returns:\n        int: The number of bytes for the filename.\n        \"\"\"\n        \n        if isinstance(bufflen, int) and bufflen >= 0:\n            # Calculate filename size using one byte per character\n            fname_len"
    },
    {
        "original": "def get_selections(pattern=None, state=None):\n    \"\"\"\n    View package state from the dpkg database.\n\n    Returns a dict of dicts containing the state, and package names:\n\n    .. code-block:: python\n\n        {'<host>':\n            {'<state>': ['pkg1',\n                         ...\n                        ]\n            },\n            ...\n        }\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.get_selections\n        salt '*' pkg.get_selections 'python-*'\n        salt '*' pkg.get_selections state=hold\n        salt '*' pkg.get_selections 'openssh*' state=hold\n    \"\"\"\n    ret = {}\n    cmd = ['dpkg', '--get-selections']\n    cmd.append(pattern if pattern else '*')\n    stdout = __salt__['cmd.run_stdout'](cmd,\n                                        output_loglevel='trace',\n                                        python_shell=False)\n    ret = _parse_selections(stdout)\n    if state:\n        return {state: ret.get(state, [])}\n    return ret",
        "sampled": "```python\nimport subprocess\n\ndef get_selections(pattern=None, state=None):\n    \"\"\"\n    View package state from the dpkg database.\n\n    Returns a dict of dicts containing the state, and package names:\n\n    .. code-block:: python\n\n        {'<host>':\n            {'<state>': ['pkg1',\n                         ...\n                        ]\n            },\n            ...\n        }\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.get_selections\n        salt '*' pkg.get_selection"
    },
    {
        "original": "def _set_conf_sockets():\n    \"\"\"Populate the conf.L2Socket and conf.L3Socket\n    according to the various use_* parameters\n    \"\"\"\n    if conf.use_bpf and not BSD:\n        Interceptor.set_from_hook(conf, \"use_bpf\", False)\n        raise ScapyInvalidPlatformException(\"BSD-like (OSX, *BSD...) only !\")\n    if conf.use_winpcapy and not WINDOWS:\n        Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n        raise ScapyInvalidPlatformException(\"Windows only !\")\n    # we are already in an Interceptor hook, use Interceptor.set_from_hook\n    if conf.use_pcap or conf.use_dnet or conf.use_winpcapy:\n        try:\n            from scapy.arch.pcapdnet import L2pcapListenSocket, L2pcapSocket, \\\n                L3pcapSocket\n        except ImportError:\n            warning(\"No pcap provider available ! pcap won't be used\")\n            Interceptor.set_from_hook(conf, \"use_winpcapy\", False)\n            Interceptor.set_from_hook(conf, \"use_pcap\", False)\n        else:\n            conf.L3socket = L3pcapSocket\n            conf.L3socket6 = functools.partial(L3pcapSocket, filter=\"ip6\")\n            conf.L2socket = L2pcapSocket\n            conf.L2listen = L2pcapListenSocket\n            return\n    if conf.use_bpf:\n        from scapy.arch.bpf.supersocket import L2bpfListenSocket, \\\n            L2bpfSocket, L3bpfSocket\n        conf.L3socket = L3bpfSocket\n        conf.L3socket6 = functools.partial(L3bpfSocket, filter=\"ip6\")\n        conf.L2socket = L2bpfSocket\n        conf.L2listen = L2bpfListenSocket\n        return\n    if LINUX:\n        from scapy.arch.linux import L3PacketSocket, L2Socket, L2ListenSocket\n        conf.L3socket = L3PacketSocket\n        conf.L3socket6 = functools.partial(L3PacketSocket, filter=\"ip6\")\n        conf.L2socket = L2Socket\n        conf.L2listen = L2ListenSocket\n        return\n    if WINDOWS:\n        from scapy.arch.windows import _NotAvailableSocket\n        from scapy.arch.windows.native import L3WinSocket, L3WinSocket6\n        conf.L3socket = L3WinSocket\n        conf.L3socket6 = L3WinSocket6\n        conf.L2socket = _NotAvailableSocket\n        conf.L2listen = _NotAvailableSocket\n        return\n    from scapy.supersocket import L3RawSocket\n    from scapy.layers.inet6 import L3RawSocket6\n    conf.L3socket = L3RawSocket\n    conf.L3socket6 = L3RawSocket6",
        "sampled": "```python\ndef _set_conf_sockets():\n    \"\"\"\n    Populate the conf.L2Socket and conf.L3Socket \n    according to the various use_* parameters\n    \"\"\"\n\n    # Initialize an empty dictionary for L2 sockets\n    l2_sockets = {}\n\n    # Initialize an empty dictionary for L3 sockets\n    l3_sockets = {'LGA1200': None, 'LGA2066': None}\n\n    # Define CPUs and their respective socket types in LGA formats"
    },
    {
        "original": "def get_lattice_quanta(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n        \"\"\"\n        Returns the dipole / polarization quanta along a, b, and c for\n        all structures.\n        \"\"\"\n        lattices = [s.lattice for s in self.structures]\n        volumes = np.array([s.lattice.volume for s in self.structures])\n\n        L = len(self.structures)\n\n        e_to_muC = -1.6021766e-13\n        cm2_to_A2 = 1e16\n        units = 1.0 / np.array(volumes)\n        units *= e_to_muC * cm2_to_A2\n\n        # convert polarizations and lattice lengths prior to adjustment\n        if convert_to_muC_per_cm2 and not all_in_polar:\n            # adjust lattices\n            for i in range(L):\n                lattice = lattices[i]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[i], a)\n        elif convert_to_muC_per_cm2 and all_in_polar:\n            for i in range(L):\n                lattice = lattices[-1]\n                l, a = lattice.lengths_and_angles\n                lattices[i] = Lattice.from_lengths_and_angles(\n                    np.array(l) * units.ravel()[-1], a)\n\n        quanta = np.array(\n            [np.array(l.lengths_and_angles[0]) for l in lattices])\n\n        return quanta",
        "sampled": "```python\nimport numpy as np\n\ndef get_lattice_quanta(self, convert_to_muC_per_cm2=True, all_in_polar=True):\n    \"\"\"\n    Returns the dipole / polarization quanta along a, b, and c for\n    all structures.\n    \n    Parameters:\n    ----------\n        convert_to_muC_per_cm2 : bool (default: True)\n            Convert to micro C/cm^2 if True, otherwise return in C/m^2.\n        all_in_polar"
    },
    {
        "original": "def tgread_bytes(self):\n        \"\"\"\n        Reads a Telegram-encoded byte array, without the need of\n        specifying its length.\n        \"\"\"\n        first_byte = self.read_byte()\n        if first_byte == 254:\n            length = self.read_byte() | (self.read_byte() << 8) | (\n                self.read_byte() << 16)\n            padding = length % 4\n        else:\n            length = first_byte\n            padding = (length + 1) % 4\n\n        data = self.read(length)\n        if padding > 0:\n            padding = 4 - padding\n            self.read(padding)\n\n        return data",
        "sampled": "```python\nimport struct\n\nclass TGReader:\n    def __init__(self, byte_array):\n        self.byte_array = byte_array\n        self.index = 0\n\n    def tgread_bytes(self, length):\n        \"\"\"\n        Reads a Telegram-encoded byte array, without the need of \n        specifying its length.\n        \n        This method reads the specified number of bytes and \n        pad them with null bytes if necessary.\n        \n        :param length: The number of bytes to read.\n        \n       "
    },
    {
        "original": "def get(self, element, selected=False):\n        \"\"\"\n        Returns the curses attribute code for the given element.\n        \"\"\"\n        if self._attribute_map is None:\n            raise RuntimeError('Attempted to access theme attribute before '\n                               'calling initialize_curses_theme()')\n\n        if selected or self._selected:\n            element = '@{0}'.format(element)\n\n        return self._attribute_map[element]",
        "sampled": "```python\ndef get(self, element, selected=False):\n    \"\"\"\n    Returns the curses attribute code for the given element.\n    \n    Args:\n        element (str): The type of element.\n        selected (bool): Whether the element is selected. Defaults to False.\n    \n    Returns:\n        int: The curses attribute code for the given element.\n    \"\"\"\n    \n    # Define a dictionary to map elements to their corresponding curses attribute codes\n    attributes = {\n        'bold': 1,\n       "
    },
    {
        "original": "def ReadHuntOutputPluginLogEntries(self,\n                                     hunt_id,\n                                     output_plugin_id,\n                                     offset,\n                                     count,\n                                     with_type=None,\n                                     cursor=None):\n    \"\"\"Reads hunt output plugin log entries.\"\"\"\n    query = (\"SELECT client_id, flow_id, log_entry_type, message, \"\n             \"UNIX_TIMESTAMP(timestamp) \"\n             \"FROM flow_output_plugin_log_entries \"\n             \"FORCE INDEX (flow_output_plugin_log_entries_by_hunt) \"\n             \"WHERE hunt_id = %s AND output_plugin_id = %s \")\n    args = [\n        db_utils.HuntIDToInt(hunt_id),\n        db_utils.OutputPluginIDToInt(output_plugin_id)\n    ]\n\n    if with_type is not None:\n      query += \"AND log_entry_type = %s \"\n      args.append(int(with_type))\n\n    query += \"ORDER BY log_id ASC LIMIT %s OFFSET %s\"\n    args.append(count)\n    args.append(offset)\n\n    cursor.execute(query, args)\n\n    ret = []\n    for (client_id_int, flow_id_int, log_entry_type, message,\n         timestamp) in cursor.fetchall():\n      ret.append(\n          rdf_flow_objects.FlowOutputPluginLogEntry(\n              hunt_id=hunt_id,\n              client_id=db_utils.IntToClientID(client_id_int),\n              flow_id=db_utils.IntToFlowID(flow_id_int),\n              output_plugin_id=output_plugin_id,\n              log_entry_type=log_entry_type,\n              message=message,\n              timestamp=mysql_utils.TimestampToRDFDatetime(timestamp)))\n\n    return ret",
        "sampled": "```python\nimport io\n\nclass HuntOutputPluginLogReader:\n    def __init__(self):\n        self.log_entries = []\n        # assume a mock log file for demonstration purposes\n\n    def read_log_file(self, filename):\n        with io.open(filename, 'r', encoding='utf-8') as f:\n            self.log_entries = [line.strip() for line in f]\n\ndef ReadHuntOutputPluginLogEntries(self,\n                                hunt_id,\n                                output_plugin_id,\n                                offset,\n"
    },
    {
        "original": "def list_nodes_full(mask='mask[id, hostname, primaryIpAddress, \\\n        primaryBackendIpAddress, processorPhysicalCoreAmount, memoryCount]',\n        call=None):\n    \"\"\"\n    Return a list of the VMs that are on the provider\n    \"\"\"\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes_full function must be called with -f or --function.'\n        )\n\n    ret = {}\n    conn = get_conn(service='SoftLayer_Account')\n    response = conn.getHardware(mask=mask)\n\n    for node in response:\n        ret[node['hostname']] = node\n    __utils__['cloud.cache_node_list'](ret, __active_provider_name__.split(':')[0], __opts__)\n    return ret",
        "sampled": "```python\ndef list_nodes_full(mask='mask[id, hostname, primaryIpAddress, primaryBackendIpAddress, processorPhysicalCoreAmount, memoryCount]',\n                    call=None):\n    \"\"\"\n    Return a list of the VMs that are on the provider\n    \"\"\"\n    \n    # Assuming we have a function to get all VMs from the provider\n    def get_vms():\n        # This is a placeholder for your actual implementation to get all VMs from the provider\n        return [\n            {'id':"
    },
    {
        "original": "def get_qubits(self):\n        \"\"\"\n        The support of all the operators in the PauliSum object.\n\n        :returns: A list of all the qubits in the sum of terms.\n        :rtype: list\n        \"\"\"\n        return list(set().union(*[term.get_qubits() for term in self.terms]))",
        "sampled": "```python\nclass PauliSum:\n    def __init__(self, terms):\n        self.terms = terms\n\n    def get_qubits(self):\n        \"\"\"\n        The support of all the operators in the PauliSum object.\n\n        :returns: A list of all the qubits in the sum of terms.\n        :rtype: list\n        \"\"\"\n        qubits = set()\n        \n        for term in self.terms:\n            # Assuming term is a dictionary with 'qubits' as"
    },
    {
        "original": "def _retrieve_config_xml(config_xml, saltenv):\n    \"\"\"\n    Helper to cache the config XML and raise a CommandExecutionError if we fail\n    to do so. If we successfully cache the file, return the cached path.\n    \"\"\"\n    ret = __salt__['cp.cache_file'](config_xml, saltenv)\n\n    if not ret:\n        raise CommandExecutionError('Failed to retrieve {0}'.format(config_xml))\n\n    return ret",
        "sampled": "```python\ndef _retrieve_config_xml(config_xml, saltenv):\n    \"\"\"\n    Helper to cache the config XML and raise a CommandExecutionError if we fail\n    to do so. If we successfully cache the file, return the cached path.\n    \"\"\"\n\n\ndef create_cache_file(config_xml):\n    # Define an empty dictionary to store cached config XMLs\n    config_cache = {}\n\n    # If a path exists for this key in our cache then just use it rather than\n    #"
    },
    {
        "original": "def get_native_type(self, java_type):\n        \"\"\"\n        Maps the Java type to a SimTypeReg representation of its native\n        counterpart. This type can be used to indicate the (well-defined) size\n        of native JNI types.\n\n        :return: A SymTypeReg with the JNI size of the given type.\n        \"\"\"\n        if java_type in ArchSoot.sizeof.keys():\n            jni_type_size = ArchSoot.sizeof[java_type]\n        else:\n            # if it's not a primitive type, we treat it as a reference\n            jni_type_size = self.native_simos.arch.bits\n        return SimTypeReg(size=jni_type_size)",
        "sampled": "```python\nclass SymTypeReg:\n    def __init__(self, name, size):\n        self.name = name\n        self.size = size\n\nclass SimTypeReg:\n    def __init__(self, size):\n        self.size = size\n\ndef get_native_type(self, java_type):\n    \"\"\"\n    Maps the Java type to a SimTypeReg representation of its native\n    counterpart. This type can be used to indicate the (well-defined) size\n    of native JNI types.\n\n"
    },
    {
        "original": "def prepare_function_symbol(self, symbol_name, basic_addr=None):\n        \"\"\"\n        Prepare the address space with the data necessary to perform relocations pointing to the given symbol.\n\n        Returns a 2-tuple. The first item is the address of the function code, the second is the address of the\n        relocation target.\n        \"\"\"\n        if self.project.loader.main_object.is_ppc64_abiv1:\n            if basic_addr is not None:\n                pointer = self.project.loader.memory.unpack_word(basic_addr)\n                return pointer, basic_addr\n\n            pseudo_hookaddr = self.project.loader.extern_object.get_pseudo_addr(symbol_name)\n            pseudo_toc = self.project.loader.extern_object.allocate(size=0x18)\n            self.project.loader.extern_object.memory.pack_word(\n                AT.from_mva(pseudo_toc, self.project.loader.extern_object).to_rva(), pseudo_hookaddr)\n            return pseudo_hookaddr, pseudo_toc\n        else:\n            if basic_addr is None:\n                basic_addr = self.project.loader.extern_object.get_pseudo_addr(symbol_name)\n            return basic_addr, basic_addr",
        "sampled": "```python\nclass AddressSpace:\n    def __init__(self):\n        self.base_address = 0\n        self.function_addresses = {}\n        self.relocation_targets = {}\n\n    def prepare_function_symbol(self, symbol_name, basic_addr=None):\n        \"\"\"\n        Prepare the address space with the data necessary to perform relocations pointing to the given symbol.\n\n        Returns a 2-tuple. The first item is the address of the function code, the second is the address of\n         relocation target.\n"
    },
    {
        "original": "def ensure_topic(self):\n        \"\"\"Verify the pub/sub topic exists.\n\n        Returns the topic qualified name.\n        \"\"\"\n        client = self.session.client('pubsub', 'v1', 'projects.topics')\n        topic = self.get_topic_param()\n        try:\n            client.execute_command('get', {'topic': topic})\n        except HttpError as e:\n            if e.resp.status != 404:\n                raise\n        else:\n            return topic\n\n        # bug in discovery doc.. apis say body must be empty but its required in the\n        # discovery api for create.\n        client.execute_command('create', {'name': topic, 'body': {}})\n        return topic",
        "sampled": "```python\nclass PubSubSystem:\n    def __init__(self, projects, topic_pattern):\n        self.projects = projects\n        self.topic_pattern = topic_pattern\n\n    def ensure_topic(self):\n        \"\"\"\n        Verify the pub/sub topic exists.\n        \n        Returns the topic qualified name.\n        \n        Assuming each project has a setup function with a setup_topics method that handles the \n                    generation and validation of topics to avoid infinite loop when multiple setups occur.\n        \n          We may will need various Renderers"
    },
    {
        "original": "def send_template_message(self, user_id, template_id, data, form_id, page=None, color=None, emphasis_keyword=None):\n        \"\"\"\n        \u53d1\u9001\u6a21\u677f\u6d88\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/debug/wxadoc/dev/api/notice.html\n        \"\"\"\n        tpl_data = optionaldict(\n            touser=user_id,\n            template_id=template_id,\n            page=page,\n            form_id=form_id,\n            data=data,\n            color=color,\n            emphasis_keyword=emphasis_keyword,\n        )\n        return self._post(\n            'cgi-bin/message/wxopen/template/send',\n            data=tpl_data\n        )",
        "sampled": "```python\nimport requests\n\nclass WeChatAPI:\n    def send_template_message(self, user_id, template_id, data, form_id, page=None, color=None, emphasis_keyword=None):\n        \"\"\"\n        \u53d1\u9001\u6a21\u677f\u6d88\u606f\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://mp.weixin.qq.com/debug/wxadoc/dev/api/notice.html\n        \"\"\"\n\n        # Construct the request URL for sending template message\n        url = 'https://api.weixin.qq.com/cgi-bin/message/template/subscribe/send"
    },
    {
        "original": "def bake(self):\n        \"\"\"\n        Bake an ``ansible-playbook`` command so it's ready to execute and\n        returns ``None``.\n\n        :return: None\n        \"\"\"\n        # Pass a directory as inventory to let Ansible merge the multiple\n        # inventory sources located under\n        self.add_cli_arg('inventory',\n                         self._config.provisioner.inventory_directory)\n        options = util.merge_dicts(self._config.provisioner.options, self._cli)\n        verbose_flag = util.verbose_flag(options)\n        if self._playbook != self._config.provisioner.playbooks.converge:\n            if options.get('become'):\n                del options['become']\n\n        self._ansible_command = sh.ansible_playbook.bake(\n            options,\n            self._playbook,\n            *verbose_flag,\n            _cwd=self._config.scenario.directory,\n            _env=self._env,\n            _out=self._out,\n            _err=self._err)\n\n        ansible_args = (list(self._config.provisioner.ansible_args) + list(\n            self._config.ansible_args))\n\n        if ansible_args:\n            if self._config.action not in ['create', 'destroy']:\n                self._ansible_command = self._ansible_command.bake(\n                    ansible_args)",
        "sampled": "```python\nimport subprocess\n\nclass Ansible:\n    def __init__(self, playbook_path):\n        self.playbook_path = playbook_path\n\n    def bake(self):\n        \"\"\"\n        Bake an ``ansible-playbook`` command so it's ready to execute and\n        returns the output as a string.\n\n        :return: The output of the ansible-playbook command\n        \"\"\"\n        \n                \n        # Create the ansible-playbook command with -i on localhost option and input file at given path\n         \n"
    },
    {
        "original": "def cluster_nodes(self, tol=0.2):\n        \"\"\"\n        Cluster nodes that are too close together using a tol.\n\n        Args:\n            tol (float): A distance tolerance. PBC is taken into account.\n        \"\"\"\n        lattice = self.structure.lattice\n\n        vfcoords = [v.frac_coords for v in self.vnodes]\n\n        # Manually generate the distance matrix (which needs to take into\n        # account PBC.\n        dist_matrix = np.array(lattice.get_all_distances(vfcoords, vfcoords))\n        dist_matrix = (dist_matrix + dist_matrix.T) / 2\n        for i in range(len(dist_matrix)):\n            dist_matrix[i, i] = 0\n        condensed_m = squareform(dist_matrix)\n        z = linkage(condensed_m)\n        cn = fcluster(z, tol, criterion=\"distance\")\n        merged_vnodes = []\n        for n in set(cn):\n            poly_indices = set()\n            frac_coords = []\n            for i, j in enumerate(np.where(cn == n)[0]):\n                poly_indices.update(self.vnodes[j].polyhedron_indices)\n                if i == 0:\n                    frac_coords.append(self.vnodes[j].frac_coords)\n                else:\n                    fcoords = self.vnodes[j].frac_coords\n                    # We need the image to combine the frac_coords properly.\n                    d, image = lattice.get_distance_and_image(frac_coords[0],\n                                                              fcoords)\n                    frac_coords.append(fcoords + image)\n            merged_vnodes.append(\n                VoronoiPolyhedron(lattice, np.average(frac_coords, axis=0),\n                                  poly_indices, self.coords))\n        self.vnodes = merged_vnodes\n        logger.debug(\"%d vertices after combination.\" % len(self.vnodes))",
        "sampled": "```python\nimport numpy as np\n\ndef distance(self, node1, node2):\n    \"\"\"\n    Calculate the distance between two nodes considering PBC.\n\n    Args:\n        node1 (tuple): Coordinates of the first node.\n        node2 (tuple): Coordinates of the second node.\n\n    Returns:\n        float: Distance between the two nodes.\n    \"\"\"\n    x = abs(node1[0] - node2[0])\n    y = abs(node1[1] - node2["
    },
    {
        "original": "def create_v4flowspec_actions(actions=None):\n    \"\"\"\n    Create list of traffic filtering actions\n    for Ipv4 Flow Specification and VPNv4 Flow Specification.\n\n    `` actions`` specifies Traffic Filtering Actions of\n    Flow Specification as a dictionary type value.\n\n    Returns a list of extended community values.\n    \"\"\"\n    from ryu.services.protocols.bgp.api.prefix import (\n        FLOWSPEC_ACTION_TRAFFIC_RATE,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION,\n        FLOWSPEC_ACTION_REDIRECT,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING,\n    )\n\n    # Supported action type for IPv4 and VPNv4.\n    action_types = {\n        FLOWSPEC_ACTION_TRAFFIC_RATE: BGPFlowSpecTrafficRateCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION: BGPFlowSpecTrafficActionCommunity,\n        FLOWSPEC_ACTION_REDIRECT: BGPFlowSpecRedirectCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING: BGPFlowSpecTrafficMarkingCommunity,\n    }\n\n    return _create_actions(actions, action_types)",
        "sampled": "```python\ndef create_v4flowspec_actions(actions=None):\n    \"\"\"\n    Create list of traffic filtering actions\n    for Ipv4 Flow Specification and VPNv4 Flow Specification.\n\n    `` actions`` specifies Traffic Filtering Actions of\n    Flow Specification as a dictionary type value.\n\n    Returns a list of extended community values.\n    \"\"\"\n\n    # Default action identifier if not specified\n    default_action = {\n        'dest-async': False,\n        'dst-addr': {'ipv4 addr':"
    },
    {
        "original": "def from_dict(input_dict):\n        \"\"\"\n        Instantiate an object of a derived class using the information\n        in input_dict (built by the to_dict method of the derived class).\n        More specifically, after reading the derived class from input_dict,\n        it calls the method _build_from_input_dict of the derived class.\n        Note: This method should not be overrided in the derived class. In case\n        it is needed, please override _build_from_input_dict instate.\n\n        :param dict input_dict: Dictionary with all the information needed to\n           instantiate the object.\n        \"\"\"\n\n        import copy\n        input_dict = copy.deepcopy(input_dict)\n        inference_class = input_dict.pop('class')\n        import GPy\n        inference_class = eval(inference_class)\n        return inference_class._build_from_input_dict(inference_class, input_dict)",
        "sampled": "```python\nclass BaseClass:\n    def __init__(self, name, age):\n        \"\"\"\n        Initializes a base class object with name and age attributes.\n\n        :param str name: The name of the object.\n        :param int age: The age of the object.\n        \"\"\"\n        self.name = name\n        self.age = age\n\n    def _build_from_input_dict(self, input_dict):\n        \"\"\"\n        Builds a BaseClass object from an input dictionary.\n\n        This method is called"
    },
    {
        "original": "def depolarizing_operators(p):\n    \"\"\"\n    Return the phase damping Kraus operators\n    \"\"\"\n    k0 = np.sqrt(1.0 - p) * I\n    k1 = np.sqrt(p / 3.0) * X\n    k2 = np.sqrt(p / 3.0) * Y\n    k3 = np.sqrt(p / 3.0) * Z\n    return k0, k1, k2, k3",
        "sampled": "depolarizing_operators\n======================\n\n### Problem Description\n\nThe depolarizing channel is a model in quantum error correction. It is used to simulate the effects of decoherence on quantum systems. The phase damping kraus operators can be represented as follows:\n\n$$D_0 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}, D_1 = D_2 = D_3 = p^{1/2} \\begin{"
    },
    {
        "original": "def input_on_stderr(prompt='', default=None, convert=None):\n    \"\"\"Output a string to stderr and wait for input.\n\n    Args:\n        prompt (str): the message to display.\n        default: the default value to return if the user\n            leaves the field empty\n        convert (callable): a callable to be used to convert\n            the value the user inserted. If None, the type of\n            ``default`` will be used.\n    \"\"\"\n\n    print(prompt, end='', file=sys.stderr)\n    value = builtins.input()\n    return _convert(value, default, convert)",
        "sampled": "```python\nimport math\n\ndef input_on_stderr(prompt='', default=None, convert=None):\n    \"\"\"Output a string to stderr and wait for input.\n\n    Args:\n        prompt (str): the message to display.\n        default: the default value to return if the user\n            leaves the field empty\n        convert (callable): a callable to be used to convert\n            the value the user inserted. If None, the type of\n            ``default`` will be used.\n    \"\"\" \n\n\ndef"
    },
    {
        "original": "def BuildCloudMetadataRequests():\n  \"\"\"Build the standard set of cloud metadata to collect during interrogate.\"\"\"\n  amazon_collection_map = {\n      \"/\".join((AMAZON_URL_BASE, \"instance-id\")): \"instance_id\",\n      \"/\".join((AMAZON_URL_BASE, \"ami-id\")): \"ami_id\",\n      \"/\".join((AMAZON_URL_BASE, \"hostname\")): \"hostname\",\n      \"/\".join((AMAZON_URL_BASE, \"public-hostname\")): \"public_hostname\",\n      \"/\".join((AMAZON_URL_BASE, \"instance-type\")): \"instance_type\",\n  }\n  google_collection_map = {\n      \"/\".join((GOOGLE_URL_BASE, \"instance/id\")): \"instance_id\",\n      \"/\".join((GOOGLE_URL_BASE, \"instance/zone\")): \"zone\",\n      \"/\".join((GOOGLE_URL_BASE, \"project/project-id\")): \"project_id\",\n      \"/\".join((GOOGLE_URL_BASE, \"instance/hostname\")): \"hostname\",\n      \"/\".join((GOOGLE_URL_BASE, \"instance/machine-type\")): \"machine_type\",\n  }\n\n  return CloudMetadataRequests(requests=_MakeArgs(amazon_collection_map,\n                                                  google_collection_map))",
        "sampled": "```python\ndef BuildCloudMetadataRequests():\n    \"\"\"\n    Build the standard set of cloud metadata to collect during interrogate.\n    \n    Returns:\n        dict: A dictionary containing the standard set of cloud metadata.\n    \"\"\"\n    \n    # Initialize an empty dictionary to store the cloud metadata\n    cloud_metadata = {}\n    \n    # Add instance metadata\n    instance_metadata = {\n        'instance_id': 'i-0123456789abcdef0',\n        'instance_type': 't2.micro',\n       "
    },
    {
        "original": "def send(self, message_type, data, connection_id, callback=None,\n             one_way=False):\n        \"\"\"\n        Send a message of message_type\n        :param connection_id: the identity for the connection to send to\n        :param message_type: validator_pb2.Message.* enum value\n        :param data: bytes serialized protobuf\n        :return: future.Future\n        \"\"\"\n        if connection_id not in self._connections:\n            raise ValueError(\"Unknown connection id: {}\".format(connection_id))\n        connection_info = self._connections.get(connection_id)\n        if connection_info.connection_type == \\\n                ConnectionType.ZMQ_IDENTITY:\n            message = validator_pb2.Message(\n                correlation_id=_generate_id(),\n                content=data,\n                message_type=message_type)\n\n            timer_tag = get_enum_name(message.message_type)\n            timer_ctx = self._get_send_response_timer(timer_tag).time()\n            fut = future.Future(\n                message.correlation_id,\n                message.content,\n                callback,\n                timeout=self._connection_timeout,\n                timer_ctx=timer_ctx)\n            if not one_way:\n                self._futures.put(fut)\n\n            self._send_receive_thread.send_message(msg=message,\n                                                   connection_id=connection_id)\n            return fut\n\n        return connection_info.connection.send(\n            message_type,\n            data,\n            callback=callback,\n            one_way=one_way)",
        "sampled": "```python\nimport validators\n\nclass MessageSender:\n    def send(self, message_type, data, connection_id, callback=None):\n        \"\"\"\n        Send a message of message_type\n        :param connection_id: the identity for the connection to send to\n        :param message_type: validators_validator_pb2.Message enum value\n        :param data: bytes serialized protobuf\n        :return: future.Future (representing the sent task)\n        \n        For simplicity in this example, we assume that sending a"
    },
    {
        "original": "def _f_preactivations(self, X):\n        \"\"\"Computes the network preactivations, i.e. the results of all intermediate linear layers before applying the\n        activation function on them\n        :param X: input data\n        :return: list of preactivations [X, XW+b, f(XW+b)W+b, ...]\n        \"\"\"\n\n        preactivations_list = list()\n        net = X\n        preactivations_list.append(X)\n\n        for W, b, i in zip(self.W_list, self.b_list, np.arange(len(self.W_list))):\n            net = np.dot(net, W)\n            net = net + b\n            if i < len(self.W_list) - 1:\n                preactivations_list.append(net)\n                net = self.act(net)\n        return preactivations_list",
        "sampled": "```python\nimport numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, weights, biases):\n        \"\"\"\n        :param weights: list of weight matrices\n        :param biases: list of bias vectors\n        \"\"\"\n        self.weights = weights\n        self.biases = biases\n\n    def _f_preactivations(self, X):\n        \"\"\"\n        Computes the network preactivations, i.e. the results of all intermediate linear layers before applying the \n        activation function on"
    },
    {
        "original": "def rels_xml_for(self, source_uri):\n        \"\"\"\n        Return rels item XML for source with *source_uri* or None if no rels\n        item is present.\n        \"\"\"\n        try:\n            rels_xml = self.blob_for(source_uri.rels_uri)\n        except KeyError:\n            rels_xml = None\n        return rels_xml",
        "sampled": "```python\nfrom typing import Optional\n\nclass Solution:\n    def rels_xml_for(self, source_uri: str) -> Optional[str]:\n        # Your solution implementation here\n        # In this case, I assume that you know how to construct the XML string from the source_uri\n        # For simplicity, let's assume we have a function called construct_rels_item_xml that takes the source_uri as input and returns the XML string\n        if source_uri:\n            xml_string = self.construct_re"
    },
    {
        "original": "def filter_results(source, results, aggressive):\n    \"\"\"Filter out spurious reports from pycodestyle.\n\n    If aggressive is True, we allow possibly unsafe fixes (E711, E712).\n\n    \"\"\"\n    non_docstring_string_line_numbers = multiline_string_lines(\n        source, include_docstrings=False)\n    all_string_line_numbers = multiline_string_lines(\n        source, include_docstrings=True)\n\n    commented_out_code_line_numbers = commented_out_code_lines(source)\n\n    has_e901 = any(result['id'].lower() == 'e901' for result in results)\n\n    for r in results:\n        issue_id = r['id'].lower()\n\n        if r['line'] in non_docstring_string_line_numbers:\n            if issue_id.startswith(('e1', 'e501', 'w191')):\n                continue\n\n        if r['line'] in all_string_line_numbers:\n            if issue_id in ['e501']:\n                continue\n\n        # We must offset by 1 for lines that contain the trailing contents of\n        # multiline strings.\n        if not aggressive and (r['line'] + 1) in all_string_line_numbers:\n            # Do not modify multiline strings in non-aggressive mode. Remove\n            # trailing whitespace could break doctests.\n            if issue_id.startswith(('w29', 'w39')):\n                continue\n\n        if aggressive <= 0:\n            if issue_id.startswith(('e711', 'e72', 'w6')):\n                continue\n\n        if aggressive <= 1:\n            if issue_id.startswith(('e712', 'e713', 'e714')):\n                continue\n\n        if aggressive <= 2:\n            if issue_id.startswith(('e704')):\n                continue\n\n        if r['line'] in commented_out_code_line_numbers:\n            if issue_id.startswith(('e26', 'e501')):\n                continue\n\n        # Do not touch indentation if there is a token error caused by\n        # incomplete multi-line statement. Otherwise, we risk screwing up the\n        # indentation.\n        if has_e901:\n            if issue_id.startswith(('e1', 'e7')):\n                continue\n\n        yield r",
        "sampled": "```python\ndef filter_results(source, results, aggressive):\n    \"\"\"\n    Filter out spurious reports from pycodestyle.\n\n    If aggressive is True, we allow possibly unsafe fixes (E711, E712).\n\n    Parameters:\n    source (str): The source code to be checked.\n    results (list): A list of tuples containing the line number and the error message.\n    aggressive (bool): Whether to allow possibly unsafe fixes.\n\n    Returns:\n    list: A filtered list of tuples"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'score') and self.score is not None:\n            _dict['score'] = self.score\n        if hasattr(self, 'confidence') and self.confidence is not None:\n            _dict['confidence'] = self.confidence\n        return _dict",
        "sampled": "```python\nimport json\n\nclass Solution:\n    def maxSubarraySum(self, nums: list[int]) -> int:\n        \"\"\"Return the maximum subarray sum of the given array.\"\"\"\n        if not nums:\n            return 0\n        \n        max_current = max_global = nums[0]\n        \n        for num in nums[1:]:\n            max_current = max(num, max_current + num)\n            if max_current > max_global:\n                max_global = max_current\n        \n        return int(max_global"
    },
    {
        "original": "def _send_offset_fetch_request(self, partitions):\n        \"\"\"Fetch the committed offsets for a set of partitions.\n\n        This is a non-blocking call. The returned future can be polled to get\n        the actual offsets returned from the broker.\n\n        Arguments:\n            partitions (list of TopicPartition): the partitions to fetch\n\n        Returns:\n            Future: resolves to dict of offsets: {TopicPartition: int}\n        \"\"\"\n        assert self.config['api_version'] >= (0, 8, 1), 'Unsupported Broker API'\n        assert all(map(lambda k: isinstance(k, TopicPartition), partitions))\n        if not partitions:\n            return Future().success({})\n\n        node_id = self.coordinator()\n        if node_id is None:\n            return Future().failure(Errors.GroupCoordinatorNotAvailableError)\n\n        # Verify node is ready\n        if not self._client.ready(node_id):\n            log.debug(\"Node %s not ready -- failing offset fetch request\",\n                      node_id)\n            return Future().failure(Errors.NodeNotReadyError)\n\n        log.debug(\"Group %s fetching committed offsets for partitions: %s\",\n                  self.group_id, partitions)\n        # construct the request\n        topic_partitions = collections.defaultdict(set)\n        for tp in partitions:\n            topic_partitions[tp.topic].add(tp.partition)\n\n        if self.config['api_version'] >= (0, 8, 2):\n            request = OffsetFetchRequest[1](\n                self.group_id,\n                list(topic_partitions.items())\n            )\n        else:\n            request = OffsetFetchRequest[0](\n                self.group_id,\n                list(topic_partitions.items())\n            )\n\n        # send the request with a callback\n        future = Future()\n        _f = self._client.send(node_id, request)\n        _f.add_callback(self._handle_offset_fetch_response, future)\n        _f.add_errback(self._failed_request, node_id, request, future)\n        return future",
        "sampled": "```python\nfrom concurrent.futures import Future\n\nclass KafkaClient:\n    def _fetch_offsets(self):\n        # Storing the existing data \n        globally_stored_offsets = {} \n        \n            \n    \n    def _send_offset_fetch_request(self, partitions):\n        \"\"\"Fetch the committed offsets for a set of partitions.\n        \n        This is a non-blocking call. The returned future can be polled to get\n        the actual offsets returned from the broker.\n\n        Arguments:\n            partitions (list of TopicPartition):"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a AudioResources object from a json dictionary.\"\"\"\n        args = {}\n        if 'total_minutes_of_audio' in _dict:\n            args['total_minutes_of_audio'] = _dict.get('total_minutes_of_audio')\n        else:\n            raise ValueError(\n                'Required property \\'total_minutes_of_audio\\' not present in AudioResources JSON'\n            )\n        if 'audio' in _dict:\n            args['audio'] = [\n                AudioResource._from_dict(x) for x in (_dict.get('audio'))\n            ]\n        else:\n            raise ValueError(\n                'Required property \\'audio\\' not present in AudioResources JSON'\n            )\n        return cls(**args)",
        "sampled": "```python\ndef _from_dict(cls, _dict):\n    obj = cls()\n    for key in obj.__dataclass_fields__:\n        if key in _dict:\n            setattr(obj, key, _dict[key])\n    return obj\n\nclass Resource:\n    def __init__(self, id: int, url: str):\n        self.id = id\n        self.url = url\n\nclass ImageResource(Resource):\n    def __init__(self, id: int, url: str, width"
    },
    {
        "original": "def get_headers(environ):\n    \"\"\"\n    Returns only proper HTTP headers.\n    \"\"\"\n    for key, value in compat.iteritems(environ):\n        key = str(key)\n        if key.startswith(\"HTTP_\") and key not in (\"HTTP_CONTENT_TYPE\", \"HTTP_CONTENT_LENGTH\"):\n            yield key[5:].replace(\"_\", \"-\").lower(), value\n        elif key in (\"CONTENT_TYPE\", \"CONTENT_LENGTH\"):\n            yield key.replace(\"_\", \"-\").lower(), value",
        "sampled": "```python\ndef get_headers(environ):\n    \"\"\"\n    Returns only proper HTTP headers.\n    \"\"\"\n\n    # Initialize an empty dictionary to store the HTTP headers\n    http_headers = {}\n\n    # Iterate over the items in the environ dictionary\n    for key, value in environ.items():\n        # Check if the key starts with 'HTTP_' (this is a WSGI convention for HTTP request headers)\n        if key.startswith('HTTP_'):\n            # Extract the actual header name by removing 'HTTP"
    },
    {
        "original": "def store(self, variables, attributes, check_encoding_set=frozenset(),\n              writer=None, unlimited_dims=None):\n        \"\"\"\n        Top level method for putting data on this store, this method:\n          - encodes variables/attributes\n          - sets dimensions\n          - sets variables\n\n        Parameters\n        ----------\n        variables : dict-like\n            Dictionary of key/value (variable name / xr.Variable) pairs\n        attributes : dict-like\n            Dictionary of key/value (attribute name / attribute) pairs\n        check_encoding_set : list-like\n            List of variables that should be checked for invalid encoding\n            values\n        writer : ArrayWriter\n        unlimited_dims : list-like\n            List of dimension names that should be treated as unlimited\n            dimensions.\n        \"\"\"\n        if writer is None:\n            writer = ArrayWriter()\n\n        variables, attributes = self.encode(variables, attributes)\n\n        self.set_attributes(attributes)\n        self.set_dimensions(variables, unlimited_dims=unlimited_dims)\n        self.set_variables(variables, check_encoding_set, writer,\n                           unlimited_dims=unlimited_dims)",
        "sampled": "```python\nclass DataStore:\n    def store(self, variables, attributes, check_encoding_set=frozenset(), writer=None, unlimited_dims=None):\n        # Encode variables and attributes\n        encoded_variables = {}\n        encoded_attributes = {}\n\n        for var_name, variable in variables.items():\n            if writer:\n                variable.values = writer.write(variable.values)\n            encoded_variables[var_name] = variable\n\n        for attr_name, attribute in attributes.items():\n            if isinstance(attribute, dict):\n                attribute['"
    },
    {
        "original": "def fit(\n            self,\n            data,\n            states,\n            estimator=None,\n            complete_samples_only=True,\n            **kwargs):\n        \"\"\"\n        Determine \u03b2s from data\n\n        Parameters\n        ----------\n        data: pandas.DataFrame\n            Dataframe containing samples from the conditional distribution, p(Y|X)\n            estimator: 'MLE' or 'MAP'\n\n        completely_samples_only: boolean (True or False)\n            Are they downsampled or complete? Defaults to True\n\n        \"\"\"\n        if estimator == 'MLE':\n            mean, variance = self.maximum_likelihood_estimator(data, states)\n        elif estimator == 'MAP':\n            raise NotImplementedError(\n                \"fit method has not been implemented using Maximum A-Priori (MAP)\")\n\n        return mean, variance",
        "sampled": "```python\nimport pandas as pd\nfrom scipy.optimize import minimize\nimport numpy as np\n\nclass EOQ:\n    def fit(self, data, states, estimator=None, complete_samples_only=True, **kwargs):\n        \"\"\"\n        Determine \u03b2s from data\n\n        Parameters\n        ----------\n        data: pandas.DataFrame\n            Dataframe containing samples from the conditional distribution, p(Y|X)\n            estimator: 'MLE' or 'MAP'\n\n        completely_samples_only: boolean (True or False)\n           "
    },
    {
        "original": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)",
        "sampled": "```python\nclass KubernetesCluster:\n    def __init__(self):\n        # Initialize an empty list to represent the running containers\n        self.running_containers = []\n\n    def add_container(self, container_id):\n        \"\"\"\n        Add a container to the cluster.\n\n        Args:\n            container_id (str): The ID of the container.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n        self.running_containers.append(container_id)\n        return True\n\n    def remove_container(self,"
    },
    {
        "original": "def balance_scores_and_dont_scale(cat_scores, not_cat_scores):\n\t\t\"\"\"\n\t\tmedian = np.median(cat_scores)\n\t\tscores = np.zeros(len(cat_scores)).astype(np.float)\n\t\tscores[cat_scores > median] = cat_scores[cat_scores > median]\n\t\tnot_cat_mask = cat_scores < median if median != 0 else cat_scores <= median\n\t\tscores[not_cat_mask] = -not_cat_scores[not_cat_mask]\n\t\t\"\"\"\n\t\tscores = np.zeros(len(cat_scores)).astype(np.float)\n\t\tscores[cat_scores > not_cat_scores] = cat_scores[cat_scores > not_cat_scores]\n\t\tscores[cat_scores < not_cat_scores] = -not_cat_scores[cat_scores < not_cat_scores]\n\t\treturn scores",
        "sampled": "```python\nimport numpy as np\n\ndef balance_scores_and_dont_scale(cat_scores, not_cat_scores):\n    \"\"\"\n    Balance cat scores and don't scale dog scores.\n    \n    Parameters:\n    cat_scores (numpy array): Scores of cats.\n    not_cat_scores (numpy array): Scores of dogs.\n    \n    Returns:\n    numpy array: Balanced scores for both cats and dogs.\n    \"\"\"\n    \n    # Calculate the median of cat scores\n    median = np.median(cat_scores)\n    \n    "
    },
    {
        "original": "def add_text(self, coords, text, color=(0, 0, 0)):\n        \"\"\"\n        Add text at a coordinate.\n\n        Args:\n            coords: Coordinates to add text at.\n            text: Text to place.\n            color: Color for text as RGB. Defaults to black.\n        \"\"\"\n        source = vtk.vtkVectorText()\n        source.SetText(text)\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputConnection(source.GetOutputPort())\n        follower = vtk.vtkFollower()\n        follower.SetMapper(mapper)\n        follower.GetProperty().SetColor(color)\n        follower.SetPosition(coords)\n        follower.SetScale(0.5)\n        self.ren.AddActor(follower)\n        follower.SetCamera(self.ren.GetActiveCamera())",
        "sampled": "```python\nclass GraphicsEditor:\n    def __init__(self):\n        self.texts = []\n\n    def add_text(self, coords, text, color=(0, 0, 0)):\n        \"\"\"\n        Add text at a coordinate.\n\n        Args:\n            coords: Coordinates to add text at.\n            text: Text to place.\n            color: Color for text as RGB. Defaults to black.\n        \"\"\"\n        self.texts.append({\n            'coords': coords,\n            'text': text"
    },
    {
        "original": "def _join_summary_file(data, summary_filename=\"msd_summary_file.h5\"):\n    \"\"\" Gets the trackinfo array by joining taste profile to the track summary file \"\"\"\n    msd = h5py.File(summary_filename)\n\n    # create a lookup table of trackid -> position\n    track_lookup = dict((t.encode(\"utf8\"), i) for i, t in enumerate(data['track'].cat.categories))\n\n    # join on trackid to the summary file to get the artist/album/songname\n    track_info = np.empty(shape=(len(track_lookup), 4), dtype=np.object)\n    with tqdm.tqdm(total=len(track_info)) as progress:\n        for song in msd['metadata']['songs']:\n            trackid = song[17]\n            if trackid in track_lookup:\n                pos = track_lookup[trackid]\n                track_info[pos] = [x.decode(\"utf8\") for x in (trackid, song[9], song[14], song[18])]\n                progress.update(1)\n\n    return track_info",
        "sampled": "```python\nimport h5py\nimport pandas as pd\n\ndef _join_summary_file(data, summary_filename=\"msd_summary_file.h5\"):\n    \"\"\"\n    Gets the trackinfo array by joining taste profile to the track summary file\n    \n    Parameters:\n    data (dict): The taste profile data.\n    summary_filename (str): The name of the output file where the combined data will be saved.\n    \n    Returns:\n    h5py.Group: The resulting HDF5 group containing the combined data"
    },
    {
        "original": "def _get_diff(self):\n        \"\"\"Get a diff between running config and a proposed file.\"\"\"\n        diff = []\n        self._create_sot_file()\n        diff_out = self._send_command(\n            \"show diff rollback-patch file {} file {}\".format(\n                \"sot_file\", self.candidate_cfg\n            ),\n            raw_text=True,\n        )\n        try:\n            diff_out = (\n                diff_out.split(\"Generating Rollback Patch\")[1]\n                .replace(\"Rollback Patch is Empty\", \"\")\n                .strip()\n            )\n            for line in diff_out.splitlines():\n                if line:\n                    if line[0].strip() != \"!\" and line[0].strip() != \".\":\n                        diff.append(line.rstrip(\" \"))\n        except (AttributeError, KeyError):\n            raise ReplaceConfigException(\n                \"Could not calculate diff. It's possible the given file doesn't exist.\"\n            )\n        return \"\\n\".join(diff)",
        "sampled": "```python\nfrom diff_match import diff\nimport json\n\nclass ConfigurationDiff:\n    def __init__(self, running_config, proposed_config):\n        self.running_config = running_config\n        self.proposed_config = proposed_config\n\n    def _get_diff(self):\n        \"\"\"Get a diff between running config and a proposed file.\"\"\"\n        \n        # Load the proposed config from JSON string to Python dictionary        \n        proposed_dict = json.loads(self.proposed_config)\n        \n        # Generate the list of lines representing"
    },
    {
        "original": "def get_partition_function(self):\n        \"\"\"\n        Returns the partition function for a given undirected graph.\n\n        A partition function is defined as\n\n        .. math:: \\sum_{X}(\\prod_{i=1}^{m} \\phi_i)\n\n        where m is the number of factors present in the graph\n        and X are all the random variables present.\n\n        Examples\n        --------\n        >>> from pgmpy.models import MarkovModel\n        >>> from pgmpy.factors.discrete import DiscreteFactor\n        >>> G = MarkovModel()\n        >>> G.add_nodes_from(['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7'])\n        >>> G.add_edges_from([('x1', 'x3'), ('x1', 'x4'), ('x2', 'x4'),\n        ...                   ('x2', 'x5'), ('x3', 'x6'), ('x4', 'x6'),\n        ...                   ('x4', 'x7'), ('x5', 'x7')])\n        >>> phi = [DiscreteFactor(edge, [2, 2], np.random.rand(4)) for edge in G.edges()]\n        >>> G.add_factors(*phi)\n        >>> G.get_partition_function()\n        \"\"\"\n        self.check_model()\n\n        factor = self.factors[0]\n        factor = factor_product(factor, *[self.factors[i] for i in\n                                          range(1, len(self.factors))])\n        if set(factor.scope()) != set(self.nodes()):\n            raise ValueError('DiscreteFactor for all the random variables not defined.')\n\n        return np.sum(factor.values)",
        "sampled": "```python\nimport numpy as np\nfrom pgmpy.models import MarkovModel\nfrom pgmpy.factors.discrete import DiscreteFactor\n\nclass Solution:\n    def get_partition_function(self):\n        \"\"\"\n        Returns the partition function for a given undirected graph.\n\n        A partition function is defined as\n\n        .. math:: \\sum_{X}(\\prod_{i=1}^{m} \\phi_i)\n\n        where m is the number of factors present in the graph\n       "
    },
    {
        "original": "def read_channel(self):\n        \"\"\"Generic handler that will read all the data from an SSH or telnet channel.\"\"\"\n        output = \"\"\n        self._lock_netmiko_session()\n        try:\n            output = self._read_channel()\n        finally:\n            # Always unlock the SSH channel, even on exception.\n            self._unlock_netmiko_session()\n        return output",
        "sampled": "```python\nimport re\n\nclass F1Fune_Framework:\n    def read_channel(self, data):\n        \"\"\"\n        This function will analyze the given data containing a path where laps are specified in minutes and seconds.\n        \n        Parameters:\n        -----------\n        data : str\n               This is the string containing lap times and racer's numbers. It is organised as follows - 'racer_number time,laps_completed'\n               \n               For example, '5 12:45,25' or '8"
    },
    {
        "original": "def get_alert_log(self,\n                      current=0,\n                      minimum=0,\n                      maximum=100,\n                      header=\"\",\n                      action_key=None):\n        \"\"\"Get the alert log.\"\"\"\n        return self.get_alert(current=current,\n                              minimum=minimum,\n                              maximum=maximum,\n                              header=header,\n                              action_key=action_key,\n                              log=True)",
        "sampled": "```python\nclass AlertLogger:\n    def get_alert_log(self, current=0, minimum=0, maximum=100, header=\"\", action_key=None):\n        \"\"\"\n        Get the alert log.\n        \n        Args:\n            current (int): The current value. Defaults to 0.\n            minimum (int): The minimum value. Defaults to 0.\n            maximum (int): The maximum value. Defaults to 100.\n            header (str): The header of the alert log. Defaults to"
    },
    {
        "original": "def config(name, reset=False, **kwargs):\n    \"\"\"\n    Modify configuration options for a given port. Multiple options can be\n    specified. To see the available options for a port, use\n    :mod:`ports.showconfig <salt.modules.freebsdports.showconfig>`.\n\n    name\n        The port name, in ``category/name`` format\n\n    reset : False\n        If ``True``, runs a ``make rmconfig`` for the port, clearing its\n        configuration before setting the desired options\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' ports.config security/nmap IPV6=off\n    \"\"\"\n    portpath = _check_portname(name)\n\n    if reset:\n        rmconfig(name)\n\n    configuration = showconfig(name, dict_return=True)\n\n    if not configuration:\n        raise CommandExecutionError(\n            'Unable to get port configuration for \\'{0}\\''.format(name)\n        )\n\n    # Get top-level key for later reference\n    pkg = next(iter(configuration))\n    conf_ptr = configuration[pkg]\n\n    opts = dict(\n        (six.text_type(x), _normalize(kwargs[x]))\n        for x in kwargs\n        if not x.startswith('_')\n    )\n\n    bad_opts = [x for x in opts if x not in conf_ptr]\n    if bad_opts:\n        raise SaltInvocationError(\n            'The following opts are not valid for port {0}: {1}'\n            .format(name, ', '.join(bad_opts))\n        )\n\n    bad_vals = [\n        '{0}={1}'.format(x, y) for x, y in six.iteritems(opts)\n        if y not in ('on', 'off')\n    ]\n    if bad_vals:\n        raise SaltInvocationError(\n            'The following key/value pairs are invalid: {0}'\n            .format(', '.join(bad_vals))\n        )\n\n    conf_ptr.update(opts)\n    _write_options(name, configuration)\n\n    new_config = showconfig(name, dict_return=True)\n    try:\n        new_config = new_config[next(iter(new_config))]\n    except (StopIteration, TypeError):\n        return False\n\n    return all(conf_ptr[x] == new_config.get(x) for x in conf_ptr)",
        "sampled": "```python\ndef config(name, reset=False, **kwargs):\n    \"\"\"\n    Modify configuration options for a given port. Multiple options can be\n    specified. To see the available options for a port, use\n    :mod:`ports.showconfig <salt.modules.freebsdports.showconfig>`.\n\n    name\n        The port name, in ``category/name`` format\n\n    reset : False\n        If ``True``, runs a ``make rmconfig`` for the port, clearing its\n       "
    },
    {
        "original": "def addTrack(self, track):\n        \"\"\"\n        Add a track to be recorded.\n\n        :param: track: An :class:`aiortc.AudioStreamTrack` or :class:`aiortc.VideoStreamTrack`.\n        \"\"\"\n        if track.kind == 'audio':\n            if self.__container.format.name == 'wav':\n                codec_name = 'pcm_s16le'\n            elif self.__container.format.name == 'mp3':\n                codec_name = 'mp3'\n            else:\n                codec_name = 'aac'\n            stream = self.__container.add_stream(codec_name)\n        else:\n            if self.__container.format.name == 'image2':\n                stream = self.__container.add_stream('png', rate=30)\n                stream.pix_fmt = 'rgb24'\n            else:\n                stream = self.__container.add_stream('libx264', rate=30)\n                stream.pix_fmt = 'yuv420p'\n        self.__tracks[track] = MediaRecorderContext(stream)",
        "sampled": "```python\nclass Tracker:\n    def __init__(self):\n        self.tracks = []\n\n    def addTrack(self, track):\n        \"\"\"\n        Add a track to be recorded.\n\n        :param: track: An aiortc.AudioStreamTrack or aiortc.VideoStreamTrack.\n        \"\"\"\n        self.tracks.append(track)\n```\n\nRemember that `aiortc` is an asyncio-based library for working with WebRTC. If you don't have it installed, you can do so by running `"
    },
    {
        "original": "def _get_partition_info(storage_system, device_path):\n    \"\"\"\n    Returns partition informations for a device path, of type\n    vim.HostDiskPartitionInfo\n    \"\"\"\n    try:\n        partition_infos = \\\n                storage_system.RetrieveDiskPartitionInfo(\n                    devicePath=[device_path])\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    log.trace('partition_info = %s', partition_infos[0])\n    return partition_infos[0]",
        "sampled": "```python\nimport vim\n\nclass HostDiskPartitionInfo:\n    def __init__(self, device_path, partition_start_sector, partition_end_sector):\n        self.device_path = device_path\n        self.partition_start_sector = partition_start_sector\n        self.partition_end_sector = partition_end_sector\n\ndef _get_partition_info(storage_system, device_path):\n    # For simplicity, let's assume we have a dictionary that maps device paths to their corresponding disk information.\n    # In a real scenario, this information"
    },
    {
        "original": "def string_asset(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the StringAsset class. Registers the decorated class\n    as the StringAsset known type.\n    \"\"\"\n    assert isinstance(class_obj, type), \"class_obj is not a Class\"\n    global _string_asset_resource_type\n    _string_asset_resource_type = class_obj\n    return class_obj",
        "sampled": "```python\nclass StringAsset:\n    def __init__(self, class_obj: type):\n        \"\"\"\n        Initialize a StringAsset instance.\n        \n        Args:\n            class_obj (type): The decorated class type.\n        \"\"\"\n        self.class_obj = class_obj\n\n\ndef string_asset(class_obj: type) -> type:\n    \"\"\"\n    Decorator to annotate the StringAsset class. Registers the decorated class\n    as the StringAsset known type.\n\n    Args:\n        (class object)\n\n    Returns:\n"
    },
    {
        "original": "def _clear_ignore(endpoint_props):\n    \"\"\"\n    Both _clear_dict and _ignore_keys in a single iteration.\n    \"\"\"\n    return dict(\n        (prop_name, prop_val)\n        for prop_name, prop_val in six.iteritems(endpoint_props)\n        if prop_name not in _DO_NOT_COMPARE_FIELDS and prop_val is not None\n    )",
        "sampled": "```python\ndef _clear_ignore(endpoint_props):\n    \"\"\"\n    Both _clear_dict and _ignore_keys in a single iteration.\n    \"\"\"\n    \n    ignore_keys = ['_is_dynamic', '_url_safe_name']\n    \n    for key, value in endpoint_props.items():\n        if key in ignore_keys:\n            del endpoint_props[key]\n        elif isinstance(value, dict):\n            # Recursively clear the dictionary\n            _clear_ignore(value)\n        \n```"
    },
    {
        "original": "def sample_stats_to_xarray(self):\r\n        \"\"\"Extract sample_stats from posterior.\"\"\"\r\n        posterior = self.posterior\r\n        posterior_model = self.posterior_model\r\n        # copy dims and coords\r\n        dims = deepcopy(self.dims) if self.dims is not None else {}\r\n        coords = deepcopy(self.coords) if self.coords is not None else {}\r\n\r\n        # log_likelihood\r\n        log_likelihood = self.log_likelihood\r\n        if log_likelihood is not None:\r\n            if isinstance(log_likelihood, str) and log_likelihood in dims:\r\n                dims[\"log_likelihood\"] = dims.pop(log_likelihood)\r\n\r\n        data = get_sample_stats_stan3(\r\n            posterior, model=posterior_model, log_likelihood=log_likelihood\r\n        )\r\n\r\n        return dict_to_dataset(data, library=self.stan, coords=coords, dims=dims)",
        "sampled": "```python\nimport xarray as xr\n\ndef sample_stats_to_xarray(self):\n    \"\"\"\n    Extract sample_stats from posterior.\n    \n    Parameters:\n    None\n    \n    Returns:\n    xarray.DataArray: Sample statistics extracted from the posterior.\n    \"\"\"\n    \n    # Assuming self.posterior is a dictionary with 'sample_stats' key\n    sample_stats = self.posterior['sample_stats']\n    \n    # Convert the dictionary to an xarray DataArray\n    da = xr.DataArray(sampleStats"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "sampled": "```python\nimport xml.etree.ElementTree as ET\n\nclass SkeletonLoader:\n    def load_skel(self, file_name):\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n        \"\"\"\n        \n        # Parse the XML file using ElementTree\n        tree = ET.parse(file_name)\n        \n        # Get the root element of the XML file\n        root = tree.getroot()\n        \n        # Initialize an empty dictionary to store"
    },
    {
        "original": "def _minigui_report_search_status(self, leaves):\n        \"\"\"Prints the current MCTS search status to stderr.\n\n        Reports the current search path, root node's child_Q, root node's\n        child_N, the most visited path in a format that can be parsed by\n        one of the STDERR_HANDLERS in minigui.ts.\n\n        Args:\n          leaves: list of leaf MCTSNodes returned by tree_search().\n         \"\"\"\n\n        root = self._player.get_root()\n\n        msg = {\n            \"id\": hex(id(root)),\n            \"n\": int(root.N),\n            \"q\": float(root.Q),\n        }\n\n        msg[\"childQ\"] = [int(round(q * 1000)) for q in root.child_Q]\n        msg[\"childN\"] = [int(n) for n in root.child_N]\n\n        ranked_children = root.rank_children()\n        variations = {}\n        for i in ranked_children[:15]:\n            if root.child_N[i] == 0 or i not in root.children:\n                break\n            c = coords.to_gtp(coords.from_flat(i))\n            child = root.children[i]\n            nodes = child.most_visited_path_nodes()\n            moves = [coords.to_gtp(coords.from_flat(m.fmove)) for m in nodes]\n            variations[c] = {\n                \"n\": int(root.child_N[i]),\n                \"q\": float(root.child_Q[i]),\n                \"moves\": [c] + moves,\n            }\n\n        if leaves:\n            path = []\n            leaf = leaves[0]\n            while leaf != root:\n                path.append(leaf.fmove)\n                leaf = leaf.parent\n            if path:\n                path.reverse()\n                variations[\"live\"] = {\n                    \"n\": int(root.child_N[path[0]]),\n                    \"q\": float(root.child_Q[path[0]]),\n                    \"moves\": [coords.to_gtp(coords.from_flat(m)) for m in path]\n                }\n\n        if variations:\n            msg[\"variations\"] = variations\n\n        dbg(\"mg-update:%s\" % json.dumps(msg, sort_keys=True))",
        "sampled": "```python\ndef _minigui_report_search_status(self, leaves):\n    \"\"\"\n    Prints the current MCTS search status to stderr.\n\n    Reports the current search path, root node's child_Q, root node's\n    child_N, the most visited path in a format that can be parsed by\n    one of the STDERR_HANDLERS in minigui.ts.\n\n    Args:\n      leaves: list of leaf MCTSNodes returned by tree_search().\n  \"\"\"\n\n    # Check if there"
    },
    {
        "original": "def _create_dataset(self, *data):\n        \"\"\"Converts input data to the appropriate Dataset\"\"\"\n        # Make sure data is a tuple of dense tensors\n        data = [self._to_torch(x, dtype=torch.FloatTensor) for x in data]\n        return TensorDataset(*data)",
        "sampled": "```python\nclass Solution:\n    def create_dataset(self, data):\n        class Dataset:\n            def __init__(self, data):\n                self.data = data\n\n        return Dataset(data)\n\n    def solve_problem(self, min_value=1, max_value=1000000):\n        num_values = 3\n        values = [self.create_dataset([0] * num_values),\n                  self.create_dataset([min_value + (max_value - min_value) * i / (num_values - "
    },
    {
        "original": "def _probvec(r, out):\n    \"\"\"\n    Fill `out` with randomly sampled probability vectors as rows.\n\n    To be complied as a ufunc by guvectorize of Numba. The inputs must\n    have the same shape except the last axis; the length of the last\n    axis of `r` must be that of `out` minus 1, i.e., if out.shape[-1] is\n    k, then r.shape[-1] must be k-1.\n\n    Parameters\n    ----------\n    r : ndarray(float)\n        Array containing random values in [0, 1).\n\n    out : ndarray(float)\n        Output array.\n\n    \"\"\"\n    n = r.shape[0]\n    r.sort()\n    out[0] = r[0]\n    for i in range(1, n):\n        out[i] = r[i] - r[i-1]\n    out[n] = 1 - r[n-1]",
        "sampled": "```python\nimport numpy as np\nfrom numba import guvectorize\n\n@guvectorize([None(\"(float64,float64[:])\")], '()->()', target='parallel')\ndef _probvec(r, out):\n    k = out.shape[0]  # assume input array (probability weights) already contains correct length \n    for i in range(k):\n        out[i] = r.copy() / r.sum()\n```\n\nNote: The provided docstring indicates that the function"
    },
    {
        "original": "def _w_within_shard(args: Dict[str, Any]):\n    \"\"\"Applies a W gate when the gate acts only within a shard.\"\"\"\n    index = args['index']\n    half_turns = args['half_turns']\n    axis_half_turns = args['axis_half_turns']\n    state = _state_shard(args)\n    pm_vect = _pm_vects(args)[index]\n    num_shard_qubits = args['num_shard_qubits']\n    shard_size = 2 ** num_shard_qubits\n\n    reshape_tuple = (2 ** (num_shard_qubits - 1 - index), 2, 2 ** index)\n    perm_state = np.reshape(\n        np.reshape(state, reshape_tuple)[:, ::-1, :], shard_size)\n    cos = np.cos(-0.5 * np.pi * half_turns)\n    sin = np.sin(-0.5 * np.pi * half_turns)\n\n    cos_axis = np.cos(np.pi * axis_half_turns)\n    sin_axis = np.sin(np.pi * axis_half_turns)\n\n    new_state = cos * state + 1j * sin * perm_state * (\n        cos_axis - 1j * sin_axis * pm_vect)\n    np.copyto(state, new_state)",
        "sampled": "```python\nfrom typing import Dict, Any\n\ndef _w_within_shard(args: Dict[str, Any]):\n    \"\"\"\n    Applies a W gate when the gate acts only within a shard.\n    \n    For simplicity, assume the W gate is applied by performing an X-ray (Pauli-X) on both target registers, and then swapping their values. \n    We can also assume that qubits are registered from 0 to num_qubits - 1.\n    \n    The function's result will"
    },
    {
        "original": "def get_type(atype, size=None):\n    \"\"\"\n    Retrieve the java type of a descriptor (e.g : I)\n    \"\"\"\n    res = TYPE_DESCRIPTOR.get(atype)\n    if res is None:\n        if atype[0] == 'L':\n            if atype.startswith('Ljava/lang'):\n                res = atype[1:-1].lstrip('java/lang/').replace('/', '.')\n            else:\n                res = atype[1:-1].replace('/', '.')\n        elif atype[0] == '[':\n            if size is None:\n                res = '%s[]' % get_type(atype[1:])\n            else:\n                res = '{}[{}]'.format(get_type(atype[1:]), size)\n        else:\n            res = atype\n            logger.debug('Unknown descriptor: \"%s\".', atype)\n    return res",
        "sampled": "```python\ndef get_type(atype, size=None):\n    \"\"\"\n    Retrieve the java type of a descriptor (e.g : I)\n    \"\"\"\n    \n    # Dictionary mapping Python types to Java types\n    type_mapping = {\n        'bool': 'Z',\n        'byte': 'B',\n        # Both Java and Python use ilogrical(or the logical opposite) abbreviations \n        #'char' is represented with i as an input code(though technically hexadecimal)\n        'char': '"
    },
    {
        "original": "def _MultiNotifyQueue(self, queue, notifications, mutation_pool=None):\n    \"\"\"Does the actual queuing.\"\"\"\n    notification_list = []\n    now = rdfvalue.RDFDatetime.Now()\n    for notification in notifications:\n      if not notification.first_queued:\n        notification.first_queued = (\n            self.frozen_timestamp or rdfvalue.RDFDatetime.Now())\n      else:\n        diff = now - notification.first_queued\n        if diff.seconds >= self.notification_expiry_time:\n          # This notification has been around for too long, we drop it.\n          logging.debug(\"Dropping notification: %s\", str(notification))\n          continue\n\n      notification_list.append(notification)\n\n    mutation_pool.CreateNotifications(\n        self.GetNotificationShard(queue), notification_list)",
        "sampled": "```python\nfrom collections import deque\n\nclass MultiNotifyQueue:\n    def _MultiNotifyQueue(self, queue, notifications, mutation_pool=None):\n        \"\"\"\n        Does the actual queuing.\n\n        Args:\n            queue (deque): The queue to be updated.\n            notifications (list): A list of tuples containing the notification type and data.\n            mutation_pool (dict, optional): A dictionary of mutations. Defaults to None.\n\n        Returns:\n            deque: The updated queue.\n        \"\"\"\n\n        # Iterate"
    },
    {
        "original": "def library_line(self, file_name):\n        \"\"\"\n        Specifies GULP library file to read species and potential parameters.\n        If using library don't specify species and potential\n        in the input file and vice versa. Make sure the elements of\n        structure are in the library file.\n\n        Args:\n            file_name: Name of GULP library file\n\n        Returns:\n            GULP input string specifying library option\n        \"\"\"\n        gulplib_set = lambda: 'GULP_LIB' in os.environ.keys()\n        readable = lambda f: os.path.isfile(f) and os.access(f, os.R_OK)\n\n        #dirpath, fname = os.path.split(file_name)\n        #if dirpath:  # Full path specified\n        #    if readable(file_name):\n        #        gin = 'library ' + file_name\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #else:\n        #    fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n        #    if readable(fpath):\n        #        gin = 'library ' + fpath\n        #    elif gulplib_set():\n        #        fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n        #        if readable(fpath):\n        #            gin = 'library ' + file_name\n        #        else:\n        #            raise GulpError('GULP Library not found')\n        #    else:\n        #        raise GulpError('GULP Library not found')\n        #gin += \"\\n\"\n        #return gin\n\n        gin = \"\"\n        dirpath, fname = os.path.split(file_name)\n        if dirpath and readable(file_name):  # Full path specified\n            gin = 'library ' + file_name\n        else:\n            fpath = os.path.join(os.getcwd(), file_name)  # Check current dir\n            if readable(fpath):\n                gin = 'library ' + fpath\n            elif gulplib_set():         # Check the GULP_LIB path\n                fpath = os.path.join(os.environ['GULP_LIB'], file_name)\n                if readable(fpath):\n                    gin = 'library ' + file_name\n        if gin:\n            return gin + \"\\n\"\n        else:\n            raise GulpError('GULP Library not found')",
        "sampled": "```python\nclass GULP:\n    def library_line(self, file_name):\n        \"\"\"\n        Specifies GULP library file to read species and potential parameters.\n        \n        Args:\n            file_name: Name of GULP library file\n\n        Returns:\n            GULP input string specifying library option\n        \"\"\"\n        \n        # Read the contents of the specified library file\n        with open(file_name, 'r') as f:\n            lines = f.readlines()\n        \n        # Initialize an empty"
    },
    {
        "original": "def decode_consumer_metadata_response(cls, data):\n        \"\"\"\n        Decode bytes to a kafka.structs.ConsumerMetadataResponse\n\n        Arguments:\n            data: bytes to decode\n        \"\"\"\n        ((correlation_id, error, nodeId), cur) = relative_unpack('>ihi', data, 0)\n        (host, cur) = read_short_string(data, cur)\n        ((port,), cur) = relative_unpack('>i', data, cur)\n\n        return kafka.structs.ConsumerMetadataResponse(error, nodeId, host, port)",
        "sampled": "```python\nimport ctypes\n\nclass ConsumerMetadataResponse(ctypes.Structure):\n    _fields_ = [\n        (\"throttle_time_ms\", ctypes.c_int32),\n        (\"topics_metadata\", ctypes.POINTER(ctypes_POINTER.kafka.structs.TopicMetadata))\n    ]\n\nclass TopicMetadata(ctypes.Structure):\n    _fields_ = [\n        (\"error_code\", ctypes.c_int16),\n        (\"topic_name\", ctypes.c_char_p),\n        (\"partitions_metadata\",  cdll.kafka.structs.Topics"
    },
    {
        "original": "def ArtifactsFromYaml(self, yaml_content):\n    \"\"\"Get a list of Artifacts from yaml.\"\"\"\n    raw_list = yaml.ParseMany(yaml_content)\n\n    # TODO(hanuszczak): I am very sceptical about that \"doing the right thing\"\n    # below. What are the real use cases?\n\n    # Try to do the right thing with json/yaml formatted as a list.\n    if (isinstance(raw_list, list) and len(raw_list) == 1 and\n        isinstance(raw_list[0], list)):\n      raw_list = raw_list[0]\n\n    # Convert json into artifact and validate.\n    valid_artifacts = []\n    for artifact_dict in raw_list:\n      # In this case we are feeding parameters directly from potentially\n      # untrusted yaml/json to our RDFValue class. However, safe_load ensures\n      # these are all primitive types as long as there is no other\n      # deserialization involved, and we are passing these into protobuf\n      # primitive types.\n      try:\n        artifact_value = rdf_artifacts.Artifact(**artifact_dict)\n        valid_artifacts.append(artifact_value)\n      except (TypeError, AttributeError, type_info.TypeValueError) as e:\n        name = artifact_dict.get(\"name\")\n        raise rdf_artifacts.ArtifactDefinitionError(\n            name, \"invalid definition\", cause=e)\n\n    return valid_artifacts",
        "sampled": "```python\nimport yaml\n\nclass Solution:\n    def ArtifactsFromYaml(self, yaml_content):\n        \"\"\"\n        Get a list of Artifacts from yaml.\n        \n        Parameters:\n        yaml_content (str): The content of the YAML file.\n        \n        Returns:\n        list: A list of artifacts extracted from the YAML content.\n        \"\"\"\n        \n        # Load the YAML content into a Python dictionary\n        data = yaml.safe_load(yaml_content)\n        \n        # Initialize an empty list to store"
    },
    {
        "original": "def _generate_alphabet_dict(iterable, reserved_tokens=None):\n  \"\"\"Create set of characters that appear in any element in the iterable.\"\"\"\n  if reserved_tokens is None:\n    reserved_tokens = RESERVED_TOKENS\n  alphabet = {c for token in iterable for c in token}\n  alphabet |= {c for token in reserved_tokens for c in token}\n  alphabet |= _ESCAPE_CHARS  # Add escape characters to alphabet set.\n  return alphabet",
        "sampled": "```python\ndef gen_alocco(iterable, reserved_tokens=None):\n    \"\"\"\n    Generate 'mc' - dictionary mapping from symbol to code point.\n    \n    `reserved_tokens` is a dictionary with string that are not to be included in the dictionary map for examples ... you can think of it .. file path separator(DB) slash of windows .. that will always be excluded but still we should pass them instead of none .\n    \n     Output:\n        A dict `res` where keys are alphabetically sorted"
    },
    {
        "original": "def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n        @returns: list of builder names to build on\n        \"\"\"\n\n        # self.builderNames is the configured list of builders\n        # available for try.  If the user supplies a list of builders,\n        # it must be restricted to the configured list.  If not, build\n        # on all of the configured builders.\n        if builderNames:\n            for b in builderNames:\n                if b not in self.builderNames:\n                    log.msg(\"%s got with builder %s\" % (self, b))\n                    log.msg(\" but that wasn't in our list: %s\"\n                            % (self.builderNames,))\n                    return []\n        else:\n            builderNames = self.builderNames\n        return builderNames",
        "sampled": "```python\nclass FilterBuilder:\n    def __init__(self, builderNames):\n        self.builderNames = builderNames\n\n    def filterBuilderList(self, builderNames):\n        \"\"\"\n        Make sure that C{builderNames} is a subset of the configured\n        C{self.builderNames}, returning an empty list if not.  If\n        C{builderNames} is empty, use C{self.builderNames}.\n\n        @returns: list of builder names to build on\n       "
    },
    {
        "original": "def next(self):\n        \"\"\"Advance the cursor.\"\"\"\n        if self.__empty:\n            raise StopIteration\n        if len(self.__data) or self._refresh():\n            if self.__manipulate:\n                _db = self.__collection.database\n                return _db._fix_outgoing(self.__data.popleft(),\n                                         self.__collection)\n            else:\n                return self.__data.popleft()\n        else:\n            raise StopIteration",
        "sampled": "```python\nclass Node:\n    def __init__(self, val=0):\n        self.val = val\n        self.next = None\n\n    def __repr__(self):\n        node = self\n        result = []\n        while node:\n            result.append(str(node.val))\n            node = node.next\n        return ' -> '.join(result)\n\n\nclass Solution:\n    def next(self):\n        \"\"\"Make a linked list traverse from head to tail.\"\"\"\n        \n Oliver ListNode instance and fill in front of it"
    },
    {
        "original": "def get_Q(self):\n        \"\"\"Get the model's estimate of Q = \\mu P \\mu^T\n\n        We can then separately extract \\mu subject to additional constraints,\n        e.g. \\mu P 1 = diag(O).\n        \"\"\"\n        Z = self.Z.detach().clone().numpy()\n        O = self.O.numpy()\n        I_k = np.eye(self.k)\n        return O @ Z @ np.linalg.inv(I_k + Z.T @ O @ Z) @ Z.T @ O",
        "sampled": "```python\nimport numpy as np\n\nclass Model:\n    def __init__(self, P):\n        self.P\u043b\u0430\u0442\u044b = P \n        self.estimate_Q()\n\n    def estimate_Q(self):\n        \"\"\"Estimate the model's Q = \u03bcP\u03bc^T\"\"\"\n        # Assuming \u03bc is available\n        self.mu = np.array(...)  # Could be extracted separately subject to additional constraints\n        self.Q = np.dot(np.dot(self.mu, self.P\u043b\u0430\u0442\u044b), self.mu.T)\n\ndef get"
    },
    {
        "original": "def fCZs(self):\n        \"\"\"\n        Get a dictionary of CZ fidelities (normalized to unity) from the specs,\n        keyed by targets (qubit-qubit pairs).\n\n        :return: A dictionary of CZ fidelities, normalized to unity.\n        :rtype: Dict[tuple(int, int), float]\n        \"\"\"\n        return {tuple(es.targets): es.fCZ for es in self.edges_specs}",
        "sampled": "```python\nfrom typing import Dict, Tuple\n\nclass Solution:\n    def fCZs(self):\n        \"\"\"\n        Get a dictionary of CZ fidelities (normalized to unity) from the specs,\n        keyed by targets (qubit-qubit pairs).\n\n        :return: A dictionary of CZ fidelities, normalized to unity.\n        :rtype: Dict[tuple(int, int), float]\n        \"\"\"\n\n        # Assume 'I' is the current unit, and 'III' is a"
    },
    {
        "original": "def merge_response_func(func, key):\n    \"\"\"\n        Use this decorator to set a new merging\n        response function to HTTP endpoints\n\n        candidate function must have the following signature\n        and be childs of BaseApi:\n        ```\n            def merge_some_function(self, response, rison_args):\n        ```\n\n    :param func: Name of the merge function where the key is allowed\n    :param key: The key name for rison selection\n    :return: None\n    \"\"\"\n\n    def wrap(f):\n        if not hasattr(f, \"_response_key_func_mappings\"):\n            f._response_key_func_mappings = dict()\n        f._response_key_func_mappings[key] = func\n        return f\n\n    return wrap",
        "sampled": "```python\nfrom functools import wraps\nfrom inspect import signature\n\nclass BaseApi:\n    def merge_some_function(self, response, rison_args):\n        raise NotImplementedError\n\n\ndef merge_response_func(func, key):\n    \"\"\"\n       Use this decorator to set a new merging response function to HTTP endpoints\n\n       candidate function must have the following signature and be childs of BaseApi:\n       ```\n           def merge_some_function(self, response, rison_args):\n       ```\n       \n    :param func: Name of the"
    },
    {
        "original": "def update(self, other):\n        \"\"\"Merges other item with this object\n\n        Args:\n            other: Object containing items to merge into this object\n                Must be a dictionary or NdMapping type\n        \"\"\"\n        if isinstance(other, NdMapping):\n            dims = [d for d in other.kdims if d not in self.kdims]\n            if len(dims) == other.ndims:\n                raise KeyError(\"Cannot update with NdMapping that has\"\n                               \" a different set of key dimensions.\")\n            elif dims:\n                other = other.drop_dimension(dims)\n            other = other.data\n        for key, data in other.items():\n            self._add_item(key, data, sort=False)\n        if self.sort:\n            self._resort()",
        "sampled": "```python\nclass NdMapping:\n    def __init__(self, data):\n        self.data = data\n\n    def update(self, other):\n        \"\"\"Merges other item with this object\n\n        Args:\n            other: Object containing items to merge into this object\n                Must be a dictionary or NdMapping type\n        \"\"\"\n        \n        # Check if the input is a dictionary or an instance of NdMapping\n        if isinstance(other, dict):\n            for key, value in other.items():\n"
    },
    {
        "original": "def get_page_properties(self, page_id):\n        \"\"\"\n        Get the page (content) properties\n        :param page_id: content_id format\n        :return: get properties\n        \"\"\"\n        url = 'rest/api/content/{page_id}/property'.format(page_id=page_id)\n        return self.get(path=url)",
        "sampled": "```python\nclass ContentManager:\n    def __init__(self):\n        self.pages = {\n            \"page1\": {\"title\": \"Page 1\", \"content\": \"This is the content of page 1\"},\n            \"page2\": {\"title\": \"Page 2\", \"content\": \"This is the content of page 2\"}\n        }\n\n    def get_page_properties(self, page_id):\n        \"\"\"\n        Get the page (content) properties\n        :param page_id:"
    },
    {
        "original": "def committed(self, partition):\n        \"\"\"Get the last committed offset for the given partition.\n\n        This offset will be used as the position for the consumer\n        in the event of a failure.\n\n        This call may block to do a remote call if the partition in question\n        isn't assigned to this consumer or if the consumer hasn't yet\n        initialized its cache of committed offsets.\n\n        Arguments:\n            partition (TopicPartition): The partition to check.\n\n        Returns:\n            The last committed offset, or None if there was no prior commit.\n        \"\"\"\n        assert self.config['api_version'] >= (0, 8, 1), 'Requires >= Kafka 0.8.1'\n        assert self.config['group_id'] is not None, 'Requires group_id'\n        if not isinstance(partition, TopicPartition):\n            raise TypeError('partition must be a TopicPartition namedtuple')\n        if self._subscription.is_assigned(partition):\n            committed = self._subscription.assignment[partition].committed\n            if committed is None:\n                self._coordinator.refresh_committed_offsets_if_needed()\n                committed = self._subscription.assignment[partition].committed\n        else:\n            commit_map = self._coordinator.fetch_committed_offsets([partition])\n            if partition in commit_map:\n                committed = commit_map[partition].offset\n            else:\n                committed = None\n        return committed",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nclass CommittedOffsetAccessor(ABC):\n    @abstractmethod\n    def committed(self, partition: 'TopicPartition') -> Optional[int]:\n        \"\"\"Get the last committed offset for the given partition.\n\n        This offset will be used as the position for the consumer in the event of a failure.\n\n        This call may block to do a remote call if the partition in question isn't assigned to this consumer or if the consumer hasn"
    },
    {
        "original": "async def export_wallet(handle: int,\n                        export_config_json: str) -> None:\n    \"\"\"\n    Exports opened wallet to the file.\n\n    :param handle: wallet handle returned by indy_open_wallet.\n    :param export_config_json: JSON containing settings for input operation.\n       {\n          \"path\": path of the file that contains exported wallet content\n          \"key\": string, Key or passphrase used for wallet export key derivation.\n                         Look to key_derivation_method param for information about supported key derivation methods.\n          \"key_derivation_method\": optional<string> algorithm to use for export key derivation:\n                                ARGON2I_MOD - derive secured wallet export key (used by default)\n                                ARGON2I_INT - derive secured wallet export key (less secured but faster)\n                                RAW - raw wallet export key provided (skip derivation).\n                                      RAW keys can be generated with generate_wallet_key call\n       }\n    :return:\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"export_wallet: >>> handle: %r, export_config_json: %r\",\n                 handle,\n                 export_config_json)\n\n    if not hasattr(export_wallet, \"cb\"):\n        logger.debug(\"export_wallet: Creating callback\")\n        export_wallet.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32))\n\n    c_export_config_json = c_char_p(export_config_json.encode('utf-8'))\n\n    await do_call('indy_export_wallet',\n                  handle,\n                  c_export_config_json,\n                  export_wallet.cb)\n\n    logger.debug(\"export_wallet: <<<\")",
        "sampled": "```python\nimport json\nfrom typing import Any\n\nasync def export_wallet(handle: int, export_config_json: str) -> None:\n    \"\"\"\n    Exports opened wallet to the file.\n\n    :param handle: wallet handle returned by indy_open_wallet.\n    :param export_config_json: JSON containing settings for input operation.\n       {\n          \"path\": path of the file that contains exported wallet content\n          \"key\": string, Key or passphrase used for wallet export key derivation.\n                        "
    },
    {
        "original": "def _system_path(self, subdir, basename=''):\n        \"\"\"\n        Gets the full path to the 'subdir/basename' file in the system binwalk directory.\n\n        @subdir   - Subdirectory inside the system binwalk directory.\n        @basename - File name inside the subdirectory.\n\n        Returns the full path to the 'subdir/basename' file.\n        \"\"\"\n        try:\n            return self._file_path(os.path.join(self.system_dir, subdir), basename)\n        except KeyboardInterrupt as e:\n            raise e\n        except Exception:\n            return None",
        "sampled": "```python\nimport os\n\nclass BinwalkSystem:\n    def __init__(self, binwalk_dir):\n        self.binwalk_dir = binwalk_dir\n\n    def _system_path(self, subdir, basename=''):\n        \"\"\"\n        Gets the full path to the 'subdir/basename' file in the system binwalk directory.\n\n        @subdir   - Subdirectory inside the system binwalk directory.\n        @basename - File name inside the subdirectory.\n\n        Returns the full path to the '"
    },
    {
        "original": "def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks is enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with\n        peer is capable of enhanced route refresh capability.\n        \"\"\"\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled",
        "sampled": "```python\nclass BgpSession:\n    def __init__(self, received_open_message, sent_open_message):\n        self.received_open_message = received_open_message\n        self.sent_open_message = sent_open_message\n\n    def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks if enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with \n        peer is capable of enhanced route refresh capability.\n        \n        Returns:\n            bool"
    },
    {
        "original": "def copy(self, folder):\n        \"\"\" Copy the message to a given folder\n\n        :param folder: Folder object or Folder id or Well-known name to\n         copy this message to\n        :type folder: str or mailbox.Folder\n        :returns: the copied message\n        :rtype: Message\n        \"\"\"\n        if self.object_id is None:\n            raise RuntimeError('Attempting to move an unsaved Message')\n\n        url = self.build_url(\n            self._endpoints.get('copy_message').format(id=self.object_id))\n\n        if isinstance(folder, str):\n            folder_id = folder\n        else:\n            folder_id = getattr(folder, 'folder_id', None)\n\n        if not folder_id:\n            raise RuntimeError('Must Provide a valid folder_id')\n\n        data = {self._cc('destinationId'): folder_id}\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        message = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: message})",
        "sampled": "```python\nimport mailbox\nfrom email.message import Message\n\nclass Mailbox:\n    def __init__(self):\n        self.mail = mailbox.Maildir('.mbox')\n\n    def copy(self, folder):\n        try:\n            # Check if the given folder is a Folder object\n            folder_id = getattr(folder, 'name', folder)\n        except AttributeError:\n            # If not, check if it is a well-known name or id\n            if isinstance(folder, str):\n                content_with_delimeter"
    },
    {
        "original": "def sum(self):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        return NumpyArrayWeld(\n            numpy_weld_impl.aggr(\n                self.expr,\n                \"+\",\n                0,\n                self.weld_type\n            ),\n            self.weld_type,\n            0\n        )",
        "sampled": "```python\nclass Solution:\n    def sum(self):\n        # Problem Description: Not provided, assuming it's a simple math problem\n        # Input and Output specifications: Not provided, assuming it's a basic arithmetic operation\n        \n        # Let's assume the problem is to calculate the sum of two numbers\n        num1 = 10  # First number\n        num2 = 20  # Second number\n        \n        result = num1 + num2  # Calculate the sum\n        \n        return result"
    },
    {
        "original": "def build(values):\n\t\t\"\"\"\n\t\tParameters\n\t\t----------\n\t\tvalues: [term, ...]\n\n\t\tReturns\n\t\t-------\n\t\tIndexStore\n\t\t\"\"\"\n\t\tidxstore = IndexStore()\n\t\tidxstore._i2val = list(values)\n\t\tidxstore._val2i = {term:i for i,term in enumerate(values)}\n\t\tidxstore._next_i = len(values)\n\t\treturn idxstore",
        "sampled": "```python\nclass Term:\n    def __init__(self, weight, id):\n        self.weight = weight\n        self.id = id\n\n\nclass IndexStore:\n    def __init__(self):\n        self.terms = {}\n        self document_frequency.StartPosition term appearcountid weightsomentumrs lac thiiiinge mincedFAF aan                           \n\n    def build(self, values):\n        for value in values:\n            if value not in self.terms:\n                # means duplicate posting document is indexed (M"
    },
    {
        "original": "def GetSitelinksFromFeed(client, feed):\n  \"\"\"Gets the sitelinks from a feed.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed: the feed used to retrieve sitelinks.\n\n  Returns:\n    A dictionary mapping the feed item ID to SiteLinkFromFeed.\n  \"\"\"\n  # Retrieve the feed's attribute mapping.\n  feed_mappings = GetFeedMapping(client, feed, PLACEHOLDER_TYPE_SITELINKS)\n\n  feed_items = {}\n\n  for feed_item in GetFeedItems(client, feed):\n    site_link_from_feed = {}\n\n    for attribute_value in feed_item['attributeValues']:\n      if attribute_value['feedAttributeId'] in feed_mappings:\n        for field_id in feed_mappings[attribute_value['feedAttributeId']]:\n          if field_id == SITE_LINK_FIELDS['TEXT']:\n            site_link_from_feed['text'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['URL']:\n            site_link_from_feed['url'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['FINAL_URLS']:\n            site_link_from_feed['finalUrls'] = attribute_value['stringValues']\n          elif field_id == SITE_LINK_FIELDS['FINAL_MOBILE_URLS']:\n            site_link_from_feed['finalMobileUrls'] = attribute_value[\n                'stringValues']\n          elif field_id == SITE_LINK_FIELDS['TRACKING_URL_TEMPLATE']:\n            site_link_from_feed['trackingUrlTemplate'] = attribute_value[\n                'stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE2']:\n            site_link_from_feed['line2'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE3']:\n            site_link_from_feed['line3'] = attribute_value['stringValue']\n          else:\n            print 'No applicable Site Link Field found for Id: %s' % field_id\n\n    feed_items[feed_item['feedItemId']] = site_link_from_feed\n\n  return feed_items",
        "sampled": "```python\nfrom google.ads.googleads.client import GoogleAdsClient\nfrom google.ads.googleads.errors import GoogleAdsException\n\nclass AdWordsClient:\n    def __init__(self, client_id, client_secret, refresh_token):\n        self.client = GoogleAdsClient.load_from_file(\n            client_id,\n            client_secret,\n            refresh_token\n        )\n\n    def GetSitelinksFromFeed(self, feed_name):\n        \"\"\"Gets the sitelinks from a feed.\n\n        Args:\n          feed"
    },
    {
        "original": "def do_hook_actions(self, actions, hook_type):\n        \"\"\" call hook actions.\n\n        Args:\n            actions (list): each action in actions list maybe in two format.\n\n                format1 (dict): assignment, the value returned by hook function will be assigned to variable.\n                    {\"var\": \"${func()}\"}\n                format2 (str): only call hook functions.\n                    ${func()}\n\n            hook_type (enum): setup/teardown\n\n        \"\"\"\n        logger.log_debug(\"call {} hook actions.\".format(hook_type))\n        for action in actions:\n\n            if isinstance(action, dict) and len(action) == 1:\n                # format 1\n                # {\"var\": \"${func()}\"}\n                var_name, hook_content = list(action.items())[0]\n                hook_content_eval = self.session_context.eval_content(hook_content)\n                logger.log_debug(\n                    \"assignment with hook: {} = {} => {}\".format(\n                        var_name, hook_content, hook_content_eval\n                    )\n                )\n                self.session_context.update_test_variables(\n                    var_name, hook_content_eval\n                )\n            else:\n                # format 2\n                logger.log_debug(\"call hook function: {}\".format(action))\n                # TODO: check hook function if valid\n                self.session_context.eval_content(action)",
        "sampled": "```python\nfrom enum import Enum\n\nclass HookType(Enum):\n    SETUP = 1\n    TEARDOWN = 2\n\ndef do_hook_actions(self, actions, hook_type):\n    \"\"\"\n    call hook actions.\n\n    Args:\n        actions (list): each action in actions list maybe in two format.\n            format1 (dict): assignment, the value returned by hook function will be assigned to variable.\n                {\"var\": \"${func()}\"}\n            format2 (str): only call"
    },
    {
        "original": "def _retrieve_output_mode(self):\n        \"\"\"Save the state of the output mode so it can be reset at the end of the session.\"\"\"\n        reg_mode = re.compile(r\"output\\s+:\\s+(?P<mode>.*)\\s+\\n\")\n        output = self.send_command(\"get system console\")\n        result_mode_re = reg_mode.search(output)\n        if result_mode_re:\n            result_mode = result_mode_re.group(\"mode\").strip()\n            if result_mode in [\"more\", \"standard\"]:\n                self._output_mode = result_mode",
        "sampled": "```python\ndef _retrieve_output_mode(self):\n    \"\"\"\n    Save the state of the output mode so it can be reset at the end of the session.\n    \n    Code to be completed for this challenge is as follows:\n    \n    **Problem Description**\n    Note: This description is a general guide and may need specific details.\n    \n        When you run a Python program, it executes in interactive mode by default. However, there are other modes of operation like 'fileinput' and 'future'."
    },
    {
        "original": "def _call_and_store(getter_func, data, field_name, error_store, index=None):\n        \"\"\"Call ``getter_func`` with ``data`` as its argument, and store any `ValidationErrors`.\n\n        :param callable getter_func: Function for getting the serialized/deserialized\n            value from ``data``.\n        :param data: The data passed to ``getter_func``.\n        :param str field_name: Field name.\n        :param int index: Index of the item being validated, if validating a collection,\n            otherwise `None`.\n        \"\"\"\n        try:\n            value = getter_func(data)\n        except ValidationError as err:\n            error_store.store_error(err.messages, field_name, index=index)\n            # When a Nested field fails validation, the marshalled data is stored\n            # on the ValidationError's valid_data attribute\n            return err.valid_data or missing\n        return value",
        "sampled": "```python\nclass ValidationError(Exception):\n    \"\"\"Represents a single validation error on a single element of an iterable.\"\"\"\n    def __init__(self, field_name, value):\n        self.field_name = field_name\n        self.value = value\n\ndef _call_and_store(getter_func, data, field_name=\"value\", error_store=[], index=None):\n    \"\"\"\n    Call ``getter_func`` with ``data`` as its argument, and store any `ValidationError`s.\n\n    :param callable getter_func"
    },
    {
        "original": "def _changes(name,\n             gid=None,\n             addusers=None,\n             delusers=None,\n             members=None):\n    \"\"\"\n    Return a dict of the changes required for a group if the group is present,\n    otherwise return False.\n    \"\"\"\n    lgrp = __salt__['group.info'](name)\n    if not lgrp:\n        return False\n\n    # User and Domain names are not case sensitive in Windows. Let's make them\n    # all lower case so we can compare properly\n    if salt.utils.platform.is_windows():\n        if lgrp['members']:\n            lgrp['members'] = [user.lower() for user in lgrp['members']]\n        if members:\n            members = [salt.utils.win_functions.get_sam_name(user).lower() for user in members]\n        if addusers:\n            addusers = [salt.utils.win_functions.get_sam_name(user).lower() for user in addusers]\n        if delusers:\n            delusers = [salt.utils.win_functions.get_sam_name(user).lower() for user in delusers]\n\n    change = {}\n    ret = {}\n    if gid:\n        try:\n            gid = int(gid)\n            if lgrp['gid'] != gid:\n                change['gid'] = gid\n        except (TypeError, ValueError):\n            ret['result'] = False\n            ret['comment'] = 'Invalid gid'\n            return ret\n\n    if members is not None and not members:\n        if set(lgrp['members']).symmetric_difference(members):\n            change['delusers'] = set(lgrp['members'])\n    elif members:\n        # if new member list if different than the current\n        if set(lgrp['members']).symmetric_difference(members):\n            change['members'] = members\n\n    if addusers:\n        users_2add = [user for user in addusers if user not in lgrp['members']]\n        if users_2add:\n            change['addusers'] = users_2add\n\n    if delusers:\n        users_2del = [user for user in delusers if user in lgrp['members']]\n        if users_2del:\n            change['delusers'] = users_2del\n\n    return change",
        "sampled": "```python\ndef _changes(name, gid=None, addusers=None, delusers=None, members=None):\n    \"\"\"\n    Return a dict of the changes required for a group if the group is present,\n    otherwise return False.\n    \"\"\"\n\n    # Check if gid is provided and it matches the name\n    if gid and str(gid) != name:\n        return False\n\n    # If addusers or delusers is not provided, return False\n    if (not addusers and not del"
    },
    {
        "original": "def generate_defect_structure(self, supercell=(1, 1, 1)):\n        \"\"\"\n        Returns Defective Vacancy structure, decorated with charge\n        Args:\n            supercell (int, [3x1], or [[]] (3x3)): supercell integer, vector, or scaling matrix\n        \"\"\"\n        defect_structure = self.bulk_structure.copy()\n        defect_structure.make_supercell(supercell)\n\n        #create a trivial defect structure to find where supercell transformation moves the lattice\n        struct_for_defect_site = Structure( self.bulk_structure.copy().lattice,\n                                             [self.site.specie],\n                                             [self.site.frac_coords],\n                                             to_unit_cell=True)\n        struct_for_defect_site.make_supercell(supercell)\n        defect_site = struct_for_defect_site[0]\n\n        poss_deflist = sorted(\n            defect_structure.get_sites_in_sphere(defect_site.coords, 2, include_index=True), key=lambda x: x[1])\n        defindex = poss_deflist[0][2]\n        defect_structure.remove_sites([defindex])\n        defect_structure.set_charge(self.charge)\n        return defect_structure",
        "sampled": "```python\nimport numpy as np\n\nclass DefectStructure:\n    def __init__(self, lattice_constant):\n        self.lattice_constant = lattice_constant\n\n    def generate_supercell(self, supercell):\n        \"\"\"\n        Returns the supercell.\n        \n        Args:\n            supercell (int or [3x1]): integer or vector indicating the number of repetition in each dimension.\n        \n        Returns:\n            [3x3]: A 3D array that contains the scaled lattice vectors.\n"
    },
    {
        "original": "def is_invalid_params(func, *args, **kwargs):\n    \"\"\" Check, whether function 'func' accepts parameters 'args', 'kwargs'.\n\n    NOTE: Method is called after funct(*args, **kwargs) generated TypeError,\n    it is aimed to destinguish TypeError because of invalid parameters from\n    TypeError from inside the function.\n\n    .. versionadded: 1.9.0\n\n    \"\"\"\n    # For builtin functions inspect.getargspec(funct) return error. If builtin\n    # function generates TypeError, it is because of wrong parameters.\n    if not inspect.isfunction(func):\n        return True\n\n    funcargs, varargs, varkwargs, defaults = inspect.getargspec(func)\n    if defaults:\n        funcargs = funcargs[:-len(defaults)]\n\n    if args and len(args) != len(funcargs):\n            return True\n    if kwargs and set(kwargs.keys()) != set(funcargs):\n        return True\n\n    if not args and not kwargs and funcargs:\n        return True\n\n    return False",
        "sampled": "```python\ndef is_invalid_params(func, *args, **kwargs):\n    \"\"\"Check, whether function 'func' accepts parameters 'args', 'kwargs'.\"\"\"\n    \n# Define a doc string for this function includes info about what the function is supposed to do.\n    \ntry:\n    # Try to call the function with the given arguments and keyword arguments\n    func(*args, **kwargs)\nexcept TypeError:\n    # If a TypeError occurs during the execution of the try block and we're not inside"
    },
    {
        "original": "def _anim_data(self, anim, fmt):\n        \"\"\"\n        Render a matplotlib animation object and return the corresponding data.\n        \"\"\"\n        (writer, _, anim_kwargs, extra_args) = ANIMATION_OPTS[fmt]\n        if extra_args != []:\n            anim_kwargs = dict(anim_kwargs, extra_args=extra_args)\n\n        if self.fps is not None: anim_kwargs['fps'] = max([int(self.fps), 1])\n        if self.dpi is not None: anim_kwargs['dpi'] = self.dpi\n        if not hasattr(anim, '_encoded_video'):\n            # Windows will throw PermissionError with auto-delete\n            with NamedTemporaryFile(suffix='.%s' % fmt, delete=False) as f:\n                anim.save(f.name, writer=writer, **anim_kwargs)\n                video = f.read()\n            f.close()\n            os.remove(f.name)\n        return video",
        "sampled": "```python\nimport matplotlib.animation as animation\nimport numpy as np\n\nclass AnimationDataRenderer:\n    def __init__(self):\n        pass\n\n    def _anim_data(self, anim, fmt):\n        \"\"\"\n        Render a matplotlib animation object and return the corresponding data.\n\n        Parameters:\n            anim (matplotlib.animation.Animation): A Matplotlib animation object.\n            fmt (str): The desired format of the data.\n\n        Returns:\n            list: A list containing the data in the specified format.\n        \"\"\"\n"
    },
    {
        "original": "def gcd2(a, b):\n  \"\"\"Greatest common divisor using Euclid's algorithm.\"\"\"\n  while a:\n    a, b = b%a, a\n  return b",
        "sampled": "```python\nimport math\n\ndef gcd2(a, b):\n    \"\"\"Greatest common divisor using Euclid's algorithm.\"\"\"\n    while b:\n        a, b = b, a % b\n    return abs(a)\n\ndef dance_action_steps(n):\n    return int(math.sqrt(88 * n))\n\ndef DancePlacements(n):\n    # Calculate number of steps in dance actions\n    max_steps = dance_action_steps(n)\n    \n    # Initialize list of arrangements with increasing actions and decreasing steps to"
    },
    {
        "original": "def max_cation_insertion(self):\n        \"\"\"\n        Maximum number of cation A that can be inserted while maintaining charge-balance.\n        No consideration is given to whether there (geometrically speaking) are Li sites to actually accommodate the extra Li.\n\n        Returns:\n            integer amount of cation. Depends on cell size (this is an 'extrinsic' function!)\n        \"\"\"\n\n        # how much 'spare charge' is left in the redox metals for reduction?\n        lowest_oxid = defaultdict(lambda: 2, {'Cu': 1})  # only Cu can go down to 1+\n        oxid_pot = sum([(spec.oxi_state - min(\n            e for e in Element(spec.symbol).oxidation_states if e >= lowest_oxid[spec.symbol])) *\n            self.comp[spec] for spec in self.comp if\n            is_redox_active_intercalation(Element(spec.symbol))])\n\n        return oxid_pot / self.cation_charge",
        "sampled": "```python\ndef max_cation_insertion(self):\n    \"\"\"\n    Maximum number of cation A that can be inserted while maintaining charge-balance.\n    \n    Returns:\n        integer amount of cation. Depends on cell size (this is an 'extrinsic' function!)\n    \"\"\"\n    \n    # Assume we have a variable to represent the number of Li sites in the cell, since this determines our maximum capacity to maintain charge balance\n    li_sites = self.li_sites  # get number of Li"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'id') and self.id is not None:\n            _dict['id'] = self.id\n        if hasattr(self, 'metadata') and self.metadata is not None:\n            _dict['metadata'] = self.metadata\n        if hasattr(self, 'collection_id') and self.collection_id is not None:\n            _dict['collection_id'] = self.collection_id\n        if hasattr(self,\n                   'result_metadata') and self.result_metadata is not None:\n            _dict['result_metadata'] = self.result_metadata._to_dict()\n        if hasattr(self, 'title') and self.title is not None:\n            _dict['title'] = self.title\n        if hasattr(self, 'code') and self.code is not None:\n            _dict['code'] = self.code\n        if hasattr(self, 'filename') and self.filename is not None:\n            _dict['filename'] = self.filename\n        if hasattr(self, 'file_type') and self.file_type is not None:\n            _dict['file_type'] = self.file_type\n        if hasattr(self, 'sha1') and self.sha1 is not None:\n            _dict['sha1'] = self.sha1\n        if hasattr(self, 'notices') and self.notices is not None:\n            _dict['notices'] = [x._to_dict() for x in self.notices]\n        if hasattr(self, '_additionalProperties'):\n            for _key in self._additionalProperties:\n                _value = getattr(self, _key, None)\n                if _value is not None:\n                    _dict[_key] = _value\n        return _dict",
        "sampled": "```python\nclass Solution:\n    def noticeTime(self, start1: int, start2: int, duration1: int, duration2: int) -> str:\n        end1 = start1 + duration1 - 60\n        end2 = start2\n        \n        if (start2 >= end1) or (end2 <= end1):\n            return \"Yes\"\n        \n        if (end2 > property_0) or (property_0 > start1):\n            return \""
    },
    {
        "original": "def nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    verbose : boolean\n        print additional information about the node\n        e.g. platform version, hvm capable, ...\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    ret = {} if verbose else []\n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    ## get list of nodes\n    try:\n        for cn in client.cmd_iter('G@virtual:physical and G@os:smartos',\n                                  'grains.items', tgt_type='compound'):\n            if not cn:\n                continue\n            node = next(six.iterkeys(cn))\n            if not isinstance(cn[node], dict) or \\\n                    'ret' not in cn[node] or \\\n                    not isinstance(cn[node]['ret'], dict):\n                continue\n            if verbose:\n                ret[node] = {}\n                ret[node]['version'] = {}\n                ret[node]['version']['platform'] = cn[node]['ret']['osrelease']\n                if 'computenode_sdc_version' in cn[node]['ret']:\n                    ret[node]['version']['sdc'] = cn[node]['ret']['computenode_sdc_version']\n                ret[node]['vms'] = {}\n                if 'computenode_vm_capable' in cn[node]['ret'] and \\\n                        cn[node]['ret']['computenode_vm_capable'] and \\\n                        'computenode_vm_hw_virt' in cn[node]['ret']:\n                    ret[node]['vms']['hw_cap'] = cn[node]['ret']['computenode_vm_hw_virt']\n                else:\n                    ret[node]['vms']['hw_cap'] = False\n                if 'computenode_vms_running' in cn[node]['ret']:\n                    ret[node]['vms']['running'] = cn[node]['ret']['computenode_vms_running']\n            else:\n                ret.append(node)\n    except SaltClientError as client_error:\n        return \"{0}\".format(client_error)\n\n    if not verbose:\n        ret.sort()\n    return ret",
        "sampled": "```python\nimport subprocess\n\ndef nodes(verbose=False):\n    \"\"\"\n    List all compute nodes\n\n    verbose : boolean\n        print additional information about the node\n        e.g. platform version, hvm capable, ...\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vmadm.nodes\n        salt-run vmadm.nodes verbose=True\n    \"\"\"\n    \n    # Use subprocess to run the command 'salt-run vmadm.nodes'\n    output = subprocess.check_output(['salt-run', '"
    },
    {
        "original": "def _locations_mirror(x):\n    \"\"\"\n    Mirrors the points in a list-of-list-of-...-of-list-of-points.\n    For example:\n    >>> _locations_mirror([[[1, 2], [3, 4]], [5, 6], [7, 8]])\n    [[[2, 1], [4, 3]], [6, 5], [8, 7]]\n\n    \"\"\"\n    if hasattr(x, '__iter__'):\n        if hasattr(x[0], '__iter__'):\n            return list(map(_locations_mirror, x))\n        else:\n            return list(x[::-1])\n    else:\n        return x",
        "sampled": "```python\ndef _locations_mirror(x):\n    \"\"\"\n    Mirrors the points in a list-of-list-of-...-of-list-of-points.\n    \n    Args:\n        x: A list of lists of ... of lists of points.\n        \n    Returns:\n        A mirrored version of the input.\n        \n    Example:\n        >>> _locations_mirror([[[1, 2], [3, 4]], [5, 6], [7, 8]])\n        [[[2, 1"
    },
    {
        "original": "def get_disk_partition_info(host_ref, disk_id, storage_system=None):\n    \"\"\"\n    Returns all partitions on a disk\n\n    host_ref\n        The reference of the ESXi host containing the disk\n\n    disk_id\n        The canonical name of the disk whose partitions are to be removed\n\n    storage_system\n        The ESXi host's storage system. Default is None.\n    \"\"\"\n    hostname = get_managed_object_name(host_ref)\n    service_instance = get_service_instance_from_managed_object(host_ref)\n    if not storage_system:\n        storage_system = get_storage_system(service_instance, host_ref,\n                                            hostname)\n\n    props = get_properties_of_managed_object(storage_system,\n                                             ['storageDeviceInfo.scsiLun'])\n    if not props.get('storageDeviceInfo.scsiLun'):\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'No devices were retrieved in host \\'{0}\\''.format(hostname))\n    log.trace(\n        '[%s] Retrieved %s devices: %s',\n        hostname,\n        len(props['storageDeviceInfo.scsiLun']),\n        ', '.join([l.canonicalName\n                   for l in props['storageDeviceInfo.scsiLun']])\n    )\n    disks = [l for l in props['storageDeviceInfo.scsiLun']\n             if isinstance(l, vim.HostScsiDisk) and\n             l.canonicalName == disk_id]\n    if not disks:\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'Disk \\'{0}\\' was not found in host \\'{1}\\''\n            ''.format(disk_id, hostname))\n    log.trace('[%s] device_path = %s', hostname, disks[0].devicePath)\n    partition_info = _get_partition_info(storage_system, disks[0].devicePath)\n    log.trace('[%s] Retrieved %s partition(s) on disk \\'%s\\'',\n              hostname, len(partition_info.spec.partition), disk_id)\n    return partition_info",
        "sampled": "```python\nimport requests\n\ndef get_disk_partition_info(host_ref, disk_id, storage_system=None):\n    \"\"\"\n    Returns all partitions on a disk\n\n    host_ref\n        The reference of the ESXi host containing the disk\n\n    disk_id\n        The canonical name of the disk whose partitions are to be removed\n\n    storage_system\n        The ESXi host's storage system. Default is None.\n    \"\"\"\n\n    # Construct API endpoint URL for getting disk partition info\n    url = f"
    },
    {
        "original": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n        _dict = {}\n        if hasattr(self, 'voices') and self.voices is not None:\n            _dict['voices'] = [x._to_dict() for x in self.voices]\n        return _dict",
        "sampled": "```python\nimport math\n\nclass Solution:\n    def averageWaitingTime(self, p):\n        waiting_time = 0\n        t = 0 # processor available at time t\n        \n        for arrival, duration in sorted(p): \n            t = max(t, arrival) + duration # newest appointment scheduling begins from time of the latest current appointment and adds to it.\n            waiting_time += (t - arrival)\n            \n        return waiting_time / len(p)\n\n\n# Example usage:\nobj = Solution()\n"
    },
    {
        "original": "def read_piezo_tensor(self):\n        \"\"\"\n        Parse the piezo tensor data\n        \"\"\"\n        header_pattern = r\"PIEZOELECTRIC TENSOR  for field in x, y, \" \\\n                         r\"z\\s+\\(C/m\\^2\\)\\s+([X-Z][X-Z]\\s+)+\\-+\"\n        row_pattern = r\"[x-z]\\s+\" + r\"\\s+\".join([r\"(\\-*[\\.\\d]+)\"] * 6)\n        footer_pattern = r\"BORN EFFECTIVE\"\n        pt_table = self.read_table_pattern(header_pattern, row_pattern,\n                                           footer_pattern, postprocess=float)\n        self.data[\"piezo_tensor\"] = pt_table",
        "sampled": "```python\nclass PiezoTensorParser:\n    def __init__(self, tensor_data):\n        \"\"\"\n        Initialize the parser with the piezo tensor data\n        \"\"\"\n        self.tensor_data = tensor_data\n\n    def read_piezo_tensor(self):\n        \"\"\"\n        Parse the piezo tensor data\n        \"\"\"\n\n# Assuming you have a function to parse and validate the input data\ndef parse_input(tensor_str):\n    rows = []\n    for line in tensor_str.splitlines():\n        if line.strip"
    },
    {
        "original": "def mask(array, predicates, new_value, ty):\n    \"\"\"\n    Returns a new array, with each element in the original array satisfying the\n    passed-in predicate set to `new_value`\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        predicates (WeldObject / Numpy.ndarray<bool>): Predicate set\n        new_value (WeldObject / Numpy.ndarray / str): mask value\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    predicates_var = weld_obj.update(predicates)\n    if isinstance(predicates, WeldObject):\n        predicates_var = predicates.obj_id\n        weld_obj.dependencies[predicates_var] = predicates\n\n    if str(ty).startswith(\"vec\"):\n        new_value_var = weld_obj.update(new_value)\n        if isinstance(new_value, WeldObject):\n            new_value_var = new_value.obj_id\n            weld_obj.dependencies[new_value_var] = new_value\n    else:\n        new_value_var = \"%s(%s)\" % (ty, str(new_value))\n\n    weld_template = ",
        "sampled": "```python\nfrom weld import WeldObject, Type, int32_type, timedelta32764_type\n\ndef mask(array, predicates, new_value, ty):\n    \"\"\"\n    Returns a new array, with each element in the original array satisfying the passed-in predicate set to `new_value`\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        predicates (WeldObject / Numpy.ndarray<bool>): Predicate set\n        new_value (WeldObject / Numpy.ndarray"
    },
    {
        "original": "def transpose(self, method):\n        \"\"\"\n        Transpose bounding box (flip or rotate in 90 degree steps)\n        :param method: One of :py:attr:`PIL.Image.FLIP_LEFT_RIGHT`,\n          :py:attr:`PIL.Image.FLIP_TOP_BOTTOM`, :py:attr:`PIL.Image.ROTATE_90`,\n          :py:attr:`PIL.Image.ROTATE_180`, :py:attr:`PIL.Image.ROTATE_270`,\n          :py:attr:`PIL.Image.TRANSPOSE` or :py:attr:`PIL.Image.TRANSVERSE`.\n        \"\"\"\n        if method not in (FLIP_LEFT_RIGHT, FLIP_TOP_BOTTOM):\n            raise NotImplementedError(\n                \"Only FLIP_LEFT_RIGHT and FLIP_TOP_BOTTOM implemented\"\n            )\n\n        image_width, image_height = self.size\n        xmin, ymin, xmax, ymax = self._split_into_xyxy()\n        if method == FLIP_LEFT_RIGHT:\n            TO_REMOVE = 1\n            transposed_xmin = image_width - xmax - TO_REMOVE\n            transposed_xmax = image_width - xmin - TO_REMOVE\n            transposed_ymin = ymin\n            transposed_ymax = ymax\n        elif method == FLIP_TOP_BOTTOM:\n            transposed_xmin = xmin\n            transposed_xmax = xmax\n            transposed_ymin = image_height - ymax\n            transposed_ymax = image_height - ymin\n\n        transposed_boxes = torch.cat(\n            (transposed_xmin, transposed_ymin, transposed_xmax, transposed_ymax), dim=-1\n        )\n        bbox = BoxList(transposed_boxes, self.size, mode=\"xyxy\")\n        # bbox._copy_extra_fields(self)\n        for k, v in self.extra_fields.items():\n            if not isinstance(v, torch.Tensor):\n                v = v.transpose(method)\n            bbox.add_field(k, v)\n        return bbox.convert(self.mode)",
        "sampled": "```python\nfrom PIL import Image\n\nclass Solution:\n    def transpose(self, method):\n        \"\"\"\n        Transpose bounding box (flip or rotate in 90 degree steps)\n        :param method: One of :py:attr:`PIL.Image.FLIP_LEFT_RIGHT`,\n          :py:attr:`PIL.Image.FLIP_TOP_BOTTOM`, :py:attr:`PIL.Image.ROTATE_90`,\n          :py:attr:`PIL.Image.ROTATE_180`,"
    },
    {
        "original": "def _check_portname(name):\n    \"\"\"\n    Check if portname is valid and whether or not the directory exists in the\n    ports tree.\n    \"\"\"\n    if not isinstance(name, string_types) or '/' not in name:\n        raise SaltInvocationError(\n            'Invalid port name \\'{0}\\' (category required)'.format(name)\n        )\n\n    path = os.path.join('/usr/ports', name)\n    if not os.path.isdir(path):\n        raise SaltInvocationError('Path \\'{0}\\' does not exist'.format(path))\n\n    return path",
        "sampled": "```python\nimport os\n\ndef _check_portname(name):\n    \"\"\"\n    Check if portname is valid and whether or not the directory exists in the\n    ports tree.\n    \"\"\"\n    \n    # Directory structure of ports: /ports/portname/\n    # Check for '.' or '..' in the name\n    if '.' in name or '..' in name:\n        return False\n    \n    # Check if portname is an empty string\n    if not name:\n        return False\n    \n     #"
    },
    {
        "original": "def save_flash_segment(self, f, segment, checksum=None):\n        \"\"\" Save the next segment to the image file, return next checksum value if provided \"\"\"\n        segment_end_pos = f.tell() + len(segment.data) + self.SEG_HEADER_LEN\n        segment_len_remainder = segment_end_pos % self.IROM_ALIGN\n        if segment_len_remainder < 0x24:\n            # Work around a bug in ESP-IDF 2nd stage bootloader, that it didn't map the\n            # last MMU page, if an IROM/DROM segment was < 0x24 bytes over the page boundary.\n            segment.data += b'\\x00' * (0x24 - segment_len_remainder)\n        return self.save_segment(f, segment, checksum)",
        "sampled": "```python\nclass Flash:\n    def __init__(self):\n        self.checksum = 0\n\n    def save_flash_segment(self, section_length, segment):\n        \"\"\"\n        Saves the next segment to the image file, and returns the next checksum value.\n\n        Args:\n            section_length (int): The length of each segment in bytes.\n            segment (bytearray): The byte array representing the current segment.\n\n        Returns:\n            int: The next checksum value if provided, otherwise None.\n       "
    },
    {
        "original": "def _mark_master_dead(self, master):\n        \"\"\"\n        Mark a master as dead. This will start the sign-in routine\n        \"\"\"\n        # if its connected, mark it dead\n        if self._syndics[master].done():\n            syndic = self._syndics[master].result()  # pylint: disable=no-member\n            self._syndics[master] = syndic.reconnect()\n        else:\n            # TODO: debug?\n            log.info(\n                'Attempting to mark %s as dead, although it is already '\n                'marked dead', master\n            )",
        "sampled": "```python\nclass MasterController:\n    def __init__(self):\n        self.master_status = {}\n\n    def _mark_master_dead(self, master_id):\n        \"\"\"\n        Mark a master as dead. This will start the sign-in routine\n        \"\"\"\n        \n        # Check if the master exists in the system\n        if master_id in self.master_status:\n            # Update the status to \"dead\"\n            self.master_status[master_id] = \"dead\"\n            \n            # Start the sign-in routine"
    },
    {
        "original": "def handle_exception(self, exc_info=None, rendered=False, source_hint=None):\n        \"\"\"Exception handling helper.  This is used internally to either raise\n        rewritten exceptions or return a rendered traceback for the template.\n        \"\"\"\n        global _make_traceback\n        if exc_info is None:\n            exc_info = sys.exc_info()\n\n        # the debugging module is imported when it's used for the first time.\n        # we're doing a lot of stuff there and for applications that do not\n        # get any exceptions in template rendering there is no need to load\n        # all of that.\n        if _make_traceback is None:\n            from jinja2.debug import make_traceback as _make_traceback\n        traceback = _make_traceback(exc_info, source_hint)\n        if rendered and self.exception_formatter is not None:\n            return self.exception_formatter(traceback)\n        if self.exception_handler is not None:\n            self.exception_handler(traceback)\n        exc_type, exc_value, tb = traceback.standard_exc_info\n        raise exc_type, exc_value, tb",
        "sampled": "```python\nimport math\n\ndef nthUglyNumber(n, a, b, c):\n    \"\"\"\n    The k-th ugly number is equal to the k-th prime number.\n    Given n and k, return the kth prime number.\n    \n    Parameters:\n    n (int): The position of the prime number to be found\n    a (int): The first factor of the ugly numbers\n    b (int): The second factor of the ugly numbers\n    c (int): The"
    },
    {
        "original": "def _numpy_bytes_to_char(arr):\n    \"\"\"Like netCDF4.stringtochar, but faster and more flexible.\n    \"\"\"\n    # ensure the array is contiguous\n    arr = np.array(arr, copy=False, order='C', dtype=np.string_)\n    return arr.reshape(arr.shape + (1,)).view('S1')",
        "sampled": "```python\nimport numpy as np\n\ndef _numpy_bytes_to_char(arr):\n    \"\"\"\n    Like netCDF4.stringtochar, but faster and more flexible.\n\n    Parameters:\n    arr (numpy array): A 1-dimensional numpy array of bytes.\n\n    Returns:\n    char_array (numpy array): The input bytes interpreted as a character array.\n                             The character type can be specified in the byteorder parameter.\n                             It can also be an output from a Pascal or serialize() call.\n    \"\"\"\n\n"
    },
    {
        "original": "def unchanged(self):\n        \"\"\"\n        Returns all keys that have been unchanged.\n\n        If the keys are in child dictionaries they will be represented with\n        . notation\n        \"\"\"\n        def _unchanged(current_dict, diffs, prefix):\n            keys = []\n            for key in current_dict.keys():\n                if key not in diffs:\n                    keys.append('{0}{1}'.format(prefix, key))\n                elif isinstance(current_dict[key], dict):\n                    if 'new' in diffs[key]:\n                        # There is a diff\n                        continue\n                    else:\n                        keys.extend(\n                            _unchanged(current_dict[key],\n                                       diffs[key],\n                                       prefix='{0}{1}.'.format(prefix, key)))\n\n            return keys\n        return sorted(_unchanged(self.current_dict, self._diffs, prefix=''))",
        "sampled": "```python\nclass DictWithParent:\n    def __init__(self, data):\n        self.data = data\n        self.parent = None\n\n    def add_parent(self, parent):\n        self.parent = parent\n\nclass Node(DictWithParent):\n    \"\"\"\n    Base class for nodes in the tree.\n    \n    Each node must have a dictionary-like structure with keys and values,\n    which could be child nodes or values. A node must also keep track of its\n    parent node if there is"
    },
    {
        "original": "def parse_criteria(criteria_string):\n        \"\"\"\n        Parses a powerful and simple string criteria and generates a proper\n        mongo syntax criteria.\n\n        Args:\n            criteria_string (str): A string representing a search criteria.\n                Also supports wild cards. E.g.,\n                something like \"*2O\" gets converted to\n                {'pretty_formula': {'$in': [u'B2O', u'Xe2O', u\"Li2O\", ...]}}\n\n                Other syntax examples:\n                    mp-1234: Interpreted as a Materials ID.\n                    Fe2O3 or *2O3: Interpreted as reduced formulas.\n                    Li-Fe-O or *-Fe-O: Interpreted as chemical systems.\n\n                You can mix and match with spaces, which are interpreted as\n                \"OR\". E.g., \"mp-1234 FeO\" means query for all compounds with\n                reduced formula FeO or with materials_id mp-1234.\n\n        Returns:\n            A mongo query dict.\n        \"\"\"\n        toks = criteria_string.split()\n\n        def parse_sym(sym):\n            if sym == \"*\":\n                return [el.symbol for el in Element]\n            else:\n                m = re.match(r\"\\{(.*)\\}\", sym)\n                if m:\n                    return [s.strip() for s in m.group(1).split(\",\")]\n                else:\n                    return [sym]\n\n        def parse_tok(t):\n            if re.match(r\"\\w+-\\d+\", t):\n                return {\"task_id\": t}\n            elif \"-\" in t:\n                elements = [parse_sym(sym) for sym in t.split(\"-\")]\n                chemsyss = []\n                for cs in itertools.product(*elements):\n                    if len(set(cs)) == len(cs):\n                        # Check for valid symbols\n                        cs = [Element(s).symbol for s in cs]\n                        chemsyss.append(\"-\".join(sorted(cs)))\n                return {\"chemsys\": {\"$in\": chemsyss}}\n            else:\n                all_formulas = set()\n                explicit_els = []\n                wild_card_els = []\n                for sym in re.findall(\n                        r\"(\\*[\\.\\d]*|\\{.*\\}[\\.\\d]*|[A-Z][a-z]*)[\\.\\d]*\", t):\n                    if (\"*\" in sym) or (\"{\" in sym):\n                        wild_card_els.append(sym)\n                    else:\n                        m = re.match(r\"([A-Z][a-z]*)[\\.\\d]*\", sym)\n                        explicit_els.append(m.group(1))\n                nelements = len(wild_card_els) + len(set(explicit_els))\n                parts = re.split(r\"(\\*|\\{.*\\})\", t)\n                parts = [parse_sym(s) for s in parts if s != \"\"]\n                for f in itertools.product(*parts):\n                    c = Composition(\"\".join(f))\n                    if len(c) == nelements:\n                        # Check for valid Elements in keys.\n                        for e in c.keys():\n                            Element(e.symbol)\n                        all_formulas.add(c.reduced_formula)\n                return {\"pretty_formula\": {\"$in\": list(all_formulas)}}\n\n        if len(toks) == 1:\n            return parse_tok(toks[0])\n        else:\n            return {\"$or\": list(map(parse_tok, toks))}",
        "sampled": "```python\nfrom collections import defaultdict\n\ndef parse_criteria(criteria_string):\n    \"\"\"\n    Parses a powerful and simple string criteria and generates a proper\n    mongo syntax criteria.\n\n    Args:\n        criteria_string (str): A string representing a search criteria.\n            Also supports wild cards. E.g.,\n            something like \"*2O\" gets converted to\n            {'pretty_formula': {'$in': [u'B2O', u'Xe2O', u\"Li2O\", ...]}"
    },
    {
        "original": "def _validate_privileges(object_type, privs, privileges):\n    \"\"\"\n    Validate the supplied privileges\n    \"\"\"\n    if object_type != 'group':\n        _perms = [_PRIVILEGES_MAP[perm]\n                for perm in _PRIVILEGE_TYPE_MAP[object_type]]\n        _perms.append('ALL')\n\n        if object_type not in _PRIVILEGES_OBJECTS:\n            raise SaltInvocationError(\n                'Invalid object_type: {0} provided'.format(object_type))\n\n        if not set(privs).issubset(set(_perms)):\n            raise SaltInvocationError(\n                'Invalid privilege(s): {0} provided for object {1}'.format(\n                privileges, object_type))\n    else:\n        if privileges:\n            raise SaltInvocationError(\n                'The privileges option should not '\n                'be set for object_type group')",
        "sampled": "```python\ndef _validate_privileges(object_type, privs, privileges):\n    \"\"\"\n    Validate the supplied privileges\n    \n    Parameters:\n    object_type (str): Type of object\n    privs (dict): Dictionary containing intermediate values for validation.\n                 It should have following keys: \n                 'priv_obj_id':'id of the current privilege dict',\n                 'priv_dict': 'privileged dictionary which should be copied to \"valid_priv_serialised\"',\n                \n                 The existing keys in `privs`"
    },
    {
        "original": "def create_customer(self, full_name, email):\n        \"\"\"\n        Creating customer user\n\n        :param full_name: str\n        :param email: str\n        :return: New customer\n        \"\"\"\n        log.warning('Creating customer...')\n        data = {'fullName': full_name, 'email': email}\n\n        return self.post('rest/servicedeskapi/customer', headers=self.experimental_headers, data=data)",
        "sampled": "```python\nclass Customer:\n    def __init__(self, full_name, email):\n        \"\"\"\n        Creating customer user\n\n        :param full_name: str\n        :param email: str\n        \"\"\"\n        \n    @classmethod\n    def create_customer(cls, full_name: str, email: str) -> 'Customer':\n            return cls(full_name, email)\n\ndef test_create_customer():\n    talent_sprint_group = Customer.create_customer('Talent Sprint Group', 'group@example.com')\n    \n"
    },
    {
        "original": "def _merge_raw(self, other):\n        \"\"\"For use with binary arithmetic.\"\"\"\n        if other is None:\n            variables = OrderedDict(self.variables)\n        else:\n            # don't align because we already called xarray.align\n            variables = expand_and_merge_variables(\n                [self.variables, other.variables])\n        return variables",
        "sampled": "```python\nclass ComplexArithmeticObject:\n    def __init__(self, real=0, imag=0):\n        self.real = real\n        self.imag = imag\n\n    def display_value(self):\n        if self.imag >= 0:\n            return f\"{self.real} + {self.imag}j\"\n        else:\n            return f\"{self.real} - {abs(self.imag)}j\"\n\n    def _merge_raw(self, other):\n        \"\"\"For use with binary arithmetic.\"\"\"\n       "
    },
    {
        "original": "def login(self, username=None, password=None, **kwargs):\n        \"\"\"Login to a reddit site.\n\n        **DEPRECATED**. Will be removed in a future version of PRAW.\n\n        https://www.reddit.com/comments/2ujhkr/\n        https://www.reddit.com/comments/37e2mv/\n\n        Look for username first in parameter, then praw.ini and finally if both\n        were empty get it from stdin. Look for password in parameter, then\n        praw.ini (but only if username matches that in praw.ini) and finally\n        if they both are empty get it with getpass. Add the variables ``user``\n        (username) and ``pswd`` (password) to your praw.ini file to allow for\n        auto-login.\n\n        A successful login will overwrite any existing authentication.\n\n        \"\"\"\n        if password and not username:\n            raise Exception('Username must be provided when password is.')\n        user = username or self.config.user\n        if not user:\n            sys.stdout.write('Username: ')\n            sys.stdout.flush()\n            user = sys.stdin.readline().strip()\n            pswd = None\n        else:\n            pswd = password or self.config.pswd\n        if not pswd:\n            import getpass\n            pswd = getpass.getpass('Password for {0}: '.format(user)\n                                   .encode('ascii', 'ignore'))\n\n        data = {'passwd': pswd,\n                'user': user}\n        self.clear_authentication()\n        self.request_json(self.config['login'], data=data)\n        # Update authentication settings\n        self._authentication = True\n        self.user = self.get_redditor(user)\n        self.user.__class__ = objects.LoggedInRedditor",
        "sampled": "```python\nimport getpass\nimport configparser\n\nclass RedditLogin:\n    def __init__(self):\n        self.config = configparser.ConfigParser()\n        self.config.read('praw.ini')\n\n    def login(self, username=None, password=None):\n        if username is None:\n            # Look for username in praw.ini\n            if 'DEFAULT' in self.config and 'user' in self.config['DEFAULT']:\n                username = self.config['DEFAULT']['user']\n            else:\n                # Get username from"
    },
    {
        "original": "def operate_magmom(self, magmom):\n        \"\"\"\n        Apply time reversal operator on the magnetic moment. Note that\n        magnetic moments transform as axial vectors, not polar vectors. \n\n        See 'Symmetry and magnetic structures', Rodr\u00edguez-Carvajal and\n        Bour\u00e9e for a good discussion. DOI: 10.1051/epjconf/20122200010\n\n        Args:\n            magmom: Magnetic moment as electronic_structure.core.Magmom\n            class or as list or np array-like\n\n        Returns:\n            Magnetic moment after operator applied as Magmom class\n        \"\"\"\n\n        magmom = Magmom(magmom)  # type casting to handle lists as input\n\n        transformed_moment = self.apply_rotation_only(magmom.global_moment) * \\\n            np.linalg.det(self.rotation_matrix) * self.time_reversal\n\n        # retains input spin axis if different from default\n        return Magmom.from_global_moment_and_saxis(transformed_moment, magmom.saxis)",
        "sampled": "```python\nimport numpy as np\nfrom typing import Union\n\nclass ElectronicStructure:\n    class Magmom:\n        def __init__(self, magmom: Union[list, np.ndarray]):\n            \"\"\"\n            Args:\n                magmom: Magnetic moment (list or np array-like)\n            \"\"\"\n            self.magmom = np.array(magmom)\n\n        @property\n        def magmom(self) -> np.ndarray:\n            return self._magmom\n\n        @magmom.setter\n        def magmom"
    },
    {
        "original": "def _packets_from_tshark_sync(self, packet_count=None, existing_process=None):\n        \"\"\"\n        Returns a generator of packets.\n        This is the sync version of packets_from_tshark. It wait for the completion of each coroutine and\n         reimplements reading packets in a sync way, yielding each packet as it arrives.\n\n        :param packet_count: If given, stops after this amount of packets is captured.\n        \"\"\"\n        # NOTE: This has code duplication with the async version, think about how to solve this\n        tshark_process = existing_process or self.eventloop.run_until_complete(self._get_tshark_process())\n        psml_structure, data = self.eventloop.run_until_complete(self._get_psml_struct(tshark_process.stdout))\n        packets_captured = 0\n\n        data = b''\n        try:\n            while True:\n                try:\n                    packet, data = self.eventloop.run_until_complete(\n                        self._get_packet_from_stream(tshark_process.stdout, data, psml_structure=psml_structure,\n                                                     got_first_packet=packets_captured > 0))\n\n                except EOFError:\n                    self._log.debug('EOF reached (sync)')\n                    break\n\n                if packet:\n                    packets_captured += 1\n                    yield packet\n                if packet_count and packets_captured >= packet_count:\n                    break\n        finally:\n            self.eventloop.run_until_complete(self._cleanup_subprocess(tshark_process))",
        "sampled": "```python\nimport asyncio\nfrom asyncio.coroutine import coroutine\n\nclass TShark:\n    async def _packets_from_tshark_sync(self, packet_count=None):\n        \"\"\"\n        Returns a generator of packets.\n        This is the async version of packets_from_tshark. It reimplements reading packets in an async way,\n         yielding each packet as it arrives.\n\n        :param packet_count: If given, stops after this amount of packets is captured.\n        \"\"\"\n        if packet_count and"
    },
    {
        "original": "def send_private_message(self):\n        \"\"\"\n        Send a new private message to another user.\n        \"\"\"\n        message_info = docs.MESSAGE_FILE\n        with self.term.open_editor(message_info) as text:\n            if not text:\n                self.term.show_notification('Canceled')\n                return\n\n            parts = text.split('\\n', 2)\n            if len(parts) == 1:\n                self.term.show_notification('Missing message subject')\n                return\n            elif len(parts) == 2:\n                self.term.show_notification('Missing message body')\n                return\n\n            recipient, subject, message = parts\n            recipient = recipient.strip()\n            subject = subject.strip()\n            message = message.rstrip()\n\n            if not recipient:\n                self.term.show_notification('Missing recipient')\n                return\n            elif not subject:\n                self.term.show_notification('Missing message subject')\n                return\n            elif not message:\n                self.term.show_notification('Missing message body')\n                return\n\n            with self.term.loader('Sending message', delay=0):\n                self.reddit.send_message(\n                    recipient, subject, message, raise_captcha_exception=True)\n                # Give reddit time to process the message\n                time.sleep(2.0)\n\n            if self.term.loader.exception:\n                raise TemporaryFileError()\n            else:\n                self.term.show_notification('Message sent!')\n                self.selected_page = self.open_inbox_page('sent')",
        "sampled": "```python\nfrom abc import ABC, abstractmethod\n\nclass MessageService(ABC):\n    @abstractmethod\n    def send_private_message(self, sender_id, recipient_id, content):\n        \"\"\"\n        Send a new private message to another user.\n\n        Args:\n            sender_id (int): The ID of the user who is sending the message.\n            recipient_id (int): The ID of the user who will receive the message.\n            content (str): The text content of the private message.\n\n        Returns"
    },
    {
        "original": "def serialize(data):\n    \"\"\"Serialize a dict into a JSON formatted string.\n\n        This function enforces rules like the separator and order of keys.\n        This ensures that all dicts are serialized in the same way.\n\n        This is specially important for hashing data. We need to make sure that\n        everyone serializes their data in the same way so that we do not have\n        hash mismatches for the same structure due to serialization\n        differences.\n\n        Args:\n            data (dict): dict to serialize\n\n        Returns:\n            str: JSON formatted string\n\n    \"\"\"\n    return rapidjson.dumps(data, skipkeys=False, ensure_ascii=False,\n                           sort_keys=True)",
        "sampled": "```python\nimport json\n\ndef serialize(data):\n    \"\"\"Serialize a dict into a JSON formatted string.\n\n        This function enforces rules like the separator and order of keys.\n        This ensures that all dicts are serialized in the same way.\n\n        This is specially important for hashing data. We need to make sure that\n        everyone serializes their data in the same way so that we do not have\n        hash mismatches for the same structure due to serialization\n        differences.\n\n        Args:\n           "
    },
    {
        "original": "def query(method='servers', server_id=None, command=None, args=None,\n          http_method='GET', root='api_root'):\n    \"\"\" Make a call to the Scaleway API.\n    \"\"\"\n\n    if root == 'api_root':\n        default_url = 'https://cp-par1.scaleway.com'\n    else:\n        default_url = 'https://api-marketplace.scaleway.com'\n\n    base_path = six.text_type(config.get_cloud_config_value(\n        root,\n        get_configured_provider(),\n        __opts__,\n        search_global=False,\n        default=default_url\n    ))\n\n    path = '{0}/{1}/'.format(base_path, method)\n\n    if server_id:\n        path += '{0}/'.format(server_id)\n\n    if command:\n        path += command\n\n    if not isinstance(args, dict):\n        args = {}\n\n    token = config.get_cloud_config_value(\n        'token', get_configured_provider(), __opts__, search_global=False\n    )\n\n    data = salt.utils.json.dumps(args)\n\n    request = __utils__[\"http.query\"](path,\n                                      method=http_method,\n                                      data=data,\n                                      status=True,\n                                      decode=True,\n                                      decode_type='json',\n                                      data_render=True,\n                                      data_renderer='json',\n                                      headers=True,\n                                      header_dict={'X-Auth-Token': token,\n                                                   'User-Agent': \"salt-cloud\",\n                                                   'Content-Type': 'application/json'})\n    if request['status'] > 299:\n        raise SaltCloudSystemExit(\n            'An error occurred while querying Scaleway. HTTP Code: {0}  '\n            'Error: \\'{1}\\''.format(\n                request['status'],\n                request['error']\n            )\n        )\n\n    # success without data\n    if request['status'] == 204:\n        return True\n\n    return salt.utils.json.loads(request['body'])",
        "sampled": "```python\ndef is_power_of_two(n):\n    \"\"\"\n    Checks if a given number is a power of two.\n    \n    A number is a power of two if it has exactly one bit set to 1 in its binary representation.\n    \n    Parameters:\n    n (int): The number to check.\n    \n    Returns:\n    bool: True if the number is a power of two, False otherwise.\n    \"\"\"\n    return n != 0 and (n & (n - 1))"
    },
    {
        "original": "def _produce_jbig2_images(jbig2_groups, root, log, options):\n    \"\"\"Produce JBIG2 images from their groups\"\"\"\n\n    def jbig2_group_futures(executor, root, groups):\n        for group, xref_exts in groups.items():\n            prefix = f'group{group:08d}'\n            future = executor.submit(\n                jbig2enc.convert_group,\n                cwd=fspath(root),\n                infiles=(img_name(root, xref, ext) for xref, ext in xref_exts),\n                out_prefix=prefix,\n            )\n            yield future\n\n    def jbig2_single_futures(executor, root, groups):\n        for group, xref_exts in groups.items():\n            prefix = f'group{group:08d}'\n            # Second loop is to ensure multiple images per page are unpacked\n            for n, xref_ext in enumerate(xref_exts):\n                xref, ext = xref_ext\n                future = executor.submit(\n                    jbig2enc.convert_single,\n                    cwd=fspath(root),\n                    infile=img_name(root, xref, ext),\n                    outfile=root / f'{prefix}.{n:04d}',\n                )\n                yield future\n\n    if options.jbig2_page_group_size > 1:\n        jbig2_futures = jbig2_group_futures\n    else:\n        jbig2_futures = jbig2_single_futures\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=options.jobs) as executor:\n        futures = jbig2_futures(executor, root, jbig2_groups)\n        for future in concurrent.futures.as_completed(futures):\n            proc = future.result()\n            log.debug(proc.stderr.decode())",
        "sampled": "```python\ndef _produce_jbig2_images(jbig2_groups, root, log, options):\n    \"\"\"\n    Produce JBIG2 images from their groups\n    \"\"\"\n    \n    # Initialize an empty list to store the produced images\n    produced_images = []\n    \n    # Iterate over each group of JBIG2 images\n    for group in jbig2_groups:\n        # Extract the related component image for this group (assuming it's a class)\n        if hasattr(group, 'related_component"
    },
    {
        "original": "async def list_pairwise(wallet_handle: int) -> str:\n    \"\"\"\n    Get list of saved pairwise.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :return: pairwise_list: list of saved pairwise\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"list_pairwise: >>> wallet_handle: %r\", wallet_handle)\n\n    if not hasattr(list_pairwise, \"cb\"):\n        logger.debug(\"list_pairwise: Creating callback\")\n        list_pairwise.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n\n    pairwise_list = await do_call('indy_list_pairwise',\n                                  c_wallet_handle,\n                                  list_pairwise.cb)\n\n    res = pairwise_list.decode()\n    logger.debug(\"list_pairwise: <<< res: %r\", res)\n    return res",
        "sampled": "```python\nasync def list_pairwise(wallet_handle: int) -> str:\n    \"\"\"\n    Get list of saved pairwise.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :return: pairwise_list: list of saved pairwise\n    \"\"\"\n    \n    # First, we need to import the necessary libraries. For this task, we will assume that 'open_wallet' and 'Pairwise' are custom classes.\n    \n    # We will retrieve a wallet object using the given handle"
    },
    {
        "original": "def send_config_set(\n        self,\n        config_commands=None,\n        exit_config_mode=False,\n        delay_factor=1,\n        max_loops=150,\n        strip_prompt=False,\n        strip_command=False,\n        config_mode_command=None,\n    ):\n        \"\"\"Remain in configuration mode.\"\"\"\n        return super(VyOSSSH, self).send_config_set(\n            config_commands=config_commands,\n            exit_config_mode=exit_config_mode,\n            delay_factor=delay_factor,\n            max_loops=max_loops,\n            strip_prompt=strip_prompt,\n            strip_command=strip_command,\n            config_mode_command=config_mode_command,\n        )",
        "sampled": "```python\nclass Device:\n    def __init__(self):\n        self.config_mode = False\n\n    def send_config_set(\n        self,\n        config_commands=None,\n        exit_config_mode=False,\n        delay_factor=1,\n        max_loops=150,\n        strip_prompt=False,\n        strip_command=False,\n        config_mode_command=None\n    ):\n        \n\"\"\"Remain in configuration mode.\"\"\"\n# Check if initilaizing config mode, if so run it.\nif not self.config_mode:   \n"
    },
    {
        "original": "def get_if_addr6(iff):\n    \"\"\"\n    Returns the main global unicast address associated with provided\n    interface, in human readable form. If no global address is found,\n    None is returned.\n    \"\"\"\n    return next((x[0] for x in in6_getifaddr()\n                 if x[2] == iff and x[1] == IPV6_ADDR_GLOBAL), None)",
        "sampled": "```python\nimport netifaces as ni\n\ndef get_if_addr6(iff):\n    \"\"\"\n    Returns the main global unicast address associated with provided\n    interface, in human readable form. If no global address is found,\n    None is returned.\n    \"\"\"\n    \n    # Try to get interface information from theIFF parameter\n    try:\n        iff_info = ni.ifaddresses(iff)[ni.AF_INET6][0]\n        \n        # Filter out temporary (DAD) addresses and those"
    },
    {
        "original": "def _get_ionic_radii(self):\n        \"\"\"\n        Computes ionic radii of elements for all sites in the structure.\n        If valence is zero, atomic radius is used.\n        \"\"\"\n        radii = []\n        vnn = VoronoiNN()\n\n        def nearest_key(sorted_vals, key):\n            i = bisect_left(sorted_vals, key)\n            if i == len(sorted_vals):\n                return sorted_vals[-1]\n            if i == 0:\n                return sorted_vals[0]\n            before = sorted_vals[i - 1]\n            after = sorted_vals[i]\n            if after - key < key - before:\n                return after\n            else:\n                return before\n\n        for i in range(len(self._structure.sites)):\n            site = self._structure.sites[i]\n            if isinstance(site.specie, Element):\n                radius = site.specie.atomic_radius\n                # Handle elements with no atomic_radius\n                # by using calculated values instead.\n                if radius is None:\n                    radius = site.specie.atomic_radius_calculated\n                if radius is None:\n                    raise ValueError(\n                        \"cannot assign radius to element {}\".format(\n                            site.specie))\n                radii.append(radius)\n                continue\n\n            el = site.specie.symbol\n            oxi_state = int(round(site.specie.oxi_state))\n            coord_no = int(round(vnn.get_cn(self._structure, i)))\n            try:\n                tab_oxi_states = sorted(map(int, _ion_radii[el].keys()))\n                oxi_state = nearest_key(tab_oxi_states, oxi_state)\n                radius = _ion_radii[el][str(oxi_state)][str(coord_no)]\n            except KeyError:\n                if vnn.get_cn(self._structure, i) - coord_no > 0:\n                    new_coord_no = coord_no + 1\n                else:\n                    new_coord_no = coord_no - 1\n                try:\n                    radius = _ion_radii[el][str(oxi_state)][str(new_coord_no)]\n                    coord_no = new_coord_no\n                except:\n                    tab_coords = sorted(\n                        map(int, _ion_radii[el][str(oxi_state)].keys()))\n                    new_coord_no = nearest_key(tab_coords, coord_no)\n                    i = 0\n                    for val in tab_coords:\n                        if val > coord_no:\n                            break\n                        i = i + 1\n                    if i == len(tab_coords):\n                        key = str(tab_coords[-1])\n                        radius = _ion_radii[el][str(oxi_state)][key]\n                    elif i == 0:\n                        key = str(tab_coords[0])\n                        radius = _ion_radii[el][str(oxi_state)][key]\n                    else:\n                        key = str(tab_coords[i - 1])\n                        radius1 = _ion_radii[el][str(oxi_state)][key]\n                        key = str(tab_coords[i])\n                        radius2 = _ion_radii[el][str(oxi_state)][key]\n                        radius = (radius1 + radius2) / 2\n\n            # implement complex checks later\n            radii.append(radius)\n        return radii",
        "sampled": "```python\nclass IonicRadiiCalculator:\n    def _get_ionic_radii(self, structure):\n        \"\"\"\n        Computes ionic radii of elements for all sites in the structure.\n        If valence is zero, atomic radius is used.\n\n        Args:\n            structure (list): A list containing site information.\n                              Each site is represented as a dictionary with keys 'element', 'valence', and 'atomic_radius'.\n\n        Returns:\n            dict: A dictionary mapping site indices to their corresponding"
    },
    {
        "original": "def tablespace_list(user=None, host=None, port=None, maintenance_db=None,\n                    password=None, runas=None):\n    \"\"\"\n    Return dictionary with information about tablespaces of a Postgres server.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.tablespace_list\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n\n    ret = {}\n\n    query = (\n        'SELECT spcname as \"Name\", pga.rolname as \"Owner\", spcacl as \"ACL\", '\n        'spcoptions as \"Opts\", pg_tablespace_location(pgts.oid) as \"Location\" '\n        'FROM pg_tablespace pgts, pg_roles pga WHERE pga.oid = pgts.spcowner'\n    )\n\n    rows = __salt__['postgres.psql_query'](query, runas=runas, host=host,\n                                           user=user, port=port,\n                                           maintenance_db=maintenance_db,\n                                           password=password)\n\n    for row in rows:\n        ret[row['Name']] = row\n        ret[row['Name']].pop('Name')\n\n    return ret",
        "sampled": "```python\nimport psycopg2\n\ndef tablespace_list(user=None, host=None, port=None, maintenance_db=None,\n                    password=None, runas=None):\n    \"\"\"\n    Return dictionary with information about tablespaces of a Postgres server.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.tablespace_list\n\n    .. versionadded:: 2015.8.0\n    \"\"\"\n\n    conn = None\n    try:\n        # Establish a connection to the database\n       "
    },
    {
        "original": "def pytorch_id(node):\n    \"\"\"Returns a unique ID for a node.\"\"\"\n    # After ONNX simplification, the scopeName is not unique anymore\n    # so append node outputs to guarantee uniqueness\n    return node.scopeName() + \"/outputs/\" + \"/\".join([o.uniqueName() for o in node.outputs()])",
        "sampled": "```python\nimport uuid\n\nclass Node:\n    id_counter = 0\n    nodes = {}\n\n    def __init__(self, name):\n        self.name = name\n\n    @classmethod\n    def pytorch_id(cls, node):\n        if node not in cls.nodes:\n            cls.nodes[node] = {'id': str(uuid.uuid4()), 'id_counter': cls.id_counter}\n            cls.id_counter += 1\n        return cls.nodes[node]['id']\n\nnode1 = Node('Node"
    },
    {
        "original": "def translate_doc(filename, destination='zh-CN', mix=True):\n    \"\"\"\n    translate a word document type of file and save the result as document and keep the exactly same file format. \n        :param filename: word doc file \n        :param destination='zh-CN': \n        :param mix=True: if True, will have original language and target language into the same doc. paragraphs by paragraphs.\n    \"\"\"\n    def tx(t): return Translator().translate(t, dest=destination).text\n    doc = Document(filename)\n    for p in doc.paragraphs:\n        txd = tx(p.text)\n\n        p.text = p.text + ('\\n' + txd if mix else '')\n\n    for table in doc.tables:\n        for row in table.rows:\n            for cell in row.cells:\n                txd = tx(cell.text)\n                p.text = cell.text + ('\\n' + txd if mix else '')\n\n    f = filename.replace('.doc', destination.lower() + '.doc')\n    doc.save(f)",
        "sampled": "```python\nimport docx\nimport pandas as pd\n\ndef translate_doc(filename, destination='zh-CN', mix=True):\n    # Read the word file using python-docx library\n    doc = docx.Document(filename)\n    \n    # Extract text from the document and split it into paragraphs\n    paragraphs = [paragraph.text for paragraph in doc.paragraphs]\n    \n    # Use googletrans library to translate text into destination language\n    from googletrans import Translator\n    \n    translator = Translator()\n    \n"
    },
    {
        "original": "def is_valid_ipv4_prefix(ipv4_prefix):\n    \"\"\"Returns True if *ipv4_prefix* is a valid prefix with mask.\n\n    Samples:\n        - valid prefix: 1.1.1.0/32, 244.244.244.1/10\n        - invalid prefix: 255.2.2.2/2, 2.2.2/22, etc.\n    \"\"\"\n    if not isinstance(ipv4_prefix, str):\n        return False\n\n    tokens = ipv4_prefix.split('/')\n    if len(tokens) != 2:\n        return False\n\n    # Validate address/mask and return\n    return is_valid_ipv4(tokens[0]) and is_valid_ip_prefix(tokens[1], 32)",
        "sampled": "```python\nimport ipaddress\n\ndef is_valid_ipv4_prefix(ipv4_prefix):\n    \"\"\"\n    Returns True if *ipv4_prefix* is a valid prefix with mask.\n\n    Samples:\n        - valid prefix: 1.1.1.0/32, 244.244.244.1/10\n        - invalid prefix: 255.2.2.2/2, 2.2.2/22, etc.\n    \"\"\"\n\n    try:\n        #"
    },
    {
        "original": "def get_media_list(self, media_type, offset, count):\n        \"\"\"\n        \u83b7\u53d6\u7d20\u6750\u5217\u8868\u3002\n\n        :param media_type: \u7d20\u6750\u7684\u7c7b\u578b\uff0c\u56fe\u7247\uff08image\uff09\u3001\u89c6\u9891\uff08video\uff09\u3001\u8bed\u97f3 \uff08voice\uff09\u3001\u56fe\u6587\uff08news\uff09\n        :param offset: \u4ece\u5168\u90e8\u7d20\u6750\u7684\u8be5\u504f\u79fb\u4f4d\u7f6e\u5f00\u59cb\u8fd4\u56de\uff0c0\u8868\u793a\u4ece\u7b2c\u4e00\u4e2a\u7d20\u6750\u8fd4\u56de\n        :param count: \u8fd4\u56de\u7d20\u6750\u7684\u6570\u91cf\uff0c\u53d6\u503c\u57281\u523020\u4e4b\u95f4\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/material/batchget_material\",\n            data={\n                \"type\": media_type,\n                \"offset\": offset,\n                \"count\": count\n            }\n        )",
        "sampled": "```python\nclass MediaService:\n    def get_media_list(self, media_type, offset, count):\n        \"\"\"\n        \u83b7\u53d6\u7d20\u6750\u5217\u8868\u3002\n\n        :param media_type: \u7d20\u6750\u7684\u7c7b\u578b\uff0c\u56fe\u7247\uff08image\uff09\u3001\u89c6\u9891\uff08video\uff09\u3001\u8bed\u97f3 \uff08voice\uff09\u3001\u56fe\u6587\uff08news\uff09\n        :param offset: \u4ece\u5168\u90e8\u7d20\u6750\u7684\u8be5\u504f\u79fb\u4f4d\u7f6e\u5f00\u59cb\u8fd4\u56de\uff0c0\u8868\u793a\u4ece\u7b2c\u4e00\u4e2a\u7d20\u6750\u8fd4\u56de\n        :param count: \u8fd4\u56de\u7d20\u6750\u7684\u6570\u91cf\uff0c\u53d6\u503c\u57281"
    },
    {
        "original": "def loopUntil(\n            self, condition=None, timeout: float = 0) -> Iterator[object]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout in seconds.\n        The yielded value is that of the condition or False when timed out.\n\n        Args:\n            condition: Predicate function that is tested after every network\n            update.\n            timeout: Maximum time in seconds to wait.\n                If 0 then no timeout is used.\n        \"\"\"\n        endTime = time.time() + timeout\n        while True:\n            test = condition and condition()\n            if test:\n                yield test\n                return\n            elif timeout and time.time() > endTime:\n                yield False\n                return\n            else:\n                yield test\n            self.waitOnUpdate(endTime - time.time() if timeout else 0)",
        "sampled": "```python\nimport asyncio\nfrom typing import Iterator, Callable, Optional\n\nclass LoopUntil:\n    def __init__(self):\n        self.condition = None\n        self.timeout = 0\n\n    async def loopUntil(\n            self,\n            condition: Optional[Callable] = None,\n            timeout: float = 0) -> Iterator[object]:\n        \"\"\"\n        Iterate until condition is met, with optional timeout in seconds.\n        The yielded value is that of the condition or False when timed out"
    },
    {
        "original": "def _should_add_constraints(cls, state):\n        \"\"\"\n        Check to see if the current address concretization variable is any of the registered\n        constrained_addrs we want to allow concretization for\n        \"\"\"\n        expr = state.inspect.address_concretization_expr\n        hit_indices = cls._to_indices(state, expr)\n\n        for action in state.preconstrainer._constrained_addrs:\n            var_indices = cls._to_indices(state, action.addr)\n            if var_indices == hit_indices:\n                return True\n        return False",
        "sampled": "```python\nclass ConstrainedClass:\n    def __init__(self):\n        # Initialize a set of constrained addresses\n        self.constrained_addrs = set()\n\n    def register_constrained_addr(self, addr):\n        # Register a constrained address\n        self.constrained_addrs.add(addr)\n\n    def _should_add_constraints(self, state):\n        \"\"\"\n        Check to see if the current address concretization variable is any of the registered \n        constrained addresses we want to allow concretization for"
    },
    {
        "original": "def options(self, context, module_options):\n        \"\"\"\n           COMMAND  Mimikatz command to execute (default: 'sekurlsa::logonpasswords')\n        \"\"\"\n        self.command = 'privilege::debug sekurlsa::logonpasswords exit'\n        if module_options and 'COMMAND' in module_options:\n            self.command = module_options['COMMAND']\n\n        self.ps_script = obfs_ps_script('powersploit/Exfiltration/Invoke-Mimikatz.ps1')",
        "sampled": "```python\nclass Mimikatz:\n    def __init__(self):\n        self.command = 'sekurlsa::logonpasswords'\n\n    def options(self, context, module_options):\n        \"\"\"\n        COMMAND  Mimikatz command to execute (default: 'sekurlsa::logonpasswords')\n        \n        Parameters:\n        context (dict): Context of the execution\n        module_options (dict): Options for the module\n\n        Returns:\n        str: The mimikatz command to"
    },
    {
        "original": "def MarkDone(self, status=None):\n    \"\"\"Marks this flow as done.\"\"\"\n    stats_collector_instance.Get().IncrementCounter(\n        \"flow_completions\", fields=[compatibility.GetName(self.__class__)])\n\n    # Notify our parent flow or hunt that we are done (if there's a parent flow\n    # or hunt).\n    if self.rdf_flow.parent_flow_id or self.rdf_flow.parent_hunt_id:\n      status = rdf_flow_objects.FlowStatus(\n          client_id=self.rdf_flow.client_id,\n          request_id=self.rdf_flow.parent_request_id,\n          response_id=self.GetNextResponseId(),\n          status=rdf_flow_objects.FlowStatus.Status.OK,\n          cpu_time_used=self.rdf_flow.cpu_time_used,\n          network_bytes_sent=self.rdf_flow.network_bytes_sent,\n          flow_id=self.rdf_flow.parent_flow_id)\n      if self.rdf_flow.parent_flow_id:\n        self.flow_responses.append(status)\n      elif self.rdf_flow.parent_hunt_id:\n        db_compat.ProcessHuntFlowDone(self.rdf_flow, status_msg=status)\n\n    self.rdf_flow.flow_state = self.rdf_flow.FlowState.FINISHED\n\n    if self.ShouldSendNotifications():\n      self.NotifyAboutEnd()",
        "sampled": "```python\nclass Solution:\n    def findDiagonalOrder(self, nums):\n        \"\"\"\n        Diagonal Traverse\n\n        Given a 2D array with row lengths of 3, output the elements along the diagonal.\n        \n        Example 1:\n            Input: nums = [[1,2,3]]\n            Output: [1,2,3]\n            \n        Example 2:\n            Input: nums = [[1],[2],[3]]\n            Output: [1,2]\n            \n       "
    },
    {
        "original": "def download_class(session, args, class_name):\n    \"\"\"\n    Try to download on-demand class.\n\n    @return: Tuple of (bool, bool), where the first bool indicates whether\n        errors occurred while parsing syllabus, the second bool indicates\n        whether the course appears to be completed.\n    @rtype: (bool, bool)\n    \"\"\"\n    logging.debug('Downloading new style (on demand) class %s', class_name)\n    return download_on_demand_class(session, args, class_name)",
        "sampled": "```python\ndef download_class(session, args, class_name):\n    \"\"\"\n    Try to download on-demand class.\n\n    @return: Tuple of (bool, bool), where the first bool indicates whether\n        errors occurred while parsing syllabus, the second bool indicates\n        whether the course appears to be completed.\n    @rtype: (bool, bool)\n    \"\"\"\n\n    # Initialize variables to track errors and completion status\n    error_occurred = False\n    is_completed = False\n\n    #"
    },
    {
        "original": "def post_message(self, message, duration=None, pause=True, style=\"info\"):\n        \"\"\" Post a message on the screen with Messenger.\n            Arguments:\n                message: The message to display.\n                duration: The time until the message vanishes. (Default: 2.55s)\n                pause: If True, the program waits until the message completes.\n                style: \"info\", \"success\", or \"error\".\n\n            You can also post messages by using =>\n                self.execute_script('Messenger().post(\"My Message\")')\n        \"\"\"\n        if not duration:\n            if not self.message_duration:\n                duration = settings.DEFAULT_MESSAGE_DURATION\n            else:\n                duration = self.message_duration\n        js_utils.post_message(\n            self.driver, message, duration, style=style)\n        if pause:\n            duration = float(duration) + 0.15\n            time.sleep(float(duration))",
        "sampled": "```python\nfrom selenium import webdriver\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.common.by import By\nimport time\n\nclass Messenger:\n    def __init__(self, driver):\n        self.driver = driver\n\n    def post(self, message, duration=None, pause=True, style=\"info\"):\n        \"\"\" Post a message on the screen with Messenger.\n            Arguments:\n                message: The message to display.\n                duration: The time until the message vanishes. (Default: 2"
    },
    {
        "original": "def _process_defpriv_part(defperms):\n    \"\"\"\n    Process part\n    \"\"\"\n    _tmp = {}\n    previous = None\n    for defperm in defperms:\n        if previous is None:\n            _tmp[_DEFAULT_PRIVILEGES_MAP[defperm]] = False\n            previous = _DEFAULT_PRIVILEGES_MAP[defperm]\n        else:\n            if defperm == '*':\n                _tmp[previous] = True\n            else:\n                _tmp[_DEFAULT_PRIVILEGES_MAP[defperm]] = False\n                previous = _DEFAULT_PRIVILEGES_MAP[defperm]\n    return _tmp",
        "sampled": "```python\ndef _process_defpriv_part(defperms):\n    \"\"\"\n    Process part of the definition permissions.\n    \n    This function will process the given definition permissions string and return a list of 3 integers.\n    \n    Each integer in the returned list represents:\n      - First integer: The actual permission with all '-' changed to '1'\n      - Second integer: The \"-C\" position for each part if exists \n      - Third integer: The \"-S\" position for each part if exists \n"
    },
    {
        "original": "def _gerritCmd(self, *args):\n        \"\"\"Construct a command as a list of strings suitable for\n        :func:`subprocess.call`.\n        \"\"\"\n        if self.gerrit_identity_file is not None:\n            options = ['-i', self.gerrit_identity_file]\n        else:\n            options = []\n        return ['ssh'] + options + [\n            '@'.join((self.gerrit_username, self.gerrit_server)),\n            '-p', str(self.gerrit_port),\n            'gerrit'\n        ] + list(args)",
        "sampled": "```python\nfrom subprocess import Popen, PIPE\n\nclass GerritClient:\n    \"\"\"A simple Gerrit client.\"\"\"\n\n    def _gerritCmd(self, *args):\n        \"\"\"Construct a command as a list of strings suitable for\n        :func:`subprocess.call`.\n        \n        For example::\n\n            _gerritCmd('describe', 'HEAD') returns ['gerrit', 'describe', '-S', '\"HEAD\"'\u64ca]\n            \n            cmd = self._gerrit"
    },
    {
        "original": "def cleanup_all(data_home=None):\n    \"\"\"\n    Cleans up all the example datasets in the data directory specified by\n    ``get_data_home`` either to clear up disk space or start from fresh.\n    \"\"\"\n    removed = 0\n    for name, meta in DATASETS.items():\n        _, ext = os.path.splitext(meta['url'])\n        removed += cleanup_dataset(name, data_home=data_home, ext=ext)\n\n    print(\n        \"Removed {} fixture objects from {}\".format(removed, get_data_home(data_home))\n    )",
        "sampled": "```python\nimport os\nimport shutil\nimport pathlib\n\ndef cleanup_all(data_home=None):\n    \"\"\"\n    Cleans up all the example datasets in the data directory specified by\n    ``get_data_home`` either to clear up disk space or start from fresh.\n    \"\"\"\n    \n    # If no data home is provided, use the default one\n    if data_home is None:\n        print(\"No data home provided. Using default.\")\n        data_home = pathlib.Path.home() / '.data'\n    \n"
    },
    {
        "original": "def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a\n        dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        return dict({\n            scenario.name: {\n                'check': scenario.check_sequence,\n                'cleanup': scenario.cleanup_sequence,\n                'converge': scenario.converge_sequence,\n                'create': scenario.create_sequence,\n                'dependency': scenario.dependency_sequence,\n                'destroy': scenario.destroy_sequence,\n                'idempotence': scenario.idempotence_sequence,\n                'lint': scenario.lint_sequence,\n                'prepare': scenario.prepare_sequence,\n                'side_effect': scenario.side_effect_sequence,\n                'syntax': scenario.syntax_sequence,\n                'test': scenario.test_sequence,\n                'verify': scenario.verify_sequence,\n            }\n            for scenario in self.all\n        })",
        "sampled": "```python\nclass MatrixBuilder:\n    def _get_matrix(self):\n        \"\"\"\n        Build a matrix of scenarios with sequence to include and returns a dict.\n\n        {\n            scenario_1: {\n                'subcommand': [\n                    'action-1',\n                    'action-2',\n                ],\n            },\n            scenario_2: {\n                'subcommand': [\n                    'action-1',\n                ],\n            },\n        }\n\n        :returns: dict\n        \"\"\"\n        \n        # Define the scenarios and"
    },
    {
        "original": "def create_container(container_name, profile, **libcloud_kwargs):\n    \"\"\"\n    Create a container in the cloud\n\n    :param container_name: Container name\n    :type  container_name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's create_container method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.create_container MyFolder profile1\n    \"\"\"\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    container = conn.create_container(container_name, **libcloud_kwargs)\n    return {\n        'name': container.name,\n        'extra': container.extra\n        }",
        "sampled": "```python\nimport libcloud.storage.drivers as drivers\n\ndef create_container(container_name, profile, **libcloud_kwargs):\n    \"\"\"\n    Create a container in the cloud\n\n    :param container_name: Container name\n    :type  container_name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's create_container method\n    :type  libcloud_kwargs: ``"
    },
    {
        "original": "def _run_hooks(config, hooks, args, environ):\n    \"\"\"Actually run the hooks.\"\"\"\n    skips = _get_skips(environ)\n    cols = _compute_cols(hooks, args.verbose)\n    filenames = _all_filenames(args)\n    filenames = filter_by_include_exclude(filenames, '', config['exclude'])\n    classifier = Classifier(filenames)\n    retval = 0\n    for hook in hooks:\n        retval |= _run_single_hook(classifier, hook, args, skips, cols)\n        if retval and config['fail_fast']:\n            break\n    if retval and args.show_diff_on_failure and git.has_diff():\n        if args.all_files:\n            output.write_line(\n                'pre-commit hook(s) made changes.\\n'\n                'If you are seeing this message in CI, '\n                'reproduce locally with: `pre-commit run --all-files`.\\n'\n                'To run `pre-commit` as part of git workflow, use '\n                '`pre-commit install`.',\n            )\n        output.write_line('All changes made by hooks:')\n        subprocess.call(('git', '--no-pager', 'diff', '--no-ext-diff'))\n    return retval",
        "sampled": "```python\ndef _run_hooks(config, hooks, args, environ):\n    \"\"\"\n    Actually run the hooks.\n    \n    Args:\n        config (dict): The configuration dictionary.\n        hooks (list): A list of hook functions to be executed.\n        args (list): A list of arguments to be passed to the hook functions.\n        environ (dict): The environment variables dictionary.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Iterate over each hook function in the list of hooks\n    for"
    },
    {
        "original": "def can_run(self):\n        \"\"\"The task can run if its status is < S_SUB and all the other dependencies (if any) are done!\"\"\"\n        all_ok = all(stat == self.S_OK for stat in self.deps_status)\n        return self.status < self.S_SUB and self.status != self.S_LOCKED and all_ok",
        "sampled": "```python\nclass Task:\n    def __init__(self, status, dependencies=None):\n        self.status = status\n        self.dependencies = dependencies if dependencies else []\n\n    def can_run(self):\n        \"\"\"The task can run if its status is < S_SUB and all the other dependencies (if any) are done!\"\"\"\n        return self.status < 2 and all(dep.done for dep in self.dependencies)\n\n\nclass Dependency:\n    def __init__(self, name, done=False):\n        self.name"
    },
    {
        "original": "def create_iteration(self, num_suggestions):\n        \"\"\"Create an iteration for the experiment group (works for grid and random).\"\"\"\n        from db.models.experiment_groups import ExperimentGroupIteration\n\n        iteration_config = BaseIterationConfig(iteration=0,\n                                               num_suggestions=num_suggestions,\n                                               experiment_ids=[])\n\n        return ExperimentGroupIteration.objects.create(\n            experiment_group=self.experiment_group,\n            data=iteration_config.to_dict())",
        "sampled": "```python\nimport random\n\nclass Iteration:\n    def __init__(self, experiment_group, num_suggestions):\n        self.experiment_group = experiment_group\n        self.num_suggestions = num_suggestions\n\n    def create_iteration(self):\n        if self.experiment_group == \"grid\":\n            # For grid, we can suggest all possible combinations of factors at different levels\n            # Here I'm assuming we have a predefined list of factors and their levels\n            factors = [\"factor1\","
    },
    {
        "original": "def citation(self):\n        \"\"\"\n        Returns the contents of the citation.bib file that describes the source\n        and provenance of the dataset or to cite for academic work.\n        \"\"\"\n        path = find_dataset_path(\n            self.name, data_home=self.data_home, fname=\"meta.json\", raises=False\n        )\n        if path is None:\n            return None\n\n        with open(path, 'r') as f:\n            return f.read()",
        "sampled": "```python\n# Assuming the citation.bib file is in a local directory\nimport os\n\nclass Citation:\n    def __init__(self, bib_file_location):\n        self.bib_file_location = bib_file_location\n\n    def check_bib_file(self):\n        \"\"\"\n        Check if the .bib file exists at the specified location.\n        \n        If it exists, returns True; else returns False.\n        \"\"\"\n        return os.path.exists(self.bib_file_location)\n\n    def get_citation_contents(self"
    },
    {
        "original": "def init_comm(self):\n        \"\"\"\n        Initializes comm and attaches streams.\n        \"\"\"\n        if self.comm:\n            return self.comm\n        comm = None\n        if self.dynamic or self.renderer.widget_mode == 'live':\n            comm = self.renderer.comm_manager.get_server_comm()\n        return comm",
        "sampled": "```python\nclass CommunicationSystem:\n    def __init__(self):\n        self.comm = None\n\n    def init_comm(self):\n        \"\"\"\n        Initializes comm and attaches streams.\n        \"\"\"\n        \n        # Initialize the communication system\n        self.comm = {}\n        \n        # Attach input stream to the communication system\n        self.attach_stream('input', 'in')\n        \n        # Attach output stream to the communication system\n        self.attach_stream('output', 'out')\n\n    def attach_stream(self, name,"
    },
    {
        "original": "def _get_facet_chempots(self, facet):\n        \"\"\"\n        Calculates the chemical potentials for each element within a facet.\n\n        Args:\n            facet: Facet of the phase diagram.\n\n        Returns:\n            { element: chempot } for all elements in the phase diagram.\n        \"\"\"\n        complist = [self.qhull_entries[i].composition for i in facet]\n        energylist = [self.qhull_entries[i].energy_per_atom for i in facet]\n        m = [[c.get_atomic_fraction(e) for e in self.elements] for c in\n             complist]\n        chempots = np.linalg.solve(m, energylist)\n        return dict(zip(self.elements, chempots))",
        "sampled": "```python\ndef _get_facet_chempots(self, facet):\n    \"\"\"\n    Calculates the chemical potentials for each element within a facet.\n\n    Args:\n        facet (dict): Facet of the phase diagram. It should contain the elements as keys and their respective properties.\n\n    Returns:\n        dict: A dictionary where each key is an element and the corresponding value is its chemical potential.\n    \"\"\"\n\n    # Initialize an empty dictionary to store the chemical potentials\n    chempots = {}\n\n"
    },
    {
        "original": "def unmangle_name(name, classname):\n        \"\"\"Remove __ from the end of _name_ if it starts with __classname__\n        return the \"unmangled\" name.\n        \"\"\"\n        if name.startswith(classname) and name[-2:] != '__':\n            return name[len(classname) - 2:]\n        return name",
        "sampled": "```python\ndef unmangle_name(name, classname):\n    \"\"\"\n    Remove __ from the end of _name_ if it starts with __classname__\n    return the \"unmangled\" name.\n    \"\"\"\n    \n    # Check if the name starts with the classname and ends with '__'\n    if name.startswith(classname + '__'):\n        # If it does, remove the last two characters ('__') from the name\n        return name[:-2]\n    \n    # If it doesn't, return"
    },
    {
        "original": "def fully_correlated_conditional(Kmn, Kmm, Knn, f, *, full_cov=False, full_output_cov=False, q_sqrt=None, white=False):\n    \"\"\"\n    This function handles conditioning of multi-output GPs in the case where the conditioning\n    points are all fully correlated, in both the prior and posterior.\n    :param Kmn: LM x N x P\n    :param Kmm: LM x LM\n    :param Knn: N x P or N x P x N x P\n    :param f: data matrix, LM x 1\n    :param q_sqrt: 1 x LM x LM  or 1 x ML\n    :param full_cov: calculate covariance between inputs\n    :param full_output_cov: calculate covariance between outputs\n    :param white: use whitened representation\n    :return:\n        - mean: N x P\n        - variance: N x P, N x P x P, P x N x N, N x P x N x P\n    \"\"\"\n    m, v = fully_correlated_conditional_repeat(Kmn, Kmm, Knn, f, full_cov=full_cov,\n                                               full_output_cov=full_output_cov, q_sqrt=q_sqrt, white=white)\n    return m[0, ...], v[0, ...]",
        "sampled": "```python\nimport numpy as np\n\ndef fully_correlated_conditional(Kmn, Kmm, Knn, f, *, full_cov=False, full_output_cov=False, q_sqrt=None, white=False):\n    \"\"\"\n    This function handles conditioning of multi-output GPs in the case where the conditioning\n    points are all fully correlated, in both the prior and posterior.\n    \n    :param Kmn: LM x N x P\n    :param Kmm: LM x LM\n    :"
    },
    {
        "original": "def CreateCampaignWithBiddingStrategy(client, bidding_strategy_id, budget_id):\n  \"\"\"Create a Campaign with a Shared Bidding Strategy.\n\n  Args:\n    client: AdWordsClient the client to run the example with.\n    bidding_strategy_id: string the bidding strategy ID to use.\n    budget_id: string the shared budget ID to use.\n\n  Returns:\n    dict An object representing a campaign.\n  \"\"\"\n  # Initialize appropriate service.\n  campaign_service = client.GetService('CampaignService', version='v201809')\n\n  # Create campaign.\n  campaign = {\n      'name': 'Interplanetary Cruise #%s' % uuid.uuid4(),\n      'budget': {\n          'budgetId': budget_id\n      },\n      'biddingStrategyConfiguration': {\n          'biddingStrategyId': bidding_strategy_id\n      },\n      'advertisingChannelType': 'SEARCH',\n      'networkSetting': {\n          'targetGoogleSearch': 'true',\n          'targetSearchNetwork': 'true',\n          'targetContentNetwork': 'true'\n      }\n  }\n\n  # Create operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': campaign\n  }\n\n  response = campaign_service.mutate([operation])\n  new_campaign = response['value'][0]\n\n  print ('Campaign with name \"%s\", ID \"%s\" and bidding scheme ID \"%s\" '\n         'was created.' %\n         (new_campaign['name'], new_campaign['id'],\n          new_campaign['biddingStrategyConfiguration']['biddingStrategyId']))\n\n  return new_campaign",
        "sampled": "```python\nimport adwords\n\ndef CreateCampaignWithBiddingStrategy(client, bidding_strategy_id, budget_id):\n  \"\"\"\n  Create a Campaign with a Shared Bidding Strategy.\n\n  Args:\n    client: AdWordsClient the client to run the example with.\n    bidding_strategy_id: string the bidding strategy ID to use.\n    budget_id: string the shared budget ID to use.\n\n  Returns:\n    dict An object representing a campaign.\n  \"\"\"\n\n  # Get an instance of AdWordsCampaign"
    },
    {
        "original": "def get_headers(data, extra_headers=None):\n    \"\"\"\n    Takes the response data as well as any additional headers and returns a\n    tuple of tuples of headers suitable for passing to start_response()\n    \"\"\"\n    response_headers = {\n        'Content-Length': str(len(data)),\n    }\n\n    if extra_headers:\n        response_headers.update(extra_headers)\n\n    return list(response_headers.items())",
        "sampled": "```python\ndef get_headers(data, extra_headers=None):\n    \"\"\"\n    Takes the response data as well as any additional headers and returns a\n    tuple of tuples of headers suitable for passing to start_response()\n    \"\"\"\n\n    # If extra_headers is None, it defaults to an empty dictionary\n    if extra_headers is None:\n        extra_headers = {}\n\n    # Combine the data and extra headers into a single dictionary\n    all_headers = dict(data.items(), **extra_headers)\n\n    # Create a"
    },
    {
        "original": "def plot_scales(self, titles=None, fig_kwargs={}, **kwargs):\n        \"\"\"\n        Plot input sensitivity for all datasets, to see which input dimensions are\n        significant for which dataset.\n\n        :param titles: titles for axes of datasets\n\n        kwargs go into plot_ARD for each kernel.\n        \"\"\"\n        from ..plotting import plotting_library as pl\n\n        if titles is None:\n            titles = [r'${}$'.format(name) for name in self.names]\n\n        M = len(self.bgplvms)\n        fig = pl().figure(rows=1, cols=M, **fig_kwargs)\n        for c in range(M):\n            canvas = self.bgplvms[c].kern.plot_ARD(title=titles[c], figure=fig, col=c+1, **kwargs)\n        return canvas",
        "sampled": "```python\nimport matplotlib.pyplot as plt\n\nclass Model:\n    def plot_scales(self, titles=None, fig_kwargs={}, **kwargs):\n        \"\"\"\n        Plot input sensitivity for all datasets, to see which input dimensions are\n        significant for which dataset.\n\n        :param titles: titles for axes of datasets\n\n        kwargs go into plot_ARD for each kernel.\n        \"\"\"\n\n        # Get the number of datasets\n        num_datasets = len(self.datasets)\n\n        # Create a figure with multiple subplots"
    },
    {
        "original": "def get_slab_regions(slab, blength=3.5):\n    \"\"\"\n    Function to get the ranges of the slab regions. Useful for discerning where\n    the slab ends and vacuum begins if the slab is not fully within the cell\n    Args:\n        slab (Structure): Structure object modelling the surface\n        blength (float, Ang): The bondlength between atoms. You generally\n            want this value to be larger than the actual bondlengths in\n            order to find atoms that are part of the slab\n    \"\"\"\n\n    fcoords, indices, all_indices = [], [], []\n    for site in slab:\n        # find sites with c < 0 (noncontiguous)\n        neighbors = slab.get_neighbors(site, blength, include_index=True,\n                                       include_image=True)\n        for nn in neighbors:\n            if nn[0].frac_coords[2] < 0:\n                # sites are noncontiguous within cell\n                fcoords.append(nn[0].frac_coords[2])\n                indices.append(nn[-2])\n                if nn[-2] not in all_indices:\n                    all_indices.append(nn[-2])\n\n    if fcoords:\n        # If slab is noncontiguous, locate the lowest\n        # site within the upper region of the slab\n        while fcoords:\n            last_fcoords = copy.copy(fcoords)\n            last_indices = copy.copy(indices)\n            site = slab[indices[fcoords.index(min(fcoords))]]\n            neighbors = slab.get_neighbors(site, blength, include_index=True,\n                                           include_image=True)\n            fcoords, indices = [], []\n            for nn in neighbors:\n                if 1 > nn[0].frac_coords[2] > 0 and \\\n                                nn[0].frac_coords[2] < site.frac_coords[2]:\n                    # sites are noncontiguous within cell\n                    fcoords.append(nn[0].frac_coords[2])\n                    indices.append(nn[-2])\n                    if nn[-2] not in all_indices:\n                        all_indices.append(nn[-2])\n\n        # Now locate the highest site within the lower region of the slab\n        upper_fcoords = []\n        for site in slab:\n            if all([nn[-1] not in all_indices for nn in\n                    slab.get_neighbors(site, blength,\n                                       include_index=True)]):\n                upper_fcoords.append(site.frac_coords[2])\n        coords = copy.copy(last_fcoords) if not fcoords else copy.copy(fcoords)\n        min_top = slab[last_indices[coords.index(min(coords))]].frac_coords[2]\n        ranges = [[0, max(upper_fcoords)], [min_top, 1]]\n    else:\n        # If the entire slab region is within the slab cell, just\n        # set the range as the highest and lowest site in the slab\n        sorted_sites = sorted(slab, key=lambda site: site.frac_coords[2])\n        ranges = [[sorted_sites[0].frac_coords[2],\n                   sorted_sites[-1].frac_coords[2]]]\n\n    return ranges",
        "sampled": "```python\ndef get_slab_regions(slab, blength=3.5):\n    \"\"\"\n    Function to get the ranges of the slab regions. Useful for discerning where\n    the slab ends and vacuum begins if the slab is not fully within the cell\n    \n    Args:\n        slab (Structure): Structure object modelling the surface\n        blength (float, Ang): The bondlength between atoms. You generally\n            want this value to be larger than the actual bondlengths in\n"
    },
    {
        "original": "def size(config, accounts=(), day=None, group=None, human=True, region=None):\n    \"\"\"size of exported records for a given day.\"\"\"\n    config = validate.callback(config)\n    destination = config.get('destination')\n    client = boto3.Session().client('s3')\n    day = parse(day)\n\n    def export_size(client, account):\n        paginator = client.get_paginator('list_objects_v2')\n        count = 0\n        size = 0\n        session = get_session(account['role'], region)\n        account_id = session.client('sts').get_caller_identity()['Account']\n        prefix = destination.get('prefix', '').rstrip('/') + '/%s' % account_id\n        prefix = \"%s/%s/%s\" % (prefix, group, day.strftime(\"%Y/%m/%d\"))\n        account['account_id'] = account_id\n        for page in paginator.paginate(\n                Bucket=destination['bucket'],\n                Prefix=prefix):\n            for k in page.get('Contents', ()):\n                size += k['Size']\n                count += 1\n        return (count, size)\n\n    total_size = 0\n    accounts_report = []\n    logging.getLogger('botocore').setLevel(logging.ERROR)\n    with ThreadPoolExecutor(max_workers=16) as w:\n        futures = {}\n        for account in config.get('accounts'):\n            if accounts and account['name'] not in accounts:\n                continue\n            futures[w.submit(export_size, client, account)] = account\n\n        for f in as_completed(futures):\n            account = futures[f]\n            count, size = f.result()\n            account.pop('role')\n            account.pop('groups')\n            total_size += size\n            if human:\n                account['size'] = GetHumanSize(size)\n            else:\n                account['size'] = size\n            account['count'] = count\n            accounts_report.append(account)\n\n    accounts_report.sort(key=operator.itemgetter('count'), reverse=True)\n    print(tabulate(accounts_report, headers='keys'))\n    log.info(\"total size:%s\", GetHumanSize(total_size))",
        "sampled": "```python\ndef size(config, accounts=(), day=None, group=None, human=True, region=None):\n    \"\"\"size of exported records for a given day.\"\"\"\n    \n    # If no accounts are provided, consider the whole configuration as default\n    if not accounts:\n        accounts = config['default_accounts']\n        \n    # Determine the group if it's not specified\n    if group is None:\n        group = config['default_group']\n        \n    # Determine the region if it's not specified\n"
    },
    {
        "original": "def p_concat_list(p):\n    \"\"\"\n    concat_list : expr_list SEMI expr_list\n                | concat_list SEMI expr_list\n    \"\"\"\n    if p[1].__class__ == node.expr_list:\n        p[0] = node.concat_list([p[1], p[3]])\n    else:\n        p[0] = p[1]\n        p[0].append(p[3])",
        "sampled": "```python\nclass Parser:\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.index = 0\n\n    def parse(self):\n        result = []\n        while self.index < len(self.tokens):\n            if self.check(MARKER_START_ASSIGNMENT) and not result:\n                # backtracking once a failure occurs \n                pass\n            elif (self.match dzia\u0142 marker_or_expr_list()) or self.match(SEMI CONCAT_LIST explodes ENERGY(), Marker_SEMI())):\n                "
    },
    {
        "original": "def extract_public_key(args):\n    \"\"\" Load an ECDSA private key and extract the embedded public key as raw binary data. \"\"\"\n    sk = _load_ecdsa_signing_key(args)\n    vk = sk.get_verifying_key()\n    args.public_keyfile.write(vk.to_string())\n    print(\"%s public key extracted to %s\" % (args.keyfile.name, args.public_keyfile.name))",
        "sampled": "```python\nimport os\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.asymmetric import ec\n\ndef extract_public_key(args):\n    # Load the private key from the given file path using PEM format\n    if args.key_path:\n        with open(args.key_path, 'rb') as key_file:\n            private_key = serialization.load_pem_private_key(\n                key_file.read(),\n                password=None,\n               "
    },
    {
        "original": "def _binary_sample(image, label, n_samples_per_label, label_count):\n        \"\"\" Sample `nsamples_per_label` points from the binary mask corresponding to `label`\n\n        Randomly sample `nsamples_per_label` point form the binary mask corresponding to `label`. Sampling with\n        replacement is used if the required `nsamples_per_label` is larger than the available `label_count`\n\n        :param image: Input 2D raster label image\n        :type image: uint8 numpy array\n        :param label: Scalar value of label to consider\n        :type label: uint8\n        :param n_samples_per_label: Number of points to sample form the binary mask\n        :type n_samples_per_label: uint32\n        :param label_count: Number of points available for `label`\n        :type label_count: uint32\n        :return: Sampled label value, row index of samples, col index of samples\n        \"\"\"\n        h_idx, w_idx = np.where(image == label)\n\n        rand_idx = np.random.choice(h_idx.size, size=n_samples_per_label, replace=label_count < n_samples_per_label)\n\n        return h_idx[rand_idx], w_idx[rand_idx]",
        "sampled": "```python\nimport numpy as np\n\ndef _binary_sample(image, label, n_samples_per_label, label_count):\n    \"\"\"\n    Sample `nsamples_per_label` points from the binary mask corresponding to `label`\n\n    Randomly sample `nsamples_per_label` point form the binary mask corresponding to `label`. Sampling with\n    replacement is used if the required `nsamples_per_label` is larger than the available `label_count`\n\n    :param image: Input 2D raster label"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a CredentialDetails object from a json dictionary.\"\"\"\n        args = {}\n        if 'credential_type' in _dict:\n            args['credential_type'] = _dict.get('credential_type')\n        if 'client_id' in _dict:\n            args['client_id'] = _dict.get('client_id')\n        if 'enterprise_id' in _dict:\n            args['enterprise_id'] = _dict.get('enterprise_id')\n        if 'url' in _dict:\n            args['url'] = _dict.get('url')\n        if 'username' in _dict:\n            args['username'] = _dict.get('username')\n        if 'organization_url' in _dict:\n            args['organization_url'] = _dict.get('organization_url')\n        if 'site_collection.path' in _dict:\n            args['site_collection_path'] = _dict.get('site_collection.path')\n        if 'client_secret' in _dict:\n            args['client_secret'] = _dict.get('client_secret')\n        if 'public_key_id' in _dict:\n            args['public_key_id'] = _dict.get('public_key_id')\n        if 'private_key' in _dict:\n            args['private_key'] = _dict.get('private_key')\n        if 'passphrase' in _dict:\n            args['passphrase'] = _dict.get('passphrase')\n        if 'password' in _dict:\n            args['password'] = _dict.get('password')\n        if 'gateway_id' in _dict:\n            args['gateway_id'] = _dict.get('gateway_id')\n        if 'source_version' in _dict:\n            args['source_version'] = _dict.get('source_version')\n        if 'web_application_url' in _dict:\n            args['web_application_url'] = _dict.get('web_application_url')\n        if 'domain' in _dict:\n            args['domain'] = _dict.get('domain')\n        if 'endpoint' in _dict:\n            args['endpoint'] = _dict.get('endpoint')\n        if 'access_key_id' in _dict:\n            args['access_key_id'] = _dict.get('access_key_id')\n        if 'secret_access_key' in _dict:\n            args['secret_access_key'] = _dict.get('secret_access_key')\n        return cls(**args)",
        "sampled": "```python\nclass CredentialDetails:\n    def __init__(self, unique_id, credential_type, is_enabled):\n        self.unique_id = unique_id\n        self.credential_type = credential_type\n        self.is_enabled = is_enabled\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"Initialize a CredentialDetails object from a json dictionary.\"\"\"\n        if '_type' in _dict and _dict['_type'] == 'credential':\n            return cls(_dict.get('unique"
    },
    {
        "original": "def transformString( self, instring ):\r\n        \"\"\"\r\n        Extension to C{L{scanString}}, to modify matching text with modified tokens that may\r\n        be returned from a parse action.  To use C{transformString}, define a grammar and\r\n        attach a parse action to it that modifies the returned token list.\r\n        Invoking C{transformString()} on a target string will then scan for matches,\r\n        and replace the matched text patterns according to the logic in the parse\r\n        action.  C{transformString()} returns the resulting transformed string.\r\n        \r\n        Example::\r\n            wd = Word(alphas)\r\n            wd.setParseAction(lambda toks: toks[0].title())\r\n            \r\n            print(wd.transformString(\"now is the winter of our discontent made glorious summer by this sun of york.\"))\r\n        Prints::\r\n            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.\r\n        \"\"\"\r\n        out = []\r\n        lastE = 0\r\n        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to\r\n        # keep string locs straight between transformString and scanString\r\n        self.keepTabs = True\r\n        try:\r\n            for t,s,e in self.scanString( instring ):\r\n                out.append( instring[lastE:s] )\r\n                if t:\r\n                    if isinstance(t,ParseResults):\r\n                        out += t.asList()\r\n                    elif isinstance(t,list):\r\n                        out += t\r\n                    else:\r\n                        out.append(t)\r\n                lastE = e\r\n            out.append(instring[lastE:])\r\n            out = [o for o in out if o]\r\n            return \"\".join(map(_ustr,_flatten(out)))\r\n        except ParseBaseException as exc:\r\n            if ParserElement.verbose_stacktrace:\r\n                raise\r\n            else:\r\n                # catch and re-raise exception from here, clears out pyparsing internal stack trace\r\n                raise exc",
        "sampled": "```python\nimport re\n\nclass Word:\n    def __init__(self, pattern):\n        self.pattern = pattern\n        self.parse_action = None\n\n    def setParseAction(self, action):\n        self.parse_action = action\n\n    def transformString(self, instring):\n        if not self.parse_action:\n            raise ValueError(\"No parse action defined\")\n        \n        matches = re.findall(self.pattern, instring)\n        \n        for match in matches:\n            replacement = self.parse_action([match])\n"
    },
    {
        "original": "def use_general_term_frequencies(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\t\tPriorFactory\n\t\t\"\"\"\n\t\ttdf = self._get_relevant_term_freq()\n\t\tbg_df = self.term_doc_mat.get_term_and_background_counts()[['background']]\n\t\tbg_df = pd.merge(tdf,\n\t\t                 bg_df,\n\t\t                 left_index=True,\n\t\t                 right_index=True,\n\t\t                 how='left').fillna(0.)\n\t\tself._store_priors_from_background_dataframe(bg_df)\n\t\treturn self",
        "sampled": "```python\nfrom collections import defaultdict\n\nclass PriorFactory:\n    def __init__(self, general_terms_frequencies):\n        # Initialize a dictionary to store the frequencies of general terms\n        self.general_terms_frequencies = {term: frequency for term, frequency in general_terms_frequencies.items()}\n\n    def use_general_term_frequencies(self):\n        # Return the frequencies of general terms as a defaultprior\n        return self.general_terms_frequencies\n\n\nclass DefaultPrior:\n    def __init__(self"
    },
    {
        "original": "def set_settings(profile, setting, value, store='local'):\n    \"\"\"\n    Configure firewall settings.\n\n    Args:\n\n        profile (str):\n            The firewall profile to configure. Valid options are:\n\n            - domain\n            - public\n            - private\n\n        setting (str):\n            The firewall setting to configure. Valid options are:\n\n            - localfirewallrules\n            - localconsecrules\n            - inboundusernotification\n            - remotemanagement\n            - unicastresponsetomulticast\n\n        value (str):\n            The value to apply to the setting. Valid options are\n\n            - enable\n            - disable\n            - notconfigured\n\n        store (str):\n            The store to use. This is either the local firewall policy or the\n            policy defined by local group policy. Valid options are:\n\n            - lgpo\n            - local\n\n            Default is ``local``\n\n    Returns:\n        bool: ``True`` if successful\n\n    Raises:\n        CommandExecutionError: If an error occurs\n        ValueError: If the parameters are incorrect\n    \"\"\"\n    # Input validation\n    if profile.lower() not in ('domain', 'public', 'private'):\n        raise ValueError('Incorrect profile: {0}'.format(profile))\n    if setting.lower() not in ('localfirewallrules',\n                               'localconsecrules',\n                               'inboundusernotification',\n                               'remotemanagement',\n                               'unicastresponsetomulticast'):\n        raise ValueError('Incorrect setting: {0}'.format(setting))\n    if value.lower() not in ('enable', 'disable', 'notconfigured'):\n        raise ValueError('Incorrect value: {0}'.format(value))\n\n    # Run the command\n    command = 'set {0}profile settings {1} {2}'.format(profile, setting, value)\n    results = _netsh_command(command=command, store=store)\n\n    # A successful run should return an empty list\n    if results:\n        raise CommandExecutionError('An error occurred: {0}'.format(results))\n\n    return True",
        "sampled": "```python\ndef set_settings(profile, setting, value, store='local'):\n    \"\"\"\n    Configure firewall settings.\n\n    Args:\n\n        profile (str):\n            The firewall profile to configure. Valid options are:\n\n            - domain\n            - public\n            - private\n\n        setting (str):\n            The firewall setting to configure. Valid options are:\n\n            - localfirewallrules\n            - localconsecrules\n            - inboundusernotification\n            - remotemanagement\n            - unicast"
    },
    {
        "original": "def combine_heads(self, x):\n    \"\"\"Combine tensor that has been split.\n\n    Args:\n      x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n\n    Returns:\n      A tensor with shape [batch_size, length, hidden_size]\n    \"\"\"\n    with tf.name_scope(\"combine_heads\"):\n      batch_size = tf.shape(x)[0]\n      length = tf.shape(x)[2]\n      x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n      return tf.reshape(x, [batch_size, length, self.hidden_size])",
        "sampled": "```python\nimport tensorflow as tf\n\nclass Model:\n    def combine_heads(self, x):\n        \"\"\"\n        Combine tensor that has been split.\n\n        Args:\n          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n\n        Returns:\n          A tensor with shape [batch_size, length, hidden_size]\n        \"\"\"\n        \n        # Reshape the input tensor to move the num_heads dimension to the middle\n        # This is because we want to use tf.reshape"
    },
    {
        "original": "def log_assist_request_without_audio(assist_request):\n    \"\"\"Log AssistRequest fields without audio data.\"\"\"\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        resp_copy = embedded_assistant_pb2.AssistRequest()\n        resp_copy.CopyFrom(assist_request)\n        if len(resp_copy.audio_in) > 0:\n            size = len(resp_copy.audio_in)\n            resp_copy.ClearField('audio_in')\n            logging.debug('AssistRequest: audio_in (%d bytes)',\n                          size)\n            return\n        logging.debug('AssistRequest: %s', resp_copy)",
        "sampled": "```python\nimport logging\n\nclass AssistRequest:\n    \"\"\"Represents an assist request.\"\"\"\n\n    def __init__(self, task):\n        self.task = task\n        self.audio_data = None\n        self.custom_info = None\n\ndef log_assist_request_without_audio(assist_request):\n    \"\"\"\n    Log AssistRequest fields without audio data.\n    \n    Parameters:\n    assist_request (AssistRequest): The assist request to log.\n    \n    Returns:\n    logging.Logger: A logger instance that represents"
    },
    {
        "original": "def get_parameter_dd(self, parameter):\n        \"\"\"\n        This method returns parameters as nested dicts in case of decision\n        diagram parameter.\n        \"\"\"\n        dag = defaultdict(list)\n        dag_elem = parameter.find('DAG')\n        node = dag_elem.find('Node')\n        root = node.get('var')\n\n        def get_param(node):\n            edges = defaultdict(list)\n            for edge in node.findall('Edge'):\n                if edge.find('Terminal') is not None:\n                    edges[edge.get('val')] = edge.find('Terminal').text\n                elif edge.find('Node') is not None:\n                    node_cpd = defaultdict(list)\n                    node_cpd[edge.find('Node').get('var')] = \\\n                        get_param(edge.find('Node'))\n                    edges[edge.get('val')] = node_cpd\n                elif edge.find('SubDAG') is not None:\n                    subdag_attribute = defaultdict(list)\n                    subdag_attribute['type'] = edge.find('SubDAG').get('type')\n                    if subdag_attribute['type'] == 'template':\n                        subdag_attribute['idref'] = \\\n                            edge.find('SubDAG').get('idref')\n                    if edge.find('SubDAG').get('var'):\n                        subdag_attribute['var'] = \\\n                            edge.find('SubDAG').get('var')\n                    if edge.find('SubDAG').get('val'):\n                        subdag_attribute['val'] = \\\n                            edge.find('SubDAG').get('val')\n                    edges[edge.get('val')] = subdag_attribute\n            return edges\n\n        if parameter.find('SubDAGTemplate') is not None:\n            SubDAGTemplate = parameter.find('SubDAGTemplate')\n            subdag_root = SubDAGTemplate.find('Node')\n            subdag_node = subdag_root.get('var')\n            subdag_dict = defaultdict(list)\n            subdag_dict[subdag_node] = get_param(subdag_root)\n            dag['SubDAGTemplate'] = subdag_dict\n            dag['id'] = SubDAGTemplate.get('id')\n        dag[root] = get_param(node)\n        return dag",
        "sampled": "```python\ndef get_parameter_dd(self, parameter):\n    \"\"\"\n    This method returns parameters as nested dicts in case of decision diagram parameter.\n    \n    Parameters:\n    parameter (dict): Input dictionary containing JSON data\n    \n    Returns:\n    dict: Nested dictionary representation of the input JSON data\n    \"\"\"\n    \n    # Check if the input is a dictionary\n    if not isinstance(parameter, dict):\n        return {}\n    \n    # Initialize an empty dictionary to store the result\n    result = {}\n    \n   "
    },
    {
        "original": "def createGUI( self ):\n        \"\"\"Create the graphical user interface.\"\"\"\n        our_font = \"Helvetica 16 bold\"\n        small_font = \"Helvetica 9 bold\"\n        self.root_frame = Frame(self.root)\n        if self.action_space == 'continuous':\n            desc = \"Running continuous-action mission.\\nUse the mouse to turn, WASD to move.\"\n        else:\n            desc = \"Running discrete-action mission.\\nUse the arrow keys to turn and move.\"\n        Label(self.root_frame, text=desc,font = our_font,wraplength=640).pack(padx=5, pady=5)\n        self.canvas = Canvas(self.root_frame, borderwidth=0, highlightthickness=0, width=640, height=480, bg=\"gray\" )\n        self.canvas.bind('<Motion>',self.onMouseMoveInCanvas)\n        self.canvas.bind('<Button-1>',self.onLeftMouseDownInCanvas)\n        self.canvas.bind('<ButtonRelease-1>',self.onLeftMouseUpInCanvas)\n        if sys.platform == 'darwin': right_mouse_button = '2' # on MacOSX, the right button is 'Button-2'\n        else:                        right_mouse_button = '3' # on Windows and Linux the right button is 'Button-3'\n        self.canvas.bind('<Button-'+right_mouse_button+'>',self.onRightMouseDownInCanvas)\n        self.canvas.bind('<ButtonRelease-'+right_mouse_button+'>',self.onRightMouseUpInCanvas)\n        self.canvas.bind('<KeyPress>',self.onKeyPressInCanvas)\n        self.canvas.bind('<KeyRelease>',self.onKeyReleaseInCanvas)\n        self.canvas.pack(padx=5, pady=5)\n        self.entry_frame = Frame(self.root_frame)\n        Label(self.entry_frame, text=\"Type '/' to enter command:\",font = small_font).pack(padx=5, pady=5, side=LEFT)\n        self.command_entry = Entry(self.entry_frame,font = small_font)\n        self.command_entry.bind('<Key>',self.onKeyInCommandEntry)\n        self.command_entry.pack(padx=5, pady=5, side=LEFT)\n        Button(self.entry_frame, text='Send', command=self.onSendCommand,font = small_font).pack(padx=5, pady=5, side=LEFT)\n        self.entry_frame.pack()\n        self.observation = Label(self.root_frame, text='observations will appear here', wraplength=640, font = small_font)\n        self.observation.pack()\n        self.reward = Label(self.root_frame, text='rewards will appear here', wraplength=640, font = small_font)\n        self.reward.pack()\n        self.root_frame.pack()\n        self.mouse_event = self.prev_mouse_event = None",
        "sampled": "```python\nclass Solution:\n    def maxArea(self, height):\n        \"\"\"\n        Given n non-negative integers a1, a2, ..., an , \n        returns the maximum area of any histogram bar.\n        \n          area[i] = max(height * width)\n                      where i = 0~n-1 and i <= larger and smaller-bar\n            \n                Stack Upgrade Rules_.\n                    1 If pop item's value >= new push item's value \n                        keep popping the stack until find out"
    },
    {
        "original": "def parse_phone(phone):\n    \"\"\"Parses the given phone, or returns ``None`` if it's invalid.\"\"\"\n    if isinstance(phone, int):\n        return str(phone)\n    else:\n        phone = re.sub(r'[+()\\s-]', '', str(phone))\n        if phone.isdigit():\n            return phone",
        "sampled": "```python\nimport re\n\ndef parse_phone(phone):\n    \"\"\"\n    Parses the given phone, or returns None if it's invalid.\n    \n    The phone should be in the format: '+<country code> <national destination code> <line number>'.\n    \n    Parameters:\n    phone (str): The phone number to parse.\n    \n    Returns:\n    str: A tuple containing the country code, national destination code and line number if the input is valid, else None.\n     \"\"\"\n\n    pattern ="
    },
    {
        "original": "def record(self,\n               event_type: str,\n               event_data: Mapping = None,\n               instance: Any = None,\n               **kwargs) -> 'Event':\n        \"\"\" Validate and record an event.\n\n        >>> record('event.action', object_instance)\n        \"\"\"\n        if not self.is_setup:\n            return\n        if not self.can_handle(event_type=event_type):\n            return\n\n        event = self.get_event(event_type=event_type,\n                               event_data=event_data,\n                               instance=instance,\n                               **kwargs)\n        self.record_event(event)\n        return event",
        "sampled": "```python\nfrom typing import Mapping, Any\n\nclass Event:\n    def __init__(self, event_type: str, event_data: Mapping = None, instance: Any = None):\n        self.event_type = event_type\n        self.event_data = event_data if event_data else {}\n        self.instance = instance\n\n    def __str__(self):\n        return f\"Event({self.event_type}, {self.event_data}, {self.instance})\"\n\n\nclass EventRecorder:\n    def record(self,\n"
    },
    {
        "original": "def get_password(self, host=None):\n        \"\"\"\n        If host=None, return the current server list (dict).\n        Else, return the host's password (or the default one if defined or None)\n        \"\"\"\n        if host is None:\n            return self._password_dict\n        else:\n            try:\n                return self._password_dict[host]\n            except (KeyError, TypeError):\n                try:\n                    return self._password_dict['default']\n                except (KeyError, TypeError):\n                    return None",
        "sampled": "```python\nclass PasswordManager:\n    def __init__(self):\n        # Initialize an empty dictionary to store server passwords\n        self.servers = {}\n        # Initialize a default password for hosts without a defined password\n        self.default_password = 'default'\n\n    def add_host(self, host, password=None):\n        # Add a new host to the dictionary with its associated password or default if not provided\n        if password is None:\n            self.servers[host] = self.default_password\n        else"
    },
    {
        "original": "def header(self):\n        \"\"\"\n        Displays the scan header, as defined by self.HEADER and self.HEADER_FORMAT.\n\n        Returns None.\n        \"\"\"\n        self.config.display.format_strings(self.HEADER_FORMAT, self.RESULT_FORMAT)\n        self.config.display.add_custom_header(self.VERBOSE_FORMAT, self.VERBOSE)\n\n        if type(self.HEADER) == type([]):\n            self.config.display.header(*self.HEADER, file_name=self.current_target_file_name)\n        elif self.HEADER:\n            self.config.display.header(self.HEADER, file_name=self.current_target_file_name)",
        "sampled": "```python\nclass Scanner:\n    HEADER = \"Scan Results\"\n    HEADER_FORMAT = \"{:>5} {:>20} {:>15}\"\n    \n    def header(self):\n        \"\"\"\n        Displays the scan header, as defined by self.HEADER and self.HEADER_FORMAT.\n\n        Returns None.\n        \"\"\"\n        \n        # Formats the header based on HDR_FOOTER_TEMPLATE\n        formatted_header = Scanner.HEADER_FORMAT.format(\"No\", \"File Name\", \"Compression\")\n        \n        # Prints the formatted"
    },
    {
        "original": "def sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n                                  airmass_absolute, aoi,\n                                  reference_irradiance=1000):\n        \"\"\"\n        Use the :py:func:`sapm_effective_irradiance` function, the input\n        parameters, and ``self.module_parameters`` to calculate\n        effective irradiance.\n\n        Parameters\n        ----------\n        poa_direct : numeric\n            The direct irradiance incident upon the module.\n\n        poa_diffuse : numeric\n            The diffuse irradiance incident on module.\n\n        airmass_absolute : numeric\n            Absolute airmass.\n\n        aoi : numeric\n            Angle of incidence in degrees.\n\n        reference_irradiance : numeric, default 1000\n            Reference irradiance by which to divide the input irradiance.\n\n        Returns\n        -------\n        effective_irradiance : numeric\n            The SAPM effective irradiance.\n        \"\"\"\n        return sapm_effective_irradiance(\n            poa_direct, poa_diffuse, airmass_absolute, aoi,\n            self.module_parameters, reference_irradiance=reference_irradiance)",
        "sampled": "```python\nclass SAPM:\n    def __init__(self, module_parameters):\n        self.module_parameters = module_parameters\n\n    def sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n                              airmass_absolute, aoi,\n                              reference_irradiance=1000):\n        \"\"\"\n        Use the :py:func:`sapm_effective_irradiance` function, the input\n        parameters, and ``self.module_parameters`` to calculate\n        effective"
    },
    {
        "original": "def convert_one(self, op: ops.Operation) -> ops.OP_TREE:\n        \"\"\"Convert a single (one- or two-qubit) operation\n\n        into ion trap native gates\n        Args:\n            op: gate operation to be converted\n\n        Returns:\n            the desired operation implemented with ion trap gates\n        \"\"\"\n\n        # Known gate name\n        if not isinstance(op, ops.GateOperation):\n            raise TypeError(\"{!r} is not a gate operation.\".format(op))\n\n        if is_native_ion_gate(op.gate):\n            return [op]\n        # one choice of known Hadamard gate decomposition\n        if isinstance(op.gate, ops.HPowGate) and op.gate.exponent == 1:\n            return [ops.Rx(np.pi).on(op.qubits[0]),\n                    ops.Ry(-1 * np.pi/2).on(op.qubits[0])]\n        # one choice of known CNOT gate decomposition\n        if isinstance(op.gate, ops.CNotPowGate) and op.gate.exponent == 1:\n            return [ops.Ry(np.pi/2).on(op.qubits[0]),\n                    MS(np.pi/4).on(op.qubits[0], op.qubits[1]),\n                    ops.Rx(-1*np.pi/2).on(op.qubits[0]),\n                    ops.Rx(-1*np.pi/2).on(op.qubits[1]),\n                    ops.Ry(-1*np.pi/2).on(op.qubits[0])]\n        # Known matrix\n        mat = protocols.unitary(op, None) if len(\n            op.qubits) <= 2 else None\n        if mat is not None and len(op.qubits) == 1:\n            gates = optimizers.single_qubit_matrix_to_phased_x_z(mat)\n            return [g.on(op.qubits[0]) for g in gates]\n        elif mat is not None and len(op.qubits) == 2:\n            return two_qubit_matrix_to_ion_operations(\n                op.qubits[0], op.qubits[1], mat)\n        else:\n            if self.ignore_failures:\n                return [op]\n            else:\n                raise TypeError(\n                    \"Don't know how to work with {!r}. \"\n                    \"It isn't a native Ion Trap operation, \"\n                    \"a 1 or 2 qubit gate with a known unitary, \"\n                    \"or composite.\".format(op.gate))",
        "sampled": "```python\nfrom typing import Dict, List\n\nclass IonTrapConverter:\n    def __init__(self):\n        # Define the mapping of operations to ion trap native gates\n        self.operation_mapping: Dict[str, List[str]] = {\n            'U1': ['X', 'S'],\n            'U2': ['X', 'S', 'Z'],\n            'CNOT': ['CX']\n        }\n\n    def convert_one(self, op: str) -> List[str]:\n        \"\"\"Convert"
    },
    {
        "original": "def GreaterThan(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"greater than\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, '>')\n    return self._query_builder",
        "sampled": "```python\nclass QueryBuilder:\n    def __init__(self):\n        # Initialize the query builder with no WHERE condition set\n        self.where_type = None\n\n    def GreaterThan(self, value):\n        \"\"\"\n        Sets the type of the WHERE clause as \"greater than\".\n\n        Args:\n          value: The value to be used in the WHERE condition.\n\n        Returns:\n          The query builder that this WHERE builder links to.\n        \"\"\"\n        \n        # If where_type is still not set, it"
    },
    {
        "original": "def _prevent_default_initializer_splitting(self, item, indent_amt):\n        \"\"\"Prevent splitting between a default initializer.\n\n        When there is a default initializer, it's best to keep it all on\n        the same line. It's nicer and more readable, even if it goes\n        over the maximum allowable line length. This goes back along the\n        current line to determine if we have a default initializer, and,\n        if so, to remove extraneous whitespaces and add a line\n        break/indent before it if needed.\n\n        \"\"\"\n        if unicode(item) == '=':\n            # This is the assignment in the initializer. Just remove spaces for\n            # now.\n            self._delete_whitespace()\n            return\n\n        if (not self._prev_item or not self._prev_prev_item or\n                unicode(self._prev_item) != '='):\n            return\n\n        self._delete_whitespace()\n        prev_prev_index = self._lines.index(self._prev_prev_item)\n\n        if (\n            isinstance(self._lines[prev_prev_index - 1], self._Indent) or\n            self.fits_on_current_line(item.size + 1)\n        ):\n            # The default initializer is already the only item on this line.\n            # Don't insert a newline here.\n            return\n\n        # Replace the space with a newline/indent combo.\n        if isinstance(self._lines[prev_prev_index - 1], self._Space):\n            del self._lines[prev_prev_index - 1]\n\n        self.add_line_break_at(self._lines.index(self._prev_prev_item),\n                               indent_amt)",
        "sampled": "```python\ndef _prevent_default_initializer_splitting(self, item, indent_amt):\n    \"\"\"\n    Prevent splitting between a default initializer.\n\n    When there is a default initializer, it's best to keep it all on the same line.\n    It's nicer and more readable, even if it goes over the maximum allowable line length.\n    This goes back along the current line to determine if we have a default initializer,\n    and, if so, to remove extraneous whitespaces and add a line break"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "sampled": "```python\nimport ipaddress\n\ndef validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \n    The validation checks that the IP is not a loopback address, \n    has exactly one '1-bit' in its binary representation and \n    does not start with 0. This is according to ASN format.\n    \n    Parameters:\n    ip (str): The IP to be validated\n    \n    Returns:\n    bool: True if the IP is valid,"
    },
    {
        "original": "def add_field_value(self, field, value):\n        \"\"\"Add a value to a field that supports multiple values, without resetting the existing values.\n\n        This should work with: labels, multiple checkbox lists, multiple select\n\n        :param field: The field name\n        :param value: The field's value\n\n        :type field: str\n        \"\"\"\n        super(Issue, self).update(fields={\"update\": {field: [{\"add\": value}]}})",
        "sampled": "```python\nclass Form:\n    def __init__(self):\n        self.form_fields = {}\n\n    def add_field_value(self, field, value):\n        \"\"\"Add a value to a field that supports multiple values, without resetting the existing values.\n\n        This should work with: labels, multiple checkbox lists, multiple select\n\n        :param field: The field name\n        :param value: The field's value\n\n        :type field: str\n        \"\"\"\n        \n        if isinstance(value, list):"
    },
    {
        "original": "def metrics(self, raw=False):\n        \"\"\"Get metrics on producer performance.\n\n        This is ported from the Java Producer, for details see:\n        https://kafka.apache.org/documentation/#producer_monitoring\n\n        Warning:\n            This is an unstable interface. It may change in future\n            releases without warning.\n        \"\"\"\n        if raw:\n            return self._metrics.metrics.copy()\n\n        metrics = {}\n        for k, v in six.iteritems(self._metrics.metrics.copy()):\n            if k.group not in metrics:\n                metrics[k.group] = {}\n            if k.name not in metrics[k.group]:\n                metrics[k.group][k.name] = {}\n            metrics[k.group][k.name] = v.value()\n        return metrics",
        "sampled": "```python\nclass ProducerMetrics:\n    def __init__(self, topic_cnt=0, bytes\ud310=0, records_denominator=None):\n        self.topic_cnt = topic_cnt\n        self.bytes_\ud310 = bytes_\ud310  # I assume you meant 'bytes' instead of 'bytes.pan'\n        self.records_denominator = records_denominator\n\n    def metrics(self, raw=False):\n        if not raw:\n            return {\"topic cnt\": self.topic_cnt}\n        metric_results = []\n        \n"
    },
    {
        "original": "def lf_conflicts(L, normalize_by_overlaps=False):\n    \"\"\"Return the **fraction of items each LF labels that are also given a\n    different (non-abstain) label by at least one other LF.**\n\n    Note that the maximum possible conflict fraction for an LF is the LF's\n        overlaps fraction, unless `normalize_by_overlaps=True`, in which case it\n        is 1.\n\n    Args:\n        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the\n            jth LF to the ith candidate\n        normalize_by_overlaps: Normalize by overlaps of the LF, so that it\n            returns the percent of LF overlaps that have conflicts.\n    \"\"\"\n    conflicts = (L != 0).T @ _conflicted_data_points(L) / L.shape[0]\n    if normalize_by_overlaps:\n        conflicts /= lf_overlaps(L)\n    return np.nan_to_num(conflicts)",
        "sampled": "```python\nimport numpy as np\nfrom scipy.sparse import csc_matrix\n\ndef lf_conflicts(L, normalize_by_overlaps=False):\n    \"\"\"\n    Return the fraction of items each LF labels that are also given a different (non-abstain) label by at least one other LF.\n\n    Args:\n        L: an n x m scipy.sparse matrix where L_{i,j} is the label given by the jth LF to the ith candidate\n        normalize_by_overlaps: Normalize by"
    },
    {
        "original": "def MatchBuildContext(self,\n                        target_os,\n                        target_arch,\n                        target_package,\n                        context=None):\n    \"\"\"Return true if target_platforms matches the supplied parameters.\n\n    Used by buildanddeploy to determine what clients need to be built.\n\n    Args:\n      target_os: which os we are building for in this run (linux, windows,\n        darwin)\n      target_arch: which arch we are building for in this run (i386, amd64)\n      target_package: which package type we are building (exe, dmg, deb, rpm)\n      context: config_lib context\n\n    Returns:\n      bool: True if target_platforms spec matches parameters.\n    \"\"\"\n    for spec in self.Get(\"ClientBuilder.target_platforms\", context=context):\n      spec_os, arch, package_name = spec.split(\"_\")\n      if (spec_os == target_os and arch == target_arch and\n          package_name == target_package):\n        return True\n    return False",
        "sampled": "```python\nclass BuildAndDeploy:\n    def __init__(self, context):\n        self.context = context\n        self.target_platforms = {}\n        \n    def set_target_platform(self, os, arch, package):\n        if os not in self.target_platforms:\n            self.target_platforms[os] = {}\n            \n        self.target_platforms[os][arch] = package\n        \n    def MatchBuildContext(self,\n                           target_os,\n                           target_arch,\n                           target_package,\n                           context=None):\n"
    },
    {
        "original": "def psubscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_patterns = dict.fromkeys(args)\n        new_patterns.update(kwargs)\n        ret_val = self.execute_command('PSUBSCRIBE', *iterkeys(new_patterns))\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val",
        "sampled": "```python\nclass PubSub:\n    def __init__(self):\n        self.patterns = {}\n        self.messages = {}\n\n    def psubscribe(self, **kwargs):\n        for channel, callback in kwargs.items():\n            if not callable(callback):\n                raise ValueError(\"Callback must be a callable\")\n            self.patterns.setdefault(channel, []).append(callback)\n\n    def publish(self, message, channel):\n        for callbacks in self.patterns.get(channel, []):\n            callbacks(message)\n\n# Test the implementation\n"
    },
    {
        "original": "def ekf_ok(self):\n        \"\"\"\n        ``True`` if the EKF status is considered acceptable, ``False`` otherwise (``boolean``).\n        \"\"\"\n        # legacy check for dronekit-python for solo\n        # use same check that ArduCopter::system.pde::position_ok() is using\n        if self.armed:\n            return self._ekf_poshorizabs and not self._ekf_constposmode\n        else:\n            return self._ekf_poshorizabs or self._ekf_predposhorizabs",
        "sampled": "```python\nclass EKF:\n    def __init__(self, xhat, P, F, H):\n        # Initial state estimation and covariance matrix\n        self.xhat = xhat\n        self.P = P\n\n        # State transition model and measurement model matrices\n        self.F = F\n        self.H = H\n\n    def ekf_ok(self):\n        \"\"\"\n        ``True`` if the EKF status is considered acceptable, ``False`` otherwise (``boolean``).\n        \n"
    },
    {
        "original": "def percentage_of_reoccurring_values_to_all_values(x):\n    \"\"\"\n    Returns the ratio of unique values, that are present in the time series\n    more than once.\n\n        # of data points occurring more than once / # of all data points\n\n    This means the ratio is normalized to the number of data points in the time series,\n    in contrast to the percentage_of_reoccurring_datapoints_to_all_datapoints.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n\n    if x.size == 0:\n        return np.nan\n\n    value_counts = x.value_counts()\n    reoccuring_values = value_counts[value_counts > 1].sum()\n\n    if np.isnan(reoccuring_values):\n        return 0\n\n    return reoccuring_values / x.size",
        "sampled": "```python\nimport numpy as np\n\ndef percentage_of_reoccurring_values_to_all_values(x):\n    \"\"\"\n    Returns the ratio of unique values, that are present in the time series\n    more than once.\n\n        # of data points occurring more than once / # of all data points\n\n    This means the ratio is normalized to the number of data points in the time series,\n    in contrast to the percentage_of_reoccurring_datapoints_to_all_datapoints.\n\n    :param x"
    },
    {
        "original": "def _job_sorting_key(self, job):\n        \"\"\"\n        Get the sorting key of a CFGJob instance.\n\n        :param CFGJob job: the CFGJob object.\n        :return: An integer that determines the order of this job in the queue.\n        :rtype: int\n        \"\"\"\n\n        if self._base_graph is None:\n            # we don't do sorting if there is no base_graph\n            return 0\n\n        MAX_JOBS = 1000000\n\n        if job.addr not in self._node_addr_visiting_order:\n            return MAX_JOBS\n\n        return self._node_addr_visiting_order.index(job.addr)",
        "sampled": "```python\nclass CFGJob:\n    def __init__(self, priority, deadline):\n        \"\"\"\n        Initialize a CFGJob instance.\n\n        :param int priority: The priority of the job.\n        :param int deadline: The deadline of the job.\n        \"\"\"\n        self.priority = priority\n        self.deadline = deadline\n\ndef _job_sorting_key(self, job):\n    \"\"\"\n    Get the sorting key of a CFGJob instance.\n\n    :param CFGJob job: the CFGJob object"
    },
    {
        "original": "def get_user_details_by_username(self, username, expand=None):\n        \"\"\"\n        Get information about a user through username\n        :param username: The user name\n        :param expand: OPTIONAL expand for get status of user.\n                Possible param is \"status\". Results are \"Active, Deactivated\"\n        :return: Returns the user details\n        \"\"\"\n        if expand:\n            url = 'rest/api/user?username={username}&expand={expand}'.format(username=username,\n                                                                             expand=expand)\n        else:\n            url = 'rest/api/user?username={username}'.format(username=username)\n\n        return self.get(url)",
        "sampled": "```python\nclass User:\n    def __init__(self, username, status):\n        self.username = username\n        self.status = status\n\nclass UserService:\n    def get_user_details_by_username(self, username, expand=None):\n        \"\"\"\n        Get information about a user through username\n        :param username: The user name\n        :param expand: OPTIONAL expand for get status of user.\n                Possible param is \"status\". Results are \"Active, Deactivated\"\n        :return: Returns the"
    },
    {
        "original": "def decode(pieces, sequence_length, model_file=None, model_proto=None,\n           reverse=False, name=None):\n  \"\"\"Decode pieces into postprocessed text.\n\n  Args:\n    pieces: A 2D int32 or string tensor [batch_size x max_length] of\n            encoded sequences.\n    sequence_length: A 1D int32 tensor [batch_size] representing the\n                   length of pieces.\n    model_file: The sentencepiece model file path.\n    model_proto: The sentencepiece model serialized proto.\n                 Either `model_file` or `model_proto` must be set.\n    reverse: Reverses the tokenized sequence (Default = false)\n    name: The name argument that is passed to the op function.\n\n  Returns:\n    text: A 1D string tensor of decoded string.\n  \"\"\"\n\n  return _gen_sentencepiece_processor_op.sentencepiece_decode(\n      pieces, sequence_length, model_file=model_file,\n      model_proto=model_proto, reverse=reverse, name=name)",
        "sampled": "```python\nimport tensorflow as tf\n\nclass SentencePieceTokenizer:\n    def __init__(self, model_file):\n        self.model_file = model_file\n        self.model_proto = None\n        self.vocab_size = 0\n\n    def _load_model(self):\n        if self.model_proto is not None:\n            return self.model_proto\n        elif tf.io.gfile.exists(self.model_file):\n            with open(self.model_file, 'rb') as f:\n                return f.read()\n        else:\n"
    },
    {
        "original": "def ParseSudoersEntry(self, entry, sudoers_config):\n    \"\"\"Parse an entry and add it to the given SudoersConfig rdfvalue.\"\"\"\n\n    key = entry[0]\n    if key in SudoersFieldParser.ALIAS_TYPES:\n      # Alias.\n      alias_entry = rdf_config_file.SudoersAlias(\n          type=SudoersFieldParser.ALIAS_TYPES.get(key), name=entry[1])\n\n      # Members of this alias, comma-separated.\n      members, _ = self._ExtractList(entry[2:], ignores=(\",\", \"=\"))\n      field = SudoersFieldParser.ALIAS_FIELDS.get(key)\n      getattr(alias_entry, field).Extend(members)\n\n      sudoers_config.aliases.append(alias_entry)\n    elif key.startswith(SudoersFieldParser.DEFAULTS_KEY):\n      # Default.\n      # Identify scope if one exists (Defaults<scope> ...)\n      scope = None\n      if len(key) > len(SudoersFieldParser.DEFAULTS_KEY):\n        scope = key[len(SudoersFieldParser.DEFAULTS_KEY) + 1:]\n\n      # There can be multiple defaults on a line, for the one scope.\n      entry = entry[1:]\n      defaults, _ = self._ExtractList(entry)\n      for default in defaults:\n        default_entry = rdf_config_file.SudoersDefault(scope=scope)\n\n        # Extract key name and value(s).\n        default_name = default\n        value = []\n        if \"=\" in default_name:\n          default_name, remainder = default_name.split(\"=\", 1)\n          value = [remainder]\n        default_entry.name = default_name\n        if entry:\n          default_entry.value = \" \".join(value)\n\n        sudoers_config.defaults.append(default_entry)\n    elif key in SudoersFieldParser.INCLUDE_KEYS:\n      # TODO(user): make #includedir more obvious in the RDFValue somewhere\n      target = \" \".join(entry[1:])\n      sudoers_config.includes.append(target)\n    else:\n      users, entry = self._ExtractList(entry)\n      hosts, entry = self._ExtractList(entry, terminators=(\"=\",))\n\n      # Remove = from <user> <host> = <specs>\n      if entry[0] == \"=\":\n        entry = entry[1:]\n\n      # Command specification.\n      sudoers_entry = rdf_config_file.SudoersEntry(\n          users=users, hosts=hosts, cmdspec=entry)\n\n      sudoers_config.entries.append(sudoers_entry)",
        "sampled": "```python\nfrom dataclasses import dataclass, field\nfrom typing import Dict\n\n@dataclass\nclass SudoersConfig:\n    proponents: Dict[str, str] = field(default_factory=dict)\n    inherit_from: list = field(default_factory=list)\n    effective_user_list: list = field(default_factory=list)\n\n\ndef parsesudoersentry(self, entry: dict, sudoers_config: SudoersConfig):\n    \"\"\"\n    Parse an entry and add it to the given SudoersConfig"
    },
    {
        "original": "def issues(self, **kwargs):\n        \"\"\"List issues related to this milestone.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of issues\n        \"\"\"\n\n        path = '%s/%s/issues' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = ProjectIssueManager(self.manager.gitlab,\n                                      parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, ProjectIssue, data_list)",
        "sampled": "```python\nfrom typing import Dict, Any\n\nclass RESTObjectList:\n    def __init__(self, items: list):\n        self.items = items\n\ndef issues(self, all: bool = False, per_page: int = 20, page: int = 1, as_list: bool = True, **kwargs) -> RESTObjectList:\n    \"\"\"\n    List issues related to this milestone.\n\n    Args:\n        all (bool): If True, return all the items without pagination\n"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a Tables object from a json dictionary.\"\"\"\n        args = {}\n        if 'location' in _dict:\n            args['location'] = Location._from_dict(_dict.get('location'))\n        if 'text' in _dict:\n            args['text'] = _dict.get('text')\n        if 'section_title' in _dict:\n            args['section_title'] = SectionTitle._from_dict(\n                _dict.get('section_title'))\n        if 'table_headers' in _dict:\n            args['table_headers'] = [\n                TableHeaders._from_dict(x) for x in (_dict.get('table_headers'))\n            ]\n        if 'row_headers' in _dict:\n            args['row_headers'] = [\n                RowHeaders._from_dict(x) for x in (_dict.get('row_headers'))\n            ]\n        if 'column_headers' in _dict:\n            args['column_headers'] = [\n                ColumnHeaders._from_dict(x)\n                for x in (_dict.get('column_headers'))\n            ]\n        if 'key_value_pairs' in _dict:\n            args['key_value_pairs'] = [\n                KeyValuePair._from_dict(x)\n                for x in (_dict.get('key_value_pairs'))\n            ]\n        if 'body_cells' in _dict:\n            args['body_cells'] = [\n                BodyCells._from_dict(x) for x in (_dict.get('body_cells'))\n            ]\n        return cls(**args)",
        "sampled": "```python\nclass Tables:\n    def __init__(self, name, rows):\n        self.name = name\n        self.rows = rows\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        return cls(_dict['name'], _dict['rows'])\n\ndef create_table(name, rows):\n    return Tables(name, rows)\n\ndef drop_table(table_name):\n    # Assuming a list of tables is available to drop tables from it\n    global all_tables  # Accessing the"
    },
    {
        "original": "def arg(self, state, index, stack_base=None):\n        \"\"\"\n        Returns a bitvector expression representing the nth argument of a function.\n\n        `stack_base` is an optional pointer to the top of the stack at the function start. If it is not\n        specified, use the current stack pointer.\n\n        WARNING: this assumes that none of the arguments are floating-point and they're all single-word-sized, unless\n        you've customized this CC.\n        \"\"\"\n        session = self.arg_session\n        if self.args is None:\n            arg_loc = [session.next_arg(False) for _ in range(index + 1)][-1]\n        else:\n            arg_loc = self.args[index]\n\n        return arg_loc.get_value(state, stack_base=stack_base)",
        "sampled": "```python\nclass BitVectorExpression:\n    def __init__(self, state, index, stack_base=None):\n        self.state = state\n        self.index = index\n        self.stack_base = stack_base\n\n    def arg(self):\n        if self.stack_base is None:\n            stack_base = self.state['stack_pointer']\n        else:\n            stack_base = self.stack_base\n        \n        # Calculate the address of the nth argument\n        address = stack_base + (self.index - 1)"
    },
    {
        "original": "def add_headerReference(self, type_, rId):\n        \"\"\"Return newly added CT_HdrFtrRef element of *type_* with *rId*.\n\n        The element tag is `w:headerReference`.\n        \"\"\"\n        headerReference = self._add_headerReference()\n        headerReference.type_ = type_\n        headerReference.rId = rId\n        return headerReference",
        "sampled": "```python\nfrom xml.etree import ElementTree as ET\n\nclass XmlHelper:\n    def add_headerReference(self, type_, rId):\n        \"\"\"Return newly added CT_HdrFtrRef element of *type_* with *rId*.\n\n        The element tag is `w:headerReference`.\n        \"\"\"\n        # Create a new CT_HdrFtrRef element\n        header_reference = ET.Element('w:headerReference')\n        \n        # Set the type attribute\n        header_reference"
    },
    {
        "original": "def cross_dir(self, forcex86=False):\n        r\"\"\"\n        Cross platform specific subfolder.\n\n        Parameters\n        ----------\n        forcex86: bool\n            Use 'x86' as current architecture even if current acritecture is\n            not x86.\n\n        Return\n        ------\n        subfolder: str\n            '' if target architecture is current architecture,\n            '\\current_target' if not.\n        \"\"\"\n        current = 'x86' if forcex86 else self.current_cpu\n        return (\n            '' if self.target_cpu == current else\n            self.target_dir().replace('\\\\', '\\\\%s_' % current)\n        )",
        "sampled": "```python\nclass System:\n    def cross_dir(self, forcex86=False):\n        r\"\"\"\n        Cross platform specific subfolder.\n\n        Parameters\n        ----------\n        forcex86: bool\n            Use 'x86' as current architecture even if current architecture is\n            not x86.\n\n        Return\n        ------\n        subfolder: str\n            '' if target architecture is current architecture,\n            '\\current_target' if not.\n        \"\"\"\n        # Get the current architecture\n        current_arch"
    },
    {
        "original": "def do_identity(args):\n    \"\"\"Executes the config commands subcommands.\n    \"\"\"\n    if args.subcommand == 'policy' and args.policy_cmd == 'create':\n        _do_identity_policy_create(args)\n    elif args.subcommand == 'policy' and args.policy_cmd == 'list':\n        _do_identity_policy_list(args)\n    elif args.subcommand == 'role' and args.role_cmd == 'create':\n        _do_identity_role_create(args)\n    elif args.subcommand == 'role' and args.role_cmd == 'list':\n        _do_identity_role_list(args)\n    else:\n        raise AssertionError(\n            '\"{}\" is not a valid subcommand of \"identity\"'.format(\n                args.subcommand))",
        "sampled": "```python\ndef do_identity(args):\n    \"\"\"\n    Executes the config commands subcommands.\n    \"\"\"\n    # Check if the subcommand is provided\n    if not args.subcommand:\n        print(\"Error: Subcommand not provided.\")\n        return\n\n    # Check if the subcommand is 'identity'\n    if args.subcommand == 'identity':\n        # Check if the identity subcommand has any arguments\n        if not args.identity:\n            print(\"Error: Identity subcommand requires an argument.\")\n"
    },
    {
        "original": "def _make_options(x):\n    \"\"\"Standardize the options tuple format.\n\n    The returned tuple should be in the format (('label', value), ('label', value), ...).\n\n    The input can be\n    * an iterable of (label, value) pairs\n    * an iterable of values, and labels will be generated\n    \"\"\"\n    # Check if x is a mapping of labels to values\n    if isinstance(x, Mapping):\n        import warnings\n        warnings.warn(\"Support for mapping types has been deprecated and will be dropped in a future release.\", DeprecationWarning)\n        return tuple((unicode_type(k), v) for k, v in x.items())\n\n    # only iterate once through the options.\n    xlist = tuple(x)\n\n    # Check if x is an iterable of (label, value) pairs\n    if all((isinstance(i, (list, tuple)) and len(i) == 2) for i in xlist):\n        return tuple((unicode_type(k), v) for k, v in xlist)\n\n    # Otherwise, assume x is an iterable of values\n    return tuple((unicode_type(i), i) for i in xlist)",
        "sampled": "```python\ndef _make_options(x):\n    \"\"\"\n    Standardize the options tuple format.\n\n    The returned tuple should be in the format (('label', value), ('label', value), ...).\n\n    The input can be\n    * an iterable of (label, value) pairs\n    * an iterable of values, and labels will be generated\n    \"\"\"\n    if all(isinstance(i, tuple) for i in x):\n        # Input is an iterable of (label, value) pairs\n"
    },
    {
        "original": "def _fetch_socket_data(self, parsed_url):\n        \"\"\" Hit a given stats socket and return the stats lines \"\"\"\n\n        self.log.debug(\"Fetching haproxy stats from socket: %s\" % parsed_url.geturl())\n\n        if parsed_url.scheme == 'tcp':\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            splitted_loc = parsed_url.netloc.split(':')\n            host = splitted_loc[0]\n            port = int(splitted_loc[1])\n            sock.connect((host, port))\n        else:\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n            sock.connect(parsed_url.path)\n        sock.send(b\"show stat\\r\\n\")\n\n        response = \"\"\n        output = sock.recv(BUFSIZE)\n        while output:\n            response += output.decode(\"ASCII\")\n            output = sock.recv(BUFSIZE)\n\n        sock.close()\n\n        return response.splitlines()",
        "sampled": "```python\nimport socket\n\nclass SocketClient:\n    def _fetch_socket_data(self, parsed_url):\n        \"\"\"\n        Hit a given stats socket and return the stats lines\n        \"\"\"\n        \n        # Extract the host and port from the parsed URL\n        host = parsed_url.netloc\n        port = int(parsed_url.path.split('/')[-1])\n        \n        # Create a socket object\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        \n        # Connect to the stats server\n       "
    },
    {
        "original": "def cpu_times(self):\n        \"\"\" return {pid: {'user': 0.0, 'sys': 0.0}}, chrome_reset \"\"\"\n        chrome_procs = self.get_chrome_procs()\n        new_pids = {p.pid for p in chrome_procs}\n        old_pids = {pid for pid in self.last_cpu_times}\n        try:\n            cpu_times = {p.pid: p.cpu_times() for p in chrome_procs}\n        except psutil.NoSuchProcess:\n            # Chrome restarted since fetching the new pids above. Better luck next time.\n            return {}, True\n        if new_pids != old_pids:\n            # We don't know when the Chrome procs were restarted, so don't\n            # return elapsed time until next run.\n            self.last_cpu_times = cpu_times\n            return {}, True\n        # Same chrome pids as last run: measure the elapsed cpu times\n        ordered_old_times = (self.last_cpu_times[p.pid] for p in chrome_procs)\n        ordered_new_times = (cpu_times[p.pid] for p in chrome_procs)\n        cpu_times_diff = {p.pid: {'user': (t[0] - l[0]) / self.interval, 'sys': (t[1] - l[1]) / self.interval}\n                for (p, t, l) in zip(chrome_procs, ordered_new_times, ordered_old_times)}\n        self.last_cpu_times = cpu_times\n        return cpu_times_diff, False",
        "sampled": "```python\nimport psutil\n\nclass ProcessController:\n    def cpu_times(self):\n        \"\"\" return {pid: {'user': 0.0, 'sys': 0.0}}, chrome_reset \"\"\"\n        \n        processes = []\n        \n        for proc in psutil.process_iter(['pid', 'name']):\n            try:\n                process_info = proc.as_dict(['pid', 'name'])\n                processes.append(process_info)\n            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.Z"
    },
    {
        "original": "def get_args(cls, dist, header=None):\n        \"\"\"\n        Yield write_script() argument tuples for a distribution's\n        console_scripts and gui_scripts entry points.\n        \"\"\"\n        if header is None:\n            header = cls.get_header()\n        spec = str(dist.as_requirement())\n        for type_ in 'console', 'gui':\n            group = type_ + '_scripts'\n            for name, ep in dist.get_entry_map(group).items():\n                cls._ensure_safe_name(name)\n                script_text = cls.template % locals()\n                args = cls._get_script_args(type_, name, header, script_text)\n                for res in args:\n                    yield res",
        "sampled": "```python\ndef get_args(cls, dist, header=None):\n    \"\"\"\n    Yield write_script() argument tuples for a distribution's\n    console_scripts and gui_scripts entry points.\n    \"\"\"\n    \n    # Check if the class has the required attributes\n    if not hasattr(cls, 'console_scripts') or not hasattr(cls, 'gui_scripts'):\n        return\n    \n    # Iterate over console scripts\n    for script in cls.console_scripts:\n        name = script.get('name')\n        entry_point = script"
    },
    {
        "original": "def _fill_form_exclude_cols(self, exclude_cols, form):\n        \"\"\"\n            fill the form with the suppressed cols, generated from exclude_cols\n        \"\"\"\n        for filter_key in exclude_cols:\n            filter_value = self._filters.get_filter_value(filter_key)\n            rel_obj = self.datamodel.get_related_obj(filter_key, filter_value)\n            if hasattr(form, filter_key):\n                field = getattr(form, filter_key)\n                field.data = rel_obj",
        "sampled": "```python\nclass FormFiller:\n    def _fill_form_exclude_cols(self, exclude_cols, form):\n        \"\"\"\n        Fill the form with the suppressed cols, generated from exclude_cols.\n\n        Args:\n            exclude_cols (list): A list of column names to be excluded.\n            form (dict): A dictionary representing the form to be filled.\n\n        Returns:\n            dict: The filled form with values for all columns except the ones specified in `exclude_cols`.\n        \"\"\"\n        \n        # Create a"
    },
    {
        "original": "def sample_within_cc(self, cc_index, nsamples=1):\n        \"\"\"\n        Returns randomly sampled points from a polygon.\n\n        Complexity of this procedure is (A/a * nsamples) where A=area(bbox(P))\n        and a=area(P) where P is the polygon of the connected component cc_index\n        \"\"\"\n        polygon = self.geometries[cc_index]['polygon']\n        samples = []\n        while len(samples) < nsamples:\n            point = PointSampler.random_point(polygon.envelope.bounds)\n            if PointSampler.contains(polygon, point):\n                samples.append(point)\n        return samples",
        "sampled": "```python\nimport numpy as np\n\nclass PointXY:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n\nclass Polygon:\n    def __init__(self, points):\n        self.points = [PointXY(x=x, y=y) for (x,y) in points]\n        if not len(points):\n            return\n\n        # Find center coordinates of bounding box (bbox)\n        nums = [p.x for p in self.points]"
    },
    {
        "original": "def register_next_step_handler_by_chat_id(self, chat_id, callback, *args, **kwargs):\n        \"\"\"\n        Registers a callback function to be notified when new message arrives after `message`.\n\n        Warning: In case `callback` as lambda function, saving next step handlers will not work.\n\n        :param chat_id:     The chat for which we want to handle new message.\n        :param callback:    The callback function which next new message arrives.\n        :param args:        Args to pass in callback func\n        :param kwargs:      Args to pass in callback func\n        \"\"\"\n        if chat_id in self.next_step_handlers.keys():\n            self.next_step_handlers[chat_id].append(Handler(callback, *args, **kwargs))\n        else:\n            self.next_step_handlers[chat_id] = [Handler(callback, *args, **kwargs)]\n\n        if self.next_step_saver is not None:\n            self.next_step_saver.start_save_timer()",
        "sampled": "```python\nclass MessageHandler:\n    def __init__(self):\n        self.call_back_handlers = {}\n\n    def register_next_step_handler_by_chat_id(self, chat_id, callback, *args, **kwargs):\n        \"\"\"\n        Registers a callback function to be notified when new message arrives after `message`.\n\n        Warning: In case `callback` as lambda function, saving next step handlers will not work.\n\n        :param chat_id:     The chat for which we want to handle new message.\n       "
    },
    {
        "original": "def _check_1d_array_object(parameter, name_param):\n    \"\"\"\n    Checks whether given parameter is a 1d array like object, and returns a numpy array object\n    \"\"\"\n    if isinstance(parameter, (np.ndarray, list, tuple, np.matrix)):\n        parameter = np.array(parameter)\n        if parameter.ndim != 1:\n            raise TypeError(\"{} should be a 1d array type object\".format(name_param))\n    else:\n        raise TypeError(\"{} should be a 1d array type object\".format(name_param))\n\n    return parameter",
        "sampled": "```python\nimport numpy as np\nimport pandas as pd\n\ndef _check_1d_array_object(parameter, name_param):\n    \"\"\"\n    Checks whether given parameter is a 1d array like object, and returns a numpy array object\n    \"\"\"\n    \n    # Check if the input is an iterable (list, tuple, etc.)\n    if not hasattr(parameter, '__iter__'):\n        raise TypeError(f\"'{name_param}' must be an iterable.\")\n    \n    # Check if the input is"
    },
    {
        "original": "def set_cursor_manager(self, manager_class):\n        \"\"\"DEPRECATED - Set this client's cursor manager.\n\n        Raises :class:`TypeError` if `manager_class` is not a subclass of\n        :class:`~pymongo.cursor_manager.CursorManager`. A cursor manager\n        handles closing cursors. Different managers can implement different\n        policies in terms of when to actually kill a cursor that has\n        been closed.\n\n        :Parameters:\n          - `manager_class`: cursor manager to use\n\n        .. versionchanged:: 3.3\n           Deprecated, for real this time.\n\n        .. versionchanged:: 3.0\n           Undeprecated.\n        \"\"\"\n        warnings.warn(\n            \"set_cursor_manager is Deprecated\",\n            DeprecationWarning,\n            stacklevel=2)\n        manager = manager_class(self)\n        if not isinstance(manager, CursorManager):\n            raise TypeError(\"manager_class must be a subclass of \"\n                            \"CursorManager\")\n\n        self.__cursor_manager = manager",
        "sampled": "```python\nclass CursorManager:\n    \"\"\"A cursor manager handles closing cursors.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the cursor manager.\"\"\"\n        self.cursors = []\n\n    def add_cursor(self, cursor):\n        \"\"\"Add a cursor to the manager.\n\n        :Parameters:\n          - `cursor`: cursor to add\n        \"\"\"\n        self.cursors.append(cursor)\n\n    def close_cursors(self):\n        \"\"\"Close all cursors managed by this manager.\"\"\"\n        for cursor in self"
    },
    {
        "original": "def status(name, sig=None):\n    \"\"\"\n    Return the status for a service, returns a bool whether the service is\n    running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>\n    \"\"\"\n    if sig:\n        return bool(__salt__['status.pid'](sig))\n\n    cmd = '{0} check {1}'.format(_cmd(), name)\n    return not __salt__['cmd.retcode'](cmd)",
        "sampled": "```python\nimport subprocess\n\ndef status(name, sig=None):\n    \"\"\"\n    Return the status for a service, returns a bool whether the service is\n    running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>\n    \"\"\"\n    if sig is not None:\n        raise ValueError(\"sig is not supported in this function\")\n    \n    try:\n        output = subprocess.check_output(['systemctl', 'status', name])\n        return b'Active:'"
    },
    {
        "original": "def orientation(self):\n        \"\"\"\n        The member of the ``WD_ORIENTATION`` enumeration corresponding to the\n        value of the ``orient`` attribute of the ``<w:pgSz>`` child element,\n        or ``WD_ORIENTATION.PORTRAIT`` if not present.\n        \"\"\"\n        pgSz = self.pgSz\n        if pgSz is None:\n            return WD_ORIENTATION.PORTRAIT\n        return pgSz.orient",
        "sampled": "```python\nfrom enum import Enum\n\nclass WD_ORIENTATION(Enum):\n    PORTRAIT = 1\n    LANDSCAPE = 2\n\nclass PageSize:\n    def __init__(self, orient=None):\n        self.orient = orient\n\n    def orientation(self):\n        \"\"\"\n        The member of the ``WD_ORIENTATION`` enumeration corresponding to the\n        value of the ``orient`` attribute of the ``<w:pgSz>`` child element,\n        or ``WD_ORIENTATION.PORTR"
    },
    {
        "original": "def random_digit_not_null_or_empty(self):\n        \"\"\"\n        Returns a random non-zero digit/number\n        between 1 and 9 or and empty string.\n        \"\"\"\n        if self.generator.random.randint(0, 1):\n            return self.generator.random.randint(1, 9)\n        else:\n            return ''",
        "sampled": "```python\nimport random\n\nclass RandomGenerator:\n    def random_digit_not_null_or_empty(self):\n        \"\"\"\n        Returns a random non-zero digit/number\n        between 1 and 9 or an empty string.\n        \"\"\"\n        \n        # Generate a list of non-zero digits (1-9)\n        digits = [str(i) for i in range(1, 10)]\n        \n        # Randomly select either a digit from the list or an empty string\n        if bool(random.get"
    },
    {
        "original": "def d3logpdf_dlink3(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Third order derivative log-likelihood function at y given link(f) w.r.t link(f)\n\n        .. math::\n            \\\\frac{d^{3} \\\\ln p(y_{i}|\\lambda(f_{i}))}{d^{3}\\\\lambda(f)} = \\\\frac{-2(v+1)((y_{i} - \\lambda(f_{i}))^3 - 3(y_{i} - \\lambda(f_{i})) \\\\sigma^{2} v))}{((y_{i} - \\lambda(f_{i})) + \\\\sigma^{2} v)^3}\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata which is not used in student t distribution\n        :returns: third derivative of likelihood evaluated at points f\n        :rtype: Nx1 array\n        \"\"\"\n        e = y - inv_link_f\n        d3lik_dlink3 = ( -(2*(self.v + 1)*(-e)*(e**2 - 3*self.v*self.sigma2)) /\n                       ((e**2 + self.sigma2*self.v)**3)\n                    )\n        return d3lik_dlink3",
        "sampled": "```python\nimport numpy as np\n\nclass Solution:\n    def d3logpdf_dlink3(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Third order derivative log-likelihood function at y given link(f) w.r.t link(f)\n        \n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata:"
    },
    {
        "original": "def get_extended_surface_mesh(self, repeat=(5, 5, 1)):\n        \"\"\"\n        Gets an extended surface mesh for to use for adsorption\n        site finding by constructing supercell of surface sites\n\n        Args:\n            repeat (3-tuple): repeat for getting extended surface mesh\n        \"\"\"\n        surf_str = Structure.from_sites(self.surface_sites)\n        surf_str.make_supercell(repeat)\n        return surf_str",
        "sampled": "```python\nimport numpy as np\n\nclass SurfaceMesh:\n    def __init__(self, surface_sites, cell_size):\n        \"\"\"\n        Initializes a surface mesh with surface sites and cell size\n\n        Args:\n            surface_sites (list): list of surface sites\n            cell_size (3-tuple): cell size for surface mesh\n        \"\"\"\n        self.surface_sites = surface_sites\n        self.cell_size = cell_size\n\n    def get_extended_surface_mesh(self, repeat):\n        \"\"\"\n        Gets an"
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "sampled": "```python\nclass EOPatchSaver:\n    def save(self, eopatch, use_tmp=True):\n        \"\"\"\n        Method which does the saving\n\n        :param eopatch: EOPatch containing the data which will be saved\n        :type eopatch: EOPatch\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\n        (i.e. final) location\n        :type use"
    },
    {
        "original": "def get_repos(self, type=github.GithubObject.NotSet, sort=github.GithubObject.NotSet,\n                  direction=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /users/:user/repos <http://developer.github.com/v3/repos>`_\n        :param type: string\n        :param sort: string\n        :param direction: string\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Repository.Repository`\n        \"\"\"\n        assert type is github.GithubObject.NotSet or isinstance(type, (str, unicode)), type\n        assert sort is github.GithubObject.NotSet or isinstance(sort, (str, unicode)), sort\n        assert direction is github.GithubObject.NotSet or isinstance(direction, (str, unicode)), direction\n        url_parameters = dict()\n        if type is not github.GithubObject.NotSet:\n            url_parameters[\"type\"] = type\n        if sort is not github.GithubObject.NotSet:\n            url_parameters[\"sort\"] = sort\n        if direction is not github.GithubObject.NotSet:\n            url_parameters[\"direction\"] = direction\n        return github.PaginatedList.PaginatedList(\n            github.Repository.Repository,\n            self._requester,\n            self.url + \"/repos\",\n            url_parameters\n        )",
        "sampled": "```python\nfrom github import Github\nfrom github.PaginatedList import PaginatedList\nfrom github.Repository import Repository\n\nclass GithubClient:\n    def __init__(self, token):\n        self.g = Github(token)\n\n    def get_repos(self, user, type=PaginatedList.NotSet, sort=PaginatedList.NotSet, direction=PaginatedList.NotSet):\n        \"\"\"\n        :calls: `GET /users/:user/repos <https://docs.github.com/en/rest/reference"
    },
    {
        "original": "def _graph_add_edge(self, src_block_id, dst_block_id, **kwargs):\n        \"\"\"\n        Add an edge onto the graph.\n\n        :param BlockID src_block_id: The block ID for source node.\n        :param BlockID dst_block_id: The block Id for destination node.\n        :param str jumpkind:         The jumpkind of the edge.\n        :param exit_stmt_idx:        ID of the statement in the source IRSB where this edge is created from. 'default'\n                                     refers to the default exit.\n        :return: None\n        \"\"\"\n\n        dst_node = self._graph_get_node(dst_block_id, terminator_for_nonexistent_node=True)\n\n        if src_block_id is None:\n            self.graph.add_node(dst_node)\n\n        else:\n            src_node = self._graph_get_node(src_block_id, terminator_for_nonexistent_node=True)\n            self.graph.add_edge(src_node, dst_node, **kwargs)",
        "sampled": "```python\nclass Graph:\n    def __init__(self):\n        self.graph = {}\n\n    def _graph_add_edge(self, src_block_id, dst_block_id, jumpkind, exit_stmt_idx):\n        \"\"\"\n        Add an edge onto the graph.\n\n        :param src_block_id: The block ID for source node.\n        :param dst_block_id: The block Id for destination node.\n        :param jumpkind:     The jumpkind of the edge.\n        :param exit_stmt_idx:"
    },
    {
        "original": "def gild(self, months=None):\n        \"\"\"Gild the Redditor or author of the content.\n\n        :param months: Specifies the number of months to gild. This parameter\n            is Only valid when the instance called upon is of type\n            Redditor. When not provided, the value defaults to 1.\n        :returns: True on success, otherwise raises an exception.\n\n        \"\"\"\n        if isinstance(self, Redditor):\n            months = int(months) if months is not None else 1\n            if months < 1:\n                raise TypeError('months must be at least 1')\n            if months > 36:\n                raise TypeError('months must be no more than 36')\n            response = self.reddit_session.request(\n                self.reddit_session.config['gild_user'].format(\n                    username=six.text_type(self)), data={'months': months})\n        elif months is not None:\n            raise TypeError('months is not a valid parameter for {0}'\n                            .format(type(self)))\n        else:\n            response = self.reddit_session.request(\n                self.reddit_session.config['gild_thing']\n                .format(fullname=self.fullname), data=True)\n        return response.status_code == 200",
        "sampled": "```python\nclass Redditor:\n    def __init__(self, name):\n        self.name = name\n        self.gilds = 0\n\n    def gild(self, months=None):\n        \"\"\"Gild the Redditor or author of the content.\n\n        :param months: Specifies the number of months to gild. This parameter\n            is Only valid when the instance called upon is of type\n            Redditor. When not provided, the value defaults to 1.\n        :"
    },
    {
        "original": "def plot(self, columns=None, **errorbar_kwargs):\n        \"\"\"\n        Produces a visual representation of the coefficients, including their standard errors and magnitudes.\n\n        Parameters\n        ----------\n        columns : list, optional\n            specify a subset of the columns to plot\n        errorbar_kwargs:\n            pass in additional plotting commands to matplotlib errorbar command\n\n        Returns\n        -------\n        ax: matplotlib axis\n            the matplotlib axis that be edited.\n\n        \"\"\"\n        from matplotlib import pyplot as plt\n\n        ax = errorbar_kwargs.pop(\"ax\", None) or plt.figure().add_subplot(111)\n\n        errorbar_kwargs.setdefault(\"c\", \"k\")\n        errorbar_kwargs.setdefault(\"fmt\", \"s\")\n        errorbar_kwargs.setdefault(\"markerfacecolor\", \"white\")\n        errorbar_kwargs.setdefault(\"markeredgewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"elinewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"capsize\", 3)\n\n        z = inv_normal_cdf(1 - self.alpha / 2)\n\n        if columns is None:\n            columns = self.hazards_.index\n\n        yaxis_locations = list(range(len(columns)))\n        symmetric_errors = z * self.standard_errors_[columns].to_frame().squeeze(axis=1).values.copy()\n        hazards = self.hazards_[columns].values.copy()\n\n        order = np.argsort(hazards)\n\n        ax.errorbar(hazards[order], yaxis_locations, xerr=symmetric_errors[order], **errorbar_kwargs)\n        best_ylim = ax.get_ylim()\n        ax.vlines(0, -2, len(columns) + 1, linestyles=\"dashed\", linewidths=1, alpha=0.65)\n        ax.set_ylim(best_ylim)\n\n        tick_labels = [columns[i] for i in order]\n\n        ax.set_yticks(yaxis_locations)\n        ax.set_yticklabels(tick_labels)\n        ax.set_xlabel(\"log(HR) (%g%% CI)\" % ((1 - self.alpha) * 100))\n\n        return ax",
        "sampled": "```python\nimport matplotlib.pyplot as plt\n\nclass Plotter:\n    def __init__(self, coefficients, standard_errors):\n        self.coefficients = coefficients\n        self.standard_errors = standard_errors\n\n    def plot(self, columns=None, **errorbar_kwargs):\n        \"\"\"\n        Produces a visual representation of the coefficients, including their standard errors and magnitudes.\n\n        Parameters\n        ----------\n        columns : list, optional\n            specify a subset of the columns to plot\n        errorbar_kwargs:\n"
    },
    {
        "original": "def _CheckConditionsShortCircuit(content_conditions, pathspec):\n  \"\"\"Checks all `content_conditions` until one yields no matches.\"\"\"\n  matches = []\n  for cond in content_conditions:\n    with vfs.VFSOpen(pathspec) as vfs_file:\n      cur_matches = list(cond.Search(vfs_file))\n    if cur_matches:\n      matches.extend(cur_matches)\n    else:  # As soon as one condition does not match, we skip the file.\n      return []  # Return no matches to indicate skipping this file.\n  return matches",
        "sampled": "```python\ndef _CheckConditionsShortCircuit(content_conditions, pathspec):\n    \"\"\"\n    Checks all `content_conditions` until one yields no matches.\n    \n    Args:\n        content_conditions (list): A list of functions that take a pathspec and return a boolean value.\n        pathspec (str): The path to check against the conditions.\n\n    Returns:\n        bool: True if any condition is met, False otherwise.\n    \"\"\"\n    \n    # Iterate over each condition in the list of content"
    },
    {
        "original": "def getNym(self, nym, role=None, isCommitted=True):\n        \"\"\"\n        Get a nym, if role is provided then get nym with that role\n        :param nym:\n        :param role:\n        :param isCommitted:\n        :return:\n        \"\"\"\n        try:\n            seqNo, txnTime, ta, actual_role, verkey = self.get(nym, isCommitted)\n        except KeyError:\n            return None\n        if role and role != actual_role:\n            return None\n        return {\n            ROLE: actual_role or None,\n            VERKEY: verkey or None,\n            f.IDENTIFIER.nm: ta or None,\n            f.SEQ_NO.nm: seqNo or None,\n            TXN_TIME: txnTime or None,\n        }",
        "sampled": "```python\nclass Nym:\n    def __init__(self, nym, role=None):\n        self.nym = nym\n        self.role = role\n\n    def getNym(self, role=None, isCommitted=True):\n        if role is not None:\n            return Nym(self.nym, role)\n        else:\n            return self.nym\n\n# Example usage:\nnym1 = Nym(\"John\")\nprint(nym1.getNym())  # Output: John"
    },
    {
        "original": "def create(self, chat_id, name, owner, user_list):\n        \"\"\"\n        \u521b\u5efa\u4f1a\u8bdd\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u4f01\u4e1a\u4f1a\u8bdd\u63a5\u53e3\u8bf4\u660e\n\n        :param chat_id: \u4f1a\u8bddid\u3002\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u6700\u957f32\u4e2a\u5b57\u7b26\u3002\u53ea\u5141\u8bb8\u5b57\u7b260-9\u53ca\u5b57\u6bcda-zA-Z,\n                        \u5982\u679c\u503c\u5185\u5bb9\u4e3a64bit\u65e0\u7b26\u53f7\u6574\u578b\uff1a\u8981\u6c42\u503c\u8303\u56f4\u5728[1, 2^63)\u4e4b\u95f4\uff0c\n                        [2^63, 2^64)\u4e3a\u7cfb\u7edf\u5206\u914d\u4f1a\u8bddid\u533a\u95f4\n        :param name: \u4f1a\u8bdd\u6807\u9898\n        :param owner: \u7ba1\u7406\u5458userid\uff0c\u5fc5\u987b\u662f\u8be5\u4f1a\u8bdduserlist\u7684\u6210\u5458\u4e4b\u4e00\n        :param user_list: \u4f1a\u8bdd\u6210\u5458\u5217\u8868\uff0c\u6210\u5458\u7528userid\u6765\u6807\u8bc6\u3002\u4f1a\u8bdd\u6210\u5458\u5fc5\u987b\u57283\u4eba\u6216\u4ee5\u4e0a\uff0c1000\u4eba\u4ee5\u4e0b\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n        return self._post(\n            'chat/create',\n            data={\n                'chatid': chat_id,\n                'name': name,\n                'owner': owner,\n                'userlist': user_list,\n            }\n        )",
        "sampled": "```python\nimport requests\n\nclass WeChatSession:\n    def create(self, chat_id, name, owner, user_list):\n        \"\"\"\n        \u521b\u5efa\u4f1a\u8bdd\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u4f01\u4e1a\u4f1a\u8bdd\u63a5\u53e3\u8bf4\u660e\n\n        :param chat_id: \u4f1a\u8bddid\u3002\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u6700\u957f32\u4e2a\u5b57\u7b26\u3002\u53ea\u5141\u8bb8\u5b57\u7b260-9\u53ca\u5b57\u6bcda-zA-Z,\n                        \u5982\u679c\u503c\u5185\u5bb9\u4e3a64bit\u65e0"
    },
    {
        "original": "def unicode_compatible(cls):\n    \"\"\"\n    Decorator for unicode compatible classes. Method ``__unicode__``\n    has to be implemented to work decorator as expected.\n    \"\"\"\n    if PY3:\n        cls.__str__ = cls.__unicode__\n        cls.__bytes__ = lambda self: self.__str__().encode(\"utf-8\")\n    else:\n        cls.__str__ = lambda self: self.__unicode__().encode(\"utf-8\")\n\n    return cls",
        "sampled": "```python\ndef unicode_compatible(cls):\n    \"\"\"\n    Decorator for unicode compatible classes. Method ``__unicode__``\n    has to be implemented to work decorator as expected.\n    \"\"\"\n    \n    class Wrapper(cls):\n        def __unicode__(self):\n            return str(self)\n        \n        def __str__(self):\n            return self.__unicode__()\n    \n    return Wrapper\n\n@unicode_compatible\nclass MyClass:\n    def __init__(self, value):\n        self.value = value\n    \n    def"
    },
    {
        "original": "def IsCloud(self, request, bios_version, services):\n    \"\"\"Test to see if we're on a cloud machine.\"\"\"\n    if request.bios_version_regex and bios_version:\n      if re.match(request.bios_version_regex, bios_version):\n        return True\n    if request.service_name_regex and services:\n      if re.search(request.service_name_regex, services):\n        return True\n    return False",
        "sampled": "```python\ndef IsCloud(self, request, bios_version, services):\n    \"\"\"\n    Test to see if we're on a cloud machine.\n    \n    Parameters:\n    request (str): The HTTP request object.\n    bios_version (str): The BIOS version of the machine.\n    services (list): A list of available services.\n\n    Returns:\n    bool: True if we're on a cloud machine, False otherwise.\n    \"\"\"\n    \n    # Check if the BIOS version is 'cloud' or"
    },
    {
        "original": "def notify_block_new(self, block):\n        \"\"\"A new block was received and passed initial consensus validation\"\"\"\n        payload = block.SerializeToString()\n        self._notify(\n            \"consensus_notifier_notify_block_new\", payload, len(payload))",
        "sampled": "```python\nclass Blockchain:\n    def __init__(self):\n        self.chain = []\n\n    def notify_block_new(self, block):\n        \"\"\"\n        A new block was received and passed initial consensus validation\n\n        Args:\n            block (dict): The newly added block to the blockchain.\n\n            Block format: {'index': int, 'previous_hash': str, 'timestamp': float,\n                        'data': array, 'hash': str}\n\n                           Index: the index of current block in blockchain.\n                          "
    },
    {
        "original": "def apply_escape_bytes(byte_data, escape, skip_printable=False):\n    \"\"\"\n    Apply the specified escape method on the given bytes.\n    :param byte_data: The byte-like object with bytes to escape.\n    :param escape: The escape method to use.\n    :param skip_printable: If True, don't escape byte_data with all 'printable ASCII' bytes. Defaults to False.\n    :return: new bytes object with the escaped bytes or byte_data itself on some no-op cases.\n    \"\"\"\n\n    if isnumber(byte_data):\n        if skip_printable:\n            return num2bytes(byte_data)\n        else:\n            byte_data = num2bytes(byte_data)\n    else:\n        assert (isinstance(byte_data, type(b'')))\n        if skip_printable and all(0x20 <= bval(ch) <= 0x7E for ch in byte_data):\n            escape = STRING_ESCAPE_RAW\n\n    if escape == STRING_ESCAPE_RAW:\n        return byte_data\n    elif escape == STRING_ESCAPE_PRINT:\n        return escape_ascii_bytes(byte_data)\n    elif escape == STRING_ESCAPE_UTF8:\n        return codecs.encode(escape_utf8(byte_data), 'utf-8')\n    elif escape == STRING_ESCAPE_BASE64:\n        return base64.b64encode(byte_data)\n    else:\n        raise UnicodeEncodeError(\"Unknown escape option\")",
        "sampled": "```python\ndef apply_escape_bytes(byte_data, escape, skip_printable=False):\n    \"\"\"\n    Apply the specified escape method on the given bytes.\n    :param byte_data: The byte-like object with bytes to escape.\n    :param escape: The escape method to use.\n    :param skip_printable: If True, don't escape byte_data with all 'printable ASCII' bytes. Defaults to False.\n    :return: new bytes object with the escaped bytes or byte_data itself on"
    },
    {
        "original": "def all_successors(self, node, skip_reached_fixedpoint=False):\n        \"\"\"\n        Returns all successors to the specific node.\n\n        :param node: A node in the graph.\n        :return:     A set of nodes that are all successors to the given node.\n        :rtype:      set\n        \"\"\"\n\n        successors = set()\n\n        stack = [ node ]\n        while stack:\n            n = stack.pop()\n            successors.add(n)\n            stack.extend(succ for succ in self.successors(n) if\n                         succ not in successors and\n                            (not skip_reached_fixedpoint or succ not in self._reached_fixedpoint)\n                         )\n\n        return successors",
        "sampled": "```python\nclass Graph:\n    def __init__(self):\n        self.nodes = {}\n\n    def add_node(self, node):\n        if node not in self.nodes:\n            self.nodes[node] = set()\n\n    def add_edge(self, from_node, to_node):\n        if from_node in self.nodes and to_node in self.nodes:\n            self.nodes[from_node].add(to_node)\n\n    def all_successors(self, node, skip_reached_fixedpoint=False):\n        \"\"\"\n        Returns all successors"
    },
    {
        "original": "def get(self, model_module, model_module_version, model_name, view_module, view_module_version, view_name):\n        \"\"\"Get a value\"\"\"\n        module_versions = self._registry[model_module]\n        # The python semver module doesn't work well, for example, it can't do match('3', '*')\n        # so we just take the first model module version.\n        #model_names = next(v for k, v in module_versions.items()\n        #                   if semver.match(model_module_version, k))\n        model_names = list(module_versions.values())[0]\n        view_modules = model_names[model_name]\n        view_versions = view_modules[view_module]\n        # The python semver module doesn't work well, so we just take the first view module version\n        #view_names = next(v for k, v in view_versions.items()\n        #                  if semver.match(view_module_version, k))\n        view_names = list(view_versions.values())[0]\n        widget_class = view_names[view_name]\n        return widget_class",
        "sampled": "```python\ndef get(self, model_module, model_module_version, model_name, view_module, view_module_version, view_name):\n    \"\"\"\n    Get a value\n    \"\"\"\n    \n    # Combine the module and version to create a unique identifier for the module\n    module_id = f\"{model_module}_{model_module_version}\"\n    \n    # Combine the view and version to create a unique identifier for the view\n    view_id = f\"{view_module}_{view_module_version}\"\n    \n    # Create"
    },
    {
        "original": "def _create_kraus_pragmas(name, qubit_indices, kraus_ops):\n    \"\"\"\n    Generate the pragmas to define a Kraus map for a specific gate on some qubits.\n\n    :param str name: The name of the gate.\n    :param list|tuple qubit_indices: The qubits\n    :param list|tuple kraus_ops: The Kraus operators as matrices.\n    :return: A QUIL string with PRAGMA ADD-KRAUS ... statements.\n    :rtype: str\n    \"\"\"\n\n    pragmas = [Pragma(\"ADD-KRAUS\",\n                      [name] + list(qubit_indices),\n                      \"({})\".format(\" \".join(map(format_parameter, np.ravel(k)))))\n               for k in kraus_ops]\n    return pragmas",
        "sampled": "```python\ndef _create_kraus_pragmas(name, qubit_indices, kraus_ops):\n    \"\"\"\n    Generate the pragmas to define a Kraus map for a specific gate on some qubits.\n\n    :param str name: The name of the gate.\n    :param list|tuple qubit_indices: The qubits\n    :param list|tuple kraus_ops: The Kraus operators as matrices.\n    :return: A QUIL string with PRAGMA ADD-K"
    },
    {
        "original": "def get_temp_file(keep=False, autoext=\"\", fd=False):\n    \"\"\"Creates a temporary file.\n\n    :param keep: If False, automatically delete the file when Scapy exits.\n    :param autoext: Suffix to add to the generated file name.\n    :param fd: If True, this returns a file-like object with the temporary\n               file opened. If False (default), this returns a file path.\n    \"\"\"\n    f = tempfile.NamedTemporaryFile(prefix=\"scapy\", suffix=autoext,\n                                    delete=False)\n    if not keep:\n        conf.temp_files.append(f.name)\n\n    if fd:\n        return f\n    else:\n        # Close the file so something else can take it.\n        f.close()\n        return f.name",
        "sampled": "```python\nimport tempfile\n\ndef get_temp_file(keep=False, autoext=\"\", fd=False):\n    \"\"\"Creates a temporary file.\n\n    :param keep: If False, automatically delete the file when Scapy exits.\n    :param autoext: Suffix to add to the generated file name.\n    :param fd: If True, this returns a file-like object with the temporary\n               file opened. If False (default), this returns a file path.\n    \"\"\"\n    \n    # Create a"
    },
    {
        "original": "def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please check ProxyMinion._post_master_init\n        to see if those changes need to be propagated.\n\n        Minions and ProxyMinions need significantly different post master setups,\n        which is why the differences are not factored out into separate helper\n        functions.\n        \"\"\"\n        if self.connected:\n            self.opts['master'] = master\n\n            # Initialize pillar before loader to make pillar accessible in modules\n            async_pillar = salt.pillar.get_async_pillar(\n                self.opts,\n                self.opts['grains'],\n                self.opts['id'],\n                self.opts['saltenv'],\n                pillarenv=self.opts.get('pillarenv')\n            )\n            self.opts['pillar'] = yield async_pillar.compile_pillar()\n            async_pillar.destroy()\n\n        if not self.ready:\n            self._setup_core()\n        elif self.connected and self.opts['pillar']:\n            # The pillar has changed due to the connection to the master.\n            # Reload the functions so that they can use the new pillar data.\n            self.functions, self.returners, self.function_errors, self.executors = self._load_modules()\n            if hasattr(self, 'schedule'):\n                self.schedule.functions = self.functions\n                self.schedule.returners = self.returners\n\n        if not hasattr(self, 'schedule'):\n            self.schedule = salt.utils.schedule.Schedule(\n                self.opts,\n                self.functions,\n                self.returners,\n                cleanup=[master_event(type='alive')])\n\n        # add default scheduling jobs to the minions scheduler\n        if self.opts['mine_enabled'] and 'mine.update' in self.functions:\n            self.schedule.add_job({\n                '__mine_interval':\n                {\n                    'function': 'mine.update',\n                    'minutes': self.opts['mine_interval'],\n                    'jid_include': True,\n                    'maxrunning': 2,\n                    'run_on_start': True,\n                    'return_job': self.opts.get('mine_return_job', False)\n                }\n            }, persist=True)\n            log.info('Added mine.update to scheduler')\n        else:\n            self.schedule.delete_job('__mine_interval', persist=True)\n\n        # add master_alive job if enabled\n        if (self.opts['transport'] != 'tcp' and\n                self.opts['master_alive_interval'] > 0 and\n                self.connected):\n            self.schedule.add_job({\n                master_event(type='alive', master=self.opts['master']):\n                {\n                    'function': 'status.master',\n                    'seconds': self.opts['master_alive_interval'],\n                    'jid_include': True,\n                    'maxrunning': 1,\n                    'return_job': False,\n                    'kwargs': {'master': self.opts['master'],\n                                'connected': True}\n                }\n            }, persist=True)\n            if self.opts['master_failback'] and \\\n                    'master_list' in self.opts and \\\n                    self.opts['master'] != self.opts['master_list'][0]:\n                self.schedule.add_job({\n                    master_event(type='failback'):\n                    {\n                        'function': 'status.ping_master',\n                        'seconds': self.opts['master_failback_interval'],\n                        'jid_include': True,\n                        'maxrunning': 1,\n                        'return_job': False,\n                        'kwargs': {'master': self.opts['master_list'][0]}\n                    }\n                }, persist=True)\n            else:\n                self.schedule.delete_job(master_event(type='failback'), persist=True)\n        else:\n            self.schedule.delete_job(master_event(type='alive', master=self.opts['master']), persist=True)\n            self.schedule.delete_job(master_event(type='failback'), persist=True)",
        "sampled": "```python\nclass Minion:\n    def __init__(self, master):\n        \"\"\"\n        Initialize the minion with a master.\n\n        :param master: The connected master node.\n        \"\"\"\n        self.master = master\n\n    def _post_master_init(self, master):\n        \"\"\"\n        Function to finish init after connecting to a master\n\n        This is primarily loading modules, pillars, etc. (since they need\n        to know which master they connected to)\n\n        If this function is changed, please"
    },
    {
        "original": "def delete_one(self, filter, collation=None):\n        \"\"\"Delete a single document matching the filter.\n\n          >>> db.test.count({'x': 1})\n          3\n          >>> result = db.test.delete_one({'x': 1})\n          >>> result.deleted_count\n          1\n          >>> db.test.count({'x': 1})\n          2\n\n        :Parameters:\n          - `filter`: A query that matches the document to delete.\n          - `collation` (optional): An instance of\n            :class:`~pymongo.collation.Collation`. This option is only supported\n            on MongoDB 3.4 and above.\n\n        :Returns:\n          - An instance of :class:`~pymongo.results.DeleteResult`.\n\n        .. versionchanged:: 3.4\n          Added the `collation` option.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        with self._socket_for_writes() as sock_info:\n            return DeleteResult(self._delete(sock_info, filter, False,\n                                             collation=collation),\n                                self.write_concern.acknowledged)",
        "sampled": "```python\nclass MongoDB:\n    def __init__(self):\n        self.collection = []\n\n    def insert_one(self, document):\n        self.collection.append(document)\n\n    def count(self, filter):\n        return sum(1 for doc in self.collection if all(key in doc and doc[key] == value for key, value in filter.items()))\n\n    def delete_one(self, filter):\n        deleted_count = 0\n        for i, doc in enumerate(self.collection):\n            if all(key in doc and"
    },
    {
        "original": "async def end_takeout(self, success):\n        \"\"\"\n        Finishes a takeout, with specified result sent back to Telegram.\n\n        Returns:\n            ``True`` if the operation was successful, ``False`` otherwise.\n        \"\"\"\n        try:\n            async with _TakeoutClient(True, self, None) as takeout:\n                takeout.success = success\n        except ValueError:\n            return False\n        return True",
        "sampled": "```python\nclass TakeoutManager:\n    async def end_takeout(self, success):\n        \"\"\"\n        Finishes a takeout, with specified result sent back to Telegram.\n\n        Args:\n            success (bool): Whether the takeout was successful or not.\n\n        Returns:\n            bool: True if the operation was successful, False otherwise.\n        \"\"\"\n        \n        # Check if the success parameter is a boolean value\n        if not isinstance(success, bool):\n            raise ValueError(\"success must be a boolean"
    },
    {
        "original": "def compute_tls13_early_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for 0-RTT data.\n        self.handshake_messages should be ClientHello only.\n        \"\"\"\n        # we use the prcs rather than the pwcs in a totally arbitrary way\n        if self.prcs is None:\n            # too soon\n            return\n\n        hkdf = self.prcs.hkdf\n\n        self.tls13_early_secret = hkdf.extract(None,\n                                               self.tls13_psk_secret)\n\n        bk = hkdf.derive_secret(self.tls13_early_secret,\n                                b\"external psk binder key\",\n                                # \"resumption psk binder key\",\n                                b\"\")\n        self.tls13_derived_secrets[\"binder_key\"] = bk\n\n        if len(self.handshake_messages) > 1:\n            # these secrets are not defined in case of HRR\n            return\n\n        cets = hkdf.derive_secret(self.tls13_early_secret,\n                                  b\"client early traffic secret\",\n                                  b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"client_early_traffic_secret\"] = cets\n\n        ees = hkdf.derive_secret(self.tls13_early_secret,\n                                 b\"early exporter master secret\",\n                                 b\"\".join(self.handshake_messages))\n        self.tls13_derived_secrets[\"early_exporter_secret\"] = ees\n\n        if self.connection_end == \"server\":\n            self.prcs.tls13_derive_keys(cets)\n        elif self.connection_end == \"client\":\n            self.pwcs.tls13_derive_keys(cets)",
        "sampled": "```python\nfrom hashlib import sha256\n\nclass TLS13:\n    def __init__(self, handshake_messages):\n        self.handshake_messages = handshake_messages\n\n    def compute_tls13_early_secrets(self):\n        \"\"\"\n        Ciphers key and IV are updated accordingly for 0-RTT data.\n        self.handshake_messages should be ClientHello only.\n        \"\"\"\n        \n        # Combine handshake messages as a single string\n        handshake_data = ''.join([msg.decode('ascii') for msg"
    },
    {
        "original": "def FromPath(cls, path, follow_symlink = True):\n    \"\"\"Returns stat information about the given OS path, calling os.[l]stat.\n\n    Args:\n      path: A path to perform `stat` on.\n      follow_symlink: True if `stat` of a symlink should be returned instead of\n        a file that it points to. For non-symlinks this setting has no effect.\n\n    Returns:\n      Stat instance, with information about the given path.\n    \"\"\"\n    # Note that we do not add type assertion for `path` here. The reason is that\n    # many of the existing system calls (e.g. `os.listdir`) return results as\n    # bytestrings in Python 2. This is fine because it also means that they also\n    # accept bytestring paths as arguments in Python 2 (e.g. `os.stat`). Having\n    # consistent types in both versions is certainly desired but it might be too\n    # much work for too little benefit.\n    precondition.AssertType(follow_symlink, bool)\n\n    if follow_symlink:\n      stat_obj = os.stat(path)\n    else:\n      stat_obj = os.lstat(path)\n\n    return cls(path=path, stat_obj=stat_obj)",
        "sampled": "```python\nimport os\n\nclass Stat:\n    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid,\n                 st_size, st_atime_ns, st_mtime_ns):\n        self.st_mode = stat.S_IMODE(st_mode)\n        self.st_ino = ino\n        self.st_dev = dev\n        self.st_nlink = nlink\n        self.st_uid = uid\n        self.st_gid = gid\n"
    },
    {
        "original": "def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt",
        "sampled": "```python\nimport matplotlib.pyplot as plt\n\ndef single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \n    Parameters:\n    obj (object): The object to be plotted. It should have 'x' and 'y' attributes.\n    \n    Returns:\n    plot (matplotlib.figure.Figure): The figure of the plot.\n    renderer (matplotlib.backends.backend_agg.FigureCanvasAgg): The renderer of the figure.\n    format (str): The format"
    },
    {
        "original": "def check_status(self, delay=0):\n        \"\"\" Checks the api endpoint in a loop\n\n        :param delay: number of seconds to wait between api calls.\n         Note Connection 'requests_delay' also apply.\n        :return: tuple of status and percentage complete\n        :rtype: tuple(str, float)\n        \"\"\"\n        if not self.item_id:\n            while not self._request_status():\n                # wait until _request_status returns True\n                yield self.status, self.completion_percentage\n                if self.item_id is None:\n                    sleep(delay)\n        else:\n            yield self.status, self.completion_percentage",
        "sampled": "```python\nimport time\nimport requests\n\nclass APIStatusChecker:\n    def __init__(self, api_endpoint):\n        self.api_endpoint = api_endpoint\n        self.status = None\n        self.percentage_complete = None\n\n    def check_status(self, delay=0):\n        \"\"\" Checks the api endpoint in a loop\n\n        :param delay: number of seconds to wait between api calls.\n         Note Connection 'requests_delay' also apply.\n        :return: tuple of status and percentage complete"
    },
    {
        "original": "def get_magnitude_of_effect_from_spin_config(motif, spin_config):\n        \"\"\"\n        Roughly, the magnitude of Jahn-Teller distortion will be:\n        * in octahedral environments, strong if e_g orbitals\n        unevenly occupied but weak if t_2g orbitals unevenly\n        occupied\n        * in tetrahedral environments always weaker\n        :param motif (str): \"oct\" or \"tet\"\n        :param spin_config (dict): dict of 'e' (e_g) and 't' (t2_g)\n        with number of electrons in each state\n        \"\"\"\n        magnitude = \"none\"\n        if motif == \"oct\":\n            e_g = spin_config[\"e_g\"]\n            t_2g = spin_config[\"t_2g\"]\n            if (e_g % 2 != 0) or (t_2g % 3 != 0):\n                magnitude = \"weak\"\n                if e_g % 2 == 1:\n                    magnitude = \"strong\"\n        elif motif == \"tet\":\n            e = spin_config[\"e\"]\n            t_2 = spin_config[\"t_2\"]\n            if (e % 3 != 0) or (t_2 % 2 != 0):\n                magnitude = \"weak\"\n        return magnitude",
        "sampled": "```python\ndef get_magnitude_of_effect_from_spin_config(motif, spin_config):\n    \"\"\"\n    Roughly, the magnitude of Jahn-Teller distortion will be:\n    * in octahedral environments, strong if e_g orbitals \n    unevenly occupied but weak if t_2g orbitals unevenly \n    occupied\n    * in tetrahedral environments always weaker\n    :param motif (str): \"oct\" or \"tet\"\n    :param spin_config ("
    },
    {
        "original": "def decode(self, targets, encoder_outputs, attention_bias):\n    \"\"\"Generate logits for each value in the target sequence.\n\n    Args:\n      targets: target values for the output sequence.\n        int tensor with shape [batch_size, target_length]\n      encoder_outputs: continuous representation of input sequence.\n        float tensor with shape [batch_size, input_length, hidden_size]\n      attention_bias: float tensor with shape [batch_size, 1, 1, input_length]\n\n    Returns:\n      float32 tensor with shape [batch_size, target_length, vocab_size]\n    \"\"\"\n    with tf.name_scope(\"decode\"):\n      # Prepare inputs to decoder layers by shifting targets, adding positional\n      # encoding and applying dropout.\n      decoder_inputs = self.embedding_softmax_layer(targets)\n      with tf.name_scope(\"shift_targets\"):\n        # Shift targets to the right, and remove the last element\n        decoder_inputs = tf.pad(\n            decoder_inputs, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n      with tf.name_scope(\"add_pos_encoding\"):\n        length = tf.shape(decoder_inputs)[1]\n        decoder_inputs += model_utils.get_position_encoding(\n            length, self.params.hidden_size)\n      if self.train:\n        mlperf_log.transformer_print(\n            key=mlperf_log.MODEL_HP_LAYER_POSTPROCESS_DROPOUT,\n            value=self.params.layer_postprocess_dropout)\n        decoder_inputs = tf.nn.dropout(\n            decoder_inputs, 1 - self.params.layer_postprocess_dropout)\n\n      # Run values\n      decoder_self_attention_bias = model_utils.get_decoder_self_attention_bias(\n          length)\n      outputs = self.decoder_stack(\n          decoder_inputs, encoder_outputs, decoder_self_attention_bias,\n          attention_bias)\n      logits = self.embedding_softmax_layer.linear(outputs)\n      return logits",
        "sampled": "```python\nimport tensorflow as tf\n\ndef decode(self, targets, encoder_outputs, attention_bias):\n    \"\"\"\n    Generate logits for each value in the target sequence.\n\n    Args:\n      targets: target values for the output sequence.\n        int tensor with shape [batch_size, target_length]\n      encoder_outputs: continuous representation of input sequence.\n        float tensor with shape [batch_size, input_length, hidden_size]\n      attention_bias: float tensor with shape [batch_size, 1, 1"
    },
    {
        "original": "def center(self) -> Point2:\n        \"\"\" Returns the central point of all units in this list \"\"\"\n        assert self\n        pos = Point2(\n            (\n                sum([unit.position.x for unit in self]) / self.amount,\n                sum([unit.position.y for unit in self]) / self.amount,\n            )\n        )\n        return pos",
        "sampled": "```python\nfrom math import sqrt\n\nclass Point2:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y\n\nclass Unit:\n    def __init__(self, point: Point2):\n        self.point = point\n\nclass UnitList:\n    def __init__(self):\n        self.units = []\n\n    def add_unit(self, unit: Unit):\n        self.units.append(unit)\n\n    @property\n    def center(self"
    },
    {
        "original": "def _validate_key(self, key):\n        \"\"\"\n        Make sure the supplied key values are within the bounds\n        specified by the corresponding dimension range and soft_range.\n        \"\"\"\n        if key == () and len(self.kdims) == 0: return ()\n        key = util.wrap_tuple(key)\n        assert len(key) == len(self.kdims)\n        for ind, val in enumerate(key):\n            kdim = self.kdims[ind]\n            low, high = util.max_range([kdim.range, kdim.soft_range])\n            if util.is_number(low) and util.isfinite(low):\n                if val < low:\n                    raise KeyError(\"Key value %s below lower bound %s\"\n                                   % (val, low))\n            if util.is_number(high) and util.isfinite(high):\n                if val > high:\n                    raise KeyError(\"Key value %s above upper bound %s\"\n                                   % (val, high))",
        "sampled": "```python\nclass Validator:\n    def __init__(self, min_bounds, max_bounds, soft_ranges):\n        \"\"\"\n        Initialize the validator.\n        \n        :param min_bounds: A tuple of minimum bounds for each dimension.\n        :param max_bounds: A tuple of maximum bounds for each dimension.\n        :param soft_ranges: A list of tuples representing the soft ranges for each dimension.\n        \"\"\"\n        \n        self.min_bounds = min_bounds\n        self.max_bounds = max_bounds\n        self.soft"
    },
    {
        "original": "def main(unusedargv):\n    \"\"\"All of the magic together.\"\"\"\n    del unusedargv\n\n    bt_table = (bigtable\n                .Client(FLAGS.cbt_project, admin=True)\n                .instance(FLAGS.cbt_instance)\n                .table(FLAGS.cbt_table))\n    assert bt_table.exists(), \"Table doesn't exist\"\n\n    # Get current game counter, updates are conditioned on this matching.\n    last_game = latest_game_number(bt_table)\n    print(\"eval_game_counter:\", last_game)\n    print()\n\n    # Get existing SGF paths so we avoid uploading duplicates\n    existing_paths = read_existing_paths(bt_table)\n    print(\"Found {} existing\".format(len(existing_paths)))\n    if existing_paths:\n        duplicates = Counter(existing_paths)\n        existing_paths = set(existing_paths)\n\n        for k, v in duplicates.most_common():\n            if v == 1:\n                break\n            print(\"{}x{}\".format(v, k))\n\n        print(\"\\tmin:\", min(existing_paths))\n        print(\"\\tmax:\", max(existing_paths))\n        print()\n\n    # Get all SGFs that match glob, skipping SGFs with existing records.\n    data = read_games(FLAGS.sgf_glob, existing_paths)\n    if data:\n        write_eval_records(bt_table, data, last_game)",
        "sampled": "```python\nimport math\n\ndef main(unusedargv):\n    \"\"\"\n    All of the magic together.\n    \n    This function takes no arguments and returns the sum of all numbers from 1 to n, \n    where n is the number of test cases. The function also prints out each result for each test case.\n    \n    Parameters:\n    unusedargv (list): A list containing command line arguments. This argument is not used in this problem.\n    \n    Returns:\n    int: The sum of"
    },
    {
        "original": "def qdii(self, min_volume=0):\n        \"\"\"\u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume:\u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        \"\"\"\n        # \u6dfb\u52a0\u5f53\u524d\u7684ctime\n        self.__qdii_url = self.__qdii_url.format(ctime=int(time.time()))\n        # \u8bf7\u6c42\u6570\u636e\n        rep = requests.get(self.__qdii_url)\n        # \u83b7\u53d6\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        fundjson = json.loads(rep.text)\n        # \u683c\u5f0f\u5316\u8fd4\u56de\u7684json\u5b57\u7b26\u4e32\n        data = self.formatjisilujson(fundjson)\n        data = {x: y for x, y in data.items() if y[\"notes\"] != \"\u4f30\u503c\u6709\u95ee\u9898\"}\n        # \u8fc7\u6ee4\u5c0f\u4e8e\u6307\u5b9a\u4ea4\u6613\u91cf\u7684\u6570\u636e\n        if min_volume:\n            data = {\n                k: data[k]\n                for k in data\n                if float(data[k][\"volume\"]) > min_volume\n            }\n\n        self.__qdii = data\n        return self.__qdii",
        "sampled": "```python\nclass Solution:\n    def qdii(self, min_volume=0):\n        \"\"\"\n        \u4ee5\u5b57\u5178\u5f62\u5f0f\u8fd4\u56deQDII\u6570\u636e\n        :param min_volume: \u6700\u5c0f\u4ea4\u6613\u91cf\uff0c\u5355\u4f4d\u4e07\u5143\n        :return:  QDII\u6570\u636e\u5b57\u5178 astronomers: \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u4e2d\u6bcf\u4e2a\u952e\u662f.abspath(key)\uff0c\u7528\u4ee5\u5bf9\u5e94\u5f84\u5185item.Total_Money splitter_share\u3002\n        \"\"\"\n        \n        # designed dictionary with default values to store qdii data \n"
    },
    {
        "original": "def full_name(self):\n        \"\"\"Return full package/distribution name, w/version\"\"\"\n        if self.requested_version is not None:\n            return '%s-%s' % (self.name, self.requested_version)\n        return self.name",
        "sampled": "```python\nclass Package:\n    def __init__(self, name, version):\n        \"\"\"\n        Initialize a package object.\n\n        Args:\n            name (str): The full package/distribution name.\n            version (str): The version of the package.\n        \"\"\"\n        self.name = name\n        self.version = version\n\n    def full_name(self):\n        \"\"\"\n        Return full package/distribution name, w/version.\n\n        Returns:\n            str: The full package/distribution name with version.\n       "
    },
    {
        "original": "def check_number_status(self, number_id):\n        \"\"\"\n        Check if a number is valid/registered in the whatsapp service\n\n        :param number_id: number id\n        :return:\n        \"\"\"\n        number_status = self.wapi_functions.checkNumberStatus(number_id)\n        return NumberStatus(number_status, self)",
        "sampled": "```python\nclass WhatsappService:\n    def __init__(self):\n        # Initialize an empty set to store registered numbers\n        self.registered_numbers = set()\n\n    def register_number(self, number_id):\n        \"\"\"\n        Register a number in the whatsapp service\n\n        :param number_id: number id\n        :return:\n        \"\"\"\n        self.registered_numbers.add(number_id)\n\n    def check_number_status(self, number_id):\n        \"\"\"\n        Check if a number is valid/registered in the"
    },
    {
        "original": "def _parse_canonical_regex(doc):\n    \"\"\"Decode a JSON regex to bson.regex.Regex.\"\"\"\n    regex = doc['$regularExpression']\n    if len(doc) != 1:\n        raise TypeError('Bad $regularExpression, extra field(s): %s' % (doc,))\n    if len(regex) != 2:\n        raise TypeError('Bad $regularExpression must include only \"pattern\"'\n                        'and \"options\" components: %s' % (doc,))\n    return Regex(regex['pattern'], regex['options'])",
        "sampled": "```python\nimport json\nimport bson.regex\n\ndef _parse_canonical_regex(doc):\n    \"\"\"\n    Decode a JSON regex to bson.regex.Regex.\n    \n    Parameters:\n    doc (dict): A dictionary representing the JSON regex.\n    \n    Returns:\n    bson.regex.Regex: The decoded regex.\n    \"\"\"\n    \n    # Map of valid actions in the JSON schema\n    actions = {\n        'EQUALITY': lambda p, v: { '$eq': [p, v] },\n       "
    },
    {
        "original": "def from_csv(cls, filename: str):\n        \"\"\"\n        Imports PDEntries from a csv.\n\n        Args:\n            filename: Filename to import from.\n\n        Returns:\n            List of Elements, List of PDEntries\n        \"\"\"\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f, delimiter=unicode2str(\",\"),\n                                quotechar=unicode2str(\"\\\"\"),\n                                quoting=csv.QUOTE_MINIMAL)\n            entries = list()\n            header_read = False\n            elements = None\n            for row in reader:\n                if not header_read:\n                    elements = row[1:(len(row) - 1)]\n                    header_read = True\n                else:\n                    name = row[0]\n                    energy = float(row[-1])\n                    comp = dict()\n                    for ind in range(1, len(row) - 1):\n                        if float(row[ind]) > 0:\n                            comp[Element(elements[ind - 1])] = float(row[ind])\n                    entries.append(PDEntry(Composition(comp), energy, name))\n        return cls(entries)",
        "sampled": "```python\nimport csv\nfrom dataclasses import dataclass\n\n@dataclass\nclass PDEntry:\n    \"\"\"Represents a Partial Differential Equation entry.\"\"\"\n    id: int\n    equation: str\n\n@dataclass\nclass Element:\n    \"\"\"Represents an element.\"\"\"\n    id: int\n    name: str\n\ndef from_csv(cls, filename: str):\n    \"\"\"\n    Imports PDEntries from a csv.\n\n    Args:\n        filename: Filename to import from.\n\n   "
    },
    {
        "original": "def _enqueue_init_updates(self):\n        \"\"\"Enqueues current routes to be shared with this peer.\"\"\"\n        assert self.state.bgp_state == const.BGP_FSM_ESTABLISHED\n        if self.is_mbgp_cap_valid(RF_RTC_UC):\n            # Enqueues all best-RTC_NLRIs to be sent as initial update to this\n            # peer.\n            self._peer_manager.comm_all_rt_nlris(self)\n            self._schedule_sending_init_updates()\n        else:\n            # Enqueues all best-path to be sent as initial update to this peer\n            # expect for RTC route-family.\n            tm = self._core_service.table_manager\n            self.comm_all_best_paths(tm.global_tables)",
        "sampled": "```python\nfrom collections import deque\n\nclass Peer:\n    def __init__(self):\n        self.routes = set()\n        self.queue = deque()\n\n    def _enqueue_init_updates(self):\n        \"\"\"Enqueues current routes to be shared with this peer.\"\"\"\n        \n        # Add all current routes to the queue\n        for route in self.routes:\n            self.queue.append(route)\n        \n        # Remove all routes from the set (to avoid duplicates)\n        self.routes.clear()\n\n    def add_route(self,"
    },
    {
        "original": "def create_security_group_rule(security_group,\n                               remote_group_id=None,\n                               direction='ingress',\n                               protocol=None,\n                               port_range_min=None,\n                               port_range_max=None,\n                               ethertype='IPv4',\n                               profile=None):\n    \"\"\"\n    Creates a new security group rule\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutron.show_security_group_rule security-group-rule-id\n\n    :param security_group: Security group name or ID to add rule\n    :param remote_group_id: Remote security group name or ID to\n            apply rule (Optional)\n    :param direction: Direction of traffic: ingress/egress,\n            default: ingress (Optional)\n    :param protocol: Protocol of packet: null/icmp/tcp/udp,\n            default: null (Optional)\n    :param port_range_min: Starting port range (Optional)\n    :param port_range_max: Ending port range (Optional)\n    :param ethertype: IPv4/IPv6, default: IPv4 (Optional)\n    :param profile: Profile to build on (Optional)\n    :return: Created security group rule information\n    \"\"\"\n    conn = _auth(profile)\n    return conn.create_security_group_rule(security_group,\n                                           remote_group_id,\n                                           direction,\n                                           protocol,\n                                           port_range_min,\n                                           port_range_max,\n                                           ethertype)",
        "sampled": "```python\nclass NeutronException(Exception):\n    pass\n\n\nclass SecurityGroupRule:\n    def __init__(self, security_group_rule_id, security_group_id, remote_group_id,\n                 direction='ingress', protocol=None, port_range_min=None,\n                 port_range_max=None, ethertype='IPv4'):\n        \"\"\"\n        Initializes a new security group rule\n\n        :param security_group_rule_id: ID of created rule\n        :param security_group_id: SaID of related Security Group that"
    },
    {
        "original": "def slice_graph(graph, node, frontier, include_frontier=False):\n        \"\"\"\n        Generate a slice of the graph from the head node to the given frontier.\n\n        :param networkx.DiGraph graph: The graph to work on.\n        :param node: The starting node in the graph.\n        :param frontier: A list of frontier nodes.\n        :param bool include_frontier: Whether the frontier nodes are included in the slice or not.\n        :return: A subgraph.\n        :rtype: networkx.DiGraph\n        \"\"\"\n\n        subgraph = networkx.DiGraph()\n\n        for frontier_node in frontier:\n            for simple_path in networkx.all_simple_paths(graph, node, frontier_node):\n                for src, dst in zip(simple_path, simple_path[1:]):\n                    if include_frontier or (src not in frontier and dst not in frontier):\n                        subgraph.add_edge(src, dst)\n        if not list(subgraph.nodes):\n            # HACK: FIXME: for infinite loop nodes, this would return an empty set, so we include the loop body itself\n            # Make sure this makes sense (EDG thinks it does)\n            if (node, node) in graph.edges:\n                subgraph.add_edge(node, node)\n        return subgraph",
        "sampled": "```python\nimport networkx as nx\n\ndef slice_graph(graph, node, frontier, include_frontier=False):\n    \"\"\"\n    Generate a slice of the graph from the head node to the given frontier.\n\n    :param networkx.DiGraph graph: The graph to work on.\n    :param node: The starting node in the graph.\n    :param frontier: A list of frontier nodes.\n    :param bool include_frontier: Whether the frontier nodes are included in the slice or not.\n   "
    },
    {
        "original": "def set_controller(self, controllers):\n        \"\"\"\n        Sets the OpenFlow controller address.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl set-controller <bridge> <target>...\n        \"\"\"\n        command = ovs_vsctl.VSCtlCommand('set-controller', [self.br_name])\n        command.args.extend(controllers)\n        self.run_command([command])",
        "sampled": "```python\nclass Controller:\n    def __init__(self):\n        self.controllers = []\n\n    def set_controller(self, controllers):\n        \"\"\"\n        Sets the OpenFlow controller address.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl set-controller <bridge> <target>...\n        \n        Args:\n            controllers (list): A list of OpenFlow controller addresses.\n        \n        Returns:\n            None\n        \"\"\"\n        self.controllers.clear()\n        for controller in"
    },
    {
        "original": "def get_sample_stats(fit, log_likelihood=None):\r\n    \"\"\"Extract sample stats from PyStan fit.\"\"\"\r\n    dtypes = {\"divergent__\": bool, \"n_leapfrog__\": np.int64, \"treedepth__\": np.int64}\r\n\r\n    ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\r\n\r\n    extraction = OrderedDict()\r\n    for chain, (pyholder, ndraws) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\r\n        if chain == 0:\r\n            for key in pyholder[\"sampler_param_names\"]:\r\n                extraction[key] = []\r\n        for key, values in zip(pyholder[\"sampler_param_names\"], pyholder[\"sampler_params\"]):\r\n            extraction[key].append(values[-ndraws:])\r\n\r\n    data = OrderedDict()\r\n    for key, values in extraction.items():\r\n        values = np.stack(values, axis=0)\r\n        dtype = dtypes.get(key)\r\n        values = values.astype(dtype)\r\n        name = re.sub(\"__$\", \"\", key)\r\n        name = \"diverging\" if name == \"divergent\" else name\r\n        data[name] = values\r\n\r\n    # log_likelihood\r\n    if log_likelihood is not None:\r\n        log_likelihood_data = get_draws(fit, variables=log_likelihood)\r\n        data[\"log_likelihood\"] = log_likelihood_data[log_likelihood]\r\n\r\n    # lp__\r\n    stat_lp = get_draws(fit, variables=\"lp__\")\r\n    data[\"lp\"] = stat_lp[\"lp__\"]\r\n\r\n    return data",
        "sampled": "```python\nimport numpy as np\nimport math\n\ndef get_sample_stats(fit, log_likelihood=None):\n    \"\"\"\n    Extract sample stats from PyStan fit.\n    \n    Parameters:\n    fit (dict): The output of a PyStan model, containing the sample statistics.\n    log_likelihood (numpy array): Log likelihood values. Default is None.\n    \n    Returns:\n    dict: A dictionary containing various sample statistics.\n          These include mean, standard deviation, and fraction of samples within certain number of"
    },
    {
        "original": "def inspect_members(self):\n        \"\"\"\n        Returns the list of all troposphere members we are able to\n        construct\n        \"\"\"\n        if not self._inspect_members:\n            TemplateGenerator._inspect_members = \\\n                self._import_all_troposphere_modules()\n        return self._inspect_members",
        "sampled": "```python\nclass Troposphere:\n    def __init__(self):\n        self.members = set()\n\n    def add_member(self, member):\n        self.members.add(member)\n\n    def get_all_members(self):\n        return sorted(list(self.members))\n\n    def inspect_members(self):\n        \"\"\"\n        Returns the list of all troposphere members we are able to construct\n        \"\"\"\n        \n_reqs = {\n    # Example restraints where \"a\" needs to be constructed before \"b\", \n    # and"
    },
    {
        "original": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        # autoscaleconnection.get_all_groups()\n        limits['Auto Scaling groups'] = AwsLimit(\n            'Auto Scaling groups',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n        # autoscaleconnection.get_all_launch_configurations()\n        limits['Launch configurations'] = AwsLimit(\n            'Launch configurations',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::LaunchConfiguration',\n        )\n        self.limits = limits\n        return limits",
        "sampled": "```python\nclass AwsLimit:\n    def __init__(self, name, limit):\n        \"\"\"\n        Initialize an AwsLimit object.\n\n        :param str name: the name of the limit\n        :param float limit: the value of the limit\n        \"\"\"\n        self.name = name\n        self.limit = limit\n\ndef get_limits(self):\n    \"\"\"\n    Return all known limits for this service, as a dict of their names to AwsLimit objects.\n\n    :returns: dict of limit names"
    },
    {
        "original": "def summary(svc_name=''):\n    \"\"\"\n    Display a summary from monit\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' monit.summary\n        salt '*' monit.summary <service name>\n    \"\"\"\n    ret = {}\n    cmd = 'monit summary'\n    res = __salt__['cmd.run'](cmd).splitlines()\n    for line in res:\n        if 'daemon is not running' in line:\n            return dict(monit='daemon is not running', result=False)\n        elif not line or svc_name not in line or 'The Monit daemon' in line:\n            continue\n        else:\n            parts = line.split('\\'')\n            if len(parts) == 3:\n                resource, name, status_ = (\n                    parts[0].strip(), parts[1], parts[2].strip()\n                )\n                if svc_name != '' and svc_name != name:\n                    continue\n                if resource not in ret:\n                    ret[resource] = {}\n                ret[resource][name] = status_\n    return ret",
        "sampled": "```python\nimport subprocess\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef summary(svc_name=''):\n    \"\"\"\n    Display a summary from monit\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' monit.summary\n        salt '*' monit.summary <service name>\n    \"\"\"\n\n    if svc_name == '':\n        # Return a list of all services that can be summarized by monit in human-readable format.\n        cmd = 'monit summary'\n        result = subprocess"
    },
    {
        "original": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a DialogSuggestionValue object from a json dictionary.\"\"\"\n        args = {}\n        if 'input' in _dict:\n            args['input'] = MessageInput._from_dict(_dict.get('input'))\n        if 'intents' in _dict:\n            args['intents'] = [\n                RuntimeIntent._from_dict(x) for x in (_dict.get('intents'))\n            ]\n        if 'entities' in _dict:\n            args['entities'] = [\n                RuntimeEntity._from_dict(x) for x in (_dict.get('entities'))\n            ]\n        return cls(**args)",
        "sampled": "```python\nclass DialogSuggestionValue:\n    def __init__(self, text, confidence):\n        \"\"\"\n        Initialize a DialogSuggestionValue object.\n\n        Args:\n            text (str): The text of the suggestion.\n            confidence (float): The confidence level of the suggestion.\n        \"\"\"\n        self.text = text\n        self.confidence = confidence\n\n    @classmethod\n    def _from_dict(cls, _dict):\n        \"\"\"\n        Initialize a DialogSuggestionValue object from a json dictionary"
    },
    {
        "original": "def unlock(self):\n        \"\"\"Lock thread.\n\n        Requires that the currently authenticated user has the modposts oauth\n        scope or has user/password authentication as a mod of the subreddit.\n\n        :returns: The json response from the server.\n\n        \"\"\"\n        url = self.reddit_session.config['unlock']\n        data = {'id': self.fullname}\n        return self.reddit_session.request_json(url, data=data)",
        "sampled": "```python\nimport requests\nimport json\n\nclass RedditAPI:\n    def __init__(self, username, password, subreddit):\n        self.username = username\n        self.password = password\n        self.subreddit = subreddit\n\n    def unlock(self):\n        \"\"\"\n        Lock thread.\n\n        Requires that the currently authenticated user has the modposts oauth scope or has user/password authentication as a mod of the subreddit.\n\n        :returns: The json response from the server.\n        \"\"\"\n        \n        # Set API endpoint"
    },
    {
        "original": "def receive_message(self, operation, request_id):\n        \"\"\"Receive a raw BSON message or raise ConnectionFailure.\n\n        If any exception is raised, the socket is closed.\n        \"\"\"\n        try:\n            return receive_message(\n                self.sock, operation, request_id, self.max_message_size)\n        except BaseException as error:\n            self._raise_connection_failure(error)",
        "sampled": "```python\nimport socket\n\nclass Connections:\n    def __init__(self, host='127.0.0.1', port=27017):\n        self.host = host\n        self.port = port\n        self.socket = None\n\n    def establish_connection(self):\n        \"\"\"Establish a connection to the server.\n\n        Returns:\n            socket.socket: The established TCP socket.\n            str: An error message if the connection fails.\n         \"\"\"\n        try:\n            self.socket = socket.create_connection(("
    },
    {
        "original": "def scrape_metrics(self, endpoint):\n        \"\"\"\n        Poll the data from prometheus and return the metrics as a generator.\n        \"\"\"\n        response = self.poll(endpoint)\n        try:\n            # no dry run if no label joins\n            if not self.label_joins:\n                self._dry_run = False\n            elif not self._watched_labels:\n                # build the _watched_labels set\n                for val in itervalues(self.label_joins):\n                    self._watched_labels.add(val['label_to_match'])\n\n            for metric in self.parse_metric_family(response):\n                yield metric\n\n            # Set dry run off\n            self._dry_run = False\n            # Garbage collect unused mapping and reset active labels\n            for metric, mapping in list(iteritems(self._label_mapping)):\n                for key in list(mapping):\n                    if key not in self._active_label_mapping[metric]:\n                        del self._label_mapping[metric][key]\n            self._active_label_mapping = {}\n        finally:\n            response.close()",
        "sampled": "```python\nimport requests\nfrom urllib.parse import urljoin\nimport xmltodict\n\nclass PrometheusScraper:\n    def __init__(self, base_url, prometheus_endpoint):\n        self.base_url = base_url\n        self.prometheus_endpoint = prometheus_endpoint\n\n    def scrape_metrics(self):\n        \"\"\"\n        Make a GET request to the prometheus endpoint and return the metrics as a generator.\n        \n        The scrape will handle polls to '/metrics' endpoints only.\n        \n        :yields"
    },
    {
        "original": "def _process_hist(self, hist):\n        \"\"\"\n        Subclassed to offset histogram by defined amount.\n        \"\"\"\n        edges, hvals, widths, lims, isdatetime = super(SideHistogramPlot, self)._process_hist(hist)\n        offset = self.offset * lims[3]\n        hvals *= 1-self.offset\n        hvals += offset\n        lims = lims[0:3] + (lims[3] + offset,)\n        return edges, hvals, widths, lims, isdatetime",
        "sampled": "```\nclass BaseClass:\n    def __init__(self, offset):\n        self.offset = offset\n\n    def _process_hist(self, hist):\n        \"\"\"\n        Process the histogram by adding an offset value to each data point.\n        \n        Parameters:\n        hist (list): The input histogram data.\n        \n        Returns:\n        list: The processed histogram with added values.\n        \"\"\"\n        \n### Introspecting the task for problem specification in details  ###\n\nThe method `_process_hist` is a subclass"
    },
    {
        "original": "def format_variable_map(variable_map, join_lines=True):\n  \"\"\"Takes a key-to-variable map and formats it as a table.\"\"\"\n  rows = []\n  rows.append((\"Key\", \"Variable\", \"Shape\", \"Type\", \"Collections\", \"Device\"))\n  var_to_collections = _get_vars_to_collections(variable_map)\n\n  sort_key = lambda item: (item[0], item[1].name)\n  for key, var in sorted(variable_map_items(variable_map), key=sort_key):\n    shape = \"x\".join(str(dim) for dim in var.get_shape().as_list())\n    dtype = repr(var.dtype.base_dtype).replace(\"tf.\", \"\")\n    coll = \", \".join(sorted(var_to_collections[var]))\n    rows.append((key, var.op.name, shape, dtype, coll, _format_device(var)))\n  return _format_table(rows, join_lines)",
        "sampled": "```python\nimport math\n\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(math.sqrt(n)) + 1):\n        if n % i == 0:\n            return False\n    return True\n\ndef find_primes(var_1, var_2):\n    primes = []\n    for num in range(var_1, var_2 + 1):\n        if is_prime(num):\n            primes.append(num)\n   "
    },
    {
        "original": "def close(self, status=STATUS_NORMAL, reason=six.b(\"\"), timeout=3):\n        \"\"\"\n        Close Websocket object\n\n        status: status code to send. see STATUS_XXX.\n\n        reason: the reason to close. This must be string.\n\n        timeout: timeout until receive a close frame.\n            If None, it will wait forever until receive a close frame.\n        \"\"\"\n        if self.connected:\n            if status < 0 or status >= ABNF.LENGTH_16:\n                raise ValueError(\"code is invalid range\")\n\n            try:\n                self.connected = False\n                self.send(struct.pack('!H', status) +\n                          reason, ABNF.OPCODE_CLOSE)\n                sock_timeout = self.sock.gettimeout()\n                self.sock.settimeout(timeout)\n                start_time = time.time()\n                while timeout is None or time.time() - start_time < timeout:\n                    try:\n                        frame = self.recv_frame()\n                        if frame.opcode != ABNF.OPCODE_CLOSE:\n                            continue\n                        if isEnabledForError():\n                            recv_status = struct.unpack(\"!H\", frame.data[0:2])[0]\n                            if recv_status != STATUS_NORMAL:\n                                error(\"close status: \" + repr(recv_status))\n                        break\n                    except:\n                        break\n                self.sock.settimeout(sock_timeout)\n                self.sock.shutdown(socket.SHUT_RDWR)\n            except:\n                pass\n\n            self.shutdown()",
        "sampled": "```python\nimport time\nfrom enum import IntEnum\n\n# Define status codes for closing a WebSockets connection.\nclass STATUS(IntEnum):\n    NORMAL = 1000\n    GOING_AWAY = 1001\n    PROTOCOL_ERROR = 1002\n    UNEXPECTED_CONDITION = 1003\n    NORMAL_CLOSING = 1005\n\n\nclass WebSockets:\n    def __init__(self):\n        # Initialize the WebSocket object with default values.\n        self.closing"
    },
    {
        "original": "async def parse_get_revoc_reg_delta_response(get_revoc_reg_delta_response: str) -> (str, str, int):\n    \"\"\"\n    Parse a GET_REVOC_REG_DELTA response to get Revocation Registry Delta in the format compatible with Anoncreds API.\n\n    :param get_revoc_reg_delta_response: response of GET_REVOC_REG_DELTA request.\n    :return: Revocation Registry Definition Id, Revocation Registry Delta json and Timestamp.\n      {\n          \"value\": Registry-specific data {\n              prevAccum: string - previous accumulator value.\n              accum: string - current accumulator value.\n              issued: array<number> - an array of issued indices.\n              revoked: array<number> an array of revoked indices.\n          },\n          \"ver\": string\n      }\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.debug(\"parse_get_revoc_reg_delta_response: >>> get_revoc_reg_delta_response: %r\",\n                 get_revoc_reg_delta_response)\n\n    if not hasattr(parse_get_revoc_reg_delta_response, \"cb\"):\n        logger.debug(\"parse_get_revoc_reg_delta_response: Creating callback\")\n        parse_get_revoc_reg_delta_response.cb = create_cb(\n            CFUNCTYPE(None, c_int32, c_int32, c_char_p, c_char_p, c_uint64))\n\n    c_get_revoc_reg_delta_response = c_char_p(get_revoc_reg_delta_response.encode('utf-8'))\n\n    (revoc_reg_def_id, revoc_reg_delta_json, timestamp) = await do_call('indy_parse_get_revoc_reg_delta_response',\n                                                                        c_get_revoc_reg_delta_response,\n                                                                        parse_get_revoc_reg_delta_response.cb)\n\n    res = (revoc_reg_def_id.decode(), revoc_reg_delta_json.decode(), timestamp)\n    logger.debug(\"parse_get_revoc_reg_delta_response: <<< res: %r\", res)\n    return res",
        "sampled": "```python\nimport json\n\nasync def parse_get_revoc_reg_delta_response(get_revoc_reg_delta_response: str) -> (str, str, int):\n    \"\"\"\n    Parse a GET_REVOC_REG_DELTA response to get Revocation Registry Delta in the format compatible with Anoncreds API.\n\n    :param get_revoc_reg_delta_response: response of GET_REVOC_REG_DELTA request.\n    :return: Revocation Registry Definition Id, Revocation Registry Delta json and Timestamp.\n    \"\"\"\n    \n   "
    },
    {
        "original": "def energy_at_conditions(self, pH, V):\n        \"\"\"\n        Get free energy for a given pH and V\n\n        Args:\n            pH (float): pH at which to evaluate free energy\n            V (float): voltage at which to evaluate free energy\n\n        Returns:\n            free energy at conditions\n        \"\"\"\n        return self.energy + self.npH * PREFAC * pH + self.nPhi * V",
        "sampled": "```python\ndef energy_at_conditions(self, pH, V):\n    \"\"\"\n    Get free energy for a given pH and V\n\n    Args:\n        pH (float): pH at which to evaluate free energy\n        V (float): voltage at which to evaluate free energy\n\n    Returns:\n        free energy at conditions\n    \"\"\" \n    # Assuming the equation of the model is as follows:\n    # delta(\u0394G) / \u0394pH = -nF * 0.05916"
    },
    {
        "original": "def get_pv_args(name, session=None, call=None):\n    \"\"\"\n    Get PV arguments for a VM\n\n    .. code-block:: bash\n\n        salt-cloud -a get_pv_args xenvm01\n\n    \"\"\"\n    if call == 'function':\n        raise SaltCloudException(\n            'This function must be called with -a or --action.'\n        )\n    if session is None:\n        log.debug('New session being created')\n        session = _get_session()\n    vm = _get_vm(name, session=session)\n    pv_args = session.xenapi.VM.get_PV_args(vm)\n    if pv_args:\n        return pv_args\n    return None",
        "sampled": "```python\nimport argparse\n\ndef get_pv_args(name, session=None, call=None):\n    \"\"\"\n    Get PV arguments for a VM\n\n    .. code-block:: bash\n\n        salt-cloud -a get_pv_args xenvm01\n\n    \"\"\"\n    \n    # Define the parser\n    parser = argparse.ArgumentParser(description='Get PV arguments for a VM')\n    \n    # Add the required argument\n    parser.add_argument('name', help='Name of the VM')\n    \n    # Parse the arguments\n"
    },
    {
        "original": "def suggest_pairs(top_n=10, per_n=3, ignore_before=300):\n    \"\"\" Find the maximally interesting pairs of players to match up\n    First, sort the ratings by uncertainty.\n    Then, take the ten highest players with the highest uncertainty\n    For each of them, call them `p1`\n    Sort all the models by their distance from p1's rating and take the 20\n    nearest rated models. ('candidate_p2s')\n    Choose pairings, (p1, p2), randomly from this list.\n\n    `top_n` will pair the top n models by uncertainty.\n    `per_n` will give each of the top_n models this many opponents\n    `ignore_before` is the model number to `filter` off, i.e., the early models.\n    Returns a list of *model numbers*, not model ids.\n    \"\"\"\n    db = sqlite3.connect(\"ratings.db\")\n    data = db.execute(\"select model_winner, model_loser from wins\").fetchall()\n    bucket_ids = [id[0] for id in db.execute(\n        \"select id from models where bucket = ?\", (fsdb.models_dir(),)).fetchall()]\n    bucket_ids.sort()\n    data = [d for d in data if d[0] in bucket_ids and d[1] in bucket_ids]\n\n    ratings = [(model_num_for(k), v[0], v[1]) for k, v in compute_ratings(data).items()]\n    ratings.sort()\n    ratings = ratings[ignore_before:]  # Filter off the first 100 models, which improve too fast.\n\n    ratings.sort(key=lambda r: r[2], reverse=True)\n\n    res = []\n    for p1 in ratings[:top_n]:\n        candidate_p2s = sorted(ratings, key=lambda p2_tup: abs(p1[1] - p2_tup[1]))[1:20]\n        choices = random.sample(candidate_p2s, per_n)\n        print(\"Pairing {}, sigma {:.2f} (Rating {:.2f})\".format(p1[0], p1[2], p1[1]))\n        for p2 in choices:\n            res.append([p1[0], p2[0]])\n            print(\"   {}, ratings delta {:.2f}\".format(p2[0], abs(p1[1] - p2[1])))\n    return res",
        "sampled": "```python\nimport numpy as np\nimport random\n\ndef suggest_pairs(top_n=10, per_n=3, ignore_before=300):\n    \"\"\"\n    Find the maximally interesting pairs of players to match up\n\n    Args:\n        top_n (int, optional): The number of highest players with the highest uncertainty to pair. Defaults to 10.\n        per_n (int, optional): The number of opponents for each of the top n models. Defaults to 3.\n        ignore_before"
    },
    {
        "original": "def from_dict(cls, d):\n        \"\"\"\n        Reconstitute a DefectEntry object from a dict representation created using\n        as_dict().\n         Args:\n            d (dict): dict representation of DefectEntry.\n         Returns:\n            DefectEntry object\n        \"\"\"\n        defect = MontyDecoder().process_decoded( d[\"defect\"])\n        uncorrected_energy = d[\"uncorrected_energy\"]\n        corrections = d.get(\"corrections\", None)\n        parameters = d.get(\"parameters\", None)\n        entry_id = d.get(\"entry_id\", None)\n\n        return cls(defect, uncorrected_energy, corrections=corrections,\n                   parameters=parameters, entry_id=entry_id)",
        "sampled": "```python\nclass DefectEntry:\n    def __init__(self, defect_id, severity, description):\n        \"\"\"\n        Initialize a DefectEntry object.\n        \n        Args:\n            defect_id (int): Unique ID for the defect\n            severity (str): Severity of the defect\n            description (str): Description of the defect\n        \"\"\"\n        self.defect_id = defect_id\n        self.severity = severity\n        self.description = description\n\n    def as_dict(self):\n        \"\"\"\n"
    },
    {
        "original": "def _get_y_scores(self, X):\n        \"\"\"\n        The ``roc_curve`` metric requires target scores that can either be the\n        probability estimates of the positive class, confidence values or non-\n        thresholded measure of decisions (as returned by \"decision_function\").\n\n        This method computes the scores by resolving the estimator methods\n        that retreive these values.\n\n        .. todo:: implement confidence values metric.\n\n        Parameters\n        ----------\n        X : ndarray or DataFrame of shape n x m\n            A matrix of n instances with m features -- generally the test data\n            that is associated with y_true values.\n        \"\"\"\n        # The resolution order of scoring functions\n        attrs = (\n            'predict_proba',\n            'decision_function',\n        )\n\n        # Return the first resolved function\n        for attr in attrs:\n            try:\n                method = getattr(self.estimator, attr, None)\n                if method:\n                    return method(X)\n            except AttributeError:\n                # Some Scikit-Learn estimators have both probability and\n                # decision functions but override __getattr__ and raise an\n                # AttributeError on access.\n                # Note that because of the ordering of our attrs above,\n                # estimators with both will *only* ever use probability.\n                continue\n\n        # If we've gotten this far, raise an error\n        raise ModelError(\n            \"ROCAUC requires estimators with predict_proba or \"\n            \"decision_function methods.\"\n        )",
        "sampled": "```python\nimport numpy as np\n\ndef _get_y_scores(self, X):\n    \"\"\"\n    The ``roc_curve`` metric requires target scores that can either be the\n    probability estimates of the positive class, confidence values or non-\n    thresholded measure of decisions (as returned by \"decision_function\").\n\n    This method computes the scores by resolving the estimator methods\n    that retreive these values.\n\n    Parameters\n    ----------\n        X : ndarray or DataFrame of shape n x m\n            "
    },
    {
        "original": "def chunk_from_mem(self, ptr):\n        \"\"\"\n        Given a pointer to a user payload, return the base of the chunk associated with that payload (i.e. the chunk\n        pointer). Returns None if ptr is null.\n\n        :param ptr: a pointer to the base of a user payload in the heap\n        :returns: a pointer to the base of the associated heap chunk, or None if ptr is null\n        \"\"\"\n        if self.state.solver.symbolic(ptr):\n            try:\n                ptr = self.state.solver.eval_one(ptr)\n            except SimSolverError:\n                l.warning(\"A pointer to a chunk is symbolic; maximizing it\")\n                ptr = self.state.solver.max_int(ptr)\n        else:\n            ptr = self.state.solver.eval(ptr)\n        return PTChunk(ptr - (2 * self._chunk_size_t_size), self.state) if ptr != 0 else None",
        "sampled": "```python\nclass MemoryAllocator:\n    def __init__(self):\n        self.heap = []  # Initialize the heap as a list of chunks\n\n    def malloc(self, size):\n        # Simulate malloc by adding a new chunk to the heap\n        self.heap.append({\"base\": None, \"size\": size, \"ptr\": len(self.heap) - 1})\n        return len(selfheap) - 1\n\n    def update_chunk_ptr(self, ptr):\n        # Update the chunk"
    },
    {
        "original": "def decode_dict_keys_to_str(src):\n    \"\"\"\n    Convert top level keys from bytes to strings if possible.\n    This is necessary because Python 3 makes a distinction\n    between these types.\n    \"\"\"\n    if not six.PY3 or not isinstance(src, dict):\n        return src\n\n    output = {}\n    for key, val in six.iteritems(src):\n        if isinstance(key, bytes):\n            try:\n                key = key.decode()\n            except UnicodeError:\n                pass\n        output[key] = val\n    return output",
        "sampled": "```python\ndef decode_dict_keys_to_str(src):\n    \"\"\"\n    Convert top level keys from bytes to strings if possible.\n    This is necessary because Python 3 makes a distinction\n    between these types.\n    \"\"\"\n    \n    # Check if the input is a dictionary\n    if isinstance(src, dict):\n        # Initialize an empty dictionary to store the decoded keys and values\n        decoded_dict = {}\n        \n        # Iterate over each key-value pair in the input dictionary\n        for key, value"
    },
    {
        "original": "def rem_or(self, start, end, instr, target=None, include_beyond_target=False):\n        \"\"\"\n        Find offsets of all requested <instr> between <start> and <end>,\n        optionally <target>ing specified offset, and return list found\n        <instr> offsets which are not within any POP_JUMP_IF_TRUE jumps.\n        \"\"\"\n        assert(start >= 0 and end <= len(self.code) and start <= end)\n\n        # Find all offsets of requested instructions\n        instr_offsets = self.inst_matches(start, end, instr, target,\n                                          include_beyond_target)\n        # Get all POP_JUMP_IF_TRUE (or) offsets\n        if self.version == 3.0:\n            jump_true_op = self.opc.JUMP_IF_TRUE\n        else:\n            jump_true_op = self.opc.POP_JUMP_IF_TRUE\n        pjit_offsets = self.inst_matches(start, end, jump_true_op)\n        filtered = []\n        for pjit_offset in pjit_offsets:\n            pjit_tgt = self.get_target(pjit_offset) - 3\n            for instr_offset in instr_offsets:\n                if instr_offset <= pjit_offset or instr_offset >= pjit_tgt:\n                    filtered.append(instr_offset)\n            instr_offsets = filtered\n            filtered = []\n        return instr_offsets",
        "sampled": "```python\nclass Solution:\n    def rem_or(self, start, end, instr, target=None, include_beyond_target=False):\n        \"\"\"\n        Find offsets of all requested <instr> between <start> and <end>,\n        optionally <target>ing specified offset, and return list found\n        <instr> offsets which are not within any POP_JUMP_IF_TRUE jumps.\n        \n        Parameters:\n        start (int): Start offset\n        end (int): End offset\n        instr (str"
    },
    {
        "original": "def scenario(ctx, dependency_name, driver_name, lint_name, provisioner_name,\n             role_name, scenario_name, verifier_name):  # pragma: no cover\n    \"\"\" Initialize a new scenario for use with Molecule. \"\"\"\n    command_args = {\n        'dependency_name': dependency_name,\n        'driver_name': driver_name,\n        'lint_name': lint_name,\n        'provisioner_name': provisioner_name,\n        'role_name': role_name,\n        'scenario_name': scenario_name,\n        'subcommand': __name__,\n        'verifier_name': verifier_name,\n    }\n\n    if verifier_name == 'inspec':\n        command_args['verifier_lint_name'] = 'rubocop'\n\n    if verifier_name == 'goss':\n        command_args['verifier_lint_name'] = 'yamllint'\n\n    if verifier_name == 'ansible':\n        command_args['verifier_lint_name'] = 'ansible-lint'\n\n    s = Scenario(command_args)\n    s.execute()",
        "sampled": "```python\nfrom typing import Dict\n\ndef scenario(ctx: Dict, dependency_name: str, driver_name: str, lint_name: str,\n             provisioner_name: str, role_name: str, scenario_name: str,\n             verifier_name: str) -> None:\n    \"\"\" Initialize a new scenario for use with Molecule. \"\"\"\n\n    # Perform addition vertically to represent an initial molecule composition (O = 6H2O)\n    mol_composition = 6 * \"H2O"
    },
    {
        "original": "def rotate_sites(self, indices=None, theta=0, axis=None, anchor=None,\n                     to_unit_cell=True):\n        \"\"\"\n        Rotate specific sites by some angle around vector at anchor.\n\n        Args:\n            indices (list): List of site indices on which to perform the\n                translation.\n            theta (float): Angle in radians\n            axis (3x1 array): Rotation axis vector.\n            anchor (3x1 array): Point of rotation.\n            to_unit_cell (bool): Whether new sites are transformed to unit\n                cell\n        \"\"\"\n\n        from numpy.linalg import norm\n        from numpy import cross, eye\n        from scipy.linalg import expm\n\n        if indices is None:\n            indices = range(len(self))\n\n        if axis is None:\n            axis = [0, 0, 1]\n\n        if anchor is None:\n            anchor = [0, 0, 0]\n\n        anchor = np.array(anchor)\n        axis = np.array(axis)\n\n        theta %= 2 * np.pi\n\n        rm = expm(cross(eye(3), axis / norm(axis)) * theta)\n        for i in indices:\n            site = self._sites[i]\n            coords = ((np.dot(rm, np.array(site.coords - anchor).T)).T + anchor).ravel()\n            new_site = PeriodicSite(\n                site.species, coords, self._lattice,\n                to_unit_cell=to_unit_cell, coords_are_cartesian=True,\n                properties=site.properties)\n            self._sites[i] = new_site",
        "sampled": "```python\nimport numpy as np\n\nclass SiteRotator:\n    def rotate_sites(self, indices=None, theta=0, axis=None, anchor=None,\n                     to_unit_cell=True):\n        \"\"\"\n        Rotate specific sites by some angle around vector at anchor.\n\n        Args:\n            indices (list): List of site indices on which to perform the\n                translation.\n            theta (float): Angle in radians\n            axis (3x1 array): Rotation axis vector.\n            anchor (3x1"
    },
    {
        "original": "def _get_status(host, services, zconf, path):\n    \"\"\"\n    :param host: Hostname or ip to fetch status from\n    :type host: str\n    :return: The device status as a named tuple.\n    :rtype: pychromecast.dial.DeviceStatus or None\n    \"\"\"\n\n    if not host:\n        for service in services.copy():\n            service_info = get_info_from_service(service, zconf)\n            host, _ = get_host_from_service_info(service_info)\n            if host:\n                _LOGGER.debug(\"Resolved service %s to %s\", service, host)\n                break\n\n    req = CC_SESSION.get(FORMAT_BASE_URL.format(host) + path, timeout=10)\n\n    req.raise_for_status()\n\n    # The Requests library will fall back to guessing the encoding in case\n    # no encoding is specified in the response headers - which is the case\n    # for the Chromecast.\n    # The standard mandates utf-8 encoding, let's fall back to that instead\n    # if no encoding is provided, since the autodetection does not always\n    # provide correct results.\n    if req.encoding is None:\n        req.encoding = 'utf-8'\n\n    return req.json()",
        "sampled": "```python\nimport pychromecast\n\ndef _get_device_status(host, services, zconf, path):\n    \"\"\"\n    Fetch the device status for a given host.\n\n    :param host: Hostname or ip to fetch status from\n    :type host: str\n    :param services: List of available Chromecast services\n    :type services: list[pychromecast.dial.ServiceConfiguration]\n    :param zconf: Configuration loaded from file (zconf.py)\n    :type"
    },
    {
        "original": "def set_state(profile, state, store='local'):\n    \"\"\"\n    Configure the firewall state.\n\n    Args:\n\n        profile (str):\n            The firewall profile to configure. Valid options are:\n\n            - domain\n            - public\n            - private\n\n        state (str):\n            The firewall state. Valid options are:\n\n            - on\n            - off\n            - notconfigured\n\n        store (str):\n            The store to use. This is either the local firewall policy or the\n            policy defined by local group policy. Valid options are:\n\n            - lgpo\n            - local\n\n            Default is ``local``\n\n    Returns:\n        bool: ``True`` if successful\n\n    Raises:\n        CommandExecutionError: If an error occurs\n        ValueError: If the parameters are incorrect\n    \"\"\"\n    # Input validation\n    if profile.lower() not in ('domain', 'public', 'private'):\n        raise ValueError('Incorrect profile: {0}'.format(profile))\n    if state.lower() not in ('on', 'off', 'notconfigured'):\n        raise ValueError('Incorrect state: {0}'.format(state))\n\n    # Run the command\n    command = 'set {0}profile state {1}'.format(profile, state)\n    results = _netsh_command(command=command, store=store)\n\n    # A successful run should return an empty list\n    if results:\n        raise CommandExecutionError('An error occurred: {0}'.format(results))\n\n    return True",
        "sampled": "```python\nclass Firewall:\n    def __init__(self):\n        self.firewall_profiles = {\n            \"domain\": [\"on\", \"off\", \"notconfigured\"],\n            \"public\": [\"on\", \"off\", \"notconfigured\"],\n            \"private\": [\"on\", \"off\", \"notconfigured\"]\n        }\n\n    def set_state(self, profile, state, store='local'):\n        \"\"\"\n        Configure the firewall state.\n\n        Args:\n\n            profile (str):\n                The firewall profile to configure"
    },
    {
        "original": "def update(self, id=None, new_data={}, **kwargs):\n        \"\"\"Update an object on the server.\n\n        Args:\n            id: ID of the object to update (can be None if not required)\n            new_data: the update data for the object\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            dict: The new object data (*not* a RESTObject)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the server cannot perform the request\n        \"\"\"\n        super(ProjectServiceManager, self).update(id, new_data, **kwargs)\n        self.id = id",
        "sampled": "```python\nclass GitlabAPI:\n    def update(self, id=None, new_data={}, **kwargs):\n        \"\"\"\n        Update an object on the server.\n\n        Args:\n            id: ID of the object to update (can be None if not required)\n            new_data: the update data for the object\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            dict: The new object data (*not* a RESTObject)\n\n        Raises:\n"
    },
    {
        "original": "def arm_and_takeoff_nogps(aTargetAltitude):\n    \"\"\"\n    Arms vehicle and fly to aTargetAltitude without GPS data.\n    \"\"\"\n\n    ##### CONSTANTS #####\n    DEFAULT_TAKEOFF_THRUST = 0.7\n    SMOOTH_TAKEOFF_THRUST = 0.6\n\n    print(\"Basic pre-arm checks\")\n    # Don't let the user try to arm until autopilot is ready\n    # If you need to disable the arming check,\n    # just comment it with your own responsibility.\n    while not vehicle.is_armable:\n        print(\" Waiting for vehicle to initialise...\")\n        time.sleep(1)\n\n\n    print(\"Arming motors\")\n    # Copter should arm in GUIDED_NOGPS mode\n    vehicle.mode = VehicleMode(\"GUIDED_NOGPS\")\n    vehicle.armed = True\n\n    while not vehicle.armed:\n        print(\" Waiting for arming...\")\n        vehicle.armed = True\n        time.sleep(1)\n\n    print(\"Taking off!\")\n\n    thrust = DEFAULT_TAKEOFF_THRUST\n    while True:\n        current_altitude = vehicle.location.global_relative_frame.alt\n        print(\" Altitude: %f  Desired: %f\" %\n              (current_altitude, aTargetAltitude))\n        if current_altitude >= aTargetAltitude*0.95: # Trigger just below target alt.\n            print(\"Reached target altitude\")\n            break\n        elif current_altitude >= aTargetAltitude*0.6:\n            thrust = SMOOTH_TAKEOFF_THRUST\n        set_attitude(thrust = thrust)\n        time.sleep(0.2)",
        "sampled": "```python\nimport math\n\ndef arm_and_takeoff_nogps(aTargetAltitude):\n    \"\"\"\n    Arms vehicle and fly to aTargetAltitude without GPS data.\n    \n    :param aTargetAltitude: Target altitude in meters.\n    \n    :return: None\n    \"\"\"\n\n    # Arm the vehicle\n    print(\"Arming the vehicle...\")\n    \n    # Take off and climb to target altitude without GPS data\n    print(\"Taking off and climbing to target altitude...\")\n    \n    #"
    },
    {
        "original": "def dbg_repr_run(self, run_addr):\n        \"\"\"\n        Debugging output of a single SimRun slice.\n\n        :param run_addr:    Address of the SimRun.\n        :return:            A string representation.\n        \"\"\"\n\n        if self.project.is_hooked(run_addr):\n            ss = \"%#x Hooked\\n\" % run_addr\n\n        else:\n            ss = \"%#x\\n\" % run_addr\n\n            # statements\n            chosen_statements = self.chosen_statements[run_addr]\n\n            vex_block = self.project.factory.block(run_addr).vex\n\n            statements = vex_block.statements\n            for i in range(0, len(statements)):\n                if i in chosen_statements:\n                    line = \"+\"\n                else:\n                    line = \"-\"\n                line += \"[% 3d] \" % i\n                line += str(statements[i])\n                ss += line + \"\\n\"\n\n            # exits\n            targets = self.chosen_exits[run_addr]\n            addr_strs = [ ]\n            for exit_stmt_id, target_addr in targets:\n                if target_addr is None:\n                    addr_strs.append(\"default\")\n                else:\n                    addr_strs.append(\"%#x\" % target_addr)\n\n            ss += \"Chosen exits: \" + \", \".join(addr_strs)\n\n        return ss",
        "sampled": "```python\ndef dbg_repr_run(self, run_addr):\n    \"\"\"\n    Debugging output of a single SimRun slice.\n\n    :param run_addr:    Address of the SimRun.\n    :return:            A string representation.\n    \"\"\"\n    \n    # Load the required data\n    run = \"SimRun Slice\"\n    \n    # Extract relevant information about the SimRun at given address\n    name = f\"Sim {run Addr} from {run}\"\n    \n    # Construct our string representation"
    },
    {
        "original": "def initialize_model(self, model):\n        \"\"\"\n        Initializes internal state and build fp32 master copy of weights.\n\n        :param model: fp16 model\n        \"\"\"\n        logging.info('Initializing fp32 clone weights')\n        self.fp16_model = model\n        self.fp16_model.zero_grad()\n        self.fp32_params = [param.to(torch.float32).detach()\n                            for param in model.parameters()]\n\n        for param in self.fp32_params:\n            param.requires_grad = True",
        "sampled": "```python\nimport torch\n\nclass ModelInitializer:\n    def initialize_model(self, model):\n        \"\"\"\n        Initializes internal state and build fp32 master copy of weights.\n\n        :param model: fp16 model\n        \"\"\"\n        \n        # Check if the input is a PyTorch Module\n        if not isinstance(model, torch.nn.Module):\n            raise ValueError(\"Input must be a PyTorch Module\")\n        \n        # Create a new copy of the model with float32 parameters\n        float32_model"
    },
    {
        "original": "def client_pause(self, timeout):\n        \"\"\"\n        Suspend all the Redis clients for the specified amount of time\n        :param timeout: milliseconds to pause clients\n        \"\"\"\n        if not isinstance(timeout, (int, long)):\n            raise DataError(\"CLIENT PAUSE timeout must be an integer\")\n        return self.execute_command('CLIENT PAUSE', str(timeout))",
        "sampled": "```python\nimport time\n\nclass RedisClient:\n    def __init__(self):\n        # Initialize Redis client with current thread ID\n        self.thread_id = str(id(threading.current_thread()))\n\n    def client_pause(self, timeout):\n        \"\"\"\n        Suspend all the Redis clients for the specified amount of time\n        :param timeout: milliseconds to pause clients\n        \"\"\"\n        \n        # Calculate sleep time in seconds from milliseconds\n        sleep_time = int(timeout / 1000)\n        \n        #"
    },
    {
        "original": "def from_scf_input(cls, workdir, scf_input, ph_ngqpt, with_becs=True, manager=None, allocate=True):\n        \"\"\"\n        Create a `PhononFlow` for phonon calculations from an `AbinitInput` defining a ground-state run.\n\n        Args:\n            workdir: Working directory of the flow.\n            scf_input: :class:`AbinitInput` object with the parameters for the GS-SCF run.\n            ph_ngqpt: q-mesh for phonons. Must be a sub-mesh of the k-mesh used for\n                electrons. e.g if ngkpt = (8, 8, 8). ph_ngqpt = (4, 4, 4) is a valid choice\n                whereas ph_ngqpt = (3, 3, 3) is not!\n            with_becs: True if Born effective charges are wanted.\n            manager: :class:`TaskManager` object. Read from `manager.yml` if None.\n            allocate: True if the flow should be allocated before returning.\n\n        Return:\n            :class:`PhononFlow` object.\n        \"\"\"\n        flow = cls(workdir, manager=manager)\n\n        # Register the SCF task\n        flow.register_scf_task(scf_input)\n        scf_task = flow[0][0]\n\n        # Make sure k-mesh and q-mesh are compatible.\n        scf_ngkpt, ph_ngqpt = np.array(scf_input[\"ngkpt\"]), np.array(ph_ngqpt)\n\n        if any(scf_ngkpt % ph_ngqpt != 0):\n            raise ValueError(\"ph_ngqpt %s should be a sub-mesh of scf_ngkpt %s\" % (ph_ngqpt, scf_ngkpt))\n\n        # Get the q-points in the IBZ from Abinit\n        qpoints = scf_input.abiget_ibz(ngkpt=ph_ngqpt, shiftk=(0, 0, 0), kptopt=1).points\n\n        # Create a PhononWork for each q-point. Add DDK and E-field if q == Gamma and with_becs.\n        for qpt in qpoints:\n            if np.allclose(qpt, 0) and with_becs:\n                ph_work = BecWork.from_scf_task(scf_task)\n            else:\n                ph_work = PhononWork.from_scf_task(scf_task, qpoints=qpt)\n\n            flow.register_work(ph_work)\n\n        if allocate: flow.allocate()\n\n        return flow",
        "sampled": "```python\nimport os\nfrom abipy import AbinitInput, PhononFlow, TaskManager\n\ndef from_scf_input(workdir, scf_input, ph_ngqpt, with_becs=True, manager=None, allocate=True):\n    \"\"\"\n    Create a `PhononFlow` for phonon calculations from an `AbinitInput` defining a ground-state run.\n\n    Args:\n        workdir: Working directory of the flow.\n        scf_input: :class:`Abinit"
    },
    {
        "original": "def _free(self, ptr):\n        \"\"\"\n        Handler for any libc `free` SimProcedure call. If the heap has faithful support for `free`, it ought to be\n        implemented in a `free` function (as opposed to the `_free` function).\n\n        :param ptr: the location in memory to be freed\n        \"\"\"\n        raise NotImplementedError(\"%s not implemented for %s\" % (self._free.__func__.__name__,\n                                                                 self.__class__.__name__))",
        "sampled": "```python\nclass Heap:\n    def __init__(self):\n        self.data = {}\n\n    def alloc(self, size):\n        addr = len(self.data)\n        data = [0] * size\n        self.data[addr] = data\n        return addr\n\n    def free(self, ptr):\n        if ptr in self.data:\n            del self.data[ptr]\n\n    def read(self, ptr, offset):\n        if ptr in self.data:\n            return self.data[ptr][offset]\n"
    },
    {
        "original": "def fix_pbc(structure, matrix=None):\n    \"\"\"\n    Set all frac_coords of the input structure within [0,1].\n\n    Args:\n        structure (pymatgen structure object):\n            input structure\n        matrix (lattice matrix, 3 by 3 array/matrix)\n            new structure's lattice matrix, if none, use\n            input structure's matrix\n\n    Return:\n        new structure with fixed frac_coords and lattice matrix\n    \"\"\"\n\n    spec = []\n    coords = []\n    if matrix is None:\n        latte = Lattice(structure.lattice.matrix)\n    else:\n        latte = Lattice(matrix)\n\n    for site in structure:\n        spec.append(site.specie)\n        coord = np.array(site.frac_coords)\n        for i in range(3):\n            coord[i] -= floor(coord[i])\n            if np.allclose(coord[i], 1):\n                coord[i] = 0\n            elif np.allclose(coord[i], 0):\n                coord[i] = 0\n            else:\n                coord[i] = round(coord[i], 7)\n        coords.append(coord)\n\n    return Structure(latte, spec, coords, site_properties=structure.site_properties)",
        "sampled": "```python\nimport numpy as np\nfrom pymatgen import Structure\n\ndef fix_pbc(structure, matrix=None):\n    \"\"\"\n    Set all frac_coords of the input structure within [0,1].\n\n    Args:\n        structure (pymatgen structure object):\n            input structure\n        matrix (lattice matrix, 3 by 3 array/matrix)\n            new structure's lattice matrix, if none, use\n            input structure's matrix\n\n    Return:\n        new structure with"
    },
    {
        "original": "def update_fields(self, updates):\n        \"\"\"\n        Update the value for a field(s) in the listitem\n\n        :param update: A dict of {'field name': newvalue}\n        \"\"\"\n\n        for field in updates:\n            if self._valid_field(field):\n                self._track_changes.add(field)\n            else:\n                raise ValueError('\"{}\" is not a valid internal field name'.format(field))\n\n        # Update existing instance of fields, or create a fields instance if needed\n        if self.fields:\n            self.fields.update(updates)\n        else:\n            self.fields = updates",
        "sampled": "```python\nclass ListItem:\n    def __init__(self, fields):\n        \"\"\"\n        Initialize a list item with a dictionary of fields\n        :param fields: A dict of field name and value\n        \"\"\"\n        self.fields = fields\n\n    def update_fields(self, updates):\n        \"\"\"\n        Update the value for a field(s) in the listitem\n\n        :param updates: A dict of {'field name': newvalue}\n        \"\"\"\n        \n uninitialized_fields = {**self.fields}\n \n"
    },
    {
        "original": "def DiamAns(cmd, **fields):\n    \"\"\"Craft Diameter answer commands\"\"\"\n    upfields, name = getCmdParams(cmd, False, **fields)\n    p = DiamG(**upfields)\n    p.name = name\n    return p",
        "sampled": "```python\ndef DiamAns(cmd, **fields):\n    \"\"\"Craft Diameter answer commands\"\"\"\n    \n    # Input Specifications\n    # The data is provided as a list of strings, where each string represents an academic record.\n    records = cmd\n    \n    # Solution\n    diameter_dict = {}\n    \n    for record in records:\n        name, friend = record.split()\n        if name not in diameter_dict:\n            diameter_dict[name] = [friend]\n        else:\n            diameter_dict[name].append"
    }
]
