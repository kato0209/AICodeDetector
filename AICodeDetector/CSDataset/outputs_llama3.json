[
    {
        "original": "def beginning_offsets(self, partitions):\n        \"\"\"Get the first offset for the given partitions.\n\n        This method does not change the current consumer position of the\n        partitions.\n\n        Note:\n            This method may block indefinitely if the partition does not exist.\n\n        Arguments:\n            partitions (list): List of TopicPartition instances to fetch\n                offsets for.\n\n        Returns:\n            ``{TopicPartition: int}``: The earliest available offsets for the\n            given partitions.\n\n        Raises:\n            UnsupportedVersionError: If the broker does not support looking\n                up the offsets by timestamp.\n            KafkaTimeoutError: If fetch failed in request_timeout_ms.\n        \"\"\"\n        offsets = self._fetcher.beginning_offsets(\n            partitions, self.config['request_timeout_ms'])\n        return offsets",
        "sampled": "```python\nfrom kafka import KafkaConsumer\n\ndef beginning_offsets(partitions):\n    consumer = KafkaConsumer()\n    offsets = {}\n    for p in partitions:\n        topic_partition = p.topic, p.partition\n        try:\n            offset = consumer.beginning_offset(topic_partition)\n            offsets[p] = offset\n        except Exception as e:\n            if isinstance(e, UnsupportedVersionError) or isinstance(e, KafkaTimeoutError):\n                raise e\n    return offsets\n```"
    },
    {
        "original": "def _get_values(cls, diff_dict, type='new'):\n        \"\"\"\n        Returns a dictionaries with the 'new' values in a diff dict.\n\n        type\n            Which values to return, 'new' or 'old'\n        \"\"\"\n        ret_dict = {}\n        for p in diff_dict.keys():\n            if type in diff_dict[p].keys():\n                ret_dict.update({p: diff_dict[p][type]})\n            else:\n                ret_dict.update(\n                    {p: cls._get_values(diff_dict[p], type=type)})\n        return ret_dict",
        "sampled": "```\ndef _get_values(cls, diff_dict, type='new'):\n    return {key: value[0] if type == 'new' else value[1] for key, value in diff_dict.items() if len(value) == 2}\n```"
    },
    {
        "original": "def get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n        \"\"\"\n        Resolve the field within the given state.\n        \"\"\"\n        # resolve field\n        field_class = state.javavm_classloader.get_class(field_class_name)\n        field_id = resolve_field(state, field_class, field_name, field_type)\n        # return field ref\n        return cls.from_field_id(obj_alloc_id, field_id)",
        "sampled": "```\ndef get_ref(cls, state, obj_alloc_id, field_class_name, field_name, field_type):\n    \"\"\"\n    Resolve the field within the given state.\n    \"\"\"\n    obj = state[obj_alloc_id]\n    if not hasattr(obj, field_class_name):\n        raise AttributeError(f\"Object {obj_alloc_id} does not have class {field_class_name}\")\n    field_cls = getattr(obj, field_class_name)\n    if not hasattr(field_cls, field_name):\n        raise AttributeError(f\""
    },
    {
        "original": "def pd_coords(self, comp):\n        \"\"\"\n        The phase diagram is generated in a reduced dimensional space\n        (n_elements - 1). This function returns the coordinates in that space.\n        These coordinates are compatible with the stored simplex objects.\n        \"\"\"\n        if set(comp.elements).difference(self.elements):\n            raise ValueError('{} has elements not in the phase diagram {}'\n                             ''.format(comp, self.elements))\n        return np.array(\n            [comp.get_atomic_fraction(el) for el in self.elements[1:]])",
        "sampled": "```\ndef pd_coords(self, comp):\n    n_elements = len(comp)\n    coords = [0] * (n_elements - 1)\n    sum_comp = sum(comp)\n    \n    for i in range(n_elements - 1):\n        coords[i] = comp[i] / sum_comp\n    \n    return coords\n```"
    },
    {
        "original": "def relative_ref(self, baseURI):\n        \"\"\"\n        Return string containing relative reference to package item from\n        *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n        return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n        \"\"\"\n        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath",
        "sampled": "```\ndef relative_ref(self, baseURI):\n    base_path = baseURI.split('/')\n    target_path = self.path.split('/')\n    i = 0\n    while i < len(base_path) and i < len(target_path) and base_path[i] == target_path[i]:\n        i += 1\n    rel_ref = '../' * (len(base_path) - i) + '/'.join(target_path[i:])\n    return rel_ref if rel_ref else '.'\n```"
    },
    {
        "original": "def is_enhanced_rr_cap_valid(self):\n        \"\"\"Checks is enhanced route refresh capability is enabled/valid.\n\n        Checks sent and received `Open` messages to see if this session with\n        peer is capable of enhanced route refresh capability.\n        \"\"\"\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled",
        "sampled": "```\ndef is_enhanced_rr_cap_valid(self):\n    open_messages_sent = self.open_messages_sent\n    open_messages_received = self.open_messages_received\n\n    if open_messages_sent and open_messages_received:\n        return 'enhanced-route-refresh-capability' in open_messages_sent and 'enhanced-route-refresh-capability' in open_messages_received\n    else:\n        return False\n```"
    },
    {
        "original": "def parse_osm_nodes_paths(osm_data):\n    \"\"\"\n    Construct dicts of nodes and paths with key=osmid and value=dict of\n    attributes.\n\n    Parameters\n    ----------\n    osm_data : dict\n        JSON response from from the Overpass API\n\n    Returns\n    -------\n    nodes, paths : tuple\n    \"\"\"\n\n    nodes = {}\n    paths = {}\n    for element in osm_data['elements']:\n        if element['type'] == 'node':\n            key = element['id']\n            nodes[key] = get_node(element)\n        elif element['type'] == 'way': #osm calls network paths 'ways'\n            key = element['id']\n            paths[key] = get_path(element)\n\n    return nodes, paths",
        "sampled": "```\ndef parse_osm_nodes_paths(osm_data):\n    nodes = {}\n    paths = {}\n\n    if 'elements' in osm_data:\n        for element in osm_data['elements']:\n            if element['type'] == 'node':\n                nodes[element['id']] = element\n            elif element['type'] == 'way':\n                way_id = element['id']\n                ways_tags = {k: v for k, v in element.items() if k != 'nodes' and k !="
    },
    {
        "original": "def get_best_electronegativity_anonymous_mapping(self, struct1, struct2):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar. If multiple substitutions are within tolerance\n        this will return the one which minimizes the difference in\n        electronegativity between the matches species.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            min_mapping (Dict): Mapping of struct1 species to struct2 species\n        \"\"\"\n        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        use_rms=True, break_on_match=True)\n\n        if matches:\n            min_X_diff = np.inf\n            for m in matches:\n                X_diff = 0\n                for k, v in m[0].items():\n                    X_diff += struct1.composition[k] * (k.X - v.X) ** 2\n                if X_diff < min_X_diff:\n                    min_X_diff = X_diff\n                    best = m[0]\n            return best",
        "sampled": "```python\nfrom pymatgen import Molecule as MoleculeClass\nfrom itertools import permutations\n\ndef get_best_electronegativity_anonymous_mapping(struct1, struct2):\n    # Get all possible permutations of mapping between species of two structures\n    perms = list(permutations(range(len(struct2.species)), len(struct1.species)))\n\n    # Initialize minimum difference in electronegativity and best mapping\n    min_diff = float('inf')\n    best_mapping = None\n\n    # Iterate over"
    },
    {
        "original": "def _convert_a_header_to_a_h2_header(self, hdr_name, hdr_value, is_sensitive, should_index):  # noqa: E501\n        # type: (str, str, Callable[[str, str], bool], Callable[[str], bool]) -> Tuple[HPackHeaders, int]  # noqa: E501\n        \"\"\" _convert_a_header_to_a_h2_header builds a HPackHeaders from a header\n        name and a value. It returns a HPackIndexedHdr whenever possible. If not,  # noqa: E501\n        it returns a HPackLitHdrFldWithoutIndexing or a\n        HPackLitHdrFldWithIncrIndexing, based on the should_index callback.\n        HPackLitHdrFldWithoutIndexing is forced if the is_sensitive callback\n        returns True and its never_index bit is set.\n        \"\"\"\n\n        # If both name and value are already indexed\n        idx = self.get_idx_by_name_and_value(hdr_name, hdr_value)\n        if idx is not None:\n            return HPackIndexedHdr(index=idx), len(self[idx])\n\n        # The value is not indexed for this headers\n\n        hdr_value = self._optimize_header_length_and_packetify(hdr_value)\n\n        # Searching if the header name is indexed\n        idx = self.get_idx_by_name(hdr_name)\n        if idx is not None:\n            if is_sensitive(\n                hdr_name,\n                hdr_value.getfieldval('data').origin()\n            ):\n                return HPackLitHdrFldWithoutIndexing(\n                    never_index=1,\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            if should_index(hdr_name):\n                return HPackLitHdrFldWithIncrIndexing(\n                    index=idx,\n                    hdr_value=hdr_value\n                ), len(\n                    HPackHdrEntry(\n                        self[idx].name(),\n                        hdr_value.getfieldval('data').origin()\n                    )\n                )\n            return HPackLitHdrFldWithoutIndexing(\n                index=idx,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    self[idx].name(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n\n        hdr_name = self._optimize_header_length_and_packetify(hdr_name)\n\n        if is_sensitive(\n            hdr_name.getfieldval('data').origin(),\n            hdr_value.getfieldval('data').origin()\n        ):\n            return HPackLitHdrFldWithoutIndexing(\n                never_index=1,\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        if should_index(hdr_name.getfieldval('data').origin()):\n            return HPackLitHdrFldWithIncrIndexing(\n                index=0,\n                hdr_name=hdr_name,\n                hdr_value=hdr_value\n            ), len(\n                HPackHdrEntry(\n                    hdr_name.getfieldval('data').origin(),\n                    hdr_value.getfieldval('data').origin()\n                )\n            )\n        return HPackLitHdrFldWithoutIndexing(\n            index=0,\n            hdr_name=hdr_name,\n            hdr_value=hdr_value\n        ), len(\n            HPackHdrEntry(\n                hdr_name.getfieldval('data').origin(),\n                hdr_value.getfieldval('data').origin()\n            )\n        )",
        "sampled": "```\nfrom typing import Callable, Tuple\n\nclass HPackHeaders:\n    pass\n\nclass HPackIndexedHdr(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithoutIndexing(HPackHeaders):\n    pass\n\nclass HPackLitHdrFldWithIncrIndexing(HPackHeaders):\n    pass\n\n\ndef _convert_a_header_to_a_h2_header(\n    hdr_name: str,\n    hdr_value: str,\n    is_sensitive: Callable[[str, str], bool"
    },
    {
        "original": "def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        \"\"\"A simple way to generate a `CREATE` transaction.\n\n            Note:\n                This method currently supports the following Cryptoconditions\n                use cases:\n                    - Ed25519\n                    - ThresholdSha256\n\n                Additionally, it provides support for the following BigchainDB\n                use cases:\n                    - Multiple inputs and outputs.\n\n            Args:\n                tx_signers (:obj:`list` of :obj:`str`): A list of keys that\n                    represent the signers of the CREATE Transaction.\n                recipients (:obj:`list` of :obj:`tuple`): A list of\n                    ([keys],amount) that represent the recipients of this\n                    Transaction.\n                metadata (dict): The metadata to be stored along with the\n                    Transaction.\n                asset (dict): The metadata associated with the asset that will\n                    be created in this Transaction.\n\n            Returns:\n                :class:`~bigchaindb.common.transaction.Transaction`\n        \"\"\"\n\n        (inputs, outputs) = cls.validate_create(tx_signers, recipients, asset, metadata)\n        return cls(cls.CREATE, {'data': asset}, inputs, outputs, metadata)",
        "sampled": "```python\nclass BigchainTransactionCreator:\n    def create(cls, tx_signers, recipients, metadata=None, asset=None):\n        from bigchaindb.common.transaction import Transaction\n        \n        # Initialize an empty transaction object \n        transaction = Transaction()\n\n        # Set transaction type to 'CREATE'\n        transaction.operation = 'CREATE'\n\n        # Add signers to transaction inputs \n        for signer in tx_signers:\n            transaction.add_input(signer)\n\n        # Add recipients and amounts to transaction outputs \n       "
    },
    {
        "original": "def utc_dt_to_local_dt(dtm):\n    \"\"\"Convert a UTC datetime to datetime in local timezone\"\"\"\n    utc_zone = mktz(\"UTC\")\n    if dtm.tzinfo is not None and dtm.tzinfo != utc_zone:\n        raise ValueError(\n            \"Expected dtm without tzinfo or with UTC, not %r\" % (\n                dtm.tzinfo\n            )\n        )\n\n    if dtm.tzinfo is None:\n        dtm = dtm.replace(tzinfo=utc_zone)\n    return dtm.astimezone(mktz())",
        "sampled": "```\nfrom datetime import datetime\nimport pytz\n\ndef utc_dt_to_local_dt(dtm):\n    utc_dt = datetime.strptime(dtm, '%Y-%m-%d %H:%M:%S')\n    utc_dt = utc_dt.replace(tzinfo=pytz.UTC)\n    local_dt = utc_dt.astimezone()\n    return local_dt.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n```"
    },
    {
        "original": "def _getScriptSettingsFromIniFile(policy_info):\n    \"\"\"\n    helper function to parse/read a GPO Startup/Shutdown script file\n\n    psscript.ini and script.ini file definitions are here\n        https://msdn.microsoft.com/en-us/library/ff842529.aspx\n        https://msdn.microsoft.com/en-us/library/dd303238.aspx\n    \"\"\"\n    _existingData = None\n    if os.path.isfile(policy_info['ScriptIni']['IniPath']):\n        with salt.utils.files.fopen(policy_info['ScriptIni']['IniPath'], 'rb') as fhr:\n            _existingData = fhr.read()\n        if _existingData:\n            try:\n                _existingData = deserialize(_existingData.decode('utf-16-le').lstrip('\\ufeff'))\n                log.debug('Have deserialized data %s', _existingData)\n            except Exception as error:\n                log.exception('An error occurred attempting to deserialize data for %s', policy_info['Policy'])\n                raise CommandExecutionError(error)\n            if 'Section' in policy_info['ScriptIni'] and policy_info['ScriptIni']['Section'].lower() in [z.lower() for z in _existingData.keys()]:\n                if 'SettingName' in policy_info['ScriptIni']:\n                    log.debug('Need to look for %s', policy_info['ScriptIni']['SettingName'])\n                    if policy_info['ScriptIni']['SettingName'].lower() in [z.lower() for z in _existingData[policy_info['ScriptIni']['Section']].keys()]:\n                        return _existingData[policy_info['ScriptIni']['Section']][policy_info['ScriptIni']['SettingName'].lower()]\n                    else:\n                        return None\n                else:\n                    return _existingData[policy_info['ScriptIni']['Section']]\n            else:\n                return None\n\n    return None",
        "sampled": "```\ndef _getScriptSettingsFrom IniFile(policy_info):\n    # Open and read the file\n    with open(policy_info, 'r') as f:\n        content = f.read()\n\n    # Initialize variables\n    scripts = {}\n    current_script = None\n\n    # Parse lines\n    for line in content.splitlines():\n        line = line.strip()\n        \n        if line.startswith('[') and ']' in line:\n            # Section header found, extract script name\n            start_br"
    },
    {
        "original": "def _get_triplet_scores(self, triangles_list):\n        \"\"\"\n        Returns the score of each of the triplets found in the current model\n\n        Parameters\n        ---------\n        triangles_list: list\n                        The list of variables forming the triangles to be updated. It is of the form of\n                        [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]\n\n        Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'var_5', 'var_4', 'var_7'}): 10.23}\n        \"\"\"\n        triplet_scores = {}\n        for triplet in triangles_list:\n\n            # Find the intersection sets of the current triplet\n            triplet_intersections = [intersect for intersect in it.combinations(triplet, 2)]\n\n            # Independent maximization\n            ind_max = sum([np.amax(self.objective[frozenset(intersect)].values) for intersect in triplet_intersections])\n\n            # Joint maximization\n            joint_max = self.objective[frozenset(triplet_intersections[0])]\n            for intersect in triplet_intersections[1:]:\n                joint_max += self.objective[frozenset(intersect)]\n            joint_max = np.amax(joint_max.values)\n            # score = Independent maximization solution - Joint maximization solution\n            score = ind_max - joint_max\n            triplet_scores[frozenset(triplet)] = score\n\n        return triplet_scores",
        "sampled": "```\ndef _get_triplet_scores(self, triangles_list):\n    scores = {}\n    for triangle in triangles_list:\n        triangle_set = frozenset(triangle)\n        # assuming the score calculation is a placeholder, replace with actual logic\n        score = len(triangle) * 2.0\n        scores[triangle_set] = score\n    return scores\n```"
    },
    {
        "original": "async def vcx_messages_update_status(msg_json: str):\n    \"\"\"\n    Update the status of messages from the specified connection\n    :param msg_json:\n    :return:\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not hasattr(vcx_messages_update_status, \"cb\"):\n        logger.debug(\"vcx_messages_update_status: Creating callback\")\n        vcx_messages_update_status.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32))\n\n    c_msg_json = c_char_p(msg_json.encode('utf-8'))\n    c_status = c_char_p(\"MS-106\".encode('utf-8'))\n\n    result = await do_call('vcx_messages_update_status',\n                           c_status,\n                           c_msg_json,\n                           vcx_messages_update_status.cb)\n\n    logger.debug(\"vcx_messages_update_status completed\")\n    return result",
        "sampled": "```\nasync def vcx_messages_update_status(msg_json: str):\n    import json\n    import asyncio\n    from aiohttp import ClientSession\n\n    async with ClientSession() as session:\n        msg_data = json.loads(msg_json)\n        connection_id = msg_data['connection_id']\n        messages = msg_data['messages']\n\n        for message in messages:\n            url = f'https://example.com/update_message_status/{connection_id}'\n            data = {'message_id': message['uid'], 'status"
    },
    {
        "original": "def recv_with_timeout(self, timeout=1):\n        \"\"\"Receive a complete ISOTP message, blocking until a message is\n        received or the specified timeout is reached.\n        If timeout is 0, then this function doesn't block and returns the\n        first frame in the receive buffer or None if there isn't any.\"\"\"\n        msg = self.ins.recv(timeout)\n        t = time.time()\n        if msg is None:\n            raise Scapy_Exception(\"Timeout\")\n        return self.basecls, msg, t",
        "sampled": "```\nimport socket\nimport time\n\nclass IsoTPSocket:\n    def __init__(self):\n        self.receive_buffer = b''\n\n    def recv_with_timeout(self, timeout=1):\n        start_time = time.time()\n        while True:\n            if self.receive_buffer:\n                return self.receive_buffer.pop(0)\n            elif time.time() - start_time > timeout:\n                return None\n```"
    },
    {
        "original": "def carmichael_of_factorized( f_list ):\n  \"\"\"Return the Carmichael function of a number that is\n  represented as a list of (prime,exponent) pairs.\n  \"\"\"\n\n  if len( f_list ) < 1: return 1\n\n  result = carmichael_of_ppower( f_list[0] )\n  for i in range( 1, len( f_list ) ):\n    result = lcm( result, carmichael_of_ppower( f_list[i] ) )\n\n  return result",
        "sampled": "```\nimport math\n\ndef carmichael_of_factorized(f_list):\n    lambda_val = 1\n    for p, e in f_list:\n        lambda_val = lcm(lambda_val, (p-1) * (p**(e-1)))\n    return lambda_val\n\ndef gcd(a, b):\n    while b:\n        a, b = b, a % b\n    return a\n\ndef lcm(a, b):\n    return a * b // gcd(a, b)\n```"
    },
    {
        "original": "def absent(name, profile=\"splunk\"):\n    \"\"\"\n    Ensure a search is absent\n\n    .. code-block:: yaml\n\n        API Error Search:\n          splunk_search.absent\n\n    The following parameters are required:\n\n    name\n        This is the name of the search in splunk\n    \"\"\"\n    ret = {\n        'name': name,\n        'changes': {},\n        'result': True,\n        'comment': '{0} is absent.'.format(name)\n    }\n\n    target = __salt__['splunk_search.get'](name, profile=profile)\n    if target:\n        if __opts__['test']:\n            ret = {}\n            ret[\"name\"] = name\n            ret['comment'] = \"Would delete {0}\".format(name)\n            ret['result'] = None\n            return ret\n\n        result = __salt__['splunk_search.delete'](name, profile=profile)\n        if result:\n            ret['comment'] = '{0} was deleted'.format(name)\n        else:\n            ret['comment'] = 'Failed to delete {0}'.format(name)\n            ret['result'] = False\n    return ret",
        "sampled": "```\nimport requests\n\ndef absent(name, profile=\"splunk\"):\n    url = f\"https://{profile}.com/api/services/search/jobs/{name}\"\n    response = requests.delete(url, auth=(\"username\", \"password\"))\n    if response.status_code == 200:\n        return True\n    else:\n        return False\n```"
    },
    {
        "original": "def GetNotificationsForAllShards(self, queue):\n    \"\"\"Returns notifications for all shards of a queue at once.\n\n    Used by worker_test_lib.MockWorker to cover all shards with a single worker.\n\n    Args:\n      queue: usually rdfvalue.RDFURN(\"aff4:/W\")\n\n    Returns:\n      List of rdf_flows.GrrNotification objects\n    \"\"\"\n    notifications_by_session_id = {}\n    for queue_shard in self.GetAllNotificationShards(queue):\n      self._GetUnsortedNotifications(\n          queue_shard, notifications_by_session_id=notifications_by_session_id)\n\n    return notifications_by_session_id.values()",
        "sampled": "```\ndef GetNotificationsForAllShards(self, queue):\n    notifications = []\n    for shard in self.GetShardNames(queue):\n        notifications.extend(self.GetNotificationsForShard(queue, shard))\n    return notifications\n```"
    },
    {
        "original": "def differing_functions_with_consts(self):\n        \"\"\"\n        :return: A list of function matches that appear to differ including just by constants\n        \"\"\"\n        different_funcs = []\n        for (func_a, func_b) in self.function_matches:\n            if not self.functions_probably_identical(func_a, func_b, check_consts=True):\n                different_funcs.append((func_a, func_b))\n        return different_funcs",
        "sampled": "```\nfrom sympy import symbols, Eq, solve\n\nclass FunctionMatcher:\n    def __init__(self, functions):\n        self.functions = functions\n\n    def differing_functions_with_consts(self):\n        matches = []\n        for i in range(len(self.functions)):\n            for j in range(i + 1, len(self.functions)):\n                x = symbols('x')\n                eq1 = Eq(eval(self.functions[i]), 0)\n                eq2 = Eq(eval(self.functions[j]), 0)\n               "
    },
    {
        "original": "def save(self, eopatch, use_tmp=True):\r\n        \"\"\" Method which does the saving\r\n\r\n        :param eopatch: EOPatch containing the data which will be saved\r\n        :type eopatch: EOPatch\r\n        :param use_tmp: If `True` data will be saved to temporary file, otherwise it will be saved to intended\r\n        (i.e. final) location\r\n        :type use_tmp: bool\r\n        \"\"\"\r\n        filename = self.tmp_filename if use_tmp else self.final_filename\r\n\r\n        if self.feature_name is None:\r\n            data = eopatch[self.feature_type]\r\n            if self.feature_type.has_dict():\r\n                data = data.get_dict()\r\n\r\n            if self.feature_type is FeatureType.BBOX:\r\n                data = tuple(data) + (int(data.crs.value),)\r\n        else:\r\n            data = eopatch[self.feature_type][self.feature_name]\r\n\r\n        file_dir = os.path.dirname(filename)\r\n        os.makedirs(file_dir, exist_ok=True)\r\n\r\n        if self.compress_level:\r\n            file_handle = gzip.GzipFile(filename, 'w', self.compress_level)\r\n        else:\r\n            file_handle = open(filename, 'wb')\r\n\r\n        with file_handle as outfile:\r\n            LOGGER.debug(\"Saving (%s, %s) to %s\", str(self.feature_type), str(self.feature_name), filename)\r\n\r\n            if self.file_format is FileFormat.NPY:\r\n                np.save(outfile, data)\r\n            elif self.file_format is FileFormat.PICKLE:\r\n                pickle.dump(data, outfile)\r\n            else:\r\n                ValueError('File {} was not saved because saving in file format {} is currently not '\r\n                           'supported'.format(filename, self.file_format))",
        "sampled": "```\nclass EOPatchSaver:\n    def save(self, eopatch, use_tmp=True):\n        if use_tmp:\n            # Save to temporary file\n            with open(\"temp.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n        else:\n            # Save to intended location\n            with open(\"eopatch.eopatch\", \"wb\") as f:\n                pickle.dump(eopatch, f)\n```"
    },
    {
        "original": "def _addAttr(self, txn, isCommitted=False) -> None:\n        \"\"\"\n        The state trie stores the hash of the whole attribute data at:\n            the did+attribute name if the data is plaintext (RAW)\n            the did+hash(attribute) if the data is encrypted (ENC)\n        If the attribute is HASH, then nothing is stored in attribute store,\n        the trie stores a blank value for the key did+hash\n        \"\"\"\n        assert get_type(txn) == ATTRIB\n        attr_type, path, value, hashed_value, value_bytes = domain.prepare_attr_for_state(txn)\n        self.state.set(path, value_bytes)\n        if attr_type != HASH:\n            self.attributeStore.set(hashed_value, value)",
        "sampled": "```\nclass StateTrie:\n    def __init__(self):\n        self.trie = {}\n\n    def _addAttr(self, txn, isCommitted=False) -> None:\n        did, attribute_name, data_type, data = txn\n        if data_type == 'RAW':\n            key = f\"{did}{attribute_name}\"\n            self.trie[key] = hash(data)\n        elif data_type == 'ENC':\n            key = f\"{did}{hash(attribute_name)}\"\n            self.tr"
    },
    {
        "original": "def _find_packages(root):\n  \"\"\"\n  Helper for ``build_index()``: Yield a list of tuples\n  ``(pkg_xml, zf, subdir)``, where:\n    - ``pkg_xml`` is an ``ElementTree.Element`` holding the xml for a\n    package\n    - ``zf`` is a ``zipfile.ZipFile`` for the package's contents.\n    - ``subdir`` is the subdirectory (relative to ``root``) where\n    the package was found (e.g. 'corpora' or 'grammars').\n  \"\"\"\n  # Find all packages.\n  packages = []\n  for dirname, subdirs, files in os.walk(root):\n    relpath = '/'.join(_path_from(root, dirname))\n    for filename in files:\n      if filename.endswith('.xml'):\n        xmlfilename = os.path.join(dirname, filename)\n        zipfilename = xmlfilename[:-4]+'.zip'\n        try: zf = zipfile.ZipFile(zipfilename)\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (zipfilename, e))\n        try: pkg_xml = ElementTree.parse(xmlfilename).getroot()\n        except Exception as e:\n          raise ValueError('Error reading file %r!\\n%s' %\n                   (xmlfilename, e))\n\n        # Check that the UID matches the filename\n        uid = os.path.split(xmlfilename[:-4])[1]\n        if pkg_xml.get('id') != uid:\n          raise ValueError('package identifier mismatch (%s '\n                   'vs %s)' % (pkg_xml.get('id'), uid))\n\n        # Check that the zipfile expands to a subdir whose\n        # name matches the uid.\n        if sum( (name!=uid and not name.startswith(uid+'/'))\n            for name in zf.namelist() ):\n          raise ValueError('Zipfile %s.zip does not expand to a '\n                   'single subdirectory %s/' % (uid, uid))\n\n        yield pkg_xml, zf, relpath\n    # Don't recurse into svn subdirectories:\n    try: subdirs.remove('.svn')\n    except ValueError: pass",
        "sampled": "```\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ET\n\ndef _find_packages(root):\n    for subdir, dirs, files in os.walk(root):\n        for file in files:\n            if file.endswith('.zip'):\n                zf = zipfile.ZipFile(os.path.join(subdir, file))\n                pkg_xml = ET.parse(zf.open('package.xml')).getroot()\n                yield (pkg_xml, zf, os.path.relpath(subdir, root))\n```"
    },
    {
        "original": "def srbt1(bt_address, pkts, *args, **kargs):\n    \"\"\"send and receive 1 packet using a bluetooth socket\"\"\"\n    a, b = srbt(bt_address, pkts, *args, **kargs)\n    if len(a) > 0:\n        return a[0][1]",
        "sampled": "```\nimport bluetooth\n\ndef srbt1(bt_address, pkts, *_args={}, **_kargs={}):\n    sock = bluetoothAMP.get_socket()\n    sock.connect((bt_address, 1))\n    sock.send(pkts)\n    data = sock.recv(1024)\n    return data\n```"
    },
    {
        "original": "def GetIPAddresses(self):\n    \"\"\"IP addresses from all interfaces.\"\"\"\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n      for address in interface.addresses:\n        if address.human_readable_address not in filtered_ips:\n          result.append(Text(address.human_readable_address))\n    return sorted(result)",
        "sampled": "```\nimport socket\nimport netifaces\n\nclass NetworkInterface:\n    def GetIPAddresses(self):\n        ip_array = []\n        for interface in netifaces.interfaces():\n            if interface == 'lo':  # ignore loopback interface\n                continue\n            lst = netifaces.ifaddresses(interface)\n            teste = {}\n            for item in lst:\n                if(item == 2): # AF_INET \n                    for thing in lst[item]:\n                        teste={\n                            \"iname\":interface,\n                           "
    },
    {
        "original": "def are_symmetrically_related(self, point_a, point_b, tol=0.001):\n        \"\"\"\n        Checks if two points are symmetrically related.\n\n        Args:\n            point_a (3x1 array): First point.\n            point_b (3x1 array): Second point.\n            tol (float): Absolute tolerance for checking distance.\n\n        Returns:\n            True if self.operate(point_a) == point_b or vice versa.\n        \"\"\"\n        if np.allclose(self.operate(point_a), point_b, atol=tol):\n            return True\n        if np.allclose(self.operate(point_b), point_a, atol=tol):\n            return True\n        return False",
        "sampled": "```\ndef operate(self, point):\n    # Assuming operate function is defined elsewhere\n    pass\n\ndef are_symmetrically_related(self, point_a, point_b, tol=0.001):\n    \"\"\"\n    Checks if two points are symmetrically related.\n\n    Args:\n        point_a (3x1 array): First point.\n        point_b (3x1 array): Second point.\n        tol (float): Absolute tolerance for checking distance.\n\n    Returns:\n        True if self.operate(point"
    },
    {
        "original": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip",
        "sampled": "```\ndef validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    parts = ip.split(\".\")\n    if len(parts) != 4:\n        return False\n    for part in parts:\n        if not part.isdigit():\n            return False\n        i = int(part)\n        if i < 0 or i > 255:\n            return False\n    return True\n```"
    },
    {
        "original": "def find_available_interfaces():\n    \"\"\"Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n\n    try:\n        # it might be good to add \"type vcan\", but that might (?) exclude physical can devices\n        command = [\"ip\", \"-o\", \"link\", \"list\", \"up\"]\n        output = subprocess.check_output(command, universal_newlines=True)\n\n    except Exception as e: # subprocess.CalledProcessError was too specific\n        log.error(\"failed to fetch opened can devices: %s\", e)\n        return []\n\n    else:\n        #log.debug(\"find_available_interfaces(): output=\\n%s\", output)\n        # output contains some lines like \"1: vcan42: <NOARP,UP,LOWER_UP> ...\"\n        # extract the \"vcan42\" of each line\n        interface_names = [line.split(\": \", 3)[1] for line in output.splitlines()]\n        log.debug(\"find_available_interfaces(): detected: %s\", interface_names)\n        return filter(_PATTERN_CAN_INTERFACE.match, interface_names)",
        "sampled": "```python\nimport subprocess\nimport logging\n\ndef find_available_interfaces():\n    \"\"\"\n    Returns the names of all open can/vcan interfaces using\n    the ``ip link list`` command. If the lookup fails, an error\n    is logged to the console and an empty list is returned.\n\n    :rtype: an iterable of :class:`str`\n    \"\"\"\n    \n    try:\n        output = subprocess.check_output([\"ip\", \"link\", \"list\"])\n        lines = output.decode(\"utf-"
    },
    {
        "original": "def save_session(self, sid, session, namespace=None):\n        \"\"\"Store the user session for a client.\n\n        The only difference with the :func:`socketio.Server.save_session`\n        method is that when the ``namespace`` argument is not given the\n        namespace associated with the class is used.\n        \"\"\"\n        return self.server.save_session(\n            sid, session, namespace=namespace or self.namespace)",
        "sampled": "```\nclass Server:\n    def __init__(self, namespace=None):\n        self.namespace = namespace\n        self.sessions = {}\n\n    def save_session(self, sid, session, namespace=None):\n        if namespace is None:\n            namespace = self.namespace\n        if namespace not in self.sessions:\n            self.sessions[namespace] = {}\n        self.sessions[namespace][sid] = session\n```"
    },
    {
        "original": "def get_public_keys_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 public keys from the v3 signature block\n        \"\"\"\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        public_keys = []\n\n        for signer in self._v3_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys",
        "sampled": "```\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\ndef get_public_keys_der_v3(self):\n    public_keys = []\n    if self.v3_signature_block:\n        for cert in self.v3_signature_block.certs:\n            pub_key = cert.public_key()\n            der_pub_key = pub_key.public_bytes(\n                encoding=serialization.Encoding.DER,\n                format=serialization.PublicFormat.SubjectPublicKeyInfo\n            )\n            public_keys.append(der"
    },
    {
        "original": "def chemical_symbols(self):\n        \"\"\"Chemical symbols char [number of atom species][symbol length].\"\"\"\n        charr = self.read_value(\"chemical_symbols\")\n        symbols = []\n        for v in charr:\n            s = \"\".join(c.decode(\"utf-8\") for c in v)\n            symbols.append(s.strip())\n\n        return symbols",
        "sampled": "```\ndef chemical_symbols(atom_species, symbol_length):\n    symbols = []\n    ascii_offset = 97  # ASCII value of 'a'\n    for i in range(atom_species):\n        symbol = \"\"\n        for j in range(symbol_length):\n            symbol += chr(ascii_offset + ((i + j) % 26))\n        symbols.append(symbol)\n    return symbols\n```"
    },
    {
        "original": "def safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)",
        "sampled": "```\nimport yaml\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    if stream is None:\n        return yaml.safe_dump_all(documents, default_flow_style=True, **kwds)\n    else:\n        yaml.safe_dump_all(documents, stream=stream, default_flow_style=True, **kwds)\n```"
    },
    {
        "original": "def _simulator_iterator(self, circuit: circuits.Circuit,\n                            param_resolver: study.ParamResolver,\n                            qubit_order: ops.QubitOrderOrList,\n                            initial_state: Union[int, np.ndarray]) -> Iterator:\n        \"\"\"See definition in `cirq.SimulatesIntermediateState`.\n\n        If the initial state is an int, the state is set to the computational\n        basis state corresponding to this state. Otherwise  if the initial\n        state is a np.ndarray it is the full initial state, either a pure state\n        or the full density matrix.  If it is the pure state it must be the\n        correct size, be normalized (an L2 norm of 1), and be safely castable\n        to an appropriate dtype for the simulator.  If it is a mixed state\n        it must be correctly sized and positive semidefinite with trace one.\n        \"\"\"\n        param_resolver = param_resolver or study.ParamResolver({})\n        resolved_circuit = protocols.resolve_parameters(circuit, param_resolver)\n        actual_initial_state = 0 if initial_state is None else initial_state\n        return self._base_iterator(resolved_circuit,\n                                   qubit_order,\n                                   actual_initial_state)",
        "sampled": "```\nfrom typing import Iterator, Union\nimport numpy as np\nfrom cirq import Circuit\n\ndef _simulator_iterator(circuit: Circuit,\n                        param_resolver,\n                        qubit_order,\n                        initial_state: Union[int, np.ndarray]) -> Iterator:\n    # Check if initial_state is an integer \n    if isinstance(initial_state, int):\n      # Set initial_state to computational basis corresponding to this integer \n      pass\n  \n    # Check if initial_state is a numpy array  \n    elif isinstance"
    },
    {
        "original": "def predictive_variance(self, mu,variance, predictive_mean=None, Y_metadata=None):\n        \"\"\"\n        Approximation to the predictive variance: V(Y_star)\n\n        The following variance decomposition is used:\n        V(Y_star) = E( V(Y_star|f_star)**2 ) + V( E(Y_star|f_star) )**2\n\n        :param mu: mean of posterior\n        :param sigma: standard deviation of posterior\n        :predictive_mean: output's predictive mean, if None _predictive_mean function will be called.\n\n        \"\"\"\n        #sigma2 = sigma**2\n        normalizer = np.sqrt(2*np.pi*variance)\n\n        fmin_v = -np.inf\n        fmin_m = np.inf\n        fmin = -np.inf\n        fmax = np.inf\n\n        from ..util.misc import safe_exp\n        # E( V(Y_star|f_star) )\n        def int_var(f,m,v):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = safe_exp(exponent)\n            #If p is zero then conditional_variance will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_variance(f)*p\n        scaled_exp_variance = [quad(int_var, fmin_v, fmax,args=(mj,s2j))[0] for mj,s2j in zip(mu,variance)]\n        exp_var = np.array(scaled_exp_variance)[:,None] / normalizer\n\n        #V( E(Y_star|f_star) ) =  E( E(Y_star|f_star)**2 ) - E( E(Y_star|f_star) )**2\n\n        #E( E(Y_star|f_star) )**2\n        if predictive_mean is None:\n            predictive_mean = self.predictive_mean(mu,variance)\n        predictive_mean_sq = predictive_mean**2\n\n        #E( E(Y_star|f_star)**2 )\n        def int_pred_mean_sq(f,m,v,predictive_mean_sq):\n            exponent = -(0.5/v)*np.square(f - m)\n            p = np.exp(exponent)\n            #If p is zero then conditional_mean**2 will overflow\n            if p < 1e-10:\n                return 0.\n            else:\n                return self.conditional_mean(f)**2*p\n\n        scaled_exp_exp2 = [quad(int_pred_mean_sq, fmin_m, fmax,args=(mj,s2j,pm2j))[0] for mj,s2j,pm2j in zip(mu,variance,predictive_mean_sq)]\n        exp_exp2 = np.array(scaled_exp_exp2)[:,None] / normalizer\n\n        var_exp = exp_exp2 - predictive_mean_sq\n\n        # V(Y_star) = E[ V(Y_star|f_star) ] + V[ E(Y_star|f_star) ]\n        # V(Y_star) = E[ V(Y_star|f_star) ] + E(Y_star**2|f_star) - E[Y_star|f_star]**2\n        return exp_var + var_exp",
        "sampled": "```\ndef predictive_variance(self, mu, variance, predictive_mean=None, Y_metadata=None):\n    if predictive_mean is None:\n      # If no predictive mean is provided assume it's 0\n      predictive_mean = 0  \n    expectation_squared = (mu - predictive_mean) ** 2 \n    variance_squared = variance ** 2 \n    return expectation_squared + variance_squared\n```"
    },
    {
        "original": "def remove_config(self, id):\n        \"\"\"\n            Remove a config\n\n            Args:\n                id (string): Full ID of the config to remove\n\n            Returns (boolean): True if successful\n\n            Raises:\n                :py:class:`docker.errors.NotFound`\n                    if no config with that ID exists\n        \"\"\"\n        url = self._url('/configs/{0}', id)\n        res = self._delete(url)\n        self._raise_for_status(res)\n        return True",
        "sampled": "```\nclass DockerConfigManager:\n    def __init__(self):\n        self.configs = {}\n\n    def remove_config(self, id):\n        if id in self.configs:\n            del self.configs[id]\n            return True\n        else:\n            raise docker.errors.NotFound(\"No config with that ID exists\")\n```"
    },
    {
        "original": "def get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    inventory = get_inventory(si)\n    container = inventory.viewManager.CreateContainerView(inventory.rootFolder, [obj_type], True)\n    for item in container.view:\n        if item._moId == obj_moid:\n            return item\n    return None",
        "sampled": "```\ndef get_mor_by_moid(si, obj_type, obj_moid):\n    \"\"\"\n    Get reference to an object of specified object type and id\n\n    si\n        ServiceInstance for the vSphere or ESXi server (see get_service_instance)\n\n    obj_type\n        Type of the object (vim.StoragePod, vim.Datastore, etc)\n\n    obj_moid\n        ID of the object\n    \"\"\"\n    \n```python    \nimport pyVmomi\n    \ndef get_service_instance():\n"
    },
    {
        "original": "def ConfigureUrls(config, external_hostname = None):\n  \"\"\"Guides the user through configuration of various URLs used by GRR.\"\"\"\n  print(\"\\n\\n-=GRR URLs=-\\n\"\n        \"For GRR to work each client has to be able to communicate with the\\n\"\n        \"server. To do this we normally need a public dns name or IP address\\n\"\n        \"to communicate with. In the standard configuration this will be used\\n\"\n        \"to host both the client facing server and the admin user interface.\\n\")\n\n  existing_ui_urn = grr_config.CONFIG.Get(\"AdminUI.url\", default=None)\n  existing_frontend_urns = grr_config.CONFIG.Get(\"Client.server_urls\")\n  if not existing_frontend_urns:\n    # Port from older deprecated setting Client.control_urls.\n    existing_control_urns = grr_config.CONFIG.Get(\n        \"Client.control_urls\", default=None)\n    if existing_control_urns is not None:\n      existing_frontend_urns = []\n      for existing_control_urn in existing_control_urns:\n        if not existing_control_urn.endswith(\"control\"):\n          raise RuntimeError(\"Invalid existing control URL: %s\" %\n                             existing_control_urn)\n\n        existing_frontend_urns.append(\n            existing_control_urn.rsplit(\"/\", 1)[0] + \"/\")\n\n      config.Set(\"Client.server_urls\", existing_frontend_urns)\n      config.Set(\"Client.control_urls\", [\"deprecated use Client.server_urls\"])\n\n  if not existing_frontend_urns or not existing_ui_urn:\n    ConfigureHostnames(config, external_hostname=external_hostname)\n  else:\n    print(\"Found existing settings:\\n  AdminUI URL: %s\\n  \"\n          \"Frontend URL(s): %s\\n\" % (existing_ui_urn, existing_frontend_urns))\n    if not RetryBoolQuestion(\"Do you want to keep this configuration?\", True):\n      ConfigureHostnames(config, external_hostname=external_hostname)",
        "sampled": "```\ndef ConfigureUrls(config, external_hostname = None):\n    if external_hostname is None:\n        external_hostname = input(\"Enter the external hostname: \")\n    config[\"AdminUI.url\"] = f\"http://{external_hostname}:8000\"\n    config[\"Client.frontend_url\"] = f\"http://{external_hostname}:8080\"\n    config[\"ClientPoll.url\"] = f\"http://{external_hostname}:8081\"\n    return config\n```"
    },
    {
        "original": "def Parse(self, cmd, args, stdout, stderr, return_val, time_taken,\n            knowledge_base):\n    \"\"\"Parse the sysctl output.\"\"\"\n    _ = stderr, time_taken, args, knowledge_base  # Unused.\n    self.CheckReturn(cmd, return_val)\n    result = rdf_protodict.AttributedDict()\n    # The KeyValueParser generates an ordered dict by default. The sysctl vals\n    # aren't ordering dependent, but there's no need to un-order it.\n    for k, v in iteritems(self.lexer.ParseToOrderedDict(stdout)):\n      key = k.replace(\".\", \"_\")\n      if len(v) == 1:\n        v = v[0]\n      result[key] = v\n    return [result]",
        "sampled": "```\nclass Parser:\n    def __init__(self):\n        self.knowledge_base = {}\n\n    def Parse(self, cmd, args, stdout, stderr, return_val, time_taken):\n        self.knowledge_base[cmd] = {\n            'args': args,\n            'stdout': stdout,\n            'stderr': stderr,\n            'return_val': return_val,\n            'time_taken': time_taken\n        }\n```"
    },
    {
        "original": "def load_skel(self, file_name):\n\n        \"\"\"\n        Loads an ASF file into a skeleton structure.\n\n        :param file_name: The file name to load in.\n\n         \"\"\"         \n\n        fid = open(file_name, 'r')\n        self.read_skel(fid)\n        fid.close()\n        self.name = file_name",
        "sampled": "```\ndef load_skel(self, file_name):\n    with open(file_name, 'r') as f:\n        content = f.read()\n        # parse ASF content into skeleton structure\n        # TO DO: implement parsing logic\n        pass\n```"
    },
    {
        "original": "def _ruby_installed(ret, ruby, user=None):\n    \"\"\"\n    Check to see if given ruby is installed.\n    \"\"\"\n    default = __salt__['rbenv.default'](runas=user)\n    for version in __salt__['rbenv.versions'](user):\n        if version == ruby:\n            ret['result'] = True\n            ret['comment'] = 'Requested ruby exists'\n            ret['default'] = default == ruby\n            break\n\n    return ret",
        "sampled": "```\ndef _ruby_installed(ret, ruby, user=None):\n    if user is not None:\n        cmd = f\"su {user} -c 'ruby -v | grep {ruby}'\"\n    else:\n        cmd = f\"ruby -v | grep {ruby}\"\n    ret[\"installed\"] = True if os.system(cmd) == 0 else False\n```"
    },
    {
        "original": "def get_projection_on_elements(self, structure):\n        \"\"\"\n        Method returning a dictionary of projections on elements.\n\n        Args:\n            structure (Structure): Input structure.\n\n        Returns:\n            a dictionary in the {Spin.up:[k index][b index][{Element:values}]]\n        \"\"\"\n        dico = {}\n        for spin in self.data.keys():\n            dico[spin] = [[defaultdict(float)\n                           for i in range(self.nkpoints)]\n                          for j in range(self.nbands)]\n\n        for iat in range(self.nions):\n            name = structure.species[iat].symbol\n            for spin, d in self.data.items():\n                for k, b in itertools.product(range(self.nkpoints),\n                                              range(self.nbands)):\n                    dico[spin][b][k][name] = np.sum(d[k, b, iat, :])\n\n        return dico",
        "sampled": "```\nclass Structure:\n    # assuming Structure class is defined somewhere\n    pass\n\nclass Element:\n    # assuming Element class is defined somewhere\n    pass\n\ndef get_projection_on_elements(self, structure):\n    \"\"\"\n    Method returning a dictionary of projections on elements.\n\n    Args:\n        structure (Structure): Input structure.\n\n    Returns:\n        A dictionary in the {Spin.up:[k index][b index][{Element:values}]}\n    \"\"\"\n    \n    projection = {}\n    \n    # iterate over"
    },
    {
        "original": "def vector_args(self, args):\n        \"\"\"\n         Yields each of the individual lane pairs from the arguments, in\n         order from most significan to least significant\n        \"\"\"\n        for i in reversed(range(self._vector_count)):\n            pieces = []\n            for vec in args:\n                pieces.append(vec[(i+1) * self._vector_size - 1 : i * self._vector_size])\n            yield pieces",
        "sampled": "```\nclass VectorArgs:\n    def __init__(self, *args):\n        self.args = args\n\n    def vector_args(self):\n        lanes = self.args.split(',')\n        lane_pairs = [lane.split('..') for lane in lanes]\n        lane_pairs.sort(key=lambda x: int(x[1]), reverse=True)\n        return [(int(pair[0]), int(pair[1])) for pair in lane_pairs]\n\n# Example usage:\nvector_args_instance = VectorArgs('0..10,"
    },
    {
        "original": "def kwargs_from_keyword(from_kwargs,to_kwargs,keyword,clean_origin=True):\n\t\"\"\"\n\tLooks for keys of the format keyword_value. \n\tAnd return a dictionary with {keyword:value} format\n\n\tParameters:\n\t-----------\n\t\tfrom_kwargs : dict\n\t\t\tOriginal dictionary\n\t\tto_kwargs : dict\n\t\t\tDictionary where the items will be appended\n\t\tkeyword : string\n\t\t\tKeyword to look for in the orginal dictionary\n\t\tclean_origin : bool\n\t\t\tIf True then the k,v pairs from the original \n\t\t\tdictionary are deleted\n\t\"\"\"\n\tfor k in list(from_kwargs.keys()):\n\t\tif '{0}_'.format(keyword) in k:\n\t\t\tto_kwargs[k.replace('{0}_'.format(keyword),'')]=from_kwargs[k]\n\t\t\tif clean_origin:\n\t\t\t\tdel from_kwargs[k]\n\treturn to_kwargs",
        "sampled": "```\ndef kwargs_from_keyword(from_kwargs, to_kwargs, keyword, clean_origin=True):\n    for key, value in list(from_kwargs.items()):\n        if key.startswith(keyword + '_'):\n            to_kwargs[key.replace(keyword + '_', '')] = value\n            if clean_origin:\n                del from_kwargs[key]\n    return to_kwargs\n```"
    },
    {
        "original": "def _RunAction(self, rule, client_id):\n    \"\"\"Run all the actions specified in the rule.\n\n    Args:\n      rule: Rule which actions are to be executed.\n      client_id: Id of a client where rule's actions are to be executed.\n\n    Returns:\n      Number of actions started.\n    \"\"\"\n    actions_count = 0\n\n    try:\n      if self._CheckIfHuntTaskWasAssigned(client_id, rule.hunt_id):\n        logging.info(\n            \"Foreman: ignoring hunt %s on client %s: was started \"\n            \"here before\", client_id, rule.hunt_id)\n      else:\n        logging.info(\"Foreman: Starting hunt %s on client %s.\", rule.hunt_id,\n                     client_id)\n\n        # hunt_name is only used for legacy hunts.\n        if rule.hunt_name:\n          flow_cls = registry.AFF4FlowRegistry.FlowClassByName(rule.hunt_name)\n          hunt_urn = rdfvalue.RDFURN(\"aff4:/hunts/%s\" % rule.hunt_id)\n          flow_cls.StartClients(hunt_urn, [client_id])\n        else:\n          hunt.StartHuntFlowOnClient(client_id, rule.hunt_id)\n\n        actions_count += 1\n\n    # There could be all kinds of errors we don't know about when starting the\n    # hunt so we catch everything here.\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception(\"Failure running foreman action on client %s: %s\",\n                        rule.hunt_id, e)\n\n    return actions_count",
        "sampled": "```\nclass ActionExecutor:\n    def __init__(self):\n        self.actions = {}\n\n    def register_action(self, action_name, func):\n        self.actions[action_name] = func\n\n    def _RunAction(self, rule, client_id):\n        count = 0\n        for action in rule['actions']:\n            if action['name'] in self.actions:\n                self.actions[action['name']](client_id)\n                count += 1\n        return count\n```"
    },
    {
        "original": "def df(self):\n        \"\"\"\n        Get data usage information.\n\n        Returns:\n            (dict): A dictionary representing different resource categories\n            and their respective data usage.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n        url = self._url('/system/df')\n        return self._result(self._get(url), True)",
        "sampled": "```\nimport docker\n\nclass DockerClient:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def df(self):\n        try:\n            info = self.client.info()\n            return {\n                'Images': info['Images'],\n                'Containers': info['Containers'],\n                'Volumes': len(self.client.volumes.list()),\n                'Networks': len(self.client.networks.list())\n            }\n        except docker.errors.APIError as e:\n            raise e\n```"
    },
    {
        "original": "def file_extension(self, category=None):\n        \"\"\"\n        :param category: audio|image|office|text|video\n        \"\"\"\n        category = category if category else self.random_element(\n            list(self.file_extensions.keys()))\n        return self.random_element(self.file_extensions[category])",
        "sampled": "```\ndef file_extension(category=None):\n    extensions = {\n        'audio': ['mp3'],\n        'image': ['jpg', 'jpeg', 'png', 'gif'],\n        'office': ['docx', 'pdf', 'pptx'],\n        'text': ['txt', 'doc'],\n        'video': ['mp4']\n    }\n    if category:\n        return extensions.get(category.lower(), [])\n    else:\n        return []\n```"
    },
    {
        "original": "def _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    \"\"\"\n    GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.\n\n    See gti_dirint signature for parameter details.\n    \"\"\"\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370, 'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)\n    disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)\n\n    dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,\n                                          False, temp_dew)\n\n    dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)\n    cos_surface_tilt = tools.cosd(surface_tilt)\n\n    # isotropic sky plus ground diffuse\n    dhi_gte_90 = (\n        (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /\n        (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))\n\n    ghi_gte_90 = dni_gte_90_proj + dhi_gte_90\n\n    return ghi_gte_90, dni_gte_90, dhi_gte_90",
        "sampled": "```\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    import numpy as np\n    \n    # Calculate air mass\n    am = pressure / (101325 * np.exp(-0.000121 * surface_tilt))\n    \n    # Calculate relative air mass\n    mam = am /"
    },
    {
        "original": "def ListChildren(self, urn, limit=None, age=NEWEST_TIME):\n    \"\"\"Lists bunch of directories efficiently.\n\n    Args:\n      urn: Urn to list children.\n      limit: Max number of children to list.\n      age: The age of the items to retrieve. Should be one of ALL_TIMES,\n        NEWEST_TIME or a range.\n\n    Returns:\n      RDFURNs instances of each child.\n    \"\"\"\n    _, children_urns = list(\n        self.MultiListChildren([urn], limit=limit, age=age))[0]\n    return children_urns",
        "sampled": "```\nclass RDFURN:\n    def __init__(self, value):\n        self.value = value\n\nclass Client:\n    def ListChildren(self, urn, limit=None, age=\"NEWEST_TIME\"):\n        # assuming we have a function get_children that returns all children\n        all_children = self.get_children(urn)\n        \n        if age == \"ALL_TIMES\":\n            filtered_children = all_children\n        elif age == \"NEWEST_TIME\":\n            filtered_children = [all_children[0]]"
    },
    {
        "original": "def _convert_validators_to_mapping(validators):\n    \"\"\" convert validators list to mapping.\n\n    Args:\n        validators (list): validators in list\n\n    Returns:\n        dict: validators mapping, use (check, comparator) as key.\n\n    Examples:\n        >>> validators = [\n                {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            ]\n        >>> _convert_validators_to_mapping(validators)\n            {\n                (\"v1\", \"eq\"): {\"check\": \"v1\", \"expect\": 201, \"comparator\": \"eq\"},\n                ('{\"b\": 1}', \"eq\"): {\"check\": {\"b\": 1}, \"expect\": 200, \"comparator\": \"eq\"}\n            }\n\n    \"\"\"\n    validators_mapping = {}\n\n    for validator in validators:\n        if not isinstance(validator[\"check\"], collections.Hashable):\n            check = json.dumps(validator[\"check\"])\n        else:\n            check = validator[\"check\"]\n\n        key = (check, validator[\"comparator\"])\n        validators_mapping[key] = validator\n\n    return validators_mapping",
        "sampled": "```\ndef _convert_validators_to_mapping(validators):\n    validator_mapping = {}\n    for validator in validators:\n        key = (str(validator[\"check\"]), validator[\"comparator\"])\n        validator_mapping[key] = validator\n    return validator_mapping\n```"
    },
    {
        "original": "def InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n  \"\"\"Take a string as a path on a client and interpolate with client data.\n\n  Args:\n    path: A single string/unicode to be interpolated.\n    knowledge_base: An rdf_client.KnowledgeBase object.\n    users: A list of string usernames, or None.\n    path_args: A dict of additional args to use in interpolation. These take\n      precedence over any system provided variables.\n    depth: A counter for recursion depth.\n\n  Returns:\n    A single string if users is None, otherwise a list of strings.\n  \"\"\"\n\n  sys_formatters = {\n      # TODO(user): Collect this during discovery from the registry.\n      # HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\\n      # Value: SystemRoot\n      \"systemroot\": \"c:\\\\Windows\"\n  }\n\n  # Override any system formatters with path_args.\n  if path_args:\n    sys_formatters.update(path_args)\n\n  if users:\n    results = []\n    for user in users:\n      # Extract and interpolate user specific formatters.\n      user = GetUserInfo(knowledge_base, user)\n      if user:\n        formatters = dict((x.name, y) for x, y in user.ListSetFields())\n        formatters.update(sys_formatters)\n        try:\n          results.append(path.format(**formatters))\n        except KeyError:\n          pass  # We may be missing values for some users.\n    return results\n  else:\n    try:\n      path = path.format(**sys_formatters)\n    except KeyError:\n      logging.warning(\"Failed path interpolation on %s\", path)\n      return \"\"\n    if \"{\" in path and depth < 10:\n      path = InterpolatePath(\n          path,\n          knowledge_base=knowledge_base,\n          users=users,\n          path_args=path_args,\n          depth=depth + 1)\n    return path",
        "sampled": "```\ndef InterpolatePath(path, knowledge_base, users=None, path_args=None, depth=0):\n    if path_args is None:\n        path_args = {}\n    if users is None:\n        user_path = knowledge_base.interpolate_string(path, **path_args)\n        return user_path\n    else:\n        result = []\n        for user in users:\n            user_path_args = path_args.copy()\n            user_path_args['user'] = user\n            user_path = knowledge_base.interpolate_string(path"
    },
    {
        "original": "def _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \"\"\"Fast Fourier transform-based Gaussian kernel density estimate (KDE).\n\n    The code was adapted from https://github.com/mfouesneau/faststats\n\n    Parameters\n    ----------\n    x : Numpy array or list\n    cumulative : bool\n        If true, estimate the cdf instead of the pdf\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    xmin : float\n        Manually set lower limit.\n    xmax : float\n        Manually set upper limit.\n\n    Returns\n    -------\n    density: A gridded 1D KDE of the input points (x)\n    xmin: minimum value of x\n    xmax: maximum value of x\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax",
        "sampled": "```\nimport numpy as np\n\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    \n     from scipy.stats import gaussian_kde\n    \n     x = np.array(x)\n     if xmin is None:\n         xmin = np.min(x)\n     if xmax is None:\n         xmax = np.max(x)\n         \n     kde = gaussian_kde(dataset=x[:, np.newaxis], bw_method='silverman')\n     xi = np.linspace(xmin,xmax"
    },
    {
        "original": "def md_options_to_metadata(options):\n    \"\"\"Parse markdown options and return language and metadata\"\"\"\n    metadata = parse_md_code_options(options)\n\n    if metadata:\n        language = metadata[0][0]\n        for lang in _JUPYTER_LANGUAGES + ['julia', 'scheme', 'c++']:\n            if language.lower() == lang.lower():\n                return lang, dict(metadata[1:])\n\n    return None, dict(metadata)",
        "sampled": "```\ndef md_options_to_metadata(options):\n    metadata = {}\n    language = None\n    for option in options:\n        if option.startswith('language:'):\n            language = option.split('language:')[1].strip()\n        elif '=' in option:\n            key, value = option.split('=', 1)\n            metadata[key.strip()] = value.strip()\n    return language, metadata\n```"
    },
    {
        "original": "def frames(\n        self,\n        *,\n        callers: Optional[Union[str, List[str]]] = None,\n        callees: Optional[Union[str, List[str]]] = None,\n        kind: Optional[TraceKind] = None,\n        limit: Optional[int] = 10,\n    ):\n        \"\"\"Display trace frames independent of the current issue.\n\n        Parameters (all optional):\n            callers: str or list[str]            filter traces by this caller name\n            callees: str or list[str]            filter traces by this callee name\n            kind: precondition|postcondition    the type of trace frames to show\n            limit: int (default: 10)            how many trace frames to display\n                                                (specify limit=None for all)\n\n        Sample usage:\n            frames callers=\"module.function\", kind=postcondition\n\n        String filters support LIKE wildcards (%, _) from SQL:\n            % matches anything (like .* in regex)\n            _ matches 1 character (like . in regex)\n        \"\"\"\n        with self.db.make_session() as session:\n            query = (\n                session.query(\n                    TraceFrame.id,\n                    CallerText.contents.label(\"caller\"),\n                    TraceFrame.caller_port,\n                    CalleeText.contents.label(\"callee\"),\n                    TraceFrame.callee_port,\n                )\n                .filter(TraceFrame.run_id == self.current_run_id)\n                .join(CallerText, CallerText.id == TraceFrame.caller_id)\n                .join(CalleeText, CalleeText.id == TraceFrame.callee_id)\n            )\n\n            if callers is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callers, query, CallerText.contents, \"callers\"\n                )\n\n            if callees is not None:\n                query = self._add_list_or_string_filter_to_query(\n                    callees, query, CalleeText.contents, \"callees\"\n                )\n\n            if kind is not None:\n                if kind not in {TraceKind.PRECONDITION, TraceKind.POSTCONDITION}:\n                    raise UserError(\n                        \"Try 'frames kind=postcondition'\"\n                        \" or 'frames kind=precondition'.\"\n                    )\n                query = query.filter(TraceFrame.kind == kind)\n\n            if limit is not None and not isinstance(limit, int):\n                raise UserError(\"'limit' should be an int or None.\")\n\n            trace_frames = query.group_by(TraceFrame.id).order_by(\n                CallerText.contents, CalleeText.contents\n            )\n\n            total_trace_frames = trace_frames.count()\n            limit = limit or total_trace_frames\n\n            self._output_trace_frames(\n                self._group_trace_frames(trace_frames, limit), limit, total_trace_frames\n            )",
        "sampled": "```\nfrom typing import Optional, Union, List\nfrom enum import Enum\n\nclass TraceKind(Enum):\n    PRECONDITION = 1\n    POSTCONDITION = 2\n\nclass TraceFrame:\n    def __init__(self, caller: str, callee: str, kind: TraceKind):\n        self.caller = caller\n        self.callee = callee\n        self.kind = kind\n\nclass Trace:\n    def __init__(self):\n        self.frames = []\n\n    def add_frame(self,"
    },
    {
        "original": "def new_batch(self):\n        \"\"\"\n        Return a new batch of X and Y by taking a chunk of data from the complete X and Y\n        \"\"\"\n        i = next(self.slicer)\n        return self.X_all[i], self.Y_all[i]",
        "sampled": "```\nclass BatchGenerator:\n    def __init__(self, X, Y, batch_size):\n        self.X = X\n        self.Y = Y\n        self.batch_size = batch_size\n        self.index = 0\n\n    def new_batch(self):\n        start = self.index\n        end = min(start + self.batch_size, len(self.X))\n        \n        batch_X = self.X[start:end]\n        batch_Y = self.Y[start:end]\n        \n        self.index += self.batch_size"
    },
    {
        "original": "def createSummary(self, log):\n        \"\"\"\n        Create nice summary logs.\n\n        @param log: log to create summary off of.\n        \"\"\"\n        warnings = self.obs.warnings\n        errors = self.obs.errors\n\n        if warnings:\n            self.addCompleteLog('%d Warnings' % len(warnings), \"\\n\".join(warnings))\n            self.warnCount = len(warnings)\n        if errors:\n            self.addCompleteLog('%d Errors' % len(errors), \"\\n\".join(errors))\n            self.errCount = len(errors)",
        "sampled": "```\ndef createSummary(self, log):\n    \"\"\"\n    Create nice summary logs.\n\n    @param log: log to create summary off of.\n    \"\"\"\n    result = {}\n    for entry in log:\n        key = entry['type']\n        if key not in result:\n            result[key] = 1\n        else:\n            result[key] += 1\n    return result\n```"
    },
    {
        "original": "def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        \"\"\"Creates a branch using the information provided in the request.\n\n        The authenticated user must have REPO_WRITE permission for the context repository to call this resource.\n\n        :param project_key: The project matching the projectKey supplied in the resource path as shown in URL.\n        :type project_key: str\n        :param repository: Name of repository where branch is created (i.e. \"my_repo\").\n        :type repository: str\n        :param name: Name of branch to create (i.e. \"my_branch\").\n        :type name: str\n        :param start_point: Name of branch to branch from.\n        :type start_point: str\n        :param message: Branch message.\n        :type message: str\n        :return:\n            200 - application/json (repository)\n            401 - application/json (errors)\n            404 - application/json (errors)\n        :rtype: requests.Response\n        \"\"\"\n\n        url = 'rest/api/1.0/projects/{projectKey}/repos/{repository}/branches'.format(projectKey=project_key,\n                                                                                      repository=repository)\n        data = {\n            \"name\": name,\n            \"startPoint\": start_point,\n            \"message\": message\n        }\n        return self.post(url, data=data)",
        "sampled": "```\nimport requests\n\nclass GitRepository:\n    def __init__(self):\n        self.url = \"https://api.bitbucket.org/2.0\"\n    \n    def create_branch(self, project_key, repository, name, start_point, message=\"\"):\n        auth_token = (\"your_username\", \"your_password\")\n        \n\t\theaders = {\n\t\t\t\"Content-Type\": \"application/json\"\n\t\t}\n\t\t\n\t\tdata = {\n\t\t\t\"name\": name,\n\t\t\t\"start\": {\n\t\t\t"
    },
    {
        "original": "def merge_option_dicts(old_opts, new_opts):\n    \"\"\"\n    Update the old_opts option dictionary with the options defined in\n    new_opts. Instead of a shallow update as would be performed by calling\n    old_opts.update(new_opts), this updates the dictionaries of all option\n    types separately.\n\n    Given two dictionaries\n        old_opts = {'a': {'x': 'old', 'y': 'old'}}\n    and\n        new_opts = {'a': {'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    this returns a dictionary\n        {'a': {'x': 'old', 'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    \"\"\"\n    merged = dict(old_opts)\n\n    for option_type, options in new_opts.items():\n        if option_type not in merged:\n            merged[option_type] = {}\n\n        merged[option_type].update(options)\n\n    return merged",
        "sampled": "```python\ndef merge_option_dicts(old_opts, new_opts):\n    merged = {}\n    all_keys = set(list(old_OPTS.keys()) + list(new_OPTS.keys()))\n    \n    for key in all_keys:\n        merged[key] = {}\n        \n        if key in old_OPTS:\n            merged[key].update(old_OPTS[key])\n        \n        if key in new_OPTS:\n            merged[key].update(newOpts[key])\n    \n    return merged\n\n# Test case\nold.opts = {'a':{'x':'old','"
    },
    {
        "original": "def get(self, file_path, ref, **kwargs):\n        \"\"\"Retrieve a single file.\n\n        Args:\n            file_path (str): Path of the file to retrieve\n            ref (str): Name of the branch, tag or commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            object: The generated RESTObject\n        \"\"\"\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)",
        "sampled": "```python\nimport requests\nfrom gitlab.exceptions import GitlabAuthenticationError, GitlabGetError\n\nclass RESTObject:\n    pass\n\nclass GitLabManager:\n    def __init__(self, url, token):\n        self.url = url\n        self.token = token\n\n    def get(self, file_path, ref, **kwargs):\n        headers = {'Authorization': f'Bearer {self.token}'}\n        params = {'ref_name': ref}\n        \n        if 'sudo' in kwargs"
    },
    {
        "original": "def clean_recipe_build(self, args):\n        \"\"\"Deletes the build files of the given recipe.\n\n        This is intended for debug purposes. You may experience\n        strange behaviour or problems with some recipes if their\n        build has made unexpected state changes. If this happens, run\n        clean_builds, or attempt to clean other recipes until things\n        work again.\n        \"\"\"\n        recipe = Recipe.get_recipe(args.recipe, self.ctx)\n        info('Cleaning build for {} recipe.'.format(recipe.name))\n        recipe.clean_build()\n        if not args.no_clean_dists:\n            self.clean_dists(args)",
        "sampled": "```\ndef clean_recipe_build(self, args):\n    import os\n    import shutil\n    recipe_build_dir = os.path.join('build', args.recipe_name)\n    if os.path.exists(recipe_build_dir):\n        shutil.rmtree(recipe_build_dir)\n    print(f\"Deleted build files for recipe {args.recipe_name}\")\n```"
    },
    {
        "original": "def calculate_bias_shape(input_shape, bias_dims):\n  \"\"\"Calculate `bias_shape` based on the `input_shape` and `bias_dims`.\n\n  Args:\n    input_shape: Shape of the input being passed into the module. The leading\n        dimension is the minibatch size.\n    bias_dims: The dimensions that bias should be applied over. The remaining\n        dimensions will get broadcasted over.\n\n  Returns:\n    bias_shape: Tuple corresponding to the shape of bias Variable to create.\n\n  Raises:\n    ValueError: If the user attempts to add bias over the minibatch dimension,\n        e.g. `bias_dims=[0]`.\n  \"\"\"\n  input_rank = len(input_shape)\n  # If None, default is to use all dimensions.\n  if bias_dims is None:\n    return input_shape[1:]\n  # If empty list, use a scalar bias.\n  elif not bias_dims:\n    return ()\n  # Otherwise, calculate bias_shape from bias_dims.\n  else:\n    bias_shape = [1] * input_rank\n    # Populate bias dimensions.\n    for dim in bias_dims:\n      dim %= input_rank\n      if dim == 0:\n        raise ValueError(\"Cannot apply bias across the minibatch dimension.\")\n      bias_shape[dim] = input_shape[dim]\n    # Strip leading unit dimensions.\n    start = input_rank\n    for dim in xrange(1, input_rank):\n      if bias_shape[dim] != 1:\n        start = dim\n        break\n    return tuple(bias_shape[start:])",
        "sampled": "```\ndef calculate_bias_shape(input_shape, bias_dims):\n    if 0 in bias_dims:\n        raise ValueError(\"Cannot add bias over the minibatch dimension\")\n    bias_shape = [input_shape[i] if i in bias_dims else 1 for i in range(len(input_shape))]\n    return tuple(bias_shape)\n```"
    },
    {
        "original": "def read_metadata(text, ext):\n    \"\"\"Return the header metadata\"\"\"\n    ext = '.' + ext.split('.')[-1]\n    lines = text.splitlines()\n\n    if ext in ['.md', '.Rmd']:\n        comment = ''\n    else:\n        comment = _SCRIPT_EXTENSIONS.get(ext, {}).get('comment', '#')\n\n    metadata, _, _, _ = header_to_metadata_and_cell(lines, comment, ext)\n    if ext in ['.r', '.R'] and not metadata:\n        metadata, _, _, _ = header_to_metadata_and_cell(lines, \"#'\", ext)\n\n    return metadata",
        "sampled": "```\ndef read_metadata(text, ext):\n    if ext == 'txt':\n        lines = text.split('\\n')\n        metadata = {}\n        for line in lines:\n            if ':' in line:\n                key, value = line.split(':', 1)\n                metadata[key.strip()] = value.strip()\n        return metadata\n    else:\n        return {}\n```"
    },
    {
        "original": "def _is_process_filtered(self, process, key=None):\n        \"\"\"Return True if the process[key] should be filtered according to the current filter\"\"\"\n        if key is None:\n            key = self.filter_key\n        try:\n            # If the item process[key] is a list, convert it to a string\n            # in order to match it with the current regular expression\n            if isinstance(process[key], list):\n                value = ' '.join(process[key])\n            else:\n                value = process[key]\n        except KeyError:\n            # If the key did not exist\n            return False\n        try:\n            return self._filter_re.match(value) is None\n        except (AttributeError, TypeError):\n            # AttributeError\n            # Filter processes crashs with a bad regular expression pattern (issue #665)\n            # TypeError\n            # Filter processes crashs if value is None (issue #1105)\n            return False",
        "sampled": "```\ndef _is_process_filtered(self, process, key=None):\n    \"\"\"\n    Return True if the process[key] should be filtered according to the current filter\n    \"\"\"\n    # Assuming self.filter is set elsewhere in your class\n    if not hasattr(self, 'filter'):\n        raise ValueError(\"Filter is not defined\")\n\n    if key is None:\n        return any(str(d) in str(self.filter) for d in process.values())\n    else:\n        return str(process.get(key)) in str"
    },
    {
        "original": "def modulation_type(self, value: int):\n        \"\"\"\n        0 - \"ASK\", 1 - \"FSK\", 2 - \"PSK\", 3 - \"APSK (QAM)\"\n\n        :param value:\n        :return:\n        \"\"\"\n        if self.__modulation_type != value:\n            self.__modulation_type = value\n            self._qad = None\n\n            self.modulation_type_changed.emit(self.__modulation_type)\n            if not self.block_protocol_update:\n                self.protocol_needs_update.emit()",
        "sampled": "```\ndef modulation_type(self, value: int):\n    modulation_types = [\"ASK\", \"FSK\", \"PSK\", \"APSK (QAM)\"]\n    if 0 <= value <= 3:\n        return modulation_types[value]\n    else:\n        return None\n```"
    },
    {
        "original": "def load(self, fname):\n        \"\"\"\n        Load signatures from a file.\n\n        @fname - Path to signature file.\n\n        Returns None.\n        \"\"\"\n        # Magic files must be ASCII, else encoding issues can arise.\n        fp = open(fname, \"r\")\n        lines = fp.readlines()\n        self.parse(lines)\n        fp.close()",
        "sampled": "```\ndef load(self, fname):\n    with open(fname, 'r') as f:\n        self.signatures = [line.strip() for line in f.readlines()]\n```"
    },
    {
        "original": "def enter_room(self, sid, room, namespace=None):\n        \"\"\"Enter a room.\n\n        The only difference with the :func:`socketio.Server.enter_room` method\n        is that when the ``namespace`` argument is not given the namespace\n        associated with the class is used.\n        \"\"\"\n        return self.server.enter_room(sid, room,\n                                      namespace=namespace or self.namespace)",
        "sampled": "```python\nclass Server:\n    def __init__(self):\n        self.rooms = {}\n\n    def enter_room(self, sid, room, namespace=None):\n        if namespace is None:\n            namespace = self.__class__.__name__\n        if room not in self.rooms:\n            self.rooms[room] = set()\n        self.rooms[room].add((sid, namespace))\n```"
    },
    {
        "original": "def _get_args(self, kwargs):\n        \"\"\"\n        Discard all keywords which aren't function-specific from the kwargs.\n\n        :param kwargs:\n        :return:\n        \"\"\"\n        _args = list()\n        _kwargs = salt.utils.args.clean_kwargs(**kwargs)\n\n        return _args, _kwargs",
        "sampled": "```\ndef _get_args(self, **):\n    return {key: value for key, value in kwargs.items() if hasattr(self.__class__, key)}\n```"
    },
    {
        "original": "def edit_caption(\n        self,\n        caption: str,\n        parse_mode: str = \"\",\n        reply_markup: Union[\n            \"pyrogram.InlineKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardMarkup\",\n            \"pyrogram.ReplyKeyboardRemove\",\n            \"pyrogram.ForceReply\"\n        ] = None\n    ) -> \"Message\":\n        \"\"\"Bound method *edit_caption* of :obj:`Message <pyrogram.Message>`\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.edit_message_caption(\n                chat_id=message.chat.id,\n                message_id=message.message_id,\n                caption=\"hello\"\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.edit_caption(\"hello\")\n\n        Args:\n            caption (``str``):\n                New caption of the message.\n\n            parse_mode (``str``, *optional*):\n                Use :obj:`MARKDOWN <pyrogram.ParseMode.MARKDOWN>` or :obj:`HTML <pyrogram.ParseMode.HTML>`\n                if you want Telegram apps to show bold, italic, fixed-width text or inline URLs in your message.\n                Defaults to Markdown.\n\n            reply_markup (:obj:`InlineKeyboardMarkup`, *optional*):\n                An InlineKeyboardMarkup object.\n\n        Returns:\n            On success, the edited :obj:`Message <pyrogram.Message>` is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n        return self._client.edit_message_caption(\n            chat_id=self.chat.id,\n            message_id=self.message_id,\n            caption=caption,\n            parse_mode=parse_mode,\n            reply_markup=reply_markup\n        )",
        "sampled": "```\nfrom pyogram import Client\nfrom pyogram.types import Message, InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply\nfrom typing import Union\n\nclass MyClient(Client):\n    def edit_caption(self, caption: str, parse_mode: str = \"\", reply_markup: Union[InlineKeyboardMarkup, ReplyKeyboardMarkup, ReplyKeyboardRemove, ForceReply] = None) -> Message:\n        return self.edit_message_caption(chat_id=self.chat.id, message_id=self.message_id,c"
    },
    {
        "original": "def _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n        \"\"\"Given M = sum(kron(a_i, b_i)), returns M' = sum(kron(b_i, a_i)).\"\"\"\n        result = np.array([[0] * 4] * 4, dtype=np.complex128)\n        order = [0, 2, 1, 3]\n        for i in range(4):\n            for j in range(4):\n                result[order[i], order[j]] = mat4x4[i, j]\n        return result",
        "sampled": "```\nimport numpy as np\nfrom scipy.linalg import kron\n\ndef _flip_kron_order(mat4x4: np.ndarray) -> np.ndarray:\n    # Get the size of sub-matrices\n    n = int(np.sqrt(mat4x4.shape[0]))\n\n    # Reshape mat4x4 into 3D array (n*n matrices)\n    mats = mat4x4.reshape(n, n, n, n)\n\n    # Flip kron order by swapping last two axes"
    },
    {
        "original": "def pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n        \"\"\"\n        Prints scheduler for user to read.\n        \"\"\"\n        print(\"=========================================\")\n        print(\"|           Hyperband Schedule          |\")\n        print(\"=========================================\")\n        if describe_hyperband:\n            # Print a message indicating what the below schedule means\n            print(\n                \"Table consists of tuples of \"\n                \"(num configs, num_resources_per_config) \"\n                \"which specify how many configs to run and \"\n                \"for how many epochs. \"\n            )\n            print(\n                \"Each bracket starts with a list of random \"\n                \"configurations which is successively halved \"\n                \"according the schedule.\"\n            )\n            print(\n                \"See the Hyperband paper \"\n                \"(https://arxiv.org/pdf/1603.06560.pdf) for more details.\"\n            )\n            print(\"-----------------------------------------\")\n        for bracket_index, bracket in enumerate(hyperband_schedule):\n            bracket_string = \"Bracket %d:\" % bracket_index\n            for n_i, r_i in bracket:\n                bracket_string += \" (%d, %d)\" % (n_i, r_i)\n            print(bracket_string)\n        print(\"-----------------------------------------\")",
        "sampled": "```\ndef pretty_print_schedule(self, hyperband_schedule, describe_hyperband=True):\n    print(\" Hyper-Band Parameters: \")\n    print(\"     - Try {} possible sets of hyperparameters\".format(len(hyperband_schedule)))\n    if describe_hyperband:\n        print(\"     - With an average of {} iterations per trial\".format(sum([len(trial) for trial in hyperband_schedule]) / len(hyperband_schedule)))\n    print(\"\")\n    \n    max_r = max(max(trial)"
    },
    {
        "original": "def shorthand(self):\n        \"\"\"Return the 6-tuple (a,b,c,d,e,f) that describes this matrix\"\"\"\n        return (self.a, self.b, self.c, self.d, self.e, self.f)",
        "sampled": "```\nclass Matrix:\n    def __init__(self, a, b, c, d, e, f):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        self.e = e\n        self.f = f\n\n    def shorthand(self):\n        return (self.a, self.b, self.c, self.d, self.e, self.f)\n```"
    },
    {
        "original": "def is_cyclic(graph):\n    \"\"\"\n    Return True if the directed graph g has a cycle. The directed graph\n    should be represented as a dictionary mapping of edges for each node.\n    \"\"\"\n    path = set()\n\n    def visit(vertex):\n        path.add(vertex)\n        for neighbour in graph.get(vertex, ()):\n            if neighbour in path or visit(neighbour):\n                return True\n        path.remove(vertex)\n        return False\n\n    return any(visit(v) for v in graph)",
        "sampled": "```python\ndef is_cyclic(graph):\n    visited = set()\n    rec_stack = set()\n\n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n\n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n\n        rec_stack.remove(node)\n        return False\n\n    for node in graph:\n        if node not in visited:\n            if dfs"
    },
    {
        "original": "def get_header_items(self):\n        \"\"\"\n        Get an iterable list of key/value pairs representing headers.\n\n        This function provides Python 2/3 compatibility as related to the\n        parsing of request headers. Python 2.7 is not compliant with\n        RFC 3875 Section 4.1.18 which requires multiple values for headers\n        to be provided. This function will return a matching list regardless\n        of Python version. It can be removed once Python 2.7 support\n        is dropped.\n\n        :return: List of tuples containing header hey/value pairs\n        \"\"\"\n        if PY2:\n            # For Python 2, process the headers manually according to\n            # W3C RFC 2616 Section 4.2.\n            items = []\n            for header in self.headers.headers:\n                # Remove \"\\n\\r\" from the header and split on \":\" to get\n                # the field name and value.\n                key, value = header[0:-2].split(\":\", 1)\n                # Add the key and the value once stripped of leading\n                # white space. The specification allows for stripping\n                # trailing white space but the Python 3 code does not\n                # strip trailing white space. Therefore, trailing space\n                # will be left as is to match the Python 3 behavior.\n                items.append((key, value.lstrip()))\n        else:\n            items = self.headers.items()\n\n        return items",
        "sampled": "```python\nfrom email import policy\nfrom email.parser import BytesParser\n\ndef get_header_items(self):\n    \"\"\"Get an iterable list of key/value pairs representing headers.\"\"\"\n    parser = BytesParser(policy=policy.default)\n    msg = parser.parsestr(self.request_text)\n    return [(k, v) for k, v in msg.items()]\n```"
    }
]
