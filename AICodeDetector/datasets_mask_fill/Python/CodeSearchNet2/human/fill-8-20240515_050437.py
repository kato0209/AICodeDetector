x_train = x_train.astype("float32") x_test = x_test.astype("float32") x_train /= 255 x_test /= 255 y_train = y_train.flatten() y_test = y_test.flatten() if FLAGS.subtract_pixel_mean: x_train_mean = np.mean(x_train, axis=0) x_train -= x_train_mean x_test -= x_train_mean print("x_train shape:" + str(x_train.shape)) print(str(x_train.shape[0]) + " train samples") print(str(x_test.shape[0]) + " test samples") # Build an iterator over training batches. training_dataset = tf.data.Dataset.from_tensor_slices( (x_train, np.int32(y_train))) training_batches = training_dataset.shuffle( 50000, reshuffle_each_iteration=True).repeat().batch(batch_size) training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches) # Build a constant-size TF dataset over the training batches.

# Build a batch-based tf1 dataset iterator. This should be the same for all datasets.
y_train_batches = tf.data.Dataset.from_tensor_slices( (x_train, np.int32(y_train))) y_train = y_train_batches.shuffle( 50000, reshuffle_each_iteration=True) y_test = y_test.shuffle( 50000, reshuffle_each_iteration=True) that is, with batch_size=heldout_size, # i.e., training the entire iterator set as a constant. heldout_dataset = tf.data.Dataset.from_tensor_slices(