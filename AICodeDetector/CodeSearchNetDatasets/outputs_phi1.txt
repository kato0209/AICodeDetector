{"prompt": "def print_help(self, classes=False):\n        \"\"\"Print the help for each Configurable class in self.classes.\n\n        If classes=False (the default), only flags and aliases are printed.\n        \"\"\"", "output": "\n        for c in self.classes:\n            print(c.help)\n\n    def add_class(self, cls):\n        \"\"\"Add a new class to the list of Configurable classes.\n\n        cls is the new class to add to the list.\n        \"\"\"\n        self.classes.append(cls)\n\n    def remove_class(self, cls):\n        \"\"\"Remove a class from the list of Configurable classes.\n\n        cls is the class to remove from the list.\n        \"\"\"\n        self.classes.remove(cls)\n\n    def", "solution": "\n        self.print_subcommands()\n        self.print_options()\n\n        if classes:\n            if self.classes:\n                print \"Class parameters\"\n                print \"----------------\"\n                print\n                for p in wrap_paragraphs(self.keyvalue_description):\n                    print p\n                    print\n\n            for cls in self.classes:\n                cls.class_print_help()\n                print\n        else:\n            print \"To see all available configurables, use `--help-all`\"\n            print"}
{"prompt": "def derive_fallback_fernet_keys(passwords, user_id):\n    \"\"\"\n    Derive a list of per-user Fernet keys from a list of master keys and a\n    username.\n\n    If a None is encountered in ``passwords``, it is forwarded.\n\n    Parameters\n    ----------\n    passwords : list[unicode]\n        List of ascii-encodable keys to derive.\n    user_id : unicode or None\n        ascii-encodable user_id to use as salt\n    \"\"\"", "output": "\n    keys = []\n    for password in passwords:\n        if password is None:\n            keys.append(None)\n        else:\n            key = derive_fernet_key(password, user_id)\n            keys.append(key)\n    return keys\n\n\ndef derive_fernet_key(password, user_id):\n    \"\"\"\n    Derive a Fernet key from a password and a user_id.\n\n    Parameters\n    ----------\n    password : unicode\n        ascii-encodable password to use as key\n    user_", "solution": "\n    # Normally I wouldn't advocate for these kinds of assertions, but we really\n    # really really don't want to mess up deriving encryption keys.\n    assert isinstance(passwords, (list, tuple)), \\\n        \"Expected list or tuple of keys, got %s.\" % type(passwords)\n\n    def derive_single_allow_none(k):\n        if k is None:\n            return None\n        return derive_single_fernet_key(k, user_id).decode('ascii')\n\n    return list(map(derive_single_allow_none, passwords))"}
{"prompt": "def format_help(self, checkerref=False):\n        \"\"\"return the help string for the given message id\"\"\"", "output": "\n        if checkerref:\n            return f\"Help message with id {self.id}\"\n        else:\n            return f\"Help message for message with id {self.id}\"\n\n", "solution": "\n        desc = self.descr\n        if checkerref:\n            desc += \" This message belongs to the %s checker.\" % self.checker.name\n        title = self.msg\n        if self.symbol:\n            msgid = \"%s (%s)\" % (self.symbol, self.msgid)\n        else:\n            msgid = self.msgid\n        if self.minversion or self.maxversion:\n            restr = []\n            if self.minversion:\n                restr.append(\"< %s\" % \".\".join([str(n) for n in self.minversion]))\n            if self.maxversion:\n                restr.append(\">= %s\" % \".\".join([str(n) for n in self.maxversion]))\n            restr = \" or \".join(restr)\n            if checkerref:\n                desc += \" It can't be emitted when using Python %s.\" % restr\n            else:\n                desc += \" This message can't be emitted when using Python %s.\" % restr\n        desc = normalize_text(\" \".join(desc.split()), indent=\"  \")\n        if title != \"%s\":\n            title = title.splitlines()[0]\n\n            return \":%s: *%s*\\n%s\" % (msgid, title.rstrip(\" \"), desc)\n        return \":%s:\\n%s\" % (msgid, desc)"}
{"prompt": "def f_get(self, *args):\n        \"\"\"Returns annotations\n\n        If len(args)>1, then returns a list of annotations.\n\n        `f_get(X)` with *X* integer will return the annotation with name `annotation_X`.\n\n        If the annotation contains only a single entry you can call `f_get()` without arguments.\n        If you call `f_get()` and the annotation contains more than one element a ValueError is\n        thrown.\n\n        \"\"\"", "output": "\n        if len(args) == 1:\n            return self.annotations[args[0]]\n        else:\n            return [self.annotations[i] for i in args]\n\n    def f_set(self, *args):\n        \"\"\"Sets annotations\n\n        If len(args)>1, then sets the annotations with names `annotation_X` where `X` is the\n        first element of `args`.\n\n        If the annotation contains only a single entry you can call `f_set()` without arguments.\n        If you call `f_set()", "solution": "\n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (str(list(self._dict.keys()))))\n            else:\n                raise AttributeError('Your annotation is empty, cannot access data.')\n\n        result_list = []\n        for name in args:\n            name = self._translate_key(name)\n            try:\n                result_list.append(self._dict[name])\n            except KeyError:\n                raise AttributeError('Your annotation does not contain %s.' % name)\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return tuple(result_list)"}
{"prompt": "def size(full_path):\r\n    \"\"\"Shows file size.\r\n    :param full_path: full path to the file.\r\n    \"\"\"", "output": "\r\n    import os.path, os\r\n    return os.path.getsize(full_path) / 1024  # convert bytes to kilobytes\r\n\r\ndef get_files_size(files):\r\n    \"\"\"Returns the total size of all files in bytes.\r\n    :param files: list of file names.\r\n    \"\"\"\r\n    total_size = 0\r\n    for file in files:\r\n        total_size += size(file)\r\n    return total_size / 1024  # convert bytes to kilobytes\r\n\r\ndef get_", "solution": "\r\n\r\n    file_size = os.path.getsize(full_path)\r\n    str_file_size = str(file_size)\r\n    print(str_file_size, 'b')\r\n\r\n    # Show size in b, kb, mb or gb depending on the dimension\r\n    if len(str_file_size) >= 10:\r\n        print('{0:.2f}'.format(file_size / 1073741824), 'gb')\r\n    elif len(str_file_size) >= 7:\r\n        print('{0:.2f}'.format(file_size / 1048576), 'mb')\r\n    elif len(str_file_size) >= 4:\r\n        print('{0:.2f}'.format(file_size / 1024), 'kb')"}
{"prompt": "def init_cinder_consumer(self, mq):\n        \"\"\"\n        Init openstack cinder mq\n\n        1. Check if enable listening cinder notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"", "output": "\n        self.mq = mq\n        self.consumer = Consumer(\n            {\n                'enable_listening': True,\n                'auto_offset_reset': 'earliest',\n                'enable_auto_commit': True,\n                'enable_request_rejection': False\n            },\n            on_message=self.on_message\n        )\n\n    def on_message(self, message):\n        \"\"\"\n        Callback function for message\n\n        1. Check if enable listening cinder notification\n        2. Create consumer\n\n        :param message: class terny", "solution": "\n        if not self.enable_component_notification(Openstack.Cinder):\n            log.debug(\"disable listening cinder notification\")\n            return\n\n        for i in range(self.config.cinder_mq_consumer_count):\n            mq.create_consumer(self.config.cinder_mq_exchange,\n                               self.config.cinder_mq_queue,\n                               ProcessFactory.process(Openstack.Cinder))\n\n        log.debug(\"enable listening openstack cinder notification.\")"}
{"prompt": "def max_global_iteration(self):\n        \"\"\"Return global iterator with last iteration number\"\"\"", "output": "\n        return max(self.iterations)\n\n", "solution": "\n        return self.indices_to_global_iterator({\n            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack\n        })"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        return self.args\n\n    def get_help(self):\n        \"\"\"\n        Extracts the help message for this CLI\n        \"\"\"\n        return self.help\n\n    def get_version(self):\n        \"\"\"\n        Extracts the version number of this CLI\n        \"\"\"\n        return self.version\n\n", "solution": "\n        ApiCli.get_arguments(self)\n\n        # Get the host group name\n        if self.args.host_group_name is not None:\n            self.host_group_name = self.args.host_group_name\n\n        # Get the list of sources separated by commas\n        if self.args.sources is not None:\n            self.sources = self.args.sources\n\n        payload = {}\n        if self.host_group_name is not None:\n            payload['name'] = self.host_group_name\n\n        if self.sources is not None:\n            source_list = str.split(self.sources, ',')\n            if 'hostnames' not in payload:\n                payload['hostnames'] = []\n\n            for s in source_list:\n                payload['hostnames'].append(s)\n        self.data = json.dumps(payload, sort_keys=True)\n        self.headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}"}
{"prompt": "def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"", "output": "\n        if not self.retry_count:\n            # First task, set retry count to 1\n            self.retry_count = 1\n            self.retry_delay = self.base_delay\n        else:\n            # Increment retry count and set delay\n            self.retry_count += 1\n            self.retry_delay *= 2\n        # Return datetime of next attempt\n        return self.retry_datetime + timedelta(seconds=self.retry_delay)\n\n", "solution": "\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay"}
{"prompt": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"", "output": "\n        code = self.code\n        words = code.split()\n        lines = []\n        current_line = \"\"\n        for word in words:\n            if len(current_line + word) + 1 <= width:\n                current_line += word + \" \"\n            else:\n                lines.append(current_line.strip())\n                current_line = word + \" \"\n        lines.append(current_line.strip())\n        for line in lines:\n            print(line)\n\n", "solution": "\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())"}
{"prompt": "def normalize(ast: Node) -> Node:\n    \"\"\"\n    Normalize an AST nodes.\n\n    all builtins containers are replace by referencable subclasses\n    \"\"\"", "output": "\n    if isinstance(ast, list):\n        return [normalize(item) for item in ast]\n    elif isinstance(ast, dict):\n        return {normalize(key): normalize(value) for key, value in ast.items()}\n    elif isinstance(ast, str):\n        if ast.isnumeric():\n            return ast.int()\n        elif ast.isalpha():\n            return ast.str()\n        elif ast.isalnum():\n            return ast.float()\n        else:\n            return ast\n    ", "solution": "\n    res = ast\n    typemap = {DictNode, ListNode, TupleNode}\n    if type(ast) is dict:\n        res = DictNode(ast)\n    elif type(ast) is list:\n        res = ListNode(ast)\n    elif type(ast) is tuple:\n        res = TupleNode(ast)\n    # in-depth change\n    if hasattr(res, 'items'):\n        for k, v in res.items():\n            res[k] = normalize(v)\n    elif hasattr(res, '__getitem__'):\n        for idx, v in zip(range(len(res)), res):\n            res[idx] = normalize(v)\n    if type(res) not in typemap and hasattr(res, '__dict__'):\n        subattr = vars(res)\n        for k, v in subattr.items():\n            setattr(res, k, normalize(v))\n    return res"}
{"prompt": "def get_memory_maps(self):\n        \"\"\"Return process's mapped memory regions as a list of nameduples.\n        Fields are explained in 'man proc'; here is an updated (Apr 2012)\n        version: http://goo.gl/fmebo\n        \"\"\"", "output": "\n        fields = ['start', 'end', 'pid', 'comm','status', 'name','memory_map']\n        memory_maps = []\n        for line in self.output.split('\\n'):\n            if not line:\n                continue\n            fields_in_line = line.split()\n            if len(fields_in_line)!= len(fields):\n                continue\n            memory_map = namedtuple('MemoryMap', fields)\n            memory_map.start = int(fields_in_line[0])\n            memory_map.", "solution": "\n        f = None\n        try:\n            f = open(\"/proc/%s/smaps\" % self.pid)\n            first_line = f.readline()\n            current_block = [first_line]\n\n            def get_blocks():\n                data = {}\n                for line in f:\n                    fields = line.split(None, 5)\n                    if len(fields) >= 5:\n                        yield (current_block.pop(), data)\n                        current_block.append(line)\n                    else:\n                        data[fields[0]] = int(fields[1]) * 1024\n                yield (current_block.pop(), data)\n\n            if first_line:  # smaps file can be empty\n                for header, data in get_blocks():\n                    hfields = header.split(None, 5)\n                    try:\n                        addr, perms, offset, dev, inode, path = hfields\n                    except ValueError:\n                        addr, perms, offset, dev, inode, path = hfields + ['']\n                    if not path:\n                        path = '[anon]'\n                    else:\n                        path = path.strip()\n                    yield (addr, perms, path,\n                           data['Rss:'],\n                           data['Size:'],\n                           data.get('Pss:', 0),\n                           data['Shared_Clean:'], data['Shared_Clean:'],\n                           data['Private_Clean:'], data['Private_Dirty:'],\n                           data['Referenced:'],\n                           data['Anonymous:'],\n                           data['Swap:'])\n            f.close()\n        except EnvironmentError:\n            # XXX - Can't use wrap_exceptions decorator as we're\n            # returning a generator;  this probably needs some\n            # refactoring in order to avoid this code duplication.\n            if f is not None:\n                f.close()\n            err = sys.exc_info()[1]\n            if err.errno in (errno.ENOENT, errno.ESRCH):\n                raise NoSuchProcess(self.pid, self._process_name)\n            if err.errno in (errno.EPERM, errno.EACCES):\n                raise AccessDenied(self.pid, self._process_name)\n            raise\n        except:\n            if f is not None:\n                f.close()\n            raise"}
{"prompt": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"", "output": "\n    if config is None:\n        config = {}\n    if 'packages' not in config:\n        config['packages'] = []\n    config['packages'].append(path)\n    import sys\n    sys.path.extend(config['packages'])\n\n", "solution": "\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added"}
{"prompt": "def _kl_pareto_pareto(a, b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Pareto.\n\n  Args:\n    a: instance of a Pareto distribution object.\n    b: instance of a Pareto distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_pareto_pareto\".\n\n  Returns:\n    Batchwise KL(a || b)\n  \"\"\"", "output": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    a_log_prob = a.log_prob(a.sample())\n    b_log_prob = b.log_prob(b.sample())\n    return a_log_prob - b_log_prob\n\n", "solution": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n        tf.math.log(a.concentration) - tf.math.log(b.concentration) +\n        b.concentration / a.concentration - 1.0,\n        tf.broadcast_to(tf.cast(np.inf, common_type), final_batch_shape))"}
{"prompt": "def flush(self):\n        \"\"\"Flush incomming socket messages.\"\"\"", "output": "\n        if self.writable:\n            self.sock.sendall(b'')\n            self.writable = False\n\n    def close(self):\n        \"\"\"Close the socket connection.\"\"\"\n        self.sock.close()\n\n", "solution": "\n        debug('flushing incomming socket messages')\n        try:\n            while True:\n                msg = self.socket.recv(self.buffer_size)\n                debug(b'< ' + msg)\n        except socket.error:\n            pass"}
{"prompt": "def vcf_records(self, format_tags=None, qualified=False):\n        \"\"\"Generates parsed VcfRecord objects.\n\n        Typically called in a for loop to process each vcf record in a\n        VcfReader. VcfReader must be opened in advanced and closed when\n        complete. Skips all headers.\n\n        Args:\n            qualified: When True, sample names are prefixed with file name\n\n        Returns:\n            Parsed VcfRecord\n\n        Raises:\n            StopIteration: when reader is exhausted.\n            TypeError: if reader is closed.\n        \"\"\"", "output": ".\n        \"\"\"\n        if format_tags is None:\n            format_tags = {}\n\n        if self.closed:\n            raise TypeError(\"Reader is closed\")\n\n        for record in self.reader:\n            if record.is_quoted():\n                record.parse_quoted_fields(format_tags)\n            elif record.is_unquoted():\n                record.parse_unquoted_fields(format_tags)\n            elif qualified:\n                record.qualify_fields(format_tags)\n\n            if not record.is_valid():", "solution": "\n        if qualified:\n            sample_names = self.qualified_sample_names\n        else:\n            sample_names = self.sample_names\n\n        for line in self._file_reader.read_lines():\n            if line.startswith(\"#\"):\n                continue\n            vcf_record = vcf.VcfRecord.parse_record(line, sample_names)\n            if format_tags:\n                vcf_record = self.modify_format_tag(vcf_record, format_tags)\n            yield vcf_record"}
{"prompt": "def main(args,parser,subparser):\n    \"\"\"the list command corresponds with listing images for an external\n       resource. This is different from listing images that are local to the\n       database, which should be done with \"images\"\n    \"\"\"", "output": "\n    if args.command == \"list\":\n        images = get_images(args.resource)\n        for image in images:\n            print(image)\n    else:\n        parser.print_help()\n\n", "solution": "\n    from sregistry.main import get_client\n    cli = get_client(quiet=args.quiet)\n    \n    for query in args.query:\n        if query in ['','*']:\n            query = None\n\n        cli.ls(query=query)"}
{"prompt": "def parse_hpo_genes(hpo_lines):\n    \"\"\"Parse HPO gene information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with hgnc symbols as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        line = line.strip()\n        if not line:\n            continue\n        fields = line.split('\\t')\n        if len(fields)!= 9:\n            continue\n        disease = fields[0]\n        symbol = fields[1]\n        if disease not in diseases:\n            diseases[disease] = []\n        diseases[disease].append(symbol)\n    return diseases\n\n", "solution": "\n    LOG.info(\"Parsing HPO genes ...\")\n    genes = {}\n    for index, line in enumerate(hpo_lines):\n        # First line is header\n        if index == 0:\n            continue\n        if len(line) < 5:\n            continue\n        gene_info = parse_hpo_gene(line)\n        hgnc_symbol = gene_info['hgnc_symbol']\n        description = gene_info['description']\n        \n        if hgnc_symbol not in genes:\n            genes[hgnc_symbol] = {\n                'hgnc_symbol': hgnc_symbol\n            }\n        \n        gene = genes[hgnc_symbol]\n        if description == 'Incomplete penetrance':\n            gene['incomplete_penetrance'] = True\n        if description == 'Autosomal dominant inheritance':\n            gene['ad'] = True\n        if description == 'Autosomal recessive inheritance':\n            gene['ar'] = True\n        if description == 'Mithochondrial inheritance':\n            gene['mt'] = True\n        if description == 'X-linked dominant inheritance':\n            gene['xd'] = True\n        if description == 'X-linked recessive inheritance':\n            gene['xr'] = True\n        if description == 'Y-linked inheritance':\n            gene['x'] = True\n        if description == 'X-linked inheritance':\n            gene['y'] = True\n    LOG.info(\"Parsing done.\")\n    return genes"}
{"prompt": "def start_tag(el):\n    \"\"\"\n    The text representation of the start tag for a tag.\n    \"\"\"", "output": "\n    return f\"<{el}>\"\n\n", "solution": "\n    return '<%s%s>' % (\n        el.tag, ''.join([' %s=\"%s\"' % (name, html_escape(value, True))\n                         for name, value in el.attrib.items()]))"}
{"prompt": "def get_file_content(url, comes_from=None, session=None):\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\"\"\"", "output": "\n    if session is None:\n        session = requests.Session()\n    response = session.get(url, stream=True)\n    response.raise_for_status()\n    if comes_from is None:\n        content = response.content\n    else:\n        content = response.content.read()\n        response = session.get(comes_from, stream=True)\n        response.raise_for_status()\n    return (response.url, content)\n\n", "solution": "\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n\n            if six.PY3:\n                return resp.url, resp.text\n            else:\n                return resp.url, resp.content\n    try:\n        with open(url) as f:\n            content = f.read()\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content"}
{"prompt": "def _get_repetition(extractor, text, *, bounds=(0, None), ignore_whitespace=False):\n  \"\"\"Tries to pull text with extractor repeatedly.\n\n  Bounds is a 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.\n  If the ubound is None, this method will execute extractor on text until extrator raises DeadEnd.\n  Otherwise, extractor will be called until it raises DeadEnd, or it has extracted ubound times.\n\n  If the number of children extracted is >= lbound, then a ParseNode with type repetition is\n  returned. Otherwise, DeadEnd is raised.\n\n  Bounds are interpreted as (lbound, ubound]\n\n  This method is used to implement:\n  - option (0, 1)\n  - zero_or_more (0, None)\n  - one_or_more (1, None)\n  - exact_repeat (n, n)\n  \"\"\"", "output": " times extractor is called on text is greater than the number of times the extractor raises DeadEnd,\n  the function will raise DeadEnd.\n\n  If ignore_whitespace is True, whitespace in the text will be ignored when extracting.\n\n  Returns:\n    The text that was extracted.\n  \"\"\"\n  count = 0\n  while True:\n    try:\n      if bounds[1] is not None and bounds[1] < len(text):\n        return extractor(text[bounds[0]:bounds[1]], ignore_whitespace=ignore_", "solution": "\n  minr, maxr = bounds\n  children = []\n\n  while maxr is None or len(children) <= maxr:\n    ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n    try:\n      child = _call_extractor(extractor, use_text)\n      child.add_ignored(ignored_ws)\n    except DeadEnd:\n      break\n\n    if child.is_empty:\n      break\n\n    children.append(child)\n    text = text[child.consumed:]\n\n\n  if len(children) >= minr:\n    return ParseNode(ParseNodeType.repetition,\n                      children=children)\n  else:\n    raise DeadEnd()"}
{"prompt": "def get_query(self, query):\n        \"\"\" Grab all issues matching a github query \"\"\"", "output": "\n        issues = []\n        for issue in self.issues:\n            if query in issue:\n                issues.append(issue)\n        return issues\n\n", "solution": "\n        issues = {}\n        for issue in self.client.get_query(query):\n            url = issue['html_url']\n            try:\n                repo = self.get_repository_from_issue(issue)\n            except ValueError as e:\n                log.critical(e)\n            else:\n                issues[url] = (repo, issue)\n        return issues"}
{"prompt": "def build(self, secret_key):\n        \"\"\"Builds a final copy of the token using the given secret key.\n\n        :param secret_key(string): The secret key that corresponds to this builder's access key.\n        \"\"\"", "output": "\n        # Your code here to build the final token using the secret key\n        pass\n\n    def verify(self, token):\n        \"\"\"Verifies the authenticity of the given token.\n\n        :param token(string): The token to verify.\n        :returns True if the token is authentic, False otherwise.\n        \"\"\"\n        # Your code here to verify the authenticity of the token\n        pass\n\n    def encrypt(self, data):\n        \"\"\"Encrypts the given data using the builder's secret key.\n\n        :param data(string): The data to encrypt.\n        :return", "solution": "\n        key = jwk.JWK(\n            kty='oct',\n            k=base64url_encode(uuid.UUID(secret_key).bytes),\n        )\n\n        header = {\n            'alg': 'dir',\n            'enc': 'A128GCM',\n            'zip': 'DEF',\n            'cty': 'JWT',\n            'kid': self._access_key,\n        }\n\n        now = int(time.time())\n\n        payload = {\n            'iat': now,\n            'nbf': now,\n        }\n\n        if self._expiration is not None:\n            payload['exp'] = int(calendar.timegm(self._expiration.utctimetuple()))\n\n        if len(self._view_identifiers) > 0:\n            payload[VIEW_IDENTIFIERS_CLAIM_NAME] = self._view_identifiers\n\n        if len(self._parameters) > 0:\n            parameters = []\n            for parameter in self._parameters:\n                serialized = {\n                    'field': parameter.field,\n                    'op': parameter.op,\n                }\n\n                if hasattr(parameter, '__iter__'):\n                    serialized['any'] = list(parameter.value)\n                else:\n                    serialized['value'] = parameter.value\n\n                parameters.append(serialized)\n\n            payload[PARAMETERS_CLAIM_NAME] = parameters\n\n        if len(self._attributes) > 0:\n            payload[ATTRIBUTES_CLAIM_NAME] = self._attributes\n\n        tok = jwe.JWE(json_encode(payload), protected=header)\n        tok.add_recipient(key)\n\n        return tok.serialize(compact=True)"}
{"prompt": "def cpu_percent(interval=0.1, percpu=False):\n    \"\"\"Return a float representing the current system-wide CPU\n    utilization as a percentage.\n\n    When interval is > 0.0 compares system CPU times elapsed before\n    and after the interval (blocking).\n\n    When interval is 0.0 or None compares system CPU times elapsed\n    since last call or module import, returning immediately.\n    In this case is recommended for accuracy that this function be\n    called with at least 0.1 seconds between calls.\n\n    When percpu is True returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n    \"\"\"", "output": " the\n    current CPU usage percentage for each core on the system.\n\n    If percpu is False returns a float representing the current\n    CPU usage percentage for the system on a single core.\n\n    If the system has no cores, returns 0.0.\n    \"\"\"\n    import psutil\n\n    cpu_times = []\n    if percpu:\n        for core in range(1, psutil.cpu_count() + 1):\n            cpu_times.append(psutil.cpu_times().user + psutil.cpu_times().system)\n    else:\n        cpu_", "solution": "\n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        # this usually indicates a float precision issue\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy\n        all_delta = t2_all - t1_all\n        busy_perc = (busy_delta / all_delta) * 100\n        return round(busy_perc, 1)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times\n        _last_cpu_times = cpu_times()\n        return calculate(t1, _last_cpu_times)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times\n        _last_per_cpu_times = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times):\n            ret.append(calculate(t1, t2))\n        return ret"}
{"prompt": "def load_filters():\n    \"\"\"\n    Loads and returns all filters.\n    \"\"\"", "output": "\n    filters = []\n    for file in os.listdir(FILTER_DIR):\n        if file.endswith('.txt'):\n            filters.append(file)\n    return filters\n\n", "solution": "\n    all_filters = {}\n    for m in JINJA_FILTERS:\n        if hasattr(m, \"filters\"):\n            all_filters.update(m.filters())\n    return all_filters"}
{"prompt": "def weighted(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"", "output": "\n    total_weight = sum(loads)\n    rnd = random.random() * total_weight\n    cumulative_weight = 0\n    for i, load in enumerate(loads):\n        cumulative_weight += load\n        if rnd < cumulative_weight:\n            return i\n\n", "solution": "\n    # weight 0 a million times more than 1:\n    weights = 1./(1e-6+numpy.array(loads))\n    sums = weights.cumsum()\n    t = sums[-1]\n    x = random()*t\n    y = random()*t\n    idx = 0\n    idy = 0\n    while sums[idx] < x:\n        idx += 1\n    while sums[idy] < y:\n        idy += 1\n    if weights[idy] > weights[idx]:\n        return idy\n    else:\n        return idx"}
{"prompt": "def print_figure(fig, fmt='png'):\n    \"\"\"Convert a figure to svg or png for inline display.\"\"\"", "output": "\n    if fmt =='svg':\n        svg = fig.to_svg()\n        display(svg)\n    elif fmt == 'png':\n        img = fig.to_image()\n        display(img)\n    else:\n        raise ValueError(\"Invalid format specified. Must be'svg' or 'png'.\")\n\n", "solution": "\n    # When there's an empty figure, we shouldn't return anything, otherwise we\n    # get big blank areas in the qt console.\n    if not fig.axes and not fig.lines:\n        return\n\n    fc = fig.get_facecolor()\n    ec = fig.get_edgecolor()\n    fig.set_facecolor('white')\n    fig.set_edgecolor('white')\n    try:\n        bytes_io = BytesIO()\n        fig.canvas.print_figure(bytes_io, format=fmt, bbox_inches='tight')\n        data = bytes_io.getvalue()\n    finally:\n        fig.set_facecolor(fc)\n        fig.set_edgecolor(ec)\n    return data"}
{"prompt": "def json_to_bounds(segments_json):\n    \"\"\"Extracts the boundaries from a json file and puts them into\n        an np array.\"\"\"", "output": "\n    bounds = []\n    for segment in segments_json:\n        x1, y1, x2, y2 = segment['bounds']\n        bounds.append([x1, y1, x2, y2])\n    return np.array(bounds)\n\n", "solution": "\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    bounds.append(bounds[-1] + segments[-1][\"duration\"]) # Add last boundary\n    f.close()\n    return np.asarray(bounds)"}
{"prompt": "def _bio_to_string(bio):\n    \"\"\"\n    Copy the contents of an OpenSSL BIO object into a Python byte string.\n    \"\"\"", "output": "\n    return bio.to_bytes()\n\n", "solution": "\n    result_buffer = _ffi.new('char**')\n    buffer_length = _lib.BIO_get_mem_data(bio, result_buffer)\n    return _ffi.buffer(result_buffer[0], buffer_length)[:]"}
{"prompt": "def create_from(cls, backend):\n        \"\"\"\n        Create device specification with values in backend configuration.\n        Args:\n            backend(Backend): backend configuration\n        Returns:\n            DeviceSpecification: created device specification\n        Raises:\n            PulseError: when an invalid backend is specified\n        \"\"\"", "output": "\n        if backend == Backend.OPENQASM:\n            return OpenQASMSpecification()\n        elif backend == Backend.QPU:\n            return QPUSpecification()\n        else:\n            raise PulseError(\"Invalid backend specified\")\n\n    return [create_from(backend) for backend in backends]\n\n", "solution": "\n        backend_config = backend.configuration()\n\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\n        try:\n            backend_default = backend.defaults()\n        except ModelValidationError:\n            from collections import namedtuple\n            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))\n\n            backend_default = BackendDefault(\n                qubit_freq_est=backend_config.defaults['qubit_freq_est'],\n                meas_freq_est=backend_config.defaults['meas_freq_est']\n            )\n\n        # system size\n        n_qubits = backend_config.n_qubits\n        n_registers = backend_config.n_registers\n        n_uchannels = backend_config.n_uchannels\n\n        if n_uchannels > 0 and n_uchannels != n_qubits:\n            raise PulseError(\"This version assumes no U-channels or #U-cannels==#qubits.\")\n\n        # frequency information\n        qubit_lo_freqs = backend_default.qubit_freq_est\n        qubit_lo_ranges = backend_config.qubit_lo_range\n        meas_lo_freqs = backend_default.meas_freq_est\n        meas_lo_ranges = backend_config.meas_lo_range\n\n        # generate channels with assuming their numberings are aligned with qubits\n        drives = [\n            DriveChannel(i, qubit_lo_freqs[i], tuple(qubit_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        measures = [\n            MeasureChannel(i, meas_lo_freqs[i], tuple(meas_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        acquires = [AcquireChannel(i) for i in range(n_qubits)]\n        controls = [ControlChannel(i) for i in range(n_uchannels)]\n\n        qubits = []\n        for i in range(n_qubits):\n            # TODO: get qubits <-> channels relationship from backend\n            qubit = Qubit(i,\n                          drive_channels=[drives[i]],\n                          control_channels=None if n_uchannels == 0 else controls[i],\n                          measure_channels=[measures[i]],\n                          acquire_channels=[acquires[i]])\n            qubits.append(qubit)\n\n        registers = [RegisterSlot(i) for i in range(n_registers)]\n        # TODO: get #mem_slots from backend\n        mem_slots = [MemorySlot(i) for i in range(len(qubits))]\n\n        return DeviceSpecification(qubits, registers, mem_slots)"}
{"prompt": "def remove_memberships(self, team, users):\n        \"\"\"\n        **Description**\n            Remove user memberships from specified team.\n\n        **Arguments**\n            - **team**: the name of the team from which user memberships are removed\n            - **users**: list of usernames which should be removed from team\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt_extended.py>`_\n        \"\"\"", "output": "ended.py>`_\n        \"\"\"\n        url = f\"https://api.teams.{team}.memberships.users\"\n        data = {\"members\": [user for user in users if user in self.members]}\n        response = requests.post(url, json=data)\n        if response.status_code!= 200:\n            raise ValueError(\"Failed to remove user memberships from team.\")\n\n", "solution": "\n\n        res = self.list_memberships(team)\n\n        if res[0] is False:\n            return res\n\n        old_memberships = res[1]\n        new_memberships = {k: v for k, v in old_memberships.items() if k not in users}\n\n        res = self.edit_team(team, new_memberships)\n\n        if res[0] is False:\n            return res\n        else:\n            return [True, None]"}
{"prompt": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"", "output": "\n    if isinstance(obj, str):\n        try:\n            return Price(float(obj))\n        except ValueError:\n            raise ValueError(f\"Invalid price: {obj}\")\n    return obj\n\n", "solution": "\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj"}
{"prompt": "def finalize(self, remove_all_handlers=True):\n        \"\"\"Finalizes the manager, closes and removes all handlers if desired.\"\"\"", "output": "\n        if remove_all_handlers:\n            for handler in self.handlers:\n                handler.close()\n            self.handlers.clear()\n        self.server.shutdown()\n\n", "solution": "\n        for tool in self._tools:\n            tool.finalize()\n        self._tools = []\n        self._stdout_to_logger = None\n        for config in (self._sp_config, self._mp_config):\n            if hasattr(config, 'close'):\n                config.close()\n        self._sp_config = None\n        self._mp_config = None\n        if remove_all_handlers:\n            self.tabula_rasa()"}
{"prompt": "def _experiments_to_circuits(qobj):\n    \"\"\"Return a list of QuantumCircuit object(s) from a qobj\n\n    Args:\n        qobj (Qobj): The Qobj object to convert to QuantumCircuits\n    Returns:\n        list: A list of QuantumCircuit objects from the qobj\n\n    \"\"\"", "output": "\n    if not isinstance(qobj, Qobj):\n        raise TypeError(\"Input must be a Qobj object\")\n\n    if not qobj.isopercalar():\n        raise ValueError(\"Input must be a scalar Qobj\")\n\n    if not qobj.isoper:\n        raise ValueError(\"Input must be an operator Qobj\")\n\n    if not qobj.isoperand:\n        raise ValueError(\"Input must be an operand Qobj\")\n\n    if not qobj.isoper:\n        return []\n\n    if qobj.dims[0", "solution": "\n    if qobj.experiments:\n        circuits = []\n        for x in qobj.experiments:\n            quantum_registers = [QuantumRegister(i[1], name=i[0])\n                                 for i in x.header.qreg_sizes]\n            classical_registers = [ClassicalRegister(i[1], name=i[0])\n                                   for i in x.header.creg_sizes]\n            circuit = QuantumCircuit(*quantum_registers,\n                                     *classical_registers,\n                                     name=x.header.name)\n            qreg_dict = {}\n            creg_dict = {}\n            for reg in quantum_registers:\n                qreg_dict[reg.name] = reg\n            for reg in classical_registers:\n                creg_dict[reg.name] = reg\n            for i in x.instructions:\n                instr_method = getattr(circuit, i.name)\n                qubits = []\n                try:\n                    for qubit in i.qubits:\n                        qubit_label = x.header.qubit_labels[qubit]\n                        qubits.append(\n                            qreg_dict[qubit_label[0]][qubit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                clbits = []\n                try:\n                    for clbit in i.memory:\n                        clbit_label = x.header.clbit_labels[clbit]\n                        clbits.append(\n                            creg_dict[clbit_label[0]][clbit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                params = []\n                try:\n                    params = i.params\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                if i.name in ['snapshot']:\n                    instr_method(\n                        i.label,\n                        snapshot_type=i.snapshot_type,\n                        qubits=qubits,\n                        params=params)\n                elif i.name == 'initialize':\n                    instr_method(params, qubits)\n                else:\n                    instr_method(*params, *qubits, *clbits)\n            circuits.append(circuit)\n        return circuits\n    return None"}
{"prompt": "def visit_assign(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"", "output": "\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name) and node.value.func.id == \"print\":\n            # check if the function call is returning a valuable value\n            return True\n        return False\n\n    def visit_FunctionDef(self, node):\n        \"\"\"check that the function definition is valid and contains a\n        return statement\n        \"\"\"\n        # check that the function definition is valid\n        if not isinstance(node, ast.FunctionDef):\n            return False\n        # check that the function", "solution": "\n        if not isinstance(node.value, astroid.Call):\n            return\n        function_node = safe_infer(node.value.func)\n        # skip class, generator and incomplete function definition\n        funcs = (astroid.FunctionDef, astroid.UnboundMethod, astroid.BoundMethod)\n        if not (\n            isinstance(function_node, funcs)\n            and function_node.root().fully_defined()\n            and not function_node.decorators\n        ):\n            return\n        if (\n            function_node.is_generator()\n            or function_node.is_abstract(pass_is_abstract=False)\n            or isinstance(function_node, astroid.AsyncFunctionDef)\n        ):\n            return\n        returns = list(\n            function_node.nodes_of_class(astroid.Return, skip_klass=astroid.FunctionDef)\n        )\n        if not returns:\n            self.add_message(\"assignment-from-no-return\", node=node)\n        else:\n            for rnode in returns:\n                if not (\n                    isinstance(rnode.value, astroid.Const)\n                    and rnode.value.value is None\n                    or rnode.value is None\n                ):\n                    break\n            else:\n                self.add_message(\"assignment-from-none\", node=node)"}
{"prompt": "def read_array(self, start, end):\n        \"\"\"Return floats from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of floats is immediately read into memory from\n        the file, making this efficient for small sequences of floats\n        whose values are all needed immediately.\n\n        \"\"\"", "output": "\n        with open(self.filename, 'r') as f:\n            f.seek(start - 1)\n            return [float(x) for x in f.read().split()]\n\n    def write_array(self, start, end, values):\n        \"\"\"Write floats from `values` to `filename` from `start` to `end` inclusive.\n\n        If the range of values specified by `start` and `end` is smaller than\n        the total number of floats in the file, the function will append the\n        remaining values to the end of the file.", "solution": "\n        f = self.file\n        f.seek(8 * (start - 1))\n        length = 1 + end - start\n        data = f.read(8 * length)\n        return ndarray(length, self.endian + 'd', data)"}
{"prompt": "def batches(dataset):\n    \"\"\"Returns a callable that chooses sequences from netcdf data.\"\"\"", "output": "\n    def batches_generator():\n        for i in range(0, len(dataset), BATCH_SIZE):\n            yield dataset[i:i+BATCH_SIZE]\n    return batches_generator\n\n", "solution": "\n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample():\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])\n\n    return sample"}
{"prompt": "def _raise_if_wrong_file_signature(stream):\n    \"\"\" Reads the 4 first bytes of the stream to check that is LASF\"\"\"", "output": "\n    if stream.read(4)!= b'LASF':\n        raise ValueError(\"Invalid file signature\")\n\n", "solution": "\n    file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE))\n    if file_sig != headers.LAS_FILE_SIGNATURE:\n        raise errors.PylasError(\n            \"File Signature ({}) is not {}\".format(file_sig, headers.LAS_FILE_SIGNATURE)\n        )"}
{"prompt": "def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"", "output": "\n        data_files = []\n        for root, dirs, files in os.walk(src_dir):\n            for file in files:\n                if file.endswith('.dat'):\n                    data_files.append(os.path.join(root, file))\n        return data_files\n\n", "solution": "\n        globs = (self.package_data.get('', [])\n                 + self.package_data.get(package, []))\n        files = self.manifest_files.get(package, [])[:]\n        for pattern in globs:\n            # Each pattern has to be converted to a platform-specific path\n            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))\n        return self.exclude_data_files(package, src_dir, files)"}
{"prompt": "def to_id(string: str) -> str:\n    \"\"\"Get a spotify ID from a URI or open.spotify URL.\n\n    Paramters\n    ---------\n    string : str\n        The string to operate on.\n\n    Returns\n    -------\n    id : str\n        The Spotify ID from the string.\n    \"\"\"", "output": "\n    # Remove any query parameters from the string\n    id = string.split(\"?\")[0]\n    # Remove any trailing whitespace\n    id = id.rstrip()\n    return id\n\n", "solution": "\n    string = string.strip()\n\n    match = _URI_RE.match(string)\n\n    if match is None:\n        match = _OPEN_RE.match(string)\n\n        if match is None:\n            return string\n        else:\n            return match.group(2)\n    else:\n        return match.group(1)"}
{"prompt": "def marquee(txt='',width=78,mark='*'):\n    \"\"\"Return the input string centered in a 'marquee'.\n\n    :Examples:\n\n        In [16]: marquee('A test',40)\n        Out[16]: '**************** A test ****************'\n\n        In [17]: marquee('A test',40,'-')\n        Out[17]: '---------------- A test ----------------'\n\n        In [18]: marquee('A test',40,' ')\n        Out[18]: '                 A test                 '\n\n    \"\"\"", "output": "\n    if len(txt) >= width:\n        return txt\n    else:\n        num_spaces = (width - len(txt)) // 2\n        return'' * num_spaces + mark +'' * num_spaces + txt\n\n", "solution": "\n    if not txt:\n        return (mark*width)[:width]\n    nmark = (width-len(txt)-2)//len(mark)//2\n    if nmark < 0: nmark =0\n    marks = mark*nmark\n    return '%s %s %s' % (marks,txt,marks)"}
{"prompt": "def add(self, gate, qargs, cargs):\n        \"\"\"Add an instruction and its context (where it's attached).\"\"\"", "output": "\n        self.instructions.append((gate, qargs, cargs))\n\n    def __str__(self):\n        \"\"\"Return a string representation of the circuit.\"\"\"\n        return '\\n'.join(str(gate) +'' +''.join(qargs) +'' +''.join(cargs) for gate, qargs, cargs in self.instructions)\n\n", "solution": "\n        if not isinstance(gate, Instruction):\n            raise QiskitError(\"attempt to add non-Instruction\" +\n                              \" to InstructionSet\")\n        self.instructions.append(gate)\n        self.qargs.append(qargs)\n        self.cargs.append(cargs)"}
{"prompt": "def read_char(self, c: str) -> bool:\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"", "output": "\n        self.current += 1\n        return True\n\n    def peekchar(self) -> str:\n        \"\"\"\n        Return the current character without incrementing current index.\n        \"\"\"\n        return self.s[self.current]\n\n    def skip_whitespace(self) -> bool:\n        \"\"\"\n        Consume all whitespace characters and return True if successful.\n        \"\"\"\n        while self.current < len(self.s) and self.s[self.current].isspace():\n            self.current += 1\n        return self.current < len(self.", "solution": "\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()"}
{"prompt": "def disassemble(msg, msg_nocr, section, co, lasti=-1, start_line=-1,\n                end_line=None, relative_pos=False, highlight='light',\n                start_offset=0, end_offset=None):\n    \"\"\"Disassemble a code object.\"\"\"", "output": "\n    if start_line == -1:\n        start_line = 0\n    if end_line is None:\n        end_line = len(msg)\n    if end_offset is None:\n        end_offset = len(msg)\n    lines = msg.split('\\n')\n    if start_line > end_line:\n        return ''\n    if start_offset < 0:\n        start_offset = 0\n    if end_offset is None:\n        end_offset = len(msg)\n    if end_offset - start_offset < len", "solution": "\n    return disassemble_bytes(msg, msg_nocr, co.co_code, lasti, co.co_firstlineno,\n                             start_line, end_line, relative_pos,\n                        co.co_varnames, co.co_names, co.co_consts,\n                        co.co_cellvars, co.co_freevars,\n                        dict(findlinestarts(co)), highlight,\n                        start_offset=start_offset, end_offset=end_offset)"}
{"prompt": "def _unpack_storm_date(date):\n        \"\"\"\n        given a packed storm date field, unpack and return 'YYYY-MM-DD' string.\n        \"\"\"", "output": "\n        year, month, day = date\n        return f\"{year:04d}-{month:02d}-{day:02d}\"\n\n    return [_unpack_storm_date(date) for date in storms]\n\n", "solution": "\n        year = (date & 0x7f) + 2000  # 7 bits\n        day = (date >> 7) & 0x01f  # 5 bits\n        month = (date >> 12) & 0x0f  # 4 bits\n        return \"%s-%s-%s\" % (year, month, day)"}
{"prompt": "def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"", "output": "\n        while True:\n            response = self.get_operation_resource(name)\n            if response.get('status') == 'completed':\n                return response\n            time.sleep(polling_interval_in_seconds)\n\n    def get_operation_resource(self, name):\n        \"\"\"\n        Retrieve a resource operation.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :return: a resource operation instance\n        :rtype: dict\n        \"\"\"\n        # implementation details not provided\n        pass\n\n", "solution": "\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result"}
{"prompt": "def cost(self, t_node, branch_length, multiplicity=2.0):\n        \"\"\"\n        returns the cost associated with a branch starting at t_node\n        t_node is time before present, the branch goes back in time\n\n        Args:\n            - t_node:           time of the node\n            - branch_length:    branch length, determines when this branch merges with sister\n            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n        \"\"\"", "output": "\n        if multiplicity == 2:\n            return branch_length\n        else:\n            return branch_length * multiplicity\n\n    def find_optimal_merging_branch(self, t_node, branch_lengths, multiplicities):\n        \"\"\"\n        Finds the optimal branch to merge with a sister.\n\n        Args:\n            - t_node:           time of the node\n            - branch_lengths:    list of branch lengths\n            - multiplicities:     list of multiplicities\n\n        Returns:\n            - The time of the branch that should", "solution": "\n        merger_time = t_node+branch_length\n        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\\\n                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity"}
{"prompt": "def run_cell_magic(self, magic_name, line, cell):\n        \"\"\"Execute the given cell magic.\n        \n        Parameters\n        ----------\n        magic_name : str\n          Name of the desired magic function, without '%' prefix.\n\n        line : str\n          The rest of the first input line as a single string.\n\n        cell : str\n          The body of the cell as a (possibly multiline) string.\n        \"\"\"", "output": "\n        magic_func = getattr(self, magic_name)\n        return magic_func(line, cell)\n\n", "solution": "\n        fn = self.find_cell_magic(magic_name)\n        if fn is None:\n            lm = self.find_line_magic(magic_name)\n            etpl = \"Cell magic function `%%%%%s` not found%s.\"\n            extra = '' if lm is None else (' (But line magic `%%%s` exists, '\n                                    'did you mean that instead?)' % magic_name )\n            error(etpl % (magic_name, extra))\n        else:\n            # Note: this is the distance in the stack to the user's frame.\n            # This will need to be updated if the internal calling logic gets\n            # refactored, or else we'll be expanding the wrong variables.\n            stack_depth = 2\n            magic_arg_s = self.var_expand(line, stack_depth)\n            with self.builtin_trap:\n                result = fn(line, cell)\n            return result"}
{"prompt": "def update_trace_watch(self):\n        \"\"\"Parses the nextflow trace file and retrieves the path of report JSON\n        files that have not been sent to the service yet.\n        \"\"\"", "output": "\n        # Parse the nextflow trace file\n        with open(self.trace_file, 'r') as f:\n            for line in f:\n                if line.startswith('Report'):\n                    report_path = line.split()[1]\n                    # Check if the report has already been sent to the service\n                    if report_path in self.sent_reports:\n                        continue\n                    # Check if the report has been sent to the service before\n                    if report_path in self.sent_reports_received:\n                        continue\n                    # Update the list of", "solution": "\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.trace_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.trace_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating trace size stamp to: {}\".format(size_stamp))\n            self.trace_sizestamp = size_stamp\n\n        with open(self.trace_file) as fh:\n\n            # Skip potential empty lines at the start of file\n            header = next(fh).strip()\n            while not header:\n                header = next(fh).strip()\n\n            # Get header mappings before parsing the file\n            hm = self._header_mapping(header)\n\n            for line in fh:\n                # Skip empty lines\n                if line.strip() == \"\":\n                    continue\n\n                fields = line.strip().split(\"\\t\")\n\n                # Skip if task ID was already processes\n                if fields[hm[\"task_id\"]] in self.stored_ids:\n                    continue\n\n                if fields[hm[\"process\"]] == \"report\":\n                    self.report_queue.append(\n                        self._expand_path(fields[hm[\"hash\"]])\n                    )\n                    self.send = True\n\n                # Add the processed trace line to the stored ids. It will be\n                # skipped in future parsers\n                self.stored_ids.append(fields[hm[\"task_id\"]])"}
{"prompt": "def permission_check(apikey, endpoint):\n        \"\"\"\n        return (user, seckey) if url end point is in allowed entry point list\n        \"\"\"", "output": "\n        # implementation details not provided\n        pass\n    \n    def get_permission(url):\n        \"\"\"\n        return (user, seckey) if url is in allowed entry point list\n        \"\"\"\n        # implementation details not provided\n        pass\n    \n    def has_permission(url):\n        \"\"\"\n        return True if url has permission to access its endpoint\n        \"\"\"\n        user, seckey = get_permission(url)\n        if user and seckey:\n            return True\n        return False\n    \n    return [url for url in urls if", "solution": "\n        try:\n            ak = APIKeys.objects.get(apikey=apikey)\n            apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n            if apitree.match(endpoint):\n                return ak.user if ak.user else AnonymousUser(), ak.seckey\n        except APIKeys.DoesNotExist:\n            pass\n        return None, None"}
{"prompt": "def __create_criterion(self, criterion_str):\n        \"\"\" instantiate criterion from config string \"\"\"", "output": "\n        criterion_config = self.config.get(criterion_str)\n        if criterion_config is None:\n            raise ValueError(f\"Invalid criterion: {criterion_str}\")\n        criterion_class = getattr(sys.modules[__name__], criterion_config[\"class\"])\n        return criterion_class(**criterion_config[\"args\"])\n\n    def __create_optimizer(self, optim_str):\n        \"\"\" instantiate optimizer from config string \"\"\"\n        optim_config = self.config.get(optim_str)", "solution": "\n        parsed = criterion_str.split(\"(\")\n        type_str = parsed[0].strip().lower()\n        parsed[1] = parsed[1].split(\")\")[0].strip()\n\n        for criterion_class in self.custom_criterions:\n            if criterion_class.get_type_string() == type_str:\n                return criterion_class(self, parsed[1])\n        raise ValueError(\n            \"Unsupported autostop criterion type: %s\" % criterion_str)"}
{"prompt": "def verboseRead(self, alphabet, context='', skipExtra=False):\n        \"\"\"Read symbol and extra from stream and explain what happens.\n        Returns the value of the symbol\n        >>> olleke.pos = 0\n        >>> l = Layout(olleke)\n        >>> l.verboseRead(WindowSizeAlphabet())\n        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288\n        4194288\n        \"\"\"", "output": "\n        if context:\n            context +=''\n        if skipExtra:\n            return ''\n        if self.pos >= len(alphabet):\n            return ''\n        symbol = alphabet[self.pos]\n        if symbol == '\\r':\n            self.pos += 1\n            return ''\n        if symbol == '\\n':\n            self.pos += 1\n            return ''\n        if symbol == '\\t':\n            self.pos += 1\n            return ''\n        if symbol =='':\n            self.pos += 1\n            return context +", "solution": "\n        #TODO 2: verbosity level, e.g. show only codes and maps in header\n        stream = self.stream\n        pos = stream.pos\n        if skipExtra:\n            length, symbol = alphabet.readTuple(stream)\n            extraBits, extra = 0, None\n        else:\n            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(\n                stream)\n        #fields: address, hex data, binary data, name of alphabet, explanation\n        hexdata = self.makeHexData(pos)\n        addressField = '{:04x}'.format(pos+7>>3) if hexdata else ''\n        bitdata = self.formatBitData(pos, length, extraBits)\n        #bitPtr moves bitdata so that the bytes are easier to read\n        #jump back to right if a new byte starts\n        if '|' in bitdata[1:]:\n            #start over on the right side\n            self.bitPtr = self.width\n        fillWidth = self.bitPtr-(len(hexdata)+len(bitdata))\n        if fillWidth<0: fillWidth = 0\n        print('{:<5s} {:<{}s} {:7s} {}'.format(\n            addressField,\n            hexdata+' '*fillWidth+bitdata, self.width,\n            context+alphabet.name,\n            symbol if skipExtra else symbol.explanation(extra),\n            ))\n        #jump to the right if we started with a '|'\n        #because we didn't jump before printing\n        if bitdata.startswith('|'): self.bitPtr = self.width\n        else: self.bitPtr -= len(bitdata)\n        return symbol if skipExtra else symbol.value(extra)"}
{"prompt": "def builtin(cls, name):\n        \"\"\"\n        Generate a default legend.\n\n        Args:\n            name (str): The name of the legend you want. Not case sensitive.\n                 'nsdoe': Nova Scotia Dept. of Energy\n                 'canstrat': Canstrat\n                 'nagmdm__6_2': USGS N. Am. Geol. Map Data Model 6.2\n                 'nagmdm__6_1': USGS N. Am. Geol. Map Data Model 6.1\n                 'nagmdm__4_3': USGS N. Am. Geol. Map Data Model 4.3\n                 'sgmc': USGS State Geologic Map Compilation\n\n            Default 'nagmdm__6_2'.\n\n        Returns:\n            Legend: The legend stored in `defaults.py`.\n        \"\"\"", "output": "_1': USGS N. Am. Geol. Map Data Model 4.1\n\n        Returns:\n            str: The name of the legend you want.\n        \"\"\"\n        legend_names = {\n            'nsdoe': 'N. S. Department of Energy',\n            'canstrat': 'Canstrat',\n            'nagmdm__6_2': 'National Association of Geol. and Map Data (N.A. Geol.) 6.2',\n            'nagmdm__6_1': 'National Association of Geol. and Map", "solution": "\n        names = {\n                 'nsdoe': LEGEND__NSDOE,\n                 'canstrat': LEGEND__Canstrat,\n                 'nagmdm__6_2': LEGEND__NAGMDM__6_2,\n                 'nagmdm__6_1': LEGEND__NAGMDM__6_1,\n                 'nagmdm__4_3': LEGEND__NAGMDM__4_3,\n                 'sgmc': LEGEND__SGMC,\n                 }\n        return cls.from_csv(text=names[name.lower()])"}
{"prompt": "def parse_email(data, strip_attachment_payloads=False):\n    \"\"\"\n    A simplified email parser\n\n    Args:\n        data: The RFC 822 message string, or MSG binary\n        strip_attachment_payloads (bool): Remove attachment payloads\n\n    Returns (dict): Parsed email data\n    \"\"\"", "output": "\n    if isinstance(data, str):\n        data = data.encode('utf-8')\n\n    if data.startswith(b'From:'):\n        data = data[5:]\n\n    if data.startswith(b'Subject:'):\n        data = data[9:]\n\n    if data.startswith(b'Date:'):\n        data = data[5:]\n\n    if data.startswith(b'Subject:'):\n        data = data[8:]\n\n    if data.startswith(b'From:'):\n        data", "solution": "\n\n    if type(data) == bytes:\n        if is_outlook_msg(data):\n            data = convert_outlook_msg(data)\n        data = data.decode(\"utf-8\", errors=\"replace\")\n    parsed_email = mailparser.parse_from_string(data)\n    headers = json.loads(parsed_email.headers_json).copy()\n    parsed_email = json.loads(parsed_email.mail_json).copy()\n    parsed_email[\"headers\"] = headers\n\n    if \"received\" in parsed_email:\n        for received in parsed_email[\"received\"]:\n            if \"date_utc\" in received:\n                if received[\"date_utc\"] is None:\n                    del received[\"date_utc\"]\n                else:\n                    received[\"date_utc\"] = received[\"date_utc\"].replace(\"T\",\n                                                                        \" \")\n\n    if \"from\" not in parsed_email:\n        if \"From\" in parsed_email[\"headers\"]:\n            parsed_email[\"from\"] = parsed_email[\"Headers\"][\"From\"]\n        else:\n            parsed_email[\"from\"] = None\n\n    if parsed_email[\"from\"] is not None:\n        parsed_email[\"from\"] = parse_email_address(parsed_email[\"from\"][0])\n\n    if \"date\" in parsed_email:\n        parsed_email[\"date\"] = parsed_email[\"date\"].replace(\"T\", \" \")\n    else:\n        parsed_email[\"date\"] = None\n    if \"reply_to\" in parsed_email:\n        parsed_email[\"reply_to\"] = list(map(lambda x: parse_email_address(x),\n                                            parsed_email[\"reply_to\"]))\n    else:\n        parsed_email[\"reply_to\"] = []\n\n    if \"to\" in parsed_email:\n        parsed_email[\"to\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"to\"]))\n    else:\n        parsed_email[\"to\"] = []\n\n    if \"cc\" in parsed_email:\n        parsed_email[\"cc\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"cc\"]))\n    else:\n        parsed_email[\"cc\"] = []\n\n    if \"bcc\" in parsed_email:\n        parsed_email[\"bcc\"] = list(map(lambda x: parse_email_address(x),\n                                       parsed_email[\"bcc\"]))\n    else:\n        parsed_email[\"bcc\"] = []\n\n    if \"delivered_to\" in parsed_email:\n        parsed_email[\"delivered_to\"] = list(\n            map(lambda x: parse_email_address(x),\n                parsed_email[\"delivered_to\"])\n        )\n\n    if \"attachments\" not in parsed_email:\n        parsed_email[\"attachments\"] = []\n    else:\n        for attachment in parsed_email[\"attachments\"]:\n            if \"payload\" in attachment:\n                payload = attachment[\"payload\"]\n                try:\n                    if \"content_transfer_encoding\" in attachment:\n                        if attachment[\"content_transfer_encoding\"] == \"base64\":\n                            payload = decode_base64(payload)\n                        else:\n                            payload = str.encode(payload)\n                    attachment[\"sha256\"] = hashlib.sha256(payload).hexdigest()\n                except Exception as e:\n                    logger.debug(\"Unable to decode attachment: {0}\".format(\n                        e.__str__()\n                    ))\n        if strip_attachment_payloads:\n            for attachment in parsed_email[\"attachments\"]:\n                if \"payload\" in attachment:\n                    del attachment[\"payload\"]\n\n    if \"subject\" not in parsed_email:\n        parsed_email[\"subject\"] = None\n\n    parsed_email[\"filename_safe_subject\"] = get_filename_safe_string(\n        parsed_email[\"subject\"])\n\n    if \"body\" not in parsed_email:\n        parsed_email[\"body\"] = None\n\n    return parsed_email"}
{"prompt": "def get_news(self, tickers=[], tags=[], sources=[], startDate=None,\n                 endDate=None, limit=100, offset=0, sortBy=\"publishedDate\",\n                 fmt='json'):\n        \"\"\"Return list of news articles matching given search terms\n            https://api.tiingo.com/docs/tiingo/news\n\n            # Dates are in YYYY-MM-DD Format.\n\n            Args:\n                tickers [string] : List of unique Stock Tickers to search\n                tags [string] : List of topics tagged by Tiingo Algorithms\n                sources [string]: List of base urls to include as news sources\n                startDate, endDate [date]: Boundaries of news search window\n                limit (int): Max results returned. Default 100, max 1000\n                offset (int): Search results offset, used for paginating\n                sortBy (string): \"publishedDate\" OR (#TODO: UPDATE THIS)\n        \"\"\"", "output": "ingo\n                sources [string] : List of news sources to search\n                startDate [string] : Start date in YYYY-MM-DD Format\n                endDate [string] : End date in YYYY-MM-DD Format\n                limit [int] : Number of news articles to return per page\n                offset [int] : Number of news articles to skip before returning\n                sortBy [string] : Sort news articles by date (ascending or descending)\n                fmt [string] : Format of news articles to return (json or xml)\n\n            Returns:\n                List", "solution": "\n        url = \"tiingo/news\"\n        params = {\n            'limit': limit,\n            'offset': offset,\n            'sortBy': sortBy,\n            'tickers': tickers,\n            'sources': sources,\n            'tags': tags,\n            'startDate': startDate,\n            'endDate': endDate\n        }\n        response = self._request('GET', url, params=params)\n        data = response.json()\n        if fmt == 'json':\n            return data\n        elif fmt == 'object':\n            return [dict_to_object(item, \"NewsArticle\") for item in data]"}
{"prompt": "def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),\n                    with_links=True, in_search=False, predicate=None):\n        \"\"\"Returns an iterator over nodes hanging below a given start node.\n\n        :param node:\n\n            Start node\n\n        :param recursive:\n\n            Whether recursively also iterate over the children of the start node's children\n\n        :param max_depth:\n\n            Maximum depth to search for\n\n        :param in_search:\n\n            if it is used during get search and if detailed info should be returned\n\n        :param with_links:\n\n            If links should be considered\n\n        :param predicate:\n\n            A predicate to filter nodes\n\n        :return: Iterator\n\n        \"\"\"", "output": " included\n\n        :param predicate:\n\n            A function that takes a node as input and returns a boolean value indicating whether the node should be included in the iteration\n\n        :return:\n\n            An iterator over nodes hanging below the start node\n\n        \"\"\"\n        if in_search:\n            yield node\n\n        if max_depth == 0 or node is None:\n            return\n\n        if predicate is not None:\n            if predicate(node):\n                yield node\n\n        if recursive:\n            for child in node.children:\n                for n in self._iter_nodes(child, recursive", "solution": "\n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run names or run indices\n            run_list = predicate\n            run_name_set = set()\n            for item in run_list:\n                if item == -1:\n                    run_name_set.add(self._root_instance.f_wildcard('$', -1))\n                elif isinstance(item, int):\n                    run_name_set.add(self._root_instance.f_idx_to_run(item))\n                else:\n                    run_name_set.add(item)\n            predicate = lambda x: _run_predicate(x, run_name_set)\n\n        if recursive:\n            return NaturalNamingInterface._recursive_traversal_bfs(node,\n                                            self._root_instance._linked_by,\n                                            max_depth, with_links,\n                                            in_search, predicate)\n        else:\n            iterator = (x for x in self._make_child_iterator(node, with_links) if\n                        predicate(x[2]))\n            if in_search:\n                return iterator # Here we return tuples: (depth, name, object)\n            else:\n                return (x[2] for x in iterator)"}
{"prompt": "def du_handler(self, args):\n    \"\"\"Handler for size command\"\"\"", "output": "\n    size = int(args[0])\n    if size < 10:\n        self.send_message(\"Error: Size must be at least 10\")\n    elif size > 50:\n        self.send_message(\"Error: Size cannot be greater than 50\")\n    else:\n        self.send_message(f\"Size: {size}\")\n\ndef send_message(self, message):\n    \"\"\"Sends a message to the specified recipient\"\"\"\n    # Code to send message to recipient\n    pass\n\n", "solution": "\n    for src, size in self.s3handler().size(args[1:]):\n      message('%s\\t%s' % (size, src))"}
{"prompt": "def gauge(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Gauge metric with the specified value\"\"\"", "output": "\n        # Code to send the metric to a monitoring system or device\n        pass\n\n    def set(self, name, value):\n        # type: (str, float) -> None\n        \"\"\"Set the value of a Gauge metric\"\"\"\n        # Code to set the value of the gauge\n        pass\n\n    def get(self, name):\n        # type: (str) -> float\n        \"\"\"Get the value of a Gauge metric\"\"\"\n        # Code to get the value of the gauge\n        pass\n\n    def delete(self, name):\n        # type: (str)", "solution": "\n\n        if self._should_send_metric(name, rate):\n            if not is_numeric(value):\n                value = float(value)\n            self._request(\n                Gauge(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )"}
{"prompt": "def watch_port_events(port, chip, pin_function_maps, event_queue,\n                      return_after_kbdint=False):\n    \"\"\"Waits for a port event. When a port event occurs it is placed onto the\n    event queue.\n\n    :param port: The port we are waiting for interrupts on (GPIOA/GPIOB).\n    :type port: int\n    :param chip: The chip we are waiting for interrupts on.\n    :type chip: :class:`pifacecommon.mcp23s17.MCP23S17`\n    :param pin_function_maps: A list of classes that have inheritted from\n        :class:`FunctionMap`\\ s describing what to do with events.\n    :type pin_function_maps: list\n    :param event_queue: A queue to put events on.\n    :type event_queue: :py:class:`multiprocessing.Queue`\n    \"\"\"", "output": "17`\n    :param pin_function_maps: A list of pin maps for the given port.\n    :type pin_function_maps: List[List[int]]\n    :param event_queue: The queue to place the port event onto.\n    :type event_queue: :class:`queue.Queue`\n    :param return_after_kbdint: If True, returns immediately after an interrupt\n                                 occurs. If False (default), waits for all interrupts\n                                 before returning.\n    :type return_after_kbdint: bool\n    :", "solution": "\n    # set up epoll\n    gpio25 = open(GPIO_INTERRUPT_DEVICE_VALUE, 'r')  # change to use 'with'?\n    epoll = select.epoll()\n    epoll.register(gpio25, select.EPOLLIN | select.EPOLLET)\n\n    while True:\n        # wait here until input\n        try:\n            events = epoll.poll()\n        except KeyboardInterrupt as e:\n            if return_after_kbdint:\n                return\n            else:\n                raise e\n        except IOError as e:\n            # ignore \"Interrupted system call\" error.\n            # I don't really like this solution. Ignoring problems is bad!\n            if e.errno != errno.EINTR:\n                raise\n\n        # find out where the interrupt came from and put it on the event queue\n        if port == pifacecommon.mcp23s17.GPIOA:\n            interrupt_flag = chip.intfa.value\n        else:\n            interrupt_flag = chip.intfb.value\n\n        if interrupt_flag == 0:\n            continue  # The interrupt has not been flagged on this board\n        else:\n            if port == pifacecommon.mcp23s17.GPIOA:\n                interrupt_capture = chip.intcapa.value\n            else:\n                interrupt_capture = chip.intcapb.value\n            event_queue.add_event(InterruptEvent(\n                interrupt_flag, interrupt_capture, chip, time.time()))\n\n    epoll.close()"}
{"prompt": "def _parse_retry_after(self, response):\n        \"\"\"Parse Retry-After header from response if it is set.\"\"\"", "output": "\n        if 'Retry-After' in response.headers:\n            try:\n                return int(response.headers['Retry-After'])\n            except ValueError:\n                pass\n        return None\n\n    def _parse_cookies(self, response):\n        \"\"\"Parse Cookies header from response if it is set.\"\"\"\n        if 'Set-Cookie' in response.headers:\n            try:\n                return response.headers['Set-Cookie'].split(';')\n            except ValueError:\n                pass\n        return []\n\n    def _parse_", "solution": "\n        value = response.headers.get('Retry-After')\n\n        if not value:\n            seconds = 0\n        elif re.match(r'^\\s*[0-9]+\\s*$', value):\n            seconds = int(value)\n        else:\n            date_tuple = email.utils.parsedate(value)\n            if date_tuple is None:\n                seconds = 0\n            else:\n                seconds = time.mktime(date_tuple) - time.time()\n        return max(0, seconds)"}
{"prompt": "def _utf8_params(params):\n    \"\"\"encode a dictionary of URL parameters (including iterables) as utf-8\"\"\"", "output": "\n    encoded_params = []\n    for key, value in params.items():\n        if isinstance(value, str):\n            encoded_params.append(value.encode('utf-8'))\n        elif isinstance(value, (list, tuple)):\n            encoded_params.append(b','.join(value).encode('utf-8'))\n        else:\n            encoded_params.append(str(value).encode('utf-8'))\n    return b''.join(encoded_params)\n\n", "solution": "\n    assert isinstance(params, dict)\n    encoded_params = []\n    for k, v in params.items():\n        if v is None:\n            continue\n        if isinstance(v, integer_types + (float,)):\n            v = str(v)\n        if isinstance(v, (list, tuple)):\n            v = [to_bytes(x) for x in v]\n        else:\n            v = to_bytes(v)\n        encoded_params.append((k, v))\n    return dict(encoded_params)"}
{"prompt": "def parse_variant_id(chrom, pos, ref, alt, variant_type):\n    \"\"\"Parse the variant id for a variant\n\n    variant_id is used to identify variants within a certain type of\n    analysis. It is not human readable since it is a md5 key.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        variant_id(str): The variant id converted to md5 string\n    \"\"\"", "output": " format\n    \"\"\"\n    variant_id = hashlib.md5(\n        chrom.encode() + pos.encode() + ref.encode() + alt.encode()\n    ).hexdigest()\n    return variant_id\n\n", "solution": "\n    return generate_md5_key([chrom, pos, ref, alt, variant_type])"}
{"prompt": "def _construct_schema(elements, nsmap):\n    \"\"\"Consruct fiona schema based on given elements\n\n    :param list Element: list of elements\n    :param dict nsmap: namespace map\n\n    :return dict: schema\n    \"\"\"", "output": "\n    schema = {}\n    for element in elements:\n        if element.tag in nsmap:\n            schema[element.tag] = element.attrib\n    return schema\n\n", "solution": "\n\n    schema = {\n        'properties': {},\n        'geometry': None\n    }\n\n    schema_key = None\n    gml_key = None\n\n    # if nsmap is defined, use it\n    if nsmap:\n        for key in nsmap:\n            if nsmap[key] == XS_NAMESPACE:\n                schema_key = key\n            if nsmap[key] in GML_NAMESPACES:\n                gml_key = key\n    # if no nsmap is defined, we have to guess\n    else:\n        gml_key = 'gml'\n        schema_key = 'xsd'\n\n    mappings = {\n        'PointPropertyType': 'Point',\n        'PolygonPropertyType': 'Polygon',\n        'LineStringPropertyType': 'LineString',\n        'MultiPointPropertyType': 'MultiPoint',\n        'MultiLineStringPropertyType': 'MultiLineString',\n        'MultiPolygonPropertyType': 'MultiPolygon',\n        'MultiGeometryPropertyType': 'MultiGeometry',\n        'GeometryPropertyType': 'GeometryCollection',\n        'SurfacePropertyType': '3D Polygon',\n        'MultiSurfacePropertyType': '3D MultiPolygon'\n    }\n\n    for element in elements:\n        data_type = element.attrib['type'].replace(gml_key + ':', '')\n        name = element.attrib['name']\n\n        if data_type in mappings:\n            schema['geometry'] = mappings[data_type]\n            schema['geometry_column'] = name\n        else:\n            schema['properties'][name] = data_type.replace(schema_key+':', '')\n\n    if schema['properties'] or schema['geometry']:\n        return schema\n    else:\n        return None"}
{"prompt": "def _flatten(child, parent):\n    \"\"\"Custom flattening method for the parse tree.\"\"\"", "output": "\n    result = []\n    for child in child:\n        result.extend(_flatten(child, parent))\n    result.append((parent, child))\n    return result\n\n", "solution": "\n    return parent.is_type(TokenType.expression) and child.node_type == parent.node_type"}
{"prompt": "def read(self, resource_id):\n        \"\"\"Read object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be read.\n        :return: object value from elasticsearch.\n        \"\"\"", "output": "\n        return self.client.get(resource_id)\n\n    def write(self, resource_id, value):\n        \"\"\"Write object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be written.\n        :param value: object value to be written in elasticsearch.\n        \"\"\"\n        return self.client.update(resource_id, value)\n\n    def delete(self, resource_id):\n        \"\"\"Delete object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be", "solution": "\n        self.logger.debug('elasticsearch::read::{}'.format(resource_id))\n        return self.driver._es.get(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )['_source']"}
{"prompt": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"", "output": "\n        # calculate the rate variation for the given node\n        rate_variation = node.branch_lengths[0] / node.branch_lengths[1]\n\n        # estimate the uncertainty in the rate variation using the previously calculated interval\n        uncertainty = interval[1] - interval[0] * rate_variation\n\n        return uncertainty\n\n    def date_uncertainty_due_to_branch_length(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the branch length to estimate\n        the", "solution": "\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None"}
{"prompt": "def score(self, x, w=None, **kwargs):\n        \"\"\"Compute R^2 coefficient of determination for a given input.\n\n        Parameters\n        ----------\n        x : ndarray (num-examples, num-inputs)\n            An array containing data to be fed into the network. Multiple\n            examples are arranged as rows in this array, with columns containing\n            the variables for each example.\n\n        Returns\n        -------\n        r2 : float\n            The R^2 correlation between the prediction of this netork and its\n            input. This can serve as one measure of the information loss of the\n            autoencoder.\n        \"\"\"", "output": " a measure of how well the network fits\n            the data.\n\n        Other Parameters\n        ----------------\n        w : ndarray (num-examples, num-inputs)\n            An array containing weights to be used in the calculation of\n            the R^2 coefficient of determination. If None, the weights are\n            assumed to be 1.\n\n        Returns\n        -------\n        r2 : float\n            The R^2 correlation between the prediction of this netork and its\n            input.\n\n        \"\"\"\n        if w is None:\n            w = np.ones(x.shape", "solution": "\n        return super(Autoencoder, self).score(x, x, w=w, **kwargs)"}
{"prompt": "def set_parent(self, parent):\n        \"\"\"Set parent ``Expression`` for this object.\n\n        Args:\n            parent (Expression): The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent must be of type ``Expression``.\n        \"\"\"", "output": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of type Expression.\")\n        self.parent = parent\n\n    def evaluate(self, context):\n        \"\"\"Evaluate this expression and return the result.\n\n        Args:\n            context (dict): A dictionary containing variables and their values.\n\n        Returns:\n            Expression: The result of evaluating this expression.\n        \"\"\"\n        return self.parent.evaluate(context)\n\n    def __str__(self):\n        return str(self.evaluate(None))\n\n    def __re", "solution": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(parent)))\n        self.parent = parent"}
{"prompt": "def _build_kernel_function_declaration(self, name='kernel'):\n        \"\"\"Build and return kernel function declaration\"\"\"", "output": "\n        args = ', '.join(self.args)\n        body = '\\n'.join(self.body)\n        return f\"{name}({args})\\n{'\\n'.join(body)}\"\n\n    def _build_function_call(self, name='kernel'):\n        \"\"\"Build and return function call\"\"\"\n        args = ', '.join(self.args)\n        body = '\\n'.join(self.body)\n        return f\"{name}({args})\\n{body}\"\n\n    def _build_function_body(self", "solution": "\n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                              type=c_ast.TypeDecl(declname=name,\n                                                  quals=[],\n                                                  type=c_ast.IdentifierType(names=['void'])))"}
{"prompt": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"", "output": " generate for each sequence.\n\n  Returns:\n    A tensor of shape [batch, samples, timesteps, h, w, c] representing the\n    visualization of the qualitative analysis of the model.\n  \"\"\"\n  # Sample from the latent distribution\n  z_samples = inputs[:, :length]\n  z_samples = tf.reshape(z_samples, [-1, length, z_samples.shape[-1]])\n\n  # Generate sequences from the sampled z_samples\n  sequences = model.generate(z_s", "solution": "\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")"}
{"prompt": "def __single_arity_fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    method: FnMethod,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"", "output": "\n    # Create a new AST node for the function\n    func_node = ast.FunctionDef(\n        name=def_name or node.name,\n        args=ast.arguments(\n            args=[ast.arg(arg=\"self\", annotation=None)],\n            vararg=None,\n            kwonlyargs=[],\n            kw_defaults=[],\n            kwarg=None,\n            defaults=[],\n        ),\n        body=[ast.Return(value=node)],\n        decorator_list=[],\n        returns=None,\n", "solution": "\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )"}
{"prompt": "def activate(fn=None):\n    \"\"\"\n    Enables the HTTP traffic interceptors.\n\n    This function can be used as decorator.\n\n    Arguments:\n        fn (function|coroutinefunction): Optional function argument\n            if used as decorator.\n\n    Returns:\n        function: decorator wrapper function, only if called as decorator,\n            otherwise ``None``.\n\n    Example::\n\n        # Standard use case\n        pook.activate()\n        pook.mock('server.com/foo').reply(404)\n\n        res = requests.get('server.com/foo')\n        assert res.status_code == 404\n        pook.disable()\n\n        # Decorator use case\n        @pook.activate\n        def test_request():\n            pook.mock('server.com/foo').reply(404)\n\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"", "output": "http://example.com/foo')\n        assert res.status_code == 404\n\n        # Example with decorator\n        @pook.activate\n        def foo():\n            # Code to intercept HTTP traffic\n            pass\n\n        foo() # Raises HTTPError\n\n    \"\"\"\n    def decorator(fn):\n        if fn is None:\n            return None\n        if not callable(fn):\n            raise TypeError(\"Argument must be callable\")\n        setattr(fn, \"_pook\", True)\n        return fn\n    return decorator if fn is not None", "solution": "\n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper"}
{"prompt": "def parse(self, string):\n        \"\"\"\n        Parse lipid definition from string:\n\n            alhead=C P, allink=A A, altail=TCC CCCC, alname=DPSM, charge=0.0\n        \"\"\"", "output": "\n        # Split the input string into individual lines\n        lines = string.strip().split('\\n')\n\n        # Extract the individual lipid definitions\n        alhead, allink, altail, alname, charge = [line.strip() for line in lines]\n\n        # Split the individual definitions into individual components\n        self.alhead = alhead.split()[1:]\n        self.allink = allink.split()[1:]\n        self.altail = altail.split()[1:]\n        self.alname = alname.split()[1:]\n", "solution": "\n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val  = val.split()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val\n            elif what.endswith(\"tail\"):\n                self.tail = val\n            elif what == \"charge\":\n                self.charge = float(val[0])\n            elif what.endswith(\"name\") and not self.name:\n                self.name = val[0]\n        if self.charge is None:\n            # Infer charge from head groups\n            self.charge = sum([headgroup_charges[bead] for bead in self.head])"}
{"prompt": "def combine_columns_aggregation(\n        df,\n        id_cols: List[str],\n        cols_for_combination: Dict[str, str],\n        agg_func: Union[str, List[str], Dict[str, str]] = 'sum'\n):\n    \"\"\"\n    Aggregates data to reproduce \"All\" category for requester\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): the columns id to group\n    - `cols_for_combination` (*dict*): colums corresponding to\n       the filters as key and their default value as value\n\n    *optional :*\n    - `agg_func` (*str*, *list* or *dict*): the function(s) to use for aggregating the data.\n       Accepted combinations are:\n       - string function name\n       - list of functions and/or function names, e.g. [np.sum, 'mean']\n       - dict of axis labels -> functions, function names or list of such.\n    \"\"\"", "output": "ination` (*dict*): the columns to combine\n    - `agg_func` (*str* or *list* or *dict*): the aggregation function to apply\n\n    *optional :*\n    - `id_col` (*str*): the column to use as the id for the group\n    - `cols_for_combination` (*dict*): the columns to combine for the group\n\n    *returns* :\n    - `pd.DataFrame`: the aggregated data\n\n    Example\n\n    >>> df = pd.DataFrame({\n   ...     'id':", "solution": "\n    requesters_cols = list(cols_for_combination.keys())\n    requester_combination = [\n        list(item) for i in range(0, len(requesters_cols) + 1)\n        for item in itertools.combinations(requesters_cols, i)]\n    dfs_result = []\n    for comb in requester_combination:\n        df_tmp = df.groupby(id_cols + comb).agg(agg_func).reset_index()\n        for key in (set(cols_for_combination.keys()) - set(comb)):\n            df_tmp[key] = cols_for_combination[key]\n        dfs_result.append(df_tmp)\n\n    return pd.concat(dfs_result, sort=False, ignore_index=True)"}
{"prompt": "def _backup_file(self, file, patch):\n        \"\"\" Creates a backup of file \"\"\"", "output": "\n        with open(file, 'rb') as f:\n            data = f.read()\n        with open(file + '.bak', 'wb') as f:\n            f.write(data)\n        for line in patch:\n            with open(file + '.bak', 'a') as f:\n                f.write(line + '\\n')\n        os.remove(file)\n\n    def _apply_patch(self, patch):\n        \"\"\" Applies patch to file and returns the result \"\"\"\n        with open(self.file, 'rb') as", "solution": "\n        dest_dir = self.quilt_pc + patch.get_name()\n        file_dir = file.get_directory()\n        if file_dir:\n            #TODO get relative path\n            dest_dir = dest_dir + file_dir\n        backup = Backup()\n        backup.backup_file(file, dest_dir, copy_empty=True)"}
{"prompt": "def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request.\n\n        Args:\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                erc(int): The expected (success) response code for the request.\n                others: Passed on to the requests package.\n\n        Raises:\n            ApiError: If anything other than the expected response code is\n                returned by the Webex Teams API endpoint.\n\n        \"\"\"", "output": "\n        if 'erc' in kwargs:\n            kwargs['erc'] = int(kwargs['erc'])\n        else:\n            kwargs['erc'] = 200\n        response = requests.delete(url, **kwargs)\n        if response.status_code!= kwargs['erc']:\n            raise ApiError('Unexpected response code: {}'.format(response.status_code))\n        return response.json()\n\n    def get_team_members(self, team_id, **kwargs):\n        \"\"\"Sends a GET request to the", "solution": "\n        check_type(url, basestring, may_be_none=False)\n\n        # Expected response code\n        erc = kwargs.pop('erc', EXPECTED_RESPONSE_CODE['DELETE'])\n\n        self.request('DELETE', url, erc, **kwargs)"}
{"prompt": "def meteor_random_id(name=None, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"", "output": "\n    if name:\n        return name + str(uuid.uuid4())[-8:]\n    else:\n        return str(uuid.uuid4())[-8:]\n\n", "solution": "\n    if name is None:\n        stream = THREAD_LOCAL.alea_random\n    else:\n        stream = THREAD_LOCAL.random_streams[name]\n    return stream.random_string(length, METEOR_ID_CHARS)"}
{"prompt": "def INIT(self):\n        \"\"\"INIT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n        .. todo::\n           - The initial delay is implemented, but probably is not in other\n             implementations. Check what other implementations do.\n        \"\"\"", "output": "\n        time.sleep(random.randint(1, 10))\n\n    def handle_message(self, message):\n        \"\"\"Handle a message from the client.\n\n        [:rfc:`2131#section-4.4.2`]::\n\n            The message should be in the format \"header1=value1,header2=value2,...\"\n\n       .. todo::\n           - The handling of the message is not implemented, but should be\n             done by the caller.\n        \"\"\"\n        # Parse the message into a dictionary of headers and values\n        headers", "solution": "\n        # NOTE: in case INIT is reached from other state, initialize attributes\n        # reset all variables.\n        logger.debug('In state: INIT')\n        if self.current_state is not STATE_PREINIT:\n            self.reset()\n        self.current_state = STATE_INIT\n        # NOTE: see previous TODO, maybe this is not needed.\n        if self.delay_selecting:\n            if self.delay_before_selecting is None:\n                delay_before_selecting = gen_delay_selecting()\n            else:\n                delay_before_selecting = self.delay_before_selecting\n        else:\n            delay_before_selecting = 0\n        self.set_timeout(self.current_state,\n                         self.timeout_delay_before_selecting,\n                         delay_before_selecting)\n        if self.timeout_select is not None:\n            self.set_timeout(STATE_SELECTING,\n                             self.timeout_selecting,\n                             self.timeout_select)"}
{"prompt": "def bisect(seq, func=bool):\n\t\"\"\"\n\tSplit a sequence into two sequences:  the first is elements that\n\treturn False for func(element) and the second for True for\n\tfunc(element).\n\tBy default, func is ``bool``, so uses the truth value of the object.\n\n\t>>> is_odd = lambda n: n%2\n\t>>> even, odd = bisect(range(5), is_odd)\n\t>>> list(odd)\n\t[1, 3]\n\t>>> list(even)\n\t[0, 2, 4]\n\n\t>>> other, zeros = bisect(reversed(range(5)))\n\t>>> list(zeros)\n\t[0]\n\t>>> list(other)\n\t[4, 3, 2, 1]\n\n\t\"\"\"", "output": " 4]\n\t\"\"\"\n\ttrue_list = [elem for elem in seq if func(elem)]\n    false_list = [elem for elem in seq if not func(elem)]\n    return true_list, false_list\n\n", "solution": "\n\tqueues = GroupbySaved(seq, func)\n\treturn queues.get_first_n_queues(2)"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default:\n            print(prompt + \" (default)\")\n        else:\n            print(prompt)\n        return input().lower() == \"y\"\n\n", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def check_type(self, value, attr, data):\n        \"\"\"Validates a value against the correct type of the field.\n\n        It calls ``_expected_types`` to get a list of valid types.\n\n        Subclasses can do one of the following:\n\n            1. They can override the ``valid_types`` property with a tuple with\n            the expected types for this field.\n\n            2. They can override the ``_expected_types`` method to return a\n            tuple of expected types for the field.\n\n            3. They can change ``check_type`` completely to customize\n            validation.\n\n        This method or the overrides must return the ``value`` parameter\n        untouched.\n        \"\"\"", "output": " raise a\n            TypeError if the value is not of the expected type.\n\n        Args:\n            value: The value to validate.\n            attr: The attribute name of the value.\n            data: The data dictionary containing the value.\n\n        Returns:\n            None\n        \"\"\"\n        if attr in self.required_attrs:\n            expected_type = self.required_types[attr]\n            if not isinstance(value, expected_type):\n                raise TypeError(\n                    \"{} must be {}\".format(attr, expected_type.__name__)", "solution": "\n        expected_types = self._expected_types()\n        if not isinstance(value, expected_types):\n            raise self._not_expected_type(\n                value, expected_types, fields=[self], field_names=attr, data=data)\n        return value"}
{"prompt": "def _get_selection_cursor(self, start, end):\n        \"\"\" Convenience method that returns a cursor with text selected between\n            the positions 'start' and 'end'.\n        \"\"\"", "output": "\n        return f\"{start}:{end}\"\n\n    def _get_selection_text(self, cursor):\n        \"\"\" Convenience method that returns the text selected by the given cursor.\n        \"\"\"\n        start, end = cursor.split(\":\")\n        return self.text[int(start):int(end)]\n\n    def _select_text(self, cursor):\n        \"\"\" Convenience method that selects the text between the given cursor and the end\n            of the text.\n        \"\"\"\n        text = self.text\n        start, end = cursor.split(", "solution": "\n        cursor = self._control.textCursor()\n        cursor.setPosition(start)\n        cursor.setPosition(end, QtGui.QTextCursor.KeepAnchor)\n        return cursor"}
{"prompt": "def delay(self, sender, receiver, packet, add_delay=0):\n        \"\"\"\n        bandwidths are inaccurate, as we don't account for parallel transfers here\n        \"\"\"", "output": "\n        if packet.bandwidth <= sender.bandwidth:\n            return\n        if packet.bandwidth >= receiver.bandwidth:\n            return\n        delay = min(sender.bandwidth, receiver.bandwidth) - packet.bandwidth\n        if delay > 0:\n            packet.bandwidth += add_delay\n            sender.bandwidth -= add_delay\n            receiver.bandwidth -= add_delay\n\n    def send_packet(self, packet):\n        \"\"\"\n        send the packet to the appropriate location based on bandwidth\n        \"\"\"\n        for location", "solution": "\n        bw = min(sender.ul_bandwidth, receiver.dl_bandwidth)\n        delay = sender.base_latency + receiver.base_latency\n        delay += len(packet) / bw\n        delay += add_delay\n        return delay"}
{"prompt": "def look_ahead_match(rating, tokens):\n    \"\"\"iterate through all tokens looking for matches of cleansed tokens\n    or token regexes, skipping tokens left empty by cleansing and\n    coping with Token objects that produce multiple space-separated\n    strings when cleansed.  Yields tokens that match.\n\n    \"\"\"", "output": "\n    for token in tokens:\n        if token.cleansed:\n            cleansed_token = token.cleansed.strip()\n            if cleansed_token in rating:\n                yield token\n        else:\n            for regex in token.regexes:\n                match = regex.search(rating)\n                if match:\n                    yield token\n                    break\n\n", "solution": "\n    ## this ensures that all cleansed tokens are non-zero length\n    all_mregexes = []\n    for m in rating.mentions:\n        mregexes = []\n        mpatterns = m.decode('utf8').split(' ')\n        for mpat in mpatterns:\n            if mpat.startswith('ur\"^') and mpat.endswith('$\"'): # is not regex\n                ## chop out the meat of the regex so we can reconstitute it below\n                mpat = mpat[4:-2]\n            else:\n                mpat = cleanse(mpat)\n            if mpat:\n                ## make a unicode raw string\n                ## https://docs.python.org/2/reference/lexical_analysis.html#string-literals\n                mpat = ur'^%s$' % mpat\n                logger.debug('look_ahead_match compiling regex: %s', mpat)\n                mregexes.append(re.compile(mpat, re.UNICODE | re.IGNORECASE))\n\n        if not mregexes:\n            logger.warn('got empty cleansed mention: %r\\nrating=%r' % (m, rating))\n\n        all_mregexes.append(mregexes)\n\n    ## now that we have all_mregexes, go through all the tokens\n    for i in range(len(tokens)):\n        for mregexes in all_mregexes:\n            if mregexes[0].match(tokens[i][0][0]):\n                ## found the start of a possible match, so iterate\n                ## through the tuples of cleansed strings for each\n                ## Token while stepping through the cleansed strings\n                ## for this mention.\n                m_j = 1\n                i_j = 0\n                last_token_matched = 0\n                matched = True\n                while m_j < len(mregexes):\n                    i_j += 1\n                    if i_j == len(tokens[i + last_token_matched][0]):\n                        i_j = 0\n                        last_token_matched += 1\n                        if i + last_token_matched == len(tokens):\n                            matched = False\n                            break\n                    target_token = tokens[i + last_token_matched][0][i_j]\n                    ## this next line is the actual string comparison\n                    if mregexes[m_j].match(target_token):\n                        m_j += 1\n                    elif target_token == '':\n                        continue\n                    else:\n                        matched = False\n                        break\n                if matched:\n                    ## yield each matched token only once\n                    toks = set()\n                    for j in xrange(last_token_matched + 1):\n                        toks.add(tokens[i + j][1])\n                    for tok in toks:\n                        yield tok"}
{"prompt": "def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):\n    \"\"\"return lines with leading/trailing whitespace and any ignored code\n    features removed\n    \"\"\"", "output": "\n    stripped_lines = []\n    for line in lines:\n        # ignore comments\n        if line.strip().startswith('#'):\n            continue\n        # ignore docstrings\n        if line.strip().startswith('\"\"\"') or line.strip().startswith('\"\"\"\"'):\n            continue\n        # ignore imports\n        if any(line.startswith(imp) for imp in ignore_imports):\n            continue\n        # remove leading/trailing whitespace\n        stripped_line = line.strip()\n        if stripped_line:", "solution": "\n    if ignore_imports:\n        tree = astroid.parse(\"\".join(lines))\n        node_is_import_by_lineno = (\n            (node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom)))\n            for node in tree.body\n        )\n        line_begins_import = {\n            lineno: all(is_import for _, is_import in node_is_import_group)\n            for lineno, node_is_import_group in groupby(\n                node_is_import_by_lineno, key=lambda x: x[0]\n            )\n        }\n        current_line_is_import = False\n\n    strippedlines = []\n    docstring = None\n    for lineno, line in enumerate(lines, start=1):\n        line = line.strip()\n        if ignore_docstrings:\n            if not docstring and any(\n                line.startswith(i) for i in ['"}
{"prompt": "def add_field(self, model, field):\n        \"\"\"Ran when a field is added to a model.\"\"\"", "output": "\n        self.model = model\n        self.field = field\n\ndef add_field_to_model(model, field):\n    \"\"\"\n    Adds a field to a model and returns the model.\n\n    Args:\n        model (Model): The model to add the field to.\n        field (Field): The field to add to the model.\n\n    Returns:\n        Model: The model with the added field.\n    \"\"\"\n    model.fields.append(field)\n    return model\n\n", "solution": "\n\n        for key in self._iterate_required_keys(field):\n            self._create_hstore_required(\n                model._meta.db_table,\n                field,\n                key\n            )"}
{"prompt": "def get_organisation_information(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of values.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        # Your code here to retrieve information about this organisation using the query_params\n        # and return it as a dictionary\n\n", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )"}
{"prompt": "def verified(context, collaborator, test, outpath=None):\n    \"\"\"Export variants which have been verified for an institute\n        and write them to an excel file.\n\n    Args:\n        collaborator(str): institute id\n        test(bool): True if the function is called for testing purposes\n        outpath(str): path to output file\n\n    Returns:\n        written_files(int): number of written or simulated files\n    \"\"\"", "output": "\n    written_files = 0\n    if test:\n        # simulate writing variants to an excel file\n        with open(f\"{outpath}/verified_variants.xlsx\", \"w\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"variant_id\", \"collaborator\", \"test_result\"])\n            for i in range(1, 101):\n                writer.writerow([f\"variant_{i}\", collaborator, \"passed\"])\n                written_files += 1\n    else:", "solution": "\n    written_files = 0\n    collaborator = collaborator or 'cust000'\n    LOG.info('Exporting verified variants for cust {}'.format(collaborator))\n\n    adapter = context.obj['adapter']\n    verified_vars = adapter.verified(institute_id=collaborator)\n    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))\n\n\n    if not verified_vars:\n        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))\n        return None\n\n    document_lines = export_verified_variants(verified_vars)\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'\n\n    # If this was a test and lines are created return success\n    if test and document_lines:\n        written_files +=1\n        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))\n        return written_files\n\n    # create workbook and new sheet\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n    workbook = Workbook(os.path.join(outpath,document_name))\n    Report_Sheet = workbook.add_worksheet()\n\n    # Write the column header\n    row = 0\n    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):\n        Report_Sheet.write(row,col,field)\n\n    # Write variant lines, after header (start at line 1)\n    for row, line in enumerate(document_lines,1): # each line becomes a row in the document\n        for col, field in enumerate(line): # each field in line becomes a cell\n            Report_Sheet.write(row,col,field)\n    workbook.close()\n\n    if os.path.exists(os.path.join(outpath,document_name)):\n        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))\n        written_files += 1\n\n    return written_files"}
{"prompt": "def _plot_result(self, traj, result_name):\n        \"\"\"Plots a state variable graph for several neurons into one figure\"\"\"", "output": "\n        fig, ax = plt.subplots()\n        for i, neuron in enumerate(traj):\n            ax.plot(neuron.time, neuron.state_variable, label=f\"Neuron {i+1}\")\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"State Variable\")\n        ax.set_title(f\"State Variables for {result_name}\")\n        ax.legend()\n        plt.show()\n\n    def _run_simulation(self, traj, result_", "solution": "\n        result = traj.f_get(result_name)\n        varname = result.record_variables[0]\n        values = result[varname]\n        times = result.t\n\n        record = result.record\n\n        for idx, celia_neuron in enumerate(record):\n            plt.subplot(len(record), 1, idx+1)\n            plt.plot(times, values[idx,:])\n            if idx==0:\n                plt.title('%s' % varname)\n            if idx==1:\n                plt.ylabel('%s' % ( varname))\n            if idx == len(record)-1:\n                plt.xlabel('t')"}
{"prompt": "def unregister_sizer(self, attr_name):\n        \"\"\"\n        Unregister the SizedImage subclass currently assigned to `attr_name`.\n\n        If a SizedImage subclass isn't already registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"", "output": "\n        if attr_name in self.__dict__:\n            del self.__dict__[attr_name]\n\n    def __setattr__(self, attr_name, value):\n        \"\"\"\n        Assign the `value` to the SizedImage subclass currently assigned to `attr_name`.\n\n        If `attr_name` is not a valid attribute name for a SizedImage subclass,\n        NotRegistered will raise.\n        \"\"\"\n        if attr_name in self.__dict__:\n            self.__dict__[attr_name] = value", "solution": "\n        if attr_name not in self._sizedimage_registry:\n            raise NotRegistered(\n                'No SizedImage subclass is registered to %s' % attr_name\n            )\n        else:\n            del self._sizedimage_registry[attr_name]"}
{"prompt": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"", "output": "\n        query = TitleQuery(**kwargs)\n        with self.assertRaises(ExpectationNotMet):\n            query.wait_for_title(title)\n\n    def assert_title_contains(self, title, substring, **kwargs):\n        \"\"\"\n        Asserts that the page's title contains the given substring.\n\n        Args:\n            title (str): The title of the page.\n            substring (str): The substring to search for.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        ", "solution": "\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()"}
{"prompt": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"", "output": "\n        response = requests.post(url, json=payload)\n        return response.json()\n\n    def __parse_group_response(self, response):\n        \"\"\"Parse group response from groupsio API\"\"\"\n        groups = []\n        for group in response:\n            members = []\n            for member in group['members']:\n                members.append({\n                    'name': member['name'],\n                    'email': member['email']\n                })\n            groups.append({\n                'name': group['name'],\n               'members': members\n            })", "solution": "\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r"}
{"prompt": "def f_shrink(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not store explored parameters to disk set or you deleted all\n            of them from disk set `force=True`.\n\n        :raises: TypeError if the trajectory was stored before.\n\n        \"\"\"", "output": "        :return:\n\n            The new parameters of the trajectory after shrinking.\n\n        \"\"\"\n        if not force:\n            # If we were not storing to disk, we can shrink the trajectory\n            new_params = []\n            for p in self.params:\n                if p.is_explored:\n                    new_params.append(p.get_value())\n                else:\n                    new_params.append(p.get_value_range()[0])\n            return Trajectory(new_params)\n        else:\n            # If we were storing to disk,", "solution": "\n        if self._stored and not force:\n            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '\n                            'not allowed.')\n\n        for param in self._explored_parameters.values():\n            param.f_unlock()\n            try:\n                param._shrink()\n            except Exception as exc:\n                self._logger.error('Could not shrink `%s` because of:`%s`' %\n                                   (param.v_full_name, repr(exc)))\n\n        # If we shrink, we do not have any explored parameters left and we can erase all\n        # run information, and the length of the trajectory is 1 again.\n        self._explored_parameters = {}\n        self._run_information = {}\n        self._single_run_ids = {}\n        self._add_run_info(0)\n        self._test_run_addition(1)"}
{"prompt": "def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        \"\"\"\n        Retrieves the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        \"\"\"", "output": "\n        # Code to retrieve data disk from virtual machine\n        data_disk = {'name': 'data_disk','size': '10G', 'location': 'disk1'}\n        return data_disk\n\n", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)"}
{"prompt": "def for_type(typ, func):\n    \"\"\"\n    Add a pretty printer for a given type.\n    \"\"\"", "output": "\n    def wrapper(func):\n        func.__pretty_printer__ = lambda self, *args, **kwargs: typ.__pretty_printer__(self, *args, **kwargs)\n        return func\n    return wrapper\n\n", "solution": "\n    oldfunc = _type_pprinters.get(typ, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _type_pprinters[typ] = func\n    return oldfunc"}
{"prompt": "def structured_mesh(shape = (2,2,2), dim = (1.,1.,1.)):\n    \"\"\"\n    Returns a structured mesh. \n    \n    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).\n    :type shape: tuple\n    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))\n    :type dim: tuple\n    \n    .. note::\n\n       This function does not use GMSH for mesh generation.\n\n\n    \n    >>> import argiope as ag\n    >>> mesh = ag.mesh.structured_mesh(shape =(10,10,10), dim=(1.,1.,1.)))\n    \"\"\"", "output": "\n    >>> mesh = argiope.structure_mesh(shape = (10, 10, 10), dim = (4., 2., 1.))\n    >>> mesh.shape\n    (10, 10, 10)\n    >>> mesh.dim\n    (4.0, 2.0, 1.0)\n    \"\"\"\n    if len(shape) == 2:\n        shape = (shape[0], 1, shape[1])\n    elif len(shape)!= 3:\n        raise ValueError(\"Shape must be 2 or 3 integers.\")\n    if len(", "solution": "\n    # PREPROCESSING\n    shape = np.array(shape)\n    dim   = np.array(dim) \n    Ne = shape.prod()\n    Nn = (shape + 1).prod()\n    # LABELS\n    nindex = np.arange(Nn) + 1\n    eindex = np.arange(Ne) + 1\n    # COORDINATES\n    coords = [ np.linspace(0., dim[i], shape[i] + 1) for i in range(len(shape))]\n    coords = np.array(np.meshgrid(*coords))\n    coords = np.array([c.swapaxes(0,1).flatten(\"F\") for c in coords]).T\n    if len(shape) == 2:\n        c = coords\n        coords = np.zeros((Nn, 3))\n        coords[:, :2] = c  \n    # CONNECTIVITY    \n    conn = _make_conn(shape)\n    # MESH INSTANCE\n    mesh = Mesh(nlabels = nindex,\n                coords  = coords,\n                elabels = eindex,\n                conn = conn,)\n    if len(shape) == 2: mesh.elements[(\"type\", \"argiope\")] = \"quad4\"\n    if len(shape) == 3: mesh.elements[(\"type\", \"argiope\")] = \"hexa8\"    \n    return mesh"}
{"prompt": "def inshape(shape, axes):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        the shape of a BoltArray\n\n    axes : tuple[int]\n        the axes to check against shape\n    \"\"\"", "output": "\n    for i, dim in enumerate(shape):\n        if i not in axes:\n            return False\n    return True\n\n", "solution": "\n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"axes not valid for an ndarray of shape: %s\" % str(shape))"}
{"prompt": "def desc(value):\n    \"\"\" str: A normalized representation for a user-provided value. \"\"\"", "output": "\n    return str(value)\n\n", "solution": "\n\n    def normalize_strings(value):\n        if isinstance(value, list):\n            value = [normalize_strings(e) for e in value]\n\n        if isinstance(value, dict):\n            value = {normalize_strings(k): normalize_strings(v) for k, v in iter(value.items())}\n\n        if isregex(value):\n            value = value.pattern\n\n        if isbytes(value):\n            value = decode_bytes(value)\n\n        if PY2:\n            if isstring(value):\n                # In Python 2, strings (``unicode`` objects) represent as ``u'...'``, so ensure\n                # the string is encoded (as a ``str`` object) for cleaner representation.\n                value = encode_string(value)\n\n        return value\n\n    value = normalize_strings(value)\n\n    return repr(value)"}
{"prompt": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True):\n        \"\"\"Recommend top K items for all users which are in the test set.\n\n        Args:\n            test: test Spark dataframe\n            top_k: top n items to return\n            remove_seen: remove items test users have already seen in the past from the recommended set.\n        \"\"\"", "output": "\n        # Remove seen items if remove_seen is True\n        if remove_seen:\n            self.test = self.test.filter(self.test['user_id'].isin(test['user_id']))\n\n        # Join test dataframe with user-item dataframe\n        joined_df = self.test.join(test.selectExpr('user_id', 'item_id', 'rating'), 'user_id')\n\n        # Calculate similarity between items\n        similarity_df = joined_df.selectExpr('user_id', 'item_id", "solution": "\n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(\n            "}
{"prompt": "def basis_state(str_state, num):\n    \"\"\"\n    Return a basis state ndarray.\n\n    Args:\n        str_state (string): a string representing the state.\n        num (int): the number of qubits\n    Returns:\n        ndarray:  state(2**num) a quantum state with basis basis state.\n     Raises:\n        QiskitError: if the dimensions is wrong\n    \"\"\"", "output": "\n    if num < 0:\n        raise QiskitError(\"Number of qubits must be non-negative.\")\n    if num == 0:\n        return np.array([1, 0])\n    if num == 1:\n        return np.array([0, 1])\n    if num % 2 == 1:\n        return np.array([0, 1])\n    return np.kron(basis_state(str_state, num-1), basis_state(str_state, 1))\n\n", "solution": "\n    n = int(str_state, 2)\n    if num >= len(str_state):\n        state = np.zeros(1 << num, dtype=complex)\n        state[n] = 1\n        return state\n    else:\n        raise QiskitError('size of bitstring is greater than num.')"}
{"prompt": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs):\n  \"\"\"Returns (batched) matmul of a SparseTensor with a Tensor.\n\n  Args:\n    sp_a: `SparseTensor` representing a (batch of) matrices.\n    b: `Tensor` representing a (batch of) matrices, with the same batch shape of\n      `sp_a`. The shape must be compatible with the shape of `sp_a` and kwargs.\n    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul`.\n\n  Returns:\n    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and\n    dtype as `sp_a` and `b`. If `sp_a` or `b` is adjointed through `kwargs` then\n    the shape is adjusted accordingly.\n  \"\"\"", "output": "word arguments to be passed to `tf.sparse_tensor_dense_matmul`.\n\n  Returns:\n    A `Tensor` representing the result of the matmul operation.\n\n  Raises:\n    ValueError: If `sp_a` and `b` have incompatible shapes, or if `sp_a` and `b` are not\n      SparseTensors or Tensors.\n  \"\"\"\n  if not isinstance(sp_a, tf.SparseTensor) or not isinstance(b, tf.Tensor):\n    raise Value", "solution": "\n  batch_shape = _get_shape(sp_a)[:-2]\n\n  # Reshape the SparseTensor into a rank 3 SparseTensors, with the\n  # batch shape flattened to a single dimension. If the batch rank is 0, then\n  # we add a batch dimension of rank 1.\n  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],\n                                           axis=0))\n  # Reshape b to stack the batch dimension along the rows.\n  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n  # Convert the SparseTensor to a matrix in block diagonal form with blocks of\n  # matrices [M, N]. This allow us to use tf.sparse_tensor_dense_matmul which\n  # only accepts rank 2 (Sparse)Tensors.\n  out = tf.sparse.sparse_dense_matmul(_sparse_block_diag(sp_a), b, **kwargs)\n\n  # Finally retrieve the original batch shape from the resulting rank 2 Tensor.\n  # Note that we avoid inferring the final shape from `sp_a` or `b` because we\n  # might have transposed one or both of them.\n  return tf.reshape(\n      out,\n      tf.concat([batch_shape, [-1], _get_shape(out)[-1:]], axis=0))"}
{"prompt": "def require_parents(packages):\n        \"\"\"\n        Exclude any apparent package that apparently doesn't include its\n        parent.\n\n        For example, exclude 'foo.bar' if 'foo' is not present.\n        \"\"\"", "output": "\n        return [p for p in packages if '.' not in p]\n\n    return [require_parents(p) for p in packages]\n\n", "solution": "\n        found = []\n        for pkg in packages:\n            base, sep, child = pkg.rpartition('.')\n            if base and base not in found:\n                continue\n            found.append(pkg)\n            yield pkg"}
{"prompt": "def make_report(self,traceback):\n        \"\"\"Return a string containing a crash report.\"\"\"", "output": "\n        report = \"Crash Report:\\n\"\n        report += \"Traceback (most recent call last):\\n\"\n        report += traceback\n        report += \"\\nSteps to reproduce the error:\\n\"\n        for step in self.steps:\n            report += step.make_report()\n        return report\n\n", "solution": "\n\n        sec_sep = self.section_sep\n\n        report = ['*'*75+'\\n\\n'+'IPython post-mortem report\\n\\n']\n        rpt_add = report.append\n        rpt_add(sys_info())\n\n        try:\n            config = pformat(self.app.config)\n            rpt_add(sec_sep)\n            rpt_add('Application name: %s\\n\\n' % self.app_name)\n            rpt_add('Current user configuration structure:\\n\\n')\n            rpt_add(config)\n        except:\n            pass\n        rpt_add(sec_sep+'Crash traceback:\\n\\n' + traceback)\n\n        return ''.join(report)"}
{"prompt": "def _fmt(self, string):\n        \"\"\"Helper method to format our name into a string.\"\"\"", "output": "\n        return string.upper()\n\n    def __str__(self):\n        \"\"\"\n        Returns a formatted string representation of the Person object.\n        \"\"\"\n        return f\"Name: {self.name}, Age: {self.age}, Gender: {self.gender}\"\n\n", "solution": "\n        if '{' not in string:\n            string = '{}.' + string\n        return string.format(self.name)"}
{"prompt": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"", "output": "\n\n  def fake_image(height, width, channels):\n    \"\"\"Creates a fake image with the given dimensions and pixel values.\"\"\"\n    image = np.zeros((height, width, channels), dtype=np.uint8)\n    for i in range(height):\n      for j in range(width):\n        image[i, j] = np.random.randint(0, 256)\n    return image\n\n  def fake_label(num_classes):\n    \"\"\"Creates a fake label with the given number of classes.\"\"\"\n    return np.random", "solution": "\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn"}
{"prompt": "def marker(self, *args):\n        \"\"\"\n        Defines markers one at a time for your graph\n        args are of the form::\n            <marker type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        see the official developers doc for the complete spec\n        APIPARAM: chm\n        \"\"\"", "output": "\n        pass\n\n    def legend(self, *args):\n        \"\"\"\n        Defines the legend for your graph\n        args are of the form::\n            <legend type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        APIPARAM: legend\n        \"\"\"\n        pass\n\n    def save(self, filename):\n        \"\"\"\n        Saves your graph to a file with the given filename.\n        \"\"\"\n        # code to save the graph to a file\n        pass\n\n    def", "solution": "\n        if len(args[0]) == 1:\n            assert args[0] in MARKERS, 'Invalid marker type: %s'%args[0]\n        assert len(args) <= 6, 'Incorrect arguments %s'%str(args)\n        args = color_args(args, 1)\n        self.markers.append(','.join(map(str,args)) )\n        return self"}
{"prompt": "def camera_event_motion_enum(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"", "output": "\n        # Get motion settings for camera_id\n        motion_settings = self.get_motion_settings(camera_id, **kwargs)\n        # Return motion settings as a dictionary\n        return motion_settings.as_dict()\n\n    def get_motion_settings(self, camera_id, **kwargs):\n        \"\"\"Return motion settings for camera_id.\"\"\"\n        # Get motion settings from database or other source\n        motion_settings = {\n            \"resolution\": (640, 480),\n            \"framerate\": 30,\n            \"rotation\": 0,\n", "solution": "\n        api = self._api_info['camera_event']\n        payload = dict({\n            '_sid': self._sid,\n            'api': api['name'],\n            'method': 'MotionEnum',\n            'version': api['version'],\n            'camId': camera_id,\n        }, **kwargs)\n        response = self._get_json_with_retry(api['url'], payload)\n\n        return MotionSetting(camera_id, response['data']['MDParam'])"}
{"prompt": "def register(self, managed_object):\n        \"\"\"\n        Register a managed object with a KMIP appliance.\n\n        Args:\n            managed_object (ManagedObject): A managed object to register. An\n                instantiatable subclass of ManagedObject from the Pie API.\n\n        Returns:\n            string: The uid of the newly registered managed object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input argument is invalid\n        \"\"\"", "output": "\n        if not isinstance(managed_object, ManagedObject):\n            raise TypeError(\"Input argument must be an instance of ManagedObject\")\n\n        try:\n            response = self.kmi_api.register(managed_object.uid, managed_object.name)\n        except ClientConnectionNotOpen as e:\n            raise ClientConnectionNotOpen(e)\n\n        if response.status_code!= 200:\n            raise KmipOperationFailure(response.text)\n\n        return managed_object.uid\n\n    def unregister(self, uid):\n        \"\"\"\n", "solution": "\n        # Check input\n        if not isinstance(managed_object, pobjects.ManagedObject):\n            raise TypeError(\"managed object must be a Pie ManagedObject\")\n\n        # Extract and create attributes\n        object_attributes = list()\n\n        if hasattr(managed_object, 'cryptographic_usage_masks'):\n            if managed_object.cryptographic_usage_masks is not None:\n                mask_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    managed_object.cryptographic_usage_masks\n                )\n                object_attributes.append(mask_attribute)\n        if hasattr(managed_object, 'operation_policy_name'):\n            if managed_object.operation_policy_name is not None:\n                opn_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    managed_object.operation_policy_name\n                )\n                object_attributes.append(opn_attribute)\n        if hasattr(managed_object, 'names'):\n            if managed_object.names:\n                for name in managed_object.names:\n                    name_attribute = self.attribute_factory.create_attribute(\n                        enums.AttributeType.NAME,\n                        name\n                    )\n                    object_attributes.append(name_attribute)\n\n        template = cobjects.TemplateAttribute(attributes=object_attributes)\n        object_type = managed_object.object_type\n\n        # Register the managed object and handle the results\n        secret = self.object_factory.convert(managed_object)\n        result = self.proxy.register(object_type, template, secret)\n\n        status = result.result_status.value\n        if status == enums.ResultStatus.SUCCESS:\n            return result.uuid\n        else:\n            reason = result.result_reason.value\n            message = result.result_message.value\n            raise exceptions.KmipOperationFailure(status, reason, message)"}
{"prompt": "def visit_return(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"", "output": "\n        if node.parent and node.parent.parent:\n            # This is a sibling node, check if it's reachable\n            if node.parent.parent.left and node.parent.parent.left == node.parent:\n                return 1\n            elif node.parent.parent.right and node.parent.parent.right == node.parent:\n                return 1\n        elif node.parent and node.parent.parent:\n            # This is a sibling node, check if it's reachable\n            if node.parent.parent.left and node.parent.parent", "solution": "\n        self._check_unreachable(node)\n        # Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, \"return\", (astroid.FunctionDef,))"}
{"prompt": "def read(self, size=-1):\n        \"\"\"\n        read([size]) -> read at most size bytes, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        Notice that when in non-blocking mode, less data than what was\n        requested may be returned, even if no size parameter was given.\n        \"\"\"", "output": "\n        if size == -1:\n            size = -1\n        data = b''\n        while True:\n            chunk = self.fp.read(size)\n            if not chunk:\n                break\n            data += chunk\n            if len(chunk) < size:\n                break\n        return data.decode('utf-8')\n\n    def write(self, data):\n        \"\"\"\n        write(data) -> write the given data to the file, return the number of bytes written.\n        \"\"\"\n        return self.fp.write(data.encode", "solution": "\n        if self.left is not None:\n            size = min(size, self.left)\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        if size < 0:\n            return ''.join(self)\n        elif not size:\n            chunk = ''\n        elif self.buf:\n            chunk = self.buf\n            self.buf = None\n        else:\n            try:\n                chunk = next(self.iterator)\n            except StopIteration:\n                return ''\n        if len(chunk) > size:\n            self.buf = chunk[size:]\n            chunk = chunk[:size]\n        if self.left is not None:\n            self.left -= len(chunk)\n        return chunk"}
{"prompt": "def balanced_rows(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"", "output": "\n    rows = [iterable[i:i+n] for i in range(0, len(iterable), n)]\n    max_len = max(len(row) for row in rows)\n    balanced = [row.ljust(max_len, fillvalue) for row in rows]\n    return '\\n'.join(balanced)\n\n", "solution": "\n\titerable, iterable_copy = itertools.tee(iterable)\n\tcount = len(tuple(iterable_copy))\n\tfor allocation in partition_items(count, n):\n\t\trow = itertools.islice(iterable, allocation)\n\t\tif allocation < n:\n\t\t\trow = itertools.chain(row, [fillvalue])\n\t\tyield tuple(row)"}
{"prompt": "def timetree_likelihood(self):\n        \"\"\"\n        Return the likelihood of the data given the current branch length in the tree\n        \"\"\"", "output": "\n        if self.is_leaf():\n            return 1.0\n        else:\n            left_branch = self.get_left_branch()\n            right_branch = self.get_right_branch()\n            if left_branch is None and right_branch is None:\n                return 1.0\n            elif left_branch is None:\n                return right_branch.likelihood()\n            elif right_branch is None:\n                return left_branch.likelihood()\n            else:\n                return left_br", "solution": "\n        LH = 0\n        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches\n            if node.up is None: # root node\n                continue\n            LH -= node.branch_length_interpolator(node.branch_length)\n\n        # add the root sequence LH and return\n        if self.aln:\n            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)\n        return LH"}
{"prompt": "def read(*paths):\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"", "output": "\n    return ''.join(paths)\n\n", "solution": "\n    filename = os.path.join(*paths)\n    with codecs.open(filename, mode='r', encoding='utf-8') as handle:\n        return handle.read()"}
{"prompt": "def bootstrap(self, path_or_uri):\n        \"\"\" Initialize a database.\n\n        :param database_path: The absolute path to the database to initialize.\n        \"\"\"", "output": "\n        self.database_path = path_or_uri\n        self.connection = sqlite3.connect(self.database_path)\n        self.cursor = self.connection.cursor()\n        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS data (key TEXT, value TEXT)\")\n\n    def insert_data(self, key: str, value: str):\n        \"\"\" Insert a key-value pair into the database.\n\n        :param key: The key to insert.\n        :param value: The value to insert.\n        \"\"\"\n        ", "solution": "\n        _logger.debug(\"Bootstrapping new database: %s\", path_or_uri)\n        self.database_uri = _urify_db(path_or_uri)\n        db = sa.create_engine(self.database_uri)\n        Base.metadata.create_all(db)"}
{"prompt": "def apply_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"", "output": "\n    if debug:\n        print(\"Applying mirroring\")\n\n    # Mirror the storage\n    mirrored_storage = {}\n    for key, value in storage.items():\n        mirrored_key = key[::-1]\n        mirrored_storage[mirrored_key] = value\n\n    return mirrored_storage\n\n", "solution": "\n    # L4. A character is depicted by a mirrored glyph if and only if (a) the\n    # resolved directionality of that character is R, and (b) the\n    # Bidi_Mirrored property value of that character is true.\n    for _ch in storage['chars']:\n        unichar = _ch['ch']\n        if mirrored(unichar) and \\\n                _embedding_direction(_ch['level']) == 'R':\n            _ch['ch'] = MIRRORED.get(unichar, unichar)\n\n    if debug:\n        debug_storage(storage)"}
{"prompt": "def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method overridden from an ancestor\"\"\"", "output": "\n    for node in class_node.ancestors():\n        if node.name == name:\n            for method in node.methods():\n                if method.is_overridden_from(node):\n                    return True\n    return False\n\n", "solution": "\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False"}
{"prompt": "def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"", "output": "\n        if final:\n            self.children[parent] = code\n        else:\n            self.children[parent] = code + \"-\" + str(len(self.children))\n\n    def add_child(self, parent, code):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 code\n        \"\"\"\n        self.children[parent] = code\n\n    def get_code(self, parent):", "solution": "\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)"}
{"prompt": "def _check_year(year, month, error, error_msg):\n    \"\"\"Checks that the year is within 50 years from now.\"\"\"", "output": "\n    from datetime import datetime, timedelta\n    now = datetime.now()\n    years_passed = now.year - year\n    if abs(years_passed) > 50:\n        raise ValueError(f\"{error_msg} ({year}/{month}/{year+years_passed})\")\n\n", "solution": "\n    if year not in xrange((now.year - 50), (now.year + 51)):\n        year = now.year\n        month = now.month\n        error = error_msg\n    return year, month, error"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return QuantumChannel(self.dimension)\n\n    def __str__(self):\n        return \"QuantumChannel\"\n\n", "solution": "\n        din, dout = self.dim\n        dtr = self._data[0].shape[0] // dout\n        stine = [None, None]\n        for i, mat in enumerate(self._data):\n            if mat is not None:\n                stine[i] = np.reshape(\n                    np.transpose(np.reshape(mat, (dout, dtr, din)), (2, 1, 0)),\n                    (din * dtr, dout))\n        return Stinespring(\n            tuple(stine),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def newick(self):\n        \"\"\"The representation of the Node in Newick format.\"\"\"", "output": "\n        if self.is_leaf():\n            return str(self.label)\n        else:\n            children = [child.newick() for child in self.children]\n            return str(self.label) + \":\" + \",\".join(children)\n\n    def is_leaf(self):\n        \"\"\"True if the Node is a leaf node, False otherwise.\"\"\"\n        return len(self.children) == 0\n\n", "solution": "\n        label = self.name or ''\n        if self._length:\n            label += ':' + self._length\n        descendants = ','.join([n.newick for n in self.descendants])\n        if descendants:\n            descendants = '(' + descendants + ')'\n        return descendants + label"}
{"prompt": "async def get_postcode_random() -> Postcode:\n    \"\"\"\n    Gets a random postcode object..\n    Acts as a middleware between us and the API, caching results.\n    :return: The PostCode object else None if the postcode does not exist.\n    \"\"\"", "output": "\n    if not hasattr(get_postcode_random, 'cache'):\n        get_postcode_random.cache = {}\n    if str(postcode) in get_postcode_random.cache:\n        return get_postcode_random.cache[str(postcode)]\n    postcode = await get_postcode_from_api()\n    get_postcode_random.cache[str(postcode)] = postcode\n    return postcode\n\n", "solution": "\n    try:\n        postcode = await fetch_postcode_random()\n    except (ApiError, CircuitBreakerError):\n        raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\n    if postcode is not None:\n        postcode.save()\n    return postcode"}
{"prompt": "def tasks(self, from_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks.\n\n        :param from_date: retrieve tasks that where updated from that date;\n            dates are converted epoch time.\n        \"\"\"", "output": "\n        if from_date is None:\n            from_date = datetime.utcnow()\n\n        query = (Task.updated_at >= from_date) & (Task.updated_at < datetime.utcnow())\n        return Task.query.filter(query).order_by(Task.id).all()\n\n    def get_tasks(self, to_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks.\n\n        :param to_date: retrieve tasks that where updated to that date;\n            dates are converted epoch time.\n", "solution": "\n        # Convert 'from_date' to epoch timestamp.\n        # Zero value (1970-01-01 00:00:00) is not allowed for\n        # 'modifiedStart' so it will be set to 1, by default.\n        ts = int(datetime_to_utc(from_date).timestamp()) or 1\n\n        consts = {\n            self.PMODIFIED_START: ts\n        }\n\n        attachments = {\n            self. PPROJECTS: True\n        }\n\n        params = {\n            self.PCONSTRAINTS: consts,\n            self.PATTACHMENTS: attachments,\n            self.PORDER: self.VOUTDATED,\n        }\n\n        while True:\n            r = self._call(self.MANIPHEST_TASKS, params)\n            yield r\n            j = json.loads(r)\n            after = j['result']['cursor']['after']\n            if not after:\n                break\n            params[self.PAFTER] = after"}
{"prompt": "def ensure_dir(path):\n    \"\"\"Ensure that a needed directory exists, creating it if it doesn't\"\"\"", "output": "\n    import os\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n", "solution": "\n    try:\n        log.info('Ensuring directory exists: %s' % path)\n        os.makedirs(path)\n    except OSError:\n        if not os.path.isdir(path):\n            raise"}
{"prompt": "def parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"", "output": " a phenotype\n        predominantly phenotypes: A list of phenotype entries\n    \n    Args:\n        lines: A list of strings representing the file\n    \n    Returns:\n        A dictionary with the parsed entries\n    \"\"\"\n    entries = {}\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if line.startswith('gene'):\n            entries['gene'] = line\n        elif line.startswith('gene/phenotype'):\n            entries['gene/phen", "solution": "\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry"}
{"prompt": "def directive(directname=None):\n    \"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"", "output": "\n    if directname is None:\n        directname = directive.__name__\n\n    def decorator(cls):\n        setattr(cls, directive.__name__, cls)\n        return cls\n\n    return decorator\n\n", "solution": "\n    global _directives\n    class_dir_list = _directives\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_dir_list, directname, f)\n        return f\n    return wrapper"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"", "output": "\n        return redis.Redis(host=self.host, port=self.port, db=self.db)\n\n    def get_key(self, key):\n        \"\"\"\n        Returns a Redis key.\n        \"\"\"\n        return self.get_conn().get(key)\n\n    def set_key(self, key, value):\n        \"\"\"\n        Sets a Redis key.\n        \"\"\"\n        return self.get_conn().set(key, value)\n\n    def incr_key(self, key, amount=1):\n        \"\"\"\n", "solution": "\n        conn = self.get_connection(self.redis_conn_id)\n        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n            )\n            self.redis = Redis(\n                host=self.host,\n                port=self.port,\n                password=self.password,\n                db=self.db)\n\n        return self.redis"}
{"prompt": "def decode(self, packet):\n        \"\"\"\n        Decode a PUBREL control packet. \n        \"\"\"", "output": "\n        # Extract the control message from the packet\n        control_message = packet[:self.control_length]\n        \n        # Extract the control code from the control message\n        control_code = int.from_bytes(control_message[:2], byteorder='big')\n        \n        # Check if the control code is valid\n        if control_code!= self.control_code:\n            raise ValueError(\"Invalid control code\")\n        \n        # Extract the message from the control message\n        message = control_message[2:]\n        \n        # Check if the message", "solution": "\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08"}
{"prompt": "def init_log( quiet, debug ):\n   \"\"\"\n   setup system logging to desired verbosity.\n   \"\"\"", "output": "\n   if quiet:\n      log_level = logging.WARNING\n   elif debug:\n      log_level = logging.DEBUG\n   else:\n      log_level = logging.INFO\n   \n   # create logger object\n   logger = logging.getLogger(__name__)\n   logger.setLevel(log_level)\n   \n   # create console handler and set level to info\n   ch = logging.StreamHandler()\n   ch.setLevel(log_level)\n   \n   # create formatter\n   formatter = logging.Formatter('", "solution": "\n   from logging.handlers import SysLogHandler\n   fmt = logging.Formatter( os.path.basename(sys.argv[0]) +\n         \".%(name)s %(levelname)s - %(message)s\")\n   facility = SysLogHandler.LOG_DAEMON\n   syslog = SysLogHandler(address='/dev/log',facility=facility)\n   syslog.setFormatter( fmt )\n   log.addHandler(syslog)\n   if not quiet:\n      console = logging.StreamHandler()\n      console.setFormatter( fmt )\n      log.addHandler(console)\n      log.setLevel(logging.INFO)\n      if debug:\n         log.setLevel(logging.DEBUG)"}
{"prompt": "def loadPlugins(self):\n        \"\"\"Load plugins in nose.plugins.builtin\n        \"\"\"", "output": "\n        import nose.plugins.builtin\n        nose.plugins.builtin.load_plugins()\n\n", "solution": "\n        from nose.plugins import builtin\n        for plug in builtin.plugins:\n            self.addPlugin(plug())\n        super(BuiltinPluginManager, self).loadPlugins()"}
{"prompt": "def verify(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :return: ``True`` if the signature is correct.\n        :rtype: bool\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"", "output": "\n        try:\n            # Verify the signature\n            if not key.verify(self.signature, self.data, self.padding,\n                             self.algorithm):\n                raise OpenSSL.crypto.Error(\"Invalid signature\")\n            return True\n        except OpenSSL.crypto.Error:\n            raise\n\n    def to_string(self):\n        \"\"\"\n        Converts this certificate signing request to a string representation.\n\n        :return: A string representation of the certificate signing request.\n        :rtype: str\n        \"\"\"\n        return \"Certificate Signing", "solution": "\n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n        if result <= 0:\n            _raise_current_error()\n\n        return result"}
{"prompt": "def add_card(self, query_params=None):\n        \"\"\"\n        Create a card for this list. Returns a Card object.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        card = Card()\n        card.query_params = query_params\n        card.list_id = self.id\n        card.list_name = self.name\n        card.list_type = self.type\n        card.list_status = self.status\n        card.list_price = self.price\n        card.list_image_url = self.image_url\n        card.list_image_alt_url = self.image_alt_url\n        card.list_", "solution": "\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)"}
{"prompt": "def get_default_value(self):\n        \"\"\"Instantiate a default value instance.\n\n        This is called when the containing HasTraits classes'\n        :meth:`__new__` method is called to ensure that a unique instance\n        is created for each HasTraits instance.\n        \"\"\"", "output": "\n        return DefaultValue()\n\n    def validate(self, value):\n        \"\"\"Validate a value.\n\n        This is called before the value is set on the HasTraits instance.\n        It should raise a ValueError if the value is not valid.\n        \"\"\"\n        pass\n\n    def __set__(self, instance, value):\n        \"\"\"Set the value on the HasTraits instance.\n\n        This is called when the value on the containing HasTraits instance\n        is set. It should validate the value and set it on the instance.\n        \"\"\"\n        self.validate", "solution": "\n        dv  = self.default_value\n        if isinstance(dv, DefaultValueGenerator):\n            return dv.generate(self.klass)\n        else:\n            return dv"}
{"prompt": "def forward(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding loss of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of loss\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def backward(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding gradient of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of gradient\n        \"\"\"\n        raise NotImplementedError\n\n    def optimize(self):\n        \"\"\"", "solution": "\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callBigDlFunc(self.bigdl_type,\n                               \"criterionForward\",\n                               self.value,\n                               jinput,\n                               input_is_table,\n                               jtarget,\n                               target_is_table)\n        return output"}
{"prompt": "def split_line(self, line, cursor_pos=None):\n        \"\"\"Split a line of text with a cursor at the given position.\n        \"\"\"", "output": "\n        if cursor_pos is None:\n            cursor_pos = len(line) // 2\n        return line[:cursor_pos], line[cursor_pos:]\n\n    def get_cursor_pos(self):\n        \"\"\"Return the current position of the cursor.\n        \"\"\"\n        return len(self.text) // 2\n\n    def move_cursor(self, direction):\n        \"\"\"Move the cursor in the specified direction.\n        \"\"\"\n        if direction == 'up':\n            self.text = self.text[:self.get_cursor", "solution": "\n        l = line if cursor_pos is None else line[:cursor_pos]\n        return self._delim_re.split(l)[-1]"}
{"prompt": "def signing_keys_as_jwks(self):\n        \"\"\"\n        Build a JWKS from the signing keys belonging to the self signer\n\n        :return: Dictionary\n        \"\"\"", "output": "\n        jwks = {}\n        for signer in self.signers:\n            jwks[signer.kid] = signer.jwk\n        return jwks\n\n    def verify_jwks(self, jwks):\n        \"\"\"\n        Verify the JWKS using the self signer's public key\n\n        :param jwks: Dictionary\n        :return: bool\n        \"\"\"\n        for signer_kid, jwk in jwks.items():\n            if not verify_jwk(jwk, signer_kid):\n", "solution": "\n        _l = [x.serialize() for x in self.self_signer.keyjar.get_signing_key()]\n        if not _l:\n            _l = [x.serialize() for x in\n                  self.self_signer.keyjar.get_signing_key(owner=self.iss)]\n        return {'keys': _l}"}
{"prompt": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"", "output": "\n    return node.name == \"import from __future__\"\n\n", "solution": "\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"", "output": "\n        # Get model parameters\n        parameters = []\n        for key, value in self.parameters.items():\n            parameters.append(f\"{key}: {value}\")\n\n        # Get model summary\n        summary = self.model.summary()\n\n        # Print report to output file\n        print(f\"Model Report:\\n{summary}\", file=output_file)\n        print(f\"Parameters: {', '.join(parameters)}\", file=output_file)\n\n    def save(self, model_file):\n        \"\"\"Save model to file.\"", "solution": "\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)"}
{"prompt": "def _draw(self, mode, vertex_list=None):\n        \"\"\"Draw vertices in the domain.\n\n        If `vertex_list` is not specified, all vertices in the domain are\n        drawn.  This is the most efficient way to render primitives.\n\n        If `vertex_list` specifies a `VertexList`, only primitives in that\n        list will be drawn.\n\n        :Parameters:\n            `mode` : int\n                OpenGL drawing mode, e.g. ``GL_POINTS``, ``GL_LINES``, etc.\n            `vertex_list` : `VertexList`\n                Vertex list to draw, or ``None`` for all lists in this domain.\n\n        \"\"\"", "output": "\n            `vertex_list` : list of Vertex objects\n                List of vertices to draw.\n\n        :Raises:\n            `ValueError` if the mode is not recognized.\n        \"\"\"\n        if mode == 0:\n            glBegin(GL_POINTS)\n        elif mode == 1:\n            glBegin(GL_LINES)\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        if vertex_list is not None:\n            glColor3fv(self.color)\n            glVertexPointer(2, GL_", "solution": "\n        glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n        for buffer, attributes in self.buffer_attributes:\n            buffer.bind()\n            for attribute in attributes:\n                attribute.enable()\n                attribute.set_pointer(attribute.buffer.ptr)\n        if vertexbuffer._workaround_vbo_finish:\n            glFinish()\n\n        if vertex_list is not None:\n            glDrawArrays(mode, vertex_list.start, vertex_list.count)\n        else:\n            starts, sizes = self.allocator.get_allocated_regions()\n            primcount = len(starts)\n            if primcount == 0:\n                pass\n            elif primcount == 1:\n                # Common case\n                glDrawArrays(mode, starts[0], int(sizes[0]))\n            elif gl_info.have_version(1, 4):\n                starts = (GLint * primcount)(*starts)\n                sizes = (GLsizei * primcount)(*sizes)\n                glMultiDrawArrays(mode, starts, sizes, primcount)\n            else:\n                for start, size in zip(starts, sizes):\n                    glDrawArrays(mode, start, size)\n\n        for buffer, _ in self.buffer_attributes:\n            buffer.unbind()\n        glPopClientAttrib()"}
{"prompt": "def parse_rrset(e_rrset, connection, zone_id):\n    \"\"\"\n    This a parser that allows the passing of any valid ResourceRecordSet\n    tag. It will spit out the appropriate ResourceRecordSet object for the tag.\n\n    :param lxml.etree._Element e_rrset: The root node of the etree parsed\n        response from the API.\n    :param Route53Connection connection: The connection instance used to\n        query the API.\n    :param str zone_id: The zone ID of the HostedZone these rrsets belong to.\n    :rtype: ResourceRecordSet\n    :returns: An instantiated ResourceRecordSet object.\n    \"\"\"", "output": "\n    :return: A ResourceRecordSet object representing the rrset with the\n        specified tag.\n    :raises ValueError: If the tag is not recognized.\n    \"\"\"\n    tag = e_rrset.tag\n    if tag == \"set\":\n        return ResourceRecordSet(connection, zone_id, e_rrset)\n    elif tag == \"set:set\":\n        return ConnectionSet(connection, zone_id, e_rrset)\n    elif tag == \"set:set:set\":\n        return ZoneSet(connection, zone_id", "solution": "\n\n    # This dict will be used to instantiate a ResourceRecordSet instance to yield.\n    kwargs = {\n        'connection': connection,\n        'zone_id': zone_id,\n    }\n    rrset_type = None\n\n    for e_field in e_rrset:\n        # Cheesy way to strip off the namespace.\n        tag_name = e_field.tag.split('}')[1]\n        field_text = e_field.text\n\n        if tag_name == 'Type':\n            # Need to store this to determine which ResourceRecordSet\n            # subclass to instantiate.\n            rrset_type = field_text\n            continue\n        elif tag_name == 'AliasTarget':\n            # A records have some special field values we need.\n            alias_hosted_zone_id, alias_dns_name = parse_rrset_alias(e_field)\n            kwargs['alias_hosted_zone_id'] = alias_hosted_zone_id\n            kwargs['alias_dns_name'] = alias_dns_name\n            # Alias A entries have no TTL.\n            kwargs['ttl'] = None\n            continue\n        elif tag_name == 'ResourceRecords':\n            kwargs['records'] = parse_rrset_record_values(e_field)\n            continue\n\n        # Map the XML tag name to a kwarg name.\n        kw_name = RRSET_TAG_TO_KWARG_MAP[tag_name]\n        # This will be the key/val pair used to instantiate the\n        # ResourceRecordSet instance.\n        kwargs[kw_name] = field_text\n\n    if not rrset_type:\n        raise Route53Error(\"No Type tag found in ListResourceRecordSetsResponse.\")\n\n    if 'records' not in kwargs:\n        # Not all rrsets have records.\n        kwargs['records'] = []\n\n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP[rrset_type]\n    return RRSetSubclass(**kwargs)"}
{"prompt": "def get_params(degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Get parameters for affine transformation\n\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"", "output": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                             np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n", "solution": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear"}
{"prompt": "def topic(self, topic_id):\n        \"\"\"Retrive the topic with `topic_id` identifier.\n\n        :param topic_id: identifier of the topic to retrieve\n        \"\"\"", "output": "\n        for topic in self.topics:\n            if topic.id == topic_id:\n                return topic\n        return None\n\n    def list_topics(self):\n        \"\"\"Return a list of all topics in the collection.\"\"\"\n        return [topic for topic in self.topics]\n\n    def create_topic(self, topic_name, topic_type, topic_attributes):\n        \"\"\"Creates a new topic with the given `topic_name`, `topic_type`, and `topic_attributes`.\n\n        Args:\n            topic_name (str):", "solution": "\n        params = {\n            self.PKEY: self.api_key\n        }\n\n        # http://example.com/t/8.json\n        response = self._call(self.TOPIC, topic_id,\n                              params=params)\n\n        return response"}
{"prompt": "def decode(data):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"", "output": "\n    decoded_data = \"\"\n    for i in range(0, len(data), 2):\n        byte = data[i:i+2]\n        decoded_byte = chr(int(byte, 2))\n        decoded_data += decoded_byte\n    return decoded_data\n\n", "solution": "\n    data = bytearray(data) # <- python 2/3 compatibility fix\n    result = bytearray()\n    pos = 0\n    while pos < len(data):\n        header_byte = data[pos]\n        if header_byte > 127:\n            header_byte -= 256\n        pos += 1\n\n        if 0 <= header_byte <= 127:\n            result.extend(data[pos:pos+header_byte+1])\n            pos += header_byte+1\n        elif header_byte == -128:\n            pass\n        else:\n            result.extend([data[pos]] * (1 - header_byte))\n            pos += 1\n\n    return bytes(result)"}
{"prompt": "def get_conn(self):\n        \"\"\"Return the FileService object.\"\"\"", "output": "\n        return self.conn\n\n    def set_conn(self, conn):\n        \"\"\"Set the FileService object to the given connection.\"\"\"\n        self.conn = conn\n\n    def get_file_size(self):\n        \"\"\"Return the size of the file.\"\"\"\n        return os.path.getsize(self.file_name)\n\n    def get_file_name(self):\n        \"\"\"Return the name of the file.\"\"\"\n        return os.path.basename(self.file_name)\n\n    def get_file_extension(self):\n        ", "solution": "\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)"}
{"prompt": "def createClient(self):\n        \"\"\"Create a UDP connection to Riemann\"\"\"", "output": "\n        # Code to create a UDP connection to Riemann\n        pass\n    \n    def sendData(self, data):\n        \"\"\"Send data over the network using the created client\"\"\"\n        # Code to send data over the network using the created client\n        pass\n    \n    def receiveData(self):\n        \"\"\"Receive data from the network using the created client\"\"\"\n        # Code to receive data from the network using the created client\n        pass\n    \n    def closeClient(self):\n        \"\"\"Close the created client\"\"\"\n        # Code to close the created client\n        ", "solution": "\n        server = self.config.get('server', '127.0.0.1')\n        port = self.config.get('port', 5555)\n\n        def connect(ip):\n            self.protocol = riemann.RiemannUDP(ip, port)\n            self.endpoint = reactor.listenUDP(0, self.protocol)\n\n        d = reactor.resolve(server)\n        d.addCallback(connect)\n        return d"}
{"prompt": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"):\n    \"\"\"Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p].\"\"\"", "output": "\n    return f\"{name}(shape(a)+[p])\"\n\n", "solution": "\n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])"}
{"prompt": "def skus(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-06-01: :class:`SkusOperations<azure.mgmt.storage.v2017_06_01.operations.SkusOperations>`\n           * 2017-10-01: :class:`SkusOperations<azure.mgmt.storage.v2017_10_01.operations.SkusOperations>`\n           * 2018-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n        \"\"\"", "output": "_02_01.operations.SkusOperations>`\n        \"\"\"\n        if self.api_version == \"2017-06-01\":\n            return SkusOperations.SkusOperations(self.name, self.location, self.sku)\n        elif self.api_version == \"2017-10-01\":\n            return SkusOperations.SkusOperations(self.name, self.location, self.sku)\n        elif self.api_version == \"2018-02-01\":\n            return SkusOperations.Sk", "solution": "\n        api_version = self._get_api_version('skus')\n        if api_version == '2017-06-01':\n            from .v2017_06_01.operations import SkusOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-03-01-preview':\n            from .v2018_03_01_preview.operations import SkusOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import SkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def delete_local_file(file_name):\n    \"\"\"\n    Deletes the file associated with the file_name passed from local storage.\n    \n    :param str file_name: Filename of the file to be deleted\n    :return str: Filename of the file that was just deleted\n    \"\"\"", "output": "\n    import os\n    \n    if os.path.exists(file_name):\n        os.remove(file_name)\n        return file_name\n    else:\n        return \"File does not exist\"\n\n", "solution": "\n\n    try:\n        os.remove(file_name)\n        log.info(f\"Deletion for {file_name} has finished\")\n        return file_name\n    except OSError:\n        pass"}
{"prompt": "def output_json(self, file_name='/tmp/ND.json'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.json'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"", "output": "\n        with open(file_name, 'w') as f:\n            json.dump(self.data, f)\n\n", "solution": "\n        complete_example = (\n            self.dataset, self.project, self.channels, self.metadata)\n        data = json.loads(self.nd_json(*complete_example))\n\n        # self.verify_json(data)\n        self.verify_path(data, VERIFY_BY_SLICE)\n\n        f = open(file_name, 'w')\n        f.write(str(data))\n        f.close()"}
{"prompt": "def open_graph(self, **args):\n        \"\"\"open a vcg graph\n        \"\"\"", "output": "\n        # code to open a vcg graph using the specified arguments\n        pass\n    \n    def close_graph(self, **args):\n        \"\"\"close a vcg graph\n        \"\"\"\n        # code to close a vcg graph using the specified arguments\n        pass\n    \n    def get_graph_properties(self, **args):\n        \"\"\"get graph properties\n        \"\"\"\n        # code to get graph properties using the specified arguments\n        pass\n    \n    def set_graph_properties(self, **args):\n        \"\"\"set graph properties\n        \"\"\"\n", "solution": "\n        self._stream.write(\"%sgraph:{\\n\" % self._indent)\n        self._inc_indent()\n        self._write_attributes(GRAPH_ATTRS, **args)"}
{"prompt": "def remove(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        \"\"\"Remove the namespace bound to the symbol `name` in the global\n        namespace cache and return that namespace.\n        Return None if the namespace did not exist in the cache.\"\"\"", "output": "\n        try:\n            del cls.__namespaces__[name]\n        except KeyError:\n            return None\n        return cls.__namespaces__[name]\n\n    def __init__(cls, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        cls.__namespaces__ = {}\n\n    def __getattr__(cls, name: sym.Symbol) -> Any:\n        \"\"\"Proxy all attribute access to the global namespace.\"\"\"\n        return getattr(cls.__namespaces", "solution": "\n        while True:\n            oldval: lmap.Map = cls._NAMESPACES.deref()\n            ns: Optional[Namespace] = oldval.entry(name, None)\n            newval = oldval\n            if ns is not None:\n                newval = oldval.dissoc(name)\n            if cls._NAMESPACES.compare_and_set(oldval, newval):\n                return ns"}
{"prompt": "def einsum_vecmul_index(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-vector multiplication.\n\n    The returned indices are to perform a matrix multiplication A.v where\n    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on v.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                  to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"", "output": "most qubits in the\n            computational basis of the operator\n        number_of_qubits (int): the total number of qubits in the operator\n\n    Returns:\n        str: the index string for the Numpy.eignsum matrix-vector multiplication\n    \"\"\"\n    # Create the matrix A and vector v\n    matrix = np.identity(2**number_of_qubits, dtype=complex)\n    vector = np.zeros(2**number_of_qubits, dtype=complex)\n    for i, j in enumerate(gate", "solution": "\n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n           \"{tens_lin}->{tens_lout}\".format(tens_lin=tens_lin,\n                                            tens_lout=tens_lout)"}
{"prompt": "def get_client(self):\n        \"\"\"\n        Obtains a client for use, whether an existing unused client\n        or a brand new one if none are available.\n        \"\"\"", "output": "\n        if self.client_available:\n            return self.client\n        else:\n            return self.get_unused_client()\n\n    def get_unused_client(self):\n        \"\"\"\n        Obtains an unused client for use, whether an existing unused\n        client or a brand new one if none are available.\n        \"\"\"\n        # Implementation omitted for brevity\n        pass\n\n    def get_product(self):\n        \"\"\"\n        Obtains a product for use, whether an existing unused product\n        or a brand new one if none are available.\n        \"\"\"", "solution": "\n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n            client = self.client_class(*self.args, **kwargs)\n        return client"}
{"prompt": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"", "output": "\n    if value is None:\n        try:\n            return semaphores[name]\n        except KeyError:\n            semaphores[name] = semaphore = Semaphore(name)\n            return semaphore\n    else:\n        semaphore = Semaphore(name)\n        semaphore.value = value\n        return semaphore\n\n", "solution": "\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle"}
{"prompt": "def delete(self, request, *args, **kwargs):\n        \"\"\"Delete auth token when `delete` request was issued.\"\"\"", "output": "\n        pass\n\n    def get(self, request, *args, **kwargs):\n        \"\"\"Return auth token when `get` request was issued.\"\"\"\n        return request.user.auth_token\n\n    def put(self, request, *args, **kwargs):\n        \"\"\"Return auth token when `put` request was issued.\"\"\"\n        return request.user.auth_token\n\n    def post(self, request, *args, **kwargs):\n        \"\"\"Return auth token when `post` request was issued.\"\"\"\n        return request.user.auth_token\n\n    ", "solution": "\n        # Logic repeated from DRF because one cannot easily reuse it\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != b'token':\n            return response.Response(status=status.HTTP_400_BAD_REQUEST)\n\n        if len(auth) == 1:\n            msg = 'Invalid token header. No credentials provided.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n        elif len(auth) > 2:\n            msg = 'Invalid token header. Token string should not contain spaces.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n\n        try:\n            token = self.model.objects.get(key=auth[1])\n        except self.model.DoesNotExist:\n            pass\n        else:\n            token.delete()\n            signals.user_logged_out.send(\n                type(self),\n                user=token.user,\n                request=request,\n            )\n        return response.Response(status=status.HTTP_204_NO_CONTENT)"}
{"prompt": "def load_pkcs7_data(type, buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with the type\n    *type*.\n\n    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        # Load pem data from buffer\n        pass\n    elif type == FILETYPE_ASN1:\n        # Load asn1 data from buffer\n        pass\n    else:\n        raise ValueError(\"Invalid file type\")\n\n    # Parse pkcs7 data\n    pkcs7 = PKCS7()\n    pkcs7.parse(buffer)\n\n    return pkcs7\n\n", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        pkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if pkcs7 == _ffi.NULL:\n        _raise_current_error()\n\n    pypkcs7 = PKCS7.__new__(PKCS7)\n    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n    return pypkcs7"}
{"prompt": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"", "output": " of the dataset.\n    shuffle: Whether to shuffle the dataset.\n    fake_data: Whether to generate fake data.\n\n  Returns:\n    A tf.data pipeline for the sprites dataset.\n  \"\"\"\n  def parse_fn(serialized_example):\n    \"\"\"Parses a single serialized example into a dictionary of features.\n\n    Args:\n      serialized_example: A string representing a single serialized example.\n\n    Returns:\n      A dictionary of features.\n    \"\"\"\n    features = {\n       'skin': tf.io.FixedLenFeature([],", "solution": "\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    "}
{"prompt": "def _prepare_subsection(data, sections, wrapper):\n    \"\"\"\n    Creates a subsection ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A newly created subsection.\n    \"\"\"", "output": "\n    subsection = {}\n    for section in sections:\n        subsection[section] = wrapper.wrap(data.get(section, ''))\n    return subsection\n\n", "solution": "\n    current = data\n    for i in sections[:-1]:\n        current = current[i]\n\n    current[sections[-1]] = wrapper()\n    return current[sections[-1]]"}
{"prompt": "def render_tag(self, context, *tag_args, **tag_kwargs):\n        \"\"\"\n        Rendering of the tag. It either assigns the value as variable, or renders it.\n        \"\"\"", "output": "\n        if tag_args or tag_kwargs:\n            raise ValueError(\"Cannot render tag with arguments or keyword arguments.\")\n        if tag_kwargs:\n            tag_kwargs = {k: v for k, v in tag_kwargs.items() if v is not None}\n        if tag_kwargs:\n            tag_kwargs = {k: str(v) for k, v in tag_kwargs.items() if isinstance(v, str)}\n        return f\"<{self.tag_name} {', '.join(tag_kw", "solution": "\n        # Be very explicit about which base functionality is used:\n        # Using super() for mixin support will not work nicely anyway here.\n        if self.as_var:\n            # Assign the value in the parent context\n            return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs)\n        else:\n            # Render the output using the BaseInclusionNode features\n            return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)"}
{"prompt": "def verify_token(self, request, *args, **kwargs):\n        \"\"\"\n        Use `token` to allow one-time access to a view.\n\n        Set the user as a class attribute or raise an `InvalidExpiredToken`.\n\n        Token expiry can be set in `settings` with `VERIFY_ACCOUNT_EXPIRY` and is\n        set in seconds.\n        \"\"\"", "output": "\n        if request.user.is_authenticated:\n            # Check if token has already expired\n            if request.user.is_active and request.user.is_superuser:\n                # Check if token has not expired yet\n                if timezone.now() > request.user.last_active + timedelta(seconds=settings.VERIFY_ACCOUNT_EXPIRY):\n                    # Token has expired, raise an InvalidExpiredToken\n                    raise InvalidExpiredToken()\n            else:\n                # Token has not expired yet\n                pass\n        else:\n            ", "solution": "\n        User = get_user_model()\n\n        try:\n            max_age = settings.VERIFY_ACCOUNT_EXPIRY\n        except AttributeError:\n            max_age = self.DEFAULT_VERIFY_ACCOUNT_EXPIRY\n\n        try:\n            email_data = signing.loads(kwargs['token'], max_age=max_age)\n        except signing.BadSignature:\n            raise self.invalid_exception_class\n\n        email = email_data['email']\n\n        try:\n            self.user = User.objects.get_by_natural_key(email)\n        except User.DoesNotExist:\n            raise self.invalid_exception_class\n\n        if self.user.email_verified:\n            raise self.permission_denied_class"}
{"prompt": "def partitions(l, partition_size):\n    \"\"\"\n    >>> list(partitions([], 10))\n    []\n    >>> list(partitions([1,2,3,4,5], 1))\n    [[1], [2], [3], [4], [5]]\n    >>> list(partitions([1,2,3,4,5], 2))\n    [[1, 2], [3, 4], [5]]\n    >>> list(partitions([1,2,3,4,5], 5))\n    [[1, 2, 3, 4, 5]]\n\n    :param list l: List to be partitioned\n    :param int partition_size: Size of partitions\n    \"\"\"", "output": ", 5]]\n    >>> list(partitions([1,2,3,4,5], 0))\n    []\n    >>> list(partitions([], 5))\n    []\n    \"\"\"\n    if partition_size == 0:\n        return [[]]\n    if len(l) == 0:\n        return []\n    result = []\n    for i in range(1, len(l) + 1):\n        for p in partitions(l[i:], partition_size - 1):\n            result.append([l[:i]] + p)\n    return result", "solution": "\n    for i in xrange(0, len(l), partition_size):\n        yield l[i:i + partition_size]"}
{"prompt": "def download_layers(self, repo_name, digest=None, destination=None):\n    \"\"\" download layers is a wrapper to do the following for a client loaded\n        with a manifest for an image:\n      \n        1. use the manifests to retrieve list of digests (get_digests)\n        2. atomically download the list to destination (get_layers)\n\n        This function uses the MultiProcess client to download layers\n        at the same time.\n    \"\"\"", "output": "\n    if not digest:\n        digest = self.get_digest()\n    if not destination:\n        destination = self.get_destination()\n    return self.client.download_layers(repo_name, digest, destination)\n\n", "solution": "\n    from sregistry.main.workers import ( Workers, download_task )\n\n    # 1. Get manifests if not retrieved\n    if not hasattr(self, 'manifests'):\n        self._get_manifests(repo_name, digest)\n\n    # Obtain list of digets, and destination for download\n    digests = self._get_digests()\n    destination = self._get_download_cache(destination)\n\n    # Create multiprocess download client\n    workers = Workers()\n\n    # Download each layer atomically\n    tasks = []\n    layers = []\n    for digest in digests:\n\n        targz = \"%s/%s.tar.gz\" % (destination, digest)\n\n        # Only download if not in cache already\n        if not os.path.exists(targz):\n            url = \"%s/%s/blobs/%s\" % (self.base, repo_name, digest)\n            tasks.append((url, self.headers, targz))\n        layers.append(targz)\n\n    # Download layers with multiprocess workers\n    if len(tasks) > 0:\n        download_layers = workers.run(func=download_task,\n                                      tasks=tasks)\n    # Create the metadata tar\n    metadata = self._create_metadata_tar(destination)\n    if metadata is not None:\n        layers.append(metadata)\n\n\n    return layers"}
{"prompt": "def process_item(self, item, spider):\n        \"\"\"\n        Process single item. Add item to items and then upload to S3 if size of items\n        >= max_chunk_size.\n        \"\"\"", "output": "\n        self.logger.info(f\"Processing item: {item}\")\n        # Add item to items list\n        self.items.append(item)\n        # Upload item to S3 if size of items is greater than or equal to max_chunk_size\n        if len(self.items) >= self.max_chunk_size:\n            self.upload_to_s3()\n\n    def upload_to_s3(self):\n        \"\"\"\n        Upload items to S3 bucket.\n        \"\"\"\n        # Code to upload items to S3", "solution": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self._upload_chunk(spider)\n\n        return item"}
{"prompt": "def handle_print(self, signame, set_print):\n        \"\"\"Set whether we print or not when this signal is caught.\"\"\"", "output": "\n        self.set = set_print\n        self.signame = signame\n\n    def __repr__(self):\n        return f\"Signal {self.signame} caught and printed.\"\n\n", "solution": "\n        if set_print:\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n        else:\n            self.sigs[signame].print_method = None\n            pass\n        return set_print"}
{"prompt": "def _trj_check_version(self, version, python, force):\n        \"\"\"Checks for version mismatch\n\n        Raises a VersionMismatchError if version of loaded trajectory and current pypet version\n        do not match. In case of `force=True` error is not raised only a warning is emitted.\n\n        \"\"\"", "output": "\n        if version == self.version:\n            return\n\n        if python == \"3\":\n            import sys\n            if sys.version_info < (3, 0):\n                raise ImportError(\"Python 3 is required for this function\")\n        else:\n            raise ImportError(\"Python 2 is required for this function\")\n\n        if force:\n            import warnings\n            warnings.warn(\"Version mismatch: loaded trajectory is out of date. Use force=False to update.\")\n\n    def _trj_check_version_python(self, version, python):\n        \"\"\"Checks for", "solution": "\n        curr_python = pypetconstants.python_version_string\n\n        if (version != VERSION or curr_python != python) and not force:\n            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '\n                                           '  but your trajectory'\n                                           ' was created with version %s and python %s.'\n                                           ' Use >>force=True<< to perform your load regardless'\n                                           ' of version mismatch.' %\n                                           (VERSION, curr_python, version, python))\n        elif version != VERSION or curr_python != python:\n            self._logger.warning('Current pypet version is %s with python %s but your trajectory'\n                                 ' was created with version %s under python %s.'\n                                 ' Yet, you enforced the load, so I will'\n                                 ' handle the trajectory despite the'\n                                 ' version mismatch.' %\n                                 (VERSION, curr_python, version, python))"}
{"prompt": "def get_branch_mutation_matrix(self, node, full_sequence=False):\n        \"\"\"uses results from marginal ancestral inference to return a joint\n        distribution of the sequence states at both ends of the branch.\n\n        Parameters\n        ----------\n        node : Phylo.clade\n            node of the tree\n        full_sequence : bool, optional\n            expand the sequence to the full sequence, if false (default)\n            the there will be one mutation matrix for each column in the\n            reduced alignment\n\n        Returns\n        -------\n        numpy.array\n            an Lxqxq stack of matrices (q=alphabet size, L (reduced)sequence length)\n        \"\"\"", "output": "\n            array of shape (n, n) where n is the number of mutations in the\n            alignment of the node to all other nodes in the tree. If\n            `full_sequence` is True, the array will have shape (n, n, len(node))\n            where len(node) is the length of the alignment.\n        \"\"\"\n        # Marginal ancestral inference\n        alignment = self.get_alignment(node)\n        marginal_alignment = self.marginal_alignment(node, alignment)\n\n        # Get sequence matrices\n        sequences", "solution": "\n        pp,pc = self.marginal_branch_profile(node)\n\n        # calculate pc_i [e^Qt]_ij pp_j for each site\n        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))\n        if len(expQt.shape)==3: # site specific model\n            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)\n        else:\n            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)\n\n        # normalize this distribution\n        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)\n        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)\n\n        # expand to full sequence if requested\n        if full_sequence:\n            return mut_matrix_stack[self.full_to_reduced_sequence_map]\n        else:\n            return mut_matrix_stack"}
{"prompt": "def meas_gate(self, circuit, qreg, op):\n        \"\"\"\n        Add measurement gates to a circuit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n            qreg (tuple(QuantumRegister,int)): quantum register being measured.\n            op (str): the basis label for the measurement.\n        \"\"\"", "output": "\n        for i in range(len(qreg)):\n            circuit.h(qreg[i])\n        circuit.measure(qreg[0], op)\n\n", "solution": "\n        if self.meas_fun is None:\n            pass\n        else:\n            self.meas_fun(circuit, qreg, op)"}
{"prompt": "def log(wave):\n    r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "\n        ValueError: If wave is not a valid waveform.\n    \"\"\"\n    if not isinstance(wave, Waveform):\n        raise ValueError(\"wave must be a Waveform object\")\n\n    if wave.num_channels!= 1:\n        raise ValueError(\"wave must be a single-channel waveform\")\n\n    if wave.num_samples!= len(wave.data):\n        raise ValueError(\"wave must have the same number of samples as data\")\n\n    if wave.sampling_rate!= wave.data[0].sampling_rate:\n        raise", "solution": "\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((min(wave._dep_vector) <= 0))\n    )\n    return _operation(wave, \"log\", \"\", np.log)"}
{"prompt": "async def save_tracks(self, *tracks):\n        \"\"\"Save one or more tracks to the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track, str]]\n            A sequence of track objects or spotify IDs\n        \"\"\"", "output": "\n        if not tracks:\n            return\n\n        for track in tracks:\n            if isinstance(track, str):\n                track_id = track\n            else:\n                track_id = track.id\n\n            if track_id in self.tracks:\n                raise ValueError(f\"Track with ID {track_id} already exists in the library.\")\n\n            self.tracks[track_id] = track\n\n        await self.library.save()\n\n    async def get_tracks(self, *tracks):\n        \"\"\"Retrieve one or more tracks from the current user", "solution": "\n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))"}
{"prompt": "def get_logging_level():\n    \"\"\"get_logging_level will configure a logging to standard out based on the user's\n    selected level, which should be in an environment variable called\n    MESSAGELEVEL. if MESSAGELEVEL is not set, the maximum level\n    (5) is assumed (all messages).\n    \"\"\"", "output": "\n    try:\n        level = int(os.environ.get('MESSAGELEVEL', 5))\n    except (ValueError, TypeError):\n        level = 5\n    return level\n\n", "solution": "\n    level = os.environ.get(\"MESSAGELEVEL\", INFO)\n\n    # User knows logging levels and set one\n    if isinstance(level, int):\n        return level\n\n    # Otherwise it's a string\n    if level == \"CRITICAL\":\n        return CRITICAL\n    elif level == \"ABORT\":\n        return ABORT\n    elif level == \"ERROR\":\n        return ERROR\n    elif level == \"WARNING\":\n        return WARNING\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"INFO\":\n        return INFO\n    elif level == \"QUIET\":\n        return QUIET\n    elif level.startswith(\"VERBOSE\"):\n        return VERBOSE3\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"DEBUG\":\n        return DEBUG\n\n    return level"}
{"prompt": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"", "output": "\n    formatted_tasks = []\n    for i, task in enumerate(tasks):\n        formatted_task = f\"[{i+1}] {task}\"\n        formatted_tasks.append(formatted_task)\n    return formatted_tasks\n\n", "solution": "\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])"}
{"prompt": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"", "output": "\n    return self.mean\n\n", "solution": "\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])"}
{"prompt": "def is_detached(self):\n        \"\"\"Check if the repo is in a detached state.\n\n        The repository is in a detached state when HEAD is not a symbolic\n        reference.\n\n        :returns: whether the repository is detached or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"", "output": "\n        try:\n            return self.repo.head.is_detached\n        except AttributeError:\n            raise RepositoryError(\"Not in a repository\")\n\n    def is_clean(self):\n        \"\"\"Check if the repository is clean.\n\n        The repository is considered clean if all of its files have been\n        modified and the working directory is clean.\n\n        :returns: whether the repository is clean or not\n        \"\"\"\n        try:\n            return self.repo.is_clean\n        except AttributeError:\n            raise RepositoryError(\"Not in a", "solution": "\n        cmd_sym = ['git', 'symbolic-ref', 'HEAD']\n\n        try:\n            self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            if e.msg.find(\"ref HEAD is not a symbolic ref\") == -1:\n                raise e\n            return True\n        else:\n            return False"}
{"prompt": "def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Publishes a batch of events to an Azure Event Grid topic.\n\n        :param topic_hostname: The host name of the topic, e.g.\n         topic1.westus2-1.eventgrid.azure.net\n        :type topic_hostname: str\n        :param events: An array of events to be published to Event Grid.\n        :type events: list[~azure.eventgrid.models.EventGridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"", "output": ".Event]\n        :param custom_headers: A dictionary of custom headers to be added to the event.\n        :type custom_headers: dict[str, str]\n        :param raw: If set to True, the events will not be serialized and will be published as is.\n        :type raw: bool\n        :param operation_config: Additional configuration options for the operation.\n        :type operation_config: dict\n        :return: None\n        :rtype: None\n        \"\"\"\n        if not events:\n            return\n\n        if not custom_headers:\n            ", "solution": "\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response"}
{"prompt": "def get_imap_capabilities(server):\n    \"\"\"\n    Returns a list of an IMAP server's capabilities\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n\n    Returns (list): A list of capabilities\n    \"\"\"", "output": "\n    capabilities = []\n    for key, value in server.server.capabilities.items():\n        capabilities.append(f\"{key}: {value}\")\n    return capabilities\n\n", "solution": "\n\n    capabilities = list(map(str, list(server.capabilities())))\n    for i in range(len(capabilities)):\n        capabilities[i] = str(capabilities[i]).replace(\"b'\",\n                                                       \"\").replace(\"'\",\n                                                                   \"\")\n    logger.debug(\"IMAP server supports: {0}\".format(capabilities))\n\n    return capabilities"}
{"prompt": "def update_path(self, board, color, path):\n        \"\"\" update win/loss count along path \"\"\"", "output": "\n        for i in range(len(path)-1):\n            x1, y1 = path[i]\n            x2, y2 = path[i+1]\n            if color =='red':\n                if x1 == x2:\n                    if y1 < y2:\n                        if y1+1 < len(board[0]) and board[y1+1][x1] =='':\n                            path[i+1] = (x1, y1+1)\n                    else:\n                        if y1-1 >= 0 and board[", "solution": "\n        wins = board.score(BLACK) >= board.score(WHITE)\n        for node in path:\n            if color == BLACK:\n                color = WHITE\n            else:\n                color = BLACK\n            if wins == (color == BLACK):\n                node.wins += 1\n            else:\n                node.losses += 1\n            if node.parent:\n                node.parent.bestchild = node.parent.best_child()"}
{"prompt": "def read(self):\n        \"\"\"\n        Load the metrics file from the given path\n        \"\"\"", "output": "\n        with open(self.path, 'r') as f:\n            metrics = f.read()\n            self.metrics = eval(metrics)\n\n    def write(self, path):\n        \"\"\"\n        Write the metrics to the given path\n        \"\"\"\n        with open(path, 'w') as f:\n            f.write(str(self.metrics))\n\n", "solution": "\n        f = open(self.path, \"r\")\n        self.manifest_json = f.read()"}
{"prompt": "def update(self, teamId, name=None, **request_parameters):\n        \"\"\"Update details for a team, by ID.\n\n        Args:\n            teamId(basestring): The team ID.\n            name(basestring): A user-friendly name for the team.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Team: A Team object with the updated Webex Teams team details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n            ValueError: If the parameter values are incorrect.\n        \"\"\"\n        if not isinstance(teamId, str):\n            raise TypeError(\"teamId must be a string\")\n        if not isinstance(name, str):\n            raise TypeError(\"name must be a string\")\n        if not request_parameters:\n            return self\n        # Update the team's name and ID in the database\n        # (implementation details not specified in the docstring)\n        return Team(teamId, name)\n\n    def delete(self, teamId, **request_parameters", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(name, basestring)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            name=name,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '/' + teamId,\n                                      json=put_data)\n\n        # Return a team object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def _odds_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The derived value\n        \"\"\"", "output": "\n        return self.field.odds\n\n    def _evens_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.evens` value.\n\n        :returns: The derived value\n        \"\"\"\n        return self.field.evens\n\n    def _calc_val(self):\n        \"\"\"Calculate a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.calc` value.\n\n        :returns: The derived value\n", "solution": "\n        if len(self.odds) == 0:\n            self.odds = [(1.00, [self.min, self.max])]\n\n        rand_val = rand.random()\n        total = 0\n        for percent,v in self.odds:\n            if total <= rand_val < total+percent:\n                found_v = v\n                break\n            total += percent\n\n        res = None\n        if isinstance(v, (tuple,list)):\n            rand_func = rand.randfloat if type(v[0]) is float else rand.randint\n\n            if len(v) == 2:\n                res = rand_func(v[0], v[1])\n            elif len(v) == 1:\n                res = v[0]\n        else:\n            res = v\n\n        return res"}
{"prompt": "def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    \"\"\"Ensures argument obj is a dictionary, and enforces that the keys/values conform to the types\n    specified by key_type, value_type.\n    \"\"\"", "output": "\n    if not isinstance(obj, dict):\n        raise TypeError(f\"{obj} is not a dictionary\")\n    for key, value in obj.items():\n        if not key_check(key):\n            raise ValueError(f\"{key} is not of type {key_type}\")\n        if not value_check(value):\n            raise ValueError(f\"{value} is not of type {value_type}\")\n\n", "solution": "\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj"}
{"prompt": "def _clear_temp(self):\n        \"\"\"\n        Clear temp directory from created csv and ods files during\n        communicator operations.\n        \"\"\"", "output": "\n        for f in os.listdir(self.temp_dir):\n            if f.endswith('.csv') or f.endswith('.ods'):\n                os.remove(os.path.join(self.temp_dir, f))\n\n", "solution": "\n        temp_files = [LOCAL_ODS, GDOCS_TRANS_CSV, GDOCS_META_CSV,\n                      LOCAL_TRANS_CSV, LOCAL_META_CSV]\n        for temp_file in temp_files:\n            file_path = os.path.join(self.temp_path, temp_file)\n            if os.path.exists(file_path):\n                os.remove(file_path)"}
{"prompt": "def to_zyz(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of ZYZ Euler angles.\n\n        Returns:\n            ndarray: Array of Euler angles.\n        \"\"\"", "output": "\n        q = self.q\n        q_conj = np.conjugate(q)\n        angle_c = 2 * np.arctan2(np.sqrt(q[0]**2 + q[1]**2 + q[2]**2), q[3])\n        angle_x = np.arcsin(q_conj[0])\n        angle_y = np.arcsin(q_conj[1])\n        angle_z = np.arcsin(q_conj[2])\n        return np.array([", "solution": "\n        mat = self.to_matrix()\n        euler = np.zeros(3, dtype=float)\n        if mat[2, 2] < 1:\n            if mat[2, 2] > -1:\n                euler[0] = math.atan2(mat[1, 2], mat[0, 2])\n                euler[1] = math.acos(mat[2, 2])\n                euler[2] = math.atan2(mat[2, 1], -mat[2, 0])\n            else:\n                euler[0] = -math.atan2(mat[1, 0], mat[1, 1])\n                euler[1] = np.pi\n        else:\n            euler[0] = math.atan2(mat[1, 0], mat[1, 1])\n        return euler"}
{"prompt": "def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Yields:\n        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.\n    \"\"\"", "output": "\n    if document_id is not None:\n        variants = adapter.find_variants(collaborator, document_id)\n    elif case_id is not None:\n        variants = adapter.find_variants(collaborator, case_id)\n    else:\n        raise ValueError(\"Either document_id or case_id must be provided\")\n\n    for variant in variants:\n        yield variant\n\n", "solution": "\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj"}
{"prompt": "def delete_uneeded(self):\n        \"\"\"\n        Delete the directory which are not registered into our structure.\n        \"\"\"", "output": "\n        for name in self.names:\n            if not name.startswith(self.root):\n                os.rmdir(os.path.join(self.root, name))\n\n", "solution": "\n\n        # We get the structure we have to apply.\n        structure = self._get_structure()\n\n        # We get the list of key which is implicitly the list of directory we do not bave to delete.\n        list_of_key = list(structure.keys())\n\n        # We move to the content of the parent as we know that we are creating only one directory.\n        # Note: if one day we will have to create multiple directory, we will have to change\n        # the following.\n        structure = structure[list_of_key[0]]\n\n        # We also set the parent directory as we are going to construct its childen.\n        parent_path = list_of_key[0]\n\n        if not parent_path.endswith(PyFunceble.directory_separator):\n            parent_path += PyFunceble.directory_separator\n\n        for root, _, _ in PyFunceble.walk(parent_path):\n            # We loop through each directories of the parent path.\n\n            # We fix the path in order to avoid issues.\n            root = Directory(root).fix_path()\n\n            if root.replace(parent_path, \"\") not in structure:\n                # The currently read directory is not in our structure.\n\n                # We delete it.\n                PyFunceble.rmtree(root)"}
{"prompt": "def is_effective_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"", "output": "\n        # Assume that the group is a dictionary with netids as keys and membership as values\n        group = self.groups[group_id]\n        return netid in group\n\n    def get_members(self, group_id):\n        \"\"\"\n        Returns a list of members in the group.\n        \"\"\"\n        # Assume that the groups is a dictionary with group_id as keys and members as values\n        groups = self.groups\n        return groups[group_id]\n\n    def get_netids(self, group_id):\n        \"\"\"\n        Returns a list", "solution": "\n        self._valid_group_id(group_id)\n\n        # GWS doesn't accept EPPNs on effective member checks, for UW users\n        netid = re.sub('@washington.edu', '', netid)\n\n        url = \"{}/group/{}/effective_member/{}\".format(self.API,\n                                                       group_id,\n                                                       netid)\n\n        try:\n            data = self._get_resource(url)\n            return True  # 200\n        except DataFailureException as ex:\n            if ex.status == 404:\n                return False\n            else:\n                raise"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for root, dirs, files in os.walk(rootpath):\r\n        for file in files:\r\n            if file.endswith('.py'):  # only look for Python files\r\n                filepath = os.path.join(root, file)\r\n                if filepath.startswith(skip):  # skip files that start with skip\r\n                    continue\r\n                with open(filepath, 'r') as f:  # read the file and extract module names\r\n                    module_name = filepath.", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def Rock(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"", "output": "\n    pass\n\n", "solution": "\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)"}
{"prompt": "def metadata(self):\n        \"\"\"Get metadata information in XML format.\"\"\"", "output": "\n        metadata = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, str):\n                metadata[key] = value\n            elif isinstance(value, int):\n                metadata[key] = str(value)\n            elif isinstance(value, bool):\n                metadata[key] = str(value).lower()\n        return metadata\n\n", "solution": "\n\n        params = {\n            self.PCTYPE: self.CTYPE_XML\n        }\n\n        response = self.call(self.CGI_BUG, params)\n\n        return response"}
{"prompt": "def put(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a PUT request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"", "output": "\n        # TODO: handle the request\n        pass\n\n    def delete(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a DELETE request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"\n        # TODO: handle the request\n        pass\n\n    def get(self, thing_", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        self.set_status(200)"}
{"prompt": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"", "output": "\n    if context is None:\n        context = {}\n\n    # Get the inferred value for the node\n    inferred_value = context.get(node.name)\n\n    # If the node is not in the context, or if the inferred value is None, return None\n    if inferred_value is None or inferred_value is None:\n        return None\n\n    # Check for ambiguity\n    if len(node.infer(context)) > 1:\n        return None\n\n    return inferred_value\n\n", "solution": "\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value"}
{"prompt": "def IsNotNone(*fields, default=None):\n    \"\"\"Selects whichever field is not None, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are None.\n    \"\"\"", "output": "\n    for field in fields:\n        value = yield field\n        if value is not None:\n            return value\n    return default\n\n", "solution": "\n\n    when_clauses = [\n        expressions.When(\n            ~expressions.Q(**{field: None}),\n            then=expressions.F(field)\n        )\n        for field in reversed(fields)\n    ]\n\n    return expressions.Case(\n        *when_clauses,\n        default=expressions.Value(default),\n        output_field=CharField()\n    )"}
{"prompt": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"", "output": " them with a comma.\n\n        If the authorization fails, the function raises an exception.\n\n        If the authorization is successful, the function redirects the user to the specified\n        callback URI with the provided attributes.\n\n        Args:\n            oauth_scope (str): The authorization scope to be used for the resource.\n            callback_uri (str): The URI to redirect the user to after authorization.\n            ax_attrs (List[str]): The attributes to be included in the redirect URI.\n\n        Returns:\n            None\n        \"\"\"\n        # Your code here\n\n", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def main(self,argv=None):\n        \"\"\"Run as a command-line script.\"\"\"", "output": "\n        if argv is None:\n            argv = sys.argv\n        try:\n            opts, args = getopt.getopt(argv, \"hv\", [\"help\", \"version\"])\n        except getopt.GetoptError:\n            print(self.usage)\n            sys.exit(2)\n        for opt, arg in opts:\n            if opt in (\"-h\", \"--help\"):\n                print(self.usage)\n                sys.exit()\n            elif opt in (\"-v\", \"--version\"):\n", "solution": "\n\n        parser = optparse.OptionParser(usage=USAGE % self.__class__.__name__)\n        newopt = parser.add_option\n        newopt('-i','--interact',action='store_true',default=False,\n               help='Interact with the program after the script is run.')\n\n        opts,args = parser.parse_args(argv)\n\n        if len(args) != 1:\n            print >> sys.stderr,\"You must supply exactly one file to run.\"\n            sys.exit(1)\n\n        self.run_file(args[0],opts.interact)"}
{"prompt": "def init_s3_keys(opt):\n    \"\"\"Initialize s3 access keys from environment variable or s3cfg config file.\"\"\"", "output": "\n    if opt.get('s3_keys'):\n        s3 = boto3.resource('s3')\n        s3_keys = s3.Bucket(opt['s3_bucket']).objects.all()\n        for obj in s3_keys:\n            s3_keys.update(obj.key)\n        opt['s3_keys'] = s3_keys\n    else:\n        config = Config(config={'s3': {'endpoint_url': 'https://s3.amazonaws.com'}})\n        ", "solution": "\n    S3Handler.S3_KEYS = S3Handler.s3_keys_from_cmdline(opt) or S3Handler.s3_keys_from_env() \\\n                        or S3Handler.s3_keys_from_s3cfg(opt)"}
{"prompt": "def _reshuffle(mat, shape):\n    \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\"", "output": "\n    n = shape[0]\n    m = shape[1]\n    indices = list(range(n*m))\n    random.shuffle(indices)\n    new_indices = [indices[i*m:(i+1)*m] for i in range(n)]\n    new_mat = [[mat[i][j] for j in indices] for i in new_indices]\n    return new_mat\n\n", "solution": "\n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),\n        (shape[3] * shape[1], shape[0] * shape[2]))"}
{"prompt": "def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"", "output": "\n        for scope in reversed(self.scopes):\n            if name in scope.symbols:\n                return scope\n        return None\n\n", "solution": "\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope"}
{"prompt": "def to_ndarray(self):\n        \"\"\"\n        Transfer JTensor to ndarray.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :return: a ndarray\n        \"\"\"", "output": "\n        raise NotImplementedError(\"SparseTensor does not support to_ndarray()\")\n\n    def __add__(self, other):\n        \"\"\"\n        Add two SparseTensor or a number to a SparseTensor.\n        :param other: a SparseTensor or a number\n        :return: a SparseTensor or a number\n        \"\"\"\n        if isinstance(other, SparseTensor):\n            return add(self, other)\n        elif isinstance(other, int):\n            return add_constant(self, other", "solution": "\n        assert self.indices is None, \"sparseTensor to ndarray is not supported\"\n        return np.array(self.storage, dtype=get_dtype(self.bigdl_type)).reshape(self.shape)"}
{"prompt": "def deprecated(msg=''):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\n\n    :param msg:\n\n        Additional message added to the warning.\n\n    \"\"\"", "output": "\n    def decorator(func):\n        \"\"\"This is the actual decorator\"\"\"\n        def wrapper(*args, **kwargs):\n            \"\"\"This is the wrapper\"\"\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n", "solution": "\n\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warning_string = \"Call to deprecated function or property `%s`.\" % func.__name__\n            warning_string = warning_string + ' ' + msg\n            warnings.warn(\n                warning_string,\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper"}
{"prompt": "def uncache_zipdir(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"", "output": "\n    import os\n    import time\n\n    # Get the current time\n    current_time = time.time()\n\n    # Get the modification time of the file\n    modified_time = os.path.getmtime(path)\n\n    # If the file has been modified since it was last cached, return\n    if current_time - modified_time < 86400:\n        return\n\n    # Otherwise, cache the file and return\n    import zipfile\n\n    with zipfile.ZipFile(path, 'r') as zip_ref:\n        for file_name in zip_", "solution": "\n    from zipimport import _zip_directory_cache as zdc\n    _uncache(path, zdc)\n    _uncache(path, sys.path_importer_cache)"}
{"prompt": "def adjust_frame(self, pos, absolute_pos):\n        \"\"\"Adjust stack frame by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"", "output": "\n        if absolute_pos:\n            pos = len(self.frames) - pos - 1\n        if pos < 0:\n            pos = 0\n        elif pos >= len(self.frames):\n            pos = len(self.frames) - 1\n        return pos\n\n    def __getitem__(self, index):\n        \"\"\"Get frame at given index. If index is a string, return\n        the corresponding frame object. If index is an integer,\n        return the corresponding frame object from the list of frames.\n        If index is out of range, raise an IndexError", "solution": "\n        if not self.curframe:\n            Mmsg.errmsg(self, \"No stack.\")\n            return\n\n        # Below we remove any negativity. At the end, pos will be\n        # the new value of self.curindex.\n        if absolute_pos:\n            if pos >= 0:\n                pos = len(self.stack)-pos-1\n            else:\n                pos = -pos-1\n        else:\n            pos += self.curindex\n\n        if pos < 0:\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the oldest frame.\")\n            return\n        elif pos >= len(self.stack):\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the newest frame.\")\n            return\n\n        self.curindex = pos\n        self.curframe = self.stack[self.curindex][0]\n        self.print_location()\n        self.list_lineno = None\n        return"}
{"prompt": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"", "output": "\n    if line in previous_line:\n        return None\n    else:\n        return line\n\n", "solution": "\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line"}
{"prompt": "def initialize(cls):\n        \"\"\"\n        Creates an **\"Example\"** entity of kind **\"NDBConfig\"** in the\n        datastore if the model is empty and raises and error to inform you that\n        you should populate the model with data.\n\n        .. note::\n\n            The *Datastore Viewer* in the ``_ah/admin/`` won't let you add\n            properties to a model if there is not an entity with that\n            property already. Therefore it is a good idea to keep the\n            **\"Example\"** entity (which has all possible properties set) in\n            the datastore.\n\n        \"\"\"", "output": " properties) in the model's data store\n            and to only add properties that are needed by the model's\n            data model.\n\n        \"\"\"\n        if not cls.model:\n            raise ValueError(\"You must create a model first\")\n\n        if not cls.model.data:\n            raise ValueError(\"The model has no data\")\n\n        # Create an empty entity with the given kind and ID\n        entity = cls.model.data.create_entity(cls.kind, cls.id)\n\n        # Set the entity's properties based on the model's data\n", "solution": "\n\n        if not len(cls.query().fetch()):\n\n            example = cls.get_or_insert('Example')\n\n            example.class_ = 'Provider class e.g. ' + \\\n                             '\"authomatic.providers.oauth2.Facebook\".'\n            example.provider_name = 'Your custom provider name e.g. \"fb\".'\n\n            # AuthorizationProvider\n            example.consumer_key = 'Consumer key.'\n            example.consumer_secret = 'Consumer secret'\n            example.provider_id = 1\n\n            # OAuth2\n            example.scope = 'coma, separated, list, of, scopes'\n\n            # AuthenticationProvider\n            example.identifier_param = 'Querystring parameter for claimed ' + \\\n                                       'id. default is \"id\"'\n\n            # Save the example\n            example.put()\n\n            # Raise an information error.\n            raise GAEError(\n                'A NDBConfig data model was created! Go to Datastore Viewer '\n                'in your dashboard and populate it with data!')"}
{"prompt": "def _config_session():\n        \"\"\"\n        Configure session for particular device\n\n        Returns:\n            tensorflow.Session\n        \"\"\"", "output": "\n        # Code to configure session for device goes here\n        pass\n\n    return _session\n\n", "solution": "\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = '0'\n        return tf.Session(config=config)"}
{"prompt": "def _ensure_executor_running(self):\n        \"\"\"ensures all workers and management thread are running\n        \"\"\"", "output": "\n        for i in range(self.num_workers):\n            if not self.executor.is_alive():\n                raise ValueError(\"All workers have been killed.\")\n        if not self.executor.is_alive():\n            raise ValueError(\"All workers have been killed.\")\n\n    def _start_executor(self):\n        \"\"\"starts the execution of workers in a separate thread\n        \"\"\"\n        self.executor = Thread(target=self.run)\n        self.executor.start()\n\n    def _join_executor(self):\n", "solution": "\n        with self._processes_management_lock:\n            if len(self._processes) != self._max_workers:\n                self._adjust_process_count()\n            self._start_queue_management_thread()"}
{"prompt": "def getPos(self):\n        \"\"\"\n        Returns the absolute position and size of the layer.\n        \n        This method is intended for use in vertex position calculation, as the border and offset have already been applied.\n        \n        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\\ .\n        The two values starting with an s are the \"start\" position, or the lower-left corner.\n        The second pair of values signify the \"end\" position, or upper-right corner.\n        \"\"\"", "output": "\n        return (self.sx, self.sy, self.ex, self.ey)\n\n", "solution": "\n        # Returns sx,sy,ex,ey\n        # sx,sy are bottom-left/lowest\n        # ex,ey are top-right/highest\n        sx,sy = self.widget.pos[0]+self.border[0]+self.offset[0],                       self.widget.pos[1]+self.border[1]+self.offset[1]\n        ex,ey = self.widget.pos[0]+self.widget.size[0]-self.border[0]+self.offset[0],   self.widget.pos[1]+self.widget.size[1]-self.border[1]+self.offset[1]\n        return sx,sy,ex,ey"}
{"prompt": "def users(self):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"", "output": "\n        return self.members\n\n    def members(self):\n        \"\"\"\n        List of members of this slack team\n        \"\"\"\n        return self.team.members\n\n    def get_member_count(self):\n        \"\"\"\n        Returns the number of members in this slack team\n        \"\"\"\n        return len(self.members)\n\n    def get_member_name(self, index):\n        \"\"\"\n        Returns the name of the member at the specified index in this slack team\n        \"\"\"\n        return self.members[index].name\n\n    def get_member_email(self", "solution": "\n        if not self._users:\n            self._users = self._call_api('users.list')['members']\n        return self._users"}
{"prompt": "def decode_instruction(instruction):\n        \"\"\"\n        Decode whole instruction and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode_instruction('4.size,4.1024;')\n        >> args == ['size', '1024']\n        >> True\n\n        :param instruction: Instruction string.\n\n        :return: list\n        \"\"\"", "output": "\n        args = instruction.split(';')\n        return [arg.strip() for arg in args]\n\n    def encode_instruction(args):\n        \"\"\"\n        Encode whole instruction and return string.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = ['size', '1024']\n        >> encode_instruction(args) == '4.size,4.1024'\n        >> True\n\n        :param args: list of args.\n\n        :return: string\n        \"\"\"\n        return ','.join(args", "solution": "\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        # Use proper encoding\n        instruction = utf8(instruction)\n\n        # Get arg size\n        elems = instruction.split(ELEM_SEP, 1)\n\n        try:\n            arg_size = int(elems[0])\n        except Exception:\n            # Expected ValueError\n            raise InvalidInstruction(\n                'Invalid arg length.' +\n                ' Possibly due to missing element separator!')\n\n        arg_str = elems[1][:arg_size]\n\n        remaining = elems[1][arg_size:]\n\n        args = [arg_str]\n\n        if remaining.startswith(ARG_SEP):\n            # Ignore the ARG_SEP to parse next arg.\n            remaining = remaining[1:]\n        elif remaining == INST_TERM:\n            # This was the last arg!\n            return args\n        else:\n            # The remaining is neither starting with ARG_SEP nor INST_TERM.\n            raise InvalidInstruction(\n                'Instruction arg (%s) has invalid length.' % arg_str)\n\n        next_args = GuacamoleInstruction.decode_instruction(remaining)\n\n        if next_args:\n            args = args + next_args\n\n        return args"}
{"prompt": "def handle_combo(self,combo,symbol,modifiers,release=False,mod=True):\n        \"\"\"\n        Handles a key combination and dispatches associated events.\n        \n        First, all keybind handlers registered via :py:meth:`add` will be handled,\n        then the pyglet event :peng3d:pgevent:`on_key_combo` with params ``(combo,symbol,modifiers,release,mod)`` is sent to the :py:class:`Peng()` instance.\n        \n        Also sends the events :peng3d:event:`peng3d:keybind.combo`\\, :peng3d:event:`peng3d:keybind.combo.press` and :peng3d:event`peng3d:keybind.combo.release`\\ .\n        \n        :params str combo: Key combination pressed\n        :params int symbol: Key pressed, passed from the same argument within pyglet\n        :params int modifiers: Modifiers held while the key was pressed\n        :params bool release: If the combo was released\n        :params bool mod: If the combo was sent without mods\n        \"\"\"", "output": "        If the event is a release event (i.e. the modifier is `True`), the function will also dispatch the associated events.\n        \n        If the event is a key press event (i.e. the modifier is `False`), the function will simply dispatch the associated events.\n        \n        :param combo: The key combination (e.g. \"ctrl+alt+delete\")\n        :param symbol: The symbol associated with the key combination (e.g. \"shift\", \"tab\", \"enter\")\n        :param modifiers: A list of boolean values indicating whether the key combination", "solution": "\n        if self.peng.cfg[\"controls.keybinds.debug\"]:\n            print(\"combo: nm=%s %s\"%(mod,combo))\n        if mod:\n            for kbname in self.keybinds.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        else:\n            for kbname in self.keybinds_nm.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        self.peng.sendPygletEvent(\"on_key_combo\",(combo,symbol,modifiers,release,mod))\n        self.peng.sendEvent(\"peng3d:keybind.combo\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        if release:\n            self.peng.sendEvent(\"peng3d:keybind.combo.release\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        else:\n            self.peng.sendEvent(\"peng3d:keybind.combo.press\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})"}
{"prompt": "def expand_tokens(tokens, equal=False):\n    \"\"\"Given a list of tokens, return a generator of the chunks of\n    text for the data in the tokens.\n    \"\"\"", "output": "\n    for token in tokens:\n        if equal:\n            yield token\n        else:\n            for chunk in token.split():\n                yield chunk\n\n", "solution": "\n    for token in tokens:\n        for pre in token.pre_tags:\n            yield pre\n        if not equal or not token.hide_when_equal:\n            if token.trailing_whitespace:\n                yield token.html() + token.trailing_whitespace\n            else:\n                yield token.html()\n        for post in token.post_tags:\n            yield post"}
{"prompt": "def replay_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use replay detection for messages protected by\n        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.\n        \"\"\"", "output": "\n        return self.get_mic().use_replay_detection\n\n    def get_mic(self):\n        \"\"\"\n        Returns the security context used by the caller.\n\n        This is a read-only property and should not be modified.\n        \"\"\"\n        return self._mic\n\n    def set_mic(self, mic):\n        \"\"\"\n        Sets the security context used by the caller.\n\n        This is a write-only property and should not be modified.\n        \"\"\"\n        self._mic = mic\n\n    mic = property(get_mic, set_mic)", "solution": "\n        return (\n            self.flags & C.GSS_C_REPLAY_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"", "output": "\n        # code to display the table as text\n        pass\n    \n    def visit_cell(self, row, col):\n        \"\"\"display a cell as text\"\"\"\n        # code to display the cell as text\n        pass\n    \n    def visit_menu(self, items):\n        \"\"\"display a menu as text\"\"\"\n        # code to display the menu as text\n        pass\n    \n    def visit_data(self, data):\n        \"\"\"display data as text\"\"\"\n        # code to display the data as text\n        pass\n    \n    def visit_error(", "solution": "\n        table_content = self.get_table_content(layout)\n        # get columns width\n        cols_width = [0] * len(table_content[0])\n        for row in table_content:\n            for index, col in enumerate(row):\n                cols_width[index] = max(cols_width[index], len(col))\n        self.default_table(layout, table_content, cols_width)\n        self.writeln()"}
{"prompt": "def variants(context, collaborator, document_id, case_id, json):\n    \"\"\"Export causatives for a collaborator in .vcf format\"\"\"", "output": "\n    # Get the variant for the given document and case\n    variant = None\n    for record in json:\n        if record['id'] == document_id:\n            for record_case in record['cases']:\n                if record_case['id'] == case_id:\n                    variant = record_case['variants'][0]\n                    break\n            if variant:\n                break\n    \n    # If no variant was found, return an empty string\n    if not variant:\n        return ''\n    \n    # Create the header row for the output\n    ", "solution": "\n    LOG.info(\"Running scout export variants\")\n    adapter = context.obj['adapter']\n    collaborator = collaborator or 'cust000'\n\n    variants = export_variants(\n        adapter,\n        collaborator,\n        document_id=document_id,\n        case_id=case_id\n    )\n\n    if json:\n        click.echo(dumps([var for var in variants]))\n        return\n\n    vcf_header = VCF_HEADER\n\n    #If case_id is given, print more complete vcf entries, with INFO,\n    #and genotypes\n    if case_id:\n        vcf_header[-1] = vcf_header[-1] + \"\\tFORMAT\"\n        case_obj = adapter.case(case_id=case_id)\n        for individual in case_obj['individuals']:\n            vcf_header[-1] = vcf_header[-1] + \"\\t\" + individual['individual_id']\n\n    #print header\n    for line in vcf_header:\n        click.echo(line)\n\n    for variant_obj in variants:\n        variant_string = get_vcf_entry(variant_obj, case_id=case_id)\n        click.echo(variant_string)"}
{"prompt": "def _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):\n    \"\"\"Formats the plot with the correct axis labels, title, ticks, and\n    so on.\"\"\"", "output": "\n    # Set up plot\n    fig, ax = plt.subplots()\n    ax.set_title(title)\n    ax.set_xlabel('Algorithm ID')\n    ax.set_ylabel('Estimated Time')\n    ax.set_xlim(0, last_bound)\n    ax.set_ylim(0, N)\n\n    # Plot estimated times for each algorithm\n    for i, algo_id in enumerate(algo_ids):\n        x = list(range(len(est_file[algo_id", "solution": "\n    import matplotlib.pyplot as plt\n    if title is None:\n        title = os.path.basename(est_file).split(\".\")[0]\n    plt.title(title)\n    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))\n    plt.gcf().subplots_adjust(bottom=0.22)\n    plt.gca().set_yticklabels(algo_ids)\n    plt.xlabel(\"Time (seconds)\")\n    plt.xlim((0, last_bound))\n    plt.tight_layout()\n    if output_file is not None:\n        plt.savefig(output_file)\n    plt.show()"}
{"prompt": "def f_remove(self, recursive=True, predicate=None):\n        \"\"\"Recursively removes the group and all it's children.\n\n        :param recursive:\n\n            If removal should be applied recursively. If not, node can only be removed\n            if it has no children.\n\n        :param predicate:\n\n            In case of recursive removal, you can selectively remove nodes in the tree.\n            Predicate which can evaluate for each node to ``True`` in order to remove the node or\n            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.\n\n        \"\"\"", "output": " all nodes.\n\n        :return:\n\n            The removed node.\n\n        \"\"\"\n        if predicate is None:\n            predicate = lambda node: True\n\n        if self.is_empty():\n            return None\n\n        if recursive:\n            nodes_to_remove = []\n            for child in self.children:\n                if predicate(child):\n                    nodes_to_remove.append(child)\n                nodes_to_remove.extend(child.f_remove(recursive, predicate))\n            if not nodes_to_remove:\n                return self\n", "solution": "\n        parent = self.f_get_parent()\n        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)"}
{"prompt": "def _wrap(text, wrap_at=120, indent=4):\n    \"\"\"\n    Return piece of text, wrapped around if needed.\n\n    :param text: text that may be too long and then needs to be wrapped.\n    :param wrap_at: the maximum line length.\n    :param indent: number of spaces to prepend to all subsequent lines after the first.\n    \"\"\"", "output": "\n    if len(text) <= wrap_at:\n        return text\n\n    lines = []\n    while len(text) > wrap_at:\n        # Find the last space before wrap_at\n        last_space = text[:wrap_at].rfind(' ')\n        if last_space == -1:\n            # No space found, just split at wrap_at\n            last_space = wrap_at\n        lines.append(text[:last_space])\n        text = text[last_space+1:]\n\n        # Wrap the text\n        wra", "solution": "\n    out = \"\"\n    curr_line_length = indent\n    space_needed = False\n    for word in text.split():\n        if curr_line_length + len(word) > wrap_at:\n            out += \"\\n\" + \" \" * indent\n            curr_line_length = indent\n            space_needed = False\n        if space_needed:\n            out += \" \"\n            curr_line_length += 1\n        out += word\n        curr_line_length += len(word)\n        space_needed = True\n    return out"}
{"prompt": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize payload of a HTTP request by removing the login, password and token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns url, headers and the sanitized payload\n        \"\"\"", "output": "\n        # Remove the login, password and token information from the payload\n        sanitized_payload = payload.replace('login=', '').replace('password=', '').replace('token=', '')\n\n        # Store the sanitized payload in the database or file\n        #...\n\n        return url, headers, sanitized_payload\n\n    # Define the database or file path\n    db_path = '/var/www/myapp/myapp.db'\n\n    # Open the database connection\n    with sqlite3.connect(db_path) as conn:\n        ", "solution": "\n        if BugzillaRESTClient.PBUGZILLA_LOGIN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_LOGIN)\n\n        if BugzillaRESTClient.PBUGZILLA_PASSWORD in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_PASSWORD)\n\n        if BugzillaRESTClient.PBUGZILLA_TOKEN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_TOKEN)\n\n        return url, headers, payload"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"", "output": "\n        for i in range(len(self.notifications)):\n            print(f\"Cycling notification {self.notifications[i]} with latest result {self.results[i]}\")\n            # code to update notification with latest result\n\n", "solution": "\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)"}
{"prompt": "def user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"", "output": "\n    return \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n\n", "solution": "\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        distro = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], platform.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], platform.libc_ver()),\n        ))\n        if libc:\n            distro[\"libc\"] = libc\n        if distro:\n            data[\"distro\"] = distro\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"OS X\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )"}
{"prompt": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"", "output": "\n        self.headers = headers\n        return self\n\n    def match_headers(self, headers):\n        \"\"\"\n        Checks if the given headers match the headers specified in the\n        headers argument.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            bool: True if the headers match, False otherwise.\n        \"\"\"\n        return set(headers) <= set(self.headers)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Calls the function specified by the name of the", "solution": "\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))"}
{"prompt": "def resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t\"\"\"Return id (or metadata) of an object, specified by chain\n\t\t\t\t(iterable or fs-style path string) of \"name\" attributes\n\t\t\t\tof its ancestors, or raises DoesNotExists error.\n\n\t\t\tRequires many calls to resolve each name in path, so use with care.\n\t\t\t\troot_id parameter allows to specify path relative to some folder_id\n\t\t\t\t(default: me/skydrive).\"\"\"", "output": "\n\t\tif objects:\n\t\t\treturn self.get_objects(path, root_id)\n\t\telse:\n\t\t\treturn self.get_file(path, root_id, listdir_limit)\n\n\ndef resolve_path_or_raise(path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t\"\"\"Like resolve_path, but returns an object if it exists, or raises DoesNotExists error.\n\t\t\tIf objects is True, returns a list of objects instead of just the id.\"\"\"\n\t\ttry:\n\t\t\treturn resolve", "solution": "\n\t\tif path:\n\t\t\tif isinstance(path, types.StringTypes):\n\t\t\t\tif not path.startswith('me/skydrive'):\n\t\t\t\t\t# Split path by both kinds of slashes\n\t\t\t\t\tpath = filter(None, it.chain.from_iterable(p.split('\\\\') for p in path.split('/')))\n\t\t\t\telse: root_id, path = path, None\n\t\t\tif path:\n\t\t\t\ttry:\n\t\t\t\t\tfor i, name in enumerate(path):\n\t\t\t\t\t\toffset = None\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tobj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)\n\t\t\t\t\t\t\ttry: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]\n\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\tif len(obj_list) < listdir_limit: raise # assuming that it's the last page\n\t\t\t\t\t\t\t\toffset = (offset or 0) + listdir_limit\n\t\t\t\t\t\t\telse: break\n\t\t\t\texcept (KeyError, ProtocolError) as err:\n\t\t\t\t\tif isinstance(err, ProtocolError) and err.code != 404: raise\n\t\t\t\t\traise DoesNotExists(root_id, path[i:])\n\t\treturn root_id if not objects else self.info(root_id)"}
{"prompt": "def require(self, key):\n        \"\"\"\n        Raises an exception if value for ``key`` is empty.\n        \"\"\"", "output": "\n        if not self.data.get(key):\n            raise ValueError(f\"Value for key '{key}' is empty.\")\n\n    def get(self, key):\n        \"\"\"\n        Returns the value for the given key, or an empty string if the key is not present.\n        \"\"\"\n        return self.data.get(key, \"\")\n\n    def set(self, key, value):\n        \"\"\"\n        Sets the value for the given key. If the key is not present, it is added to the dictionary with an empty string as its value.\n        ", "solution": "\n        value = self.get(key)\n        if not value:\n            raise ValueError('\"{}\" is empty.'.format(key))\n        return value"}
{"prompt": "def Rconverter(Robj, dataframe=False):\n    \"\"\"\n    Convert an object in R's namespace to one suitable\n    for ipython's namespace.\n\n    For a data.frame, it tries to return a structured array.\n    It first checks for colnames, then names.\n    If all are NULL, it returns np.asarray(Robj), else\n    it tries to construct a recarray\n\n    Parameters\n    ----------\n\n    Robj: an R object returned from rpy2\n    \"\"\"", "output": "\n\n    if dataframe:\n        # try to construct a dataframe\n        try:\n            import pandas as pd\n            return pd.DataFrame(Robj)\n        except ImportError:\n            pass\n\n    # check for colnames\n    if not robj.colnames:\n        colnames = ['col1', 'col2', 'col3']\n        if not all(col in robj.names for col in colnames):\n            raise ValueError(\"Dataframe is missing required column(s)\")\n\n    # check for names\n    if not robj", "solution": "\n    is_data_frame = ro.r('is.data.frame')\n    colnames = ro.r('colnames')\n    rownames = ro.r('rownames') # with pandas, these could be used for the index\n    names = ro.r('names')\n\n    if dataframe:\n        as_data_frame = ro.r('as.data.frame')\n        cols = colnames(Robj)\n        _names = names(Robj)\n        if cols != ri.NULL:\n            Robj = as_data_frame(Robj)\n            names = tuple(np.array(cols))\n        elif _names != ri.NULL:\n            names = tuple(np.array(_names))\n        else: # failed to find names\n            return np.asarray(Robj)\n        Robj = np.rec.fromarrays(Robj, names = names)\n    return np.asarray(Robj)"}
{"prompt": "def pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"", "output": "\n    import os\n    return os.path.exists(str(pid))\n\n", "solution": "\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError as exc:\n        logging.debug(\"No process[%s]: %s\", exc.errno, exc)\n        return exc.errno == errno.EPERM\n    else:\n        p = psutil.Process(pid)\n        return p.status != psutil.STATUS_ZOMBIE"}
{"prompt": "def start_hb(self, callback):\n        \"\"\"Start the heartbeating and call the callback if the kernel dies.\"\"\"", "output": "\n        self.kernel.start_hb(callback)\n\n    def stop_hb(self, callback):\n        \"\"\"Stop the heartbeating and call the callback if the kernel dies.\"\"\"\n        self.kernel.stop_hb(callback)\n\n    def reset(self):\n        \"\"\"Reset the kernel and its state to its initial state.\"\"\"\n        self.kernel.reset()\n\n", "solution": "\n        if not self._beating:\n            self._kernel_alive = True\n\n            def ping_or_dead():\n                self.hb_stream.flush()\n                if self._kernel_alive:\n                    self._kernel_alive = False\n                    self.hb_stream.send(b'ping')\n                    # flush stream to force immediate socket send\n                    self.hb_stream.flush()\n                else:\n                    try:\n                        callback()\n                    except:\n                        pass\n                    finally:\n                        self.stop_hb()\n\n            def beat_received(msg):\n                self._kernel_alive = True\n\n            self.hb_stream.on_recv(beat_received)\n            loop = ioloop.IOLoop.instance()\n            self._hb_periodic_callback = ioloop.PeriodicCallback(ping_or_dead, self.time_to_dead*1000, loop)\n            loop.add_timeout(time.time()+self.first_beat, self._really_start_hb)\n            self._beating= True"}
{"prompt": "def _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"", "output": "\n    rows = []\n    while noise:\n        row = noise[:8]\n        noise = noise[8:]\n        if len(row)!= 8:\n            break\n        rows.append(row)\n    return rows\n\n", "solution": "\n    data = zip(\n        noise[\"freq\"],\n        noise[\"nf\"],\n        np.abs(noise[\"rc\"]),\n        np.angle(noise[\"rc\"]),\n        noise[\"res\"],\n    )\n    for freq, nf, rcmag, rcangle, res in data:\n        yield freq, nf, rcmag, rcangle, res"}
{"prompt": "def is_lesser(a, b):\n    \"\"\"\n    Verify that an item *a* is <= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"", "output": "\n    return a <= b\n\n", "solution": "\n\n    if type(a) != type(b):\n        return False\n\n    if isinstance(a, str) and isinstance(b, str):\n        return a == b\n    elif isinstance(a, bool) and isinstance(b, bool):\n        return a == b\n    elif isinstance(a, list) and isinstance(b, list):\n        for element in a:\n            flag = 0\n            for e in b:\n                if is_lesser(element, e):\n                    flag = 1\n                    break\n            if not flag:\n                return False\n        return True\n    elif isinstance(a, dict) and isinstance(b, dict):\n        if is_lesser(list(a.keys()), list(b.keys())):\n            for key, val in a.items():\n                if not is_lesser(val, b[key]):\n                    return False\n            return True\n        return False\n    elif isinstance(a, int) and isinstance(b, int):\n        return a <= b\n    elif isinstance(a, float) and isinstance(b, float):\n        return a <= b\n\n    return False"}
{"prompt": "def isreshapeable(new, old):\n    \"\"\"\n    Check to see if a proposed tuple of axes is a valid reshaping of\n    the old axes by ensuring that they can be factored.\n\n    Parameters\n    ----------\n    new : tuple\n        tuple of proposed axes\n\n    old : tuple\n        tuple of old axes\n    \"\"\"", "output": "\n    # Check that the new axes are valid\n    for axis in new:\n        if axis < 0:\n            raise ValueError(\"New axes must be non-negative integers\")\n\n    # Check that the old axes are valid\n    for axis in old:\n        if axis < 0:\n            raise ValueError(\"Old axes must be non-negative integers\")\n\n    # Check that the new axes can be factored by the old axes\n    for axis in new:\n        if old.count(axis) > 1:\n            raise ValueError(\"New axes cannot be factored by old axes", "solution": "\n\n    new, old = tupleize(new), tupleize(old)\n\n    if not prod(new) == prod(old):\n        raise ValueError(\"Total size of new keys must remain unchanged\")"}
{"prompt": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"", "output": "\n    def generate_node(node):\n        if isinstance(node, ast.AST):\n            return node\n        elif isinstance(node, list):\n            return [generate_node(n) for n in node]\n        elif isinstance(node, tuple):\n            return tuple(generate_node(n) for n in node)\n        else:\n            return node\n    \n    def generate_sequence(node):\n        if isinstance(node, ast.AST):\n            return node\n        elif isinstance(node, list):\n            ", "solution": "\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator"}
{"prompt": "def next_opcode(code, offset):\n    \"\"\"Return the next opcode and offset as a tuple. Tuple (-100,\n    -1000) is returned when reaching the end.\"\"\"", "output": "\n    if code[offset] == 99:\n        return (-100, offset)\n    elif code[offset] == 1:\n        return (-101, offset)\n    elif code[offset] == 2:\n        return (-102, offset)\n    elif code[offset] == 3:\n        return (-103, offset)\n    elif code[offset] == 4:\n        return (-104, offset)\n    elif code[offset] == 5:\n        return (-105, offset)\n    elif code[offset] == 6:\n        return (-", "solution": "\n    n = len(code)\n    while offset < n:\n        op = code[offset]\n        offset += 1\n        if op >= HAVE_ARGUMENT:\n            offset += 2\n            pass\n        yield op, offset\n        pass\n    yield -100, -1000\n    pass"}
{"prompt": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"", "output": "alar `Tensor` representing the expected rank of `x`.\n    data:  Numeric `Tensor` representing the data to be used in the operation.\n    summarize:  String `Tensor` representing a summary of the operation.\n    message:  String `Tensor` representing a message for the operation.\n    name:  String `Tensor` representing the name of the operation.\n\n  Returns:\n    Assertion op that checks that `x` has rank equal to `rank` or smaller.\n\n  Raises:\n    ValueError:  If `rank", "solution": "\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)"}
{"prompt": "def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "output": ")\n        :type request_id: str\n        :param project_id: Optional, unique project_id that you might add to achieve\n            full idempotence (for example when client call times out)\n        :type project_id: str\n        :return: Instance template representation as object\n        :rtype: dict\n        \"\"\"\n        if request_id is None:\n            request_id = str(uuid.uuid4())\n        if project_id is None:\n            project_id = str(uuid.uuid4())\n        instance_", "solution": "\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"}
{"prompt": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"", "output": "\n        if self.allow_reconnect:\n            self.switch_server()\n        else:\n            self.disconnect()\n        raise e\n\n    def switch_server(self):\n        \"\"\"\n        Switch the server to which the protocol is currently connected.\n        \"\"\"\n        self.server = self.protocols[self.server_index]\n\n    def disconnect(self):\n        \"\"\"\n        Disconnect the protocol from the server.\n        \"\"\"\n        self.server.disconnect()\n        self.server = None\n\n    def parse_message(self, message", "solution": "\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)"}
{"prompt": "def get_generator(self, path, *args, **kw_args):\n        \"\"\"\n        Get a generator that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the generator one row at a time.\n        Unlike the direct access queue, this generator also returns the remainder elements.\n        Additional arguments are forwarded to get_queue.\n        See the get_queue method for documentation of these parameters.\n\n        :param path:\n        :return: A generator that iterates over the rows in the dataset.\n        \"\"\"", "output": "\n        queue = self.get_queue(path, *args, **kw_args)\n        while True:\n            row = queue.get()\n            if row is None:\n                break\n            yield row\n            queue.task_done()\n\n    def get_queue(self, path, *args, **kw_args):\n        \"\"\"\n        Get a queue that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the queue one row at a time.\n        Unlike the direct access queue, this queue also returns the remainder elements.\n        ", "solution": "\n        q = self.get_queue(path=path, *args, **kw_args)\n\n        try:\n            # This generator just implements a standard access pattern for the direct access queue.\n            for guard in q.iter():\n                with guard as batch:\n                    batch_copy = batch.copy()\n\n                for row in batch_copy:\n                    yield row\n\n            last_batch = self.get_remainder(path, q.block_size)\n            for row in last_batch:\n                yield row\n\n        finally:\n            q.close()"}
{"prompt": "def running_under_virtualenv():\n    \"\"\"\n    Return True if we're running inside a virtualenv, False otherwise.\n\n    \"\"\"", "output": "\n    import os\n    import sys\n\n    virtualenv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"venv\"))\n    if virtualenv_path in sys.path:\n        return True\n    else:\n        return False\n\n", "solution": "\n    if hasattr(sys, 'real_prefix'):\n        return True\n    elif sys.prefix != getattr(sys, \"base_prefix\", sys.prefix):\n        return True\n\n    return False"}
{"prompt": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the String instance\n\n        :param list pre: The prerequisites list (optional, default=None)\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"", "output": "\n        if pre is None:\n            pre = []\n        if shortest:\n            return ''.join(pre)\n        else:\n            return ''.join(pre + [str(i) for i in range(1, len(pre) + 2)])\n\n", "solution": "\n        if pre is None:\n            pre = []\n\n        if self.value is not None and rand.maybe():\n            return utils.val(self.value, pre, shortest=shortest)\n\n        length = super(String, self).build(pre, shortest=shortest)\n        res = rand.data(length, self.charset)\n        return res"}
{"prompt": "def chimera_block_quotient(G, blocks):\n    \"\"\"\n    Extract the blocks from a graph, and returns a\n    block-quotient graph according to the acceptability\n    functions block_good and eblock_good\n\n    Inputs:\n        G: a networkx graph\n        blocks: a tuple of tuples\n\n    \"\"\"", "output": "\n    # Extract the blocks from the graph\n    block_set = set()\n    for block in blocks:\n        block_set.add(block[0])\n        block_set.add(block[1])\n\n    # Create a new graph to store the quotient\n    Q = nx.Graph()\n\n    # Iterate over the nodes in G\n    for node in G.nodes():\n        # Check if the node is in block_set\n        if node in block_set:\n            # If it is, add an edge to the quotient graph\n            for", "solution": "\n    from networkx import Graph\n    from itertools import product\n\n    BG = Graph()\n    blockid = {}\n    for i, b in enumerate(blocks):\n        BG.add_node(i)\n        if not b or not all(G.has_node(x) for x in b):\n            continue\n        for q in b:\n            if q in blockid:\n                raise(RuntimeError, \"two blocks overlap\")\n            blockid[q] = i\n\n    for q, u in blockid.items():\n        ublock = blocks[u]\n        for p in G[q]:\n            if p not in blockid:\n                continue\n            v = blockid[p]\n            if BG.has_edge(u, v) or u == v:\n                continue\n            vblock = blocks[v]\n\n            if ublock[0][2] == vblock[0][2]:\n                block_edges = zip(ublock, vblock)\n            else:\n                block_edges = product(ublock, vblock)\n\n            if all(G.has_edge(x, y) for x, y in block_edges):\n                BG.add_edge(u, v)\n\n    return BG"}
{"prompt": "def process_commands(self):\n        \"\"\"Handle debugger commands.\"\"\"", "output": "\n        pass\n\n    def execute_commands(self):\n        \"\"\"Execute debugger commands.\"\"\"\n        while True:\n            command = input()\n            if command == 'quit':\n                break\n            elif command =='step':\n                self.execute_step()\n            elif command == 'list':\n                self.list_variables()\n            else:\n                print(\"Invalid command.\")\n\n    def execute_step(self):\n        \"\"\"Execute one step in the debugger.\"\"\"\n        pass\n\n    def list_variables(self):\n        \"\"\"List", "solution": "\n        if self.core.execution_status != 'No program':\n            self.setup()\n            self.location()\n            pass\n        leave_loop = run_hooks(self, self.preloop_hooks)\n        self.continue_running = False\n\n        while not leave_loop:\n            try:\n                run_hooks(self, self.precmd_hooks)\n                # bdb had a True return to leave loop.\n                # A more straight-forward way is to set\n                # instance variable self.continue_running.\n                leave_loop = self.process_command()\n                if leave_loop or self.continue_running: break\n            except EOFError:\n                # If we have stacked interfaces, pop to the next\n                # one.  If this is the last one however, we'll\n                # just stick with that.  FIXME: Possibly we should\n                # check to see if we are interactive.  and not\n                # leave if that's the case. Is this the right\n                # thing?  investigate and fix.\n                if len(self.debugger.intf) > 1:\n                    del self.debugger.intf[-1]\n                    self.last_command = ''\n                else:\n                    if self.debugger.intf[-1].output:\n                        self.debugger.intf[-1].output.writeline('Leaving')\n                        raise Mexcept.DebuggerQuit\n                        pass\n                    break\n                pass\n            pass\n        return run_hooks(self, self.postcmd_hooks)"}
{"prompt": "def table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user.\"\"\"", "output": "\n    if auth is None:\n        auth = get_default_auth()\n    if eager:\n        # eager mode allows for tables to be created and deleted without requiring user intervention\n        # this is useful for testing and development purposes\n        tables = {\n            name: {\n                \"user\": name,\n                \"table\": Table(name, auth=auth)\n            }\n        }\n    else:\n        # non-eager mode allows for tables to be created and modified by the user\n        tables = {\n            name: {\n                \"user\": name,", "solution": "\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    table = dynamodb.get_table(name)\n    return Table(table=table, eager=eager)"}
{"prompt": "def _is_raising(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node raise an exception\"\"\"", "output": "\n    for node in body:\n        if isinstance(node, ast.Raise):\n            return True\n    return False\n\n", "solution": "\n    for node in body:\n        if isinstance(node, astroid.Raise):\n            return True\n    return False"}
{"prompt": "def get_assembly_size(assembly_file):\n    \"\"\"Returns the number of nucleotides and the size per contig for the\n    provided assembly file path\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n\n    Returns\n    -------\n    assembly_size : int\n        Size of the assembly in nucleotides\n    contig_size : dict\n        Length of each contig (contig name as key and length as value)\n\n    \"\"\"", "output": "\n    with open(assembly_file, 'r') as f:\n        contig_size = {}\n        for line in f:\n            if line.startswith('>'):\n                contig_name = line.strip()[1:]\n                contig_size[contig_name] = 0\n            else:\n                contig_size[contig_name] += len(line.strip())\n    return sum(contig_size.values()), contig_size\n\n", "solution": "\n\n    assembly_size = 0\n    contig_size = {}\n    header = \"\"\n\n    with open(assembly_file) as fh:\n        for line in fh:\n\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n\n            if line.startswith(\">\"):\n                header = line.strip()[1:]\n                contig_size[header] = 0\n\n            else:\n                line_len = len(line.strip())\n                assembly_size += line_len\n                contig_size[header] += line_len\n\n    return assembly_size, contig_size"}
{"prompt": "def compute_index_key(self, to_instance):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n        \"\"\"", "output": "\n        return (to_instance.id, to_instance.name)\n\n    def get_link_key(self, to_instance):\n        \"\"\"\n        Compute the key that can be used to identify a link on the page.\n        \"\"\"\n        return (to_instance.id, to_instance.name, to_instance.url)\n\n    def get_link_id(self, link_key):\n        \"\"\"\n        Compute the ID of a link based on its key.\n        \"\"\"\n        return link_key[0]\n\n    def get_instance_", "solution": "\n        kwargs = dict()\n        for attr in self.key_map.values():\n            if _is_null(to_instance, attr):\n                return None\n            \n            if attr in to_instance.__dict__:\n                kwargs[attr] = to_instance.__dict__[attr]\n            else:\n                kwargs[attr] = getattr(to_instance, attr)\n\n        return frozenset(tuple(kwargs.items()))"}
{"prompt": "def p_boolean_expression(self, p):\n        \"\"\"\n        expression : expression LE          expression\n                   | expression LESSTHAN    expression\n                   | expression DOUBLEEQUAL expression\n                   | expression NOTEQUAL    expression\n                   | expression GE          expression\n                   | expression GT          expression\n                   | expression AND         expression\n                   | expression OR          expression\n        \"\"\"", "output": "\n        if len(p) == 2:\n            p[0] = p[1]\n        elif len(p) == 3:\n            p[0] = p[1] + p[2]\n        else:\n            raise ValueError(\"Invalid boolean expression\")\n\n    def p_expression_group(self, p):\n        \"\"\"\n        expression : LPAREN expression RPAREN\n        \"\"\"\n        p[0] = p[2]\n\n    def p_error(self, p):\n        raise ValueError(\"Invalid syntax\")\n\n    return yacc", "solution": "\n        p[0] = BinaryOperationNode(left=p[1],\n                                   operator=p[2],\n                                   right=p[3])"}
{"prompt": "def pairwise_reproducibility(df, plot=False):\n    \"\"\"\n    Calculate the reproducibility of LA-ICPMS based on unique pairs of repeat analyses.\n    \n    Pairwise differences are fit with a half-Cauchy distribution, and the median and \n    95% confidence limits are returned for each analyte.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataset\n    \n    plot : bool\n        Whether or not to plot the resulting error distributions.\n    \n    Returns\n    -------\n    pdiffs : pandas.DataFrame\n        Unique pairwise differences for all analytes.\n    rep_dists : dict of scipy.stats.halfcauchy\n        Half-Cauchy distribution objects fitted to the\n        differences.\n    rep_stats : dict of tuples\n        The 50% and 95% quantiles of the half-cauchy\n        distribution.\n    (fig, axs) : matplotlib objects\n        The figure. If not made, returnes (None, None) placeholder\n    \n    \"\"\"", "output": "pandas.DataFrame\n        A table of pairwise Reproducibility scores for each analyte.\n    \"\"\"\n    # Calculate pairwise differences\n    pairs = list(combinations(df.columns, 2))\n    differences = []\n    for pair in pairs:\n        analyte1, analyte2 = pair\n        diff = abs(df[analyte1] - df[analyte2])\n        differences.append(diff)\n    \n    # Fit half-Cauchy distribution to pairwise differences\n    from scipy.stats import half", "solution": "\n    \n    ans = df.columns.values\n    pdifs = []\n    \n    # calculate differences between unique pairs\n    for ind, d in df.groupby(level=0):\n        d.index = d.index.droplevel(0)\n\n        difs = []\n        for i, r in d.iterrows():\n            t = d.loc[i+1:, :]\n            difs.append(t[ans] - r[ans])\n\n        pdifs.append(pd.concat(difs))\n    pdifs = pd.concat(pdifs).abs()\n\n    # calculate stats\n    rep_stats = {}\n    rep_dists = {}\n    errfn = stats.halfcauchy\n    \n    for a in ans:\n        d = pdifs.loc[:, a].dropna().values\n        hdist = errfn.fit(d, floc=0)\n        rep_dists[a] = errfn(*hdist)\n        rep_stats[a] = rep_dists[a].ppf((0.5, 0.95))\n    \n    # make plot\n    if not plot:\n        return pdifs, rep_dists, rep_stats, (None, None)\n    \n    fig, axs = plt.subplots(1, len(ans), figsize=[len(ans) * 2, 2])\n    for a, ax in zip(ans, axs):\n        d = pdifs.loc[:, a].dropna().values\n        hist, edges, _ = ax.hist(d, 30)\n        ax.plot(edges, rep_dists[a].pdf(edges) * (sum(hist) * np.mean(np.diff(edges))))\n        ax.set_title(a, loc='left')\n\n    return pdifs, rep_dists, rep_stats, (fig, axs)"}
{"prompt": "def remove_redundant_nodes(self, preserve_lengths=True):\n        \"\"\"\n        Remove all nodes which have only a single child, and attach their\n        grandchildren to their parent.  The resulting tree has the minimum\n        number of internal nodes required for the number of leaves.\n        :param preserve_lengths: If true, branch lengths of removed nodes are \\\n        added to those of their children.\n        \"\"\"", "output": "\n        # Create a dictionary to store the children of each node\n        children = {}\n        for node in self.nodes:\n            children[node] = []\n        for edge in self.edges:\n            children[edge[0]].append(edge[1])\n            children[edge[1]].append(edge[0])\n\n        # Remove redundant nodes\n        to_remove = []\n        for node in children:\n            if len(children[node]) == 1:\n                to_remove.append(node)\n\n        # Remove redundant nodes and attach children to", "solution": "\n        for n in self.walk(mode='postorder'):\n            while n.ancestor and len(n.ancestor.descendants) == 1:\n                grandfather = n.ancestor.ancestor\n                father = n.ancestor\n                if preserve_lengths:\n                    n.length += father.length\n\n                if grandfather:\n                    for i, child in enumerate(grandfather.descendants):\n                        if child is father:\n                            del grandfather.descendants[i]\n                    grandfather.add_descendant(n)\n                    father.ancestor = None\n                else:\n                    self.descendants = n.descendants\n                    if preserve_lengths:\n                        self.length = n.length"}
{"prompt": "def _set_start(self):\n        \"\"\" Sets the start timestamp and formatted time to the current time. \"\"\"", "output": "\n        self.start = time.time()\n        self.start_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\n    def _set_end(self):\n        \"\"\" Sets the end timestamp and formatted time to the current time. \"\"\"\n        self.end = time.time()\n        self.end_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\n    def _get_start(self):\n        ", "solution": "\n        init_time = time.time()\n        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        run_info_dict = self._run_information[self.v_crun]\n        run_info_dict['timestamp'] = init_time\n        run_info_dict['time'] = formatted_time\n        if self._environment_hexsha is not None:\n            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]"}
{"prompt": "def return_type(type_name, formatter=None):\n    \"\"\"Specify that this function returns a typed value.\n\n    Args:\n        type_name (str): A type name known to the global typedargs type system\n        formatter (str): An optional name of a formatting function specified\n            for the type given in type_name.\n    \"\"\"", "output": "\n    if formatter:\n        return f\"typedargs.{type_name}.{formatter}\"\n    else:\n        return f\"typedargs.{type_name}\"\n\n", "solution": "\n\n    def _returns(func):\n        annotated(func)\n        func.metadata.typed_returnvalue(type_name, formatter)\n        return func\n\n    return _returns"}
{"prompt": "def load_metamodel(resource=None, load_globals=True):\n    \"\"\"\n    Load and return a metamodel expressed in ooaofooa from a *resource*.\n    The resource may be either a filename, a path, or a list of filenames\n    and/or paths.\n    \"\"\"", "output": "\n    if load_globals:\n        globals().update(load_metamodel.__globals__)\n    if resource is None:\n        return metamodel\n    elif isinstance(resource, str):\n        with open(resource, 'r') as f:\n            return metamodel.parse(f.read())\n    elif isinstance(resource, list):\n        return metamodel.parse('\\n'.join(resource))\n    else:\n        raise TypeError('resource must be a filename, path,", "solution": "\n    loader = _mk_loader(resource, load_globals)\n    return loader.build_metamodel()"}
{"prompt": "def set_s3_credentials(secret_key_id, secret_access_key):\n    \"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"", "output": "\n    import boto3\n    \n    # Create a new S3 client with the specified credentials\n    s3_client = boto3.client('s3', aws_access_key_id=secret_access_key_id, aws_secret_access_key=secret_key_id)\n    \n    return s3_client\n\n", "solution": "\n    if(secret_key_id is None):\n        raise H2OValueError(\"Secret key ID must be specified\")\n\n    if(secret_access_key is None):\n        raise H2OValueError(\"Secret access key must be specified\")\n    \n    if(not secret_key_id):\n        raise H2OValueError(\"Secret key ID must not be empty\")\n    \n    if(not secret_access_key):\n        raise H2OValueError(\"Secret access key must not be empty\")\n    \n    \n    params = {\"secret_key_id\": secret_key_id,\n              \"secret_access_key\": secret_access_key\n              }\n    \n    h2o.api(endpoint=\"POST /3/PersistS3\", data=params)\n    print(\"Credentials successfully set.\")"}
{"prompt": "def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"", "output": "\n        # Tar the local file or directory\n        tar = tarfile.open(path, 'r')\n        tar.add(path)\n        tar.close()\n\n        # Upload to s3\n        s3 = boto3.client('s3', region_name='us-west-2')\n        s3.upload_file(path, bucket, key)\n\n    def get_file_hash(self, path, algorithm):\n        \"\"\"\n        Calculate the hash value of a file\n\n        :param path: file path\n        :type path: str\n", "solution": "\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)"}
{"prompt": "def _make_default_operation_costs(self, allow_spaces=False):\n        \"\"\"\n        sets 1.0 cost for every replacement, insertion, deletion and transposition\n        \"\"\"", "output": "\n        cost = 1.0\n        if allow_spaces:\n            cost *= len(self.s)\n        return cost\n\n    def _make_operation_costs(self, allow_spaces=False):\n        \"\"\"\n        sets 1.0 cost for every operation\n        \"\"\"\n        cost = 1.0\n        if allow_spaces:\n            cost *= len(self.s) - 1\n        return cost\n\n    def _make_costs(self, allow_spaces=False):\n        \"\"\"\n        returns a list of costs for", "solution": "\n        self.operation_costs = dict()\n        self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[\"\"] = 1.0\n            if allow_spaces:\n                current_costs[\" \"] = 1.0\n            self.operation_costs[a] = current_costs\n        # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0437\u0438\u0446\u0438\u0438\n        for a, b in itertools.permutations(self.alphabet, 2):\n            self.operation_costs[a + b] = {b + a: 1.0}\n        # \u043f\u0440\u043e\u0431\u0435\u043b\u044b\n        if allow_spaces:\n            self.operation_costs[\" \"] = {c: 1.0 for c in self.alphabet}\n            self.operation_costs[\" \"][\"\"] = 1.0"}
{"prompt": "def _dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"", "output": "\n    return os.path.exists(os.path.join(db_dirname, user_id, 'dir'))\n\n", "solution": "\n    return db.execute(\n        select(\n            [func.count(directories.c.name)],\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname,\n            ),\n        )\n    ).scalar() != 0"}
{"prompt": "def create(self, name, description=None, color=None):\n        \"\"\"\n        Creates a new label and returns the response\n\n        :param name: The label name\n        :type name: str\n\n        :param description: An optional description for the label. The name is\n            used if no description is provided.\n        :type description: str\n\n        :param color: The hex color for the label (ex: 'ff0000' for red). If no\n            color is provided, a random one will be assigned.\n        :type color: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": " label object\n        :rtype: Label\n        \"\"\"\n        if description is None:\n            description = name\n\n        if color is None:\n            color = random_color()\n\n        label = Label(name, description, color)\n        self.labels.append(label)\n        return label\n\n    def get_labels(self):\n        \"\"\"\n        Returns a list of all the labels in the form of Label objects.\n\n        :returns: A list of Label objects\n        :rtype: List[Label]\n        \"\"\"\n        return self.labels\n\n    ", "solution": "\n        data = {\n            'name': name,\n            'title': name,\n            'description': description or name,\n            'appearance': {\n                'color': color or random_color()\n            }\n        }\n        # Yes, it's confusing. the `/tags/` endpoint is used for labels\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.TAGS.value,\n            params=data\n        )"}
{"prompt": "def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"Add someone to a team by Person ID or email address.\n\n        Add someone to a team by Person ID or email address; optionally making\n        them a moderator.\n\n        Args:\n            teamId(basestring): The team ID.\n            personId(basestring): The person ID.\n            personEmail(basestring): The email address of the person.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the details of the\n            created team membership.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": " if the person is a moderator.\n            **request_parameters: Additional parameters to include in the API request.\n\n        Returns:\n            dict: A dictionary representing the newly created person.\n\n        Raises:\n            ValueError: If the person ID or email address is invalid.\n        \"\"\"\n        if not personId and not personEmail:\n            raise ValueError(\"Person ID or email address is required.\")\n\n        if personId:\n            request_parameters[\"personId\"] = personId\n\n        if personEmail:\n            request_parameters[\"personEmail\"] = personEmail\n\n        ", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def visit(self, visit_uri):\n        \"\"\"\n        Navigate to the given URL. The URL can either be a relative URL or an absolute URL. The\n        behavior of either depends on the driver. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"http://google.com\")\n\n        For drivers which can run against an external application, such as the Selenium driver,\n        giving an absolute URL will navigate to that page. This allows testing applications running\n        on remote servers. For these drivers, setting :data:`capybara.app_host` will make the\n        remote server the default. For example::\n\n            capybara.app_host = \"http://google.com\"\n            session.visit(\"/\")  # visits the Google homepage\n\n        Args:\n            visit_uri (str): The URL to navigate to.\n        \"\"\"", "output": " will set the\n        capybara application hostname.\n\n        :param visit_uri: The URL to visit.\n        :return: The result of visiting the URL.\n        \"\"\"\n        if self.driver_name == \"selenium\":\n            from selenium import webdriver\n\n            options = webdriver.ChromeOptions()\n            options.add_argument(\"--headless\")\n            driver = webdriver.Chrome(options=options)\n            driver.get(visit_uri)\n            return driver.page_source\n        else:\n            # Assume it", "solution": "\n\n        self.raise_server_error()\n\n        visit_uri = urlparse(visit_uri)\n\n        if capybara.app_host:\n            uri_base = urlparse(capybara.app_host)\n        elif self.server:\n            uri_base = urlparse(\"http://{}:{}\".format(self.server.host, self.server.port))\n        else:\n            uri_base = None\n\n        visit_uri = ParseResult(\n            scheme=visit_uri.scheme or (uri_base.scheme if uri_base else \"\"),\n            netloc=visit_uri.netloc or (uri_base.netloc if uri_base else \"\"),\n            path=visit_uri.path,\n            params=visit_uri.params,\n            query=visit_uri.query,\n            fragment=visit_uri.fragment)\n\n        self.driver.visit(visit_uri.geturl())"}
{"prompt": "def from_join(cls, join: Join) -> 'ConditionalJoin':\n        \"\"\"Creates a new :see:ConditionalJoin from the\n        specified :see:Join object.\n\n        Arguments:\n            join:\n                The :see:Join object to create the\n                :see:ConditionalJoin object from.\n\n        Returns:\n            A :see:ConditionalJoin object created from\n            the :see:Join object.\n        \"\"\"", "output": "\n        return cls(join.left, join.right, join.condition)\n\n    def __init__(self, left: Join, right: Join, condition: str):\n        self.left = left\n        self.right = right\n        self.condition = condition\n\n    def __repr__(self) -> str:\n        return f\"<ConditionalJoin {self.left} {self.condition} {self.right}>\"\n\n    def __str__(self) -> str:\n        return f\"({self.left} {self.condition} {self.", "solution": "\n\n        return cls(\n            join.table_name,\n            join.parent_alias,\n            join.table_alias,\n            join.join_type,\n            join.join_field,\n            join.nullable\n        )"}
{"prompt": "def process_tokens(self, tokens):\n        \"\"\"process tokens from the current module to search for module/block\n        level options\n        \"\"\"", "output": "\n        for token in tokens:\n            if token.startswith(\"module:\"):\n                module_name = token.split(\":\")[1]\n                if module_name not in self.modules:\n                    self.modules[module_name] = []\n            elif token.startswith(\"block:\"):\n                block_name = token.split(\":\")[1]\n                if block_name not in self.blocks:\n                    self.blocks[block_name] = []\n            elif token.startswith(\"options:\"):\n                option_name = token", "solution": "\n        control_pragmas = {\"disable\", \"enable\"}\n        for (tok_type, content, start, _, _) in tokens:\n            if tok_type != tokenize.COMMENT:\n                continue\n            match = OPTION_RGX.search(content)\n            if match is None:\n                continue\n\n            first_group = match.group(1)\n            if (\n                first_group.strip() == \"disable-all\"\n                or first_group.strip() == \"skip-file\"\n            ):\n                if first_group.strip() == \"disable-all\":\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(\"disable-all\", \"skip-file\"),\n                    )\n                self.add_message(\"file-ignored\", line=start[0])\n                self._ignore_file = True\n                return\n            try:\n                opt, value = first_group.split(\"=\", 1)\n            except ValueError:\n                self.add_message(\n                    \"bad-inline-option\", args=first_group.strip(), line=start[0]\n                )\n                continue\n            opt = opt.strip()\n            if opt in self._options_methods or opt in self._bw_options_methods:\n                try:\n                    meth = self._options_methods[opt]\n                except KeyError:\n                    meth = self._bw_options_methods[opt]\n                    # found a \"(dis|en)able-msg\" pragma deprecated suppression\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(opt, opt.replace(\"-msg\", \"\")),\n                    )\n                for msgid in utils._splitstrip(value):\n                    # Add the line where a control pragma was encountered.\n                    if opt in control_pragmas:\n                        self._pragma_lineno[msgid] = start[0]\n\n                    try:\n                        if (opt, msgid) == (\"disable\", \"all\"):\n                            self.add_message(\n                                \"deprecated-pragma\",\n                                line=start[0],\n                                args=(\"disable=all\", \"skip-file\"),\n                            )\n                            self.add_message(\"file-ignored\", line=start[0])\n                            self._ignore_file = True\n                            return\n                        meth(msgid, \"module\", start[0])\n                    except exceptions.UnknownMessageError:\n                        self.add_message(\"bad-option-value\", args=msgid, line=start[0])\n            else:\n                self.add_message(\"unrecognized-inline-option\", args=opt, line=start[0])"}
{"prompt": "def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"", "output": "\n        if name not in self.flags:\n            self.flags[name] = old\n        elif self.flags[name]!= old:\n            raise ValueError(f\"flag '{name}' is not valid: {old}!= {self.flags[name]}\")\n        if new not in self.flags:\n            self.flags[new] = new\n        elif self.flags[new]!= new:\n            raise ValueError(f\"flag '{new}' is not valid: {new}!= {self.flags[new]}\")\n\n    ", "solution": "\n        for key,value in new.iteritems():\n            assert len(value) == 2, \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[0], (dict, Config)), \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[1], basestring), \"Bad flag: %r:%s\"%(key,value)"}
{"prompt": "def images(self, query=None):\n    \"\"\"List local images in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container or collection name|tag|uri\n\n    \"\"\"", "output": "\n    images = []\n    for item in self.db:\n        if query is None or query in item['name'] or query in item['tag'] or query in item['uri']:\n            images.append(item)\n    return images\n\n", "solution": "\n    from sregistry.database.models import Collection, Container\n\n    rows = []\n    if query is not None:   \n        like = \"%\" + query + \"%\"\n        containers = Container.query.filter(or_(Container.name == query,\n                                                Container.tag.like(like),\n                                                Container.uri.like(like),\n                                                Container.name.like(like))).all() \n    else:\n        containers = Container.query.all()\n\n    if len(containers) > 0:\n        message = \"  [date]   [client]\\t[uri]\"\n        bot.custom(prefix='Containers:', message=message, color=\"RED\")\n        for c in containers:\n            uri = c.get_uri()\n            created_at = c.created_at.strftime('%B %d, %Y')\n            rows.append([created_at, \"   [%s]\" %c.client, uri])\n        bot.table(rows) \n    return containers"}
{"prompt": "def load_tiff_multipage(tiff_filename, dtype='float32'):\n    \"\"\"\n    Load a multipage tiff into a single variable in x,y,z format.\n\n    Arguments:\n        tiff_filename:     Filename of source data\n        dtype:             data type to use for the returned tensor\n\n    Returns:\n        Array containing contents from input tiff file in xyz order\n    \"\"\"", "output": "\n    with TiffFile(tiff_filename) as tiff:\n        # Get dimensions of source data\n        x_dim, y_dim, z_dim = tiff.series[0].shape\n\n        # Create empty array to hold data\n        data = np.zeros((x_dim, y_dim, z_dim), dtype=dtype)\n\n        # Loop through each page of data\n        for page in tiff.pages:\n            # Get data from page\n            page_data = page.asarray()\n\n            # Add page data to array\n            ", "solution": "\n    if not os.path.isfile(tiff_filename):\n        raise RuntimeError('could not find file \"%s\"' % tiff_filename)\n\n    # load the data from multi-layer TIF files\n    data = tiff.imread(tiff_filename)\n\n    im = []\n\n    while True:\n\n        Xi = numpy.array(data, dtype=dtype)\n        if Xi.ndim == 2:\n            Xi = Xi[numpy.newaxis, ...]  # add slice dimension\n        im.append(Xi)\n\n        try:\n            data.seek(data.tell()+1)\n        except EOFError:\n            break  # this just means hit end of file (not really an error)\n\n    im = numpy.concatenate(im, axis=0)  # list of 2d -> tensor\n    im = numpy.rollaxis(im, 1)\n    im = numpy.rollaxis(im, 2)\n\n    return im"}
{"prompt": "def with_setup(setup=None, teardown=None):\n    \"\"\"Decorator to add setup and/or teardown methods to a test function::\n\n      @with_setup(setup, teardown)\n      def test_something():\n          \" ... \"\n\n    Note that `with_setup` is useful *only* for test functions, not for test\n    methods or inside of TestCase subclasses.\n    \"\"\"", "output": "\n    def decorator(func):\n        func.setup = setup\n        func.teardown = teardown\n        return func\n    return decorator\n\n", "solution": "\n    def decorate(func, setup=setup, teardown=teardown):\n        if setup:\n            if hasattr(func, 'setup'):\n                _old_s = func.setup\n                def _s():\n                    setup()\n                    _old_s()\n                func.setup = _s\n            else:\n                func.setup = setup\n        if teardown:\n            if hasattr(func, 'teardown'):\n                _old_t = func.teardown\n                def _t():\n                    _old_t()\n                    teardown()\n                func.teardown = _t\n            else:\n                func.teardown = teardown\n        return func\n    return decorate"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idx : np.array(N)\n            Estimated indeces for the segment boundaries in frames.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        est_idx = []\n        est_labels = []\n        for i in range(len(self.frames)):\n            segments = self.frames[i]\n            if len(segments) == 0:\n                est_idx.append([])\n                est_labels.append([])\n            else:\n                est_idx_i, est_labels_i = estimate_segments(segments)\n                est_idx.append(est_idx_i)\n                est_labels.append(est_lab", "solution": "\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"label_norm_feats\"],\n                        floor=self.config[\"label_norm_floor\"],\n                        min_db=self.config[\"label_norm_min_db\"])\n\n        # Find the labels using 2D-FMCs\n        est_labels = compute_similarity(F, self.in_bound_idxs,\n                                        dirichlet=self.config[\"dirichlet\"],\n                                        xmeans=self.config[\"xmeans\"],\n                                        k=self.config[\"k\"],\n                                        offset=self.config[\"2dfmc_offset\"])\n\n        # Post process estimations\n        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,\n                                                           est_labels)\n\n        return self.in_bound_idxs, est_labels"}
{"prompt": "def find_handfile(names=None):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u660e\u786e\u6307\u5b9a\u6216\u9010\u7ea7\u641c\u7d22\u7236\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7ec8\u7aef\u4f20\u5165\u7684\u914d\u7f6e\n    :return: ``handfile`` \u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a None\n    :rtype: str\n    \"\"\"", "output": "\u8def\u5f84\n    :return: \u8fd4\u56de\u4e00\u4e2a\u5305\u540d\u79f0\uff0c\u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5219\u8fd4\u56de None\n    \"\"\"\n    import os\n\n    if names is None:\n        names = os.getcwd()\n\n    for name in os.listdir(names):\n        if os.path.isfile(os.path.join(names, name)):\n            return name\n\n    return None\n\n", "solution": "\n    # \u5982\u679c\u6ca1\u6709\u660e\u786e\u6307\u5b9a\uff0c\u5219\u5305\u542b env \u4e2d\u7684\u503c\n    names = names or [env.handfile]\n\n    # \u82e5\u65e0 ``.py`` \u6269\u5c55\u540d\uff0c\u5219\u4f5c\u4e3a\u5f85\u67e5\u8be2\u540d\u79f0\uff0c\u8ffd\u52a0\u5230 names \u672b\u5c3e\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name \u4e2d\u662f\u5426\u5305\u542b\u8def\u5f84\u5143\u7d20\n    if os.path.dirname(names[0]):\n        # \u82e5\u5b58\u5728\uff0c\u5219\u6269\u5c55 Home \u8def\u5f84\u6807\u5fd7\uff0c\u5e76\u6d4b\u8bd5\u662f\u5426\u5b58\u5728\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    return os.path.abspath(expanded)\n    else:\n        # \u5426\u5219\uff0c\u9010\u7ea7\u5411\u4e0a\u641c\u7d22\uff0c\u76f4\u5230\u6839\u8def\u5f84\n        path = '.'\n\n        # \u5728\u5230\u7cfb\u7edf\u6839\u8def\u5f84\u4e4b\u524d\u505c\u6b62\n        while os.path.split(os.path.abspath(path))[1]:\n            for name in names:\n                joined = os.path.join(path, name)\n                if os.path.exists(joined):\n                    if name.endswith('.py') or _is_package(joined):\n                        return os.path.abspath(joined)\n            path = os.path.join('..', path)\n\n    return None"}
{"prompt": "def remote_upload(self, remote_url, folder_id=None, headers=None):\n        \"\"\"Used to make a remote file upload to openload.co\n\n        Note:\n            If folder_id is not provided, the file will be uploaded to ``Home`` folder.\n\n        Args:\n            remote_url (str): direct link of file to be remotely downloaded.\n            folder_id (:obj:`str`, optional): folder-ID to upload to.\n            headers (:obj:`dict`, optional): additional HTTP headers (e.g. Cookies or HTTP Basic-Auth)\n\n        Returns:\n            dict: dictionary containing (\"id\": uploaded file id, \"folderid\"). ::\n\n                {\n                    \"id\": \"12\",\n                    \"folderid\": \"4248\"\n                }\n\n        \"\"\"", "output": "Auth).\n\n        Returns:\n            :obj:`dict`: A dictionary containing the response from the remote server.\n\n        Raises:\n            :obj:`urllib.error.HTTPError`: If the server returns an HTTP error code.\n            :obj:`urllib.error.URLError`: If there is an error connecting to the server.\n        \"\"\"\n        if headers is None:\n            headers = {}\n\n        if folder_id is None:\n            folder_id = \"Home\"\n\n        if folder_id not in self.folders", "solution": "\n\n        kwargs = {'folder': folder_id, 'headers': headers}\n        params = {'url': remote_url}\n        params.update({key: value for key, value in kwargs.items() if value})\n\n        return self._get('remotedl/add', params=params)"}
{"prompt": "def list(self, **request_parameters):\n        \"\"\"List all roles.\n\n        Args:\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            GeneratorContainer: A GeneratorContainer which, when iterated,\n            yields the roles returned by the Webex Teams query.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        if not request_parameters:\n            return GeneratorContainer(roles=self.roles)\n\n        try:\n            endpoint = request_parameters.pop('endpoint')\n        except KeyError:\n            raise TypeError('Parameter \"endpoint\" is required')\n\n        try:\n            max_results = request_parameters.pop('max_results')\n        except KeyError:\n            max_results = None\n\n        if not isinstance(endpoint, str):\n            raise TypeError('Parameter \"endpoint\" must be a string')\n\n        if not", "solution": "\n        # API request - get items\n        items = self._session.get_items(\n            API_ENDPOINT,\n            params=request_parameters\n        )\n\n        # Yield role objects created from the returned JSON objects\n        for item in items:\n            yield self._object_factory(OBJECT_TYPE, item)"}
{"prompt": "def delete_all_checkpoints(self, path):\n        \"\"\"Delete all checkpoints for the given path.\"\"\"", "output": "\n        checkpoints = [checkpoint for checkpoint in self.checkpoints if checkpoint.path == path]\n        for checkpoint in checkpoints:\n            self.checkpoints.remove(checkpoint)\n\n", "solution": "\n        with self.engine.begin() as db:\n            delete_remote_checkpoints(db, self.user_id, path)"}
{"prompt": "def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"", "output": "\n        self.queue.append((trigger, proc))\n\n    def _runHdlProc(self) -> None:\n        \"\"\"\n        Runs all HDL processes in the queue\n        \"\"\"\n        while self.queue:\n            trigger, proc = self.queue.pop(0)\n            try:\n                for line in proc():\n                    yield line\n            except StopIteration:\n                pass\n\n    def runHdlProc(self) -> None:\n        \"\"\"\n        Runs all HDL processes in the queue\n        \"\"\"\n        for line in self._run", "solution": "\n        # first process in time has to plan executing of apply values on the\n        # end of this time\n        if not self._applyValPlaned:\n            # (apply on end of this time to minimalize process reevaluation)\n            self._scheduleApplyValues()\n\n        if isEvDependentOn(trigger, proc):\n            if self.now == 0:\n                return  # pass event dependent on startup\n            self._seqProcsToRun.append(proc)\n        else:\n            self._combProcsToRun.append(proc)"}
{"prompt": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"", "output": "        \"\"\"\n        if date is None:\n            date = datetime.date.today()\n\n        # Get the conversion rate for the given currency and date\n        if currency == 'USD':\n            rate = 1.0\n        elif currency == 'EUR':\n            rate = 0.85\n        elif currency == 'GBP':\n            rate = 0.75\n        else:\n            raise ValueError('Invalid currency')\n\n        # Convert the amount\n        value = amount * rate\n\n        # Convert the value to the new currency\n        if currency == 'USD':\n            ", "solution": "\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1"}
{"prompt": "def user_config_file(self):\n        \"\"\"Get the absolute path to the user config file.\"\"\"", "output": "\n        return os.path.abspath(os.path.expanduser(\"~/.config/myapp/myconfig.txt\"))\n\n    def set_config_file(self, config_file):\n        \"\"\"Set the absolute path to the user config file.\"\"\"\n        os.environ[\"MYAPP_CONFIG_FILE\"] = config_file\n\n    config_file = property(get_config_file, set_config_file)\n\n", "solution": "\n        return os.path.join(\n            get_user_config_dir(self.app_name, self.app_author),\n            self.filename)"}
{"prompt": "def build_url_field(self, field_name, model_class):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"", "output": "\n        return field_name\n\n    def get_queryset(self):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"\n        return self.queryset\n\n    def get_url(self):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"", "solution": "\n        field, kwargs = super().build_url_field(field_name, model_class)\n\n        view = self.root.context[\"view\"]\n        kwargs[\"view_name\"] = view.get_url_name(\"detail\")\n\n        return field, kwargs"}
{"prompt": "def list_templates(self, extensions=None, filter_func=None):\n        \"\"\"Returns a list of templates for this environment.  This requires\n        that the loader supports the loader's\n        :meth:`~BaseLoader.list_templates` method.\n\n        If there are other files in the template folder besides the\n        actual templates, the returned list can be filtered.  There are two\n        ways: either `extensions` is set to a list of file extensions for\n        templates, or a `filter_func` can be provided which is a callable that\n        is passed a template name and should return `True` if it should end up\n        in the result list.\n\n        If the loader does not support that, a :exc:`TypeError` is raised.\n\n        .. versionadded:: 2.4\n        \"\"\"", "output": "\n        takes in a filename and returns a boolean indicating whether or not\n        that file should be included in the list.\n\n        :param extensions: A list of file extensions for templates.\n        :param filter_func: A callable that takes in a filename and returns a\n            boolean indicating whether or not that file should be included in\n            the list.\n        :return: A list of filenames for templates.\n        \"\"\"\n        if extensions is not None:\n            return [f for f in os.listdir(self.template_dir) if f.endswith", "solution": "\n        x = self.loader.list_templates()\n        if extensions is not None:\n            if filter_func is not None:\n                raise TypeError('either extensions or filter_func '\n                                'can be passed, but not both')\n            filter_func = lambda x: '.' in x and \\\n                                    x.rsplit('.', 1)[1] in extensions\n        if filter_func is not None:\n            x = ifilter(filter_func, x)\n        return x"}
{"prompt": "def find_debugged_frame(frame):\n    \"\"\"Find the first frame that is a debugged frame. We do this\n    Generally we want traceback information without polluting it with\n    debugger frames. We can tell these because those are frames on the\n    top which don't have f_trace set. So we'll look back from the top\n    to find the fist frame where f_trace is set.\n    \"\"\"", "output": "\n    while frame:\n        if frame.f_code.co_filename.startswith(\"<\"):\n            return frame\n        frame = frame.f_back\n    return None\n\n", "solution": "\n    f_prev = f = frame\n    while f is not None and f.f_trace is None:\n        f_prev = f\n        f = f.f_back\n        pass\n    if f_prev:\n        val = f_prev.f_locals.get('tracer_func_frame')\n        if val == f_prev:\n            if f_prev.f_back:\n                f_prev = f_prev.f_back\n                pass\n            pass\n        pass\n    else:\n        return frame\n    return f_prev"}
{"prompt": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"", "output": " global scope.\n\n  Returns:\n    A dictionary with the caller's function arguments.\n  \"\"\"\n  frame = inspect.currentframe().f_back\n  arguments = {}\n  for arg_name, arg_value in frame.f_locals.items():\n    if arg_name.startswith('parent_'):\n      arguments[arg_name[7:]] = arg_value\n  return arguments\n\n", "solution": "\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args"}
{"prompt": "def _process_bit_id(self, node):\n        \"\"\"Process an Id or IndexedId node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"", "output": "\n        if isinstance(node, Id):\n            return [(node.name, node.index)]\n        elif isinstance(node, IndexedId):\n            return [(node.name, i) for i in range(node.start, node.end+1)]\n        else:\n            raise ValueError(\"Invalid node type: {}\".format(type(node)))\n\n    def _process_bit_range(self, node):\n        \"\"\"Process a BitRange or BitRangeIndexed node as a range or register type.\n\n        Return a list of tuples (Register,start", "solution": "\n        # pylint: disable=inconsistent-return-statements\n        reg = None\n        if node.name in self.dag.qregs:\n            reg = self.dag.qregs[node.name]\n        elif node.name in self.dag.cregs:\n            reg = self.dag.cregs[node.name]\n        else:\n            raise QiskitError(\"expected qreg or creg name:\",\n                              \"line=%s\" % node.line,\n                              \"file=%s\" % node.file)\n\n        if node.type == \"indexed_id\":\n            # An indexed bit or qubit\n            return [(reg, node.index)]\n        elif node.type == \"id\":\n            # A qubit or qreg or creg\n            if not self.bit_stack[-1]:\n                # Global scope\n                return [(reg, j) for j in range(reg.size)]\n            else:\n                # local scope\n                if node.name in self.bit_stack[-1]:\n                    return [self.bit_stack[-1][node.name]]\n                raise QiskitError(\"expected local bit name:\",\n                                  \"line=%s\" % node.line,\n                                  \"file=%s\" % node.file)\n        return None"}
{"prompt": "def _backup(self):\n        \"\"\"\n        Save the current database into the inactive-db.json file.\n        \"\"\"", "output": "\n        with open('inactive-db.json', 'w') as f:\n            json.dump(self.db, f)\n\n    def _restore(self):\n        \"\"\"\n        Restore the current database from the inactive-db.json file.\n        \"\"\"\n        with open('inactive-db.json', 'r') as f:\n            self.db = json.load(f)\n\n    def __getitem__(self, key):\n        \"\"\"\n        Return the value associated with the given key in the database.\n\n        Args:\n        - key", "solution": "\n\n        if PyFunceble.CONFIGURATION[\"inactive_database\"]:\n            # The database subsystem is activated.\n\n            # We save the current database state into the database file.\n            Dict(PyFunceble.INTERN[\"inactive_db\"]).to_json(self.inactive_db_path)"}
{"prompt": "def get_pandas_df(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"", "output": ";\"\n        >>> df = hh.get_pandas_df(sql)\n        \"\"\"\n        if isinstance(hql, str):\n            hql = [hql]\n        if schema == 'default':\n            schema = self.schema\n        if schema == 'public':\n            schema = self.public_schema\n        if schema == 'testing':\n            schema = self.testing_schema\n        if schema == 'production':\n            schema = self.production_schema\n        if schema =='staging':\n            sche", "solution": "\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df"}
{"prompt": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"", "output": "\n        for item in self.items:\n            if item.identifier == identifier:\n                return item.abspath\n        return None\n\n    def get_item_content(self, identifier):\n        \"\"\"Return content of item with given identifier.\n\n        :param identifier: item identifier\n        :returns: content of item with given identifier\n        \"\"\"\n        for item in self.items:\n            if item.identifier == identifier:\n                return item.content\n        return None\n\n    def get_item_size(self, identifier):\n        \"\"\"Return size of item with", "solution": "\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath"}
{"prompt": "def record(self, func):\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"", "output": "\n        self.functions.append(func)\n\n    def unregister(self, func):\n        \"\"\"Removes the function with the given name from the list of\n        registered functions.\n        \"\"\"\n        if func in self.functions:\n            self.functions.remove(func)\n\n    def make_setup_state(self, state):\n        \"\"\"Called when the blueprint is registered on the application.\n        This function is called with the state as argument as returned by\n        the :meth:`make_setup` method.\n        \"\"\"\n        pass\n\n    ", "solution": "\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n            warn(Warning('The blueprint was already registered once '\n                         'but is getting modified now.  These changes '\n                         'will not show up.'))\n        self.deferred_functions.append(func)"}
{"prompt": "def find_sourcemap_comment(filepath, block_size=100):\n    \"\"\"\n    Seeks and removes the sourcemap comment. If found, the sourcemap line is\n    returned.\n\n    Bundled output files can have massive amounts of lines, and the sourceMap\n    comment is always at the end. So, to extract it efficiently, we read out the\n    lines of the file starting from the end. We look back at most 2 lines.\n\n    :param:filepath: path to output bundle file containing the sourcemap comment\n    :param:blocksize: integer saying how many bytes to read at once\n    :return:string with the sourcemap comment or None\n    \"\"\"", "output": " representing the number of lines to read at a time\n    :return: sourcemap comment as a string, or None if no sourcemap comment is found\n    \"\"\"\n    with open(filepath, 'r') as f:\n        lines = f.readlines()\n        f.seek(0, 2)\n        end_of_file = f.tell()\n        f.seek(end_of_file - blocksize, 0)\n        sourcemap_comment = None\n        for i in range(end_of_file - blocksize, -1, -blocksize):\n", "solution": "\n\n    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end\n\n    block_number = -1\n    # blocks of size block_size, in reverse order starting from the end of the file\n    blocks = []\n    sourcemap = None\n\n    try:\n        # open file in binary read+write mode, so we can seek with negative offsets\n        of = io.open(filepath, 'br+')\n        # figure out what's the end byte\n        of.seek(0, os.SEEK_END)\n        block_end_byte = of.tell()\n\n        # track back for maximum MAX_TRACKBACK lines and while we can track back\n        while block_end_byte > 0 and MAX_TRACKBACK > 0:\n            if (block_end_byte - block_size > 0):\n                # read the last block we haven't yet read\n                of.seek(block_number*block_size, os.SEEK_END)\n                blocks.append(of.read(block_size))\n            else:\n                # file too small, start from begining\n                of.seek(0, os.SEEK_SET)\n                # only read what was not read\n                blocks = [of.read(block_end_byte)]\n\n            # update variables that control while loop\n            content = b''.join(reversed(blocks))\n            lines_found = content.count(b'\\n')\n            MAX_TRACKBACK -= lines_found\n            block_end_byte -= block_size\n            block_number -= 1\n\n            # early check and bail out if we found the sourcemap comment\n            if SOURCEMAPPING_URL_COMMENT in content:\n                offset = 0\n                # splitlines eats the last \\n if its followed by a blank line\n                lines = content.split(b'\\n')\n                for i, line in enumerate(lines):\n                    if line.startswith(SOURCEMAPPING_URL_COMMENT):\n                        offset = len(line)\n                        sourcemap = line\n                        break\n                while i+1 < len(lines):\n                    offset += 1  # for the newline char\n                    offset += len(lines[i+1])\n                    i += 1\n                # track back until the start of the comment, and truncate the comment\n                if sourcemap:\n                    offset += 1  # for the newline before the sourcemap comment\n                    of.seek(-offset, os.SEEK_END)\n                    of.truncate()\n                return force_text(sourcemap)\n    finally:\n        of.close()\n    return sourcemap"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        if default:\n            print(prompt)\n        return input().lower() in ['y', 'yes', 'ok', 'on']\n\n", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def delete(self, tag_id):\n        \"\"\"\n        Delete the specified InactivityAlert\n\n        :param tag_id: The tag ID to delete\n        :type tag_id: str\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        try:\n            self.alert.delete(tag_id)\n        except logentries_api.exceptions.ServerException as e:\n            raise ServerException(e)\n\n    def list(self):\n        \"\"\"\n        List all InactivityAlerts\n\n        :return: A list of all InactivityAlerts\n        :rtype: list[InactivityAlert]\n        \"\"\"\n        return self.alert.list()\n\n    def create(self, alert_data):\n        \"\"\"\n        Create a new InactivityAlert with the specified alert data\n\n        :param alert", "solution": "\n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )"}
{"prompt": "def normal_left_dclick(self, event):\n        \"\"\" Handles the left mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the duration\n        of the view.\n\n        \"\"\"", "output": "\n        if self.tool == 'normal':\n            self.view = self.view_normal\n        else:\n            self.view = self.view_left\n\n    def normal_right_dclick(self, event):\n        \"\"\" Handles the right mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-", "solution": "\n        x = event.x\n        y = event.y\n\n        # First determine what component or components we are going to hittest\n        # on.  If our component is a container, then we add its non-container\n        # components to the list of candidates.\n#        candidates = []\n        component = self.component\n#        if isinstance(component, Container):\n#            candidates = get_nested_components(self.component)\n#        else:\n#            # We don't support clicking on unrecognized components\n#            return\n#\n#        # Hittest against all the candidate and take the first one\n#        item = None\n#        for candidate, offset in candidates:\n#            if candidate.is_in(x-offset[0], y-offset[1]):\n#                item = candidate\n#                break\n\n        if hasattr(component, \"element\"):\n            if component.element is not None:\n                component.active_tool = self\n                component.element.edit_traits(kind=\"livemodal\")\n                event.handled = True\n                component.active_tool = None\n                component.request_redraw()\n        return"}
{"prompt": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"", "output": "\n        # Read the number of points\n        num_points = int.from_bytes(laszip_vlr.read(4), byteorder='little')\n        \n        # Read the compressed data\n        compressed_data = laszip_vlr.read(num_points * point_format.size)\n        \n        # Unpack the compressed data\n        unpacked_data = []\n        for i in range(num_points):\n            unpacked_data.append(struct.unpack(point_format.format, compressed_data[i*point_format.", "solution": "\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points"}
{"prompt": "def projector(state, flatten=False):\n    \"\"\"\n    maps a pure state to a state matrix\n\n    Args:\n        state (ndarray): the number of qubits\n        flatten (bool): determine if state matrix of column work\n    Returns:\n        ndarray:  state_mat(2**num, 2**num) if flatten is false\n        ndarray:  state_mat(4**num) if flatten is true stacked on by the column\n    \"\"\"", "output": "\n    if flatten:\n        return state.reshape(2**state.shape[0], 2**state.shape[0])\n    else:\n        return np.kron(state, np.eye(2))\n\n", "solution": "\n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix"}
{"prompt": "def export_transcripts(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"", "output": "\n    transcripts = adapter.get_transcripts(build=build)\n    for transcript in transcripts:\n        yield transcript\n\n", "solution": "\n    LOG.info(\"Exporting all transcripts\")\n\n    for tx_obj in adapter.transcripts(build=build):\n        yield tx_obj"}
{"prompt": "def write_table(self, table):\n        \"\"\"Send DDL to create the specified `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"", "output": "\n        # Connect to the database\n        db = self.connect()\n\n        # Create a cursor object\n        cursor = db.cursor()\n\n        # Create the table if it doesn't exist\n        cursor.execute(\"CREATE TABLE IF NOT EXISTS %s (\\n  %s \\n)\" % (table.name, table.columns))\n\n        # Insert data into the table\n        for row in table.data:\n            cursor.execute(\"INSERT INTO %s VALUES (%s)\" % (table.name, \", \".join(str(val)", "solution": "\n        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)\n        for sql in serial_key_sql + table_sql:\n            self.execute(sql)"}
{"prompt": "def visit_module(self, node):\n        \"\"\"visit module : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"", "output": "\n        if node.name in self.globals:\n            self.consumption_analysis[node.name] = self.globals[node.name]\n        for child in node.body:\n            self.visit(child)\n\n    def visit_FunctionDef(self, node):\n        \"\"\"visit function definition : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"\n        self.consumption_analysis[node.name] = 0\n        for child in node.body:\n            self.visit(child)\n\n    ", "solution": "\n        self._to_consume = [NamesConsumer(node, \"module\")]\n        self._postponed_evaluation_enabled = is_postponed_evaluation_enabled(node)\n\n        for name, stmts in node.locals.items():\n            if utils.is_builtin(name) and not utils.is_inside_except(stmts[0]):\n                if self._should_ignore_redefined_builtin(stmts[0]) or name == \"__doc__\":\n                    continue\n                self.add_message(\"redefined-builtin\", args=name, node=stmts[0])"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return QuantumChannel(self.dimension)\n\n    def __str__(self):\n        return \"QuantumChannel\"\n\n", "solution": "\n        return SuperOp(\n            np.transpose(self._data),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"", "output": ".exec.dynamic.partition\", \"true\", \"-hive.exec.dynamic.partition.mode\", \"nonstrict\"]\n        \"\"\"\n        return [f\"-{k}\" if k.startswith('-') else k for k in d.keys()] + list(d.values())\n\n    def _execute_hiveconf(hh, params):\n        \"\"\"\n        This function executes a list of hiveconf commands\n        with given parameters.\n\n        :param hh:\n        :type hh: HiveCliHook\n        :param params:", "solution": "\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )"}
{"prompt": "def trimmed_pred_default(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"", "output": "\n  return True\n\n", "solution": "\n  return isinstance(node, ParseNode) and (node.is_empty or node.is_type(ParseNodeType.terminal))"}
{"prompt": "def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"", "output": "\n    # Check that the waveforms have the same time units\n    if check_dep_units:\n        if wave_a.time_units!= wave_b.time_units:\n            raise ValueError(\"Waveforms must have the same time units.\")\n    \n    # Check that the waveforms have the same frequency units\n    if check_dep_units:\n        if wave_a.frequency_units!= wave_b.frequency_units:\n            raise ValueError(\"Waveforms must have the same frequency units.\")\n    \n    # Check that the waveforms have the same frequency", "solution": "\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))"}
{"prompt": "def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs):\n    \"\"\"Write optical flow to file.\n\n    If the flow is not quantized, it will be saved as a .flo file losslessly,\n    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy\n    will be concatenated horizontally into a single image if quantize is True.)\n\n    Args:\n        flow (ndarray): (h, w, 2) array of optical flow.\n        filename (str): Output filepath.\n        quantize (bool): Whether to quantize the flow and save it to 2 jpeg\n            images. If set to True, remaining args will be passed to\n            :func:`quantize_flow`.\n        concat_axis (int): The axis that dx and dy are concatenated,\n            can be either 0 or 1. Ignored if quantize is False.\n    \"\"\"", "output": " filename.\n        quantize (bool): Whether to save as a.flo file or a jpeg image.\n        concat_axis (int): Concatenate the dx and dy horizontally if quantize is True.\n        *args: Passed to numpy.savetxt.\n        **kwargs: Passed to numpy.savetxt.\n\n    Returns:\n        None\n    \"\"\"\n    if quantize:\n        flow = flow.astype(np.uint16)\n        np.savetxt(filename, flow, delimiter=' ', fmt='%d')\n    else:", "solution": "\n    if not quantize:\n        with open(filename, 'wb') as f:\n            f.write('PIEH'.encode('utf-8'))\n            np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f)\n            flow = flow.astype(np.float32)\n            flow.tofile(f)\n            f.flush()\n    else:\n        assert concat_axis in [0, 1]\n        dx, dy = quantize_flow(flow, *args, **kwargs)\n        dxdy = np.concatenate((dx, dy), axis=concat_axis)\n        imwrite(dxdy, filename)"}
{"prompt": "def get_singularity_version(singularity_version=None):\n    \"\"\"get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    \"\"\"", "output": "\n    if singularity_version is None:\n        try:\n            singularity_version = os.environ['SINGULARITY']\n        except KeyError:\n            try:\n                singularity_version = os.popen('version').read().strip()\n            except:\n                pass\n    if singularity_version is None:\n        try:\n            singularity_version = subprocess.check_output(['version', '--version']).strip()\n        except:\n            pass\n    return singularity_version\n\n", "solution": "\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version"}
{"prompt": "def sign(pkey, data, digest):\n    \"\"\"\n    Sign a data string using the given key and message digest.\n\n    :param pkey: PKey to sign with\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: signature\n\n    .. versionadded:: 0.11\n    \"\"\"", "output": "\n    return pkey.sign(data, digest)\n\n", "solution": "\n    data = _text_to_bytes_and_warn(\"data\", data)\n\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError(\"No such digest method\")\n\n    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()\n    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)\n\n    _lib.EVP_SignInit(md_ctx, digest_obj)\n    _lib.EVP_SignUpdate(md_ctx, data, len(data))\n\n    length = _lib.EVP_PKEY_size(pkey._pkey)\n    _openssl_assert(length > 0)\n    signature_buffer = _ffi.new(\"unsigned char[]\", length)\n    signature_length = _ffi.new(\"unsigned int *\")\n    final_result = _lib.EVP_SignFinal(\n        md_ctx, signature_buffer, signature_length, pkey._pkey)\n    _openssl_assert(final_result == 1)\n\n    return _ffi.buffer(signature_buffer, signature_length[0])[:]"}
{"prompt": "def p_navigation_step_2(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR\"\"\"", "output": "\n        p[0] = p[1] + p[3] + p[5]\n\n    def p_navigation_step_3(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT identifier RSQBR\"\"\"\n        p[0] = p[1] + p[3] + p[5]\n\n    def p_navigation_step_4(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT identifier RSQBR\"\"\"\n        p[0] = p[1] + p[3] + p", "solution": "\n        p[0] = NavigationStepNode(key_letter=p[2],\n                                  rel_id=p[4],\n                                  phrase=p[6])"}
{"prompt": "def _get_request(self, endpoint):\n        \"\"\"Do actual GET request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for GET request\n        \"\"\"", "output": "\n        url = f\"https://api.gc.ca/v1/{endpoint}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json()\n\n    def _post_request(self, endpoint, data):\n        \"\"\"Do actual POST request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for POST request\n        data -- data to be sent in body of POST request\n        \"\"\"\n        url = f\"https://api.gc.ca/v1", "solution": "\n        try:\n            response = requests.get(endpoint)\n        except requests.exceptions.RequestException:\n            raise GoldenCheetahNotAvailable(endpoint)\n        \n        if response.text.startswith('unknown athlete'):\n            match = re.match(\n                pattern='unknown athlete (?P<athlete>.+)',\n                string=response.text)\n            raise AthleteDoesNotExist(\n                athlete=match.groupdict()['athlete'])\n\n        elif response.text == 'file not found':\n            match = re.match(\n                pattern='.+/activity/(?P<filename>.+)',\n                string=endpoint)\n            raise ActivityDoesNotExist(\n                filename=match.groupdict()['filename'])\n\n        return response"}
{"prompt": "def _line_segment_with_colons(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"", "output": "\n    segment = linefmt.format(colwidth)\n    if align == 'left':\n        segment = '|'+ segment\n    elif align == 'right':\n        segment = segment + '|'\n    elif align == 'center':\n        segment = '|'+ segment +'|'\n    return segment\n\n", "solution": "\n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w"}
{"prompt": "def report(self):\n        \"\"\"\n        Print network statistics.\n        \"\"\"", "output": "\n        print(f\"Total devices: {len(self.devices)}\")\n        print(f\"Total connections: {len(self.connections)}\")\n        print(f\"Average connection speed: {self.avg_speed}\")\n        print(f\"Max connection speed: {max(self.speeds)}\")\n        print(f\"Min connection speed: {min(self.speeds)}\")\n        print(f\"Average connection duration: {sum(self.durations) / len(self.durations)}\")\n\n", "solution": "\n        logging.info(\"network inputs: %s\", \" \".join(map(str, self.input_variables)))\n        logging.info(\"network targets: %s\", \" \".join(map(str, self.target_variables)))\n        logging.info(\"network parameters: %s\", \" \".join(map(str, self.all_parameters)))\n        logging.info(\"parameter count: %d\", self.parameter_count)"}
{"prompt": "def init_handlers(self):\n        \"\"\"Create the default handlers.\"\"\"", "output": "\n        self.handlers = {}\n        for i in range(1, 11):\n            self.handlers[str(i)] = lambda *args, **kwargs: None\n\n    def handle_request(self, request):\n        \"\"\"Process a request.\"\"\"\n        handler = self.handlers.get(request.method)\n        if not handler:\n            return HTTPStatus(405)\n        return handler(request)\n\n    def __call__(self, environ, start_response):\n        \"\"\"Process the request.\"\"\"\n        start_response('200 OK',", "solution": "\n        self._handlers = {}\n        self._esc_handlers = {}\n        for handler in _default_handlers:\n            handler(\n                shell=self.shell, prefilter_manager=self, config=self.config\n            )"}
{"prompt": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"", "output": " a dict of {'00': 0.5, '01': 0.5, '10': 0.5, '11': 0.5}\n        or a matrix of size 2x2, where the rows and columns represent the outcomes of the experiment.\n\n    Returns:\n        float: The mean value of the observable.\n\n    Raises:\n        ValueError: if observable is not in the correct format.\n    \"\"\"\n    if isinstance(observable, dict):\n        observable = list(observable.values())\n    elif isinstance(observable", "solution": "\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp"}
{"prompt": "def _get_object_name(self, line):\n        \"\"\" Get second token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_name(\"  def func():  \")\n        'func'\n        >>> docwriter._get_object_name(\"  class Klass(object):  \")\n        'Klass'\n        >>> docwriter._get_object_name(\"  class Klass:  \")\n        'Klass'\n        \"\"\"", "output": "\n        tokens = line.split()\n        if len(tokens) > 1:\n            return tokens[1]\n        else:\n            return line\n\n    def _get_method_name(self, line):\n        \"\"\" Get third token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_method_name(\"  def func():  \")\n        'def'\n        >>> docwriter._get_method_name(\"  class Klass(object):  \")\n        'Klass'\n        >>> doc", "solution": "\n        name = line.split()[1].split('(')[0].strip()\n        # in case we have classes which are not derived from object\n        # ie. old style classes\n        return name.rstrip(':')"}
{"prompt": "def _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"", "output": "\n    metrics = {\n        \"dag_id\": namespace.dag_id,\n        \"task_id\": namespace.task_id,\n        \"execution_date\": namespace.execution_date\n    }\n    return metrics\n\n", "solution": "\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics"}
{"prompt": "def get_func_sourcecode(func):\n    \"\"\"\n    Try to get sourcecode using standard inspect.getsource().\n    If the function comes from a module which has been created dynamically\n    (not from the filesystem), then it tries to read the sourcecode on the\n    filesystem anyway.\n    WARNING: can do weird things if the filesystem code slightly differs from\n             the original module code.\n    \"\"\"", "output": "\n    try:\n        sourcecode = inspect.getsource(func)\n    except IOError:\n        # This is expected if the function is not found in the filesystem\n        # or if the file is not readable.\n        return \"\"\n    return sourcecode\n\n", "solution": "\n\n    def getsource(func):\n        lines, lnum = getsourcelines(func)\n        return ''.join(lines)\n\n    def getsourcelines(func):\n        lines, lnum = findsource(func)\n        return inspect.getblock(lines[lnum:]), lnum + 1\n\n    def findsource(func):\n        file = getfile(func)  # file path\n        module = inspect.getmodule(func, file)\n        lines = linecache.getlines(file, module.__dict__)\n        code = func.__code__\n        lnum = code.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]):\n                break\n            lnum = lnum - 1  # pragma: no cover\n        return lines, lnum\n\n    def getfile(func):\n        module = inspect.getmodule(func)\n        return module.__file__\n\n    try:\n        return inspect.getsource(func)\n    except Exception:\n        return getsource(func)"}
{"prompt": "def x_fit(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"", "output": "\r\n        if test_length <= self.length:\r\n            return True\r\n        else:\r\n            return False\r\n    \r\n    def y_fit(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"\r\n        if test_length <= self.width:\r\n            return True\r\n        else:\r\n            return False\r\n    \r\n    def is_parallel(self, line):\r\n        \"\"\" Check if two lines are parallel. \"\"\"\r\n        if self.slope ==", "solution": "\r\n        if (self.x + test_length) >= self.xmax:\r\n            return False\r\n        else:\r\n            return True"}
{"prompt": "def list_files(self, extensions=None):\n        \"\"\"\n        List the ports contents by file type or all.\n        :param extensions: string extensions, single string or list of extensions.\n        :return: A list of full path names of each file.\n        \"\"\"", "output": "\n        if extensions is None:\n            return [os.path.join(self.root, f) for f in os.listdir(self.root) if os.path.isfile(os.path.join(self.root, f))]\n        elif isinstance(extensions, str):\n            return [os.path.join(self.root, f) for f in os.listdir(self.root) if f.endswith(extensions)]\n        elif isinstance(extensions, list):\n            return [os.path.join(", "solution": "\n        if self.type.lower() != 'directory':\n            raise ValueError(\"Port type is not == directory\")\n\n        filesystem_location = self.path\n\n        for root, dirs, files in os.walk(filesystem_location):\n            if extensions is None:\n                return [os.path.join(root, f) for f in files]\n            elif not isinstance(extensions, list):\n                extensions = [extensions]\n\n            subset_files = []\n\n            for f in files:\n                for extension in extensions:\n                    if f.lower().endswith(extension.lower()):\n                        subset_files.append(os.path.join(root, f))\n                        break\n            return subset_files"}
{"prompt": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"", "output": "\n        if to_chw:\n            image = self.image.transpose(2, 0, 1)\n        else:\n            image = self.image\n        return image\n\n    def get_label(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get label list from ImageFrame\n        \"\"\"\n        if to_chw:\n            label = self.label.transpose(2, 0, 1)\n        else:\n            label = self.label\n        return label\n\n    def get_metadata(self, float_", "solution": "\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)"}
{"prompt": "def generate_key(self, type, bits):\n        \"\"\"\n        Generate a key pair of the given type, with the given number of bits.\n\n        This generates a key \"into\" the this object.\n\n        :param type: The key type.\n        :type type: :py:data:`TYPE_RSA` or :py:data:`TYPE_DSA`\n        :param bits: The number of bits.\n        :type bits: :py:data:`int` ``>= 0``\n        :raises TypeError: If :py:data:`type` or :py:data:`bits` isn't\n            of the appropriate type.\n        :raises ValueError: If the number of bits isn't an integer of\n            the appropriate size.\n        :return: ``None``\n        \"\"\"", "output": ":`type` is not one of the allowed values.\n        :raises ValueError: If :py:data:`bits` is not a positive integer.\n        :return: A tuple of two strings representing the generated key.\n        :rtype: tuple of two strings\n        \"\"\"\n        if type == TYPE_RSA:\n            return self.generate_rsa_key(bits)\n        elif type == TYPE_DSA:\n            return self.generate_dsa_key(bits)\n        else:\n            raise TypeError(\"Unsupported key type", "solution": "\n        if not isinstance(type, int):\n            raise TypeError(\"type must be an integer\")\n\n        if not isinstance(bits, int):\n            raise TypeError(\"bits must be an integer\")\n\n        if type == TYPE_RSA:\n            if bits <= 0:\n                raise ValueError(\"Invalid number of bits\")\n\n            # TODO Check error return\n            exponent = _lib.BN_new()\n            exponent = _ffi.gc(exponent, _lib.BN_free)\n            _lib.BN_set_word(exponent, _lib.RSA_F4)\n\n            rsa = _lib.RSA_new()\n\n            result = _lib.RSA_generate_key_ex(rsa, bits, exponent, _ffi.NULL)\n            _openssl_assert(result == 1)\n\n            result = _lib.EVP_PKEY_assign_RSA(self._pkey, rsa)\n            _openssl_assert(result == 1)\n\n        elif type == TYPE_DSA:\n            dsa = _lib.DSA_new()\n            _openssl_assert(dsa != _ffi.NULL)\n\n            dsa = _ffi.gc(dsa, _lib.DSA_free)\n            res = _lib.DSA_generate_parameters_ex(\n                dsa, bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL\n            )\n            _openssl_assert(res == 1)\n\n            _openssl_assert(_lib.DSA_generate_key(dsa) == 1)\n            _openssl_assert(_lib.EVP_PKEY_set1_DSA(self._pkey, dsa) == 1)\n        else:\n            raise Error(\"No such key type\")\n\n        self._initialized = True"}
{"prompt": "def v2_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"", "output": "\n        pass\n\n    def v3_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"\n        pass\n\n    def v4_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"\n        pass\n\n    def v5_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"\n        pass\n\n    def v6_runner_on_skipped(self, result, **kwargs", "solution": "\n        if self._display.verbosity > 1:\n            self._print_task()\n            self.last_skipped = False\n\n            line_length = 120\n            spaces = \" \" * (31 - len(result._host.name) - 4)\n\n            line = \"  * {}{}- {}\".format(\n                colorize(result._host.name, \"not_so_bold\"),\n                spaces,\n                colorize(\"skipped\", \"skipped\"),\n            )\n\n            reason = result._result.get(\"skipped_reason\", \"\") or result._result.get(\n                \"skip_reason\", \"\"\n            )\n            if len(reason) < 50:\n                line += \" -- {}\".format(reason)\n                print(\"{} {}---------\".format(line, \"-\" * (line_length - len(line))))\n            else:\n                print(\"{} {}\".format(line, \"-\" * (line_length - len(line))))\n                print(self._indent_text(reason, 8))\n                print(reason)"}
{"prompt": "def _estimate_gas(self,\n                      master_copy: str,\n                      initializer: bytes,\n                      funder: str,\n                      payment_token: str) -> int:\n        \"\"\"\n        Gas estimation done using web3 and calling the node\n        Payment cannot be estimated, as no ether is in the address. So we add some gas later.\n        :param master_copy: Master Copy of Gnosis Safe already deployed\n        :param initializer: Data initializer to send to GnosisSafe setup method\n        :param funder: Address that should get the payment (if payment set)\n        :param payment_token: Address if a token is used. If not set, 0x0 will be ether\n        :return: Total gas estimation\n        \"\"\"", "output": "Gnosis Safe)\n        :param payment_token: Token that Gnosis Safe will use to send the payment\n        :return: Estimated gas used in the transaction\n        \"\"\"\n        # Add gas estimation here\n        gas_used = self.web3.eth.estimate_gas(initializer + payment_token)\n        return gas_used\n\n    def _send_transaction(self,\n                          master_copy: str,\n                          initializer: bytes,\n                          funder: str,\n                          payment_token: str) -> int:\n        \"\"\"\n        Sends a transaction", "solution": "\n\n        # Estimate the contract deployment. We cannot estimate the refunding, as the safe address has not any fund\n        gas: int = self._build_proxy_contract_creation_constructor(\n            master_copy,\n            initializer,\n            funder,\n            payment_token,\n            0).estimateGas()\n\n        # We estimate the refund as a new tx\n        if payment_token == NULL_ADDRESS:\n            # Same cost to send 1 ether than 1000\n            gas += self.w3.eth.estimateGas({'to': funder, 'value': 1})\n        else:\n            # Top should be around 52000 when storage is needed (funder no previous owner of token),\n            # we use value 1 as we are simulating an internal call, and in that calls you don't pay for the data.\n            # If it was a new tx sending 5000 tokens would be more expensive than sending 1 because of data costs\n            try:\n                gas += get_erc20_contract(self.w3,\n                                          payment_token).functions.transfer(funder, 1).estimateGas({'from':\n                                                                                                    payment_token})\n            except ValueError as exc:\n                raise InvalidERC20Token from exc\n\n        return gas"}
{"prompt": "def reg_on_abort(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"", "output": "\n        self.abort_callbacks.append((callable_object, args, kwargs))\n\n    def execute(self, callable_object, *args, **kwargs):\n        \"\"\"\n        Execute the given callable object with the given arguments and keyword arguments.\n        If an abort callback is registered for the callable object, it will be called with the\n        exception object and arguments passed to it.\n        \"\"\"\n        try:\n            result = callable_object(*args, **kwargs)\n        except Exception as e:\n            for callback in self", "solution": "\n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'abort', persistent, *args, **kwargs)\n        self.abort_callbacks.append(event)\n        return event"}
{"prompt": "def wa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wolfram|Alpha. Returns the first\n\tresult, if available.\n\t\"\"\"", "output": "\n    # Construct the query string\n    query = \"?client=\" + client + \"&event=\" + event + \"&channel=\" + channel + \"&nick=\" + nick + \"&rest=\" + rest\n    \n    # Send the query and get the first result\n    try:\n        response = requests.get(query)\n        result = response.json()\n        return result['results'][0]['value']\n    except:\n        return None\n\n", "solution": "\n\tclient = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])\n\tres = client.query(rest)\n\treturn next(res.results).text"}
{"prompt": "def update_(self, sct_dict, conf_arg=True):\n        \"\"\"Update values of configuration section with dict.\n\n        Args:\n            sct_dict (dict): dict indexed with option names. Undefined\n                options are discarded.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"", "output": "\n        for opt, val in sct_dict.items():\n            if opt in self.options:\n                self.set(opt, val)\n            elif conf_arg:\n                pass\n            else:\n                self.discard(opt)\n\n    def set(self, opt, val):\n        \"\"\"Set value of option in configuration section.\n\n        Args:\n            opt (str): option name.\n            val (any): value to set.\n        \"\"\"\n        self.options[opt] = val\n\n    def discard(self, opt):\n        \"\"\"Disc", "solution": "\n        for opt, val in sct_dict.items():\n            if opt not in self.def_:\n                continue\n            if not conf_arg or self.def_[opt].conf_arg:\n                self[opt] = val"}
{"prompt": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"", "output": "\n        for file in os.listdir(self.directory):\n            if re.search(self.regex, file):\n                os.system(f\"cp {self.directory}/{file} {self.directory}/poke_{file}\")\n        return True\n\n", "solution": "\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)"}
{"prompt": "def breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds and returns breadcrumb trail structure for 'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        :rtype: list|str\n        \"\"\"", "output": "\n        trail = []\n        if isinstance(tree_alias, str):\n            trail.append(tree_alias)\n        else:\n            trail.append(tree_alias.alias)\n        trail.append(tree_alias.label)\n        trail.append(tree_alias.id)\n        trail.append(tree_alias.parent_id)\n        trail.append(tree_alias.depth)\n        trail.append(tree_alias.is_root)\n        trail.append(tree_alias.is_", "solution": "\n        tree_alias, sitetree_items = self.init_tree(tree_alias, context)\n\n        if not sitetree_items:\n            return ''\n\n        current_item = self.get_tree_current_item(tree_alias)\n\n        breadcrumbs = []\n\n        if current_item is not None:\n\n            context_ = self.current_page_context\n            check_access = self.check_access\n            get_item_by_id = self.get_item_by_id\n\n            def climb(base_item):\n                "}
{"prompt": "def getCSV(self):\n        \"\"\"\n        Returns\n        -------\n        filename: str\n        \"\"\"", "output": "\n        return self.filename\n\n    def setFilename(self, filename):\n        \"\"\"\n        Parameters\n        ----------\n        filename: str\n        \"\"\"\n        self.filename = filename\n\n    def getData(self):\n        \"\"\"\n        Returns\n        -------\n        data: List[List[str]]\n        \"\"\"\n        if not hasattr(self, '_data'):\n            self._data = []\n        return self._data\n\n    def setData(self, data):\n        \"\"\"\n        Parameters\n        ----------\n        data: List[List[str]]\n        ", "solution": "\n        import getpass\n        import gspread\n\n        user = raw_input(\"Insert Google username:\")\n        password = getpass.getpass(prompt=\"Insert password:\")\n        name = raw_input(\"SpreadSheet filename on Drive:\")\n        sheet = raw_input(\"Sheet name (first sheet is default):\")\n\n        cl = gspread.login(user, password)\n        sh = cl.open(name)\n\n        if not (sheet.strip()):\n            ws = sh.sheet1\n            sheet = \"1\"\n        else:\n            ws = sh.worksheet(sheet)\n\n        filename = name + '-worksheet_' + sheet + '.csv'\n        with open(filename, 'wb') as f:\n            writer = UnicodeWriter(f)\n            writer.writerows(ws.get_all_values())\n\n        return filename"}
{"prompt": "def hunt_repeated_yaml_keys(data):\n    \"\"\"Parses yaml and returns a list of repeated variables and\n       the line on which they occur\n    \"\"\"", "output": "\n    repeated_keys = []\n    yaml_data = yaml.safe_load(data)\n    for key in yaml_data.keys():\n        if isinstance(yaml_data[key], dict):\n            repeated_keys.extend(hunt_repeated_yaml_keys(yaml_data[key]))\n        elif yaml_data[key] in yaml_data.values() and key not in repeated_keys:\n            repeated_keys.append(key)\n    return repeated_keys\n\n", "solution": "\n    loader = yaml.Loader(data)\n\n    def compose_node(parent, index):\n        # the line number where the previous token has ended (plus empty lines)\n        line = loader.line\n        node = Composer.compose_node(loader, parent, index)\n        node.__line__ = line + 1\n        return node\n\n    def construct_mapping(node, deep=False):\n        mapping = dict()\n        errors = dict()\n        for key_node, value_node in node.value:\n            key = key_node.value\n            if key in mapping:\n                if key in errors:\n                    errors[key].append(key_node.__line__)\n                else:\n                    errors[key] = [mapping[key], key_node.__line__]\n\n            mapping[key] = key_node.__line__\n\n        return errors\n\n    loader.compose_node = compose_node\n    loader.construct_mapping = construct_mapping\n    data = loader.get_single_data()\n    return data"}
{"prompt": "def deserialize_dataframe(reader, data_type_id):\n    \"\"\"\n    Deserialize a dataframe.\n\n    Parameters\n    ----------\n    reader : file\n        File-like object to read from. Must be opened in binary mode.\n    data_type_id : dict\n        Serialization format of the raw data.\n        See the azureml.DataTypeIds class for constants.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe object.\n    \"\"\"", "output": "\n    # Parse header\n    header = reader.read(16)\n    num_columns = int.from_bytes(header[12:16], byteorder='big')\n    num_rows = int.from_bytes(header[8:12], byteorder='big')\n\n    # Parse data\n    data = []\n    for i in range(num_rows):\n        row = []\n        for j in range(num_columns):\n            data_type = DataTypeIds.get_id(data_type_id)\n            value_size", "solution": "\n    _not_none('reader', reader)\n    _not_none_or_empty('data_type_id', data_type_id)\n\n    serializer = _SERIALIZERS.get(data_type_id)\n    if serializer is None:\n        raise UnsupportedDatasetTypeError(data_type_id)\n    return serializer[1](reader=reader)"}
{"prompt": "def asin(wave):\n    r\"\"\"\n    Return the arc sine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.asin\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "ises:\n    \"\"\"\n    if wave.x_data is None or wave.y_data is None:\n        raise ValueError(\"Waveform data not provided.\")\n    if len(wave.x_data)!= len(wave.y_data):\n        raise ValueError(\"Waveform data length does not match.\")\n    if wave.x_data[0]!= 0 or wave.y_data[0]!= 0:\n        raise ValueError(\"Waveform data does not start at 0.\")\n    if wave.x_data[-1]!= wave.x_data", "solution": "\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"asin\", \"rad\", np.arcsin)"}
{"prompt": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"", "output": "\n        for child in self.children:\n            child.propagate_event_dependency()\n\n    def propagate_event_dependency(self):\n        \"\"\"\n        Propagate event dependency flag to child statements\n        \"\"\"\n        self.event_dependent = True\n\n    def __str__(self):\n        return f\"{self.name} ({self.event_dependent})\"\n\n", "solution": "\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()"}
{"prompt": "def allclose(a, b):\n    \"\"\"\n    Test that a and b are close and match in shape.\n\n    Parameters\n    ----------\n    a : ndarray\n        First array to check\n\n    b : ndarray\n        First array to check\n    \"\"\"", "output": "\n    if a.shape!= b.shape:\n        return False\n    diff = abs(a - b)\n    if not np.all(diff <= 1e-8):\n        return False\n    return True\n\n", "solution": "\n    from numpy import allclose\n    return (a.shape == b.shape) and allclose(a, b)"}
{"prompt": "def signFix(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"", "output": "\n    if val < 0:\n        val = -val\n    return val if val < (2**(width-1)) else (2**width-1)\n\n", "solution": "\n    if val > 0:\n        msb = 1 << (width - 1)\n        if val & msb:\n            val -= mask(width) + 1\n    return val"}
{"prompt": "def list(self):\n        \"\"\"\n        Get all current labels\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        try:\n            return self.__response\n        except ServerException as e:\n            raise e\n\n    def labels(self):\n        \"\"\"\n        Get all current labels\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        return self.list()['labels']\n\n    def data(self):\n        \"\"\"\n        ", "solution": "\n        return self._post(\n            request='list',\n            uri=ApiUri.TAGS.value,\n        ).get('tags')"}
{"prompt": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"", "output": "\n        if not self.is_visible():\n            menu.append(action)\n        else:\n            widget = self.widget()\n            if widget:\n                widget.shortcut_context = widget\n                if not defer_shortcut:\n                    widget.bind(on_press=lambda widget, event: menu.append(action))\n            else:\n                menu.append(action)\n    \n    def remove_menu_action(self, menu, action):\n        \"\"\"Remove action from menu\n        \n        If the action is not in the menu, do nothing.\n", "solution": "\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)"}
{"prompt": "def from_string(bnf: str, entry=None, *optional_inherit) -> Grammar:\n    \"\"\"\n    Create a Grammar from a string\n    \"\"\"", "output": "\n    if entry is None:\n        entry = {}\n    if optional_inherit:\n        entry.update(optional_inherit)\n    return Grammar(**entry)\n\n", "solution": "\n    inherit = [Grammar] + list(optional_inherit)\n    scope = {'grammar': bnf, 'entry': entry}\n    return build_grammar(tuple(inherit), scope)"}
{"prompt": "def parse_statement(self):\n    \"\"\"Parse a single statement.\n\n    Returns:\n      Either a `BindingStatement`, `ImportStatement`, `IncludeStatement`, or\n      `None` if no more statements can be parsed (EOF reached).\n    \"\"\"", "output": "\n    while True:\n        # Parse a single statement\n        if self.current_char == ';':\n            self.eat(';')\n            return BindingStatement(self.current_token)\n        elif self.current_char == ':':\n            self.eat(':')\n            return ImportStatement(self.current_token)\n        elif self.current_char == '<':\n            self.eat('<')\n            return IncludeStatement(self.current_token)\n        else:\n            return None\n\n", "solution": "\n    self._skip_whitespace_and_comments()\n    if self._current_token.kind == tokenize.ENDMARKER:\n      return None\n\n    # Save off location, but ignore char_num for any statement-level errors.\n    stmt_loc = self._current_location(ignore_char_num=True)\n    binding_key_or_keyword = self._parse_selector()\n    statement = None\n    if self._current_token.value != '=':\n      if binding_key_or_keyword == 'import':\n        module = self._parse_selector(scoped=False)\n        statement = ImportStatement(module, stmt_loc)\n      elif binding_key_or_keyword == 'include':\n        str_loc = self._current_location()\n        success, filename = self._maybe_parse_basic_type()\n        if not success or not isinstance(filename, str):\n          self._raise_syntax_error('Expected file path as string.', str_loc)\n        statement = IncludeStatement(filename, stmt_loc)\n      else:\n        self._raise_syntax_error(\"Expected '='.\")\n    else:  # We saw an '='.\n      self._advance_one_token()\n      value = self.parse_value()\n      scope, selector, arg_name = parse_binding_key(binding_key_or_keyword)\n      statement = BindingStatement(scope, selector, arg_name, value, stmt_loc)\n\n    assert statement, 'Internal parsing error.'\n\n    if (self._current_token.kind != tokenize.NEWLINE and\n        self._current_token.kind != tokenize.ENDMARKER):\n      self._raise_syntax_error('Expected newline.')\n    elif self._current_token.kind == tokenize.NEWLINE:\n      self._advance_one_token()\n\n    return statement"}
{"prompt": "def add_xpaths_to_stream_item(si):\n    \"\"\"Mutably tag tokens with xpath offsets.\n\n    Given some stream item, this will tag all tokens from all taggings\n    in the document that contain character offsets. Note that some\n    tokens may not have computable xpath offsets, so an xpath offset\n    for those tokens will not be set. (See the documentation and\n    comments for ``char_offsets_to_xpaths`` for what it means for a\n    token to have a computable xpath.)\n\n    If a token can have its xpath offset computed, it is added to its\n    set of offsets with a ``OffsetType.XPATH_CHARS`` key.\n    \"\"\"", "output": " token does not have an xpath offset, it will not be tagged.\n\n    Args:\n        si (stream.StreamItem): The stream item to tag.\n\n    Returns:\n        stream.StreamItem: The stream item with all tokens tagged.\n    \"\"\"\n    for token in si.tokens:\n        if token.has_computable_xpath_offset:\n            token.tagged = True\n    return si\n\n", "solution": "\n    def sentences_to_xpaths(sentences):\n        tokens = sentences_to_char_tokens(sentences)\n        offsets = char_tokens_to_char_offsets(tokens)\n        return char_offsets_to_xpaths(html, offsets)\n\n    def xprange_to_offset(xprange):\n        return Offset(type=OffsetType.XPATH_CHARS,\n                      first=xprange.start_offset, length=0,\n                      xpath=xprange.start_xpath,\n                      content_form='clean_html', value=None,\n                      xpath_end=xprange.end_xpath,\n                      xpath_end_offset=xprange.end_offset)\n\n    html = unicode(si.body.clean_html, 'utf-8')\n    for sentences in si.body.sentences.itervalues():\n        tokens = sentences_to_char_tokens(sentences)\n        for token, xprange in izip(tokens, sentences_to_xpaths(sentences)):\n            if xprange is None:\n                continue\n            offset = xprange_to_offset(xprange)\n            token.offsets[OffsetType.XPATH_CHARS] = offset"}
{"prompt": "def fill_parameters(self, path, blocks, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill all blocks sequentially.\n        :type blocks: list of deepy.layers.Block\n        \"\"\"", "output": "\n        for block in blocks:\n            block.fill_parameters(path, exclude_free_params, check_parameters)\n\n    def get_parameters(self, path, block, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill block.\n        :type block: deepy.layers.Block\n        \"\"\"\n        block.get_parameters(path, exclude_free_params, check_parameters)\n\n    def get_parameters_from_file(self, path, block, exclude", "solution": "\n        if not os.path.exists(path):\n            raise Exception(\"model {} does not exist\".format(path))\n        # Decide which parameters to load\n        normal_params = sum([nn.parameters for nn in blocks], [])\n        all_params = sum([nn.all_parameters for nn in blocks], [])\n        # Load parameters\n        if path.endswith(\".gz\"):\n            opener = gzip.open if path.lower().endswith('.gz') else open\n            handle = opener(path, 'rb')\n            saved_params = pickle.load(handle)\n            handle.close()\n            # Write parameters\n            if len(all_params) != len(saved_params):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(saved_params)))\n            for target, source in zip(all_params, saved_params):\n                if not exclude_free_params or target not in normal_params:\n                    target.set_value(source)\n        elif path.endswith(\".npz\"):\n            arrs = np.load(path)\n            # Write parameters\n            if len(all_params) != len(arrs.keys()):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(arrs.keys())))\n            for target, idx in zip(all_params, range(len(arrs.keys()))):\n                if not exclude_free_params or target not in normal_params:\n                    source = arrs['arr_%d' % idx]\n                    target.set_value(source)\n        else:\n            raise Exception(\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path)"}
{"prompt": "def get_endpoint_path(self, endpoint_id):\n    \"\"\"return the first fullpath to a folder in the endpoint based on\n       expanding the user's home from the globus config file. This\n       function is fragile but I don't see any other way to do it.\n    \n       Parameters\n       ==========\n       endpoint_id: the endpoint id to look up the path for\n\n    \"\"\"", "output": "\n    # expand the user's home from the globus config file\n    home = expanduser('~')\n    \n    # split the home into a list of folders\n    folders = home.split('/')\n    \n    # loop through the folders until we find the one with the given endpoint_id\n    for folder in folders:\n        if endpoint_id in folder:\n            return folder\n    \n    # if we get here, we didn't find the endpoint_id in any of the folders\n    return ''\n\n", "solution": "\n    config = os.path.expanduser(\"~/.globusonline/lta/config-paths\")\n    if not os.path.exists(config):\n        bot.error('%s not found for a local Globus endpoint.')\n        sys.exit(1)\n\n    path = None\n\n    # Read in the config and get the root path\n\n    config = [x.split(',')[0] for x in read_file(config)]\n    for path in config:\n        if os.path.exists(path):\n            break\n\n    # If we don't have an existing path, exit\n\n    if path is None:\n        bot.error('No path was found for a local Globus endpoint.')\n        sys.exit(1)\n\n    return path"}
{"prompt": "def _request_activity_list(self, athlete):\n        \"\"\"Actually do the request for activity list\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"", "output": "\n        # Code to get activity list for athlete\n        activity_list = get_activity_list(athlete)\n        # Cache the activity list for future requests\n        self.activity_lists[athlete] = activity_list\n        return activity_list\n\n    def get_activity_list(self, athlete):\n        \"\"\"Get activity list for athlete\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"\n        if athlete not in self.activity_lists:\n            self._request_activity_list(athlete)\n        return self.activity_lists", "solution": "\n        response = self._get_request(self._athlete_endpoint(athlete))\n        response_buffer = StringIO(response.text)\n\n        activity_list = pd.read_csv(\n            filepath_or_buffer=response_buffer,\n            parse_dates={'datetime': ['date', 'time']},\n            sep=',\\s*',\n            engine='python'\n        )\n        activity_list.rename(columns=lambda x: x.lower(), inplace=True)\n        activity_list.rename(\n            columns=lambda x: '_' + x if x[0].isdigit() else x, inplace=True)\n\n        activity_list['has_hr'] = activity_list.average_heart_rate.map(bool)\n        activity_list['has_spd'] = activity_list.average_speed.map(bool)\n        activity_list['has_pwr'] = activity_list.average_power.map(bool)\n        activity_list['has_cad'] = activity_list.average_heart_rate.map(bool)\n        activity_list['data'] = pd.Series(dtype=np.dtype(\"object\"))\n        return activity_list"}
{"prompt": "def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):\n    \"\"\"\n    Reconnect model signals to unit to run simulation with simulation model\n    but use original unit interfaces for communication\n\n    :param synthesisedUnitOrIntf: interface where should be signals\n        replaced from signals from modelCls\n    :param modelCls: simulation model form where signals\n        for synthesisedUnitOrIntf should be taken\n    \"\"\"", "output": "\n    for signal in synthesisedUnitOrIntf:\n        signal.reconnect(modelCls)\n\n", "solution": "\n    obj = synthesisedUnitOrIntf\n    subInterfaces = obj._interfaces\n\n\n    if subInterfaces:\n        for intf in subInterfaces:\n            # proxies are destroyed on original interfaces and only proxies on\n            # array items will remain\n            reconnectUnitSignalsToModel(intf, modelCls)\n    else:\n        # reconnect signal from model\n        s = synthesisedUnitOrIntf\n        s._sigInside = getattr(modelCls, s._sigInside.name)"}
{"prompt": "def lines_without_stdlib(self):\n        \"\"\"Filters code from standard library from self.lines.\"\"\"", "output": "\n        return [line for line in self.lines if not line.startswith(\"import \")]\n\n", "solution": "\n        prev_line = None\n        current_module_path = inspect.getabsfile(inspect.currentframe())\n        for module_path, lineno, runtime in self.lines:\n            module_abspath = os.path.abspath(module_path)\n            if not prev_line:\n                prev_line = [module_abspath, lineno, runtime]\n            else:\n                if (not check_standard_dir(module_path) and\n                        module_abspath != current_module_path):\n                    yield prev_line\n                    prev_line = [module_abspath, lineno, runtime]\n                else:\n                    prev_line[2] += runtime\n        yield prev_line"}
{"prompt": "def from_ascii(path, seperator=None, names=True, skip_lines=0, skip_after=0, **kwargs):\n    \"\"\"\n    Create an in memory DataFrame from an ascii file (whitespace seperated by default).\n\n    >>> ds = vx.from_ascii(\"table.asc\")\n    >>> ds = vx.from_ascii(\"table.csv\", seperator=\",\", names=[\"x\", \"y\", \"z\"])\n\n    :param path: file path\n    :param seperator: value seperator, by default whitespace, use \",\" for comma seperated values.\n    :param names: If True, the first line is used for the column names, otherwise provide a list of strings with names\n    :param skip_lines: skip lines at the start of the file\n    :param skip_after: skip lines at the end of the file\n    :param kwargs:\n    :rtype: DataFrame\n    \"\"\"", "output": ":param seperator: separator used in the file (default: None)\n    :param names: whether to include column names (default: True)\n    :param skip_lines: number of lines to skip at the beginning of the file (default: 0)\n    :param skip_after: number of lines to skip after the header (default: 0)\n    :param kwargs: additional arguments passed to pandas.read_csv()\n    :return: in memory DataFrame\n    \"\"\"\n    df = pd.read_csv(path, **kwargs)\n    ", "solution": "\n\n    import vaex.ext.readcol as rc\n    ds = vaex.dataframe.DataFrameArrays(path)\n    if names not in [True, False]:\n        namelist = names\n        names = False\n    else:\n        namelist = None\n    data = rc.readcol(path, fsep=seperator, asdict=namelist is None, names=names, skipline=skip_lines, skipafter=skip_after, **kwargs)\n    if namelist:\n        for name, array in zip(namelist, data.T):\n            ds.add_column(name, array)\n    else:\n        for name, array in data.items():\n            ds.add_column(name, array)\n    return ds"}
{"prompt": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"", "output": "  Args:\n    x: A `Tensor`. The value at which to evaluate the normal distribution function.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `x`.\n  \"\"\"\n  with ops.name_scope(name, \"ndtr\", [x]) as name:\n    x = ops.convert_to_tensor(x, name=\"x\")\n    return 0.5 * math_ops.erfc(x / math_ops.sqrt(2))\n\n", "solution": "\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)"}
{"prompt": "def _all_dirs(base_path):\n        \"\"\"\n        Return all dirs in base_path, relative to base_path\n        \"\"\"", "output": "\n        return [name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name))]\n\n    # Get all dirs in base_path\n    all_dirs = _all_dirs(base_path)\n\n    # Filter out dirs that are not in all_dirs\n    return [dir for dir in dirs if dir in all_dirs]\n\n", "solution": "\n        for root, dirs, files in os.walk(base_path, followlinks=True):\n            for dir in dirs:\n                yield os.path.relpath(os.path.join(root, dir), base_path)"}
{"prompt": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"", "output": "\n  with open( file, 'w' ) as f:\n    f.write( encoding_list[ encoding_name ] )\n\n", "solution": "\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )"}
{"prompt": "def _set_kernel_manager(self, kernel_manager):\n        \"\"\" Disconnect from the current kernel manager (if any) and set a new\n            kernel manager.\n        \"\"\"", "output": "\n        if kernel_manager is not None:\n            kernel_manager.disconnect()\n        self.kernel_manager = None\n\n    def _get_kernel_manager(self):\n        \"\"\" Get the current kernel manager (if any).\n        \"\"\"\n        return self.kernel_manager\n\n    kernel_manager = property(_get_kernel_manager, _set_kernel_manager)\n\n    def __init__(self, name, ip_address):\n        self.name = name\n        self.ip_address = ip_address\n        self.status = \"stopped\"\n", "solution": "\n        # Disconnect the old kernel manager, if necessary.\n        old_manager = self._kernel_manager\n        if old_manager is not None:\n            old_manager.started_kernel.disconnect(self._started_kernel)\n            old_manager.started_channels.disconnect(self._started_channels)\n            old_manager.stopped_channels.disconnect(self._stopped_channels)\n\n            # Disconnect the old kernel manager's channels.\n            old_manager.sub_channel.message_received.disconnect(self._dispatch)\n            old_manager.shell_channel.message_received.disconnect(self._dispatch)\n            old_manager.stdin_channel.message_received.disconnect(self._dispatch)\n            old_manager.hb_channel.kernel_died.disconnect(\n                self._handle_kernel_died)\n\n            # Handle the case where the old kernel manager is still listening.\n            if old_manager.channels_running:\n                self._stopped_channels()\n\n        # Set the new kernel manager.\n        self._kernel_manager = kernel_manager\n        if kernel_manager is None:\n            return\n\n        # Connect the new kernel manager.\n        kernel_manager.started_kernel.connect(self._started_kernel)\n        kernel_manager.started_channels.connect(self._started_channels)\n        kernel_manager.stopped_channels.connect(self._stopped_channels)\n\n        # Connect the new kernel manager's channels.\n        kernel_manager.sub_channel.message_received.connect(self._dispatch)\n        kernel_manager.shell_channel.message_received.connect(self._dispatch)\n        kernel_manager.stdin_channel.message_received.connect(self._dispatch)\n        kernel_manager.hb_channel.kernel_died.connect(self._handle_kernel_died)\n\n        # Handle the case where the kernel manager started channels before\n        # we connected.\n        if kernel_manager.channels_running:\n            self._started_channels()"}
{"prompt": "def subscribe_to_trades(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"", "output": "\n        if pair not in self.trades:\n            self.trades[pair] = {}\n\n        if 'last_updated' not in kwargs:\n            kwargs['last_updated'] = datetime.utcnow()\n\n        if 'data' not in kwargs:\n            kwargs['data'] = []\n\n        kwargs['data'].append({\n            'pair': pair,\n            'last_updated': kwargs['last_updated'],\n            'data': kwargs['data']\n        })\n\n        return self.subscribe", "solution": "\n        identifier = ('trades', pair)\n        self._subscribe('trades', identifier, symbol=pair, **kwargs)"}
{"prompt": "def interact(self, display_banner=None):\n        \"\"\"Closely emulate the interactive Python console.\"\"\"", "output": "\n        if display_banner is not None:\n            print(display_banner)\n        while True:\n            try:\n                line = input()\n                if line == \"exit\":\n                    break\n                print(line)\n            except EOFError:\n                break\n        print(\"Goodbye!\")\n\n    def __init__(self, code_string):\n        self.code_string = code_string\n        self.console = InteractiveConsole(code_string)\n\n    def run(self):\n        self.console.interact()\n\n", "solution": "\n\n        # batch run -> do not interact\n        if self.exit_now:\n            return\n\n        if display_banner is None:\n            display_banner = self.display_banner\n\n        if isinstance(display_banner, basestring):\n            self.show_banner(display_banner)\n        elif display_banner:\n            self.show_banner()\n\n        more = False\n\n        if self.has_readline:\n            self.readline_startup_hook(self.pre_readline)\n            hlen_b4_cell = self.readline.get_current_history_length()\n        else:\n            hlen_b4_cell = 0\n        # exit_now is set by a call to %Exit or %Quit, through the\n        # ask_exit callback.\n\n        while not self.exit_now:\n            self.hooks.pre_prompt_hook()\n            if more:\n                try:\n                    prompt = self.prompt_manager.render('in2')\n                except:\n                    self.showtraceback()\n                if self.autoindent:\n                    self.rl_do_indent = True\n\n            else:\n                try:\n                    prompt = self.separate_in + self.prompt_manager.render('in')\n                except:\n                    self.showtraceback()\n            try:\n                line = self.raw_input(prompt)\n                if self.exit_now:\n                    # quick exit on sys.std[in|out] close\n                    break\n                if self.autoindent:\n                    self.rl_do_indent = False\n\n            except KeyboardInterrupt:\n                #double-guard against keyboardinterrupts during kbdint handling\n                try:\n                    self.write('\\nKeyboardInterrupt\\n')\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n                    more = False\n                except KeyboardInterrupt:\n                    pass\n            except EOFError:\n                if self.autoindent:\n                    self.rl_do_indent = False\n                    if self.has_readline:\n                        self.readline_startup_hook(None)\n                self.write('\\n')\n                self.exit()\n            except bdb.BdbQuit:\n                warn('The Python debugger has exited with a BdbQuit exception.\\n'\n                     'Because of how pdb handles the stack, it is impossible\\n'\n                     'for IPython to properly format this particular exception.\\n'\n                     'IPython will resume normal operation.')\n            except:\n                # exceptions here are VERY RARE, but they can be triggered\n                # asynchronously by signal handlers, for example.\n                self.showtraceback()\n            else:\n                self.input_splitter.push(line)\n                more = self.input_splitter.push_accepts_more()\n                if (self.SyntaxTB.last_syntax_error and\n                    self.autoedit_syntax):\n                    self.edit_syntax_error()\n                if not more:\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    self.run_cell(source_raw, store_history=True)\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n\n        # Turn off the exit flag, so the mainloop can be restarted if desired\n        self.exit_now = False"}
{"prompt": "async def add(self, dialog: Dialog):\n        \"\"\"\n        Adds a new dialog to the set and returns the added dialog.\n        :param dialog: The dialog to add.\n        \"\"\"", "output": "\n        self.dialogs.add(dialog)\n        return dialog\n\n    async def get_dialog(self, dialog_id: int) -> Dialog:\n        \"\"\"\n        Returns the dialog with the given ID.\n        :param dialog_id: The ID of the dialog to get.\n        :return: The dialog with the given ID.\n        \"\"\"\n        return self.dialogs.get(dialog_id)\n\n    async def remove_dialog(self, dialog: Dialog):\n        \"\"\"\n        Removes the dialog from the set and returns", "solution": "\n        if dialog is None or not isinstance(dialog, Dialog):\n            raise TypeError('DialogSet.add(): dialog cannot be None and must be a Dialog or derived class.')\n\n        if dialog.id in self._dialogs:\n            raise TypeError(\"DialogSet.add(): A dialog with an id of '%s' already added.\" % dialog.id)\n\n        # dialog.telemetry_client = this._telemetry_client;\n        self._dialogs[dialog.id] = dialog\n\n        return self"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.\n\n        Subclasses may override this method to alter loop functionality.\n        \"\"\"", "output": "\n        while True:\n            event = self.onEvent()\n            if event is not None:\n                yield event\n\n    def onEvent(self):\n        \"\"\"\n        Subclasses may override this method to handle events from Skype.\n\n        Returns None if no events are expected from Skype.\n        \"\"\"\n        return None\n\n", "solution": "\n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()"}
{"prompt": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"", "output": "\n  if validate_args:\n    return perm\n  else:\n    if name is None:\n      raise ValueError(\"If `validate_args` is False, `name` must be provided.\")\n    else:\n      raise ValueError(f\"If `validate_args` is False, `name` must be None.\")\n\n", "solution": "\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions"}
{"prompt": "def create_cookie(self, delete=None):\n        \"\"\"\n        Creates the value for ``Set-Cookie`` HTTP header.\n\n        :param bool delete:\n            If ``True`` the cookie value will be ``deleted`` and the\n            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.\n\n        \"\"\"", "output": "\n        if delete is None:\n            delete = self.delete\n\n        cookie_value = '; '.join([\n            self.key,\n            self.value,\n            self.version,\n            self.expires,\n            self.max_age,\n            self.domain,\n            self.path,\n            self.secure,\n            self.httponly,\n            self.samesite,\n        ])\n\n        if delete:\n            cookie_value += '; Expires=Thu, 01-Jan-1970 00:00:01 GMT'\n\n        return cookie", "solution": "\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        # Work-around for issue #11, failure of WebKit-based browsers to accept\n        # cookies set as part of a redirect response in some circumstances.\n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )"}
{"prompt": "def derivative(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the numerical derivative of a waveform's dependent variable vector.\n\n    The method used is the `backwards differences\n    <https://en.wikipedia.org/wiki/\n    Finite_difference#Forward.2C_backward.2C_and_central_differences>`_ method\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.derivative\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"", "output": ": Minimum value of independent variable to calculate derivative for (default is None)\n    :type  indep_min: float\n\n    :param indep_max: Maximum value of independent variable to calculate derivative for (default is None)\n    :type  indep_max: float\n\n    :return: Numerical derivative of waveform's dependent variable vector\n    :rtype: :py:class:`peng.Waveform`\n    \"\"\"\n    if indep_min is None:\n        indep_min = wave.min()\n    if indep_max is None:", "solution": "\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_indep = np.diff(ret._indep_vector)\n    delta_dep = np.diff(ret._dep_vector)\n    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))\n    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))\n    ret._dep_vector = np.divide(delta_dep, delta_indep)\n    ret.dep_name = \"derivative({0})\".format(ret._dep_name)\n    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, \"/\")\n    return ret"}
{"prompt": "def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"", "output": "\n        if isinstance(target_arn, TopicArn):\n            self.client.publish(TargetArn=target_arn, Message=message)\n        elif isinstance(target_arn, EndpointArn):\n            self.client.publish(TargetArn=target_arn, Message=message)\n        else:\n            raise ValueError(\"Invalid target_arn type. Must be a TopicArn or an EndpointArn.\")\n\n    def get_message_count(self, topic_or_endpoint):\n        \"\"\"\n        Returns the number of", "solution": "\n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )"}
{"prompt": "def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_delete_permission, but also passes the obj parameter in.\n        \"\"\"", "output": "\n        if obj is None:\n            return self.has_delete_permission(request)\n        else:\n            return self.has_delete_permission(request, obj)\n\n    def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_delete_permission, but also passes the obj parameter in.\n        \"\"\"\n        if obj is None:\n            return False\n        else:\n            return obj.user.is_authenticated and obj", "solution": "\n        if settings.TREE_EDITOR_OBJECT_PERMISSIONS:\n            opts = self.opts\n            r = request.user.has_perm(opts.app_label + '.' + opts.get_delete_permission(), obj)\n        else:\n            r = True\n\n        return r and super(TreeEditor, self).has_delete_permission(request, obj)"}
{"prompt": "def _clean(self, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Remove all signals from this interface (used after unit is synthesized\n        and its parent is connecting its interface to this unit)\n        \"\"\"", "output": "\n        if rmConnetions:\n            self.connetions = []\n        if lockNonExternal:\n            self.external = False\n\n    def connect(self, unit):\n        \"\"\"\n        Connect this interface to the given unit (which is assumed to be connected\n        by a signal)\n        \"\"\"\n        self.connetions.append(unit)\n\n    def disconnect(self, unit):\n        \"\"\"\n        Disconnect this interface from the given unit (which is assumed to be\n        connected by a signal)\n        \"\"\"\n        self.connetions.remove(", "solution": "\n\n        if self._interfaces:\n            for i in self._interfaces:\n                i._clean(rmConnetions=rmConnetions,\n                         lockNonExternal=lockNonExternal)\n        else:\n            self._sigInside = self._sig\n            del self._sig\n\n        if lockNonExternal and not self._isExtern:\n            self._isAccessible = False"}
{"prompt": "def bytesize(self, byteorder='@'):\n        \"\"\"Compute the byte size after serialization.\n\n        Args:\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@``, ``=``, ``<``, ``>``, and ``!``.\n                Default is ``@`` -- the native order.\n\n        Returns:\n            int: Size in number of bytes after serialization.\n        \"\"\"", "output": ":\n            int: The byte size of the serialized data.\n\n        \"\"\"\n        if byteorder == '@':\n            return len(self.data)\n        elif byteorder == '=':\n            return len(self.data.encode('utf-8'))\n        elif byteorder == '<':\n            return len(self.data.encode('utf-16be'))\n        elif byteorder == '>':\n            return len(self.data.encode('utf-16le'))\n        elif byteorder == '!':", "solution": "\n        # Use 8 bytes to store the seed integer\n        seed_size = struct.calcsize(byteorder+'q')\n        # Use 4 bytes to store the number of hash values\n        length_size = struct.calcsize(byteorder+'i')\n        # Use 4 bytes to store each hash value as we are using the lower 32 bit\n        hashvalue_size = struct.calcsize(byteorder+'I')\n        return seed_size + length_size + len(self) * hashvalue_size"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a FTP connection object\n        \"\"\"", "output": "\n        return ftplib.FTP('ftp.example.com', 'username', 'password')\n\n    def upload_file(self, local_file, remote_file):\n        \"\"\"\n        Uploads a file from the local file path to the remote server using FTP.\n\n        Args:\n            local_file (str): The path to the local file to be uploaded.\n            remote_file (str): The path to the remote server to upload the file to.\n\n        Returns:\n            bool: True if the upload was successful, False otherwise.\n        \"\"\"\n        try:", "solution": "\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn"}
{"prompt": "def commonprefix(items):\n    \"\"\"Get common prefix for completions\n\n    Return the longest common prefix of a list of strings, but with special\n    treatment of escape characters that might precede commands in IPython,\n    such as %magic functions. Used in tab completion.\n\n    For a more general function, see os.path.commonprefix\n    \"\"\"", "output": "\n    if not items:\n        return \"\"\n\n    # Get the shortest item in the list\n    shortest = min(items, key=len)\n\n    # Iterate over the characters in the shortest item\n    for i, char in enumerate(shortest):\n        # If the character is a backslash, check if the previous character\n        # is a backslash and skip it if it is\n        if char == \"\\\\\":\n            if i > 0 and items[0][i-1] == \"\\\\\":\n                continue\n        # If the character is a tab", "solution": "\n    # the last item will always have the least leading % symbol\n    # min / max are first/last in alphabetical order\n    first_match  = ESCAPE_RE.match(min(items))\n    last_match  = ESCAPE_RE.match(max(items))\n    # common suffix is (common prefix of reversed items) reversed\n    if first_match and last_match:\n        prefix = os.path.commonprefix((first_match.group(0)[::-1], last_match.group(0)[::-1]))[::-1]\n    else:\n        prefix = ''\n\n    items = [s.lstrip(ESCAPE_CHARS) for s in items]\n    return prefix+os.path.commonprefix(items)"}
{"prompt": "def _graph_wrap(func, graph):\n    \"\"\"Constructs function encapsulated in the graph.\"\"\"", "output": "\n    def wrapped_func(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return wrapped_func\n\n", "solution": "\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped"}
{"prompt": "def avail_archs(self):\n        \"\"\" Initialize the dictionary of architectures for disassembling via capstone\"\"\"", "output": "\n        self.architectures = {}\n        for arch in capstone.Cs(capstone.CS_ARCH_X86):\n            self.architectures[arch.name] = []\n\n    def disassemble_arch(self, arch: capstone.Cs):\n        \"\"\" Disassemble an instruction in a given architecture\"\"\"\n        # Implementation of disassembling goes here\n        pass\n\n    def add_arch(self, arch: capstone.Cs):\n        \"\"\" Add an architecture to the dictionary of available architectures\"\"\"\n        self.architectures[arch.name]", "solution": "\n\n        return {\n            ARM32:   (CS_ARCH_ARM,   CS_MODE_ARM),\n            ARM64:   (CS_ARCH_ARM64, CS_MODE_LITTLE_ENDIAN),\n            ARM_TB:  (CS_ARCH_ARM,   CS_MODE_THUMB),\n            MIPS32:  (CS_ARCH_MIPS,  CS_MODE_MIPS32),\n            MIPS64:  (CS_ARCH_MIPS,  CS_MODE_MIPS64),\n            SPARC32: (CS_ARCH_SPARC, CS_MODE_BIG_ENDIAN),\n            SPARC64: (CS_ARCH_SPARC, CS_MODE_V9),\n            SYSTEMZ: (CS_ARCH_SYSZ,  CS_MODE_BIG_ENDIAN),\n            X86_16:  (CS_ARCH_X86,   CS_MODE_16),\n            X86_32:  (CS_ARCH_X86,   CS_MODE_32),\n            X86_64:  (CS_ARCH_X86,   CS_MODE_64),\n        }"}
{"prompt": "def html_to_xhtml(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n    \"\"\"", "output": "\n    from html.parser import HTMLParser\n\n    class XHTMLParser(HTMLParser):\n        def __init__(self):\n            super().__init__()\n            self.stack = []\n\n        def handle_starttag(self, tag, attrs):\n            self.stack.append(f\"<{tag}\")\n\n        def handle_endtag(self, tag):\n            self.stack.append(f\"</{tag}\")\n\n        def handle_data(self, data):\n            self.stack.append(data)\n\n        def handle_entityref", "solution": "\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag"}
{"prompt": "def p_gate_op_2(self, program):\n        \"\"\"\n        gate_op : id id_list ';'\n        \"\"\"", "output": "\n        gate_name = program[1]\n        operands = program[3:]\n        self.gates[gate_name] = (operands, None)\n\n    def p_id_list(self, program):\n        \"\"\"\n        id_list : id\n        \"\"\"\n        self.ids.append(program[1])\n\n    def p_id(self, program):\n        \"\"\"\n        id : NUMBER\n        \"\"\"\n        self.ids.append(program[1])\n\n    def p_number(self, program):\n        \"\"\"\n        number :", "solution": "\n        program[0] = node.CustomUnitary([program[1], program[2]])\n        # To verify:\n        # 1. id is declared as a gate in global scope\n        # 2. everything in the id_list is declared as a bit in local scope\n        self.verify_as_gate(program[1], program[2])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def insert_and_get(self, **fields):\n        \"\"\"Creates a new record in the database and then gets\n        the entire row.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            fields:\n                The fields of the row to create.\n\n        Returns:\n            The model instance representing the row that was created.\n        \"\"\"", "output": "\n        instance = self.model(**fields)\n        instance.save()\n        return instance\n\n    def on_conflict(self, *args, **kwargs):\n        \"\"\"Called when a conflict occurs in the database.\n\n        This can be used to perform custom actions, such as updating\n        the fields of an existing record or creating a new record.\n\n        Arguments:\n            *args:\n                The arguments passed to the function.\n            **kwargs:\n                The keyword arguments passed to the function.\n\n        Returns:\n            None\n        \"\"\"\n        # Replace", "solution": "\n\n        if not self.conflict_target and not self.conflict_action:\n            # no special action required, use the standard Django create(..)\n            return super().create(**fields)\n\n        compiler = self._build_insert_compiler([fields])\n        rows = compiler.execute_sql(return_id=False)\n\n        columns = rows[0]\n\n        # get a list of columns that are officially part of the model and preserve the fact that the attribute name\n        # might be different than the database column name\n        model_columns = {}\n        for field in self.model._meta.local_concrete_fields:\n            model_columns[field.column] = field.attname\n\n        # strip out any columns/fields returned by the db that\n        # are not present in the model\n        model_init_fields = {}\n        for column_name, column_value in columns.items():\n            try:\n                model_init_fields[model_columns[column_name]] = column_value\n            except KeyError:\n                pass\n\n        return self.model(**model_init_fields)"}
{"prompt": "def strip_email_quotes(text):\n    \"\"\"Strip leading email quotation characters ('>').\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines of the input text.\n\n    Parameters\n    ----------\n    text : str\n\n    Examples\n    --------\n\n    Simple uses::\n\n        In [2]: strip_email_quotes('> > text')\n        Out[2]: 'text'\n\n        In [3]: strip_email_quotes('> > text\\\\n> > more')\n        Out[3]: 'text\\\\nmore'\n\n    Note how only the common prefix that appears in all lines is stripped::\n\n        In [4]: strip_email_quotes('> > text\\\\n> > more\\\\n> more...')\n        Out[4]: '> text\\\\n> more\\\\nmore...'\n\n    So if any line has no quote marks ('>') , then none are stripped from any\n    of them ::\n    \n        In [5]: strip_email_quotes('> > text\\\\n> > more\\\\nlast different')\n        Out[5]: '> > text\\\\n> > more\\\\nlast different'\n    \"\"\"", "output": "Out[3]:'more'\n\n    Combining lines with different quotes::\n\n        In [4]: strip_email_quotes('> > \"text\"\\\\n> > more\"')\n        Out[4]: 'textmore'\n\n    \"\"\"\n    lines = text.split('\\n')\n    stripped_lines = []\n    for line in lines:\n        stripped_line = line.strip()\n        if stripped_line.startswith(\"> \"):\n            stripped_line = stripped_line[2:]\n        stripped_lines.append(stri", "solution": "\n    lines = text.splitlines()\n    matches = set()\n    for line in lines:\n        prefix = re.match(r'^(\\s*>[ >]*)', line)\n        if prefix:\n            matches.add(prefix.group(1))\n        else:\n            break\n    else:\n        prefix = long_substr(list(matches))\n        if prefix:\n            strip = len(prefix)\n            text = '\\n'.join([ ln[strip:] for ln in lines])\n    return text"}
{"prompt": "def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        \"\"\"\n        Deletes the specified VM Image from the image repository that is\n        associated with the specified subscription.\n\n        vm_image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        \"\"\"", "output": "\n        # Delete the VM Image from the image repository\n        # associated with the specified subscription\n        pass\n\n    def list_subscriptions(self):\n        \"\"\"\n        Returns a list of all the subscriptions associated with the\n        specified account.\n        \"\"\"\n        # Retrieve a list of all the subscriptions associated\n        # with the specified account\n        pass\n\n    def list_images(self):\n        \"\"\"\n        Returns a list of all the images associated with the specified\n        account.\n        \"\"\"\n        # Retrieve a list of all the images associated with\n        # the specified", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)"}
{"prompt": "def _add_column_kwargs(self, kwargs, column):\n        \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\n        `Column <sqlalchemy.schema.Column>`.\n        \"\"\"", "output": "\n        for key, value in column.kwargs.items():\n            if key in kwargs:\n                kwargs[key].append(value)\n            else:\n                kwargs[key] = [value]\n\n    def _add_column(self, column):\n        \"\"\"Add a `Column` object to the database session.\"\"\"\n        self.session.add(column)\n\n    def _commit_transaction(self):\n        \"\"\"Commit the current transaction to the database.\"\"\"\n        self.session.commit()\n\n    def _rollback_transaction(", "solution": "\n        if column.nullable:\n            kwargs[\"allow_none\"] = True\n        kwargs[\"required\"] = not column.nullable and not _has_default(column)\n\n        if hasattr(column.type, \"enums\"):\n            kwargs[\"validate\"].append(validate.OneOf(choices=column.type.enums))\n\n        # Add a length validator if a max length is set on the column\n        # Skip UUID columns\n        # (see https://github.com/marshmallow-code/marshmallow-sqlalchemy/issues/54)\n        if hasattr(column.type, \"length\"):\n            try:\n                python_type = column.type.python_type\n            except (AttributeError, NotImplementedError):\n                python_type = None\n            if not python_type or not issubclass(python_type, uuid.UUID):\n                kwargs[\"validate\"].append(validate.Length(max=column.type.length))\n\n        if hasattr(column.type, \"scale\"):\n            kwargs[\"places\"] = getattr(column.type, \"scale\", None)"}
{"prompt": "def check_for_old_config(ipython_dir=None):\n    \"\"\"Check for old config files, and present a warning if they exist.\n\n    A link to the docs of the new config is included in the message.\n\n    This should mitigate confusion with the transition to the new\n    config system in 0.11.\n    \"\"\"", "output": "\n    if ipython_dir is None:\n        ipython_dir = Path.home() / \".ipython\"\n\n    if not ipython_dir.exists():\n        raise ValueError(f\"{ipython_dir} does not exist.\")\n\n    old_config_files = [\n        \"jupyter_notebook.ipynb_checkpoints\",\n        \"jupyter_notebook.ipynb_kernels\",\n        \"jupyter_notebook.ipynb_magics\",\n    ]\n\n    for file in old", "solution": "\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n\n    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']\n    warned = False\n    for cfg in old_configs:\n        f = os.path.join(ipython_dir, cfg)\n        if os.path.exists(f):\n            if filehash(f) == old_config_md5.get(cfg, ''):\n                os.unlink(f)\n            else:\n                warnings.warn(\"Found old IPython config file %r (modified by user)\"%f)\n                warned = True\n\n    if warned:\n        warnings.warn("}
{"prompt": "def getField(self, fld_name):\n        \"\"\" Return :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n\n        Returns:\n            str: String value (scaled if numeric) for the field.\n        \"\"\"", "output": "\n        field_content = getattr(self, fld_name)\n        if isinstance(field_content, (int, float)):\n            return str(field_content)\n        else:\n            return field_content\n\n    def setField(self, fld_name, value):\n        \"\"\" Set :class:`~ekmmeters.Field` content and scale it.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n            value (str): A string value (", "solution": "\n        result = \"\"\n        if fld_name in self.m_req:\n            result = self.m_req[fld_name][MeterData.StringValue]\n        else:\n            ekm_log(\"Requested nonexistent field: \" + fld_name)\n\n        return result"}
{"prompt": "def _mk_index(self):\n        \"\"\"create the index for this set\"\"\"", "output": "\n        self.index = {}\n        for item in self.items:\n            for i, subitem in enumerate(item):\n                if subitem not in self.index:\n                    self.index[subitem] = []\n                self.index[subitem].append((i, item))\n\n    def get_index(self, item):\n        \"\"\"return the index of the item in the index dictionary\"\"\"\n        return self.index.get(item, [])\n\n    def get_items(self, index):\n        \"\"\"return the items at the given index\"\"\"\n        ", "solution": "\n        index = defaultdict(list)\n        for line_no, line in enumerate(self._stripped_lines):\n            if line:\n                index[line].append(line_no)\n        return index"}
{"prompt": "def _find_blob_start(self):\n        \"\"\"Find first blob from selection.\n        \"\"\"", "output": "\n        for i, blob in enumerate(self.blobs):\n            if blob.selected:\n                return i\n        return -1\n\n    def _find_blob_end(self):\n        \"\"\"Find last blob from selection.\n        \"\"\"\n        for i in range(len(self.blobs)-1, -1, -1):\n            if self.blobs[i].selected:\n                return i\n        return -1\n\n    def _find_blob_center(self):\n        \"\"\"Find center point of blob.\n        \"\"\"\n        x_", "solution": "\n\n        # Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        # Check which is the blob time offset\n        blob_time_start = self.t_start\n\n        # Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx\n\n        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start\n\n        return blob_start"}
{"prompt": "def update(self, ava):\n        \"\"\"\n        Implements the dict.update() method\n        \"\"\"", "output": "\n        for key, value in ava.items():\n            self[key] = value\n\n    def __getitem__(self, key):\n        \"\"\"\n        Implements the dict.__getitem__() method\n        \"\"\"\n        return super().__getitem__(key)\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Implements the dict.__setitem__() method\n        \"\"\"\n        super().__setitem__(key, value)\n\n    def __delitem__(self, key):\n        \"\"\"\n        ", "solution": "\n        for key, val in ava.items():\n            self[key] = val"}
{"prompt": "def multiqc(institute_id, case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"", "output": "\n    # Load multiqc report for the case\n    report = load_report(institute_id, case_name)\n    \n    # Filter reports to only include multiqc cases\n    multiqc_reports = [report for report in report if report['severity'] =='multiqc']\n    \n    return multiqc_reports\n\n", "solution": "\n    data = controllers.multiqc(store, institute_id, case_name)\n    if data['case'].get('multiqc') is None:\n        return abort(404)\n    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))\n    filename = os.path.basename(data['case']['multiqc'])\n    return send_from_directory(out_dir, filename)"}
{"prompt": "def get_user_token(user, purpose, minutes_valid):\n    \"\"\"Return login token info for given user.\"\"\"", "output": "\n    # Replace with actual code to get token\n    token = \"1234567890abcdef\"\n    return token\n\n", "solution": "\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }"}
{"prompt": "def write_meta(self):\n        \"\"\"ucds, descriptions and units are written as attributes in the hdf5 file, instead of a seperate file as\n         the default :func:`Dataset.write_meta`.\n         \"\"\"", "output": "\n        self.hdf5_file.attrs['ucds'] = self.ucds\n        self.hdf5_file.attrs['descriptions'] = self.descriptions\n        self.hdf5_file.attrs['units'] = self.units\n\n    def write_data(self):\n        \"\"\"The data is written to the hdf5 file as a group. This function should be implemented by the child\n        class.\n        \"\"\"\n        raise NotImplementedError(\"write_data() must be implemented by the child class\")\n\n    def", "solution": "\n        with h5py.File(self.filename, \"r+\") as h5file_output:\n            h5table_root = h5file_output[self.h5table_root_name]\n            if self.description is not None:\n                h5table_root.attrs[\"description\"] = self.description\n            h5columns = h5table_root if self._version == 1 else h5table_root['columns']\n            for column_name in self.columns.keys():\n                h5dataset = None\n                if column_name in h5columns:\n                    h5dataset = h5columns[column_name]\n                else:\n                    for group in h5columns.values():\n                        if 'type' in group.attrs:\n                            if group.attrs['type'] in ['csr_matrix']: \n                                for name, column in group.items():\n                                    if name == column_name:\n                                        h5dataset = column\n                if h5dataset is None:\n                    raise ValueError('column {} not found'.format(column_name))\n                for name, values in [(\"ucd\", self.ucds), (\"unit\", self.units), (\"description\", self.descriptions)]:\n                    if column_name in values:\n                        value = ensure_string(values[column_name], cast=True)\n                        h5dataset.attrs[name] = value\n                    else:\n                        if name in h5columns.attrs:\n                            del h5dataset.attrs[name]"}
{"prompt": "def rename(self, columns=None):\n        \"\"\"\n        Change names of columns in the frame.\n\n        Dict key is an index or name of the column whose name is to be set.\n        Dict value is the new name of the column.\n\n        :param columns: dict-like transformations to apply to the column names\n        \"\"\"", "output": "\n        if columns is None:\n            return self\n        if not isinstance(columns, dict):\n            raise TypeError(\"columns must be a dict-like object\")\n        for name, new_name in columns.items():\n            if not isinstance(name, str):\n                raise ValueError(\"keys of columns must be strings\")\n            if not isinstance(new_name, str):\n                raise ValueError(\"values of columns must be strings\")\n        return self.copy(deep=True).rename(columns=columns)\n\n    def drop(self", "solution": "\n        assert_is_type(columns, None, dict)\n        new_names = self.names\n        ncols = self.ncols\n\n        for col, name in columns.items():\n            col_index = None\n            if is_type(col, int) and (-ncols <= col < ncols):\n                col_index = (col + ncols) % ncols  # handle negative indices\n            elif is_type(col, str) and col in self.names:\n                col_index = self.names.index(col)  # lookup the name\n\n            if col_index is not None:\n                new_names[col_index] = name\n\n        return self.set_names(new_names)"}
{"prompt": "def get_root_schema_dict(record):\n    \"\"\"Return a root jsonschema for a given record\n\n    A root schema includes the $schema attribute and all sub-record\n    schemas and definitions.\n    \"\"\"", "output": "\n    root_schema = {\n        \"$schema\": \"http://json-schema.org/schema#\",\n        \"type\": \"object\",\n        \"properties\": {},\n        \"required\": []\n    }\n\n    def add_property(name, subschema):\n        root_schema[\"properties\"][name] = subschema\n\n    def add_subrecord(name, subschema):\n        if \"type\" in subschema:\n            root_schema[\"required\"].append(name)\n        add_property(name", "solution": "\n    state = SchemaGeneratorState()\n    schema = get_schema_dict(record, state)\n    del state.record_schemas[record._schema_name]\n    if state.record_schemas:\n        schema['definitions'] = dict()\n        for name, sub_schema in state.record_schemas.iteritems():\n            schema['definitions'][name] = sub_schema\n    return schema"}
{"prompt": "def _check_multi_statement_line(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"", "output": "\n        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n            return True\n        elif isinstance(node, ast.Assign) and len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n            return True\n        elif isinstance(node, ast.AugAssign) and isinstance(node.target, ast.Name) and isinstance(node.value, ast.Str):\n            return True\n        elif isinstance(node, ast.", "solution": "\n        # Do not warn about multiple nested context managers\n        # in with statements.\n        if isinstance(node, nodes.With):\n            return\n        # For try... except... finally..., the two nodes\n        # appear to be on the same line due to how the AST is built.\n        if isinstance(node, nodes.TryExcept) and isinstance(\n            node.parent, nodes.TryFinally\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.If)\n            and not node.parent.orelse\n            and self.config.single_line_if_stmt\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.ClassDef)\n            and len(node.parent.body) == 1\n            and self.config.single_line_class_stmt\n        ):\n            return\n        self.add_message(\"multiple-statements\", node=node)\n        self._visited_lines[line] = 2"}
{"prompt": "def _validate(value, optdict, name=\"\"):\n    \"\"\"return a validated value for an option according to its type\n\n    optional argument name is only used for error message formatting\n    \"\"\"", "output": "\n    if value is None:\n        if name:\n            raise ValueError(\"Option '{}' is required but not provided\".format(name))\n        return None\n    if isinstance(value, bool):\n        return str(value).lower()\n    try:\n        return optdict[type(value).__name__](value)\n    except KeyError:\n        raise ValueError(\"Option '{}' is not a valid type for value '{}'\".format(name, type(value).__name__))\n\n", "solution": "\n    try:\n        _type = optdict[\"type\"]\n    except KeyError:\n        # FIXME\n        return value\n    return _call_validator(_type, optdict, name, value)"}
{"prompt": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"", "output": "response': True, 'timeout': 60}\n        :type extra_options: dict\n        :return: the response object\n        :rtype: requests.models.Response\n        \"\"\"\n        if extra_options is None:\n            extra_options = {}\n\n        if headers is None:\n            headers = {}\n\n        if data is None:\n            data = {}\n\n        if 'timeout' not in extra_options:\n            extra_options['timeout'] = 60\n\n        if'verify' not in extra_options:\n            extra_options['verify'] = True\n\n        ", "solution": "\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)"}
{"prompt": "def print_annotation(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"", "output": "\n        for key, value in self.annotations.items():\n            print(f\"{key}: {value}\")\n\n", "solution": "\n        for path, ann in self.annotation.items():\n            print(\"{}: {}\".format(path, ann['value']))"}
{"prompt": "def info(self, **kwargs):\n        \"\"\"\n        Get the detailed information about a particular credit record. This is \n        currently only supported with the new credit model found in TV. These \n        ids can be found from any TV credit response as well as the tv_credits \n        and combined_credits methods for people.\n\n        The episodes object returns a list of episodes and are generally going \n        to be guest stars. The season array will return a list of season \n        numbers.  Season credits are credits that were marked with the \n        \"add to every season\" option in the editing interface and are \n        assumed to be \"season regulars\".\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\" or \"add to every second\" feature.\n\n        Returns:\n            A dictionary containing the requested information.\n        \"\"\"\n        if not hasattr(self, '_id'):\n            raise ValueError(\"This method only works with new credit models.\")\n        if not hasattr(self, 'episodes'):\n            raise ValueError(\"This method only works with new credit models.\")\n        if not hasattr(self,'season_array'):\n            raise ValueError(\"This method only works with new credit models.\")\n\n        episode = self.episodes.get(self._id", "solution": "\n        path = self._get_credit_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def parse_options():\n    \"\"\"\n    Commandline options arguments parsing.\n\n    :return: parsed commandline arguments.\n    :rtype: optparse.Values.\n    \"\"\"", "output": "\n    parser = optparse.OptionParser()\n    parser.add_option(\"-f\", \"--file\", dest=\"file\", help=\"specify a file\")\n    parser.add_option(\"-o\", \"--output\", dest=\"output\", help=\"specify an output file\")\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\", dest=\"verbose\", help=\"print verbose output\")\n    return parser.parse_args()\n\n", "solution": "\n\n    version = \"%%prog {version}\".format(version=__version__)\n    parser = OptionParser(version=version)\n    parser.add_option(\n        \"-s\", \"--server\", action=\"store\", dest=\"server\",\n        type=\"string\", default=\"\", metavar=\"SERVER\",\n        help=\"server name, IP address or unix socket path\"\n    )\n    parser.add_option(\n        \"-p\", \"--port\", action=\"store\", type=\"int\", dest=\"port\",\n        default=9001, metavar=\"PORT\", help=\"port number\"\n    )\n    parser.add_option(\n        \"-P\", \"--programs\", action=\"store\", dest=\"programs\", type=\"string\", default=\"\",\n        metavar=\"PROGRAMS\", help=\"comma separated programs list, or empty for all programs in supervisord response\"\n    )\n    parser.add_option(\n        \"-u\", \"--username\", action=\"store\", dest=\"username\", type=\"string\", default=\"\",\n        metavar=\"USERNAME\", help=\"supervisord user\"\n    )\n    parser.add_option(\n        \"-S\", \"--password\", action=\"store\", dest=\"password\", type=\"string\", default=\"\",\n        metavar=\"PASSWORD\", help=\"supervisord user password\"\n    )\n    parser.add_option(\n        \"-q\", \"--quiet\", metavar=\"QUIET\", action=\"store_true\", default=False, dest=\"quiet\", help=\"be quiet\"\n    )\n    parser.add_option(\n        \"--stopped-state\", action=\"store\", dest=\"stopped_state\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,\n        metavar=\"STOPPED_STATE\", help=\"stopped state\"\n    )\n    parser.add_option(\n        \"--network-errors-exit-code\", action=\"store\", dest=\"network_errors_exit_code\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,\n        metavar=\"NETWORK_ERRORS_EXIT_CODE\", help=\"network errors exit code\"\n    )\n\n    options = parser.parse_args(sys.argv)[0]\n    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument\n\n    # check mandatory command line options supplied\n    if not options.server:\n        parser.error(\"Required server address option missing\")\n    if options.username and not options.password:\n        parser.error(\"Required supervisord user password\")\n\n    return options"}
{"prompt": "def _create_hstore_unique(self, model, field, keys):\n        \"\"\"Creates a UNIQUE constraint for the specified hstore keys.\"\"\"", "output": "\n        hstore = model.hstore()\n        for key in keys:\n            hstore.add_key_if_not_exists(field, key)\n        return hstore.get_constraint(field + '_unique')\n\n    def _create_hstore_distinct(self, model, field, keys):\n        \"\"\"Creates a DISTINCT constraint for the specified hstore keys.\"\"\"\n        hstore = model.hstore()\n        for key in keys:\n            hstore.add_key_if_not_exists(field", "solution": "\n\n        name = self._unique_constraint_name(\n            model._meta.db_table, field, keys)\n        columns = [\n            '(%s->\\'%s\\')' % (field.column, key)\n            for key in keys\n        ]\n        sql = self.sql_hstore_unique_create.format(\n            name=self.quote_name(name),\n            table=self.quote_name(model._meta.db_table),\n            columns=','.join(columns)\n        )\n        self.execute(sql)"}
{"prompt": "def create_trace(\n        turn_activity: Activity,\n        name: str,\n        value: object = None,\n        value_type: str = None,\n        label: str = None,\n    ) -> Activity:\n        \"\"\"Creates a trace activity based on this activity.\n\n        :param turn_activity:\n        :type turn_activity: Activity\n        :param name: The value to assign to the trace activity's <see cref=\"Activity.name\"/> property.\n        :type name: str\n        :param value: The value to assign to the trace activity's <see cref=\"Activity.value\"/> property., defaults to None\n        :param value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property, defaults to None\n        :param value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :param label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"", "output": "'s <see cref=\"Activity.value\"/> property.\n        :type value: object\n        :param value_type: The type of the value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property.\n        :type value_type: str\n        :param label: The label to assign to the trace activity.\n        :type label: str\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"\n        activity = Activity(\n            name=name,\n            value=value,\n            value_type=value", "solution": "\n\n        from_property = (\n            ChannelAccount(\n                id=turn_activity.recipient.id, name=turn_activity.recipient.name\n            )\n            if turn_activity.recipient is not None\n            else ChannelAccount()\n        )\n        if value_type is None and value is not None:\n            value_type = type(value).__name__\n\n        reply = Activity(\n            type=ActivityTypes.trace,\n            timestamp=datetime.utcnow(),\n            from_property=from_property,\n            recipient=ChannelAccount(\n                id=turn_activity.from_property.id, name=turn_activity.from_property.name\n            ),\n            reply_to_id=turn_activity.id,\n            service_url=turn_activity.service_url,\n            channel_id=turn_activity.channel_id,\n            conversation=ConversationAccount(\n                is_group=turn_activity.conversation.is_group,\n                id=turn_activity.conversation.id,\n                name=turn_activity.conversation.name,\n            ),\n            name=name,\n            label=label,\n            value_type=value_type,\n            value=value,\n        )\n        return reply"}
{"prompt": "def voucher_code(request):\n    \"\"\" A view *just* for entering a voucher form. \"\"\"", "output": "\n    # Check if the request is valid\n    if not request.is_valid():\n        return HttpResponseBadRequest()\n\n    # Check if the voucher code already exists\n    voucher_codes = VoucherCode.objects.all()\n    if request.POST.get('voucher_code') in voucher_codes:\n        return HttpResponseBadRequest()\n\n    # Create a new voucher code\n    voucher_code = VoucherCode(\n        name=request.POST['name'],\n        description=request.POST['description'],\n", "solution": "\n\n    VOUCHERS_FORM_PREFIX = \"vouchers\"\n\n    # Handle the voucher form *before* listing products.\n    # Products can change as vouchers are entered.\n    v = _handle_voucher(request, VOUCHERS_FORM_PREFIX)\n    voucher_form, voucher_handled = v\n\n    if voucher_handled:\n        messages.success(request, \"Your voucher code was accepted.\")\n        return redirect(\"dashboard\")\n\n    data = {\n        \"voucher_form\": voucher_form,\n    }\n\n    return render(request, \"registrasion/voucher_code.html\", data)"}
{"prompt": "def create(cls, name_value, name_type):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"", "output": "\n        return cls(name_value, name_type)\n\n    def __init__(self, name_value, name_type):\n        self.name_value = name_value\n        self.name_type = name_type\n\n", "solution": "\n        if isinstance(name_value, Name.NameValue):\n            value = name_value\n        elif isinstance(name_value, str):\n            value = cls.NameValue(name_value)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_value'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_value', type(Name.NameValue),\n                                       type(name_value)))\n\n        if isinstance(name_type, Name.NameType):\n            n_type = name_type\n        elif isinstance(name_type, Enum):\n            n_type = cls.NameType(name_type)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_type'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_type', type(Name.NameType),\n                                       type(name_type)))\n\n        return Name(name_value=value,\n                    name_type=n_type)"}
{"prompt": "def loop_gtk(kernel):\n    \"\"\"Start the kernel, coordinating with the GTK event loop\"\"\"", "output": "\n    import gi\n    gi.require_version('Gtk', '3.0')\n    from gi.repository import Gtk\n    \n    # Create the GTK window\n    window = Gtk.Window(title=\"Hello World\")\n    window.set_size_request(200, 200)\n    window.set_position(Gtk.WindowPosition.CENTER)\n    \n    # Add a button to the window\n    button = Gtk.Button(label=\"Click me\")\n    button.connect(\"clicked\", lambda _: window.show", "solution": "\n    from .gui.gtkembed import GTKEmbed\n\n    gtk_kernel = GTKEmbed(kernel)\n    gtk_kernel.start()"}
{"prompt": "def single_request_timeout(self, value):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"", "output": "\n        return value\n\n    def validate_request(self, request):\n        \"\"\"Validates a single HTTP REST API request.\"\"\"\n        # Check request method\n        if request.method not in ['GET', 'POST', 'PUT', 'DELETE']:\n            raise ValueError('Invalid request method')\n\n        # Check request path\n        if not request.path.startswith('/'):\n            raise ValueError('Invalid request path')\n\n        # Check request body\n        if request.body:\n            raise ValueError('Request body cannot be empty')\n\n        # Check request", "solution": "\n        check_type(value, int)\n        assert value is None or value > 0\n        self._single_request_timeout = value"}
{"prompt": "def search(self, pattern=\"*\", raw=True, search_raw=True,\n                                                        output=False):\n        \"\"\"Search the database using unix glob-style matching (wildcards\n        * and ?).\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        search_raw : bool\n          If True, search the raw input, otherwise, the parsed input\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"", "output": "_range`\n        \"\"\"\n        if raw:\n            return self.get_range(pattern)\n        else:\n            return self.parse_range(pattern, search_raw)\n\n    def get_range(self, pattern, start=None, end=None):\n        \"\"\"\n        Returns a list of tuples representing the range of values that match\n        the given pattern.\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        start, end : int\n          The start and end of the range to return\n        \"\"\"\n", "solution": "\n        tosearch = \"source_raw\" if search_raw else \"source\"\n        if output:\n            tosearch = \"history.\" + tosearch\n        self.writeout_cache()\n        return self._run_sql(\"WHERE %s GLOB ?\" % tosearch, (pattern,),\n                                    raw=raw, output=output)"}
{"prompt": "async def get_bots(self, limit, offset):\n        \"\"\"Gets an object of bots on DBL\"\"\"", "output": "\n        # Your code here\n        pass\n\n    async def get_tweets(self, limit, offset):\n        \"\"\"Gets an object of tweets on DBL\"\"\"\n        # Your code here\n        pass\n\n    async def get_mentions(self, limit, offset):\n        \"\"\"Gets an object of mentions on DBL\"\"\"\n        # Your code here\n        pass\n\n    async def get_replies(self, limit, offset):\n        \"\"\"Gets an object of replies on DBL\"\"\"\n        # Your code here\n        pass\n\n    async", "solution": "\n        if limit > 500:\n            limit = 50\n        return await self.request('GET', '{}/bots?limit={}&offset={}'.format(self.BASE, limit, offset))"}
{"prompt": "def group_transactions(self):\n        # type: () -> List[List[Transaction]]\n        \"\"\"\n        Groups transactions in the bundle by address.\n        \"\"\"", "output": "\n        groups = {}\n        for transaction in self.transactions:\n            if transaction.address in groups:\n                groups[transaction.address].append(transaction)\n            else:\n                groups[transaction.address] = [transaction]\n        return list(groups.values())\n\n    def validate_transactions(self):\n        # type: () -> bool\n        \"\"\"\n        Validates transactions in the bundle.\n        \"\"\"\n        for transaction in self.transactions:\n            if not transaction.is_valid():\n                return False\n        return True\n\n    ", "solution": "\n        groups = []\n\n        if self:\n            last_txn = self.tail_transaction\n            current_group = [last_txn]\n            for current_txn in self.transactions[1:]:\n                # Transactions are grouped by address, so as long as the\n                # address stays consistent from one transaction to\n                # another, we are still in the same group.\n                if current_txn.address == last_txn.address:\n                    current_group.append(current_txn)\n                else:\n                    groups.append(current_group)\n                    current_group = [current_txn]\n\n                last_txn = current_txn\n\n            if current_group:\n                groups.append(current_group)\n\n        return groups"}
{"prompt": "def authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"", "output": "\n\n        # If we don't have a callback URI, we need to authenticate ourselves\n        if callback_uri is None:\n            callback_uri = \"/callback\"\n\n        # Construct the URL for the redirect\n        url = callback_uri + \"?\" + \"&\".join(ax_attrs)\n\n        # Send the GET request and parse the response\n        response = requests.get(url, auth=(self.username, self.password))\n        response.raise_for_status()\n        data = response.json()\n\n        # Extract the authentication URL from the response\n        auth", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def write_byte_data(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"", "output": "\n        self.spi.writebytes([addr, cmd, val])\n\n    def read_byte_data(self, addr, cmd):\n        \"\"\"Read a byte of data from the specified cmd register of the device.\n        \"\"\"\n        return self.spi.readbytes([addr, cmd])[1]\n\n    def write_word_data(self, addr, cmd, val):\n        \"\"\"Write a word of data to the specified cmd register of the device.\n        \"\"\"\n        self.spi.writebytes([addr, cmd, (val >> 8) & 0x", "solution": "\n        assert self._device is not None, 'Bus must be opened before operations are made against it!'\n        # Construct a string of data to send with the command register and byte value.\n        data = bytearray(2)\n        data[0] = cmd & 0xFF\n        data[1] = val & 0xFF\n        # Send the data to the device.\n        self._select_device(addr)\n        self._device.write(data)"}
{"prompt": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"", "output": "\n        # Remove the client from the list of clients\n        self.clients.remove(client)\n\n        # If there are no more clients, close the socket\n        if not self.clients:\n            self.close()\n\n    def close(self):\n        # type: () -> None\n        \"\"\"Close the socket.\"\"\"\n        pass\n\n", "solution": "\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()"}
{"prompt": "def send(self, data):\n        \"\"\"\n        Send encoded instructions to Guacamole guacd server.\n        \"\"\"", "output": "\n        # Code to send encoded instructions to Guacamole guacd server\n        pass\n    \n    def receive(self):\n        \"\"\"\n        Receive instructions from Guacamole guacd server.\n        \"\"\"\n        # Code to receive instructions from Guacamole guacd server\n        pass\n    \n    def stop(self):\n        \"\"\"\n        Stop Guacamole guacd server.\n        \"\"\"\n        # Code to stop Guacamole guacd server\n        pass\n\ndef guacamole_guacd_server(in", "solution": "\n        self.logger.debug('Sending data: %s' % data)\n        self.client.sendall(data.encode())"}
{"prompt": "def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"", "output": "\n        return H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n\n    def substr(self, start, end):\n        \"\"\"\n        Extract a substring from the target column.\n\n        :param start: The starting index of the substring.\n        :param end: The ending index of the substring.\n        :returns: H2OFrame containing the extracted substring.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"substr\", self, start, end))\n\n    def nchar(self", "solution": "\n        fr = H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n        fr._ex._cache.nrows = self.nrow\n        return fr"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report gathered analysis data in human readable form.\"\"\"", "output": "\n        # Your code here to generate the report\n        pass\n\n", "solution": "\n        if self.verbose > 1:\n            with pprint_nosort():\n                pprint.pprint(self.results)\n\n        if self.verbose > 0:\n            print('Runtime (per repetition): {:.2g} s'.format(\n                self.results['Runtime (per repetition) [s]']),\n                file=output_file)\n        if self.verbose > 0:\n            print('Iterations per repetition: {!s}'.format(\n                self.results['Iterations per repetition']),\n                file=output_file)\n        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(\n            self.results['Runtime (per cacheline update) [cy/CL]']),\n            file=output_file)\n        print('MEM volume (per repetition): {:.0f} Byte'.format(\n            self.results['MEM volume (per repetition) [B]']),\n            file=output_file)\n        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),\n              file=output_file)\n        if self.verbose > 0:\n            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),\n                  file=output_file)\n        print('', file=output_file)\n\n        if not self.no_phenoecm:\n            print(\"Data Transfers:\")\n            print(\"{:^8} |\".format(\"cache\"), end='')\n            for metrics in self.results['data transfers'].values():\n                for metric_name in sorted(metrics):\n                    print(\" {:^14}\".format(metric_name), end='')\n                print()\n                break\n            for cache, metrics in sorted(self.results['data transfers'].items()):\n                print(\"{!s:^8} |\".format(cache), end='')\n                for k, v in sorted(metrics.items()):\n                    print(\" {!s:^14}\".format(v), end='')\n                print()\n            print()\n\n            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '\n                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(\n                **{k: float(v) for k, v in self.results['ECM'].items()}),\n                file=output_file)\n            print('T_OL assumes that two loads per cycle may be retiered, which is true for '\n                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '\n                  'BDW, SKL and SKX, but it also depends on AGU availability.',\n                  file=output_file)"}
{"prompt": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"", "output": "\n        if timeout is None:\n            timeout = self.timeout\n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            if self.is_kernel_ready():\n                return True\n            time.sleep(0.1)\n        return False\n\n    def is_kernel_ready(self):\n        \"\"\"method to check if the kernel is ready\"\"\"\n        # implementation of checking if the kernel is ready\n        pass\n\n", "solution": "\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True"}
{"prompt": "def load(self, pkcs11dll_filename=None, *init_string):\n        \"\"\"\n        load a PKCS#11 library\n\n        :type pkcs11dll_filename: string\n        :param pkcs11dll_filename: the library name.\n          If this parameter is not set then the environment variable\n          `PYKCS11LIB` is used instead\n        :returns: a :class:`PyKCS11Lib` object\n        :raises: :class:`PyKCS11Error` (-1): when the load fails\n        \"\"\"", "output": "        \"\"\"\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.environ.get('PYKCS11LIB')\n\n        if pkcs11dll_filename is None:\n            raise ValueError('pkcs11dll_filename parameter is required')\n\n        try:\n            return PyKCS11Lib(pkcs11dll_filename)\n        except:\n            raise PyKCS11Error(-1)\n\n    def save(self, lib, pkcs11dll_filename=None):\n        \"\"\"\n", "solution": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.getenv(\"PYKCS11LIB\")\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"No PKCS11 library specified (set PYKCS11LIB env variable)\")\n        rv = self.lib.Load(pkcs11dll_filename)\n        if rv == 0:\n            raise PyKCS11Error(-1, pkcs11dll_filename)"}
{"prompt": "def load_publickey(type, buffer):\n    \"\"\"\n    Load a public key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n    :type buffer: A Python string object, either unicode or bytestring.\n    :return: The PKey object.\n    :rtype: :class:`PKey`\n    \"\"\"", "output": "\n    if type == FILETYPE_PEM:\n        # Load PEM public key from buffer\n        return load_pem_public_key(buffer)\n    elif type == FILETYPE_ASN1:\n        # Load ASN.1 public key from buffer\n        return load_asn1_public_key(buffer)\n    else:\n        raise ValueError(\"Invalid file type\")\n\n", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public = True\n    return pkey"}
{"prompt": "def words(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"", "output": "\n        return self.text.split()\n\n    def syllables(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c\n        \"\"\"\n        vowels = \"aeiouy\"\n        count = 0\n        for word in self.words():\n            for i in range(len(word)):\n                if i == 0 and word[i] in vowels:\n                    count += 1\n                elif word[i] in vowels and word[", "solution": "\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))"}
{"prompt": "def parse_forensic_report(feedback_report, sample, msg_date,\n                          nameservers=None, dns_timeout=2.0,\n                          strip_attachment_payloads=False,\n                          parallel=False):\n    \"\"\"\n    Converts a DMARC forensic report and sample to a ``OrderedDict``\n\n    Args:\n        feedback_report (str): A message's feedback report as a string\n        sample (str): The RFC 822 headers or RFC 822 message sample\n        msg_date (str): The message's date header\n        nameservers (list): A list of one or more nameservers to use\n        (Cloudflare's public DNS resolvers by default)\n        dns_timeout (float): Sets the DNS timeout in seconds\n        strip_attachment_payloads (bool): Remove attachment payloads from\n        forensic report results\n        parallel (bool): Parallel processing\n\n    Returns:\n        OrderedDict: A parsed report and sample\n    \"\"\"", "output": " as a string in the format \"YYYY-MM-DD\"\n        nameservers (List[str], optional): A list of nameservers to use for the conversion\n        dns_timeout (float, optional): The timeout for DNS lookups in seconds\n        strip_attachment_payloads (bool, optional): Whether to strip any attachment payloads from the report\n        parallel (bool, optional): Whether to convert the report in parallel\n\n    Returns:\n        OrderedDict: A dictionary with the following keys:\n            - \"feedback_report\": The feedback report as a", "solution": "\n    delivery_results = [\"delivered\", \"spam\", \"policy\", \"reject\", \"other\"]\n\n    try:\n        parsed_report = OrderedDict()\n        report_values = feedback_report_regex.findall(feedback_report)\n        for report_value in report_values:\n            key = report_value[0].lower().replace(\"-\", \"_\")\n            parsed_report[key] = report_value[1]\n\n        if \"arrival_date\" not in parsed_report:\n            if msg_date is None:\n                raise InvalidForensicReport(\n                    \"Forensic sample is not a valid email\")\n            parsed_report[\"arrival_date\"] = msg_date.isoformat()\n\n        if \"version\" not in parsed_report:\n            parsed_report[\"version\"] = 1\n\n        if \"user_agent\" not in parsed_report:\n            parsed_report[\"user_agent\"] = None\n\n        if \"delivery_result\" not in parsed_report:\n            parsed_report[\"delivery_result\"] = None\n        else:\n            for delivery_result in delivery_results:\n                if delivery_result in parsed_report[\"delivery_result\"].lower():\n                    parsed_report[\"delivery_result\"] = delivery_result\n                    break\n        if parsed_report[\"delivery_result\"] not in delivery_results:\n            parsed_report[\"delivery_result\"] = \"other\"\n\n        arrival_utc = human_timestamp_to_datetime(\n            parsed_report[\"arrival_date\"], to_utc=True)\n        arrival_utc = arrival_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n        parsed_report[\"arrival_date_utc\"] = arrival_utc\n\n        ip_address = parsed_report[\"source_ip\"]\n        parsed_report_source = get_ip_address_info(ip_address,\n                                                   nameservers=nameservers,\n                                                   timeout=dns_timeout,\n                                                   parallel=parallel)\n        parsed_report[\"source\"] = parsed_report_source\n        del parsed_report[\"source_ip\"]\n\n        if \"identity_alignment\" not in parsed_report:\n            parsed_report[\"authentication_mechanisms\"] = []\n        elif parsed_report[\"identity_alignment\"] == \"none\":\n            parsed_report[\"authentication_mechanisms\"] = []\n            del parsed_report[\"identity_alignment\"]\n        else:\n            auth_mechanisms = parsed_report[\"identity_alignment\"]\n            auth_mechanisms = auth_mechanisms.split(\",\")\n            parsed_report[\"authentication_mechanisms\"] = auth_mechanisms\n            del parsed_report[\"identity_alignment\"]\n\n        if \"auth_failure\" not in parsed_report:\n            parsed_report[\"auth_failure\"] = \"dmarc\"\n        auth_failure = parsed_report[\"auth_failure\"].split(\",\")\n        parsed_report[\"auth_failure\"] = auth_failure\n\n        optional_fields = [\"original_envelope_id\", \"dkim_domain\",\n                           \"original_mail_from\", \"original_rcpt_to\"]\n        for optional_field in optional_fields:\n            if optional_field not in parsed_report:\n                parsed_report[optional_field] = None\n\n        parsed_sample = parse_email(\n            sample,\n            strip_attachment_payloads=strip_attachment_payloads)\n\n        if \"reported_domain\" not in parsed_report:\n            parsed_report[\"reported_domain\"] = parsed_sample[\"from\"][\"domain\"]\n\n        sample_headers_only = False\n        number_of_attachments = len(parsed_sample[\"attachments\"])\n        if number_of_attachments < 1 and parsed_sample[\"body\"] is None:\n            sample_headers_only = True\n        if sample_headers_only and parsed_sample[\"has_defects\"]:\n            del parsed_sample[\"defects\"]\n            del parsed_sample[\"defects_categories\"]\n            del parsed_sample[\"has_defects\"]\n        parsed_report[\"sample_headers_only\"] = sample_headers_only\n        parsed_report[\"sample\"] = sample\n        parsed_report[\"parsed_sample\"] = parsed_sample\n\n        return parsed_report\n\n    except KeyError as error:\n        raise InvalidForensicReport(\"Missing value: {0}\".format(\n            error.__str__()))\n\n    except Exception as error:\n        raise InvalidForensicReport(\n            \"Unexpected error: {0}\".format(error.__str__()))"}
{"prompt": "def map_peaks_to_image(peaks, r=4, vox_dims=(2, 2, 2), dims=(91, 109, 91),\n                       header=None):\n    \"\"\" Take a set of discrete foci (i.e., 2-D array of xyz coordinates)\n    and generate a corresponding image, convolving each focus with a\n    hard sphere of radius r.\"\"\"", "output": "\n    if header is None:\n        header = {}\n    header['Author'] = 'John Doe'\n    header['Date'] = datetime.now().strftime('%Y-%m-%d')\n    header['Subject'] = 'Peak Map'\n    \n    # Generate 2D array of voxel coordinates\n    voxel_coords = np.indices(dims).reshape(3, -1).T\n    \n    # Generate 2D array of distances from each focus to each voxel\n    distances = np.l", "solution": "\n    data = np.zeros(dims)\n    for p in peaks:\n        valid = get_sphere(p, r, vox_dims, dims)\n        valid = valid[:, ::-1]\n        data[tuple(valid.T)] = 1\n    return nifti1.Nifti1Image(data, None, header=header)"}
{"prompt": "def iter_parse(fiql_str):\n    \"\"\"Iterate through the FIQL string. Yield a tuple containing the\n    following FIQL components for each iteration:\n\n      - preamble: Any operator or opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - comparison: The comparison portion of a FIQL constraint or ``None``\n        if yielding the last portion of the string.\n      - argument: The argument portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n\n    For usage see :func:`parse_str_to_expression`.\n\n    Args:\n        fiql_str (string): The FIQL formatted string we want to parse.\n\n    Yields:\n        tuple: Preamble, selector, comparison, argument.\n    \"\"\"", "output": " string.\n      - value: The value portion of a FIQL constraint or an empty string if\n        yielding the last portion of the string.\n\n    The function should not modify the input string.\n    \"\"\"\n    stack = []\n    for c in fiql_str:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if stack:\n                yield ('preamble', stack.pop())\n        elif c == '&':\n            yield ('selector', None)\n        elif c == '", "solution": "\n    while len(fiql_str):\n        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)\n        if len(constraint_match) < 2:\n            yield (constraint_match[0], None, None, None)\n            break\n        yield (\n            constraint_match[0],\n            unquote_plus(constraint_match[1]),\n            constraint_match[4],\n            unquote_plus(constraint_match[6]) \\\n                    if constraint_match[6] else None\n        )\n        fiql_str = constraint_match[8]"}
{"prompt": "def shape(self):\n        \"\"\"Returns the shape of the data.\"\"\"", "output": "\n        return len(self.data)\n\n    def mean(self):\n        \"\"\"Returns the mean of the data.\"\"\"\n        return sum(self.data) / len(self.data)\n\n    def median(self):\n        \"\"\"Returns the median of the data.\"\"\"\n        sorted_data = sorted(self.data)\n        n = len(sorted_data)\n        if n % 2 == 0:\n            return (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n        else:\n            return sorted", "solution": "\n        # TODO cache\n        first = self.first().shape\n        shape = self._rdd.map(lambda x: x.shape[0]).sum()\n        return (shape,) + first[1:]"}
{"prompt": "def _wait_job_completion(self):\n        \"\"\"Wait for the cache to be empty before resizing the pool.\"\"\"", "output": "\n        while not self.cache:\n            time.sleep(self.pool_size)\n\n    def _resize_pool(self):\n        \"\"\"Resize the pool to a new size if necessary.\"\"\"\n        if len(self.cache) < self.pool_size:\n            self.cache += [None] * (self.pool_size - len(self.cache))\n\n    def __getitem__(self, key):\n        \"\"\"\n        Get the value associated with the given key from the cache.\n\n        Args:\n            key: The key to retrieve the value", "solution": "\n        # Issue a warning to the user about the bad effect of this usage.\n        if len(self._pending_work_items) > 0:\n            warnings.warn(\"Trying to resize an executor with running jobs: \"\n                          \"waiting for jobs completion before resizing.\",\n                          UserWarning)\n            mp.util.debug(\"Executor {} waiting for jobs completion before\"\n                          \" resizing\".format(self.executor_id))\n        # Wait for the completion of the jobs\n        while len(self._pending_work_items) > 0:\n            time.sleep(1e-3)"}
{"prompt": "def energy(self, state=None):\n        \"\"\"Calculates the length of the route.\"\"\"", "output": "\n        if state is None:\n            state = self.current_state\n        if state == \"home\":\n            return 0\n        else:\n            return len(state)\n\n    def update(self, state=None):\n        \"\"\"Updates the route based on the current state and the next state.\"\"\"\n        if state is None:\n            state = self.current_state\n        if state == \"home\":\n            return\n        else:\n            next_state = state.split(\"-\")[1]\n            distance = len(next_state) - len(state", "solution": "\n        state = self.state if state is None else state\n        route = state\n        e = 0\n        if self.distance_matrix:\n            for i in range(len(route)):\n                e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n        else:\n            for i in range(len(route)):\n                e += distance(self.cities[route[i-1]], self.cities[route[i]])\n        return e"}
{"prompt": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"", "output": "\n        params = {'language': language, 'page': page}\n        params.update(kwargs)\n        response = requests.get(f'{self.base_url}/movies', params=params)\n        response.raise_for_status()\n        return response.json()\n\n    def get_movie_info(self, movie_id: str, **kwargs):\n        \"\"\"\n        Get information about a movie.\n\n        Args:\n            movie_id: The ID of the movie.\n            language: (optional) ISO 639-1 code.", "solution": "\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def get_clinvar_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"", "output": "\n        # clinvar portal returns a string with a format: SUB[0-9]\n        # We extract the first part of the string and replace the first two characters with 'SUB'\n        clinvar_subm_id = submission_id[4:].replace('SUB', 'SUB', 1)\n        return clinvar_subm_id\n\n    def get_clinvar_submission_id(self, clinvar_subm_id):\n        \"\"\"Returns the submission ID for a Clinvar submission object\n\n            Args:\n                clinvar_sub", "solution": "\n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id"}
{"prompt": "def dict_factory(self, cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        return {col[0]: col[1] for col in cursor.description}\n\n", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp:\n                d[col[0]] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":  # or name == Field.Meter_Time:\n                continue\n            if name not in self.m_all_fields:\n                continue\n            if (str(val) != \"None\") and ((val > 0) or (val < 0)):\n                d[name] = str(val)\n        return d"}
{"prompt": "def streams(self, url, **params):\n        \"\"\"Attempts to find a plugin and extract streams from the *url*.\n\n        *params* are passed to :func:`Plugin.streams`.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"", "output": "\n        plugin = Plugin.find(url, **params)\n        if not plugin:\n            raise NoPluginError(url)\n        plugin.streams(url, **params)\n\n    def parse(self, url, **params):\n        \"\"\"Attempts to extract information from the streams of the plugin.\n\n        *params* are passed to :func:`Plugin.streams`.\n\n        Raises :exc:`NoStreamError` if no stream is found.\n        \"\"\"\n        plugin = Plugin.find(url, **params)\n        if not plugin:\n            raise NoStream", "solution": "\n\n        plugin = self.resolve_url(url)\n        return plugin.streams(**params)"}
{"prompt": "def map(self, func):\n        \"\"\"\n        Process all data with given function.\n        The scheme of function should be x,y -> x,y.\n        \"\"\"", "output": "\n        new_data = []\n        for data_point in self.data:\n            new_data_point = func(data_point)\n            new_data.append(new_data_point)\n        return new_data\n\n", "solution": "\n        if self._train_set:\n            self._train_set = map(func, self._train_set)\n        if self._valid_set:\n            self._valid_set = map(func, self._valid_set)\n        if self._test_set:\n            self._test_set = map(func, self._test_set)"}
{"prompt": "def positions_func(inputs, pad=0):\n    \"\"\"\n    A layer filling i-th column of a 2D tensor with\n    1+ln(1+i) when it contains a meaningful symbol\n    and with 0 when it contains PAD\n    \"\"\"", "output": "\n    output = []\n    for row in inputs:\n        symbols = [1 if x!= pad else 0 for x in row]\n        positions = [int(symbols[i] + log(1+i)) for i in range(len(row))]\n        output.append(positions)\n    return output\n\n", "solution": "\n    position_inputs = kb.cumsum(kb.ones_like(inputs, dtype=\"float32\"), axis=1)\n    position_inputs *= kb.cast(kb.not_equal(inputs, pad), \"float32\")\n    return kb.log(1.0 + position_inputs)"}
{"prompt": "def _with_loc(f: W) -> W:\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"", "output": "\n    def wrapper(text: str, *args, **kwargs) -> W:\n        for line in text.split('\\n'):\n            yield W(line, *args, **kwargs)\n    return wrapper\n\n", "solution": "\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)"}
{"prompt": "def p_creg_decl(self, program):\n        \"\"\"\n           creg_decl : CREG indexed_id\n        \"\"\"", "output": "\n        index = program.split()[1]\n        if index.isdigit():\n            index = int(index)\n            if index < 0 or index >= len(program):\n                raise ValueError(\"Index out of range\")\n            program = program[:index] + \"CREG \" + program[index+1:]\n        return program\n\n    def p_creg_inc(self, program):\n        \"\"\"\n           creg_inc : INC indexed_id\n        \"\"\"\n        index = program.split()[1]\n        if index.isdigit():", "solution": "\n        program[0] = node.Creg([program[2]])\n        if program[2].name in self.external_functions:\n            raise QasmError(\"CREG names cannot be reserved words. \"\n                            + \"Received '\" + program[2].name + \"'\")\n        if program[2].index == 0:\n            raise QasmError(\"CREG size must be positive\")\n        self.update_symtab(program[0])"}
{"prompt": "def order_verification(self, institute, case, user, link, variant):\n        \"\"\"Create an event for a variant verification for a variant\n        and an event for a variant verification for a case\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"", "output": "\n        updated_variant = {}\n        updated_variant['institute'] = institute['name']\n        updated_variant['case'] = case['name']\n        updated_variant['user'] = user['name']\n        updated_variant['link'] = link\n        updated_variant['variant'] = variant['name']\n        return updated_variant\n\n", "solution": "\n        LOG.info(\"Creating event for ordering validation for variant\" \\\n                    \" {0}\".format(variant['display_name']))\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'sanger_ordered': True}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating event for ordering sanger for case\" \\\n                    \" {0}\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"prompt": "def update_product_set(\n        self,\n        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"", "output": "\n        if not location:\n            raise ValueError(\"location parameter is required\")\n        if not product_set_id:\n            raise ValueError(\"product_set_id parameter is required\")\n        if not update_mask:\n            raise ValueError(\"update_mask parameter is required\")\n        if not project_id:\n            raise ValueError(\"project_id parameter is required\")\n        if not retry:\n            raise ValueError(\"retry parameter is required\")\n        if not timeout:\n            raise ValueError(\"timeout parameter is required\")\n        if not metadata:\n            ", "solution": "\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)"}
{"prompt": "def get_mic(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Calculates a cryptographic message integrity code (MIC) over an application message, and\n        returns that MIC in a token. This is in contrast to :meth:`wrap` which calculates a MIC\n        over a message, optionally encrypts it and returns the original message and the MIC packed\n        into a single token. The peer application can then verify the MIC to ensure the associated\n        message has not been changed in transit.\n\n        :param message: The message to calculate a MIC for\n        :type message: bytes\n        :param qop_req: The quality of protection required. It is recommended to not change this\n            from the default as most GSSAPI implementations do not support it.\n        :returns: A MIC for the message calculated using this security context's cryptographic keys\n        :rtype: bytes\n        \"\"\"", "output": ": The message to calculate the MIC for.\n        :param qop_req: The quality of protection required for the MIC calculation. Defaults to\n                       GSS_C_QOP_DEFAULT, which is the recommended value for GSS.\n        :return: A tuple containing the MIC and the associated message.\n        \"\"\"\n        mic = self.wrap(message, qop_req)\n        return mic, message\n\n    def wrap(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Encrypts a", "solution": "\n        if not (self.flags & C.GSS_C_INTEG_FLAG):\n            raise GSSException(\"No integrity protection negotiated.\")\n        if not (self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)):\n            raise GSSException(\"Protection not yet ready.\")\n\n        minor_status = ffi.new('OM_uint32[1]')\n        output_token_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer[0].length = len(message)\n        c_str_message = ffi.new('char[]', message)\n        message_buffer[0].value = c_str_message\n        retval = C.gss_get_mic(\n            minor_status,\n            self._ctx[0],\n            ffi.cast('gss_qop_t', qop_req),\n            message_buffer,\n            output_token_buffer\n        )\n        try:\n            if GSS_ERROR(retval):\n                if minor_status[0] and self.mech_type:\n                    raise _exception_for_status(retval, minor_status[0], self.mech_type)\n                else:\n                    raise _exception_for_status(retval, minor_status[0])\n\n            output_token = _buf_to_str(output_token_buffer[0])\n            return output_token\n        finally:\n            if output_token_buffer[0].length != 0:\n                C.gss_release_buffer(minor_status, output_token_buffer)"}
{"prompt": "def extractHolidayDate(self, setting_holiday):\n        \"\"\" Read a single holiday date from meter buffer.\n\n        Args:\n            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)\n\n        Returns:\n            tuple: Holiday tuple, elements are strings.\n\n            =============== ======================\n            Holiday         Holiday 0-19 as string\n            Day             Day 1-31 as string\n            Month           Monty 1-12 as string\n            =============== ======================\n\n        \"\"\"", "output": "\n        if setting_holiday < 0 or setting_holiday >= Extents.Holidays:\n            raise ValueError(\"Invalid holiday value\")\n\n        holiday_names = [\"New Year's Day\", \"Independence Day\", \"Thanksgiving Day\", \"Christmas Day\"]\n        holiday_dates = [\"January 1\", \"February 14\", \"March 1\", \"April 1\", \"May 1\", \"June 1\", \"July 4\", \"August 1\", \"September 1\", \"October 1\", \"November 1\", \"December 25\", \"Christmas Day\"]\n\n        holiday_name = holiday_names[setting_holiday]", "solution": "\n        ret = namedtuple(\"result\", [\"Holiday\", \"Month\", \"Day\"])\n        setting_holiday += 1\n        ret.Holiday = str(setting_holiday)\n\n        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):\n            ekm_log(\"Out of bounds:  holiday \" + str(setting_holiday))\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n\n        idxday = \"Holiday_\" + str(setting_holiday) + \"_Day\"\n        idxmon = \"Holiday_\" + str(setting_holiday) + \"_Mon\"\n        if idxmon not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        if idxday not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        ret.Day = self.m_hldy[idxday][MeterData.StringValue]\n        ret.Month = self.m_hldy[idxmon][MeterData.StringValue]\n        return ret"}
{"prompt": "def getMaxStmIdForStm(stm):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"", "output": "\n    max_id = float('-inf')\n    for a in stm['assignments']:\n        if a['_instId'] > max_id:\n            max_id = a['_instId']\n    return max_id\n\n", "solution": "\n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId"}
{"prompt": "def delete(self, endpoint, headers):\n        \"\"\"\n        Method to delete an item or all items\n\n        headers['If-Match'] must contain the _etag identifier of the element to delete\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (deletion information)\n        :rtype: dict\n        \"\"\"", "output": "\n        response = {}\n        if headers.get('If-Match') and headers['If-Match'].startswith('etag:'):\n            response = {'status': 'deleted'}\n        else:\n            response = {'status': 'not_deleted'}\n        return response\n\n", "solution": "\n        response = self.get_response(method='DELETE', endpoint=endpoint, headers=headers)\n\n        logger.debug(\"delete, response: %s\", response)\n        if response.status_code != 204:  # pragma: no cover - should not happen ...\n            resp = self.decode(response=response)\n\n        resp = {\"_status\": \"OK\"}\n        return resp"}
{"prompt": "def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param safe_dag_id: a dag_id with only alphanumeric characters\n        :param safe_task_id: a task_id with only alphanumeric characters\n        :param random_uuid: a uuid\n        :return: ``str`` valid Pod name of appropriate length\n        \"\"\"", "output": ": The safe Dag ID\n        :param safe_task_id: The safe Task ID\n        :param safe_uuid: The safe UUID\n        :return: The valid pod name\n        \"\"\"\n        if len(safe_dag_id) > 253 or len(safe_task_id) > 253 or len(safe_uuid) > 253:\n            raise ValueError(\"Invalid pod name\")\n\n        if not re.match(r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?", "solution": "\n        MAX_POD_ID_LEN = 253\n\n        safe_key = safe_dag_id + safe_task_id\n\n        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid\n\n        return safe_pod_id"}
{"prompt": "def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"", "output": "\n        os.system(f\"{os.environ['OS']} && {func}\")\n\n    def start(self):\n        \"\"\"Start the sandbox\"\"\"\n        self.run(self.start_func)\n\n    def stop(self):\n        \"\"\"Stop the sandbox\"\"\"\n        self.run(self.stop_func)\n\n    def start_func(self):\n        \"\"\"Run'self.func' when the sandbox is started\"\"\"\n        pass\n\n    def stop_func(self):\n        \"\"\"Run'self.func' when the sandbox is stopped\"\"\"\n        pass", "solution": "\n        try:\n            self._copy(self)\n            if _file:\n                __builtin__.file = self._file\n            __builtin__.open = self._open\n            self._active = True\n            return func()\n        finally:\n            self._active = False\n            if _file:\n                __builtin__.file = _file\n            __builtin__.open = _open\n            self._copy(_os)"}
{"prompt": "def _isint(string):\n    \"\"\"\n    >>> _isint(\"123\")\n    True\n    >>> _isint(\"123.45\")\n    False\n    \"\"\"", "output": "\n    try:\n        int(string)\n        return True\n    except ValueError:\n        return False\n\n", "solution": "\n    return type(string) is int or \\\n        (isinstance(string, _binary_type) or\n         isinstance(string, string_types)) and \\\n        _isconvertible(int, string)"}
{"prompt": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Get request payload and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"", "output": "defaults to KMIP 1.0.\n\n        Returns:\n            A tuple containing a GetResponse object and any remaining data\n            stream.\n\n        Raises:\n            ValueError: If the input stream is not a BytearrayStream object.\n        \"\"\"\n        if not isinstance(input_stream, BytearrayStream):\n            raise ValueError(\"Input stream must be a BytearrayStream object\")\n\n        # Read the Get request payload\n        payload_length = input_stream.read_u32()\n        payload = input_stream.read(pay", "solution": "\n        super(GetRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_FORMAT_TYPE, local_stream):\n            self._key_format_type = primitives.Enumeration(\n                enum=enums.KeyFormatType,\n                tag=enums.Tags.KEY_FORMAT_TYPE\n            )\n            self._key_format_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_COMPRESSION_TYPE, local_stream):\n            self._key_compression_type = primitives.Enumeration(\n                enum=enums.KeyCompressionType,\n                tag=enums.Tags.KEY_COMPRESSION_TYPE\n            )\n            self._key_compression_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(\n                enums.Tags.KEY_WRAPPING_SPECIFICATION,\n                local_stream\n        ):\n            self._key_wrapping_specification = \\\n                objects.KeyWrappingSpecification()\n            self._key_wrapping_specification.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)"}
{"prompt": "def get_latex_maybe_optional_arg(self, pos):\n        \"\"\"\n        Parses the latex content given to the constructor (and stored in `self.s`),\n        starting at position `pos`, to attempt to parse an optional argument.\n\n        Attempts to parse an optional argument. If this is successful, we return\n        a tuple `(node, pos, len)` if success where `node` is a\n        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.\n        \"\"\"", "output": "\n        node = None\n        while pos < len(self.s):\n            if self.s[pos] == '{':\n                # Found the start of a group\n                if node is None:\n                    # No node was found yet\n                    node = LatexGroupNode()\n                else:\n                    # We were already in a group\n                    return None\n            elif self.s[pos] == '}':\n                # Found the end of a group\n                if node is not None:\n                    # We were in a group, so we're done\n                    return (node", "solution": "\n\n        tok = self.get_token(pos, brackets_are_chars=False, environments=False)\n        if (tok.tok == 'brace_open' and tok.arg == '['):\n            return self.get_latex_braced_group(pos, brace_type='[')\n\n        return None"}
{"prompt": "def _label__get(self):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"", "output": "\n        if self.__label is None:\n            return None\n        else:\n            return self.__label.text\n\n    label = property(_label__get, _label__set)\n\n    def _set_text(self, text):\n        \"\"\"\n        Set or get any text associated with this element.\n        \"\"\"\n        if self.__text is None:\n            self.__text = text\n        else:\n            self.__text += text\n\n    text = property(_get_text, _set_text)\n\n    def _get_style(self):\n", "solution": "\n        id = self.get('id')\n        if not id:\n            return None\n        result = _label_xpath(self, id=id)\n        if not result:\n            return None\n        else:\n            return result[0]"}
{"prompt": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"", "output": "\n    return set(string.split(','))\n\n", "solution": "\n    return set(text.strip() for text in string.split(',') if text.strip())"}
{"prompt": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"", "output": "\n        if not quiet:\n            print(f\"Applying next patch: {self.name}\")\n        if force:\n            self.apply_force()\n        else:\n            self.apply_normal_actions()\n\n    def apply_force(self):\n        \"\"\" Apply force to patch \"\"\"\n        if self.force_type == \"normal\":\n            self.apply_normal_actions()\n        elif self.force_type == \"hard\":\n            self.apply_hard_actions()\n        elif self.force_type == \"soft\":\n            self", "solution": "\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())"}
{"prompt": "def _data_sanity_checks(self, explore_iterable):\n        \"\"\"Checks if data values are  valid.\n\n        Checks if the data values are supported by the parameter and if the values are of the same\n        type as the default value.\n\n        \"\"\"", "output": "\n        for data in explore_iterable:\n            if not isinstance(data, (int, float, str)):\n                raise ValueError(\"Invalid data type. All data values must be of type int, float, or str.\")\n            if isinstance(data, str):\n                try:\n                    float(data)\n                except ValueError:\n                    raise ValueError(\"Invalid data value. All data values must be numeric.\")\n            elif isinstance(data, (int, float)):\n                if not isinstance(self.default, (int, float)):\n                    ", "solution": "\n        data_list = []\n\n        for val in explore_iterable:\n\n            if not self.f_supports(val):\n                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))\n\n            if not self._values_of_same_type(val, self._default):\n                raise TypeError(\n                    'Data of `%s` is not of the same type as the original entry value, '\n                    'new type is %s vs old type %s.' %\n                    (self.v_full_name, str(type(val)), str(type(self._default))))\n\n            data_list.append(val)\n\n        if len(data_list) == 0:\n            raise ValueError('Cannot explore an empty list!')\n\n        return data_list"}
{"prompt": "def _cursor_position_changed(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"", "output": "\n        if self.cursor_position == 0:\n            self.tip = \"Start\"\n        elif self.cursor_position == len(self.s):\n            self.tip = \"End\"\n        else:\n            prev_char = self.s[self.cursor_position - 1]\n            curr_char = self.s[self.cursor_position]\n            if prev_char == curr_char:\n                self.tip = \"Mismatch\"\n            elif prev_char == \" \":\n                self.tip =", "solution": "\n        cursor = self._text_edit.textCursor()\n        if cursor.position() <= self._start_position:\n            self.hide()\n        else:\n            position, commas = self._find_parenthesis(self._start_position + 1)\n            if position != -1:\n                self.hide()"}
{"prompt": "def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"", "output": "\n        return [resource for resource in reversed(self.resources_by_order) if resource is not None]\n\n    def resources_by_order(self, resources):\n        \"\"\"\n        Returns a list of `FragmentResource` objects sorted by their order of first appearance.\n        \"\"\"\n        return [resource for resource in reversed(resources) if resource is not None]\n\n    def __str__(self):\n        return f\"Fragment(name='{self.name}', resources={', '.join(str(resource) for resource in self.resources)})\"\n\n", "solution": "\n        seen = set()\n        # seen.add always returns None, so 'not seen.add(x)' is always True,\n        # but will only be called if the value is not already in seen (because\n        # 'and' short-circuits)\n        return [x for x in self._resources if x not in seen and not seen.add(x)]"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    def wrapper(node, *args, **kwargs):\n        result = f(node, *args, **kwargs)\n        result['positions'] = node.positions\n        return result\n    return wrapper\n\n", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def s2p(self):\n        \"\"\"Return 2 proton separation energy\"\"\"", "output": "\n        return 2 * self.charge**2 * 1.6\n\n    def p2s(self):\n        \"\"\"Return 2 proton separation energy\"\"\"\n        return 2 * self.charge**2 * 0.6\n\n    def p2p(self):\n        \"\"\"Return 2 proton-proton separation energy\"\"\"\n        return 2 * self.charge**2 * 0.5\n\n    def p2pp(self):\n        \"\"\"Return 2 proton-proton-proton separation energy\"\"\"\n        return 2 * self.charge**2 * 0.25\n\n    def p2pp", "solution": "\n        M_P = 7.28897050         # proton mass excess in MeV\n        f = lambda parent, daugther: -parent + daugther + 2 * M_P\n        return self.derived('s2p', (-2, 0), f)"}
{"prompt": "def clear_xcom_data(self, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"", "output": "\n        if session is None:\n            session = self.session\n        xcom_data = session.query(XCom).filter_by(task_id=self.id).all()\n        session.delete(xcom_data)\n        session.commit()\n\n", "solution": "\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()"}
{"prompt": "def as_property_description(self):\n        \"\"\"\n        Get the property description.\n\n        Returns a dictionary describing the property.\n        \"\"\"", "output": "\n        return {'name': self.name, 'value': self.value, 'unit': self.unit}\n\n", "solution": "\n        description = deepcopy(self.metadata)\n\n        if 'links' not in description:\n            description['links'] = []\n\n        description['links'].append(\n            {\n                'rel': 'property',\n                'href': self.href_prefix + self.href,\n            }\n        )\n        return description"}
{"prompt": "def save(self, fname=None):\n        \"\"\"\n        Download the chart from the URL into a filename as a PNG\n\n        The filename defaults to the chart title (chtt) if any\n        \"\"\"", "output": "\n        if fname is None:\n            fname = self.title\n        with open(fname + \".png\", \"wb\") as f:\n            f.write(requests.get(self.url).content)\n        return fname\n\n    def show(self):\n        \"\"\"\n        Displays the chart using matplotlib\n        \"\"\"\n        import matplotlib.pyplot as plt\n        plt.plot(self.x, self.y)\n        plt.show()\n\n", "solution": "\n        if not fname:\n            fname = self.getname()\n        assert fname != None, 'You must specify a filename to save to'\n        if not fname.endswith('.png'):\n            fname += '.png'\n        try:\n            urlretrieve(self.url, fname)\n        except Exception:\n            raise IOError('Problem saving %s to file'%fname)\n        return fname"}
{"prompt": "def opt(parser: Union[Parser, Sequence[Input]]) -> OptionalParser:\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"", "output": "\n    if isinstance(parser, Parser):\n        return [parser]\n    else:\n        return []\n\n", "solution": "\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return OptionalParser(parser)"}
{"prompt": "def log(self):\n        \"\"\"\n        Print on screen and on file the percentages for each status.\n        \"\"\"", "output": "\n        print(f\"{self.status}: {self.percentage:.2f}%\")\n\n", "solution": "\n\n        if (\n            PyFunceble.CONFIGURATION[\"show_percentage\"]\n            and PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0\n        ):\n            # * We are allowed to show the percentage on screen.\n            # and\n            # * The number of tested is greater than 0.\n\n            # We initiate the output file.\n            output = (\n                PyFunceble.OUTPUT_DIRECTORY\n                + PyFunceble.OUTPUTS[\"parent_directory\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"percentage\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"filenames\"][\"percentage\"]\n            )\n\n            # We delete the output file if it does exist.\n            File(output).delete()\n\n            # We calculate the percentage of each statuses.\n            self._calculate()\n\n            if not PyFunceble.CONFIGURATION[\"quiet\"]:\n                # The quiet mode is activated.\n\n                # We print a new line.\n                print(\"\\n\")\n\n                # We print the percentage header on file and screen.\n                Prints(None, \"Percentage\", output).header()\n\n                # We construct the different lines/data to print on screen and file.\n                lines_to_print = [\n                    [\n                        PyFunceble.STATUS[\"official\"][\"up\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"up\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"up\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"down\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"down\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"down\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"invalid\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"invalid\"])\n                        + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"invalid\"],\n                    ],\n                ]\n\n                if PyFunceble.CONFIGURATION[\"syntax\"]:\n                    # We are checking for syntax.\n\n                    # We update the denomination of the UP.\n                    lines_to_print[0][0] = PyFunceble.STATUS[\"official\"][\"valid\"]\n\n                    # And we unset the INACTIVE line.\n                    del lines_to_print[1]\n\n                for to_print in lines_to_print:\n                    # We loop throught the different line to print.\n                    # (one line for each status.)\n\n                    # And we print the current status line on file and screen.\n                    Prints(to_print, \"Percentage\", output).data()\n\n        elif PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0:\n            # * We are not allowed to show the percentage on screen.\n            # but\n            # * The number of tested is greater than 0.\n\n            # We run the calculation.\n            # Note: The following is needed, because all counter calculation are\n            # done by this class.\n            self._calculate()"}
{"prompt": "def grid(self, *args):\n        \"\"\"\n        Apply a grid to your chart\n        args are of the form::\n            <x axis step size>,\n            <y axis step size>,\n            <length of line segment>,\n            <length of blank segment>\n            <x offset>,\n            <y offset>\n        APIPARAM: chg\n        \"\"\"", "output": "\n        x_axis_step = args[0]\n        y_axis_step = args[1]\n        line_segment = args[2]\n        blank_segment = args[3]\n        x_offset = args[4]\n        y_offset = args[5]\n\n        for y in range(self.height):\n            for x in range(self.width):\n                if x % x_axis_step == 0 and y % y_axis_step == 0:\n                    if x % 2 == 0:\n                        if y % 2 == 0", "solution": "\n        grids =  map(str,map(float,args))\n        self['chg'] = ','.join(grids).replace('None','')\n        return self"}
{"prompt": "def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"", "output": "type filter_doc: dict\n        :param mongo_db: The name of the database to use for the query. If not provided, the default database will be used.\n        :type mongo_db: str\n        :param kwargs: Additional keyword arguments to pass to the mongo collection's delete_one method.\n        :return: The result of the delete operation.\n        :rtype: bool\n        \"\"\"\n        if mongo_db is None:\n            mongo_db = self.mongo_db\n        result = mongo_collection.delete_one", "solution": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)"}
{"prompt": "def _check_relative_import(\n        self, modnode, importnode, importedmodnode, importedasname\n    ):\n        \"\"\"check relative import. node is either an Import or From node, modname\n        the imported module name.\n        \"\"\"", "output": "\n        if modnode.level == 0:\n            # top-level module\n            if modnode.module == \"__init__\":\n                # top-level module is __init__.py\n                if modnode.asname:\n                    # top-level module has an asname\n                    asname = modnode.asname\n                else:\n                    # top-level module has no asname\n                    asname = None\n            else:\n                # top-level module is not __init__.py\n                asname = None\n        else:\n            # sub-module", "solution": "\n        if not self.linter.is_message_enabled(\"relative-import\"):\n            return None\n        if importedmodnode.file is None:\n            return False  # built-in module\n        if modnode is importedmodnode:\n            return False  # module importing itself\n        if modnode.absolute_import_activated() or getattr(importnode, \"level\", None):\n            return False\n        if importedmodnode.name != importedasname:\n            # this must be a relative import...\n            self.add_message(\n                \"relative-import\",\n                args=(importedasname, importedmodnode.name),\n                node=importnode,\n            )\n            return None\n        return None"}
{"prompt": "def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str\n        \"\"\"", "output": "\n        # Code to download the file from Google Cloud Storage\n        # If filename is provided, write the file to disk\n        if filename:\n            with open(filename, 'wb') as f:\n                f.write(response.content)\n\n    def upload(self, bucket_name, object_name, filename):\n        \"\"\"\n        Upload a file to Google Cloud Storage.\n\n        :param bucket_name: The bucket to upload to.\n        :type bucket_name: str\n        :param object_name: The object to upload the file to.\n        :type object_", "solution": "\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()"}
{"prompt": "def parse(self, s):\n        \"\"\"Parses the input string, and returns a reference to the created AST's root\"\"\"", "output": "\n        # Your code here to parse the input string and create an AST\n        # Return the root node of the AST\n        pass\n\n", "solution": "\n        with self.lock:\n            try:\n                return self.parser.parse(s, lexer=self.lexer)\n            except InvalidIEMLObjectArgument as e:\n                raise CannotParse(s, str(e))\n            except CannotParse as e:\n                e.s = s\n                raise e"}
{"prompt": "def allocate_ids(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"", "output": "\n        project = 'projects/{}'.format(self.project_id)\n        datastore = 'datastores/{}'.format(self.datastore_id)\n        request = {\n            'name': 'allocateIds',\n            'project': project,\n            'datastore': datastore,\n            'ids': partial_keys\n        }\n        response = requests.post(request)\n        return response.json()['ids']\n\n    def get_all_keys(self):\n        \"\"\"\n        Retrieve all keys for", "solution": "\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']"}
{"prompt": "def _insert_plain_text_into_buffer(self, cursor, text):\n        \"\"\" Inserts text into the input buffer using the specified cursor (which\n            must be in the input buffer), ensuring that continuation prompts are\n            inserted as necessary.\n        \"\"\"", "output": "\n        if cursor < 0 or cursor >= len(self.buffer):\n            return False\n        if text == \"\":\n            return True\n        if text[-1] == \"\\n\":\n            text = text[:-1]\n        if cursor + len(text) > len(self.buffer):\n            return False\n        self.buffer[cursor:cursor+len(text)] = text\n        return True\n\n    def _get_input_from_user(self):\n        \"\"\" Returns the input buffer as a string.\n        \"\"\"\n        return \"\".join(", "solution": "\n        lines = text.splitlines(True)\n        if lines:\n            cursor.beginEditBlock()\n            cursor.insertText(lines[0])\n            for line in lines[1:]:\n                if self._continuation_prompt_html is None:\n                    cursor.insertText(self._continuation_prompt)\n                else:\n                    self._continuation_prompt = \\\n                        self._insert_html_fetching_plain_text(\n                            cursor, self._continuation_prompt_html)\n                cursor.insertText(line)\n            cursor.endEditBlock()"}
{"prompt": "def get_total_contributors(self, repo):\n        \"\"\"\n        Retrieves the number of contributors to a repo in the organization.\n        Also adds to unique contributor list.\n        \"\"\"", "output": "\n        url = f\"{repo}/contributors\"\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            self.total_contributors += data[\"total\"]\n            self.contributors.extend(data[\"contributors\"])\n            self.unique_contributors.add(data[\"contributor\"])\n        else:\n            raise ValueError(\"Error retrieving contributors\")\n\n    def get_contributor_url(self, repo, contributor):\n        ", "solution": "\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors"}
{"prompt": "def _unreg_event(self, event_list, event):\n        \"\"\" Tries to remove a registered event without triggering it \"\"\"", "output": "\n        for i in range(len(event_list)):\n            if event_list[i] == event:\n                del event_list[i]\n                break\n        return len(event_list) > 0\n\n    def _unreg_all(self, event_list):\n        \"\"\" Tries to remove all unregistered events without triggering them \"\"\"\n        for i in range(len(event_list)-1, -1, -1):\n            if self._unreg_event(event_list, event_list[i]):\n                del event_list[", "solution": "\n        try:\n            self.log.debug(\"Removing event {0}({1},{2})\".format(event['function'].__name__, event['args'], event['kwargs']))\n        except AttributeError:\n            self.log.debug(\"Removing event {0}\".format(str(event)))\n\n        try:\n            event_list.remove(event)\n        except ValueError:\n            try:\n                self.log.warn(\"Unable to remove event {0}({1},{2}) , not found in list: {3}\".format(event['function'].__name__, event['args'], event['kwargs'], event_list))\n            except AttributeError:\n                self.log.debug(\"Unable to remove event {0}\".format(str(event)))\n            raise KeyError('Unable to unregister the specified event from the signals specified')"}
{"prompt": "def get_system_per_cpu_times():\n    \"\"\"Return system per-CPU times as a list of named tuples.\"\"\"", "output": "\n    system_per_cpu = []\n    for i in range(1, 101):\n        cpu_times = psutil.cpu_times_percent(percpu=True)\n        cpu_time = psutil.cpu_times()[i-1]\n        system_per_cpu.append(SystemPerCPU(cpu_time))\n    return system_per_cpu\n\n", "solution": "\n    ret = []\n    for cpu_t in _psutil_mswindows.get_system_cpu_times():\n        user, system, idle = cpu_t\n        item = _cputimes_ntuple(user, system, idle)\n        ret.append(item)\n    return ret"}
{"prompt": "def queue_startfile(self, cmdfile):\n        \"\"\"Arrange for file of debugger commands to get read in the\n        process-command loop.\"\"\"", "output": "\n        self.queue.append(cmdfile)\n\n    def queue_endfile(self):\n        \"\"\"Remove the last file from the queue and return it to the\n        previous file.\"\"\"\n        if len(self.queue) > 0:\n            lastfile = self.queue.pop()\n            self.previousfile = lastfile\n            return lastfile\n        else:\n            return None\n\n    def run(self):\n        \"\"\"Run the debugger.\"\"\"\n        while True:\n            if self.queue_startfile(None):\n                return\n            el", "solution": "\n        expanded_cmdfile = os.path.expanduser(cmdfile)\n        is_readable = Mfile.readable(expanded_cmdfile)\n        if is_readable:\n            self.cmd_queue.append('source ' + expanded_cmdfile)\n        elif is_readable is None:\n            self.errmsg(\"source file '%s' doesn't exist\" % expanded_cmdfile)\n        else:\n            self.errmsg(\"source file '%s' is not readable\" %\n                        expanded_cmdfile)\n            pass\n        return"}
{"prompt": "def read_infile(infile: Union[Path, str], from_words=False,\n                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,\n                tag_column: int = TAG_COLUMN, max_sents: int = -1,\n                read_only_words: bool = False) -> List[Tuple[List, Union[List, None]]]:\n    \"\"\"Reads input file in CONLL-U format\n\n    Args:\n        infile: a path to a file\n        word_column: column containing words (default=1)\n        pos_column: column containing part-of-speech labels (default=3)\n        tag_column: column containing fine-grained tags (default=5)\n        max_sents: maximal number of sents to read\n        read_only_words: whether to read only words\n\n    Returns:\n        a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``\n        in case ``read_only_words = True``\n    \"\"\"", "output": "from_words: if True, assumes that the file contains words separated by whitespace\n        word_column: the index of the column containing the words (default: WORD_COLUMN)\n        pos_column: the index of the column containing the part-of-speech tags (default: POS_COLUMN)\n        tag_column: the index of the column containing the part-of-speech tags (default: TAG_COLUMN)\n        max_sents: the maximum number of sentences to read from the file (default: -1, read all)\n        read_only", "solution": "\n    answer, curr_word_sent, curr_tag_sent = [], [], []\n    if from_words:\n        word_column, read_only_words = 0, True\n    with open(infile, \"r\", encoding=\"utf8\") as fin:\n        for line in fin:\n            line = line.strip()\n            if line.startswith(\"#\"):\n                continue\n            if line == \"\":\n                if len(curr_word_sent) > 0:\n                    if read_only_words:\n                        curr_tag_sent = None\n                    answer.append((curr_word_sent, curr_tag_sent))\n                curr_tag_sent, curr_word_sent = [], []\n                if len(answer) == max_sents:\n                    break\n                continue\n            splitted = line.split(\"\\t\")\n            index = splitted[0]\n            if not from_words and not index.isdigit():\n                continue\n            curr_word_sent.append(splitted[word_column])\n            if not read_only_words:\n                pos, tag = splitted[pos_column], splitted[tag_column]\n                tag = pos if tag == \"_\" else \"{},{}\".format(pos, tag)\n                curr_tag_sent.append(tag)\n        if len(curr_word_sent) > 0:\n            if read_only_words:\n                curr_tag_sent = None\n            answer.append((curr_word_sent, curr_tag_sent))\n    return answer"}
{"prompt": "def fuzzed(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"", "output": "\n        return \"Fuzzed object: \" + str(self.fuzz)\n\n", "solution": "\n        try:\n            if self.config.strong_fuzz:\n                fuzzer = PJFMutators(self.config)\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                    else:\n                        return urllib.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                else:\n                    if type(self.config.json) in [list, dict]:\n                        return fuzzer.fuzz(json.dumps(self.config.json))\n                    else:\n                        return fuzzer.fuzz(self.config.json)\n            else:\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                    else:\n                        return urllib.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                else:\n                    return self.get_fuzzed(self.config.indent, self.config.utf8)\n        except Exception as e:\n            raise PJFBaseException(e.message if hasattr(e, \"message\") else str(e))"}
{"prompt": "def validate(self, request):\n        \"\"\"Validate JSON-RPC request.\n\n        :param request: RPC request object\n        :type request: dict\n\n        \"\"\"", "output": "\n        if not isinstance(request, dict):\n            return False\n\n        if'method' not in request:\n            return False\n\n        if not isinstance(request['method'], str):\n            return False\n\n        if not request['params']:\n            return False\n\n        if not isinstance(request['params'], list):\n            return False\n\n        for param in request['params']:\n            if not isinstance(param, dict):\n                return False\n\n            if 'name' not in param:\n                return False\n\n            if not isinstance(param['name'", "solution": "\n\n        try:\n            validate_version(request)\n            validate_method(request)\n            validate_params(request)\n            validate_id(request)\n        except (AssertionError, KeyError) as error:\n            invalid_request(error)"}
{"prompt": "def action_notify(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"", "output": "\n        for subscriber in self.subscribers:\n            subscriber.notify(action)\n\n    def notify(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"\n        for subscriber in self.subscribers:\n            subscriber.notify(action)\n\n", "solution": "\n        message = json.dumps({\n            'messageType': 'actionStatus',\n            'data': action.as_action_description(),\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass"}
{"prompt": "def update_mongo_compound_variants(self, bulk):\n        \"\"\"Update the compound information for a bulk of variants in the database\n\n            Args:\n                bulk(dict): {'_id': scout.models.Variant}\n\n        \"\"\"", "output": "\n        for variant in bulk:\n            compound = variant.get_mongo_compound()\n            if compound:\n                variant.set_mongo_compound(compound)\n\n", "solution": "\n        requests = []\n        for var_id in bulk:\n            var_obj = bulk[var_id]\n            if not var_obj.get('compounds'):\n                continue\n            # Add a request to update compounds\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': var_obj['compounds']\n                    }\n                })\n            requests.append(operation)\n\n        if not requests:\n            return\n\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating compounds failed\")\n            raise err"}
{"prompt": "def module_list(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"", "output": "\n    import os\n    return [name for name in os.listdir(path) if not name.startswith('.')]\n\n", "solution": "\n    # sys.path has the cwd as an empty string, but isdir/listdir need it as '.'\n    if path == '':\n        path = '.'\n\n    if os.path.isdir(path):\n        folder_list = os.listdir(path)\n    elif path.endswith('.egg'):\n        try:\n            folder_list = [f for f in zipimporter(path)._files]\n        except:\n            folder_list = []\n    else:\n        folder_list = []\n\n    if not folder_list:\n        return []\n\n    # A few local constants to be used in loops below\n    isfile = os.path.isfile\n    pjoin = os.path.join\n    basename = os.path.basename\n\n    def is_importable_file(path):\n        "}
{"prompt": "def modify_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"", "output": "\n    import redbaron\n    conf = {\n        \"theme\": \"light\",\n        \"language\": \"python\",\n        \"punctuation\": \"!\"\n    }\n    redbaron.confirm_changes(conf)\n    conf[\"theme\"] = \"dark\"\n    redbaron.confirm_changes(conf)\n    conf[\"language\"] = \"java\"\n    redbaron.confirm_changes(conf)\n    conf[\"punctuation\"] = \"?\"\n    redbaron.confirm_changes(conf)\n    return conf\n\n", "solution": "\n    import redbaron\n    import ubelt as ub\n    conf_path = 'docs/conf.py'\n\n    source = ub.readfrom(conf_path)\n    red = redbaron.RedBaron(source)\n\n    # Insert custom extensions\n    extra_extensions = [\n        '\"sphinxcontrib.napoleon\"'\n    ]\n\n    ext_node = red.find('name', value='extensions').parent\n    ext_node.value.value.extend(extra_extensions)\n\n    # Overwrite theme to read-the-docs\n    theme_node = red.find('name', value='html_theme').parent\n    theme_node.value.value = '\"sphinx_rtd_theme\"'\n\n    ub.writeto(conf_path, red.dumps())"}
{"prompt": "def show(self, *args, **kwargs):\n        \"\"\"\n        Shows the chart URL in a webbrowser\n\n        Other arguments passed to webbrowser.open\n        \"\"\"", "output": "\n        import webbrowser\n        webbrowser.open(self.chart_url, *args, **kwargs)\n\n", "solution": "\n        from webbrowser import open as webopen\n        return webopen(str(self), *args, **kwargs)"}
{"prompt": "def getView(self,name):\n        \"\"\"\n        Returns the view with name ``name``\\ .\n        \n        Raises a :py:exc:`ValueError` if the view does not exist.\n        \"\"\"", "output": "\n        if name not in self.views:\n            raise ValueError(\"View '{}' does not exist\".format(name))\n        return self.views[name]\n    \n    def setView(self,name,view):\n        \"\"\"\n        Sets the view with name ``name`` to be the view object passed in.\n        \n        Raises a :py:exc:`ValueError` if the view object is not callable.\n        \"\"\"\n        if not callable(view):\n            raise ValueError(\"View object must be callable\")\n        self.", "solution": "\n        if name not in self.views:\n            raise ValueError(\"Unknown world view\")\n        return self.views[name]"}
{"prompt": "def _process_execute_error(self, msg):\n        \"\"\" Process a reply for an execution request that resulted in an error.\n        \"\"\"", "output": "\n        if self.verbose:\n            print(msg)\n        self.reply = msg\n\n    def _process_invalid_input(self, msg):\n        \"\"\" Process an invalid input message.\n        \"\"\"\n        if self.verbose:\n            print(msg)\n        self.reply = msg\n\n    def _process_unknown_error(self, msg):\n        \"\"\" Process an unknown error message.\n        \"\"\"\n        if self.verbose:\n            print(msg)\n        self.reply = msg\n\n    def execute(self):\n        \"\"\" Execute", "solution": "\n        content = msg['content']\n        # If a SystemExit is passed along, this means exit() was called - also\n        # all the ipython %exit magic syntax of '-k' to be used to keep\n        # the kernel running\n        if content['ename']=='SystemExit':\n            keepkernel = content['evalue']=='-k' or content['evalue']=='True'\n            self._keep_kernel_on_exit = keepkernel\n            self.exit_requested.emit(self)\n        else:\n            traceback = ''.join(content['traceback'])\n            self._append_plain_text(traceback)"}
{"prompt": "def s_add(self, path, function, method=None, type_cast=None):\n        \"\"\"Function for registering a simple path.\n\n        Args:\n            path (str): Path to be matched.\n            function (function): Function to associate with this path.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DELETE. You may use whatever fits your situation though.\n                Defaults to None.\n            type_cast (dict, optional): Mapping between the param name and\n                one of `int`, `float` or `bool`. The value reflected by the\n                provided param name will than be casted to the given type.\n                Defaults to None.\n        \"\"\"", "output": " or `str`. If provided, the value will be\n                converted to the type specified in the `type_cast` argument.\n                Defaults to None.\n\n        Returns:\n            function: The registered function.\n\n        Raises:\n            ValueError: If the path already exists.\n        \"\"\"\n        if method is None:\n            method = 'POST' if function.startswith('post') else 'GET'\n        if type_cast is None:\n            type_cast = {}\n\n        def wrapper(func):\n            self.paths[path] = (func", "solution": "\n        with self._lock:\n            try:\n                path = '^/{}'.format(path.lstrip('/'))\n                path = '{}/$'.format(path.rstrip('/'))\n                path = path.replace('<', '(?P<')\n                path = path.replace('>', '>[^/]*)')\n\n                self.add(path, function, method, type_cast)\n            except Exception:\n                pass"}
{"prompt": "def login(request, template_name='ci/login.html',\n          redirect_field_name=REDIRECT_FIELD_NAME,\n          authentication_form=AuthenticationForm):\n    \"\"\"\n    Displays the login form and handles the login action.\n    \"\"\"", "output": "\n    # Render the login form template\n    return render(request, template_name, {\n       'redirect_field_name': redirect_field_name,\n        'authentication_form': authentication_form\n    })\n\n", "solution": "\n    redirect_to = request.POST.get(redirect_field_name,\n                                   request.GET.get(redirect_field_name, ''))\n\n    if request.method == \"POST\":\n        form = authentication_form(request, data=request.POST)\n        if form.is_valid():\n\n            # Ensure the user-originating redirection url is safe.\n            if not is_safe_url(url=redirect_to, host=request.get_host()):\n                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n            # Okay, security check complete. Get the user object from auth api.\n            user = form.get_user()\n            request.session['user_token'] = user[\"token\"]\n            request.session['user_email'] = user[\"email\"]\n            request.session['user_permissions'] = user[\"permissions\"]\n            request.session['user_id'] = user[\"id\"]\n            request.session['user_list'] = user[\"user_list\"]\n\n            if not settings.HIDE_DASHBOARDS:\n                # Set user dashboards because they are slow to change\n                dashboards = ciApi.get_user_dashboards(user[\"id\"])\n                dashboard_list = list(dashboards['results'])\n                if len(dashboard_list) > 0:\n                    request.session['user_dashboards'] = \\\n                        dashboard_list[0][\"dashboards\"]\n                    request.session['user_default_dashboard'] = \\\n                        dashboard_list[0][\"default_dashboard\"][\"id\"]\n                else:\n                    request.session['user_dashboards'] = []\n                    request.session['user_default_dashboard'] = None\n\n            # Get the user access tokens too and format for easy access\n            tokens = ciApi.get_user_service_tokens(\n                params={\"user_id\": user[\"id\"]})\n            token_list = list(tokens['results'])\n            user_tokens = {}\n            if len(token_list) > 0:\n                for token in token_list:\n                    user_tokens[token[\"service\"][\"name\"]] = {\n                        \"token\": token[\"token\"],\n                        \"url\": token[\"service\"][\"url\"] + \"/api/v1\"\n                    }\n            request.session['user_tokens'] = user_tokens\n\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = authentication_form(request)\n\n    current_site = get_current_site(request)\n\n    context = {\n        'form': form,\n        redirect_field_name: redirect_to,\n        'site': current_site,\n        'site_name': current_site.name,\n    }\n\n    return TemplateResponse(request, template_name, context)"}
{"prompt": "def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"", "output": "return: A tuple containing the file-like object and the offset\n        \"\"\"\n        return (fileobj, offset)\n\n    def get_io_read_task(self, fileobj, offset):\n        \"\"\"Get an IO read task for the requested offset\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to read from\n\n        :type offset: integer\n        :param offset: The offset to read the data from in the file-", "solution": "\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            }\n        )"}
{"prompt": "def __notify_listeners(self, data, stats):\n        \"\"\" notify all listeners about aggregate data and stats \"\"\"", "output": "\n        for listener in self.listeners:\n            listener.notify(data, stats)\n\n    def __init__(self, listeners):\n        self.listeners = listeners\n\n", "solution": "\n        for listener in self.listeners:\n            listener.on_aggregated_data(data, stats)"}
{"prompt": "def utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"", "output": "\n    try:\n        return unicode_str.encode('utf-8')\n    except UnicodeEncodeError:\n        return unicode_str.encode('utf-8', 'ignore')\n\n", "solution": "\n    if six.PY2 and isinstance(unicode_str, __unicode__):\n        return unicode_str.encode('utf-8')\n\n    return unicode_str"}
{"prompt": "def csoftmax_for_slice(input):\n    \"\"\" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n    Returns:\n        output: A list of [csoftmax results, masks]\n    \"\"\"", "output": "\n    input = input[0]\n    input_len = input.size(0)\n    input = input.unsqueeze(1)\n    input = input.expand(input_len, input_len, -1)\n    input = input.contiguous().view(-1, input.size(2))\n    input = F.normalize(input, p=1, dim=1)\n    input = input.unsqueeze(0)\n    input = input.expand(input_len, -1, -1)\n    input = input", "solution": "\n\n    [ten, u] = input\n\n    shape_t = ten.shape\n    shape_u = u.shape\n\n    ten -= tf.reduce_mean(ten)\n    q = tf.exp(ten)\n    active = tf.ones_like(u, dtype=tf.int32)\n    mass = tf.constant(0, dtype=tf.float32)\n    found = tf.constant(True, dtype=tf.bool)\n\n    def loop(q_, mask, mass_, found_):\n        q_list = tf.dynamic_partition(q_, mask, 2)\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,\n        #  1 element if true\n\n        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])\n        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])\n\n        # condition verification and mask modification\n        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,\n                                                 2)  # 0 when u is bigger than p, 1 when u is less than p\n\n        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)\n        split_u = tf.dynamic_partition(u, less_mask, 2)\n\n        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])\n        mass_ += tf.reduce_sum(split_u[1])\n\n        mask = mask * (tf.ones_like(less_mask) - less_mask)\n\n        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),\n                         lambda: False,\n                         lambda: True)\n\n        alpha = tf.reshape(alpha, q_.shape)\n\n        return alpha, mask, mass_, found_\n\n    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,\n                                         body=loop,\n                                         loop_vars=(q, active, mass, found))\n\n    return [csoft, mask_]"}
{"prompt": "def lazy(function, *args, **kwargs):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n        args: Positional arguments, passed directly to ``function``.\n\n        kwargs: Keyword arguments, passed directly to ``function``.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function`` with\n            the specified arguments.\n\n    \"\"\"", "output": "\n    def callable_lazy(*args, **kwargs):\n        return function(*args, **kwargs)\n\n    return callable_lazy\n\n", "solution": "\n\n    NOT_EVALUATED = object()\n    retval = [NOT_EVALUATED]\n\n    def evaluate():\n        if retval[0] is NOT_EVALUATED:\n            retval[0] = function(*args, **kwargs)\n        return retval[0]\n\n    return evaluate"}
{"prompt": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"", "output": "_structure: A structure with the same structure as `to_structure`, but with\n      possibly tiled elements from `from_structure`.\n\n  Raises:\n    ValueError: If `from_structure` and `to_structure` have different structures.\n  \"\"\"\n  if not isinstance(from_structure, (list, tuple)):\n    raise ValueError(\"from_structure must be a list or tuple\")\n  if not isinstance(to_structure, (list, tuple)):\n    raise ValueError(\"to_structure must be", "solution": "\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)"}
{"prompt": "def main():\n    \"\"\"Initialize and run command line interface.\"\"\"", "output": "\n    import argparse\n    import sys\n\n    # Create argument parser\n    parser = argparse.ArgumentParser(description='Process some integers.')\n    parser.add_argument('integers', metavar='N', type=int, nargs='+',\n                        help='an integer to be processed')\n    parser.add_argument('-o', '--operation', default='multiply',\n                        help='the operation to perform on the integers')\n    parser.add_argument('-s', '--sum', action='store_true',\n                        help='sum", "solution": "\n    # Create and populate parser\n    parser = create_parser()\n\n    # Parse given arguments\n    args = parser.parse_args()\n\n    # Checking arguments\n    check_arguments(args, parser)\n\n    # BUSINESS LOGIC IS FOLLOWING\n    run(parser, args)"}
{"prompt": "def from_tuplelist(tuple_list):\n        \"\"\"\n        Populates a Layout from a list containing virtual\n        qubits---(QuantumRegister, int) tuples---, or None.\n\n        Args:\n            tuple_list (list):\n                e.g.: [qr[0], None, qr[2], qr[3]]\n        Returns:\n            Layout: the corresponding Layout object\n        Raises:\n            LayoutError: If the elements are not (Register, integer) or None\n        \"\"\"", "output": "\n        layout = Layout()\n        for i, qubit in enumerate(tuple_list):\n            if qubit is None:\n                layout[i] = None\n            elif isinstance(qubit, tuple) and len(qubit) == 2 and isinstance(qubit[0], QuantumRegister):\n                layout[i] = qubit[0]\n            elif isinstance(qubit, int):\n                layout[i] = qubit\n            else:\n                raise LayoutError(\"Invalid tuple element: {}\".format(qubit))\n        return layout", "solution": "\n        out = Layout()\n        for physical, virtual in enumerate(tuple_list):\n            if virtual is None:\n                continue\n            elif Layout.is_virtual(virtual):\n                if virtual in out._v2p:\n                    raise LayoutError('Duplicate values not permitted; Layout is bijective.')\n                out[virtual] = physical\n            else:\n                raise LayoutError(\"The list should contain elements of the form\"\n                                  \" (Register, integer) or None\")\n        return out"}
{"prompt": "def reject_connection(self, reason=None):\n        \"\"\"\n        Reject a pending connection.\n        \"\"\"", "output": "\n        if reason is not None:\n            self.reason = reason\n        self.status = 'disconnected'\n        self.timestamp = None\n        self.error = None\n        self.connection_type = None\n        self.is_connected = False\n\n    def is_connection_closed(self):\n        \"\"\"\n        Check if the connection is closed.\n        \"\"\"\n        return self.status == 'disconnected'\n\n    def is_connection_open(self):\n        \"\"\"\n        Check if the connection is open.\n        \"\"\"\n        return self.status!=", "solution": "\n        assert self.pending, \"Connection is not pending.\"\n        if reason is None:\n            reason = ConnectionRefusedError()\n        self._accept_d.errback(reason)"}
{"prompt": "def signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"", "output": "\n    if len(frame) == 8:\n        return \"SHA1\"\n    elif len(frame) == 20:\n        return \"SHA256\"\n    elif len(frame) == 64:\n        return \"SHA512\"\n    else:\n        raise ValueError(\"Invalid frame length\")\n\n", "solution": "\n    if not frame: return None\n    code = frame.f_code\n    return (code.co_name, code.co_filename, code.co_firstlineno)"}
{"prompt": "async def parse_tag_results(soup):\n    \"\"\"\n    Parse a page of tag or trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of tags, Nothing else really useful there\n    \"\"\"", "output": "\n    tags = []\n    for tag in soup.find_all(class_=\"tag\"):\n        tags.append(tag)\n    return tags\n\n", "solution": "\n    soup = soup.find_all('td', class_='tc3')\n    tags = []\n    for item in soup:\n        tags.append(item.a.string)\n    return tags"}
{"prompt": "def mean(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the mean for expression, possibly on a grid defined by binby.\n\n        Example:\n\n        >>> df.mean(\"x\")\n        -0.067131491264005971\n        >>> df.mean(\"(x**2+y**2)**0.5\", binby=\"E\", shape=4)\n        array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}\n        \"\"\"", "output": ".24264069,   6.08276253,   8.48528137])\n\n        Parameters:\n        expression (str): The expression to calculate the mean of.\n        binby (str or list): The column(s) to bin by.\n        limits (tuple): The lower and upper limits for the binning.\n        shape (int): The number of bins to use for the histogram.\n        selection (bool): Whether to include only the selected values in the calculation.\n        delay (bool): Whether to delay the calculation until the result is actually available.\n        ", "solution": "\n        return self._compute_agg('mean', expression, binby, limits, shape, selection, delay, edges, progress)\n        logger.debug(\"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\", expression, binby, limits, shape, selection, delay)\n        expression = _ensure_strings_from_expressions(expression)\n        selection = _ensure_strings_from_expressions(selection)\n        binby = _ensure_strings_from_expressions(binby)\n\n        @delayed\n        def calculate(expression, limits):\n            task = tasks.TaskStatistic(self, binby, shape, limits, weight=expression, op=tasks.OP_ADD_WEIGHT_MOMENTS_01, selection=selection)\n            self.executor.schedule(task)\n            progressbar.add_task(task, \"mean for %s\" % expression)\n            return task\n\n        @delayed\n        def finish(*stats_args):\n            stats = np.array(stats_args)\n            counts = stats[..., 0]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                mean = stats[..., 1] / counts\n            return vaex.utils.unlistify(waslist, mean)\n        waslist, [expressions, ] = vaex.utils.listify(expression)\n        progressbar = vaex.utils.progressbars(progress)\n        limits = self.limits(binby, limits, delay=True)\n        stats = [calculate(expression, limits) for expression in expressions]\n        var = finish(*stats)\n        return self._delay(delay, var)"}
{"prompt": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"", "output": "\n  import numpy as np\n  \n  # Define a function to generate a random image and label\n  def generate_image_label():\n    # Generate a random image with shape (28, 28, 1)\n    image = np.random.randint(0, 256, size=(28, 28, 1))\n    # Generate a random label between 0 and 9\n    label = np.random.randint(0, 10)\n    # Return the image and label as a tuple\n    return image, label\n  \n  # Generate a list of fake images", "solution": "\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data"}
{"prompt": "def min(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the minimum for given expressions, possibly on a grid defined by binby.\n\n\n        Example:\n\n        >>> df.min(\"x\")\n        array(-128.293991)\n        >>> df.min([\"x\", \"y\"])\n        array([-128.293991 ,  -71.5523682])\n        >>> df.min(\"x\", binby=\"x\", shape=5, limits=[-10, 10])\n        array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}, the last dimension is of shape (2)\n        \"\"\"", "output": "\", limits=[-128, 127])\n        array([-128.293991,  -71.5523682])\n\n        :param expression: A string or list of strings representing the expressions to calculate the minimum for.\n        :param binby: A string or list of strings representing the variables to bin by.\n        :param limits: A tuple of two floats representing the lower and upper limits for the binning.\n        :param shape: A tuple of two integers representing the shape of the grid to bin by.\n        :param selection: A boolean indicating whether to only consider the minimum for the", "solution": "\n        return self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress)\n        @delayed\n        def finish(result):\n            return result[..., 0]\n        return self._delay(delay, finish(self.minmax(expression, binby=binby, limits=limits, shape=shape, selection=selection, delay=delay, progress=progress)))"}
{"prompt": "def _create_value(self, *args, **kwargs):\n        \"\"\"\n        Lowest value generator.\n\n        Separated from __call__, because it seems that python\n        cache __call__ reference on module import\n        \"\"\"", "output": "\n        return None\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Returns the lowest value generated by the generator function.\n        \"\"\"\n        value = self._create_value(*args, **kwargs)\n        if value is not None:\n            self.cache[args] = value\n        return value\n\n", "solution": "\n        if not len(args):\n            raise TypeError('Object instance is not provided')\n\n        if self.by_instance:\n            field_type = args[0]\n        else:\n            field_type = args[0].__class__\n\n        function = self.registry.get(field_type, self.default)\n\n        if function is None:\n            raise TypeError(\"no match %s\" % field_type)\n\n        return function(*args, **kwargs)"}
{"prompt": "def join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"", "output": "\n    prev_line = ''\n    for line in iterator:\n        if line.endswith('\\\\'):\n            yield prev_line + line\n        else:\n            yield prev_line + line + '\\n'\n        prev_line = line\n    yield prev_line\n\n", "solution": "\n    lines = []\n    for line in iterator:\n        if not line.endswith('\\\\'):\n            if lines:\n                lines.append(line)\n                yield ''.join(lines)\n                lines = []\n            else:\n                yield line\n        else:\n            lines.append(line.strip('\\\\'))"}
{"prompt": "def handle(self):\n        \"\"\"\n        Handle the backend of the given status.\n        \"\"\"", "output": "\n        # Your code here\n        pass\n\n    def get_status(self):\n        \"\"\"\n        Returns the status of the backend.\n        \"\"\"\n        # Your code here\n        pass\n\n    def get_data(self):\n        \"\"\"\n        Returns the data associated with the status.\n        \"\"\"\n        # Your code here\n        pass\n\n    def set_data(self, data):\n        \"\"\"\n        Sets the data associated with the status.\n        \"\"\"\n        # Your code here\n        pass\n\n    def get_data_for_status(self, status):", "solution": "\n\n        # We initiate the source we are going to parse to the Generate class.\n        source = \"URL\"\n\n        if self.catched.lower() not in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n            # The parsed status is not in the list of invalid.\n\n            # We generate the status file with the catched status.\n            Generate(self.catched, source).status_file()\n        else:\n            # The parsed status is in the list of invalid.\n\n            # We generate the status file with the parsed status.\n            Generate(self.catched, \"SYNTAX\").status_file()\n\n        # We return the parsed status.\n        return self.catched"}
{"prompt": "def _segmentPath(self, path):\n        \"\"\"Return list of valid *path* segments.\"\"\"", "output": "\n        segments = []\n        for segment in path.split(\"/\"):\n            if segment == \"..\":\n                segments.pop()\n            elif segment!= \".\" and segment!= \"\":\n                segments.append(segment)\n        return segments\n\n    def _getPath(self, path, segments):\n        \"\"\"Return list of valid *path* segments.\"\"\"\n        if not segments:\n            return [path]\n        new_paths = []\n        for segment in segments:\n            if segment.startswith(\"*\"):\n                if segment", "solution": "\n        parts = []\n        model = self._filesystemWidget.model()\n\n        # Separate root path from remainder.\n        remainder = path\n\n        while True:\n            if remainder == model.root.path:\n                break\n\n            if remainder:\n                parts.append(remainder)\n\n            head, tail = os.path.split(remainder)\n            if head == remainder:\n                break\n\n            remainder = head\n\n        parts.append(model.root.path)\n        return parts"}
{"prompt": "def p_gate_op_5(self, program):\n        \"\"\"\n        gate_op : BARRIER id_list ';'\n        \"\"\"", "output": "\n        gate_name = program[1]\n        operands = program[3:]\n        self.gates[gate_name] = (operands, None)\n\n    def p_gate_op_6(self, program):\n        \"\"\"\n        gate_op : BEGIN gate_name ';'\n        \"\"\"\n        gate_name = program[2]\n        self.gates[gate_name] = (None, None)\n\n    def p_gate_op_7(self, program):\n        \"\"\"\n        gate_op : END gate_name '", "solution": "\n        program[0] = node.Barrier([program[2]])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def luhn_check(card_number):\n    \"\"\" checks to make sure that the card passes a luhn mod-10 checksum \"\"\"", "output": "\n    digits = [int(d) for d in str(card_number)]\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = sum(odd_digits)\n    for digit in even_digits:\n        checksum += sum(divmod(digit*2, 10))\n    return checksum % 10 == 0\n\n", "solution": "\n    sum = 0\n    num_digits = len(card_number)\n    oddeven = num_digits & 1\n\n    for count in range(0, num_digits):\n        digit = int(card_number[count])\n\n        if not ((count & 1) ^ oddeven):\n            digit *= 2\n        if digit > 9:\n            digit -= 9\n\n        sum += digit\n\n    return (sum % 10) == 0"}
{"prompt": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"", "output": ".]],\n    #     [[ 0.,  0.,  0.,  0.,  0.]],\n    #     [[ 0.,  0.,  0.,  0.,  0.]]]\n\n    # A queue capable of holding 2 elements.\n    _make_empty_queue_for(2, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.]],\n    #     [[ 0.,  0.,  0.,  0.,  0.]]]\n\n    # A queue capable of holding 1 element.\n    _make_empty_queue", "solution": "\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)"}
{"prompt": "def join(self):\n        \"\"\"Waits until all there are no processes enqueued.\"\"\"", "output": "\n        while not self.processes:\n            time.sleep(1)\n\n    def process(self):\n        \"\"\"Processes the next process in the queue.\"\"\"\n        process = self.processes.pop(0)\n        process.run()\n\n    def enqueue(self, process):\n        \"\"\"Adds a process to the end of the queue.\"\"\"\n        self.processes.append(process)\n\n    def run(self):\n        \"\"\"Runs the processes in the queue.\"\"\"\n        self.join()\n        for process in self.processes:\n            ", "solution": "\n        while True:\n            with self._processes_lock:\n                if not self._processes and self._processing_semaphore.locked():\n                    return True\n            gevent.sleep(0.1)"}
{"prompt": "def phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    \"\"\"\n    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)\n    data from a noise diode measurement on the target\n    \"\"\"", "output": "\n    # Calculate the number of coarse channels\n    num_channels = chan_per_coarse * feedtype.count('c')\n    \n    # Calculate the number of samples per channel\n    samples_per_channel = int(len(Idat) / num_channels)\n    \n    # Initialize the phase offset array\n    phase_offsets = np.zeros((num_channels, len(tsamp)))\n    \n    # Loop over the coarse channels\n    for i in range(num_channels):\n        # Calculate the", "solution": "\n    #Fold noise diode data and calculate ON OFF diferences for U and V\n    if feedtype=='l':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Vdiff = V_ON-V_OFF\n        poffset = np.arctan2(-1*Vdiff,Udiff)\n\n    if feedtype=='c':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Qdiff = Q_ON-Q_OFF\n        poffset = np.arctan2(Udiff,Qdiff)\n\n    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)\n\n    #Correct for problems created by discontinuity in arctan\n    #Find whether phase offsets have increasing or decreasing slope\n    y = coarse_p[:6]\n    x = np.arange(y.size)\n    m = np.polyfit(x,y,1)[0]\n\n    for i in range(coarse_p.size-3):\n        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):\n            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next\n\n    return coarse_p"}
{"prompt": "def filter_on(self, filt=None, analyte=None, samples=None, subset=None, show_status=False):\n        \"\"\"\n        Turns data filters on for particular analytes and samples.\n\n        Parameters\n        ----------\n        filt : optional, str or array_like\n            Name, partial name or list of names of filters. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            filters with 'cluster' in the name are activated.\n            Defaults to all filters.\n        analyte : optional, str or array_like\n            Name or list of names of analytes. Defaults to all analytes.\n        samples : optional, array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n\n        Returns\n        -------\n        None\n        \"\"\"", "output": " str or array_like\n            Name or list of names of analytes to activate.\n            Defaults to all analytes.\n        samples : optional, str or array_like\n            Name or list of names of samples to activate.\n            Defaults to all samples.\n        subset : optional, str or array_like\n            Name or list of names of samples to subset.\n            Defaults to all samples.\n        show_status : optional, bool\n            If True, prints a status message after activating filters.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            try:\n                self.data[s].filt.on(analyte, filt)\n            except:\n                warnings.warn(\"filt.on failure in sample \" + s)\n\n        if show_status:\n            self.filter_status(subset=subset)\n        return"}
{"prompt": "def _add_https(self, q):\n        \"\"\"for push, pull, and other api interactions, the user can optionally\n           define a custom registry. If the registry name doesn't include http\n           or https, add it.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"", "output": "\n        if 'http' in q and 'https' in q:\n            registry = q['registry']\n            if registry not in self.registries:\n                self.registries[registry] = {'push': [], 'pull': []}\n            self.registries[registry]['push'].append(q['name'])\n            self.registries[registry]['pull'].append(q['name'])\n        else:\n            for r in self.registries:\n                if q['registry'] in r:\n                    r", "solution": "\n\n        # If image uses http or https, add back\n        if not q['registry'].startswith('http'):\n\n            if q['original'].startswith('http:'):\n                q['registry'] = 'http://%s' % q['registry']\n\n            elif q['original'].startswith('https:'):\n                q['registry'] = 'https://%s' % q['registry']\n\n            # Otherwise, guess from the user's environment\n            else:\n\n                prefix = 'https://'\n\n                # The user can set an environment variable to specify nohttps\n                nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')\n                if nohttps != None:\n                    prefix = 'http://'\n                q['registry'] = '%s%s' %(prefix, q['registry'])\n\n        return q"}
{"prompt": "def clean_tempfiles():\n  \"\"\"Clean up temp files\"\"\"", "output": "\n  for file in tempfiles:\n    file.close()\n    os.remove(file.name)\n\n", "solution": "\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)"}
{"prompt": "def get_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return section resource for given sis id.\n        \"\"\"", "output": "\n        section = None\n        for section in self.sections:\n            if section.sis_id == sis_section_id:\n                section = section.as_dict(params)\n                return section\n        return section\n\n", "solution": "\n        return self.get_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)"}
{"prompt": "async def get_tracks(self, *, limit=20, offset=0) -> List[Track]:\n        \"\"\"Get a list of the songs saved in the current Spotify user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        \"\"\"", "output": "\n        # Make API request to Spotify API\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\n                f\"https://api.spotify.com/v1/me/tracks\",\n                params={\"limit\": limit, \"offset\": offset},\n            ) as response:\n                response_data = await response.json()\n\n        # Parse response data into Track objects\n        tracks = []\n        for item in response_data[\"items\"]:\n            track = Track(\n                name=item[\"name\"],\n                artist", "solution": "\n        data = await self.user.http.saved_tracks(limit=limit, offset=offset)\n\n        return [Track(self.__client, item['track']) for item in data['items']]"}
{"prompt": "def get_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be read.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"", "output": "\n        while True:\n            try:\n                return self.buffer_element\n            except BufferElementError:\n                time.sleep(0.1)\n\n    def set_direct(self, value):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be read.\n\n        :param value: The value to set the buffer element to.\n        \"\"\"\n        while True:\n            try:\n                self.buffer_element = value\n                return\n            except BufferElementError:\n                time.sleep(0.1)\n\n    ", "solution": "\n\n        read_idx = self.__get_idx()\n\n        if read_idx is QueueClosed:\n            return QueueClosed\n\n        # Once the guard is released, read_idx will be placed into write_queue.\n        return self.Guard(self.write_queue, self.arys, lambda: read_idx)"}
{"prompt": "def _get_format_from_style(self, token, style):\n        \"\"\" Returns a QTextCharFormat for token by reading a Pygments style.\n        \"\"\"", "output": "\n        from pygments import highlight\n        from pygments.formatters import HtmlFormatter\n        from pygments.lexers import get_lexer_by_name\n\n        # Get lexer and formatter for style\n        lexer = get_lexer_by_name(style)\n        formatter = HtmlFormatter()\n\n        # Highlight tokens in token string\n        highlighted_tokens = highlight(token, lexer, formatter)\n\n        # Combine tokens into formatted string\n        return ''.join(highlighted_tokens)\n\n    ", "solution": "\n        result = QtGui.QTextCharFormat()\n        for key, value in style.style_for_token(token).items():\n            if value:\n                if key == 'color':\n                    result.setForeground(self._get_brush(value))\n                elif key == 'bgcolor':\n                    result.setBackground(self._get_brush(value))\n                elif key == 'bold':\n                    result.setFontWeight(QtGui.QFont.Bold)\n                elif key == 'italic':\n                    result.setFontItalic(True)\n                elif key == 'underline':\n                    result.setUnderlineStyle(\n                        QtGui.QTextCharFormat.SingleUnderline)\n                elif key == 'sans':\n                    result.setFontStyleHint(QtGui.QFont.SansSerif)\n                elif key == 'roman':\n                    result.setFontStyleHint(QtGui.QFont.Times)\n                elif key == 'mono':\n                    result.setFontStyleHint(QtGui.QFont.TypeWriter)\n        return result"}
{"prompt": "def p_closed_proposition_list(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition\n                                    | closed_proposition\"\"\"", "output": "\n        p[0] = [p[1]] + p[0][1:]\n\n    def p_closed_proposition_list_alt(self, p):\n        \"\"\" closed_proposition_list_alt :  closed_proposition_list_alt SLASH closed_proposition\n                                      | closed_proposition\"\"\"\n        p[0] = [p[1]] + p[0][1:]\n\n    def p_closed_proposition_list_alt_alt(self, p):\n        \"\"\" closed_proposition_list_alt_alt :  closed", "solution": "\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[0] = p[1] + [p[4]]"}
{"prompt": "def docker(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"", "output": "\n        future = Future()\n        thread = Thread(target=lambda: method(*args, **kwargs), args=(future,))\n        thread.start()\n        return future\n\n    def __getattr__(self, name):\n        \"\"\"Proxy any method calls to a docker container\"\"\"\n        return lambda *args, **kwargs: self.docker(getattr(self.container, name), *args, **kwargs)\n\n    def __enter__(self):\n        \"\"\"Start the container\"\"\"\n        self.container = Container(...)\n        return self\n\n    def __exit", "solution": "\n        return self.executor.submit(self._docker, method, *args, **kwargs)"}
{"prompt": "def restore_context(self) -> bool:\n        \"\"\"Rollback to previous saved position.\"\"\"", "output": "\n        if self.position is not None:\n            self.position = 0\n            return True\n        else:\n            return False\n\n", "solution": "\n        self._cursor.position = self._contexts.pop()\n        return False"}
{"prompt": "def wave_interp_option(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpOption` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"", "output": "\n    if not isinstance(obj, WaveInterpOption):\n        raise RuntimeError(f\"{obj} is not a valid WaveInterpOption pseudo-type object\")\n\n    # Replace *[argument_name]* with the name of the argument the contract is attached to\n    obj.token = f\"*{obj.token.split('*')[1]}*\"\n\n", "solution": "\n    exdesc = pexdoc.pcontracts.get_exdesc()\n    if not isinstance(obj, str):\n        raise ValueError(exdesc)\n    if obj.upper() in [\"CONTINUOUS\", \"STAIRCASE\"]:\n        return None\n    raise ValueError(exdesc)"}
{"prompt": "def run(self, dag):\n        \"\"\"\n        Pick a layout by assigning n circuit qubits to device qubits 0, .., n-1.\n\n        Args:\n            dag (DAGCircuit): DAG to find layout for.\n\n        Raises:\n            TranspilerError: if dag wider than self.coupling_map\n        \"\"\"", "output": "\n        if len(dag) > len(self.coupling_map):\n            raise TranspilerError(\"DAG wider than coupling map\")\n\n        # Pick a random circuit qubit and device qubit\n        circuit_qubits = list(dag.qubits())\n        device_qubits = list(range(len(dag.qubits())))\n        circuit_qubit, device_qubit = random.sample(circuit_qubits, 2)\n\n        # Apply the circuit to the device\n        circuit = dag.circuit(circ", "solution": "\n        num_dag_qubits = sum([qreg.size for qreg in dag.qregs.values()])\n        if num_dag_qubits > self.coupling_map.size():\n            raise TranspilerError('Number of qubits greater than device.')\n        self.property_set['layout'] = Layout.generate_trivial_layout(*dag.qregs.values())"}
{"prompt": "def update(self,dt):\n        \"\"\"\n        Should be called regularly to move the actor.\n        \n        This method does nothing if the :py:attr:`enabled` property is set to False.\n        \n        This method is called automatically and should not be called manually.\n        \"\"\"", "output": "\n        pass\n    \n    enabled = property(get_enabled, set_enabled)\n    \n    def get_position(self):\n        \"\"\"\n        Returns the current position of the actor as a tuple of two floats.\n        \"\"\"\n        # This is where the actual implementation of the function would go\n        pass\n    \n    position = property(get_position, set_position)\n\n", "solution": "\n        if not self.enabled:\n            return\n        dy = self.speed * dt * self.move\n        x,y,z = self.actor._pos\n        newpos = x,dy+y,z\n        self.actor.pos = newpos"}
{"prompt": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"", "output": "\n\t\timport urllib.request\n\t\timport ssl\n\n\t\t# Create an SSL context\n\t\tcontext = ssl.create_default_context()\n\n\t\t# If no authorize URL is provided, use the default one\n\t\tif authorize_url is None:\n\t\t\tauthorize_url = \"https://example.com/authorize\"\n\n\t\t# Open the URL and read the authorization code\n\t\twith urllib.request.urlopen(authorize_url, context=context) as f:\n\t\t\tcode = f.read().decode(\"utf-8\")\n\n\t\t# Start the", "solution": "\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()"}
{"prompt": "def transcode_to_stream(input_filename, date_format=None):\n    \"\"\"\n    Read a JSON or CSV file and convert it into a JSON stream, which will\n    be saved in an anonymous temp file.\n    \"\"\"", "output": "\n    import json\n    import tempfile\n    import shutil\n\n    # Create a temporary file to hold the JSON stream\n    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n        # Read the input file and convert it to JSON\n        with open(input_filename, 'r') as input_file:\n            json_data = json.load(input_file)\n\n        # Convert the JSON data to a JSON stream\n        json_stream = json.dumps(json_data)\n\n        # Write the JSON stream to", "solution": "\n    tmp = tempfile.TemporaryFile()\n    for entry in open_json_or_csv_somehow(input_filename,\n                                          date_format=date_format):\n        tmp.write(json.dumps(entry, ensure_ascii=False).encode('utf-8'))\n        tmp.write(b'\\n')\n    tmp.seek(0)\n    return tmp"}
{"prompt": "def visit_classdef(self, node):\n        \"\"\"visit an astroid.Class node\n\n         * set the locals_type and instance_attrs_type mappings\n         * set the implements list and build it\n         * optionally tag the node with a unique id\n        \"\"\"", "output": "\n        # set locals_type and instance_attrs_type mappings\n        self.locals_type = {}\n        self.instance_attrs_type = {}\n        for child in node.get_children():\n            if isinstance(child, astroid.AssignName):\n                self.locals_type[child.name] = child.type\n            elif isinstance(child, astroid.AssignAttr):\n                self.instance_attrs_type[child.attrname] = child.type\n        # set implements list and build it\n", "solution": "\n        if hasattr(node, \"locals_type\"):\n            return\n        node.locals_type = collections.defaultdict(list)\n        if self.tag:\n            node.uid = self.generate_id()\n        # resolve ancestors\n        for baseobj in node.ancestors(recurs=False):\n            specializations = getattr(baseobj, \"specializations\", [])\n            specializations.append(node)\n            baseobj.specializations = specializations\n        # resolve instance attributes\n        node.instance_attrs_type = collections.defaultdict(list)\n        for assignattrs in node.instance_attrs.values():\n            for assignattr in assignattrs:\n                self.handle_assignattr_type(assignattr, node)\n        # resolve implemented interface\n        try:\n            node.implements = list(interfaces(node, self.inherited_interfaces))\n        except astroid.InferenceError:\n            node.implements = ()"}
{"prompt": "def _create_input_transactions(self, addy):\n        # type: (Address) -> None\n        \"\"\"\n        Creates transactions for the specified input address.\n        \"\"\"", "output": "\n        # TODO: Implement transaction creation logic here\n        pass\n\n    def _validate_transaction(self, transaction):\n        # type: (Transaction) -> bool\n        \"\"\"\n        Validates a single transaction.\n        \"\"\"\n        # TODO: Implement transaction validation logic here\n        pass\n\n    def _broadcast_transaction(self, transaction):\n        # type: (Transaction) -> None\n        \"\"\"\n        Broadcasts a single transaction to all connected nodes.\n        \"\"\"\n        # TODO: Implement transaction broadcasting logic here\n        pass\n\n    def _broadcast", "solution": "\n        self._transactions.append(ProposedTransaction(\n            address=addy,\n            tag=self.tag,\n\n            # Spend the entire address balance; if necessary, we will\n            # add a change transaction to the bundle.\n            value=-addy.balance,\n        ))\n\n        # Signatures require additional transactions to store, due to\n        # transaction length limit.\n        # Subtract 1 to account for the transaction we just added.\n        for _ in range(addy.security_level - 1):\n            self._transactions.append(ProposedTransaction(\n                address=addy,\n                tag=self.tag,\n\n                # Note zero value; this is a meta transaction.\n                value=0,\n            ))"}
{"prompt": "def _define(self):\n        \"\"\"Calculate a subcircuit that implements this initialization\n\n        Implements a recursive initialization algorithm, including optimizations,\n        from \"Synthesis of Quantum Logic Circuits\" Shende, Bullock, Markov\n        https://arxiv.org/abs/quant-ph/0406176v5\n\n        Additionally implements some extra optimizations: remove zero rotations and\n        double cnots.\n        \"\"\"", "output": "\n        # Base case: this is the first initialization\n        if len(self.inputs) == 0:\n            return self\n\n        # Recursive case: apply all possible optimizations\n        for i in range(len(self.inputs)):\n            input_ = self.inputs[i]\n            output = self.outputs[i]\n\n            # Remove zero rotations\n            if input_ == 0:\n                continue\n\n            # Double cnots\n            if input_ == 2:\n                output = output[::-1]\n\n            # Apply all possible optimizations\n", "solution": "\n        # call to generate the circuit that takes the desired vector to zero\n        disentangling_circuit = self.gates_to_uncompute()\n\n        # invert the circuit to create the desired vector from zero (assuming\n        # the qubits are in the zero state)\n        initialize_instr = disentangling_circuit.to_instruction().inverse()\n\n        q = QuantumRegister(self.num_qubits, 'q')\n        initialize_circuit = QuantumCircuit(q, name='init_def')\n        for qubit in q:\n            initialize_circuit.append(Reset(), [qubit])\n        initialize_circuit.append(initialize_instr, q[:])\n\n        self.definition = initialize_circuit.data"}
{"prompt": "def generate_query(command):\n    \"\"\"Add header, checksum and footer to command data.\"\"\"", "output": "\n    header = \"X-Powered-By: Python/{}\\n\".format(sys.version_info.major)\n    checksum = hashlib.sha256(command.encode()).hexdigest()\n    footer = \"End of Service\\n\"\n    return header + command + checksum + footer\n\n", "solution": "\n    data = bytearray(command)\n    c = checksum(data)\n    data.append(c >> 8)\n    data.append(c & 0xFF)\n    data.replace(b'\\xFE', b'\\xFE\\xF0')\n\n    data = bytearray.fromhex(\"FEFE\") + data + bytearray.fromhex(\"FE0D\")\n    return data"}
{"prompt": "def copy(self):\n        \"\"\"\n        :returns: datasketch.MinHash -- A copy of this MinHash by exporting its state.\n        \"\"\"", "output": "\n        return datasketch.MinHash(self.size, self.seed, self.hash_function)\n\n    def export(self):\n        \"\"\"\n        :returns: dict -- A dictionary containing the state of the MinHash object.\n        \"\"\"\n        return {'size': self.size,'seed': self.seed, 'hash_function': self.hash_function}\n\n    def __eq__(self, other):\n        if isinstance(other, datasketch.MinHash):\n            return self.size == other.size and self.seed == other.", "solution": "\n        return MinHash(seed=self.seed, hashfunc=self.hashfunc,\n                hashvalues=self.digest(),\n                permutations=self.permutations)"}
{"prompt": "def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Publish runbook draft.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The parameters supplied to the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"", "output": ".\n        :type runbook_name: str\n        :param custom_headers: Optional custom headers to be added to the publish request.\n        :type custom_headers: dict\n        :param raw: Optional boolean value indicating whether to return the raw response or not.\n        :type raw: bool\n        :param polling: Optional boolean value indicating whether to poll for completion or not.\n        :type polling: bool\n        :param operation_config: Optional configuration parameters for the operation.\n        :type operation_config: dict\n        :return: The response from the publish runbook operation.", "solution": "\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _is_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async keyword.\"\"\"", "output": "\n    return \":async\" in o.get(\"meta\", \"\").lower()\n\n", "solution": "\n    return (  # type: ignore\n        Maybe(o.meta)\n        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))\n        .or_else_get(False)\n    )"}
{"prompt": "def add_value(self, name, value):\n        \"\"\"\n        Add a new value to the list.\n\n        :param str name: name of the value that is being parsed\n        :param str value: value that is being parsed\n        :raises ietfparse.errors.MalformedLinkValue:\n            if *strict mode* is enabled and a validation error\n            is detected\n\n        This method implements most of the validation mentioned in\n        sections 5.3 and 5.4 of :rfc:`5988`.  The ``_rfc_values``\n        dictionary contains the appropriate values for the attributes\n        that get special handling.  If *strict mode* is enabled, then\n        only values that are acceptable will be added to ``_values``.\n\n        \"\"\"", "output": "ictionary is used to store the valid values for each name.\n        \"\"\"\n        if self.strict:\n            if name in self._rfc_values:\n                raise errors.MalformedLinkValue(\n                    \"Duplicate name '{}' in link\".format(name)\n                )\n            self._rfc_values[name] = value\n\n    def parse(self, link):\n        \"\"\"\n        Parse a link and return its components.\n\n        :param link: a string representing a link\n        :return: a tuple containing the scheme, netloc, path", "solution": "\n        try:\n            if self._rfc_values[name] is None:\n                self._rfc_values[name] = value\n            elif self.strict:\n                if name in ('media', 'type'):\n                    raise errors.MalformedLinkValue(\n                        'More than one {} parameter present'.format(name))\n                return\n        except KeyError:\n            pass\n\n        if self.strict and name in ('title', 'title*'):\n            return\n\n        self._values.append((name, value))"}
{"prompt": "def sp_search_query(query):\n    \"\"\"Translate a Mopidy search query to a Spotify search query\"\"\"", "output": "\n    query = query.replace(' ', '+')\n    query = query.replace('(', '%3C')\n    query = query.replace(')', '%3E')\n    return query\n\n", "solution": "\n\n    result = []\n\n    for (field, values) in query.items():\n        field = SEARCH_FIELD_MAP.get(field, field)\n        if field is None:\n            continue\n\n        for value in values:\n            if field == 'year':\n                value = _transform_year(value)\n                if value is not None:\n                    result.append('%s:%d' % (field, value))\n            elif field == 'any':\n                result.append('\"%s\"' % value)\n            else:\n                result.append('%s:\"%s\"' % (field, value))\n\n    return ' '.join(result)"}
{"prompt": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"", "output": "\n        data = {}\n        for filename in self.filenames:\n            with open(filename, 'r') as f:\n                lines = f.readlines()\n                executed = sum(1 for line in lines if line.strip())\n                data[filename] = executed\n        if fullpath:\n            return {filename: data[filename] for filename in data}\n        else:\n            return data\n\n", "solution": "\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ"}
{"prompt": "def read_hex_integer(self) -> bool:\n    \"\"\"\n    read a hexadecimal number\n    Read the following BNF rule else return False::\n\n        readHexInteger = [\n            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+\n        ]\n    \"\"\"", "output": "\n    if not self.current_token:\n        return False\n    hex_digits = set('0123456789abcdefABCDEF')\n    if not set(self.current_token).issubset(hex_digits):\n        return False\n    self.next_token()\n    return True\n\n", "solution": "\n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    c = self._stream.peek_char\n    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):\n        self._stream.incpos()\n        while not self.read_eof():\n            c = self._stream.peek_char\n            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):\n                break\n            self._stream.incpos()\n        return self._stream.validate_context()\n    return self._stream.restore_context()"}
{"prompt": "def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"", "output": "\n        self.app = app\n        self.init_blueprint()\n\n    def init_blueprint(self):\n        \"\"\"\n        Initializes the Flask-Allows blueprint against the provided application\n        \"\"\"\n        self.blueprint = Blueprint('allows', __name__, url_prefix='/allows')\n\n    def register_blueprint(self):\n        \"\"\"\n        Registers the Flask-Allows blueprint against the provided application\n        \"\"\"\n        self.app.register_blueprint(self.blueprint)\n\n    def allow_users(self, users):\n        \"\"\"\n        ", "solution": "\n        if not hasattr(app, \"extensions\"):  # pragma: no cover\n            app.extensions = {}\n        app.extensions[\"allows\"] = self\n\n        @app.before_request\n        def start_context(*a, **k):\n            self.overrides.push(Override())\n            self.additional.push(Additional())\n\n        @app.after_request\n        def cleanup(response):\n            self.clear_all_overrides()\n            self.clear_all_additional()\n            return response"}
{"prompt": "def raw_dict_factory(cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types, including raw read hex strings.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        return {col[0]: col[1] for col in cursor.description}\n\n    return db.create_function('raw_dict_factory', 2, raw_dict_factory)\n\n", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp or name == Field.Meter_Address:\n                d[name] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":\n                d[name] = str(val)\n                continue\n        return d"}
{"prompt": "def _onSelectItem(self, selection, previousSelection):\n        \"\"\"Handle selection of item in listing.\"\"\"", "output": "\n        if selection in previousSelection:\n            return previousSelection\n        else:\n            previousSelection.append(selection)\n            return previousSelection\n\n    def _onSort(self, sorting, previousSorting):\n        \"\"\"Handle sorting of items in listing.\"\"\"\n        if sorting in previousSorting:\n            return previousSorting\n        else:\n            previousSorting.append(sorting)\n            return previousSorting\n\n    def onSelectItemChanged(self, selection, previousSelection):\n        \"\"\"Handle item selection change.\"\"\"\n        self.", "solution": "\n        self._acceptButton.setEnabled(True)\n        del self._selected[:]\n        item = self._filesystemWidget.model().item(selection)\n        self._selected.append(item.path)"}
{"prompt": "def _srvc_set_config(self, trajectory):\n        \"\"\"Sets a config value to the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"", "output": "\n        if trajectory.name in self.config:\n            self.config[trajectory.name] = trajectory.value\n        else:\n            self.config = trajectory.value\n\n    def _srvc_get_config(self, trajectory):\n        \"\"\"Returns the config value for the given trajectory or None if the trajectory was not set\"\"\"\n        if trajectory.name in self.config:\n            return self.config[trajectory.name]\n        else:\n            return None\n\n    def _srvc_set_value(self, trajectory, value):\n        \"\"\"S", "solution": "\n        def _set_config(name, value, comment):\n            if not trajectory.f_contains('config.'+name, shortcuts=False):\n                trajectory.f_add_config(Parameter, name, value, comment=comment)\n\n        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:\n            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]\n            value = getattr(self, attr_name)\n            _set_config('hdf5.overview.' + table_name,\n                         value,\n                         comment='Whether or not to have an overview '\n                                 'table with that name')\n\n        _set_config('hdf5.purge_duplicate_comments',\n                    self._purge_duplicate_comments,\n                    comment='Whether comments of results and'\n                            ' derived parameters should only'\n                            ' be stored for the very first instance.'\n                            ' Works only if the summary tables are'\n                            ' active.')\n\n        _set_config('hdf5.results_per_run', self._results_per_run,\n                    comment='Expected number of results per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.derived_parameters_per_run',\n                    self._derived_parameters_per_run,\n                    comment='Expected number of derived parameters per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.complevel', self._complevel,\n                    comment='Compression Level (0 no compression '\n                            'to 9 highest compression)')\n\n        _set_config('hdf5.complib', self._complib,\n                    comment='Compression Algorithm')\n\n        _set_config('hdf5.encoding', self._encoding,\n                    comment='Encoding for unicode characters')\n\n        _set_config('hdf5.fletcher32', self._fletcher32,\n                    comment='Whether to use fletcher 32 checksum')\n\n        _set_config('hdf5.shuffle', self._shuffle,\n                    comment='Whether to use shuffle filtering.')\n\n        _set_config('hdf5.pandas_format', self._pandas_format,\n                    comment="}
{"prompt": "def draw(self):\n        \"\"\"\n        Draws all layers of this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"", "output": "\n        glBindVertexArray(self.vao)\n        glBindBuffer(GL_ARRAY_BUFFER, self.vbo)\n        glBufferData(GL_ARRAY_BUFFER, self.array.nbytes, self.array, GL_STATIC_DRAW)\n        glBindVertexArray(0)\n        glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n        glBindVertexArray(1)\n\n", "solution": "\n        super(LayeredWidget,self).draw()\n        for layer,_ in self.layers:\n            layer._draw()"}
{"prompt": "def get_exitcodes_terminated_worker(processes):\n    \"\"\"Return a formated string with the exitcodes of terminated workers.\n\n    If necessary, wait (up to .25s) for the system to correctly set the\n    exitcode of one terminated worker.\n    \"\"\"", "output": "\n    # Create a dictionary to store the exitcodes of terminated workers\n    exitcodes = {}\n\n    # Loop through the list of processes and check for terminated workers\n    for process in processes:\n        if process.poll() is not None:\n            # The process is still running, so it has terminated\n            exitcode = process.returncode\n            exitcodes[process.pid] = exitcode\n\n    # If there are no terminated workers, return an empty string\n    if not exitcodes:\n        return \"\"\n\n    # Sort the exitcodes by their order in the list of processes\n    ", "solution": "\n    patience = 5\n\n    # Catch the exitcode of the terminated workers. There should at least be\n    # one. If not, wait a bit for the system to correctly set the exitcode of\n    # the terminated worker.\n    exitcodes = [p.exitcode for p in list(processes.values())\n                 if p.exitcode is not None]\n    while len(exitcodes) == 0 and patience > 0:\n        patience -= 1\n        exitcodes = [p.exitcode for p in list(processes.values())\n                     if p.exitcode is not None]\n        time.sleep(.05)\n\n    return _format_exitcodes(exitcodes)"}
{"prompt": "def eintr_retry(exc_type, f, *args, **kwargs):\n    \"\"\"Calls a function.  If an error of the given exception type with\n    interrupted system call (EINTR) occurs calls the function again.\n    \"\"\"", "output": "\n    import signal\n    import time\n\n    def signal_handler(signum, frame):\n        raise exc_type(\"Interrupted system call\")\n\n    signal.signal(signal.SIGINT, signal_handler)\n\n    try:\n        return f(*args, **kwargs)\n    except exc_type as e:\n        return e\n\n", "solution": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type as exc:\n            if exc.errno != EINTR:\n                raise\n        else:\n            break"}
{"prompt": "def _colorify(self, data):\n        \"\"\"\n        Retun colored string.\n\n        :param data: The string to colorify.\n        :type data: str\n\n        :return: A colored string.\n        :rtype: str\n        \"\"\"", "output": "\n        return \"\\033[1;31m\" + data + \"\\033[0m\"\n\n    def _print_colored_string(self, data):\n        \"\"\"\n        Print colored string.\n\n        :param data: The string to colorize.\n        :type data: str\n\n        :return: None\n        :rtype: None\n        \"\"\"\n        print(self._colorify(data))\n\n    def print_colored_string(self, data):\n        \"\"\"\n        Prints a colored string.\n\n        :param data: The string to colorize.\n        :", "solution": "\n\n        if self.template in [\"Generic\", \"Less\"]:\n            # The template is in the list of template that need the coloration.\n\n            if (\n                self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                # We print the data with a green background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.GREEN + data\n            elif self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The status is in the list of down status.\n\n                # We print the data with a red background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.RED + data\n            else:\n                # The status is not in the list of up and down status.\n\n                # We print the data with a cyan background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.CYAN + data\n\n        # We return the data.\n        return data"}
{"prompt": "def __eof_qubit(rho):\n    \"\"\"\n    Compute the Entanglement of Formation of a 2-qubit density matrix.\n\n    Args:\n        rho ((array_like): (4,4) array_like, input density matrix.\n\n    Returns:\n        float: The entanglement of formation.\n    \"\"\"", "output": "\n    a, b, c, d = rho\n    return (a*d - b*c)**2\n\n", "solution": "\n    c = concurrence(rho)\n    c = 0.5 + 0.5 * np.sqrt(1 - c * c)\n    return shannon_entropy([c, 1 - c])"}
{"prompt": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"", "output": "\n        accesses = {}\n        for var in self.variables:\n            accesses[var] = []\n            for source, dest in self.edges:\n                if var in (source, dest):\n                    accesses[var].append(self.sympy_exprs[source] + self.sympy_exprs[dest])\n        return accesses\n\n", "solution": "\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses"}
{"prompt": "def deserialize(bstr):\n    \"\"\"\n    Attempts to deserialize a bytestring into an audiosegment.\n\n    :param bstr: The bytestring serialized via an audiosegment's serialize() method.\n    :returns: An AudioSegment object deserialized from `bstr`.\n    \"\"\"", "output": "\n    # Code to deserialize the bytestring into an AudioSegment object goes here\n    pass\n\n", "solution": "\n    d = pickle.loads(bstr)\n    seg = pickle.loads(d['seg'])\n    return AudioSegment(seg, d['name'])"}
{"prompt": "def create_validator(data_struct_dict, name=None):\n    \"\"\"\n    create a Validator instance from data_struct_dict\n\n    :param data_struct_dict: a dict describe validator's fields, like the dict `to_dict()` method returned.\n    :param name: name of Validator class \n\n    :return: Validator instance\n    \"\"\"", "output": "\n    if name is None:\n        name = \"Validator\"\n    return type(name, (object,), data_struct_dict)\n\n", "solution": "\n\n    if name is None:\n        name = 'FromDictValidator'\n    attrs = {}\n    for field_name, field_info in six.iteritems(data_struct_dict):\n        field_type = field_info['type']\n        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):\n            field_info['validator'] = create_validator(field_info['validator'])\n        attrs[field_name] = create_field(field_info)\n    name = force_str(name)\n    return type(name, (Validator, ), attrs)"}
{"prompt": "def _store(self, lines, buffer=None, store='source'):\n        \"\"\"Store one or more lines of input.\n\n        If input lines are not newline-terminated, a newline is automatically\n        appended.\"\"\"", "output": "\n        if buffer is None:\n            buffer = []\n        buffer.extend(lines)\n        if store =='source':\n            self.source_lines.extend(lines)\n        elif store == 'destination':\n            self.destination_lines.extend(lines)\n        return buffer\n\n    def _get_lines(self, store='source'):\n        \"\"\"Return a list of lines of input, either from source or destination.\"\"\"\n        if store =='source':\n            return self.source_lines\n        elif store == 'destination", "solution": "\n        \n        if buffer is None:\n            buffer = self._buffer\n            \n        if lines.endswith('\\n'):\n            buffer.append(lines)\n        else:\n            buffer.append(lines+'\\n')\n        setattr(self, store, self._set_source(buffer))"}
{"prompt": "def selection_redo(self, name=\"default\", executor=None):\n        \"\"\"Redo selection, for the name.\"\"\"", "output": "\n        if executor is None:\n            executor = self.executor\n        if executor is None:\n            return\n        if name == \"default\":\n            name = self.default_name\n        if name is None:\n            return\n        if name in executor.selected_items:\n            executor.selected_items.remove(name)\n            executor.redo_selection()\n        else:\n            executor.selected_items.add(name)\n            executor.redo_selection()\n\n    def redo_selection(self):\n        \"\"\"", "solution": "\n        logger.debug(\"redo\")\n        executor = executor or self.executor\n        assert self.selection_can_redo(name=name)\n        selection_history = self.selection_histories[name]\n        index = self.selection_history_indices[name]\n        next = selection_history[index + 1]\n        self.selection_history_indices[name] += 1\n        self.signal_selection_changed.emit(self)\n        logger.debug(\"redo: selection history is %r, index is %r\", selection_history, index)"}
{"prompt": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET):\n        \"\"\"Fetch questions from the Kitsune url.\n\n        :param category: the category of items to fetch\n        :offset: obtain questions after offset\n        :returns: a generator of questions\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/{category}/{category}?offset={offset}\"\n        while True:\n            response = requests.get(url)\n            if response.status_code == 200:\n                for question in response.json():\n                    yield question\n            else:\n                break\n\n", "solution": "\n        if not offset:\n            offset = DEFAULT_OFFSET\n\n        kwargs = {\"offset\": offset}\n        items = super().fetch(category, **kwargs)\n\n        return items"}
{"prompt": "def make_reports(self, stats, old_stats):\n        \"\"\"render registered reports\"\"\"", "output": "\n        reports = []\n        for report, data in stats.items():\n            old_report_data = old_stats.get(report, {})\n            new_data = {}\n            for key, value in data.items():\n                if key in old_report_data:\n                    new_data[key] = value\n                else:\n                    new_data[key] = old_report_data.get(key, value)\n            reports.append(f\"Report for {report}: {new_data}\")\n        return reports\n\n", "solution": "\n        sect = Section(\"Report\", \"%s statements analysed.\" % (self.stats[\"statement\"]))\n        for checker in self.report_order():\n            for reportid, r_title, r_cb in self._reports[checker]:\n                if not self.report_is_enabled(reportid):\n                    continue\n                report_sect = Section(r_title)\n                try:\n                    r_cb(report_sect, stats, old_stats)\n                except EmptyReportError:\n                    continue\n                report_sect.report_id = reportid\n                sect.append(report_sect)\n        return sect"}
{"prompt": "def format_lines(statements, lines):\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be \"1-2, 5-11, 13-14\".\n\n    \"\"\"", "output": " 5-10, 11-13, 14\".\n\n    Args:\n        statements: A list of integers representing line numbers.\n        lines: A list of integers representing line numbers.\n\n    Returns:\n        A string with the formatted lines.\n    \"\"\"\n    if not statements or not lines:\n        return \"\"\n\n    # Sort the statements and lines in ascending order\n    statements.sort()\n    lines.sort()\n\n    # Initialize the result with the first line\n    result = str(lines[0])\n\n    # Iterate over the statements and lines\n    i = 0", "solution": "\n    pairs = []\n    i = 0\n    j = 0\n    start = None\n    statements = sorted(statements)\n    lines = sorted(lines)\n    while i < len(statements) and j < len(lines):\n        if statements[i] == lines[j]:\n            if start == None:\n                start = lines[j]\n            end = lines[j]\n            j += 1\n        elif start:\n            pairs.append((start, end))\n            start = None\n        i += 1\n    if start:\n        pairs.append((start, end))\n    ret = ', '.join(map(nice_pair, pairs))\n    return ret"}
{"prompt": "def _check_function(self):\n        \"\"\" make some basic checks on the function to make sure it is valid\"\"\"", "output": "\n        if not callable(self.function):\n            raise TypeError(\"Function must be callable\")\n        if not isinstance(self.args, (list, tuple)):\n            raise TypeError(\"Args must be a list or tuple\")\n        if not isinstance(self.kwargs, dict):\n            raise TypeError(\"Kwargs must be a dictionary\")\n        if not isinstance(self.return_type, type):\n            raise TypeError(\"Return type must be a type\")\n        if not isinstance(self.description, str):\n            raise TypeError", "solution": "\n        # note, callable is valid for Python 2 and Python 3.2 onwards but\n        # not inbetween\n        if not callable(self._function):\n            raise RuntimeError(\n                \"provided function '{0}' is not callable\".\n                format(str(self._function)))\n        from inspect import getargspec\n        arg_info = getargspec(self._function)\n        if len(arg_info.args) != 1:\n            print str(arg_info)\n            raise RuntimeError(\n                \"provided function should have one argument but found \"\n                \"{0}\".format(len(arg_info.args)))"}
{"prompt": "def span(self, index):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"", "output": "\n        return (ord(self.mnemonic[index][0]), ord(self.mnemonic[index][-1]))\n\n    def __getitem__(self, index):\n        \"\"\"Return the mnemonic at the given index\n        \"\"\"\n        return self.mnemonic[index]\n\n    def __len__(self):\n        \"\"\"Return the number of mnemonics in the list\n        \"\"\"\n        return len(self.mnemonic)\n\n", "solution": "\n        lower = self.value0+sum(1<<x for x in self.extraTable[:index])\n        upper = lower+(1<<self.extraTable[index])\n        return lower, upper-1"}
{"prompt": "def paid_invoices_by_date(request, form):\n    \"\"\" Shows the number of paid invoices containing given products or\n    categories per day. \"\"\"", "output": "\n    paid_invoices = {}\n    for invoice in request.invoice_set.all():\n        paid_invoices[invoice.date] = invoice.paid_amount\n    \n    # Determine products or categories per day\n    if form.product_slug:\n        products = set(form.product_slug.split())\n    else:\n        products = set()\n    if form.category_slug:\n        categories = set(form.category_slug.split())\n    else:\n        categories = set()\n    ", "solution": "\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )"}
{"prompt": "def get(m, k, default=None):\n    \"\"\"Return the value of k in m. Return default if k not found in m.\"\"\"", "output": "\n    return m.get(k, default)\n\n", "solution": "\n    if isinstance(m, IAssociative):\n        return m.entry(k, default=default)\n\n    try:\n        return m[k]\n    except (KeyError, IndexError, TypeError) as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n        return default"}
{"prompt": "def _send_coroutine():\n    \"\"\"\n    Creates a running coroutine to receive message instances and send\n    them in a futures executor.\n    \"\"\"", "output": "\n    while True:\n        message = yield\n        # send message to executor\n        # (e.g. using a queue, socket, etc.)\n\n", "solution": "\n    with PoolExecutor() as executor:\n        while True:\n            msg = yield\n            future = executor.submit(msg.send)\n            future.add_done_callback(_exception_handler)"}
{"prompt": "def cb_list_groups(self, *args, **kwargs):\n        \"\"\"List all the check groups that pylint knows about\n\n        These should be useful to know what check groups someone can disable\n        or enable.\n        \"\"\"", "output": "\n        return []\n\n    def _get_mapping(self, *args, **kwargs):\n        \"\"\"Return a dictionary mapping pylint check names to their groups\n\n        This is used by the `_get_mapping` method to determine the correct\n        mapping between pylint check names and their groups.\n        \"\"\"\n        mapping = {}\n        for check in self.checks:\n            name = check.name\n            if name in mapping:\n                mapping[name].append(check.group)\n            else:\n                mapping[name] = [", "solution": "\n        for check in self.linter.get_checker_names():\n            print(check)\n        sys.exit(0)"}
{"prompt": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"", "output": "\n        self.data[name] = data\n\n    def get_column(self, name):\n        \"\"\"Get a column from the DataFrame\n\n        :param str name: name of column\n        :return: numpy array with the data\n        \"\"\"\n        return self.data[name]\n\n    def get_column_names(self):\n        \"\"\"Get a list of column names\n\n        :return: list of column names\n        \"\"\"\n        return list(self.data.keys())\n\n    def get_column_data(self, name):\n        \"\"\"Get a column data from", "solution": "\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))"}
{"prompt": "def get_objects_from_form(variant_ids, form_fields, object_type):\n    \"\"\"Extract the objects to be saved in the clinvar database collection.\n       object_type param specifies if these objects are variant or casedata objects\n\n       Args:\n        variant_ids(list): list of database variant ids\n        form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER\n        object_type(str): either 'variant' or 'case_data'\n\n       Returns:\n        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'\n    \"\"\"", "output": "'\n\n       Returns:\n        list: a list of dictionaries, where each dictionary represents a clinvar object to be saved in the database\n    \"\"\"\n    objects = []\n    for variant_id in variant_ids:\n        variant = form_fields.get(CLINVAR_HEADER, {}).get(variant_id)\n        if variant:\n            objects.append({\n                'variant_id': variant_id,\n                'variant_name': variant['name'],\n                'variant_chromosome': variant['chromosome'", "solution": "\n    submission_fields = []\n    if object_type == 'variant':\n        submission_fields = CLINVAR_HEADER\n    else: #collect casedata objects\n        submission_fields = CASEDATA_HEADER\n\n    # A list of objects (variants of casedata info) to be saved into clinvar database collection\n    submission_objects = []\n\n    # Loop over the form fields and collect the data:\n    for variant_id in variant_ids: # loop over the variants\n\n        subm_obj = {} # A new submission object for each\n\n        # Don't included casedata for a variant unless specified by user\n        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:\n            continue\n\n        subm_obj['csv_type'] = object_type\n        subm_obj['case_id'] = form_fields.get('case_id')\n        subm_obj['category'] = form_fields.get('category@'+variant_id)\n\n        for key, values in submission_fields.items(): # loop over the form info fields\n            field_value = form_fields.get(key+'@'+variant_id)\n            if field_value and not field_value == '-':\n                if key == 'ref_seq': # split this field into\n                    refseq_raw = field_value.split('|')\n                    subm_obj['ref_seq'] = refseq_raw[0]\n                    subm_obj['hgvs'] = refseq_raw[1]\n                else:\n                    subm_obj[key] = field_value\n\n        # Create a unique ID for the database\n        # For casedata : = caseID_sampleID_variantID\n        # For variants : ID = caseID_variantID\n        if object_type == 'casedata':\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])\n        else:\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id\n\n        submission_objects.append(subm_obj)\n\n    return submission_objects"}
{"prompt": "def evaluate(self, x, y=None, batch_size=32):\n        \"\"\"\n        Evaluate a model on a given dataset in distributed mode.\n\n        # Arguments\n        x: Input data. A Numpy array or RDD of Sample.\n        y: Labels. A Numpy array. Default is None if x is already RDD of Sample.\n        batch_size: Number of samples per gradient update.\n        \"\"\"", "output": "\n        if isinstance(x, np.ndarray):\n            x = Sample.from_ndarray(x)\n        if y is not None:\n            y = np.array(y)\n        if isinstance(x, RDD):\n            x = x.map(lambda s: s.features)\n        if y is not None:\n            y = y.reshape(-1, 1)\n        x = x.map(lambda s: s.features)\n        if isinstance(x, np.ndarray):\n            x = x.tolist", "solution": "\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            evaluation_data = to_sample_rdd(x, y)\n        elif isinstance(x, RDD) and not y:\n            evaluation_data = x\n        else:\n            raise TypeError(\"Unsupported evaluation data type: %s\" % type(x))\n        return callBigDlFunc(self.bigdl_type, \"evaluate\",\n                             self.value,\n                             evaluation_data,\n                             batch_size)"}
{"prompt": "def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `loop*` expression.\"\"\"", "output": "\n    target = node.target\n    iter = node.iter\n    body = node.body\n    orelse = node.orelse\n    return For(target=target, iter=iter, body=body, orelse=orelse)\n\n", "solution": "\n    assert node.op == NodeOp.LOOP\n\n    with ctx.new_symbol_table(\"loop\"):\n        binding_names = []\n        init_bindings: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            init_bindings.extend(init_ast.dependencies)\n            binding_name = genname(munge(binding.name))\n            binding_names.append(binding_name)\n            init_bindings.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LOOP\n            )\n\n        loop_result_name = genname(\"loop\")\n        with ctx.new_recur_point(\n            node.loop_id, RecurType.LOOP, binding_names=binding_names\n        ):\n            loop_body_ast: List[ast.AST] = []\n            body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n            loop_body_ast.extend(body_ast.dependencies)\n            loop_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],\n                    value=body_ast.node,\n                )\n            )\n            loop_body_ast.append(ast.Break())\n\n            return GeneratedPyAST(\n                node=_load_attr(loop_result_name),\n                dependencies=list(\n                    chain(\n                        [\n                            ast.Assign(\n                                targets=[\n                                    ast.Name(id=loop_result_name, ctx=ast.Store())\n                                ],\n                                value=ast.NameConstant(None),\n                            )\n                        ],\n                        init_bindings,\n                        [\n                            ast.While(\n                                test=ast.NameConstant(True),\n                                body=loop_body_ast,\n                                orelse=[],\n                            )\n                        ],\n                    )\n                ),\n            )"}
{"prompt": "def get_model(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"", "output": "\n        if len(model_name)!= 2 or '.' not in model_name:\n            return None\n        parts = model_name.split('.')\n        if len(parts)!= 2:\n            return None\n        module_name, class_name = parts\n        try:\n            module = __import__(module_name)\n            return getattr(module, class_name)\n        except ImportError:\n            return None\n\n    def get_price(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2", "solution": "\n        klass = None\n        try:\n            module_name, class_name = model_name.rsplit('.', 1)\n            mod = __import__(module_name, fromlist=[class_name])\n            klass = getattr(mod, class_name)\n        except ImportError, e:\n            self.error('Cannot find app %s %s' % (model_name, e))\n\n        return klass"}
{"prompt": "def compare(self, cost_map):\n        \"\"\"\n        Compare to previous records and return whether the given cost is a new best.\n        :return: True if the given cost is a new best\n        \"\"\"", "output": "\n        if not hasattr(self, 'previous_cost') or cost_map['cost'] > self.previous_cost:\n            self.previous_cost = cost_map['cost']\n            return True\n        else:\n            return False\n\n", "solution": "\n        cri_val = cost_map[self._criteria]\n        if self._best_criteria is None:\n            self._best_criteria = cri_val\n            return True\n        else:\n            if self._smaller_is_better and cri_val < self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            elif not self._smaller_is_better and cri_val > self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            else:\n                return False"}
{"prompt": "def execute(self):\n        \"\"\"\n        Execute R script\n        \"\"\"", "output": "\n        # Code to execute R script goes here\n        pass\n\n    def get_output(self):\n        \"\"\"\n        Retrieve output from R script\n        \"\"\"\n        # Code to retrieve output from R script goes here\n        pass\n\n    def get_status(self):\n        \"\"\"\n        Retrieve status of R script\n        \"\"\"\n        # Code to retrieve status of R script goes here\n        pass\n\n", "solution": "\n        rprocess = OrderedDict()\n        commands = OrderedDict([\n            (self.file, ['Rscript', self.file] + self.cmd),\n        ])\n        for cmd_name, cmd in commands.items():\n            rprocess[cmd_name] = self.run_command_under_r_root(cmd)\n        \n        return self.decode_cmd_out(completed_cmd=rprocess[self.file])"}
{"prompt": "def _duplicated_isinstance_types(node):\n        \"\"\"Get the duplicated types from the underlying isinstance calls.\n\n        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.\n        :returns: Dictionary of the comparison objects from the isinstance calls,\n                  to duplicate values from consecutive calls.\n        :rtype: dict\n        \"\"\"", "output": "\n        duplicates = {}\n        for i in range(len(node.values) - 1):\n            if isinstance(node.values[i], astroid.isinstance.Is) and \\\n                    isinstance(node.values[i+1], astroid.isinstance.Is):\n                if node.values[i].left is not node.values[i+1].left:\n                    if node.values[i].right is not node.values[i+1].right:\n                        key = (node.values[i].left, node.values[i", "solution": "\n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if inferred.name != \"isinstance\":\n                continue\n\n            isinstance_object = call.args[0].as_string()\n            isinstance_types = call.args[1]\n\n            if isinstance_object in all_types:\n                duplicated_objects.add(isinstance_object)\n\n            if isinstance(isinstance_types, astroid.Tuple):\n                elems = [\n                    class_type.as_string() for class_type in isinstance_types.itered()\n                ]\n            else:\n                elems = [isinstance_types.as_string()]\n            all_types[isinstance_object].update(elems)\n\n        # Remove all keys which not duplicated\n        return {\n            key: value for key, value in all_types.items() if key in duplicated_objects\n        }"}
{"prompt": "def digest(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_Digest\n\n        :param data: the data to be digested\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: the computed digest\n        :rtype: list of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          You can easly convert it to a binary string with:\n          ``bytes(ckbytelistDigest)``\n          or, for Python 2:\n          ``''.join(chr(i) for i in ckbytelistDigest)``\n\n        \"\"\"", "output": "        The C_Digest algorithm is used to compute a digest of the input data.\n        The digest is computed using the specified digesting mechanism.\n        The digest is computed in a streaming fashion, meaning that the digest\n        is computed incrementally as the input data is processed.\n        \"\"\"\n        if isinstance(data, (list, tuple)):\n            data = b''.join(data)\n        if isinstance(data, bytes):\n            data = data\n        digest = [mecha.hash_function(data)]\n        while True:\n            data = digest[-", "solution": "\n        digest = ckbytelist()\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_DigestInit(self.session, m)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get digest size\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual digest data\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest"}
{"prompt": "def file_hash(self, source, cu):\n        \"\"\"Compute a hash that changes if the file needs to be re-reported.\"\"\"", "output": "\n        if source == \"system\":\n            return cu\n        else:\n            return hashlib.sha256(source.encode()).hexdigest()\n\n    def hash_file(self, source, cu):\n        \"\"\"Compute a hash of the contents of a file.\"\"\"\n        if source == \"system\":\n            return cu\n        else:\n            with open(source, \"rb\") as f:\n                return hashlib.sha256(f.read()).hexdigest()\n\n    def hash_all_files(self, sources):\n        \"\"\"Compute", "solution": "\n        m = Hasher()\n        m.update(source)\n        self.coverage.data.add_to_hash(cu.filename, m)\n        return m.digest()"}
{"prompt": "def voronoi(script, hole_num=50, target_layer=None, sample_layer=None, thickness=0.5, backward=True):\n    \"\"\" Turn a model into a surface with Voronoi style holes in it\n\n    References:\n    http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html\n\n    Requires FilterScript object\n\n    Args:\n        script: the FilterScript object to write the filter to. Does not\n            work with a script filename.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"", "output": " (FilterScript): A FilterScript object containing the model to be turned into a surface with Voronoi style holes\n        hole_num (int): Number of holes to create in the surface\n        target_layer (str): Name of the layer to use as the target for the Voronoi surface\n        sample_layer (str): Name of the layer to use as the sample for the Voronoi surface\n        thickness (float): Thickness of the Voronoi surface\n        backward (bool): If True, the surface will be oriented in reverse direction\n\n    Returns:\n        None\n    ", "solution": "\n\n    if target_layer is None:\n        target_layer = script.current_layer()\n    if sample_layer is None:\n        # Current layer is currently not changed after poisson_disk is run\n        sampling.poisson_disk(script, sample_num=hole_num)\n        sample_layer = script.last_layer()\n\n    vert_color.voronoi(script, target_layer=target_layer, source_layer=sample_layer, backward=backward)\n    select.vert_quality(script, min_quality=0.0, max_quality=thickness)\n    if backward:\n        select.invert(script)\n    delete.selected(script)\n    smooth.laplacian(script, iterations=3)\n\n    return None"}
{"prompt": "def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"", "output": "\n        return sum(indices.values())\n\n    def global_iterator_to_indices(self, i):\n        \"\"\"\n        Transform a global iterator integer to a dictionary of indices.\n\n        Inverse of indices_to_global_iterator().\n        \"\"\"\n        indices = {}\n        for j, index in enumerate(self.indices):\n            indices[j] = i % index\n            i //= index\n        return indices\n\n    def global_iterator_to_key(self, i):\n        \"\"\"\n        Transform a global iterator integer to a key", "solution": "\n        global_iterator = self.subs_consts(self.global_iterator().subs(indices))\n        return global_iterator"}
{"prompt": "def _value(self, dtype=None, name=None, as_ref=False):  # pylint: disable=g-doc-args\n  \"\"\"Get the value returned by `tf.convert_to_tensor(distribution)`.\n\n  Note: this function may mutate the distribution instance state by caching\n  the concretized `Tensor` value.\n\n  Args:\n    dtype: Must return a `Tensor` with the given `dtype` if specified.\n    name: If the conversion function creates a new `Tensor`, it should use the\n      given `name` if specified.\n    as_ref: `as_ref` is true, the function must return a `Tensor` reference,\n      such as a `Variable`.\n  Returns:\n    concretized_distribution_value: `Tensor` identical to\n    `tf.convert_to_tensor(distribution)`.\n\n  #### Examples\n\n  ```python\n  tfd = tfp.distributions\n  x = tfd.Normal(0.5, 1).set_tensor_conversion(tfd.Distribution.mean)\n\n  x._value()\n  # ==> tf.convert_to_tensor(x) ==> 0.5\n\n  x._value() + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n\n  x + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n  ```\n\n  \"\"\"", "output": " this `name`.\n    as_ref: If True, the returned value is a reference to the `Tensor` object.\n\n  Returns:\n    The value returned by the conversion function.\n\n  Raises:\n    ValueError: If `dtype` is not specified and the conversion function does not return a `Tensor`.\n  \"\"\"\n  if dtype is None:\n    if not callable(self.convert_to_tensor):\n      raise ValueError(\"dtype must be specified if conversion function does not return a Tensor\")\n    return self.convert_", "solution": "\n  # pylint: disable=protected-access\n  if as_ref:\n    raise NotImplementedError(\n        'Cannot convert a `Distribution` to a reference '\n        '(e.g., `tf.Variable`).')\n  if self._concrete_value is None:\n    if self._convert_to_tensor_fn is None:\n      raise NotImplementedError(\n          'Failed to convert object of type {} to Tensor. Contents: {}. '\n          'Call `distribution.set_tensor_conversion(lambda self: ...)` to '\n          'enable `tf.convert_to_tensor` capability. For example: '\n          '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`'\n          ' results in `tf.convert_to_tensor(x)` being identical to '\n          '`x.mean()`.'.format(type(self), self))\n    with self._name_scope('value'):\n      self._concrete_value = (self._convert_to_tensor_fn(self)\n                              if callable(self._convert_to_tensor_fn)\n                              else self._convert_to_tensor_fn)\n      if not tf.is_tensor(self._concrete_value):\n        self._concrete_value = tfd._convert_to_tensor(\n            value=self._concrete_value,\n            name=name or 'concrete_value',\n            dtype=dtype,\n            dtype_hint=self.dtype)\n  return self._concrete_value"}
{"prompt": "def url_equal(first, second, ignore_scheme=False, ignore_netloc=False, ignore_path=False, ignore_params=False,\n              ignore_query=False, ignore_fragment=False):\n    \"\"\"\n    Compare two URLs and return True if they are equal, some parts of the URLs can be ignored\n    :param first: URL\n    :param second: URL\n    :param ignore_scheme: ignore the scheme\n    :param ignore_netloc: ignore the netloc\n    :param ignore_path: ignore the path\n    :param ignore_params: ignore the params\n    :param ignore_query: ignore the query string\n    :param ignore_fragment: ignore the fragment\n    :return: result of comparison\n    \"\"\"", "output": "param ignore_params: ignore the parameters\n    :param ignore_query: ignore the query string\n    :param ignore_fragment: ignore the fragment\n    :return: True if the URLs are equal, False otherwise\n    \"\"\"\n    first_parts = first.split('/')\n    second_parts = second.split('/')\n    if len(first_parts)!= len(second_parts):\n        return False\n    for i in range(len(first_parts)):\n        if ignore_scheme and first_parts[i].startswith('http'", "solution": "\n    # <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    firstp = urlparse(first)\n    secondp = urlparse(second)\n\n    return ((firstp.scheme == secondp.scheme or ignore_scheme) and\n            (firstp.netloc == secondp.netloc or ignore_netloc) and\n            (firstp.path == secondp.path or ignore_path) and\n            (firstp.params == secondp.params or ignore_params) and\n            (firstp.query == secondp.query or ignore_query) and\n            (firstp.fragment == secondp.fragment or ignore_fragment))"}
{"prompt": "def get_users_for_course(self, course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given course id.\n        \"\"\"", "output": "\n        # implementation not provided\n        pass\n\n    def get_course_for_user(self, user_id, course_id, params={}):\n        \"\"\"\n        Returns the course id for the given user and course id.\n        \"\"\"\n        # implementation not provided\n        pass\n\n    def get_course_for_user_and_course(self, user_id, course_id, params={}):\n        \"\"\"\n        Returns the course id for the given user and course id.\n        \"\"\"\n        # implementation not provided\n        pass\n\n    def get_course_", "solution": "\n        url = COURSES_API.format(course_id) + \"/users\"\n        data = self._get_paged_resource(url, params=params)\n        users = []\n        for datum in data:\n            users.append(CanvasUser(data=datum))\n        return users"}
{"prompt": "def make_signing_service(config, entity_id):\n    \"\"\"\n    Given configuration initiate a SigningService instance\n\n    :param config: The signing service configuration\n    :param entity_id: The entity identifier\n    :return: A SigningService instance\n    \"\"\"", "output": "\n    # Code to initiate the SigningService instance\n    signing_service = SigningService(config, entity_id)\n    return signing_service\n\n", "solution": "\n\n    _args = dict([(k, v) for k, v in config.items() if k in KJ_SPECS])\n    _kj = init_key_jar(**_args)\n\n    if config['type'] == 'internal':\n        signer = InternalSigningService(entity_id, _kj)\n    elif config['type'] == 'web':\n        _kj.issuer_keys[config['iss']] = _kj.issuer_keys['']\n        del _kj.issuer_keys['']\n        signer = WebSigningServiceClient(config['iss'], config['url'],\n                                         entity_id, _kj)\n    else:\n        raise ValueError('Unknown signer type: {}'.format(config['type']))\n\n    return signer"}
{"prompt": "def delete(filename, delete_v1=True, delete_v2=True):\n    \"\"\"Remove tags from a file.\n\n    Keyword arguments:\n\n    * delete_v1 -- delete any ID3v1 tag\n    * delete_v2 -- delete any ID3v2 tag\n    \"\"\"", "output": "\n    with open(filename, 'rb') as f:\n        data = f.read()\n\n    if delete_v1:\n        data = data.replace(b'ID3', b'')\n    if delete_v2:\n        data = data.replace(b'ID3', b'')\n\n    with open(filename, 'wb') as f:\n        f.write(data)\n\n", "solution": "\n\n    f = open(filename, 'rb+')\n\n    if delete_v1:\n        try:\n            f.seek(-128, 2)\n        except IOError:\n            pass\n        else:\n            if f.read(3) == b'TAG':\n                f.seek(-128, 2)\n                f.truncate()\n\n    # technically an insize=0 tag is invalid, but we delete it anyway\n    # (primarily because we used to write it)\n    if delete_v2:\n        f.seek(0, 0)\n        idata = f.read(10)\n        try:\n            id3, vmaj, vrev, flags, insize = unpack('>3sBBB4s', idata)\n        except struct.error:\n            id3, insize = b'', -1\n        insize = BitPaddedInt(insize)\n        if id3 == b'ID3' and insize >= 0:\n            delete_bytes(f, insize + 10, 0)"}
{"prompt": "def state(self):\n        \"\"\"Return internal state, useful for testing.\"\"\"", "output": "\n        return (self.x, self.y, self.z)\n\n", "solution": "\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}"}
{"prompt": "def get_mint_tree(tokens_stream):\n    \"\"\"\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    \"\"\"", "output": "\n    mint_tree = {}\n    for line in tokens_stream:\n        if not line:\n            continue\n        tag, value = line.split(':', 1)\n        mint_tree[tag] = value\n    return mint_tree\n\n", "solution": "\n    smart_stack = RecursiveStack()\n    block_parser.parse(tokens_stream, smart_stack)\n    return MintTemplate(body=smart_stack.stack)"}
{"prompt": "def from_yaml(cls, defaults, **kwargs):\n        \"\"\"Creates a new instance of a rule by merging two dictionaries.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"", "output": "\n        return cls(defaults, **kwargs)\n\n    def to_yaml(self, filepath):\n        \"\"\"Writes the rule to a YAML file at the specified filepath.\n\n        Args:\n            filepath (str): The path to the YAML file to write to.\n\n        Returns:\n            None\n        \"\"\"\n        with open(filepath, 'w') as f:\n            yaml.dump(self.defaults, f)\n\n    def __repr__(self):\n        return f\"Rule({self.name}, {", "solution": "\n        # TODO: I hate myself for this. Fix it later mmkay?\n        if \"token\" not in defaults:\n            kwargs[\"token\"] = None\n\n        defaults = copy.deepcopy(defaults)\n        return cls(\n            defaults=defaults,\n            token=kwargs.pop(\"token\"),\n            directory=kwargs.pop(\"directory\"),\n            **kwargs\n        )"}
{"prompt": "def getPhysicalInterface(self, physicalInterfaceId, draft=False):\n        \"\"\"\n        Get a physical interface.\n        Parameters:\n          - physicalInterfaceId (string)\n          - draft (boolean)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/interfaces/{physicalInterfaceId}/draft\" if draft else f\"{self.base_url}/interfaces/{physicalInterfaceId}\"\n        response = self.session.get(url)\n        if response.status_code!= 200:\n            raise APIException(f\"Failed to get physical interface: {response.status_code}\")\n        return response.json()\n\n    def createPhysicalInterface(self, physicalInterfaceId, draft=False):\n        \"\"\"\n        Create a physical interface.\n        ", "solution": "\n        if draft:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"/draft\", physicalInterfaceId)\n        else:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"\", physicalInterfaceId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"physical interface retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a physical interface\", resp)\n        return resp.json()"}
{"prompt": "def get(cls):\n        \"\"\"\n        Get the status while testing for an IP or domain.\n\n        .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"", "output": "\n        return cls.INTERN[\"to_test\"]\n\n    def set(cls, value):\n        \"\"\"\n        Set the status while testing for an IP or domain.\n\n       .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"\n        cls.INTERN[\"to_test\"] = value\n\n    return property(get, set)\n\n", "solution": "\n\n        if \"to_test\" in PyFunceble.INTERN and PyFunceble.INTERN[\"to_test\"]:\n            expiration_date = ExpirationDate().get()\n\n            if expiration_date is False:\n                return cls.handle(status=\"invalid\")\n\n            if expiration_date == PyFunceble.STATUS[\"official\"][\"up\"]:\n                return expiration_date, \"WHOIS\"\n\n            return cls.handle(status=\"inactive\")\n\n        raise NotImplementedError(\"We expect `INTERN['to_test']` to be set.\")"}
{"prompt": "def execute(option):\n    \"\"\"A script that melody calls with each valid set of options. This\n    script runs the required code and returns the results.\"\"\"", "output": "\n    if option == \"\":\n        return []\n    elif option == \"major\":\n        return [1, 2, 3]\n    elif option == \"minor\":\n        return [4, 5, 6]\n    elif option == \"harmonic\":\n        return [7, 8, 9]\n    else:\n        return []\n\n", "solution": "\n\n    namelist_option = []\n    makefile_option = []\n    flags = \"\"\n    for entry in option:\n        key = entry.keys()[0]\n        if key == \"Problem Size\":\n            namelist_option.append({\"SIZE\": entry[key]})\n        elif key == \"F90\":\n            makefile_option.append(entry)\n        else:\n            flags += entry[key] + \" \"\n    makefile_option.append({\"F90FLAGS\": flags})\n\n    namelist = create_input(namelist_option, \"namelist\",\n                            template_location=\"templates\")\n\n    makefile_include = create_input(makefile_option, \"Makefile.include\",\n                                    template_location=\"templates\")\n\n    benchmark_base = \"shallow\"\n\n    # save the input files in the appropriate place\n    location = benchmark_base + \"/original/namelist\"\n    my_file = open(location, 'w')\n    my_file.write(namelist)\n    my_file.flush()\n\n    location = benchmark_base + \"/common/Makefile.include\"\n    my_file = open(location, 'w')\n    my_file.write(makefile_include)\n    my_file.flush()\n\n    # compile shallow if required\n    base_path = benchmark_base + \"/original\"\n    import subprocess\n    make_process = subprocess.Popen([\"make\", \"clean\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    make_process = subprocess.Popen([\"make\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    # run shallow\n    make_process = subprocess.Popen([\"./shallow_base\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n    # _ = make_process.stderr.read()\n    stdout = make_process.stdout.read()\n\n    # determine if the results are correct. We will need to look at\n    # the results from stdout but for the moment we assume they are\n    # correct\n\n    # extract the required outputs\n    for line in stdout.split(\"\\n\"):\n        if \"Time-stepping\" in line:\n            total_time = line.split()[2]\n\n    return True, total_time"}
{"prompt": "def _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the next full form from the input stream.\"\"\"", "output": "\n    # Read the first token from the stream\n    token = ctx.read_token()\n    if not token:\n        raise ValueError(\"Input stream is empty\")\n\n    # Check if the token is a closing parenthesis or an opening parenthesis\n    if token == \")\":\n        # Parse the full form by recursively calling this function\n        form = _read_next(ctx)\n        if not form.is_full():\n            raise ValueError(\"Input stream is not a full form\")\n        return form\n    elif token == \"(\":\n        #", "solution": "\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))"}
{"prompt": "def __tomo_linear_inv(freqs, ops, weights=None, trace=None):\n    \"\"\"\n    Reconstruct a matrix through linear inversion.\n\n    Args:\n        freqs (list[float]): list of observed frequences.\n        ops (list[np.array]): list of corresponding projectors.\n        weights (list[float] or array_like):\n            weights to be used for weighted fitting.\n        trace (float or None): trace of returned operator.\n\n    Returns:\n        numpy.array: A numpy array of the reconstructed operator.\n    \"\"\"", "output": " matrix.\n\n    \"\"\"\n    if trace is None:\n        trace = np.trace(ops[0].conj().T @ ops[0])\n\n    if weights is None:\n        weights = np.ones_like(freqs)\n\n    if len(freqs)!= len(ops):\n        raise ValueError(\"Length of freqs and ops must be the same.\")\n\n    if len(freqs)!= len(weights):\n        raise ValueError(\"Length of freqs and weights must be the same.\")\n\n    if trace == 0:\n        raise ValueError(\"Trace", "solution": "\n    # get weights matrix\n    if weights is not None:\n        W = np.array(weights)\n        if W.ndim == 1:\n            W = np.diag(W)\n\n    # Get basis S matrix\n    S = np.array([vectorize(m).conj()\n                  for m in ops]).reshape(len(ops), ops[0].size)\n    if weights is not None:\n        S = np.dot(W, S)  # W.S\n\n    # get frequencies vec\n    v = np.array(freqs)  # |f>\n    if weights is not None:\n        v = np.dot(W, freqs)  # W.|f>\n    Sdg = S.T.conj()  # S^*.W^*\n    inv = np.linalg.pinv(np.dot(Sdg, S))  # (S^*.W^*.W.S)^-1\n\n    # linear inversion of freqs\n    ret = devectorize(np.dot(inv, np.dot(Sdg, v)))\n    # renormalize to input trace value\n    if trace is not None:\n        ret = trace * ret / np.trace(ret)\n    return ret"}
{"prompt": "def check_upload_status(self, video_id):\n        \"\"\"\n        Checks the video upload status\n        Newly uploaded videos may be in the processing state\n\n        Authentication is required\n\n        Returns:\n            True if video is available\n            otherwise a dict containes upload_state and detailed message\n            i.e. {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n        \"\"\"", "output": "\n        # code to check video upload status\n        #...\n        return {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n\n    def get_video_metadata(self, video_id):\n        \"\"\"\n        Returns metadata for a given video ID\n        Metadata includes title, description, and uploader\n\n        Authentication is required\n\n        Returns:\n            A dictionary containing metadata for the video\n            {\"title\": \"\", \"description\": \"\", \"uploader\": \"\"}\n        \"\"\"\n        # code to get video metadata\n        #...\n        return {\"title", "solution": "\n        # Raise ApiError if not authenticated\n        if not self.authenticated:\n            raise ApiError(_(\"Authentication is required\"))\n\n        entry = self.fetch_video(video_id)\n        upload_status = Api.yt_service.CheckUploadStatus(entry)\n\n        if upload_status is not None:\n            video_upload_state = upload_status[0]\n            detailed_message = upload_status[1]\n            return {\"upload_state\": video_upload_state, \"detailed_message\": detailed_message}\n        else:\n            return True"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        if self.farthest is None:\n            self.farthest = status.farthest\n        if self.expected is None:\n            self.expected = status.expected\n        if status.farthest is None or status.farthest > self.farthest:\n            self.farthest = status.farthest\n        if status.expected is None or status.expected > self.expected:\n            self.expected = status.expected\n        return self\n\n    def __str__(self) -> str:\n        return f\"Failed to", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def stream(self, id, task, type, follow=False, offset=0, origin=\"start\", plain=False):\n        \"\"\" This endpoint streams a task's stderr/stdout logs.\n\n            https://www.nomadproject.io/api/client.html#stream-logs\n\n            arguments:\n              - id: (str) allocation_id required\n              - task: (str) name of the task inside the allocation to stream logs from\n              - type: (str) Specifies the stream to stream. Either \"stderr|stdout\"\n              - follow: (bool) default false\n              - offset: (int) default 0\n              - origin: (str) either start|end, default \"start\"\n              - plain: (bool) Return just the plain text without framing. default False\n            returns: (str) text\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.BadRequestNomadException\n        \"\"\"", "output": "- follow: (bool) Follow the stream after the task is completed\n              - offset: (int) The starting offset for the stream (default: 0)\n              - origin: (str) The origin of the stream (default: \"start\")\n              - plain: (bool) If True, return the logs as is without any formatting (default: False)\n\n            returns:\n              - (str) The logs for the specified task, with any formatting applied.\n\n        \"\"\"\n        url = f\"{self.url}/allocations/{id}/streams/{task}/", "solution": "\n        params = {\n            \"task\": task,\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n            \"plain\": plain\n        }\n        return self.request(id, params=params, method=\"get\").text"}
{"prompt": "def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):\n    \"\"\"\n    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)\n    and returns the result.\n    \"\"\"", "output": "\n    # Calculate the start and end times of the segmentation\n    start_time = min(times)\n    end_time = max(times)\n\n    # Calculate the start and end frequencies of the segmentation\n    start_freq = min(frequencies)\n    end_freq = max(frequencies)\n\n    # Calculate the start and end times of the stft segmentation\n    start_stft_time = min(stft_times)\n    end_stft_time = max(stft_times)\n\n    # Calculate the", "solution": "\n    assert mask.shape == (frequencies.shape[0], times.shape[0]), \"Times is shape {} and frequencies is shape {}, but mask is shaped {}\".format(\n        times.shape, frequencies.shape, mask.shape\n    )\n    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))\n\n    if len(stft_times) > len(times):\n        all_j = [j for j in range(len(stft_times))]\n        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]\n        all_i = [all_j[idx] for idx in idxs]\n    else:\n        all_i = [i for i in range(len(times))]\n        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]\n        all_j = [all_i[idx] for idx in idxs]\n\n    for i, j in zip(all_i, all_j):\n        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])\n\n    return result"}
{"prompt": "def sync_labels(self, repo):\n        \"\"\"Creates a local map of github labels/milestones to asana tags.\"\"\"", "output": "\n        labels = {}\n        for label in repo.labels:\n            if label.name not in labels:\n                labels[label.name] = []\n            labels[label.name].append(label.number)\n        for milestone in repo.milestones:\n            if milestone.name not in labels:\n                labels[milestone.name] = []\n            labels[milestone.name].append(milestone.number)\n        for asana_tag in repo.tags:\n            if asana_tag.name not in labels:\n                ", "solution": "\n\n        logging.info(\"syncing new github.com labels to tags\")\n\n        # create label tag map\n        ltm = self.app.data.get(\"label-tag-map\", {})\n\n        # loop over labels, if they don't have tags, make them\n        for label in repo.get_labels():\n            tag_id = ltm.get(label.name, None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=label.name,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % label.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", label.name, tag['id'])\n                ltm[label.name] = tag['id']\n\n        # loop over milestones, if they don't have tags, make them\n        for ms in repo.get_milestones(state=\"all\"):\n            tag_id = ltm.get(_ms_label(ms.id), None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=ms.title,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % ms.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", ms.title, tag['id'])\n                ltm[_ms_label(ms.id)] = tag['id']\n\n        self.app.data['label-tag-map'] = ltm\n        return ltm"}
{"prompt": "def recurse(self, k, wait=False, wait_index=None, timeout='5m'):\n        \"\"\"Recursively get the tree below the given key\"\"\"", "output": "\n        if wait:\n            time.sleep(int(timeout))\n        if key in self.tree:\n            return self.tree[key]\n        if key == '':\n            return []\n        if wait_index is not None:\n            wait_index += 1\n        result = []\n        for child in self.tree.values():\n            if isinstance(child, dict):\n                result.extend(child.get(k, []))\n            elif isinstance(child, list):\n                result.extend(recurse(child, wait, wait", "solution": "\n        k = k.lstrip('/')\n        url = '{}/{}'.format(self.endpoint, k)\n        params = {}\n        params['recurse'] = 'true'\n        if wait:\n            params['wait'] = timeout\n            if not wait_index:\n                params['index'] = self.index(k, recursive=True)\n            else:\n                params['index'] = wait_index\n        r = requests.get(url, params=params)\n        if r.status_code == 404:\n            raise KeyDoesNotExist(\"Key \" + k + \" does not exist\")\n        if r.status_code != 200:\n            raise KVStoreError('GET returned {}'.format(r.status_code))\n        entries = {} \n        for e in r.json():\n            if e['Value']:\n                entries[e['Key']] = base64.b64decode(e['Value'])\n            else:\n                entries[e['Key']] = ''\n        return entries"}
{"prompt": "def delayed(f):\n    \"\"\"Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    \"\"\"", "output": "collect()\n    [1.0, 2.0, 3.0, 4.0]\n\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        with vaex.delayed_context():\n            return f(*args, **kwargs)\n    return wrapper\n\n", "solution": "\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped"}
{"prompt": "def read(cls, path, sc=None, min_partitions=1, bigdl_type=\"float\"):\n        \"\"\"\n        Read images as Image Frame\n        if sc is defined, Read image as DistributedImageFrame from local file system or HDFS\n        if sc is null, Read image as LocalImageFrame from local file system\n        :param path path to read images\n        if sc is defined, path can be local or HDFS. Wildcard character are supported.\n        if sc is null, path is local directory/image file/image file with wildcard character\n        :param sc SparkContext\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :return ImageFrame\n        \"\"\"", "output": ":param sc: sparkContext for parallel processing\n        :param min_partitions: minimum number of partitions for image processing\n        :param bigdl_type: type of bigdl job\n        :return: Spark DataFrame with image data and metadata\n        \"\"\"\n        if sc is None:\n            from bigdl.spark.image import ImageFrame\n            return ImageFrame.read(path, min_partitions, bigdl_type)\n        else:\n            from bigdl.spark.image import DistributedImageFrame\n            return DistributedImageFrame.read(path, sc,", "solution": "\n        return ImageFrame(jvalue=callBigDlFunc(bigdl_type, \"read\", path, sc, min_partitions))"}
{"prompt": "def _is_package(path):\n    \"\"\"\n    \u5224\u65ad\u4f20\u5165\u7684\u8def\u5f84\u662f\u5426\u4e3a\u4e00\u4e2a Python \u6a21\u5757\u5305\n\n    :param str path: \u5f85\u5224\u65ad\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u662f\uff0c\u5219\u4f20\u5165 path \u4e3a\u4e00\u4e2a Python \u5305\uff0c\u5426\u5219\u4e0d\u662f\n    :rtype: bool\n    \"\"\"", "output": "\n    import importlib.util\n    spec = importlib.util.spec_from_file_location(\"package\", path)\n    package = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(package)\n    return isinstance(package, type(\"package\"))\n\n", "solution": "\n    def _exists(s):\n        return os.path.exists(os.path.join(path, s))\n\n    return (\n        os.path.isdir(path) and\n        (_exists('__init__.py') or _exists('__init__.pyc'))\n    )"}
{"prompt": "def get_settings(self, client_name=None):\n    \"\"\"get all settings, either for a particular client if a name is provided,\n       or across clients.\n\n       Parameters\n       ==========\n       client_name: the client name to return settings for (optional)\n\n    \"\"\"", "output": "\n    if client_name:\n        return self.clients[client_name].settings\n    else:\n        return {client_name: self.clients[client_name].settings for client_name in self.clients}\n\n", "solution": "\n    settings = read_client_secrets()\n    if client_name is not None and client_name in settings:\n        return settings[client_name]           \n    return settings"}
{"prompt": "def _evolve(self, state, qargs=None):\n        \"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"", "output": "\n        # Convert input to a numpy array if a statevector is provided\n        if isinstance(state, Statevector):\n            state = state.data\n\n        # Check that the operator dimension matches the specified subsystem dimensions\n        if qargs is not None:\n            operator_dims = [len(qargs), len(qargs)]\n            if operator_dims!= state.shape:\n                raise QiskitError(\"The operator dimension does not match the specified subsystem dimensions.\")\n\n        # Apply the operator to the state\n        if qargs is not None:\n            operator = np", "solution": "\n        state = self._format_state(state)\n        if qargs is None:\n            if state.shape[0] != self._input_dim:\n                raise QiskitError(\n                    \"Operator input dimension is not equal to state dimension.\"\n                )\n            if state.ndim == 1:\n                # Return evolved statevector\n                return np.dot(self.data, state)\n            # Return evolved density matrix\n            return np.dot(\n                np.dot(self.data, state), np.transpose(np.conj(self.data)))\n        # Subsystem evolution\n        return self._evolve_subsystem(state, qargs)"}
{"prompt": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair response payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the private key unique identifier or\n                the public key unique identifier is missing from the encoded\n                payload.\n        \"\"\"", "output": "        Returns:\n            A tuple of the form (key_material, key_type, key_value).\n\n        Raises:\n            ValueError: If the input buffer is not a valid CreateKeyPair response.\n        \"\"\"\n        # Read the header and footer of the response\n        header_len = input_buffer.read(4)\n        if len(header_len)!= 4:\n            raise ValueError(\"Invalid response header length\")\n        if header_len!= b'\\x00\\x00\\x01\\x00':\n            raise ValueError(\"Invalid", "solution": "\n        super(CreateKeyPairResponsePayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._private_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER\n            )\n            self._private_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"private key unique identifier.\"\n            )\n\n        if self.is_tag_next(\n                enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._public_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER\n            )\n            self._public_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"public key unique identifier.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        self.is_oversized(local_buffer)"}
{"prompt": "def validate(self):\n        \"\"\"\n        Error check the attributes of the ActivateRequestPayload object.\n        \"\"\"", "output": "\n        if not isinstance(self.name, str) or not isinstance(self.user_id, int) or not isinstance(self.request_id, int):\n            raise ValueError(\"Invalid attribute type\")\n        if not isinstance(self.action, str) or not isinstance(self.request_payload, dict):\n            raise ValueError(\"Invalid attribute type\")\n        if not isinstance(self.request_id, int):\n            raise ValueError(\"Invalid attribute type\")\n        if self.request_payload.get(\"user_id\")!=", "solution": "\n        if self.unique_identifier is not None:\n            if not isinstance(self.unique_identifier,\n                              attributes.UniqueIdentifier):\n                msg = \"invalid unique identifier\"\n                raise TypeError(msg)\n        if self.compromise_occurrence_date is not None:\n            if not isinstance(self.compromise_occurrence_date,\n                              primitives.DateTime):\n                msg = \"invalid compromise time\"\n                raise TypeError(msg)\n        if not isinstance(self.revocation_reason, objects.RevocationReason):\n            msg = \"invalid revocation reason\"\n            raise TypeError(msg)"}
{"prompt": "def receive_ack_renewing(self, pkt):\n        \"\"\"Receive ACK in RENEWING state.\"\"\"", "output": "\n        if pkt.getlayer(ICMP) is not None and pkt[ICMP].type == 0 and pkt[ICMP].code == 0x00:\n            # Found the start of the RENEWING response\n            # Renew the RENEWING connection\n            self.renew_connection(pkt)\n\n    def renew_connection(self, pkt):\n        \"\"\"Renew the RENEWING connection if it is not already renewed.\"\"\"\n        if pkt.getlayer(ICMP) is not None and pkt[ICMP].type ==", "solution": "\n        logger.debug(\"C3. Received ACK?, in RENEWING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in RENEWING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"prompt": "def _get_server_info_from_logs(self):\n        \"\"\"\n        Check server's output log, and determine its scheme / IP / port (helper method for `._launch_server()`).\n\n        This method is polled during process startup. It looks at the server output log and checks for a presence of\n        a particular string (\"INFO: Open H2O Flow in your web browser:\") which indicates that the server is\n        up-and-running. If the method detects this string, it extracts the server's scheme, ip and port and returns\n        them; otherwise it returns None.\n\n        :returns: (scheme, ip, port) tuple if the server has already started, None otherwise.\n        \"\"\"", "output": " returns None.\n\n        Returns:\n            A tuple containing the scheme (e.g. \"http\"), ip address (e.g. \"127.0.0.1\"), and port number (e.g. \"8080\")\n            of the server. If the server is not up-and-running, the corresponding element in the tuple is None.\n        \"\"\"\n        # Check server's output log for \"INFO: Open H2O Flow in your web browser:\" string\n        for line in self.server_output_log:\n            if \"INFO: Open H2O Flow in your", "solution": "\n        searchstr = \"INFO: Open H2O Flow in your web browser:\"\n        with open(self._stdout, \"rt\") as f:\n            for line in f:\n                if searchstr in line:\n                    url = line[line.index(searchstr) + len(searchstr):].strip().rstrip(\"/\")\n                    parts = url.split(\":\")\n                    assert len(parts) == 3 and (parts[0] == \"http\" or parts[1] == \"https\") and parts[2].isdigit(), \\\n                        \"Unexpected URL: %s\" % url\n                    return parts[0], parts[1][2:], int(parts[2])\n        return None"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        result = f(*args, **kwargs)\n        if isinstance(result, tuple) and len(result) == 2:\n            node, production = result\n            node.position = production\n        return result\n    return wrapper\n\n", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def comparison_stats(df, els=['Mg', 'Sr', 'Ba', 'Al', 'Mn']):\n    \"\"\"\n    Compute comparison stats for test and LAtools data.\n    \n    Population-level similarity assessed by a Kolmogorov-Smirnov test.\n    \n    Individual similarity assessed by a pairwise Wilcoxon signed rank test.\n    \n    Trends in residuals assessed by regression analysis, where significance of\n    the slope and intercept is determined by t-tests (both relative to zero).\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataframe containing reference ('X/Ca_r'), test user \n        ('X/Ca_t') and LAtools ('X123') data.\n    els : list\n        list of elements (names only) to plot.\n    \n    Returns\n    -------\n    pandas.DataFrame\n    \n    \"\"\"", "output": "df (pandas.DataFrame): A DataFrame containing test results.\n    els (list): A list of element names to include in the comparison.\n    \n    Returns:\n    A dictionary containing comparison statistics.\n    \"\"\"\n    # Compute test statistics\n    test_stats = {}\n    test_stats['KS'] = df[els].apply(lambda x: x['Mg'].rank()).sum()\n    test_stats['WKS'] = test_stats['KS'] / len(df)\n    test_stats['KS_pval'] = 1 -", "solution": "\n    \n    # get corresponding analyte and ratio names\n    As = []\n    Rs = []\n    analytes = [c for c in df.columns if ('_r' not in c) and ('_t' not in c)]\n    ratios = [c for c in df.columns if ('_r' in c)]\n\n    for e in els:\n        if e == 'Sr':\n            As.append('Sr88')\n        elif e == 'Mg':\n            As.append('Mg24')\n        else:\n            As.append([a for a in analytes if e in a][0])\n        Rs.append([r for r in ratios if e in r][0][:-2])\n        \n    yt_stats = []\n    yl_stats = []\n    \n    for i, (e, a) in enumerate(zip(Rs, As)):\n        if a == 'Ba138':\n            m = 1e3\n            u = '$\\mu$mol/mol'\n        else:\n            m = 1\n            u = 'mmol/mol'\n        \n        x = df.loc[:, e + '_r'].values * m\n        yt = df.loc[:, e + '_t'].values * m\n        yl = df.loc[:, a].values * m\n        \n        yt_stats.append(summary_stats(x, yt, e))\n        yl_stats.append(summary_stats(x, yl, e))\n    \n    yt_stats = pd.concat(yt_stats).T\n    yl_stats = pd.concat(yl_stats).T\n    \n    return pd.concat([yt_stats, yl_stats], keys=['Test User', 'LAtools']).T"}
{"prompt": "def get_pk_value_on_save(self, instance):\n        \"\"\"Generate ID if required.\"\"\"", "output": "\n        if not hasattr(instance, 'pk'):\n            instance.pk = str(uuid.uuid4())\n        return instance.pk\n\n    def save(self, instance):\n        \"\"\"Save the model instance.\"\"\"\n        instance.is_valid = True\n        instance.is_active = True\n        instance.is_superuser = True\n        instance.is_staff = True\n        instance.is_superuser_superuser = True\n        instance.is_superuser_staff = True\n        instance.is_active_superuser =", "solution": "\n        value = super(AleaIdField, self).get_pk_value_on_save(instance)\n        if not value:\n            value = self.get_seeded_value(instance)\n        return value"}
{"prompt": "def _send(self, data, content_type):\n        \"\"\"\n        Sends a GCM message with the given content type\n        \"\"\"", "output": "\n        # Code to send the GCM message with the given content type\n        pass\n    \n    def _receive(self):\n        \"\"\"\n        Receives a GCM message from the server\n        \"\"\"\n        # Code to receive the GCM message from the server\n        pass\n    \n    def _handle(self, data, content_type):\n        \"\"\"\n        Handles a GCM message with the given content type\n        \"\"\"\n        # Code to handle the GCM message\n        pass\n    \n    def _send_and_receive(self, data,", "solution": "\n\n        headers = {\n            \"Content-Type\": content_type,\n            \"Authorization\": \"key=%s\" % (self.api_key),\n            \"Content-Length\": str(len(data))\n        }\n\n        request = Request(self.api_url, data, headers)\n        return urlopen(request).read().decode(self.encoding)"}
{"prompt": "def make_clean_visible(_html, tag_replacement_char=' '):\n    \"\"\"\n    Takes an HTML-like Unicode string as input and returns a UTF-8\n    encoded string with all tags replaced by whitespace. In particular,\n    all Unicode characters inside HTML are replaced with a single\n    whitespace character.\n\n    This does not detect comments, style, script, link.  It also does\n    do anything with HTML-escaped characters.  All of these are\n    handled by the clean_html pre-cursor step.\n\n    Pre-existing whitespace of any kind (newlines, tabs) is converted\n    to single spaces ' ', which has the same byte length (and\n    character length).\n\n    This is a simple state machine iterator without regexes\n    \"\"\"", "output": "pace in the input is preserved in the output.\n\n    :param _html: A Unicode string containing HTML.\n    :param tag_replacement_char: The character to replace HTML tags with.\n    :return: A UTF-8 encoded string with all tags replaced by whitespace.\n    \"\"\"\n    # Convert the input to UTF-8 and replace all tags with whitespace\n    cleaned_html = clean_html(_html, tag_replacement_char)\n\n    # Convert the cleaned HTML back to UTF-8 and return it\n    return cleaned_html.encode('utf-", "solution": "\n    def non_tag_chars(html):\n        n = 0\n        while n < len(html):\n            angle = html.find('<', n)\n            if angle == -1:\n                yield html[n:]\n                n = len(html)\n                break\n            yield html[n:angle]\n            n = angle\n\n            while n < len(html):\n                nl = html.find('\\n', n)\n                angle = html.find('>', n)\n                if angle == -1:\n                    yield ' ' * (len(html) - n)\n                    n = len(html)\n                    break\n                elif nl == -1 or angle < nl:\n                    yield ' ' * (angle + 1 - n)\n                    n = angle + 1\n                    break\n                else:\n                    yield ' ' * (nl - n) + '\\n'\n                    n = nl + 1\n                    # do not break\n\n    if not isinstance(_html, unicode):\n        _html = unicode(_html, 'utf-8')\n\n    # Protect emails by substituting with unique key\n    _html = fix_emails(_html)\n\n    #Strip tags with previous logic\n    non_tag = ''.join(non_tag_chars(_html))\n\n    return non_tag.encode('utf-8')"}
{"prompt": "def match(self, request):\n        \"\"\"\n        Matches a given Request instance contract against the registered mocks.\n\n        If a mock passes all the matchers, its response will be returned.\n\n        Arguments:\n            request (pook.Request): Request contract to match.\n\n        Raises:\n            pook.PookNoMatches: if networking is disabled and no mock matches\n                with the given request contract.\n\n        Returns:\n            pook.Response: the mock response to be used by the interceptor.\n        \"\"\"", "output": "\n        if not self.networking:\n            raise pook.PookNoMatches(\"No networking available.\")\n\n        for mock in self.mocks:\n            if mock.matches(request):\n                return mock.response\n\n        raise pook.PookNoMatches(\"No mock matches with the given request.\")\n\n    def __init__(self, mocks):\n        self.mocks = mocks\n        self.networking = False\n\n", "solution": "\n        # Trigger engine-level request filters\n        for test in self.filters:\n            if not test(request, self):\n                return False\n\n        # Trigger engine-level request mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Store list of mock matching errors for further debugging\n        match_errors = []\n\n        # Try to match the request against registered mock definitions\n        for mock in self.mocks[:]:\n            try:\n                # Return the first matched HTTP request mock\n                matches, errors = mock.match(request.copy())\n                if len(errors):\n                    match_errors += errors\n                if matches:\n                    return mock\n            except PookExpiredMock:\n                # Remove the mock if already expired\n                self.mocks.remove(mock)\n\n        # Validate that we have a mock\n        if not self.should_use_network(request):\n            msg = 'pook error!\\n\\n'\n\n            msg += (\n                '=> Cannot match any mock for the '\n                'following request:\\n{}'.format(request)\n            )\n\n            # Compose unmatch error details, if debug mode is enabled\n            if self.debug:\n                err = '\\n\\n'.join([str(err) for err in match_errors])\n                if err:\n                    msg += '\\n\\n=> Detailed matching errors:\\n{}\\n'.format(err)\n\n            # Raise no matches exception\n            raise PookNoMatches(msg)\n\n        # Register unmatched request\n        self.unmatched_reqs.append(request)"}
{"prompt": "def discrete_best_alpha(data, alpharangemults=(0.9,1.1), n_alpha=201, approximate=True, verbose=True):\n    \"\"\"\n    Use the maximum L to determine the most likely value of alpha\n\n    *alpharangemults* [ 2-tuple ]\n        Pair of values indicating multiplicative factors above and below the\n        approximate alpha from the MLE alpha to use when determining the\n        \"exact\" alpha (by directly maximizing the likelihood function)\n    \"\"\"", "output": "\n    from scipy.optimize import minimize\n\n    def loglikelihood(alpha):\n        ll = 0\n        for i in range(len(data)):\n            ll += data[i] * (alpha - alpharangemults[0] * i)**alpharangemults[1]\n        return -ll\n\n    if approximate:\n        res = minimize(loglikelihood, alpharangemults[1] / alpharangemults[0], method='Nelder-Mead')\n        approx_alpha = res.x[", "solution": "\n\n    xmins = np.unique(data)\n    if approximate:\n        alpha_of_xmin = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n    else:\n        alpha_approx = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n        alpharanges = [(0.9*a,1.1*a) for a in alpha_approx]\n        alpha_of_xmin = [ most_likely_alpha(data,xmin,alpharange=ar,n_alpha=n_alpha) for xmin,ar in zip(xmins,alpharanges) ]\n    ksvalues = [ discrete_ksD(data, xmin, alpha) for xmin,alpha in zip(xmins,alpha_of_xmin) ]\n\n    best_index = argmin(ksvalues)\n    best_alpha = alpha_of_xmin[best_index]\n    best_xmin = xmins[best_index]\n    best_ks = ksvalues[best_index]\n    best_likelihood = discrete_likelihood(data, best_xmin, best_alpha)\n\n    if verbose:\n        print(\"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % (\n                best_alpha, best_xmin, best_ks, best_likelihood,\n                (data<best_xmin).sum(), (data>=best_xmin).sum()))\n\n    return best_alpha,best_xmin,best_ks,best_likelihood"}
{"prompt": "def _convert_date(date_string, date_format):\n    \"\"\"\n    Convert a date in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"", "output": "\n    return datetime.datetime.strptime(date_string, date_format).timestamp()\n\n", "solution": "\n    if date_format != 'epoch':\n        return datetime.strptime(date_string, date_format).timestamp()\n    else:\n        return float(date_string)"}
{"prompt": "def create_session(token=None):\n    \"\"\"\n    Create a github3.py session connected to GitHub.com\n\n    If token is not provided, will attempt to use the GITHUB_API_TOKEN\n    environment variable if present.\n    \"\"\"", "output": "\n    import github3\n\n    if token is None:\n        token = os.environ.get(\"GITHUB_API_TOKEN\")\n\n    if token is None:\n        raise ValueError(\"GITHUB_API_TOKEN environment variable not set\")\n\n    session = github3.login(token=token)\n    return session\n\n", "solution": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN', None)\n\n    gh_session = github3.login(token=token)\n\n    if gh_session is None:\n        raise RuntimeError('Invalid or missing GITHUB_API_TOKEN')\n\n    return gh_session"}
{"prompt": "def load(self, source):\n        \"\"\"\n        Opens the source file.\n        \"\"\"", "output": "\n        # Code to open the source file\n        pass\n    \n    def save(self, destination):\n        \"\"\"\n        Saves the current state of the object to the destination file.\n        \"\"\"\n        # Code to save the current state of the object to the destination file\n        pass\n\nclass File:\n    def __init__(self, name):\n        self.name = name\n    \n    def open(self):\n        # Code to open the file\n        pass\n    \n    def save(self, destination):\n        # Code to save the file to the destination", "solution": "\n        self.source = open(self.source, 'rb')    \n        self.loaded = True"}
{"prompt": "def unregisterObserver(self, observer):\n        \"\"\" Remove an observer from the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"", "output": "\n        self.observers.remove(observer)\n\n    def update(self, value):\n        \"\"\" Update the meter with the given value.\n\n        Args:\n            value (int): The value to update the meter with.\n\n        Returns:\n            None\n        \"\"\"\n        for obs in self.observers:\n            obs.update(value)\n\n", "solution": "\n        if observer in self.m_observers:\n            self.m_observers.remove(observer)\n        pass"}
{"prompt": "def query(self, minhash, size):\n        \"\"\"\n        Giving the MinHash and size of the query set, retrieve\n        keys that references sets with containment with respect to\n        the query set greater than the threshold.\n\n        Args:\n            minhash (datasketch.MinHash): The MinHash of the query set.\n            size (int): The size (number of unique items) of the query set.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"", "output": "\n        for key, value in self.items():\n            if value > self.threshold:\n                yield key\n\n    def __iter__(self):\n        \"\"\"\n        Return an iterator over the keys of the hash table.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"\n        return iter(self.keys())\n\n    def __len__(self):\n        \"\"\"\n        Return the number of keys in the hash table.\n\n        Returns:\n            `int` the number of keys in the hash table.\n        \"\"\"\n        return len(self.keys", "solution": "\n        for i, index in enumerate(self.indexes):\n            u = self.uppers[i]\n            if u is None:\n                continue\n            b, r = self._get_optimal_param(u, size)\n            for key in index[r]._query_b(minhash, b):\n                yield key"}
{"prompt": "def get_security_repository(self):\n        \"\"\" Security repository \"\"\"", "output": "\n        return \"https://example.com/security\"\n\n", "solution": "\n        from .repositories import SecurityRepository\n\n        if not self.security_repo:\n            self.security_repo = SecurityRepository(self.session)\n        return self.security_repo"}
{"prompt": "def Counter(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"", "output": "\n    return (a - b) / delta\n\n", "solution": "\n    if b < a:\n        return None \n\n    return (b - a) / float(delta)"}
{"prompt": "def cli_command_restart(self, msg):\n        \"\"\"\\\n        restart the subprocess\n        i. we set our state to RESTARTING - on restarting we still send heartbeat\n        ii. we kill the subprocess\n        iii. we start again\n        iv. if its started we set our state to RUNNING, else we set it to WAITING\n\n        :param msg:\n        :return:\n        \"\"\"", "output": "\n        if self.state == 'RESTARTING':\n            return\n        elif self.state == 'WAITING':\n            self.state = 'RUNNING'\n        elif self.state == 'KILLED':\n            self.kill()\n        else:\n            self.start()\n\n    def start(self):\n        \"\"\"\\\n        start the subprocess\n        i. we set our state to RUNNING\n        ii. we wait for it to finish\n        iii. if its finished we set our state to WAITING\n        iv. if its", "solution": "\n        info = ''\n        if self.state == State.RUNNING and self.sprocess and self.sprocess.proc:\n            self.state = State.RESTARTING\n            self.sprocess.set_exit_callback(self.proc_exit_cb_restart)\n            self.sprocess.proc.kill()\n            info = 'killed'\n            # TODO: check if process is really dead etc.\n        return info"}
{"prompt": "def delete_dir(bucket_name, root_path,\n               aws_access_key_id=None, aws_secret_access_key=None,\n               aws_profile=None):\n    \"\"\"Delete all objects in the S3 bucket named ``bucket_name`` that are\n    found in the ``root_path`` directory.\n\n    Parameters\n    ----------\n    bucket_name : `str`\n        Name of an S3 bucket.\n    root_path : `str`\n        Directory in the S3 bucket that will be deleted.\n    aws_access_key_id : `str`\n        The access key for your AWS account. Also set\n        ``aws_secret_access_key``.\n    aws_secret_access_key : `str`\n        The secret key for your AWS account.\n    aws_profile : `str`, optional\n        Name of AWS profile in :file:`~/.aws/credentials`. Use this instead\n        of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based\n        credentials.\n\n    Raises\n    ------\n    ltdconveyor.s3.S3Error\n        Thrown by any unexpected faults from the S3 API.\n    \"\"\"", "output": "access_key_id : `str`, optional\n        The AWS access key ID.\n    aws_secret_access_key : `str`, optional\n        The AWS secret access key.\n    aws_profile : `str`, optional\n        The AWS profile.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    import boto3\n\n    s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id,\n                        aws_secret_access_key=aws_secret_access_key,\n                        region_name", "solution": "\n    logger = logging.getLogger(__name__)\n\n    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key)\n    s3 = session.resource('s3')\n    client = s3.meta.client\n\n    # Normalize directory path for searching patch prefixes of objects\n    if not root_path.endswith('/'):\n        root_path.rstrip('/')\n\n    paginator = client.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=bucket_name, Prefix=root_path)\n\n    keys = dict(Objects=[])\n    for item in pages.search('Contents'):\n        try:\n            keys['Objects'].append({'Key': item['Key']})\n        except TypeError:  # item is None; nothing to delete\n            continue\n        # Delete immediately when 1000 objects are listed\n        # the delete_objects method can only take a maximum of 1000 keys\n        if len(keys['Objects']) >= 1000:\n            try:\n                client.delete_objects(Bucket=bucket_name, Delete=keys)\n            except Exception:\n                message = 'Error deleting objects from %r' % root_path\n                logger.exception(message)\n                raise S3Error(message)\n            keys = dict(Objects=[])\n\n    # Delete remaining keys\n    if len(keys['Objects']) > 0:\n        try:\n            client.delete_objects(Bucket=bucket_name, Delete=keys)\n        except Exception:\n            message = 'Error deleting objects from %r' % root_path\n            logger.exception(message)\n            raise S3Error(message)"}
{"prompt": "def find_launcher_class(clsname, kind):\n    \"\"\"Return a launcher for a given clsname and kind.\n\n    Parameters\n    ==========\n    clsname : str\n        The full name of the launcher class, either with or without the\n        module path, or an abbreviation (MPI, SSH, SGE, PBS, LSF,\n        WindowsHPC).\n    kind : str\n        Either 'EngineSet' or 'Controller'.\n    \"\"\"", "output": "\n    if kind == 'EngineSet':\n        return f\"{clsname}Engine\"\n    elif kind == 'Controller':\n        return f\"{clsname}Controller\"\n    else:\n        raise ValueError(\"Invalid kind parameter. Must be 'EngineSet' or 'Controller'.\")\n\n", "solution": "\n    if '.' not in clsname:\n        # not a module, presume it's the raw name in apps.launcher\n        if kind and kind not in clsname:\n            # doesn't match necessary full class name, assume it's\n            # just 'PBS' or 'MPI' prefix:\n            clsname = clsname + kind + 'Launcher'\n        clsname = 'IPython.parallel.apps.launcher.'+clsname\n    klass = import_item(clsname)\n    return klass"}
{"prompt": "def get_end_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 23:59:59 on that day\n    \"\"\"", "output": "\n    if isinstance(timestamp, datetime.datetime):\n        return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)\n    elif isinstance(timestamp, datetime.date):\n        return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)\n    else:\n        raise ValueError(\"Input must be a datetime or date\")\n\n", "solution": "\n    return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for root, dirs, files in os.walk(rootpath):\r\n        for file in files:\r\n            if file.endswith('.py'):  # only look for Python files\r\n                filepath = os.path.join(root, file)\r\n                if filepath.startswith(rootpath):  # skip the root path\r\n                    continue  # don't recurse into subdirectories\r\n                if filepath in modules:  # only add to existing modules\r\n                    continue  # don't add", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def institutes(self, institute_ids=None):\n        \"\"\"Fetch all institutes.\n        \n        Args:\n            institute_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"", "output": "\n        if institute_ids is None:\n            institute_ids = []\n        res = self.db.instruments.find({\"institute_id\": {\"$in\": institute_ids}})\n        return res\n    \n    def publications(self, publication_ids=None):\n        \"\"\"Fetch all publications.\n        \n        Args:\n            publication_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"\n        if publication_ids is None:\n            publication_ids = []\n", "solution": "\n        query = {}\n        if institute_ids:\n            query['_id'] = {'$in': institute_ids}\n        LOG.debug(\"Fetching all institutes\")\n        return self.institute_collection.find(query)"}
{"prompt": "def send(self, node, message):\n        \"\"\"\n        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        :returns success\n        :rtype bool\n        \"\"\"", "output": "\n        if not self.connected:\n            return False\n        try:\n            self.sock.sendall(message.encode())\n            return True\n        except:\n            return False\n\n    def receive(self, node):\n        \"\"\"\n        Receive a message from a node. Returns False if the connection appears to be dead either before or after actually trying to receive the message.\n\n        :param node: target node\n        :type node: Node\n        :returns success\n        :rtype bool\n        \"\"\"\n        if not self.connected:\n            return False\n", "solution": "\n\n        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        self._connections[node].send(message)\n        if self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        return True"}
{"prompt": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"", "output": "\n        while True:\n            try:\n                model.create(project_id)\n                break\n            except:\n                time.sleep(1)\n        return model\n\n", "solution": "\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()"}
{"prompt": "def read_msg(self):\n        \"\"\"Read one message unit. It's possible however that\n        more than one message will be set in a receive, so we will\n        have to buffer that for the next read.\n        EOFError will be raised on EOF.\n        \"\"\"", "output": "\n        if self.pos < 0:\n            raise EOFError\n        if self.pos >= len(self.msg):\n            raise EOFError\n        chunk = self.msg[self.pos:]\n        self.pos += len(chunk)\n        return chunk\n\n    def write_msg(self, msg):\n        \"\"\"Write one message unit.\n        \"\"\"\n        if self.pos < 0:\n            self.pos = 0\n        if self.pos >= len(msg):\n            self.pos = len(msg)\n        self.msg = msg", "solution": "\n        if self.state == 'connected':\n            if 0 == len(self.buf):\n                self.buf = self.inout.recv(Mtcpfns.TCP_MAX_PACKET)\n                if 0 == (self.buf):\n                    self.state = 'disconnected'\n                    raise EOFError\n                pass\n            self.buf, data = Mtcpfns.unpack_msg(self.buf)\n            return data.decode('utf-8')\n        else:\n            raise IOError(\"read_msg called in state: %s.\" % self.state)"}
{"prompt": "def _wakeup(self):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"", "output": "\n        # code to issue wakeup command to device\n        pass\n    \n    def _set_mode(self, mode):\n        \"\"\"\n        set mode of device to specified value.\n        \"\"\"\n        # code to set mode of device to specified value\n        pass\n    \n    def _get_mode(self):\n        \"\"\"\n        get current mode of device.\n        \"\"\"\n        # code to get current mode of device\n        pass\n    \n    mode = property(_get_mode, _set_mode, _wakeup)\n\n", "solution": "\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def get_home_dir(require_writable=False):\n    \"\"\"Return the 'home' directory, as a unicode string.\n\n    * First, check for frozen env in case of py2exe\n    * Otherwise, defer to os.path.expanduser('~')\n    \n    See stdlib docs for how this is determined.\n    $HOME is first priority on *ALL* platforms.\n    \n    Parameters\n    ----------\n    \n    require_writable : bool [default: False]\n        if True:\n            guarantees the return value is a writable directory, otherwise\n            raises HomeDirError\n        if False:\n            The path is resolved, but it is not guaranteed to exist or be writable.\n    \"\"\"", "output": "able directory\n        if False:\n            guarantees the return value is a non-writable directory\n    \n    Returns\n    -------\n    str\n        the 'home' directory, or None if the 'home' directory is not found\n    \"\"\"\n    import os\n    import sys\n    \n    # Determine the 'home' directory based on the current environment\n    home_dir = os.environ.get('HOME', os.path.expanduser('~'))\n    \n    # If 'home' is not found, return None\n    if home_dir ==", "solution": "\n\n    # first, check py2exe distribution root directory for _ipython.\n    # This overrides all. Normally does not exist.\n\n    if hasattr(sys, \"frozen\"): #Is frozen by py2exe\n        if '\\\\library.zip\\\\' in IPython.__file__.lower():#libraries compressed to zip-file\n            root, rest = IPython.__file__.lower().split('library.zip')\n        else:\n            root=os.path.join(os.path.split(IPython.__file__)[0],\"../../\")\n        root=os.path.abspath(root).rstrip('\\\\')\n        if _writable_dir(os.path.join(root, '_ipython')):\n            os.environ[\"IPYKITROOT\"] = root\n        return py3compat.cast_unicode(root, fs_encoding)\n    \n    homedir = os.path.expanduser('~')\n    # Next line will make things work even when /home/ is a symlink to\n    # /usr/home as it is on FreeBSD, for example\n    homedir = os.path.realpath(homedir)\n    \n    if not _writable_dir(homedir) and os.name == 'nt':\n        # expanduser failed, use the registry to get the 'My Documents' folder.\n        try:\n            import _winreg as wreg\n            key = wreg.OpenKey(\n                wreg.HKEY_CURRENT_USER,\n                \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n            )\n            homedir = wreg.QueryValueEx(key,'Personal')[0]\n            key.Close()\n        except:\n            pass\n    \n    if (not require_writable) or _writable_dir(homedir):\n        return py3compat.cast_unicode(homedir, fs_encoding)\n    else:\n        raise HomeDirError('%s is not a writable dir, '\n                'set $HOME environment variable to override' % homedir)"}
{"prompt": "def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default sys.stdout\n    stderr: default sys.stderr\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"", "output": " be redirected to this file for each simulation run\n\n    Returns\n    -------\n    \"\"\"\n    import subprocess\n    import os\n\n    # Get the simulation directory path\n    sim_dir = simulation_dir_path\n\n    # Get the weather data path\n    if weather_data_or_epw_path:\n        weather_data_path = weather_data_or_epw_path\n    else:\n        weather_data_path = epm_or_idf_path\n\n    # Get the simulation output file path\n    sim_out_path = os.path.", "solution": "\n    # work with absolute paths\n    simulation_dir_path = os.path.abspath(simulation_dir_path)\n\n    # check dir path\n    if not os.path.isdir(simulation_dir_path):\n        raise NotADirectoryError(\"Simulation directory does not exist: '%s'.\" % simulation_dir_path)\n\n    # epm\n    if not isinstance(epm_or_idf_path, Epm):\n        # we don't copy file directly because we want to manage it's external files\n        # could be optimized (use _copy_without_read_only)\n        epm = Epm.from_idf(epm_or_idf_path)\n    else:\n        epm = epm_or_idf_path\n\n    # create idf\n    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".idf\")\n    epm.to_idf(simulation_idf_path)\n\n    # weather data\n    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".epw\")\n    if isinstance(weather_data_or_epw_path, WeatherData):\n        weather_data_or_epw_path.to_epw(simulation_epw_path)\n    else:\n        # no need to load: we copy directly\n        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)\n\n    # copy epw if needed (depends on os/eplus version)\n    temp_epw_path = get_simulated_epw_path()\n    if temp_epw_path is not None:\n        _copy_without_read_only(simulation_epw_path, temp_epw_path)\n\n    # prepare command\n    eplus_relative_cmd = get_simulation_base_command()\n    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)\n\n    # idf\n    idf_command_style = get_simulation_input_command_style(\"idf\")\n    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        idf_file_cmd = simulation_idf_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # epw\n    epw_command_style = get_simulation_input_command_style(\"epw\")\n    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        epw_file_cmd = simulation_epw_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # command list\n    simulation_command_style = get_simulation_command_style()\n    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:\n        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]\n    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:\n        cmd_l = [eplus_cmd, \"-w\", epw_file_cmd, \"-r\", idf_file_cmd]\n    else:\n        raise RuntimeError(\"should not be here\")\n\n    # launch calculation\n    run_subprocess(\n        cmd_l,\n        cwd=simulation_dir_path,\n        stdout=stdout,\n        stderr=stderr,\n        beat_freq=beat_freq\n    )\n\n    # if needed, we delete temp weather data (only on Windows, see above)\n    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):\n        os.remove(os.path.join(temp_epw_path))"}
{"prompt": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"", "output": "\n    return ast.Attribute(value=ast.Name(id=name, ctx=ctx), attr=name, ctx=ctx)\n\n", "solution": "\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)"}
{"prompt": "def _prepare_query_values(self):\n        \"\"\"Extra prep on query values by converting\n        dictionaries into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"", "output": "\n        for key, value in self.items():\n            if isinstance(value, dict):\n                self[key] = HStoreValue(value)\n\n    def __str__(self):\n        return '&'.join([f'{key}={value}' for key, value in self.items()])\n\n    def __repr__(self):\n        return str(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, HStore):\n            return False\n        for key, value in self.items():\n            if", "solution": "\n\n        new_query_values = []\n        for field, model, val in self.query.values:\n            if isinstance(val, dict):\n                val = HStoreValue(val)\n\n            new_query_values.append((\n                field,\n                model,\n                val\n            ))\n\n        self.query.values = new_query_values"}
{"prompt": "def get_handler(level: str, fmt: str) -> logging.Handler:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"", "output": "\n    if level.lower() == 'debug':\n        return logging.StreamHandler()\n    elif level.lower() == 'info':\n        return logging.InfoHandler()\n    elif level.lower() == 'warning':\n        return logging.WarningHandler()\n    elif level.lower() == 'error':\n        return logging.ErrorHandler()\n    else:\n        return logging.StreamHandler()\n\n", "solution": "\n    handler: logging.Handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler"}
{"prompt": "def _get_course_content_from_ecommerce(course_id, site_code=None):\n    \"\"\"\n    Get course information using the Ecommerce course api.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str): course key of the course\n        site_code (str): site code\n\n    Returns:\n        course information from Ecommerce\n    \"\"\"", "output": "\n    import requests\n\n    url = f\"https://ecommerce.example.com/{course_id}\"\n    if site_code:\n        url += f\"/{site_code}\"\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException:\n        return {}\n\n", "solution": "\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        api_response = api.courses(course_id).get()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            'An error occurred while retrieving data for course run [%s] from the Catalog API.',\n            course_id,\n            exc_info=True\n        )\n        return {}\n\n    return {\n        'title': api_response.get('name'),\n        'verification_deadline': api_response.get('verification_deadline')\n    }"}
{"prompt": "def container_services(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`ContainerServicesOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicesOperations>`\n        \"\"\"", "output": "\n        return ContainerServicesOperations(self.api_version)\n\n    @property\n    def security_groups(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`SecurityGroupsOperations<azure.mgmt.securitygroup.v2017_07_01.operations.SecurityGroupsOperations>`\n        \"\"\"\n        return SecurityGroupsOperations(self.api_version)\n\n    @property\n    def security_rules(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-", "solution": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def doc_from_xml(document_element_name, inner_xml,\n                     xmlns='http://schemas.microsoft.com/windowsazure'):\n        \"\"\"Wraps the specified xml in an xml root element with default azure\n        namespaces\"\"\"", "output": "\n        root = ET.fromstring(inner_xml)\n        root.set('xmlns', xmlns)\n        return ET.tostring(root).decode('utf-8')\n\n    # Create a list of azure resource names\n    resource_names = [f'{azure_prefix}://{resource}' for resource in resources]\n\n    # Create a list of azure document names\n    document_names = [f'{document_prefix}://{document}' for document in documents]\n\n    # Create a list of azure document elements\n    document", "solution": "\n        xml = ''.join(['<', document_element_name,\n                      ' xmlns=\"{0}\">'.format(xmlns)])\n        xml += inner_xml\n        xml += ''.join(['</', document_element_name, '>'])\n        return xml"}
{"prompt": "def lessThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"", "output": "\n        if left[0] < right[0]:\n            return -1\n        elif left[0] > right[0]:\n            return 1\n        else:\n            if left[1] < right[1]:\n                return -1\n            elif left[1] > right[1]:\n                return 1\n            else:\n                return 0\n\n    def __lt__(self, other):\n        \"\"\"Return True if *self* is less than *other*.\"\"\"\n        return self.lessThan(self.point, other.point) < 0\n\n", "solution": "\n        sourceModel = self.sourceModel()\n        if sourceModel:\n            leftItem = sourceModel.item(left)\n            rightItem = sourceModel.item(right)\n\n            if (isinstance(leftItem, Directory)\n                and not isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.AscendingOrder\n\n            elif (not isinstance(leftItem, Directory)\n                and isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.DescendingOrder\n\n        return super(FilesystemSortProxy, self).lessThan(left, right)"}
{"prompt": "def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        \"\"\" get properties from element tree element \"\"\"", "output": "\n        properties = {}\n        if include_id:\n            properties['id'] = element.get('id')\n        if id_prefix_to_skip:\n            for prefix in id_prefix_to_skip:\n                if element.startswith(prefix):\n                    element = element[len(prefix):]\n                    break\n        if use_title_as_id:\n            properties['title'] = element.get('title')\n        for child in element.getchildren():\n            child_properties = get_entry_properties_from_element(child, include_", "solution": "\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('./atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('./atom:author/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('./atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('./atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties"}
{"prompt": "def source_address(self):\n        \"\"\"Return the authorative source of the link.\"\"\"", "output": "\n        return self.link.split('/')[-1]\n\n    def title(self):\n        \"\"\"Return the title of the link.\"\"\"\n        return self.link.split('#')[0].strip()\n\n    def text(self):\n        \"\"\"Return the text content of the link.\"\"\"\n        return self.link.split('#')[1].strip()\n\n    def href(self):\n        \"\"\"Return the href attribute of the link.\"\"\"\n        return self.link.split('#')[1].split('\"')[0]\n\n    def image", "solution": "\n        # If link is a sender, source is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address"}
{"prompt": "def _serialize(xp_ast):\n    \"\"\"Generate token strings which, when joined together, form a valid\n    XPath serialization of the AST.\"\"\"", "output": "\n    if xp_ast is None:\n        return \"\"\n    elif isinstance(xp_ast, tuple):\n        return \"(\" + \"\".join(_serialize(x) for x in xp_ast) + \")\"\n    elif isinstance(xp_ast, list):\n        return \"[\" + \"\".join(_serialize(x) for x in xp_ast) + \"]\"\n    elif isinstance(xp_ast, dict):\n        return \"{\" + \"\".join(_serialize(k) + \":\" + _serialize(v) for", "solution": "\n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield(tok)\n    elif isinstance(xp_ast, str):\n        yield(repr(xp_ast))"}
{"prompt": "def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"", "output": "\n        # Create a key with the prefix and delimiter\n        key = prefix + delimiter + bucket_name\n\n        # Check if the key exists in the bucket\n        if not self.bucket.get(key):\n            return False\n\n        return True\n\n    def get_prefixes(self, bucket_name, delimiter):\n        \"\"\"\n        Returns a list of all keys in a bucket that have the same prefix as the bucket name\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param delimiter: the delimiter", "solution": "\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist"}
{"prompt": "def group(self, name):\n        \"\"\"GROUP command.\n        \"\"\"", "output": "\n        if not self.groups:\n            self.groups.append(Group(name))\n        else:\n            for group in self.groups:\n                if group.name == name:\n                    group.members.append(self)\n                    return group\n            self.groups.append(Group(name))\n        return None\n\n    def __init__(self, name):\n        self.name = name\n        self.members = []\n\n", "solution": "\n        args = name\n\n        code, message = self.command(\"GROUP\", args)\n        if code != 211:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 4)\n        try:\n            total = int(parts[0])\n            first = int(parts[1])\n            last  = int(parts[2])\n            group = parts[3]\n        except (IndexError, ValueError):\n            raise NNTPDataError(\"Invalid GROUP status '%s'\" % message)\n\n        return total, first, last, group"}
{"prompt": "def get(self, thing_id='0', property_name=None):\n        \"\"\"\n        Handle a GET request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        \"\"\"", "output": "\n        if property_name is None:\n            property_name = self.url.path.split('/')[-1]\n        # TODO: Implement GET request logic here\n        pass\n\n    def post(self, thing_id='0', property_name=None):\n        \"\"\"\n        Handle a POST request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        \"\"\"\n        if property_name is None:\n            property_name = self.url.path.split('/", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        if thing.has_property(property_name):\n            self.set_header('Content-Type', 'application/json')\n            self.write(json.dumps({\n                property_name: thing.get_property(property_name),\n            }))\n        else:\n            self.set_status(404)"}
{"prompt": "def plexp_inv(P, xmin, alpha, guess=1.):\n    \"\"\"\n    Inverse CDF for a piecewise PDF as defined in eqn. 3.10\n    of Clauset et al.\n\n    (previous version was incorrect and lead to weird discontinuities in the\n    distribution function)\n    \"\"\"", "output": "\n    x = []\n    y = []\n    for i in range(len(P)-1):\n        if P[i] < guess and P[i+1] > guess:\n            x.append(guess)\n            y.append(P[i])\n        elif P[i] > guess and P[i+1] < guess:\n            x.append(guess)\n            y.append(P[i+1])\n        elif P[i] == guess:\n            x.append(guess)\n            y.append(", "solution": "\n    def equation(x,prob):\n        return plexp_cdf(x, xmin, alpha)-prob\n    # http://stackoverflow.com/questions/19840425/scipy-optimize-faster-root-finding-over-2d-grid\n    def solver(y, x0=guess):\n        return scipy.optimize.fsolve(equation, guess, args=(y,))\n    f = np.vectorize(solver)\n    return f(P)"}
{"prompt": "def start(self, context):\n\t\t\"\"\"Construct the SQLAlchemy engine and session factory.\"\"\"", "output": "\n\t\tself.engine = create_engine(self.config['DATABASE_URL'])\n\t\tself.Session = sessionmaker(bind=self.engine)\n\t\tself.session = self.Session()\n\n\tdef execute(self, query):\n\t\t\"\"\"Execute a SQL query on the database.\"\"\"\n\t\treturn self.session.execute(query)\n\n\tdef commit(self):\n\t\t\"\"\"Commit the current transaction on the database.\"\"\"\n\t\tself.session.commit()\n\n\tdef rollback(self):\n\t\t\"\"\"Roll back the current transaction on the database.\"", "solution": "\n\t\t\n\t\tif __debug__:\n\t\t\tlog.info(\"Connecting SQLAlchemy database layer.\", extra=dict(\n\t\t\t\t\turi = redact_uri(self.uri),\n\t\t\t\t\tconfig = self.config,\n\t\t\t\t\talias = self.alias,\n\t\t\t\t))\n\t\t\n\t\t# Construct the engine.\n\t\tengine = self.engine = create_engine(self.uri, **self.config)\n\t\t\n\t\t# Construct the session factory.\n\t\tself.Session = scoped_session(sessionmaker(bind=engine))\n\t\t\n\t\t# Test the connection.\n\t\tengine.connect().close()\n\t\t\n\t\t# Assign the engine to our database alias.\n\t\tcontext.db[self.alias] = engine"}
{"prompt": "def _matches_filepath_pattern(self, filepath):\n        \"\"\"Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns\"\"\"", "output": "\n        for pattern in self.filepath_patterns:\n            if re.match(pattern, filepath):\n                return True\n        return False\n\n    def _get_filepath_patterns(self):\n        \"\"\"This function returns a list of regex patterns that match the filepath\"\"\"\n        filepath_patterns = []\n        for pattern in self.filepath_patterns:\n            filepath_patterns.append(re.compile(pattern))\n        return filepath_patterns\n\n    def _get_filepath_matches(self, filepath):", "solution": "\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False"}
{"prompt": "def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"", "output": "\n    import time\n    import hashlib\n\n    # Generate a random token\n    token = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10))\n\n    # Convert the token to a string and encode it\n    token_str = token.encode('utf-8')\n\n    # Calculate the expiration time of the token\n    expiration_time = int(time.time()) + minutes_valid\n\n    # Add the token string and expiration time to the user's auth data\n    user_data = {'", "solution": "\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()"}
{"prompt": "def _control_key_down(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is down.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        \"\"\"", "output": "\n        if include_command and 'Control' in modifiers:\n            return True\n        elif 'Cmd' in modifiers:\n            return True\n        else:\n            return False\n\n    def _control_key_up(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is up.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        ", "solution": "\n        # Note that on Mac OS, ControlModifier corresponds to the Command key\n        # while MetaModifier corresponds to the Control key.\n        if sys.platform == 'darwin':\n            down = include_command and (modifiers & QtCore.Qt.ControlModifier)\n            return bool(down) ^ bool(modifiers & QtCore.Qt.MetaModifier)\n        else:\n            return bool(modifiers & QtCore.Qt.ControlModifier)"}
{"prompt": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"", "output": "\n        if not isinstance(cls, type):\n            raise TypeError(f\"{cls} is not a valid token class.\")\n        if not issubclass(cls, Token):\n            raise TypeError(f\"{cls} is not a valid token class.\")\n        return cls(*args, **kwargs)\n\n    return class_or_func(cls, compat_validate_token)\n\n", "solution": "\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data"}
{"prompt": "def timestamp_YmdHMS(value):\n    \"\"\"Convert timestamp string to time in seconds since epoch.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.\n\n    Args:\n        value: A timestamp string in the format '%Y%m%d%H%M%S'.\n\n    Returns:\n        The time in seconds since epoch as an integer.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n\n    Note: The timezone is assumed to be UTC/GMT.\n    \"\"\"", "output": "\n    try:\n        return int(time.mktime(time.strptime(value, '%Y%m%d%H%M%S')))\n    except ValueError:\n        raise ValueError(\"Invalid timestamp format: {}\".format(value))\n\n", "solution": "\n    i = int(value)\n    S = i\n    M = S//100\n    H = M//100\n    d = H//100\n    m = d//100\n    Y = m//100\n    return int(calendar.timegm((\n        Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0)\n    ))"}
{"prompt": "def report_by_type_stats(sect, stats, _):\n    \"\"\"make a report of\n\n    * percentage of different types documented\n    * percentage of different types with a bad name\n    \"\"\"", "output": "\n    types = set()\n    bad_names = set()\n    for s in sect:\n        for t in s.split():\n            if t in stats:\n                types.add(t)\n            elif len(t) > 1 and t[0] == t[1]:\n                bad_names.add(t)\n    if len(types) == 0:\n        return \"No types found.\"\n    else:\n        num_types = len(types)\n        num_bad_names = len(bad_names)\n        percent_types = round", "solution": "\n    # percentage of different types documented and/or with a bad name\n    nice_stats = {}\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        try:\n            total = stats[node_type]\n        except KeyError:\n            raise exceptions.EmptyReportError()\n        nice_stats[node_type] = {}\n        if total != 0:\n            try:\n                documented = total - stats[\"undocumented_\" + node_type]\n                percent = (documented * 100.0) / total\n                nice_stats[node_type][\"percent_documented\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_documented\"] = \"NC\"\n            try:\n                percent = (stats[\"badname_\" + node_type] * 100.0) / total\n                nice_stats[node_type][\"percent_badname\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_badname\"] = \"NC\"\n    lines = (\"type\", \"number\", \"old number\", \"difference\", \"%documented\", \"%badname\")\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        new = stats[node_type]\n        lines += (\n            node_type,\n            str(new),\n            \"NC\",\n            \"NC\",\n            nice_stats[node_type].get(\"percent_documented\", \"0\"),\n            nice_stats[node_type].get(\"percent_badname\", \"0\"),\n        )\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))"}
{"prompt": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"", "output": "\n        for bound_var in bound_vars:\n            if bound_var.name == ns_qualified_sym.name:\n                return bound_var\n        raise ValueError(f\"No Var bound to {ns_qualified_sym.name}\")\n\n    # Create a list of all the Var objects in the namespace\n    var_list = [var for var in vars.values()]\n\n    # Create a list of all the Var objects that are currently bound to names in the namespace\n    bound_vars = [find_safe(var) for var in var_list", "solution": "\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v"}
{"prompt": "def start(self):\n        \"\"\"Start the logger background synchronization service. This allows you to not need to\n        worry about syncing with ConnectorDB - you just insert into the Logger, and the Logger\n        will by synced every syncperiod.\"\"\"", "output": "\n        self.sync_event.clear()\n        self.sync_period = config.getint('Logger','syncperiod')\n        self.sync_event.wait()\n\n    def stop(self):\n        \"\"\"Stop the logger background synchronization service. This allows you to continue using\n        the Logger without syncing with ConnectorDB - you just insert into the Logger, and the Logger\n        will be synced every syncperiod.\"\"\"\n        self.sync_event.set()\n        self.sync_period = 0\n\n    def write(self, message):\n", "solution": "\n\n        with self.synclock:\n            if self.syncthread is not None:\n                logging.warn(\n                    \"Logger: Start called on a syncer that is already running\")\n                return\n\n        self.sync()  # Attempt a sync right away\n        self.__setsync()"}
{"prompt": "def make_path_relative(path, rel_to):\n    \"\"\"\n    Make a filename relative, where the filename path, and it is\n    relative to rel_to\n\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/another-place/src/Directory')\n        '../../../something/a-file.pth'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/home/user/src/Directory')\n        '../../../usr/share/something/a-file.pth'\n        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')\n        'a-file.pth'\n    \"\"\"", "output": "')\n        '../../../something/a-file.pth'\n\n    If rel_to is not a subdirectory of path, the function returns path unchanged.\n    \"\"\"\n    if rel_to.startswith(path):\n        rel_to = rel_to[len(path):]\n    if rel_to.startswith('/'):\n        rel_to = rel_to[1:]\n    if rel_to.endswith('/'):\n        rel_to = rel_to[:-1]\n    if rel_to!= path", "solution": "\n    path_filename = os.path.basename(path)\n    path = os.path.dirname(path)\n    path = os.path.normpath(os.path.abspath(path))\n    rel_to = os.path.normpath(os.path.abspath(rel_to))\n    path_parts = path.strip(os.path.sep).split(os.path.sep)\n    rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep)\n    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:\n        path_parts.pop(0)\n        rel_to_parts.pop(0)\n    full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename]\n    if full_parts == ['']:\n        return '.' + os.path.sep\n    return os.path.sep.join(full_parts)"}
{"prompt": "def _rotations_to_disentangle(local_param):\n        \"\"\"\n        Static internal method to work out Ry and Rz rotation angles used\n        to disentangle the LSB qubit.\n        These rotations make up the block diagonal matrix U (i.e. multiplexor)\n        that disentangles the LSB.\n\n        [[Ry(theta_1).Rz(phi_1)  0   .   .   0],\n         [0         Ry(theta_2).Rz(phi_2) .  0],\n                                    .\n                                        .\n          0         0           Ry(theta_2^n).Rz(phi_2^n)]]\n        \"\"\"", "output": ".\n                                   .\n                                   .\n                                    [Rz(theta_n).Rz(phi_n).  0]]\n\n        where Ry and Rz are rotation angles around the x and y axes respectively,\n        phi is the angle between the x-axis and the axis of rotation, and theta\n        is the angle between the z-axis and the axis of rotation.\n\n        Args:\n            local_param (int): The value of the parameter to be rotated.\n\n        Returns:\n            Tuple[int, int]: A tuple of two integers representing the Ry", "solution": "\n        remaining_vector = []\n        thetas = []\n        phis = []\n\n        param_len = len(local_param)\n\n        for i in range(param_len // 2):\n            # Ry and Rz rotations to move bloch vector from 0 to \"imaginary\"\n            # qubit\n            # (imagine a qubit state signified by the amplitudes at index 2*i\n            # and 2*(i+1), corresponding to the select qubits of the\n            # multiplexor being in state |i>)\n            (remains,\n             add_theta,\n             add_phi) = Initialize._bloch_angles(local_param[2 * i: 2 * (i + 1)])\n\n            remaining_vector.append(remains)\n\n            # rotations for all imaginary qubits of the full vector\n            # to move from where it is to zero, hence the negative sign\n            thetas.append(-add_theta)\n            phis.append(-add_phi)\n\n        return remaining_vector, thetas, phis"}
{"prompt": "def crcMeterRead(self, raw_read, def_buf):\n        \"\"\" Internal read CRC wrapper.\n\n        Args:\n            raw_read (str): Bytes with implicit string cast from serial read\n            def_buf (SerialBlock): Populated read buffer.\n\n        Returns:\n            bool:  True if passed CRC equals calculated CRC.\n        \"\"\"", "output": "\n        crc = 0xFFFF\n        for byte in raw_read:\n            crc ^= byte\n            for i in range(8):\n                if crc & 1:\n                    crc = (crc >> 1) ^ def_buf.crc\n                else:\n                    crc >>= 1\n        return crc == def_buf.crc\n\n    def crcMeterWrite(self, raw_write, def_buf):\n        \"\"\" Internal write CRC wrapper.\n\n        Args:\n            raw_write (str): Bytes with implicit string cast from", "solution": "\n        try:\n            if len(raw_read) == 0:\n                ekm_log(\"(\" + self.m_context + \") Empty return read.\")\n                return False\n            sent_crc = self.calc_crc16(raw_read[1:-2])\n            logstr = \"(\" + self.m_context + \")CRC sent = \" + str(def_buf[\"crc16\"][MeterData.StringValue])\n            logstr += \" CRC calc = \" + sent_crc\n            ekm_log(logstr)\n            if int(def_buf[\"crc16\"][MeterData.StringValue], 16) == int(sent_crc, 16):\n                return True\n\n        # A cross simple test lines on a USB serial adapter, these occur every\n        # 1000 to 2000 reads, and they show up here as a bad unpack or\n        # a bad crc type call.  In either case, we suppress them a log will\n        # become quite large.  ekmcrc errors come through as type errors.\n        # Failures of int type conversion in 16 bit conversion occur as value\n        # errors.\n        except struct.error:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except TypeError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except ValueError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        return False"}
{"prompt": "def is_bare_exception(self, node):\n        \"\"\"\n        Checks if the node is a bare exception name from an except block.\n\n        \"\"\"", "output": "\n        if node.value == \"except\":\n            for child in node.children:\n                if isinstance(child, Tree) and child.data == \"Exception\":\n                    return True\n        return False\n\n    def is_bare_class(self, node):\n        \"\"\"\n        Checks if the node is a bare class name from an except block.\n\n        \"\"\"\n        if node.value == \"except\":\n            for child in node.children:\n                if isinstance(child, Tree) and child.data == \"Class\":\n                    return True\n        return False\n\n    ", "solution": "\n        return isinstance(node, Name) and node.id in self.current_except_names"}
{"prompt": "def add_gene_links(gene_obj, build=37):\n    \"\"\"Update a gene object with links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"", "output": "\n    gene_obj['links'] = {\n        'build37': f'https://www.ncbi.nlm.nih.gov/gene/{build}/',\n        'build38': f'https://www.ncbi.nlm.nih.gov/gene/{build}/',\n        'build39': f'https://www.ncbi.nlm.nih.gov/gene/{build}/'\n    }\n    return gene_obj\n\n", "solution": "\n    try:\n        build = int(build)\n    except ValueError:\n        build = 37\n    # Add links that use the hgnc_id\n    hgnc_id = gene_obj['hgnc_id']\n\n    gene_obj['hgnc_link'] = genenames(hgnc_id)\n    gene_obj['omim_link'] = omim(hgnc_id)\n    # Add links that use ensembl_id\n    if not 'ensembl_id' in gene_obj:\n        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')\n    else:\n        ensembl_id = gene_obj['ensembl_id']\n    ensembl_37_link = ensembl(ensembl_id, build=37)\n    ensembl_38_link = ensembl(ensembl_id, build=38)\n    gene_obj['ensembl_37_link'] = ensembl_37_link\n    gene_obj['ensembl_38_link'] = ensembl_38_link\n    gene_obj['ensembl_link'] = ensembl_37_link\n    if build == 38:\n        gene_obj['ensembl_link'] = ensembl_38_link\n    gene_obj['hpa_link'] = hpa(ensembl_id)\n    gene_obj['string_link'] = string(ensembl_id)\n    gene_obj['reactome_link'] = reactome(ensembl_id)\n    gene_obj['clingen_link'] = clingen(hgnc_id)\n    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)\n    gene_obj['exac_link'] = exac(ensembl_id)\n    # Add links that use entrez_id\n    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))\n    # Add links that use omim id\n    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))\n    # Add links that use hgnc_symbol\n    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])\n    # Add links that use vega_id\n    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))\n    # Add links that use ucsc link\n    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))"}
{"prompt": "def cqt_frequencies(n_bins, fmin, bins_per_octave=12, tuning=0.0):\n    \"\"\"Compute the center frequencies of Constant-Q bins.\n\n    Examples\n    --------\n    >>> # Get the CQT frequencies for 24 notes, starting at C2\n    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))\n    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,\n            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])\n\n    Parameters\n    ----------\n    n_bins  : int > 0 [scalar]\n        Number of constant-Q bins\n\n    fmin    : float > 0 [scalar]\n        Minimum frequency\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)`\n        Deviation from A440 tuning in fractional bins (cents)\n\n    Returns\n    -------\n    frequencies : np.ndarray [shape=(n_bins,)]\n        Center frequency for each CQT bin\n    \"\"\"", "output": ".384,   87.768,\n             88.936,   97.  ,   97.  ,   98.  ,   99.  ,  100.  ,\n            101.  ,  105.  ,  106.  ,  107.  ,  108.  ,  109.  ,\n            110.  ,  111.  ,  112.  ,  113.  ,  114.  ,  115.  ,\n            116.  ,  117.  ,  118.  ,  119.  ,  120.  ,", "solution": "\n\n    correction = 2.0**(float(tuning) / bins_per_octave)\n    frequencies = 2.0**(np.arange(0, n_bins, dtype=float) / bins_per_octave)\n\n    return correction * fmin * frequencies"}
{"prompt": "def export_keying_material(self, label, olen, context=None):\n        \"\"\"\n        Obtain keying material for application use.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param: olen - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: the exported key material bytes or None\n        \"\"\"", "output": "\n        if context is None:\n            context = {}\n        if label not in context:\n            return None\n        keying_material = context[label]\n        if len(keying_material) < olen:\n            return None\n        return keying_material[:olen]\n\n", "solution": "\n        outp = _no_zero_allocator(\"unsigned char[]\", olen)\n        context_buf = _ffi.NULL\n        context_len = 0\n        use_context = 0\n        if context is not None:\n            context_buf = context\n            context_len = len(context)\n            use_context = 1\n        success = _lib.SSL_export_keying_material(self._ssl, outp, olen,\n                                                  label, len(label),\n                                                  context_buf, context_len,\n                                                  use_context)\n        _openssl_assert(success == 1)\n        return _ffi.buffer(outp, olen)[:]"}
{"prompt": "def register_function(scope=None, as_property=False, name=None):\n    \"\"\"Decorator to register a new function with vaex.\n\n    Example:\n\n    >>> import vaex\n    >>> df = vaex.example()\n    >>> @vaex.register_function()\n    >>> def invert(x):\n    >>>     return 1/x\n    >>> df.x.invert()\n\n\n    >>> import numpy as np\n    >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))\n    >>> @vaex.register_function(as_property=True, scope='dt')\n    >>> def dt_relative_day(x):\n    >>>     return vaex.functions.dt_dayofyear(x)/365.\n    >>> df.departure.dt.relative_day\n    \"\"\"", "output": "', '2016-01-01'),\n    >>>                        arrival=np.arange('2016-01-01', '2017-01-01'))\n    >>> df.x.invert()\n    array([1.       , 0.5      , 0.33333333, 0.25     , 0.2      , 0.16666667,\n            0.14285714, 0.125    , 0.11111111, 0.1      , 0.090909091, 0.08333333])\n\n    :param scope: The vaex DataFrame scope", "solution": "\n    prefix = ''\n    if scope:\n        prefix = scope + \"_\"\n        if scope not in scopes:\n            raise KeyError(\"unknown scope\")\n    def wrapper(f, name=name):\n        name = name or f.__name__\n        # remove possible prefix\n        if name.startswith(prefix):\n            name = name[len(prefix):]\n        full_name = prefix + name\n        if scope:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.expression.ds.func, full_name)\n                    args = (self.expression, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            if as_property:\n                setattr(scopes[scope], name, property(closure()))\n            else:\n                setattr(scopes[scope], name, closure())\n        else:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.ds.func, full_name)\n                    args = (self, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            setattr(vaex.expression.Expression, name, closure())\n        vaex.expression.expression_namespace[prefix + name] = f\n        return f  # we leave the original function as is\n    return wrapper"}
{"prompt": "def iter_hierarchy(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over parent-child relations\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.\n        :param int sleep: The amount of time to sleep between pages. Defaults to 0 seconds.\n        :rtype: iter[tuple[str,str]]\n        \"\"\"", "output": "\n        if size is None:\n            size = 500\n        if sleep is None:\n            sleep = 0\n\n        # Get the parent-child relationships for the given ontology\n        relationships = get_relationships(ontology, size=size)\n\n        # Iterate over the relationships and yield each parent-child pair\n        for parent, child in relationships:\n            yield parent, child\n\n        # Wait for the specified amount of time before continuing\n        time.sleep(sleep)\n\n    def get_relationships(ontology, size=None, sleep=None):\n        \"\"\"", "solution": "\n        for term in self.iter_terms(ontology=ontology, size=size, sleep=sleep):\n            try:\n                hierarchy_children_link = term['_links'][HIERARCHICAL_CHILDREN]['href']\n            except KeyError:  # there's no children for this one\n                continue\n\n            response = requests.get(hierarchy_children_link).json()\n\n            for child_term in response['_embedded']['terms']:\n                yield term['label'], child_term['label']"}
{"prompt": "def closeAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"", "output": "\n        slot_data = self.getSlotData(slot)\n        if slot_data:\n            for session in slot_data:\n                session.close()\n\n", "solution": "\n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"Load PanelApp panels into scout database\n    \n    If no panel_id load all PanelApp panels \n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panel_id(str): The panel app panel id\n    \"\"\"", "output": "\n    panels = adapter.get_all_panels()\n    if panel_id:\n        panels = [p for p in panels if p.id == panel_id]\n    else:\n        panels = panels\n    for panel in panels:\n        institute_id = panel.get('institute_id', institute)\n        panel_app = scout.PanelApp(panel_id=panel.id, institute_id=institute_id)\n        scout.session.add(panel_app)\n\n", "solution": "\n    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err"}
{"prompt": "def reads_py(s, **kwargs):\n    \"\"\"Read a .py notebook from a string and return the NotebookNode object.\"\"\"", "output": "\n    from IPython.core.display import display, HTML\n    from traitlets import TraitError\n    try:\n        nb = nbformat.reads(s, as_version=4, **kwargs)\n    except TraitError as e:\n        raise TraitError(e)\n    display(HTML(str(nb)))\n    return NotebookNode(nb)\n\n", "solution": "\n    nbf, nbm, s = parse_py(s, **kwargs)\n    if nbf == 2:\n        nb = v2.to_notebook_py(s, **kwargs)\n    elif nbf == 3:\n        nb = v3.to_notebook_py(s, **kwargs)\n    else:\n        raise NBFormatError('Unsupported PY nbformat version: %i' % nbf)\n    return nb"}
{"prompt": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"", "output": "\n    import os\n    import stat\n\n    # Ignore umask for directories\n    os.umask(mode)\n\n    # Create the directory\n    try:\n        os.mkdir(path)\n    except OSError:\n        pass\n\n", "solution": "\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"}
{"prompt": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"", "output": " elements based on the `model` structure.)\n      leaf_name: A `str` name corresponding to the leaf nodes in the graph.\n\n    Returns:\n      A `tuple` of `tuple`s of dependencies. Each `tuple` represents a node in\n      the graph and contains the names of its dependencies.\n    \"\"\"\n    # TODO(b/124567897): Implement this function.\n    pass\n\n", "solution": "\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))"}
{"prompt": "def cli_encrypt(context, key):\n    \"\"\"\n    Encrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to encrypt to disk before attempting to\n    upload, allowing uploads retries and segmented encrypted objects.\n\n    See :py:mod:`swiftly.cli.encrypt` for context usage information.\n\n    See :py:class:`CLIEncrypt` for more information.\n    \"\"\"", "output": "\n    # TODO: Implement encryption logic here\n    pass\n\n", "solution": "\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):\n                stdout.write(chunk)\n            stdout.flush()"}
{"prompt": "def _confirm_exit(self):\n        \"\"\"confirm shutdown on ^C\n        \n        A second ^C, or answering 'y' within 5s will cause shutdown,\n        otherwise original SIGINT handler will be restored.\n        \n        This doesn't work on Windows.\n        \"\"\"", "output": "\n        import msvcrt\n        if msvcrt.getch() == '\\x1b':  # on Windows, use msvcrt.getch() instead\n            return\n        else:\n            import signal\n            signal.signal(signal.SIGINT, signal.SIG_DFL)\n            raise SystemExit(\"Exiting due to ^C\")\n\n    def __enter__(self):\n        \"\"\"enter context manager\"\"\"\n        self.start_time = time.time()\n        return self\n\n    def __exit__(self,", "solution": "\n        # FIXME: remove this delay when pyzmq dependency is >= 2.1.11\n        time.sleep(0.1)\n        sys.stdout.write(\"Shutdown Notebook Server (y/[n])? \")\n        sys.stdout.flush()\n        r,w,x = select.select([sys.stdin], [], [], 5)\n        if r:\n            line = sys.stdin.readline()\n            if line.lower().startswith('y'):\n                self.log.critical(\"Shutdown confirmed\")\n                ioloop.IOLoop.instance().stop()\n                return\n        else:\n            print \"No answer for 5s:\",\n        print \"resuming operation...\"\n        # no answer, or answer is no:\n        # set it back to original SIGINT handler\n        # use IOLoop.add_callback because signal.signal must be called\n        # from main thread\n        ioloop.IOLoop.instance().add_callback(self._restore_sigint_handler)"}
{"prompt": "def send_raw(self, stream, msg_list, flags=0, copy=True, ident=None):\n        \"\"\"Send a raw message via ident path.\n\n        This method is used to send a already serialized message.\n\n        Parameters\n        ----------\n        stream : ZMQStream or Socket\n            The ZMQ stream or socket to use for sending the message.\n        msg_list : list\n            The serialized list of messages to send. This only includes the\n            [p_header,p_parent,p_content,buffer1,buffer2,...] portion of\n            the message.\n        ident : ident or list\n            A single ident or a list of idents to use in sending.\n        \"\"\"", "output": "the message list.\n        flags : int, optional\n            The flags to use for sending the message. Defaults to 0.\n        copy : bool, optional\n            If True, the message is copied before being sent. If False, the\n            message is sent as is. Defaults to True.\n        ident : str, optional\n            The path to use for sending the message. Defaults to the\n            current context's identity path.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if copy:\n            msg_list = msg_list.copy()\n        for i,", "solution": "\n        to_send = []\n        if isinstance(ident, bytes):\n            ident = [ident]\n        if ident is not None:\n            to_send.extend(ident)\n\n        to_send.append(DELIM)\n        to_send.append(self.sign(msg_list))\n        to_send.extend(msg_list)\n        stream.send_multipart(msg_list, flags, copy=copy)"}
{"prompt": "def resolve_expression(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"", "output": "\n        return HStoreColumn(*args, **kwargs)\n\n    def __repr__(self) -> str:\n        return f\"<HStoreColumn '{self.name}'>\"\n\n\nclass HStoreRow:\n    \"\"\"\n    A class representing a row in a HStore table.\n\n    Args:\n        *values: The values in the row.\n\n    Attributes:\n        *columns: A list of :see:HStoreColumn objects representing the columns in the row.\n    \"\"\"\n\n    def __init__(self, *values: List[Any], columns", "solution": "\n\n        original_expression = super().resolve_expression(*args, **kwargs)\n        expression = HStoreColumn(\n            original_expression.alias,\n            original_expression.target,\n            self.key\n        )\n        return expression"}
{"prompt": "def insert_default_options():\n    \"\"\"insert default options to sys.argv\n    \"\"\"", "output": "\n    default_options = {\n        \"verbose\": False,\n        \"debug\": False,\n        \"log_file\": None\n    }\n    for option, value in default_options.items():\n        if option in sys.argv:\n            index = sys.argv.index(option)\n            sys.argv.insert(index+1, value)\n\n", "solution": "\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)"}
{"prompt": "def get_components(self):\n        \"\"\"\n        Returns the components of this name, as a sequence of 2-tuples.\n\n        :return: The components of this name.\n        :rtype: :py:class:`list` of ``name, value`` tuples.\n        \"\"\"", "output": "\n        return [(name, value) for name, value in self.__dict__.items() if name!= '_name']\n\n    def set_components(self, components):\n        \"\"\"\n        Sets the components of this name, updating the internal state.\n\n        :param components: The components to set.\n        :type components: :py:class:`list` of ``name, value`` tuples.\n        \"\"\"\n        self.__dict__.update(components)\n\n    components = property(get_components, set_components)\n\n    def", "solution": "\n        result = []\n        for i in range(_lib.X509_NAME_entry_count(self._name)):\n            ent = _lib.X509_NAME_get_entry(self._name, i)\n\n            fname = _lib.X509_NAME_ENTRY_get_object(ent)\n            fval = _lib.X509_NAME_ENTRY_get_data(ent)\n\n            nid = _lib.OBJ_obj2nid(fname)\n            name = _lib.OBJ_nid2sn(nid)\n\n            # ffi.string does not handle strings containing NULL bytes\n            # (which may have been generated by old, broken software)\n            value = _ffi.buffer(_lib.ASN1_STRING_data(fval),\n                                _lib.ASN1_STRING_length(fval))[:]\n            result.append((_ffi.string(name), value))\n\n        return result"}
{"prompt": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the pages\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "output": "\n        for page in range(1, 11):\n            yield {\n                'category': category,\n                'page': page,\n                **kwargs\n            }\n\n", "solution": "\n        from_date = kwargs['from_date']\n        reviews_api = kwargs['reviews_api']\n\n        mediawiki_version = self.client.get_version()\n        logger.info(\"MediaWiki version: %s\", mediawiki_version)\n\n        if reviews_api:\n            if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1):\n                fetcher = self.__fetch_1_27(from_date)\n            else:\n                logger.warning(\"Reviews API only available in MediaWiki >= 1.27\")\n                logger.warning(\"Using the Pages API instead\")\n                fetcher = self.__fetch_pre1_27(from_date)\n        else:\n            fetcher = self.__fetch_pre1_27(from_date)\n\n        for page_reviews in fetcher:\n            yield page_reviews"}
{"prompt": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"", "output": "\n    session = create_session_class()()  # create a new session object\n    try:\n        yield session  # yield the session object to the caller\n    finally:\n        session.close()  # close the session\n\n", "solution": "\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()"}
{"prompt": "def disable_report(self, reportid):\n        \"\"\"disable the report of the given id\"\"\"", "output": "\n        if reportid in self.reports:\n            self.reports[reportid] = False\n\n    def enable_report(self, reportid):\n        \"\"\"enable the report of the given id\"\"\"\n        if reportid in self.reports:\n            self.reports[reportid] = True\n\n    def get_report(self, reportid):\n        \"\"\"return the report of the given id\"\"\"\n        return self.reports.get(reportid, False)\n\n    def has_reports(self):\n        \"\"\"return True if there are reports, False otherwise\"\"\"\n        return bool", "solution": "\n        reportid = reportid.upper()\n        self._reports_state[reportid] = False"}
{"prompt": "def chrono(ctx, app_id, sentence_file,\n           json_flag, sentence, doc_time, request_id):\n    # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA\n    \"\"\"Extract expression expressing date and time and normalize its value \"\"\"", "output": "\n    import re\n    import datetime\n\n    # Extract date and time from sentence\n    match = re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', sentence)\n    if match:\n        sentence_time = datetime.datetime.strptime(match.group(), '%m/%d/%Y %H:%M:%S')\n    else:\n        sentence_time = None\n\n    # Extract expression expressing date and time from JSON\n    if json_flag:", "solution": "\n\n    app_id = clean_app_id(app_id)\n    sentence = clean_sentence(sentence, sentence_file)\n\n    api = GoolabsAPI(app_id)\n    ret = api.chrono(\n        sentence=sentence,\n        doc_time=doc_time,\n        request_id=request_id,\n    )\n\n    if json_flag:\n        click.echo(format_json(api.response.json()))\n        return\n\n    for pair in ret['datetime_list']:\n        click.echo(u'{0}: {1}'.format(text(pair[0]), pair[1]))"}
{"prompt": "def _get_unpacking_extra_info(node, infered):\n    \"\"\"return extra information to add to the message for unpacking-non-sequence\n    and unbalanced-tuple-unpacking errors\n    \"\"\"", "output": "\n    if isinstance(node, ast.Tuple):\n        return [_get_unpacking_extra_info(elt, infered) for elt in node.elts]\n    elif isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.Not):\n        return _get_unpacking_extra_info(node.operand, infered)\n    elif isinstance(node, ast.BinOp) and isinstance(node.op, ast.Mod):\n        return _get_unpacking_extra_info", "solution": "\n    more = \"\"\n    infered_module = infered.root().name\n    if node.root().name == infered_module:\n        if node.lineno == infered.lineno:\n            more = \" %s\" % infered.as_string()\n        elif infered.lineno:\n            more = \" defined at line %s\" % infered.lineno\n    elif infered.lineno:\n        more = \" defined at line %s of %s\" % (infered.lineno, infered_module)\n    return more"}
{"prompt": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"", "output": "\n        print(info)\n\n    def display_options(self):\n        \"\"\" Displays the options menu. \"\"\"\n        print(\"Please select an option:\")\n        for i, option in enumerate(self.options):\n            print(f\"{i+1}. {option}\")\n\n    def handle_input(self, input):\n        \"\"\" Handles user input. \"\"\"\n        if input.isdigit() and int(input) in range(1, len(self.options)+1):\n            return int(input) - 1\n        else:\n            return -", "solution": "\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )"}
{"prompt": "def getMappingsOnDeviceType(self, typeId, draft=False):\n        \"\"\"\n        Get all the mappings for a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/device-type/{typeId}/mappings\"\n        params = {}\n        if draft:\n            params[\"draft\"] = \"true\"\n        response = self.session.get(url, params=params)\n        if response.status_code!= 200:\n            raise APIException(f\"Failed to get mappings for {typeId}\")\n        return response.json()\n\n    def getMappingsForDeviceTypeById(self, typeId, id):\n        \"\"\"\n        Get the mappings for", "solution": "\n        if draft:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"/draft\", typeId)\n        else:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"\", typeId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"All device type mappings retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting all device type mappings\", resp)\n        return resp.json()"}
{"prompt": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"", "output": "\n    if not username:\n        return [f\"{user.username}'s Video List\" for user in request.user.users]\n    else:\n        videos = []\n        for user in request.user.users:\n            if user.username == username:\n                videos.append(f\"{user.username}'s Video List\")\n        return videos\n\n", "solution": "\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )"}
{"prompt": "def weekday(year_or_num, month=None, day=None, full=False):\n    \"\"\"Simple tag - returns the weekday of the given (year, month, day) or of given (weekday_number).\n\n    Usage (in template):\n\n    {% weekday 2014 3 3 %}\n\n    Result: Mon\n\n    Return abbreviation by default. To return full name: pass full=True\n\n    {% weekday 2014 3 3 full=True %}\n\n    Result: Monday\n\n\n    When only number of weekday is given then 0 is considered as \"Monday\"\n\n    {% weekday 0 full=True %}\n\n    Result: Monday\n\n    \"\"\"", "output": " weekday 3 %}\n\n    Result: Wed\n\n    \"\"\"\n    weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    if month is None:\n        month = 1\n    if day is None:\n        day = 1\n    weekday_num = (month - 1) * 31 + day - 1\n    if full:\n        return weekdays[weekday_num % 7]\n    else:\n        return weekdays[weekday_num]\n\n", "solution": "\n    if any([month, day]) and not all([month, day]):\n        raise TemplateSyntaxError(\"weekday accepts 1 or 3 arguments plus optional 'full' argument\")\n\n    try:\n        if all([year_or_num, month, day]):\n            weekday_num = date(*map(int, (year_or_num, month, day))).weekday()\n        else:\n            weekday_num = year_or_num\n        if full:\n            return WEEKDAYS[weekday_num]\n        else:\n            return WEEKDAYS_ABBR[weekday_num]\n    except Exception:\n        return"}
{"prompt": "def clinvar_submissions(store, user_id, institute_id):\n    \"\"\"Get all Clinvar submissions for a user and an institute\"\"\"", "output": "\n    submissions = []\n    for submission in store.submissions.all():\n        if submission.user_id == user_id and submission.institute_id == institute_id:\n            submissions.append(submission)\n    return submissions\n\n", "solution": "\n    submissions = list(store.clinvar_submissions(user_id, institute_id))\n    return submissions"}
{"prompt": "def get_table(self, arch, pattern, colored=False, verbose=False):\n        \"\"\"\n        This function is used in sys command (when user want to find a specific syscall)\n\n        :param Architecture for syscall table;\n        :param Searching pattern;\n        :param Flag for verbose output\n        :return Return a printable table of matched syscalls\n        \"\"\"", "output": "\n        table = []\n        for arch_inst in arch:\n            for pattern_inst in pattern:\n                if arch_inst.name == pattern_inst.arch and arch_inst.pattern == pattern_inst.pattern:\n                    table.append(f\"{arch_inst.name} {pattern_inst.pattern}\")\n                    if verbose:\n                        print(f\"{arch_inst.name} {pattern_inst.pattern}\")\n                    break\n        return table\n\n", "solution": "\n\n        rawtable = self.search(arch, pattern)\n        if len(rawtable) == 0:\n            return None\n\n        used_hd = self.__fetch_used_headers(rawtable, verbose)\n        table   = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]\n\n        for command in rawtable:\n            cur_tb_field = []\n            for hd in used_hd:\n                value = command[hd]\n                cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))\n            table.append(cur_tb_field)\n        return DoubleTable(table)"}
{"prompt": "def str_variant(store, institute_id, case_name, variant_id):\n    \"\"\"Pre-process an STR variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n        \"\"\"", "output": "variant_obj>\n        }\n    \"\"\"\n    detailed_information = {\n        'institute': store.get_institute(institute_id),\n        'case': store.get_case(case_name),\n        'variant': store.get_variant(variant_id)\n    }\n    return detailed_information\n\n", "solution": "\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj =  store.variant(variant_id)\n\n    # fill in information for pilup view\n    variant_case(store, case_obj, variant_obj)\n\n    variant_obj['callers'] = callers(variant_obj, category='str')\n\n    # variant_obj['str_ru']\n    # variant_obj['str_repid']\n    # variant_obj['str_ref']\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }"}
{"prompt": "def index(context, update):\n    \"\"\"Create indexes for the database\"\"\"", "output": "\n    indexes = {}\n    for column in update:\n        index_name = f\"{column}_index\"\n        index_value = index_name + \"_value\"\n        index_type = index_name + \"_type\"\n        indexes[index_name] = {\n            \"name\": index_name,\n            \"value\": index_value,\n            \"type\": index_type\n        }\n    return indexes\n\n", "solution": "\n    LOG.info(\"Running scout index\")\n    adapter = context.obj['adapter']\n    \n    if update:\n        adapter.update_indexes()\n    else:\n        adapter.load_indexes()"}
{"prompt": "def remove_pid_file(self):\n        \"\"\"Remove the pid file.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"", "output": "\n        pass\n\n    def remove_pid_file_safely(self):\n        \"\"\"Remove the pid file safely.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        either an empty list or a string indicating that the pid file\n        could not be removed.\n        \"\"\"\n        try:\n            os.remove(self.pid_file)\n        except OSError:\n            pass\n        return []\n\n    def get_pid_file(self):\n        \"\"\"Return the", "solution": "\n        pid_file = os.path.join(self.profile_dir.pid_dir, self.name + u'.pid')\n        if os.path.isfile(pid_file):\n            try:\n                self.log.info(\"Removing pid file: %s\" % pid_file)\n                os.remove(pid_file)\n            except:\n                self.log.warn(\"Error removing the pid file: %s\" % pid_file)"}
{"prompt": "def getoutput_pexpect(self, cmd):\n        \"\"\"Run a command and return its stdout/stderr as a string.\n\n        Parameters\n        ----------\n        cmd : str\n          A command to be executed in the system shell.\n\n        Returns\n        -------\n        output : str\n          A string containing the combination of stdout and stderr from the\n        subprocess, in whatever order the subprocess originally wrote to its\n        file descriptors (so the order of the information in this string is the\n        correct order as would be seen if running the command in a terminal).\n        \"\"\"", "output": "\n        \"\"\"\n        import subprocess\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        out, err = p.communicate()\n        return out.decode('utf-8') + err.decode('utf-8')\n\n    def getoutput_pexpect_lines(self, cmd):\n        \"\"\"Run a command and return its stdout/stderr as a list of strings.\n\n        Parameters\n        ----------\n        cmd : str\n          A command to be executed", "solution": "\n        try:\n            return pexpect.run(self.sh, args=['-c', cmd]).replace('\\r\\n', '\\n')\n        except KeyboardInterrupt:\n            print('^C', file=sys.stderr, end='')"}
{"prompt": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"", "output": "\n        return hash(value)\n\n    def verify_signature(self, value, signature):\n        \"\"\"Verifies the signature for the given value\"\"\"\n        return hash(value) == signature\n\n", "solution": "\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)"}
{"prompt": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"", "output": "\n        config = Configurables()\n        defaults = {\n            \"theme\": \"Light\",\n            \"font\": \"Arial\",\n            \"logo\": \"https://example.com/logo.png\"\n        }\n        for key, value in defaults.items():\n            config[key] = value\n        with open(\"default.cfg\", \"w\") as f:\n            config.write(f)\n\n", "solution": "\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)"}
{"prompt": "def _republish_displaypub(self, content, eid):\n        \"\"\"republish individual displaypub content dicts\"\"\"", "output": "\n        for key, value in content.items():\n            if isinstance(value, dict):\n                self._republish_displaypub(value, eid)\n            else:\n                content[key] = {'eid': eid, 'content': value}\n        return content\n\n    def publish_displaypubs(self, eids):\n        \"\"\"Publish individual displaypub content dicts to a list of eids\"\"\"\n        displaypubs = []\n        for eid in eids:\n            displaypubs.append(self.get_displaypub(", "solution": "\n        try:\n            ip = get_ipython()\n        except NameError:\n            # displaypub is meaningless outside IPython\n            return\n        md = content['metadata'] or {}\n        md['engine'] = eid\n        ip.display_pub.publish(content['source'], content['data'], md)"}
{"prompt": "def __make_points_for_label(self, ts, data, label, prefix, gun_stats):\n        \"\"\"x\n        Make a set of points for `this` label\n\n        overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        \"\"\"", "output": "\n        points = []\n        for i, t in enumerate(ts):\n            if label in data[i]:\n                value = data[i][label]\n                if value is None:\n                    continue\n                if value < 0:\n                    value = 0\n                elif value > 100:\n                    value = 100\n                points.append((value, prefix + str(i)))\n        overall_quantiles = sorted(overall_quantiles)\n        overall_meta = sorted(overall_meta)\n        net_codes = sorted(net_codes)\n", "solution": "\n        label_points = list()\n\n        label_points.extend(\n            (\n                # overall quantiles for label\n                self.__make_points(\n                    prefix + \"overall_quantiles\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_quantile_fields(data)\n                ),\n                # overall meta (gun status) for label\n                self.__make_points(\n                    prefix + \"overall_meta\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_overall_meta_fields(data, gun_stats)\n                ),\n                # net codes for label\n                self.__make_points(\n                    prefix + \"net_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_netcodes_fields(data)\n                ),\n                # proto codes for label\n                self.__make_points(\n                    prefix + \"proto_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_protocodes_fields(data)\n                )\n            )\n        )\n        # histograms, one row for each bin\n        if self.histograms:\n            for bin_, count in zip(data[\"interval_real\"][\"hist\"][\"bins\"],\n                                   data[\"interval_real\"][\"hist\"][\"data\"]):\n                label_points.append(\n                    self.__make_points(\n                        prefix + \"histograms\",\n                        {\"label\": label},\n                        ts,\n                        {\"bin\": bin_, \"count\": count}\n                    )\n                )\n        return label_points"}
{"prompt": "def app_url_defaults(self, f):\n        \"\"\"Same as :meth:`url_defaults` but application wide.\n        \"\"\"", "output": "\n        self.application_url = f.__name__\n        return f\n\n    def url_defaults(self, url, args=None, kwargs=None):\n        \"\"\"Returns a dictionary of default values for a given URL.\n\n        If the URL has a default value, it is included in the returned dictionary.\n        If the URL does not have a default value, the function returns an empty dictionary.\n\n        Args:\n            url (str): The URL to get default values for.\n            args (list, optional): List of arguments to pass to the function.\n            kw", "solution": "\n        self.record_once(lambda s: s.app.url_default_functions\n            .setdefault(None, []).append(f))\n        return f"}
{"prompt": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"", "output": "\n        with open(tmp_file, 'w') as f:\n            for row in table:\n                f.write('\\t'.join(row) + '\\n')\n\n    def load_data(self, file):\n        \"\"\"\n        Reads a tab-delimited file and returns a list of lists\n        \"\"\"\n        data = []\n        with open(file, 'r') as f:\n            for line in f:\n                row = line.strip().split('\\t')\n                data.append(row)\n        return data\n\n    def query", "solution": "\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)"}
{"prompt": "def  make_html_items( self, items ):\n        \"\"\" convert a field's content into some valid HTML \"\"\"", "output": "\n        html_items = []\n        for item in items:\n            if item is None:\n                html_items.append( '' )\n            elif isinstance( item, str ):\n                html_items.append( '<p>{}</p>'.format( item ) )\n            elif isinstance( item, list ):\n                html_items.append( '<ul>' )\n                for subitem in item:\n                    if subitem is None:\n                        html_items.append( '' )\n                    elif isinstance( subitem, str ):", "solution": "\n        lines = []\n        for item in items:\n            if item.lines:\n                lines.append( self.make_html_code( item.lines ) )\n            else:\n                lines.append( self.make_html_para( item.words ) )\n\n        return string.join( lines, '\\n' )"}
{"prompt": "def p_relate_statement_2(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase\"\"\"", "output": "\n        p[0] = f\"{p[1]} {p[3]} {p[4]}\"\n\n    def p_relate_statement_3(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name DELETE rel_id\"\"\"\n        p[0] = f\"{p[1]} {p[3]}\"\n\n    def p_relate_statement_4(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name MERGE rel_id SAME_AS rel_id\"\"\"\n        p", "solution": "\n        p[0] = RelateNode(from_variable_name=p[2],\n                          to_variable_name=p[4],\n                          rel_id=p[6],\n                          phrase=p[8])"}
{"prompt": "def load_exons(self, exons, genes=None, build='37'):\n        \"\"\"Create exon objects and insert them into the database\n        \n        Args:\n            exons(iterable(dict))\n        \"\"\"", "output": "\n        if genes is None:\n            genes = set()\n        for exon in exons:\n            gene = exon['gene']\n            if gene not in genes:\n                gene_id = self.get_gene_id(gene)\n                if gene_id is None:\n                    gene_id = self.get_gene_id(gene, build=build)\n                if gene_id is None:\n                    raise ValueError('Invalid gene name: {}'.format(gene))\n                gene_obj = Gene(", "solution": "\n        genes = genes or self.ensembl_genes(build)\n        for exon in exons:\n            exon_obj = build_exon(exon, genes)\n            if not exon_obj:\n                continue\n            \n            res = self.exon_collection.insert_one(exon_obj)"}
{"prompt": "def enrich(self, gmt):\n        \"\"\"use local mode\n         \n        p = p-value computed using the Fisher exact test (Hypergeometric test)  \n\n        Not implemented here:\n\n            combine score = log(p)\u00b7z\n\n        see here: http://amp.pharm.mssm.edu/Enrichr/help#background&q=4\n        \n        columns contain:\n            \n            Term Overlap P-value Adjusted_P-value Genes\n\n        \"\"\"", "output": "\n        pass\n\n", "solution": "\n        if isscalar(self.background):\n            if isinstance(self.background, int) or self.background.isdigit():\n                self._bg = int(self.background)\n            elif isinstance(self.background, str):\n                # self.background = set(reduce(lambda x,y: x+y, gmt.values(),[]))\n                self._bg = self.get_background()\n                self._logger.info(\"Background: found %s genes\"%(len(self._bg)))\n            else:\n                raise Exception(\"Unsupported background data type\")\n        else:\n            # handle array object: nd.array, list, tuple, set, Series\n            try:\n                it = iter(self.background)\n                self._bg = set(self.background)\n            except TypeError:\n                self._logger.error(\"Unsupported background data type\")\n        # statistical testing\n        hgtest = list(calc_pvalues(query=self._gls, gene_sets=gmt, \n                                   background=self._bg))\n        if len(hgtest) > 0:\n            terms, pvals, olsz, gsetsz, genes = hgtest\n            fdrs, rej = multiple_testing_correction(ps = pvals, \n                                                    alpha=self.cutoff,\n                                                    method='benjamini-hochberg')\n            # save to a dataframe\n            odict = OrderedDict()\n            odict['Term'] = terms\n            odict['Overlap'] = list(map(lambda h,g: \"%s/%s\"%(h, g), olsz, gsetsz))\n            odict['P-value'] = pvals\n            odict['Adjusted P-value'] = fdrs\n            # odict['Reject (FDR< %s)'%self.cutoff ] = rej\n            odict['Genes'] = [\";\".join(g) for g in genes]\n            res = pd.DataFrame(odict)\n            return res\n        return"}
{"prompt": "def populate_from_sequence(seq: list, r: ref(Edge), sr: state.StateRegister):\n    \"\"\" function that connect each other one sequence of MatchExpr. \"\"\"", "output": "\n    for e in seq:\n        sr.connect(e.expr1, e.expr2)\n\n", "solution": "\n    base_state = r\n    # we need to detect the last state of the sequence\n    idxlast = len(seq) - 1\n    idx = 0\n    for m in seq:\n        # alternatives are represented by builtin list\n        if isinstance(m, list):\n            # so recursively connect all states of each alternative sequences.\n            for item in m:\n                populate_from_sequence(item, r, sr)\n        elif isinstance(m, MatchExpr):\n            # from the current state, have we a existing edge for this event?\n            eX = r().get_next_edge(m)\n            if eX is None:\n                sX = None\n                if idx != idxlast:\n                    sX = state.State(sr)\n                    sX.matchDefault(base_state().s)\n                else:\n                    # last state of sequence return to the base\n                    sX = base_state().s\n                eX = Edge(sX)\n                r().next_edge[id(sX)] = eX\n                m.attach(r().s, sX, sr)\n            r = ref(eX)\n        idx += 1"}
{"prompt": "def mr_reader(job, input_stream, loads=core.loads):\n    \"\"\" Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects\n\n    Can be used as job.reader in luigi.hadoop.JobTask\n    \"\"\"", "output": "\n    for line in input_stream:\n        yield loads(line)\n\n", "solution": "\n    for line in input_stream:\n        yield loads(line),"}
{"prompt": "def add_from_raw_data(self, raw_data, data_type_id, name, description):\n        \"\"\"\n        Upload already serialized raw data as a new dataset.\n\n        Parameters\n        ----------\n        raw_data: bytes\n            Dataset contents to upload.\n        data_type_id : str\n            Serialization format of the raw data.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n                'ARFF'\n            See the azureml.DataTypeIds class for constants.\n        name : str\n            Name for the new dataset.\n        description : str\n            Description for the new dataset.\n\n        Returns\n        -------\n        SourceDataset\n            Dataset that was just created.\n            Use open(), read_as_binary(), read_as_text() or to_dataframe() on\n            the dataset object to get its contents as a stream, bytes, str or\n            pandas DataFrame.\n        \"\"\"", "output": "        name: str\n            Name of the dataset.\n        description: str\n            Description of the dataset.\n\n        Returns\n        -------\n        str\n            ID of the newly created dataset.\n        \"\"\"\n        # Parse the raw data based on the specified data type\n        if data_type_id == 'PlainText':\n            data = raw_data.decode('utf-8')\n        elif data_type_id == 'GenericCSV':\n            data = raw_data.decode('utf-8').split('\\n')\n        elif data_", "solution": "\n        _not_none('raw_data', raw_data)\n        _not_none_or_empty('data_type_id', data_type_id)\n        _not_none_or_empty('name', name)\n        _not_none_or_empty('description', description)\n\n        return self._upload(raw_data, data_type_id, name, description)"}
{"prompt": "def write_bit(self, value, bit_num, address):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write to.\n        :type bit_num: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"", "output": "\n        # Get the bit value at the specified address\n        bit_value = (self.memory[address] >> bit_num) & 1\n\n        # Set the bit value to the given value\n        self.memory[address] &= ~(1 << bit_num)\n\n        # Set the bit value in the specified address\n        self.memory[address] |= (value << bit_num)\n\n    def read_bit(self, bit_num, address):\n        \"\"\"Reads the value given to the bit in the address specified.\n\n        :param bit_num: The", "solution": "\n        bit_mask = get_bit_mask(bit_num)\n        old_byte = self.read(address)\n         # generate the new byte\n        if value:\n            new_byte = old_byte | bit_mask\n        else:\n            new_byte = old_byte & ~bit_mask\n        self.write(new_byte, address)"}
{"prompt": "def filter(self, *filters, **kwargs):\n        \"\"\"\n        Returns this Query instance with the query args combined with\n        existing set with AND.\n\n        kwargs are simply passed to a new Filter object and combined to any\n        other filters with AND.\n\n        By default, everything is combined using AND. If you provide\n        multiple filters in a single filter call, those are ANDed\n        together. If you provide multiple filters in multiple filter\n        calls, those are ANDed together.\n\n        If you want something different, use the F class which supports\n        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\n        filter once with the resulting Filter instance.\n        \"\"\"", "output": " for logical AND and ``|`` for logical OR.\n\n        Example:\n        >>> q = Query(name='Alice', age=25)\n        >>> q.filter(name='Bob', age=30)\n        <Query object with name='Bob' and age=30>\n        >>> q.filter(name='Alice')\n        <Query object with name='Alice' and age=25>\n        >>> q.filter(name='Bob', age=25)\n        <Query object with name='Bob' and age=25>\n        >>> q.filter(name='Charlie')\n", "solution": "\n        f = list(filters)\n\n        if kwargs:\n            f += [Filter(**kwargs)]\n\n        return self._clone(filters=f)"}
{"prompt": "def topological_nodes(self):\n        \"\"\"\n        Yield nodes in topological order.\n\n        Returns:\n            generator(DAGNode): node in topological order\n        \"\"\"", "output": "\n        # create a dictionary to store the in-degree of each node\n        in_degree = {node: 0 for node in self.nodes}\n\n        # count the in-degree of each node\n        for node in self.nodes:\n            for neighbor in node.neighbors:\n                in_degree[neighbor] += 1\n\n        # create a queue to store nodes with in-degree 0\n        queue = [node for node in in_degree if in_degree[node] == 0]\n\n        # traverse the graph in topological order\n        while queue:", "solution": "\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))"}
{"prompt": "def peng_power(snum):\n    r\"\"\"\n    Return engineering suffix and its floating point equivalent of a number.\n\n    :py:func:`peng.peng` lists the correspondence between suffix and floating\n    point exponent.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: named tuple in which the first item is the engineering suffix and\n            the second item is the floating point equivalent of the suffix\n            when the number is represented in engineering notation.\n\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_power\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_power(peng.peng(1235.6789E3, 3, False))\n        EngPower(suffix='M', exp=1000000.0)\n    \"\"\"", "output": "og\n     [[[eng\n      [[[eng\n       [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        [[[eng\n         [[[eng\n        ", "solution": "\n    suffix = \" \" if snum[-1].isdigit() else snum[-1]\n    return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])"}
{"prompt": "def get_next_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by NPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n        .. versionadded:: 0.15\n        \"\"\"", "output": "\n        if not self.protos:\n            return b''\n        return self.protos[0]\n\n    def get_next_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by the caller.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n       .. versionadded:: 0.15\n        \"\"\"\n        if not self.negotiations:\n            return b''\n        return self.negotiations[0]\n\n    def get_", "solution": "\n        _warn_npn()\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_next_proto_negotiated(self._ssl, data, data_len)\n\n        return _ffi.buffer(data[0], data_len[0])[:]"}
{"prompt": "def update_virtual_meta(self):\n        \"\"\"Will read back the virtual column etc, written by :func:`DataFrame.write_virtual_meta`. This will be done when opening a DataFrame.\"\"\"", "output": "\n        pass\n\n    def write_virtual_meta(self):\n        \"\"\"Will write the virtual column etc, written by :func:`DataFrame.update_virtual_meta`. This will be done when saving a DataFrame.\"\"\"\n        pass\n\n    def get_column_names(self):\n        \"\"\"Returns a list of column names for the DataFrame.\"\"\"\n        return list(self.columns)\n\n    def get_column_index(self, col_name):\n        \"\"\"Returns the index of the column with the given name in the DataFrame.\n\n        Parameters:\n        ", "solution": "\n        import astropy.units\n        try:\n            path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n            if os.path.exists(path):\n                meta_info = vaex.utils.read_json_or_yaml(path)\n                if 'virtual_columns' not in meta_info:\n                    return\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])\n                self.variables.update(meta_info[\"variables\"])\n                self.ucds.update(meta_info[\"ucds\"])\n                self.descriptions.update(meta_info[\"descriptions\"])\n                units = {key: astropy.units.Unit(value) for key, value in meta_info[\"units\"].items()}\n                self.units.update(units)\n        except:\n            logger.exception(\"non fatal error\")"}
{"prompt": "def delete(self, teamId):\n        \"\"\"Delete a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be deleted.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        if not isinstance(teamId, str):\n            raise TypeError(\"Team ID must be a string.\")\n        if not isinstance(self.teams, list):\n            raise TypeError(\"Teams must be a list.\")\n        if not self.teams:\n            return\n        for team in self.teams:\n            if team.id == teamId:\n                self.teams.remove(team)\n                return\n        raise ApiError(\"Team not found.\")\n\n", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '/' + teamId)"}
{"prompt": "def per_triangle(script, sidedim=0, textdim=1024, border=2, method=1):\n    \"\"\"Trivial Per-Triangle parameterization\n\n    \"\"\"", "output": "\n    if method == 1:\n        # Use a simple method to parameterize the script\n        # by creating a grid of triangles and filling in the\n        # triangles with the characters of the script.\n        chars = list(script)\n        rows = len(chars) // textdim\n        cols = len(chars) // rows\n        grid = [[''] * cols for _ in range(rows)]\n        for i, char in enumerate(chars):\n            row = i // cols\n            col = i % cols\n            grid[row][", "solution": "\n    filter_xml = ''.join([\n        '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n',\n        '    <Param name=\"sidedim\"',\n        'value=\"%d\"' % sidedim,\n        'description=\"Quads per line\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"',\n        '/>\\n',\n        '    <Param name=\"textdim\"',\n        'value=\"%d\"' % textdim,\n        'description=\"Texture Dimension (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Gives an indication on how big the texture is\"',\n        '/>\\n',\n        '    <Param name=\"border\"',\n        'value=\"%d\"' % border,\n        'description=\"Inter-Triangle border (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"',\n        '/>\\n',\n        '    <Param name=\"method\"',\n        'value=\"%d\"' % method,\n        'description=\"Method\"',\n        'enum_val0=\"Basic\"',\n        'enum_val1=\"Space-optimizing\"',\n        'enum_cardinality=\"2\"',\n        'type=\"RichEnum\"',\n        'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"'\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None"}
{"prompt": "def add_virtual_columns_cartesian_velocities_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", vlong=\"vlong\", vlat=\"vlat\", distance=None):\n        \"\"\"Concert velocities from a cartesian to a spherical coordinate system\n\n        TODO: errors\n\n        :param x: name of x column (input)\n        :param y:         y\n        :param z:         z\n        :param vx:       vx\n        :param vy:       vy\n        :param vz:       vz\n        :param vr: name of the column for the radial velocity in the r direction (output)\n        :param vlong: name of the column for the velocity component in the longitude direction  (output)\n        :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)\n        :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance\n        :return:\n        \"\"\"", "output": "param vx:        vx\n        :param vy:        vy\n        :param vz:        vz\n        :param vr:         vr\n        :param vlong:      vlong\n        :param vlat:       vlat\n        :param distance:   distance from the origin (input)\n        :return:\n        \"\"\"\n        if distance is None:\n            distance = self.distance\n        vx_spherical = vx * distance\n        vy_spherical = vy * distance\n        vz_spherical = vz * distance\n        ", "solution": "\n        # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm\n        if distance is None:\n            distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n        self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\".format(**locals()))\n        self.add_virtual_column(vlong, \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)\".format(**locals()))\n        self.add_virtual_column(vlat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )\".format(**locals()))"}
{"prompt": "def check_token(token):\n    \"\"\" Verify http header token authentification \"\"\"", "output": "\n    if token == \"my_token\":\n        return True\n    else:\n        return False\n\n", "solution": "\n    user = models.User.objects(api_key=token).first()\n    return user or None"}
{"prompt": "def validate_value(self, value):\n        \"\"\"\n        Validate new property value before setting it.\n\n        value -- New value\n        \"\"\"", "output": "\n        if not isinstance(value, int):\n            raise ValueError(\"New property value must be an integer.\")\n        if value < 0:\n            raise ValueError(\"New property value cannot be negative.\")\n        self.value = value\n\n", "solution": "\n        if 'readOnly' in self.metadata and self.metadata['readOnly']:\n            raise PropertyError('Read-only property')\n\n        try:\n            validate(value, self.metadata)\n        except ValidationError:\n            raise PropertyError('Invalid property value')"}
{"prompt": "def pack_metadata_statement(self, metadata, receiver='', iss='', lifetime=0,\n                                sign_alg=''):\n        \"\"\"\n        Given a MetadataStatement instance create a signed JWT.\n\n        :param metadata: Original metadata statement as a MetadataStatement\n            instance\n        :param receiver: Receiver (audience) of the JWT\n        :param iss: Issuer ID if different from default\n        :param lifetime: jWT signature life time\n        :param sign_alg: JWT signature algorithm\n        :return: A JWT\n        \"\"\"", "output": "\n        # Create payload with metadata\n        payload = {\n           'metadata': metadata.to_dict(),\n           'receiver': receiver,\n            'iss': iss,\n            'lifetime': lifetime,\n           'sign_alg': sign_alg\n        }\n\n        # Sign payload with private key\n        private_key = self.private_key\n        payload_str = json.dumps(payload)\n        signature = jwt.encode(payload_str, private_key, algorithm=self.sign_alg)\n\n        # Create JWT", "solution": "\n\n        return self.self_signer.sign(metadata, receiver=receiver, iss=iss,\n                                     lifetime=lifetime, sign_alg=sign_alg)"}
{"prompt": "def contact(self, id):\n        \"\"\"\n        Retrieve all details for a specific contact, including fields such as birthday and mood.\n\n        Args:\n            id (str): user identifier to lookup\n\n        Returns:\n            SkypeContact: resulting contact object\n        \"\"\"", "output": "\n        # code to retrieve contact details based on id\n        #...\n        return contact_object\n\n    def update(self, contact_id, updated_contact):\n        \"\"\"\n        Update the details of a specific contact.\n\n        Args:\n            contact_id (str): user identifier of contact to update\n            updated_contact (SkypeContact): updated contact object with updated details\n\n        Returns:\n            bool: True if update was successful, False otherwise\n        \"\"\"\n        # code to update contact details based on contact_id\n        #...\n        return True  # placeholder value,", "solution": "\n        try:\n            json = self.skype.conn(\"POST\", \"{0}/users/batch/profiles\".format(SkypeConnection.API_USER),\n                                   json={\"usernames\": [id]}, auth=SkypeConnection.Auth.SkypeToken).json()\n            contact = SkypeContact.fromRaw(self.skype, json[0])\n            if contact.id not in self.contactIds:\n                self.contactIds.append(contact.id)\n            return self.merge(contact)\n        except SkypeApiException as e:\n            if len(e.args) >= 2 and getattr(e.args[1], \"status_code\", None) == 403:\n                # Not a contact, so no permission to retrieve information.\n                return None\n            raise"}
{"prompt": "def str_repeat(x, repeats):\n    \"\"\"Duplicate each string in a column.\n\n    :param int repeats: number of times each string sample is to be duplicated.\n    :returns: an expression containing the duplicated strings\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.repeat(3)\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0        SomethingSomethingSomething\n    1  very prettyvery prettyvery pretty\n    2        is comingis comingis coming\n    3                          ourourour\n    4                       way.way.way.\n    \"\"\"", "output": " coming\n      3  our\n      4  way.\n    >>> df.text.str_repeat(2, repeats=3)\n      #  text  text  text\n      0  Something  Something\n      1  very pretty very pretty\n      2  is coming   is coming\n      3  our      our\n      4  way.     way.\n    \"\"\"\n    return df.text.str.repeat(x, repeats=repeats)\n\n", "solution": "\n    sl = _to_string_sequence(x).repeat(repeats)\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)"}
{"prompt": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"", "output": "\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if len(args) > 0 and len(kwargs) > 0:\n                warnings.warn(args[0] + \" \" + msg, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n", "solution": "\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap"}
{"prompt": "def get_last_activities(self, n):\n        \"\"\"Get all activity data for the last activity\n\n        Keyword arguments:\n        \"\"\"", "output": "\n        return self.activity_data[-n:]\n\n", "solution": "\n        filenames = self.get_activity_list().iloc[-n:].filename.tolist()\n        last_activities = [self.get_activity(f) for f in filenames]\n        return last_activities"}
{"prompt": "def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"", "output": "\n    # Update the user's role\n    user = get_user_by_id(user_id)\n    user.role = update_role\n    \n    # Add a new institute\n    add_institute(user)\n    \n    # Remove an existing institute\n    remove_institute(user)\n    \n    # Remove an admin\n    remove_admin(user)\n    \n    # Return the updated user object\n    return user\n\n", "solution": "\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)"}
{"prompt": "def _update_base(self):\n        \"\"\"update the base, including the URL for GitLab and the API endpoint.\n        \"\"\"", "output": "\n        url = \"https://gitlab.com/api/v4/projects/{}/base\"\n        endpoint = \"https://gitlab.com/api/v4/projects/{}/repositories\"\n        for project in self.projects:\n            project_url = url.format(project.id)\n            project_endpoint = endpoint.format(project.id)\n            project_data = project.to_dict()\n            project_response = requests.post(project_endpoint, json=project_data)\n            project.base", "solution": "\n        self.base = self._get_and_update_setting('SREGISTRY_GITLAB_BASE',\n                                                 \"https://gitlab.com/\")\n        self.api_base = \"%s/api/v4\" % self.base.strip('/')\n        self.artifacts = self._get_and_update_setting('SREGISTRY_GITLAB_FOLDER',\n                                                      'build')\n\n        self.job = self._get_and_update_setting('SREGISTRY_GITLAB_JOB', 'build')\n\n        bot.debug('      Api: %s' % self.api_base)\n        bot.debug('Artifacts: %s' % self.artifacts)\n        bot.debug('      Job: %s' % self.job)"}
{"prompt": "def send(self, message):\n        \"\"\" Send a message object\n\n            :type message: data.OutgoingMessage\n            :param message: The message to send\n            :rtype: data.OutgoingMessage\n            :returns: The sent message with populated fields\n            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)\n            :raises MessageSendError: generic errors\n            :raises AuthError: provider authentication failed\n            :raises LimitsError: sending limits exceeded\n            :raises CreditError: not enough money on the account\n        \"\"\"", "output": " enough credit\n            :raises ProviderError: an error occurred with the provider\n        \"\"\"\n        provider = message.provider\n        if provider not in self.router.providers:\n            raise ProviderError(f\"Unknown provider: {provider}\")\n\n        try:\n            message.validate()\n        except AssertionError as e:\n            raise MessageSendError(f\"Invalid message: {e}\")\n\n        if not message.is_valid():\n            raise AuthError(f\"Provider {provider} failed to validate message: {message}\")\n\n", "solution": "\n        # Which provider to use?\n        provider_name = self._default_provider  # default\n        if message.provider is not None:\n            assert message.provider in self._providers, \\\n                'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)\n            provider = self.get_provider(message.provider)\n        else:\n            # Apply routing\n            if message.routing_values is not None: # Use the default provider when no routing values are given\n                # Routing values are present\n                provider_name = self.router(message, *message.routing_values) or self._default_provider\n                assert provider_name in self._providers, \\\n                    'Routing function returned an unknown provider name: {}'.format(provider_name)\n            provider = self.get_provider(provider_name)\n\n        # Set message provider name\n        message.provider = provider.name\n\n        # Send the message using the provider\n        message = provider.send(message)\n\n        # Emit the send event\n        self.onSend(message)\n\n        # Finish\n        return message"}
{"prompt": "def string_arg(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"", "output": "\n    if isinstance(obj, str):\n        return obj\n    elif isinstance(obj, int):\n        return str(obj)\n    elif isinstance(obj, float):\n        return str(obj)\n    elif isinstance(obj, list):\n        return \", \".join(string_arg(ctx, i) for i in obj)\n    elif isinstance(obj, tuple):\n        return \", \".join(string_arg(ctx, i) for i in obj)\n    elif isinstance(obj, dict):\n        return \", \".", "solution": "\n    if hasattr(obj, 'compute'):\n        obj = next(obj.compute(ctx), False)\n    return to_string(obj)"}
{"prompt": "def on_kill(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"", "output": "\n        self.query = None\n        self.result = None\n        self.status = \"cancelled\"\n\n", "solution": "\n        if self.query_execution_id:\n            self.log.info('\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die')\n            self.log.info(\n                'Stopping Query with executionId - %s', self.query_execution_id\n            )\n            response = self.hook.stop_query(self.query_execution_id)\n            http_status_code = None\n            try:\n                http_status_code = response['ResponseMetadata']['HTTPStatusCode']\n            except Exception as ex:\n                self.log.error('Exception while cancelling query', ex)\n            finally:\n                if http_status_code is None or http_status_code != 200:\n                    self.log.error('Unable to request query cancel on athena. Exiting')\n                else:\n                    self.log.info(\n                        'Polling Athena for query with id %s to reach final state', self.query_execution_id\n                    )\n                    self.hook.poll_query_status(self.query_execution_id)"}
{"prompt": "def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"", "output": "\n    try:\n        from cryptography.fernet import Fernet\n    except ImportError:\n        raise AirflowException(\"Fernet encryption not available\")\n\n    try:\n        return Fernet.generate_key()\n    except ValueError:\n        raise AirflowException(\"Fernet key generation failed\")\n\n", "solution": "\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet"}
{"prompt": "def update_board(self, query_params=None):\n        \"\"\"\n        Update this board's information. Returns a new board.\n        \"\"\"", "output": "\n        # Update the board's information based on the query parameters\n        #...\n        return new_board\n\n", "solution": "\n        board_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_board(board_json)"}
{"prompt": "def compose_u3(theta1, phi1, lambda1, theta2, phi2, lambda2):\n        \"\"\"Return a triple theta, phi, lambda for the product.\n\n        u3(theta, phi, lambda)\n           = u3(theta1, phi1, lambda1).u3(theta2, phi2, lambda2)\n           = Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda2)\n           = u3(theta', phi1 + phi', lambda2 + lambda')\n\n        Return theta, phi, lambda.\n        \"\"\"", "output": " Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Rz(theta2)\n\n        Args:\n            theta1 (float): The first rotation angle in radians.\n            phi1 (float): The first rotation angle in radians.\n            lambda1 (float): The first rotation angle in radians.\n            theta2 (float): The second rotation angle in radians.\n            phi2 (float): The second rotation angle in radians.\n            lambda2 (float): The second rotation angle in radians.\n\n        Returns:", "solution": "\n        # Careful with the factor of two in yzy_to_zyz\n        thetap, phip, lambdap = Optimize1qGates.yzy_to_zyz((lambda1 + phi2), theta1, theta2)\n        (theta, phi, lamb) = (thetap, phi1 + phip, lambda2 + lambdap)\n\n        return (theta, phi, lamb)"}
{"prompt": "def filter_trim(self, start=1, end=1, filt=True):\n        \"\"\"\n        Remove points from the start and end of filter regions.\n        \n        Parameters\n        ----------\n        start, end : int\n            The number of points to remove from the start and end of\n            the specified filter.\n        filt : valid filter string or bool\n            Which filter to trim. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if filt:\n            self.active_filters = [f for f in self.active_filters if f!= \"\"]\n        if start > len(self.active_filters):\n            start = len(self.active_filters)\n        if end > len(self.active_filters):\n            end = len(self.active_filters)\n        self.active_filters = self.active_filters[start-1:end]\n        \n    def filter_plot(self, plot_data, filt=True):\n", "solution": "\n        params = locals()\n        del(params['self'])\n            \n        f = self.filt.grab_filt(filt)\n        nf = filters.trim(f, start, end)\n        \n        self.filt.add('trimmed_filter',\n                    nf,\n                    'Trimmed Filter ({:.0f} start, {:.0f} end)'.format(start, end),\n                    params, setn=self.filt.maxset + 1)"}
{"prompt": "def verify_reg_list(self, obj, object_type):\n        \"\"\"Verify a list of registers.\"\"\"", "output": "\n        if not isinstance(obj, list):\n            raise TypeError(f\"{object_type} must be a list\")\n        for reg in obj:\n            if not isinstance(reg, str):\n                raise TypeError(f\"{reg} must be a string\")\n        return True\n\n    def verify_memory_location(self, obj, object_type):\n        \"\"\"Verify a memory location.\"\"\"\n        if not isinstance(obj, int):\n            raise TypeError(f\"{object_type} must be an integer\")\n        if obj <", "solution": "\n        # We expect the object to be a bitlist or an idlist, we don't care.\n        # We will iterate it and ensure everything in it is declared as a bit,\n        # and throw if not.\n        for children in obj.children:\n            self.verify_reg(children, object_type)"}
{"prompt": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"", "output": "\n        if slope is None:\n            slope = self.slope\n        y_intercept = self.y_intercept\n        x_mean = self.x_mean\n        numerator = sum((tip - y_intercept) * (tip - y_intercept) for tip in self.tips)\n        denominator = sum((tip - x_mean) * (tip - x_mean) for tip in self.tips)\n        if denominator == 0:\n            return {'slope': slope, 'y_intercept': y_intercept", "solution": "\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model"}
{"prompt": "def to_keep(datetimes,\n            years=0, months=0, weeks=0, days=0,\n            hours=0, minutes=0, seconds=0,\n            firstweekday=SATURDAY, now=None):\n    \"\"\"\n    Return a set of datetimes that should be kept, out of ``datetimes``.\n\n    Keeps up to ``years``, ``months``, ``weeks``, ``days``,\n    ``hours``, ``minutes``, and ``seconds`` in the past.\n\n    When keeping weeks, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes after this point will always be\n    kept.\n    \"\"\"", "output": " to keep weeks that end on the last\n    weekday in the past. If a datetime falls on a weekend, it is kept\n    even if it falls on the last weekday in the past.\n\n    If a datetime falls on a weekday that is not in the range of\n    the first weekday (e.g. Monday, Tuesday, etc.), it is kept even if it\n    falls on the last weekday in the past.\n\n    If a datetime falls on a weekend that is not in the range of the\n    first weekday (e.g. Saturday, Sunday, etc.), it", "solution": "\n    datetimes = set(datetimes)\n    return (filters.Years.filter(datetimes, number=years, now=now) |\n            filters.Months.filter(datetimes, number=months, now=now) |\n            filters.Weeks.filter(datetimes, number=weeks,\n                                 firstweekday=firstweekday, now=now) |\n            filters.Days.filter(datetimes, number=days, now=now) |\n            filters.Hours.filter(datetimes, number=hours, now=now) |\n            filters.Minutes.filter(datetimes, number=minutes, now=now) |\n            filters.Seconds.filter(datetimes, number=seconds, now=now))"}
{"prompt": "def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"Creates Grant object with the given params\n\n        :param client_id: ID of the client\n        :param code:\n        :param request: OAuthlib request object\n        \"\"\"", "output": "\n        self.client_id = client_id\n        self.code = code\n        self.request = request\n        self.args = args\n        self.kwargs = kwargs\n\n    def __repr__(self):\n        return f\"Grant(client_id='{self.client_id}', code='{self.code}', request='{self.request}', args={self.args}, kwargs={self.kwargs})\"\n\n", "solution": "\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()"}
{"prompt": "def parse_hpo_diseases(hpo_lines):\n    \"\"\"Parse hpo disease phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with mim numbers as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        fields = line.strip().split('\\t')\n        if len(fields) == 2:\n            disease, pheno = fields\n            if disease not in diseases:\n                diseases[disease] = []\n            diseases[disease].append(pheno)\n    return diseases\n\n", "solution": "\n    diseases = {}\n    LOG.info(\"Parsing hpo diseases...\")\n    for index, line in enumerate(hpo_lines):\n        # First line is a header\n        if index == 0:\n            continue\n        # Skip empty lines\n        if not len(line) > 3:\n            continue\n        # Parse the info\n        disease_info = parse_hpo_disease(line)\n        # Skip the line if there where no info\n        if not disease_info:\n            continue\n        disease_nr = disease_info['disease_nr']\n        hgnc_symbol = disease_info['hgnc_symbol']\n        hpo_term = disease_info['hpo_term']\n        source = disease_info['source']\n        disease_id = \"{0}:{1}\".format(source, disease_nr)\n        \n        if disease_id not in diseases:\n            diseases[disease_id] = {\n                'disease_nr': disease_nr,\n                'source': source,\n                'hgnc_symbols': set(),\n                'hpo_terms': set(),\n            }\n\n        if hgnc_symbol:\n            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)\n        if hpo_term:\n            diseases[disease_id]['hpo_terms'].add(hpo_term)\n\n    LOG.info(\"Parsing done.\")\n    return diseases"}
{"prompt": "def _check_currency_format(self, format=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            format (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"", "output": "\n        if format is None:\n            format = self.currency_format\n        if format == 'USD':\n            return 'United States of Dollar'\n        elif format == 'EUR':\n            return 'Euro'\n        elif format == 'JPY':\n            return 'Japanese Yen'\n        elif format == 'CAD':\n            return 'Canadian Dollar'\n        else:\n            return 'Unknown currency format'\n\n    def _get_currency_format(self):\n        \"\"\"\n        Summary.\n\n        Args:\n            format (TYPE, optional): Description\n\n", "solution": "\n        defaults = self.settings['currency']['format']\n        if hasattr(format, '__call__'):\n            format = format()\n        if is_str(format) and re.match('%v', format):\n\n            # Create and return positive, negative and zero formats:\n            return {\n                'pos': format,\n                'neg': format.replace(\"-\", \"\").replace(\"%v\", \"-%v\"),\n                'zero': format\n            }\n        elif not format or not format['por'] or not re.match('%v',\n                                                             format['pos']):\n            self.settings['currency']['format'] = {\n                'pos': defaults,\n                'neg': defaults.replace(\"%v\", \"-%v\"),\n                'zero': defaults\n            }\n            return self.settings\n\n        return format"}
{"prompt": "def setMeterPassword(self, new_pwd, pwd=\"00000000\"):\n        \"\"\" Serial Call to set meter password.  USE WITH CAUTION.\n\n        Args:\n            new_pwd (str): 8 digit numeric password to set\n            pwd (str): Old 8 digit numeric password.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"", "output": "\n        if not isinstance(new_pwd, str) or not isinstance(pwd, str):\n            return False\n        if len(new_pwd)!= 8 or len(pwd)!= 8:\n            return False\n        if new_pwd == pwd:\n            return False\n        if not new_pwd.isdigit() or not pwd.isdigit():\n            return False\n        if int(new_pwd) > 9999 or int(pwd) > 9999:\n            return False\n        if new_pwd", "solution": "\n        result = False\n        self.setContext(\"setMeterPassword\")\n        try:\n            if len(new_pwd) != 8 or len(pwd) != 8:\n                self.writeCmdMsg(\"Passwords must be exactly eight characters.\")\n                self.setContext(\"\")\n                return result\n\n            if not self.request(False):\n                self.writeCmdMsg(\"Pre command read failed: check serial line.\")\n            else:\n                if not self.serialCmdPwdAuth(pwd):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_pwd = binascii.hexlify(new_pwd.zfill(8))\n                    req_str = \"015731023030323028\" + req_pwd + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setMeterPassword): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"", "output": "\n        message = \"\"\n        for arg in self.args:\n            if isinstance(arg, str):\n                message += arg\n            else:\n                message += str(arg)\n        return message\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of this LogRecord.\n        \"\"\"\n        return f\"[{self.level}] {self.getMessage()}\"\n\n", "solution": "\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)"}
{"prompt": "def add_observer(self, callback, ntype, sender):\n        \"\"\"Add an observer callback to this notification center.\n\n        The given callback will be called upon posting of notifications of\n        the given type/sender and will receive any additional arguments passed\n        to post_notification.\n\n        Parameters\n        ----------\n        callback : callable\n            The callable that will be called by :meth:`post_notification`\n            as ``callback(ntype, sender, *args, **kwargs)\n        ntype : hashable\n            The notification type. If None, all notifications from sender\n            will be posted.\n        sender : hashable\n            The notification sender. If None, all notifications of ntype\n            will be posted.\n        \"\"\"", "output": ", all notifications will be sent.\n        sender : hashable\n            The notification sender. If None, notifications will be sent to all\n            connected centers.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if ntype is None:\n            self.notifications.append((sender, callback))\n        else:\n            self.notifications.append((ntype, callback, sender))\n\n    def post_notification(self, ntype, sender, *args, **kwargs):\n        \"\"\"Post a notification of the given type/sender with the given arguments.", "solution": "\n        assert(callback != None)\n        self.registered_types.add(ntype)\n        self.registered_senders.add(sender)\n        self.observers.setdefault((ntype,sender), set()).add(callback)"}
{"prompt": "def cli_run():\n    \"\"\"docstring for argparse\"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='Runs a command line tool')\n    parser.add_argument('tool', help='the name of the tool to run')\n    parser.add_argument('args', nargs='*', help='arguments for the tool')\n    args = parser.parse_args()\n    # code to run the tool with the given arguments\n    return 0\n\n", "solution": "\n    parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow')\n    parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+')\n    parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda')\n    args = parser.parse_args()\n    main(args)"}
{"prompt": "def send_email(self, to, kind, **kwargs):\n        \"\"\" Sends an e-mail to the given address.\n\n        to: The address\n        kind: the ID for an e-mail kind; it should point to a subdirectory of\n            self.template_prefix containing subject.txt and message.html, which\n            are django templates for the subject and HTML message respectively.\n\n        context: a context for rendering the e-mail.\n\n        \"\"\"", "output": "\n        subject = kwargs.get('subject')\n        message = kwargs.get('message')\n        template_prefix = self.template_prefix\n\n        # Get the subject and message from the context\n        subject_path = template_prefix +'subject.txt'\n        message_path = template_prefix +'message.html'\n        subject_context = {'subject': subject,'message': message}\n        message_context = {'subject': subject,'message': message}\n        for key, value in subject_context.items():\n            subject_path", "solution": "\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)"}
{"prompt": "def list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"", "output": "\n    if len(lst) == 0:\n        return content\n    else:\n        result = \"\"\n        for i, item in enumerate(lst):\n            if isinstance(item, list):\n                result += \"\\n\" + \" \" * indent + \"(\" + list_to_str(item, content, indent + 2) + \")\\n\"\n            else:\n                result += \" \" * indent + str(item) + \"\\n\"\n        return result\n\n", "solution": "\n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content"}
{"prompt": "def add_btn_cancel(self,label_cancel):\n        \"\"\"\n        Adds a cancel button to let the user cancel whatever choice they were given.\n        \n        This widget can be triggered by setting the label ``label_cancel`` to a string.\n        \n        This widget will be positioned slightly below the main label and to the right\n        of the confirm button.\n        \"\"\"", "output": "\n        btn = tk.Button(self,text=\"Cancel\",command=lambda:self.config(text=\"\"))\n        btn.grid(row=1,column=0,sticky=\"w\")\n        self.grid_cancel.add(btn)\n        self.label_cancel = label_cancel\n    \n    def add_btn_yes(self,label_yes):\n        \"\"\"\n        Adds a yes button to let the user answer yes to whatever choice they were given.\n        \n        This widget can be triggered by setting the label ``label_yes", "solution": "\n        # Cancel Button\n        self.wbtn_cancel = button.Button(\"btn_cancel\",self,self.window,self.peng,\n                        pos=lambda sw,sh, bw,bh: (sw/2+4,sh/2-bh/2-bh*2),\n                        size=[0,0],\n                        label=label_cancel,\n                        borderstyle=self.borderstyle\n                        )\n        self.wbtn_cancel.size = lambda sw,sh: (self.wbtn_cancel._label.font_size*8,self.wbtn_cancel._label.font_size*2)\n        self.addWidget(self.wbtn_cancel)\n        \n        def f():\n            self.doAction(\"cancel\")\n            self.exitDialog()\n        self.wbtn_cancel.addAction(\"click\",f)"}
{"prompt": "def _link_field_to_dict(field):\n        \"\"\" Utility for ripping apart github's Link header field.\n        It's kind of ugly.\n        \"\"\"", "output": "\n        key, value = field.split(':', 1)\n        return {key.strip(): value.strip()}\n\n    return [_link_field_to_dict(field) for field in fields]\n\n", "solution": "\n\n        if not field:\n            return dict()\n\n        return dict([\n            (\n                part.split('; ')[1][5:-1],\n                part.split('; ')[0][1:-1],\n            ) for part in field.split(', ')\n        ])"}
{"prompt": "def _get_protobuf_kind(kind):\n    \"\"\"Converts py_zipkin's Kind to Protobuf's Kind.\n\n    :param kind: py_zipkin's Kind.\n    :type kind: py_zipkin.Kind\n    :return: correcponding protobuf's kind value.\n    :rtype: zipkin_pb2.Span.Kind\n    \"\"\"", "output": "\n    if kind == py_zipkin.Kind.DOCUMENT:\n        return zipkin_pb2.Span.Kind.DOCUMENT\n    elif kind == py_zipkin.Kind.NOTE:\n        return zipkin_pb2.Span.Kind.NOTE\n    elif kind == py_zipkin.Kind.ACTION:\n        return zipkin_pb2.Span.Kind.ACTION\n    elif kind == py_zipkin.Kind.REPLY:\n        return zipkin_pb2.Span.Kind.REPLY\n", "solution": "\n    if kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    return None"}
{"prompt": "def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"", "output": "\n    if node.tag == \"do\":\n        for child in node.children:\n            _assert_recur_is_tail(child)\n    elif node.tag == \"if\":\n        for child in node.children:\n            if child.tag == \"then\":\n                _assert_recur_is_tail(child)\n            elif child.tag == \"else\":\n                _assert_recur_is_tail(child)\n    elif node.tag == \"then\":\n        for child in node.children:\n            if child.tag", "solution": "\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)"}
{"prompt": "def remove_namespaces(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"", "output": "\n    for elem in root.iter():\n        elem.tag = etree.QName(elem).localname\n\n", "solution": "\n    for elem in root.getiterator():\n        if not hasattr(elem.tag, 'find'):\n            continue\n\n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i + 1:]\n\n    objectify.deannotate(root, cleanup_namespaces=True)"}
{"prompt": "def _block_tuple(iterator, dtypes, bsize=-1):\n    \"\"\"Pack rdd of tuples as tuples of arrays or scipy.sparse matrices.\"\"\"", "output": "\n    if bsize == -1:\n        bsize = iterator.getNumPartitions()\n    blocks = iterator.getNumBlocks()\n    size = blocks * dtypes[0].itemsize\n    if size > 2**31 - 1:\n        raise ValueError(\"Tuple size exceeds maximum integer value\")\n    if bsize > blocks:\n        raise ValueError(\"Number of blocks exceeds number of partitions\")\n    if size > 2**31 - 1:\n        raise ValueError(\"Tuple size exceeds maximum integer value\")\n    if bsize == 1:\n        return iterator", "solution": "\n    i = 0\n    blocked_tuple = None\n    for tuple_i in iterator:\n        if blocked_tuple is None:\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n\n        if (bsize > 0) and (i >= bsize):\n            yield tuple(_pack_accumulated(x, dtype)\n                        for x, dtype in zip(blocked_tuple, dtypes))\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n            i = 0\n\n        for x_j, x in zip(tuple_i, blocked_tuple):\n            x.append(x_j)\n        i += 1\n    if i > 0:\n        yield tuple(_pack_accumulated(x, dtype)\n                    for x, dtype in zip(blocked_tuple, dtypes))"}
{"prompt": "def ddos_custom_policies(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-11-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_11_01.operations.DdosCustomPoliciesOperations>`\n           * 2018-12-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_12_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n        \"\"\"", "output": "-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n\n        Returns:\n            None\n        \"\"\"\n        # Your code here\n        pass\n\n    def get_custom_policy(self, version: str) -> DdosCustomPoliciesOperations:\n        \"\"\"\n        Returns a reference to the custom policy for the specified version.\n\n        Args:\n            version (str): The version of the policy to retrieve.\n\n        Returns:\n", "solution": "\n        api_version = self._get_api_version('ddos_custom_policies')\n        if api_version == '2018-11-01':\n            from .v2018_11_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2018-12-01':\n            from .v2018_12_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2019-02-01':\n            from .v2019_02_01.operations import DdosCustomPoliciesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def update(self):\n        \"\"\"Update repository from its remote.\n\n        Calling this method, the repository will be synchronized with\n        the remote repository using 'fetch' command for 'heads' refs.\n        Any commit stored in the local copy will be removed; refs\n        will be overwritten.\n\n        :raises RepositoryError: when an error occurs updating the\n            repository\n        \"\"\"", "output": "\n        try:\n            self.fetch()\n            self.checkout()\n        except RepositoryError:\n            raise\n\n    def fetch(self):\n        \"\"\"Fetch changes from the remote repository.\n\n        :raises RepositoryError: when an error occurs fetching changes\n        \"\"\"\n        try:\n            subprocess.check_call(['git', 'fetch', '--tags'])\n        except subprocess.CalledProcessError:\n            raise RepositoryError('Failed to fetch changes from remote repository')\n\n    def checkout(self):\n        \"\"\"Check", "solution": "\n        cmd_update = ['git', 'fetch', 'origin', '+refs/heads/*:refs/heads/*', '--prune']\n        self._exec(cmd_update, cwd=self.dirpath, env=self.gitenv)\n\n        logger.debug(\"Git %s repository updated into %s\",\n                     self.uri, self.dirpath)"}
{"prompt": "def setZeroResettableKWH(self, password=\"00000000\"):\n        \"\"\" Serial call to zero resettable kWh registers.\n\n        Args:\n            password (str): Optional password.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"", "output": "\n        # Code to set zero resettable kWh registers\n        #...\n        return True  # or False if ACK is required\n\n", "solution": "\n        result = False\n        self.setContext(\"setZeroResettableKWH\")\n        try:\n            if not self.requestA():\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_str = \"0157310230304433282903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success: 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def add_heart_failure_handler(self, handler):\n        \"\"\"add a new handler for heart failure\"\"\"", "output": "\n        self.failure_handlers.append(handler)\n\n    def remove_heart_failure_handler(self, handler):\n        \"\"\"remove a handler for heart failure\"\"\"\n        if handler in self.failure_handlers:\n            self.failure_handlers.remove(handler)\n\n    def fire_heart_failure_handler(self):\n        \"\"\"fire a heart failure handler\"\"\"\n        for handler in self.failure_handlers:\n            handler()\n\n", "solution": "\n        self.log.debug(\"heartbeat::new heart failure handler: %s\", handler)\n        self._failure_handlers.add(handler)"}
{"prompt": "def like_hashtag(self, hashtag, amount=None):\n    \"\"\" Likes last medias from hashtag \"\"\"", "output": "\n    if amount is None:\n        amount = len(hashtag)\n    for i in range(amount, 0, -1):\n        print(f\"Liked last {i} medias from {hashtag}\")\n\n", "solution": "\n    self.logger.info(\"Going to like media with hashtag #%s.\" % hashtag)\n    medias = self.get_total_hashtag_medias(hashtag, amount)\n    return self.like_medias(medias)"}
{"prompt": "def __coord_fft_hz(n, sr=22050, **_kwargs):\n    \"\"\"Get the frequencies for FFT bins\"\"\"", "output": "\n    return np.fft.fftfreq(n, 1/sr)\n\n", "solution": "\n    n_fft = 2 * (n - 1)\n    # The following code centers the FFT bins at their frequencies\n    # and clips to the non-negative frequency range [0, nyquist]\n    basis = core.fft_frequencies(sr=sr, n_fft=n_fft)\n    fmax = basis[-1]\n    basis -= 0.5 * (basis[1] - basis[0])\n    basis = np.append(np.maximum(0, basis), [fmax])\n    return basis"}
{"prompt": "def main(ctx, log_level, keeper_hostname, username, password):\n    \"\"\"ltd is a command-line client for LSST the Docs.\n\n    Use ltd to upload new site builds, and to work with the LTD Keeper API.\n    \"\"\"", "output": "\n    import ltd\n    import json\n\n    # Set up the logger\n    logger = ltd.setup_logging(log_level, keeper_hostname, username, password)\n\n    # Get the list of existing sites\n    existing_sites = ltd.get_existing_sites()\n\n    # Create a new site\n    site = ltd.create_new_site(existing_sites)\n\n    # Upload the site to the LTD Keeper API\n    ltd.upload_site(site)\n\n    # Get the list of existing builds\n    existing_builds =", "solution": "\n    ch = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    ch.setFormatter(formatter)\n\n    logger = logging.getLogger('ltdconveyor')\n    logger.addHandler(ch)\n    logger.setLevel(log_level.upper())\n\n    # Subcommands should use the click.pass_obj decorator to get this\n    # ctx.obj object as the first argument.\n    ctx.obj = {\n        'keeper_hostname': keeper_hostname,\n        'username': username,\n        'password': password,\n        'token': None\n    }"}
{"prompt": "def _fetch_from_archive(self, method, args):\n        \"\"\"Fetch data from the archive\n\n        :param method: the name of the command to execute\n        :param args: the arguments required by the command\n        \"\"\"", "output": "\n        if method == 'get':\n            # code to retrieve data from the archive\n            pass\n        elif method == 'update':\n            # code to update data in the archive\n            pass\n        elif method == 'delete':\n            # code to delete data from the archive\n            pass\n        else:\n            raise ValueError('Invalid method')\n\n    def execute(self):\n        \"\"\"Execute the commands in the archive\n\n        :return: a list of the results of each command\n        \"\"\"\n        results = []\n        for command in self.commands:\n            ", "solution": "\n        if not self.archive:\n            raise ArchiveError(cause=\"Archive not provided\")\n\n        data = self.archive.retrieve(method, args, None)\n\n        if isinstance(data, nntplib.NNTPTemporaryError):\n            raise data\n\n        return data"}
{"prompt": "def attach_dynamic_tree_items(tree_alias, src_tree_items):\n        \"\"\"Attaches dynamic sitetrees items registered with `register_dynamic_trees()`\n        to an initial (source) items list.\n\n        :param str|unicode tree_alias:\n        :param list src_tree_items:\n        :rtype: list\n        \"\"\"", "output": "\n        if not src_tree_items:\n            return []\n\n        tree_items = []\n        for item in src_tree_items:\n            if isinstance(item, dict):\n                tree_items.extend(attach_dynamic_tree_items(tree_alias, item.values()))\n            else:\n                tree_items.append(item)\n\n        return [tree_items]\n\n    def register_dynamic_trees(tree_alias):\n        \"\"\"Registers dynamic sitetrees with `attach_dynamic_tree_items()`.\n\n", "solution": "\n        if not _DYNAMIC_TREES:\n            return src_tree_items\n\n        # This guarantees that a dynamic source stays intact,\n        # no matter how dynamic sitetrees are attached.\n        trees = deepcopy(_DYNAMIC_TREES)\n\n        items = []\n        if not src_tree_items:\n            if _IDX_ORPHAN_TREES in trees and tree_alias in trees[_IDX_ORPHAN_TREES]:\n                for tree in trees[_IDX_ORPHAN_TREES][tree_alias]:\n                    items.extend(tree.dynamic_items)\n        else:\n\n            # TODO Seems to be underoptimized %)\n\n            # Tree item attachment by alias.\n            for static_item in list(src_tree_items):\n                items.append(static_item)\n                if not static_item.alias:\n                    continue\n\n                idx = _IDX_TPL % (tree_alias, static_item.alias)\n                if idx not in trees:\n                    continue\n\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    for dyn_item in tree.dynamic_items:\n                        if dyn_item.parent is None:\n                            dyn_item.parent = static_item\n                        # Unique IDs are required for the same trees attached\n                        # to different parents.\n                        dyn_item.id = generate_id_for(dyn_item)\n                        items.append(dyn_item)\n\n            # Tree root attachment.\n            idx = _IDX_TPL % (tree_alias, None)\n            if idx in _DYNAMIC_TREES:\n                trees = deepcopy(_DYNAMIC_TREES)\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    items.extend(tree.dynamic_items)\n\n        return items"}
{"prompt": "def snip_print(str,width = 75,print_full = 0,header = ''):\n    \"\"\"Print a string snipping the midsection to fit in width.\n\n    print_full: mode control:\n      - 0: only snip long strings\n      - 1: send to page() directly.\n      - 2: snip long strings and ask for full length viewing with page()\n    Return 1 if snipping was necessary, 0 otherwise.\"\"\"", "output": "\n    if print_full == 0:\n        return 1\n    elif print_full == 1:\n        print(header)\n        print(str[:width])\n        return 1\n    elif print_full == 2:\n        print(header)\n        print(str[:width])\n        print(str[width:])\n        return 1\n    else:\n        raise ValueError(\"print_full must be 0, 1, or 2.\")\n\n", "solution": "\n\n    if print_full == 1:\n        page(header+str)\n        return 0\n\n    print header,\n    if len(str) < width:\n        print str\n        snip = 0\n    else:\n        whalf = int((width -5)/2)\n        print str[:whalf] + ' <...> ' + str[-whalf:]\n        snip = 1\n    if snip and print_full == 2:\n        if raw_input(header+' Snipped. View (y/n)? [N]').lower() == 'y':\n            page(str)\n    return snip"}
{"prompt": "def _choi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Choi representation to the Chi representation.\"\"\"", "output": "\n    # Get the Choi matrix\n    choi_matrix = data[:input_dim**2]\n    choi_matrix = np.reshape(choi_matrix, (input_dim, input_dim))\n    \n    # Get the eigenvalues and eigenvectors of the Choi matrix\n    eigenvalues, eigenvectors = np.linalg.eig(choi_matrix)\n    \n    # Sort the eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::", "solution": "\n    num_qubits = int(np.log2(input_dim))\n    return _transform_to_pauli(data, num_qubits)"}
{"prompt": "def rst_msg(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg()\"\"\"", "output": "\n        # Convert ReStructuredText to plain text\n        plain_text = text.getvalue()\n        plain_text = re.sub(r'\\n\\s*\\n', '\\n\\n', plain_text)\n        plain_text = re.sub(r'\\n\\s*\\n', '\\n\\n', plain_text)\n        plain_text = re.sub(r'\\n\\s*\\n', '\\n\\n', plain_text)\n        plain_text = re.sub(r'\\n\\s", "solution": "\n        text = Mformat.rst_text(text,\n                                'plain' == self.debugger.settings['highlight'],\n                                self.debugger.settings['width'])\n        return self.msg(text)"}
{"prompt": "def measure_topology(fbasename=None, log=None, ml_version=ml_version):\n    \"\"\"Measures mesh topology\n\n    Args:\n        fbasename (str): input filename.\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            vert_num (int): number of vertices\n            edge_num (int): number of edges\n            face_num (int): number of faces\n            unref_vert_num (int): number or unreferenced vertices\n            boundry_edge_num (int): number of boundary edges\n            part_num (int): number of parts (components) in the mesh.\n            manifold (bool): True if mesh is two-manifold, otherwise false.\n            non_manifold_edge (int): number of non_manifold edges.\n            non_manifold_vert (int): number of non-manifold verices\n            genus (int or str): genus of the mesh, either a number or\n                'undefined' if the mesh is non-manifold.\n            holes (int or str): number of holes in the mesh, either a number\n                or 'undefined' if the mesh is non-manifold.\n\n    \"\"\"", "output": "_num (int): number of boundary edges\n            unref_edge_num (int): number or unreferenced edges\n            boundry_edge_num (int): number of boundary edges\n            unref_face_num (int): number or unreferenced faces\n            boundry_face_num (int): number of boundary faces\n            unref_unref_vert_num (int): number or unreferenced vertices\n            boundry_unref_vert_num (int): number of boundary vertices\n            unref_unref_edge_num (", "solution": "\n    ml_script1_file = 'TEMP3D_measure_topology.mlx'\n    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)\n    compute.measure_topology(ml_script1)\n    ml_script1.save_to_file(ml_script1_file)\n    ml_script1.run_script(log=log, script_file=ml_script1_file)\n    topology = ml_script1.topology\n    return topology"}
{"prompt": "def import_submodule(mod, subname, fullname):\n    \"\"\"m = import_submodule(mod, subname, fullname)\"\"\"", "output": "\n    from importlib import import_module\n    return getattr(import_module(mod), subname)\n\n", "solution": "\n    # Require:\n    # if mod == None: subname == fullname\n    # else: mod.__name__ + \".\" + subname == fullname\n\n    global found_now\n    if fullname in found_now and fullname in sys.modules:\n        m = sys.modules[fullname]\n    else:\n        print 'Reloading', fullname\n        found_now[fullname] = 1\n        oldm = sys.modules.get(fullname, None)\n\n        if mod is None:\n            path = None\n        elif hasattr(mod, '__path__'):\n            path = mod.__path__\n        else:\n            return None\n\n        try:\n            # This appears to be necessary on Python 3, because imp.find_module()\n            # tries to import standard libraries (like io) itself, and we don't\n            # want them to be processed by our deep_import_hook.\n            with replace_import_hook(original_import):\n                fp, filename, stuff = imp.find_module(subname, path)\n        except ImportError:\n            return None\n\n        try:\n            m = imp.load_module(fullname, fp, filename, stuff)\n        except:\n            # load_module probably removed name from modules because of\n            # the error.  Put back the original module object.\n            if oldm:\n                sys.modules[fullname] = oldm\n            raise\n        finally:\n            if fp: fp.close()\n\n        add_submodule(mod, m, fullname, subname)\n\n    return m"}
{"prompt": "def velocity_from_bundle(self, bundle):\n        \"\"\"[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value.\"\"\"", "output": "\n        return bundle.velocity\n\n    def coefficient_bundle(self):\n        \"\"\"[DEPRECATED] Return coefficient bundle, given the `velocity_from_bundle()` return value.\"\"\"\n        return self.velocity_from_bundle\n\n    def coefficient(self):\n        \"\"\"[DEPRECATED] Return coefficient, given the `velocity_from_bundle()` return value.\"\"\"\n        return self.velocity_from_bundle().coefficient\n\n    def velocity_from_force(self, force):\n        \"\"\"[DEPRECATED] Return", "solution": "\n\n        coefficients, days_per_set, T, twot1 = bundle\n        coefficient_count = coefficients.shape[2]\n\n        # Chebyshev derivative:\n\n        dT = np.empty_like(T)\n        dT[0] = 0.0\n        dT[1] = 1.0\n        dT[2] = twot1 + twot1\n        for i in range(3, coefficient_count):\n            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n        dT *= 2.0\n        dT /= days_per_set\n\n        return (dT.T * coefficients).sum(axis=2)"}
{"prompt": "def update(self):\n        \"\"\"Update cameras and motion settings with latest from API.\"\"\"", "output": "\n        # Code to update cameras and motion settings goes here\n        pass\n\ndef get_latest_camera_motion(camera_ids: List[str], motion_settings: List[str]) -> str:\n    \"\"\"\n    Returns the latest camera ID and motion setting from the given lists.\n\n    Args:\n        camera_ids (List[str]): A list of camera IDs.\n        motion_settings (List[str]): A list of motion settings.\n\n    Returns:\n        str: A string containing the latest camera ID and motion setting.\n    \"\"\"\n    latest_camera", "solution": "\n        cameras = self._api.camera_list()\n        self._cameras_by_id = {v.camera_id: v for i, v in enumerate(cameras)}\n\n        motion_settings = []\n        for camera_id in self._cameras_by_id.keys():\n            motion_setting = self._api.camera_event_motion_enum(camera_id)\n            motion_settings.append(motion_setting)\n\n        self._motion_settings_by_id = {\n            v.camera_id: v for i, v in enumerate(motion_settings)}"}
{"prompt": "def get_refs(self, location):\n        \"\"\"Return map of named refs (branches or tags) to commit hashes.\"\"\"", "output": "\n        refs = {}\n        for ref in self.repo.refs:\n            if ref.location == location:\n                refs[ref.name] = ref.hexsha\n        return refs\n\n    def get_commits(self, location):\n        \"\"\"Return list of commit hashes for named refs at specified location.\"\"\"\n        commits = []\n        for ref in self.repo.refs:\n            if ref.location == location:\n                commits.append(ref.hexsha)\n        return commits\n\n    def get_merges(", "solution": "\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs/remotes/'):\n                ref_name = ref[len('refs/remotes/'):]\n            elif ref.startswith('refs/heads/'):\n                ref_name = ref[len('refs/heads/'):]\n            elif ref.startswith('refs/tags/'):\n                ref_name = ref[len('refs/tags/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv"}
{"prompt": "def main():\n    \"\"\"The main function. Instantiates a GameState object and then\n    enters a REPL-like main loop, waiting for input, updating the state\n    based on the input, then outputting the new state.\"\"\"", "output": "\n    game_state = GameState()\n    while True:\n        user_input = input(\"Enter a command (or 'quit' to exit): \")\n        if user_input == \"quit\":\n            break\n        game_state.update(user_input)\n        print(game_state)\n\n", "solution": "\n\n    state = GameState()\n    print(state)\n    while state.running:\n        input = get_single_char()\n\n        state, should_advance = state.handle_input(input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)"}
{"prompt": "def delete_policy_name(self, name):\n        \"\"\"**Description**\n            Delete the policy with the given name.\n\n        **Arguments**\n            - name: the name of the policy to delete\n\n        **Success Return Value**\n            The JSON object representing the now-deleted policy.\n\n        **Example**\n            `examples/delete_policy.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_policy.py>`_\n\n        \"\"\"", "output": "\n        path = f\"{self.api_endpoint}/policies/{name}\"\n        response = self.session.delete(path)\n        return response.json()\n\n    def list_policies(self):\n        \"\"\"**Description**\n            List all policies.\n\n        **Arguments**\n            None\n\n        **Success Return Value**\n            A list of dictionaries, where each dictionary represents a policy and contains the following keys:\n            - name: the name of the policy\n            - description: the description of the policy\n            - created_at: the", "solution": "\n        res = requests.get(self.url + '/api/policies', headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        # Find the policy with the given name and delete it\n        for policy in res.json()[\"policies\"]:\n            if policy[\"name\"] == name:\n                return self.delete_policy_id(policy[\"id\"])\n\n        return [False, \"No policy with name {}\".format(name)]"}
{"prompt": "def share_vm_image(self, vm_image_name, permission):\n        \"\"\"\n        Share an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to share\n        permission:\n            The sharing permission: public, msdn, or private\n        \"\"\"", "output": "\n        # Code to share the virtual machine image with the required permission\n        #...\n        pass\n\n    def unshare_vm_image(self, vm_image_name, permission):\n        \"\"\"\n        Unshare an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to unshare\n        permission:\n            The sharing permission: public, msdn, or private\n        \"\"\"\n", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )"}
{"prompt": "def get_stats(self, username='', password='', organization='llnl', force=True):\n        \"\"\"\n        Retrieves the traffic for the users of the given organization.\n        Requires organization admin credentials token to access the data.\n        \"\"\"", "output": "\n        if not username or not password:\n            raise ValueError(\"Username and password cannot be empty.\")\n        if not organization:\n            raise ValueError(\"Organization name cannot be empty.\")\n        \n        # Retrieve traffic data for the given username and password\n        # and return it as a dictionary\n        return {'username': username, 'password': password, 'traffic': traffic_data}\n    \n    def get_stats_for_organization(self, organization='llnl'):\n        \"\"\"\n        Retrieves the traffic data for the given organization.\n        ", "solution": "\n        date = str(datetime.date.today())\n        referrers_file_path =  ('../github_stats_output/referrers.csv')\n        views_file_path =  ('../github_stats_output/views.csv')\n        clones_file_path =  ('../github_stats_output/clones.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            my_github.get_traffic()\n            views_row_count = my_github.check_data_redundancy(file_path=views_file_path,\n                dict_to_check=self.views)\n            clones_row_count = my_github.check_data_redundancy(file_path=clones_file_path,\n                dict_to_check=self.clones)\n            my_github.write_to_file(referrers_file_path=referrers_file_path,\n                views_file_path=views_file_path,\n                clones_file_path=clones_file_path,\n                views_row_count=views_row_count,\n                clones_row_count=clones_row_count)\n            my_github.write_json(dict_to_write=self.referrers_json,\n                path_ending_type='traffic_popular_referrers')\n            my_github.write_json(dict_to_write=self.views_json,\n                path_ending_type='traffic_views')\n            my_github.write_json(dict_to_write=self.clones_json,\n                path_ending_type='traffic_clones')\n            my_github.write_json(dict_to_write=self.releases_json,\n                path_ending_type='releases')\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')"}
{"prompt": "def save_setting(self, setting_name, value):\n        \"\"\"Saves the setting value into the database.\"\"\"", "output": "\n        # Code to save the setting value into the database\n        pass\n    \n    def get_setting(self, setting_name):\n        \"\"\"Returns the value of the setting with the given name.\"\"\"\n        # Code to retrieve the value of the setting with the given name\n        pass\n    \n    def reset_setting(self, setting_name):\n        \"\"\"Resets the value of the setting with the given name to its default value.\"\"\"\n        # Code to reset the value of the setting with the given name to its default value\n        pass\n    \n    def validate_setting", "solution": "\n        setting = self.get_setting(setting_name)\n        if setting is None:\n            setting = models.DashboardWidgetSettings.objects.create(\n                widget_name=self.get_name(),\n                setting_name=setting_name,\n                value=value)\n        setting.value = value\n        setting.save()\n        return setting"}
{"prompt": "def get_extended_key_usage_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the extendedKeyUsage\n    extension.\n    \"\"\"", "output": "\n    extended_key_usage = certificate.get_extension_for_oid(ExtensionOID.EXTENDED_KEY_USAGE)\n    return extended_key_usage.value\n\n", "solution": "\n    try:\n        return certificate.extensions.get_extension_for_oid(\n            x509.oid.ExtensionOID.EXTENDED_KEY_USAGE\n        ).value\n    except x509.ExtensionNotFound:\n        return None"}
{"prompt": "def join_html_attrs(attrs):\n    \"\"\"Joins the map structure into HTML attributes.\n\n    The return value is a 2-tuple ``(template, ordered_values)``. It should be\n    passed into :class:`markupsafe.Markup` to prevent XSS attacked.\n\n    e.g.::\n\n        >>> join_html_attrs({'href': '/', 'data-active': 'true'})\n        ('data-active=\"{0}\" href=\"{1}\"', ['true', '/'])\n    \"\"\"", "output": "\n    template =''.join(f'{k}=\"{v}\"' for k, v in attrs.items())\n    ordered_values = sorted(attrs.keys())\n    return template, ordered_values\n\n", "solution": "\n    attrs = collections.OrderedDict(freeze_dict(attrs or {}))\n    template = ' '.join('%s=\"{%d}\"' % (k, i) for i, k in enumerate(attrs))\n    return template, list(attrs.values())"}
{"prompt": "def restore_sys_module_state(self):\n        \"\"\"Restore the state of the sys module.\"\"\"", "output": "\n        # Code to restore the state of the sys module goes here\n        pass\n\ndef restore_sys_module_state_in_automation(sys_module: SystemModule, code_to_restore: str) -> None:\n    \"\"\"\n    Restores the state of the sys module after automating a task.\n\n    Args:\n        sys_module (SystemModule): The sys module to restore the state of.\n        code_to_restore (str): The code to restore the state of the sys module from.\n\n    Returns:\n        None\n    \"\"\"\n", "solution": "\n        try:\n            for k, v in self._orig_sys_module_state.iteritems():\n                setattr(sys, k, v)\n        except AttributeError:\n            pass\n        # Reset what what done in self.init_sys_modules\n        if self._orig_sys_modules_main_mod is not None:\n            sys.modules[self._orig_sys_modules_main_name] = self._orig_sys_modules_main_mod"}
{"prompt": "def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    \"\"\"Recover from quantized flow.\n\n    Args:\n        dx (ndarray): Quantized dx.\n        dy (ndarray): Quantized dy.\n        max_val (float): Maximum value used when quantizing.\n        denorm (bool): Whether to multiply flow values with width/height.\n\n    Returns:\n        ndarray: Dequantized flow.\n    \"\"\"", "output": "\n    flow = np.sqrt(dx**2 + dy**2)\n    if denorm:\n        flow *= max_val\n    return flow\n\n", "solution": "\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow"}
{"prompt": "def addCases(self, tupesValStmnts):\n        \"\"\"\n        Add multiple case statements from iterable of tuleles\n        (caseVal, statements)\n        \"\"\"", "output": "\n        for caseVal, statements in tupesValStmnts:\n            self.addCase(caseVal, statements)\n\n    def addCase(self, caseVal, statements):\n        \"\"\"\n        Add a single case statement to the case manager\n        \"\"\"\n        self.cases[caseVal] = statements\n\n    def getStatements(self, caseVal):\n        \"\"\"\n        Returns a list of statements for the given case value\n        \"\"\"\n        return self.cases.get(caseVal, [])\n\n    def getStatementsByIndex(self, index):\n", "solution": "\n        s = self\n        for val, statements in tupesValStmnts:\n            s = s.Case(val, statements)\n        return s"}
{"prompt": "def create_endpoint(port=None, service_name=None, host=None, use_defaults=True):\n    \"\"\"Creates a new Endpoint object.\n\n    :param port: TCP/UDP port. Defaults to 0.\n    :type port: int\n    :param service_name: service name as a str. Defaults to 'unknown'.\n    :type service_name: str\n    :param host: ipv4 or ipv6 address of the host. Defaults to the\n    current host ip.\n    :type host: str\n    :param use_defaults: whether to use defaults.\n    :type use_defaults: bool\n    :returns: zipkin Endpoint object\n    \"\"\"", "output": ":param use_defaults: If True, the default values for port and host are used.\n    If False, the values are set to None.\n    :type use_defaults: bool\n    :return: A new Endpoint object.\n    :rtype: Endpoint\n    \"\"\"\n    if port is None:\n        port = 0\n    if host is None:\n        host = socket.gethostbyname(socket.gethostname())\n    if use_defaults:\n        return Endpoint(port=port, host=host, service_name=service_", "solution": "\n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            try:\n                host = socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n                host = '127.0.0.1'\n\n    ipv4 = None\n    ipv6 = None\n\n    if host:\n        # Check ipv4 or ipv6.\n        try:\n            socket.inet_pton(socket.AF_INET, host)\n            ipv4 = host\n        except socket.error:\n            # If it's not an ipv4 address, maybe it's ipv6.\n            try:\n                socket.inet_pton(socket.AF_INET6, host)\n                ipv6 = host\n            except socket.error:\n                # If it's neither ipv4 or ipv6, leave both ip addresses unset.\n                pass\n\n    return Endpoint(\n        ipv4=ipv4,\n        ipv6=ipv6,\n        port=port,\n        service_name=service_name,\n    )"}
{"prompt": "def main():\n    \"\"\"\n    Parse argv for options and arguments, and start schema generation.\n    \"\"\"", "output": "\n    import optparse\n    parser = optparse.OptionParser()\n    parser.add_option(\"-o\", \"--output\", dest=\"output\", help=\"output file name\")\n    (options, args) = parser.parse_args()\n\n    if not options.output:\n        parser.error(\"Output file name is required\")\n\n    schema = generate_schema(options.input)\n    with open(options.output, \"w\") as f:\n        f.write(schema)\n\n", "solution": "\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-c\", \"--component\", dest=\"component\", metavar=\"NAME\",\n                      help=\"export sql schema for the component named NAME\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-d\", \"--derived-attributes\", dest=\"derived\",\n                      help=\"include derived attributes in the schema\",\n                      action=\"store_true\", default=False)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql schema to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\", \n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    loader = ooaofooa.Loader()\n    for filename in args:\n        loader.filename_input(filename)\n\n    c = loader.build_component(opts.component, opts.derived)\n    xtuml.persist_database(c, opts.output)"}
{"prompt": "def disconnect(self, mol):\n        \"\"\"Break covalent bonds between metals and organic atoms under certain conditions.\n\n        The algorithm works as follows:\n\n        - Disconnect N, O, F from any metal.\n        - Disconnect other non-metals from transition metals + Al (but not Hg, Ga, Ge, In, Sn, As, Tl, Pb, Bi, Po).\n        - For every bond broken, adjust the charges of the begin and end atoms accordingly.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: The molecule with metals disconnected.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"", "output": "rdchem.Mol\n        :return: The disconnected molecule.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"\n        # Check if the molecule is disconnected\n        if mol.HasSubstructMatch(metals):\n            return mol\n\n        # Check if the molecule is in a transition metal\n        if mol.HasSubstructMatch(['[NX3;H2;-1]', '[#6;-3;H2]', '[#6;-3;-3]', '[#6;-3;-3]',", "solution": "\n        log.debug('Running MetalDisconnector')\n        # Remove bonds that match SMARTS\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?\n                orders.append(int(mol.GetBondBetweenAtoms(i, j).GetBondTypeAsDouble()))\n                rwmol.RemoveBond(i, j)\n            # Adjust neighbouring charges accordingly\n            mol = rwmol.GetMol()\n            for n, (i, j) in enumerate(pairs):\n                chg = orders[n]\n                atom1 = mol.GetAtomWithIdx(i)\n                atom1.SetFormalCharge(atom1.GetFormalCharge() + chg)\n                atom2 = mol.GetAtomWithIdx(j)\n                atom2.SetFormalCharge(atom2.GetFormalCharge() - chg)\n                log.info('Removed covalent bond between %s and %s', atom1.GetSymbol(), atom2.GetSymbol())\n        Chem.SanitizeMol(mol)\n        return mol"}
{"prompt": "def accept_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"", "output": "\n        if text is None:\n            text = self.text\n        if isinstance(text, str):\n            text = re.compile(text)\n        if not text.search(self.text):\n            raise ModalNotFound(f\"Modal '{self.name}' not found.\")\n        self.text = text\n        self.wait = wait or self.wait\n        self.executor.submit(self.execute)\n\n    def execute(self):\n        \"\"\"\n        Execute the wrapped code and display the result.\n        \"\"\"\n        try", "solution": "\n\n        wait = wait or capybara.default_max_wait_time\n        with self.driver.accept_modal(\"alert\", text=text, wait=wait):\n            yield"}
{"prompt": "def build_type(s_dt):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.\n    \"\"\"", "output": "\n    result = {}\n    for key, value in s_dt.items():\n        if isinstance(value, dict):\n            value = build_type(value)\n        if key.endswith('_type'):\n            result[key[:-6]] = value\n        else:\n            result[key] = value\n    return result\n\n", "solution": "\n    s_cdt = nav_one(s_dt).S_CDT[17]()\n    if s_cdt:\n        return build_core_type(s_cdt)\n    \n    s_edt = nav_one(s_dt).S_EDT[17]()\n    if s_edt:\n        return build_enum_type(s_edt)\n    \n    s_udt = nav_one(s_dt).S_UDT[17]()\n    if s_udt:\n        return build_user_type(s_udt)"}
{"prompt": "def _get_obj_count_difference(objs1, objs2):\n    \"\"\"Returns count difference in two collections of Python objects.\"\"\"", "output": "\n    count = 0\n    for obj1 in objs1:\n        if obj1 not in objs2:\n            count += 1\n    for obj2 in objs2:\n        if obj2 not in objs1:\n            count += 1\n    return count\n\n", "solution": "\n    clean_obj_list1 = _process_in_memory_objects(objs1)\n    clean_obj_list2 = _process_in_memory_objects(objs2)\n    obj_count_1 = _get_object_count_by_type(clean_obj_list1)\n    obj_count_2 = _get_object_count_by_type(clean_obj_list2)\n    return obj_count_1 - obj_count_2"}
{"prompt": "def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"", "output": "\n        return self.options\n\n", "solution": "\n        options = 0\n        if self._ignore_uppercase:\n            options |= 1\n        if self._ignore_digits:\n            options |= 2\n        if self._ignore_urls:\n            options |= 4\n        if self._find_repeat_words:\n            options |= 8\n        if self._ignore_latin:\n            options |= 16\n        if self._flag_latin:\n            options |= 128\n        if self._by_words:\n            options |= 256\n        if self._ignore_capitalization:\n            options |= 512\n        if self._ignore_roman_numerals:\n            options |= 2048\n        return options"}
{"prompt": "def get_all_intervals(self):\n        \"\"\"Returns the true list of intervals including the empty intervals.\"\"\"", "output": "\n        intervals = []\n        start = None\n        for interval in sorted(self.intervals):\n            if start is None:\n                start = interval[0]\n            elif interval[0] > start:\n                intervals.append((start, interval[0]))\n                start = interval[0]\n        if start is not None:\n            intervals.append((start, float('inf')))\n        return intervals\n\n    def __len__(self):\n        return len(self.intervals)\n\n    def __getitem__(self, i):", "solution": "\n        ints = sorted(self.get_intervals(True))\n        if self.tier_type == 'IntervalTier':\n            if not ints:\n                ints.append((self.xmin, self.xmax, ''))\n            else:\n                if ints[0][0] > self.xmin:\n                    ints.insert(0, (self.xmin, ints[0][0], ''))\n                if ints[-1][1] < self.xmax:\n                    ints.append((ints[-1][1], self.xmax, ''))\n                p = ints[-1]\n                for index, i in reversed(list(enumerate(ints[:-1], 1))):\n                    if p[0] - i[1] != 0:\n                        ints.insert(index, (i[1], p[0], ''))\n                    p = i\n        return ints"}
{"prompt": "def info_update(self, obj_id, data):\n\t\t\"\"\"Update metadata with of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"", "output": "\n\t\tif obj_id in self.metadata:\n\t\t\tself.metadata[obj_id].update(data)\n\t\telse:\n\t\t\tself.metadata[obj_id] = Metadata(**data)\n\n", "solution": "\n\t\treturn self(obj_id, method='put', data=data, auth_header=True)"}
{"prompt": "def hide_error_message(self):\n        \"\"\"\n        This function hides the error message when all values are correct.\n        \"\"\"", "output": "\n        if not all(self.values):\n            self.error = \"All values are correct.\"\n            return True\n        else:\n            return False\n\n", "solution": "\n        self.ui.error_label.setScaledContents(False)  # Warning image hiden.\n        self.ui.error_text_label.hide()"}
{"prompt": "def conditional_entropy(X, Y, base=2):\n    \"\"\"Calculates the conditional entropy, H(X|Y), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    Y: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    base: integer (default: 2)\n        The base in which to calculate conditional entropy\n\n    Returns\n    ----------\n    conditional_entropy: float\n        The conditional entropy calculated according to the equation H(X|Y) = H(X,Y) - H(Y)\n\n    \"\"\"", "output": " to the formula H(X|Y) = -sum(p * log2(p) for each p in the set of probabilities that X is in Y)\n    \"\"\"\n    # Calculate the set of probabilities that X is in Y\n    probabilities = []\n    for x in X:\n        for y in Y:\n            if x == y:\n                probabilities.append(1)\n            else:\n                probabilities.append(0)\n    \n    # Calculate the conditional entropy\n    entropy = 0\n    for i in range(len(", "solution": "\n    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)"}
{"prompt": "def update_policy(self, id, policy):\n        \"\"\" Create policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - name\n                - policy\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"", "output": "\n        url = f\"{self.base_url}/policies/{id}\"\n        headers = {\"Authorization\": f\"Bearer {self.token}\"}\n        response = requests.put(url, headers=headers, json=policy)\n        if response.status_code!= 200:\n            raise nomad.api.exceptions.URLNotFoundNomadException(url)\n        return response\n\n    def list_policies(self):\n        \"\"\" List policies.\n\n            https://www.nomadproject.io/api/policies.", "solution": "\n        return self.request(\"policy\", id, json=policy, method=\"post\")"}
{"prompt": "def mark(self, value=1):\n        \"\"\"Record an event with the derive.\n\n        :param value: counter value to record\n        \"\"\"", "output": "\n        self.counter += value\n        self.derive.append(value)\n\n    def get_counter(self):\n        \"\"\"Return the current value of the counter.\n\n        :return: current value of the counter\n        \"\"\"\n        return self.counter\n\n    def get_derive(self):\n        \"\"\"Return the current value of the derive.\n\n        :return: current value of the derive\n        \"\"\"\n        return self.derive[-1]\n\n    def reset(self):\n        \"\"\"Reset the counter and derive to initial values.\"\"\"\n        self.counter", "solution": "\n        last = self.last.get_and_set(value)\n        if last <= value:\n            value = value - last\n        super(Derive, self).mark(value)"}
{"prompt": "def __fetch_pull_requests(self, from_date, to_date):\n        \"\"\"Fetch the pull requests\"\"\"", "output": "\n        # Code to fetch pull requests from the API\n        #...\n        pass\n    \n    def __parse_pull_request(self, pull_request):\n        \"\"\"Parse a pull request\"\"\"\n        # Code to parse the pull request\n        #...\n        pass\n    \n    def __store_pull_request(self, pull_request):\n        \"\"\"Store a pull request\"\"\"\n        # Code to store the pull request\n        #...\n        pass\n    \n    def fetch_and_parse_pull_requests(self, from_date, to_date", "solution": "\n\n        raw_pulls = self.client.pulls(from_date=from_date)\n        for raw_pull in raw_pulls:\n            pull = json.loads(raw_pull)\n\n            if str_to_datetime(pull['updated_at']) > to_date:\n                return\n\n            self.__init_extra_pull_fields(pull)\n            for field in TARGET_PULL_FIELDS:\n\n                if not pull[field]:\n                    continue\n\n                if field == 'user':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'merged_by':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'review_comments':\n                    pull[field + '_data'] = self.__get_pull_review_comments(pull['number'])\n                elif field == 'requested_reviewers':\n                    pull[field + '_data'] = self.__get_pull_requested_reviewers(pull['number'])\n                elif field == 'commits':\n                    pull[field + '_data'] = self.__get_pull_commits(pull['number'])\n\n            yield pull"}
{"prompt": "def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\n    Connect src (signals/interfaces/values) to all destinations\n\n    :param exclude: interfaces on any level on src or destinations\n        which should be excluded from connection process\n    :param fit: auto fit source width to destination width\n    \"\"\"", "output": "\n    if exclude is None:\n        exclude = set()\n\n    for dest in destinations:\n        if fit:\n            dest_width = len(dest)\n            if src_width > dest_width:\n                raise ValueError(\n                    f\"Source width ({src_width}) is greater than destination width ({dest_width})\"\n                )\n            if src_width > dest_width - len(exclude):\n                raise ValueError(\n                    f\"Source width ({src_width}) is greater than maximum allowed width \"\n                    f\"({dest_width - len", "solution": "\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts"}
{"prompt": "def addLogicalInterfaceToDeviceType(self, typeId, logicalInterfaceId):\n        \"\"\"\n        Adds a logical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface\n            - description (string) - optional (not used)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        data = {\n            \"typeId\": typeId,\n            \"logicalInterfaceId\": logicalInterfaceId\n        }\n        response = self.service.create_interface(data)\n        if response.get(\"error\", {}).get(\"code\", 0)!= 0:\n            raise APIException(\"Failed to add logical interface to device type\")\n\n    def getLogicalInterfaceId(self, typeId):\n        \"\"\"\n        Returns the id returned by the platform on creation of a logical interface for a given device type.\n        Parameters:\n            - typeId (string)", "solution": "\n        req = ApiClient.allDeviceTypeLogicalInterfacesUrl % (self.host, \"/draft\", typeId)\n        body = {\"id\" : logicalInterfaceId}\n#       body = {\"name\" : \"required but not used!!!\", \"id\" : logicalInterfaceId, \"schemaId\" : schemaId}\n#       if description:\n#           body[\"description\"] = description\n        resp = requests.post(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"}, data=json.dumps(body),\n                        verify=self.verify)\n        if resp.status_code == 201:\n            self.logger.debug(\"Logical interface added to a device type\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error adding logical interface to a device type\", resp)\n        return resp.json()"}
{"prompt": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"", "output": "\n        # Code to fetch information about the repository goes here\n        pass\n\n    def update(self, changes):\n        \"\"\"Update the repository with the latest changes.\"\"\"\n        # Code to update the repository with the latest changes goes here\n        pass\n\n    def history(self):\n        \"\"\"Return a list of all the repositories owned by the user.\"\"\"\n        # Code to return a list of all the repositories owned by the user goes here\n        pass\n\n", "solution": "\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text"}
{"prompt": "def target_address(self):\n        \"\"\"Return the authorative target of the link.\"\"\"", "output": "\n        return self.link.split(\":\")[0]\n\n    def author_name(self):\n        \"\"\"Return the author's name.\"\"\"\n        return self.link.split(\":\")[1].strip()\n\n    def title(self):\n        \"\"\"Return the title of the link.\"\"\"\n        return self.link.split(\"#\")[0].strip()\n\n    def text(self):\n        \"\"\"Return the text of the link.\"\"\"\n        return self.link.split(\"#\")[1].strip()\n\n    def image_url(self):\n        \"\"\"Return", "solution": "\n        # If link is a receiver, target is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_receiver:\n            return self._pn_link.target.address\n        else:\n            return self._pn_link.remote_target.address"}
{"prompt": "def recv_unsub(self, id_=None):\n        \"\"\"DDP unsub handler.\"\"\"", "output": "\n        if id_ is None:\n            return self.sub\n        else:\n            return self.sub[id_]\n\n    def recv_ack(self, id_=None):\n        \"\"\"DDP ack handler.\"\"\"\n        if id_ is None:\n            return self.ack\n        else:\n            return self.ack[id_]\n\n    def recv_nack(self, id_=None):\n        \"\"\"DDP nack handler.\"\"\"\n        if id_ is None:\n            return self.nack\n        else:\n            ", "solution": "\n        if id_:\n            self.api.unsub(id_)\n        else:\n            self.reply('nosub')"}
{"prompt": "def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):\n    \"\"\"\n    Conduct semi-supervised learning and error-rate estimation for MS1, MS2 and transition-level data. \n    \"\"\"", "output": "dr_max_iter):\n    \"\"\"\n    Evaluates the effect of classifiers on a dataset using a hyperparameter search.\n\n    Parameters:\n    infile (str): Input file path.\n    outfile (str): Output file path.\n    classifier (str): Classifier model file path.\n    xgb_autotune (bool): Whether to use XGBoost hyperparameter tuning.\n    apply_weights (bool): Whether to apply weights to the dataset.\n    xeval_fraction (float): Fraction of the dataset to use for hyperparameter", "solution": "\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    if not apply_weights:\n        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()\n    else:\n        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()"}
{"prompt": "def cleanup(controller, engines):\n    \"\"\"Cleanup routine to shut down all subprocesses we opened.\"\"\"", "output": "\n    for engine in engines:\n        engine.terminate()\n    for engine in engines:\n        engine.wait()\n\n", "solution": "\n    import signal, time\n    \n    print('Starting cleanup')\n    print('Stopping engines...')\n    for e in engines:\n        e.send_signal(signal.SIGINT)\n    print('Stopping controller...')\n    # so it can shut down its queues\n    controller.send_signal(signal.SIGINT)\n    time.sleep(0.1)\n    print('Killing controller...')\n    controller.kill()\n    print('Cleanup done')"}
{"prompt": "def reset(self):\n        \"\"\"Reset the input buffer and associated state.\"\"\"", "output": "\n        self.buffer = b''\n        self.pos = 0\n        self.endpos = 0\n        self.eof = False\n        self.eol = False\n        self.line = ''\n\n    def tell(self):\n        \"\"\"Return the current position in the input buffer.\"\"\"\n        return self.pos\n\n    def seek(self, pos, whence=0):\n        \"\"\"Set the current position in the input buffer.\"\"\"\n        if whence == 0:\n            self.pos = pos\n        elif whence == 1:\n            self.pos += pos\n", "solution": "\n        super(IPythonInputSplitter, self).reset()\n        self._buffer_raw[:] = []\n        self.source_raw = ''\n        self.cell_magic_parts = []\n        self.processing_cell_magic = False"}
{"prompt": "def for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"", "output": "\n    def __str__(self):\n        return func.__name__\n    type_module.add_pretty_printer(type_name, __str__)\n\n", "solution": "\n    key = (type_module, type_name)\n    oldfunc = _deferred_type_pprinters.get(key, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _deferred_type_pprinters[key] = func\n    return oldfunc"}
{"prompt": "def domain(self, domain=None, last_domain=None):\n        \"\"\"\n        Manage the case that we want to test only a domain.\n\n        :param domain: The domain or IP to test.\n        :type domain: str\n\n        :param last_domain:\n            The last domain to test if we are testing a file.\n        :type last_domain: str\n\n        :param return_status: Tell us if we need to return the status.\n        :type return_status: bool\n        \"\"\"", "output": "\n        if domain is None:\n            domain = self.domain\n        if last_domain is None:\n            last_domain = self.last_domain\n\n        if domain == last_domain:\n            return True\n\n        if domain.endswith('.com') and last_domain.endswith('.com'):\n            return True\n\n        if domain.endswith('.org') and last_domain.endswith('.org'):\n            return True\n\n        if domain.endswith('.net') and last_domain.endswith('.net", "solution": "\n\n        # We print the header.\n        self._print_header()\n\n        if domain:\n            # A domain is given.\n\n            # We format and set the domain we are testing and treating.\n            PyFunceble.INTERN[\"to_test\"] = self._format_domain(domain)\n        else:\n            # A domain is not given.\n\n            # We set the domain we are testing and treating to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # The domain is given (Not None).\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # We test and get the status of the domain.\n                status, _ = self.status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_domain, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the domain and the status.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the tested domain and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None"}
{"prompt": "def varimp(self, use_pandas=False):\n        \"\"\"\n        Pretty print the variable importances, or return them in a list/pandas DataFrame.\n\n        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n\n        :returns: A dictionary of lists or Pandas DataFrame instances.\n        \"\"\"", "output": "\n        if use_pandas:\n            import pandas as pd\n            return pd.DataFrame(self.importances, index=self.features)\n        else:\n            return {feature: [importance] for feature, importance in self.importances.items()}\n\n    def export(self, filename):\n        \"\"\"\n        Export the variable importances to a file.\n\n        :param str filename: The name of the file to export the variable importances to.\n        \"\"\"\n        with open(filename, 'w') as f:\n            for feature,", "solution": "\n        return {model.model_id: model.varimp(use_pandas) for model in self.models}"}
{"prompt": "def statementsAreSame(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"", "output": "\n    if len(statements) == 0:\n        return True\n    first_statement = statements[0]\n    for statement in statements[1:]:\n        if statement!= first_statement:\n            return False\n    return True\n\n", "solution": "\n    iterator = iter(statements)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        return True\n\n    return all(first.isSame(rest) for rest in iterator)"}
{"prompt": "def get_meta_netnode():\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"", "output": "\n    return idc.get_netnode()\n\n", "solution": "\n    node_name = \"$ {org:s}.{application:s}\".format(\n        org=IDA_SETTINGS_ORGANIZATION,\n        application=IDA_SETTINGS_APPLICATION)\n    return netnode.Netnode(node_name)"}
{"prompt": "def setPin(self, old_pin, new_pin):\n        \"\"\"\n        C_SetPIN\n\n        :param old_pin: old PIN\n        :param new_pin: new PIN\n        \"\"\"", "output": "\n        self.pins[old_pin] = new_pin\n\n    def getPin(self, pin):\n        \"\"\"\n        C_GetPIN\n\n        :param pin: PIN to get\n        :return: PIN value\n        \"\"\"\n        return self.pins.get(pin)\n\n    def __del__(self):\n        \"\"\"\n        C_Del\n\n        :return: None\n        \"\"\"\n        self.motor.stop()\n\n", "solution": "\n        old_pin1 = ckbytelist(old_pin)\n        new_pin1 = ckbytelist(new_pin)\n        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def divide(df, new_column, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"", "output": "\n    # Check if the formula is valid\n    if not isinstance(df[column_1], str) or not isinstance(df[column_2], str):\n        raise ValueError(\"Column names must be strings\")\n    if not isinstance(df[new_column], str):\n        raise ValueError(\"New column name must be a string\")\n    \n    # Calculate the new value for the new column\n    new_value = (df[column_1] + df[column_2]) / 2\n    \n    # Update the DataFrame with the new column\n", "solution": "\n    return _basic_math_operation(df, new_column, column_1, column_2, op='truediv')"}
{"prompt": "def _setup_template_file(self, template_file_path):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_file_path: str\n            Document template file path.\n        \"\"\"", "output": "\n        self.template = {}\n\n    def add_variable(self, variable_name, variable_value):\n        \"\"\" Add variable to self.template\n\n        Parameters\n        ----------\n        variable_name: str\n            Variable name.\n        variable_value: str\n            Variable value.\n        \"\"\"\n        self.template[variable_name] = variable_value\n\n    def get_variable(self, variable_name):\n        \"\"\" Get variable value from self.template\n\n        Parameters\n        ----------\n        variable_name: str\n            Variable name.\n\n        Returns\n        -------\n", "solution": "\n        try:\n            template_file = template_file_path\n            template_env = get_environment_for(template_file_path)\n            template = template_env.get_template(os.path.basename(template_file))\n        except:\n            raise\n        else:\n            self._template_file = template_file\n            self._template_env = template_env\n            self.template = template"}
{"prompt": "def pxconfig(self, line):\n        \"\"\"configure default targets/blocking for %px magics\"\"\"", "output": "\n        if line.startswith(\"pxconfig\"):\n            parts = line.split()\n            if len(parts) == 2:\n                try:\n                    config = int(parts[1])\n                    if config > 0:\n                        self.pxconfig_target = config\n                except ValueError:\n                    pass\n        return line\n\n    def __init__(self, targets=None, blocking=True):\n        self.targets = targets or []\n        self.blocking = blocking\n        self.pxconfig_target = None\n\n    def __enter__(self", "solution": "\n        args = magic_arguments.parse_argstring(self.pxconfig, line)\n        if args.targets:\n            self.view.targets = self._eval_target_str(args.targets)\n        if args.block is not None:\n            self.view.block = args.block\n        if args.set_verbose is not None:\n            self.verbose = args.set_verbose"}
{"prompt": "def l(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"", "output": "\n    return [member for member in members if member is not None]\n\n", "solution": "\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"prompt": "def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n    \"\"\"wrapper\"\"\"", "output": "\n    import requests\n    from tqdm import tqdm\n    \n    # download video from URL\n    response = requests.get(url, stream = True, **kwargs)\n    \n    # get video ID from response headers\n    video_id = response.headers.get('id')\n    \n    # create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # create progress bar\n    total_size = int(response.headers.get('", "solution": "\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'\n    hostname = urlparse(url).hostname\n    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):\n        return False\n    upid = match1(url, r'.+/content/(\\w+)')\n\n    fc2video_download_by_upid(upid, output_dir, merge, info_only)"}
{"prompt": "def tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"", "output": "\n    binary_str = bin(deci_num)[2:]  # remove '0b' prefix\n    padding = '0' * (len(binary_str) % len)  # add leading zeros if necessary\n    if len(padding) > 0:\n        binary_str = padding + binary_str  # add leading zeros if necessary\n    bitfield = [binary_str[i:i+len] for i in range(0, len(binary_str), len)]  # split into chunks of len bits\n    return''.join(bitfield)  ", "solution": "\n    bitstr = \"\".join(map(lambda y: str((deci_num >> y) & 1), range(len - 1, -1, -1)))\n    return bitstr"}
{"prompt": "def export_mt_variants(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of lines to include in the document\n    \"\"\"", "output": "\n    document_lines = []\n    for variant in variants:\n        line = f\"{variant['chromosome']}:{variant['position']}-{variant['position']+1}\\t{variant['reference']}\\t{variant['alternate']}\\t{sample_id}\\n\"\n        document_lines.append(line)\n    return document_lines\n\n", "solution": "\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines"}
{"prompt": "def write_sky_params_to_file(self):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"", "output": "\n        with open(self.params_file, 'w') as f:\n            for key, value in self.sky_params.items():\n                f.write(f\"{key}: {value}\\n\")\n\n", "solution": "\n\n        inp_file = self.sky_file + '_params.txt'\n        lg.info('Writing Inputs to file : ' + inp_file)\n\n        f = open(inp_file, 'w')\n\n        f.write('verbose= ' + str(self.verbose) + '\\n')\n        f.write('band_count= ' + str(self.num_bands) + '\\n')\n        f.write('band_centres_data= ')\n        f.write(\",\".join([str(wave) for wave in self.wavelengths]) + '\\n')\n        f.write('partition= ' + self.partition + '\\n')\n        f.write('vn= ' + str(self.vn) + '\\n')\n        f.write('hn= ' + str(self.hn) + '\\n')\n        f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.write('theta_points= ')\n        f.write(\",\".join([str(theta) for theta in self.theta_points]) + '\\n')\n        f.write('type= ' + self.sky_type + '\\n')\n        f.write('azimuth= ' + str(self.sky_azimuth) + '\\n')\n        f.write('zenith= ' + str(self.sky_zenith) + '\\n')\n        f.write('sky_save_fp= ' + inp_file.strip('_params.txt') + '\\n')\n        f.write('sky_image_save_fp= ' + self.sky_file + '.ppm' + '\\n')\n        f.write('sky_image_size= 256' + '\\n')\n        if self.sky_type == 'hlideal':\n            f.write('C= ' + str(self.sky_c) + '\\n')\n            f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.flush()\n        f.close()"}
{"prompt": "def defrag(filt, threshold=3, mode='include'):\n    \"\"\"\n    'Defragment' a filter.\n\n    Parameters\n    ----------\n    filt : boolean array\n        A filter\n    threshold : int\n        Consecutive values equal to or below this threshold\n        length are considered fragments, and will be removed.\n    mode : str\n        Wheter to change False fragments to True ('include')\n        or True fragments to False ('exclude')\n\n    Returns\n    -------\n    defragmented filter : boolean array\n    \"\"\"", "output": "\n    if mode == 'include':\n        mask = filt\n    elif mode == 'exclude':\n        mask = np.logical_not(filt)\n    else:\n        raise ValueError(\"Invalid mode. Must be 'include' or 'exclude'.\")\n\n    defragmented = np.zeros_like(filt)\n    current_fragment = False\n    current_length = 0\n\n    for i in range(filt.shape[0]):\n        if filt[i]:\n            if current_fragment:\n                ", "solution": "\n    if bool_2_indices(filt) is None:\n        return filt\n\n    if mode == 'include':\n        inds = bool_2_indices(~filt) + 1\n        rep = True\n    if mode == 'exclude':\n        inds = bool_2_indices(filt) + 1\n        rep = False\n\n    rem = (np.diff(inds) <= threshold)[:, 0]\n\n    cfilt = filt.copy()\n    if any(rem):\n        for lo, hi in inds[rem]:\n            cfilt[lo:hi] = rep\n\n    return cfilt"}
{"prompt": "def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return Sailthru purchase item object\"\"\"", "output": "\n    purchase_item = {\n        \"course_id\": course_id,\n        \"course_url\": course_url,\n        \"cost_in_cents\": cost_in_cents,\n        \"mode\": mode,\n        \"course_data\": course_data,\n        \"sku\": sku\n    }\n    return purchase_item\n\n", "solution": "\n\n    # build item description\n    item = {\n        'id': \"{}-{}\".format(course_id, mode),\n        'url': course_url,\n        'price': cost_in_cents,\n        'qty': 1,\n    }\n\n    # get title from course info if we don't already have it from Sailthru\n    if 'title' in course_data:\n        item['title'] = course_data['title']\n    else:\n        # can't find, just invent title\n        item['title'] = 'Course {} mode: {}'.format(course_id, mode)\n\n    if 'tags' in course_data:\n        item['tags'] = course_data['tags']\n\n    # add vars to item\n    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)\n\n    item['vars']['purchase_sku'] = sku\n\n    return item"}
{"prompt": "def _kl_normal_normal(n_a, n_b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n  Args:\n    n_a: instance of a Normal distribution object.\n    n_b: instance of a Normal distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_normal_normal\".\n\n  Returns:\n    Batchwise KL(n_a || n_b)\n  \"\"\"", "output": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    kl = 0.5 * (1 + tf.log(n_a.variance) - 1 - tf.log(n_b.variance) +\n               (n_a.mean - n_b.mean)**2 / (2 * n_a.variance))\n    return tf.reduce_mean(kl)\n\n", "solution": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))"}
{"prompt": "def scan(self, *key_ranges, **kwargs):\n        \"\"\"Scan for FCs in the given id ranges.\n\n        :param key_ranges:\n          ``key_ranges`` should be a list of pairs of ranges. The first\n          value is the lower bound id and the second value is the\n          upper bound id. Use ``()`` in either position to leave it\n          unbounded. If no ``key_ranges`` are given, then all FCs in\n          the store are returned.\n        :param [str] feature_names:\n          A list of feature names to retrieve. When ``None``, all\n          features are retrieved. Wildcards are allowed.\n        :rtype: Iterable of ``(content_id, FC)``\n        \"\"\"", "output": "If given, only return FCs that have a name in this list.\n        :param [str] owner_names:\n          If given, only return FCs that have an owner in this list.\n        :param [str] status:\n          If given, only return FCs that have a status in this list.\n        :param [str] price:\n          If given, only return FCs that have a price in this list.\n        :param [str] quantity:\n          If given, only return FCs that have a quantity in this list.\n        :return:\n          ", "solution": "\n        for hit in self._scan(*key_ranges, **kwargs):\n            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])"}
{"prompt": "def encode_span(self, span):\n        \"\"\"Encodes a single span to protobuf.\"\"\"", "output": "\n        encoded_span = span.SerializeToString()\n        return encoded_span\n\n    def encode_message(self, message):\n        \"\"\"Encodes a message to protobuf.\"\"\"\n        encoded_message = message.SerializeToString()\n        return encoded_message\n\n    def encode_span_message(self, span_message):\n        \"\"\"Encodes a span message to protobuf.\"\"\"\n        encoded_span_message = span_message.SerializeToString()\n        return encoded_span_message\n\n    def decode_span(self,", "solution": "\n        if not protobuf.installed():\n            raise ZipkinError(\n                'protobuf encoding requires installing the protobuf\\'s extra '\n                'requirements. Use py-zipkin[protobuf] in your requirements.txt.'\n            )\n\n        pb_span = protobuf.create_protobuf_span(span)\n        return protobuf.encode_pb_list([pb_span])"}
{"prompt": "def verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"", "output": "\n    if signalItem in ['in', 'out']:\n        return'register'\n    elif signalItem in ['we', 'w']:\n        return 'wire'\n    else:\n        return 'unknown'\n\n", "solution": "\n    driver_cnt = len(signalItem.drivers)\n    if signalItem._const or driver_cnt > 1 or\\\n       arr_any(signalItem.drivers, _isEventDependentDriver):\n        return SIGNAL_TYPE.REG\n    else:\n        if driver_cnt == 1:\n            d = signalItem.drivers[0]\n            if not isinstance(d, (Assignment, PortItem)):\n                return SIGNAL_TYPE.REG\n\n        return SIGNAL_TYPE.WIRE"}
{"prompt": "def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"", "output": "\n        try:\n            self.client.get_bucket(Bucket=bucket_name)\n            return True\n        except ClientError:\n            return False\n\n    def list_buckets(self):\n        \"\"\"\n        List all buckets.\n\n        :return: a list of all the buckets\n        :rtype: List[str]\n        \"\"\"\n        return [bucket.name for bucket in self.client.list_buckets()]\n\n    def create_bucket(self, bucket_name):\n        \"\"\"\n        Create a new bucket with the given name", "solution": "\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"}
{"prompt": "def get_event_hub(self, hub_name):\n        \"\"\"\n        Retrieves an existing event hub.\n\n        hub_name:\n            Name of the event hub.\n        \"\"\"", "output": "\n        # Code to retrieve the event hub with the given name\n        #...\n        return event_hub\n\n    def create_event_hub(self, hub_name):\n        \"\"\"\n        Creates a new event hub.\n\n        hub_name:\n            Name of the event hub.\n        \"\"\"\n        # Code to create a new event hub with the given name\n        #...\n        return event_hub\n\n    def delete_event_hub(self, hub_name):\n        \"\"\"\n        Deletes an existing event hub.\n\n        hub_name:\n            Name of", "solution": "\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)"}
{"prompt": "def get_plot_data(self):\n        \"\"\" Generates the JSON report to plot the gene boxes\n\n        Following the convention of the reports platform, this method returns\n        a list of JSON/dict objects with the information about each entry in\n        the abricate file. The information contained in this JSON is::\n\n            {contig_id: <str>,\n             seqRange: [<int>, <int>],\n             gene: <str>,\n             accession: <str>,\n             coverage: <float>,\n             identity: <float>\n             }\n\n        Note that the `seqRange` entry contains the position in the\n        corresponding contig, not the absolute position in the whole assembly.\n\n        Returns\n        -------\n        json_dic : list\n            List of JSON/dict objects with the report data.\n        \"\"\"", "output": "The seqRange field contains two integers representing the start and end\n        positions of the sequence in the abricate file. The gene field contains\n        the name of the gene. The accession field contains the accession number\n        of the gene. The coverage field contains the percentage of the gene covered\n        by the sequence, rounded to two decimal places. The identity field\n        contains the percentage identity of the gene, rounded to two decimal places.\n\n        The JSON object returned by this method is sorted by the accession number\n        in ascending order.\n\n        Returns:\n            A list of JSON/", "solution": "\n\n        json_dic = {\"plotData\": []}\n        sample_dic = {}\n        sample_assembly_map = {}\n\n        for entry in self.storage.values():\n\n            sample_id = re.match(\"(.*)_abr\", entry[\"log_file\"]).groups()[0]\n            if sample_id not in sample_dic:\n                sample_dic[sample_id] = {}\n\n            # Get contig ID using the same regex as in `assembly_report.py`\n            # template\n            contig_id = self._get_contig_id(entry[\"reference\"])\n            # Get database\n            database = entry[\"database\"]\n            if database not in sample_dic[sample_id]:\n                sample_dic[sample_id][database] = []\n\n            # Update the sample-assembly correspondence dict\n            if sample_id not in sample_assembly_map:\n                sample_assembly_map[sample_id] = entry[\"infile\"]\n\n            sample_dic[sample_id][database].append(\n                {\"contig\": contig_id,\n                 \"seqRange\": entry[\"seq_range\"],\n                 \"gene\": entry[\"gene\"].replace(\"'\", \"\"),\n                 \"accession\": entry[\"accession\"],\n                 \"coverage\": entry[\"coverage\"],\n                 \"identity\": entry[\"identity\"],\n                 },\n            )\n\n        for sample, data in sample_dic.items():\n            json_dic[\"plotData\"].append(\n                {\n                    \"sample\": sample,\n                    \"data\": {\"abricateXrange\": data},\n                    \"assemblyFile\": sample_assembly_map[sample]\n                }\n            )\n\n        return json_dic"}
{"prompt": "def usi(self):\n        \"\"\"\n        Gets an USI string for the move.\n        For example a move from 7A to 8A would be `7a8a` or `7a8a+` if it is\n        a promotion.\n        \"\"\"", "output": "\n        if self.rank == 'A':\n            return '7a8a'\n        elif self.rank == 'B':\n            return '7a8b'\n        elif self.rank == 'C':\n            return '7a9c'\n        elif self.rank == 'D':\n            return '7a9d'\n        elif self.rank == 'E':\n            return '7a9e'\n        elif self.rank == 'F':\n            return '7a9f'\n        elif self.rank == '", "solution": "\n        if self:\n            if self.drop_piece_type:\n                return '{0}*{1}'.format(PIECE_SYMBOLS[self.drop_piece_type].upper(), SQUARE_NAMES[self.to_square])\n            else:\n                return SQUARE_NAMES[self.from_square] + SQUARE_NAMES[self.to_square] + \\\n                       ('+' if self.promotion else '')\n        else:\n            return '0000'"}
{"prompt": "def load_institute(adapter, internal_id, display_name, sanger_recipients=None):\n    \"\"\"Load a institute into the database\n\n        Args:\n            adapter(MongoAdapter)\n            internal_id(str)\n            display_name(str)\n            sanger_recipients(list(email))\n    \"\"\"", "output": "\n    institute = {\n        'adapter': adapter,\n        'internal_id': internal_id,\n        'display_name': display_name,\n       'sanger_recipients': sanger_recipients\n    }\n    db.institutes.insert_one(institute)\n\n", "solution": "\n\n    institute_obj = build_institute(\n        internal_id=internal_id,\n        display_name=display_name,\n        sanger_recipients=sanger_recipients\n    )\n    log.info(\"Loading institute {0} with display name {1}\" \\\n             \" into database\".format(internal_id, display_name))\n\n    adapter.add_institute(institute_obj)"}
{"prompt": "def compute_nc(X, G):\n    \"\"\"Computes the novelty curve from the self-similarity matrix X and\n        the gaussian kernel G.\"\"\"", "output": "\n    n = X.shape[0]\n    m = G.shape[0]\n    nc = np.zeros((n, m))\n    for i in range(n):\n        for j in range(m):\n            nc[i, j] = np.exp(-0.5 * (X[i] - G[j])**2)\n    return nc\n\n", "solution": "\n    N = X.shape[0]\n    M = G.shape[0]\n    nc = np.zeros(N)\n\n    for i in range(M // 2, N - M // 2 + 1):\n        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)\n\n    # Normalize\n    nc += nc.min()\n    nc /= nc.max()\n    return nc"}
{"prompt": "def _get_boot_time():\n    \"\"\"Return system boot time (epoch in seconds)\"\"\"", "output": "\n    import time\n    return int(time.time())\n\n", "solution": "\n    f = open('/proc/stat', 'r')\n    try:\n        for line in f:\n            if line.startswith('btime'):\n                return float(line.strip().split()[1])\n        raise RuntimeError(\"line not found\")\n    finally:\n        f.close()"}
{"prompt": "def get_plugin_of_type(self, plugin_class):\n        \"\"\"\n        Retrieve a plugin of desired class, KeyError raised otherwise\n        \"\"\"", "output": "\n        try:\n            return self.plugins[plugin_class]\n        except KeyError:\n            raise KeyError(f\"No plugin of class {plugin_class} found\")\n\n    def add_plugin(self, plugin_class, plugin_instance):\n        \"\"\"\n        Add a plugin of desired class, ValueError raised otherwise\n        \"\"\"\n        if plugin_class in self.plugins:\n            raise ValueError(f\"Plugin of class {plugin_class} already exists\")\n        self.plugins[plugin_class] = plugin_instance\n\n", "solution": "\n        logger.debug(\"Searching for plugin: %s\", plugin_class)\n        matches = [plugin for plugin in self.plugins.values() if isinstance(plugin, plugin_class)]\n        if matches:\n            if len(matches) > 1:\n                logger.debug(\n                    \"More then one plugin of type %s found. Using first one.\",\n                    plugin_class)\n            return matches[-1]\n        else:\n            raise KeyError(\"Requested plugin type not found: %s\" % plugin_class)"}
{"prompt": "def calculate_linear_predictor(model_matrix, model_coefficients, offset=None,\n                               name=None):\n  \"\"\"Computes `model_matrix @ model_coefficients + offset`.\"\"\"", "output": "\n  if offset is None:\n    offset = 0.0\n  return model_matrix @ model_coefficients + offset\n\n", "solution": "\n  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',\n                               [model_matrix, model_coefficients, offset]):\n    predicted_linear_response = tf.linalg.matvec(model_matrix,\n                                                 model_coefficients)\n    if offset is not None:\n      predicted_linear_response += offset\n    return predicted_linear_response"}
{"prompt": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"", "output": "\n    # Get the hero name from the context dictionary\n    hero_name = context.get('hero_name')\n\n    # Calculate the length of the hero name\n    length = len(hero_name)\n\n    # Create the HTML code for the slider\n    slider_html = f'<div class=\"slider\">{hero_name}</div>'\n\n    # Return the slider HTML code as a string\n    return slider_html\n\n", "solution": "\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }"}
{"prompt": "def get_color(self, color, intensity=0):\n        \"\"\" Returns a QColor for a given color code, or None if one cannot be\n            constructed.\n        \"\"\"", "output": "\n        if color == \"red\":\n            return QColor(255, 0, 0)\n        elif color == \"green\":\n            return QColor(0, 255, 0)\n        elif color == \"blue\":\n            return QColor(0, 0, 255)\n        elif color == \"yellow\":\n            return QColor(255, 255, 0)\n        elif color == \"purple\":\n            return QColor(128, 0, 128)\n        elif color == \"orange\":\n            return QColor(255, 165, 0)\n        ", "solution": "\n        if color is None:\n            return None\n\n        # Adjust for intensity, if possible.\n        if color < 8 and intensity > 0:\n            color += 8\n\n        constructor = self.color_map.get(color, None)\n        if isinstance(constructor, basestring):\n            # If this is an X11 color name, we just hope there is a close SVG\n            # color name. We could use QColor's static method\n            # 'setAllowX11ColorNames()', but this is global and only available\n            # on X11. It seems cleaner to aim for uniformity of behavior.\n            return QtGui.QColor(constructor)\n\n        elif isinstance(constructor, (tuple, list)):\n            return QtGui.QColor(*constructor)\n\n        return None"}
{"prompt": "def update_z(self, z, indices=None):\n        \"\"\"\n        Update partial or entire z.\n\n        Args:\n            z (numpy.ndarray or list): to-be-updated z\n            indices (numpy.ndarray or list or optional): to-be-updated qubit indices\n\n        Returns:\n            Pauli: self\n\n        Raises:\n            QiskitError: when updating whole z, the number of qubits must be the same.\n        \"\"\"", "output": "\n        if indices is None:\n            indices = np.arange(len(z))\n        if len(z)!= len(indices):\n            raise QiskitError(\"Number of qubits must be the same.\")\n        z[indices] = 1 - z[indices]\n        return self\n\n    def measure_z(self, indices=None):\n        \"\"\"\n        Measure partial or entire z.\n\n        Args:\n            indices (numpy.ndarray or list or optional): to-be-measured qubit indices\n\n        Returns:\n            ", "solution": "\n        z = _make_np_bool(z)\n        if indices is None:\n            if len(self._z) != len(z):\n                raise QiskitError(\"During updating whole z, you can not \"\n                                  \"change the number of qubits.\")\n            self._z = z\n        else:\n            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):\n                indices = [indices]\n            for p, idx in enumerate(indices):\n                self._z[idx] = z[p]\n\n        return self"}
{"prompt": "def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"", "output": "\n        if atol is None:\n            atol = self.atol\n        if rtol is None:\n            rtol = self.rtol\n        return np.allclose(np.trace(self), 0, atol=atol, rtol=rtol)\n\n    def is_hermitian(self, atol=None, rtol=None):\n        \"\"\"Return True if the matrix is Hermitian.\"\"\"\n        if atol is None:\n            atol = self.atol\n        if rtol is None:\n            ", "solution": "\n        if self._data[1] is not None:\n            return False\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        accum = 0j\n        for op in self._data[0]:\n            accum += np.dot(np.transpose(np.conj(op)), op)\n        return is_identity_matrix(accum, rtol=rtol, atol=atol)"}
{"prompt": "def delete_clinvar_object(self, object_id, object_type, submission_id):\n        \"\"\"Remove a variant object from clinvar database and update the relative submission object\n\n            Args:\n                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)\n                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"", "output": "            Returns:\n                bool: True if the object was successfully removed from clinvar_collection database, False otherwise.\n        \"\"\"\n        # Check if object_id is valid\n        if object_id not in self.clinvar_collection:\n            return False\n\n        # Remove object from clinvar_collection database\n        self.clinvar_collection.pop(object_id)\n\n        # Check if object was successfully removed from clinvar_collection database\n        if object_type == 'case_data' and 'clinvar_submission' in self.clinvar_collection", "solution": "\n\n        LOG.info(\"Deleting clinvar object %s (%s)\", object_id, object_type)\n\n        # If it's a variant object to be removed:\n        #   remove reference to it in the submission object 'variant_data' list field\n        #   remove the variant object from clinvar collection\n        #   remove casedata object from clinvar collection\n        #   remove reference to it in the submission object 'caset_data' list field\n\n        # if it's a casedata object to be removed:\n        #   remove reference to it in the submission object 'caset_data' list field\n        #   remove casedata object from clinvar collection\n\n        result = ''\n\n        if object_type == 'variant_data':\n            # pull out a variant from submission object\n            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })\n\n            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )\n            linking_id = variant_object.get(\"linking_id\") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together\n\n            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata\n            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )\n\n        else: # remove case_data but keep variant in submission\n            # delete the object itself from clinvar_collection\n             result = self.clinvar_collection.delete_one( {'_id': object_id } )\n\n        # in any case remove reference to it in the submission object 'caset_data' list field\n        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n\n        return updated_submission"}
{"prompt": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"", "output": "\n        if dtype == HdlType.HArray:\n            return bitAddr + 1\n        else:\n            raise ValueError(f\"Invalid HdlType: {dtype}\")\n\n    def _dumpToHdlType(self) -> HdlType:\n        \"\"\"\n        Convert this transaction template instance to its HdlType\n\n        :return: HdlType of this transaction template instance\n        \"\"\"\n        if self.is_array:\n            return HdlType.HArray\n        else:\n            raise ValueError(\"Transaction template is not array\")\n\n    ", "solution": "\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd"}
{"prompt": "def select(selector, obj):\n    \"\"\"Appy selector to obj and return matching nodes.\n\n    If only one node is found, return it, otherwise return a list of matches.\n    Returns False on syntax error. None if no results found.\n    \"\"\"", "output": "\n    matches = []\n    for node in obj:\n        try:\n            if selector(node):\n                matches.append(node)\n        except SyntaxError:\n            pass\n    if len(matches) == 1:\n        return matches[0]\n    else:\n        return matches\n\n", "solution": "\n\n    parser = Parser(obj)\n    try:\n        return parser.parse(selector)\n    except SelectorSyntaxError as e:\n        log.exception(e)\n        return False"}
{"prompt": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"", "output": "\n    parts = host.split(\":\")\n    if len(parts) == 1:\n        return (parts[0], None)\n    else:\n        return (parts[0], int(parts[1]))\n\n", "solution": "\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"}
{"prompt": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"", "output": "\n        if pkt[0] == \"NACK\":\n            self.nak_count += 1\n            self.nak_duration += pkt[1]\n            self.nak_payload += pkt[2:]\n            if self.nak_count == 3:\n                self.nak_rebinding()\n        else:\n            self.nak_count = 0\n            self.nak_duration = 0\n            self.nak_payload = \"\"\n\n    def nak_rebinding(self):\n        \"\"\"NACK in REB", "solution": "\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"prompt": "def diseases(context):\n    \"\"\"Show all diseases in the database\"\"\"", "output": "\n    diseases = []\n    for patient in context.patients:\n        for disease in patient.diseases:\n            diseases.append(disease)\n    return diseases\n\n", "solution": "\n    LOG.info(\"Running scout view diseases\")\n    adapter = context.obj['adapter']\n\n    disease_objs = adapter.disease_terms()\n\n    nr_diseases = disease_objs.count()\n    if nr_diseases == 0:\n        click.echo(\"No diseases found\")\n    else:\n        click.echo(\"Disease\")\n        for disease_obj in adapter.disease_terms():\n            click.echo(\"{0}\".format(disease_obj['_id']))\n        LOG.info(\"{0} diseases found\".format(nr_diseases))"}
{"prompt": "def infix_to_postfix(nodes, *, recurse_types=None):\n  \"\"\"Convert a list of nodes in infix order to a list of nodes in postfix order.\n\n  E.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +\n  \"\"\"", "output": "\n  if recurse_types is None:\n    recurse_types = set()\n\n  def helper(nodes, stack, output):\n    if not nodes:\n      return\n\n    for node in nodes:\n      if isinstance(node, Node):\n        if node.type in recurse_types:\n          helper(node.children, stack, output)\n        else:\n          output.append(node)\n      else:\n        raise TypeError(f\"Expected Node, got {type(node)}\")\n\n    while stack:\n      node = stack.", "solution": "\n  output = []\n  operators = []\n\n  for node in nodes:\n    if isinstance(node, OperatorNode):\n      # Drain out all operators whose precedence is gte the node's...\n      cmp_operator = node.operator\n      while operators:\n        current_operator = operators[-1].operator\n        if current_operator.precedence > cmp_operator.precedence or \\\n           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:\n          output.append(operators.pop())\n        else:\n          break\n      operators.append(node)\n    else:\n      if recurse_types is not None and node.node_type in recurse_types:\n        output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))\n      else:\n        output.append(node)\n\n  return output + list(reversed(operators))"}
{"prompt": "def get(cls, key, default=None):\n        \"\"\"\n        Resembles the :meth:`dict.get` method.\n\n        :returns:\n            A configuration dictionary for specified provider.\n\n        \"\"\"", "output": "\n        return cls.providers.get(key, default)\n\n    @classmethod\n    def set(cls, key, value):\n        \"\"\"\n        Resembles the :meth:`dict.set` method.\n\n        :param key:\n            The key for the configuration dictionary.\n        :param value:\n            The value for the configuration dictionary.\n\n        \"\"\"\n        cls.providers[key] = value\n\n    @classmethod\n    def reset(cls):\n        \"\"\"\n        Resets all configuration dictionaries to their default values.\n\n        ", "solution": "\n\n        # Query datastore.\n        result = cls.query(cls.provider_name == key).get()\n\n        if result:\n            result_dict = result.to_dict()\n\n            # Use NDBOpenIDStore by default\n            result_dict['store'] = NDBOpenIDStore\n\n            # Convert coma-separated values to list. Currently only scope is\n            # csv.\n            for i in ('scope', ):\n                prop = result_dict.get(i)\n                if prop:\n                    result_dict[i] = [s.strip() for s in prop.split(',')]\n                else:\n                    result_dict[i] = None\n\n            return result_dict\n        else:\n            return default"}
{"prompt": "def dump(obj, file, reducers=None, protocol=None):\n    \"\"\"Replacement for pickle.dump() using _LokyPickler.\"\"\"", "output": "\n    if reducers is None:\n        reducers = []\n    if protocol is None:\n        protocol = _LokyPickler.DEFAULT_PROTOCOL\n    with open(file, 'wb') as f:\n        pickle.dump(obj, f, reducers=reducers, protocol=protocol)\n\n", "solution": "\n    global _LokyPickler\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)"}
{"prompt": "def rule(cls, rulename=None, erase=False):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"", "output": "\n    def decorator(method):\n        setattr(cls, method.__name__, method)\n        if erase:\n            method = None\n        if rulename:\n            setattr(cls, rulename, method)\n        return method\n    return decorator\n\n", "solution": "\n    if not hasattr(cls, '_rules'):\n        raise TypeError(\n            \"%s didn't seems to be a BasicParser subsclasse\" % cls.__name__)\n    class_hook_list = cls._hooks\n    class_rule_list = cls._rules\n\n    def wrapper(f):\n        nonlocal rulename\n        add_method(cls)(f)\n        if rulename is None:\n            rulename = f.__name__\n        if not erase and (rulename in class_hook_list or rulename in class_rule_list):\n            raise TypeError(\"%s is already define has rule or hook\" % rulename)\n        if '.' not in rulename:\n            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename\n        set_one(class_rule_list, rulename, f)\n        return f\n    return wrapper"}
{"prompt": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"", "output": "\n    import urllib.request\n    import hashlib\n\n    root = os.path.expanduser(root)\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    if filename is None:\n        filename = os.path.basename(url)\n\n    fpath = os.path.join(root, filename)\n    if os.path.exists(fpath) and not md5:\n        return fpath\n\n    with urllib.request.urlopen(url) as response, open(f", "solution": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )"}
{"prompt": "def _in_buffer(self, position=None):\n        \"\"\" Returns whether the current cursor (or, if specified, a position) is\n            inside the editing region.\n        \"\"\"", "output": "\n        if position is None:\n            position = self.position\n        return position >= 0 and position < len(self.text)\n\n    def _move_cursor(self, direction=None):\n        \"\"\" Move the cursor in the specified direction.\n        \"\"\"\n        if direction is None:\n            direction = self.direction\n        if direction == 'up':\n            self.position -= 1\n        elif direction == 'down':\n            self.position += 1\n        elif direction == 'left':\n            self.position -= len(self.text)\n        elif", "solution": "\n        cursor = self._control.textCursor()\n        if position is None:\n            position = cursor.position()\n        else:\n            cursor.setPosition(position)\n        line = cursor.blockNumber()\n        prompt_line = self._get_prompt_cursor().blockNumber()\n        if line == prompt_line:\n            return position >= self._prompt_pos\n        elif line > prompt_line:\n            cursor.movePosition(QtGui.QTextCursor.StartOfBlock)\n            prompt_pos = cursor.position() + len(self._continuation_prompt)\n            return position >= prompt_pos\n        return False"}
{"prompt": "def split_type(self, typename):\n        \"\"\"\n        Given a potentially complex type, split it into its base type and specializers\n        \"\"\"", "output": "\n        # TODO: Implement this function\n        pass\n\ndef type_from_string(s: str) -> Type:\n    \"\"\"\n    Given a string representing a type, split it into its base type and specializers\n    \"\"\"\n    # TODO: Implement this function\n    pass\n\n", "solution": "\n\n        name = self._canonicalize_type(typename)\n        if '(' not in name:\n            return name, False, []\n\n        base, sub = name.split('(')\n        if len(sub) == 0 or sub[-1] != ')':\n            raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub)\n\n        sub = sub[:-1]\n\n        subs = sub.split(',')\n        return base, True, subs"}
{"prompt": "def get_response_code(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the HTTP response code in 'int'\n    \"\"\"", "output": "\n    import urllib.request\n    import time\n    \n    try:\n        with urllib.request.urlopen(url, timeout=timeout) as response:\n            return int(response.read().decode('utf-8'))\n    except:\n        return -1\n\n", "solution": "\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except HTTPError, e:\n        return e.getcode()\n    except Exception, _:\n        fail(\"Couldn't reach the URL '%s'\" % url)\n    else:\n        return req.getcode()"}
{"prompt": "def asAMP(cls):\n        \"\"\"\n        Returns the exception's name in an AMP Command friendly format.\n\n        For example, given a class named ``ExampleExceptionClass``, returns\n        ``\"EXAMPLE_EXCEPTION_CLASS\"``.\n        \"\"\"", "output": "\n        return cls.__name__\n\n    def __init__(self, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return \"AMP Command: \" + self.asAMP()\n\n    def __repr__(self):\n        return str(self)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.asAMP() == other.asAMP()\n        else:\n            ", "solution": "\n        parts = groupByUpperCase(cls.__name__)\n        return cls, \"_\".join(part.upper() for part in parts)"}
{"prompt": "def to_grayscale(img):\n    \"\"\"Convert PIL image to numpy grayscale array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (gray, alpha): both numpy arrays.\n    \"\"\"", "output": "\n    # Convert image to grayscale\n    gray = img.convert('L')\n\n    # Convert image to numpy alpha array\n    alpha = np.array(img)\n\n    return gray, alpha\n\n", "solution": "\n    gray = numpy.asarray(ImageOps.grayscale(img)).astype(numpy.float)\n\n    imbands = img.getbands()\n    alpha = None\n    if 'A' in imbands:\n        alpha = numpy.asarray(img.split()[-1]).astype(numpy.float)\n\n    return gray, alpha"}
{"prompt": "def _handle_options(self, options):\n        \"\"\"\n        Handle the data from the options.\n\n        :param options: The list of options from the rule.\n        :type options: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"", "output": "\n        domains = []\n        for option in options:\n            if option.startswith(\"domain:\"):\n                domain = option.split(\":\")[1]\n                domains.append(domain)\n        return domains\n\n    def _handle_recursive_rule(self, rule):\n        \"\"\"\n        Recursively handle the data from the rule.\n\n        :param rule: The rule to handle.\n        :type rule: dict\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"\n        domains = []\n", "solution": "\n\n        # We initiate a variable which will save our result\n        result = []\n\n        # We initiate the regex which will be used to extract the domain listed\n        # under the option domain=\n        regex_domain_option = r\"domain=(.*)\"\n\n        for option in options:\n            # We loop through the list of option.\n            try:\n                # We try to extract the list of domains from the currently read\n                # option.\n                domains = Regex(\n                    option, regex_domain_option, return_data=True, rematch=True, group=0\n                ).match()[-1]\n\n                if domains:\n                    # We could extract something.\n\n                    if self.aggressive:  # pragma: no cover\n                        result.extend(\n                            [\n                                x\n                                for x in domains.split(\"|\")\n                                if x and not x.startswith(\"~\")\n                            ]\n                        )\n                    else:\n                        # We return True.\n                        return True\n            except TypeError:\n                pass\n\n        # We return the result.\n        return result"}
{"prompt": "def to_fmt(self) -> fmt.indentable:\n    \"\"\"\n    Return an Fmt representation for pretty-printing\n    \"\"\"", "output": "\n    return fmt.indentable(str(self))\n\n", "solution": "\n    qual = \"scope\"\n    txt = fmt.sep(\" \", [qual])\n    name = self.show_name()\n    if name != \"\":\n        txt.lsdata.append(name)\n    if len(self._hsig) > 0 or len(self.mapTypeTranslate) > 0:\n        lsb = []\n        if len(self.mapTypeTranslate) > 0:\n            lsb.append(\"translate:\\n\")\n            lsb.append(fmt.end(\"\\n\", self.mapTypeTranslate.to_fmt()))\n        for k in sorted(self._hsig.keys()):\n            s = self._hsig[k]\n            lsb.append(fmt.end(\"\\n\", [s.to_fmt()]))\n        block = fmt.block(\":\\n\", \"\", fmt.tab(lsb))\n        txt.lsdata.append(block)\n    return txt"}
{"prompt": "def when_children_replaced ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being replaced on a\n            specified object.\n        \"\"\"", "output": "\n        self.listeners.setdefault(object, []).append((listener, remove))\n\n    def remove_listener ( self, listener, object ):\n        \"\"\" Removes a listener for a specified object.\n        \"\"\"\n        if object in self.listeners:\n            self.listeners[object].remove((listener, object))\n\n    def when_children_replaced ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being replaced on a\n            specified object.\n        \"\"\"\n        self.remove_listener (listener", "solution": "\n        object.on_trait_change( listener, \"subgraphs\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"clusters\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"nodes\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"edges\", remove = remove,\n                                dispatch = \"fast_ui\" )"}
{"prompt": "def tokenize_line(line):\n    \"\"\"\n    Tokenize a line:\n    * split tokens on whitespace\n    * treat quoted strings as a single token\n    * drop comments\n    * handle escaped spaces and comment delimiters\n    \"\"\"", "output": "\n    tokens = []\n    in_quotes = False\n    in_comment = False\n    for i in range(len(line)):\n        if line[i] == '\"' and not in_quotes:\n            in_quotes = True\n        elif line[i] == '\"' and in_quotes:\n            in_quotes = False\n        elif line[i] == '#' and not in_comment:\n            in_comment = True\n        elif line[i] == '#' and in_comment:\n            ", "solution": "\n    ret = []\n    escape = False\n    quote = False\n    tokbuf = \"\"\n    ll = list(line)\n    while len(ll) > 0:\n        c = ll.pop(0)\n        if c.isspace():\n            if not quote and not escape:\n                # end of token\n                if len(tokbuf) > 0:\n                    ret.append(tokbuf)\n\n                tokbuf = \"\"\n            elif quote:\n                # in quotes\n                tokbuf += c\n            elif escape:\n                # escaped space\n                tokbuf += c\n                escape = False\n            else:\n                tokbuf = \"\"\n\n            continue\n\n        if c == '\\\\':\n            escape = True\n            continue\n        elif c == '\"':\n            if not escape:\n                if quote:\n                    # end of quote\n                    ret.append(tokbuf)\n                    tokbuf = \"\"\n                    quote = False\n                    continue\n                else:\n                    # beginning of quote\n                    quote = True\n                    continue\n        elif c == ';':\n            if not escape:\n                # comment \n                ret.append(tokbuf)\n                tokbuf = \"\"\n                break\n            \n        # normal character\n        tokbuf += c\n        escape = False\n\n    if len(tokbuf.strip(\" \").strip(\"\\n\")) > 0:\n        ret.append(tokbuf)\n\n    return ret"}
{"prompt": "def tasks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`TasksOperations<azure.mgmt.containerregistry.v2018_09_01.operations.TasksOperations>`\n        \"\"\"", "output": "\n        return self.container_registry.tasks\n\n    def versions(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`VersionsOperations<azure.mgmt.containerregistry.v2018_09_01.operations.VersionsOperations>`\n        \"\"\"\n        return self.container_registry.versions\n\n    def containers(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`ContainersOperations<azure.mgmt.containerreg", "solution": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from .v2018_09_01.operations import TasksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def f_supports(self, data):\n        \"\"\"Checks if input data is supported by the parameter.\"\"\"", "output": "\n        # Your code here\n        pass\n\n    def f_is_compatible(self, data):\n        \"\"\"Checks if input data is compatible with the parameter.\"\"\"\n        # Your code here\n        pass\n\n    def f_is_accurate(self, data):\n        \"\"\"Checks if input data is accurate.\"\"\"\n        # Your code here\n        pass\n\n    def f_is_upright(self, data):\n        \"\"\"Checks if input data is upright.\"\"\"\n        # Your code here\n        pass\n\n    def f_is_downright(self,", "solution": "\n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)"}
{"prompt": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"", "output": "ensor` of shape `batch_shape`.\n  \"\"\"\n  # Compute the mean and variance of the unmasked time series.\n  unmasked_mean = tf.math.divide_no_nan(\n      tf.reduce_sum(time_series_tensor, axis=0),\n      tf.reduce_sum(~broadcast_mask, axis=0))\n  unmasked_variance = tf.math.divide_no_nan(\n      tf.reduce_sum(\n          tf.math.squared_difference(", "solution": "\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance"}
{"prompt": "def flush(self, full=False, all=False, force=False):\n        \"\"\"\n        Call ``.save()`` on all modified entities in the session. Use when you\n        want to flush changes to Redis, but don't want to lose your local\n        session cache.\n\n        See the ``.commit()`` method for arguments and their meanings.\n        \"\"\"", "output": "\n        modified = self.modified\n        if force:\n            modified = set()\n        modified.discard(self)\n        if full:\n            modified.add(self)\n        for entity in modified:\n            entity.flush(full=full, all=all, force=force)\n        if full or all:\n            self.save()\n\n    def commit(self, full=False, all=False, force=False):\n        \"\"\"\n        Call ``.save()`` on all modified entities in the session. Use when you\n        want to commit changes to Redis", "solution": "\n        self._init()\n\n        return self.save(*self.known.values(), full=full, all=all, force=force)"}
{"prompt": "def format_price_commas(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with commas\n    \"\"\"", "output": "\n    return \"{:,.0f}\".format(price)\n\n", "solution": "\n    if price is None:\n        return None\n    if price >= 0:\n        return jinja2.Markup('&pound;{:,.2f}'.format(price))\n    else:\n        return jinja2.Markup('-&pound;{:,.2f}'.format(-price))"}
{"prompt": "def _distribute_jobs(self):\n        \"\"\"\n        Distributes jobs from the active job set to any waiting get_job\n        callbacks.\n        \"\"\"", "output": "\n        while self.active_job_set:\n            job = self.active_job_set.pop()\n            job()\n\n    def _schedule_jobs(self):\n        \"\"\"\n        Schedules jobs from the get_job callbacks to the active job set.\n        \"\"\"\n        while self.get_job_callbacks:\n            callback = self.get_job_callbacks.pop()\n            callback()\n\n    def run(self):\n        \"\"\"\n        Runs the job set and schedules the jobs.\n\n        This method runs the job set and schedules", "solution": "\n\n        while (self._active_js.job_available()\n                and len(self._ready_callbacks) > 0):\n            job = self._active_js.get_job()\n            self._job_sources[job] = self._active_js\n            callback = self._ready_callbacks.popleft()\n            callback(job)"}
{"prompt": "def finalize(self, success_or_fail, message=''):\n        \"\"\"\n        :param success_or_fail: string that is 'success' or 'fail'\n        :param message:\n        \"\"\"", "output": "\n        if success_or_fail =='success':\n            self.result = True\n        else:\n            self.result = False\n        self.message = message\n\n", "solution": "\n        self.logit.debug('String OutputPorts: %s' % self.__string_output_ports)\n        if self.__string_output_ports:\n            with open(os.path.join(self.output_path, 'ports.json'), 'w') as opf:\n                json.dump(self.__string_output_ports, opf, indent=4)\n            self.logit.debug('Ports.json written to %s' % os.path.join(self.output_path, 'ports.json'))\n\n        with open(os.path.join(self.base_path, 'status.json'), 'w') as sf:\n            json.dump({'status': success_or_fail, 'reason': message}, sf, indent=4)"}
{"prompt": "def get_users_for_sis_course_id(self, sis_course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given sis course id.\n        \"\"\"", "output": "\n        # Your code here\n        pass\n\n", "solution": "\n        return self.get_users_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"prompt": "def sync_folder(self, path, bucket):\n        \"\"\"Syncs a local directory with an S3 bucket.\n     \n        Currently does not delete files from S3 that are not in the local directory.\n\n        path: The path to the directory to sync to S3\n        bucket: The name of the bucket on S3\n        \"\"\"", "output": "\n        import boto3\n        s3 = boto3.resource('s3')\n        bucket = s3.Bucket(bucket)\n        for obj in bucket.objects.all():\n            if obj.key!= path:\n                obj.delete()\n\n", "solution": "\n        bucket = self.conn.get_bucket(bucket)\n        local_files = self._get_local_files(path)\n        s3_files = self._get_s3_files(bucket)\n        for filename, hash in local_files.iteritems():\n            s3_key = s3_files[filename]\n            if s3_key is None:\n                s3_key = Key(bucket)\n                s3_key.key = filename\n                s3_key.etag = '\"!\"'\n            \n            if s3_key.etag[1:-1] != hash[0]:\n                s3_key.set_contents_from_filename(join(path, filename), md5=hash)"}
{"prompt": "def new(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"", "output": "\n        return type(self)(*args, **kwargs)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the object.\n        \"\"\"\n        return f\"{type(self).__name__}({', '.join(f'{k}={v!r}' for k, v in self.__dict__.items())})\"\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of the object.\n        \"\"\"\n        return str(self.__dict__)\n\n", "solution": "\n        inst = self.clazz()\n        self.storage.append(inst)\n        \n        # set all attributes with an initial default value\n        referential_attributes = dict()\n        for name, ty in self.attributes:\n            if name not in self.referential_attributes:\n                value = self.default_value(ty)\n                setattr(inst, name, value)\n            \n        # set all positional arguments\n        for attr, value in zip(self.attributes, args):\n            name, ty = attr\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n            \n        # set all named arguments\n        for name, value in kwargs.items():\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n        \n        if not referential_attributes:\n            return inst\n        \n        # batch relate referential attributes \n        for link in self.links.values():\n            if set(link.key_map.values()) - set(referential_attributes.keys()):\n                continue\n             \n            kwargs = dict()\n            for key, value in link.key_map.items():\n                kwargs[key] = referential_attributes[value]\n            \n            if not kwargs:\n                continue\n            \n            for other_inst in link.to_metaclass.query(kwargs):\n                relate(other_inst, inst, link.rel_id, link.phrase)\n        \n        for name, value in referential_attributes.items():\n            if getattr(inst, name) != value:\n                logger.warning('unable to assign %s to %s', name, inst)\n                \n        return inst"}
{"prompt": "def plotBrightLimitInV(gBright, pdf=False, png=False):\n  \"\"\"\n  Plot the bright limit of Gaia in V as a function of (V-I).\n\n  Parameters\n  ----------\n\n  gBright - The bright limit of Gaia in G\n  \"\"\"", "output": "\n\n  import matplotlib.pyplot as plt\n  import numpy as np\n\n  V = np.linspace(0, 100, 1000)\n  plt.plot(V, gBright, label='Gaia Bright Limit')\n\n  if pdf:\n    plt.savefig('gaia_bright_limit.pdf')\n  elif png:\n    plt.savefig('gaia_bright_limit.png')\n  else:\n    plt.show()\n\n", "solution": "\n  vmini=np.linspace(0.0,6.0,1001)\n  gminv=gminvFromVmini(vmini)\n  vBright=gBright-gminv\n\n  fig=plt.figure(figsize=(10,6.5))\n  plt.plot(vmini,vBright,'b-')\n  plt.xlabel('$(V-I)$')\n  plt.ylabel('Bright limit of Gaia in $V$')\n  plt.xlim(0,6)\n  plt.ylim(5,11)\n  plt.grid(which='both')\n  plt.title(\"Bright limit in $G$: {0}\".format(gBright))\n\n  if (pdf):\n    plt.savefig('VBandBrightLimit.pdf')\n  elif (png):\n    plt.savefig('VBandBrightLimit.png')\n  else:\n    plt.show()"}
{"prompt": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"", "output": "\n        if tablename in self.tables:\n            table = self.tables[tablename]\n        else:\n            table = Table(tablename, description, expectedrows)\n            self.tables[tablename] = table\n        return table\n\n    def _get_or_create_table_description(self, tablename, description, expectedrows=None):\n        \"\"\"Returns the description of a table, or if the table already exists, creates it.\"\"\"\n        table = self._all_get_or_create_table(tablename", "solution": "\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table"}
{"prompt": "def _align(self, axes, key_shape=None):\n        \"\"\"\n        Align local bolt array so that axes for iteration are in the keys.\n\n        This operation is applied before most functional operators.\n        It ensures that the specified axes are valid, and might transpose/reshape\n        the underlying array so that the functional operators can be applied\n        over the correct records.\n\n        Parameters\n        ----------\n        axes: tuple[int]\n            One or more axes that will be iterated over by a functional operator\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"", "output": "\"\"\"\n        if key_shape is None:\n            key_shape = self.key_shape\n\n        if not isinstance(axes, tuple):\n            axes = (axes,)\n\n        if len(axes) == 0:\n            return self\n\n        if len(axes) == 1:\n            if axes[0] < 0 or axes[0] >= len(self.shape):\n                raise IndexError(\"Axis out of bounds\")\n            return self.copy()\n\n        if len(axes) > len(self.shape):\n            raise IndexError", "solution": "\n\n        # ensure that the key axes are valid for an ndarray of this shape\n        inshape(self.shape, axes)\n\n        # compute the set of dimensions/axes that will be used to reshape\n        remaining = [dim for dim in range(len(self.shape)) if dim not in axes]\n        key_shape = key_shape if key_shape else [self.shape[axis] for axis in axes]\n        remaining_shape = [self.shape[axis] for axis in remaining]\n        linearized_shape = [prod(key_shape)] + remaining_shape\n\n        # compute the transpose permutation\n        transpose_order = axes + remaining\n\n        # transpose the array so that the keys being mapped over come first, then linearize keys\n        reshaped = self.transpose(*transpose_order).reshape(*linearized_shape)\n\n        return reshaped"}
{"prompt": "def info(self):\n        \"\"\" Returns a description of the trait.\"\"\"", "output": "\n        return f\"{self.name} is a {self.trait_type} trait with a value of {self.value}.\"\n\n", "solution": "\n        result = 'any of ' + repr(self.values)\n        if self._allow_none:\n            return result + ' or None'\n        return result"}
{"prompt": "def update_affinity_group(self, affinity_group_name, label,\n                              description=None):\n        \"\"\"\n        Updates the label and/or the description for an affinity group for the\n        specified subscription.\n\n        affinity_group_name:\n            The name of the affinity group.\n        label:\n            A name for the affinity group. The name can be up to 100 characters\n            in length.\n        description:\n            A description for the affinity group. The description can be up to\n            1024 characters in length.\n        \"\"\"", "output": "\n        # Update the label\n        self.affinity_groups[affinity_group_name]['label'] = label\n\n        # Update the description\n        if description is not None:\n            self.affinity_groups[affinity_group_name]['description'] = description\n\n    def get_affinity_group(self, affinity_group_name):\n        \"\"\"\n        Returns a dictionary containing the label and/or the description for the\n        specified affinity group.\n\n        affinity_group_name:\n            The name of the affinity group.\n        \"\"\"\n        return", "solution": "\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        _validate_not_none('label', label)\n        return self._perform_put(\n            '/' + self.subscription_id + '/affinitygroups/' +\n            _str(affinity_group_name),\n            _XmlSerializer.update_affinity_group_to_xml(label, description))"}
{"prompt": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"", "output": "\n        return {\n            \"process_name\": proc.name,\n            \"process_id\": proc.pid,\n            \"process_ctx\": ctx\n        }\n\n    def deserialize(data):\n        \"\"\"\n        Deserialize HWProcess instance\n\n        :param data: dictionary containing HWProcess data\n        \"\"\"\n        return HWProcess(**data)\n\n    return serialize, deserialize\n\n", "solution": "\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )"}
{"prompt": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"", "output": "\n  if tol is None:\n    return 0.0\n  else:\n    return tf.convert_to_tensor(tol, dtype=dtype, validate_args=validate_args)\n\n", "solution": "\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol"}
{"prompt": "def get_item_objects(self, query_params=None):\n        \"\"\"\n        Get the items for this checklist. Returns a list of ChecklistItem objects.\n        \"\"\"", "output": "\n        items = []\n        for item in self.items:\n            if query_params is not None:\n                item_params = {k: v for k, v in query_params.items() if k in item.params}\n                item = item.with_params(item_params)\n            items.append(item)\n        return items\n\n", "solution": "\n        card = self.get_card()\n        checklistitems_list = []\n        for checklistitem_json in self.get_items(query_params):\n            checklistitems_list.append(self.create_checklist_item(card.id, self.id, checklistitem_json))\n\n        return checklistitems_list"}
{"prompt": "def onTWriteCallback__init(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"", "output": "\n        self.sim = sim\n        self.sim.add_callback(self.__onTWriteCallback__)\n\n    def __onTWriteCallback__(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"\n        # Code to modify state of sim and trigger callback\n        pass\n\n", "solution": "\n        yield from self.onTWriteCallback(sim)\n        self.intf.t._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)\n        self.intf.o._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)"}
{"prompt": "def GetParam(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"", "output": "\r\n    if tag in params: return params[tag][param] if param in params[tag] else default\r\n    else: return default\r\n    \r\ndef GetAllParams(tag, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing all tag parameters\"\"\"\r\n    if tag in params: return params[tag] if isinstance(params[tag], list) else [params[tag]] if isinstance(params[tag], dict) else [] if params[tag] is None else [params[tag]] if isinstance(params[tag],", "solution": "\r\n    if tag.HasParam(param):\r\n        return tag.GetParam(param)\r\n    else:\r\n        if default == __SENTINEL:\r\n            raise KeyError\r\n        else:\r\n            return default"}
{"prompt": "def _connection_failed(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after connection failure detected.\"\"\"", "output": "\n        pass\n\n    def _send_message(self, message):\n        \"\"\"Sends a message over the network.\"\"\"\n        pass\n\n    def _receive_message(self):\n        \"\"\"Receives a message from the network.\"\"\"\n        pass\n\n    def _send_data(self, data):\n        \"\"\"Sends data over the network.\"\"\"\n        pass\n\n    def _receive_data(self):\n        \"\"\"Receives data from the network.\"\"\"\n        pass\n\n    def _send_ping(self, ping_id):\n        \"\"\"Sends", "solution": "\n        if not self._error:\n            LOG.error(\"Connection failed: %s\", str(error))\n            self._error = error"}
{"prompt": "def sequence_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use out-of-sequence message detection for messages\n        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.\n        \"\"\"", "output": "\n        return self.get_mic().is_out_of_sequence\n\n    def get_mic(self):\n        \"\"\"\n        Returns the security context's messagemic object.\n        \"\"\"\n        return self.mic_manager.get_mic(self.context)\n\n    def wrap(self, message):\n        \"\"\"\n        Wrap a message in a security context.\n\n        Args:\n            message: A string representing the message to be wrapped.\n\n        Returns:\n            A new security context with the wrapped message.\n        \"\"\"\n        mic = self.get_mic()\n", "solution": "\n        return (\n            self.flags & C.GSS_C_SEQUENCE_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def domain_to_fqdn(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"", "output": "\n    if proto is None:\n        proto = 'example.com'\n    return f\"{proto}.{domain}\"\n\n", "solution": "\n    from .generic import get_site_proto\n    proto = proto or get_site_proto()\n    fdqn = '{proto}://{domain}'.format(proto=proto, domain=domain)\n    return fdqn"}
{"prompt": "def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns a\n        serializable object for ``o``, or calls the base implementation (to\n        raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could implement\n        default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                return JSONEncoder.default(self, o)\n        \"\"\"", "output": "\n        raise TypeError(\"Unserializable object: %r\" % o)\n\n    def encode(self, o):\n        \"\"\"Encode the given object as a JSON string.\n\n        If the object is already a string, it is returned unchanged.\n        If the object is an iterable, the result is a list of strings\n        representing the JSON values of the iterable.\n\n        If the object is not iterable, the result is a string\n        representation of the object.\n\n        If the object cannot be encoded, a TypeError is raised.\n        \"\"\"\n        if is", "solution": "\n        if isinstance(o, datetime):\n            return http_date(o)\n        if isinstance(o, uuid.UUID):\n            return str(o)\n        if hasattr(o, '__html__'):\n            return text_type(o.__html__())\n        return _json.JSONEncoder.default(self, o)"}
{"prompt": "def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Merges two `Reservation`s.\n\n        Merge the specified `Reservation`s into a new `Reservation`. The two\n        `Reservation`s being merged must have same properties.\n\n        :param reservation_order_id: Order Id of the reservation\n        :type reservation_order_id: str\n        :param sources: Format of the resource id should be\n         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n        \"\"\"", "output": ".Reservation/resourceIds/{resourceId}\n        :type sources: str\n        :param custom_headers: Custom headers to be added to the request\n        :type custom_headers: dict\n        :param raw: If set to True, returns the direct response from the server. If set to False, returns the\n         parsed response object.\n        :type raw: bool\n        :param polling: If set to True, polls the server for new reservations. If set to False, does not poll.\n        :type polling: bool\n        :param operation_config: Additional", "solution": "\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _get_concatenation(extractors, text, *, ignore_whitespace=True):\n  \"\"\"Returns a concatenation ParseNode whose children are the nodes returned by each of the\n  methods in the extractors enumerable.\n\n  If ignore_whitespace is True, whitespace will be ignored and then attached to the child it\n  preceeded.\n  \"\"\"", "output": "\n  if ignore_whitespace:\n    text = text.replace(' ', '')\n  children = []\n  for extractor in extractors:\n    child = extractor(text)\n    children.append(child)\n    text = child.text\n  return ParseNode(children)\n\n", "solution": "\n  ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n\n  extractor, *remaining = extractors\n\n  child = _call_extractor(extractor, use_text)\n  child.add_ignored(ignored_ws)\n\n  # TODO: Should I set node.position = -len(text) for the case that ignored whitespace will cause\n  #       the first child's position to not be the whitespace, and therefore the concatenation's\n  #       position will be the first non-whitespace? I think not, but I'm adding this note in\n  #       case that causes an issue I'm not seeing at the moment.\n  node = ParseNode(ParseNodeType.concatenation, children=[child])\n\n  if remaining:\n    # child.consumed will include ignored whitespace, so we base the text we pass on on text rather\n    # than use_text.\n    return node.merged(_get_concatenation(remaining,\n                                          text[child.consumed:],\n                                          ignore_whitespace=ignore_whitespace))\n  else:\n    return node"}
{"prompt": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"", "output": "\n        # create a new column with the transformed x and y values\n        self.add_column(xnew, ynew)\n        # create a new column with the original x and y values\n        self.add_column(x, y)\n        # create a new column with the rotated x and y values\n        self.add_column(xnew, ynew)\n        # rotate the x and y columns in place\n        self.data[x] = self.data[x].apply(lambda x: (xnew, y))\n        self.data[xnew]", "solution": "\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])"}
{"prompt": "def _call(self, resource, params):\n        \"\"\"Call to get a resource.\n\n        :param method: resource to get\n        :param params: dict with the HTTP parameters needed to get\n            the given resource\n        \"\"\"", "output": "\n        if resource == 'get':\n            return self.get(**params)\n        elif resource == 'post':\n            return self.post(**params)\n        else:\n            raise ValueError('Invalid resource: {}'.format(resource))\n\n    def __call__(self, resource, params):\n        \"\"\"Call the API with the given resource and parameters.\n\n        :param resource: the resource to call\n        :param params: a dictionary with the parameters needed to call\n            the given resource\n        :return: the result of the call as a dictionary\n        \"\"\"\n", "solution": "\n        url = self.URL % {'base': self.base_url, 'resource': resource}\n\n        if self.api_token:\n            params[self.PKEY] = self.api_token\n\n        logger.debug(\"Redmine client requests: %s params: %s\",\n                     resource, str(params))\n\n        r = self.fetch(url, payload=params, verify=False)\n\n        return r.text"}
{"prompt": "def maintained_selection():\n    \"\"\"Maintain selection during context\n\n    Example:\n        >>> with maintained_selection():\n        ...     # Modify selection\n        ...     cmds.select('node', replace=True)\n        >>> # Selection restored\n\n    \"\"\"", "output": "\n    global SELECTION\n    SELECTION = True\n\n", "solution": "\n\n    previous_selection = cmds.ls(selection=True)\n    try:\n        yield\n    finally:\n        if previous_selection:\n            cmds.select(previous_selection,\n                        replace=True,\n                        noExpand=True)\n        else:\n            cmds.select(deselect=True,\n                        noExpand=True)"}
{"prompt": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"", "output": "`.\n\n    Returns:\n      A `tfd.Distribution` object representing the marginal distribution of the\n      GP over function values at `index_points`. The distribution has batch shape\n      `[b1,..., bB]` and event shape `[e, f1,..., fF]`.\n    \"\"\"\n    raise NotImplementedError\n\n", "solution": "\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')"}
{"prompt": "def _reset(self, index, total, percentage_step, length):\n        \"\"\"Resets to the progressbar to start a new one\"\"\"", "output": "\n        self.index = index\n        self.total = total\n        self.percentage_step = percentage_step\n        self.length = length\n        self.current_value = 0\n        self.start_time = time.time()\n\n    def update(self, value):\n        \"\"\"Updates the progressbar\"\"\"\n        self.current_value = value\n        self.percentage = int((self.current_value / self.total) * 100)\n        self.elapsed_time = time.time() - self.start_time\n        self.length =", "solution": "\n        self._start_time = datetime.datetime.now()\n        self._start_index = index\n        self._current_index = index\n        self._percentage_step = percentage_step\n        self._total = float(total)\n        self._total_minus_one = total - 1\n        self._length = length\n        self._norm_factor = total * percentage_step / 100.0\n        self._current_interval = int((index + 1.0) / self._norm_factor)"}
{"prompt": "def _swap_ops_from_edge(edge, layout):\n    \"\"\"Generate list of ops to implement a SWAP gate along a coupling edge.\"\"\"", "output": "\n    # Get the indices of the two nodes connected by the edge\n    node1_idx, node2_idx = edge\n    # Generate a list of ops to swap the nodes\n    swap_ops = []\n    for i in range(layout.num_nodes):\n        if i!= node1_idx and i!= node2_idx:\n            swap_ops.append(SWAP(i, node1_idx, node2_idx))\n    return swap_ops\n\n", "solution": "\n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    # TODO shouldn't be making other nodes not by the DAG!!\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]"}
{"prompt": "def frames(self, key=None, timeoutSecs=60, **kwargs):\n    if not (key is None or isinstance(key, (basestring, Key))):\n        raise Exception(\"frames: key should be string or Key type %s %s\" % (type(key), key))\n\n    params_dict = {\n        'find_compatible_models': 0,\n        'row_offset': 0, # is offset working yet?\n        'row_count': 5,\n    }\n    \"\"\"\n    Return a single Frame or all of the Frames in the h2o cluster.  The\n    frames are contained in a list called \"frames\" at the top level of the\n    result.  Currently the list is unordered.\n    TODO:\n    When find_compatible_models is implemented then the top level \n    dict will also contain a \"models\" list.\n    \"\"\"", "output": " the Video object.\n\n    If the key parameter is provided, the function will return the Frame(s) with the highest score(s) for that key.\n    If the key parameter is not provided, the function will return the highest scoring Frame(s) in the Video object.\n\n    If the Video object is empty or the timeoutSecs parameter is 0, the function will return an empty list.\n\n    :param key: A string or Key type representing the key to search for.\n    :param timeoutSecs: An integer representing the maximum number of seconds to wait for a frame to be available.\n", "solution": "\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'frames', False)\n    \n    # key can be type Key? (from h2o_xl) str(key) should return\n    if key:\n        if isinstance(key, Key):\n            keyStr = key.frame\n        else:\n            keyStr = key\n        result = self.do_json_request('3/Frames.json/' + keyStr, timeout=timeoutSecs, params=params_dict)\n    else:\n        result = self.do_json_request('3/Frames.json', timeout=timeoutSecs, params=params_dict)\n    return result"}
{"prompt": "def queryGitHubFromFile(self, filePath, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub GraphQL query from a file.\n\n        Can only be used with GraphQL queries.\n        For REST queries, see the 'queryGitHub' method.\n\n        Args:\n            filePath (str): A relative or absolute path to a file containing\n                a GraphQL query.\n                File may use comments and multi-line formatting.\n                .. _GitHub GraphQL Explorer:\n                   https://developer.github.com/v4/explorer/\n            gitvars (Optional[Dict]): All query variables.\n                Defaults to empty.\n                GraphQL Only.\n            verbosity (Optional[int]): Changes output verbosity levels.\n                If < 0, all extra printouts are suppressed.\n                If == 0, normal print statements are displayed.\n                If > 0, additional status print statements are displayed.\n                Defaults to 0.\n            **kwargs: Keyword arguments for the 'queryGitHub' method.\n\n        Returns:\n            Dict: A JSON style dictionary.\n\n        \"\"\"", "output": "docs/graphql/reference/explorers.html\n            gitvars (dict): A dictionary of variables to be used in the query.\n            verbosity (int): The level of verbosity.\n\n        Returns:\n            dict: A dictionary containing the results of the query.\n        \"\"\"\n        if verbosity > 0:\n            print(f\"Submitting GraphQL query from {filePath}...\")\n\n        with open(filePath, \"r\") as f:\n            query = f.read()\n\n        if \"variables\" in gitvars:\n            query = query", "solution": "\n        gitquery = self._readGQL(filePath, verbose=(verbosity >= 0))\n        return self.queryGitHub(gitquery, gitvars=gitvars, verbosity=verbosity, **kwargs)"}
{"prompt": "def get_revision(self, location):\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"", "output": "\n        max_revision = 0\n        for file in os.listdir(location):\n            if os.path.isfile(os.path.join(location, file)):\n                revision = 0\n                with open(os.path.join(location, file), 'r') as f:\n                    for line in f:\n                        if line.startswith('+'):\n                            revision += 1\n                        else:\n                            break\n                if revision > max_revision:\n                    max_revision = revision\n        return max_revision\n\n", "solution": "\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, files in os.walk(location):\n            if self.dirname not in dirs:\n                dirs[:] = []\n                continue    # no sense walking uncontrolled subdirs\n            dirs.remove(self.dirname)\n            entries_fn = os.path.join(base, self.dirname, 'entries')\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = self._get_svn_url_rev(base)\n\n            if base == location:\n                base_url = dirurl + '/'   # save the root url\n            elif not dirurl or not dirurl.startswith(base_url):\n                dirs[:] = []\n                continue    # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return revision"}
{"prompt": "def scientific_notation_elements(self, value, locale):\n        \"\"\" Returns normalized scientific notation components of a value.\"\"\"", "output": "\n        if value == 0:\n            return [0, 0, 0]\n        exponent = int(math.floor(math.log10(abs(value))))\n        mantissa = value / 10**exponent\n        return [int(mantissa * 10**6), int(exponent), int(mantissa * 10**3)]\n\n    def scientific_notation_string(self, value, locale):\n        \"\"\" Returns a string representation of a value in scientific notation with the specified locale.\"\"\"\n        if value == 0:\n            return \"0\"\n        comp", "solution": "\n        # Normalize value to only have one lead digit.\n        exp = value.adjusted()\n        value = value * get_decimal_quantum(exp)\n        assert value.adjusted() == 0\n\n        # Shift exponent and value by the minimum number of leading digits\n        # imposed by the rendering pattern. And always make that number\n        # greater or equal to 1.\n        lead_shift = max([1, min(self.int_prec)]) - 1\n        exp = exp - lead_shift\n        value = value * get_decimal_quantum(-lead_shift)\n\n        # Get exponent sign symbol.\n        exp_sign = ''\n        if exp < 0:\n            exp_sign = babel.numbers.get_minus_sign_symbol(locale)\n        elif self.exp_plus:\n            exp_sign = babel.numbers.get_plus_sign_symbol(locale)\n\n        # Normalize exponent value now that we have the sign.\n        exp = abs(exp)\n\n        return value, exp, exp_sign"}
{"prompt": "def difflag1(self):\n        \"\"\"\n        Conduct a diff-1 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"", "output": "\n        return H2OFrame._expr(expr=ExprNode(\"difflag\", self, -1))\n\n    def difflag2(self):\n        \"\"\"\n        Conduct a diff-2 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the two previous-row elements in the same frame.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"difflag\", self, -2))\n\n    def difflag3(self):\n        \"\"\"", "solution": "\n        if self.ncols > 1:\n            raise H2OValueError(\"Only single-column frames supported\")\n        if self.types[self.columns[0]] not in {\"real\", \"int\", \"bool\"}:\n            raise H2OValueError(\"Numeric column expected\")\n        fr = H2OFrame._expr(expr=ExprNode(\"difflag1\", self), cache=self._ex._cache)\n        return fr"}
{"prompt": "async def _run_spider_hook(self, hook_func):\n        \"\"\"\n        Run hook before/after spider start crawling\n        :param hook_func: aws function\n        :return:\n        \"\"\"", "output": "\n        await hook_func(self)\n\n    def start_spider(self, spider):\n        \"\"\"\n        Start spider\n        :param spider: Spider object\n        :return:\n        \"\"\"\n        self.spider = spider\n        self.hook_func = spider.hook_func\n        self.hook = spider.hook\n        self.start_time = time.time()\n\n    async def stop_spider(self):\n        \"\"\"\n        Stop spider\n        :return:\n        \"\"\"\n        self.hook = None\n        self.hook_func", "solution": "\n        if callable(hook_func):\n            try:\n                aws_hook_func = hook_func(weakref.proxy(self))\n                if isawaitable(aws_hook_func):\n                    await aws_hook_func\n            except Exception as e:\n                self.logger.error(f'<Hook {hook_func.__name__}: {e}')"}
{"prompt": "def save(self, name=None, output='png', dirc=None):\n        \"\"\"Saves Bloch sphere to file of type ``format`` in directory ``dirc``.\n        Args:\n            name (str):\n                Name of saved image. Must include path and format as well.\n                i.e. '/Users/Paul/Desktop/bloch.png'\n                This overrides the 'format' and 'dirc' arguments.\n            output (str):\n                Format of output image.\n            dirc (str):\n                Directory for output images. Defaults to current working directory.\n        \"\"\"", "output": " to current directory.\n        Returns:\n            str:\n                Path to saved image.\n        \"\"\"\n        if name is None:\n            name = f\"{self.name}_{self.format}\"\n        path = os.path.join(dirc, name)\n        # Save image to file\n        #...\n        return path\n\n", "solution": "\n\n        self.render()\n        if dirc:\n            if not os.path.isdir(os.getcwd() + \"/\" + str(dirc)):\n                os.makedirs(os.getcwd() + \"/\" + str(dirc))\n        if name is None:\n            if dirc:\n                self.fig.savefig(os.getcwd() + \"/\" + str(dirc) + '/bloch_' +\n                                 str(self.savenum) + '.' + output)\n            else:\n                self.fig.savefig(os.getcwd() + '/bloch_' + str(self.savenum) +\n                                 '.' + output)\n        else:\n            self.fig.savefig(name)\n        self.savenum += 1\n        if self.fig:\n            plt.close(self.fig)"}
{"prompt": "def rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some counts about it.\n     \"\"\"", "output": "\n     # create a KBA s3 path for the work unit\n     s3_path = f\"s3://{work_unit.bucket_name}/{work_unit.key}\"\n     \n     # create a rejester object for the work unit\n     rejester = KBA().get_object(s3_path)\n     \n     # fetch the work unit from the rejester\n     fetch_work_unit(rejester)\n     \n     # save some counts about the work unit\n     work_unit.num_objects_fetched =", "solution": "\n     #fname = 'verify-chunks-%d-%d' % (os.getpid(), time.time())\n     fname = work_unit.key.strip().split('/')[-1]\n     \n     output_dir_path = work_unit.data.get('output_dir_path', '/mnt')\n     u = uuid.uuid3(uuid.UUID(int=0), work_unit.key.strip())\n     path1 = u.hex[0]\n     path2 = u.hex[1]\n     fpath = os.path.join(output_dir_path, path1, path2, fname)\n     if not os.path.exists(os.path.dirname(fpath)):\n          os.makedirs(os.path.dirname(fpath))\n\n     output = gzip.open(fpath + '-out.gz', 'wb')\n\n     expected_si_count = int(fname.split('-')[1])\n\n     max_tries = 20\n     tries = 0\n     while tries < max_tries:\n          try:\n               exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, stream_ids = \\\n                   attempt_fetch(work_unit, fpath)\n               if si_count != expected_si_count:\n                    print 'retrying because si_count = %d != %d expected_si_count' % (si_count, expected_si_count)\n                    sys.stdout.flush()\n                    tries += 1\n                    continue\n               else:\n                    print 'succeeded in reading si_count = %d' % (si_count,)\n                    sys.stdout.flush()\n               output.write( '%s\\t%d\\t%d\\t%d\\t%d\\t%s\\t%s\\n' % (\n                         exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, \n                         work_unit.key.strip(), ','.join(['%s|%s' % tup for tup in stream_ids])) )\n               break\n          except Exception, exc:\n               print 'broken?'\n               print traceback.format_exc(exc)\n               sys.stdout.flush()\n               tries += 1\n               output.write(traceback.format_exc(exc))\n\n     output.close()"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        return self.args\n\n    def get_help(self):\n        \"\"\"\n        Extracts the help message for this CLI\n        \"\"\"\n        return self.help\n\n", "solution": "\n\n        AlarmModify.get_arguments(self)\n        self._alarm_id = self.args.alarm_id if self.args.alarm_id is not None else None\n        self.get_api_parameters()"}
{"prompt": "def html_tags_for(self, asset_type, *args, **kwargs):\n        \"\"\"Return html tags for urls of asset_type\n        \"\"\"", "output": "\n        if asset_type == 'image':\n            return ['<img src=\"{}\" alt=\"{}\" width=\"{}\" height=\"{}\"/>'.format(url, asset_type, *args, **kwargs) for url in self.image_urls]\n        elif asset_type =='script':\n            return ['<script src=\"{}\" type=\"{}\"></script>'.format(url, asset_type, *args, **kwargs) for url in self.script_urls]\n        elif asset_type == 'link':\n            return ['", "solution": "\n        html = []\n        for ref in self.depends:\n            html.append(self._ref(ref).html_tags_for(asset_type, *args, **kwargs))\n        if asset_type in self.typed_bundles:\n            html.append(render_asset_html_tags(asset_type, self.urls_for_self(asset_type, *args, **kwargs)))\n        return \"\\n\".join(html)"}
{"prompt": "def read_key(self, key, bucket_name=None):\n        \"\"\"\n        Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"", "output": "\n        if bucket_name is None:\n            bucket_name = self.bucket_name\n\n        # Create a new S3 client\n        s3_client = boto3.client('s3')\n\n        # Get the contents of the file\n        response = s3_client.get_object(Bucket=bucket_name, Key=key)\n\n        # Read the contents of the file\n        contents = response['Body'].read().decode('utf-8')\n\n        return contents\n\n    def write_key(self, key, contents, bucket_", "solution": "\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')"}
{"prompt": "def _map_type_to_dict(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding dictionary in root.\n\n        \"\"\"", "output": "\n        if type_name == 'RESULT':\n            return {'status': self.status, 'timestamp': self.timestamp, 'data': self.data}\n        else:\n            return {}\n\n    def __repr__(self):\n        return f\"<Result {self.status} {self.timestamp} {self.data}>\"\n\n", "solution": "\n        root = self._root_instance\n\n        if type_name == RESULT:\n            return root._results\n        elif type_name == PARAMETER:\n            return root._parameters\n        elif type_name == DERIVED_PARAMETER:\n            return root._derived_parameters\n        elif type_name == CONFIG:\n            return root._config\n        elif type_name == LEAF:\n            return root._other_leaves\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"prompt": "def create(self, public=False, **kwargs):\n        \"\"\"Creates the device. Attempts to create private devices by default,\n        but if public is set to true, creates public devices.\n\n        You can also set other default properties by passing in the relevant information.\n        For example, setting a device with the given nickname and description::\n\n            dev.create(nickname=\"mydevice\", description=\"This is an example\")\n\n        Furthermore, ConnectorDB supports creation of a device's streams immediately,\n        which can considerably speed up device setup::\n\n            dev.create(streams={\n                \"stream1\": {\"schema\": '{\\\"type\\\":\\\"number\\\"}'}\n            })\n\n        Note that the schema must be encoded as a string when creating in this format.\n        \"\"\"", "output": "=True)\n\n        Returns:\n            The newly created device object.\n        \"\"\"\n        if public:\n            # Create public device\n            return PublicDevice(**kwargs)\n        else:\n            # Create private device\n            return PrivateDevice(**kwargs)\n\n    def __repr__(self):\n        return f\"<PrivateDevice(name='{self.name}', nickname='{self.nickname}', description='{self.description}', streams=True)>\"\n\n", "solution": "\n        kwargs[\"public\"] = public\n        self.metadata = self.db.create(self.path, kwargs).json()"}
{"prompt": "def _check_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"", "output": "\n        if not node.__init__:\n            raise ValueError(\"The node does not have an __init__ method.\")\n        for ancestor in node.__class__.__mro__:\n            if ancestor.__name__ == \"object\":\n                continue\n            if hasattr(node, ancestor.__name__):\n                continue\n            if ancestor.__name__ in node.__dict__:\n                continue\n            raise ValueError(f\"The node does not have an __init__ method for {ancestor.__name__}.\")\n\n    def _check_", "solution": "\n        if not self.linter.is_message_enabled(\n            \"super-init-not-called\"\n        ) and not self.linter.is_message_enabled(\"non-parent-init-called\"):\n            return\n        klass_node = node.parent.frame()\n        to_call = _ancestors_to_call(klass_node)\n        not_called_yet = dict(to_call)\n        for stmt in node.nodes_of_class(astroid.Call):\n            expr = stmt.func\n            if not isinstance(expr, astroid.Attribute) or expr.attrname != \"__init__\":\n                continue\n            # skip the test if using super\n            if (\n                isinstance(expr.expr, astroid.Call)\n                and isinstance(expr.expr.func, astroid.Name)\n                and expr.expr.func.name == \"super\"\n            ):\n                return\n            try:\n                for klass in expr.expr.infer():\n                    if klass is astroid.Uninferable:\n                        continue\n                    # The infered klass can be super(), which was\n                    # assigned to a variable and the `__init__`\n                    # was called later.\n                    #\n                    # base = super()\n                    # base.__init__(...)\n\n                    if (\n                        isinstance(klass, astroid.Instance)\n                        and isinstance(klass._proxied, astroid.ClassDef)\n                        and is_builtin_object(klass._proxied)\n                        and klass._proxied.name == \"super\"\n                    ):\n                        return\n                    if isinstance(klass, objects.Super):\n                        return\n                    try:\n                        del not_called_yet[klass]\n                    except KeyError:\n                        if klass not in to_call:\n                            self.add_message(\n                                \"non-parent-init-called\", node=expr, args=klass.name\n                            )\n            except astroid.InferenceError:\n                continue\n        for klass, method in not_called_yet.items():\n            cls = node_frame_class(method)\n            if klass.name == \"object\" or (cls and cls.name == \"object\"):\n                continue\n            self.add_message(\"super-init-not-called\", args=klass.name, node=node)"}
{"prompt": "def analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Calculates average Fano Factor of a network.\n\n        :param traj:\n\n            Trajectory container\n\n            Expects:\n\n            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons\n\n            Adds:\n\n            `results.statistics.mean_fano_factor`: Average Fano Factor\n\n        :param network:\n\n            The BRIAN network\n\n        :param current_subrun:\n\n            BrianParameter\n\n        :param subrun_list:\n\n            Upcoming subruns, analysis is only performed if subruns is empty,\n            aka the final subrun has finished.\n\n        :param network_dict:\n\n            Dictionary of items shared among componetns\n\n        \"\"\"", "output": " run number\n\n        :param subrun_list:\n\n            List of subruns\n\n        :param network_dict:\n\n            Dictionary of network connections\n\n        :return:\n\n            None\n\n        \"\"\"\n        # Calculate average Fano Factor\n        fano_factor = traj.results.monitors.spikes_e.get_fano_factor(network)\n        network_dict[current_subrun]['mean_fano_factor'] = np.mean(fano_factor)\n\n        # Add subrun to list\n        subrun_list.append(current", "solution": "\n        #Check if we finished all subruns\n        if len(subrun_list)==0:\n            spikes_e = traj.results.monitors.spikes_e\n\n            time_window = traj.parameters.analysis.statistics.time_window\n            start_time = traj.parameters.simulation.durations.initial_run\n            end_time = start_time+traj.parameters.simulation.durations.measurement_run\n            neuron_ids = traj.parameters.analysis.statistics.neuron_ids\n\n            mean_ff = self._compute_mean_fano_factor(\n                neuron_ids, spikes_e, time_window, start_time, end_time)\n\n            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '\n                                                                      'Factor over all '\n                                                                      'exc neurons')\n\n            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))"}
{"prompt": "def _get_properties(config):\n    \"\"\"Returns a tuple of property classes and names.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"", "output": "\n    properties = []\n    for key, value in config.items():\n        if isinstance(value, str):\n            properties.append(value)\n        elif isinstance(value, dict):\n            properties.extend(_get_properties(value))\n        else:\n            properties.append(key)\n    return tuple(properties)\n\n", "solution": "\n    property_classes = {BUILTIN_PROPERTY}\n    property_names = set()  # Not returning 'property', it has its own check.\n    if config is not None:\n        property_classes.update(config.property_classes)\n        property_names.update(\n            (prop.rsplit(\".\", 1)[-1] for prop in config.property_classes)\n        )\n    return property_classes, property_names"}
{"prompt": "def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list:\n        \"\"\"\n        Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent passes incoming utterances batch through skills filter,\n        agent skills, skills processor. Batch of dialog IDs can be provided, in\n        other case utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        \"\"\"", "output": ".\n\n        Returns:\n            Batch of responses corresponding to incoming utterances.\n        \"\"\"\n        responses_batch = []\n        for utterance, utterance_id in zip(utterances_batch, utterances_ids):\n            if utterance_id is not None:\n                dialog_id = utterance_id\n            else:\n                dialog_id = utterance.dialog_id\n            response = self.agent.process_utterance(utterance, dialog_id)\n            responses_batch.append(response)\n        return responses_batch", "solution": "\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n        batch_history = [self.history[utt_id] for utt_id in ids]\n        responses = []\n\n        filtered = self.skills_filter(utterances_batch, batch_history)\n\n        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):\n            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]\n\n            if skill_i_utt_indexes:\n                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]\n                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]\n                res = [(None, 0.)] * batch_size\n                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)\n\n                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):\n                    res[i] = (predicted, confidence)\n\n                responses.append(res)\n\n        responses = self.skills_processor(utterances_batch, batch_history, *responses)\n\n        return responses"}
{"prompt": "def _add_label_from_dict(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a dictionary.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        label = query_params.get('label')\n        if label is not None:\n            self.label = label\n        return self\n\n    def _add_label_from_file(self, filename):\n        \"\"\"\n        Add a label to this card, from a file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            for line in f:\n                if line.startswith('label:'):\n                    label = line.split(':')[1].strip", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri + '/labels',\n            http_method='POST',\n            query_params=query_params or {}\n        )"}
{"prompt": "def diff_lorenz(value_array, sigma, beta, rho):\n    \"\"\"The Lorenz attractor differential equation\n\n    :param value_array: 3d array containing the x,y, and z component values.\n    :param sigma: Constant attractor parameter\n    :param beta: FConstant attractor parameter\n    :param rho: Constant attractor parameter\n\n    :return: 3d array of the Lorenz system evaluated at `value_array`\n\n    \"\"\"", "output": "\n    x, y, z = value_array\n    dxdt = sigma * (x - y)\n    dydt = x * (rho - z) - y\n    dzdt = x * y - beta * z\n    return np.array([dxdt, dydt, dzdt])\n\n", "solution": "\n    diff_array = np.zeros(3)\n    diff_array[0] = sigma * (value_array[1]-value_array[0])\n    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]\n    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]\n\n    return diff_array"}
{"prompt": "def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"", "output": "\n        topic = pahoMessage.topic\n        if topic == \"device\":\n            # Extract source device information from message\n            source_name = pahoMessage.payload\n            # Call registered device command callback with source device information\n            self.device_commands[source_name].callback(source_name, pahoMessage.payload)\n\n    def onConnected(self, client, userdata, flags, rc):\n        \"\"\"\n        Callback function for when the gateway connects to the device.\n        \"\"\"\n        # Call registered device command callbacks with connection information\n        for", "solution": "\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)"}
{"prompt": "def wait_next_block_factory(app, timeout=None):\n    \"\"\"Creates a `wait_next_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a new block to appear.\n\n    :param app: the app-instance the function should work for\n    :param timeout: timeout in seconds\n    \"\"\"", "output": "\n    import time\n\n    def wait_next_block():\n        while True:\n            app.wait_next_block(timeout)\n\n    return wait_next_block\n\n", "solution": "\n\n    chain = app.services.chain\n\n    # setup new block callbacks and events\n    new_block_evt = gevent.event.Event()\n\n    def _on_new_block(app):\n        log.DEV('new block mined')\n        new_block_evt.set()\n    chain.on_new_head_cbs.append(_on_new_block)\n\n    def wait_next_block():\n        bn = chain.chain.head.number\n        chain.consensus_manager.log('waiting for new block', block=bn)\n        new_block_evt.wait(timeout)\n        new_block_evt.clear()\n        if chain.chain.head.number > bn:\n            chain.consensus_manager.log('new block event', block=chain.chain.head.number)\n        elif chain.chain.head.number == bn:\n            chain.consensus_manager.log('wait_next_block timed out', block=bn)\n\n    return wait_next_block"}
{"prompt": "def _set_affiliation(self, v, load=False):\n        \"\"\"\n    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_affiliation is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_affiliation() directly.\n    \"\"\"", "output": "\n        if load:\n            self.value = v\n        else:\n            self.value = self.get_default_value()\n            self.value = v\n\n    affiliation = property(_get_affiliation, _set_affiliation)\n\n    def _get_default_value(self):\n        \"\"\"\n    Returns the default value for this variable, mapped from YANG variable /universe/individual/affiliation (identityref)\n    \"\"\"\n        return {\n            'name': '',\n            'address': '',\n            'city': '',\n            ", "solution": "\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=RestrictedClassType(\n                    base_type=unicode,\n                    restriction_type=\"dict_key\",\n                    restriction_arg={\n                        u\"napalm-star-wars:EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"napalm-star-wars:REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                    },\n                ),\n                is_leaf=True,\n                yang_name=\"affiliation\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                namespace=\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                defining_module=\"napalm-star-wars\",\n                yang_type=\"identityref\",\n                is_config=True,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": "}
{"prompt": "def run(self, cmd, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `cmd' using builtin function eval\n        and if that builtin exec.  Arguments `globals_' and `locals_'\n        are the dictionaries to use for local and global variables. By\n        default, the value of globals is globals(), the current global\n        variables. If `locals_' is not given, it becomes a copy of\n        `globals_'.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n        . Also see `run_eval' if what you want to run is an\n        run_eval'able expression have that result returned and\n        `run_call' if you want to debug function run_call.\n        \"\"\"", "output": "run() returns a tuple of two values: the first value\n        is the result of the debugger run, and the second value is a\n        list of strings representing the lines of code that were\n        executed during the debugger run.\n\n        \"\"\"\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        try:\n            result = eval(cmd, globals_, locals_)\n        except:\n            result = None\n        lines = []\n        try:\n            exec", "solution": "\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(cmd, types.CodeType):\n            self.eval_string = cmd\n            cmd = cmd+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(cmd, globals_, locals_)\n        except SyntaxError:\n            try:\n                exec(cmd, globals_, locals_)\n            except DebuggerQuit:\n                pass\n            except DebuggerQuit:\n                pass\n            pass\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return retval"}
{"prompt": "def make_links_absolute(self, base_url=None, resolve_base_href=True,\n                            handle_failures=None):\n        \"\"\"\n        Make all links in the document absolute, given the\n        ``base_url`` for the document (the full URL where the document\n        came from), or if no ``base_url`` is given, then the ``.base_url``\n        of the document.\n\n        If ``resolve_base_href`` is true, then any ``<base href>``\n        tags in the document are used *and* removed from the document.\n        If it is false then any such tag is ignored.\n\n        If ``handle_failures`` is None (default), a failure to process\n        a URL will abort the processing.  If set to 'ignore', errors\n        are ignored.  If set to 'discard', failing URLs will be removed.\n        \"\"\"", "output": " any links within those tags are\n        resolved to their absolute URLs.\n\n        If handle_failures is provided, then any errors encountered while\n        resolving links will be raised as a ``LinkResolverError``.\n\n        Returns the modified document as a string.\n        \"\"\"\n        if base_url is None:\n            base_url = self.base_url\n\n        if handle_failures is None:\n            handle_failures = LinkResolverError\n\n        # First, we need to find all the links in the document\n        links = []\n        for line in self.", "solution": "\n        if base_url is None:\n            base_url = self.base_url\n            if base_url is None:\n                raise TypeError(\n                    \"No base_url given, and the document has no base_url\")\n        if resolve_base_href:\n            self.resolve_base_href()\n\n        if handle_failures == 'ignore':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return href\n        elif handle_failures == 'discard':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return None\n        elif handle_failures is None:\n            def link_repl(href):\n                return urljoin(base_url, href)\n        else:\n            raise ValueError(\n                \"unexpected value for handle_failures: %r\" % handle_failures)\n\n        self.rewrite_links(link_repl)"}
{"prompt": "def signin_card(card: SigninCard) -> Attachment:\n        \"\"\"\n        Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative\n        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.\n        :param card:\n        :return:\n        \"\"\"", "output": "\n        if not isinstance(card, SigninCard):\n            raise TypeError(\"Input must be a SigninCard object.\")\n        if card.channel_type == \"private\":\n            return Attachment(\n                \"https://www.example.com/signin_card.jpg\",\n                \"Sign In\",\n                \"Please click on the link to view the signin card.\",\n                \"https://www.example.com/signin_card_confirmation.jpg\",\n                \"Sign In\",\n                \"Confirm your signin.\",\n                \"https://www.", "solution": "\n        if not isinstance(card, SigninCard):\n            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.signin_card,\n                          content=card)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Clear GDoc Spreadsheet by sending empty csv file.\n        \"\"\"", "output": "\n        subprocess.run(['gdoc','spreadsheet', '--clear', '--name', 'gdoc.csv'])\n\n", "solution": "\n        empty_file_path = os.path.join(self.temp_path, 'empty.csv')\n        try:\n            empty_file = open(empty_file_path, 'w')\n            empty_file.write(',')\n            empty_file.close()\n        except IOError as e:\n            raise PODocsError(e)\n\n        self._upload_file_to_gdoc(empty_file_path, content_type='text/csv')\n\n        os.remove(empty_file_path)"}
{"prompt": "def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):\n        \"\"\"\n        Add an item to the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.\n        :param int quantity: The quantity of item to be added.\n        :return: A response having added an item to the current basket.\n        :rtype: requests.Response\n        \"\"\"", "output": "\n        if quantity == 1:\n            response = requests.post(f\"{self.BASKET_URL}/items\", json={\"item_id\": item.id, \"variant\": variant})\n        else:\n            response = requests.post(f\"{self.BASKET_URL}/items/{item.id}\", json={\"item_id\": item.id, \"variant\": variant, \"quantity\": quantity})\n        return response\n\n    def remove_item_from_basket(self, item_id, variant=V", "solution": "\n        item_type = item.type\n\n        if item_type == 'Pizza':\n            return self.add_pizza_to_basket(item, variant, quantity)\n        elif item_type == 'Side':\n            return self.add_side_to_basket(item, quantity)\n        return None"}
{"prompt": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"", "output": "\n        stack = []\n        for char in expression:\n            if char == '(':\n                stack.append(char)\n            elif char == ')':\n                if not stack:\n                    raise ValueError(\"Mismatched parentheses\")\n                stack.pop()\n            elif not char.isalnum():\n                raise ValueError(\"Invalid character in expression\")\n        if stack:\n            raise ValueError(\"Mismatched parentheses\")\n        return True\n\n", "solution": "\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)"}
{"prompt": "def plot(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"", "output": "\n        if fig is None:\n            fig, ax = plt.subplots()\n        else:\n            ax = fig.axes[0]\n        x = np.arange(len(self.data))\n        ax.plot(x, self.data)\n        ax.set_xlabel('Time')\n        ax.set_ylabel('Data')\n        ax.set_title('Model Prediction')\n        return fig\n\n", "solution": "\n        if not fig:\n            fig = plt.gcf()\n\n        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n        ax = fig.add_subplot(1, 1, 1)\n\n        sorted_overlapping_ports = sorted(\n            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],\n            key=lambda x: x[1])\n\n        yticks_labels = []\n        yticks = []\n        xticks_labels = []\n        xticks = []\n\n        # Plot configuration\n        height = 0.9\n\n        i = 0\n        # T_OL\n        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +\n                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))\n        for p, c in sorted_overlapping_ports:\n            ax.barh(i, c, height, align='center', color=colors.pop(),\n                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')\n            if i == len(sorted_overlapping_ports) - 1:\n                ax.text(c / 2.0, i, '$T_\\mathrm{OL}$', ha='center', va='center')\n            yticks_labels.append(p)\n            yticks.append(i)\n            i += 1\n        xticks.append(sorted_overlapping_ports[-1][1])\n        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))\n\n        # T_nOL + memory transfers\n        y = 0\n        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \\\n                 [(119. / 255, 194. / 255., 255. / 255.)]\n        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:\n            ax.barh(i, v, height, y, align='center', color=colors.pop())\n            ax.text(y + v / 2.0, i, '$T_\\mathrm{' + k + '}$', ha='center', va='center')\n            xticks.append(y + v)\n            xticks_labels.append('{:.1f}'.format(y + v))\n            y += v\n        yticks_labels.append('LD')\n        yticks.append(i)\n\n        ax.tick_params(axis='y', which='both', left='off', right='off')\n        ax.tick_params(axis='x', which='both', top='off')\n        ax.set_xlabel('t [cy]')\n        ax.set_ylabel('execution port')\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticks_labels)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xticks_labels, rotation='vertical')\n        ax.xaxis.grid(alpha=0.7, linestyle='--')\n        fig.savefig(self._args.ecm_plot)"}
{"prompt": "def reduceCnf(cnf):\n    \"\"\"\n    I just found a remarkably large bug in my SAT solver and found an\n    interesting solution.\n    Remove all b | -b\n    (-b | b) & (b | -a) & (-b | a) & (a | -a)\n    becomes\n    (b | -a) & (-b | a)\n\n    Remove all (-e) & (-e)\n    (-e | a) & (-e | a) & (-e | a) & (-e | a)\n    becomes\n    (-e | a)\n    (-b | b | c) becomes nothing, not (c)\n    \"\"\"", "output": "    (a | a)\n\n    Replace all (p | q) & (p | -p) & (q | -q) & (-q | -q)\n    becomes\n    (p | -p) & (q | -q)\n\n    Replace all (p | q) & (p | -p) & (q | -q) & (-q | -q)\n    becomes\n    (p | -p) & (q | -q)\n\n    Replace all (p | q) & (p | -p) & (q | -", "solution": "\n    output = Cnf()\n    for x in cnf.dis:\n        dont_add = False\n        for y in x:\n            for z in x:\n                if z == -y:\n                    dont_add = True\n                    break\n            if dont_add: break\n        if dont_add: continue\n        # TODO: Is this necessary anymore? Probably not. Do statistical analysis.\n        if x not in output.dis:\n            output.dis |= frozenset([x])\n    return output"}
{"prompt": "def edit(self,index=None):\n        \"\"\"Edit a block.\n\n        If no number is given, use the last block executed.\n\n        This edits the in-memory copy of the demo, it does NOT modify the\n        original source file.  If you want to do that, simply open the file in\n        an editor and use reload() when you make changes to the file.  This\n        method is meant to let you change a block during a demonstration for\n        explanatory purposes, without damaging your original script.\"\"\"", "output": "\n        if index is None:\n            index = len(self.blocks) - 1\n        block = self.blocks[index]\n        block.edit()\n        self.blocks[index] = block\n\n    def execute(self, command):\n        \"\"\"Execute a command in the demo.\n\n        This executes the given command in the demo, modifying the in-memory\n        copy of the demo.  The command should be a string that can be\n        executed in the demo, such as a Python expression or a function call.\n        The function should return None, indicating that the command", "solution": "\n\n        index = self._get_index(index)\n        if index is None:\n            return\n        # decrease the index by one (unless we're at the very beginning), so\n        # that the default demo.edit() call opens up the sblock we've last run\n        if index>0:\n            index -= 1\n\n        filename = self.shell.mktempfile(self.src_blocks[index])\n        self.shell.hooks.editor(filename,1)\n        new_block = file_read(filename)\n        # update the source and colored block\n        self.src_blocks[index] = new_block\n        self.src_blocks_colored[index] = self.ip_colorize(new_block)\n        self.block_index = index\n        # call to run with the newly edited index\n        self()"}
{"prompt": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"", "output": "\n    # Check for duplicate names\n    if len(set(named.keys()))!= len(named):\n        raise ValueError(\"Duplicate names in enum statement\")\n\n    # Check for duplicate values\n    if len(set(named.values()))!= len(named):\n        raise ValueError(\"Duplicate values in enum statement\")\n\n    # Build the enum statement\n    enum_list = []\n    for key, value in named.items():\n        enum_list.append(f\"{key} = {value}\")\n\n    return f\"enum {{{',", "solution": "\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)"}
{"prompt": "def check_part(state, name, part_msg, missing_msg=None, expand_msg=None):\n    \"\"\"Return child state with name part as its ast tree\"\"\"", "output": "\n    if missing_msg is None:\n        missing_msg = f\"Missing {name} in {state}\"\n    if expand_msg is None:\n        expand_msg = f\"Expanded {name} in {state}\"\n    if name not in state:\n        return {name: missing_msg}\n    child_state = state[name]\n    if isinstance(child_state, dict):\n        return {name: check_part(child_state, part_msg, missing_msg, expand_msg)}\n    elif isinstance(child_state", "solution": "\n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    if not part_msg:\n        part_msg = name\n    append_message = {\"msg\": expand_msg, \"kwargs\": {\"part\": part_msg}}\n\n    has_part(state, name, missing_msg, append_message[\"kwargs\"])\n\n    stu_part = state.student_parts[name]\n    sol_part = state.solution_parts[name]\n\n    assert_ast(state, sol_part, append_message[\"kwargs\"])\n\n    return part_to_child(stu_part, sol_part, append_message, state)"}
{"prompt": "def list_datasets(self, get_global_public):\n        \"\"\"\n        Lists datasets in resources. Setting 'get_global_public' to 'True'\n        will retrieve all public datasets in cloud. 'False' will get user's\n        public datasets.\n\n        Arguments:\n            get_global_public (bool): True if user wants all public datasets in\n                                      cloud. False if user wants only their\n                                      public datasets.\n\n        Returns:\n            dict: Returns datasets in JSON format\n\n        \"\"\"", "output": "\n        if get_global_public:\n            return self.public_datasets\n        else:\n            return self.user_public_datasets\n\n    def get_dataset_info(self, dataset_name):\n        \"\"\"\n        Returns information about a dataset.\n\n        Arguments:\n            dataset_name (str): Name of the dataset to retrieve information for.\n\n        Returns:\n            dict: Returns dataset information in JSON format\n\n        \"\"\"\n        # Code to retrieve dataset information\n        #...\n        return dataset_info\n\n    def add_dat", "solution": "\n        appending = \"\"\n        if get_global_public:\n            appending = \"public\"\n        url = self.url() + \"/resource/{}dataset/\".format(appending)\n        req = self.remote_utils.get_url(url)\n\n        if req.status_code is not 200:\n            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))\n        else:\n            return req.json()"}
{"prompt": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"", "output": "\n        return type_name in self.known_types\n\n    def get_known_type(self, type_name):\n        \"\"\"Get the known type of a type name.\n\n        Args:\n            type_name (str): The name of the type to get the known type for.\n\n        Returns:\n            str: The known type of the type name, or None if the type is not known.\n        \"\"\"\n        if type_name in self.known_types:\n            return self.known_types[type_name]\n        else:\n            return None\n\n    def", "solution": "\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False"}
{"prompt": "async def fetch_nearby(lat: float, long: float, limit: int = 10) -> Optional[List[Dict]]:\n    \"\"\"\n    Gets wikipedia articles near a given set of coordinates.\n    :raise ApiError: When there was an error connecting to the API.\n\n    todo cache\n    \"\"\"", "output": "\n    # Your code here\n    # Use a library like Wikipedia API to get the nearby articles\n    # Return a list of dictionaries containing the articles\n    # If no articles are found, return None\n\n", "solution": "\n    request_url = f\"https://en.wikipedia.org/w/api.php?action=query\" \\\n                  f\"&list=geosearch\" \\\n                  f\"&gscoord={lat}%7C{long}\" \\\n                  f\"&gsradius=10000\" \\\n                  f\"&gslimit={limit}\" \\\n                  f\"&format=json\"\n\n    async with ClientSession() as session:\n        try:\n            async with session.get(request_url) as request:\n                if request.status == 404:\n                    return None\n                data = (await request.json())[\"query\"][\"geosearch\"]\n\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n        except JSONDecodeError as dec_err:\n            logger.error(f\"Could not decode data: {dec_err}\")\n            raise ApiError(f\"Could not decode data: {dec_err}\")\n        except KeyError:\n            return None\n        else:\n            for location in data:\n                location.pop(\"ns\")\n                location.pop(\"primary\")\n            return data"}
{"prompt": "def create_symmetric_key(self, algorithm, length):\n        \"\"\"\n        Create a symmetric key.\n\n        Args:\n            algorithm(CryptographicAlgorithm): An enumeration specifying the\n                algorithm for which the created key will be compliant.\n            length(int): The length of the key to be created. This value must\n                be compliant with the constraints of the provided algorithm.\n\n        Returns:\n            dict: A dictionary containing the key data, with the following\n                key/value fields:\n                * value - the bytes of the key\n                * format - a KeyFormatType enumeration for the bytes format\n\n        Raises:\n            InvalidField: Raised when the algorithm is unsupported or the\n                length is incompatible with the algorithm.\n            CryptographicFailure: Raised when the key generation process\n                fails.\n\n        Example:\n            >>> engine = CryptographyEngine()\n            >>> key = engine.create_symmetric_key(\n            ...     CryptographicAlgorithm.AES, 256)\n        \"\"\"", "output": " the format of the key (e.g. \"PEM\")\n        \"\"\"\n        # Implementation details go here\n\n    def encrypt_data(self, data, key):\n        \"\"\"\n        Encrypt the given data using the provided symmetric key.\n\n        Args:\n            data(bytes): The data to be encrypted.\n            key(dict): The symmetric key to be used for encryption.\n\n        Returns:\n            bytes: The encrypted data.\n        \"\"\"\n        # Implementation details go here\n\n    def decrypt_data(self, data, key):\n        \"\"\"\n        Decrypt", "solution": "\n        if algorithm not in self._symmetric_key_algorithms.keys():\n            raise exceptions.InvalidField(\n                \"The cryptographic algorithm {0} is not a supported symmetric \"\n                \"key algorithm.\".format(algorithm)\n            )\n\n        cryptography_algorithm = self._symmetric_key_algorithms.get(algorithm)\n\n        if length not in cryptography_algorithm.key_sizes:\n            raise exceptions.InvalidField(\n                \"The cryptographic length ({0}) is not valid for \"\n                \"the cryptographic algorithm ({1}).\".format(\n                    length, algorithm.name\n                )\n            )\n\n        self.logger.info(\n            \"Generating a {0} symmetric key with length: {1}\".format(\n                algorithm.name, length\n            )\n        )\n\n        key_bytes = os.urandom(length // 8)\n        try:\n            cryptography_algorithm(key_bytes)\n        except Exception as e:\n            self.logger.exception(e)\n            raise exceptions.CryptographicFailure(\n                \"Invalid bytes for the provided cryptographic algorithm.\")\n\n        return {'value': key_bytes, 'format': enums.KeyFormatType.RAW}"}
{"prompt": "def _get_organisations(self):\n        \"\"\"\n        :returns: list of organisations, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"", "output": "\n        organisations = []\n        for org in self.data:\n            if org['country'] not in organisations:\n                organisations.append(org['country'])\n        return sorted(organisations)\n\n    def get_most_active_organisation(self):\n        \"\"\"\n        :returns: name of the organization with the highest number of activities\n        :rtype: str\n        \"\"\"\n        organisation_counts = {}\n        for org in self.data:\n            if org['country'] not in organisation_counts:\n                organisation_counts", "solution": "\n        organisations = []\n        for child in self.vcard.getChildren():\n            if child.name == \"ORG\":\n                organisations.append(child.value)\n        return sorted(organisations)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Completely resets the database. This means that all information in\n        the local cache and on disc will be erased.\n        \"\"\"", "output": "\n        self.local_cache.clear()\n        self.discard_cache.clear()\n\n    def get(self, key: str) -> str:\n        \"\"\"\n        Returns the value associated with the given key in the local cache.\n        If the key is not found in the local cache, returns the value associated\n        with the key in the disc cache.\n        \"\"\"\n        if key in self.local_cache:\n            return self.local_cache[key]\n        else:\n            return self.discard_cache.get(key, \"\")\n\n    def", "solution": "\n        if not os.path.isdir(self.fdir):\n            os.makedirs(self.fdir, exist_ok=True)\n            return\n\n        for f in os.listdir(self.fdir):\n            del self[f]"}
{"prompt": "def reset_selective(self, regex=None):\n        \"\"\"Clear selective variables from internal namespaces based on a\n        specified regular expression.\n\n        Parameters\n        ----------\n        regex : string or compiled pattern, optional\n            A regular expression pattern that will be used in searching\n            variable names in the users namespaces.\n        \"\"\"", "output": "\n        if regex is not None:\n            self.selective = {k: v for k, v in self.selective.items() if not regex.search(k)}\n        else:\n            self.selective = {}\n\n    def __getitem__(self, key):\n        \"\"\"\n        Returns the value of the variable with the given key in the current\n        namespace.\n\n        Parameters\n        ----------\n        key : string\n            The key of the variable to be retrieved.\n\n        Returns\n        -------\n        value : any type\n            The value of the variable with", "solution": "\n        if regex is not None:\n            try:\n                m = re.compile(regex)\n            except TypeError:\n                raise TypeError('regex must be a string or compiled pattern')\n            # Search for keys in each namespace that match the given regex\n            # If a match is found, delete the key/value pair.\n            for ns in self.all_ns_refs:\n                for var in ns:\n                    if m.search(var):\n                        del ns[var]"}
{"prompt": "def load_from_path(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"", "output": "\n    if os.path.isdir(path):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                if file.endswith('.yaml') or file.endswith('.yml'):\n                    spec = load_from_path(os.path.join(root, file))\n                    if spec:\n                        return spec\n        return None\n    else:\n        return load_spec_from_file(path)\n\n", "solution": "\n\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)"}
{"prompt": "def _has_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has this role name\n        \"\"\"", "output": "\n        if isinstance(role_name_or_list, str):\n            return role_name_or_list in self.roles\n        else:\n            return all(role in self.roles for role in role_name_or_list)\n\n    def has_any_role(self, role_list):\n        \"\"\"\n        Whether the user has any of the roles in the given list\n        \"\"\"\n        return any(self._has_role(role) for role in role_list)\n\n    def has_all_roles(self, role_list):", "solution": "\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])"}
{"prompt": "def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"", "output": "utures.Future\n        :return: A future object representing the result of the submitted task\n        \"\"\"\n        future = executor.submit(task.run, tag=tag)\n        return future\n\n    def map(self, executor, func, args, tag=None):\n        \"\"\"Applies a function to each element of an iterable and returns a list of the results\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type func: callable\n        :", "solution": "\n        logger.debug(\n            \"Submitting task %s to executor %s for transfer request: %s.\" % (\n                task, executor, self.transfer_id)\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future))\n        return future"}
{"prompt": "def _get_streams(self):\n        \"\"\"\n        Find the streams for OlympicChannel\n        :return:\n        \"\"\"", "output": "\n        streams = []\n        for stream in self.channel.streams:\n            if stream.name.lower() == 'olympic channel':\n                streams.append(stream)\n        return streams\n\n    def _get_stream_info(self, stream):\n        \"\"\"\n        Find information about a given OlympicChannel stream\n        :param stream:\n        :return:\n        \"\"\"\n        info = {}\n        for key, value in stream.info.items():\n            if key == 'location':\n                info['location'] = value\n            elif key ==", "solution": "\n        match = self._url_re.match(self.url)\n        type_of_stream = match.group('type')\n        lang = re.search(r\"/../\", self.url).group(0)\n\n        if type_of_stream == 'tv':\n            path = re.search(r\"tv/.*-\\d/$\", self.url).group(0)\n\n            return self._get_live_streams(lang, path)\n        elif type_of_stream == 'playback':\n            path = re.search(r\"/playback/.*/$\", self.url).group(0)\n            return self._get_vod_streams()"}
{"prompt": "def compute_ffmc2d(X):\n    \"\"\"Computes the 2D-Fourier Magnitude Coefficients.\"\"\"", "output": "\n    n = X.shape[0]\n    m = X.shape[1]\n    kx = 2 * np.pi / n\n    ky = 2 * np.pi / m\n    kx2 = kx ** 2\n    ky2 = ky ** 2\n    kx3 = kx ** 3\n    ky3 = ky ** 3\n    kx4 = kx ** 4\n    ky4 = ky ** 4\n    \n    # Compute the Fourier Transform of the input signal\n    X_fft = np.fft.ff", "solution": "\n    # 2d-fft\n    fft2 = scipy.fftpack.fft2(X)\n\n    # Magnitude\n    fft2m = magnitude(fft2)\n\n    # FFTshift and flatten\n    fftshift = scipy.fftpack.fftshift(fft2m).flatten()\n\n    #cmap = plt.cm.get_cmap('hot')\n    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\",\n    #    aspect=\"auto\", cmap=cmap)\n    #plt.show()\n\n    # Take out redundant components\n    return fftshift[:fftshift.shape[0] // 2 + 1]"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            self.prefilter_line(line, continue_prompt)\n\n    def prefilter_line(self, line, continue_prompt):\n        \"\"\"Prefilter a single line of text.\n\n        This is the main entry point for prefiltering a single line of\n        input.  This simply calls :meth:`prefilter_word` for each word of\n        input.\n\n        This covers cases where there are multiple words in the user entry,\n        which is the case when the user goes back to a single line of\n        ", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"", "output": "\n        # Generate a self-signed certificate\n        cert = OpenSSL.crypto.X509()\n        cert.set_version(2)\n        cert.get_subject().CN = service_name\n        cert.set_issuer(cert.get_subject())\n        cert.set_pubkey(OpenSSL.crypto.PKey())\n        cert.set_serial_number(1000)\n        cert.gmtime_adj_notBefore(0)\n        cert.gmtime_adj_notAfter(315360000)  # 10 years\n        cert", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_get(\n            '/' + self.subscription_id + '/services/hostedservices/' +\n            _str(service_name) + '/certificates/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',\n            Certificate)"}
{"prompt": "def get_default_args(func):\n    \"\"\"\n    returns a dictionary of arg_name:default_values for the input function\n    \"\"\"", "output": "\n    signature = inspect.signature(func)\n    return {param.name: param.default for param in signature.parameters.values()}\n\n", "solution": "\n    args, _, _, defaults, *rest = inspect.getfullargspec(func)\n    return dict(zip(reversed(args), reversed(defaults)))"}
{"prompt": "def index():\n    \"\"\"Display a list of all user institutes.\"\"\"", "output": "\n    institutes = {\n        \"New York\": \"National Aeronautics and Space Administration\",\n        \"Los Angeles\": \"United States of America\",\n        \"Chicago\": \"Chicago\",\n        \"Houston\": \"Houston\",\n        \"Phoenix\": \"Phoenix\",\n        \"Philadelphia\": \"Portugal\",\n        \"San Antonio\": \"San Antonio\",\n        \"San Diego\": \"Dallas\",\n        \"Dallas\": \"San Diego\",\n        \"San Jose\": \"San Jose\"\n    }\n    for institute, info in institutes.items():\n        print(f\"{inst", "solution": "\n    institute_objs = user_institutes(store, current_user)\n    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())\n                        for institute_obj in institute_objs if institute_obj)\n    return dict(institutes=institutes_count)"}
{"prompt": "def _plotting(self, rank_metric, results, graph_num, outdir, \n                  format, figsize, pheno_pos='', pheno_neg=''):\n        \"\"\" Plotting API.\n            :param rank_metric: sorted pd.Series with rankings values.\n            :param results: self.results\n            :param data: preprocessed expression table\n\n        \"\"\"", "output": "\n        # Set up plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Plot ranking curve\n        ax.plot(rank_metric, color='blue', linestyle='-', label='Rankings')\n\n        # Plot expression values on the y-axis\n        ax.plot(results.index, results.values, color='red', linewidth=2, label=pheno_pos)\n        ax.plot(results.index, results.values, color='green', linewidth=2, label=pheno_neg", "solution": "\n        \n        # no values need to be returned\n        if self._outdir is None: return\n        #Plotting\n        top_term = self.res2d.index[:graph_num]\n        # multi-threading\n        pool = Pool(self._processes)\n        for gs in top_term:\n            hit = results.get(gs)['hits_indices']\n            NES = 'nes' if self.module != 'ssgsea' else 'es'\n            term = gs.replace('/','_').replace(\":\",\"_\")\n            outfile = '{0}/{1}.{2}.{3}'.format(self.outdir, term, self.module, self.format)\n            # gseaplot(rank_metric=rank_metric, term=term, hits_indices=hit,\n            #           nes=results.get(gs)[NES], pval=results.get(gs)['pval'], \n            #           fdr=results.get(gs)['fdr'], RES=results.get(gs)['RES'],\n            #           pheno_pos=pheno_pos, pheno_neg=pheno_neg, figsize=figsize,\n            #           ofname=outfile)\n            pool.apply_async(gseaplot, args=(rank_metric, term, hit, results.get(gs)[NES],\n                                              results.get(gs)['pval'],results.get(gs)['fdr'],\n                                              results.get(gs)['RES'],\n                                              pheno_pos, pheno_neg, \n                                              figsize, 'seismic', outfile))\n            if self.module == 'gsea':\n                outfile2 = \"{0}/{1}.heatmap.{2}\".format(self.outdir, term, self.format)\n                # heatmap(df=self.heatmat.iloc[hit, :], title=term, ofname=outfile2, \n                #         z_score=0, figsize=(self._width, len(hit)/2))\n                pool.apply_async(heatmap, args=(self.heatmat.iloc[hit, :], 0, term, \n                                               (self._width, len(hit)/2+2), 'RdBu_r',\n                                                True, True, outfile2))\n        pool.close()\n        pool.join()"}
{"prompt": "def get(self, requestId):\n        \"\"\"\n        Gets details of a device management request.\n        It accepts requestId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"", "output": "\n        try:\n            # code to get details of device management request\n            return requestDetails\n        except APIException as e:\n            raise e\n\ndef post(self, requestId, requestDetails):\n    \"\"\"\n    Creates a new device management request.\n    It accepts requestId (string) and requestDetails (dictionary) as parameters\n    \"\"\"\n    # code to create new device management request\n    return requestId\n\ndef put(self, requestId, requestDetails):\n    \"\"\"\n    Updates an existing device management request.\n    It accepts requestId (string", "solution": "\n        url = MgmtRequests.mgmtSingleRequest % (requestId)\n        r = self._apiClient.get(url)\n\n        if r.status_code == 200:\n            return r.json()\n        else:\n            raise ApiException(r)"}
{"prompt": "def load(self, reload=False):\n        \"\"\"\n        Load the config and defaults from files.\n        \"\"\"", "output": "\n        if reload:\n            self.clear()\n            self.defaults.clear()\n            self.config.clear()\n            return True\n        else:\n            return False\n\n    def save(self, filename=None):\n        \"\"\"\n        Save the config and defaults to a file.\n        \"\"\"\n        if filename is None:\n            filename = \"config.txt\"\n        with open(filename, \"w\") as f:\n            for key, value in self.config.items():\n                f.write(f\"{key} = {value}\\n\")\n", "solution": "\n        if reload or not self._loaded:\n            # load defaults\n            if self._defaults_file and type(self._defaults_file) == str:\n                self._defaults_file = File(self._defaults_file, parent=self._parent)\n            defaults = {}\n            if self._defaults_file:\n                defaults = yaml.safe_load(self._defaults_file.read().replace('\\t', '    '))\n\n            # load data\n            data = {}\n            if self.exists:\n                data = yaml.safe_load(self.read().replace('\\t', '    '))\n\n            # initialise with the loaded data\n            self._defaults = defaults\n            self._data = copy.deepcopy(self._defaults)\n            self.update(data=data)\n\n            # if specified, apply environment variables\n            if self._apply_env:\n                self.update(ConfigEnv(self._env_prefix))\n\n            self._loaded = True\n\n        return self"}
{"prompt": "def walk_files(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr``.\n    \"\"\"", "output": "\n    for root, dirs, files in os.walk(mgr.root):\n        for file in files:\n            yield os.path.join(root, file)\n\n", "solution": "\n    for dir_, subdirs, files in walk_files(mgr):\n        for file_ in files:\n            yield file_"}
{"prompt": "def data_types(self):\n        \"\"\"Return a list of data types.\"\"\"", "output": "\n        return [type(self).__name__]\n\n    def __str__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return str(self.data)\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the object.\"\"\"\n        return f\"{self.__class__.__name__}({self.data})\"\n\n    def __eq__(self, other):\n        \"\"\"Return True if the object is equal to the other object, False otherwise.\"\"\"\n        return isinstance(other, self.__class", "solution": "\n        data = self.gencloud.project_data(self.id)\n        return sorted(set(d.type for d in data))"}
{"prompt": "def ProductsForm(category, products):\n    \"\"\" Produces an appropriate _ProductsForm subclass for the given render\n    type. \"\"\"", "output": "\n    form = super().ProductsForm(category)\n    for product in products:\n        form.add_field(\n            label=product.name,\n            value=product.description,\n            widget=forms.HiddenInput(attrs={'class': 'form-control'}),\n            required=False\n        )\n    return form\n\n", "solution": "\n\n    # Each Category.RENDER_TYPE value has a subclass here.\n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    # Produce a subclass of _ProductsForm which we can alter the base_fields on\n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm"}
{"prompt": "def chop(array, epsilon=1e-10):\n    \"\"\"\n    Truncate small values of a complex array.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n    \"\"\"", "output": "\n    return np.where(np.abs(array) < epsilon, 0, array)\n\n", "solution": "\n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[abs(ret.real) < epsilon] = 0.0\n        ret.imag[abs(ret.imag) < epsilon] = 0.0\n    return ret"}
{"prompt": "def make_clean_html(raw, stream_item=None, encoding=None):\n    \"\"\"Get a clean text representation of presumed HTML.\n\n    Treat `raw` as though it is HTML, even if we have no idea what it\n    really is, and attempt to get a properly formatted HTML document\n    with all HTML-escaped characters converted to their unicode.\n\n    This is called below by the `clean_html` transform stage, which\n    interprets MIME-type.  If `character_encoding` is not provided,\n    and `stream_item` is provided, then this falles back to\n    :attr:`streamcorpus.StreamItem.body.encoding`.\n\n    :param str raw: raw text to clean up\n    :param stream_item: optional stream item with encoding metadata\n    :type stream_item: :class:`streamcorpus.StreamItem`\n    :returns: UTF-8-encoded byte string of cleaned HTML text\n    :returntype: :class:`str`\n\n    \"\"\"", "output": " is not None, the function will attempt to decode\n    the stream item using the provided character encoding.\n\n    If `stream_item` is not None, the function will attempt to decode\n    the stream item using the provided character encoding, but only\n    if the stream item is a valid MIME type (i.e. not just a text/plain\n    file).\n\n    If `encoding` is provided, it will be used to decode the stream\n    item, but only if `stream_item` is not None.\n\n    Returns a string containing the clean text representation of the\n    raw", "solution": "\n    # Fix emails by protecting the <,> from HTML\n    raw = fix_emails(raw)\n    raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding)\n    if raw_decoded is None:\n        # give up on decoding it... maybe this should use force_unicode\n        raw_decoded = raw\n\n    # default attempt uses vanilla lxml.html\n    try:\n        root = lxml.html.document_fromstring(raw_decoded)\n    except ValueError, exc:\n        if 'with encoding declaration' in str(exc):\n            root = lxml.html.document_fromstring(raw)\n        else:\n            raise\n\n    # While we have the document parsed as a DOM, let's strip attributes.\n    # (The HTML cleaner seems to only support whitelisting attributes.\n    # As of now, we just want to blacklist a few.)\n    lxml.etree.strip_attributes(root, 'class', 'id')\n\n    # if that worked, then we will be able to generate a\n    # valid HTML string\n    fixed_html = lxml.html.tostring(root, encoding=unicode)\n\n    # remove any ^M characters\n    fixed_html = string.replace(fixed_html, '\\r', ' ')\n\n    # We drop utf8 characters that are above 0xFFFF as\n    # Lingpipe seems to be doing the wrong thing with them.\n    fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html)\n\n    # construct a Cleaner that removes any ``<script>`` tags,\n    # Javascript, like an ``onclick`` attribute, comments, style\n    # tags or attributes, ``<link>`` tags\n    cleaner = lxml.html.clean.Cleaner(\n        scripts=True, javascript=True,\n        comments=True,\n        # do not remove <html> <head> <title> etc\n        page_structure=False,\n        remove_tags=['base'],\n        style=True, links=True)\n\n    # now get the really sanitized HTML\n    _clean_html = cleaner.clean_html(fixed_html)\n\n    # generate pretty HTML in utf-8\n    _clean_html = lxml.html.tostring(\n        lxml.html.document_fromstring(_clean_html),\n        method='html', encoding='utf-8',\n        pretty_print=True,\n        # include_meta_content_type=True\n        )\n\n    return uniform_html(_clean_html)"}
{"prompt": "def func(self):\n        \"\"\"Alternative naming, you can use `node.func.name` instead of `node.f_func`\"\"\"", "output": "\n        return node.func.name\n\n    # Replace all occurrences of `node` with `func` in the AST\n    new_ast = ast.fix_missing_locations(new_ast)\n\n    # Check if the modified AST is valid\n    try:\n        compile(new_ast, \"<string>\", \"exec\")\n    except SyntaxError:\n        return False\n\n    return True\n\n", "solution": "\n        if self._func is None:\n            self._func = NNTreeNodeFunc(self)\n        return self._func"}
{"prompt": "def load_java_messages_to_ignore():\n    \"\"\"\n    Load in pickle file that contains dict structure with bad java messages to ignore per unit test\n    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.\n\n    :return:\n    \"\"\"", "output": "\n    with open('g_ok_java_messages.pkl', 'rb') as f:\n        g_ok_java_messages = pickle.load(f)\n\n", "solution": "\n    global g_ok_java_messages\n    global g_java_message_pickle_filename\n\n    if os.path.isfile(g_java_message_pickle_filename):\n        with open(g_java_message_pickle_filename,'rb') as tfile:\n            g_ok_java_messages = pickle.load(tfile)\n    else:\n        g_ok_java_messages[\"general\"] = []"}
{"prompt": "def rowCount(self, parent):\n        \"\"\"Return number of children *parent* index has.\"\"\"", "output": "\n        return len(parent)\n\n    def isLeaf(self, parent):\n        \"\"\"True if *parent* has no children, False otherwise.\"\"\"\n        return len(parent) == 1\n\n    def getPath(self, root, path):\n        \"\"\"Append *path* to *root* and return new *root*.\"\"\"\n        if root is None:\n            return None\n        path.append(root[1])\n        if len(root[0]) == 1:\n            return (root[0][0], root[1], path)\n        else:", "solution": "\n        if parent.column() > 0:\n            return 0\n\n        if parent.isValid():\n            item = parent.internalPointer()\n        else:\n            item = self.root\n\n        return len(item.children)"}
{"prompt": "def sh(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (integer) and OUTPUT (string)\n    \"\"\"", "output": "\n    import subprocess\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out,err = p.communicate()\n    rc = p.returncode\n    if escape:\n        out = out.replace('\\n', '\\\\n')\n        err = err.replace('\\n', '\\\\n')\n    return rc, out\n\n", "solution": "\n\n    if escape:\n        cmd = quote(cmd)\n\n    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n\n    return (retcode, output)"}
{"prompt": "def abort(self, jobs=None, targets=None, block=None):\n        \"\"\"Abort specific jobs from the execution queues of target(s).\n\n        This is a mechanism to prevent jobs that have already been submitted\n        from executing.\n\n        Parameters\n        ----------\n\n        jobs : msg_id, list of msg_ids, or AsyncResult\n            The jobs to be aborted\n            \n            If unspecified/None: abort all outstanding jobs.\n\n        \"\"\"", "output": "\n        if jobs is None:\n            jobs = []\n        if isinstance(jobs, AsyncResult):\n            jobs = [jobs]\n        for job in jobs:\n            if job.successful():\n                job.cancel()\n            else:\n                job.result()\n\n    def execute(self, jobs=None, targets=None, block=None):\n        \"\"\"Execute specific jobs from the execution queues of target(s).\n\n        This is a mechanism to execute jobs that have already been submitted\n        from executing.\n\n        Parameters\n        ----------\n\n        jobs : msg_", "solution": "\n        block = self.block if block is None else block\n        jobs = jobs if jobs is not None else list(self.outstanding)\n        targets = self._build_targets(targets)[0]\n        \n        msg_ids = []\n        if isinstance(jobs, (basestring,AsyncResult)):\n            jobs = [jobs]\n        bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)\n        if bad_ids:\n            raise TypeError(\"Invalid msg_id type %r, expected str or AsyncResult\"%bad_ids[0])\n        for j in jobs:\n            if isinstance(j, AsyncResult):\n                msg_ids.extend(j.msg_ids)\n            else:\n                msg_ids.append(j)\n        content = dict(msg_ids=msg_ids)\n        for t in targets:\n            self.session.send(self._control_socket, 'abort_request',\n                    content=content, ident=t)\n        error = False\n        if block:\n            self._flush_ignored_control()\n            for i in range(len(targets)):\n                idents,msg = self.session.recv(self._control_socket,0)\n                if self.debug:\n                    pprint(msg)\n                if msg['content']['status'] != 'ok':\n                    error = self._unwrap_exception(msg['content'])\n        else:\n            self._ignored_control_replies += len(targets)\n        if error:\n            raise error"}
{"prompt": "def format_execution_time(self, start=None, end=None):\n        \"\"\"\n        Format the calculated time into a human readable format.\n\n        :param start: A starting time.\n        :type start: int|str\n\n        :param stop: A ending time.\n        :type stop: int|str\n\n        :return: A human readable date.\n        :rtype: str\n        \"\"\"", "output": "\n        start = int(start) if start else time.time()\n        stop = int(stop) if stop else time.time()\n\n        return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(stop - start))\n\n    def format_output(self, output):\n        \"\"\"\n        Formats the output based on the specified format.\n\n        :param output: The output to be formatted.\n        :type output: str\n\n        :return: The formatted output.\n        :rtype: str\n        \"\"\"", "solution": "\n\n        # We return the formatted execution time.\n        return \":\".join(list(self._calculate(start, end).values()))"}
{"prompt": "def _all_load_skeleton(self, traj_node, hdf5_group):\n        \"\"\"Reloads skeleton data of a tree node\"\"\"", "output": "\n        skeleton_data = hdf5_group['skeleton'][traj_node]\n        self.x = skeleton_data[0]\n        self.y = skeleton_data[1]\n        self.z = skeleton_data[2]\n        self.dx = skeleton_data[3]\n        self.dy = skeleton_data[4]\n        self.dz = skeleton_data[5]\n        self.parent = skeleton_data[6]\n        self.children = skeleton_data[7]\n\n    def _all_unload_", "solution": "\n        if traj_node.v_annotations.f_is_empty():\n            self._ann_load_annotations(traj_node, hdf5_group)\n        if traj_node.v_comment == '':\n            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)\n            if comment is None:\n                comment = ''\n            traj_node.v_comment = comment"}
{"prompt": "def overlapping(self, variant_obj):\n        \"\"\"Return overlapping variants.\n\n        Look at the genes that a variant overlaps to.\n        Then return all variants that overlap these genes.\n\n        If variant_obj is sv it will return the overlapping snvs and oposite\n        There is a problem when SVs are huge since there are to many overlapping variants.\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"", "output": "\n        if variant_obj['sv'] == 'SV':\n            return [variant_obj]\n        else:\n            overlapping_genes = set(variant_obj['gene']) & set(variant_obj['other_gene'])\n            overlapping_variants = []\n            for variant in variant_obj['variants']:\n                if set(variant['gene']) & overlapping_genes:\n                    overlapping_variants.append(variant)\n            return overlapping_variants\n\n", "solution": "\n        #This is the category of the variants that we want to collect\n        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'\n\n        query = {\n            '$and': [\n                {'case_id': variant_obj['case_id']},\n                {'category': category},\n                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}\n            ]\n        }\n\n        sort_key = [('rank_score', pymongo.DESCENDING)]\n        # We collect the 30 most severe overlapping variants\n        variants = self.variant_collection.find(query).sort(sort_key).limit(30)\n\n        return variants"}
{"prompt": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"", "output": "\n        if not isinstance(self.name, str):\n            raise TypeError(\"Name attribute must be a string.\")\n        if not isinstance(self.age, int):\n            raise TypeError(\"Age attribute must be an integer.\")\n        if not isinstance(self.secret_data, str):\n            raise TypeError(\"SecretData attribute must be a string.\")\n\n    def encrypt(self):\n        \"\"\"\n        Encrypt the contents of the SecretData object using a simple encryption algorithm.\n\n        Returns:\n            str: the encrypted contents of the SecretData object.\n        \"\"\"", "solution": "\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))"}
{"prompt": "def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"", "output": "\n        return np.array([[1, 0, 0], [0, 1, 0], [0, 0, -1]])\n\n    def __str__(self):\n        return \"U3\"\n\n", "solution": "\n        theta, phi, lam = self.params\n        return numpy.array(\n            [[\n                numpy.cos(theta / 2),\n                -numpy.exp(1j * lam) * numpy.sin(theta / 2)\n            ],\n             [\n                 numpy.exp(1j * phi) * numpy.sin(theta / 2),\n                 numpy.exp(1j * (phi + lam)) * numpy.cos(theta / 2)\n             ]],\n            dtype=complex)"}
{"prompt": "def _merge_with_other_stm(self, other: \"IfContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"", "output": "\n        self.statements.append(other.statements)\n        self.truth_values.append(other.truth_values)\n        self.evidence.append(other.evidence)\n\n    def __str__(self) -> str:\n        \"\"\"\n        Returns a string representation of the statement\n        \"\"\"\n        return f\"If {self.truth_values[0]} then {self.statements[0]}\"\n\n", "solution": "\n        merge = self._merge_statement_lists\n        newCases = []\n        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):\n            newCases.append((c, merge(caseA, caseB)))\n\n        self.cases = newCases\n\n        if self.default is not None:\n            self.default = merge(self.default, other.default)\n\n        self._on_merge(other)"}
{"prompt": "def  main( argv ):\n    \"\"\"main program loop\"\"\"", "output": "\n    if len( argv ) < 2:\n        print( \"usage: python3 word_count_tool.py <file> <word>\" )\n        return\n    \n    try:\n        with open( argv[1], 'r' ) as f:\n            text = f.read()\n    except:\n        print( \"Error: could not read file\" )\n        return\n    \n    words = text.split()\n    count = 0\n    for word in words:\n        if word == argv[2]:\n            count += 1\n    \n", "solution": "\n\n    global output_dir\n\n    try:\n        opts, args = getopt.getopt( sys.argv[1:], \\\n                                    \"hb\",         \\\n                                    [\"help\", \"backup\"] )\n    except getopt.GetoptError:\n        usage()\n        sys.exit( 2 )\n\n    if args == []:\n        usage()\n        sys.exit( 1 )\n\n    # process options\n    #\n    output_dir = None\n    do_backup  = None\n\n    for opt in opts:\n        if opt[0] in ( \"-h\", \"--help\" ):\n            usage()\n            sys.exit( 0 )\n\n        if opt[0] in ( \"-b\", \"--backup\" ):\n            do_backup = 1\n\n    # create context and processor\n    source_processor = SourceProcessor()\n\n    # retrieve the list of files to process\n    file_list = make_file_list( args )\n    for filename in file_list:\n        source_processor.parse_file( filename )\n\n        for block in source_processor.blocks:\n            beautify_block( block )\n\n        new_name = filename + \".new\"\n        ok       = None\n\n        try:\n            file = open( new_name, \"wt\" )\n            for block in source_processor.blocks:\n                for line in block.lines:\n                    file.write( line )\n                    file.write( \"\\n\" )\n            file.close()\n        except:\n            ok = 0"}
{"prompt": "def checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"", "output": "\n    if b.funcname == \"hello\":\n        return True\n    else:\n        return False\n\n", "solution": "\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True"}
{"prompt": "def gene_variants(self, query=None,\n                   category='snv', variant_type=['clinical'],\n                   nr_of_variants=50, skip=0):\n        \"\"\"Return all variants seen in a given gene.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            query(dict): A dictionary with querys for the database, including\n            variant_type: 'clinical', 'research'\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n        \"\"\"", "output": "variants(int): The maximum number of variants to return\n            skip(int): The number of variants to skip before returning\n\n        Returns:\n            A list of dictionaries with variant information. Each dictionary\n            has the following keys:\n            - 'variant_id': The variant's unique identifier\n            - 'variant_type': The type of variant\n            - 'variant_category': The category of variant\n            - 'variant_chromosome': The chromosome of variant\n            - 'variant_start': The start position of variant\n            - 'variant_end':", "solution": "\n\n        mongo_variant_query = self.build_variant_query(query=query,\n                                   category=category, variant_type=variant_type)\n\n        sorting = [('rank_score', pymongo.DESCENDING)]\n\n        if nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        result = self.variant_collection.find(\n            mongo_variant_query\n            ).sort(sorting).skip(skip).limit(nr_of_variants)\n\n        return result"}
{"prompt": "def filter_exclude_downhole(self, threshold, filt=True, samples=None, subset=None):\n        \"\"\"\n        Exclude all points down-hole (after) the first excluded data.\n\n        Parameters\n        ----------\n        threhold : int\n            The minimum number of contiguous excluded data points\n            that must exist before downhole exclusion occurs.\n        file : valid filter string or bool\n            Which filter to consider. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if samples is None:\n            samples = self.active_samples\n\n        if subset is None:\n            subset = self.active_subset\n\n        if filt:\n            active_subset = subset\n        else:\n            active_subset = None\n\n        excluded = []\n        for i, sample in enumerate(samples):\n            if sample in active_subset:\n                excluded.append(i)\n                if len(excluded) > threshold:\n                    excluded.pop(0)\n\n        return excluded\n\n    def filter", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            self.data[s].filter_exclude_downhole(threshold, filt)"}
{"prompt": "def match(self, path):\n        \"\"\"Return route handler with arguments if path matches this route.\n\n        Arguments:\n          path (str): Request path\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the path.\n        \"\"\"", "output": "\n        if path == self.path:\n            return (self.handler, self.args, self.kwargs)\n        else:\n            return None\n\n", "solution": "\n        match = self._re.search(path)\n        if match is None:\n            return None\n        kwargs_indexes = match.re.groupindex.values()\n        args_indexes = [i for i in range(1, match.re.groups + 1)\n                          if i not in kwargs_indexes]\n        args = [match.group(i) for i in args_indexes]\n        kwargs = {}\n        for name, index in match.re.groupindex.items():\n            kwargs[name] = match.group(index)\n        return self._callback, args, kwargs"}
{"prompt": "def get_int(errmsg, arg, default=1, cmdname=None):\n    \"\"\"If arg is an int, use that otherwise take default.\"\"\"", "output": "\n    try:\n        return int(arg)\n    except (ValueError, TypeError):\n        if cmdname is None:\n            raise ValueError(errmsg)\n        else:\n            return default\n\n", "solution": "\n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n                       (cmdname, str(arg)))\n            else:\n                errmsg('Expecting an integer, got: %s.' % str(arg))\n                pass\n            raise ValueError\n    return default"}
{"prompt": "def get(*args, **kwargs):\n    \"\"\"Get users.\"\"\"", "output": "\n    return users.get(*args, **kwargs)\n\n", "solution": "\n    from invenio.modules.oauth2server.models import Client\n    q = Client.query\n    return q.count(), q.all()"}
{"prompt": "def verify(\n            cls,\n            timestamp: int,\n            message_hash: SHA512Hash,\n            signature: bytes,\n    ) -> bool:\n        \"\"\"\n        Verify a given NIST message hash and signature for a beacon value.\n\n        :param timestamp: The timestamp of the record being verified.\n        :param message_hash:\n            The hash that was carried out over the message.\n            This is an object belonging to the `Crypto.Hash` module.\n        :param signature: The signature that needs to be validated.\n        :return: True if verification is correct. False otherwise.\n        \"\"\"", "output": " True if the signature is valid for the given timestamp and message hash, False otherwise.\n        \"\"\"\n        # Verify the timestamp\n        if timestamp!= int(time.time()):\n            return False\n\n        # Verify the message hash\n        if message_hash!= SHA512Hash.new(message_hash):\n            return False\n\n        # Verify the signature\n        public_key = cls.get_public_key()\n        if not public_key.verify(signature, message_hash):\n            return False\n\n        return True\n\n    @classmethod\n    def get_public", "solution": "\n\n        # Determine verifier type to use based on timestamp.\n        if timestamp < 1496176860:\n            verifier = cls._VERIFIER_20130905\n        elif timestamp < 1502202360:\n            verifier = None\n        else:\n            verifier = cls._VERIFIER_20170808\n\n        # If a verifier exists to handle this problem, use it directly.\n        # Else, we cannot verify the record and must mark it invalid.\n        if verifier:\n            result = verifier.verify(\n                message_hash,\n                signature,\n            )\n        else:\n            result = False\n\n        # Convert 1 to 'True', 'False' otherwise\n        if isinstance(result, int):\n            result = True if result == 1 else False\n\n        return result"}
{"prompt": "def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from function bodies.\"\"\"", "output": "\n        if node.body and isinstance(node.body[0], ast.Pass):\n            return None\n        return node\n\n    def visit_Assign(self, node: ast.Assign) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from assignments.\"\"\"\n        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n            if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name) and node.value.func.", "solution": "\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )"}
{"prompt": "def register_success(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        succeeded\n        \"\"\"", "output": "\n        self.nickname = nick\n        self.message = message\n        self.channel = channel\n        self.cmd_channel = cmd_channel\n        self.success = True\n\n    def unregister_success(self):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        failed\n        \"\"\"\n        self.nickname = None\n        self.message = None\n        self.channel = None\n        self.cmd_channel = None\n        self.success =", "solution": "\n        # the boss will tell what channel to join\n        self.channel = cmd_channel\n        self.conn.join(self.channel)\n        \n        # indicate that registered so we'll stop trying\n        self.registered.set()"}
{"prompt": "def csv_to_po(trans_csv_path, meta_csv_path, locale_root,\n              po_files_path, header=None):\n    \"\"\"\n    Converts GDocs spreadsheet generated csv file into po file.\n    :param trans_csv_path: path to temporary file with translations\n    :param meta_csv_path: path to temporary file with meta information\n    :param locale_root: path to locale root folder containing directories\n                        with languages\n    :param po_files_path: path from lang directory to po file\n    \"\"\"", "output": "\n    # Read translations from temporary csv file\n    translations = pd.read_csv(trans_csv_path)\n\n    # Read meta information from temporary csv file\n    meta = pd.read_csv(meta_csv_path)\n\n    # Create a dictionary with language codes as keys and empty lists as values\n    lang_dict = {lang: [] for lang in translations['lang'].unique()}\n\n    # Add translations and meta information to the dictionary\n    for i, row in translations.iterrows():\n        lang_dict[row['lang']].append", "solution": "\n    pattern = \"^\\w+.*po$\"\n    for root, dirs, files in os.walk(locale_root):\n        for f in filter(lambda x: re.match(pattern, x), files):\n            os.remove(os.path.join(root, f))\n\n    # read title row and prepare descriptors for po files in each lang\n    trans_reader = UnicodeReader(trans_csv_path)\n    meta_reader = UnicodeReader(meta_csv_path)\n    try:\n        title_row = trans_reader.next()\n    except StopIteration:\n        # empty file\n        return\n\n    trans_languages = _prepare_locale_dirs(title_row[3:], locale_root)\n\n    po_files = {}\n\n    meta_reader.next()\n    # go through every row in downloaded csv file\n    for trans_row, meta_row in izip_longest(trans_reader, meta_reader):\n        filename = trans_row[0].rstrip()\n        metadata = meta_row[0].rstrip() if meta_row else METADATA_EMPTY\n        comment = trans_row[1]\n        msgid = trans_row[2]\n\n        if filename not in po_files:\n            _prepare_polib_files(po_files, filename, trans_languages,\n                                 locale_root, po_files_path, header)\n\n        _write_entries(po_files[filename], trans_languages, msgid,\n                       trans_row[3:], metadata, comment)\n    for filename in po_files:\n        for lang in po_files[filename]:\n            po_files[filename][lang].save()\n\n    trans_reader.close()\n    meta_reader.close()"}
{"prompt": "def _add_header(self):\n        \"\"\"Add email header info.\"\"\"", "output": "\n        self.add_header('From','sender@example.com')\n        self.add_header('To','recipient@example.com')\n        self.add_header('Subject', 'Subject line of email')\n\n    def _write_email(self, message):\n        \"\"\"Write email message to file.\"\"\"\n        with open('email.txt', 'a') as f:\n            f.write(message + '\\n\\n')\n\n    def _close_file(self):\n        \"\"\"Close file.\"\"\"\n        self.wfile.close", "solution": "\n        self.message[\"From\"] = self.from_\n        self.message[\"Subject\"] = self.subject\n        if self.to:\n            self.message[\"To\"] = self.list_to_string(self.to)\n        if self.cc:\n            self.message[\"Cc\"] = self.list_to_string(self.cc)\n        if self.bcc:\n            self.message[\"Bcc\"] = self.list_to_string(self.bcc)"}
{"prompt": "def url_dequery(url):\n    \"\"\"Return a URL with the query component removed.\n\n    :param url: URL to dequery.\n    :type url: str\n    :rtype: str\n    \"\"\"", "output": "\n    if '?' in url:\n        return url.split('?')[0]\n    else:\n        return url\n\n", "solution": "\n    url = urlparse.urlparse(url)\n    return urlparse.urlunparse((url.scheme,\n                                url.netloc,\n                                url.path,\n                                url.params,\n                                '',\n                                url.fragment))"}
{"prompt": "def rendered_content(self):\n        \"\"\"Returns a 'minified' version of the javascript content\"\"\"", "output": "\n        # Remove comments\n        js_content = re.sub(r'//.*|/\\*[\\s\\S]*?\\*/', '', self.js_content)\n        # Remove whitespace\n        js_content = re.sub(r'\\s+','', js_content)\n        # Remove empty lines\n        js_content = '\\n'.join([line for line in js_content.split('\\n') if line.strip()])\n        return js_content\n\n    def get_title(self):\n        \"\"\"Returns the title", "solution": "\n        template = self.resolve_template(self.template_name)\n        if django.VERSION[1] < 8:\n            if template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        else:\n            if template.template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        # if no minified template exists, minify the response\n        content = super(MinifiedJsTemplateResponse, self).rendered_content\n        content = jsmin.jsmin(content)\n        return content"}
{"prompt": "def get_except_handler_name(self, node):\n        \"\"\"\n        Helper to get the exception name from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"", "output": "\n        if hasattr(node, 'type'):\n            if node.type == 'except_clause':\n                return node.name\n        return None\n\n    def visit_ExceptHandler(self, node):\n        \"\"\"\n        Helper to visit an ExceptHandler node in both py2 and py3.\n\n        \"\"\"\n        name = self.get_except_handler_name(node)\n        if name:\n            self.generic_visit(node)\n            self.exception_names.add(name)\n\n    def visit_Try(self, node):\n        ", "solution": "\n        name = node.name\n        if not name:\n            return None\n\n        if version_info < (3,):\n            return name.id\n        return name"}
{"prompt": "def _get_live_streams(self, lang, path):\n        \"\"\"\n        Get the live stream in a particular language\n        :param lang:\n        :param path:\n        :return:\n        \"\"\"", "output": "\n        # Your code here to get the live stream in the specified language and path\n        # For example:\n        # return requests.get(f\"https://www.googleapis.com/language/translate/v2?key=YOUR_API_KEY&q={lang}&source=en&target=es\").json()[\"data\"][\"translations\"]\n        pass\n\n    def _get_translations(self, lang, path):\n        \"\"\"\n        Get the translations for a particular language and path\n        :param lang:\n        :param path:\n", "solution": "\n        res = self.session.http.get(self._live_api_url.format(lang, path))\n        live_res = self.session.http.json(res)['default']['uid']\n        post_data = '{\"channel_url\":\"/api/channels/%s/\"}' % live_res\n        try:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['stream_url']\n        except BaseException:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['channel_url']\n        return HLSStream.parse_variant_playlist(self.session, stream_data)"}
{"prompt": "def rotatePoint(self, pointX, pointY):\n        \"\"\"\n        Rotates a point relative to the mesh origin by the angle specified in the angle property.\n        Uses the angle formed between the segment linking the point of interest to the origin and\n        the parallel intersecting the origin. This angle is called beta in the code.\n        \"\"\"", "output": "\n        dx = pointX - self.x\n        dy = pointY - self.y\n        angle = math.atan2(dy, dx)\n        beta = math.pi - angle\n        x = self.x + math.cos(beta)\n        y = self.y + math.sin(beta)\n        return (x, y)\n\n    def intersect(self, other):\n        \"\"\"\n        Returns True if the line segment connecting the endpoints of this line segment\n        intersects with the line segment connecting the endpoints of the other line segment.\n        \"\"\"", "solution": "    \n        if(self.angle == 0 or self.angle == None):\n            return(pointX, pointY)\n              \n        # 1. Compute the segment length\n        length = math.sqrt((pointX - self.xll) ** 2 + (pointY - self.yll) ** 2)\n        \n        # 2. Compute beta\n        beta = math.acos((pointX - self.xll) / length) \n        if(pointY < self.yll):\n            beta = math.pi * 2 - beta\n           \n        # 3. Compute offsets\n        offsetX = math.cos(beta) * length - math.cos(self._angle_rd + beta) * length\n        offsetY = math.sin(self._angle_rd + beta) * length - math.sin(beta) * length \n        return (pointX - offsetX, pointY + offsetY)"}
{"prompt": "def get_term_by_sis_id(self, sis_term_id):\n        \"\"\"\n        Return a term resource for the passed SIS ID.\n        \"\"\"", "output": "\n        for term in self.terms:\n            if term.sis_id == sis_term_id:\n                return term\n        return None\n\n", "solution": "\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term"}
{"prompt": "def set_hvac_mode(self, index, hvac_mode):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"", "output": "\n        if index < 0 or index >= len(self.hvac_modes):\n            raise ValueError(\"Invalid hvac mode index\")\n        self.hvac_modes[index] = hvac_mode\n\n    def get_hvac_mode(self, index):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"\n        if index < 0 or index >= len(self.hvac_modes):\n            raise ValueError(\"Invalid hvac mode index\")\n        return self.hv", "solution": "\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)"}
{"prompt": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"", "output": "\n        if prefix == 'ur':\n            # Check for bad escapes in a raw string\n            for char in string_body:\n                if char == '\\\\':\n                    if start_row < len(self.lines) - 1:\n                        next_char = self.lines[start_row + 1][0]\n                        if next_char == 'u':\n                            # Check for bad escape in a raw string\n                            if len(string_body) > 1 and string_body[1] == 'u':\n                                # Check for bad escape in a raw string\n                                if len", "solution": "\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.for flat segmentation.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        # <YOUR CODE HERE>\n        pass\n\n    def process(self, frame: np.array) -> Tuple[np.array, np.array]:\n        \"\"\"\n        Processes a single frame.\n        Parameters\n        ----------\n        frame : np.array\n            A 2D numpy array representing a frame.\n        Returns\n        -------\n        Tuple[np.array, np.array]\n            A tuple containing the estimated times for the segment boundaries in frame indeces and the estimated labels for the segments.\n        \"\"\"\n        # <YOUR CODE HERE>", "solution": "\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)"}
{"prompt": "def finish_registration(self, heart):\n        \"\"\"Second half of engine registration, called after our HeartMonitor\n        has received a beat from the Engine's Heart.\"\"\"", "output": "\n        pass\n\n    def start_heartbeat(self):\n        \"\"\"Starts the HeartMonitor and our Engine's Heart, waiting for them to\n        receive a beat.\"\"\"\n        while True:\n            if self.heartbeat_available:\n                self.heartbeat_available = False\n                self.engine_heartbeat()\n            else:\n                time.sleep(0.1)\n\n    def heartbeat_available(self):\n        \"\"\"Checks if the HeartMonitor is still available to receive a beat.\"\"\"\n        # This is just a placeholder function - you would need to", "solution": "\n        try:\n            (eid,queue,reg,purge) = self.incoming_registrations.pop(heart)\n        except KeyError:\n            self.log.error(\"registration::tried to finish nonexistant registration\", exc_info=True)\n            return\n        self.log.info(\"registration::finished registering engine %i:%r\", eid, queue)\n        if purge is not None:\n            purge.stop()\n        control = queue\n        self.ids.add(eid)\n        self.keytable[eid] = queue\n        self.engines[eid] = EngineConnector(id=eid, queue=queue, registration=reg,\n                                    control=control, heartbeat=heart)\n        self.by_ident[queue] = eid\n        self.queues[eid] = list()\n        self.tasks[eid] = list()\n        self.completed[eid] = list()\n        self.hearts[heart] = eid\n        content = dict(id=eid, queue=self.engines[eid].queue.decode('ascii'))\n        if self.notifier:\n            self.session.send(self.notifier, \"registration_notification\", content=content)\n        self.log.info(\"engine::Engine Connected: %i\", eid)"}
{"prompt": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"", "output": "): Start date in the format 'YYYY-MM-DD'\n                endDate (string): End date in the format 'YYYY-MM-DD'\n                fmt (string): Format of the date in the ticker (json or xml)\n                frequency (string): Frequency of the data in the ticker (daily or hourly)\n\n            Returns:\n                dict: Dictionary containing the latest price for the given stock ticker\n                      and date range. If no data is available, returns None.\n        \"\"\"\n        # Check if ticker is supported\n        if ticker not in self.", "solution": "\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")"}
{"prompt": "def _plant_trie(strings: _List[str]) -> dict:\n    \"\"\"Create a Trie out of a list of words and return an atomic regex pattern.\n\n    The corresponding Regex should match much faster than a simple Regex union.\n    \"\"\"", "output": "\n    trie = {}\n    for string in strings:\n        node = trie\n        for char in string:\n            node = node.setdefault(char, {})\n        node['$'] = string\n\n    return trie\n\n", "solution": "\n    # plant the trie\n    trie = {}\n    for string in strings:\n        d = trie\n        for char in string:\n            d[char] = char in d and d[char] or {}\n            d = d[char]\n        d[''] = None  # EOS\n    return trie"}
{"prompt": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"", "output": "\n    \"\"\"\n    if log:\n        in_profile = np.log(in_profile)\n    norm_profile = in_profile / in_profile.sum(axis=1)[:, np.newaxis]\n    if return_offset:\n        offset = np.log(1/in_profile.sum(axis=1))\n        return norm_profile, offset\n    else:\n        return norm_profile\n\n", "solution": "\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)"}
{"prompt": "def delete_dashboard(self, dashboard):\n        \"\"\"**Description**\n            Deletes a dashboard.\n\n        **Arguments**\n            - **dashboard**: the dashboard object as returned by :func:`~SdcClient.get_dashboards`.\n\n        **Success Return Value**\n            `None`.\n\n        **Example**\n            `examples/delete_dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboard.py>`_\n        \"\"\"", "output": "\n        if dashboard is not None:\n            self.sdcclient.delete_dashboard(dashboard)\n\n    def get_system_status(self, system):\n        \"\"\"**Description**\n            Returns the status of a system.\n\n        **Arguments**\n            - **system**: the system object as returned by :func:`~SdcClient.get_systems`.\n\n        **Success Return Value**\n            A dictionary with the following keys:\n            - **status**: the status of the system as a string.\n            - **message**: a message describing", "solution": "\n        if 'id' not in dashboard:\n            return [False, \"Invalid dashboard format\"]\n\n        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        return [True, None]"}
{"prompt": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"", "output": "\n  def masked_initializer(shape, dtype, partition_info):\n    if partition_info is None:\n      return initializer(shape, dtype)\n    else:\n      return initializer(shape, dtype, partition_info) * mask\n  return masked_initializer\n\n", "solution": "\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer"}
{"prompt": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"", "output": "\n    import os\n    os.system(f'mongod -u {ctx.guild}')\n\n", "solution": "\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")"}
{"prompt": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"", "output": "\n        return cls(address_book, supported_private_objects, version,\n            localize_dates)\n\n    def __init__(self, address_book, supported_private_objects, version,\n            localize_dates):\n        self.address_book = address_book\n        self.supported_private_objects = supported_private_objects\n        self.version = version\n        self.localize_dates = localize_dates\n\n    def get_address(self, name):\n        \"\"\"Get the address associated with the given name.\"\"\"\n        return self.", "solution": "\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)"}
{"prompt": "def sample_variants(self, variants, sample_name, category = 'snv'):\n        \"\"\"Given a list of variants get variant objects found in a specific patient\n\n        Args:\n            variants(list): a list of variant ids\n            sample_name(str): a sample display name\n            category(str): 'snv', 'sv' ..\n\n        Returns:\n            result(iterable(Variant))\n        \"\"\"", "output": "\n        result = []\n        for variant in variants:\n            variant_obj = Variant.objects.get(id=variant)\n            if category in variant_obj.categories and variant_obj.patient == self.id:\n                result.append(variant_obj)\n        return result\n\n", "solution": "\n        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))\n        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample\n\n        query = {\n            '$and': [\n                {'_id' : { '$in' : variants}},\n                {'category' : category},\n                {'samples': {\n                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }\n                }}\n            ]\n        }\n\n        result = self.variant_collection.find(query)\n        return result"}
{"prompt": "def _register_by_id_managed_msg(self, msgid, line, is_disabled=True):\n        \"\"\"If the msgid is a numeric one, then register it to inform the user\n        it could furnish instead a symbolic msgid.\"\"\"", "output": "\n        if is_disabled:\n            return\n        if msgid.isdigit():\n            try:\n                id = int(msgid)\n            except ValueError:\n                return\n            if id in self.msgid_to_furniture:\n                return\n            self.msgid_to_furniture[id] = line\n            return\n        return\n    \n    def _unregister_by_id_managed_msg(self, msgid):\n        \"\"\"Remove the furniture associated with the given msgid from the internal\n        dictionary of registered furniture", "solution": "\n        try:\n            message_definitions = self.msgs_store.get_message_definitions(msgid)\n            for message_definition in message_definitions:\n                if msgid == message_definition.msgid:\n                    MessagesHandlerMixIn.__by_id_managed_msgs.append(\n                        (\n                            self.current_name,\n                            message_definition.msgid,\n                            message_definition.symbol,\n                            line,\n                            is_disabled,\n                        )\n                    )\n        except UnknownMessageError:\n            pass"}
{"prompt": "def pretty(obj, verbose=False, max_width=79, newline='\\n'):\n    \"\"\"\n    Pretty print the object's representation.\n    \"\"\"", "output": "\n    if verbose:\n        print(obj)\n    else:\n        pprint(obj, width=max_width, newline=newline)\n\n", "solution": "\n    stream = StringIO()\n    printer = RepresentationPrinter(stream, verbose, max_width, newline)\n    printer.pretty(obj)\n    printer.flush()\n    return stream.getvalue()"}
{"prompt": "def __match_intervals(intervals_from, intervals_to, strict=True):  # pragma: no cover\n    \"\"\"Numba-accelerated interval matching algorithm.\n\n    \"\"\"", "output": "\n    if strict:\n        intervals_from = sorted(intervals_from)\n        intervals_to = sorted(intervals_to)\n    else:\n        intervals_from = sorted(intervals_from + [intervals_from[-1] + 1])\n        intervals_to = sorted(intervals_to + [-intervals_to[0] - 1])\n    matches = []\n    i, j = 0, 0\n    while i < len(intervals_from) and j < len(intervals_to):\n        if", "solution": "\n    # sort index of the interval starts\n    start_index = np.argsort(intervals_to[:, 0])\n\n    # sort index of the interval ends\n    end_index = np.argsort(intervals_to[:, 1])\n\n    # and sorted values of starts\n    start_sorted = intervals_to[start_index, 0]\n    # and ends\n    end_sorted = intervals_to[end_index, 1]\n\n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n\n    output = np.empty(len(intervals_from), dtype=numba.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]\n\n        # Find the intervals that start after our query ends\n        after_query = search_ends[i]\n        # And the intervals that end after our query begins\n        before_query = search_starts[i]\n\n        # Candidates for overlapping have to (end after we start) and (begin before we end)\n        candidates = set(start_index[:after_query]) & set(end_index[before_query:])\n\n        # Proceed as before\n        if len(candidates) > 0:\n            output[i] = __match_interval_overlaps(query, intervals_to, candidates)\n        elif strict:\n            # Numba only lets us use compile-time constants in exception messages\n            raise ParameterError\n        else:\n            # Find the closest interval\n            # (start_index[after_query] - query[1]) is the distance to the next interval\n            # (query[0] - end_index[before_query])\n            dist_before = np.inf\n            dist_after = np.inf\n            if search_starts[i] > 0:\n                dist_before = query[0] - end_sorted[search_starts[i]-1]\n            if search_ends[i] + 1 < len(intervals_to):\n                dist_after = start_sorted[search_ends[i]+1] - query[1]\n            if dist_before < dist_after:\n                output[i] = end_index[search_starts[i]-1]\n            else:\n                output[i] = start_index[search_ends[i]+1]\n    return output"}
{"prompt": "def on_key_press(self,symbol,modifiers):\n        \"\"\"\n        Keyboard event handler handling only the escape key.\n        \n        If an escape key press is detected, mouse exclusivity is toggled via :py:meth:`PengWindow.toggle_exclusivity()`\\ .\n        \"\"\"", "output": "\n        if symbol == \"escape\":\n            self.toggle_exclusivity()\n\nclass PengWindow:\n    \"\"\"\n    The PengWindow class represents the main window of the Peng game. It has a title, a left-click button, a right-click button, and a set of other widgets.\n    \"\"\"\n    def __init__(self,title,left_click_button,right_click_button,widgets):\n        self.title = title\n        self.left_click_button = left_click_button\n        self.right_click_", "solution": "\n        if symbol == key.ESCAPE:\n            self.world.peng.window.toggle_exclusivity()\n            return pyglet.event.EVENT_HANDLED"}
{"prompt": "def pprint_simple_array(val, displaywidth, msg_nocr, msg, lineprefix=''):\n    \"\"\"Try to pretty print a simple case where a list is not nested.\n    Return True if we can do it and False if not. \"\"\"", "output": "\n    if isinstance(val, list):\n        if displaywidth > 0:\n            # Try to print each element on a new line\n            for elem in val:\n                if not pprint_simple_array(elem, displaywidth - len(lineprefix), msg_nocr, msg, lineprefix +' '):\n                    return False\n            # If all elements fit, return True\n            return True\n        else:\n            # If we're not printing anything, just return False\n            return False\n    else:\n        # If we're not a list, just", "solution": "\n\n    if type(val) != list:\n        return False\n\n    numeric = True\n    for i in range(len(val)):\n        if not (type(val[i]) in [bool, float, int]):\n            numeric = False\n            if not (type(val[i]) in [bool, float, int, bytes]):\n                return False\n            pass\n        pass\n    mess = columnize([repr(v) for v in val],\n                     opts={\"arrange_array\": True,\n                           \"lineprefix\": lineprefix,\n                           \"displaywidth\": int(displaywidth)-3,\n                           'ljust': not numeric})\n    msg_nocr(mess)\n    return True"}
{"prompt": "def size(self, source):\n    \"\"\"Get the size component of the given s3url. If it is a\n       directory, combine the sizes of all the files under\n       that directory. Subdirectories will not be counted unless\n       --recursive option is set.\n    \"\"\"", "output": "\n    if source.path.endswith('/'):\n        # If the path is a directory, recursively count files\n        size = 0\n        for f in source.listdir():\n            if f.is_file():\n                size += f.size()\n            elif f.is_dir() and not source.recursive:\n                # If the directory is not a subdirectory, just count\n                # its files\n                size += size(f)\n        return size\n    else:\n        # If the path is not a directory, just get its size", "solution": "\n    result = []\n    for src in self.source_expand(source):\n      size = 0\n      for f in self.s3walk(src):\n        size += f['size']\n      result.append((src, size))\n\n    return result"}
{"prompt": "def parse_compounds(compound_info, case_id, variant_type):\n    \"\"\"Get a list with compounds objects for this variant.\n\n        Arguments:\n            compound_info(str): A Variant dictionary\n            case_id (str): unique family id\n            variant_type(str): 'research' or 'clinical'\n\n        Returns:\n            compounds(list(dict)): A list of compounds\n    \"\"\"", "output": "\n    compounds = []\n    for compound in compound_info:\n        if compound['case_id'] == case_id and compound['variant_type'] == variant_type:\n            compounds.append(compound)\n    return compounds\n\n", "solution": "\n    # We need the case to construct the correct id\n    compounds = []\n    if compound_info:\n        for family_info in compound_info.split(','):\n            splitted_entry = family_info.split(':')\n            # This is the family id\n            if splitted_entry[0] == case_id:\n                for compound in splitted_entry[1].split('|'):\n                    splitted_compound = compound.split('>')\n                    compound_obj = {}\n                    compound_name = splitted_compound[0]\n                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +\n                                                               [variant_type, case_id])\n\n                    try:\n                        compound_score = float(splitted_compound[1])\n                    except (TypeError, IndexError):\n                        compound_score = 0.0\n\n                    compound_obj['score'] = compound_score\n                    compound_obj['display_name'] = compound_name\n\n                    compounds.append(compound_obj)\n\n    return compounds"}
{"prompt": "def any_slug_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"", "output": "\r\n    import random, string\r\n    slug = ''.join(random.choices(string.ascii_lowercase + string.digits, k=10)) # generate random slug\r\n    return kwargs.get('default', slug) # return default value if slug is not provided in kwargs or default value is None\r\n    # return slug if slug is not provided in kwargs or default value is None, otherwise return None.\r\n    # return slug if slug is not None else kwargs.get('default', slug) # return default value if", "solution": "\r\n    letters = ascii_letters + digits + '_-'\r\n    return xunit.any_string(letters = letters, max_length = field.max_length)"}
{"prompt": "def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"", "output": "\n        # Update C\n        self.C = np.linalg.solve(self.X.T.dot(self.X), self.X.T.dot(self.y))\n\n        # Update U\n        self.U = np.linalg.solve(self.X.T.dot(self.X), self.X.T.dot(self.y - self.C.dot(self.X.dot(self.U))))\n\n        # Update R\n        self.R = np.diag(self.y - self.C.", "solution": "\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()"}
{"prompt": "def _deduplicate_indexed_slices(values, indices):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"", "output": "plicated version of `indices` and `summed_values` is the sum of all\n      `values` associated with those indices.\n    \"\"\"\n    unique_indices, summed_values = unique_with_sum(indices, values)\n    return summed_values, unique_indices\n\n", "solution": "\n    unique_indices, new_index_positions = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values,\n                                            new_index_positions,\n                                            tf.shape(unique_indices)[0])\n    return (summed_values, unique_indices)"}
{"prompt": "def token_validate_with_login(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            request_token: The token you generated for the user to approve.\n            username: The user's username on TMDb.\n            password: The user's password on TMDb.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\n        # TODO: Implement token validation logic here\n        pass\n\n    def get_movie_info(self, movie_id: str, **kwargs):\n        \"\"\"\n        Retrieve information about a movie from TMDb.\n\n        Args:\n            movie_id: The ID of the movie to retrieve information for.\n            **kwargs: Additional keyword arguments to pass to the TMDb API.\n\n        Returns:\n            A dict containing information about the movie, including its title,\n            release date, runtime, and rating.\n        \"\"\"\n        # TODO: Implement movie", "solution": "\n        path = self._get_path('token_validate_with_login')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def setLang(self,lang):\n        \"\"\"\n        Sets the default language for all domains.\n        \n        For recommendations regarding the format of the language code, see\n        :py:class:`TranslationManager`\\ .\n        \n        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n        :py:meth:`translate_lazy()` will override this setting.\n        \n        Also note that the code won't be checked for existence or plausibility.\n        This may cause the fallback strings to be displayed instead if the language\n        does not exist.\n        \n        Calling this method will cause the ``setlang`` action and the\n        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both\n        action and event will be triggered even if the language did not actually change.\n        \n        This method also automatically updates the :confval:`i18n.lang` config value.\n        \"\"\"", "output": " in a way that's\n        unlikely to be useful.\n        \"\"\"\n        self.lang = lang\n    \n    def translate(self,word):\n        \"\"\"\n        Translates a single word from the default language to the user's language.\n        \n        Returns the translated word as a string.\n        \"\"\"\n        # TODO: Implement translation logic here\n        pass\n    \n    def translate_lazy(self,word):\n        \"\"\"\n        Translates a single word from the user's language to the default language.\n        \n        Returns the translated word as a", "solution": "\n        self.lang = lang\n        self.peng.cfg[\"i18n.lang\"] = lang\n        \n        if lang not in self.cache:\n            self.cache[lang]={}\n        \n        self.doAction(\"setlang\")\n        self.peng.sendEvent(\"peng3d:i18n.set_lang\",{\"lang\":self.lang,\"i18n\":self})"}
{"prompt": "def end_group(self, dedent=0, close=''):\n        \"\"\"End a group. See `begin_group` for more details.\"\"\"", "output": "\n        if dedent > 0:\n            self.indent -= dedent\n        if close:\n            self.group += '\\n' + close\n        self.group += '\\n'\n        return self\n\n    def begin_group(self, dedent=0, close=''):\n        \"\"\"Begin a group. See `end_group` for more details.\"\"\"\n        if dedent > 0:\n            self.indent += dedent\n        if close:\n            self.group += '\\n' + close\n        self.group += '\\n'", "solution": "\n        self.indentation -= dedent\n        group = self.group_stack.pop()\n        if not group.breakables:\n            self.group_queue.remove(group)\n        if close:\n            self.text(close)"}
{"prompt": "def revoke_token(self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"Revoke an access or refresh token.\n        \"\"\"", "output": "\n        revoke_token_endpoint = self.revoke_token_endpoint(token, token_type_hint, request, *args, **kwargs)\n        response = requests.post(revoke_token_endpoint, json={\"revoke\": True})\n        return response.json()\n\n    def revoke_token_endpoint(self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"\n        Generate the revocation token endpoint for the given token and token type hint.\n        \"\"\"\n        revoke_token", "solution": "\n        if token_type_hint:\n            tok = self._tokengetter(**{token_type_hint: token})\n        else:\n            tok = self._tokengetter(access_token=token)\n            if not tok:\n                tok = self._tokengetter(refresh_token=token)\n\n        if tok:\n            request.client_id = tok.client_id\n            request.user = tok.user\n            tok.delete()\n            return True\n\n        msg = 'Invalid token supplied.'\n        log.debug(msg)\n        request.error_message = msg\n        return False"}
{"prompt": "def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"", "output": "\n    for annotation in node.annotations:\n        if annotation.name == \"postponed_evaluation\":\n            return annotation.parent.name == \"annotation\"\n    return False\n\n", "solution": "\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )"}
{"prompt": "def _log(cls, level, msg, **kwargs):\n        \"\"\"\n        Logs a message with pre-formatted prefix.\n\n        :param int level:\n            Logging level as specified in the\n            `login module <http://docs.python.org/2/library/logging.html>`_ of\n            Python standard library.\n\n        :param str msg:\n            The actual message.\n\n        \"\"\"", "output": "\n        prefix = kwargs.get('prefix')\n        if prefix:\n            prefix = '[' + prefix + ']'\n        log_format = kwargs.get('format')\n        if log_format:\n            msg = prefix + log_format.format(msg)\n        logging.log(level, msg)\n\n    return _log\n\n", "solution": "\n\n        logger = getattr(cls, '_logger', None) or authomatic.core._logger\n        logger.log(\n            level, ': '.join(\n                ('authomatic', cls.__name__, msg)), **kwargs)"}
{"prompt": "def print_list_lines(self, filename, first, last):\n        \"\"\"The printing (as opposed to the parsing part of a 'list'\n        command.\"\"\"", "output": "\n        with open(filename, 'r') as f:\n            for i, line in enumerate(f):\n                if i >= first and i <= last:\n                    print(line.strip())\n\n    def print_help(self, filename):\n        \"\"\"Prints a 'help' message to the console.\"\"\"\n        print(\"List format:\")\n        print(\"  -h, --help\")\n        print(\"  -l, --list\")\n        print(\"  -f, --file\")\n        print(\"  -d, --directory\")\n        print(\"  -", "solution": "\n        try:\n            Colors = self.color_scheme_table.active_colors\n            ColorsNormal = Colors.Normal\n            tpl_line = '%%s%s%%s %s%%s' % (Colors.lineno, ColorsNormal)\n            tpl_line_em = '%%s%s%%s %s%%s%s' % (Colors.linenoEm, Colors.line, ColorsNormal)\n            src = []\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno)\n                if not line:\n                    break\n\n                if lineno == self.curframe.f_lineno:\n                    line = self.__format_line(tpl_line_em, filename, lineno, line, arrow = True)\n                else:\n                    line = self.__format_line(tpl_line, filename, lineno, line, arrow = False)\n\n                src.append(line)\n                self.lineno = lineno\n\n            print >>io.stdout, ''.join(src)\n\n        except KeyboardInterrupt:\n            pass"}
{"prompt": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"", "output": "\n  # Compute the inverse of the linop.\n  inv_linop = linop.T.dot(linop)\n  # Compute the mean of the original mean and the mean of the linearized mean.\n  new_mean = mean + linop.dot(dist)\n  # Compute the mean of the propagated mean and the mean of the original mean.\n  propagated_mean = new_mean + inv_linop.dot(dist)\n  return propagated_mean\n\n", "solution": "\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]"}
{"prompt": "def get_cdata(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"", "output": "\n        if len(args) == 1 and isinstance(args[0], (list, tuple)):\n            return args[0]\n        else:\n            return _ffi_backend.buffer(args)\n\n    def set_cdata(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Sets the cdata of the buffer to the provided arguments\n        \"\"\"\n        _ffi_backend.buffer = args\n\n    cdata = property(get_cdata, set_cdata)\n\n", "solution": "\n        res = tuple([\n            self.from_buffer(x) for x in args\n        ])\n\n        if len(res) == 0:\n            return None\n        elif len(res) == 1:\n            return res[0]\n        else:\n            return res"}
{"prompt": "def sourceWatchdog(self):\n        \"\"\"Watchdog timer function. \n\n        Recreates sources which have not generated events in 10*interval if\n        they have watchdog set to true in their configuration\n        \"\"\"", "output": "\n        while True:\n            if self.watchdog:\n                self.recreate_source()\n            time.sleep(10*self.interval)\n\n    def recreate_source(self):\n        \"\"\"Recreates a source if it has not generated an event in 10*interval\"\"\"\n        source = self.source_config.get_source()\n        if source:\n            self.watchdog = True\n            self.source = source\n            self.source_config.set_source(source)\n\n", "solution": "\n        for i, source in enumerate(self.sources):\n            if not source.config.get('watchdog', False):\n                continue \n            sn = repr(source)\n            last = self.lastEvents.get(source, None)\n            if last:\n                try:\n                    if last < (time.time()-(source.inter*10)):\n                        log.msg(\"Trying to restart stale source %s: %ss\" % (\n                            sn, int(time.time() - last)\n                        ))\n\n                        s = self.sources.pop(i)\n                        try:\n                            s.t.stop()\n                        except Exception as e:\n                            log.msg(\"Could not stop timer for %s: %s\" % (\n                                sn, e))\n\n                        config = copy.deepcopy(s.config)\n\n                        del self.lastEvents[source]\n                        del s, source\n\n                        source = self.createSource(config)\n\n                        reactor.callLater(0, self._startSource, source)\n                except Exception as e:\n                    log.msg(\"Could not reset source %s: %s\" % (\n                        sn, e))"}
{"prompt": "def get_key_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type, and returns the first key in that list\n        Note, if there are many keys that match the criteria, the one you get back will be random from that list\n        :returns: A key object that matches the criteria\n        \"\"\"", "output": "\n        for key in self.keys:\n            if key.purpose == purpose and key.key_type == key_type:\n                return key\n        return None\n\n", "solution": "\n        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]\n        try:\n            return key[0]\n        except IndexError:\n            return None"}
{"prompt": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"", "output": "\n        # Get the trajectory data for the specified range\n        trj_data = traj[start:stop]\n\n        # Get the number of frames in the trajectory\n        num_frames = len(trj_data)\n\n        # Get the time of the first frame\n        first_frame_time = trj_data[0][0]\n\n        # Loop through the trajectory data and add each frame to the `run` overview table\n        for i in range(num_frames):\n            frame_time = trj_data[i][0]\n            frame_data = tr", "solution": "\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()"}
{"prompt": "def to_service(self, service, version):\n        \"\"\"Sets the service name and version the request should target\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"", "output": "\n        self.service = service\n        self.version = version\n        return self\n\n    def to_dict(self):\n        \"\"\"Returns a dictionary representation of the request\n\n        Returns:\n            A dictionary with the following keys:\n            - service (str): the name of the service as displayed in the services.json file\n            - version (str): the version of the service as displayed in the services.json file\n            - method (str): the HTTP method used for the request (e.g. GET, POST, PUT, DELETE)\n            - path (str): the", "solution": "\n        service_url = self._service_locator.get_service_url(service, version)\n        return self.__copy_and_set('service_url', self.__strip_trailing_slashes(service_url))"}
{"prompt": "def _cmd(self, cmd, *args, **kw):\n        \"\"\"\n        write a single command, with variable number of arguments. after the\n        command, the device must return ACK\n        \"\"\"", "output": "\n        if cmd == \"ACK\":\n            return\n        if cmd == \"SET\":\n            if len(args)!= 1:\n                raise ValueError(\"SET requires one argument\")\n            if not isinstance(args[0], int):\n                raise TypeError(\"SET argument must be an integer\")\n            self.registers[args[0]] = 1\n        elif cmd == \"GET\":\n            if len(args)!= 1:\n                raise ValueError(\"GET requires one argument\")\n            if not isinstance(args[0], int):\n                raise TypeError", "solution": "\n        ok = kw.setdefault('ok', False)\n\n        self._wakeup()\n        if args:\n            cmd = \"%s %s\" % (cmd, ' '.join(str(a) for a in args))\n        for i in xrange(3):\n            log.info(\"send: \" + cmd)\n            self.port.write(cmd + '\\n')\n            if ok:\n                ack = self.port.read(len(self.OK))  # read OK\n                log_raw('read', ack)\n                if ack == self.OK:\n                    return\n            else:\n                ack = self.port.read(len(self.ACK))  # read ACK\n                log_raw('read', ack)\n                if ack == self.ACK:\n                    return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        for line in lines:\n            self.prefilter_line(line, continue_prompt)\n\n    def prefilter_line(self, line, continue_prompt):\n        \"\"\"Prefilter a single line of text.\n\n        This is the main entry point for prefiltering a single line of\n        input.  This simply calls :meth:`prefilter_word` for each word of\n        input.\n\n        This covers cases where there are multiple words in the user entry,\n        which is the case when the user goes back to a single line of\n        ", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def app_template_filter(self, name=None):\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"", "output": "\n        if name is None:\n            name = self.name\n        app.template_filters[name] = self\n\n    def template_filter(self, name=None):\n        \"\"\"Return the registered template filter, or the default filter if\n        it has not been registered.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        :return: the registered template filter, or the default filter if\n                 it has not been registered.\n        \"\"\"\n        if name is None:\n            name = self.name\n        return", "solution": "\n        def decorator(f):\n            self.add_app_template_filter(f, name=name)\n            return f\n        return decorator"}
{"prompt": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"", "output": "\n  return x[:index] + (replacement,) + x[index+1:]\n\n", "solution": "\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new"}
{"prompt": "def converted(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"", "output": "\n        return {\n            \"name\": str(self.name),\n            \"age\": int(self.age),\n            \"gender\": str(self.gender),\n            \"city\": str(self.city),\n            \"country\": str(self.country),\n            \"price\": float(self.price)\n        }\n\n", "solution": "\n        if self._converted is None:\n            self._converted = self.converter(self.name, self.value)\n        return self._converted"}
{"prompt": "def categories(self):\n        \"\"\"\n        Return the list of levels for an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"", "output": "\n        raise ValueError(\"Cannot apply 'categories' to a multi-column categorical frame.\")\n\n    def get_dtype(self):\n        \"\"\"\n        Return the data type of the column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"\n        raise ValueError(\"Cannot apply 'get_dtype' to a multi-column categorical frame.\")\n\n    def get_category(self, value):\n        \"\"\"\n        Return the category for a given value in the column.\n\n        This function can only be applied to single-column categ", "solution": "\n        if self.ncols != 1:\n            raise H2OValueError(\"This operation only applies to a single factor column\")\n        if self.types[self.names[0]] != \"enum\":\n            raise H2OValueError(\"Input is not a factor. This operation only applies to a single factor column\")\n        return self.levels()[0]"}
{"prompt": "def mixin(cls, mixin_cls):\n        \"\"\"Decorator for mixing in additional functionality into field type\n\n        Example:\n\n        >>> @Integer.mixin\n        ... class IntegerPostgresExtensions:\n        ...     postgres_type = 'INT'\n        ...\n        ...     def postgres_dump(self, obj):\n        ...         self.dump(obj) + \"::integer\"\n\n        Is roughly equivalent to:\n\n        >>> Integer.postgres_type = 'INT'\n        ...\n        ... def postgres_dump(self, obj):\n        ...     self.dump(obj) + \"::integer\"\n        ...\n        ... Integer.postgres_dump = postgres_dump\n\n        \"\"\"", "output": "):\n       ...     self.dump(obj) + \"::integer\"\n\n        :param mixin_cls: The class to mix in\n        :return: The class with the mixin added\n        \"\"\"\n        field_type = mixin_cls.__name__\n        class NewClass(cls, mixin_cls):\n            pass\n        return NewClass\n\n    return mixin\n\n", "solution": "\n        for item_name in dir(mixin_cls):\n            if item_name.startswith(\"__\"):\n                # don't copy magic properties\n                continue\n            item = getattr(mixin_cls, item_name)\n\n            if isinstance(item, types.MethodType):\n                # unbound method will cause problems\n                # so get the underlying function instead\n                item = item.im_func\n\n            setattr(cls, item_name, item)\n        return mixin_cls"}
{"prompt": "def get_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the annotations within the times.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_ref_annotation_data_between_times` for the format.\n\n        :param str id_tier: Name of the tier.\n        :param int start: Start time of the annotation.\n        :param int end: End time of the annotation.\n        :returns: List of annotations within that time.\n        :raises KeyError: If the tier is non existent.\n        \"\"\"", "output": " ValueError: If start or end is before the first or last time in the tier.\n        \"\"\"\n        start_index = self.get_time_index(start)\n        end_index = self.get_time_index(end)\n        if start_index is None or end_index is None:\n            raise ValueError(\"Invalid start or end time\")\n        if start_index > end_index:\n            raise ValueError(\"Start time must be before end time\")\n        annotations = []\n        for time_index in range(start_index, end_index +", "solution": "\n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_between_times(\n                id_tier, start, end)\n        anns = ((self.timeslots[a[0]], self.timeslots[a[1]], a[2])\n                for a in self.tiers[id_tier][0].values())\n        return sorted(a for a in anns if a[1] >= start and a[0] <= end)"}
{"prompt": "def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"", "output": "\n    # Get the name of the `recur` node\n    name = node.func.id\n\n    # Get the `deftype` attribute of the `recur` node\n    deftype = None\n    for attr in node.func.attr:\n        if attr.name == \"deftype\":\n            deftype = attr.value\n            break\n\n    # If the `deftype` attribute is not found, return None\n    if deftype is None:\n        return None\n\n    # Generate the Python AST node for the `deft", "solution": "\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"prompt": "def _random_adjspecies_pair():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"", "output": "\n    species = random.choice(species_list)\n    describer = random.choice(describer_list)\n    return (species, describer)\n\n", "solution": "\n    describer, desc_position = random_describer()\n    if desc_position == 'prefix':\n        return (describer, random_species())\n    elif desc_position == 'suffix':\n        return (random_species(), describer)"}
{"prompt": "def _run_cmd_line_code(self):\n        \"\"\"Run code or file specified at the command-line\"\"\"", "output": "\n        if not self.cmd_line_args:\n            return\n        cmd = self.cmd_line_args[0]\n        args = self.cmd_line_args[1:]\n        if cmd == 'run':\n            self.run_code(args)\n        elif cmd == 'run_file':\n            self.run_file(args)\n        else:\n            raise ValueError(f\"Invalid command: {cmd}\")\n\n    def run_code(self, args):\n        \"\"\"Run code specified by the given arguments\"\"\"\n        # Your code", "solution": "\n        if self.code_to_run:\n            line = self.code_to_run\n            try:\n                self.log.info(\"Running code given at command line (c=): %s\" %\n                              line)\n                self.shell.run_cell(line, store_history=False)\n            except:\n                self.log.warn(\"Error in executing line in user namespace: %s\" %\n                              line)\n                self.shell.showtraceback()\n\n        # Like Python itself, ignore the second if the first of these is present\n        elif self.file_to_run:\n            fname = self.file_to_run\n            try:\n                self._exec_file(fname)\n            except:\n                self.log.warn(\"Error in executing file in user namespace: %s\" %\n                              fname)\n                self.shell.showtraceback()"}
{"prompt": "def _first_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute first glimpse position using down-sampled image.\n        \"\"\"", "output": "\n        # Compute down-sampled image features\n        features = self.down_sampler(x_t)\n        # Compute attention weights for each glimpse\n        attention_weights = self.attention(features)\n        # Compute weighted sum of features for glimpse\n        glimpse_features = features * attention_weights\n        # Compute first glimpse by taking average of glimpse features\n        first_glimpse = glimpse_features.mean(dim=1)\n        return first_glimpse\n\n    def _second_glimpse_sensor(self,", "solution": "\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad"}
{"prompt": "def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"", "output": "\n        return f\"{self.m_req.name} {self.m_req.format} {self.m_req.crc}\"\n\n", "solution": "\n        for fld in self.m_blk_a:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                self.m_req[fld] = self.m_blk_a[fld]\n        pass"}
{"prompt": "def add_attachment(self, filename, open_file):\n        \"\"\"\n        Adds an attachment to this card.\n        \"\"\"", "output": "\n        self.attachments.append((filename, open_file))\n\n    def get_attachments(self):\n        \"\"\"\n        Returns a list of tuples containing the filename and the file object for each attachment.\n        \"\"\"\n        return self.attachments\n\n", "solution": "\n        fields = {\n            'api_key': self.client.api_key,\n            'token': self.client.user_auth_token\n        }\n\n        content_type, body = self.encode_multipart_formdata(\n            fields=fields,\n            filename=filename,\n            file_values=open_file\n        )\n\n        return self.fetch_json(\n            uri_path=self.base_uri + '/attachments',\n            http_method='POST',\n            body=body,\n            headers={'Content-Type': content_type},\n        )"}
{"prompt": "def configure(self, options, conf):\n        \"\"\"\n        Configure plugin.\n        \"\"\"", "output": "\n        # Your code here\n        pass\n\ndef get_configured_plugins(plugins, config):\n    \"\"\"\n    Returns a list of plugin objects that match the configuration.\n\n    Args:\n        plugins (List[Plugin]): A list of Plugin objects.\n        config (Dict): A dictionary of configuration.\n\n    Returns:\n        List[Plugin]: A list of Plugin objects that match the configuration.\n    \"\"\"\n    matching_plugins = []\n    for plugin in plugins:\n        if plugin.configure(config):\n            matching_plugins.append(plugin", "solution": "\n        try:\n            self.status.pop('active')\n        except KeyError:\n            pass\n        super(Coverage, self).configure(options, conf)\n        if conf.worker:\n            return\n        if self.enabled:\n            try:\n                import coverage\n            except ImportError:\n                log.error(\"Coverage not available: \"\n                          \"unable to import coverage module\")\n                self.enabled = False\n                return\n        self.conf = conf\n        self.coverErase = options.cover_erase\n        self.coverTests = options.cover_tests\n        self.coverPackages = []\n        if options.cover_packages:\n            for pkgs in [tolist(x) for x in options.cover_packages]:\n                self.coverPackages.extend(pkgs)\n        self.coverInclusive = options.cover_inclusive\n        if self.coverPackages:\n            log.info(\"Coverage report will include only packages: %s\",\n                     self.coverPackages)\n        self.coverHtmlDir = None\n        if options.cover_html:\n            self.coverHtmlDir = options.cover_html_dir\n            log.debug('Will put HTML coverage report in %s', self.coverHtmlDir)\n        self.coverBranches = options.cover_branches\n        self.coverXmlFile = None\n        if options.cover_min_percentage:\n            self.coverMinPercentage = int(options.cover_min_percentage.rstrip('%'))\n        if options.cover_xml:\n            self.coverXmlFile = options.cover_xml_file\n            log.debug('Will put XML coverage report in %s', self.coverXmlFile)\n        if self.enabled:\n            self.status['active'] = True\n            self.coverInstance = coverage.coverage(auto_data=False,\n                branch=self.coverBranches, data_suffix=None)"}
{"prompt": "def _select_next_server(self):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"", "output": "\n        server = None\n        for pool in self.server_pool:\n            if pool.is_available():\n                server = pool.select_server()\n                if server:\n                    break\n        if server:\n            self.server = server\n            return True\n        else:\n            return False\n\n    def _connect_to_server(self):\n        \"\"\"\n        Attempts to connect to the selected server.\n        \"\"\"\n        if not self.server:\n            return False\n        try:\n            self.server.connect()\n            return True\n        except:\n", "solution": "\n\n        while True:\n            if len(self._server_pool) == 0:\n                self._current_server = None\n                raise ErrNoServers\n\n            now = time.monotonic()\n            s = self._server_pool.pop(0)\n            if self.options[\"max_reconnect_attempts\"] > 0:\n                if s.reconnects > self.options[\"max_reconnect_attempts\"]:\n                    # Discard server since already tried to reconnect too many times\n                    continue\n\n            # Not yet exceeded max_reconnect_attempts so can still use\n            # this server in the future.\n            self._server_pool.append(s)\n            if s.last_attempt is not None and now < s.last_attempt + self.options[\"reconnect_time_wait\"]:\n                # Backoff connecting to server if we attempted recently.\n                yield from asyncio.sleep(self.options[\"reconnect_time_wait\"], loop=self._loop)\n            try:\n                s.last_attempt = time.monotonic()\n                r, w = yield from asyncio.open_connection(\n                    s.uri.hostname,\n                    s.uri.port,\n                    loop=self._loop,\n                    limit=DEFAULT_BUFFER_SIZE)\n                self._current_server = s\n\n                # We keep a reference to the initial transport we used when\n                # establishing the connection in case we later upgrade to TLS\n                # after getting the first INFO message. This is in order to\n                # prevent the GC closing the socket after we send CONNECT\n                # and replace the transport.\n                #\n                # See https://github.com/nats-io/asyncio-nats/issues/43\n                self._bare_io_reader = self._io_reader = r\n                self._bare_io_writer = self._io_writer = w\n                break\n            except Exception as e:\n                s.last_attempt = time.monotonic()\n                s.reconnects += 1\n\n                self._err = e\n                if self._error_cb is not None:\n                    yield from self._error_cb(e)\n                continue"}
{"prompt": "def time_remaining_est(self, completed_iter):\n        \"\"\"Estimate the remaining time left.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            est_time: Estimated time remaining.\n        \"\"\"", "output": "\n        est_time = self.total_time - (completed_iter * self.time_per_iter)\n        return est_time\n\n", "solution": "\n        if completed_iter:\n            t_r_est = (time.time() - self.t_start) / \\\n                completed_iter*(self.iter-completed_iter)\n        else:\n            t_r_est = 0\n        date_time = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=t_r_est)\n        time_string = \"%02d:%02d:%02d:%02d\" % \\\n            (date_time.day - 1, date_time.hour, date_time.minute, date_time.second)\n\n        return time_string"}
{"prompt": "def bkg_subtract(self, analytes=None, errtype='stderr', focus_stage='despiked'):\n        \"\"\"\n        Subtract calculated background from data.\n\n        Must run bkg_calc first!\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte(s) to subtract.\n        errtype : str\n            Which type of error to propagate. default is 'stderr'.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.\n        \"\"\"", "output": "despiked'.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if analytes is None:\n            analytes = self.analytes\n        if isinstance(analytes, str):\n            analytes = [analytes]\n        for a in analytes:\n            if a not in self.analytes:\n                raise ValueError(f\"{a} not found in data.\")\n        if focus_stage == 'despiked':\n            self.data -= self.bkg_calc(analytes, errtype)\n        elif focus_stage ==", "solution": "\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if focus_stage == 'despiked':\n            if 'despiked' not in self.stages_complete:\n                focus_stage = 'rawdata'\n\n        # make uncertainty-aware background interpolators\n        bkg_interps = {}\n        for a in analytes:\n            bkg_interps[a] = un_interp1d(x=self.bkg['calc']['uTime'],\n                                         y=un.uarray(self.bkg['calc'][a]['mean'],\n                                                     self.bkg['calc'][a][errtype]))\n        self.bkg_interps = bkg_interps\n\n        # apply background corrections\n        with self.pbar.set(total=len(self.data), desc='Background Subtraction') as prog:\n            for d in self.data.values():\n                # [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), None, focus_stage=focus_stage) for a in analytes]\n                [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), ~d.sig, focus_stage=focus_stage) for a in analytes]\n                d.setfocus('bkgsub')\n\n                prog.update()\n\n        self.stages_complete.update(['bkgsub'])\n        self.focus_stage = 'bkgsub'\n        return"}
{"prompt": "def find_bp(self, filename, lineno, frame):\n        \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n        Called only if we know there is a bpt at this\n        location.  Returns breakpoint that was triggered and a flag\n        that indicates if it is ok to delete a temporary breakpoint.\n\n        \"\"\"", "output": "\n        bp = frame.f_locals.get('bp')\n        if bp is None:\n            return None, False\n        if bp.is_delete:\n            return bp, True\n        return bp, False\n\n    def __call__(self, frame):\n        \"\"\"Called when a breakpoint is hit.\n\n        Called only if we know there is a bpt at this\n        location.  Returns a tuple of the form (filename, lineno,\n        frame), where filename is the name of the file being\n        executed, lineno", "solution": "\n        possibles = self.bplist[filename, lineno]\n        for i in range(0, len(possibles)):\n            b = possibles[i]\n            if not b.enabled:\n                continue\n            if not checkfuncname(b, frame):\n                continue\n            # Count every hit when bp is enabled\n            b.hits += 1\n            if not b.condition:\n                # If unconditional, and ignoring, go on to next, else\n                # break\n                if b.ignore > 0:\n                    b.ignore = b.ignore -1\n                    continue\n                else:\n                    # breakpoint and marker that's ok to delete if\n                    # temporary\n                    return (b, True)\n            else:\n                # Conditional bp.\n                # Ignore count applies only to those bpt hits where the\n                # condition evaluates to true.\n                try:\n                    val = eval(b.condition, frame.f_globals, frame.f_locals)\n                    if val:\n                        if b.ignore > 0:\n                            b.ignore = b.ignore -1\n                            # continue\n                        else:\n                            return (b, True)\n                    # else:\n                    #   continue\n                except:\n                    # if eval fails, most conservative thing is to\n                    # stop on breakpoint regardless of ignore count.\n                    # Don't delete temporary, as another hint to user.\n                    return (b, False)\n                pass\n            pass\n        return (None, None)"}
{"prompt": "def _to_dataframe(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"", "output": "\n        # Your code here to convert the dataset to a pandas DataFrame\n        pass\n\n    def _to_csv(self, filename: str):\n        \"\"\"Read and write the dataset contents to a CSV file with the given filename.\"\"\"\n        # Your code here to convert the dataset to a CSV file with the given filename\n        pass\n\n    def _to_excel(self, filename: str):\n        \"\"\"Read and write the dataset contents to an Excel file with the given filename.\"\"\"\n        # Your code here to convert the dataset to an Excel file with the given filename\n        pass", "solution": "\n        #TODO: figure out why passing in the opened stream directly gives invalid data\n        data = self.read_as_binary()\n        reader = BytesIO(data)\n        return deserialize_dataframe(reader, self.data_type_id)"}
{"prompt": "def create_tfs_git_client(url, token=None):\n    \"\"\"\n    Creates a TFS Git Client to pull Git repo info\n    \"\"\"", "output": "\n    import tfs\n    import git\n\n    if token:\n        git.Git.token = token\n\n    # Create a TFS Git client\n    client = tfs.GitClient(url)\n\n    # Get the repository path\n    repo_path = client.get_repository_path(repository_name=\"my_repo\")\n\n    # Get the repository info\n    repo_info = client.get_repository_info(repository_path=repo_path)\n\n    # Get the commit history\n    commit_history", "solution": "\n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'\n        raise RuntimeError(msg, url)\n\n    return tfs_git_client"}
{"prompt": "def canFetchMore(self, index):\n        \"\"\"Return if more data available for *index*.\"\"\"", "output": "\n        return True\n\n    def fetchMore(self, index):\n        \"\"\"Fetch more data for *index*.\"\"\"\n        # Code to fetch more data for *index*\n        pass\n\n    def get_data(self, index):\n        \"\"\"Return the data for *index*.\"\"\"\n        # Code to get the data for *index*\n        pass\n\n", "solution": "\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        return item.canFetchMore()"}
{"prompt": "def info_formatter(info):\n    \"\"\"Produce a sequence of formatted lines from info.\n\n    `info` is a sequence of pairs (label, data).  The produced lines are\n    nicely formatted, ready to print.\n\n    \"\"\"", "output": "\n    for label, data in info:\n        yield label + ': '\n        yield from data.split('\\n')\n        yield ''\n\n", "solution": "\n    label_len = max([len(l) for l, _d in info])\n    for label, data in info:\n        if data == []:\n            data = \"-none-\"\n        if isinstance(data, (list, tuple)):\n            prefix = \"%*s:\" % (label_len, label)\n            for e in data:\n                yield \"%*s %s\" % (label_len+1, prefix, e)\n                prefix = \"\"\n        else:\n            yield \"%*s: %s\" % (label_len, label, data)"}
{"prompt": "async def handle_request(self, request: Request\n                             ) -> typing.Tuple[AsyncGeneratorType, Response]:\n        \"\"\"\n        Wrap request with middleware.\n        :param request:\n        :return:\n        \"\"\"", "output": "\n        # Wrap request with middleware\n        response = await self.app(request)\n\n        # Return response\n        return response.__aiter__(), response\n\n    # Create app instance\n    app = MiddlewareApp(middlewares)\n\n    # Create server\n    server = make_server('', PORT, app)\n\n    # Start server\n    async with server:\n        await server.serve_forever()\n\n", "solution": "\n        callback_result, response = None, None\n\n        await self._run_request_middleware(request)\n        try:\n            callback_result, response = await request.fetch_callback(self.sem)\n        except NotImplementedParseError as e:\n            self.logger.error(e)\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')\n        await self._run_response_middleware(request, response)\n        await self._process_response(request=request, response=response)\n        return callback_result, response"}
{"prompt": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"", "output": "\n    if mongodb is None:\n        raise ValueError(\"mongodb argument is required\")\n    if authdb is None:\n        raise ValueError(\"authdb argument is required\")\n\n    if uri is None:\n        uri = f\"mongodb://{username}:{password}@{host}:{port}/{authdb}\"\n\n    return MongoClient(uri, timeout=timeout, *args, **kwargs)\n\n", "solution": "\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client"}
{"prompt": "def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"", "output": "\n        try:\n            # Construct URL for job search\n            search_url = f\"https://www.googleapis.com/jobs/v1/jobs?q={project_id}&key={self.api_key}\"\n\n            # Construct URL for job details\n            job_url = f\"{search_url}&q={job_id}\"\n\n            # Construct URL for job search results\n            results_url = f\"{job_url}&key={self.api_key}\"\n\n            # Make HTTP GET request to job search results\n            response = requests.", "solution": "\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise"}
{"prompt": "def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"", "output": "_target: datetime.date\n    :return: The date in the list closest to the target date\n    :rtype: datetime.date\n    \"\"\"\n    if before_target is None:\n        before_target = target_dt\n    closest_date = None\n    closest_delta = None\n    for date in date_list:\n        delta = abs(date - target_dt)\n        if closest_delta is None or delta < closest_delta:\n            closest_date = date\n            closest_delta", "solution": "\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()"}
{"prompt": "def _has_parent_of_type(node, node_type, statement):\n    \"\"\"Check if the given node has a parent of the given type.\"\"\"", "output": "\n    for parent in statement.parents:\n        if parent.node_type == node_type:\n            return True\n    return False\n\n", "solution": "\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)"}
{"prompt": "def _wrap_command(cmds, cls, strict=True):\n    \"\"\"Wrap a setup command\n\n    Parameters\n    ----------\n    cmds: list(str)\n        The names of the other commands to run prior to the command.\n    strict: boolean, optional\n        Whether to raise errors when a pre-command fails.\n    \"\"\"", "output": "\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if strict:\n                if not any(cmd in cmds for cmd in args):\n                    raise ValueError(f\"Pre-command '{args[0]}' failed: {cls.__name__} not found.\")\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n", "solution": "\n    class WrappedCommand(cls):\n\n        def run(self):\n            if not getattr(self, 'uninstall', None):\n                try:\n                    [self.run_command(cmd) for cmd in cmds]\n                except Exception:\n                    if strict:\n                        raise\n                    else:\n                        pass\n            # update package data\n            update_package_data(self.distribution)\n\n            result = cls.run(self)\n            return result\n    return WrappedCommand"}
{"prompt": "def as_tuple(self):\n        \"\"\"\n        :rtype: (str, object)\n        \"\"\"", "output": "\n        return (self.name, self.value)\n\n", "solution": "\n        if self._as_tuple is None:\n            self._as_tuple = self.converted.items()[0]\n        return self._as_tuple"}
{"prompt": "def add_virtual_columns_cartesian_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", alpha=\"l\", delta=\"b\", distance=\"distance\", radians=False, center=None, center_name=\"solar_position\"):\n        \"\"\"Convert cartesian to spherical coordinates.\n\n\n\n        :param x:\n        :param y:\n        :param z:\n        :param alpha:\n        :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).\n        :param distance:\n        :param radians:\n        :param center:\n        :param center_name:\n        :return:\n        \"\"\"", "output": ":param distance: name for distance, ranges from 0 to infinity.\n        :param radians: if True, alpha and delta are in radians.\n        :param center: name of the column to use as the center of the spherical coordinate system.\n        :param center_name: name of the column to use as the name of the center of the spherical coordinate system.\n        :return:\n        \"\"\"\n        if radians:\n            alpha = alpha * (3.14159 / 180)\n            delta = delta * (3.14159 / 180)\n\n        if center is None", "solution": "\n        transform = \"\" if radians else \"*180./pi\"\n\n        if center is not None:\n            self.add_variable(center_name, center)\n        if center is not None and center[0] != 0:\n            x = \"({x} - {center_name}[0])\".format(**locals())\n        if center is not None and center[1] != 0:\n            y = \"({y} - {center_name}[1])\".format(**locals())\n        if center is not None and center[2] != 0:\n            z = \"({z} - {center_name}[2])\".format(**locals())\n        self.add_virtual_column(distance, \"sqrt({x}**2 + {y}**2 + {z}**2)\".format(**locals()))\n        # self.add_virtual_column(alpha, \"((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}\".format(**locals()))\n        self.add_virtual_column(alpha, \"arctan2({y}, {x}){transform}\".format(**locals()))\n        self.add_virtual_column(delta, \"(-arccos({z}/{distance})+pi/2){transform}\".format(**locals()))"}
{"prompt": "def unix_ts(dtval):\n    \"\"\"Convert datetime into a unix timestamp.\n    This is the equivalent to Python 3's int(datetime.timestamp()).\n\n    :param dt: datetime to convert\n    \"\"\"", "output": "\n    return int(dt.timestamp())\n\n", "solution": "\n    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())\n    delta = (dtval - epoch)\n    return delta.days * 24 * 3600 + delta.seconds"}
{"prompt": "def path_dispatch_kwarg(mname, path_default, returns_model):\n    \"\"\"\n    Parameterized decorator for methods that accept path as a second\n    argument.\n    \"\"\"", "output": "\n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n            if 'path' in kwargs:\n                path = kwargs.pop('path')\n            else:\n                path = path_default\n            return func(self, path, *args, **kwargs)\n        wrapper.mname = mname\n        wrapper.path_default = path_default\n        wrapper.returns_model = returns_model\n        return wrapper\n    return decorator\n\n", "solution": "\n    def _wrapper(self, path=path_default, **kwargs):\n        prefix, mgr, mgr_path = _resolve_path(path, self.managers)\n        result = getattr(mgr, mname)(path=mgr_path, **kwargs)\n        if returns_model and prefix:\n            return _apply_prefix(prefix, result)\n        else:\n            return result\n    return _wrapper"}
{"prompt": "def get_label ( self, object ):\n        \"\"\" Gets the label to display for a specified object.\n        \"\"\"", "output": "\n        return object.label\n\n", "solution": "\n        label = self.label\n        if label[:1] == '=':\n            return label[1:]\n\n        label = xgetattr( object, label, '' )\n\n        if self.formatter is None:\n            return label\n\n        return self.formatter( object, label )"}
{"prompt": "def create(self, request):\n        \"\"\"\n        Log in django staff user\n        \"\"\"", "output": "\n        # Code to log in to django staff user\n        pass\n\n    def logout(self, request):\n        \"\"\"\n        Log out django staff user\n        \"\"\"\n        # Code to log out of django staff user\n        pass\n\n    def is_staff(self, request):\n        \"\"\"\n        Returns True if the user is a staff member, False otherwise\n        \"\"\"\n        # Code to check if the user is a staff member\n        return True or False\n\n", "solution": "\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n        # from django.utils.decorators import method_decorator\n        # from django.views.decorators.debug import sensitive_post_parameters\n        # sensitive_post_parameters_m = method_decorator(sensitive_post_parameters())\n\n        login_form = AuthenticationForm(request, data=request.data)\n\n        if not login_form.is_valid():\n            raise serializers.ValidationError(login_form.errors)\n\n        auth_login(request, login_form.get_user())\n\n        serializer = UserSerializer(request.user)\n        return Response(serializer.data, status=status.HTTP_200_OK)"}
{"prompt": "def remove(self, recursive=True, ignore_error=True):\n        \"\"\"\n        Remove the directory.\n        \"\"\"", "output": "\n        if ignore_error:\n            shutil.rmtree(self.path, ignore_errors=True)\n        else:\n            shutil.rmtree(self.path)\n\n    def move(self, new_parent, new_name):\n        \"\"\"\n        Move the directory to a new parent directory.\n\n        Args:\n            new_parent (str): The new parent directory.\n            new_name (str): The new name of the directory.\n\n        Returns:\n            str: The new path of the moved directory.\n        \"\"\"\n        new_path", "solution": "\n        try:\n            if recursive or self._cleanup == 'recursive':\n                shutil.rmtree(self.path)\n            else:\n                os.rmdir(self.path)\n        except Exception as e:\n            if not ignore_error:\n                raise e"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        if self.farthest is None:\n            self.farthest = status.farthest\n        if self.expected is None:\n            self.expected = status.expected\n        if status.farthest is None or status.farthest > self.farthest:\n            self.farthest = status.farthest\n        if status.expected is None or status.expected > self.expected:\n            self.expected = status.expected\n        return self\n\n    def __str__(self) -> str:\n        return f\"Failed to", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def activity(self, value):\n        \"\"\"\n        Used to set TurnContext._activity when a context object is created. Only takes instances of Activities.\n        :param value:\n        :return:\n        \"\"\"", "output": "\n        if isinstance(value, Activity):\n            self.activity = value\n        else:\n            raise TypeError(\"activity must be an instance of Activity\")\n\n    def __str__(self):\n        return f\"TurnContext(activity={self.activity})\"\n\n", "solution": "\n        if not isinstance(value, Activity):\n            raise TypeError('TurnContext: cannot set `activity` to a type other than Activity.')\n        else:\n            self._activity = value"}
{"prompt": "def _post_cutout_no_chunking_blosc(self, token, channel,\n                                       x_start, y_start, z_start,\n                                       data, resolution):\n        \"\"\"\n        Accepts data in zyx. !!!\n        \"\"\"", "output": "\n        # Calculate the dimensions of the cutout\n        x_end = x_start + resolution\n        y_end = y_start + resolution\n        z_end = z_start + resolution\n\n        # Cut out the data\n        cutout = data[z_start:z_end, y_start:y_end, x_start:x_end]\n\n        # Compress the cutout using Blosc\n        compressed_cutout = Blosc().compress(cutout)\n\n        # Return the compressed cutout\n        return compressed_cutout\n\n    def", "solution": "\n        data = numpy.expand_dims(data, axis=0)\n        blosc_data = blosc.pack_array(data)\n        url = self.url(\"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\".format(\n            token, channel,\n            resolution,\n            x_start, x_start + data.shape[3],\n            y_start, y_start + data.shape[2],\n            z_start, z_start + data.shape[1]\n        ))\n\n        req = self.remote_utils.post_url(url, data=blosc_data, headers={\n            'Content-Type': 'application/octet-stream'\n        })\n\n        if req.status_code is not 200:\n            raise RemoteDataUploadError(req.text)\n        else:\n            return True"}
{"prompt": "def current(self):\n        \"\"\" bool: Whether this window is the window in which commands are being executed. \"\"\"", "output": "\n        return self.window_id is not None and self.window_id == self.window.window_id\n\n    def set_window_id(self, window_id):\n        \"\"\" Set the ID of the window.\n\n        Args:\n            window_id (int): The new ID of the window.\n\n        Returns:\n            bool: True if the window was successfully set, False otherwise.\n        \"\"\"\n        if not isinstance(window_id, int):\n            return False\n        if window_id < 0:\n            return False\n        self.window_id", "solution": "\n        try:\n            return self.driver.current_window_handle == self.handle\n        except self.driver.no_such_window_error:\n            return False"}
{"prompt": "def autodetect_files(self):\n        \"\"\" Attempt to detect requirements files in the current working directory \"\"\"", "output": "\n        files = os.listdir('.')\n        for file in files:\n            if file.endswith('.txt') and '-' in file:\n                try:\n                    with open(file, 'r') as f:\n                        requirements = f.read()\n                        if requirements:\n                            self.requirements.append(file)\n                except:\n                    pass\n\n    def parse_requirements(self):\n        \"\"\" Parse the requirements file and extract the package names and versions \"\"\"\n        for file in self.requirements:\n            with open(file,", "solution": "\n        if self._is_valid_requirements_file('requirements.txt'):\n            self.filenames.append('requirements.txt')\n\n        if self._is_valid_requirements_file('requirements.pip'):  # pragma: nocover\n            self.filenames.append('requirements.pip')\n\n        if os.path.isdir('requirements'):\n            for filename in os.listdir('requirements'):\n                file_path = os.path.join('requirements', filename)\n                if self._is_valid_requirements_file(file_path):\n                    self.filenames.append(file_path)\n        self._check_inclusions_recursively()"}
{"prompt": "def format_extension(self):\n        \"\"\"The format extension of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n        \"\"\"", "output": "\n        return self.extension[1:]\n\n    def get_asset_path(self):\n        \"\"\"The path to the asset file.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.get_asset_path()\n           'models.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.get_asset_path()\n           'min.js'\n        \"\"\"\n        ", "solution": "\n        for extension in reversed(self.extensions):\n            compiler = self.environment.compilers.get(extension)\n            if not compiler and self.environment.mimetypes.get(extension):\n                return extension"}
