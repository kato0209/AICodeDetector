{"prompt": "def print_help(self, classes=False):\n        \"\"\"Print the help for each Configurable class in self.classes.\n\n        If classes=False (the default), only flags and aliases are printed.\n        \"\"\"", "output": "\n        flags = self.flags.keys()\n        aliases = self.aliases.keys()\n        aliases.sort()\n        flags.sort()\n        if classes:\n            classes.sort()\n            flags = [c for c in classes if c in flags]\n            aliases = [a for a in aliases if a in flags]\n        flags = [f for f in self.flags if f not in aliases]\n        aliases = [a for a in self.aliases if a not in flags]\n        flags = [f for f in self.flags if f not in aliases]\n        flags = [f for f in self.flags if f not in aliases]\n        flags = [f for f in self.flags if f not in aliases]\n        flags = [f for f in self.flag", "solution": "\n        self.print_subcommands()\n        self.print_options()\n\n        if classes:\n            if self.classes:\n                print \"Class parameters\"\n                print \"----------------\"\n                print\n                for p in wrap_paragraphs(self.keyvalue_description):\n                    print p\n                    print\n\n            for cls in self.classes:\n                cls.class_print_help()\n                print\n        else:\n            print \"To see all available configurables, use `--help-all`\"\n            print"}
{"prompt": "def derive_fallback_fernet_keys(passwords, user_id):\n    \"\"\"\n    Derive a list of per-user Fernet keys from a list of master keys and a\n    username.\n\n    If a None is encountered in ``passwords``, it is forwarded.\n\n    Parameters\n    ----------\n    passwords : list[unicode]\n        List of ascii-encodable keys to derive.\n    user_id : unicode or None\n        ascii-encodable user_id to use as salt\n    \"\"\"", "output": "\n    derived = []\n    for password in passwords:\n        if password is None:\n            derived.append(password)\n            continue\n        derived.append(derive_fernet_key(password, user_id))\n    return derived\n\n\ndef derive_fernet_key(password, user_id):\n    \"\"\"\n    Derive a per-user Fernet key from a master key.\n\n    Parameters\n    ----------\n    password : unicode\n        ascii-encodable password to derive the key from.\n    user_id : unicode or None\n        ascii-encodable user_id to use as salt\n\n    Returns\n    -------\n    unicode\n        derive the key from the password.\n    \"\"\"\n    salt = _derive_sal", "solution": "\n    # Normally I wouldn't advocate for these kinds of assertions, but we really\n    # really really don't want to mess up deriving encryption keys.\n    assert isinstance(passwords, (list, tuple)), \\\n        \"Expected list or tuple of keys, got %s.\" % type(passwords)\n\n    def derive_single_allow_none(k):\n        if k is None:\n            return None\n        return derive_single_fernet_key(k, user_id).decode('ascii')\n\n    return list(map(derive_single_allow_none, passwords))"}
{"prompt": "def format_help(self, checkerref=False):\n        \"\"\"return the help string for the given message id\"\"\"", "output": "\n        help = super(Checker, self).format_help(checkerref)\n        if checkerref:\n            help = help.replace('%', '%%')\n        return help\n\n    def format_usage(self, usage):\n        \"\"\"return a usage string for the given message id\"\"\"\n        return usage.replace('%', '%%')\n\n    def format_heading(self, heading):\n        \"\"\"return a string with heading wrapped in quotes\"\"\"\n        return \"%s\\n%s\" % (heading, \"=\" * len(heading))\n\n    def report_error(self, msg, *args, **kwds):\n        \"\"\"report an error, which is an error that should be ignored.  The\n           arguments are a standard string which should uniquely identify\n           the error, followed by zero or more substitution arguments\n           ", "solution": "\n        desc = self.descr\n        if checkerref:\n            desc += \" This message belongs to the %s checker.\" % self.checker.name\n        title = self.msg\n        if self.symbol:\n            msgid = \"%s (%s)\" % (self.symbol, self.msgid)\n        else:\n            msgid = self.msgid\n        if self.minversion or self.maxversion:\n            restr = []\n            if self.minversion:\n                restr.append(\"< %s\" % \".\".join([str(n) for n in self.minversion]))\n            if self.maxversion:\n                restr.append(\">= %s\" % \".\".join([str(n) for n in self.maxversion]))\n            restr = \" or \".join(restr)\n            if checkerref:\n                desc += \" It can't be emitted when using Python %s.\" % restr\n            else:\n                desc += \" This message can't be emitted when using Python %s.\" % restr\n        desc = normalize_text(\" \".join(desc.split()), indent=\"  \")\n        if title != \"%s\":\n            title = title.splitlines()[0]\n\n            return \":%s: *%s*\\n%s\" % (msgid, title.rstrip(\" \"), desc)\n        return \":%s:\\n%s\" % (msgid, desc)"}
{"prompt": "def f_get(self, *args):\n        \"\"\"Returns annotations\n\n        If len(args)>1, then returns a list of annotations.\n\n        `f_get(X)` with *X* integer will return the annotation with name `annotation_X`.\n\n        If the annotation contains only a single entry you can call `f_get()` without arguments.\n        If you call `f_get()` and the annotation contains more than one element a ValueError is\n        thrown.\n\n        \"\"\"", "output": "\n        if len(args) == 1:\n            return args[0]\n        elif len(args) == 0:\n            return {}\n        else:\n            raise ValueError(\"f_get() takes 0 or 1 arguments\")\n\n    def f_set(self, *args):\n        \"\"\"Sets annotations\n\n        `f_set(X, Y)` with *X* integer and *Y* string will set the annotation with name `annotation_X`\n        to value *Y*.\n\n        If the annotation contains only a single entry you can call `f_set()` without arguments.\n        If you call `f_set()` and the annotation contains more than one element a ValueError is\n        thrown.\n\n        \"\"\"\n        if len(args) == 1:\n            annotation = args[0]\n            annotation.name = annotation.name.replace(\"_\", \" \")\n            annotation.value = annotation.", "solution": "\n\n        if len(args) == 0:\n            if len(self._dict) == 1:\n                return self._dict[list(self._dict.keys())[0]]\n            elif len(self._dict) > 1:\n                raise ValueError('Your annotation contains more than one entry: '\n                                 '`%s` Please use >>f_get<< with one of these.' %\n                                 (str(list(self._dict.keys()))))\n            else:\n                raise AttributeError('Your annotation is empty, cannot access data.')\n\n        result_list = []\n        for name in args:\n            name = self._translate_key(name)\n            try:\n                result_list.append(self._dict[name])\n            except KeyError:\n                raise AttributeError('Your annotation does not contain %s.' % name)\n\n        if len(args) == 1:\n            return result_list[0]\n        else:\n            return tuple(result_list)"}
{"prompt": "def size(full_path):\r\n    \"\"\"Shows file size.\r\n    :param full_path: full path to the file.\r\n    \"\"\"", "output": "\r\n    statinfo = os.stat(full_path)\r\n    size = statinfo.st_size\r\n    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']\r\n    i = 0\r\n    while size >= 1024 and i < len(units):\r\n        size /= 1024.0\r\n        i += 1\r\n    return \"%3.1f %s\" % (size, units[i])\r\n\r\n\r\ndef file_exists(full_path):\r\n    \"\"\"Checks if file exists or not.\r\n    :param full_path: full path to the file.\r\n    \"\"\"\r\n    return os.path.isfile(full_path)\r\n\r\n\r\ndef file_is_empty(full_path):\r\n    \"\"\"Checks if file is empty or not.\r\n    :param full_path: full path to the file.\r\n    \"\"\"\r\n    return os.stat(full_path", "solution": "\r\n\r\n    file_size = os.path.getsize(full_path)\r\n    str_file_size = str(file_size)\r\n    print(str_file_size, 'b')\r\n\r\n    # Show size in b, kb, mb or gb depending on the dimension\r\n    if len(str_file_size) >= 10:\r\n        print('{0:.2f}'.format(file_size / 1073741824), 'gb')\r\n    elif len(str_file_size) >= 7:\r\n        print('{0:.2f}'.format(file_size / 1048576), 'mb')\r\n    elif len(str_file_size) >= 4:\r\n        print('{0:.2f}'.format(file_size / 1024), 'kb')"}
{"prompt": "def init_cinder_consumer(self, mq):\n        \"\"\"\n        Init openstack cinder mq\n\n        1. Check if enable listening cinder notification\n        2. Create consumer\n\n        :param mq: class ternya.mq.MQ\n        \"\"\"", "output": "\n        topic = CONF.cinder.topic\n        topic = topic.replace('topic', 'consumer')\n        topic = topic.replace('topic', 'consumer1')\n        topic = topic.replace('topic', 'consumer2')\n        topic = topic.replace('topic', 'consumer3')\n        topic = topic.replace('topic', 'consumer4')\n        topic = topic.replace('topic', 'consumer5')\n        topic = topic.replace('topic', 'consumer6')\n        topic = topic.replace('topic', 'consumer7')\n        topic = topic.replace('topic', 'consumer8')\n        topic = topic.replace('topic', 'consumer9')\n        topic", "solution": "\n        if not self.enable_component_notification(Openstack.Cinder):\n            log.debug(\"disable listening cinder notification\")\n            return\n\n        for i in range(self.config.cinder_mq_consumer_count):\n            mq.create_consumer(self.config.cinder_mq_exchange,\n                               self.config.cinder_mq_queue,\n                               ProcessFactory.process(Openstack.Cinder))\n\n        log.debug(\"enable listening openstack cinder notification.\")"}
{"prompt": "def max_global_iteration(self):\n        \"\"\"Return global iterator with last iteration number\"\"\"", "output": "\n        return self.global_iter\n\n    def global_iteration(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_done(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_done(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_not_done(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_not_done(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_not_done(self):\n        \"\"\"Return global iterator number\"\"\"\n        return self.global_iter\n\n    def global_iterations_not_", "solution": "\n        return self.indices_to_global_iterator({\n            symbol_pos_int(var_name): end-1 for var_name, start, end, incr in self._loop_stack\n        })"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        parser = argparse.ArgumentParser(\n            prog='python -m pysnptools.snpreader '\n                    '--snpreader-type {} '\n                    '--snpreader-file {} '\n                    '--snpreader-snip {} '\n                    '--snpreader-snp-min {} '\n                    '--snpreader-snp-max {} '\n                    '--snpreader-snp-step {} '\n                    '--snpreader-snp-min-count {} '\n                    '--snpreader-snp-max-count {} '\n                    '--snpreader-snp-min-coverage {} '\n                    '--snpreader-snp-max-coverage {} '\n", "solution": "\n        ApiCli.get_arguments(self)\n\n        # Get the host group name\n        if self.args.host_group_name is not None:\n            self.host_group_name = self.args.host_group_name\n\n        # Get the list of sources separated by commas\n        if self.args.sources is not None:\n            self.sources = self.args.sources\n\n        payload = {}\n        if self.host_group_name is not None:\n            payload['name'] = self.host_group_name\n\n        if self.sources is not None:\n            source_list = str.split(self.sources, ',')\n            if 'hostnames' not in payload:\n                payload['hostnames'] = []\n\n            for s in source_list:\n                payload['hostnames'].append(s)\n        self.data = json.dumps(payload, sort_keys=True)\n        self.headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}"}
{"prompt": "def next_retry_datetime(self):\n        \"\"\"\n        Get datetime of the next retry if the task instance fails. For exponential\n        backoff, retry_delay is used as base and will be converted to seconds.\n        \"\"\"", "output": "\n        retry_delay = self.retry_delay\n        retry_delay = retry_delay.total_seconds()\n        retry_delay = retry_delay.total_seconds() if retry_delay > 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay < 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay > 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay < 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay > 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay < 0 else 0\n        retry_delay = retry_delay.total_seconds() if retry_delay > ", "solution": "\n        delay = self.task.retry_delay\n        if self.task.retry_exponential_backoff:\n            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))\n            # deterministic per task instance\n            hash = int(hashlib.sha1(\"{}#{}#{}#{}\".format(self.dag_id,\n                                                         self.task_id,\n                                                         self.execution_date,\n                                                         self.try_number)\n                                    .encode('utf-8')).hexdigest(), 16)\n            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)\n            modded_hash = min_backoff + hash % min_backoff\n            # timedelta has a maximum representable value. The exponentiation\n            # here means this value can be exceeded after a certain number\n            # of tries (around 50 if the initial delay is 1s, even fewer if\n            # the delay is larger). Cap the value here before creating a\n            # timedelta object so the operation doesn't fail.\n            delay_backoff_in_seconds = min(\n                modded_hash,\n                timedelta.max.total_seconds() - 1\n            )\n            delay = timedelta(seconds=delay_backoff_in_seconds)\n            if self.task.max_retry_delay:\n                delay = min(self.task.max_retry_delay, delay)\n        return self.end_date + delay"}
{"prompt": "def showCode(self, width=80):\n        \"\"\"Show all words of the code in a nice format.\n        \"\"\"", "output": "\n        words = []\n        for line in self.lines:\n            words.extend(line.split())\n        words.sort()\n        words = [word for word in words if word]\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']\n        words = [word for word in words if word[0]!= '#']", "solution": "\n        #make table of all symbols with binary strings\n        symbolStrings = [\n            (self.bitPattern(s.index), self.mnemonic(s.index))\n            for s in self\n            ]\n        #determine column widths the way Lisp programmers do it\n        leftColWidth, rightColWidth = map(max, map(\n            map,\n            repeat(len),\n            zip(*symbolStrings)\n            ))\n        colwidth = leftColWidth+rightColWidth\n        columns = 81//(colwidth+2)\n        rows = -(-len(symbolStrings)//columns)\n        def justify(bs):\n            b,s = bs\n            return b.rjust(leftColWidth)+':'+s.ljust(rightColWidth)\n        for i in range(rows):\n            print(' '.join(map(justify, symbolStrings[i::rows])).rstrip())"}
{"prompt": "def normalize(ast: Node) -> Node:\n    \"\"\"\n    Normalize an AST nodes.\n\n    all builtins containers are replace by referencable subclasses\n    \"\"\"", "output": "\n    normalized = ast.clone()\n    normalized.visit(ast.builtins)\n    normalized.visit(ast.containers)\n    normalized.visit(ast.subscripts)\n    normalized.visit(ast.attributes)\n    normalized.visit(ast.functions)\n    normalized.visit(ast.classes)\n    normalized.visit(ast.functiondefs)\n    normalized.visit(ast.classes)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    normalized.visit(ast.expr)\n    ", "solution": "\n    res = ast\n    typemap = {DictNode, ListNode, TupleNode}\n    if type(ast) is dict:\n        res = DictNode(ast)\n    elif type(ast) is list:\n        res = ListNode(ast)\n    elif type(ast) is tuple:\n        res = TupleNode(ast)\n    # in-depth change\n    if hasattr(res, 'items'):\n        for k, v in res.items():\n            res[k] = normalize(v)\n    elif hasattr(res, '__getitem__'):\n        for idx, v in zip(range(len(res)), res):\n            res[idx] = normalize(v)\n    if type(res) not in typemap and hasattr(res, '__dict__'):\n        subattr = vars(res)\n        for k, v in subattr.items():\n            setattr(res, k, normalize(v))\n    return res"}
{"prompt": "def get_memory_maps(self):\n        \"\"\"Return process's mapped memory regions as a list of nameduples.\n        Fields are explained in 'man proc'; here is an updated (Apr 2012)\n        version: http://goo.gl/fmebo\n        \"\"\"", "output": "\n        regions = []\n        for pid in pids():\n            try:\n                proc = Process(pid)\n            except NoSuchProcess:\n                continue\n            regions.append((proc.memory_map(), proc.pid))\n        return regions\n\n    def get_threads(self):\n        \"\"\"Return process's threads as a list of namedtuples.\"\"\"\n        threads = []\n        for pid in pids():\n            try:\n                proc = Process(pid)\n            except NoSuchProcess:\n                continue\n            thread = proc.threads()\n            threads.append((thread, proc.pid))\n        return threads\n\n    def get_thread_ids(self):\n        \"\"\"Return process's thread ids as a list of ints.\"\"\"\n        thread_ids = []\n        for pid in pid", "solution": "\n        f = None\n        try:\n            f = open(\"/proc/%s/smaps\" % self.pid)\n            first_line = f.readline()\n            current_block = [first_line]\n\n            def get_blocks():\n                data = {}\n                for line in f:\n                    fields = line.split(None, 5)\n                    if len(fields) >= 5:\n                        yield (current_block.pop(), data)\n                        current_block.append(line)\n                    else:\n                        data[fields[0]] = int(fields[1]) * 1024\n                yield (current_block.pop(), data)\n\n            if first_line:  # smaps file can be empty\n                for header, data in get_blocks():\n                    hfields = header.split(None, 5)\n                    try:\n                        addr, perms, offset, dev, inode, path = hfields\n                    except ValueError:\n                        addr, perms, offset, dev, inode, path = hfields + ['']\n                    if not path:\n                        path = '[anon]'\n                    else:\n                        path = path.strip()\n                    yield (addr, perms, path,\n                           data['Rss:'],\n                           data['Size:'],\n                           data.get('Pss:', 0),\n                           data['Shared_Clean:'], data['Shared_Clean:'],\n                           data['Private_Clean:'], data['Private_Dirty:'],\n                           data['Referenced:'],\n                           data['Anonymous:'],\n                           data['Swap:'])\n            f.close()\n        except EnvironmentError:\n            # XXX - Can't use wrap_exceptions decorator as we're\n            # returning a generator;  this probably needs some\n            # refactoring in order to avoid this code duplication.\n            if f is not None:\n                f.close()\n            err = sys.exc_info()[1]\n            if err.errno in (errno.ENOENT, errno.ESRCH):\n                raise NoSuchProcess(self.pid, self._process_name)\n            if err.errno in (errno.EPERM, errno.EACCES):\n                raise AccessDenied(self.pid, self._process_name)\n            raise\n        except:\n            if f is not None:\n                f.close()\n            raise"}
{"prompt": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"", "output": "\n    if config is None:\n        config = _default_config\n    if config.path_style is None:\n        # path_style is None means \"don't touch sys.path\"\n        return\n    old_path = sys.path[:]\n    try:\n        sys.path.insert(0, path)\n    finally:\n        sys.path = old_path\n\n\ndef _remove_duplicates(items):\n    \"\"\"Remove duplicates from an iterable of items.\"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in items if x not in seen and not seen_add(x)]\n\n\ndef _is_in_path(path):\n    \"\"\"Is the given path currently in sys.path?\"\"\"\n    return path in sys.path\n\n\ndef _set_syspath(path):\n    \"\"\"Set the system path to include path.", "solution": "\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added"}
{"prompt": "def _kl_pareto_pareto(a, b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(a || b) with a and b Pareto.\n\n  Args:\n    a: instance of a Pareto distribution object.\n    b: instance of a Pareto distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_pareto_pareto\".\n\n  Returns:\n    Batchwise KL(a || b)\n  \"\"\"", "output": "\n  with ops.name_scope(name, \"kl_pareto_pareto\", [a, b]) as name:\n    a_dist = a.probs\n    b_dist = b.probs\n    a_dist = a_dist.log_prob(b_dist)\n    a_dist = a_dist.probs\n    a_dist = a_dist.log_prob(b_dist)\n    a_dist = a_dist.probs\n    a_dist = a_dist.log_prob(b_dist)\n    a_dist = a_dist.probs\n    a_dist = a_dist.log_prob(b_dist)\n    a_dist = a_dist.probs\n    a_dist = a_dist.log_prob(b_dist)\n    a_dist = a_dist.probs\n    a_dist = a_", "solution": "\n  with tf.name_scope(name or \"kl_pareto_pareto\"):\n    # Consistent with\n    # http://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf, page 55\n    # Terminology is different from source to source for Pareto distributions.\n    # The 'concentration' parameter corresponds to 'a' in that source, and the\n    # 'scale' parameter corresponds to 'm'.\n    final_batch_shape = distribution_util.get_broadcast_shape(\n        a.concentration, b.concentration, a.scale, b.scale)\n    common_type = dtype_util.common_dtype(\n        [a.concentration, b.concentration, a.scale, b.scale], tf.float32)\n    return tf.where(\n        a.scale >= b.scale,\n        b.concentration * (tf.math.log(a.scale) - tf.math.log(b.scale)) +\n        tf.math.log(a.concentration) - tf.math.log(b.concentration) +\n        b.concentration / a.concentration - 1.0,\n        tf.broadcast_to(tf.cast(np.inf, common_type), final_batch_shape))"}
{"prompt": "def flush(self):\n        \"\"\"Flush incomming socket messages.\"\"\"", "output": "\n        self._sock_impl.flush()\n\n    def fileno(self):\n        \"\"\"Return the file descriptor for this socket.\"\"\"\n        return self._sock_impl.fileno()\n\n    def accept(self):\n        \"\"\"Wait for an incoming connection.\"\"\"\n        return self._sock_impl.accept()\n\n    def makefile(self, mode='r', bufsize=-1):\n        \"\"\"A factory method to create a file object from raw bytes.\"\"\"\n        return io.BytesIO(self._sock_impl.makefile(mode, bufsize))\n\n    def makefile_stderr(self, mode='w', bufsize=-1):\n        \"\"\"A factory method to create a file object from raw bytes.\"\"\"\n        return io.BytesIO(self._sock_impl.makefile_stderr(mode, bufsize))\n\n    def makefile_stdin(self, mode='r', bufsize=-1):\n", "solution": "\n        debug('flushing incomming socket messages')\n        try:\n            while True:\n                msg = self.socket.recv(self.buffer_size)\n                debug(b'< ' + msg)\n        except socket.error:\n            pass"}
{"prompt": "def vcf_records(self, format_tags=None, qualified=False):\n        \"\"\"Generates parsed VcfRecord objects.\n\n        Typically called in a for loop to process each vcf record in a\n        VcfReader. VcfReader must be opened in advanced and closed when\n        complete. Skips all headers.\n\n        Args:\n            qualified: When True, sample names are prefixed with file name\n\n        Returns:\n            Parsed VcfRecord\n\n        Raises:\n            StopIteration: when reader is exhausted.\n            TypeError: if reader is closed.\n        \"\"\"", "output": "\n        vcf_record = VcfRecord()\n        vcf_record.header = self.header\n        vcf_record.samples = []\n        vcf_record.sample_names = []\n        vcf_record.sample_ids = []\n        vcf_record.sample_types = []\n        vcf_record.sample_descriptions = []\n        vcf_record.sample_ids_qual = []\n        vcf_record.sample_types_qual = []\n        vcf_record.sample_descriptions_qual = []\n        vcf_record.sample_ids_qual.extend(\n            self.sample_ids_qual\n        )\n        vcf_record.sample_types_qual.extend(\n            self.sample_types_qual\n        )\n        vc", "solution": "\n        if qualified:\n            sample_names = self.qualified_sample_names\n        else:\n            sample_names = self.sample_names\n\n        for line in self._file_reader.read_lines():\n            if line.startswith(\"#\"):\n                continue\n            vcf_record = vcf.VcfRecord.parse_record(line, sample_names)\n            if format_tags:\n                vcf_record = self.modify_format_tag(vcf_record, format_tags)\n            yield vcf_record"}
{"prompt": "def main(args,parser,subparser):\n    \"\"\"the list command corresponds with listing images for an external\n       resource. This is different from listing images that are local to the\n       database, which should be done with \"images\"\n    \"\"\"", "output": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "solution": "\n    from sregistry.main import get_client\n    cli = get_client(quiet=args.quiet)\n    \n    for query in args.query:\n        if query in ['','*']:\n            query = None\n\n        cli.ls(query=query)"}
{"prompt": "def parse_hpo_genes(hpo_lines):\n    \"\"\"Parse HPO gene information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with hgnc symbols as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        line = line.strip()\n        if line == '':\n            continue\n        \n        gene, gene_id, gene_name, gene_type = line.split('\\t')\n        gene = gene.strip()\n        gene_id = gene_id.strip()\n        gene_name = gene_name.strip()\n        gene_type = gene_type.strip()\n        \n        diseases[gene] = gene_id\n        \n    return diseases\n\ndef parse_hpo_gene_ids(hpo_lines):\n    \"\"\"Parse HPO gene information\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with ", "solution": "\n    LOG.info(\"Parsing HPO genes ...\")\n    genes = {}\n    for index, line in enumerate(hpo_lines):\n        # First line is header\n        if index == 0:\n            continue\n        if len(line) < 5:\n            continue\n        gene_info = parse_hpo_gene(line)\n        hgnc_symbol = gene_info['hgnc_symbol']\n        description = gene_info['description']\n        \n        if hgnc_symbol not in genes:\n            genes[hgnc_symbol] = {\n                'hgnc_symbol': hgnc_symbol\n            }\n        \n        gene = genes[hgnc_symbol]\n        if description == 'Incomplete penetrance':\n            gene['incomplete_penetrance'] = True\n        if description == 'Autosomal dominant inheritance':\n            gene['ad'] = True\n        if description == 'Autosomal recessive inheritance':\n            gene['ar'] = True\n        if description == 'Mithochondrial inheritance':\n            gene['mt'] = True\n        if description == 'X-linked dominant inheritance':\n            gene['xd'] = True\n        if description == 'X-linked recessive inheritance':\n            gene['xr'] = True\n        if description == 'Y-linked inheritance':\n            gene['x'] = True\n        if description == 'X-linked inheritance':\n            gene['y'] = True\n    LOG.info(\"Parsing done.\")\n    return genes"}
{"prompt": "def start_tag(el):\n    \"\"\"\n    The text representation of the start tag for a tag.\n    \"\"\"", "output": "\n    if el is None:\n        return '<' + el.tag\n    else:\n        return '<' + el.tag + '>'\n\n\ndef end_tag(el):\n    \"\"\"\n    The text representation of the end tag for a tag.\n    \"\"\"\n    if el is None:\n        return '</' + el.tag + '>'\n    else:\n        return '</' + el.tag + '>'\n\n\ndef start_tag_handle(el):\n    \"\"\"\n    The text representation of the start tag for a tag.\n    \"\"\"\n    if el is None:\n        return '<' + el.tag\n    else:\n        return '<' + el.tag + '>'\n\n\ndef end_tag_handle(el):\n    \"\"\"\n    The text representation of the end tag for a tag.\n    \"\"\"\n    if el is None:\n        return '", "solution": "\n    return '<%s%s>' % (\n        el.tag, ''.join([' %s=\"%s\"' % (name, html_escape(value, True))\n                         for name, value in el.attrib.items()]))"}
{"prompt": "def get_file_content(url, comes_from=None, session=None):\n    \"\"\"Gets the content of a file; it may be a filename, file: URL, or\n    http: URL.  Returns (location, content).  Content is unicode.\"\"\"", "output": "\n    if session is None:\n        session = requests.Session()\n    response = session.get(url, stream=True, verify=False)\n    response.raise_for_status()\n    content_type = response.headers.get('content-type')\n    if content_type.startswith('application/octet-stream'):\n        return response.url, response.content\n    if content_type.startswith('text/'):\n        return response.url, response.text\n    if url.startswith('http'):\n        return url, response.content\n    raise ValueError(\"Unsupported URL type: %s\" % url)\n\n\ndef url_to_path(url):\n    \"\"\"Converts a relative URL into an absolute path (which may be a\n    directory).\n    \"\"\"\n    base, path = posixpath.split(url)\n    if not path:\n        path = '/'\n    return posixpath.normpath(os.path.abspath", "solution": "\n    if session is None:\n        raise TypeError(\n            \"get_file_content() missing 1 required keyword argument: 'session'\"\n        )\n\n    match = _scheme_re.search(url)\n    if match:\n        scheme = match.group(1).lower()\n        if (scheme == 'file' and comes_from and\n                comes_from.startswith('http')):\n            raise InstallationError(\n                'Requirements file %s references URL %s, which is local'\n                % (comes_from, url))\n        if scheme == 'file':\n            path = url.split(':', 1)[1]\n            path = path.replace('\\\\', '/')\n            match = _url_slash_drive_re.match(path)\n            if match:\n                path = match.group(1) + ':' + path.split('|', 1)[1]\n            path = urllib_parse.unquote(path)\n            if path.startswith('/'):\n                path = '/' + path.lstrip('/')\n            url = path\n        else:\n            # FIXME: catch some errors\n            resp = session.get(url)\n            resp.raise_for_status()\n\n            if six.PY3:\n                return resp.url, resp.text\n            else:\n                return resp.url, resp.content\n    try:\n        with open(url) as f:\n            content = f.read()\n    except IOError as exc:\n        raise InstallationError(\n            'Could not open requirements file: %s' % str(exc)\n        )\n    return url, content"}
{"prompt": "def _get_repetition(extractor, text, *, bounds=(0, None), ignore_whitespace=False):\n  \"\"\"Tries to pull text with extractor repeatedly.\n\n  Bounds is a 2-tuple of (lbound, ubound) where lbound is a number and ubound is a number or None.\n  If the ubound is None, this method will execute extractor on text until extrator raises DeadEnd.\n  Otherwise, extractor will be called until it raises DeadEnd, or it has extracted ubound times.\n\n  If the number of children extracted is >= lbound, then a ParseNode with type repetition is\n  returned. Otherwise, DeadEnd is raised.\n\n  Bounds are interpreted as (lbound, ubound]\n\n  This method is used to implement:\n  - option (0, 1)\n  - zero_or_more (0, None)\n  - one_or_more (1, None)\n  - exact_repeat (n, n)\n  \"\"\"", "output": "bound, ubound) where lbound is a number and ubound is a number or\n  None. Bounds are inclusive.\n\n  Args:\n    extractor: A function that extracts text from a ParseNode.\n    text: A string to be extracted.\n    bounds: An optional tuple of (start, end) where start and end are numbers.\n    ignore_whitespace: Whether to ignore whitespace in text before extraction begins.\n\n  Returns:\n    A ParseNode with type repetition if extraction succeeds, DeadEnd if extraction fails.\n  \"\"\"\n  try:\n    node = extractor(text)\n  except DeadEnd:\n    return node\n  while node.type!= repetition:\n    node = extractor", "solution": "\n  minr, maxr = bounds\n  children = []\n\n  while maxr is None or len(children) <= maxr:\n    ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n    try:\n      child = _call_extractor(extractor, use_text)\n      child.add_ignored(ignored_ws)\n    except DeadEnd:\n      break\n\n    if child.is_empty:\n      break\n\n    children.append(child)\n    text = text[child.consumed:]\n\n\n  if len(children) >= minr:\n    return ParseNode(ParseNodeType.repetition,\n                      children=children)\n  else:\n    raise DeadEnd()"}
{"prompt": "def get_query(self, query):\n        \"\"\" Grab all issues matching a github query \"\"\"", "output": "\n        query = query.lower()\n        issues = []\n        for issue in self.issues:\n            if issue.title.lower().startswith(query):\n                issues.append(issue)\n        return issues\n\n    def get_issue(self, issue_id):\n        \"\"\" Grab a single issue \"\"\"\n        for issue in self.issues:\n            if issue.id == issue_id:\n                return issue\n        return None\n\n    def get_issues(self, query):\n        \"\"\" Grab all issues matching a github query \"\"\"\n        query = query.lower()\n        issues = []\n        for issue in self.issues:\n            if issue.title.lower().startswith(query):\n                issues.append(issue)\n        return issues\n\n    def get_issue_comments(self, issue_id):\n        \"\"\" Grab all ", "solution": "\n        issues = {}\n        for issue in self.client.get_query(query):\n            url = issue['html_url']\n            try:\n                repo = self.get_repository_from_issue(issue)\n            except ValueError as e:\n                log.critical(e)\n            else:\n                issues[url] = (repo, issue)\n        return issues"}
{"prompt": "def build(self, secret_key):\n        \"\"\"Builds a final copy of the token using the given secret key.\n\n        :param secret_key(string): The secret key that corresponds to this builder's access key.\n        \"\"\"", "output": "\n        builder = copy.deepcopy(self)\n        builder.secret_key = secret_key\n        builder.access_key = secret_key\n        builder.access_secret = secret_key\n        return builder\n\n    def build(self, access_key, secret_key):\n        \"\"\"Builds a final copy of the token using the given access key and secret key.\n\n        :param access_key(string): The access key that corresponds to this builder's secret key.\n        :param secret_key(string): The secret key that corresponds to this builder's secret key.\n        \"\"\"\n        builder = copy.deepcopy(self)\n        builder.access_key = access_key\n        builder.secret_key = secret_key\n        builder.access_secret = secret_key\n        return builder\n\n    def build(self, access_key, secret_key, scope):\n", "solution": "\n        key = jwk.JWK(\n            kty='oct',\n            k=base64url_encode(uuid.UUID(secret_key).bytes),\n        )\n\n        header = {\n            'alg': 'dir',\n            'enc': 'A128GCM',\n            'zip': 'DEF',\n            'cty': 'JWT',\n            'kid': self._access_key,\n        }\n\n        now = int(time.time())\n\n        payload = {\n            'iat': now,\n            'nbf': now,\n        }\n\n        if self._expiration is not None:\n            payload['exp'] = int(calendar.timegm(self._expiration.utctimetuple()))\n\n        if len(self._view_identifiers) > 0:\n            payload[VIEW_IDENTIFIERS_CLAIM_NAME] = self._view_identifiers\n\n        if len(self._parameters) > 0:\n            parameters = []\n            for parameter in self._parameters:\n                serialized = {\n                    'field': parameter.field,\n                    'op': parameter.op,\n                }\n\n                if hasattr(parameter, '__iter__'):\n                    serialized['any'] = list(parameter.value)\n                else:\n                    serialized['value'] = parameter.value\n\n                parameters.append(serialized)\n\n            payload[PARAMETERS_CLAIM_NAME] = parameters\n\n        if len(self._attributes) > 0:\n            payload[ATTRIBUTES_CLAIM_NAME] = self._attributes\n\n        tok = jwe.JWE(json_encode(payload), protected=header)\n        tok.add_recipient(key)\n\n        return tok.serialize(compact=True)"}
{"prompt": "def cpu_percent(interval=0.1, percpu=False):\n    \"\"\"Return a float representing the current system-wide CPU\n    utilization as a percentage.\n\n    When interval is > 0.0 compares system CPU times elapsed before\n    and after the interval (blocking).\n\n    When interval is 0.0 or None compares system CPU times elapsed\n    since last call or module import, returning immediately.\n    In this case is recommended for accuracy that this function be\n    called with at least 0.1 seconds between calls.\n\n    When percpu is True returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n    \"\"\"", "output": "s to system CPU time elapsed since\n    module import, second element refers to system CPU time elapsed\n    before module import, third element refers to system CPU time\n    elapsed after module import, fourth element refers to system\n    CPU time elapsed since last call or module import.\n\n    When percpu is False returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to system CPU time elapsed since\n    module import, second element refers to system CPU time elapsed\n    before module import, third element refers to system CPU time\n    elapsed after module import, fourth element refers to system\n    ", "solution": "\n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n\n    def calculate(t1, t2):\n        t1_all = sum(t1)\n        t1_busy = t1_all - t1.idle\n\n        t2_all = sum(t2)\n        t2_busy = t2_all - t2.idle\n\n        # this usually indicates a float precision issue\n        if t2_busy <= t1_busy:\n            return 0.0\n\n        busy_delta = t2_busy - t1_busy\n        all_delta = t2_all - t1_all\n        busy_perc = (busy_delta / all_delta) * 100\n        return round(busy_perc, 1)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times\n        _last_cpu_times = cpu_times()\n        return calculate(t1, _last_cpu_times)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times\n        _last_per_cpu_times = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times):\n            ret.append(calculate(t1, t2))\n        return ret"}
{"prompt": "def load_filters():\n    \"\"\"\n    Loads and returns all filters.\n    \"\"\"", "output": "\n    filters = []\n    for f in glob.glob(FILTERS_FILE):\n        name, ext = os.path.splitext(f)\n        if ext == '.xml':\n            filters.append(Filter(f))\n    return filters\n\n\ndef load_filters_xml():\n    \"\"\"\n    Loads and returns all filters as XML files.\n    \"\"\"\n    filters = []\n    for f in glob.glob(FILTERS_FILE):\n        name, ext = os.path.splitext(f)\n        if ext == '.xml':\n            filters.append(FilterXML(f))\n    return filters\n\n\ndef load_filters_yaml():\n    \"\"\"\n    Loads and returns all filters as YAML files.\n    \"\"\"\n    filters = []\n    for f in glob.glob(FILTERS_FILE):", "solution": "\n    all_filters = {}\n    for m in JINJA_FILTERS:\n        if hasattr(m, \"filters\"):\n            all_filters.update(m.filters())\n    return all_filters"}
{"prompt": "def weighted(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"", "output": "\n    load1, load2 = loads\n    weight1 = load1 / load2\n    weight2 = 1 - weight1\n    return weight1, weight2\n\n\ndef weighted_choice(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"\n    load1, load2 = loads\n    weight1 = load1 / load2\n    weight2 = 1 - weight1\n    return weight1 * random.random() + weight2\n\n\ndef weighted_choice(loads):\n    \"\"\"Pick two at random using inverse load as weight.\n\n    Return the less loaded of the two.\n    \"\"\"\n    load1, load2 = loads\n    weight1 = ", "solution": "\n    # weight 0 a million times more than 1:\n    weights = 1./(1e-6+numpy.array(loads))\n    sums = weights.cumsum()\n    t = sums[-1]\n    x = random()*t\n    y = random()*t\n    idx = 0\n    idy = 0\n    while sums[idx] < x:\n        idx += 1\n    while sums[idy] < y:\n        idy += 1\n    if weights[idy] > weights[idx]:\n        return idy\n    else:\n        return idx"}
{"prompt": "def print_figure(fig, fmt='png'):\n    \"\"\"Convert a figure to svg or png for inline display.\"\"\"", "output": "\n    if fmt =='svg':\n        fig.savefig(SVG_OUTPUT, format='svg')\n    elif fmt == 'png':\n        fig.savefig(PNG_OUTPUT, format='png')\n    else:\n        raise ValueError(f\"Invalid figure format: {fmt}\")\n\n\ndef show_fig(fig, fmt='png'):\n    \"\"\"Display a figure in Jupyter notebook.\"\"\"\n    if fmt =='svg':\n        fig.show()\n    elif fmt == 'png':\n        fig.savefig(PNG_OUTPUT)\n    else:\n        raise ValueError(f\"Invalid figure format: {fmt}\")\n\n\ndef show_figs(figs, fmt='png'):\n    \"\"\"Display a figure in Jupyter notebook.\"\"\"\n    if fmt =='svg':\n        for fig in ", "solution": "\n    # When there's an empty figure, we shouldn't return anything, otherwise we\n    # get big blank areas in the qt console.\n    if not fig.axes and not fig.lines:\n        return\n\n    fc = fig.get_facecolor()\n    ec = fig.get_edgecolor()\n    fig.set_facecolor('white')\n    fig.set_edgecolor('white')\n    try:\n        bytes_io = BytesIO()\n        fig.canvas.print_figure(bytes_io, format=fmt, bbox_inches='tight')\n        data = bytes_io.getvalue()\n    finally:\n        fig.set_facecolor(fc)\n        fig.set_edgecolor(ec)\n    return data"}
{"prompt": "def json_to_bounds(segments_json):\n    \"\"\"Extracts the boundaries from a json file and puts them into\n        an np array.\"\"\"", "output": "\n    segments_bounds = []\n    for segment in segments_json:\n        segments_bounds.append(segment['bounds'])\n    return np.array(segments_bounds)\n\n\ndef json_to_segments(segments_json):\n    \"\"\"Extracts the segments from a json file and puts them into\n        an np array.\"\"\"\n    segments_segments = []\n    for segment in segments_json:\n        segments_segments.append(segment['segments'])\n    return np.array(segments_segments)\n\n\ndef json_to_segments_bounds(segments_json):\n    \"\"\"Extracts the boundaries from a json file and puts them into\n        an np array.\"\"\"\n    segments_bounds = []", "solution": "\n    f = open(segments_json)\n    segments = json.load(f)[\"segments\"]\n    bounds = []\n    for segment in segments:\n        bounds.append(segment[\"start\"])\n    bounds.append(bounds[-1] + segments[-1][\"duration\"]) # Add last boundary\n    f.close()\n    return np.asarray(bounds)"}
{"prompt": "def _bio_to_string(bio):\n    \"\"\"\n    Copy the contents of an OpenSSL BIO object into a Python byte string.\n    \"\"\"", "output": "\n    return bio.read()\n\n\ndef _string_to_bio(s):\n    \"\"\"\n    Create an OpenSSL BIO object from a Python byte string.\n    \"\"\"\n    return BIO.MemoryBuffer(s)\n\n\ndef _asn1_int_to_int(asn1_int):\n    \"\"\"\n    Convert an ASN.1 integer to a Python integer.\n    \"\"\"\n    return asn1_int.to_bytes(1, byteorder='big', signed=False)[0]\n\n\ndef _asn1_string_to_int(asn1_string):\n    \"\"\"\n    Convert an ASN.1 string to a Python integer.\n    \"\"\"\n    return int.from_bytes(asn1_string.to_bytes(1, byteorder='big', signed=False), byteorder='big')\n\n\n", "solution": "\n    result_buffer = _ffi.new('char**')\n    buffer_length = _lib.BIO_get_mem_data(bio, result_buffer)\n    return _ffi.buffer(result_buffer[0], buffer_length)[:]"}
{"prompt": "def create_from(cls, backend):\n        \"\"\"\n        Create device specification with values in backend configuration.\n        Args:\n            backend(Backend): backend configuration\n        Returns:\n            DeviceSpecification: created device specification\n        Raises:\n            PulseError: when an invalid backend is specified\n        \"\"\"", "output": "\n        device_spec = cls(backend)\n        device_spec._backend = backend\n        return device_spec\n\n    @property\n    def backend(self):\n        \"\"\"\n        Backend configuration\n        \"\"\"\n        return self._backend\n\n    @property\n    def name(self):\n        \"\"\"\n        Device name\n        \"\"\"\n        return self._name\n\n    @property\n    def manufacturer(self):\n        \"\"\"\n        Manufacturer name\n        \"\"\"\n        return self._manufacturer\n\n    @property\n    def model(self):\n        \"\"\"\n        Model name\n        \"\"\"\n        return self._model\n\n    @property\n    def serial(self):\n        \"\"\"\n        Serial number\n        \"\"\"\n        return self._serial\n\n    @property\n    def firmware_version(self):\n        \"\"\"\n        Fir", "solution": "\n        backend_config = backend.configuration()\n\n        # TODO : Remove usage of config.defaults when backend.defaults() is updated.\n        try:\n            backend_default = backend.defaults()\n        except ModelValidationError:\n            from collections import namedtuple\n            BackendDefault = namedtuple('BackendDefault', ('qubit_freq_est', 'meas_freq_est'))\n\n            backend_default = BackendDefault(\n                qubit_freq_est=backend_config.defaults['qubit_freq_est'],\n                meas_freq_est=backend_config.defaults['meas_freq_est']\n            )\n\n        # system size\n        n_qubits = backend_config.n_qubits\n        n_registers = backend_config.n_registers\n        n_uchannels = backend_config.n_uchannels\n\n        if n_uchannels > 0 and n_uchannels != n_qubits:\n            raise PulseError(\"This version assumes no U-channels or #U-cannels==#qubits.\")\n\n        # frequency information\n        qubit_lo_freqs = backend_default.qubit_freq_est\n        qubit_lo_ranges = backend_config.qubit_lo_range\n        meas_lo_freqs = backend_default.meas_freq_est\n        meas_lo_ranges = backend_config.meas_lo_range\n\n        # generate channels with assuming their numberings are aligned with qubits\n        drives = [\n            DriveChannel(i, qubit_lo_freqs[i], tuple(qubit_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        measures = [\n            MeasureChannel(i, meas_lo_freqs[i], tuple(meas_lo_ranges[i]))\n            for i in range(n_qubits)\n        ]\n        acquires = [AcquireChannel(i) for i in range(n_qubits)]\n        controls = [ControlChannel(i) for i in range(n_uchannels)]\n\n        qubits = []\n        for i in range(n_qubits):\n            # TODO: get qubits <-> channels relationship from backend\n            qubit = Qubit(i,\n                          drive_channels=[drives[i]],\n                          control_channels=None if n_uchannels == 0 else controls[i],\n                          measure_channels=[measures[i]],\n                          acquire_channels=[acquires[i]])\n            qubits.append(qubit)\n\n        registers = [RegisterSlot(i) for i in range(n_registers)]\n        # TODO: get #mem_slots from backend\n        mem_slots = [MemorySlot(i) for i in range(len(qubits))]\n\n        return DeviceSpecification(qubits, registers, mem_slots)"}
{"prompt": "def remove_memberships(self, team, users):\n        \"\"\"\n        **Description**\n            Remove user memberships from specified team.\n\n        **Arguments**\n            - **team**: the name of the team from which user memberships are removed\n            - **users**: list of usernames which should be removed from team\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt_extended.py>`_\n        \"\"\"", "output": "\n        team = self.team(team)\n        memberships = self.memberships(team)\n        memberships = [m for m in memberships if m.username in users]\n        for m in memberships:\n            self.remove_membership(team, m)\n\n    def remove_membership(self, team, membership):\n        \"\"\"\n        **Description**\n            Remove user membership from specified team.\n\n        **Arguments**\n            - **team**: the name of the team from which user membership is removed\n            - **membership**: the membership to be removed\n\n        **Example**\n            `examples/user_team_mgmt_extended.py <https://github.com/draios/python-", "solution": "\n\n        res = self.list_memberships(team)\n\n        if res[0] is False:\n            return res\n\n        old_memberships = res[1]\n        new_memberships = {k: v for k, v in old_memberships.items() if k not in users}\n\n        res = self.edit_team(team, new_memberships)\n\n        if res[0] is False:\n            return res\n        else:\n            return [True, None]"}
{"prompt": "def price_converter(obj):\n    \"\"\"Ensures that string prices are converted into Price objects.\"\"\"", "output": "\n    if isinstance(obj, str):\n        return Price(obj)\n    return obj\n\n\ndef price_list_converter(obj):\n    \"\"\"Ensures that a list of strings are converted into Price objects.\"\"\"\n    if isinstance(obj, list):\n        return [price_converter(x) for x in obj]\n    return obj\n\n\ndef price_list_converter_with_default(converter, default_value):\n    \"\"\"Ensures that a list of strings are converted into Price objects.\"\"\"\n    if isinstance(obj, list):\n        return [converter(obj[0]) if obj[0] else default_value for obj in obj]\n    return obj\n\n\ndef price_list_converter_with_default_with_default(converter, default_value):\n    \"\"\"Ensures that a list of strings are converted into Price objects.\"\"\"\n    if isinstance(obj, list):\n", "solution": "\n    if isinstance(obj, str):\n        obj = PriceClass.parse(obj)\n    return obj"}
{"prompt": "def finalize(self, remove_all_handlers=True):\n        \"\"\"Finalizes the manager, closes and removes all handlers if desired.\"\"\"", "output": "\n        self.manager.finalize()\n        if remove_all_handlers:\n            self.manager.remove_all_handlers()\n\n    def shutdown(self, wait=True):\n        \"\"\"Shuts down the manager.\n\n        All handlers are removed and closed, but the manager itself is\n        still available for reuse by other parts of the framework.\n\n        :param wait: If ``True``, blocks until the manager has shutdown.\n        \"\"\"\n        self.manager.shutdown(wait)\n        self.manager.close()\n\n    def wait(self):\n        \"\"\"Waits for the manager to shutdown.\n\n        All handlers are removed and closed, but the manager itself is\n        still available for reuse by other parts of the framework.\n        \"\"\"\n        self.manager.wait()\n\n    def start_", "solution": "\n        for tool in self._tools:\n            tool.finalize()\n        self._tools = []\n        self._stdout_to_logger = None\n        for config in (self._sp_config, self._mp_config):\n            if hasattr(config, 'close'):\n                config.close()\n        self._sp_config = None\n        self._mp_config = None\n        if remove_all_handlers:\n            self.tabula_rasa()"}
{"prompt": "def _experiments_to_circuits(qobj):\n    \"\"\"Return a list of QuantumCircuit object(s) from a qobj\n\n    Args:\n        qobj (Qobj): The Qobj object to convert to QuantumCircuits\n    Returns:\n        list: A list of QuantumCircuit objects from the qobj\n\n    \"\"\"", "output": "\n    circuits = []\n    for experiment in qobj.experiments:\n        circuit = QuantumCircuit(experiment.num_qubits)\n        circuits.append(circuit)\n        for qreg in experiment.qregs:\n            circuit.h(qreg)\n        for creg in experiment.cregs:\n            circuit.cx(qreg, creg)\n    return circuits\n\n\ndef _circuit_to_qobj(circuit):\n    \"\"\"Return a Qobj object from a QuantumCircuit object\n\n    Args:\n        circuit (QuantumCircuit): The QuantumCircuit object to convert\n    Returns:\n        Qobj: The Qobj object from the QuantumCircuit object\n\n    \"\"\"\n", "solution": "\n    if qobj.experiments:\n        circuits = []\n        for x in qobj.experiments:\n            quantum_registers = [QuantumRegister(i[1], name=i[0])\n                                 for i in x.header.qreg_sizes]\n            classical_registers = [ClassicalRegister(i[1], name=i[0])\n                                   for i in x.header.creg_sizes]\n            circuit = QuantumCircuit(*quantum_registers,\n                                     *classical_registers,\n                                     name=x.header.name)\n            qreg_dict = {}\n            creg_dict = {}\n            for reg in quantum_registers:\n                qreg_dict[reg.name] = reg\n            for reg in classical_registers:\n                creg_dict[reg.name] = reg\n            for i in x.instructions:\n                instr_method = getattr(circuit, i.name)\n                qubits = []\n                try:\n                    for qubit in i.qubits:\n                        qubit_label = x.header.qubit_labels[qubit]\n                        qubits.append(\n                            qreg_dict[qubit_label[0]][qubit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                clbits = []\n                try:\n                    for clbit in i.memory:\n                        clbit_label = x.header.clbit_labels[clbit]\n                        clbits.append(\n                            creg_dict[clbit_label[0]][clbit_label[1]])\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                params = []\n                try:\n                    params = i.params\n                except Exception:  # pylint: disable=broad-except\n                    pass\n                if i.name in ['snapshot']:\n                    instr_method(\n                        i.label,\n                        snapshot_type=i.snapshot_type,\n                        qubits=qubits,\n                        params=params)\n                elif i.name == 'initialize':\n                    instr_method(params, qubits)\n                else:\n                    instr_method(*params, *qubits, *clbits)\n            circuits.append(circuit)\n        return circuits\n    return None"}
{"prompt": "def visit_assign(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"", "output": "\n        if node.targets[0].id is self.func_name:\n            node.return_type = self.return_type\n            node.is_call = True\n        else:\n            node.return_type = None\n            node.is_call = False\n\n    def visit_expr(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"\n        if node.targets[0].id is self.func_name:\n            node.return_type = self.return_type\n            node.is_call = True\n        else:\n            node.return_type = None\n            node.is_call = False\n\n    def visit_expr_list(self, node):\n        \"\"\"check that if assigning to a function call, the function is\n        possibly returning something valuable\n        \"\"\"\n        if node.targets[0].", "solution": "\n        if not isinstance(node.value, astroid.Call):\n            return\n        function_node = safe_infer(node.value.func)\n        # skip class, generator and incomplete function definition\n        funcs = (astroid.FunctionDef, astroid.UnboundMethod, astroid.BoundMethod)\n        if not (\n            isinstance(function_node, funcs)\n            and function_node.root().fully_defined()\n            and not function_node.decorators\n        ):\n            return\n        if (\n            function_node.is_generator()\n            or function_node.is_abstract(pass_is_abstract=False)\n            or isinstance(function_node, astroid.AsyncFunctionDef)\n        ):\n            return\n        returns = list(\n            function_node.nodes_of_class(astroid.Return, skip_klass=astroid.FunctionDef)\n        )\n        if not returns:\n            self.add_message(\"assignment-from-no-return\", node=node)\n        else:\n            for rnode in returns:\n                if not (\n                    isinstance(rnode.value, astroid.Const)\n                    and rnode.value.value is None\n                    or rnode.value is None\n                ):\n                    break\n            else:\n                self.add_message(\"assignment-from-none\", node=node)"}
{"prompt": "def read_array(self, start, end):\n        \"\"\"Return floats from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of floats is immediately read into memory from\n        the file, making this efficient for small sequences of floats\n        whose values are all needed immediately.\n\n        \"\"\"", "output": "\n        array = np.fromfile(self.file, dtype=np.float32)\n        array[...] = start\n        array[...] = end\n        return array\n\n    def read_string(self, start, end):\n        \"\"\"Return strings from `start` to `end` inclusive, indexed from 1.\n\n        The entire range of strings is immediately read into memory from\n        the file, making this efficient for small sequences of strings\n        whose values are all needed immediately.\n\n        \"\"\"\n        array = np.fromfile(self.file, dtype=np.string_)\n        array[...] = start\n        array[...] = end\n        return array\n\n    def read_header(self):\n        \"\"\"Read header from file.\n\n        Header consists of two sections, a header line and an optional\n", "solution": "\n        f = self.file\n        f.seek(8 * (start - 1))\n        length = 1 + end - start\n        data = f.read(8 * length)\n        return ndarray(length, self.endian + 'd', data)"}
{"prompt": "def batches(dataset):\n    \"\"\"Returns a callable that chooses sequences from netcdf data.\"\"\"", "output": "\n    \n    def batches_fn(batch):\n        \n        batch['lat'] = batch['lat'].astype(np.float32)\n        batch['lon'] = batch['lon'].astype(np.float32)\n        batch['time'] = batch['time'].astype(np.float32)\n        batch['time'] = batch['time'].astype(np.int32)\n        \n        batch['lat'] -= batch['lat'].min()\n        batch['lat'] = np.clip(batch['lat'], -batch['lat'].min(), batch['lat'].max())\n        \n        batch['lon'] -= batch['lon'].min()\n        batch['lon'] = np.clip(batch['lon'], -batch['lon'].min(), ", "solution": "\n    seq_lengths = dataset.variables['seqLengths'].data\n    seq_begins = np.concatenate(([0], np.cumsum(seq_lengths)[:-1]))\n\n    def sample():\n        chosen = np.random.choice(\n            list(range(len(seq_lengths))), BATCH_SIZE, replace=False)\n        return batch_at(dataset.variables['inputs'].data,\n                        dataset.variables['targetClasses'].data,\n                        seq_begins[chosen],\n                        seq_lengths[chosen])\n\n    return sample"}
{"prompt": "def _raise_if_wrong_file_signature(stream):\n    \"\"\" Reads the 4 first bytes of the stream to check that is LASF\"\"\"", "output": "\n    stream.seek(4)\n    signature = stream.read(4)\n    if signature!= b\"LASF\":\n        raise LASFError(\"Wrong file signature\")\n\n\ndef _read_header(stream):\n    \"\"\" Reads the header of the LASF file \"\"\"\n    header = {}\n    signature = stream.read(4)\n    if signature!= b\"LASF\":\n        raise LASFError(\"Wrong file signature\")\n\n    header.update(dict(zip([\"version\", \"size\", \"offset\", \"checksum\"],\n                             stream.read(16))))\n    header.update(dict(zip([\"magic\", \"magic\", \"magic\", \"magic\"],\n                             stream.read(16))))\n    header.update(dict(zip([\"magic", "solution": "\n    file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE))\n    if file_sig != headers.LAS_FILE_SIGNATURE:\n        raise errors.PylasError(\n            \"File Signature ({}) is not {}\".format(file_sig, headers.LAS_FILE_SIGNATURE)\n        )"}
{"prompt": "def find_data_files(self, package, src_dir):\n        \"\"\"Return filenames for package's data files in 'src_dir'\"\"\"", "output": "\n        package_dir = os.path.join(self.package_dir, package)\n        package_files = os.listdir(package_dir)\n        package_files = [f for f in package_files if f.endswith('.dat')]\n        package_files = [os.path.join(package_dir, file) for file in package_files]\n        package_files = [file for file in package_files if os.path.exists(file)]\n        return package_files\n\n    def write_data_files(self, package, src_dir):\n        \"\"\"Write package data files to'src_dir'\"\"\"\n        package_dir = os.path.join(self.package_dir, package)\n        package_files = self.find_data_files(package, src_dir)\n        for file in package_files:\n            shutil.copy(file, package_dir)\n\n    def write_package(self, package, src_dir):\n", "solution": "\n        globs = (self.package_data.get('', [])\n                 + self.package_data.get(package, []))\n        files = self.manifest_files.get(package, [])[:]\n        for pattern in globs:\n            # Each pattern has to be converted to a platform-specific path\n            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))\n        return self.exclude_data_files(package, src_dir, files)"}
{"prompt": "def to_id(string: str) -> str:\n    \"\"\"Get a spotify ID from a URI or open.spotify URL.\n\n    Paramters\n    ---------\n    string : str\n        The string to operate on.\n\n    Returns\n    -------\n    id : str\n        The Spotify ID from the string.\n    \"\"\"", "output": "\n    uri = string.split(\":\")\n    if len(uri) == 1:\n        return uri[0]\n    else:\n        return \"open:\" + uri[1]\n\n\ndef to_uri(id: str) -> str:\n    \"\"\"Convert a Spotify ID to a URI string.\n\n    Paramters\n    ---------\n    id : str\n        The Spotify ID to convert.\n\n    Returns\n    -------\n    uri : str\n        The URI of the Spotify ID.\n    \"\"\"\n    return f\"spotify:track:{id}\"\n\n\ndef to_spotify_uri(id: str) -> str:\n    \"\"\"Convert a URI to a Spotify ID.\n\n    Paramters\n    ---------\n    id : str\n        The URI of the Spotify ID.", "solution": "\n    string = string.strip()\n\n    match = _URI_RE.match(string)\n\n    if match is None:\n        match = _OPEN_RE.match(string)\n\n        if match is None:\n            return string\n        else:\n            return match.group(2)\n    else:\n        return match.group(1)"}
{"prompt": "def marquee(txt='',width=78,mark='*'):\n    \"\"\"Return the input string centered in a 'marquee'.\n\n    :Examples:\n\n        In [16]: marquee('A test',40)\n        Out[16]: '**************** A test ****************'\n\n        In [17]: marquee('A test',40,'-')\n        Out[17]: '---------------- A test ----------------'\n\n        In [18]: marquee('A test',40,' ')\n        Out[18]: '                 A test                 '\n\n    \"\"\"", "output": "\n    txt = txt.center(width)\n    txt = txt.ljust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n    txt = txt.rjust(width)\n    txt = txt.center(width)\n", "solution": "\n    if not txt:\n        return (mark*width)[:width]\n    nmark = (width-len(txt)-2)//len(mark)//2\n    if nmark < 0: nmark =0\n    marks = mark*nmark\n    return '%s %s %s' % (marks,txt,marks)"}
{"prompt": "def add(self, gate, qargs, cargs):\n        \"\"\"Add an instruction and its context (where it's attached).\"\"\"", "output": "\n        gate.qargs = qargs\n        gate.cargs = cargs\n        gate.qregs = qregs\n        gate.cregs = cregs\n        gate.qstate = qstate\n        gate.cstate = cstate\n        gate.qreg_names = qreg_names\n        gate.creg_names = creg_names\n        gate.qreg_values = qreg_values\n        gate.creg_values = creg_values\n        gate.qreg_values_names = qreg_values_names\n        gate.creg_values_names = creg_values_names\n        gate.qreg", "solution": "\n        if not isinstance(gate, Instruction):\n            raise QiskitError(\"attempt to add non-Instruction\" +\n                              \" to InstructionSet\")\n        self.instructions.append(gate)\n        self.qargs.append(qargs)\n        self.cargs.append(cargs)"}
{"prompt": "def read_char(self, c: str) -> bool:\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"", "output": "\n        ch = c.peek()\n        if ch == b'\\n':\n            self.index += 1\n            self.line = self.line[self.index:]\n            return True\n        return False\n\n    def peek_char(self) -> 'char':\n        \"\"\"\n        Peek the next c head byte, increment current index and return it.\n        \"\"\"\n        ch = c.peek()\n        if ch == b'\\n':\n            self.index += 1\n            self.line = self.line[self.index:]\n            return ch\n        return ch\n\n    def peek_line(self) -> str:\n        \"\"\"\n        Peek the next line head byte, increment current index and return it.\n        \"\"\"\n        ch = c.peek()\n        if ch == b'\\n':", "solution": "\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()"}
{"prompt": "def disassemble(msg, msg_nocr, section, co, lasti=-1, start_line=-1,\n                end_line=None, relative_pos=False, highlight='light',\n                start_offset=0, end_offset=None):\n    \"\"\"Disassemble a code object.\"\"\"", "output": "\n    co = co or _Code()\n    co.co_filename = co.co_filename.replace('\\\\', '/')\n    co.co_firstlineno = co.co_firstlineno or 0\n    co.co_lnotab = co.co_lnotab or ''\n    co.co_stacksize = co.co_stacksize or 0\n    co.co_flags = co.co_flags or 0\n    co.co_code = co.co_code or ''\n    co.co_consts = co.co_consts or ()\n    co.co_names = co.co_varnames = co.co_consts + co.co_names\n    co.co_varnames = tuple(co.co_varnames[:])\n    co.co_code = ", "solution": "\n    return disassemble_bytes(msg, msg_nocr, co.co_code, lasti, co.co_firstlineno,\n                             start_line, end_line, relative_pos,\n                        co.co_varnames, co.co_names, co.co_consts,\n                        co.co_cellvars, co.co_freevars,\n                        dict(findlinestarts(co)), highlight,\n                        start_offset=start_offset, end_offset=end_offset)"}
{"prompt": "def _unpack_storm_date(date):\n        \"\"\"\n        given a packed storm date field, unpack and return 'YYYY-MM-DD' string.\n        \"\"\"", "output": "\n        return date[:4] + '-' + date[4:6] + '-' + date[6:]\n\n    def _unpack_storm_time(time):\n        \"\"\"\n        given a packed storm time field, unpack and return 'HH:MM:SS' string.\n        \"\"\"\n        return time[:3] + '-' + time[3:5] + '-' + time[5:]\n\n    def _unpack_storm_time(time):\n        \"\"\"\n        given a packed storm time field, unpack and return 'HH:MM:SS' string.\n        \"\"\"\n        return time[:3] + '-' + time[3:5] + '-' + time[5:]\n\n    def _unpack_storm_time(time):\n        \"\"\"\n        given a packed stor", "solution": "\n        year = (date & 0x7f) + 2000  # 7 bits\n        day = (date >> 7) & 0x01f  # 5 bits\n        month = (date >> 12) & 0x0f  # 4 bits\n        return \"%s-%s-%s\" % (year, month, day)"}
{"prompt": "def poll_operation_until_done(self, name, polling_interval_in_seconds):\n        \"\"\"\n        Poll backup operation state until it's completed.\n\n        :param name: the name of the operation resource\n        :type name: str\n        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n        :type polling_interval_in_seconds: int\n        :return: a resource operation instance.\n        :rtype: dict\n        \"\"\"", "output": "\n        poller = polling_interval_operation(polling_interval_in_seconds)\n        poller.name = name\n        poller._client = self._client\n        poller._config = self._config\n        poller._serializer = self._serializer\n        poller._deserializer = self._deserializer\n        poller._api_version = self._api_version\n        poller._operation_type = 'BackupOperation'\n        poller._operation_id = str(uuid.uuid4())\n        poller._operation_metadata = dict()\n        poller._operation_metadata['polling_interval_in_seconds'] = polling_interval_in_seconds\n        return poller\n\n    def poll_operation_until_done_with_metadata(self, name, ", "solution": "\n        while True:\n            result = self.get_operation(name)\n\n            state = result['metadata']['common']['state']\n            if state == 'PROCESSING':\n                self.log.info('Operation is processing. Re-polling state in {} seconds'\n                              .format(polling_interval_in_seconds))\n                time.sleep(polling_interval_in_seconds)\n            else:\n                return result"}
{"prompt": "def cost(self, t_node, branch_length, multiplicity=2.0):\n        \"\"\"\n        returns the cost associated with a branch starting at t_node\n        t_node is time before present, the branch goes back in time\n\n        Args:\n            - t_node:           time of the node\n            - branch_length:    branch length, determines when this branch merges with sister\n            - multiplicity:     2 if merger is binary, higher if this is a polytomy\n        \"\"\"", "output": "\n        if branch_length == 0:\n            return 1.0\n        branch_length *= multiplicity\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n        branch_length += self.t_node\n", "solution": "\n        merger_time = t_node+branch_length\n        return self.integral_merger_rate(merger_time) - self.integral_merger_rate(t_node)\\\n                 - np.log(self.total_merger_rate(merger_time))*(multiplicity-1.0)/multiplicity"}
{"prompt": "def run_cell_magic(self, magic_name, line, cell):\n        \"\"\"Execute the given cell magic.\n        \n        Parameters\n        ----------\n        magic_name : str\n          Name of the desired magic function, without '%' prefix.\n\n        line : str\n          The rest of the first input line as a single string.\n\n        cell : str\n          The body of the cell as a (possibly multiline) string.\n        \"\"\"", "output": "\n        \n        # Split the line into cells\n        cells = line.strip().split(None, 1)\n        \n        # Execute the cell magic\n        exec(cells[0], self.shell.user_ns)\n        \n        # Execute the rest of the cell\n        for cell in cells[1:]:\n            exec(cell, self.shell.user_ns)\n        \n        \nclass Magics(MagicsBase):\n    \"\"\"Magics class for IPython that allows easy customization of IPython\n    user interface (magics, hotkeys, etc.).\n    \"\"\"\n    \n    \n    @line_magic\n    def autoindent(self, parameter_s='', cell=None):\n        \"\"\"Automatically indent IPython code cells.\n        \n        ", "solution": "\n        fn = self.find_cell_magic(magic_name)\n        if fn is None:\n            lm = self.find_line_magic(magic_name)\n            etpl = \"Cell magic function `%%%%%s` not found%s.\"\n            extra = '' if lm is None else (' (But line magic `%%%s` exists, '\n                                    'did you mean that instead?)' % magic_name )\n            error(etpl % (magic_name, extra))\n        else:\n            # Note: this is the distance in the stack to the user's frame.\n            # This will need to be updated if the internal calling logic gets\n            # refactored, or else we'll be expanding the wrong variables.\n            stack_depth = 2\n            magic_arg_s = self.var_expand(line, stack_depth)\n            with self.builtin_trap:\n                result = fn(line, cell)\n            return result"}
{"prompt": "def update_trace_watch(self):\n        \"\"\"Parses the nextflow trace file and retrieves the path of report JSON\n        files that have not been sent to the service yet.\n        \"\"\"", "output": "\n        trace_files = []\n        trace_file = self.trace_file\n        if trace_file is not None:\n            trace_files.append(trace_file)\n            trace_file = self.trace_file\n        while trace_file is not None:\n            trace_files.append(trace_file)\n            trace_file = self.trace_file\n        return trace_files\n\n    def start_trace_watch(self):\n        \"\"\"Starts parsing and retrieving the trace files that have not been\n        sent to the service yet.\n        \"\"\"\n        self.trace_files = self.update_trace_watch()\n        self.trace_files = self.trace_files + self.trace_files\n        self.trace_files = self.trace_files + self.trace_files\n        self.trace_files = self.trace_files + self.trace_files\n        self.trace_file", "solution": "\n\n        # Check the size stamp of the tracefile. Only proceed with the parsing\n        # if it changed from the previous size.\n        size_stamp = os.path.getsize(self.trace_file)\n        self.trace_retry = 0\n        if size_stamp and size_stamp == self.trace_sizestamp:\n            return\n        else:\n            logger.debug(\"Updating trace size stamp to: {}\".format(size_stamp))\n            self.trace_sizestamp = size_stamp\n\n        with open(self.trace_file) as fh:\n\n            # Skip potential empty lines at the start of file\n            header = next(fh).strip()\n            while not header:\n                header = next(fh).strip()\n\n            # Get header mappings before parsing the file\n            hm = self._header_mapping(header)\n\n            for line in fh:\n                # Skip empty lines\n                if line.strip() == \"\":\n                    continue\n\n                fields = line.strip().split(\"\\t\")\n\n                # Skip if task ID was already processes\n                if fields[hm[\"task_id\"]] in self.stored_ids:\n                    continue\n\n                if fields[hm[\"process\"]] == \"report\":\n                    self.report_queue.append(\n                        self._expand_path(fields[hm[\"hash\"]])\n                    )\n                    self.send = True\n\n                # Add the processed trace line to the stored ids. It will be\n                # skipped in future parsers\n                self.stored_ids.append(fields[hm[\"task_id\"]])"}
{"prompt": "def permission_check(apikey, endpoint):\n        \"\"\"\n        return (user, seckey) if url end point is in allowed entry point list\n        \"\"\"", "output": "\n        allowed_endpoints = getattr(settings, 'ALLOWED_ENDPOINTS', [])\n        allowed_endpoints = [x.strip() for x in allowed_endpoints.split(',')]\n        allowed_endpoints = [x.split(':') for x in allowed_endpoints]\n        allowed_endpoints = [x for x in allowed_endpoints if len(x) == 2]\n        allowed_endpoints = [x for x in allowed_endpoints if x[0].endswith('/')]\n        allowed_endpoints = [x for x in allowed_endpoints if x[1].startswith('/')]\n        allowed_endpoints = [x for x in allowed_endpoints if x[1].endswith('/')]\n        allowed_endpoints = [x for x in allowed_endpoints if x[1].startswith('/')]\n        allowed_endpoints = [x for x in allowed_endpoints if x[1].endswith('/')]\n        allowed_endpoints = [x for x in allowed_", "solution": "\n        try:\n            ak = APIKeys.objects.get(apikey=apikey)\n            apitree = cPickle.loads(ak.apitree.encode(\"ascii\"))\n            if apitree.match(endpoint):\n                return ak.user if ak.user else AnonymousUser(), ak.seckey\n        except APIKeys.DoesNotExist:\n            pass\n        return None, None"}
{"prompt": "def __create_criterion(self, criterion_str):\n        \"\"\" instantiate criterion from config string \"\"\"", "output": "\n        criterion_class = eval(criterion_str)\n        criterion = criterion_class(self.opt, shared=False)\n        return criterion\n\n    def __create_optimizer(self):\n        \"\"\"\n        Create optimizer with specific params and initialize scheduler.\n        \"\"\"\n        if self.opt.get('optimizer') is None:\n            raise ConfigurationError('Optimizer must have a valid name')\n        optimizer_name = self.opt['optimizer']\n        if optimizer_name =='sgd':\n            optimizer = optim.SGD(self.parameters(), lr=self.opt['lr'])\n        elif optimizer_name == 'adam':\n            optimizer = optim.Adam(self.parameters(), lr=self.opt['lr'])\n        else:\n            raise ConfigurationError('Invalid optimizer %s' % optimizer_name)\n        scheduler = lr_scheduler.MultiStepLR(", "solution": "\n        parsed = criterion_str.split(\"(\")\n        type_str = parsed[0].strip().lower()\n        parsed[1] = parsed[1].split(\")\")[0].strip()\n\n        for criterion_class in self.custom_criterions:\n            if criterion_class.get_type_string() == type_str:\n                return criterion_class(self, parsed[1])\n        raise ValueError(\n            \"Unsupported autostop criterion type: %s\" % criterion_str)"}
{"prompt": "def verboseRead(self, alphabet, context='', skipExtra=False):\n        \"\"\"Read symbol and extra from stream and explain what happens.\n        Returns the value of the symbol\n        >>> olleke.pos = 0\n        >>> l = Layout(olleke)\n        >>> l.verboseRead(WindowSizeAlphabet())\n        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288\n        4194288\n        \"\"\"", "output": "\n        symbol = self.readSymbol(alphabet, context, skipExtra)\n        if symbol is None:\n            return None\n        else:\n            return symbol\n\n    def verboseWrite(self, alphabet, context='', skipExtra=False):\n        \"\"\"Write symbol and extra into stream and explain what happens.\n        Returns the value of the symbol\n        >>> olleke.pos = 0\n        >>> l = Layout(olleke)\n        >>> l.verboseWrite(WindowSizeAlphabet())\n        0000  1b                   1011 WSIZE   windowsize=(1<<22)-16=4194288\n        4194288\n        \"\"\"\n        symbol = self.writeSymbol(alphabet, context, skipExtra)\n        if symbol is None:", "solution": "\n        #TODO 2: verbosity level, e.g. show only codes and maps in header\n        stream = self.stream\n        pos = stream.pos\n        if skipExtra:\n            length, symbol = alphabet.readTuple(stream)\n            extraBits, extra = 0, None\n        else:\n            length, symbol, extraBits, extra = alphabet.readTupleAndExtra(\n                stream)\n        #fields: address, hex data, binary data, name of alphabet, explanation\n        hexdata = self.makeHexData(pos)\n        addressField = '{:04x}'.format(pos+7>>3) if hexdata else ''\n        bitdata = self.formatBitData(pos, length, extraBits)\n        #bitPtr moves bitdata so that the bytes are easier to read\n        #jump back to right if a new byte starts\n        if '|' in bitdata[1:]:\n            #start over on the right side\n            self.bitPtr = self.width\n        fillWidth = self.bitPtr-(len(hexdata)+len(bitdata))\n        if fillWidth<0: fillWidth = 0\n        print('{:<5s} {:<{}s} {:7s} {}'.format(\n            addressField,\n            hexdata+' '*fillWidth+bitdata, self.width,\n            context+alphabet.name,\n            symbol if skipExtra else symbol.explanation(extra),\n            ))\n        #jump to the right if we started with a '|'\n        #because we didn't jump before printing\n        if bitdata.startswith('|'): self.bitPtr = self.width\n        else: self.bitPtr -= len(bitdata)\n        return symbol if skipExtra else symbol.value(extra)"}
{"prompt": "def builtin(cls, name):\n        \"\"\"\n        Generate a default legend.\n\n        Args:\n            name (str): The name of the legend you want. Not case sensitive.\n                 'nsdoe': Nova Scotia Dept. of Energy\n                 'canstrat': Canstrat\n                 'nagmdm__6_2': USGS N. Am. Geol. Map Data Model 6.2\n                 'nagmdm__6_1': USGS N. Am. Geol. Map Data Model 6.1\n                 'nagmdm__4_3': USGS N. Am. Geol. Map Data Model 4.3\n                 'sgmc': USGS State Geologic Map Compilation\n\n            Default 'nagmdm__6_2'.\n\n        Returns:\n            Legend: The legend stored in `defaults.py`.\n        \"\"\"", "output": "3\n                 'nagmdm__4_2': USGS N. Am. Geol. Map Data Model 4.2\n                 'nagmdm__4_1': USGS N. Am. Geol. Map Data Model 4.1\n                 'nagmdm__4_0': USGS N. Am. Geol. Map Data Model 4.0\n                 'nagmdm__3_1': USGS N. Am. Geol. Map Data Model 3.1\n                 'nagmdm__3_0': USGS N. Am. Geol. Map Data Model 3.0\n                 'nagmdm__", "solution": "\n        names = {\n                 'nsdoe': LEGEND__NSDOE,\n                 'canstrat': LEGEND__Canstrat,\n                 'nagmdm__6_2': LEGEND__NAGMDM__6_2,\n                 'nagmdm__6_1': LEGEND__NAGMDM__6_1,\n                 'nagmdm__4_3': LEGEND__NAGMDM__4_3,\n                 'sgmc': LEGEND__SGMC,\n                 }\n        return cls.from_csv(text=names[name.lower()])"}
{"prompt": "def parse_email(data, strip_attachment_payloads=False):\n    \"\"\"\n    A simplified email parser\n\n    Args:\n        data: The RFC 822 message string, or MSG binary\n        strip_attachment_payloads (bool): Remove attachment payloads\n\n    Returns (dict): Parsed email data\n    \"\"\"", "output": "\n    msg = email.message_from_string(data)\n    payloads = msg.get_payload()\n    payloads = payloads.decode() if isinstance(payloads, bytes) else payloads\n    payloads = payloads.split(b'\\r\\n')\n    payloads = [p.split(b'\\r\\n') for p in payloads]\n    payloads = [p for p in payloads if len(p) > 1]\n    payloads = [p for p in payloads if p[0]!= 'From']\n    payloads = [p for p in payloads if p[1]!= 'To']\n    payloads = [p for p in payloads if p[2]!= 'Date']\n    payloads = [p for p in payloads if p[3]!= 'Subject']\n    payloads = [p for p in payloads if p[4", "solution": "\n\n    if type(data) == bytes:\n        if is_outlook_msg(data):\n            data = convert_outlook_msg(data)\n        data = data.decode(\"utf-8\", errors=\"replace\")\n    parsed_email = mailparser.parse_from_string(data)\n    headers = json.loads(parsed_email.headers_json).copy()\n    parsed_email = json.loads(parsed_email.mail_json).copy()\n    parsed_email[\"headers\"] = headers\n\n    if \"received\" in parsed_email:\n        for received in parsed_email[\"received\"]:\n            if \"date_utc\" in received:\n                if received[\"date_utc\"] is None:\n                    del received[\"date_utc\"]\n                else:\n                    received[\"date_utc\"] = received[\"date_utc\"].replace(\"T\",\n                                                                        \" \")\n\n    if \"from\" not in parsed_email:\n        if \"From\" in parsed_email[\"headers\"]:\n            parsed_email[\"from\"] = parsed_email[\"Headers\"][\"From\"]\n        else:\n            parsed_email[\"from\"] = None\n\n    if parsed_email[\"from\"] is not None:\n        parsed_email[\"from\"] = parse_email_address(parsed_email[\"from\"][0])\n\n    if \"date\" in parsed_email:\n        parsed_email[\"date\"] = parsed_email[\"date\"].replace(\"T\", \" \")\n    else:\n        parsed_email[\"date\"] = None\n    if \"reply_to\" in parsed_email:\n        parsed_email[\"reply_to\"] = list(map(lambda x: parse_email_address(x),\n                                            parsed_email[\"reply_to\"]))\n    else:\n        parsed_email[\"reply_to\"] = []\n\n    if \"to\" in parsed_email:\n        parsed_email[\"to\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"to\"]))\n    else:\n        parsed_email[\"to\"] = []\n\n    if \"cc\" in parsed_email:\n        parsed_email[\"cc\"] = list(map(lambda x: parse_email_address(x),\n                                      parsed_email[\"cc\"]))\n    else:\n        parsed_email[\"cc\"] = []\n\n    if \"bcc\" in parsed_email:\n        parsed_email[\"bcc\"] = list(map(lambda x: parse_email_address(x),\n                                       parsed_email[\"bcc\"]))\n    else:\n        parsed_email[\"bcc\"] = []\n\n    if \"delivered_to\" in parsed_email:\n        parsed_email[\"delivered_to\"] = list(\n            map(lambda x: parse_email_address(x),\n                parsed_email[\"delivered_to\"])\n        )\n\n    if \"attachments\" not in parsed_email:\n        parsed_email[\"attachments\"] = []\n    else:\n        for attachment in parsed_email[\"attachments\"]:\n            if \"payload\" in attachment:\n                payload = attachment[\"payload\"]\n                try:\n                    if \"content_transfer_encoding\" in attachment:\n                        if attachment[\"content_transfer_encoding\"] == \"base64\":\n                            payload = decode_base64(payload)\n                        else:\n                            payload = str.encode(payload)\n                    attachment[\"sha256\"] = hashlib.sha256(payload).hexdigest()\n                except Exception as e:\n                    logger.debug(\"Unable to decode attachment: {0}\".format(\n                        e.__str__()\n                    ))\n        if strip_attachment_payloads:\n            for attachment in parsed_email[\"attachments\"]:\n                if \"payload\" in attachment:\n                    del attachment[\"payload\"]\n\n    if \"subject\" not in parsed_email:\n        parsed_email[\"subject\"] = None\n\n    parsed_email[\"filename_safe_subject\"] = get_filename_safe_string(\n        parsed_email[\"subject\"])\n\n    if \"body\" not in parsed_email:\n        parsed_email[\"body\"] = None\n\n    return parsed_email"}
{"prompt": "def get_news(self, tickers=[], tags=[], sources=[], startDate=None,\n                 endDate=None, limit=100, offset=0, sortBy=\"publishedDate\",\n                 fmt='json'):\n        \"\"\"Return list of news articles matching given search terms\n            https://api.tiingo.com/docs/tiingo/news\n\n            # Dates are in YYYY-MM-DD Format.\n\n            Args:\n                tickers [string] : List of unique Stock Tickers to search\n                tags [string] : List of topics tagged by Tiingo Algorithms\n                sources [string]: List of base urls to include as news sources\n                startDate, endDate [date]: Boundaries of news search window\n                limit (int): Max results returned. Default 100, max 1000\n                offset (int): Search results offset, used for paginating\n                sortBy (string): \"publishedDate\" OR (#TODO: UPDATE THIS)\n        \"\"\"", "output": "articles to return\n                limit [int]: Maximum number of news articles to return\n                offset [int]: Number of news articles to skip before returning news articles\n                sortBy [string]: Sorting criteria for news articles\n                fmt [string]: Format of returned data\n\n            Returns:\n                List of news articles\n        \"\"\"\n        endpoint = \"news/\"\n        params = {\"tickers\": tickers, \"tags\": tags, \"sources\": sources,\n                  \"startDate\": startDate, \"endDate\": endDate, \"limit\": limit,\n                  \"offset\": offset, \"sortBy\": sortBy}\n        if fmt == \"json\":", "solution": "\n        url = \"tiingo/news\"\n        params = {\n            'limit': limit,\n            'offset': offset,\n            'sortBy': sortBy,\n            'tickers': tickers,\n            'sources': sources,\n            'tags': tags,\n            'startDate': startDate,\n            'endDate': endDate\n        }\n        response = self._request('GET', url, params=params)\n        data = response.json()\n        if fmt == 'json':\n            return data\n        elif fmt == 'object':\n            return [dict_to_object(item, \"NewsArticle\") for item in data]"}
{"prompt": "def _iter_nodes(self, node, recursive=False, max_depth=float('inf'),\n                    with_links=True, in_search=False, predicate=None):\n        \"\"\"Returns an iterator over nodes hanging below a given start node.\n\n        :param node:\n\n            Start node\n\n        :param recursive:\n\n            Whether recursively also iterate over the children of the start node's children\n\n        :param max_depth:\n\n            Maximum depth to search for\n\n        :param in_search:\n\n            if it is used during get search and if detailed info should be returned\n\n        :param with_links:\n\n            If links should be considered\n\n        :param predicate:\n\n            A predicate to filter nodes\n\n        :return: Iterator\n\n        \"\"\"", "output": "\n        if in_search:\n            depth = max_depth\n        else:\n            depth = 0\n        depth = depth + 1\n        stack = [node]\n        while stack:\n            node = stack.pop()\n            if predicate and predicate(node):\n                continue\n            if max_depth and depth > max_depth:\n                break\n            if with_links and node.links:\n                for link in node.links:\n                    if predicate and predicate(link):\n                        continue\n                    stack.append(link)\n            yield node\n            if recursive:\n                for child in node.children:\n                    for node in _iter_nodes(child, recursive, max_depth, with_links, in_search, predicate):\n                        yield node\n\n    def search(self, node, predicate", "solution": "\n        def _run_predicate(x, run_name_set):\n            branch = x.v_run_branch\n            return branch == 'trajectory' or branch in run_name_set\n\n        if max_depth is None:\n            max_depth = float('inf')\n\n        if predicate is None:\n            predicate = lambda x: True\n        elif isinstance(predicate, (tuple, list)):\n            # Create a predicate from a list of run names or run indices\n            run_list = predicate\n            run_name_set = set()\n            for item in run_list:\n                if item == -1:\n                    run_name_set.add(self._root_instance.f_wildcard('$', -1))\n                elif isinstance(item, int):\n                    run_name_set.add(self._root_instance.f_idx_to_run(item))\n                else:\n                    run_name_set.add(item)\n            predicate = lambda x: _run_predicate(x, run_name_set)\n\n        if recursive:\n            return NaturalNamingInterface._recursive_traversal_bfs(node,\n                                            self._root_instance._linked_by,\n                                            max_depth, with_links,\n                                            in_search, predicate)\n        else:\n            iterator = (x for x in self._make_child_iterator(node, with_links) if\n                        predicate(x[2]))\n            if in_search:\n                return iterator # Here we return tuples: (depth, name, object)\n            else:\n                return (x[2] for x in iterator)"}
{"prompt": "def du_handler(self, args):\n    \"\"\"Handler for size command\"\"\"", "output": "\n    size = self.du_size\n    size = int(size)\n    if size < 0:\n      size = 0\n    elif size == 0:\n      size = 1\n    else:\n      size = min(size, MAX_SIZE)\n    self.du_size = size\n    self.du_handler.send(self.du_size)\n\n  def du_handler(self, args):\n    \"\"\"Handler for du command\"\"\"\n    du = self.du\n    du = int(du)\n    if du < 0:\n      du = 0\n    elif du == 0:\n      du = 1\n    else:\n      du = min(du, MAX_SIZE)\n    self.du = du\n    self.du_handler.send(self.du)\n\n  def du_handler(self, args):\n    \"\"\"Handl", "solution": "\n    for src, size in self.s3handler().size(args[1:]):\n      message('%s\\t%s' % (size, src))"}
{"prompt": "def gauge(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Gauge metric with the specified value\"\"\"", "output": "\n        body = {\"metric\": name, \"value\": value, \"rate\": rate}\n        self.gauge(name, body)\n\n    def gauge_batch(self, name, values, rate=1):\n        # type: (str, List[float], float) -> None\n        \"\"\"Send a Gauge metric with the specified values\"\"\"\n        body = {\"metric\": name, \"values\": values, \"rate\": rate}\n        self.gauge_batch(name, body)\n\n    def gauge_counter(self, name, value, rate=1):\n        # type: (str, float, float) -> None\n        \"\"\"Send a Gauge metric with the specified value\"\"\"\n        body = {\"metric\": name, \"value\": value, \"rate\": rate}\n        self.gauge_counter(name, body)\n\n    def gauge_", "solution": "\n\n        if self._should_send_metric(name, rate):\n            if not is_numeric(value):\n                value = float(value)\n            self._request(\n                Gauge(\n                    self._create_metric_name_for_request(name),\n                    value,\n                    rate\n                ).to_request()\n            )"}
{"prompt": "def watch_port_events(port, chip, pin_function_maps, event_queue,\n                      return_after_kbdint=False):\n    \"\"\"Waits for a port event. When a port event occurs it is placed onto the\n    event queue.\n\n    :param port: The port we are waiting for interrupts on (GPIOA/GPIOB).\n    :type port: int\n    :param chip: The chip we are waiting for interrupts on.\n    :type chip: :class:`pifacecommon.mcp23s17.MCP23S17`\n    :param pin_function_maps: A list of classes that have inheritted from\n        :class:`FunctionMap`\\ s describing what to do with events.\n    :type pin_function_maps: list\n    :param event_queue: A queue to put events on.\n    :type event_queue: :py:class:`multiprocessing.Queue`\n    \"\"\"", "output": "event_queue: The event queue to place the event on.\n    :type event_queue: :class:`pifacecommon.mcp23s17.EventQueue`\n    :param return_after_kbdint: Whether to return the event after a kbdint\n        has been received.\n    :type return_after_kbdint: bool\n    \"\"\"\n    event_queue.put_nowait(\n        EventQueueEvent(port, EventQueueEvent.PORT_EVENT, chip,\n                        pin_function_maps, return_after_kbdint))\n\n\ndef watch_port_interrupts(port, chip, pin_function_maps, event_queue,\n                          return_after_kbdint=False):\n    \"\"\"Waits for a port interrupt. When a port interrup", "solution": "\n    # set up epoll\n    gpio25 = open(GPIO_INTERRUPT_DEVICE_VALUE, 'r')  # change to use 'with'?\n    epoll = select.epoll()\n    epoll.register(gpio25, select.EPOLLIN | select.EPOLLET)\n\n    while True:\n        # wait here until input\n        try:\n            events = epoll.poll()\n        except KeyboardInterrupt as e:\n            if return_after_kbdint:\n                return\n            else:\n                raise e\n        except IOError as e:\n            # ignore \"Interrupted system call\" error.\n            # I don't really like this solution. Ignoring problems is bad!\n            if e.errno != errno.EINTR:\n                raise\n\n        # find out where the interrupt came from and put it on the event queue\n        if port == pifacecommon.mcp23s17.GPIOA:\n            interrupt_flag = chip.intfa.value\n        else:\n            interrupt_flag = chip.intfb.value\n\n        if interrupt_flag == 0:\n            continue  # The interrupt has not been flagged on this board\n        else:\n            if port == pifacecommon.mcp23s17.GPIOA:\n                interrupt_capture = chip.intcapa.value\n            else:\n                interrupt_capture = chip.intcapb.value\n            event_queue.add_event(InterruptEvent(\n                interrupt_flag, interrupt_capture, chip, time.time()))\n\n    epoll.close()"}
{"prompt": "def _parse_retry_after(self, response):\n        \"\"\"Parse Retry-After header from response if it is set.\"\"\"", "output": "\n        retry_after = response.getheader('Retry-After')\n        if retry_after:\n            retry_after = retry_after.strip()\n            if retry_after.lower() == '0':\n                # No Retry-After header means no retry.\n                return None\n            # Convert Retry-After header to seconds.\n            retry_after = retry_after.split(';')[0].strip()\n            try:\n                return int(retry_after)\n            except ValueError:\n                # Invalid Retry-After header means no retry.\n                return None\n\n    def _parse_retry_after_seconds(self, response):\n        \"\"\"Parse Retry-After-Seconds header from response if it is set.\"\"\"\n        retry_after_seconds = response.getheader('Retry-", "solution": "\n        value = response.headers.get('Retry-After')\n\n        if not value:\n            seconds = 0\n        elif re.match(r'^\\s*[0-9]+\\s*$', value):\n            seconds = int(value)\n        else:\n            date_tuple = email.utils.parsedate(value)\n            if date_tuple is None:\n                seconds = 0\n            else:\n                seconds = time.mktime(date_tuple) - time.time()\n        return max(0, seconds)"}
{"prompt": "def _utf8_params(params):\n    \"\"\"encode a dictionary of URL parameters (including iterables) as utf-8\"\"\"", "output": "\n    if isinstance(params, dict):\n        utf8_params = dict((k, v.encode('utf-8') if isinstance(v, bytes) else v)\n                           for k, v in params.items())\n    else:\n        utf8_params = params\n    return utf8_params\n\n\ndef _utf8_query(query):\n    \"\"\"encode a query string as utf-8\"\"\"\n    if isinstance(query, dict):\n        utf8_query = dict((k, v.encode('utf-8') if isinstance(v, bytes) else v)\n                          for k, v in query.items())\n    else:\n        utf8_query = query\n    return utf8_query\n\n\ndef _encode_params(params):\n    \"\"\"Encode the given parameters as a URL query string.\"\"\"\n    utf8_params = _utf8_params(params)\n    return urllib.parse.urlencode(utf8_params)", "solution": "\n    assert isinstance(params, dict)\n    encoded_params = []\n    for k, v in params.items():\n        if v is None:\n            continue\n        if isinstance(v, integer_types + (float,)):\n            v = str(v)\n        if isinstance(v, (list, tuple)):\n            v = [to_bytes(x) for x in v]\n        else:\n            v = to_bytes(v)\n        encoded_params.append((k, v))\n    return dict(encoded_params)"}
{"prompt": "def parse_variant_id(chrom, pos, ref, alt, variant_type):\n    \"\"\"Parse the variant id for a variant\n\n    variant_id is used to identify variants within a certain type of\n    analysis. It is not human readable since it is a md5 key.\n\n    Args:\n        chrom(str)\n        pos(str)\n        ref(str)\n        alt(str)\n        variant_type(str): 'clinical' or 'research'\n\n    Returns:\n        variant_id(str): The variant id converted to md5 string\n    \"\"\"", "output": "\n    variant_id = str(chrom) + str(pos) + str(ref) + str(alt)\n    if variant_type == 'clinical':\n        variant_id = variant_id.replace('-', '')\n        variant_id = variant_id.replace('_', '')\n        variant_id = variant_id.replace('.', '')\n        variant_id = variant_id.replace(':', '')\n        variant_id = variant_id.replace(';', '')\n        variant_id = variant_id.replace('-', '')\n        variant_id = variant_id.replace('_', '')\n        variant_id = variant_id.replace('.', '')\n        variant_id = variant_id.replace(':', '')\n        variant_id = variant_id.replace(';', '')\n        variant_id = variant_id.replace('-', '')\n        variant_id = variant_", "solution": "\n    return generate_md5_key([chrom, pos, ref, alt, variant_type])"}
{"prompt": "def _construct_schema(elements, nsmap):\n    \"\"\"Consruct fiona schema based on given elements\n\n    :param list Element: list of elements\n    :param dict nsmap: namespace map\n\n    :return dict: schema\n    \"\"\"", "output": "\n    schema = {}\n    for element in elements:\n        tag = element.tag\n        namespace = nsmap.get(tag, None)\n        if namespace is not None:\n            schema[namespace] = element\n    return schema\n\n\ndef _construct_geometry(geometry, geometrytype, geometryformat, geometryunits):\n    \"\"\"Consruct fiona geometry based on given geometry\n\n    :param dict geometry: geometry\n    :param str geometrytype: geometrytype\n    :param str geometryformat: geometryformat\n    :param str geometryunits: geometryunits\n\n    :return dict: geometry\n    \"\"\"\n    geometry = {}\n    geometry['geometry'] = geometry\n    geometry['type'] = geometrytype\n    geometry['format'] = geometryformat\n", "solution": "\n\n    schema = {\n        'properties': {},\n        'geometry': None\n    }\n\n    schema_key = None\n    gml_key = None\n\n    # if nsmap is defined, use it\n    if nsmap:\n        for key in nsmap:\n            if nsmap[key] == XS_NAMESPACE:\n                schema_key = key\n            if nsmap[key] in GML_NAMESPACES:\n                gml_key = key\n    # if no nsmap is defined, we have to guess\n    else:\n        gml_key = 'gml'\n        schema_key = 'xsd'\n\n    mappings = {\n        'PointPropertyType': 'Point',\n        'PolygonPropertyType': 'Polygon',\n        'LineStringPropertyType': 'LineString',\n        'MultiPointPropertyType': 'MultiPoint',\n        'MultiLineStringPropertyType': 'MultiLineString',\n        'MultiPolygonPropertyType': 'MultiPolygon',\n        'MultiGeometryPropertyType': 'MultiGeometry',\n        'GeometryPropertyType': 'GeometryCollection',\n        'SurfacePropertyType': '3D Polygon',\n        'MultiSurfacePropertyType': '3D MultiPolygon'\n    }\n\n    for element in elements:\n        data_type = element.attrib['type'].replace(gml_key + ':', '')\n        name = element.attrib['name']\n\n        if data_type in mappings:\n            schema['geometry'] = mappings[data_type]\n            schema['geometry_column'] = name\n        else:\n            schema['properties'][name] = data_type.replace(schema_key+':', '')\n\n    if schema['properties'] or schema['geometry']:\n        return schema\n    else:\n        return None"}
{"prompt": "def _flatten(child, parent):\n    \"\"\"Custom flattening method for the parse tree.\"\"\"", "output": "\n    if isinstance(child, list):\n        for subchild in child:\n            for item in _flatten(subchild, parent):\n                yield item\n    else:\n        for item in _flatten(child, parent):\n            yield item\n\n\ndef flatten(root):\n    \"\"\"Produce a flat list from a root node.\"\"\"\n    items = []\n    for child in root:\n        items.extend(_flatten(child, []))\n    return items\n\n\nclass ParseResults(object):\n    \"\"\"Holds results from a parsing operation.\n\n    Attributes:\n        text -- The text of the entire document parsed.\n        parse -- The parse actions used to build this document.\n        parse action -- The parse action for the next token produced.\n        start -- The start symbol", "solution": "\n    return parent.is_type(TokenType.expression) and child.node_type == parent.node_type"}
{"prompt": "def read(self, resource_id):\n        \"\"\"Read object in elasticsearch using the resource_id.\n        :param resource_id: id of the object to be read.\n        :return: object value from elasticsearch.\n        \"\"\"", "output": "\n        client = elasticsearch.Elasticsearch(self.hosts)\n        body = {'query': {'match': {'_id': resource_id}},'size': 1}\n        hits = client.search(self.index_name, body=body)\n        if len(hits) > 0:\n            return hits[0]['_source']\n        else:\n            return None\n\n    def search(self, query, size=10, sort=None, fields=None):\n        \"\"\"Search for objects in elasticsearch using the query.\n        :param query: query to be used for searching.\n        :param size: number of objects to be returned.\n        :param sort: sort object by specified field.\n        :param fields: fields to be returned in response.\n        :return: list of objects in elasticsearch.\n        \"\"\"\n        client = elasticsearch.Elasticsearch(self.hosts)\n        body = ", "solution": "\n        self.logger.debug('elasticsearch::read::{}'.format(resource_id))\n        return self.driver._es.get(\n            index=self.driver._index,\n            id=resource_id,\n            doc_type='_doc'\n        )['_source']"}
{"prompt": "def date_uncertainty_due_to_rate(self, node, interval=(0.05, 0.095)):\n        \"\"\"use previously calculated variation of the rate to estimate\n        the uncertainty in a particular numdate due to rate variation.\n\n        Parameters\n        ----------\n        node : PhyloTree.Clade\n            node for which the confidence interval is to be calculated\n        interval : tuple, optional\n            Array of length two, or tuple, defining the bounds of the confidence interval\n\n        \"\"\"", "output": "\n        date = self.dates[node.id]\n        rate = self.rates[node.id]\n        interval_lower = interval[0]\n        interval_upper = interval[1]\n        interval_lower_due_to_rate = (\n            interval_lower - rate * date\n        )  # the uncertainty due to the rate is given by the difference between rate and date (which is given by the difference between interval_lower and interval_upper)\n        interval_upper_due_to_rate = (\n            interval_upper + rate * date\n        )  # the uncertainty due to the rate is given by the difference between rate and date (which is given by the difference between interval_", "solution": "\n        if hasattr(node, \"numdate_rate_variation\"):\n            from scipy.special import erfinv\n            nsig = [np.sqrt(2.0)*erfinv(-1.0 + 2.0*x) if x*(1.0-x) else 0\n                    for x in interval]\n            l,c,u = [x[1] for x in node.numdate_rate_variation]\n            return np.array([c + x*np.abs(y-c) for x,y in zip(nsig, (l,u))])\n\n        else:\n            return None"}
{"prompt": "def score(self, x, w=None, **kwargs):\n        \"\"\"Compute R^2 coefficient of determination for a given input.\n\n        Parameters\n        ----------\n        x : ndarray (num-examples, num-inputs)\n            An array containing data to be fed into the network. Multiple\n            examples are arranged as rows in this array, with columns containing\n            the variables for each example.\n\n        Returns\n        -------\n        r2 : float\n            The R^2 correlation between the prediction of this netork and its\n            input. This can serve as one measure of the information loss of the\n            autoencoder.\n        \"\"\"", "output": "\n        if w is None:\n            w = self.w\n        pred = self.predict(x)\n        pred = pred.reshape((pred.shape[0], -1))\n        pred = pred.dot(w)\n        return 1 - np.trace(pred * (pred * (1 - pred)))\n\n    def predict(self, x):\n        \"\"\"Predict the output of a single example.\n\n        Parameters\n        ----------\n        x : ndarray (num-examples, num-inputs)\n            An array containing data to be fed into the network. Multiple\n            examples are arranged as rows in this array, with columns containing\n            the variables for each example.\n\n        Returns\n        -------\n        output : ndarray (num-examples, num-outputs)\n            An array containing the predicted outputs of the example", "solution": "\n        return super(Autoencoder, self).score(x, x, w=w, **kwargs)"}
{"prompt": "def set_parent(self, parent):\n        \"\"\"Set parent ``Expression`` for this object.\n\n        Args:\n            parent (Expression): The ``Expression`` which contains this object.\n\n        Raises:\n            FiqlObjectException: Parent must be of type ``Expression``.\n        \"\"\"", "output": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of type Expression.\")\n\n        self._parent = parent\n\n    @property\n    def parent(self):\n        \"\"\"Get the parent ``Expression`` for this object.\n\n        Returns:\n            Expression: The parent ``Expression`` for this object.\n        \"\"\"\n        return self._parent\n\n    @property\n    def name(self):\n        \"\"\"Get the name of this object.\n\n        Returns:\n            str: The name of this object.\n        \"\"\"\n        return self._name\n\n    @property\n    def type(self):\n        \"\"\"Get the type of this object.\n\n        Returns:\n            str: The type of this object.\n        \"\"\"\n        return self._type\n\n    @property\n    def value(self):\n        \"\"\"Get the value of this object.\n\n        Returns:\n            str: The value of this object.\n        \"\"\"\n", "solution": "\n        if not isinstance(parent, Expression):\n            raise FiqlObjectException(\"Parent must be of %s not %s\" % (\n                Expression, type(parent)))\n        self.parent = parent"}
{"prompt": "def _build_kernel_function_declaration(self, name='kernel'):\n        \"\"\"Build and return kernel function declaration\"\"\"", "output": "\n        kernel_name = self.kernel_name + '(' + name + ')'\n        kernel_function = self.kernel_function\n        kernel_function_name = kernel_name + '()'\n        kernel_function_declaration = '\\n'.join([\n            '__kernel void'+ kernel_function_name + '(', kernel_function,\n            ') {',\n           ''+ kernel_function_name + '();',\n            '}',\n        ])\n        return kernel_function_declaration\n\n    def _build_kernel_function_definition(self, name='kernel'):\n        \"\"\"Build and return kernel function definition\"\"\"\n        kernel_name = self.kernel_name + '(' + name + ')'\n        kernel_function = self.kernel_function\n        kernel_function_name = kernel_name + '()'\n        kernel_function_definition = '\\n'.join([", "solution": "\n        array_declarations, array_dimensions = self._build_array_declarations(with_init=False)\n        scalar_declarations = self._build_scalar_declarations(with_init=False)\n        const_declarations = self._build_const_declartions(with_init=False)\n        return c_ast.FuncDecl(args=c_ast.ParamList(params=array_declarations + scalar_declarations +\n                                                          const_declarations),\n                              type=c_ast.TypeDecl(declname=name,\n                                                  quals=[],\n                                                  type=c_ast.IdentifierType(names=['void'])))"}
{"prompt": "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n                                   length=8):\n  \"\"\"Visualizes a qualitative analysis of a given model.\n\n  Args:\n    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n      h, w, c].\n    model: A DisentangledSequentialVAE model.\n    samples: Number of samples to draw from the latent distributions.\n    batch_size: Number of sequences to generate.\n    length: Number of timesteps to generate for each sequence.\n  \"\"\"", "output": "\n  samples = min(samples, inputs.shape[0])\n  latents = model.sample(samples, batch_size)\n  latents = latents.reshape([samples, length, -1])\n  inputs = tf.transpose(inputs, [0, 2, 1])\n  inputs = tf.reshape(inputs, [-1, length, 1, 1])\n  inputs = tf.tile(inputs, [samples, 1, 1])\n  inputs = tf.reshape(inputs, [samples, length, 1, 1])\n  inputs = tf.transpose(inputs, [0, 2, 1])\n  inputs = tf.reshape(inputs, [-1, length, 1, 1])\n  inputs = tf.tile(inputs, [samples, 1, 1])\n  inputs = tf.reshape(inputs, [samples, length, 1, 1])\n  inputs = tf.transpose(inputs, [0, 2, 1])\n  inputs = tf.reshape(inputs, [-1, length, 1, 1])\n  ", "solution": "\n  average = lambda dist: tf.reduce_mean(\n      input_tensor=dist.mean(), axis=0)  # avg over samples\n  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n                                    samples=samples)\n    visualize_reconstruction(inputs, average(reconstruct()))\n    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n                             name=\"static_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n                             name=\"dynamic_prior\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n                             name=\"swap_static\")\n    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n                             name=\"swap_dynamic\")\n\n  with tf.compat.v1.name_scope(\"generation\"):\n    generate = functools.partial(model.generate, batch_size=batch_size,\n                                 length=length, samples=samples)\n    image_summary(average(generate(fix_static=True)), \"fix_static\")\n    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")"}
{"prompt": "def __single_arity_fn_to_py_ast(\n    ctx: GeneratorContext,\n    node: Fn,\n    method: FnMethod,\n    def_name: Optional[str] = None,\n    meta_node: Optional[MetaNode] = None,\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"", "output": "\n    func_name = node.name\n    func_type = node.type\n    func_args = node.args\n    func_defaults = node.defaults\n    func_decorators = node.decorator_list\n    func_returns = node.returns\n    func_type_annotation = node.type_annotation\n    func_type_params = node.type_params\n    func_type_returns = node.type_returns\n    func_type_varargs = node.type_varargs\n    func_type_varkw = node.type_varkw\n    func_type_kwarg = node.type_kwarg\n    func_type_annotations = node.type_annotations\n    func_type_is_class = node.type_is_class\n    func_type_is_union = node.type_is_union\n    ", "solution": "\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )"}
{"prompt": "def activate(fn=None):\n    \"\"\"\n    Enables the HTTP traffic interceptors.\n\n    This function can be used as decorator.\n\n    Arguments:\n        fn (function|coroutinefunction): Optional function argument\n            if used as decorator.\n\n    Returns:\n        function: decorator wrapper function, only if called as decorator,\n            otherwise ``None``.\n\n    Example::\n\n        # Standard use case\n        pook.activate()\n        pook.mock('server.com/foo').reply(404)\n\n        res = requests.get('server.com/foo')\n        assert res.status_code == 404\n        pook.disable()\n\n        # Decorator use case\n        @pook.activate\n        def test_request():\n            pook.mock('server.com/foo').reply(404)\n\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"", "output": ".mock('server.com/foo').reply(404)\n            res = requests.get('server.com/foo')\n            assert res.status_code == 404\n    \"\"\"\n    if fn:\n        return fn\n    else:\n        def wrapper(*args, **kwargs):\n            pook.enable()\n            return fn(*args, **kwargs)\n        return wrapper\n\n\ndef disable():\n    \"\"\"\n    Disables the HTTP traffic interceptors.\n\n    This function can be used as decorator.\n\n    Returns:\n        function: decorator wrapper function, only if called as decorator,\n            otherwise ``None``.\n\n    Example::\n\n        # Standard use case\n        pook.disable()\n        pook.mock('server.com/foo').reply(404)\n\n        res = requests.get('server.com/foo')\n        assert ", "solution": "\n    # If not used as decorator, activate the engine and exit\n    if not isfunction(fn):\n        _engine.activate()\n        return None\n\n    # If used as decorator for an async coroutine, wrap it\n    if iscoroutinefunction is not None and iscoroutinefunction(fn):\n        return activate_async(fn, _engine)\n\n    @functools.wraps(fn)\n    def wrapper(*args, **kw):\n        _engine.activate()\n        try:\n            fn(*args, **kw)\n        finally:\n            _engine.disable()\n\n    return wrapper"}
{"prompt": "def parse(self, string):\n        \"\"\"\n        Parse lipid definition from string:\n\n            alhead=C P, allink=A A, altail=TCC CCCC, alname=DPSM, charge=0.0\n        \"\"\"", "output": "\n        lines = string.splitlines()\n        lip_def = LipDef()\n        lip_def.charge = 0.0\n        lip_def.name = \"\"\n        lip_def.alname = \"\"\n        lip_def.alhead = \"\"\n        lip_def.altail = \"\"\n        lip_def.alink = \"\"\n        lip_def.alink_charge = 0.0\n        lip_def.alink_charge_unit = \"\"\n        lip_def.alink_charge_charge = 0.0\n        lip_def.alink_charge_charge_unit = \"\"\n        lip_def.alink_charge_charge_charge_unit = \"\"\n        li", "solution": "\n        fields = [i.split(\"=\") for i in string.split(', ')]\n        for what, val in fields:\n            what = what.strip()\n            val  = val.split()\n            if what.endswith(\"head\"):\n                self.head = val\n            elif what.endswith(\"link\"):\n                self.link = val\n            elif what.endswith(\"tail\"):\n                self.tail = val\n            elif what == \"charge\":\n                self.charge = float(val[0])\n            elif what.endswith(\"name\") and not self.name:\n                self.name = val[0]\n        if self.charge is None:\n            # Infer charge from head groups\n            self.charge = sum([headgroup_charges[bead] for bead in self.head])"}
{"prompt": "def combine_columns_aggregation(\n        df,\n        id_cols: List[str],\n        cols_for_combination: Dict[str, str],\n        agg_func: Union[str, List[str], Dict[str, str]] = 'sum'\n):\n    \"\"\"\n    Aggregates data to reproduce \"All\" category for requester\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list*): the columns id to group\n    - `cols_for_combination` (*dict*): colums corresponding to\n       the filters as key and their default value as value\n\n    *optional :*\n    - `agg_func` (*str*, *list* or *dict*): the function(s) to use for aggregating the data.\n       Accepted combinations are:\n       - string function name\n       - list of functions and/or function names, e.g. [np.sum, 'mean']\n       - dict of axis labels -> functions, function names or list of such.\n    \"\"\"", "output": "to apply\n       to the columns to group by\n\n    ---\n\n    ### Example\n\n    ```\n    id_cols = ['user_id', 'device_id']\n    cols_for_combination = {\n        'user_id': 'user_id',\n        'device_id': 'device_id'\n    }\n    agg_func ='sum'\n    ```\n\n    ---\n\n    ### Example\n\n    ```\n    id_cols = ['user_id', 'device_id']\n    cols_for_combination = {\n        'user_id': 'user_id',\n        'device_id': 'device_id'\n    }\n    agg_func = 'count'\n    ```\n\n    ---\n\n    ### Example\n\n    ```\n    id_cols = ['user_id', 'device_id']\n    cols_for_combination = {\n        '", "solution": "\n    requesters_cols = list(cols_for_combination.keys())\n    requester_combination = [\n        list(item) for i in range(0, len(requesters_cols) + 1)\n        for item in itertools.combinations(requesters_cols, i)]\n    dfs_result = []\n    for comb in requester_combination:\n        df_tmp = df.groupby(id_cols + comb).agg(agg_func).reset_index()\n        for key in (set(cols_for_combination.keys()) - set(comb)):\n            df_tmp[key] = cols_for_combination[key]\n        dfs_result.append(df_tmp)\n\n    return pd.concat(dfs_result, sort=False, ignore_index=True)"}
{"prompt": "def _backup_file(self, file, patch):\n        \"\"\" Creates a backup of file \"\"\"", "output": "\n        backup_file = tempfile.NamedTemporaryFile(delete=False)\n        backup_file.write(file.read())\n        backup_file.close()\n        shutil.copy(file.name, backup_file.name)\n        patch.append(\n            patch.Patch(\n                file.name,\n                backup_file.name,\n                diff=file.diff,\n                backup=file.backup,\n                backup_file=backup_file.name,\n                backup_file_diff=backup_file.diff,\n            )\n        )\n\n    def _backup_file_diff(self, file, patch):\n        \"\"\" Creates a backup of file diff \"\"\"\n        backup_file = tempfile.NamedTemporaryFile(delete=False)\n        backup_file.write(file.read())\n        backup_file.close()\n        shutil.", "solution": "\n        dest_dir = self.quilt_pc + patch.get_name()\n        file_dir = file.get_directory()\n        if file_dir:\n            #TODO get relative path\n            dest_dir = dest_dir + file_dir\n        backup = Backup()\n        backup.backup_file(file, dest_dir, copy_empty=True)"}
{"prompt": "def delete(self, url, **kwargs):\n        \"\"\"Sends a DELETE request.\n\n        Args:\n            url(basestring): The URL of the API endpoint.\n            **kwargs:\n                erc(int): The expected (success) response code for the request.\n                others: Passed on to the requests package.\n\n        Raises:\n            ApiError: If anything other than the expected response code is\n                returned by the Webex Teams API endpoint.\n\n        \"\"\"", "output": "\n        kwargs['headers'] = kwargs.get('headers', {})\n        kwargs['headers']['X-Teams-API-Key'] = self.api_key\n        kwargs['headers']['X-Teams-API-Version'] = self.api_version\n        kwargs['headers']['Accept'] = 'application/json'\n        kwargs['headers']['Content-Type'] = 'application/json'\n        kwargs['headers']['X-Teams-API-Signature'] = self.signature\n        kwargs['headers']['X-Teams-API-Signature-Method'] = 'HMAC-SHA256'\n        kwargs['headers']['X-Teams-API-Signature-Version'] = '2'\n\n        try:\n            response = requests.delete(url, **kwargs)\n        except requests.exceptions.RequestException as e:\n            raise ApiError(e.response.text)\n\n        if response.status_code!= er", "solution": "\n        check_type(url, basestring, may_be_none=False)\n\n        # Expected response code\n        erc = kwargs.pop('erc', EXPECTED_RESPONSE_CODE['DELETE'])\n\n        self.request('DELETE', url, erc, **kwargs)"}
{"prompt": "def meteor_random_id(name=None, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"", "output": "\n    if name is None:\n        name ='meteor_random_id_%s' % str(uuid.uuid4())\n    return meteor_random_id_from_string(name, length)\n\n\ndef meteor_random_id_from_string(name, length=17):\n    \"\"\"Generate a new ID, optionally using namespace of given `name`.\"\"\"\n    length = min(length, 16)\n    return uuid.UUID(bytes=uuid.UUID(int=random.randrange(256),\n                                          version=4).bytes).hex[:length]\n\n\ndef meteor_random_id_from_uuid(uuid):\n    \"\"\"Generate a new ID, optionally using namespace of given `uuid`.\"\"\"\n    return uuid.uuid\n\n\ndef meteor_random_id_from_datetime(datetime):\n    \"\"\"Generate a new ID, optionally ", "solution": "\n    if name is None:\n        stream = THREAD_LOCAL.alea_random\n    else:\n        stream = THREAD_LOCAL.random_streams[name]\n    return stream.random_string(length, METEOR_ID_CHARS)"}
{"prompt": "def INIT(self):\n        \"\"\"INIT state.\n\n        [:rfc:`2131#section-4.4.1`]::\n\n            The client SHOULD wait a random time between one and ten\n            seconds to desynchronize the use of DHCP at startup\n\n        .. todo::\n           - The initial delay is implemented, but probably is not in other\n             implementations. Check what other implementations do.\n        \"\"\"", "output": "\n        self.delay = random.randint(0, 10) * 10\n        self.delay = min(self.delay, 60)\n        self.delay = max(self.delay, 60)\n\n    def DHCPDISCOVER(self, ipaddr, mac):\n        \"\"\"DHCPDISCOVER state.\n\n        [:rfc:`2131#section-4.4.2`]::\n\n            DHCP MUST send a DHCPDISCOVER message to the server\n            with the IP address and MAC address supplied in DHCP\n            options message\n\n       .. todo::\n           - Implement DHCPDISCOVER message handling\n        \"\"\"\n        self.dhcpDiscover(ipaddr, mac)\n\n    def DHCPREQUEST(self, ipaddr, mac):", "solution": "\n        # NOTE: in case INIT is reached from other state, initialize attributes\n        # reset all variables.\n        logger.debug('In state: INIT')\n        if self.current_state is not STATE_PREINIT:\n            self.reset()\n        self.current_state = STATE_INIT\n        # NOTE: see previous TODO, maybe this is not needed.\n        if self.delay_selecting:\n            if self.delay_before_selecting is None:\n                delay_before_selecting = gen_delay_selecting()\n            else:\n                delay_before_selecting = self.delay_before_selecting\n        else:\n            delay_before_selecting = 0\n        self.set_timeout(self.current_state,\n                         self.timeout_delay_before_selecting,\n                         delay_before_selecting)\n        if self.timeout_select is not None:\n            self.set_timeout(STATE_SELECTING,\n                             self.timeout_selecting,\n                             self.timeout_select)"}
{"prompt": "def bisect(seq, func=bool):\n\t\"\"\"\n\tSplit a sequence into two sequences:  the first is elements that\n\treturn False for func(element) and the second for True for\n\tfunc(element).\n\tBy default, func is ``bool``, so uses the truth value of the object.\n\n\t>>> is_odd = lambda n: n%2\n\t>>> even, odd = bisect(range(5), is_odd)\n\t>>> list(odd)\n\t[1, 3]\n\t>>> list(even)\n\t[0, 2, 4]\n\n\t>>> other, zeros = bisect(reversed(range(5)))\n\t>>> list(zeros)\n\t[0]\n\t>>> list(other)\n\t[4, 3, 2, 1]\n\n\t\"\"\"", "output": "\t[2, 4]\n\t\"\"\"\n\tseq = list(seq)\n\ti = bisect.bisect_left(seq, func(seq[0]))\n\tj = bisect.bisect_right(seq, func(seq[-1]))\n\treturn seq[:i], seq[j:]\n\ndef partition(seq, pred=bool):\n\t\"\"\"\n\tSplit a sequence into two sequences:  the first is elements that\n\tsatisfy pred(element) and the second for elements that\n\tdo not.\n\n\t>>> is_odd = lambda n: n%2\n\t>>> even, odd = partition(range(5), is_odd)\n\t>>> list(odd)\n\t[1, 3]\n\t>>> list(even)", "solution": "\n\tqueues = GroupbySaved(seq, func)\n\treturn queues.get_first_n_queues(2)"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        okay = True\n        while okay:\n            okay = raw_input(prompt + \": \").lower() in (\"y\", \"yes\")\n            if okay:\n                return default\n        return default\n\n    def confirmYesNo(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"\n        okay = True\n        while okay:\n            okay = raw_input(prompt + \": \").lower() in (\"n\", \"no\")\n            if okay:\n                return default\n        return default\n\n    def confirmYesNoWithDefault(self, prompt, default):\n", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def check_type(self, value, attr, data):\n        \"\"\"Validates a value against the correct type of the field.\n\n        It calls ``_expected_types`` to get a list of valid types.\n\n        Subclasses can do one of the following:\n\n            1. They can override the ``valid_types`` property with a tuple with\n            the expected types for this field.\n\n            2. They can override the ``_expected_types`` method to return a\n            tuple of expected types for the field.\n\n            3. They can change ``check_type`` completely to customize\n            validation.\n\n        This method or the overrides must return the ``value`` parameter\n        untouched.\n        \"\"\"", "output": "\n        field_type = self._expected_types.get(attr)\n        if field_type is None:\n            return value\n\n        if isinstance(field_type, tuple):\n            field_type = field_type[0]\n\n        if not isinstance(field_type, type):\n            raise exceptions.ValidationError(\n                \"Field of type %s is not valid. Valid types are %s.\"\n                % (field_type, self._expected_types)\n            )\n\n        return field_type(value)\n\n    def deconstruct(self):\n        \"\"\"Deconstructs a field from the data.\"\"\"\n        name, path, args, kwargs = super(JSONField, self).deconstruct()\n        kwargs[\"required\"] = False\n        return name, path, args, kwargs\n\n    def contribute_to_class(self, cls, name):\n        super(JSONField, self).contribute_to_class(cls, name)\n        set", "solution": "\n        expected_types = self._expected_types()\n        if not isinstance(value, expected_types):\n            raise self._not_expected_type(\n                value, expected_types, fields=[self], field_names=attr, data=data)\n        return value"}
{"prompt": "def _get_selection_cursor(self, start, end):\n        \"\"\" Convenience method that returns a cursor with text selected between\n            the positions 'start' and 'end'.\n        \"\"\"", "output": "\n        selection = Selection(start, end)\n        selection.select()\n        return selection\n\n    def _get_selection_range(self, start, end):\n        \"\"\" Convenience method that returns a range with text selected between\n            the positions'start' and 'end'.\n        \"\"\"\n        selection = Selection(start, end)\n        selection.select()\n        return selection\n\n    def _get_selection_text(self):\n        \"\"\" Convenience method that returns the text selected in the current\n            selection (if any).\n        \"\"\"\n        selection = self._get_selection_cursor()\n        return selection.text\n\n    def _get_selection_index(self):\n        \"\"\" Convenience method that returns the index of the current selection\n            in the text of the document.\n        \"\"\"\n        selection = self._get_selection_cursor()", "solution": "\n        cursor = self._control.textCursor()\n        cursor.setPosition(start)\n        cursor.setPosition(end, QtGui.QTextCursor.KeepAnchor)\n        return cursor"}
{"prompt": "def delay(self, sender, receiver, packet, add_delay=0):\n        \"\"\"\n        bandwidths are inaccurate, as we don't account for parallel transfers here\n        \"\"\"", "output": "\n        delay = packet.time - self._last_time\n        self._last_time = packet.time\n        if add_delay:\n            delay += self._delay\n        return delay\n\n    def packet_length(self, packet):\n        \"\"\"\n        packet length is the length of the packet in bytes\n        \"\"\"\n        return len(packet.data)\n\n    def packet_error(self, packet):\n        \"\"\"\n        packet error is 0 if packet is valid, -1 if packet is invalid\n        \"\"\"\n        return packet.error\n\n    def packet_ack(self, packet):\n        \"\"\"\n        packet ack is 0 if packet is valid, -1 if packet is invalid\n        \"\"\"\n        return packet.ack\n\n    def packet_ack_error(self, packet):\n        \"\"\"\n        packet ack error is ", "solution": "\n        bw = min(sender.ul_bandwidth, receiver.dl_bandwidth)\n        delay = sender.base_latency + receiver.base_latency\n        delay += len(packet) / bw\n        delay += add_delay\n        return delay"}
{"prompt": "def look_ahead_match(rating, tokens):\n    \"\"\"iterate through all tokens looking for matches of cleansed tokens\n    or token regexes, skipping tokens left empty by cleansing and\n    coping with Token objects that produce multiple space-separated\n    strings when cleansed.  Yields tokens that match.\n\n    \"\"\"", "output": "\n    cleansed = [t.strip() for t in tokens]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if t]\n    cleansed = [t for t in cleansed if ", "solution": "\n    ## this ensures that all cleansed tokens are non-zero length\n    all_mregexes = []\n    for m in rating.mentions:\n        mregexes = []\n        mpatterns = m.decode('utf8').split(' ')\n        for mpat in mpatterns:\n            if mpat.startswith('ur\"^') and mpat.endswith('$\"'): # is not regex\n                ## chop out the meat of the regex so we can reconstitute it below\n                mpat = mpat[4:-2]\n            else:\n                mpat = cleanse(mpat)\n            if mpat:\n                ## make a unicode raw string\n                ## https://docs.python.org/2/reference/lexical_analysis.html#string-literals\n                mpat = ur'^%s$' % mpat\n                logger.debug('look_ahead_match compiling regex: %s', mpat)\n                mregexes.append(re.compile(mpat, re.UNICODE | re.IGNORECASE))\n\n        if not mregexes:\n            logger.warn('got empty cleansed mention: %r\\nrating=%r' % (m, rating))\n\n        all_mregexes.append(mregexes)\n\n    ## now that we have all_mregexes, go through all the tokens\n    for i in range(len(tokens)):\n        for mregexes in all_mregexes:\n            if mregexes[0].match(tokens[i][0][0]):\n                ## found the start of a possible match, so iterate\n                ## through the tuples of cleansed strings for each\n                ## Token while stepping through the cleansed strings\n                ## for this mention.\n                m_j = 1\n                i_j = 0\n                last_token_matched = 0\n                matched = True\n                while m_j < len(mregexes):\n                    i_j += 1\n                    if i_j == len(tokens[i + last_token_matched][0]):\n                        i_j = 0\n                        last_token_matched += 1\n                        if i + last_token_matched == len(tokens):\n                            matched = False\n                            break\n                    target_token = tokens[i + last_token_matched][0][i_j]\n                    ## this next line is the actual string comparison\n                    if mregexes[m_j].match(target_token):\n                        m_j += 1\n                    elif target_token == '':\n                        continue\n                    else:\n                        matched = False\n                        break\n                if matched:\n                    ## yield each matched token only once\n                    toks = set()\n                    for j in xrange(last_token_matched + 1):\n                        toks.add(tokens[i + j][1])\n                    for tok in toks:\n                        yield tok"}
{"prompt": "def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports):\n    \"\"\"return lines with leading/trailing whitespace and any ignored code\n    features removed\n    \"\"\"", "output": "\n    lines = lines.split('\\n')\n    ignored = []\n    ignored2 = []\n    for line in lines:\n        line = line.strip()\n        if line == '' or line[0] == '#':\n            continue\n        if ignore_comments and line[0] == '#':\n            continue\n        if ignore_docstrings and line[:4] == '\"\"\"':\n            continue\n        if ignore_imports and line[:12] == 'from ':\n            continue\n        ignored.append(line)\n    return ignored\n\n\ndef strip_comments(lines):\n    \"\"\"return lines with leading whitespace removed\n    \"\"\"\n    lines = lines.split('\\n')\n    for line in lines:\n        line = line.strip()\n        if line == '' or line[0] == '#':\n            continue\n        yield line\n\n\ndef strip_docstrings(lines):\n    \"\"\"return ", "solution": "\n    if ignore_imports:\n        tree = astroid.parse(\"\".join(lines))\n        node_is_import_by_lineno = (\n            (node.lineno, isinstance(node, (astroid.Import, astroid.ImportFrom)))\n            for node in tree.body\n        )\n        line_begins_import = {\n            lineno: all(is_import for _, is_import in node_is_import_group)\n            for lineno, node_is_import_group in groupby(\n                node_is_import_by_lineno, key=lambda x: x[0]\n            )\n        }\n        current_line_is_import = False\n\n    strippedlines = []\n    docstring = None\n    for lineno, line in enumerate(lines, start=1):\n        line = line.strip()\n        if ignore_docstrings:\n            if not docstring and any(\n                line.startswith(i) for i in ['"}
{"prompt": "def add_field(self, model, field):\n        \"\"\"Ran when a field is added to a model.\"\"\"", "output": "\n        field_name = field.name\n        field_type = field.db_type(connection=self.connection)\n        field_default = field.default\n        field_choices = field.choices\n        field_null = field.null\n        field_default = field.get_default()\n        field_choices = field.get_choices()\n        field_unique = field.unique\n        field_many = field.many_to_many\n        field_many_to_many = field.many_to_many\n        field_many_to_one = field.many_to_one\n        field_one_to_many = field.one_to_many\n        field_one_to_one = field.one_to_one\n        field_unique = field.unique\n        ", "solution": "\n\n        for key in self._iterate_required_keys(field):\n            self._create_hstore_required(\n                model._meta.db_table,\n                field,\n                key\n            )"}
{"prompt": "def get_organisation_information(self, query_params=None):\n        \"\"\"\n        Get information fot this organisation. Returns a dictionary of values.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        query_params['format'] = 'json'\n        return self.get_query_result(\n            'organisations/organisation_information',\n            query_params=query_params,\n            result_type='dict',\n            require_auth=True\n        )\n\n    def get_organisations_information(self, query_params=None):\n        \"\"\"\n        Get information fot all organisations. Returns a list of dictionaries.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n        query_params['format'] = 'json'\n        return self.get_query_result(\n            'organisations/organisations_information',\n            query_params=query_params,\n            result_type='list',\n            require_auth=True\n        )\n\n    def get_organisation", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri,\n            query_params=query_params or {}\n        )"}
{"prompt": "def verified(context, collaborator, test, outpath=None):\n    \"\"\"Export variants which have been verified for an institute\n        and write them to an excel file.\n\n    Args:\n        collaborator(str): institute id\n        test(bool): True if the function is called for testing purposes\n        outpath(str): path to output file\n\n    Returns:\n        written_files(int): number of written or simulated files\n    \"\"\"", "output": "\n    written_files = 0\n    verified_files = 0\n    verified_files_in_test = 0\n    verified_files_in_test_no_verified = 0\n    verified_files_in_test_no_verified_no_verified = 0\n    verified_files_in_test_no_verified_no_verified_no_verified = 0\n    verified_files_in_test_no_verified_no_verified_no_verified_no_verified = 0\n    verified_files_in_test_no_verified_no_verified_no_verified_no_verified_no_verified = 0\n    verified_files_in_test_no_verified_no_verified_no_verified_no_verified_no_verified_no_verifi", "solution": "\n    written_files = 0\n    collaborator = collaborator or 'cust000'\n    LOG.info('Exporting verified variants for cust {}'.format(collaborator))\n\n    adapter = context.obj['adapter']\n    verified_vars = adapter.verified(institute_id=collaborator)\n    LOG.info('FOUND {} verified variants for institute {}'.format(len(verified_vars), collaborator))\n\n\n    if not verified_vars:\n        LOG.warning('There are no verified variants for institute {} in database!'.format(collaborator))\n        return None\n\n    document_lines = export_verified_variants(verified_vars)\n\n    today = datetime.datetime.now().strftime('%Y-%m-%d')\n    document_name = '.'.join(['verified_variants', collaborator, today]) + '.xlsx'\n\n    # If this was a test and lines are created return success\n    if test and document_lines:\n        written_files +=1\n        LOG.info('Success. Verified variants file contains {} lines'.format(len(document_lines)))\n        return written_files\n\n    # create workbook and new sheet\n    # set up outfolder\n    if not outpath:\n        outpath = str(os.getcwd())\n    workbook = Workbook(os.path.join(outpath,document_name))\n    Report_Sheet = workbook.add_worksheet()\n\n    # Write the column header\n    row = 0\n    for col,field in enumerate(VERIFIED_VARIANTS_HEADER):\n        Report_Sheet.write(row,col,field)\n\n    # Write variant lines, after header (start at line 1)\n    for row, line in enumerate(document_lines,1): # each line becomes a row in the document\n        for col, field in enumerate(line): # each field in line becomes a cell\n            Report_Sheet.write(row,col,field)\n    workbook.close()\n\n    if os.path.exists(os.path.join(outpath,document_name)):\n        LOG.info('Success. Verified variants file of {} lines was written to disk'. format(len(document_lines)))\n        written_files += 1\n\n    return written_files"}
{"prompt": "def _plot_result(self, traj, result_name):\n        \"\"\"Plots a state variable graph for several neurons into one figure\"\"\"", "output": "\n        \n        # get the data\n        neurons = traj.get_neurons()\n        \n        # get the data\n        neurons_data = traj.get_data()\n        \n        # get the data\n        neurons_data_std = traj.get_data_std()\n        \n        # get the data\n        neurons_data_mean = traj.get_data_mean()\n        \n        # get the data\n        neurons_data_std = neurons_data_std.reshape((len(neurons_data_std), 1))\n        \n        # get the data\n        neurons_data_mean = neurons_data_mean.reshape((len(neurons_data_mean), 1))\n        \n        # get the data\n        neurons_data_std = neurons_data_st", "solution": "\n        result = traj.f_get(result_name)\n        varname = result.record_variables[0]\n        values = result[varname]\n        times = result.t\n\n        record = result.record\n\n        for idx, celia_neuron in enumerate(record):\n            plt.subplot(len(record), 1, idx+1)\n            plt.plot(times, values[idx,:])\n            if idx==0:\n                plt.title('%s' % varname)\n            if idx==1:\n                plt.ylabel('%s' % ( varname))\n            if idx == len(record)-1:\n                plt.xlabel('t')"}
{"prompt": "def unregister_sizer(self, attr_name):\n        \"\"\"\n        Unregister the SizedImage subclass currently assigned to `attr_name`.\n\n        If a SizedImage subclass isn't already registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"", "output": "\n        if attr_name in self._registered_sizers:\n            del self._registered_sizers[attr_name]\n\n    def register_sizer(self, attr_name, sizer):\n        \"\"\"\n        Register a SizedImage subclass to `attr_name`.\n\n        If a SizedImage subclass is already registered to `attr_name`\n        NotRegistered will raise.\n        \"\"\"\n        if attr_name in self._registered_sizers:\n            raise NotRegistered(\"SizedImage subclass already registered to '{}'\".format(attr_name))\n\n        self._registered_sizers[attr_name] = sizer\n\n    def register_sizers(self, attr_name, sizers):\n        \"\"\"\n        Register multiple SizedImage subclasse", "solution": "\n        if attr_name not in self._sizedimage_registry:\n            raise NotRegistered(\n                'No SizedImage subclass is registered to %s' % attr_name\n            )\n        else:\n            del self._sizedimage_registry[attr_name]"}
{"prompt": "def assert_no_title(self, title, **kwargs):\n        \"\"\"\n        Asserts that the page doesn't have the given title.\n\n        Args:\n            title (str | RegexObject): The string that the title should include.\n            **kwargs: Arbitrary keyword arguments for :class:`TitleQuery`.\n\n        Returns:\n            True\n\n        Raises:\n            ExpectationNotMet: If the assertion hasn't succeeded during the wait time.\n        \"\"\"", "output": "\n        title = self.normalize_title(title)\n        title_regexp = re.compile(title)\n        title_regexp_str = title_regexp.pattern\n        title_regexp_regex = title_regexp.search\n        title_regexp_search = title_regexp.search\n        title_regexp_findall = title_regexp.findall\n        title_regexp_find = title_regexp.find\n        title_regexp_finditer = title_regexp.finditer\n        title_regexp_find = title_regexp.search\n        title_regexp_finditer = title_regexp.finditer\n        title_regexp_find = title_regexp.search\n        title_regexp_finditer = title", "solution": "\n\n        query = TitleQuery(title, **kwargs)\n\n        @self.synchronize(wait=query.wait)\n        def assert_no_title():\n            if query.resolves_for(self):\n                raise ExpectationNotMet(query.negative_failure_message)\n\n            return True\n\n        return assert_no_title()"}
{"prompt": "def __fetch(self, url, payload):\n        \"\"\"Fetch requests from groupsio API\"\"\"", "output": "\n        request = requests.Request(\n            method='GET',\n            url=url,\n            params=payload,\n            headers=self.__headers,\n        )\n        response = requests.request(\n            request,\n            verify=self.__verify,\n            timeout=self.__timeout,\n        )\n        return response\n\n    def __fetch_data(self, response):\n        \"\"\"Fetch data from groupsio API\"\"\"\n        response_json = response.json()\n        response_data = response_json.get('data')\n        if response_data:\n            return response_data\n        else:\n            return None\n\n    def __fetch_groups(self, response):\n        \"\"\"Fetch groups from groupsio API\"\"\"\n        response_json = response.json()\n        response_groups = response_json.get('groups')\n        if ", "solution": "\n\n        r = requests.get(url, params=payload, auth=self.auth, verify=self.verify)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            raise e\n\n        return r"}
{"prompt": "def f_shrink(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not store explored parameters to disk set or you deleted all\n            of them from disk set `force=True`.\n\n        :raises: TypeError if the trajectory was stored before.\n\n        \"\"\"", "output": "\n        if self.trajectory is None:\n            raise TypeError(f\"The trajectory has not been stored before.\")\n\n        if force:\n            self.shrink_trajectory()\n        else:\n            self.shrink_trajectory(force=True)\n\n    def shrink_trajectory(self, force=False):\n        \"\"\" Shrinks the trajectory and removes all exploration ranges from the parameters.\n        Only possible if the trajectory has not been stored to disk before or was loaded as new.\n\n        :param force:\n\n            Usually you cannot shrink the trajectory if it has been stored to disk,\n            because there's no guarantee that it is actually shrunk if there\n            still exist explored parameters on disk. In case you are certain that\n            you did not ", "solution": "\n        if self._stored and not force:\n            raise TypeError('Your trajectory is already stored to disk or database, shrinking is '\n                            'not allowed.')\n\n        for param in self._explored_parameters.values():\n            param.f_unlock()\n            try:\n                param._shrink()\n            except Exception as exc:\n                self._logger.error('Could not shrink `%s` because of:`%s`' %\n                                   (param.v_full_name, repr(exc)))\n\n        # If we shrink, we do not have any explored parameters left and we can erase all\n        # run information, and the length of the trajectory is 1 again.\n        self._explored_parameters = {}\n        self._run_information = {}\n        self._single_run_ids = {}\n        self._add_run_info(0)\n        self._test_run_addition(1)"}
{"prompt": "def get_data_disk(self, service_name, deployment_name, role_name, lun):\n        \"\"\"\n        Retrieves the specified data disk from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disk.\n        \"\"\"", "output": "\n        disks = self.list_data_disks(service_name, deployment_name, role_name, lun)\n        return disks[0]\n\n    def list_data_disks(self, service_name, deployment_name, role_name, lun):\n        \"\"\"\n        Retrieves the specified data disks from a virtual machine.\n\n        service_name:\n            The name of the service.\n        deployment_name:\n            The name of the deployment.\n        role_name:\n            The name of the role.\n        lun:\n            The Logical Unit Number (LUN) for the disks.\n        \"\"\"\n        disks = []\n        disks_response = self.connection.request(self.api_version, 'GET',\n                                                   '/services/{0}/deployments/{1}/roles/{2}/disks'\n                                                   ", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('deployment_name', deployment_name)\n        _validate_not_none('role_name', role_name)\n        _validate_not_none('lun', lun)\n        return self._perform_get(\n            self._get_data_disk_path(\n                service_name, deployment_name, role_name, lun),\n            DataVirtualHardDisk)"}
{"prompt": "def for_type(typ, func):\n    \"\"\"\n    Add a pretty printer for a given type.\n    \"\"\"", "output": "\n    def printer(val):\n        return func(val)\n    printer.__name__ = typ.__name__\n    printer.__qualname__ = typ.__qualname__\n    return printer\n\n\ndef pretty_print(obj, indent=0):\n    \"\"\"\n    Pretty print a Python object.\n    \"\"\"\n    printer = PrettyPrinter(indent=indent)\n    return printer.pprint(obj)\n\n\ndef pretty_print_json(obj, indent=0):\n    \"\"\"\n    Pretty print a Python object as JSON.\n    \"\"\"\n    printer = PrettyPrinter(indent=indent)\n    return printer.pprint_json(obj, sort_keys=True)\n\n\ndef pretty_print_yaml(obj, indent=0):\n    \"\"\"\n    Pretty print a Python object ", "solution": "\n    oldfunc = _type_pprinters.get(typ, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _type_pprinters[typ] = func\n    return oldfunc"}
{"prompt": "def structured_mesh(shape = (2,2,2), dim = (1.,1.,1.)):\n    \"\"\"\n    Returns a structured mesh. \n    \n    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).\n    :type shape: tuple\n    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))\n    :type dim: tuple\n    \n    .. note::\n\n       This function does not use GMSH for mesh generation.\n\n\n    \n    >>> import argiope as ag\n    >>> mesh = ag.mesh.structured_mesh(shape =(10,10,10), dim=(1.,1.,1.)))\n    \"\"\"", "output": "\n    \n    mesh = ag.mesh.Mesh(shape)\n    mesh.dim = dim\n    return mesh\n    \n    \ndef triangular_mesh(shape = (2,2,2), dim = (1.,1.,1.)):\n    \"\"\"\n    Returns a triangular mesh. \n    \n    :arg shape: 2 or 3 integers (eg: shape = (10, 10, 10)).\n    :type shape: tuple\n    :arg dim: 2 or 3 floats (eg: dim = (4., 2., 1.))\n    :type dim: tuple\n    \n   .. note::\n\n       This function does not use GMSH for mesh generation.\n\n\n    \n    >>> import argiope as ag\n    >>> mesh = ag", "solution": "\n    # PREPROCESSING\n    shape = np.array(shape)\n    dim   = np.array(dim) \n    Ne = shape.prod()\n    Nn = (shape + 1).prod()\n    # LABELS\n    nindex = np.arange(Nn) + 1\n    eindex = np.arange(Ne) + 1\n    # COORDINATES\n    coords = [ np.linspace(0., dim[i], shape[i] + 1) for i in range(len(shape))]\n    coords = np.array(np.meshgrid(*coords))\n    coords = np.array([c.swapaxes(0,1).flatten(\"F\") for c in coords]).T\n    if len(shape) == 2:\n        c = coords\n        coords = np.zeros((Nn, 3))\n        coords[:, :2] = c  \n    # CONNECTIVITY    \n    conn = _make_conn(shape)\n    # MESH INSTANCE\n    mesh = Mesh(nlabels = nindex,\n                coords  = coords,\n                elabels = eindex,\n                conn = conn,)\n    if len(shape) == 2: mesh.elements[(\"type\", \"argiope\")] = \"quad4\"\n    if len(shape) == 3: mesh.elements[(\"type\", \"argiope\")] = \"hexa8\"    \n    return mesh"}
{"prompt": "def inshape(shape, axes):\n    \"\"\"\n    Checks to see if a list of axes are contained within an array shape.\n\n    Parameters\n    ----------\n    shape : tuple[int]\n        the shape of a BoltArray\n\n    axes : tuple[int]\n        the axes to check against shape\n    \"\"\"", "output": "\n    shape = tuple(shape)\n    axes = tuple(axes)\n    if axes == ():\n        return True\n    axes = tuple(axes)\n    if len(axes)!= len(shape):\n        return False\n    for ax, shp in zip(axes, shape):\n        if ax not in (0, len(shape) - 1):\n            return False\n        if shp!= shape[ax]:\n            return False\n    return True\n\n\ndef in1d(array, value):\n    \"\"\"\n    Checks to see if a value is contained within an array.\n\n    Parameters\n    ----------\n    array : array\n        an array to check against\n\n    value : object\n        the value to check against\n    \"\"\"\n    return value in array\n\n\ndef in2d(array, x, y):\n    \"\"\"\n    Check", "solution": "\n    valid = all([(axis < len(shape)) and (axis >= 0) for axis in axes])\n    if not valid:\n        raise ValueError(\"axes not valid for an ndarray of shape: %s\" % str(shape))"}
{"prompt": "def desc(value):\n    \"\"\" str: A normalized representation for a user-provided value. \"\"\"", "output": "\n    return str(value)\n\n\ndef to_str(value):\n    \"\"\" str: A normalized representation for a user-provided value. \"\"\"\n    return str(value)\n\n\ndef to_bool(value):\n    \"\"\" bool: A normalized representation for a user-provided value. \"\"\"\n    return bool(value)\n\n\ndef to_int(value):\n    \"\"\" int: A normalized representation for a user-provided value. \"\"\"\n    return int(value)\n\n\ndef to_float(value):\n    \"\"\" float: A normalized representation for a user-provided value. \"\"\"\n    return float(value)\n\n\ndef to_list(value):\n    \"\"\" list: A normalized representation for a user-provided value. \"\"\"\n    return list(value)\n\n\ndef to_dict(value):\n    \"\"\" dict: A normalized representation for a user-provided value. \"\"\"\n    return dict(value)\n\n\ndef to_set(", "solution": "\n\n    def normalize_strings(value):\n        if isinstance(value, list):\n            value = [normalize_strings(e) for e in value]\n\n        if isinstance(value, dict):\n            value = {normalize_strings(k): normalize_strings(v) for k, v in iter(value.items())}\n\n        if isregex(value):\n            value = value.pattern\n\n        if isbytes(value):\n            value = decode_bytes(value)\n\n        if PY2:\n            if isstring(value):\n                # In Python 2, strings (``unicode`` objects) represent as ``u'...'``, so ensure\n                # the string is encoded (as a ``str`` object) for cleaner representation.\n                value = encode_string(value)\n\n        return value\n\n    value = normalize_strings(value)\n\n    return repr(value)"}
{"prompt": "def recommend_k_items_slow(self, test, top_k=10, remove_seen=True):\n        \"\"\"Recommend top K items for all users which are in the test set.\n\n        Args:\n            test: test Spark dataframe\n            top_k: top n items to return\n            remove_seen: remove items test users have already seen in the past from the recommended set.\n        \"\"\"", "output": "\n        seen = set()\n        seen_add = seen.add\n        seen_rem = seen.remove\n        seen_update = seen.update\n        seen_clear = seen.clear\n        seen_iter = seen.iter\n        seen_reverse = seen.reverse\n\n        top_k_items = test.groupby('user_id').agg(\n            F.explode(\n                F.array(\n                    F.struct([\n                        F.col('user_id'),\n                        F.col('item_id'),\n                    ])\n                )\n            ).alias('items')\n        ).groupby('user_id').agg(\n            F.collect_list('items').alias('items')\n        ).groupby('user_id').", "solution": "\n\n        # TODO: remove seen\n        if remove_seen:\n            raise ValueError(\"Not implemented\")\n\n        self.get_user_affinity(test)\\\n            .write.mode(\"overwrite\")\\\n            .saveAsTable(self.f(\"{prefix}user_affinity\"))\n\n        # user_affinity * item_similarity\n        # filter top-k\n        query = self.f(\n            "}
{"prompt": "def basis_state(str_state, num):\n    \"\"\"\n    Return a basis state ndarray.\n\n    Args:\n        str_state (string): a string representing the state.\n        num (int): the number of qubits\n    Returns:\n        ndarray:  state(2**num) a quantum state with basis basis state.\n     Raises:\n        QiskitError: if the dimensions is wrong\n    \"\"\"", "output": "\n    basis_state = np.zeros(2**num, dtype=np.complex128)\n    basis_state[0] = 1\n    basis_state[num] = -1\n    basis_state[num+1] = 1\n    basis_state[num+2] = -1\n    basis_state[num+3] = -1\n    basis_state[num+4] = -1\n    basis_state[num+5] = -1\n    basis_state[num+6] = -1\n    basis_state[num+7] = -1\n    basis_state[num+8] = -1\n    basis_state[num+9] = -1\n    basis_state[", "solution": "\n    n = int(str_state, 2)\n    if num >= len(str_state):\n        state = np.zeros(1 << num, dtype=complex)\n        state[n] = 1\n        return state\n    else:\n        raise QiskitError('size of bitstring is greater than num.')"}
{"prompt": "def _sparse_tensor_dense_matmul(sp_a, b, **kwargs):\n  \"\"\"Returns (batched) matmul of a SparseTensor with a Tensor.\n\n  Args:\n    sp_a: `SparseTensor` representing a (batch of) matrices.\n    b: `Tensor` representing a (batch of) matrices, with the same batch shape of\n      `sp_a`. The shape must be compatible with the shape of `sp_a` and kwargs.\n    **kwargs: Keyword arguments to `tf.sparse_tensor_dense_matmul`.\n\n  Returns:\n    product: A dense (batch of) matrix-shaped Tensor of the same batch shape and\n    dtype as `sp_a` and `b`. If `sp_a` or `b` is adjointed through `kwargs` then\n    the shape is adjusted accordingly.\n  \"\"\"", "output": "sparse ops,\n    then `product` will also be sparse (if `sp_a` or `b` is sparse).\n  \"\"\"\n  sp_a = ops.convert_to_tensor(sp_a, name=\"sp_a\")\n  b = ops.convert_to_tensor(b, name=\"b\")\n  sp_a_shape = sp_a.get_shape()\n  b_shape = b.get_shape()\n  sp_a_rank = sp_a_shape.get_rank()\n  b_rank = b_shape.get_rank()\n  sp_a_dtype = sp_a.dtype\n  b_dtype = b.dtype\n  assert sp_a_rank is not None\n  assert b_rank is not None\n  assert sp_a_rank == b_rank\n  assert sp_a_dtype is not None\n  assert ", "solution": "\n  batch_shape = _get_shape(sp_a)[:-2]\n\n  # Reshape the SparseTensor into a rank 3 SparseTensors, with the\n  # batch shape flattened to a single dimension. If the batch rank is 0, then\n  # we add a batch dimension of rank 1.\n  sp_a = tf.sparse.reshape(sp_a, tf.concat([[-1], _get_shape(sp_a)[-2:]],\n                                           axis=0))\n  # Reshape b to stack the batch dimension along the rows.\n  b = tf.reshape(b, tf.concat([[-1], _get_shape(b)[-1:]], axis=0))\n\n  # Convert the SparseTensor to a matrix in block diagonal form with blocks of\n  # matrices [M, N]. This allow us to use tf.sparse_tensor_dense_matmul which\n  # only accepts rank 2 (Sparse)Tensors.\n  out = tf.sparse.sparse_dense_matmul(_sparse_block_diag(sp_a), b, **kwargs)\n\n  # Finally retrieve the original batch shape from the resulting rank 2 Tensor.\n  # Note that we avoid inferring the final shape from `sp_a` or `b` because we\n  # might have transposed one or both of them.\n  return tf.reshape(\n      out,\n      tf.concat([batch_shape, [-1], _get_shape(out)[-1:]], axis=0))"}
{"prompt": "def require_parents(packages):\n        \"\"\"\n        Exclude any apparent package that apparently doesn't include its\n        parent.\n\n        For example, exclude 'foo.bar' if 'foo' is not present.\n        \"\"\"", "output": "\n        packages = set(packages)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        packages.discard(None)\n        package", "solution": "\n        found = []\n        for pkg in packages:\n            base, sep, child = pkg.rpartition('.')\n            if base and base not in found:\n                continue\n            found.append(pkg)\n            yield pkg"}
{"prompt": "def make_report(self,traceback):\n        \"\"\"Return a string containing a crash report.\"\"\"", "output": "\n        crash_lines = []\n        crash_lines.append(\n            'Crash Report:\\n')\n        crash_lines.append('Date: %s\\n' % self.date)\n        crash_lines.append('Time: %s\\n' % self.time)\n        crash_lines.append('Machine: %s\\n' % self.machine)\n        crash_lines.append('Process ID: %s\\n' % self.pid)\n        crash_lines.append('Process name: %s\\n' % self.name)\n        crash_lines.append('Process cmdline: %s\\n' % self.cmdline)\n        crash_lines.append('Process arguments: %s\\n' % str(self.args))\n        crash_lines.append('')\n        crash_lines.append('Traceback:\\n')\n        crash_lines.append('", "solution": "\n\n        sec_sep = self.section_sep\n\n        report = ['*'*75+'\\n\\n'+'IPython post-mortem report\\n\\n']\n        rpt_add = report.append\n        rpt_add(sys_info())\n\n        try:\n            config = pformat(self.app.config)\n            rpt_add(sec_sep)\n            rpt_add('Application name: %s\\n\\n' % self.app_name)\n            rpt_add('Current user configuration structure:\\n\\n')\n            rpt_add(config)\n        except:\n            pass\n        rpt_add(sec_sep+'Crash traceback:\\n\\n' + traceback)\n\n        return ''.join(report)"}
{"prompt": "def _fmt(self, string):\n        \"\"\"Helper method to format our name into a string.\"\"\"", "output": "\n        return string\n\n    def _format_args(self, args):\n        \"\"\"Helper method to format our arguments into a string.\"\"\"\n        return ', '.join(\n            _fmt(arg) for arg in args\n        )\n\n    def _format_kwargs(self, kwargs):\n        \"\"\"Helper method to format our kwargs into a string.\"\"\"\n        return ', '.join(\n            _fmt(key) + '=' + _fmt(value)\n            for key, value in kwargs.items()\n        )\n\n    def _format_signature(self, sig):\n        \"\"\"Helper method to format our signature into a string.\"\"\"\n        return ', '.join(\n            _fmt(key) + '=' + _fmt(value)\n            for key, value in sig.items()\n        )\n\n    def _format_signature_with_defaults(self, sig):\n        \"\"\"Helper method to format our ", "solution": "\n        if '{' not in string:\n            string = '{}.' + string\n        return string.format(self.name)"}
{"prompt": "def build_fake_input_fns(batch_size):\n  \"\"\"Builds fake MNIST-style data for unit testing.\"\"\"", "output": "\n  del batch_size\n  images = tf.constant(\n      np.random.normal(size=(batch_size, 784)), dtype=tf.float32, name='images')\n  labels = tf.constant(\n      np.random.normal(size=(batch_size, 10), dtype=tf.int64), dtype=tf.int64,\n      name='labels')\n  images = tf.reshape(images, [-1, 28, 28, 1])\n  labels = tf.reshape(labels, [-1, 10])\n  return images, labels\n\n\ndef mnist_input_fn_builder(batch_size, num_epochs=None, shuffle=True):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  def _input_fn():\n    \"\"\"Returns features and labels of size [batch_size, 784].\"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices((\n        images.", "solution": "\n  random_sample = np.random.rand(batch_size, *IMAGE_SHAPE).astype(\"float32\")\n\n  def train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size).repeat()\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  def eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(\n        random_sample).map(lambda row: (row, 0)).batch(batch_size)\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return train_input_fn, eval_input_fn"}
{"prompt": "def marker(self, *args):\n        \"\"\"\n        Defines markers one at a time for your graph\n        args are of the form::\n            <marker type>,\n            <color>,\n            <data set index>,\n            <data point>,\n            <size>,\n            <priority>\n        see the official developers doc for the complete spec\n        APIPARAM: chm\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        if len(args[0]) == 1:\n            assert args[0] in MARKERS, 'Invalid marker type: %s'%args[0]\n        assert len(args) <= 6, 'Incorrect arguments %s'%str(args)\n        args = color_args(args, 1)\n        self.markers.append(','.join(map(str,args)) )\n        return self"}
{"prompt": "def camera_event_motion_enum(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"", "output": "\n        motion_settings = self.motion_settings[camera_id]\n        return motion_settings.enum\n\n    def camera_event_motion_settings(self, camera_id, **kwargs):\n        \"\"\"Return motion settings matching camera_id.\"\"\"\n        motion_settings = self.motion_settings[camera_id]\n        return motion_settings.settings\n\n    def camera_event_motion_setting(self, camera_id, **kwargs):\n        \"\"\"Return motion setting matching camera_id.\"\"\"\n        motion_settings = self.motion_settings[camera_id]\n        return motion_settings.setting\n\n    def camera_event_motion_setting_value(self, camera_id, **kwargs):\n        \"\"\"Return motion setting matching camera_id.\"\"\"\n        motion_settings = self.motion_settings[camera_id]\n        return motion_settings.setting_value\n\n    def camera_event_motion_setting_", "solution": "\n        api = self._api_info['camera_event']\n        payload = dict({\n            '_sid': self._sid,\n            'api': api['name'],\n            'method': 'MotionEnum',\n            'version': api['version'],\n            'camId': camera_id,\n        }, **kwargs)\n        response = self._get_json_with_retry(api['url'], payload)\n\n        return MotionSetting(camera_id, response['data']['MDParam'])"}
{"prompt": "def register(self, managed_object):\n        \"\"\"\n        Register a managed object with a KMIP appliance.\n\n        Args:\n            managed_object (ManagedObject): A managed object to register. An\n                instantiatable subclass of ManagedObject from the Pie API.\n\n        Returns:\n            string: The uid of the newly registered managed object.\n\n        Raises:\n            ClientConnectionNotOpen: if the client connection is unusable\n            KmipOperationFailure: if the operation result is a failure\n            TypeError: if the input argument is invalid\n        \"\"\"", "output": "\n        if not isinstance(managed_object, ManagedObject):\n            raise TypeError('managed_object must be a subclass of ManagedObject')\n\n        managed_object_uid = managed_object.uid\n        managed_object_name = managed_object.name\n        managed_object_type = managed_object.type\n\n        # Register the managed object with the appliance\n        managed_object_uid = self.client.register_managed_object(managed_object_uid,\n                                                                 managed_object_name,\n                                                                 managed_object_type)\n\n        # Return the uid of the managed object\n        return managed_object_uid\n\n    def register_managed_object_with_uid(self, managed_object_uid):\n        \"\"\"\n        Register a managed object with a ", "solution": "\n        # Check input\n        if not isinstance(managed_object, pobjects.ManagedObject):\n            raise TypeError(\"managed object must be a Pie ManagedObject\")\n\n        # Extract and create attributes\n        object_attributes = list()\n\n        if hasattr(managed_object, 'cryptographic_usage_masks'):\n            if managed_object.cryptographic_usage_masks is not None:\n                mask_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.CRYPTOGRAPHIC_USAGE_MASK,\n                    managed_object.cryptographic_usage_masks\n                )\n                object_attributes.append(mask_attribute)\n        if hasattr(managed_object, 'operation_policy_name'):\n            if managed_object.operation_policy_name is not None:\n                opn_attribute = self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    managed_object.operation_policy_name\n                )\n                object_attributes.append(opn_attribute)\n        if hasattr(managed_object, 'names'):\n            if managed_object.names:\n                for name in managed_object.names:\n                    name_attribute = self.attribute_factory.create_attribute(\n                        enums.AttributeType.NAME,\n                        name\n                    )\n                    object_attributes.append(name_attribute)\n\n        template = cobjects.TemplateAttribute(attributes=object_attributes)\n        object_type = managed_object.object_type\n\n        # Register the managed object and handle the results\n        secret = self.object_factory.convert(managed_object)\n        result = self.proxy.register(object_type, template, secret)\n\n        status = result.result_status.value\n        if status == enums.ResultStatus.SUCCESS:\n            return result.uuid\n        else:\n            reason = result.result_reason.value\n            message = result.result_message.value\n            raise exceptions.KmipOperationFailure(status, reason, message)"}
{"prompt": "def visit_return(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"", "output": "\n        right = self.visit(node.value)\n        if node.orelse and node.orelse[-1].orelse:\n            right += '\\n' + self.visit(node.orelse[-1])\n        return right\n\n    def visit_tryexcept(self, node):\n        \"\"\"\n        1 - check is the node has a try clause (if it does, that's some\n        unreachable code)\n        2 - check is the node has a finally clause (if it does, that's some\n        unreachable code)\n        3 - check is the node has a finally clause (if it does, that's some\n        unreachable code)\n        4 - check is the node has a finally clause (if it does, that's some\n        unreachable code)\n        5", "solution": "\n        self._check_unreachable(node)\n        # Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, \"return\", (astroid.FunctionDef,))"}
{"prompt": "def read(self, size=-1):\n        \"\"\"\n        read([size]) -> read at most size bytes, returned as a string.\n\n        If the size argument is negative or omitted, read until EOF is reached.\n        Notice that when in non-blocking mode, less data than what was\n        requested may be returned, even if no size parameter was given.\n        \"\"\"", "output": "\n        if size < 0:\n            size = -1\n        elif size == 0:\n            return ''\n        else:\n            size = min(size, self.length)\n        chunk = self.read(size)\n        if not chunk:\n            self.eof = 1\n        return chunk\n\n    def readline(self, size=-1):\n        \"\"\"\n        readline([size]) -> next line from the file, as a string.\n\n        Read and decode the next line from the file, unless EOF is reached.\n        If size is non-negative it sets the size of the internal buffer\n        to size before reading.  size defaults to -1, meaning no limit.\n        \"\"\"\n        if size < 0:\n            size = -1\n        elif size == 0:\n            return ''\n        else:\n            size = min(size, self.length)\n        ", "solution": "\n        if self.left is not None:\n            size = min(size, self.left)\n        if self.closed:\n            raise ValueError('I/O operation on closed file')\n        if size < 0:\n            return ''.join(self)\n        elif not size:\n            chunk = ''\n        elif self.buf:\n            chunk = self.buf\n            self.buf = None\n        else:\n            try:\n                chunk = next(self.iterator)\n            except StopIteration:\n                return ''\n        if len(chunk) > size:\n            self.buf = chunk[size:]\n            chunk = chunk[:size]\n        if self.left is not None:\n            self.left -= len(chunk)\n        return chunk"}
{"prompt": "def balanced_rows(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but balance the rows to minimize fill per row.\n\tbalanced_rows(3, 'ABCDEFG', 'x') --> ABC DEx FGx\"\n\t\"\"\"", "output": "\n\tgrouper = groupby(iterable, fillvalue=fillvalue)\n\tbalanced = groupby(n, grouper)\n\tbalanced = list(balanced)\n\tbalanced.sort(key=itemgetter(0))\n\treturn balanced\n\n\ndef grouper(n, iterable, fillvalue=None):\n\t\"\"\"\n\tLike grouper, but group the items into fixed-size chunks.\n\tgrouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n\t\"\"\"\n\targs = [iter(iterable)] * n\n\treturn zip_longest(*args, fillvalue=fillvalue)\n\n\ndef zip_longest(*args, fillvalue=None):\n\t\"\"\"\n\tLike itertools.izip_longest, but fills missing values with fillvalue.\n", "solution": "\n\titerable, iterable_copy = itertools.tee(iterable)\n\tcount = len(tuple(iterable_copy))\n\tfor allocation in partition_items(count, n):\n\t\trow = itertools.islice(iterable, allocation)\n\t\tif allocation < n:\n\t\t\trow = itertools.chain(row, [fillvalue])\n\t\tyield tuple(row)"}
{"prompt": "def timetree_likelihood(self):\n        \"\"\"\n        Return the likelihood of the data given the current branch length in the tree\n        \"\"\"", "output": "\n        likelihood = 0\n        for node in self.tree.traverse(lambda node: node.is_leaf):\n            likelihood += node.branch_length\n        return likelihood\n\n    def timetree_update(self, new_branch_length):\n        \"\"\"\n        Update the branch length in the tree given the new branch length\n        \"\"\"\n        self.tree.branch_length = new_branch_length\n\n    def timetree_update_all(self):\n        \"\"\"\n        Update the branch length in the tree for all branches\n        \"\"\"\n        for node in self.tree.traverse(lambda node: node.is_leaf):\n            node.branch_length = self.tree.branch_length\n\n    def timetree_update_leaf(self, new_branch_length):\n        \"\"\"\n        Update the branch length in the tree given a leaf node\n        \"\"\"\n        self.", "solution": "\n        LH = 0\n        for node in self.tree.find_clades(order='preorder'):  # sum the likelihood contributions of all branches\n            if node.up is None: # root node\n                continue\n            LH -= node.branch_length_interpolator(node.branch_length)\n\n        # add the root sequence LH and return\n        if self.aln:\n            LH += self.gtr.sequence_logLH(self.tree.root.cseq, pattern_multiplicity=self.multiplicity)\n        return LH"}
{"prompt": "def read(*paths):\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"", "output": "\n    with open(Path(*paths), 'r') as f:\n        return f.read()\n\n\ndef write_file(*paths):\n    \"\"\"Build a file path from *paths* and write the contents.\"\"\"\n    with open(Path(*paths), 'w') as f:\n        return f.read()\n\n\ndef write_text(text, *paths):\n    \"\"\"Build a file path from *paths* and write the contents.\"\"\"\n    with open(Path(*paths), 'w') as f:\n        f.write(text)\n\n\ndef write_text_file(text, *paths):\n    \"\"\"Build a file path from *paths* and write the contents.\"\"\"\n    write_file(text, *paths)\n\n\ndef write_text_file_lines(lines, *paths):\n    \"\"\"Build a file path from *paths* and write the contents.\"\"\"\n    write_file_lines(", "solution": "\n    filename = os.path.join(*paths)\n    with codecs.open(filename, mode='r', encoding='utf-8') as handle:\n        return handle.read()"}
{"prompt": "def bootstrap(self, path_or_uri):\n        \"\"\" Initialize a database.\n\n        :param database_path: The absolute path to the database to initialize.\n        \"\"\"", "output": "\n        database_path = path_or_uri\n        database = Database(database_path)\n        database.initialize()\n\n    def commit(self):\n        \"\"\" Commit all changes to the database.\n\n        All changes are written to the database file on disk.\n        \"\"\"\n        self.database.commit()\n\n    def rollback(self):\n        \"\"\" Rollback all changes to the database.\n\n        All changes are written to the database file on disk.\n        \"\"\"\n        self.database.rollback()\n\n    def close(self, force=False):\n        \"\"\" Close the database connection.\n\n        :param force: Force closing the database connection if True.\n        \"\"\"\n        self.database.close(force)\n\n    def execute(self, statement, parameters=None):\n        \"\"\" Execute a SQL statement and return the ", "solution": "\n        _logger.debug(\"Bootstrapping new database: %s\", path_or_uri)\n        self.database_uri = _urify_db(path_or_uri)\n        db = sa.create_engine(self.database_uri)\n        Base.metadata.create_all(db)"}
{"prompt": "def apply_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"", "output": "\n    storage['mirroring'] = debug\n\ndef apply_mirroring_and_mirroring_and_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring, mirroring, mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"\n    storage['mirroring'] = debug\n    storage['mirroring_and_mirroring'] = debug\n\ndef apply_mirroring_and_mirroring_and_mirroring_and_mirroring(storage, debug):\n    \"\"\"Applies L4: mirroring, mirroring, mirroring, mirroring\n\n    See: http://unicode.org/reports/tr9/#L4\n\n    \"\"\"\n    storage['mirroring'] = debug\n    storage['mirroring_", "solution": "\n    # L4. A character is depicted by a mirrored glyph if and only if (a) the\n    # resolved directionality of that character is R, and (b) the\n    # Bidi_Mirrored property value of that character is true.\n    for _ch in storage['chars']:\n        unichar = _ch['ch']\n        if mirrored(unichar) and \\\n                _embedding_direction(_ch['level']) == 'R':\n            _ch['ch'] = MIRRORED.get(unichar, unichar)\n\n    if debug:\n        debug_storage(storage)"}
{"prompt": "def overrides_a_method(class_node: astroid.node_classes.NodeNG, name: str) -> bool:\n    \"\"\"return True if <name> is a method overridden from an ancestor\"\"\"", "output": "\n    node = next(class_node.ancestors(lambda n: n.name == name))\n    return isinstance(node, astroid.node_classes.ClassDef) and node.is_method\n\n\ndef has_no_arguments(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return True if <node> has no arguments.\"\"\"\n    return len(node.args) == 0\n\n\ndef has_no_returns(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return True if <node> has no returns.\"\"\"\n    return len(node.returns) == 0\n\n\ndef has_no_yields(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"return True if <node> has no yields.\"\"\"\n    return len(node.yields) == 0\n\n\ndef has_no_yield", "solution": "\n    for ancestor in class_node.ancestors():\n        if name in ancestor and isinstance(ancestor[name], astroid.FunctionDef):\n            return True\n    return False"}
{"prompt": "def _add_empty_child(self, parent, code, final=False):\n        \"\"\"\n        \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u043a \u0432\u0435\u0440\u0448\u0438\u043d\u0435 parent \u043f\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u0443 \u0441 \u043a\u043e\u0434\u043e\u043c code\n        \"\"\"", "output": "\n        code = code.strip()\n        code = code.replace('\\t', '')\n        code = code.replace('\\n','')\n        code = code.replace('\\r', '')\n        code = code.replace(',', '')\n        code = code.replace(';', '')\n        code = code.replace('\"', '')\n        code = code.replace('\\'', '')\n        code = code.replace('(', '')\n        code = code.replace(')', '')\n        code = code.replace('[', '')\n        code = code.replace(']', '')\n        code = code.replace('{', '')\n        code = code.replace('}', '')\n        code = code.replace(',', '')\n        code = code.replace(';', '')\n        code = code.replace('\"', '')\n        code = code.replace('\\'', '')\n        code = code", "solution": "\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)"}
{"prompt": "def _check_year(year, month, error, error_msg):\n    \"\"\"Checks that the year is within 50 years from now.\"\"\"", "output": "\n    if year < 50:\n        raise ValueError(error_msg)\n    return year\n\n\ndef _check_month(month, error, error_msg):\n    \"\"\"Checks that the month is within 12 months from now.\"\"\"\n    if month < 12:\n        raise ValueError(error_msg)\n    return month\n\n\ndef _check_day(day, error, error_msg):\n    \"\"\"Checks that the day is within 31 days from now.\"\"\"\n    if day < 31:\n        raise ValueError(error_msg)\n    return day\n\n\ndef _check_hour(hour, error, error_msg):\n    \"\"\"Checks that the hour is within 24 hours from now.\"\"\"\n    if hour < 24:\n        raise ValueError(error_msg)\n    return hour\n\n\ndef _check_minute(minute, error, error_msg):\n    \"\"\"Checks that the minute is ", "solution": "\n    if year not in xrange((now.year - 50), (now.year + 51)):\n        year = now.year\n        month = now.month\n        error = error_msg\n    return year, month, error"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return QuantumChannel(self.num_qubits, self.num_qubits, self)\n\n    def depth(self):\n        \"\"\"Return the depth of the QuantumChannel.\"\"\"\n        return QuantumChannel.depth(self.num_qubits, self.num_qubits, self)\n\n    def depth_from_label(self, label):\n        \"\"\"Return the depth of the QuantumChannel.\"\"\"\n        return QuantumChannel.depth_from_label(self.num_qubits, self.num_qubits, self, label)\n\n    def depth_from_labels(self, labels):\n        \"\"\"Return the depth of the QuantumChannel.\"\"\"\n        return QuantumChannel.depth_from_labels(self.num_qubits, self.num_qubits, self, labels)\n\n    def depth_from_labels_and_depths(self, labels, depths):\n        \"\"\"Return the depth of the QuantumChannel.\"\"\"\n        return ", "solution": "\n        din, dout = self.dim\n        dtr = self._data[0].shape[0] // dout\n        stine = [None, None]\n        for i, mat in enumerate(self._data):\n            if mat is not None:\n                stine[i] = np.reshape(\n                    np.transpose(np.reshape(mat, (dout, dtr, din)), (2, 1, 0)),\n                    (din * dtr, dout))\n        return Stinespring(\n            tuple(stine),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def newick(self):\n        \"\"\"The representation of the Node in Newick format.\"\"\"", "output": "\n        node = Node()\n        node.name = self.name\n        node.label = self.label\n        node.children = self.children\n        return node\n\n    def copy(self):\n        \"\"\"Copy this node and its children.\"\"\"\n        node = Node()\n        node.name = self.name\n        node.label = self.label\n        node.children = self.children[:]\n        return node\n\n    def copy_name(self):\n        \"\"\"Copy this node's name.\"\"\"\n        node = Node()\n        node.name = self.name\n        return node\n\n    def copy_id(self):\n        \"\"\"Copy this node's id.\"\"\"\n        node = Node()\n        node.id = self.id\n        return node\n\n    def copy_type(self):\n        \"\"\"Copy this node's type.\"\"\"\n        node = Node()", "solution": "\n        label = self.name or ''\n        if self._length:\n            label += ':' + self._length\n        descendants = ','.join([n.newick for n in self.descendants])\n        if descendants:\n            descendants = '(' + descendants + ')'\n        return descendants + label"}
{"prompt": "async def get_postcode_random() -> Postcode:\n    \"\"\"\n    Gets a random postcode object..\n    Acts as a middleware between us and the API, caching results.\n    :return: The PostCode object else None if the postcode does not exist.\n    \"\"\"", "output": "\n    cache_key = f'postcode_{randint(0, 10000000)}'\n    postcode_cache = cache.get(cache_key)\n    if postcode_cache is None:\n        postcode_cache = PostCode.get_postcode_from_api()\n        cache.set(cache_key, postcode_cache, timeout=CACHE_TIMEOUT)\n    return postcode_cache\n\n\n@middleware\nasync def cache_postcode_random(request: Request, response: Response) -> Response:\n    \"\"\"\n    Caches the postcode object returned by the API.\n    Acts as a middleware between us and the API, caching results.\n    :return: The postcode object else None if the postcode does not exist.\n    \"\"\"\n    cache_key = f'postcode_{randint(0, 10000000)", "solution": "\n    try:\n        postcode = await fetch_postcode_random()\n    except (ApiError, CircuitBreakerError):\n        raise CachingError(f\"Requested postcode is not cached, and can't be retrieved.\")\n\n    if postcode is not None:\n        postcode.save()\n    return postcode"}
{"prompt": "def tasks(self, from_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks.\n\n        :param from_date: retrieve tasks that where updated from that date;\n            dates are converted epoch time.\n        \"\"\"", "output": "\n        from_date = utils.parse_date(from_date)\n        query = TaskQuery(self.model, self._meta.db_table)\n        query = query.filter(\n            TaskStatus._meta.date_updated >= from_date,\n        )\n        tasks = query.order_by(TaskStatus._meta.date_updated.desc()).all()\n        return tasks\n\n    def tasks_completed(self, from_date=DEFAULT_DATETIME):\n        \"\"\"Retrieve tasks completed.\n\n        :param from_date: retrieve tasks that where updated from that date;\n            dates are converted epoch time.\n        \"\"\"\n        from_date = utils.parse_date(from_date)\n        query = TaskQuery(self.model, self._meta.db_table)\n        query = query.filter(\n            TaskStatus._meta.", "solution": "\n        # Convert 'from_date' to epoch timestamp.\n        # Zero value (1970-01-01 00:00:00) is not allowed for\n        # 'modifiedStart' so it will be set to 1, by default.\n        ts = int(datetime_to_utc(from_date).timestamp()) or 1\n\n        consts = {\n            self.PMODIFIED_START: ts\n        }\n\n        attachments = {\n            self. PPROJECTS: True\n        }\n\n        params = {\n            self.PCONSTRAINTS: consts,\n            self.PATTACHMENTS: attachments,\n            self.PORDER: self.VOUTDATED,\n        }\n\n        while True:\n            r = self._call(self.MANIPHEST_TASKS, params)\n            yield r\n            j = json.loads(r)\n            after = j['result']['cursor']['after']\n            if not after:\n                break\n            params[self.PAFTER] = after"}
{"prompt": "def ensure_dir(path):\n    \"\"\"Ensure that a needed directory exists, creating it if it doesn't\"\"\"", "output": "\n    dirname = os.path.dirname(path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n\ndef write_yaml(filename, data):\n    \"\"\"Write a YAML file to the given path\"\"\"\n    with open(filename, 'w') as f:\n        yaml.dump(data, f)\n\n\ndef write_json(filename, data):\n    \"\"\"Write a JSON file to the given path\"\"\"\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n\ndef write_csv(filename, data):\n    \"\"\"Write a CSV file to the given path\"\"\"\n    with open(filename, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n\ndef write_json(filename, data):\n    \"\"\"Write a JSON file to the given path\"\"\"\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\n\n", "solution": "\n    try:\n        log.info('Ensuring directory exists: %s' % path)\n        os.makedirs(path)\n    except OSError:\n        if not os.path.isdir(path):\n            raise"}
{"prompt": "def parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"", "output": "what this means\n    \n    Arguments:\n        lines: List of lines from the file\n    \n    Returns:\n        List of dictionaries describing the different mim numbers\n    \"\"\"\n    \n    genes = []\n    phenotypes = []\n    moved = []\n    predominantly_phenotypes = []\n    \n    for line in lines:\n        \n        # Skip comments\n        if line[0] == '#':\n            continue\n        \n        # Skip blank lines\n        if line[0] == '\\n':\n            continue\n        \n        # Skip empty lines\n        if line[0] == '':\n            continue\n        \n        # Split on whitespace\n        line = line.split()\n        \n        # Skip empty lines\n        if len(line) == 0:", "solution": "\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry"}
{"prompt": "def directive(directname=None):\n    \"\"\"Attach a class to a parsing class and register it as a parser directive.\n\n        The class is registered with its name unless directname is provided.\n    \"\"\"", "output": "\n    if directname:\n        directive = directives[directname]\n    else:\n        directive = directives[\"DEFAULT\"]\n    directive.name = directive.name.replace(\"-\", \"_\")\n    directive.factory = directive.factory.replace(\"-\", \"_\")\n    directive.factory = directive.factory.replace(\"_\", \"-\")\n    directive.factory = directive.factory.lower()\n    directive.factory = directive.factory.replace(\"-\", \"_\")\n    directive.factory = directive.factory.replace(\"_\", \"-\")\n    directive.factory = directive.factory.replace(\"-\", \"_\")\n    directive.factory = directive.factory.replace(\"-\", \"_\")\n    directive.factory = directive.factory.replace(\"-\", \"_\")\n    ", "solution": "\n    global _directives\n    class_dir_list = _directives\n\n    def wrapper(f):\n        nonlocal directname\n        if directname is None:\n            directname = f.__name__\n        f.ns_name = directname\n        set_one(class_dir_list, directname, f)\n        return f\n    return wrapper"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"", "output": "\n        return self.redis_connection\n\n    def get_redis_connection(self):\n        \"\"\"\n        Returns a Redis connection.\n        \"\"\"\n        return self.redis_connection\n\n    def get_redis_client(self):\n        \"\"\"\n        Returns a Redis client.\n        \"\"\"\n        return self.redis_client\n\n    def get_redis_pool(self):\n        \"\"\"\n        Returns a Redis pool.\n        \"\"\"\n        return self.redis_pool\n\n    def get_redis_pool(self):\n        \"\"\"\n        Returns a Redis pool.\n        \"\"\"\n        return self.redis_pool\n\n    def get_redis_pool(self):\n        \"\"\"\n        Returns a Redis pool.\n        \"\"\"\n        return self.redis_pool\n\n    def get_redis_pool(self):\n        \"\"\"\n        Returns a Redis pool.\n        \"\"\"\n        return self.", "solution": "\n        conn = self.get_connection(self.redis_conn_id)\n        self.host = conn.host\n        self.port = conn.port\n        self.password = None if str(conn.password).lower() in ['none', 'false', ''] else conn.password\n        self.db = conn.extra_dejson.get('db', None)\n\n        if not self.redis:\n            self.log.debug(\n                'Initializing redis object for conn_id \"%s\" on %s:%s:%s',\n                self.redis_conn_id, self.host, self.port, self.db\n            )\n            self.redis = Redis(\n                host=self.host,\n                port=self.port,\n                password=self.password,\n                db=self.db)\n\n        return self.redis"}
{"prompt": "def decode(self, packet):\n        \"\"\"\n        Decode a PUBREL control packet. \n        \"\"\"", "output": "\n        \n        packet = packet[8:24]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]\n        \n        packet = struct.unpack('>I', packet)[0]", "solution": "\n        self.encoded = packet\n        lenLen = 1\n        while packet[lenLen] & 0x80:\n            lenLen += 1\n        packet_remaining = packet[lenLen+1:]\n        self.msgId  = decode16Int(packet_remaining)\n        self.dup = (packet[0] & 0x08) == 0x08"}
{"prompt": "def init_log( quiet, debug ):\n   \"\"\"\n   setup system logging to desired verbosity.\n   \"\"\"", "output": "\n   if quiet:\n      loglevel = logging.ERROR\n   else:\n      loglevel = logging.DEBUG\n   logging.basicConfig(level=loglevel)\n   logging.captureWarnings(True)\n   logging.addLevelName(logging.DEBUG, \"DEBUG\")\n   logging.addLevelName(logging.INFO, \"INFO\")\n   logging.addLevelName(logging.WARNING, \"WARNING\")\n   logging.addLevelName(logging.ERROR, \"ERROR\")\n   logging.addLevelName(logging.CRITICAL, \"CRITICAL\")\n   logging.addLevelName(logging.NOTSET, \"NOTSET\")\n\ndef init_logging( quiet, debug ):\n   \"\"\"\n   setup system logging to desired verbosity.\n   \"\"\"\n", "solution": "\n   from logging.handlers import SysLogHandler\n   fmt = logging.Formatter( os.path.basename(sys.argv[0]) +\n         \".%(name)s %(levelname)s - %(message)s\")\n   facility = SysLogHandler.LOG_DAEMON\n   syslog = SysLogHandler(address='/dev/log',facility=facility)\n   syslog.setFormatter( fmt )\n   log.addHandler(syslog)\n   if not quiet:\n      console = logging.StreamHandler()\n      console.setFormatter( fmt )\n      log.addHandler(console)\n      log.setLevel(logging.INFO)\n      if debug:\n         log.setLevel(logging.DEBUG)"}
{"prompt": "def loadPlugins(self):\n        \"\"\"Load plugins in nose.plugins.builtin\n        \"\"\"", "output": "\n        plugins = self.getPlugins()\n        for plugin in plugins:\n            plugin.load()\n\n    def makeResult(self):\n        \"\"\"Make result object from test run.\n        \"\"\"\n        result = TestResult()\n        result.tests = self.tests\n        result.failures = self.failures\n        result.errors = self.errors\n        result.skips = self.skips\n        result.wasSuccessful = self.wasSuccessful\n        return result\n\n    def makeErrorResult(self):\n        \"\"\"Make error result object from test run.\n        \"\"\"\n        return ErrorResult(self.makeResult())\n\n    def makeFailureResult(self):\n        \"\"\"Make failure result object from test run.\n        \"\"\"\n        return FailureResult(self.makeResult())\n\n    def makeSkipResult(self):\n        \"\"\"Make ", "solution": "\n        from nose.plugins import builtin\n        for plug in builtin.plugins:\n            self.addPlugin(plug())\n        super(BuiltinPluginManager, self).loadPlugins()"}
{"prompt": "def verify(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :return: ``True`` if the signature is correct.\n        :rtype: bool\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"", "output": "\n        sig = key.sign(self.signature)\n        if sig is None:\n            return False\n        return sig.verify(\n            self.signature,\n            self.signature,\n            digestmod=hashlib.sha1,\n            sigencode=lambda sig: sig.digest(),\n        )\n\n    def verify(self, pkey):\n        \"\"\"\n        Verifies the signature on this certificate signing request.\n\n        :param PKey key: A public key.\n\n        :return: ``True`` if the signature is correct.\n        :rtype: bool\n\n        :raises OpenSSL.crypto.Error: If the signature is invalid or there is a\n            problem verifying the signature.\n        \"\"\"\n        sig = key.sign(self.signature)\n        if sig is None:\n            return False\n", "solution": "\n        if not isinstance(pkey, PKey):\n            raise TypeError(\"pkey must be a PKey instance\")\n\n        result = _lib.X509_REQ_verify(self._req, pkey._pkey)\n        if result <= 0:\n            _raise_current_error()\n\n        return result"}
{"prompt": "def add_card(self, query_params=None):\n        \"\"\"\n        Create a card for this list. Returns a Card object.\n        \"\"\"", "output": "\n        return self._add_card_with_http_info(query_params=query_params)\n\n    def add_card_with_http_info(self, query_params=None):\n        \"\"\"\n        Create a card for this list. Returns a Card object.\n        \"\"\"\n        return self._add_card_with_http_info(query_params=query_params, **operation_config)\n\n    def list_cards(self, query_params=None, headers=None):\n        \"\"\"\n        List cards for this list.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.list_cards(query_params={}, callback=callback_function)\n\n        :param callback ", "solution": "\n        card_json = self.fetch_json(\n            uri_path=self.base_uri + '/cards',\n            http_method='POST',\n            query_params=query_params or {}\n        )\n\n        return self.create_card(card_json)"}
{"prompt": "def get_default_value(self):\n        \"\"\"Instantiate a default value instance.\n\n        This is called when the containing HasTraits classes'\n        :meth:`__new__` method is called to ensure that a unique instance\n        is created for each HasTraits instance.\n        \"\"\"", "output": "\n        return DefaultValue(self)\n\n    def __instancecheck__(self, instance):\n        \"\"\"Check whether the instance is an instance of HasTraits.\n\n        HasTraits instances are always instances of HasTraits.\n        \"\"\"\n        return isinstance(instance, self.__class__)\n\n    def __subclasscheck__(self, subclass):\n        \"\"\"Check whether the class is a subclass of HasTraits.\n\n        HasTraits classes are always subclasses of HasTraits.\n        \"\"\"\n        return issubclass(subclass, self.__class__)\n\n    def __instancecheck__(self,\n                          instance,\n                          msg=None,\n                          obj_type=None,\n                          obj_subtype=None,\n                          obj_is_subtype=None,\n                          obj_is_instance=None):\n        \"\"\"Check whether the instance is an ", "solution": "\n        dv  = self.default_value\n        if isinstance(dv, DefaultValueGenerator):\n            return dv.generate(self.klass)\n        else:\n            return dv"}
{"prompt": "def forward(self, input, target):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding loss of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of loss\n        \"\"\"", "output": "\n        loss = self.criterion(input, target)\n        return loss\n\n    def backward(self, loss):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes a loss and computes the gradient of the loss with respect to the parameters,\n        compared with `input`\n\n        :param loss: scalar or list of scalar\n        :return: gradient of loss with respect to parameters\n        \"\"\"\n        loss.backward()\n\n    def step(self):\n        \"\"\"\n        NB: It's for debug only, please use optimizer.optimize() in production.\n        Takes a batch of inputs and computes the corresponding gradients with respect to the parameters,\n        compared with `input`\n\n        :return: gradient of loss with ", "solution": "\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callBigDlFunc(self.bigdl_type,\n                               \"criterionForward\",\n                               self.value,\n                               jinput,\n                               input_is_table,\n                               jtarget,\n                               target_is_table)\n        return output"}
{"prompt": "def split_line(self, line, cursor_pos=None):\n        \"\"\"Split a line of text with a cursor at the given position.\n        \"\"\"", "output": "\n        split_line = []\n        cursor_pos = cursor_pos or self.cursor_pos\n        split_line.append(line)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        split_line.append(cursor_pos)\n        ", "solution": "\n        l = line if cursor_pos is None else line[:cursor_pos]\n        return self._delim_re.split(l)[-1]"}
{"prompt": "def signing_keys_as_jwks(self):\n        \"\"\"\n        Build a JWKS from the signing keys belonging to the self signer\n\n        :return: Dictionary\n        \"\"\"", "output": "\n        jwks_obj = {}\n        for key in self.signing_keys:\n            jwks_obj[key.public_key] = key.public_key\n        return jwks_obj\n\n    def sign_message(self, message):\n        \"\"\"\n        Sign the message using the signing keys\n\n        :param message: Message to sign\n        :return: Signature\n        \"\"\"\n        signature = Signature.sign_message(self.signing_keys, message)\n        return signature\n\n    def verify_message(self, message, signature):\n        \"\"\"\n        Verify the message signature using the signing keys\n\n        :param message: Message to verify\n        :param signature: Signature to verify\n        :return: True if verified, False otherwise\n        \"\"\"\n        return Signature.verify_message(self.sign", "solution": "\n        _l = [x.serialize() for x in self.self_signer.keyjar.get_signing_key()]\n        if not _l:\n            _l = [x.serialize() for x in\n                  self.self_signer.keyjar.get_signing_key(owner=self.iss)]\n        return {'keys': _l}"}
{"prompt": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"", "output": "\n    return isinstance(node, astroid.ImportFrom) and node.level == 0\n\n\ndef is_from_imported_module(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a imported module.\"\"\"\n    return isinstance(node, astroid.ImportFrom) and node.level == 1\n\n\ndef is_from_imported_package(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a imported package.\"\"\"\n    return isinstance(node, astroid.ImportFrom) and node.level == 2\n\n\ndef is_from_imported_classdef(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a imported classdef.\"\"\"\n    return isinstance(node, astroid.ImportFrom) and node.level", "solution": "\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report generated model in human readable form.\"\"\"", "output": "\n        header = [\"Model\", \"Parameters\", \"Trainable Parameters\", \"Optimized Parameters\", \"Test Parameters\"]\n        header.append(\"-\" * len(header[0]))\n        header.append(\"-\" * len(header[1]))\n        header.append(\"-\" * len(header[2]))\n        header.append(\"-\" * len(header[3]))\n        header.append(\"-\" * len(header[4]))\n        header.append(\"-\" * len(header[5]))\n        header.append(\"-\" * len(header[6]))\n        header.append(\"-\" * len(header[7]))\n        header.append(\"-\" * len(header[8]))\n        header.append(\"-\" * len(header[9]))\n        header.append(\"-\" * len(header[10]))\n        header.append(\"-\" * len(header[11]))\n        header.", "solution": "\n        if self._args and self._args.verbose > 2:\n            pprint(self.results)\n\n        for dimension, lc_info in self.results['dimensions'].items():\n            print(\"{}D layer condition:\".format(dimension), file=output_file)\n            for cache, lc_solution in sorted(lc_info['caches'].items()):\n                print(cache+\": \", end='', file=output_file)\n                if lc_solution['lt'] is sympy.true:\n                    print(\"unconditionally fulfilled\", file=output_file)\n                else:\n                    if lc_solution['eq'] is None:\n                        print(\"{}\".format(lc_solution['lt']), file=output_file)\n                    elif type(lc_solution['eq']) is not list:\n                        print(\"{}\".format(lc_solution['eq']), file=output_file)\n                    else:\n                        for solu in lc_solution['eq']:\n                            for s, v in solu.items():\n                                print(\"{} <= {}\".format(s, v), file=output_file)"}
{"prompt": "def _draw(self, mode, vertex_list=None):\n        \"\"\"Draw vertices in the domain.\n\n        If `vertex_list` is not specified, all vertices in the domain are\n        drawn.  This is the most efficient way to render primitives.\n\n        If `vertex_list` specifies a `VertexList`, only primitives in that\n        list will be drawn.\n\n        :Parameters:\n            `mode` : int\n                OpenGL drawing mode, e.g. ``GL_POINTS``, ``GL_LINES``, etc.\n            `vertex_list` : `VertexList`\n                Vertex list to draw, or ``None`` for all lists in this domain.\n\n        \"\"\"", "output": "\n        if vertex_list is None:\n            vertex_list = self.vertex_list\n\n        if vertex_list is not None:\n            for vertex in vertex_list:\n                self._draw_vertex(vertex)\n        else:\n            for vertex in self.vertices:\n                self._draw_vertex(vertex)\n\n    def _draw_vertex(self, vertex):\n        \"\"\"Draw a vertex.\n\n        Override this method to implement custom vertex drawing.\n\n        :Parameters:\n            `vertex` : `Vertex`\n                Vertex to draw.\n\n        \"\"\"\n        pass\n\n    def _draw_line(self, line):\n        \"\"\"Draw a line.\n\n        Override this method to implement custom line drawing.\n\n        :Parameters:\n            `line` : `Line`\n                Line to draw.\n\n        \"\"\"\n", "solution": "\n        glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n        for buffer, attributes in self.buffer_attributes:\n            buffer.bind()\n            for attribute in attributes:\n                attribute.enable()\n                attribute.set_pointer(attribute.buffer.ptr)\n        if vertexbuffer._workaround_vbo_finish:\n            glFinish()\n\n        if vertex_list is not None:\n            glDrawArrays(mode, vertex_list.start, vertex_list.count)\n        else:\n            starts, sizes = self.allocator.get_allocated_regions()\n            primcount = len(starts)\n            if primcount == 0:\n                pass\n            elif primcount == 1:\n                # Common case\n                glDrawArrays(mode, starts[0], int(sizes[0]))\n            elif gl_info.have_version(1, 4):\n                starts = (GLint * primcount)(*starts)\n                sizes = (GLsizei * primcount)(*sizes)\n                glMultiDrawArrays(mode, starts, sizes, primcount)\n            else:\n                for start, size in zip(starts, sizes):\n                    glDrawArrays(mode, start, size)\n\n        for buffer, _ in self.buffer_attributes:\n            buffer.unbind()\n        glPopClientAttrib()"}
{"prompt": "def parse_rrset(e_rrset, connection, zone_id):\n    \"\"\"\n    This a parser that allows the passing of any valid ResourceRecordSet\n    tag. It will spit out the appropriate ResourceRecordSet object for the tag.\n\n    :param lxml.etree._Element e_rrset: The root node of the etree parsed\n        response from the API.\n    :param Route53Connection connection: The connection instance used to\n        query the API.\n    :param str zone_id: The zone ID of the HostedZone these rrsets belong to.\n    :rtype: ResourceRecordSet\n    :returns: An instantiated ResourceRecordSet object.\n    \"\"\"", "output": "\n    zone = connection.zones.get(zone_id)\n    records = zone.resource_record_sets.get(e_rrset.tag)\n    return ResourceRecordSet(records, zone)\n\n\ndef parse_rrset_list(e_rrset_list, connection, zone_id):\n    \"\"\"\n    This a parser that allows the passing of any valid ResourceRecordSet\n    tag. It will spit out the appropriate ResourceRecordSet object for the tag.\n\n    :param lxml.etree._Element e_rrset_list: The root node of the etree parsed\n        response from the API.\n    :param Route53Connection connection: The connection instance used to\n        query the API.\n    :param str zone_id: The zone ID of the Hosted", "solution": "\n\n    # This dict will be used to instantiate a ResourceRecordSet instance to yield.\n    kwargs = {\n        'connection': connection,\n        'zone_id': zone_id,\n    }\n    rrset_type = None\n\n    for e_field in e_rrset:\n        # Cheesy way to strip off the namespace.\n        tag_name = e_field.tag.split('}')[1]\n        field_text = e_field.text\n\n        if tag_name == 'Type':\n            # Need to store this to determine which ResourceRecordSet\n            # subclass to instantiate.\n            rrset_type = field_text\n            continue\n        elif tag_name == 'AliasTarget':\n            # A records have some special field values we need.\n            alias_hosted_zone_id, alias_dns_name = parse_rrset_alias(e_field)\n            kwargs['alias_hosted_zone_id'] = alias_hosted_zone_id\n            kwargs['alias_dns_name'] = alias_dns_name\n            # Alias A entries have no TTL.\n            kwargs['ttl'] = None\n            continue\n        elif tag_name == 'ResourceRecords':\n            kwargs['records'] = parse_rrset_record_values(e_field)\n            continue\n\n        # Map the XML tag name to a kwarg name.\n        kw_name = RRSET_TAG_TO_KWARG_MAP[tag_name]\n        # This will be the key/val pair used to instantiate the\n        # ResourceRecordSet instance.\n        kwargs[kw_name] = field_text\n\n    if not rrset_type:\n        raise Route53Error(\"No Type tag found in ListResourceRecordSetsResponse.\")\n\n    if 'records' not in kwargs:\n        # Not all rrsets have records.\n        kwargs['records'] = []\n\n    RRSetSubclass = RRSET_TYPE_TO_RSET_SUBCLASS_MAP[rrset_type]\n    return RRSetSubclass(**kwargs)"}
{"prompt": "def get_params(degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Get parameters for affine transformation\n\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"", "output": "\n        seq = list()\n        params = []\n        for degree in degrees:\n            for translate in translate:\n                for scale_range in scale_ranges:\n                    for shear in shears:\n                        params.append(\n                            (degree, translate, scale_range, shear, img_size)\n                        )\n                        seq.append(\n                            (degree, translate, scale_range, shear, img_size)\n                        )\n        return seq\n\n    def __call__(self, degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Call function to apply affine transformation\n\n        Args:\n            degrees (sequence): Degrees of rotation.\n            translate (sequence): Translation.\n            scale_ranges (sequence): Range of scale.\n            ", "solution": "\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear"}
{"prompt": "def topic(self, topic_id):\n        \"\"\"Retrive the topic with `topic_id` identifier.\n\n        :param topic_id: identifier of the topic to retrieve\n        \"\"\"", "output": "\n        return self._get(\"/topic/%s\" % topic_id)\n\n    def topic_exists(self, topic_id):\n        \"\"\"Check if a topic exists with the given `topic_id`.\n\n        :param topic_id: identifier of the topic to check\n        \"\"\"\n        return self._get(\"/topic/%s\" % topic_id) is not None\n\n    def topic_create(self, topic):\n        \"\"\"Create a topic.\n\n        :param topic: topic dictionary\n        \"\"\"\n        return self._post(\"/topic\", topic)\n\n    def topic_update(self, topic_id, topic):\n        \"\"\"Update a topic.\n\n        :param topic_id: identifier of the topic to update\n        :param topic: topic dictionary\n        \"\"\"\n        return self._put(\"/topic/%s\" % topic_id, topic)\n\n    def topic_delete(self, topic_id):", "solution": "\n        params = {\n            self.PKEY: self.api_key\n        }\n\n        # http://example.com/t/8.json\n        response = self._call(self.TOPIC, topic_id,\n                              params=params)\n\n        return response"}
{"prompt": "def decode(data):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"", "output": "\n    bits = data.split('|')\n    bits = [int(bit, 2) for bit in bits]\n    return PackBit(bits)\n\n\ndef decode(bits):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"\n    bits = bits.split('|')\n    bits = [int(bit, 2) for bit in bits]\n    return PackBit(bits)\n\n\ndef decode(bits):\n    \"\"\"\n    Decodes a PackBit encoded data.\n    \"\"\"\n    bits = bits.split('|')\n    bits = [int(bit, 2) for bit in bits]\n    return PackBit(bits)\n\n\ndef decode(bits):\n    \"\"\"\n", "solution": "\n    data = bytearray(data) # <- python 2/3 compatibility fix\n    result = bytearray()\n    pos = 0\n    while pos < len(data):\n        header_byte = data[pos]\n        if header_byte > 127:\n            header_byte -= 256\n        pos += 1\n\n        if 0 <= header_byte <= 127:\n            result.extend(data[pos:pos+header_byte+1])\n            pos += header_byte+1\n        elif header_byte == -128:\n            pass\n        else:\n            result.extend([data[pos]] * (1 - header_byte))\n            pos += 1\n\n    return bytes(result)"}
{"prompt": "def get_conn(self):\n        \"\"\"Return the FileService object.\"\"\"", "output": "\n        if self._conn is None:\n            self._conn = FileService(self.configuration)\n        return self._conn\n\n    def list_containers(self, params=None):\n        \"\"\"List containers.\n\n        :param params: dict of parameters to send to the API call.\n        :returns: list of dictionaries containing metadata and container list\n        \"\"\"\n        containers = []\n        containers.extend(self._conn.list_containers(params))\n        return containers\n\n    def inspect_container(self, container_name):\n        \"\"\"Inspect a container.\n\n        :param container_name: name of the container to inspect.\n        :returns: dictionary containing metadata and container list\n        \"\"\"\n        container = self._conn.inspect_container(container_name)\n        return container\n\n    def put_object(self, container_name, object_name, fobj, param", "solution": "\n        conn = self.get_connection(self.conn_id)\n        service_options = conn.extra_dejson\n        return FileService(account_name=conn.login,\n                           account_key=conn.password, **service_options)"}
{"prompt": "def createClient(self):\n        \"\"\"Create a UDP connection to Riemann\"\"\"", "output": "\n        self.client = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self.client.bind(('127.0.0.1', 0))\n        self.client.sendto('Riemann', self.server)\n        self.client.settimeout(1.0)\n        self.client.setblocking(0)\n\n    def closeClient(self):\n        \"\"\"Close the UDP connection to Riemann\"\"\"\n        self.client.close()\n\n    def clientLoop(self):\n        \"\"\"Loop until a client disconnects\"\"\"\n        while 1:\n            try:\n                data, addr = self.client.recvfrom(1024)\n                print \"Received from %s:%s\" % (addr[0], addr[1])\n                print \"Message: %s\" % data\n            except socket.timeout:\n                print \"Client timed out\"\n            except socket.error:", "solution": "\n        server = self.config.get('server', '127.0.0.1')\n        port = self.config.get('port', 5555)\n\n        def connect(ip):\n            self.protocol = riemann.RiemannUDP(ip, port)\n            self.endpoint = reactor.listenUDP(0, self.protocol)\n\n        d = reactor.resolve(server)\n        d.addCallback(connect)\n        return d"}
{"prompt": "def _multi_gamma_sequence(self, a, p, name=\"multi_gamma_sequence\"):\n    \"\"\"Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p].\"\"\"", "output": "\n    return tf.reshape(a, tf.concat([tf.shape(a), [p]], axis=0))\n\n\ndef _digamma_sequence(self, a, p, name=\"digamma_sequence\"):\n    \"\"\"Creates sequence used in digamma; shape = shape(a)+[p].\"\"\"\n    return tf.reshape(a, tf.concat([tf.shape(a), [p]], axis=0))\n\n\ndef _digamma_sequence_v2(self, a, p, name=\"digamma_sequence_v2\"):\n    \"\"\"Creates sequence used in digamma; shape = shape(a)+[p].\"\"\"\n    return tf.reshape(a, tf.concat([tf.shape(a), [p]], axis=0))\n\n\ndef _digamma_sequence_v2(self, a, p, name=\"digamma_sequence_v2\"):\n    \"\"\"Creates sequence used in digamma", "solution": "\n    with self._name_scope(name):\n      # Linspace only takes scalars, so we'll add in the offset afterwards.\n      seq = tf.linspace(\n          tf.constant(0., dtype=self.dtype), 0.5 - 0.5 * p, tf.cast(\n              p, tf.int32))\n      return seq + tf.expand_dims(a, [-1])"}
{"prompt": "def skus(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-06-01: :class:`SkusOperations<azure.mgmt.storage.v2017_06_01.operations.SkusOperations>`\n           * 2017-10-01: :class:`SkusOperations<azure.mgmt.storage.v2017_10_01.operations.SkusOperations>`\n           * 2018-02-01: :class:`SkusOperations<azure.mgmt.storage.v2018_02_01.operations.SkusOperations>`\n           * 2018-03-01-preview: :class:`SkusOperations<azure.mgmt.storage.v2018_03_01_preview.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n        \"\"\"", "output": "01_preview.operations.SkusOperations>`\n           * 2018-04-01: :class:`SkusOperations<azure.mgmt.storage.v2018_04_01.operations.SkusOperations>`\n           * 2018-06-01: :class:`SkusOperations<azure.mgmt.storage.v2018_06_01.operations.SkusOperations>`\n           * 2018-07-01: :class:`SkusOperations<azure.mgmt.storage.v2018_07_01.operations.SkusOperations>`\n           * 2018-08-01: :class:`SkusOperations<azure.mgmt.storage.v2018_08_01.", "solution": "\n        api_version = self._get_api_version('skus')\n        if api_version == '2017-06-01':\n            from .v2017_06_01.operations import SkusOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import SkusOperations as OperationClass\n        elif api_version == '2018-03-01-preview':\n            from .v2018_03_01_preview.operations import SkusOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import SkusOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def delete_local_file(file_name):\n    \"\"\"\n    Deletes the file associated with the file_name passed from local storage.\n    \n    :param str file_name: Filename of the file to be deleted\n    :return str: Filename of the file that was just deleted\n    \"\"\"", "output": "\n    \n    file_path = os.path.join(\"data\", file_name)\n    \n    try:\n        os.remove(file_path)\n    except FileNotFoundError:\n        print(\"File not found\")\n    \n    return file_name\n</cell>\n<cell>\n# Delete the file associated with the file name\ndelete_local_file(\"test.txt\")\n</cell>\n<text>\nNow that we have deleted the file, we can move it to a different location on the local file system. Let's move the file to a different location on the local file system.\n\n\n</text>\n<cell>\n# Move the file to a different location on the local file system\nos.rename(\"test.txt\", \"test.txt2\")\n</cell>\n<text>\nNow that ", "solution": "\n\n    try:\n        os.remove(file_name)\n        log.info(f\"Deletion for {file_name} has finished\")\n        return file_name\n    except OSError:\n        pass"}
{"prompt": "def output_json(self, file_name='/tmp/ND.json'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.json'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"", "output": "\n        with open(file_name, 'w') as outfile:\n            json.dump(self.__dict__, outfile, indent=4, sort_keys=True)\n\n    def output_jsonl(self, file_name='/tmp/ND.jsonl'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.jsonl'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"\n        with open(file_name, 'w') as outfile:\n            json.dump(self.__dict__, outfile, indent=4, sort_keys=True)\n\n    def output_jsonls(self, file_name='/tmp/ND.jsonls'):\n        \"\"\"\n        Arguments:\n            file_name(str : '/tmp/ND.jsonls'): The file name to store the json to\n\n        Returns:\n            None\n        \"\"\"\n        with open(file_name, 'w') as outfile:\n            json.dump(self.__dict__, ", "solution": "\n        complete_example = (\n            self.dataset, self.project, self.channels, self.metadata)\n        data = json.loads(self.nd_json(*complete_example))\n\n        # self.verify_json(data)\n        self.verify_path(data, VERIFY_BY_SLICE)\n\n        f = open(file_name, 'w')\n        f.write(str(data))\n        f.close()"}
{"prompt": "def open_graph(self, **args):\n        \"\"\"open a vcg graph\n        \"\"\"", "output": "\n        graph = self.graph\n        graph.open()\n        return graph\n\n    def close(self, **args):\n        \"\"\"close a vcg graph\n        \"\"\"\n        graph = self.graph\n        graph.close()\n\n    def close_vcg(self, **args):\n        \"\"\"close a vcg graph\n        \"\"\"\n        graph = self.graph\n        graph.close_vcg()\n\n    def close_all(self, **args):\n        \"\"\"close all vcg graphs\n        \"\"\"\n        graph = self.graph\n        graph.close_all()\n\n    def close_all_vcg(self, **args):\n        \"\"\"close all vcg graphs\n        \"\"\"\n        graph = self.graph\n        graph.close_all_vcg()\n\n    def close_", "solution": "\n        self._stream.write(\"%sgraph:{\\n\" % self._indent)\n        self._inc_indent()\n        self._write_attributes(GRAPH_ATTRS, **args)"}
{"prompt": "def remove(cls, name: sym.Symbol) -> Optional[\"Namespace\"]:\n        \"\"\"Remove the namespace bound to the symbol `name` in the global\n        namespace cache and return that namespace.\n        Return None if the namespace did not exist in the cache.\"\"\"", "output": "\n        namespace = cls.global_namespace.get(name)\n        if namespace is None:\n            return None\n        cls.global_namespace.pop(name)\n        return namespace\n\n    def namespace(\n        self, name: sym.Symbol, override: bool = False\n    ) -> \"Namespace\":\n        \"\"\"Get the namespace bound to the symbol `name` in the global\n        namespace cache.\n\n        If `override` is True, return the namespace bound to the symbol\n        `name` from the global namespace cache. Otherwise, if the\n        namespace does not exist in the global namespace cache,\n        create a new namespace and return the new namespace.\"\"\"\n        namespace = self.global_namespace.get(name)\n        if namespace is None:\n            namespace = Namespace(\n                name,\n                self,\n                override=override,", "solution": "\n        while True:\n            oldval: lmap.Map = cls._NAMESPACES.deref()\n            ns: Optional[Namespace] = oldval.entry(name, None)\n            newval = oldval\n            if ns is not None:\n                newval = oldval.dissoc(name)\n            if cls._NAMESPACES.compare_and_set(oldval, newval):\n                return ns"}
{"prompt": "def einsum_vecmul_index(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-vector multiplication.\n\n    The returned indices are to perform a matrix multiplication A.v where\n    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on v.\n\n    Args:\n        gate_indices (list[int]): the indices of the right matrix subsystems\n                                  to contract with the left matrix.\n        number_of_qubits (int): the total number of qubits for the right matrix.\n\n    Returns:\n        str: An indices string for the Numpy.einsum function.\n    \"\"\"", "output": "for Numpy.eignsum matrix-vector multiplication.\n    \"\"\"\n    indices = []\n    for gate_index in gate_indices:\n        indices.append(str(gate_index) + \"_\" + str(number_of_qubits))\n    return \",\".join(indices)\n\n\ndef einsum_vecmul_index_to_string(gate_indices, number_of_qubits):\n    \"\"\"Return the index string for Numpy.eignsum matrix-vector multiplication.\n\n    The returned indices are to perform a matrix multiplication A.v where\n    the matrix A is an M-qubit matrix, vector v is an N-qubit vector, and\n    M <= N, and identity matrices are implied on the subsystems where A has no\n    support on v.", "solution": "\n\n    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,\n                                                                    number_of_qubits)\n\n    # Combine indices into matrix multiplication string format\n    # for numpy.einsum function\n    return \"{mat_l}{mat_r}, \".format(mat_l=mat_l, mat_r=mat_r) + \\\n           \"{tens_lin}->{tens_lout}\".format(tens_lin=tens_lin,\n                                            tens_lout=tens_lout)"}
{"prompt": "def get_client(self):\n        \"\"\"\n        Obtains a client for use, whether an existing unused client\n        or a brand new one if none are available.\n        \"\"\"", "output": "\n        if self.client is not None:\n            return self.client\n\n        client = self.client_class(self.host, self.port)\n        self.client = client\n        return client\n\n    def close_client(self):\n        \"\"\"\n        Closes the client and releases any resources associated with it.\n        \"\"\"\n        self.client = None\n\n    def wait_for_connection(self):\n        \"\"\"\n        Waits for the client to connect and returns.\n        \"\"\"\n        self.client = None\n        self.connect()\n\n    def connect(self):\n        \"\"\"\n        Connects to the remote server and establishes a connection\n        with the server.\n        \"\"\"\n        self.sock = socket.create_connection((self.host, self.port))\n        self.sock.settimeout(self.timeout)\n\n    def disconnect(self):\n        \"\"\"\n        Disconnects from the remote ", "solution": "\n        client = None\n        try:\n            client = self.clients.get(block=False)\n        except queue.Empty:\n            pass\n        if not client:\n            self.client_id += 1\n            kwargs = dict(self.kwargs)\n            kwargs['verbose_id'] = kwargs.get(\n                'verbose_id', '') + str(self.client_id)\n            client = self.client_class(*self.args, **kwargs)\n        return client"}
{"prompt": "def _sem_open(name, value=None):\n    \"\"\" Construct or retrieve a semaphore with the given name\n\n    If value is None, try to retrieve an existing named semaphore.\n    Else create a new semaphore with the given value\n    \"\"\"", "output": "\n    if value is None:\n        return _get_semaphore(name)\n    else:\n        return Semaphore(value)\n\n\ndef _sem_unlink(name):\n    \"\"\" Delete a semaphore\n    \"\"\"\n    try:\n        os.unlink(name)\n    except OSError:\n        pass\n\n\ndef _sem_close(name):\n    \"\"\" Close a semaphore\n    \"\"\"\n    try:\n        os.close(name)\n    except OSError:\n        pass\n\n\ndef _sem_unlink(name):\n    \"\"\" Delete a semaphore\n    \"\"\"\n    try:\n        os.unlink(name)\n    except OSError:\n        pass\n\n\ndef _sem_close(name):\n    \"\"\" Close a semaphore\n    \"\"\"\n    try:\n        os.close(name)\n    except OSError:\n        pass\n\n\ndef _sem_unlink(name):\n", "solution": "\n    if value is None:\n        handle = pthread.sem_open(ctypes.c_char_p(name), 0)\n    else:\n        handle = pthread.sem_open(ctypes.c_char_p(name), SEM_OFLAG, SEM_PERM,\n                                  ctypes.c_int(value))\n\n    if handle == SEM_FAILURE:\n        e = ctypes.get_errno()\n        if e == errno.EEXIST:\n            raise FileExistsError(\"a semaphore named %s already exists\" % name)\n        elif e == errno.ENOENT:\n            raise FileNotFoundError('cannot find semaphore named %s' % name)\n        elif e == errno.ENOSYS:\n            raise NotImplementedError('No semaphore implementation on this '\n                                      'system')\n        else:\n            raiseFromErrno()\n\n    return handle"}
{"prompt": "def delete(self, request, *args, **kwargs):\n        \"\"\"Delete auth token when `delete` request was issued.\"\"\"", "output": "\n        request.user.auth_token.delete()\n        return redirect(self.get_success_url())\n\n    def logout(self, request, *args, **kwargs):\n        \"\"\"Logout user when `logout` request was issued.\"\"\"\n        request.session.flush()\n        request.session.clear()\n        return redirect(self.get_success_url())\n\n    def form_valid(self, form):\n        \"\"\"Logout user when `logout` request was issued.\"\"\"\n        request.session.flush()\n        request.session.clear()\n        request.user = None\n        return redirect(self.get_success_url())\n\n\nclass LoginView(LoginRequiredMixin, View):\n    \"\"\"Login view for user.\"\"\"\n\n    template_name = 'accounts/login.html'\n    form_class = AuthenticationForm\n    success_url = reverse_lazy", "solution": "\n        # Logic repeated from DRF because one cannot easily reuse it\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != b'token':\n            return response.Response(status=status.HTTP_400_BAD_REQUEST)\n\n        if len(auth) == 1:\n            msg = 'Invalid token header. No credentials provided.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n        elif len(auth) > 2:\n            msg = 'Invalid token header. Token string should not contain spaces.'\n            return response.Response(msg, status=status.HTTP_400_BAD_REQUEST)\n\n        try:\n            token = self.model.objects.get(key=auth[1])\n        except self.model.DoesNotExist:\n            pass\n        else:\n            token.delete()\n            signals.user_logged_out.send(\n                type(self),\n                user=token.user,\n                request=request,\n            )\n        return response.Response(status=status.HTTP_204_NO_CONTENT)"}
{"prompt": "def load_pkcs7_data(type, buffer):\n    \"\"\"\n    Load pkcs7 data from the string *buffer* encoded with the type\n    *type*.\n\n    :param type: The file type (one of FILETYPE_PEM or FILETYPE_ASN1)\n    :param buffer: The buffer with the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"", "output": "\n    pkcs7 = pkcs7_lib.PKCS7()\n    pkcs7.set_version(version)\n    pkcs7.set_type(type)\n    pkcs7.set_data(buffer)\n    return pkcs7\n\n\ndef load_pkcs7_stream(stream, password):\n    \"\"\"\n    Load pkcs7 data from the stream *stream* encoded with the type\n    *FILETYPE_PEM* or *FILETYPE_ASN1*.\n\n    :param stream: The stream with the pkcs7 data.\n    :param password: The password used to encrypt the pkcs7 data.\n    :return: The PKCS7 object\n    \"\"\"\n    pkcs7 = pkcs7_lib.PKCS7()\n    ", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        pkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        pkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if pkcs7 == _ffi.NULL:\n        _raise_current_error()\n\n    pypkcs7 = PKCS7.__new__(PKCS7)\n    pypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n    return pypkcs7"}
{"prompt": "def create_sprites_dataset(characters, actions, directions, channels=3,\n                           length=8, shuffle=False, fake_data=False):\n  \"\"\"Creates a tf.data pipeline for the sprites dataset.\n\n  Args:\n    characters: A list of (skin, hair, top, pants) tuples containing\n      relative paths to the sprite png image for each attribute.\n    actions: A list of Actions.\n    directions: A list of Directions.\n    channels: Number of image channels to yield.\n    length: Desired length of the sequences.\n    shuffle: Whether or not to shuffle the characters and sequences\n      start frame.\n    fake_data: Boolean for whether or not to yield synthetic data.\n\n  Returns:\n    A tf.data.Dataset yielding (seq, skin label index, hair label index,\n    top label index, pants label index, action label index, skin label\n    name, hair label_name, top label name, pants label name, action\n    label name) tuples.\n  \"\"\"", "output": "\n    A tf.data pipeline for the sprites dataset.\n  \"\"\"\n  dataset = tf.data.Dataset.from_tensor_slices(\n      (characters, actions, directions))\n  if fake_data:\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(length)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n  else:\n    dataset = dataset.shuffle(length)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n  dataset = dataset.map(\n      lambda characters, actions, directions: _load_sprites(\n          characters, actions, directions, channels),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  return dataset\n\n\ndef _load_sprites(characters, actions, directions, channels):\n", "solution": "\n  if fake_data:\n    dummy_image = tf.random.normal([HEIGHT, WIDTH, CHANNELS])\n  else:\n    basedir = download_sprites()\n\n  action_names = [action.name for action in actions]\n  action_metadata = [(action.start_row, action.frames) for action in actions]\n\n  direction_rows = [direction.row_offset for direction in directions]\n\n  chars = tf.data.Dataset.from_tensor_slices(characters)\n  act_names = tf.data.Dataset.from_tensor_slices(action_names).repeat()\n  acts_metadata = tf.data.Dataset.from_tensor_slices(action_metadata).repeat()\n  dir_rows = tf.data.Dataset.from_tensor_slices(direction_rows).repeat()\n\n  if shuffle:\n    chars = chars.shuffle(len(characters))\n\n  dataset = tf.data.Dataset.zip((chars, act_names, acts_metadata, dir_rows))\n\n  skin_table = tf.contrib.lookup.index_table_from_tensor(sorted(SKIN_COLORS))\n  hair_table = tf.contrib.lookup.index_table_from_tensor(sorted(HAIRSTYLES))\n  top_table = tf.contrib.lookup.index_table_from_tensor(sorted(TOPS))\n  pants_table = tf.contrib.lookup.index_table_from_tensor(sorted(PANTS))\n  action_table = tf.contrib.lookup.index_table_from_tensor(sorted(action_names))\n\n  def process_example(attrs, act_name, act_metadata, dir_row_offset):\n    "}
{"prompt": "def _prepare_subsection(data, sections, wrapper):\n    \"\"\"\n    Creates a subsection ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A newly created subsection.\n    \"\"\"", "output": "\n    section = Section(\n        name=data['name'],\n        description=data['description'],\n        wrapper=wrapper,\n    )\n    sections.append(section)\n    return section\n\n\ndef _prepare_section(data, sections, wrapper):\n    \"\"\"\n    Creates a section ready to be filled.\n    :param data: Semi-parsed dictionary.\n    :param sections: A list of sections.\n    :param wrapper: A wrapping object for key-value pairs.\n    :return: A newly created section.\n    \"\"\"\n    section = Section(\n        name=data['name'],\n        description=data['description'],\n        wrapper=wrapper,\n    )\n    sections.append(section)\n    return section\n\n\ndef _prepare_sections(data, section", "solution": "\n    current = data\n    for i in sections[:-1]:\n        current = current[i]\n\n    current[sections[-1]] = wrapper()\n    return current[sections[-1]]"}
{"prompt": "def render_tag(self, context, *tag_args, **tag_kwargs):\n        \"\"\"\n        Rendering of the tag. It either assigns the value as variable, or renders it.\n        \"\"\"", "output": "\n        tag = tag_kwargs.pop('tag', None)\n        tag_args = tag_args or []\n        tag_kwargs = tag_kwargs or {}\n        tag = tag or self.tag\n        tag_args = tag_args or []\n        tag_kwargs = tag_kwargs or {}\n        tag_kwargs.setdefault('context', context)\n        tag_kwargs.setdefault('request', self.request)\n        tag_kwargs.setdefault('template_name', self.template_name)\n        tag_kwargs.setdefault('template_loader', self.template_loader)\n        tag_kwargs.setdefault('tags', self.tags)\n        tag_kwargs.setdefault('nodelist', self.nodelist)\n        tag_kwargs.setdefault('nodelist_template', self.nodelist_template)\n        tag_kwargs.set", "solution": "\n        # Be very explicit about which base functionality is used:\n        # Using super() for mixin support will not work nicely anyway here.\n        if self.as_var:\n            # Assign the value in the parent context\n            return BaseAssignmentNode.render_tag(self, context, *tag_args, **tag_kwargs)\n        else:\n            # Render the output using the BaseInclusionNode features\n            return BaseInclusionNode.render_tag(self, context, *tag_args, **tag_kwargs)"}
{"prompt": "def verify_token(self, request, *args, **kwargs):\n        \"\"\"\n        Use `token` to allow one-time access to a view.\n\n        Set the user as a class attribute or raise an `InvalidExpiredToken`.\n\n        Token expiry can be set in `settings` with `VERIFY_ACCOUNT_EXPIRY` and is\n        set in seconds.\n        \"\"\"", "output": "\n        token = request.META.get('HTTP_AUTHORIZATION')\n        if not token:\n            raise InvalidToken()\n        try:\n            token = token.split()[1]\n        except IndexError:\n            raise InvalidToken()\n        try:\n            user = User.objects.get(token=token)\n        except User.DoesNotExist:\n            raise InvalidToken()\n        if not user.is_active:\n            raise InvalidToken()\n        return user\n\n    def get_user(self, user_id):\n        \"\"\"\n        Override this method to return a user object instead of an `AnonymousUser`.\n        \"\"\"\n        try:\n            return User.objects.get(pk=user_id)\n        except User.DoesNotExist:\n            return None\n\n    def get_user_by_token(self, token):\n        \"\"\"\n        Override this method to return a user object instead of ", "solution": "\n        User = get_user_model()\n\n        try:\n            max_age = settings.VERIFY_ACCOUNT_EXPIRY\n        except AttributeError:\n            max_age = self.DEFAULT_VERIFY_ACCOUNT_EXPIRY\n\n        try:\n            email_data = signing.loads(kwargs['token'], max_age=max_age)\n        except signing.BadSignature:\n            raise self.invalid_exception_class\n\n        email = email_data['email']\n\n        try:\n            self.user = User.objects.get_by_natural_key(email)\n        except User.DoesNotExist:\n            raise self.invalid_exception_class\n\n        if self.user.email_verified:\n            raise self.permission_denied_class"}
{"prompt": "def partitions(l, partition_size):\n    \"\"\"\n    >>> list(partitions([], 10))\n    []\n    >>> list(partitions([1,2,3,4,5], 1))\n    [[1], [2], [3], [4], [5]]\n    >>> list(partitions([1,2,3,4,5], 2))\n    [[1, 2], [3, 4], [5]]\n    >>> list(partitions([1,2,3,4,5], 5))\n    [[1, 2, 3, 4, 5]]\n\n    :param list l: List to be partitioned\n    :param int partition_size: Size of partitions\n    \"\"\"", "output": "\n    partitions = []\n    partitions.append(l)\n    partition_size = len(l)\n    while partition_size > 0:\n        partition = partitions[partition_size - 1]\n        partition_size -= 1\n        partition.insert(0, partition.pop(0))\n    return partitions\n\n\ndef partitions_with_replacements(l, partition_size):\n    \"\"\"\n    >>> list(partitions_with_replacements([], 10))\n    []\n    >>> list(partitions_with_replacements([1,2,3,4,5], 1))\n    [[1], [2], [3], [4], [5]]\n    >>> list(partitions_with_replacements([1,2,3,4,5], 2))\n    [[1, 2], [3, 4], [5]]\n    >>> list(partitions_with_replacement", "solution": "\n    for i in xrange(0, len(l), partition_size):\n        yield l[i:i + partition_size]"}
{"prompt": "def download_layers(self, repo_name, digest=None, destination=None):\n    \"\"\" download layers is a wrapper to do the following for a client loaded\n        with a manifest for an image:\n      \n        1. use the manifests to retrieve list of digests (get_digests)\n        2. atomically download the list to destination (get_layers)\n\n        This function uses the MultiProcess client to download layers\n        at the same time.\n    \"\"\"", "output": "\n    \n    manifests = self.manifests[repo_name]\n    \n    layers = []\n    \n    for manifest in manifests:\n        \n        layer_digests = manifest.get_digests(digest)\n        \n        for layer_digest in layer_digests:\n            \n            layer_name = manifest.get_layer_name(layer_digest)\n            \n            layer_url = manifest.get_layer_url(layer_digest)\n            \n            layer_name = layer_name.split('/')[-1]\n            \n            layer_url = layer_url.split('/')[-1]\n            \n            layer_url = layer_url.replace('.zip', '', 1)\n            \n            layer_url = layer_url.replace('.tar', '', 1)\n            \n            layer_url = layer_", "solution": "\n    from sregistry.main.workers import ( Workers, download_task )\n\n    # 1. Get manifests if not retrieved\n    if not hasattr(self, 'manifests'):\n        self._get_manifests(repo_name, digest)\n\n    # Obtain list of digets, and destination for download\n    digests = self._get_digests()\n    destination = self._get_download_cache(destination)\n\n    # Create multiprocess download client\n    workers = Workers()\n\n    # Download each layer atomically\n    tasks = []\n    layers = []\n    for digest in digests:\n\n        targz = \"%s/%s.tar.gz\" % (destination, digest)\n\n        # Only download if not in cache already\n        if not os.path.exists(targz):\n            url = \"%s/%s/blobs/%s\" % (self.base, repo_name, digest)\n            tasks.append((url, self.headers, targz))\n        layers.append(targz)\n\n    # Download layers with multiprocess workers\n    if len(tasks) > 0:\n        download_layers = workers.run(func=download_task,\n                                      tasks=tasks)\n    # Create the metadata tar\n    metadata = self._create_metadata_tar(destination)\n    if metadata is not None:\n        layers.append(metadata)\n\n\n    return layers"}
{"prompt": "def process_item(self, item, spider):\n        \"\"\"\n        Process single item. Add item to items and then upload to S3 if size of items\n        >= max_chunk_size.\n        \"\"\"", "output": "\n        item['item_url'] = item['url']\n        item['item_id'] = item['url'].split('/')[-1]\n        item['item_size'] = item['size']\n        item['item_date'] = item['date']\n        item['item_modified'] = item['modified']\n        item['item_created'] = item['created']\n        item['item_modified'] = item['modified']\n        item['item_created'] = item['created']\n        item['item_modified'] = item['modified']\n        item['item_modified'] = item['modified']\n        item['item_modified'] = item['modified']\n        item['item_modified'] = item['modified']\n        item['item_modified'] = item['modified']\n        item['item_modified'] = item['modified']\n", "solution": "\n        self.items.append(item)\n        if len(self.items) >= self.max_chunk_size:\n            self._upload_chunk(spider)\n\n        return item"}
{"prompt": "def handle_print(self, signame, set_print):\n        \"\"\"Set whether we print or not when this signal is caught.\"\"\"", "output": "\n        self.print_signal = set_print\n\n    def print_exception(self, signum, frame):\n        \"\"\"Print exception traceback to stderr.\"\"\"\n        traceback.print_exception(signum, frame, self.tbformat,\n                                  self.tb_offset)\n\n    def print_stack_trace(self, signum, frame):\n        \"\"\"Print stack trace to stderr.\"\"\"\n        traceback.print_stack(frame, self.tbformat,\n                              self.tb_offset)\n\n    def print_exception_from_traceback(self, exc_info):\n        \"\"\"Print exception traceback to stderr and re-raise it.\"\"\"\n        traceback.print_exception(*exc_info)\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n    def print_tb(self, frame=None):\n        \"\"\"Print the calling stack up to current exception.\"\"\"\n        if frame is None:\n            frame = ", "solution": "\n        if set_print:\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n        else:\n            self.sigs[signame].print_method = None\n            pass\n        return set_print"}
{"prompt": "def _trj_check_version(self, version, python, force):\n        \"\"\"Checks for version mismatch\n\n        Raises a VersionMismatchError if version of loaded trajectory and current pypet version\n        do not match. In case of `force=True` error is not raised only a warning is emitted.\n\n        \"\"\"", "output": "\n        trj = self.trj\n        trj_version = trj.version\n        trj_python_version = trj.python_version\n        trj_python_version_str = trj_python_version.__str__()\n        trj_python_version_str_lower = trj_python_version_str.lower()\n        trj_python_version_str_lower = trj_python_version_str_lower.replace('.', '')\n        trj_python_version_str_lower = trj_python_version_str_lower.replace('-', '')\n        trj_python_version_str_lower = trj_python_version_str_lower.replace('.', '')\n        trj_python_version_str_lower = trj_python_version_str_lower.replace", "solution": "\n        curr_python = pypetconstants.python_version_string\n\n        if (version != VERSION or curr_python != python) and not force:\n            raise pex.VersionMismatchError('Current pypet version is %s used under python %s '\n                                           '  but your trajectory'\n                                           ' was created with version %s and python %s.'\n                                           ' Use >>force=True<< to perform your load regardless'\n                                           ' of version mismatch.' %\n                                           (VERSION, curr_python, version, python))\n        elif version != VERSION or curr_python != python:\n            self._logger.warning('Current pypet version is %s with python %s but your trajectory'\n                                 ' was created with version %s under python %s.'\n                                 ' Yet, you enforced the load, so I will'\n                                 ' handle the trajectory despite the'\n                                 ' version mismatch.' %\n                                 (VERSION, curr_python, version, python))"}
{"prompt": "def get_branch_mutation_matrix(self, node, full_sequence=False):\n        \"\"\"uses results from marginal ancestral inference to return a joint\n        distribution of the sequence states at both ends of the branch.\n\n        Parameters\n        ----------\n        node : Phylo.clade\n            node of the tree\n        full_sequence : bool, optional\n            expand the sequence to the full sequence, if false (default)\n            the there will be one mutation matrix for each column in the\n            reduced alignment\n\n        Returns\n        -------\n        numpy.array\n            an Lxqxq stack of matrices (q=alphabet size, L (reduced)sequence length)\n        \"\"\"", "output": "\n        branch_length = self.branch_length(node)\n        branch_lengths = [branch_length] * len(node.clades)\n        branch_lengths.append(branch_length)\n        branch_lengths = np.array(branch_lengths)\n        branch_lengths = branch_lengths.reshape((branch_lengths.shape[0], 1))\n\n        branch_lengths_stack = np.hstack(branch_lengths)\n        branch_lengths_stack = branch_lengths_stack.reshape((branch_lengths_stack.shape[0], branch_lengths_stack.shape[1]))\n\n        branch_lengths_stack = branch_lengths_stack.reshape((branch_lengths_stack.shape[0], branch_lengths_stack.shape[1], 1))\n\n        branch_lengths_stack = branch_lengths_stack.reshape((branch_", "solution": "\n        pp,pc = self.marginal_branch_profile(node)\n\n        # calculate pc_i [e^Qt]_ij pp_j for each site\n        expQt = self.gtr.expQt(self._branch_length_to_gtr(node))\n        if len(expQt.shape)==3: # site specific model\n            mut_matrix_stack = np.einsum('ai,aj,ija->aij', pc, pp, expQt)\n        else:\n            mut_matrix_stack = np.einsum('ai,aj,ij->aij', pc, pp, expQt)\n\n        # normalize this distribution\n        normalizer = mut_matrix_stack.sum(axis=2).sum(axis=1)\n        mut_matrix_stack = np.einsum('aij,a->aij', mut_matrix_stack, 1.0/normalizer)\n\n        # expand to full sequence if requested\n        if full_sequence:\n            return mut_matrix_stack[self.full_to_reduced_sequence_map]\n        else:\n            return mut_matrix_stack"}
{"prompt": "def meas_gate(self, circuit, qreg, op):\n        \"\"\"\n        Add measurement gates to a circuit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n            qreg (tuple(QuantumRegister,int)): quantum register being measured.\n            op (str): the basis label for the measurement.\n        \"\"\"", "output": "\n        circuit.append(Measurement(op[0], qreg))\n\n    def measure_all(self, circuit):\n        \"\"\"\n        Add measurement to all qubits in a circuit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n        \"\"\"\n        for qreg in circuit.qregs:\n            self.measure(qreg, qreg)\n\n    def measure_all_borrowed(self, circuit):\n        \"\"\"\n        Add measurement to all qubits in a circuit that are not the target qubit.\n\n        Args:\n            circuit (QuantumCircuit): circuit to add measurement to.\n        \"\"\"\n        for qreg in circuit.qregs:\n            self.", "solution": "\n        if self.meas_fun is None:\n            pass\n        else:\n            self.meas_fun(circuit, qreg, op)"}
{"prompt": "def log(wave):\n    r\"\"\"\n    Return the natural logarithm of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for peng.wave_functions.log\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "\n    # Get the waveform and check the domain of the waveform\n    wave = wave.waveform\n    wave_domain = wave.domain\n\n    # Get the waveform domain and check the domain of the waveform\n    wave_domain_domain = wave_domain.domain\n\n    # Get the waveform domain and check the domain of the waveform\n    wave_domain_domain_domain = wave_domain_domain.domain\n\n    # Get the waveform domain and check the domain of the waveform\n    wave_domain_domain_domain_domain = wave_domain_domain_domain.domain\n\n    # Get the waveform domain and check the domain of the waveform\n    wave_domain_domain_domain_domain_domain = wave_domain_domain_domain_domain.domain\n\n    # Get the waveform domain and check the domain of the ", "solution": "\n    pexdoc.exh.addex(\n        ValueError, \"Math domain error\", bool((min(wave._dep_vector) <= 0))\n    )\n    return _operation(wave, \"log\", \"\", np.log)"}
{"prompt": "async def save_tracks(self, *tracks):\n        \"\"\"Save one or more tracks to the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track, str]]\n            A sequence of track objects or spotify IDs\n        \"\"\"", "output": "\n        await self.client.library.save_tracks(tracks)\n\n    async def delete_tracks(self, *tracks):\n        \"\"\"Delete one or more tracks from the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tracks : Sequence[Union[Track, str]]\n            A sequence of track objects or spotify IDs\n        \"\"\"\n        await self.client.library.delete_tracks(tracks)\n\n    async def add_tags(self, *tags):\n        \"\"\"Add one or more tags to the current user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        tags : Sequence[Union[Tag, str]]\n            A sequence of tag objects or spotify IDs\n        \"\"\"\n        await self.client.library.add_tags(tags)\n\n    async def remove_tags(self, *tags):", "solution": "\n        _tracks = [(obj if isinstance(obj, str) else obj.id) for obj in tracks]\n        await self.user.http.save_tracks(','.join(_tracks))"}
{"prompt": "def get_logging_level():\n    \"\"\"get_logging_level will configure a logging to standard out based on the user's\n    selected level, which should be in an environment variable called\n    MESSAGELEVEL. if MESSAGELEVEL is not set, the maximum level\n    (5) is assumed (all messages).\n    \"\"\"", "output": "\n    level = None\n    if os.environ.get(\"MESSAGELEVEL\"):\n        level = os.environ.get(\"MESSAGELEVEL\")\n    elif os.environ.get(\"LOGLEVEL\"):\n        level = os.environ.get(\"LOGLEVEL\")\n    else:\n        level = 5\n    return level\n\n\ndef setup_logging(logging_level=None):\n    \"\"\"setup_logging will configure a logging to a specified level. if logging_level\n    is None, the maximum level (5) is assumed (all messages).\n    \"\"\"\n    level = setup_logging_level(logging_level)\n    logging.basicConfig(level=level, format=\"%(asctime)s %(levelname)s %(message)s\")\n\n\ndef setup_logging_level(logging_level):\n    \"\"\"setup_logging_level will configure a logging to a specified level. if logging_", "solution": "\n    level = os.environ.get(\"MESSAGELEVEL\", INFO)\n\n    # User knows logging levels and set one\n    if isinstance(level, int):\n        return level\n\n    # Otherwise it's a string\n    if level == \"CRITICAL\":\n        return CRITICAL\n    elif level == \"ABORT\":\n        return ABORT\n    elif level == \"ERROR\":\n        return ERROR\n    elif level == \"WARNING\":\n        return WARNING\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"INFO\":\n        return INFO\n    elif level == \"QUIET\":\n        return QUIET\n    elif level.startswith(\"VERBOSE\"):\n        return VERBOSE3\n    elif level == \"LOG\":\n        return LOG\n    elif level == \"DEBUG\":\n        return DEBUG\n\n    return level"}
{"prompt": "def format_task_numbers_with_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"", "output": "\n    task_numbers = []\n    for task in tasks:\n        task_numbers.append(task.number)\n        if task.link:\n            task_numbers.append(task.link)\n    return \", \".join(task_numbers)\n\n\ndef format_task_numbers_without_links(tasks):\n    \"\"\"Returns formatting for the tasks section of asana.\"\"\"\n    task_numbers = []\n    for task in tasks:\n        task_numbers.append(task.number)\n    return \", \".join(task_numbers)\n\n\ndef format_status_with_links(status):\n    \"\"\"Returns formatting for the status section of asana.\"\"\"\n    status_numbers = []\n    for status in status:\n        status_numbers.append(status.number)\n        if status.link:\n            status_numbers.append(status.", "solution": "\n\n    project_id = data.get('asana-project', None)\n\n    def _task_format(task_id):\n        if project_id:\n            asana_url = tool.ToolApp.make_asana_url(project_id, task_id)\n            return \"[#%d](%s)\" % (task_id, asana_url)\n        else:\n            return \"#%d\" % task_id\n\n    return \"\\n\".join([_task_format(tid) for tid in tasks])"}
{"prompt": "def _mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"", "output": "\n    return self._mode\n\n  def mode(self):\n    \"\"\"The mode of the von Mises-Fisher distribution is the mean direction.\"\"\"\n    return self._mode\n\n  def modes(self):\n    \"\"\"The modes of the von Mises-Fisher distribution.\"\"\"\n    return self._modes\n\n  def modes_squared(self):\n    \"\"\"The modes of the von Mises-Fisher distribution squared\"\"\"\n    return self._modes_squared\n\n  def modes_squared(self):\n    \"\"\"The modes of the von Mises-Fisher distribution squared\"\"\"\n    return self._modes_squared\n\n  def modes_squared(self):\n    \"\"\"The modes of the von Mises-Fisher distribution squ", "solution": "\n    return (self.mean_direction +\n            tf.zeros_like(self.concentration)[..., tf.newaxis])"}
{"prompt": "def is_detached(self):\n        \"\"\"Check if the repo is in a detached state.\n\n        The repository is in a detached state when HEAD is not a symbolic\n        reference.\n\n        :returns: whether the repository is detached or not\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"", "output": "\n        head = self.head\n        if head is None:\n            return False\n        if head.commit:\n            return False\n        if head.commit.tree:\n            return False\n        if head.commit.treeish:\n            return False\n        return True\n\n    def head(self, refspec=None):\n        \"\"\"Get the head commit of the repository.\n\n        :param refspec: optional refspec to restrict the head to\n        :returns: the head commit of the repository\n\n        :raises RepositoryError: when an error occurs checking the state\n            of the repository\n        \"\"\"\n        head = self.head\n        if head is None:\n            raise RepositoryError(\n                \"Cannot get head of detached repository\")\n        if refspec:\n            head = self.heads.get(refspec)\n", "solution": "\n        cmd_sym = ['git', 'symbolic-ref', 'HEAD']\n\n        try:\n            self._exec(cmd_sym, cwd=self.dirpath, env=self.gitenv)\n        except RepositoryError as e:\n            if e.msg.find(\"ref HEAD is not a symbolic ref\") == -1:\n                raise e\n            return True\n        else:\n            return False"}
{"prompt": "def publish_events(\n            self, topic_hostname, events, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Publishes a batch of events to an Azure Event Grid topic.\n\n        :param topic_hostname: The host name of the topic, e.g.\n         topic1.westus2-1.eventgrid.azure.net\n        :type topic_hostname: str\n        :param events: An array of events to be published to Event Grid.\n        :type events: list[~azure.eventgrid.models.EventGridEvent]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`\n        \"\"\"", "output": "rest:optionsforoperations>`.\n        :return: None or ClientRawResponse if raw=true\n        :rtype: None or ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`EventGridErrorException<azure.eventgrid.models.EventGridErrorException>`\n        \"\"\"\n        events_object = models.EventGridEventBatch(\n            events=events, custom_headers=custom_headers,\n        )\n\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname,'str'),\n           'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id,'str')", "solution": "\n        # Construct URL\n        url = self.publish_events.metadata['url']\n        path_format_arguments = {\n            'topicHostname': self._serialize.url(\"topic_hostname\", topic_hostname, 'str', skip_quote=True)\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if custom_headers:\n            header_parameters.update(custom_headers)\n\n        # Construct body\n        body_content = self._serialize.body(events, '[EventGridEvent]')\n\n        # Construct and send request\n        request = self._client.post(url, query_parameters)\n        response = self._client.send(\n            request, header_parameters, body_content, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise HttpOperationError(self._deserialize, response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(None, response)\n            return client_raw_response"}
{"prompt": "def get_imap_capabilities(server):\n    \"\"\"\n    Returns a list of an IMAP server's capabilities\n\n    Args:\n        server (imapclient.IMAPClient): An instance of imapclient.IMAPClient\n\n    Returns (list): A list of capabilities\n    \"\"\"", "output": "\n    capabilities = []\n    capabilities.append(('AUTHENTICATE', server.AUTHENTICATE))\n    capabilities.append(('CAPABILITY', server.CAPABILITY))\n    capabilities.append(('CLOSE', server.CLOSE))\n    capabilities.append(('COPY', server.COPY))\n    capabilities.append(('CREDENTIALS', server.CREDENTIALS))\n    capabilities.append(('DELETE', server.DELETE))\n    capabilities.append(('ENABLE', server.ENABLE))\n    capabilities.append(('EXPUNGE', server.EXPUNGE))\n    capabilities.append(('FETCH', server.F", "solution": "\n\n    capabilities = list(map(str, list(server.capabilities())))\n    for i in range(len(capabilities)):\n        capabilities[i] = str(capabilities[i]).replace(\"b'\",\n                                                       \"\").replace(\"'\",\n                                                                   \"\")\n    logger.debug(\"IMAP server supports: {0}\".format(capabilities))\n\n    return capabilities"}
{"prompt": "def update_path(self, board, color, path):\n        \"\"\" update win/loss count along path \"\"\"", "output": "\n        wins = 0\n        losses = 0\n        for move in path:\n            move = move.strip()\n            move = move.split()\n            move = move[0]\n            move = move[1:]\n            move = move.lower()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move.upper()\n            move = move", "solution": "\n        wins = board.score(BLACK) >= board.score(WHITE)\n        for node in path:\n            if color == BLACK:\n                color = WHITE\n            else:\n                color = BLACK\n            if wins == (color == BLACK):\n                node.wins += 1\n            else:\n                node.losses += 1\n            if node.parent:\n                node.parent.bestchild = node.parent.best_child()"}
{"prompt": "def read(self):\n        \"\"\"\n        Load the metrics file from the given path\n        \"\"\"", "output": "\n        metrics = Metrics()\n        metrics.load(path)\n        return metrics\n\n    def dump(self, metrics):\n        \"\"\"\n        Dump the metrics file to the given path\n        \"\"\"\n        metrics.save(path)\n\n    def dump_to_file(self, metrics):\n        \"\"\"\n        Dump the metrics file to the given path\n        \"\"\"\n        metrics.dump_to_file(path)\n\n    def dump_to_json(self, metrics):\n        \"\"\"\n        Dump the metrics file to the given path\n        \"\"\"\n        metrics.dump_to_json(path)\n\n    def dump_to_yaml(self, metrics):\n        \"\"\"\n        Dump the metrics file to the given path\n        \"\"\"\n        metrics.dump_to_yaml(path)\n\n    def ", "solution": "\n        f = open(self.path, \"r\")\n        self.manifest_json = f.read()"}
{"prompt": "def update(self, teamId, name=None, **request_parameters):\n        \"\"\"Update details for a team, by ID.\n\n        Args:\n            teamId(basestring): The team ID.\n            name(basestring): A user-friendly name for the team.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            Team: A Team object with the updated Webex Teams team details.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        check_type(teamId, basestring,\n                   may_be_none=False)\n        check_type(name, 'basestring', may_be_none=False)\n        check_type(request_parameters, dict)\n\n        resource_path = '/v2/teams/{teamId}'\n        path_params = {}\n        if teamId is not None:\n            path_params['teamId'] = teamId\n        if name is not None:\n            path_params['name'] = name\n\n        query_params = {}\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.\\\n            select_header_accept(['application/json'])\n        if not header_params['Accept']:\n", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(name, basestring)\n\n        put_data = dict_from_items_with_values(\n            request_parameters,\n            name=name,\n        )\n\n        # API request\n        json_data = self._session.put(API_ENDPOINT + '/' + teamId,\n                                      json=put_data)\n\n        # Return a team object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def _odds_val(self):\n        \"\"\"Determine a new random value derived from the\n        defined :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The derived value\n        \"\"\"", "output": "\n        return self.odds\n\n    @_odds_val.setter\n    def _odds_val(self, value):\n        \"\"\"Set the value of a field based on its :any:`gramfuzz.fields.Field.odds` value.\n\n        :param value: The new value\n        \"\"\"\n        self.odds = value\n\n    @property\n    def odds_val(self):\n        \"\"\"Get the value of a field based on its :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The value\n        \"\"\"\n        return self._odds_val()\n\n    @property\n    def odds_str(self):\n        \"\"\"Get the value of a field based on its :any:`gramfuzz.fields.Field.odds` value.\n\n        :returns: The value\n        \"\"\"\n        return str(self.odds)\n\n    @property", "solution": "\n        if len(self.odds) == 0:\n            self.odds = [(1.00, [self.min, self.max])]\n\n        rand_val = rand.random()\n        total = 0\n        for percent,v in self.odds:\n            if total <= rand_val < total+percent:\n                found_v = v\n                break\n            total += percent\n\n        res = None\n        if isinstance(v, (tuple,list)):\n            rand_func = rand.randfloat if type(v[0]) is float else rand.randint\n\n            if len(v) == 2:\n                res = rand_func(v[0], v[1])\n            elif len(v) == 1:\n                res = v[0]\n        else:\n            res = v\n\n        return res"}
{"prompt": "def _check_key_value_types(obj, key_type, value_type, key_check=isinstance, value_check=isinstance):\n    \"\"\"Ensures argument obj is a dictionary, and enforces that the keys/values conform to the types\n    specified by key_type, value_type.\n    \"\"\"", "output": "\n    if not isinstance(obj, dict):\n        raise TypeError(\n            \"Argument obj must be a dictionary, but instead is of type {}\".format(type(obj))\n        )\n    for key in obj:\n        if not isinstance(key, key_type):\n            raise TypeError(\n                \"Argument key must be a {}, but instead is of type {}\".format(key_type, type(key))\n            )\n        if not isinstance(obj[key], value_type):\n            raise TypeError(\n                \"Argument value must be a {}, but instead is of type {}\".format(value_type, type(obj[key]))\n            )\n        if key_check(obj[key])!= value_type:\n            raise ValueError(\n                \"Argument key must be a {}, but instead is of type {}\".format(value_type, type(obj[key]))\n            )\n\n\ndef _check_key_value_pairs(obj, key_type, value_type, key_", "solution": "\n    if not isinstance(obj, dict):\n        raise_with_traceback(_type_mismatch_error(obj, dict))\n\n    if key_type is str:\n        key_type = string_types\n\n    if value_type is str:\n        value_type = string_types\n\n    for key, value in obj.items():\n        if key_type and not key_check(key, key_type):\n            raise_with_traceback(\n                CheckError(\n                    'Key in dictionary mismatches type. Expected {key_type}. Got {obj_repr}'.format(\n                        key_type=repr(key_type), obj_repr=repr(key)\n                    )\n                )\n            )\n        if value_type and not value_check(value, value_type):\n            raise_with_traceback(\n                CheckError(\n                    'Value in dictionary mismatches expected type for key {key}. Expected value '\n                    'of type {vtype}. Got value {value} of type {obj_type}.'.format(\n                        vtype=repr(value_type), obj_type=type(value), key=key, value=value\n                    )\n                )\n            )\n    return obj"}
{"prompt": "def _clear_temp(self):\n        \"\"\"\n        Clear temp directory from created csv and ods files during\n        communicator operations.\n        \"\"\"", "output": "\n        self._clear_temp_files()\n        self._clear_temp_files()\n\n    def _clear_temp_files(self):\n        \"\"\"\n        Clear temp directory from created csv and ods files during\n        communicator operations.\n        \"\"\"\n        shutil.rmtree(self._temp_dir)\n\n    def _write_csv(self, csv_file):\n        \"\"\"\n        Write csv data to csv file and create ods file.\n        \"\"\"\n        csv_file.seek(0)\n        csv_writer = csv.writer(csv_file, dialect='excel')\n        csv_writer.writerows(self._csv_data)\n\n        ods_file = open(self._ods_filename, \"wb\")\n        ods_writer = csv.writer(ods_file, dialect='excel')\n        ods_", "solution": "\n        temp_files = [LOCAL_ODS, GDOCS_TRANS_CSV, GDOCS_META_CSV,\n                      LOCAL_TRANS_CSV, LOCAL_META_CSV]\n        for temp_file in temp_files:\n            file_path = os.path.join(self.temp_path, temp_file)\n            if os.path.exists(file_path):\n                os.remove(file_path)"}
{"prompt": "def to_zyz(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of ZYZ Euler angles.\n\n        Returns:\n            ndarray: Array of Euler angles.\n        \"\"\"", "output": "\n        angles = self.to_euler()\n        angles = angles.reshape((3, 3))\n        return angles\n\n    def to_euler(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of Euler angles.\n\n        Returns:\n            ndarray: Array of Euler angles.\n        \"\"\"\n        angles = self.to_quaternion()\n        angles = angles.reshape((3, 3))\n        return angles\n\n    def to_quaternion(self):\n        \"\"\"Converts a unit-length quaternion to a sequence\n        of quaternions.\n\n        Returns:\n            ndarray: Array of quaternions.\n        \"\"\"\n        angles = self.to_euler()\n        angles = angles.reshape((", "solution": "\n        mat = self.to_matrix()\n        euler = np.zeros(3, dtype=float)\n        if mat[2, 2] < 1:\n            if mat[2, 2] > -1:\n                euler[0] = math.atan2(mat[1, 2], mat[0, 2])\n                euler[1] = math.acos(mat[2, 2])\n                euler[2] = math.atan2(mat[2, 1], -mat[2, 0])\n            else:\n                euler[0] = -math.atan2(mat[1, 0], mat[1, 1])\n                euler[1] = np.pi\n        else:\n            euler[0] = math.atan2(mat[1, 0], mat[1, 1])\n        return euler"}
{"prompt": "def export_variants(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Yields:\n        variant_obj(scout.Models.Variant): Variants marked as causative ordered by position.\n    \"\"\"", "output": "\n    variant_query = {\n        'collaborator': collaborator,\n        'document_id': document_id,\n        'case_id': case_id,\n    }\n    variant_collection = adapter.db.variants.find(variant_query)\n    for variant_obj in variant_collection:\n        variant_obj.export_causative_variants(adapter)\n\n\ndef export_variants_as_json(adapter, collaborator, document_id=None, case_id=None):\n    \"\"\"Export causative variants for a collaborator\n\n    Args:\n        adapter(MongoAdapter)\n        collaborator(str)\n        document_id(str): Search for a specific variant\n        case_id(str): Search causative variants for a case\n\n    Returns:\n        variant_", "solution": "\n\n    # Store the variants in a list for sorting\n    variants = []\n    if document_id:\n        yield adapter.variant(document_id)\n        return\n\n    variant_ids = adapter.get_causatives(\n        institute_id=collaborator,\n        case_id=case_id\n        )\n    ##TODO add check so that same variant is not included more than once\n    for document_id in variant_ids:\n\n        variant_obj = adapter.variant(document_id)\n        chrom = variant_obj['chromosome']\n        # Convert chromosome to integer for sorting\n        chrom_int = CHROMOSOME_INTEGERS.get(chrom)\n        if not chrom_int:\n            LOG.info(\"Unknown chromosome %s\", chrom)\n            continue\n\n        # Add chromosome and position to prepare for sorting\n        variants.append((chrom_int, variant_obj['position'], variant_obj))\n\n    # Sort varants based on position\n    variants.sort(key=lambda x: (x[0], x[1]))\n\n    for variant in variants:\n        variant_obj = variant[2]\n        yield variant_obj"}
{"prompt": "def delete_uneeded(self):\n        \"\"\"\n        Delete the directory which are not registered into our structure.\n        \"\"\"", "output": "\n        for path in self.uneeded_dirs:\n            try:\n                os.remove(path)\n            except OSError:\n                pass\n\n    def register_uneeded_dir(self, path, is_dir=True):\n        \"\"\"\n        Register a directory into our structure.\n\n        :param path: Path of the directory to register.\n        :param is_dir: Flag to indicate if the directory is a directory.\n        \"\"\"\n        if is_dir:\n            self.uneeded_dirs.append(path)\n        else:\n            self.uneeded_dirs.remove(path)\n\n    def register_uneeded_file(self, path, is_file=True):\n        \"\"\"\n        Register a file into our structure.\n\n        :param path: Path of the file to register.\n        :param is_file: Flag to indicate if the file is a file.\n        \"\"\"\n", "solution": "\n\n        # We get the structure we have to apply.\n        structure = self._get_structure()\n\n        # We get the list of key which is implicitly the list of directory we do not bave to delete.\n        list_of_key = list(structure.keys())\n\n        # We move to the content of the parent as we know that we are creating only one directory.\n        # Note: if one day we will have to create multiple directory, we will have to change\n        # the following.\n        structure = structure[list_of_key[0]]\n\n        # We also set the parent directory as we are going to construct its childen.\n        parent_path = list_of_key[0]\n\n        if not parent_path.endswith(PyFunceble.directory_separator):\n            parent_path += PyFunceble.directory_separator\n\n        for root, _, _ in PyFunceble.walk(parent_path):\n            # We loop through each directories of the parent path.\n\n            # We fix the path in order to avoid issues.\n            root = Directory(root).fix_path()\n\n            if root.replace(parent_path, \"\") not in structure:\n                # The currently read directory is not in our structure.\n\n                # We delete it.\n                PyFunceble.rmtree(root)"}
{"prompt": "def is_effective_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"", "output": "\n        members = self.groups[group_id]\n        return netid in members\n\n    def has_member(self, group_id, netid):\n        \"\"\"\n        Returns True if the netid is in the group, False otherwise.\n        \"\"\"\n        members = self.groups[group_id]\n        return netid in members\n\n    def get_members(self, group_id):\n        \"\"\"\n        Returns the list of netids in the group.\n        \"\"\"\n        members = self.groups[group_id]\n        return members\n\n    def get_members_count(self, group_id):\n        \"\"\"\n        Returns the number of members in the group.\n        \"\"\"\n        return len(self.groups[group_id])\n\n    def get_member_count(self, group_id):\n        \"\"\"\n        Returns the number of members in the group.\n        \"\"\"\n        return len(self.groups[group_id])\n\n    def get_", "solution": "\n        self._valid_group_id(group_id)\n\n        # GWS doesn't accept EPPNs on effective member checks, for UW users\n        netid = re.sub('@washington.edu', '', netid)\n\n        url = \"{}/group/{}/effective_member/{}\".format(self.API,\n                                                       group_id,\n                                                       netid)\n\n        try:\n            data = self._get_resource(url)\n            return True  # 200\n        except DataFailureException as ex:\n            if ex.status == 404:\n                return False\n            else:\n                raise"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for dirpath, dirnames, filenames in os.walk(rootpath):\r\n        # ignore hidden directories\r\n        dirnames[:] = [d for d in dirnames if not d.startswith('.')]\r\n        filenames[:] = [f for f in filenames if not f.startswith('.')]\r\n        for filename in filenames:\r\n            path = os.path.join(dirpath, filename)\r\n            if skip is not None and filename in skip:\r\n                continue\r\n            ext = os.path.splitext(path)[1]\r\n            if ext not in SKIP_EXTS:\r\n                modules[path] = path\r\n    return modules\r\n\r\n\r\ndef import_module(modname, path=None):\r\n    \"\"\"\r\n    Import a module from path.\r\n    \"\"\"\r\n    if path is None:\r\n        path = sys.path\r\n    modpath = path\r\n    if modpath", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def Rock(*args, **kwargs):\n    \"\"\"\n    Graceful deprecation for old class name.\n    \"\"\"", "output": "\n    warnings.warn(\"The Rock class has been renamed to RockSample.\", DeprecationWarning, stacklevel=2)\n    return RockSample(*args, **kwargs)\n\n\nclass RockSample(Sample):\n    \"\"\"\n    Sample class with Rock properties.\n    \"\"\"\n    rock = Rock\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize a sample with Rock properties.\n        \"\"\"\n        super(RockSample, self).__init__(*args, **kwargs)\n        self.rock.rock = self\n\n    def draw(self):\n        \"\"\"\n        Draw the sample with Rock properties.\n        \"\"\"\n        self.rock.draw()\n\n\nclass RockSample2(Sample):\n    \"\"\"\n    Sample class with Rock properties.\n    \"\"\"\n    ", "solution": "\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        w = \"The 'Rock' class was renamed 'Component'. \"\n        w += \"Please update your code.\"\n        warnings.warn(w, DeprecationWarning, stacklevel=2)\n\n    return Component(*args, **kwargs)"}
{"prompt": "def metadata(self):\n        \"\"\"Get metadata information in XML format.\"\"\"", "output": "\n        xml = etree.Element('metadata')\n        root = etree.Element('metadata', xmlns=NS_XML)\n        root.append(etree.Element('key', name=self.name,\n                                    value=self.value))\n        return xml\n\n    def metadata_get(self):\n        \"\"\"Get metadata information in XML format.\"\"\"\n        return self.metadata()\n\n    def metadata_update(self, metadata):\n        \"\"\"Update metadata information.\"\"\"\n        metadata = etree.fromstring(metadata)\n        metadata_root = etree.Element('metadata', xmlns=NS_XML)\n        for key, value in metadata.items():\n            metadata_root.append(etree.Element('key', name=key,\n                                               value=value))\n        self.append(metadata_root)\n\n    def metadata_add(self, metadata):\n        \"\"\"", "solution": "\n\n        params = {\n            self.PCTYPE: self.CTYPE_XML\n        }\n\n        response = self.call(self.CGI_BUG, params)\n\n        return response"}
{"prompt": "def put(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a PUT request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"", "output": "\n        thing_id = int(thing_id)\n        action_name = self.action_name(action_name)\n        action_id = self.action_id(action_id)\n\n        thing = self.thing(thing_id)\n        thing.put(action_name, action_id)\n\n    def put(self, thing_id='0', action_name=None, action_id=None):\n        \"\"\"\n        Handle a PUT request.\n\n        TODO: this is not yet defined in the spec\n\n        thing_id -- ID of the thing this request is for\n        action_name -- name of the action from the URL path\n        action_id -- the action ID from the URL path\n        \"\"\"\n        thing_id = int(thing_id)\n        action_name = self.action_name(action_name)\n        action_id = self.action_id(action_id)", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        self.set_status(200)"}
{"prompt": "def safe_infer(\n    node: astroid.node_classes.NodeNG, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"", "output": "\n    try:\n        inferred = next(node.infer(context=context))\n    except astroid.InferenceError:\n        return None\n    return inferred\n\n\ndef infer_call_arguments(\n    call: astroid.node_classes.Call, context=None\n) -> Optional[astroid.node_classes.NodeNG]:\n    \"\"\"Return the inferred arguments for the given call.\n\n    Return None if inference failed or if there is some ambiguity (more\n    than one node has been inferred).\n    \"\"\"\n    try:\n        inferred = next(call.infer_call_arguments(context=context))\n    except astroid.InferenceError:\n        return None\n    return inferred\n\n\ndef infer_call_returns(\n    call", "solution": "\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value"}
{"prompt": "def IsNotNone(*fields, default=None):\n    \"\"\"Selects whichever field is not None, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are None.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are None.\n    \"\"\"", "output": "\n    return CaseWhen(\n        IsNone(fields),\n        default=default,\n        otherwise=DefaultIfEmpty(fields),\n    )\n\n\ndef IsNotEmpty(*fields, default=None):\n    \"\"\"Selects whichever field is not empty, in the specified order.\n\n    Arguments:\n        fields:\n            The fields to attempt to get a value from,\n            in order.\n\n        default:\n            The value to return in case all values are empty.\n\n    Returns:\n        A Case-When expression that tries each field and\n        returns the specified default value when all of\n        them are empty.\n    \"\"\"\n    return CaseWhen(\n        IsNotNone(fields),\n        default=default,\n        otherwise=Default", "solution": "\n\n    when_clauses = [\n        expressions.When(\n            ~expressions.Q(**{field: None}),\n            then=expressions.F(field)\n        )\n        for field in reversed(fields)\n    ]\n\n    return expressions.Case(\n        *when_clauses,\n        default=expressions.Value(default),\n        output_field=CharField()\n    )"}
{"prompt": "def authorize_redirect(self, oauth_scope, callback_uri=None,\n                           ax_attrs=[\"name\",\"email\",\"language\",\"username\"]):\n        \"\"\"Authenticates and authorizes for the given Google resource.\n\n        Some of the available resources are:\n\n        * Gmail Contacts - http://www.google.com/m8/feeds/\n        * Calendar - http://www.google.com/calendar/feeds/\n        * Finance - http://finance.google.com/finance/feeds/\n\n        You can authorize multiple resources by separating the resource\n        URLs with a space.\n        \"\"\"", "output": "\n        oauth_scope = self.oauth_scope(oauth_scope)\n        callback_uri = self.oauth_callback(callback_uri)\n        authorize_url = self.authorize_url(oauth_scope)\n        oauth_url = self.oauth_url(oauth_scope)\n\n        oauth_request = OAuthRequest.from_consumer_and_token(\n            self.consumer, token=oauth_token, http_method=http_method,\n            http_url=oauth_url, parameters=parameters)\n        oauth_request.sign_request(signature_method,\n                                   consumer, token)\n\n        oauth_url = oauth_url.replace('%oauth_token%', token)\n        oauth_url = oauth_url.replace('%oauth_callback%', callback_uri)\n        oauth_url = oauth_url.replace('%oauth_consumer_key%', self.consumer.key)\n", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs,\n                                 oauth_scope=oauth_scope)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def main(self,argv=None):\n        \"\"\"Run as a command-line script.\"\"\"", "output": "\n        parser = argparse.ArgumentParser(\n            description='Run an experiment with different parameters')\n        parser.add_argument('--experiment',\n                            type=str,\n                            help='The experiment to run')\n        parser.add_argument('--seed',\n                            type=int,\n                            help='The random seed for the experiment')\n        parser.add_argument('--gpu',\n                            type=int,\n                            help='The GPU ID to use')\n        parser.add_argument('--num_epochs',\n                            type=int,\n                            help='The number of epochs to run')\n        parser.add_argument('--batch_size',\n                            type=int,\n                            help='The batch size to use')\n        parser.add_argument('--learning_rate',\n                            type=float,\n                            help='The learning rate for the optimizer')\n        parser.add_argument('--momentum',\n                            type=float,\n                            help='The momentum value for the optimizer')\n        parser.add_argument('--weight_decay',\n                            type=float,\n                            help='", "solution": "\n\n        parser = optparse.OptionParser(usage=USAGE % self.__class__.__name__)\n        newopt = parser.add_option\n        newopt('-i','--interact',action='store_true',default=False,\n               help='Interact with the program after the script is run.')\n\n        opts,args = parser.parse_args(argv)\n\n        if len(args) != 1:\n            print >> sys.stderr,\"You must supply exactly one file to run.\"\n            sys.exit(1)\n\n        self.run_file(args[0],opts.interact)"}
{"prompt": "def init_s3_keys(opt):\n    \"\"\"Initialize s3 access keys from environment variable or s3cfg config file.\"\"\"", "output": "\n    access_key_id = opt.get('aws_access_key_id')\n    secret_access_key = opt.get('aws_secret_access_key')\n    bucket = opt.get('aws_bucket')\n    if not access_key_id or not secret_access_key or not bucket:\n        raise RuntimeError('aws_access_key_id, aws_secret_access_key, and aws_bucket must be set.')\n    access_key_id = access_key_id.strip()\n    secret_access_key = secret_access_key.strip()\n    bucket = bucket.strip()\n    access_key_id = access_key_id.replace('\"', '')\n    secret_access_key = secret_access_key.replace('\"', '')\n    access_key_id = access_key_id.strip()\n    secret_access_key = secret_access_key.strip()\n", "solution": "\n    S3Handler.S3_KEYS = S3Handler.s3_keys_from_cmdline(opt) or S3Handler.s3_keys_from_env() \\\n                        or S3Handler.s3_keys_from_s3cfg(opt)"}
{"prompt": "def _reshuffle(mat, shape):\n    \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\"", "output": "\n    mat = _ascontiguousarray(mat)\n    shape = tuple(shape)\n    mat.shape = shape\n    mat._shape = shape\n    mat._strides = (mat._shape[i] for i in (0, 1))\n    mat._data = mat._data.reshape(shape)\n    return mat\n\n\ndef _ascontiguousarray(arr):\n    \"\"\"\n    Convert an array to a contiguous array.\n\n    Parameters\n    ----------\n    arr : ndarray\n        Input array.\n\n    Returns\n    -------\n    arr : ndarray\n        Contiguous array.\n    \"\"\"\n    arr = arr.copy()\n    arr.flags.writeable = False\n    return arr\n\n\ndef _asarray_tuplesafe(arr):\n    \"\"\"", "solution": "\n    return np.reshape(\n        np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)),\n        (shape[3] * shape[1], shape[0] * shape[2]))"}
{"prompt": "def get_by_symbol_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"", "output": "\n        scope = self.scopes.get(name)\n        if scope is None:\n            raise KeyError(f\"No scope named {name}\")\n        return scope\n\n    def get_by_name(self, name: str) -> Scope:\n        \"\"\" Retrieve a Set of all signature by name \"\"\"\n        scope = self.scopes.get(name)\n        if scope is None:\n            raise KeyError(f\"No scope named {name}\")\n        return scope\n\n    def get_by_names(self, names: Iterable[str]) -> Scope:\n        \"\"\" Retrieve a Set of all signature by names \"\"\"\n        scope = self.scopes.get(names)\n        if scope is None:\n            raise KeyError(f\"No scope named {names}\")\n        return scope\n\n    def get_by_names_and_symbols(self, names: Iterable[str], symbol", "solution": "\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope"}
{"prompt": "def to_ndarray(self):\n        \"\"\"\n        Transfer JTensor to ndarray.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :return: a ndarray\n        \"\"\"", "output": "\n        return self.to_array()\n\n    def to_array(self):\n        \"\"\"\n        Transfer JTensor to ndarray.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :return: a ndarray\n        \"\"\"\n        arr = np.zeros(self.shape, dtype=np.int64)\n        arr[self.indices] = self.values\n        return arr\n\n    def to_indexed_slice(self, indices):\n        \"\"\"\n        Transfer JTensor to IndexedSlice.\n        As SparseTensor may generate an very big ndarray, so we don't support this function for SparseTensor.\n        :param indices: an IndexedSlice\n        :return: an IndexedSlice\n        \"\"\"\n        return IndexedSlice(self.indices, indices, self)\n\n", "solution": "\n        assert self.indices is None, \"sparseTensor to ndarray is not supported\"\n        return np.array(self.storage, dtype=get_dtype(self.bigdl_type)).reshape(self.shape)"}
{"prompt": "def deprecated(msg=''):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\n\n    :param msg:\n\n        Additional message added to the warning.\n\n    \"\"\"", "output": "\n    def outer(func):\n        def inner(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return inner\n    return outer\n\n\ndef require_args(*required_args):\n    \"\"\"This is a decorator which can be used to mark functions\n    as requiring one or more arguments. If the function does not\n    receive any arguments, a :exc:`ValueError` is raised.\n\n    :param required_args:\n\n        One or more required positional arguments.\n\n    \"\"\"\n    def outer(func):\n        def inner(*args, **kwargs):\n            if len(args) > len(required_args):\n                raise ValueError('Not enough arguments received')\n            for arg in required_args:\n                if arg not in kwargs:\n                    raise ValueError('Missing required argument: %s' % arg)\n            return func", "solution": "\n\n    def wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warning_string = \"Call to deprecated function or property `%s`.\" % func.__name__\n            warning_string = warning_string + ' ' + msg\n            warnings.warn(\n                warning_string,\n                category=DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return wrapper"}
{"prompt": "def uncache_zipdir(path):\n    \"\"\"Ensure that the importer caches dont have stale info for `path`\"\"\"", "output": "\n    zipdir = path.split(os.sep)\n    zipdir.sort()\n    zipdir.reverse()\n    zippath = os.path.join(*zipdir)\n    if zippath in sys.path_importer_cache:\n        del sys.path_importer_cache[zippath]\n    else:\n        sys.path.insert(0, zippath)\n\ndef fix_zipimporter_cache(path_importer_cache, path_importer_cache_only):\n    \"\"\"Ensure that importer caches dont have stale info for `path`\"\"\"\n    if path_importer_cache_only:\n        return\n    zipimporters = sys.path_importer_cache.copy()\n    _uncache(zipimporters)\n    zipimporters.clear()\n    zipimpor", "solution": "\n    from zipimport import _zip_directory_cache as zdc\n    _uncache(path, zdc)\n    _uncache(path, sys.path_importer_cache)"}
{"prompt": "def adjust_frame(self, pos, absolute_pos):\n        \"\"\"Adjust stack frame by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"", "output": "\n        if absolute_pos:\n            pos = abs(pos)\n        if pos < 0:\n            pos += self.stack_size\n        return pos\n\n    def adjust_stack(self, pos, absolute_pos):\n        \"\"\"Adjust stack by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"\n        if absolute_pos:\n            pos = abs(pos)\n        if pos < 0:\n            pos += self.stack_size\n        return pos\n\n    def adjust_locals(self, pos, absolute_pos):\n        \"\"\"Adjust locals by pos positions. If absolute_pos then\n        pos is an absolute number. Otherwise it is a relative number.\n\n        A negative number indexes from the other end.\"\"\"", "solution": "\n        if not self.curframe:\n            Mmsg.errmsg(self, \"No stack.\")\n            return\n\n        # Below we remove any negativity. At the end, pos will be\n        # the new value of self.curindex.\n        if absolute_pos:\n            if pos >= 0:\n                pos = len(self.stack)-pos-1\n            else:\n                pos = -pos-1\n        else:\n            pos += self.curindex\n\n        if pos < 0:\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the oldest frame.\")\n            return\n        elif pos >= len(self.stack):\n            Mmsg.errmsg(self,\n                        \"Adjusting would put us beyond the newest frame.\")\n            return\n\n        self.curindex = pos\n        self.curframe = self.stack[self.curindex][0]\n        self.print_location()\n        self.list_lineno = None\n        return"}
{"prompt": "def filter_unused_variable(line, previous_line=''):\n    \"\"\"Return line if used, otherwise return None.\"\"\"", "output": "\n    if line.find('unused-variable') >= 0:\n        if line == previous_line:\n            return None\n        else:\n            return line\n    else:\n        return line\n\n\ndef check_namespace_indentation(line):\n    \"\"\"Check line for namespace indentation.\"\"\"\n    if line.find('namespace ') >= 0:\n        error(line, 0, 'namespace should be indented with colons')\n\n\ndef check_namespace_indentation_end(line):\n    \"\"\"Check line for namespace indentation end.\"\"\"\n    if line.find('namespace ') >= 0:\n        error(line, 0, 'namespace should be indented with colons')\n\n\ndef check_namespace_indentation_before(line):\n    \"\"\"Check line for namespace indentation before.\"\"\"\n    if line.find('namespace ') >= 0:\n        error(", "solution": "\n    if re.match(EXCEPT_REGEX, line):\n        return re.sub(r' as \\w+:$', ':', line, count=1)\n    elif multiline_statement(line, previous_line):\n        return line\n    elif line.count('=') == 1:\n        split_line = line.split('=')\n        assert len(split_line) == 2\n        value = split_line[1].lstrip()\n        if ',' in split_line[0]:\n            return line\n\n        if is_literal_or_name(value):\n            # Rather than removing the line, replace with it \"pass\" to avoid\n            # a possible hanging block with no body.\n            value = 'pass' + get_line_ending(line)\n\n        return get_indentation(line) + value\n    else:\n        return line"}
{"prompt": "def initialize(cls):\n        \"\"\"\n        Creates an **\"Example\"** entity of kind **\"NDBConfig\"** in the\n        datastore if the model is empty and raises and error to inform you that\n        you should populate the model with data.\n\n        .. note::\n\n            The *Datastore Viewer* in the ``_ah/admin/`` won't let you add\n            properties to a model if there is not an entity with that\n            property already. Therefore it is a good idea to keep the\n            **\"Example\"** entity (which has all possible properties set) in\n            the datastore.\n\n        \"\"\"", "output": "\n        cls._kind = cls.kind()\n        cls._properties = {}\n        cls._properties[cls.KIND] = cls.kind()\n        cls._properties[cls.NAME] = cls.name()\n        cls._properties[cls.DESCRIPTION] = cls.description()\n        cls._properties[cls.CREATED] = cls.created()\n        cls._properties[cls.UPDATED] = cls.updated()\n        cls._properties[cls.KEY] = cls.key()\n        cls._properties[cls.VALUE] = cls.value()\n        cls._properties[cls.VALUETYPE] = cls.value_type()\n        cls._properties[cls.VALUEKIND] = cls.value_kind()\n        cls._properties[cls.VALUEKINDVALUE", "solution": "\n\n        if not len(cls.query().fetch()):\n\n            example = cls.get_or_insert('Example')\n\n            example.class_ = 'Provider class e.g. ' + \\\n                             '\"authomatic.providers.oauth2.Facebook\".'\n            example.provider_name = 'Your custom provider name e.g. \"fb\".'\n\n            # AuthorizationProvider\n            example.consumer_key = 'Consumer key.'\n            example.consumer_secret = 'Consumer secret'\n            example.provider_id = 1\n\n            # OAuth2\n            example.scope = 'coma, separated, list, of, scopes'\n\n            # AuthenticationProvider\n            example.identifier_param = 'Querystring parameter for claimed ' + \\\n                                       'id. default is \"id\"'\n\n            # Save the example\n            example.put()\n\n            # Raise an information error.\n            raise GAEError(\n                'A NDBConfig data model was created! Go to Datastore Viewer '\n                'in your dashboard and populate it with data!')"}
{"prompt": "def _config_session():\n        \"\"\"\n        Configure session for particular device\n\n        Returns:\n            tensorflow.Session\n        \"\"\"", "output": "\n        session = tf.Session()\n        session.run(tf.global_variables_initializer())\n        return session\n\n    def _config_model(self):\n        \"\"\"\n        Configure model for particular device\n\n        Returns:\n            tensorflow.Model\n        \"\"\"\n        model = tf.saved_model.load(\n            self.model_path,\n            tags=[tf.saved_model.tag_constants.SERVING],\n            signature_def_map={\n                tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n                    self.predict_signature\n            }\n        )\n        return model\n\n    def _config_serving_input_receiver_fn(self):\n        \"\"\"\n        Configure serving input receiver for particular device\n\n        Returns:\n            tensorflow.ServingInputReceiver\n        \"\"\"\n", "solution": "\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = '0'\n        return tf.Session(config=config)"}
{"prompt": "def _ensure_executor_running(self):\n        \"\"\"ensures all workers and management thread are running\n        \"\"\"", "output": "\n        executor = self.executor\n        if executor is None:\n            return\n        executor.shutdown()\n        executor.wait()\n\n    def shutdown(self, wait=True):\n        \"\"\"shutdown this worker\n        \"\"\"\n        self.shutdown_event.clear()\n        self.executor.shutdown(wait)\n        self.executor.wait()\n\n    def terminate(self):\n        \"\"\"terminate this worker\n        \"\"\"\n        self.shutdown_event.set()\n\n    def join(self):\n        \"\"\"wait until all workers are done\n        \"\"\"\n        self.shutdown_event.wait()\n\n    def terminate(self):\n        \"\"\"terminate this worker\n        \"\"\"\n        self.shutdown_event.set()\n\n    def join(self):\n        \"\"\"wait until all workers are done\n        \"\"\"\n        self.shutdown_event.wait()", "solution": "\n        with self._processes_management_lock:\n            if len(self._processes) != self._max_workers:\n                self._adjust_process_count()\n            self._start_queue_management_thread()"}
{"prompt": "def getPos(self):\n        \"\"\"\n        Returns the absolute position and size of the layer.\n        \n        This method is intended for use in vertex position calculation, as the border and offset have already been applied.\n        \n        The returned value is a 4-tuple of ``(sx,sy,ex,ey)``\\ .\n        The two values starting with an s are the \"start\" position, or the lower-left corner.\n        The second pair of values signify the \"end\" position, or upper-right corner.\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        # Returns sx,sy,ex,ey\n        # sx,sy are bottom-left/lowest\n        # ex,ey are top-right/highest\n        sx,sy = self.widget.pos[0]+self.border[0]+self.offset[0],                       self.widget.pos[1]+self.border[1]+self.offset[1]\n        ex,ey = self.widget.pos[0]+self.widget.size[0]-self.border[0]+self.offset[0],   self.widget.pos[1]+self.widget.size[1]-self.border[1]+self.offset[1]\n        return sx,sy,ex,ey"}
{"prompt": "def users(self):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"", "output": "\n        return self.users()\n\n    def users_info(self):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"\n        return self.users_info()\n\n    def users_info_by_id(self, user_id):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"\n        return self.users_info_by_id(user_id)\n\n    def users_info_by_name(self, name):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"\n        return self.users_info_by_name(name)\n\n    def users_info_by_email(self, email):\n        \"\"\"\n        List of users of this slack team\n        \"\"\"\n        return self.users_info_by_email(email)\n\n    def users_info_by_ids(self, user_ids):\n        \"\"\"\n        List of users of this slack ", "solution": "\n        if not self._users:\n            self._users = self._call_api('users.list')['members']\n        return self._users"}
{"prompt": "def decode_instruction(instruction):\n        \"\"\"\n        Decode whole instruction and return list of args.\n        Usually, returned arg[0] is the instruction opcode.\n\n        example:\n        >> args = decode_instruction('4.size,4.1024;')\n        >> args == ['size', '1024']\n        >> True\n\n        :param instruction: Instruction string.\n\n        :return: list\n        \"\"\"", "output": "\n        instruction = instruction.strip()\n        instruction = instruction.split()\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = instruction.split(';')\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = instruction.split(',')\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = instruction.split(';')\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = instruction.split(',')\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = instruction.split(';')\n        instruction = instruction[1:]\n        instruction = instruction[:-1]\n        instruction = ", "solution": "\n        if not instruction.endswith(INST_TERM):\n            raise InvalidInstruction('Instruction termination not found.')\n\n        # Use proper encoding\n        instruction = utf8(instruction)\n\n        # Get arg size\n        elems = instruction.split(ELEM_SEP, 1)\n\n        try:\n            arg_size = int(elems[0])\n        except Exception:\n            # Expected ValueError\n            raise InvalidInstruction(\n                'Invalid arg length.' +\n                ' Possibly due to missing element separator!')\n\n        arg_str = elems[1][:arg_size]\n\n        remaining = elems[1][arg_size:]\n\n        args = [arg_str]\n\n        if remaining.startswith(ARG_SEP):\n            # Ignore the ARG_SEP to parse next arg.\n            remaining = remaining[1:]\n        elif remaining == INST_TERM:\n            # This was the last arg!\n            return args\n        else:\n            # The remaining is neither starting with ARG_SEP nor INST_TERM.\n            raise InvalidInstruction(\n                'Instruction arg (%s) has invalid length.' % arg_str)\n\n        next_args = GuacamoleInstruction.decode_instruction(remaining)\n\n        if next_args:\n            args = args + next_args\n\n        return args"}
{"prompt": "def handle_combo(self,combo,symbol,modifiers,release=False,mod=True):\n        \"\"\"\n        Handles a key combination and dispatches associated events.\n        \n        First, all keybind handlers registered via :py:meth:`add` will be handled,\n        then the pyglet event :peng3d:pgevent:`on_key_combo` with params ``(combo,symbol,modifiers,release,mod)`` is sent to the :py:class:`Peng()` instance.\n        \n        Also sends the events :peng3d:event:`peng3d:keybind.combo`\\, :peng3d:event:`peng3d:keybind.combo.press` and :peng3d:event`peng3d:keybind.combo.release`\\ .\n        \n        :params str combo: Key combination pressed\n        :params int symbol: Key pressed, passed from the same argument within pyglet\n        :params int modifiers: Modifiers held while the key was pressed\n        :params bool release: If the combo was released\n        :params bool mod: If the combo was sent without mods\n        \"\"\"", "output": "d.symbol`\\, :peng3d:event:`peng3d:keybind.modifier`\\, :peng3d:event:`peng3d:keybind.release`\\, :peng3d:event:`peng3d:keybind.mod` as kwargs to the event handler.\n        \n        \n        :param combo: key combo string\n        :param symbol: key symbol\n        :param modifiers: key modifiers\n        :param release: release flag\n        :param mod: modifier flag\n        \"\"\"\n        \n        symbol = symbol.lower()\n        \n        if symbol == 'escape':\n            self.close()\n        elif symbol == 'backspace':\n            self.back", "solution": "\n        if self.peng.cfg[\"controls.keybinds.debug\"]:\n            print(\"combo: nm=%s %s\"%(mod,combo))\n        if mod:\n            for kbname in self.keybinds.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        else:\n            for kbname in self.keybinds_nm.get(combo,[]):\n                self.kbname[kbname](symbol,modifiers,release)\n        self.peng.sendPygletEvent(\"on_key_combo\",(combo,symbol,modifiers,release,mod))\n        self.peng.sendEvent(\"peng3d:keybind.combo\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        if release:\n            self.peng.sendEvent(\"peng3d:keybind.combo.release\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})\n        else:\n            self.peng.sendEvent(\"peng3d:keybind.combo.press\",{\"peng\":self.peng,\"combo\":combo,\"symbol\":symbol,\"modifiers\":modifiers,\"release\":release,\"mod\":mod})"}
{"prompt": "def expand_tokens(tokens, equal=False):\n    \"\"\"Given a list of tokens, return a generator of the chunks of\n    text for the data in the tokens.\n    \"\"\"", "output": "\n    chunks = []\n    for token in tokens:\n        if equal:\n            chunks.append(token)\n        else:\n            chunks.append(token.text)\n        yield chunks\n\n\ndef expand_chunks(chunks, equal=False):\n    \"\"\"Given a list of chunks, return a generator of the text\n    from the chunks.\n    \"\"\"\n    for chunk in chunks:\n        if equal:\n            yield chunk\n        else:\n            yield chunk.text\n\n\ndef expand_chunks_list(chunks_list, equal=False):\n    \"\"\"Given a list of chunks, return a generator of the text\n    from the chunks.\n    \"\"\"\n    for chunk in chunks_list:\n        if equal:\n            yield chunk\n        else:\n            yield chunk.text\n\n\ndef expand_chunks_", "solution": "\n    for token in tokens:\n        for pre in token.pre_tags:\n            yield pre\n        if not equal or not token.hide_when_equal:\n            if token.trailing_whitespace:\n                yield token.html() + token.trailing_whitespace\n            else:\n                yield token.html()\n        for post in token.post_tags:\n            yield post"}
{"prompt": "def replay_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use replay detection for messages protected by\n        :meth:`get_mic` and :meth:`wrap`. False if replay detection cannot be used.\n        \"\"\"", "output": "\n        return self._replay_detection\n\n    @property\n    def replay_detection(self):\n        \"\"\"\n        Whether replay detection is enabled.\n        \"\"\"\n        return self._replay_detection\n\n    @property\n    def replay_detection_enabled(self):\n        \"\"\"\n        Whether replay detection is enabled.\n        \"\"\"\n        return self._replay_detection\n\n    @property\n    def replay_detection_enabled(self):\n        \"\"\"\n        Whether replay detection is enabled.\n        \"\"\"\n        return self._replay_detection\n\n    @property\n    def replay_detection_enabled(self):\n        \"\"\"\n        Whether replay detection is enabled.\n        \"\"\"\n        return self._replay_detection\n\n    @property\n    def replay_detection_enabled(self):\n        \"\"\"\n        Whether replay detection is enabled.\n        \"\"\"\n        return self._replay_detection", "solution": "\n        return (\n            self.flags & C.GSS_C_REPLAY_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"", "output": "\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        layout.text(self.table_text)\n\n    def visit_table(self, layout):\n        \"\"\"display a table as text\"\"\"\n        ", "solution": "\n        table_content = self.get_table_content(layout)\n        # get columns width\n        cols_width = [0] * len(table_content[0])\n        for row in table_content:\n            for index, col in enumerate(row):\n                cols_width[index] = max(cols_width[index], len(col))\n        self.default_table(layout, table_content, cols_width)\n        self.writeln()"}
{"prompt": "def variants(context, collaborator, document_id, case_id, json):\n    \"\"\"Export causatives for a collaborator in .vcf format\"\"\"", "output": "\n    vcf_file = \"%s.vcf\" % case_id\n    vcf_file_format = \"vcf\"\n    vcf_file_extension = \".vcf\"\n    vcf_file_extension_case = \".vcf.gz\"\n    vcf_file_extension_cases = \".vcf.gz\"\n    vcf_file_extension_cases_case = \".vcf.gz\"\n    vcf_file_extension_cases_cases = \".vcf.gz\"\n    vcf_file_extension_cases_cases = \".vcf.gz\"\n    vcf_file_extension_cases_cases = \".vcf.gz\"\n    vcf_file_extension", "solution": "\n    LOG.info(\"Running scout export variants\")\n    adapter = context.obj['adapter']\n    collaborator = collaborator or 'cust000'\n\n    variants = export_variants(\n        adapter,\n        collaborator,\n        document_id=document_id,\n        case_id=case_id\n    )\n\n    if json:\n        click.echo(dumps([var for var in variants]))\n        return\n\n    vcf_header = VCF_HEADER\n\n    #If case_id is given, print more complete vcf entries, with INFO,\n    #and genotypes\n    if case_id:\n        vcf_header[-1] = vcf_header[-1] + \"\\tFORMAT\"\n        case_obj = adapter.case(case_id=case_id)\n        for individual in case_obj['individuals']:\n            vcf_header[-1] = vcf_header[-1] + \"\\t\" + individual['individual_id']\n\n    #print header\n    for line in vcf_header:\n        click.echo(line)\n\n    for variant_obj in variants:\n        variant_string = get_vcf_entry(variant_obj, case_id=case_id)\n        click.echo(variant_string)"}
{"prompt": "def _plot_formatting(title, est_file, algo_ids, last_bound, N, output_file):\n    \"\"\"Formats the plot with the correct axis labels, title, ticks, and\n    so on.\"\"\"", "output": "\n    \n    # Get the data\n    data, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _", "solution": "\n    import matplotlib.pyplot as plt\n    if title is None:\n        title = os.path.basename(est_file).split(\".\")[0]\n    plt.title(title)\n    plt.yticks(np.arange(0, 1, 1 / float(N)) + 1 / (float(N) * 2))\n    plt.gcf().subplots_adjust(bottom=0.22)\n    plt.gca().set_yticklabels(algo_ids)\n    plt.xlabel(\"Time (seconds)\")\n    plt.xlim((0, last_bound))\n    plt.tight_layout()\n    if output_file is not None:\n        plt.savefig(output_file)\n    plt.show()"}
{"prompt": "def f_remove(self, recursive=True, predicate=None):\n        \"\"\"Recursively removes the group and all it's children.\n\n        :param recursive:\n\n            If removal should be applied recursively. If not, node can only be removed\n            if it has no children.\n\n        :param predicate:\n\n            In case of recursive removal, you can selectively remove nodes in the tree.\n            Predicate which can evaluate for each node to ``True`` in order to remove the node or\n            ``False`` if the node should be kept. Leave ``None`` if you want to remove all nodes.\n\n        \"\"\"", "output": "\n        if recursive and self._children:\n            for child in self._children:\n                child._remove(recursive=recursive, predicate=predicate)\n        else:\n            if predicate is not None:\n                self._children = [child for child in self._children if predicate(child)]\n            else:\n                self._children = []\n\n    def _remove(self, recursive=True, predicate=None):\n        \"\"\"Recursively removes the group and all it's children.\n\n        :param recursive:\n\n            If removal should be applied recursively. If not, node can only be removed\n            if it has no children.\n\n        :param predicate:\n\n            In case of recursive removal, you can selectively remove nodes in the tree.\n            Predicate which can evaluate for each node to ``True`` in order to remove the node or", "solution": "\n        parent = self.f_get_parent()\n        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)"}
{"prompt": "def _wrap(text, wrap_at=120, indent=4):\n    \"\"\"\n    Return piece of text, wrapped around if needed.\n\n    :param text: text that may be too long and then needs to be wrapped.\n    :param wrap_at: the maximum line length.\n    :param indent: number of spaces to prepend to all subsequent lines after the first.\n    \"\"\"", "output": "\n    lines = text.splitlines()\n    if len(lines) >= wrap_at:\n        indent = indent *''\n        return '\\n'.join(\n            '%s%s%s' % (indent, line,'' * (wrap_at - len(line))) for line in lines\n        )\n    else:\n        return text\n\n\ndef format_dict(d, indent=0, wrap_at=120, sort_keys=True):\n    \"\"\"\n    Format a dictionary into a string.\n\n    :param d: dictionary to format.\n    :param indent: number of spaces to prepend to all subsequent lines after the first.\n    :param wrap_at: the maximum line length.\n    :param sort_keys: sort dictionary keys by their value.\n    \"\"\"\n    lines = []\n    if sort_keys:\n        d = sorted(d.items())\n    for k, v in d:\n", "solution": "\n    out = \"\"\n    curr_line_length = indent\n    space_needed = False\n    for word in text.split():\n        if curr_line_length + len(word) > wrap_at:\n            out += \"\\n\" + \" \" * indent\n            curr_line_length = indent\n            space_needed = False\n        if space_needed:\n            out += \" \"\n            curr_line_length += 1\n        out += word\n        curr_line_length += len(word)\n        space_needed = True\n    return out"}
{"prompt": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize payload of a HTTP request by removing the login, password and token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns url, headers and the sanitized payload\n        \"\"\"", "output": "\n        url, headers, payload = sanitize_url(url, headers, payload)\n        url, headers, payload = sanitize_for_archive_url(url, headers, payload)\n        url, headers, payload = sanitize_for_archive_headers(url, headers, payload)\n        url, headers, payload = sanitize_for_archive_payload(url, headers, payload)\n        url, headers, payload = sanitize_for_archive_headers_payload(url, headers, payload)\n        url, headers, payload = sanitize_for_archive_headers_payload(\n            url, headers, payload\n        )\n        url, headers, payload = sanitize_for_archive_headers_payload(\n            url, headers, payload\n        )\n        url, headers, payload = sanitize_for_archive_", "solution": "\n        if BugzillaRESTClient.PBUGZILLA_LOGIN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_LOGIN)\n\n        if BugzillaRESTClient.PBUGZILLA_PASSWORD in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_PASSWORD)\n\n        if BugzillaRESTClient.PBUGZILLA_TOKEN in payload:\n            payload.pop(BugzillaRESTClient.PBUGZILLA_TOKEN)\n\n        return url, headers, payload"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"", "output": "\n        if self.notifications:\n            notification = self.notifications[0]\n            self.notifications = []\n            self.send_notification(notification)\n\n    def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        if self.notifications:\n            notification = self.notifications[0]\n            self.notifications = []\n            self.send_notification(notification)\n\n    def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        if self.notifications:\n            notification = self.notifications[0]\n            self.notifications = []\n            self.send_notification(notification)\n\n    def cycle(self):\n        \"\"\"\n        Cycles through notifications with latest results from data feeds.\n        \"\"\"\n        if self.", "solution": "\n        messages = self.poll_datafeeds()\n        notifications = self.process_notifications(messages)\n\n        self.draw_notifications(notifications)"}
{"prompt": "def user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"", "output": "\n    return \"%s (%s)\" % (\n        platform.python_implementation(),\n        platform.platform(),\n        platform.python_version(),\n    )\n\n\ndef user_agent2():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    return \"%s (%s)\" % (\n        platform.python_implementation(),\n        platform.platform(),\n        platform.python_version(),\n    )\n\n\ndef user_agent_string():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    return user_agent() + \" Python/\" + platform.python_version()\n\n\ndef user_agent_string2():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    return user_agent() + \" Python/\" + platform.python_version() + \" on \" + platform.platform", "solution": "\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        distro = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], platform.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], platform.libc_ver()),\n        ))\n        if libc:\n            distro[\"libc\"] = libc\n        if distro:\n            data[\"distro\"] = distro\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"OS X\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )"}
{"prompt": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"", "output": "\n        self._headers_present = headers\n        return self\n\n    def headers_match(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n               .headers_match([('Content-Type', 'application/json'),\n                                ('Authorization', 'Basic dXNlcm5hbWU6cGFzc3dvcmQ='),\n                                ('", "solution": "\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))"}
{"prompt": "def resolve_path(self, path, root_id='me/skydrive', objects=False, listdir_limit=500):\n\t\t\"\"\"Return id (or metadata) of an object, specified by chain\n\t\t\t\t(iterable or fs-style path string) of \"name\" attributes\n\t\t\t\tof its ancestors, or raises DoesNotExists error.\n\n\t\t\tRequires many calls to resolve each name in path, so use with care.\n\t\t\t\troot_id parameter allows to specify path relative to some folder_id\n\t\t\t\t(default: me/skydrive).\"\"\"", "output": "\n\t\tpath = path.rstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.rstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.rstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = path.lstrip('/')\n\t\tpath = ", "solution": "\n\t\tif path:\n\t\t\tif isinstance(path, types.StringTypes):\n\t\t\t\tif not path.startswith('me/skydrive'):\n\t\t\t\t\t# Split path by both kinds of slashes\n\t\t\t\t\tpath = filter(None, it.chain.from_iterable(p.split('\\\\') for p in path.split('/')))\n\t\t\t\telse: root_id, path = path, None\n\t\t\tif path:\n\t\t\t\ttry:\n\t\t\t\t\tfor i, name in enumerate(path):\n\t\t\t\t\t\toffset = None\n\t\t\t\t\t\twhile True:\n\t\t\t\t\t\t\tobj_list = self.listdir(root_id, offset=offset, limit=listdir_limit)\n\t\t\t\t\t\t\ttry: root_id = dict(it.imap(op.itemgetter('name', 'id'), obj_list))[name]\n\t\t\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\t\t\tif len(obj_list) < listdir_limit: raise # assuming that it's the last page\n\t\t\t\t\t\t\t\toffset = (offset or 0) + listdir_limit\n\t\t\t\t\t\t\telse: break\n\t\t\t\texcept (KeyError, ProtocolError) as err:\n\t\t\t\t\tif isinstance(err, ProtocolError) and err.code != 404: raise\n\t\t\t\t\traise DoesNotExists(root_id, path[i:])\n\t\treturn root_id if not objects else self.info(root_id)"}
{"prompt": "def require(self, key):\n        \"\"\"\n        Raises an exception if value for ``key`` is empty.\n        \"\"\"", "output": "\n        if not self.get(key):\n            raise KeyError(key)\n\n    def copy(self):\n        \"\"\"\n        Returns a shallow copy of this ``MultiDict``.\n        \"\"\"\n        return MultiDict(self)\n\n    def urlencode(self, safe='/', encoding=None):\n        \"\"\"\n        Returns a string suitable for use in a URL.  ``MultiDict`` keys\n        are assumed to be URL encoded unless ``encoding`` is given.\n        \"\"\"\n        if encoding:\n            return ''.join(self.lists())\n        else:\n            return ''.join(self.lists(encoding))\n\n    def lists(self, encoding=None):\n        \"\"\"\n        Returns a list of lists, where each sublist corresponds to a\n        separate key.\n        \"\"\"\n        return list(self.iteritems(encoding))\n\n    def items(self, ", "solution": "\n        value = self.get(key)\n        if not value:\n            raise ValueError('\"{}\" is empty.'.format(key))\n        return value"}
{"prompt": "def Rconverter(Robj, dataframe=False):\n    \"\"\"\n    Convert an object in R's namespace to one suitable\n    for ipython's namespace.\n\n    For a data.frame, it tries to return a structured array.\n    It first checks for colnames, then names.\n    If all are NULL, it returns np.asarray(Robj), else\n    it tries to construct a recarray\n\n    Parameters\n    ----------\n\n    Robj: an R object returned from rpy2\n    \"\"\"", "output": "\n    # check colnames\n    if Robj.colnames:\n        colnames = Robj.colnames\n    else:\n        colnames = Robj.names\n    # check names\n    if Robj.names:\n        names = Robj.names\n    else:\n        names = Robj.names\n    # construct recarray\n    if dataframe:\n        recarray = np.recarray(len(colnames), dtype=object)\n        for i, col in enumerate(colnames):\n            recarray[i] = Robj[col]\n    else:\n        recarray = np.asarray(Robj)\n    return recarray\n\n\ndef Rconverter(Robj, dataframe=False):\n    \"\"\"\n    Convert an object in R's namespace to one suitable\n    for ipython's ", "solution": "\n    is_data_frame = ro.r('is.data.frame')\n    colnames = ro.r('colnames')\n    rownames = ro.r('rownames') # with pandas, these could be used for the index\n    names = ro.r('names')\n\n    if dataframe:\n        as_data_frame = ro.r('as.data.frame')\n        cols = colnames(Robj)\n        _names = names(Robj)\n        if cols != ri.NULL:\n            Robj = as_data_frame(Robj)\n            names = tuple(np.array(cols))\n        elif _names != ri.NULL:\n            names = tuple(np.array(_names))\n        else: # failed to find names\n            return np.asarray(Robj)\n        Robj = np.rec.fromarrays(Robj, names = names)\n    return np.asarray(Robj)"}
{"prompt": "def pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"", "output": "\n    return pid in pids\n\n\ndef pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    return pid in pids\n\n\ndef pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    return pid in pids\n\n\ndef pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    return pid in pids\n\n\ndef pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    return pid in pids\n\n\ndef pid_exists(pid):\n    \"\"\"Check whether pid exists in the current process table.\"\"\"\n    return pid in pids\n\n\ndef ", "solution": "\n    if pid < 0:\n        return False\n    try:\n        os.kill(pid, 0)\n    except OSError as exc:\n        logging.debug(\"No process[%s]: %s\", exc.errno, exc)\n        return exc.errno == errno.EPERM\n    else:\n        p = psutil.Process(pid)\n        return p.status != psutil.STATUS_ZOMBIE"}
{"prompt": "def start_hb(self, callback):\n        \"\"\"Start the heartbeating and call the callback if the kernel dies.\"\"\"", "output": "\n        self._hb()\n        callback()\n\n    def stop_hb(self):\n        \"\"\"Stop the heartbeating and wait for the callback.\"\"\"\n        self._hb()\n        self._wait_callback()\n\n    def _hb(self):\n        \"\"\"Stop the heartbeating and wait for the callback.\"\"\"\n        self._hb()\n        self._wait_callback()\n\n    def _wait_callback(self):\n        \"\"\"Wait for the heartbeat callback.\"\"\"\n        if self._callback:\n            self._callback()\n\n    def _send_heartbeat(self):\n        \"\"\"Send a heartbeat to the kernel.\"\"\"\n        self._send_signal(signal.SIGUSR1)\n\n    def _send_signal(self, sig):\n        \"\"\"Send a signal to the kernel.\"\"\"\n        signal.signal(sig, self._signal_", "solution": "\n        if not self._beating:\n            self._kernel_alive = True\n\n            def ping_or_dead():\n                self.hb_stream.flush()\n                if self._kernel_alive:\n                    self._kernel_alive = False\n                    self.hb_stream.send(b'ping')\n                    # flush stream to force immediate socket send\n                    self.hb_stream.flush()\n                else:\n                    try:\n                        callback()\n                    except:\n                        pass\n                    finally:\n                        self.stop_hb()\n\n            def beat_received(msg):\n                self._kernel_alive = True\n\n            self.hb_stream.on_recv(beat_received)\n            loop = ioloop.IOLoop.instance()\n            self._hb_periodic_callback = ioloop.PeriodicCallback(ping_or_dead, self.time_to_dead*1000, loop)\n            loop.add_timeout(time.time()+self.first_beat, self._really_start_hb)\n            self._beating= True"}
{"prompt": "def _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"", "output": "\n    chunks = math.ceil(float(len(noise)) / CHUNK_SIZE)\n    chunks_remaining = len(noise) % CHUNK_SIZE\n    if chunks_remaining:\n        chunks += 1\n    return chunks\n\n\ndef _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"\n    chunks = math.ceil(float(len(noise)) / CHUNK_SIZE)\n    chunks_remaining = len(noise) % CHUNK_SIZE\n    if chunks_remaining:\n        chunks += 1\n    return chunks\n\n\ndef _chunk_noise(noise):\n    \"\"\"Chunk input noise data into valid Touchstone file rows.\"\"\"\n    chunks = ", "solution": "\n    data = zip(\n        noise[\"freq\"],\n        noise[\"nf\"],\n        np.abs(noise[\"rc\"]),\n        np.angle(noise[\"rc\"]),\n        noise[\"res\"],\n    )\n    for freq, nf, rcmag, rcangle, res in data:\n        yield freq, nf, rcmag, rcangle, res"}
{"prompt": "def is_lesser(a, b):\n    \"\"\"\n    Verify that an item *a* is <= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"", "output": "\n    return a < b\n\n\ndef is_greater(a, b):\n    \"\"\"\n    Verify that an item *a* is >= then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"\n    return a > b\n\n\ndef is_equal(a, b):\n    \"\"\"\n    Verify that an item *a* is == then an item *b*\n    \n    :param a: An item\n    :param b: Another item\n    :return: True or False\n    \"\"\"\n    return a == b\n\n\ndef is_not_equal(a, b):\n    \"\"\"\n    Verify that an item *a* is!= then an item *b*\n    \n    :param a: An item\n", "solution": "\n\n    if type(a) != type(b):\n        return False\n\n    if isinstance(a, str) and isinstance(b, str):\n        return a == b\n    elif isinstance(a, bool) and isinstance(b, bool):\n        return a == b\n    elif isinstance(a, list) and isinstance(b, list):\n        for element in a:\n            flag = 0\n            for e in b:\n                if is_lesser(element, e):\n                    flag = 1\n                    break\n            if not flag:\n                return False\n        return True\n    elif isinstance(a, dict) and isinstance(b, dict):\n        if is_lesser(list(a.keys()), list(b.keys())):\n            for key, val in a.items():\n                if not is_lesser(val, b[key]):\n                    return False\n            return True\n        return False\n    elif isinstance(a, int) and isinstance(b, int):\n        return a <= b\n    elif isinstance(a, float) and isinstance(b, float):\n        return a <= b\n\n    return False"}
{"prompt": "def isreshapeable(new, old):\n    \"\"\"\n    Check to see if a proposed tuple of axes is a valid reshaping of\n    the old axes by ensuring that they can be factored.\n\n    Parameters\n    ----------\n    new : tuple\n        tuple of proposed axes\n\n    old : tuple\n        tuple of old axes\n    \"\"\"", "output": "\n    old_axes = tuple(old)\n    new_axes = tuple(new)\n    old_axes = tuple(old_axes)\n    new_axes = tuple(new_axes)\n    if old_axes!= new_axes:\n        return False\n    return True\n\n\ndef _is_same_subplot_params(subplot_params, old_subplot_params):\n    \"\"\"\n    Check to see if the subplot parameters have the same axes as the\n    old subplot parameters.\n\n    Parameters\n    ----------\n    subplot_params : dict\n        Dictionary of subplot parameters (e.g., left, right, bottom,\n        top)\n\n    old_subplot_params : dict\n        Dictionary of old subplot parameters (e.g., left, right, bottom,", "solution": "\n\n    new, old = tupleize(new), tupleize(old)\n\n    if not prod(new) == prod(old):\n        raise ValueError(\"Total size of new keys must remain unchanged\")"}
{"prompt": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"", "output": "\n    return GeneratedPyAST(gen_ast)\n\n\ndef _simple_ast_generator(gen_ast, source_ref):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"\n    return GeneratedPyAST(gen_ast, source_ref=source_ref)\n\n\ndef _simple_ast_generator(gen_ast, source_ref, source_ref_type):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"\n    return GeneratedPyAST(gen_ast, source_ref=source_ref, source_ref=source_ref_type)\n\n\ndef _simple_ast_generator(gen_ast, source_ref, source_ref_type, source_ref_type):\n    \"\"\"Wrap simpler AST", "solution": "\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx: GeneratorContext, form: LispForm) -> GeneratedPyAST:\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator"}
{"prompt": "def next_opcode(code, offset):\n    \"\"\"Return the next opcode and offset as a tuple. Tuple (-100,\n    -1000) is returned when reaching the end.\"\"\"", "output": "\n    code = code.strip()\n    offset = abs(offset)\n    if code == '':\n        return (-100, -1000)\n    if code[0] == '-':\n        code = code[1:]\n    if code[-1] == '-':\n        code = code[:-1]\n    code = code.split()\n    if len(code) == 1:\n        return (-100, -1000)\n    if len(code) == 2:\n        op, offset = code\n        if op == '-':\n            return (-offset, offset)\n        else:\n            return (-offset, offset)\n    else:\n        return (-100, -1000)\n\n\ndef next_opcode_and_offset(code, offset):\n    \"\"\"Return the next opcode and offset as a tuple. Tuple (-100,\n    -1000) is returned when reaching the end", "solution": "\n    n = len(code)\n    while offset < n:\n        op = code[offset]\n        offset += 1\n        if op >= HAVE_ARGUMENT:\n            offset += 2\n            pass\n        yield op, offset\n        pass\n    yield -100, -1000\n    pass"}
{"prompt": "def assert_rank_at_most(x, rank, data=None, summarize=None, message=None,\n                        name=None):\n  \"\"\"Assert `x` has rank equal to `rank` or smaller.\n\n  Example of adding a dependency to an operation:\n\n  ```python\n  with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):\n    output = tf.reduce_sum(x)\n  ```\n\n  Args:\n    x:  Numeric `Tensor`.\n    rank:  Scalar `Tensor`.\n    data:  The tensors to print out if the condition is False.  Defaults to\n      error message and first few entries of `x`.\n    summarize: Print this many entries of each tensor.\n    message: A string to prefix to the default message.\n    name: A name for this operation (optional).\n      Defaults to \"assert_rank_at_most\".\n\n  Returns:\n    Op raising `InvalidArgumentError` unless `x` has specified rank or lower.\n    If static checks determine `x` has correct rank, a `no_op` is returned.\n\n  Raises:\n    ValueError:  If static checks determine `x` has wrong rank.\n  \"\"\"", "output": "    name: A name for this operation (optional).\n\n  Returns:\n    Op raising `InvalidArgumentError` if `x` has rank `rank` or smaller.\n  \"\"\"\n  with ops.name_scope(name, \"assert_rank_at_most\", [x, rank]):\n    x = ops.convert_to_tensor(x, name=\"x\")\n    rank = ops.convert_to_tensor(rank, name=\"rank\")\n    if x.get_shape().ndims is not None:\n      x_rank = x.get_shape().ndims\n    else:\n      x_rank = array_ops.rank(x)\n    if x_rank!= array_ops.rank(rank):\n      raise ValueError(\n          \"Expected %s has rank %d, got rank %d\" % (\n              message, x_rank, array_ops.rank(rank", "solution": "\n  with tf.compat.v2.name_scope(name or 'assert_rank_at_most'):\n    return tf.compat.v1.assert_less_equal(\n        tf.rank(x), rank, data=data, summarize=summarize, message=message)"}
{"prompt": "def insert_instance_template(self, body, request_id=None, project_id=None):\n        \"\"\"\n        Inserts instance template using body specified\n        Must be called with keyword arguments rather than positional.\n\n        :param body: Instance template representation as object according to\n            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n        :type body: dict\n        :param request_id: Optional, unique request_id that you might add to achieve\n            full idempotence (for example when client call times out repeating the request\n            with the same request id will not create a new instance template again)\n            It should be in UUID format as defined in RFC 4122\n        :type request_id: str\n        :param project_id: Optional, Google Cloud Platform project ID where the\n            Compute Engine Instance exists. If set to None or missing,\n            the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"", "output": "Cloud project ID to use for instance template\n            creation\n        :type project_id: str\n        \"\"\"\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = {'instanceTemplate': body}\n        body = ", "solution": "\n        response = self.get_conn().instanceTemplates().insert(\n            project=project_id,\n            body=body,\n            requestId=request_id\n        ).execute(num_retries=self.num_retries)\n        try:\n            operation_name = response[\"name\"]\n        except KeyError:\n            raise AirflowException(\n                \"Wrong response '{}' returned - it should contain \"\n                \"'name' field\".format(response))\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)"}
{"prompt": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"", "output": "\n        if e.args[0] in (\n            ProtocolError,\n            ConnectionClosed,\n            ConnectionAborted,\n            ConnectionResetError,\n        ):\n            self.disconnect(e.reason)\n        else:\n            super(BaseProtocol, self)._process_op_err(e)\n\n    def connection_made(self, transport):\n        \"\"\"\n        Called when the connection is established.\n        \"\"\"\n        self.transport = transport\n        self.stream = transport.get_pipe_transport(0)\n        self.stream.set_close_callback(self.on_close)\n        self.on_handshake()\n\n    def connection_lost(self, exc):\n        \"\"\"\n        Called when the connection is lost.\n        \"\"\"\n        self.on_close()\n\n    def data_received(self, data):\n", "solution": "\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)"}
{"prompt": "def get_generator(self, path, *args, **kw_args):\n        \"\"\"\n        Get a generator that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the generator one row at a time.\n        Unlike the direct access queue, this generator also returns the remainder elements.\n        Additional arguments are forwarded to get_queue.\n        See the get_queue method for documentation of these parameters.\n\n        :param path:\n        :return: A generator that iterates over the rows in the dataset.\n        \"\"\"", "output": "\n        queue = self.get_queue(path)\n        generator = self.generator\n        generator.start()\n        try:\n            for row in queue:\n                yield row\n                generator.get_remainder()\n        except StopIteration:\n            generator.stop()\n            raise\n        finally:\n            generator.join()\n\n    def get_queue(self, path):\n        \"\"\"\n        Get a queue that allows convenient access to the streamed data.\n        Elements from the dataset are returned from the queue one row at a time.\n        Unlike the direct access queue, this queue also returns the remainder elements.\n        Additional arguments are forwarded to get_generator.\n        See the get_generator method for documentation of these parameters.\n\n        :param path:\n        :return: A queue that iterates over the rows in the dataset.\n        \"\"\"\n        ", "solution": "\n        q = self.get_queue(path=path, *args, **kw_args)\n\n        try:\n            # This generator just implements a standard access pattern for the direct access queue.\n            for guard in q.iter():\n                with guard as batch:\n                    batch_copy = batch.copy()\n\n                for row in batch_copy:\n                    yield row\n\n            last_batch = self.get_remainder(path, q.block_size)\n            for row in last_batch:\n                yield row\n\n        finally:\n            q.close()"}
{"prompt": "def running_under_virtualenv():\n    \"\"\"\n    Return True if we're running inside a virtualenv, False otherwise.\n\n    \"\"\"", "output": "\n    return hasattr(sys,'real_prefix')\n\n\ndef running_under_pypy():\n    \"\"\"\n    Return True if we're running inside a pypy environment, False otherwise.\n\n    \"\"\"\n    return hasattr(sys, 'pypy_translation_info')\n\n\ndef running_under_pypy_rev():\n    \"\"\"\n    Return True if we're running inside a pypy environment, False otherwise.\n\n    \"\"\"\n    return hasattr(sys, 'pypy_translation_info') and sys.pypy_translation_info\n\n\ndef running_under_pypy_and_rev():\n    \"\"\"\n    Return True if we're running inside a pypy environment, False otherwise.\n\n    \"\"\"\n    return running_under_pypy() or running_under_pypy_rev()\n\n\ndef running_", "solution": "\n    if hasattr(sys, 'real_prefix'):\n        return True\n    elif sys.prefix != getattr(sys, \"base_prefix\", sys.prefix):\n        return True\n\n    return False"}
{"prompt": "def build(self, pre=None, shortest=False):\n        \"\"\"Build the String instance\n\n        :param list pre: The prerequisites list (optional, default=None)\n        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated.\n        \"\"\"", "output": "\n        self.pre = pre\n        self.shortest = shortest\n        self.string = ''\n        self.strings = []\n        self.strings_index = 0\n        self.strings_length = 0\n        self.strings_lengths = []\n        self.strings_indexs = []\n        self.strings_lengths_index = 0\n        self.strings_lengths_length = 0\n        self.strings_indexs_length = 0\n        self.strings_lengths_indexs = []\n        self.strings_lengths_lengths = []\n        self.strings_indexs_lengths = []\n        self.strings_lengths_index = 0\n        self.strings_lengths_length = 0\n        self.strings_indexs_length = 0\n        self.strings_lengths_indexs = []\n        self.strings_lengths_", "solution": "\n        if pre is None:\n            pre = []\n\n        if self.value is not None and rand.maybe():\n            return utils.val(self.value, pre, shortest=shortest)\n\n        length = super(String, self).build(pre, shortest=shortest)\n        res = rand.data(length, self.charset)\n        return res"}
{"prompt": "def chimera_block_quotient(G, blocks):\n    \"\"\"\n    Extract the blocks from a graph, and returns a\n    block-quotient graph according to the acceptability\n    functions block_good and eblock_good\n\n    Inputs:\n        G: a networkx graph\n        blocks: a tuple of tuples\n\n    \"\"\"", "output": "\n    block_good = block_good(G, blocks)\n    eblock_good = eblock_good(G, blocks)\n    block_quotient = block_quotient(G, blocks)\n    eblock_quotient = eblock_quotient(G, blocks)\n    return block_good, eblock_good, block_quotient, eblock_quotient\n\n\ndef block_good(G, blocks):\n    \"\"\"\n    Extract the blocks from a graph, and returns a\n    block-good graph according to the acceptability\n    functions block_good and eblock_good\n\n    Inputs:\n        G: a networkx graph\n        blocks: a tuple of tuples\n\n    \"\"\"\n    block_good = block_good(G, blocks)\n    eblock_", "solution": "\n    from networkx import Graph\n    from itertools import product\n\n    BG = Graph()\n    blockid = {}\n    for i, b in enumerate(blocks):\n        BG.add_node(i)\n        if not b or not all(G.has_node(x) for x in b):\n            continue\n        for q in b:\n            if q in blockid:\n                raise(RuntimeError, \"two blocks overlap\")\n            blockid[q] = i\n\n    for q, u in blockid.items():\n        ublock = blocks[u]\n        for p in G[q]:\n            if p not in blockid:\n                continue\n            v = blockid[p]\n            if BG.has_edge(u, v) or u == v:\n                continue\n            vblock = blocks[v]\n\n            if ublock[0][2] == vblock[0][2]:\n                block_edges = zip(ublock, vblock)\n            else:\n                block_edges = product(ublock, vblock)\n\n            if all(G.has_edge(x, y) for x, y in block_edges):\n                BG.add_edge(u, v)\n\n    return BG"}
{"prompt": "def process_commands(self):\n        \"\"\"Handle debugger commands.\"\"\"", "output": "\n        debugger_commands = self.debugger_commands()\n        if debugger_commands:\n            self.debugger_commands = debugger_commands\n            self.debugger_command_index = 0\n            self.debugger_command_queue = []\n            self.debugger_command_queue.append(debugger_commands)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue.append(None)\n            self.debugger_command_queue", "solution": "\n        if self.core.execution_status != 'No program':\n            self.setup()\n            self.location()\n            pass\n        leave_loop = run_hooks(self, self.preloop_hooks)\n        self.continue_running = False\n\n        while not leave_loop:\n            try:\n                run_hooks(self, self.precmd_hooks)\n                # bdb had a True return to leave loop.\n                # A more straight-forward way is to set\n                # instance variable self.continue_running.\n                leave_loop = self.process_command()\n                if leave_loop or self.continue_running: break\n            except EOFError:\n                # If we have stacked interfaces, pop to the next\n                # one.  If this is the last one however, we'll\n                # just stick with that.  FIXME: Possibly we should\n                # check to see if we are interactive.  and not\n                # leave if that's the case. Is this the right\n                # thing?  investigate and fix.\n                if len(self.debugger.intf) > 1:\n                    del self.debugger.intf[-1]\n                    self.last_command = ''\n                else:\n                    if self.debugger.intf[-1].output:\n                        self.debugger.intf[-1].output.writeline('Leaving')\n                        raise Mexcept.DebuggerQuit\n                        pass\n                    break\n                pass\n            pass\n        return run_hooks(self, self.postcmd_hooks)"}
{"prompt": "def table(name, auth=None, eager=True):\n    \"\"\"Returns a given table for the given user.\"\"\"", "output": "\n    table = Table(name, metadata, schema=schema, autoload=True)\n    table.create(bind=engine)\n    if auth:\n        table.authenticate(bind=engine, user=auth[0], password=auth[1])\n    if eager:\n        table.options(eagerload('*'))\n    return table\n\n\ndef drop_table(table_name, schema=None, bind=None, checkfirst=True):\n    \"\"\"Drops the table from the database.\"\"\"\n    table = Table(table_name, metadata, schema=schema, autoload=True)\n    table.drop(bind=bind, checkfirst=checkfirst)\n\n\ndef drop_view_if_exists(view_name, schema=None, bind=None):\n    \"\"\"Drops the view if it exists.\"\"\"\n    meta = Meta", "solution": "\n    auth = auth or []\n    dynamodb = boto.connect_dynamodb(*auth)\n\n    table = dynamodb.get_table(name)\n    return Table(table=table, eager=eager)"}
{"prompt": "def _is_raising(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node raise an exception\"\"\"", "output": "\n    for node in body:\n        if isinstance(node, ast.Raise):\n            return True\n    return False\n\n\ndef _is_returning(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node return something\"\"\"\n    for node in body:\n        if isinstance(node, ast.Return):\n            return True\n    return False\n\n\ndef _is_yielding(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node yield something\"\"\"\n    for node in body:\n        if isinstance(node, ast.Yield):\n            return True\n    return False\n\n\ndef _is_break(body: typing.List) -> bool:\n    \"\"\"Return true if the given statement node break\"\"\"\n    for node in body:\n        if isinstance(node, ast.Break):\n            return True\n    return False\n\n\ndef _is_continue(body: typing.", "solution": "\n    for node in body:\n        if isinstance(node, astroid.Raise):\n            return True\n    return False"}
{"prompt": "def get_assembly_size(assembly_file):\n    \"\"\"Returns the number of nucleotides and the size per contig for the\n    provided assembly file path\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n\n    Returns\n    -------\n    assembly_size : int\n        Size of the assembly in nucleotides\n    contig_size : dict\n        Length of each contig (contig name as key and length as value)\n\n    \"\"\"", "output": "\n    assembly_size = 0\n    contig_size = {}\n    with open(assembly_file) as assembly_file:\n        for line in assembly_file:\n            line = line.strip()\n            if line.startswith(\"contig\"):\n                contig_name = line.replace(\"contig\", \"\").strip()\n                contig_size[contig_name] = len(contig_name)\n                assembly_size += len(contig_name)\n    return assembly_size, contig_size\n\n\ndef assembly_to_fasta(assembly_file, output_file):\n    \"\"\"Converts a given assembly file to a fasta file\n\n    Parameters\n    ----------\n    assembly_file : str\n        Path to assembly file.\n    output_file : str\n        Path to output fasta file.\n\n", "solution": "\n\n    assembly_size = 0\n    contig_size = {}\n    header = \"\"\n\n    with open(assembly_file) as fh:\n        for line in fh:\n\n            # Skip empty lines\n            if line.strip() == \"\":\n                continue\n\n            if line.startswith(\">\"):\n                header = line.strip()[1:]\n                contig_size[header] = 0\n\n            else:\n                line_len = len(line.strip())\n                assembly_size += line_len\n                contig_size[header] += line_len\n\n    return assembly_size, contig_size"}
{"prompt": "def compute_index_key(self, to_instance):\n        \"\"\"\n        Compute the index key that can be used to identify an instance\n        on the link.\n        \"\"\"", "output": "\n        instance_uuid = to_instance['uuid']\n        instance_uuid = instance_uuid.replace('-', '')\n        instance_uuid = instance_uuid.replace(' ', '')\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        instance_uuid = instance_uuid.lower()\n        instance_uuid = instance_uuid.upper()\n        ", "solution": "\n        kwargs = dict()\n        for attr in self.key_map.values():\n            if _is_null(to_instance, attr):\n                return None\n            \n            if attr in to_instance.__dict__:\n                kwargs[attr] = to_instance.__dict__[attr]\n            else:\n                kwargs[attr] = getattr(to_instance, attr)\n\n        return frozenset(tuple(kwargs.items()))"}
{"prompt": "def p_boolean_expression(self, p):\n        \"\"\"\n        expression : expression LE          expression\n                   | expression LESSTHAN    expression\n                   | expression DOUBLEEQUAL expression\n                   | expression NOTEQUAL    expression\n                   | expression GE          expression\n                   | expression GT          expression\n                   | expression AND         expression\n                   | expression OR          expression\n        \"\"\"", "output": "\n        expr = p[1]\n        if expr == 'LE':\n            p[0] = p[2]\n        elif expr == 'LESSTHAN':\n            p[0] = p[3]\n        elif expr == 'DOUBLEEQUAL':\n            p[0] = p[4]\n        elif expr == 'NOTEQUAL':\n            p[0] = p[5]\n        elif expr == 'GE':\n            p[0] = p[6]\n        elif expr == 'GT':\n            p[0] = p[7]\n        elif expr == 'AND':\n            p[0] = p[8]\n        elif expr == 'OR':\n            p[0] = p[9]\n\n    def p_expression_list_1(self, p):\n        \"\"\"\n        expression : expression\n                   | expression COMMA expression\n", "solution": "\n        p[0] = BinaryOperationNode(left=p[1],\n                                   operator=p[2],\n                                   right=p[3])"}
{"prompt": "def pairwise_reproducibility(df, plot=False):\n    \"\"\"\n    Calculate the reproducibility of LA-ICPMS based on unique pairs of repeat analyses.\n    \n    Pairwise differences are fit with a half-Cauchy distribution, and the median and \n    95% confidence limits are returned for each analyte.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataset\n    \n    plot : bool\n        Whether or not to plot the resulting error distributions.\n    \n    Returns\n    -------\n    pdiffs : pandas.DataFrame\n        Unique pairwise differences for all analytes.\n    rep_dists : dict of scipy.stats.halfcauchy\n        Half-Cauchy distribution objects fitted to the\n        differences.\n    rep_stats : dict of tuples\n        The 50% and 95% quantiles of the half-cauchy\n        distribution.\n    (fig, axs) : matplotlib objects\n        The figure. If not made, returnes (None, None) placeholder\n    \n    \"\"\"", "output": "        Dictionary of scipy.stats.halfcauchy distributions for each analyte.\n    \n    \"\"\"\n    \n    # Calculate pairwise differences for all analytes\n    pdiffs = df.groupby(['analyte','repeat']).apply(pairwise_differences)\n    \n    # Calculate median and 95% confidence limits for each analyte\n    rep_dists = {}\n    for analyte, rep in pdiffs.groupby(['analyte']):\n        rep_dists[analyte] = scipy.stats.halfcauchy(rep.values, loc=rep.median(), scale=rep.std() * 2)\n    \n    # Calculate error distributions for each analyte\n    errs = pdiffs.groupby(['", "solution": "\n    \n    ans = df.columns.values\n    pdifs = []\n    \n    # calculate differences between unique pairs\n    for ind, d in df.groupby(level=0):\n        d.index = d.index.droplevel(0)\n\n        difs = []\n        for i, r in d.iterrows():\n            t = d.loc[i+1:, :]\n            difs.append(t[ans] - r[ans])\n\n        pdifs.append(pd.concat(difs))\n    pdifs = pd.concat(pdifs).abs()\n\n    # calculate stats\n    rep_stats = {}\n    rep_dists = {}\n    errfn = stats.halfcauchy\n    \n    for a in ans:\n        d = pdifs.loc[:, a].dropna().values\n        hdist = errfn.fit(d, floc=0)\n        rep_dists[a] = errfn(*hdist)\n        rep_stats[a] = rep_dists[a].ppf((0.5, 0.95))\n    \n    # make plot\n    if not plot:\n        return pdifs, rep_dists, rep_stats, (None, None)\n    \n    fig, axs = plt.subplots(1, len(ans), figsize=[len(ans) * 2, 2])\n    for a, ax in zip(ans, axs):\n        d = pdifs.loc[:, a].dropna().values\n        hist, edges, _ = ax.hist(d, 30)\n        ax.plot(edges, rep_dists[a].pdf(edges) * (sum(hist) * np.mean(np.diff(edges))))\n        ax.set_title(a, loc='left')\n\n    return pdifs, rep_dists, rep_stats, (fig, axs)"}
{"prompt": "def remove_redundant_nodes(self, preserve_lengths=True):\n        \"\"\"\n        Remove all nodes which have only a single child, and attach their\n        grandchildren to their parent.  The resulting tree has the minimum\n        number of internal nodes required for the number of leaves.\n        :param preserve_lengths: If true, branch lengths of removed nodes are \\\n        added to those of their children.\n        \"\"\"", "output": "\n        children = self.children\n        children_count = len(children)\n        children_to_remove = []\n        children_to_add = []\n        children_to_add.append(children[0])\n        children_to_remove.append(children[0])\n        children_to_add.append(children[1])\n        children_to_remove.append(children[1])\n        children_to_add.append(children[2])\n        children_to_remove.append(children[2])\n        children_to_add.append(children[3])\n        children_to_remove.append(children[3])\n        children_to_add.append(children[4])\n        children_to_remove.append(children[4])\n        children_to_add.append(children[5])\n        children_to_remove.append(children[5])\n        children_to_add.append(", "solution": "\n        for n in self.walk(mode='postorder'):\n            while n.ancestor and len(n.ancestor.descendants) == 1:\n                grandfather = n.ancestor.ancestor\n                father = n.ancestor\n                if preserve_lengths:\n                    n.length += father.length\n\n                if grandfather:\n                    for i, child in enumerate(grandfather.descendants):\n                        if child is father:\n                            del grandfather.descendants[i]\n                    grandfather.add_descendant(n)\n                    father.ancestor = None\n                else:\n                    self.descendants = n.descendants\n                    if preserve_lengths:\n                        self.length = n.length"}
{"prompt": "def _set_start(self):\n        \"\"\" Sets the start timestamp and formatted time to the current time. \"\"\"", "output": "\n        self.start_time = datetime.datetime.now()\n        self.start_time_formatted = time.strftime(self.start_time_format)\n\n    def _set_end(self):\n        \"\"\" Sets the end timestamp and formatted time to the current time. \"\"\"\n        self.end_time = datetime.datetime.now()\n        self.end_time_formatted = time.strftime(self.end_time_format)\n\n    def _set_duration(self):\n        \"\"\" Sets the duration to the current time. \"\"\"\n        self.duration = datetime.datetime.now() - self.start_time\n\n    def _set_duration_in_seconds(self):\n        \"\"\" Sets the duration to the current time. \"\"\"\n        self.duration_in_seconds = self.duration.total_seconds()\n\n    def _set_duration_in_minutes(self):\n        \"\"\" Sets the duration to the current time. \"\"\"\n        self.duration_in_minutes = self.duration.total_seconds() / 60\n\n    def _set_duration_in_hours(self):\n        \"\"\" Sets the duration ", "solution": "\n        init_time = time.time()\n        formatted_time = datetime.datetime.fromtimestamp(init_time).strftime('%Y_%m_%d_%Hh%Mm%Ss')\n        run_info_dict = self._run_information[self.v_crun]\n        run_info_dict['timestamp'] = init_time\n        run_info_dict['time'] = formatted_time\n        if self._environment_hexsha is not None:\n            run_info_dict['short_environment_hexsha'] = self._environment_hexsha[0:7]"}
{"prompt": "def return_type(type_name, formatter=None):\n    \"\"\"Specify that this function returns a typed value.\n\n    Args:\n        type_name (str): A type name known to the global typedargs type system\n        formatter (str): An optional name of a formatting function specified\n            for the type given in type_name.\n    \"\"\"", "output": "\n    globals()[type_name] = TypedArg(type_name, formatter)\n\n\ndef return_types(type_names, formatters=None):\n    \"\"\"Specify that this function returns multiple typed values.\n\n    Args:\n        type_names (list): A list of type names known to the global typedargs\n            type system\n        formatters (list): A list of formatting functions specified\n            for the types given in type_names.\n    \"\"\"\n    globals()[type_names] = TypedArgs(type_names, formatters)\n\n\ndef return_types_or_none(type_names, formatters=None):\n    \"\"\"Specify that this function returns multiple typed values.\n\n    Args:\n        type_names (list): A list of type names known", "solution": "\n\n    def _returns(func):\n        annotated(func)\n        func.metadata.typed_returnvalue(type_name, formatter)\n        return func\n\n    return _returns"}
{"prompt": "def load_metamodel(resource=None, load_globals=True):\n    \"\"\"\n    Load and return a metamodel expressed in ooaofooa from a *resource*.\n    The resource may be either a filename, a path, or a list of filenames\n    and/or paths.\n    \"\"\"", "output": "\n    metamodel = MetaModel(resource)\n    if load_globals:\n        metamodel.load_globals()\n    return metamodel\n\n\ndef load_model(resource=None, load_globals=True):\n    \"\"\"\n    Load and return a model expressed in ooaofooa from a *resource*.\n    The resource may be either a filename, a path, or a list of filenames\n    and/or paths.\n    \"\"\"\n    metamodel = load_metamodel(resource, load_globals)\n    return metamodel.load_model()\n\n\ndef load_model_from_file(filename, load_globals=True):\n    \"\"\"\n    Load and return a model expressed in ooaofooa from a file.\n    The file may be either a filename, a path, or a list of filenames\n    and/or ", "solution": "\n    loader = _mk_loader(resource, load_globals)\n    return loader.build_metamodel()"}
{"prompt": "def set_s3_credentials(secret_key_id, secret_access_key):\n    \"\"\"Creates a new Amazon S3 client internally with specified credentials.\n    There are no validations done to the credentials. Incorrect credentials are thus revealed with first S3 import call.\n    \n    secretKeyId Amazon S3 Secret Key ID (provided by Amazon)\n    secretAccessKey Amazon S3 Secret Access Key (provided by Amazon)\n    \"\"\"", "output": "\n    \n    session = boto3.session.Session()\n    \n    s3_client = session.client('s3')\n    \n    s3_client.create_bucket(Bucket=secret_key_id)\n    \n    s3_client.put_object(Bucket=secret_key_id, Key=secret_access_key, Body=b'')\n    \n    return s3_client\n</cell>\n<cell>\ns3_client = set_s3_credentials('secret-key','secret-access')\n</cell>\n<text>\nNow we can import the dataset and inspect its contents.\n\n\n</text>\n<cell>\nimport boto3\nimport botocore\nimport pandas as pd\n</cell>\n<cell>\ns3 = boto3.", "solution": "\n    if(secret_key_id is None):\n        raise H2OValueError(\"Secret key ID must be specified\")\n\n    if(secret_access_key is None):\n        raise H2OValueError(\"Secret access key must be specified\")\n    \n    if(not secret_key_id):\n        raise H2OValueError(\"Secret key ID must not be empty\")\n    \n    if(not secret_access_key):\n        raise H2OValueError(\"Secret access key must not be empty\")\n    \n    \n    params = {\"secret_key_id\": secret_key_id,\n              \"secret_access_key\": secret_access_key\n              }\n    \n    h2o.api(endpoint=\"POST /3/PersistS3\", data=params)\n    print(\"Credentials successfully set.\")"}
{"prompt": "def tar_and_s3_upload(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and upload to s3\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"", "output": "\n        tar = tarfile.open(path)\n        tar.extractall(path=path)\n        tar.close()\n        bucket.upload_file(key, path)\n\n    def tar_and_s3_download(self, path, key, bucket):\n        \"\"\"\n        Tar the local file or directory and download to local file\n\n        :param path: local file or directory\n        :type path: str\n        :param key: s3 key\n        :type key: str\n        :param bucket: s3 bucket\n        :type bucket: str\n        :return: None\n        \"\"\"\n        tar = tarfile.open(path)\n        tar.extractall(path=path)\n        tar.close()\n        bucket.download_file(key, path)\n\n    def tar_and_s3_upload_and_download(self, path, key, ", "solution": "\n        with tempfile.TemporaryFile() as temp_file:\n            if os.path.isdir(path):\n                files = [os.path.join(path, name) for name in os.listdir(path)]\n            else:\n                files = [path]\n            with tarfile.open(mode='w:gz', fileobj=temp_file) as tar_file:\n                for f in files:\n                    tar_file.add(f, arcname=os.path.basename(f))\n            temp_file.seek(0)\n            self.s3_hook.load_file_obj(temp_file, key, bucket, replace=True)"}
{"prompt": "def _make_default_operation_costs(self, allow_spaces=False):\n        \"\"\"\n        sets 1.0 cost for every replacement, insertion, deletion and transposition\n        \"\"\"", "output": "\n        costs = [1.0]\n        costs.extend(self._default_operations)\n        if allow_spaces:\n            costs.extend(self._default_spaces)\n        return costs\n\n    def _make_default_operations(self):\n        \"\"\"\n        returns list of default operations (insertion, deletion, transposition)\n        \"\"\"\n        operations = []\n        operations.extend(self._default_operations)\n        return operations\n\n    def _make_default_spaces(self):\n        \"\"\"\n        returns list of default spaces (insertion, deletion, transposition)\n        \"\"\"\n        spaces = []\n        spaces.extend(self._default_spaces)\n        return spaces\n\n    def _make_default_operations_costs(self, allow_spaces=False):\n        \"\"\"\n        ", "solution": "\n        self.operation_costs = dict()\n        self.operation_costs[\"\"] = {c: 1.0 for c in list(self.alphabet) + [' ']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[\"\"] = 1.0\n            if allow_spaces:\n                current_costs[\" \"] = 1.0\n            self.operation_costs[a] = current_costs\n        # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0437\u0438\u0446\u0438\u0438\n        for a, b in itertools.permutations(self.alphabet, 2):\n            self.operation_costs[a + b] = {b + a: 1.0}\n        # \u043f\u0440\u043e\u0431\u0435\u043b\u044b\n        if allow_spaces:\n            self.operation_costs[\" \"] = {c: 1.0 for c in self.alphabet}\n            self.operation_costs[\" \"][\"\"] = 1.0"}
{"prompt": "def _dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"", "output": "\n    dirname = os.path.join(db_dirname, user_id)\n    return os.path.exists(dirname)\n\n\ndef _ensure_dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of ensure_dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"\n    dirname = os.path.join(db_dirname, user_id)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n\ndef _ensure_dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    Internal implementation of ensure_dir_exists.\n\n    Expects a db-style path name.\n    \"\"\"\n    dirname = os.path.join(db_dirname, user_id)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n\ndef _ensure_dir_exists(db, user_id, db_dirname):\n    \"\"\"\n    ", "solution": "\n    return db.execute(\n        select(\n            [func.count(directories.c.name)],\n        ).where(\n            and_(\n                directories.c.user_id == user_id,\n                directories.c.name == db_dirname,\n            ),\n        )\n    ).scalar() != 0"}
{"prompt": "def create(self, name, description=None, color=None):\n        \"\"\"\n        Creates a new label and returns the response\n\n        :param name: The label name\n        :type name: str\n\n        :param description: An optional description for the label. The name is\n            used if no description is provided.\n        :type description: str\n\n        :param color: The hex color for the label (ex: 'ff0000' for red). If no\n            color is provided, a random one will be assigned.\n        :type color: str\n\n        :returns: The response of your post\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        body = {\n            'name': name,\n            'description': description,\n            'color': color\n        }\n\n        response = self.client.post('/labels', json=body)\n\n        return response\n\n    def list(self, page=None, per_page=None):\n        \"\"\"\n        Lists all labels and returns the response\n\n        :param page: The page number of the labels to return\n        :type page: int\n\n        :param per_page: The number of labels to return per page\n        :type per_page: int\n\n        :returns: The response of your get\n        :rtype: dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        response = self.client.get('/labels')\n\n        if ", "solution": "\n        data = {\n            'name': name,\n            'title': name,\n            'description': description or name,\n            'appearance': {\n                'color': color or random_color()\n            }\n        }\n        # Yes, it's confusing. the `/tags/` endpoint is used for labels\n        return self._post(\n            request=ApiActions.CREATE.value,\n            uri=ApiUri.TAGS.value,\n            params=data\n        )"}
{"prompt": "def create(self, teamId, personId=None, personEmail=None,\n               isModerator=False, **request_parameters):\n        \"\"\"Add someone to a team by Person ID or email address.\n\n        Add someone to a team by Person ID or email address; optionally making\n        them a moderator.\n\n        Args:\n            teamId(basestring): The team ID.\n            personId(basestring): The person ID.\n            personEmail(basestring): The email address of the person.\n            isModerator(bool): Set to True to make the person a team moderator.\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            TeamMembership: A TeamMembership object with the details of the\n            created team membership.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": ").\n\n        Returns:\n            MyDict: JSON response. Access the object's properties by using\n            the dot notation or the bracket notation.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            MalformedRequest: If the request body created is invalid.\n            ApiError: If the DNA Center cloud returns an error.\n        \"\"\"\n        check_type(team_id, basestring,\n                   may_be_none=False)\n        check_type(person_id, basestring,\n                   may_be_none=False)\n        check_type(person_email, basestring,\n                   may_be_none=False)\n        check_type(isModerator, bool,\n                   may_be_none=False)\n        if person", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n        check_type(personId, basestring)\n        check_type(personEmail, basestring)\n        check_type(isModerator, bool)\n\n        post_data = dict_from_items_with_values(\n            request_parameters,\n            teamId=teamId,\n            personId=personId,\n            personEmail=personEmail,\n            isModerator=isModerator,\n        )\n\n        # API request\n        json_data = self._session.post(API_ENDPOINT, json=post_data)\n\n        # Return a team membership object created from the response JSON data\n        return self._object_factory(OBJECT_TYPE, json_data)"}
{"prompt": "def visit(self, visit_uri):\n        \"\"\"\n        Navigate to the given URL. The URL can either be a relative URL or an absolute URL. The\n        behavior of either depends on the driver. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"http://google.com\")\n\n        For drivers which can run against an external application, such as the Selenium driver,\n        giving an absolute URL will navigate to that page. This allows testing applications running\n        on remote servers. For these drivers, setting :data:`capybara.app_host` will make the\n        remote server the default. For example::\n\n            capybara.app_host = \"http://google.com\"\n            session.visit(\"/\")  # visits the Google homepage\n\n        Args:\n            visit_uri (str): The URL to navigate to.\n        \"\"\"", "output": "host = \"http://example.com\"\n\n        To navigate to a relative URL, use the :meth:`.Session.visit` method. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"//bar\")\n\n        To navigate to an absolute URL, use the :meth:`.Session.visit` method. ::\n\n            session.visit(\"/foo\")\n            session.visit(\"/bar\")\n\n        To navigate to a relative URL and then back to the current URL, use the :meth:`.Session.back` method.\n        ::\n\n            session.visit(\"/foo\")\n            session.back()\n\n        To navigate to an absolute URL and then back to the current URL, use the :meth:`.Session.back` method.\n        ::\n\n            session.", "solution": "\n\n        self.raise_server_error()\n\n        visit_uri = urlparse(visit_uri)\n\n        if capybara.app_host:\n            uri_base = urlparse(capybara.app_host)\n        elif self.server:\n            uri_base = urlparse(\"http://{}:{}\".format(self.server.host, self.server.port))\n        else:\n            uri_base = None\n\n        visit_uri = ParseResult(\n            scheme=visit_uri.scheme or (uri_base.scheme if uri_base else \"\"),\n            netloc=visit_uri.netloc or (uri_base.netloc if uri_base else \"\"),\n            path=visit_uri.path,\n            params=visit_uri.params,\n            query=visit_uri.query,\n            fragment=visit_uri.fragment)\n\n        self.driver.visit(visit_uri.geturl())"}
{"prompt": "def from_join(cls, join: Join) -> 'ConditionalJoin':\n        \"\"\"Creates a new :see:ConditionalJoin from the\n        specified :see:Join object.\n\n        Arguments:\n            join:\n                The :see:Join object to create the\n                :see:ConditionalJoin object from.\n\n        Returns:\n            A :see:ConditionalJoin object created from\n            the :see:Join object.\n        \"\"\"", "output": "\n        return cls(\n            table=join.table,\n            condition=join.condition,\n            on_clause=join.on_clause,\n            join_type=join.join_type,\n            type_=join.type_,\n            type_text=join.type_text,\n        )\n\n    def as_from_clause(self) -> FromClause:\n        \"\"\"Creates a new :see:FromClause object from the\n        current :see:ConditionalJoin object.\n\n        Returns:\n            A :see:FromClause object created from the\n            current :see:ConditionalJoin object.\n        \"\"\"\n        return FromClause(\n            table=self.table,\n            condition=self.condition,\n            ", "solution": "\n\n        return cls(\n            join.table_name,\n            join.parent_alias,\n            join.table_alias,\n            join.join_type,\n            join.join_field,\n            join.nullable\n        )"}
{"prompt": "def process_tokens(self, tokens):\n        \"\"\"process tokens from the current module to search for module/block\n        level options\n        \"\"\"", "output": "\n        tokens = tokens[:]\n        while tokens and tokens[0][0] in ('\"', \"'\"):\n            token = tokens.pop(0)\n            if token[0][0] in '([{':\n                tokens.pop(0)\n                tokens.insert(0, ('name', token))\n            elif token[0][0] in '}])':\n                tokens.pop(0)\n                tokens.insert(0, ('body', token))\n            else:\n                tokens.pop(0)\n                tokens.insert(0, ('body', token))\n        return tokens\n\n    def process(self, tokens):\n        \"\"\"process tokens from the current module to search for module/block\n        level options\n        \"\"\"\n        tokens = process_tokens(self, tokens)\n        tokens = process_tokens(self, tokens)\n        tokens = process_", "solution": "\n        control_pragmas = {\"disable\", \"enable\"}\n        for (tok_type, content, start, _, _) in tokens:\n            if tok_type != tokenize.COMMENT:\n                continue\n            match = OPTION_RGX.search(content)\n            if match is None:\n                continue\n\n            first_group = match.group(1)\n            if (\n                first_group.strip() == \"disable-all\"\n                or first_group.strip() == \"skip-file\"\n            ):\n                if first_group.strip() == \"disable-all\":\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(\"disable-all\", \"skip-file\"),\n                    )\n                self.add_message(\"file-ignored\", line=start[0])\n                self._ignore_file = True\n                return\n            try:\n                opt, value = first_group.split(\"=\", 1)\n            except ValueError:\n                self.add_message(\n                    \"bad-inline-option\", args=first_group.strip(), line=start[0]\n                )\n                continue\n            opt = opt.strip()\n            if opt in self._options_methods or opt in self._bw_options_methods:\n                try:\n                    meth = self._options_methods[opt]\n                except KeyError:\n                    meth = self._bw_options_methods[opt]\n                    # found a \"(dis|en)able-msg\" pragma deprecated suppression\n                    self.add_message(\n                        \"deprecated-pragma\",\n                        line=start[0],\n                        args=(opt, opt.replace(\"-msg\", \"\")),\n                    )\n                for msgid in utils._splitstrip(value):\n                    # Add the line where a control pragma was encountered.\n                    if opt in control_pragmas:\n                        self._pragma_lineno[msgid] = start[0]\n\n                    try:\n                        if (opt, msgid) == (\"disable\", \"all\"):\n                            self.add_message(\n                                \"deprecated-pragma\",\n                                line=start[0],\n                                args=(\"disable=all\", \"skip-file\"),\n                            )\n                            self.add_message(\"file-ignored\", line=start[0])\n                            self._ignore_file = True\n                            return\n                        meth(msgid, \"module\", start[0])\n                    except exceptions.UnknownMessageError:\n                        self.add_message(\"bad-option-value\", args=msgid, line=start[0])\n            else:\n                self.add_message(\"unrecognized-inline-option\", args=opt, line=start[0])"}
{"prompt": "def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"", "output": "\n        if new is None:\n            new = {}\n        if old is None:\n            old = {}\n        if new!= old:\n            self.flags[name.lower()] = new\n\n    def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"\n        if new is None:\n            new = {}\n        if old is None:\n            old = {}\n        if new!= old:\n            self.flags[name.lower()] = new\n\n    def _flags_changed(self, name, old, new):\n        \"\"\"ensure flags dict is valid\"\"\"\n        if new is None:\n            new = {}\n        if old is None:\n            old = {}\n        if new!= old:\n            self.flags[name.lower()] = new\n\n    def _flags_changed(self, name, old, new):\n", "solution": "\n        for key,value in new.iteritems():\n            assert len(value) == 2, \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[0], (dict, Config)), \"Bad flag: %r:%s\"%(key,value)\n            assert isinstance(value[1], basestring), \"Bad flag: %r:%s\"%(key,value)"}
{"prompt": "def images(self, query=None):\n    \"\"\"List local images in the database, optionally with a query.\n\n       Paramters\n       =========\n       query: a string to search for in the container or collection name|tag|uri\n\n    \"\"\"", "output": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "solution": "\n    from sregistry.database.models import Collection, Container\n\n    rows = []\n    if query is not None:   \n        like = \"%\" + query + \"%\"\n        containers = Container.query.filter(or_(Container.name == query,\n                                                Container.tag.like(like),\n                                                Container.uri.like(like),\n                                                Container.name.like(like))).all() \n    else:\n        containers = Container.query.all()\n\n    if len(containers) > 0:\n        message = \"  [date]   [client]\\t[uri]\"\n        bot.custom(prefix='Containers:', message=message, color=\"RED\")\n        for c in containers:\n            uri = c.get_uri()\n            created_at = c.created_at.strftime('%B %d, %Y')\n            rows.append([created_at, \"   [%s]\" %c.client, uri])\n        bot.table(rows) \n    return containers"}
{"prompt": "def load_tiff_multipage(tiff_filename, dtype='float32'):\n    \"\"\"\n    Load a multipage tiff into a single variable in x,y,z format.\n\n    Arguments:\n        tiff_filename:     Filename of source data\n        dtype:             data type to use for the returned tensor\n\n    Returns:\n        Array containing contents from input tiff file in xyz order\n    \"\"\"", "output": "\n    tiff = tifffile.TiffFile(tiff_filename)\n    tiff.verify()\n    tiff.pages()\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].data\n    tiff.pages[0].", "solution": "\n    if not os.path.isfile(tiff_filename):\n        raise RuntimeError('could not find file \"%s\"' % tiff_filename)\n\n    # load the data from multi-layer TIF files\n    data = tiff.imread(tiff_filename)\n\n    im = []\n\n    while True:\n\n        Xi = numpy.array(data, dtype=dtype)\n        if Xi.ndim == 2:\n            Xi = Xi[numpy.newaxis, ...]  # add slice dimension\n        im.append(Xi)\n\n        try:\n            data.seek(data.tell()+1)\n        except EOFError:\n            break  # this just means hit end of file (not really an error)\n\n    im = numpy.concatenate(im, axis=0)  # list of 2d -> tensor\n    im = numpy.rollaxis(im, 1)\n    im = numpy.rollaxis(im, 2)\n\n    return im"}
{"prompt": "def with_setup(setup=None, teardown=None):\n    \"\"\"Decorator to add setup and/or teardown methods to a test function::\n\n      @with_setup(setup, teardown)\n      def test_something():\n          \" ... \"\n\n    Note that `with_setup` is useful *only* for test functions, not for test\n    methods or inside of TestCase subclasses.\n    \"\"\"", "output": "\n    def decorator(func):\n        setup = getattr(func,'setUp', None)\n        teardown = getattr(func, 'tearDown', None)\n        if setup or teardown:\n            def _setup(self):\n                if setup:\n                    setup(self)\n                    _run_setup(self)\n            def _teardown(self):\n                if teardown:\n                    teardown(self)\n                    _run_teardown(self)\n            func.setUp = _setup\n            func.tearDown = _teardown\n        return func\n    return decorator\n\n\ndef skip(reason):\n    \"\"\"Decorator to skip a test case.\n\n    When used as a decorated function, tests are skipped unless\n    explicitly asked to do so::\n\n      @skip(\"reason\")\n      def test_something", "solution": "\n    def decorate(func, setup=setup, teardown=teardown):\n        if setup:\n            if hasattr(func, 'setup'):\n                _old_s = func.setup\n                def _s():\n                    setup()\n                    _old_s()\n                func.setup = _s\n            else:\n                func.setup = setup\n        if teardown:\n            if hasattr(func, 'teardown'):\n                _old_t = func.teardown\n                def _t():\n                    _old_t()\n                    teardown()\n                func.teardown = _t\n            else:\n                func.teardown = teardown\n        return func\n    return decorate"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.\n        Returns\n        -------\n        est_idx : np.array(N)\n            Estimated indeces for the segment boundaries in frames.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation\n        \n        # Define the segmentation mask\n        segmentation = self.segmentation", "solution": "\n        # Preprocess to obtain features, times, and input boundary indeces\n        F = self._preprocess()\n\n        # Normalize\n        F = U.normalize(F, norm_type=self.config[\"label_norm_feats\"],\n                        floor=self.config[\"label_norm_floor\"],\n                        min_db=self.config[\"label_norm_min_db\"])\n\n        # Find the labels using 2D-FMCs\n        est_labels = compute_similarity(F, self.in_bound_idxs,\n                                        dirichlet=self.config[\"dirichlet\"],\n                                        xmeans=self.config[\"xmeans\"],\n                                        k=self.config[\"k\"],\n                                        offset=self.config[\"2dfmc_offset\"])\n\n        # Post process estimations\n        self.in_bound_idxs, est_labels = self._postprocess(self.in_bound_idxs,\n                                                           est_labels)\n\n        return self.in_bound_idxs, est_labels"}
{"prompt": "def find_handfile(names=None):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u660e\u786e\u6307\u5b9a\u6216\u9010\u7ea7\u641c\u7d22\u7236\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7ec8\u7aef\u4f20\u5165\u7684\u914d\u7f6e\n    :return: ``handfile`` \u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a None\n    :rtype: str\n    \"\"\"", "output": "\n    handfile = None\n    if names:\n        handfile = find_handfile(names)\n        if not handfile:\n            handfile = find_handfile(get_config('handfile'))\n    return handfile\n\n\ndef find_handfile(names=None, path=None):\n    \"\"\"\n    \u5c1d\u8bd5\u5b9a\u4f4d ``handfile`` \u6587\u4ef6\uff0c\u660e\u786e\u6307\u5b9a\u6216\u9010\u7ea7\u641c\u7d22\u7236\u8def\u5f84\n\n    :param str names: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u540d\uff0c\u4e3b\u8981\u7528\u4e8e\u8c03\u8bd5\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7ec8\u7aef\u4f20\u5165\u7684\u914d\u7f6e\n    :param str path: \u53ef\u9009\uff0c\u5f85\u67e5\u627e\u7684\u6587\u4ef6\u6240\u5728\u7684\u7edd", "solution": "\n    # \u5982\u679c\u6ca1\u6709\u660e\u786e\u6307\u5b9a\uff0c\u5219\u5305\u542b env \u4e2d\u7684\u503c\n    names = names or [env.handfile]\n\n    # \u82e5\u65e0 ``.py`` \u6269\u5c55\u540d\uff0c\u5219\u4f5c\u4e3a\u5f85\u67e5\u8be2\u540d\u79f0\uff0c\u8ffd\u52a0\u5230 names \u672b\u5c3e\n    if not names[0].endswith('.py'):\n        names += [names[0] + '.py']\n\n    # name \u4e2d\u662f\u5426\u5305\u542b\u8def\u5f84\u5143\u7d20\n    if os.path.dirname(names[0]):\n        # \u82e5\u5b58\u5728\uff0c\u5219\u6269\u5c55 Home \u8def\u5f84\u6807\u5fd7\uff0c\u5e76\u6d4b\u8bd5\u662f\u5426\u5b58\u5728\n        for name in names:\n            expanded = os.path.expanduser(name)\n            if os.path.exists(expanded):\n                if name.endswith('.py') or _is_package(expanded):\n                    return os.path.abspath(expanded)\n    else:\n        # \u5426\u5219\uff0c\u9010\u7ea7\u5411\u4e0a\u641c\u7d22\uff0c\u76f4\u5230\u6839\u8def\u5f84\n        path = '.'\n\n        # \u5728\u5230\u7cfb\u7edf\u6839\u8def\u5f84\u4e4b\u524d\u505c\u6b62\n        while os.path.split(os.path.abspath(path))[1]:\n            for name in names:\n                joined = os.path.join(path, name)\n                if os.path.exists(joined):\n                    if name.endswith('.py') or _is_package(joined):\n                        return os.path.abspath(joined)\n            path = os.path.join('..', path)\n\n    return None"}
{"prompt": "def remote_upload(self, remote_url, folder_id=None, headers=None):\n        \"\"\"Used to make a remote file upload to openload.co\n\n        Note:\n            If folder_id is not provided, the file will be uploaded to ``Home`` folder.\n\n        Args:\n            remote_url (str): direct link of file to be remotely downloaded.\n            folder_id (:obj:`str`, optional): folder-ID to upload to.\n            headers (:obj:`dict`, optional): additional HTTP headers (e.g. Cookies or HTTP Basic-Auth)\n\n        Returns:\n            dict: dictionary containing (\"id\": uploaded file id, \"folderid\"). ::\n\n                {\n                    \"id\": \"12\",\n                    \"folderid\": \"4248\"\n                }\n\n        \"\"\"", "output": "\n        upload_url = f\"https://openload.co/api/v2/upload/file/{self.id}/\"\n        if folder_id:\n            upload_url += f\"folder/{folder_id}/\"\n        upload_url += remote_url\n        headers = headers or {}\n        headers['Content-Type'] = 'application/octet-stream'\n        headers['Accept'] = 'application/json'\n        headers['Authorization'] = f'Bearer {self.token}'\n        headers['Accept-Encoding'] = 'gzip, deflate'\n        headers['Accept-Language'] = 'en-US,en;q=0.9'\n        headers['Connection'] = 'keep-alive'\n        headers['Content-Length'] = str(len(remote_url))\n        headers['Host'] = 'openload.co'\n        headers['User-Agent", "solution": "\n\n        kwargs = {'folder': folder_id, 'headers': headers}\n        params = {'url': remote_url}\n        params.update({key: value for key, value in kwargs.items() if value})\n\n        return self._get('remotedl/add', params=params)"}
{"prompt": "def list(self, **request_parameters):\n        \"\"\"List all roles.\n\n        Args:\n            **request_parameters: Additional request parameters (provides\n                support for parameters that may be added in the future).\n\n        Returns:\n            GeneratorContainer: A GeneratorContainer which, when iterated,\n            yields the roles returned by the Webex Teams query.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        check_type(\n           'request_parameters',\n            dict(request_parameters=dict),\n            root_validator.Schema(\n                {\n                    'limit': int,\n                    'offset': int,\n                   'sort': string_types,\n                    'filter': string_types,\n                    'include': string_types,\n                    'exclude': string_types,\n                    'includeAll': bool,\n                    'excludeAll': bool,\n                    'includeInactive': bool,\n                    'excludeInactive': bool,\n                    'includeArchived': bool,\n                    'excludeArchived': bool,\n                    'includeHidden': bool,\n                    'excludeHidden': bool,\n                    'includeArchivedHidden': bool,", "solution": "\n        # API request - get items\n        items = self._session.get_items(\n            API_ENDPOINT,\n            params=request_parameters\n        )\n\n        # Yield role objects created from the returned JSON objects\n        for item in items:\n            yield self._object_factory(OBJECT_TYPE, item)"}
{"prompt": "def delete_all_checkpoints(self, path):\n        \"\"\"Delete all checkpoints for the given path.\"\"\"", "output": "\n        checkpoint_paths = glob.glob(os.path.join(path, \"*\"))\n        for checkpoint_path in checkpoint_paths:\n            shutil.rmtree(checkpoint_path)\n\n\nclass CheckpointSaver(Saver):\n    \"\"\"Checkpoint saver for distributed training.\n\n    This class implements a `CheckpointSaver` interface for distributed training.\n    All the checkpoints will be saved in the `path` directory.\n\n    Args:\n        checkpoint_dir: directory to save checkpoints.\n        checkpoint_filename_prefix: prefix of the checkpoint filenames.\n        checkpoint_max_to_keep: maximum number of checkpoints to keep.\n        checkpoint_num_to_keep: number of checkpoints to keep.\n        checkpoint_keep_checkpoint_every_n_hours: number of hours between checkpoints.\n        checkpoint_", "solution": "\n        with self.engine.begin() as db:\n            delete_remote_checkpoints(db, self.user_id, path)"}
{"prompt": "def _addHdlProcToRun(self, trigger: SimSignal, proc) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"", "output": "\n        self.hdl_procs.append(proc)\n\n    def _addHdlProcToRun(self, trigger: SimSignal, proc: SimProcedure) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"\n        self.hdl_procs.append(proc)\n\n    def _addHdlProcToRun(self, trigger: SimSignal, proc: SimProcedure) -> None:\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"\n        self.hdl_procs.append(proc)", "solution": "\n        # first process in time has to plan executing of apply values on the\n        # end of this time\n        if not self._applyValPlaned:\n            # (apply on end of this time to minimalize process reevaluation)\n            self._scheduleApplyValues()\n\n        if isEvDependentOn(trigger, proc):\n            if self.now == 0:\n                return  # pass event dependent on startup\n            self._seqProcsToRun.append(proc)\n        else:\n            self._combProcsToRun.append(proc)"}
{"prompt": "def convert(self, amount, currency, new_currency='EUR', date=None):\n        \"\"\"Convert amount from a currency to another one.\n\n        :param float amount: The amount of `currency` to convert.\n        :param str currency: The currency to convert from.\n        :param str new_currency: The currency to convert to.\n        :param datetime.date date: Use the conversion rate of this date. If this\n            is not given, the most recent rate is used.\n\n        :return: The value of `amount` in `new_currency`.\n        :rtype: float\n\n        >>> from datetime import date\n        >>> c = CurrencyConverter()\n        >>> c.convert(100, 'EUR', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', date=date(2014, 3, 28))\n        72.67...\n        >>> c.convert(100, 'BGN', date=date(2010, 11, 21))\n        Traceback (most recent call last):\n        RateNotFoundError: BGN has no rate for 2010-11-21\n        \"\"\"", "output": "', 'EUR', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', 'USD', date=date(2014, 3, 28))\n        137.5...\n        >>> c.convert(100, 'USD', 'USD', date=date(2014, 3, 28))\n        13", "solution": "\n        for c in currency, new_currency:\n            if c not in self.currencies:\n                raise ValueError('{0} is not a supported currency'.format(c))\n\n        if date is None:\n            date = self.bounds[currency].last_date\n        else:\n            try:\n                date = date.date()  # fallback if input was a datetime object\n            except AttributeError:\n                pass\n\n        r0 = self._get_rate(currency, date)\n        r1 = self._get_rate(new_currency, date)\n\n        return float(amount) / r0 * r1"}
{"prompt": "def user_config_file(self):\n        \"\"\"Get the absolute path to the user config file.\"\"\"", "output": "\n        return os.path.join(self.home_dir, '.config', 'pywikibot')\n\n    @property\n    def config_filename(self):\n        \"\"\"Get the absolute path to the config file.\"\"\"\n        return os.path.join(self.home_dir, 'pywikibot', 'config.py')\n\n    @property\n    def data_dir(self):\n        \"\"\"Get the absolute path to the data directory.\"\"\"\n        return os.path.join(self.home_dir, 'data')\n\n    @property\n    def data_dir_template(self):\n        \"\"\"Get the template for the data directory.\"\"\"\n        return os.path.join(self.data_dir, 'templates')\n\n    @property\n    def data_dir_templates(self):\n        \"\"\"Get the template for the data directory templates.\"\"\"\n        return [self.data_dir_template % {'name': self.name()}\n                for name in ('commons', 'user', 'group','sysop')]\n\n", "solution": "\n        return os.path.join(\n            get_user_config_dir(self.app_name, self.app_author),\n            self.filename)"}
{"prompt": "def build_url_field(self, field_name, model_class):\n        \"\"\"\n        This is needed due to DRF's model serializer uses the queryset to build url name\n\n        # TODO: Move this to own serializer mixin or fix problem elsewhere?\n        \"\"\"", "output": "\n        field = model_class._meta.get_field(field_name)\n        field_name = field.related_query_name()\n        url_field = field.get_internal_type()\n        url_field_name = url_field.replace('_', '-')\n        url_field_name = url_field_name.lower()\n        url_field = f'{url_field_name}_id'\n        return url_field\n\n    def build_url_kwargs(self, obj):\n        \"\"\"\n        Build url kwargs for serializer based on model field\n        \"\"\"\n        kwargs = {}\n        for field_name, field in self._fields.items():\n            kwargs[field_name] = self.build_url_field(field_name, obj)\n        return kwargs\n\n    def build_url(self, obj):\n        \"\"\"\n        Build url for serializer based on model field\n        \"\"\"", "solution": "\n        field, kwargs = super().build_url_field(field_name, model_class)\n\n        view = self.root.context[\"view\"]\n        kwargs[\"view_name\"] = view.get_url_name(\"detail\")\n\n        return field, kwargs"}
{"prompt": "def list_templates(self, extensions=None, filter_func=None):\n        \"\"\"Returns a list of templates for this environment.  This requires\n        that the loader supports the loader's\n        :meth:`~BaseLoader.list_templates` method.\n\n        If there are other files in the template folder besides the\n        actual templates, the returned list can be filtered.  There are two\n        ways: either `extensions` is set to a list of file extensions for\n        templates, or a `filter_func` can be provided which is a callable that\n        is passed a template name and should return `True` if it should end up\n        in the result list.\n\n        If the loader does not support that, a :exc:`TypeError` is raised.\n\n        .. versionadded:: 2.4\n        \"\"\"", "output": "\n        extensions = extensions or []\n        filter_func = filter_func or self.filter_templates\n        loader = self\n        template_paths = []\n        template_paths.extend(loader.list_templates(extensions))\n        template_paths.extend(loader.list_templates(\n            extensions, filter_func=filter_func))\n        return template_paths\n\n    def list_template_source(self, template_name, template_path=None):\n        \"\"\"List the source files for a given template.  This requires\n        that the loader supports the loader's\n        :meth:`~BaseLoader.list_template_source` method.\n\n        If there are other files in the template folder besides the\n        actual templates, the returned list can be filtered.  There are two\n        ways: ", "solution": "\n        x = self.loader.list_templates()\n        if extensions is not None:\n            if filter_func is not None:\n                raise TypeError('either extensions or filter_func '\n                                'can be passed, but not both')\n            filter_func = lambda x: '.' in x and \\\n                                    x.rsplit('.', 1)[1] in extensions\n        if filter_func is not None:\n            x = ifilter(filter_func, x)\n        return x"}
{"prompt": "def find_debugged_frame(frame):\n    \"\"\"Find the first frame that is a debugged frame. We do this\n    Generally we want traceback information without polluting it with\n    debugger frames. We can tell these because those are frames on the\n    top which don't have f_trace set. So we'll look back from the top\n    to find the fist frame where f_trace is set.\n    \"\"\"", "output": "\n    frame = frame.f_back\n    while frame is not None:\n        if frame.f_trace is not None:\n            return frame\n        frame = frame.f_back\n    return None\n\n\ndef extract_stack_info(frame, limit=None):\n    \"\"\"Extract the stack trace from the given frame object.\n\n    Return a list of tuples (filename, lineno, function name, text).\n    Each item in the list is a triple (filename,\n    line number, function name, text), with the filename being the full path\n    of the file where the function was found, and the line number\n    indicating the line within the file on which the function was\n    found, the function name being everything from (possibly\n    abbreviated) to the right, and text being the ", "solution": "\n    f_prev = f = frame\n    while f is not None and f.f_trace is None:\n        f_prev = f\n        f = f.f_back\n        pass\n    if f_prev:\n        val = f_prev.f_locals.get('tracer_func_frame')\n        if val == f_prev:\n            if f_prev.f_back:\n                f_prev = f_prev.f_back\n                pass\n            pass\n        pass\n    else:\n        return frame\n    return f_prev"}
{"prompt": "def parent_frame_arguments():\n  \"\"\"Returns parent frame arguments.\n\n  When called inside a function, returns a dictionary with the caller's function\n  arguments. These are positional arguments and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function argument names are overloaded before invoking\n  this method, then values will reflect the overloaded value. For this reason,\n  we recommend calling `parent_frame_arguments` at the beginning of the\n  function.\n  \"\"\"", "output": "\n  frame = sys._getframe(1)\n  parent_frame = frame.f_back\n  parent_frame_args = {}\n  while parent_frame is not None:\n    parent_frame_args[parent_frame.f_code.co_varnames[0]] = parent_frame\n    parent_frame = parent_frame.f_back\n  return parent_frame_args\n\n\ndef parent_frame_locals():\n  \"\"\"Returns parent frame locals.\n\n  When called inside a function, returns a dictionary with the caller's function\n  locals. These are local variables and keyword arguments (**kwargs),\n  while variable arguments (*varargs) are excluded.\n\n  When called at global scope, this will return an empty dictionary, since there\n  are no arguments.\n\n  WARNING: If caller function ", "solution": "\n  # All arguments and the names used for *varargs, and **kwargs\n  arg_names, variable_arg_name, keyword_arg_name, local_vars = (\n      tf_inspect._inspect.getargvalues(  # pylint: disable=protected-access\n          # Get the first frame of the caller of this method.\n          tf_inspect._inspect.stack()[1][0]))  # pylint: disable=protected-access\n\n  # Remove the *varargs, and flatten the **kwargs. Both are\n  # nested lists.\n  local_vars.pop(variable_arg_name, {})\n  keyword_args = local_vars.pop(keyword_arg_name, {})\n\n  final_args = {}\n  # Copy over arguments and their values. In general, local_vars\n  # may contain more than just the arguments, since this method\n  # can be called anywhere in a function.\n  for arg_name in arg_names:\n    final_args[arg_name] = local_vars.pop(arg_name)\n  final_args.update(keyword_args)\n\n  return final_args"}
{"prompt": "def _process_bit_id(self, node):\n        \"\"\"Process an Id or IndexedId node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"", "output": "\n        bits = []\n        bits.append(node.name)\n        bits.append(node.index)\n        return bits\n\n    def _process_bit_node(self, node):\n        \"\"\"Process a BitNode node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"\n        bits = []\n        bits.append(node.name)\n        bits.append(node.index)\n        return bits\n\n    def _process_bit_range(self, node):\n        \"\"\"Process a BitRange node as a bit or register type.\n\n        Return a list of tuples (Register,index).\n        \"\"\"\n        bits = []\n        bits.append(node.name)\n        bits.append(node.index)\n        return bits\n\n    def _process_bit_ranges(self, node):\n        \"\"\"Process a BitRange", "solution": "\n        # pylint: disable=inconsistent-return-statements\n        reg = None\n        if node.name in self.dag.qregs:\n            reg = self.dag.qregs[node.name]\n        elif node.name in self.dag.cregs:\n            reg = self.dag.cregs[node.name]\n        else:\n            raise QiskitError(\"expected qreg or creg name:\",\n                              \"line=%s\" % node.line,\n                              \"file=%s\" % node.file)\n\n        if node.type == \"indexed_id\":\n            # An indexed bit or qubit\n            return [(reg, node.index)]\n        elif node.type == \"id\":\n            # A qubit or qreg or creg\n            if not self.bit_stack[-1]:\n                # Global scope\n                return [(reg, j) for j in range(reg.size)]\n            else:\n                # local scope\n                if node.name in self.bit_stack[-1]:\n                    return [self.bit_stack[-1][node.name]]\n                raise QiskitError(\"expected local bit name:\",\n                                  \"line=%s\" % node.line,\n                                  \"file=%s\" % node.file)\n        return None"}
{"prompt": "def _backup(self):\n        \"\"\"\n        Save the current database into the inactive-db.json file.\n        \"\"\"", "output": "\n        backup = Backup()\n        backup.backup()\n\n    def _backup_restore(self):\n        \"\"\"\n        Backup the inactive-db.json file from the backup directory.\n        \"\"\"\n        backup = Backup()\n        backup.backup()\n        backup.unbackup()\n\n    def _backup_restore_from_backup(self):\n        \"\"\"\n        Backup the inactive-db.json file from the backup directory.\n        \"\"\"\n        backup = Backup()\n        backup.backup()\n        backup.unbackup()\n        backup.backup()\n\n    def _backup_restore_from_backups(self):\n        \"\"\"\n        Backup the inactive-db.json ", "solution": "\n\n        if PyFunceble.CONFIGURATION[\"inactive_database\"]:\n            # The database subsystem is activated.\n\n            # We save the current database state into the database file.\n            Dict(PyFunceble.INTERN[\"inactive_db\"]).to_json(self.inactive_db_path)"}
{"prompt": "def get_pandas_df(self, hql, schema='default'):\n        \"\"\"\n        Get a pandas dataframe from a Hive query\n\n        :param hql: hql to be executed.\n        :type hql: str or list\n        :param schema: target schema, default to 'default'.\n        :type schema: str\n        :return: result of hql execution\n        :rtype: DataFrame\n\n        >>> hh = HiveServer2Hook()\n        >>> sql = \"SELECT * FROM airflow.static_babynames LIMIT 100\"\n        >>> df = hh.get_pandas_df(sql)\n        >>> len(df.index)\n        100\n\n        :return: pandas.DateFrame\n        \"\"\"", "output": "\n        import pandas as pd\n        from pandas.io.sql import read_sql\n\n        if isinstance(hql, str):\n            hql = [hql]\n        pandas_df = read_sql(hql, schema=schema)\n        return pandas_df\n\n    @provide_session\n    def bulk_load(self, table, files, target_fields=None, commit_every=1000):\n        \"\"\"\n        Load data from a list of files\n\n        :param table: target table\n        :type table: str\n        :param files: list of files to load\n        :type files: list of str\n        :param target_fields: fields to load from target table\n        :type target_fields: list of str\n        :param commit_every: The number of rows to load in one transaction\n        :type commit_every: int\n        \"\"\"", "solution": "\n        import pandas as pd\n        res = self.get_results(hql, schema=schema)\n        df = pd.DataFrame(res['data'])\n        df.columns = [c[0] for c in res['header']]\n        return df"}
{"prompt": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"", "output": "\n        return os.path.join(self.get_item_path(identifier), identifier)\n\n    def get_item_path(self, identifier):\n        \"\"\"Return path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: path from which the item content can be accessed\n        \"\"\"\n        return os.path.join(self.get_item_path(identifier), identifier)\n\n    def get_item_size(self, identifier):\n        \"\"\"Return size of item content.\n\n        :param identifier: item identifier\n        :returns: size of item content\n        \"\"\"\n        return os.path.getsize(self.get_item_path(identifier))\n\n    def get_item_checksum(self, identifier):\n        \"\"\"Return checksum of item content.\n\n        :param identifier: item identifier\n        :returns: checksum of item content\n        \"\"\"\n        return hashlib.md5(self.get_item_path(identifier", "solution": "\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath"}
{"prompt": "def record(self, func):\n        \"\"\"Registers a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"", "output": "\n        self.setup_states[func.__name__] = func\n\n    def teardown(self, func):\n        \"\"\"Unregisters a function that is called when the blueprint is\n        registered on the application.  This function is called with the\n        state as argument as returned by the :meth:`make_setup_state`\n        method.\n        \"\"\"\n        if func.__name__ in self.setup_states:\n            del self.setup_states[func.__name__]\n\n    def register(self, blueprint):\n        \"\"\"Registers the given blueprint.\n\n        The blueprint must already be registered.\n        \"\"\"\n        blueprint.register(self)\n        self.blueprints.append(blueprint)\n\n    def unregister(self, blueprint):\n        \"\"\"Unregisters the given blueprint.\n\n        The blueprint must already be registe", "solution": "\n        if self._got_registered_once and self.warn_on_modifications:\n            from warnings import warn\n            warn(Warning('The blueprint was already registered once '\n                         'but is getting modified now.  These changes '\n                         'will not show up.'))\n        self.deferred_functions.append(func)"}
{"prompt": "def find_sourcemap_comment(filepath, block_size=100):\n    \"\"\"\n    Seeks and removes the sourcemap comment. If found, the sourcemap line is\n    returned.\n\n    Bundled output files can have massive amounts of lines, and the sourceMap\n    comment is always at the end. So, to extract it efficiently, we read out the\n    lines of the file starting from the end. We look back at most 2 lines.\n\n    :param:filepath: path to output bundle file containing the sourcemap comment\n    :param:blocksize: integer saying how many bytes to read at once\n    :return:string with the sourcemap comment or None\n    \"\"\"", "output": "\n    source_map = SourceMap()\n    with open(filepath, 'rb') as f:\n        lines = f.readlines()\n    f.seek(0)\n    f.seek(-2, 2)\n    f.truncate()\n    f.seek(0, 2)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, 2)\n    f.seek(0, 0)\n    f.seek(-2, ", "solution": "\n\n    MAX_TRACKBACK = 2  # look back at most 2 lines, catching potential blank line at the end\n\n    block_number = -1\n    # blocks of size block_size, in reverse order starting from the end of the file\n    blocks = []\n    sourcemap = None\n\n    try:\n        # open file in binary read+write mode, so we can seek with negative offsets\n        of = io.open(filepath, 'br+')\n        # figure out what's the end byte\n        of.seek(0, os.SEEK_END)\n        block_end_byte = of.tell()\n\n        # track back for maximum MAX_TRACKBACK lines and while we can track back\n        while block_end_byte > 0 and MAX_TRACKBACK > 0:\n            if (block_end_byte - block_size > 0):\n                # read the last block we haven't yet read\n                of.seek(block_number*block_size, os.SEEK_END)\n                blocks.append(of.read(block_size))\n            else:\n                # file too small, start from begining\n                of.seek(0, os.SEEK_SET)\n                # only read what was not read\n                blocks = [of.read(block_end_byte)]\n\n            # update variables that control while loop\n            content = b''.join(reversed(blocks))\n            lines_found = content.count(b'\\n')\n            MAX_TRACKBACK -= lines_found\n            block_end_byte -= block_size\n            block_number -= 1\n\n            # early check and bail out if we found the sourcemap comment\n            if SOURCEMAPPING_URL_COMMENT in content:\n                offset = 0\n                # splitlines eats the last \\n if its followed by a blank line\n                lines = content.split(b'\\n')\n                for i, line in enumerate(lines):\n                    if line.startswith(SOURCEMAPPING_URL_COMMENT):\n                        offset = len(line)\n                        sourcemap = line\n                        break\n                while i+1 < len(lines):\n                    offset += 1  # for the newline char\n                    offset += len(lines[i+1])\n                    i += 1\n                # track back until the start of the comment, and truncate the comment\n                if sourcemap:\n                    offset += 1  # for the newline before the sourcemap comment\n                    of.seek(-offset, os.SEEK_END)\n                    of.truncate()\n                return force_text(sourcemap)\n    finally:\n        of.close()\n    return sourcemap"}
{"prompt": "def confirm(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"", "output": "\n        okay = True\n        while okay:\n            okay = raw_input(prompt).lower() in ('y', 'yes')\n            if okay:\n                return default\n            else:\n                print 'Aborting'\n                sys.exit(1)\n\n    def prompt(self, prompt, default):\n        \"\"\" Called when a dangerous action is about to be done to make sure\n        it's okay. `prompt' is printed; user response is returned.\"\"\"\n        okay = True\n        while okay:\n            okay = raw_input(prompt).lower() in ('y', 'yes')\n            if okay:\n                return default\n            else:\n                print 'Aborting'\n                sys.exit(1)", "solution": "\n        while True:\n            try:\n                self.write_confirm(prompt, default)\n                reply = self.readline('').strip().lower()\n            except EOFError:\n                return default\n            if reply in ('y', 'yes'):\n                return True\n            elif reply in ('n', 'no'):\n                return False\n            else:\n                self.msg(\"Please answer y or n.\")\n                pass\n            pass\n        return default"}
{"prompt": "def delete(self, tag_id):\n        \"\"\"\n        Delete the specified InactivityAlert\n\n        :param tag_id: The tag ID to delete\n        :type tag_id: str\n\n        :raises: This will raise a\n            :class:`ServerException <logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        uri = '/tags/%s/inactivity_alerts/%s' % (self.client.base_url, tag_id)\n        return self.client.delete(uri)\n\n    def list(self, tag_id=None, page_size=100, page=0, sort_by='created_at',\n             sort_order='desc', filters=None):\n        \"\"\"\n        List the InactivityAlerts associated with the specified tag\n\n        :param tag_id: The tag ID to list\n        :type tag_id: str\n        :param page_size: The number of results to return per page\n        :type page_size: int\n        :param page: The page to return\n        :type page: int\n        :param sort_by: The field to sort by\n        :type sort_by: str\n        :param sort_order: The direction to sort by\n        :type sort_order", "solution": "\n        tag_url = 'https://logentries.com/rest/{account_id}/api/tags/{tag_id}'\n\n        self._api_delete(\n            url=tag_url.format(\n                account_id=self.account_id,\n                tag_id=tag_id\n            )\n        )"}
{"prompt": "def normal_left_dclick(self, event):\n        \"\"\" Handles the left mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the duration\n        of the view.\n\n        \"\"\"", "output": "\n        tool = self.tool\n        tool.tool_view.set_active(tool)\n\n    def normal_right_dclick(self, event):\n        \"\"\" Handles the right mouse button being double-clicked when the tool\n        is in the 'normal' state.\n\n        If the event occurred on this tool's component (or any contained\n        component of that component), the method opens a Traits UI view on the\n        object referenced by the 'element' trait of the component that was\n        double-clicked, setting the tool as the active tool for the duration\n        of the view.\n\n        \"\"\"\n        tool = self.tool\n        tool.tool_view.set_active(tool)\n\n    def normal_left_mousewheel(self, event):\n        \"\"\" ", "solution": "\n        x = event.x\n        y = event.y\n\n        # First determine what component or components we are going to hittest\n        # on.  If our component is a container, then we add its non-container\n        # components to the list of candidates.\n#        candidates = []\n        component = self.component\n#        if isinstance(component, Container):\n#            candidates = get_nested_components(self.component)\n#        else:\n#            # We don't support clicking on unrecognized components\n#            return\n#\n#        # Hittest against all the candidate and take the first one\n#        item = None\n#        for candidate, offset in candidates:\n#            if candidate.is_in(x-offset[0], y-offset[1]):\n#                item = candidate\n#                break\n\n        if hasattr(component, \"element\"):\n            if component.element is not None:\n                component.active_tool = self\n                component.element.edit_traits(kind=\"livemodal\")\n                event.handled = True\n                component.active_tool = None\n                component.request_redraw()\n        return"}
{"prompt": "def _read_compressed_points_data(self, laszip_vlr, point_format):\n        \"\"\" reads the compressed point record\n        \"\"\"", "output": "\n        point_record = laszip_vlr.read_vlr_point(point_format)\n        point_record = point_record[0]\n        point_record = point_record[1]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n        point_record = point_record[0]\n", "solution": "\n        offset_to_chunk_table = struct.unpack(\"<q\", self.stream.read(8))[0]\n        size_of_point_data = offset_to_chunk_table - self.stream.tell()\n\n        if offset_to_chunk_table <= 0:\n            logger.warning(\n                \"Strange offset to chunk table: {}, ignoring it..\".format(\n                    offset_to_chunk_table\n                )\n            )\n            size_of_point_data = -1  # Read everything\n\n        points = record.PackedPointRecord.from_compressed_buffer(\n            self.stream.read(size_of_point_data),\n            point_format,\n            self.header.point_count,\n            laszip_vlr,\n        )\n        return points"}
{"prompt": "def projector(state, flatten=False):\n    \"\"\"\n    maps a pure state to a state matrix\n\n    Args:\n        state (ndarray): the number of qubits\n        flatten (bool): determine if state matrix of column work\n    Returns:\n        ndarray:  state_mat(2**num, 2**num) if flatten is false\n        ndarray:  state_mat(4**num) if flatten is true stacked on by the column\n    \"\"\"", "output": "\n    state_mat = np.zeros((2**state.bit_length(), 2**state.bit_length()))\n    state_mat[state.flatten()] = 1\n    if flatten:\n        state_mat = state_mat.flatten()\n    return state_mat\n\n\ndef projector_2(state, flatten=False):\n    \"\"\"\n    maps a pure state to a state matrix\n\n    Args:\n        state (ndarray): the number of qubits\n        flatten (bool): determine if state matrix of column work\n    Returns:\n        ndarray:  state_mat(2**num, 2**num) if flatten is false\n        ndarray:  state_mat(4**num) if flatten is true stacked on by the column\n    \"\"\"\n    state_mat = np.zeros((", "solution": "\n    density_matrix = np.outer(state.conjugate(), state)\n    if flatten:\n        return density_matrix.flatten(order='F')\n    return density_matrix"}
{"prompt": "def export_transcripts(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"", "output": "\n    transcripts = adapter.transcripts\n    \n    if build:\n        transcripts = transcripts.filter(build=build)\n    \n    for transcript in transcripts:\n        yield transcript\n\n\ndef export_transcripts_batch(adapter, build='37'):\n    \"\"\"Export all transcripts from the database\n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        build(str)\n    \n    Yields:\n        transcript(scout.models.Transcript)\n    \"\"\"\n    transcripts = adapter.transcripts\n    \n    if build:\n        transcripts = transcripts.filter(build=build)", "solution": "\n    LOG.info(\"Exporting all transcripts\")\n\n    for tx_obj in adapter.transcripts(build=build):\n        yield tx_obj"}
{"prompt": "def write_table(self, table):\n        \"\"\"Send DDL to create the specified `table`\n\n        :Parameters:\n          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.\n\n        Returns None\n        \"\"\"", "output": "\n        sql = table.to_sql()\n        self.execute(sql)\n\n    def execute(self, sql):\n        \"\"\"Execute the specified `sql`\n\n        :Parameters:\n          - `sql`: an SQL statement to execute.\n\n        Returns None\n        \"\"\"\n        self.cursor.execute(sql)\n\n    def executemany(self, sql, param_list):\n        \"\"\"Execute the specified `sql` with multiple parameters\n\n        :Parameters:\n          - `sql`: an SQL statement to execute.\n          - `param_list`: a list of parameters to pass to the SQL statement.\n\n        Returns None\n        \"\"\"\n        self.cursor.executemany(sql, param_list)\n\n    def executescript(self, script):\n        \"\"\"Execute the specified `script", "solution": "\n        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)\n        for sql in serial_key_sql + table_sql:\n            self.execute(sql)"}
{"prompt": "def visit_module(self, node):\n        \"\"\"visit module : update consumption analysis variable\n        checks globals doesn't overrides builtins\n        \"\"\"", "output": "\n        node.globals = self.globals\n        node.locals = self.locals\n        node.body = self.body\n        node.decorator_list = self.decorator_list\n        node.returns = self.returns\n        node.type_ignores = self.type_ignores\n        node.type_vars = self.type_vars\n        node.type_vars_with_annotations = self.type_vars_with_annotations\n        node.type_ignores[0] = self.type_ignores[0]\n        node.type_ignores[1] = self.type_ignores[1]\n        node.type_ignores[2] = self.type_ignores[2]\n        node.type_ignores[3] = self.type_ignores[3]\n        node.type_ignores[4] = self.type_ignores[4]\n        node.type_", "solution": "\n        self._to_consume = [NamesConsumer(node, \"module\")]\n        self._postponed_evaluation_enabled = is_postponed_evaluation_enabled(node)\n\n        for name, stmts in node.locals.items():\n            if utils.is_builtin(name) and not utils.is_inside_except(stmts[0]):\n                if self._should_ignore_redefined_builtin(stmts[0]) or name == \"__doc__\":\n                    continue\n                self.add_message(\"redefined-builtin\", args=name, node=stmts[0])"}
{"prompt": "def transpose(self):\n        \"\"\"Return the transpose of the QuantumChannel.\"\"\"", "output": "\n        return QuantumChannel(self.num_qubits, self.num_qubits)\n\n    def inverse(self):\n        \"\"\"Return the inverse of the QuantumChannel.\"\"\"\n        return QuantumChannel(self.num_qubits, self.num_qubits)\n\n    def depth(self):\n        \"\"\"Return the depth of this QuantumChannel.\"\"\"\n        return QuantumChannel.depth(self.num_qubits)\n\n    def depth_from_label(self, label):\n        \"\"\"Return the depth of this QuantumChannel from its label.\"\"\"\n        return QuantumChannel.depth_from_label(self.num_qubits, label)\n\n    def num_qubits(self):\n        \"\"\"Return the number of qubits.\"\"\"\n        return self.num_qubits\n\n    def num_qubits_of(self, qubit):\n        \"\"\"Return the number of qubits of `qubit`.\"\"\"\n        return self.num_qubit", "solution": "\n        return SuperOp(\n            np.transpose(self._data),\n            input_dims=self.output_dims(),\n            output_dims=self.input_dims())"}
{"prompt": "def _prepare_hiveconf(d):\n        \"\"\"\n        This function prepares a list of hiveconf params\n        from a dictionary of key value pairs.\n\n        :param d:\n        :type d: dict\n\n        >>> hh = HiveCliHook()\n        >>> hive_conf = {\"hive.exec.dynamic.partition\": \"true\",\n        ... \"hive.exec.dynamic.partition.mode\": \"nonstrict\"}\n        >>> hh._prepare_hiveconf(hive_conf)\n        [\"-hiveconf\", \"hive.exec.dynamic.partition=true\",\\\n \"-hiveconf\", \"hive.exec.dynamic.partition.mode=nonstrict\"]\n        \"\"\"", "output": "\n        conf = []\n        for k, v in d.items():\n            if k == \"hiveconf\":\n                conf.append(v)\n            else:\n                conf.append(\"%s=%s\" % (k, v))\n        return conf\n\n    @apply_defaults\n    def list_tables(self, db_name, pattern=None, max_tables=None,\n                    desc=False, **kwargs):\n        \"\"\"\n        List tables in a database.\n\n        :param db_name:\n            Name of the database to list tables from.\n        :type db_name: str\n        :param pattern:\n            Filter the list of tables to those matching the given pattern.\n        :type pattern: str\n        :param max_tables:\n            Limit the list to max_tables tables.\n        :type max_tables: int\n        :param desc:\n            Print the table ", "solution": "\n        if not d:\n            return []\n        return as_flattened_list(\n            zip([\"-hiveconf\"] * len(d),\n                [\"{}={}\".format(k, v) for k, v in d.items()])\n        )"}
{"prompt": "def trimmed_pred_default(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"", "output": "\n  return node.predicate\n\n\ndef trimmed_pred_value(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"\n  return node.predicate\n\n\ndef trimmed_pred_value(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"\n  return node.predicate\n\n\ndef trimmed_pred_value(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"\n  return node.predicate\n\n\ndef trimmed_pred_value(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"\n  return node.predicate\n\n\ndef trimmed_pred_value(node, parent):\n  \"\"\"The default predicate used in Node.trimmed.\"\"\"\n  return node.predicate\n\n\ndef trimmed_", "solution": "\n  return isinstance(node, ParseNode) and (node.is_empty or node.is_type(ParseNodeType.terminal))"}
{"prompt": "def _verify_compatibility(wave_a, wave_b, check_dep_units=True):\n    \"\"\"Verify that two waveforms can be combined with various mathematical functions.\"\"\"", "output": "\n    wave_a = wave_a.copy()\n    wave_b = wave_b.copy()\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n    wave_a.set_channels(None)\n    wave_b.set_channels(None)\n", "solution": "\n    exobj = pexdoc.exh.addex(RuntimeError, \"Waveforms are not compatible\")\n    ctuple = (\n        bool(wave_a.indep_scale != wave_b.indep_scale),\n        bool(wave_a.dep_scale != wave_b.dep_scale),\n        bool(wave_a.indep_units != wave_b.indep_units),\n        (bool(wave_a.dep_units != wave_b.dep_units) if check_dep_units else False),\n        bool(wave_a.interp != wave_b.interp),\n    )\n    exobj(any(ctuple))"}
{"prompt": "def flowwrite(flow, filename, quantize=False, concat_axis=0, *args, **kwargs):\n    \"\"\"Write optical flow to file.\n\n    If the flow is not quantized, it will be saved as a .flo file losslessly,\n    otherwise a jpeg image which is lossy but of much smaller size. (dx and dy\n    will be concatenated horizontally into a single image if quantize is True.)\n\n    Args:\n        flow (ndarray): (h, w, 2) array of optical flow.\n        filename (str): Output filepath.\n        quantize (bool): Whether to quantize the flow and save it to 2 jpeg\n            images. If set to True, remaining args will be passed to\n            :func:`quantize_flow`.\n        concat_axis (int): The axis that dx and dy are concatenated,\n            can be either 0 or 1. Ignored if quantize is False.\n    \"\"\"", "output": "\n        concat_axis (int): Axis along which to concatenate the flows.\n        *args: Additional arguments to pass to :func:`quantize_flow`.\n        **kwargs: Additional arguments to pass to :func:`quantize_flow`.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If flow is not 2-dimensional.\n    \"\"\"\n    if flow.ndim!= 2:\n        raise ValueError('flow must be 2-dimensional')\n\n    flow_uint8 = flow.astype(np.uint8)\n    flow_uint8 = flow_uint8[:, :, 0] if flow_uint8.ndim == 2 else flow_uint8\n    flow_uint8 = flow_uint8[:, :, 1] if flow_uint8.ndim == 2 else flow_uint8\n\n    if quantize:\n        flow_uint8 = quantize_flow(flow_uint", "solution": "\n    if not quantize:\n        with open(filename, 'wb') as f:\n            f.write('PIEH'.encode('utf-8'))\n            np.array([flow.shape[1], flow.shape[0]], dtype=np.int32).tofile(f)\n            flow = flow.astype(np.float32)\n            flow.tofile(f)\n            f.flush()\n    else:\n        assert concat_axis in [0, 1]\n        dx, dy = quantize_flow(flow, *args, **kwargs)\n        dxdy = np.concatenate((dx, dy), axis=concat_axis)\n        imwrite(dxdy, filename)"}
{"prompt": "def get_singularity_version(singularity_version=None):\n    \"\"\"get_singularity_version will determine the singularity version for a\n       build first, an environmental variable is looked at, followed by \n       using the system version.\n\n       Parameters\n       ==========\n       singularity_version: if not defined, look for in environment. If still\n       not find, try finding via executing --version to Singularity. Only return\n       None if not set in environment or installed.\n    \"\"\"", "output": "\n    if singularity_version is None:\n        singularity_version = os.environ.get('SINGULARITY_VERSION', None)\n    if singularity_version is None:\n        singularity_version = find_version_from_command_line()\n    if singularity_version is None:\n        singularity_version = find_version_from_system()\n    return singularity_version\n\n\ndef get_singularity_version_from_system():\n    \"\"\"get_singularity_version_from_system will attempt to determine the \n       singularity version from the system version. \n    \"\"\"\n    version = platform.system()\n    if version == 'Darwin':\n        version = platform.mac_ver()[0]\n    elif version == 'Linux':\n        version = platform.linux_distribution()[0].lower()\n    return version\n\n\ndef get_singularity_", "solution": "\n\n    if singularity_version is None:        \n        singularity_version = os.environ.get(\"SINGULARITY_VERSION\")\n        \n    if singularity_version is None:\n        try:\n            cmd = ['singularity','--version']\n            output = run_command(cmd)\n\n            if isinstance(output['message'],bytes):\n                output['message'] = output['message'].decode('utf-8')\n            singularity_version = output['message'].strip('\\n')\n            bot.info(\"Singularity %s being used.\" % singularity_version)\n            \n        except:\n            singularity_version = None\n            bot.warning(\"Singularity version not found, so it's likely not installed.\")\n\n    return singularity_version"}
{"prompt": "def sign(pkey, data, digest):\n    \"\"\"\n    Sign a data string using the given key and message digest.\n\n    :param pkey: PKey to sign with\n    :param data: data to be signed\n    :param digest: message digest to use\n    :return: signature\n\n    .. versionadded:: 0.11\n    \"\"\"", "output": "\n    h = hashlib.sha1()\n    h.update(data)\n    h.update(digest)\n    h.update(pkey.private_bytes(\n        serialization.Encoding.PEM,\n        serialization.PrivateFormat.TraditionalOpenSSL,\n        serialization.NoEncryption(),\n    ))\n    return h.hexdigest()\n\n\ndef verify(pkey, signature, data):\n    \"\"\"\n    Verify a signature against a data string.\n\n    :param pkey: PKey to verify with\n    :param signature: signature to verify\n    :param data: data to be signed\n    :return: boolean\n\n   .. versionadded:: 0.11\n    \"\"\"\n    h = hashlib.sha1()\n    h.update(data)\n    h.", "solution": "\n    data = _text_to_bytes_and_warn(\"data\", data)\n\n    digest_obj = _lib.EVP_get_digestbyname(_byte_string(digest))\n    if digest_obj == _ffi.NULL:\n        raise ValueError(\"No such digest method\")\n\n    md_ctx = _lib.Cryptography_EVP_MD_CTX_new()\n    md_ctx = _ffi.gc(md_ctx, _lib.Cryptography_EVP_MD_CTX_free)\n\n    _lib.EVP_SignInit(md_ctx, digest_obj)\n    _lib.EVP_SignUpdate(md_ctx, data, len(data))\n\n    length = _lib.EVP_PKEY_size(pkey._pkey)\n    _openssl_assert(length > 0)\n    signature_buffer = _ffi.new(\"unsigned char[]\", length)\n    signature_length = _ffi.new(\"unsigned int *\")\n    final_result = _lib.EVP_SignFinal(\n        md_ctx, signature_buffer, signature_length, pkey._pkey)\n    _openssl_assert(final_result == 1)\n\n    return _ffi.buffer(signature_buffer, signature_length[0])[:]"}
{"prompt": "def p_navigation_step_2(self, p):\n        \"\"\"navigation_step : ARROW identifier LSQBR identifier DOT phrase RSQBR\"\"\"", "output": "\n        p[0] = p[2]\n\n    def p_navigation_step_3(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT phrase RSQBR\"\"\"\n        p[0] = p[2]\n\n    def p_navigation_step_4(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT DOT identifier\"\"\"\n        p[0] = p[2]\n\n    def p_navigation_step_5(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT DOT DOT DOT identifier\"\"\"\n        p[0] = p[2]\n\n    def p_navigation_step_6(self, p):\n        \"\"\"navigation_step : ARROW identifier DOT DOT DOT DOT ", "solution": "\n        p[0] = NavigationStepNode(key_letter=p[2],\n                                  rel_id=p[4],\n                                  phrase=p[6])"}
{"prompt": "def _get_request(self, endpoint):\n        \"\"\"Do actual GET request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for GET request\n        \"\"\"", "output": "\n        try:\n            resp = self.session.get(endpoint)\n        except requests.exceptions.RequestException as e:\n            raise GCException(e)\n        if resp.status_code!= 200:\n            raise GCException(\n                \"Got status code %d from GC.\" % resp.status_code)\n        return resp\n\n    def _post_request(self, endpoint, data):\n        \"\"\"Do actual POST request to GC REST API\n        Also validates responses.\n\n        Keyword arguments:\n        endpoint -- full endpoint for POST request\n        data -- data to POST\n        \"\"\"\n        try:\n            resp = self.session.post(endpoint, data=data)\n        except requests.exceptions.RequestException as e:\n            raise GCException(e)\n        if resp.status_code!= 200:\n            raise GCException(\n                \"Got status ", "solution": "\n        try:\n            response = requests.get(endpoint)\n        except requests.exceptions.RequestException:\n            raise GoldenCheetahNotAvailable(endpoint)\n        \n        if response.text.startswith('unknown athlete'):\n            match = re.match(\n                pattern='unknown athlete (?P<athlete>.+)',\n                string=response.text)\n            raise AthleteDoesNotExist(\n                athlete=match.groupdict()['athlete'])\n\n        elif response.text == 'file not found':\n            match = re.match(\n                pattern='.+/activity/(?P<filename>.+)',\n                string=endpoint)\n            raise ActivityDoesNotExist(\n                filename=match.groupdict()['filename'])\n\n        return response"}
{"prompt": "def _line_segment_with_colons(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"", "output": "\n    segment = linefmt.format(align=align, colwidth=colwidth)\n    return segment.replace(':','')\n\n\ndef _pipe_segment_with_colons(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"\n    segment = linefmt.format(align=align, colwidth=colwidth)\n    return segment.replace('|','')\n\n\ndef _pipe_segment_with_colon_and_space(linefmt, align, colwidth):\n    \"\"\"Return a segment of a horizontal line with optional colons which\n    indicate column's alignment (as in `pipe` output format).\"\"\"\n", "solution": "\n    fill = linefmt.hline\n    w = colwidth\n    if align in [\"right\", \"decimal\"]:\n        return (fill[0] * (w - 1)) + \":\"\n    elif align == \"center\":\n        return \":\" + (fill[0] * (w - 2)) + \":\"\n    elif align == \"left\":\n        return \":\" + (fill[0] * (w - 1))\n    else:\n        return fill[0] * w"}
{"prompt": "def report(self):\n        \"\"\"\n        Print network statistics.\n        \"\"\"", "output": "\n        stats = self.statistics\n        stats.print_stats()\n        stats.print_net()\n\n    def start_client(self, client=None):\n        \"\"\"\n        Start a client (server) thread to handle requests.\n        \"\"\"\n        if client is None:\n            client = self.client\n        if self.started:\n            raise RuntimeError(\"Cannot start a client that has already started\")\n        self.started = True\n        thread = threading.Thread(target=self.handle_request, args=(client,))\n        thread.daemon = True\n        thread.start()\n\n    def handle_request(self, client):\n        \"\"\"\n        Handle a single request from a client.\n        \"\"\"\n        try:\n            request = self.make_request(client)\n        except EOFError:\n            request = None\n        if request is not None:\n", "solution": "\n        logging.info(\"network inputs: %s\", \" \".join(map(str, self.input_variables)))\n        logging.info(\"network targets: %s\", \" \".join(map(str, self.target_variables)))\n        logging.info(\"network parameters: %s\", \" \".join(map(str, self.all_parameters)))\n        logging.info(\"parameter count: %d\", self.parameter_count)"}
{"prompt": "def init_handlers(self):\n        \"\"\"Create the default handlers.\"\"\"", "output": "\n        self.handlers = DefaultHandlers(self)\n        self.handlers.reset()\n\n    def default_error_handler(self, error):\n        \"\"\"Default error handler.\"\"\"\n        exc = error.exc_info[1]\n        if isinstance(exc, HTTPError):\n            exc.log_request(\n                self.application,\n                self.request,\n                self.response,\n                exc_info=error.exc_info[2],\n            )\n        else:\n            error.log_request(self.application, self.request, exc_info=True)\n\n    def default_request_error_handler(self, error):\n        \"\"\"Default error handler.\"\"\"\n        exc = error.exc_info[1]\n        if isinstance(exc, HTTPError):\n            exc.log_request(self.application, self.request, exc_info=True)\n        else:\n            error.log_request(self.application, self.request, ", "solution": "\n        self._handlers = {}\n        self._esc_handlers = {}\n        for handler in _default_handlers:\n            handler(\n                shell=self.shell, prefilter_manager=self, config=self.config\n            )"}
{"prompt": "def average_data(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n        counts (dict): a dict of outcomes from an experiment\n        observable (dict or matrix or list): The observable to be averaged over.\n        As an example, ZZ on qubits can be given as:\n        * dict: {\"00\": 1, \"11\": 1, \"01\": -1, \"10\": -1}\n        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]\n        * matrix diagonal (list): [1, -1, -1, 1]\n\n    Returns:\n        Double: Average of the observable\n    \"\"\"", "output": "0, ], [0, 0, 0, -1, ]]\n        * list: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0, ], [0, 0, 0, -1, ]]\n\n    Returns:\n        float: The average of the diagonal observable.\n\n    \"\"\"\n    sum_i = 0\n    for state in counts:\n        sum_i += counts[state] * observable[state]\n    return sum_i / len(counts)\n\n\ndef average_data_matrix(counts, observable):\n    \"\"\"Compute the mean value of an diagonal observable.\n\n    Takes in a diagonal observable in dictionary, list or matrix format and then\n    calculates the sum_i value(i) P(i) where value(i) is the value of the\n    observable for state i.\n\n    Args:\n", "solution": "\n    if not isinstance(observable, dict):\n        observable = make_dict_observable(observable)\n    temp = 0\n    tot = sum(counts.values())\n    for key in counts:\n        if key in observable:\n            temp += counts[key] * observable[key] / tot\n    return temp"}
{"prompt": "def _get_object_name(self, line):\n        \"\"\" Get second token in line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_name(\"  def func():  \")\n        'func'\n        >>> docwriter._get_object_name(\"  class Klass(object):  \")\n        'Klass'\n        >>> docwriter._get_object_name(\"  class Klass:  \")\n        'Klass'\n        \"\"\"", "output": "\n        tokens = line.split(' ', 1)\n        if len(tokens) == 2:\n            return tokens[1]\n        else:\n            return None\n\n    def _get_object_type(self, line):\n        \"\"\" Get object type from line\n        >>> docwriter = ApiDocWriter('sphinx')\n        >>> docwriter._get_object_type(\"  def func():  \")\n        'function'\n        >>> docwriter._get_object_type(\"  class Klass(object):  \")\n        'class'\n        >>> docwriter._get_object_type(\"  class Klass:  \")\n        'class'\n        \"\"\"\n        tokens = line.split(' ', 1)\n        if len(tokens) == 2:\n            return tokens[0]\n        else:\n            return None\n\n    def _get_object_doc(self, line):\n        \"\"\" Get ", "solution": "\n        name = line.split()[1].split('(')[0].strip()\n        # in case we have classes which are not derived from object\n        # ie. old style classes\n        return name.rstrip(':')"}
{"prompt": "def _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is from airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"", "output": "\n    metrics = {}\n    func_args = namespace.parser.parse_args([])\n    metrics[\"{func_name}\".format(func_name=func_name)] = func_args.func(**func_args.__dict__)\n    return metrics\n\n\ndef _build_metrics(func_name, namespace):\n    \"\"\"\n    Builds metrics dict from function args\n    It assumes that function arguments is airflow.bin.cli module's function\n    and has Namespace instance where it optionally contains \"dag_id\", \"task_id\",\n    and \"execution_date\".\n\n    :param func_name: name of function\n    :param namespace: Namespace instance from argparse\n    :return: dict with metrics\n    \"\"\"\n    metrics = {}\n    func_args = namespace.parser.parse_args([])\n    ", "solution": "\n\n    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),\n               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}\n\n    assert isinstance(namespace, Namespace)\n    tmp_dic = vars(namespace)\n    metrics['dag_id'] = tmp_dic.get('dag_id')\n    metrics['task_id'] = tmp_dic.get('task_id')\n    metrics['execution_date'] = tmp_dic.get('execution_date')\n    metrics['host_name'] = socket.gethostname()\n\n    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))\n    log = Log(\n        event='cli_{}'.format(func_name),\n        task_instance=None,\n        owner=metrics['user'],\n        extra=extra,\n        task_id=metrics.get('task_id'),\n        dag_id=metrics.get('dag_id'),\n        execution_date=metrics.get('execution_date'))\n    metrics['log'] = log\n    return metrics"}
{"prompt": "def get_func_sourcecode(func):\n    \"\"\"\n    Try to get sourcecode using standard inspect.getsource().\n    If the function comes from a module which has been created dynamically\n    (not from the filesystem), then it tries to read the sourcecode on the\n    filesystem anyway.\n    WARNING: can do weird things if the filesystem code slightly differs from\n             the original module code.\n    \"\"\"", "output": "\n    try:\n        co = compile(func, '<string>', 'exec')\n    except SyntaxError:\n        # Try to get the sourcecode from the compiled bytecode\n        co = compile('def f():\\n  pass', '<string>', 'exec')\n    co = co.co_code\n    filename = co.co_filename\n    if filename == '<string>':\n        filename = __file__\n    fn = None\n    try:\n        fn = open(filename, 'rb')\n    except IOError:\n        pass\n    if fn is None:\n        try:\n            fn = open(filename, 'rt')\n        except IOError:\n            pass\n    if fn is None:\n        # Try to get the sourcecode from the original module\n        try:\n            fn = sys.module", "solution": "\n\n    def getsource(func):\n        lines, lnum = getsourcelines(func)\n        return ''.join(lines)\n\n    def getsourcelines(func):\n        lines, lnum = findsource(func)\n        return inspect.getblock(lines[lnum:]), lnum + 1\n\n    def findsource(func):\n        file = getfile(func)  # file path\n        module = inspect.getmodule(func, file)\n        lines = linecache.getlines(file, module.__dict__)\n        code = func.__code__\n        lnum = code.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]):\n                break\n            lnum = lnum - 1  # pragma: no cover\n        return lines, lnum\n\n    def getfile(func):\n        module = inspect.getmodule(func)\n        return module.__file__\n\n    try:\n        return inspect.getsource(func)\n    except Exception:\n        return getsource(func)"}
{"prompt": "def x_fit(self, test_length):\r\n        \"\"\" Test to see if the line can has enough space for the given length. \"\"\"", "output": "\r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        ", "solution": "\r\n        if (self.x + test_length) >= self.xmax:\r\n            return False\r\n        else:\r\n            return True"}
{"prompt": "def list_files(self, extensions=None):\n        \"\"\"\n        List the ports contents by file type or all.\n        :param extensions: string extensions, single string or list of extensions.\n        :return: A list of full path names of each file.\n        \"\"\"", "output": "\n        extensions = extensions or []\n        extensions = extensions or ['.' + ext for ext in self._extensions]\n        files = []\n        for root, dirs, files in os.walk(self._path):\n            for filename in files:\n                if any(filename.endswith(ext) for ext in extensions):\n                    files.append(os.path.join(root, filename))\n        return files\n\n    def list_ports(self, extensions=None):\n        \"\"\"\n        List the ports contents by file type or all.\n        :param extensions: string extensions, single string or list of extensions.\n        :return: A list of full path names of each port.\n        \"\"\"\n        extensions = extensions or []\n        extensions = extensions or ['.' + ext for ext in self._extensions]\n        ", "solution": "\n        if self.type.lower() != 'directory':\n            raise ValueError(\"Port type is not == directory\")\n\n        filesystem_location = self.path\n\n        for root, dirs, files in os.walk(filesystem_location):\n            if extensions is None:\n                return [os.path.join(root, f) for f in files]\n            elif not isinstance(extensions, list):\n                extensions = [extensions]\n\n            subset_files = []\n\n            for f in files:\n                for extension in extensions:\n                    if f.lower().endswith(extension.lower()):\n                        subset_files.append(os.path.join(root, f))\n                        break\n            return subset_files"}
{"prompt": "def get_image(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"", "output": "\n        ims = []\n        for img in self.images:\n            if img.float_key.lower() == float_key.lower():\n                ims.append(img)\n        if to_chw:\n            ims = [im.transpose((2, 0, 1)).astype(np.float32) for im in ims]\n        return ims\n\n    def get_images(self, float_key=\"floats\", to_chw=True):\n        \"\"\"\n        get image list from ImageFrame\n        \"\"\"\n        ims = []\n        for img in self.images:\n            if img.float_key.lower() == float_key.lower():\n                ims.append(img)\n        if to_chw:\n            ims = [im.transpose((2, 0, 1)).astype(np.float32) for im in im", "solution": "\n        tensors = callBigDlFunc(self.bigdl_type,\n                                   \"localImageFrameToImageTensor\", self.value, float_key, to_chw)\n        return map(lambda tensor: tensor.to_ndarray(), tensors)"}
{"prompt": "def generate_key(self, type, bits):\n        \"\"\"\n        Generate a key pair of the given type, with the given number of bits.\n\n        This generates a key \"into\" the this object.\n\n        :param type: The key type.\n        :type type: :py:data:`TYPE_RSA` or :py:data:`TYPE_DSA`\n        :param bits: The number of bits.\n        :type bits: :py:data:`int` ``>= 0``\n        :raises TypeError: If :py:data:`type` or :py:data:`bits` isn't\n            of the appropriate type.\n        :raises ValueError: If the number of bits isn't an integer of\n            the appropriate size.\n        :return: ``None``\n        \"\"\"", "output": "\n        if not isinstance(type, TYPE_RSA):\n            raise TypeError(\"type must be of type TYPE_RSA\")\n        if not isinstance(bits, int):\n            raise TypeError(\"bits must be an integer\")\n        if bits < 0:\n            raise ValueError(\"bits must be non-negative\")\n        if bits > self._key_bits:\n            raise ValueError(\"bits must be less than or equal to \"\n                             \"the key size\")\n        self._key_type = type\n        self._key_bits = bits\n\n    def generate_key(self, bits):\n        \"\"\"\n        Generate a key pair of the given type, with the given number of bits.\n\n        This generates a key \"into\" the this object.\n\n        :param bits: The number of bits.\n        :type bits: :py:data:`int` ``>= 0``\n        :raises TypeError: If :py:data", "solution": "\n        if not isinstance(type, int):\n            raise TypeError(\"type must be an integer\")\n\n        if not isinstance(bits, int):\n            raise TypeError(\"bits must be an integer\")\n\n        if type == TYPE_RSA:\n            if bits <= 0:\n                raise ValueError(\"Invalid number of bits\")\n\n            # TODO Check error return\n            exponent = _lib.BN_new()\n            exponent = _ffi.gc(exponent, _lib.BN_free)\n            _lib.BN_set_word(exponent, _lib.RSA_F4)\n\n            rsa = _lib.RSA_new()\n\n            result = _lib.RSA_generate_key_ex(rsa, bits, exponent, _ffi.NULL)\n            _openssl_assert(result == 1)\n\n            result = _lib.EVP_PKEY_assign_RSA(self._pkey, rsa)\n            _openssl_assert(result == 1)\n\n        elif type == TYPE_DSA:\n            dsa = _lib.DSA_new()\n            _openssl_assert(dsa != _ffi.NULL)\n\n            dsa = _ffi.gc(dsa, _lib.DSA_free)\n            res = _lib.DSA_generate_parameters_ex(\n                dsa, bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL\n            )\n            _openssl_assert(res == 1)\n\n            _openssl_assert(_lib.DSA_generate_key(dsa) == 1)\n            _openssl_assert(_lib.EVP_PKEY_set1_DSA(self._pkey, dsa) == 1)\n        else:\n            raise Error(\"No such key type\")\n\n        self._initialized = True"}
{"prompt": "def v2_runner_on_skipped(self, result, **kwargs):\n        \"\"\"Run when a task is skipped.\"\"\"", "output": "\n        self.log.debug('Running v2 runner on_skipped for task %s', result.task_id)\n        task_result = self.tasks.get(result.task_id)\n        if task_result and task_result.is_skipped:\n            task_result.on_skipped(result, **kwargs)\n\n    def v2_runner_on_failed(self, result, **kwargs):\n        \"\"\"Run when a task fails.\"\"\"\n        self.log.debug('Running v2 runner on_failed for task %s', result.task_id)\n        task_result = self.tasks.get(result.task_id)\n        if task_result and task_result.is_failed:\n            task_result.on_failed(result, **kwargs)\n\n    def v2_runner_on_unreachable(self, result, **kwargs):\n        \"\"\"Run when a task un", "solution": "\n        if self._display.verbosity > 1:\n            self._print_task()\n            self.last_skipped = False\n\n            line_length = 120\n            spaces = \" \" * (31 - len(result._host.name) - 4)\n\n            line = \"  * {}{}- {}\".format(\n                colorize(result._host.name, \"not_so_bold\"),\n                spaces,\n                colorize(\"skipped\", \"skipped\"),\n            )\n\n            reason = result._result.get(\"skipped_reason\", \"\") or result._result.get(\n                \"skip_reason\", \"\"\n            )\n            if len(reason) < 50:\n                line += \" -- {}\".format(reason)\n                print(\"{} {}---------\".format(line, \"-\" * (line_length - len(line))))\n            else:\n                print(\"{} {}\".format(line, \"-\" * (line_length - len(line))))\n                print(self._indent_text(reason, 8))\n                print(reason)"}
{"prompt": "def _estimate_gas(self,\n                      master_copy: str,\n                      initializer: bytes,\n                      funder: str,\n                      payment_token: str) -> int:\n        \"\"\"\n        Gas estimation done using web3 and calling the node\n        Payment cannot be estimated, as no ether is in the address. So we add some gas later.\n        :param master_copy: Master Copy of Gnosis Safe already deployed\n        :param initializer: Data initializer to send to GnosisSafe setup method\n        :param funder: Address that should get the payment (if payment set)\n        :param payment_token: Address if a token is used. If not set, 0x0 will be ether\n        :return: Total gas estimation\n        \"\"\"", "output": "will be used\n        :return: Gas estimated for payment (including gas for token if needed)\n        \"\"\"\n        web3 = Web3(Web3.HTTPProvider(GNOSIS_URL))\n        contract = w3.eth.contract(address=master_copy, abi=GNOSIS_ABI,\n                                   bytecode=GNOSIS_BYTECODE)\n        txn_hash = contract.functions.estimate_gas(initializer, funder, payment_token).transact()\n        txn_receipt = web3.eth.waitForTransactionReceipt(txn_hash)\n        return txn_receipt.gasUsed\n\n    def _estimate_g", "solution": "\n\n        # Estimate the contract deployment. We cannot estimate the refunding, as the safe address has not any fund\n        gas: int = self._build_proxy_contract_creation_constructor(\n            master_copy,\n            initializer,\n            funder,\n            payment_token,\n            0).estimateGas()\n\n        # We estimate the refund as a new tx\n        if payment_token == NULL_ADDRESS:\n            # Same cost to send 1 ether than 1000\n            gas += self.w3.eth.estimateGas({'to': funder, 'value': 1})\n        else:\n            # Top should be around 52000 when storage is needed (funder no previous owner of token),\n            # we use value 1 as we are simulating an internal call, and in that calls you don't pay for the data.\n            # If it was a new tx sending 5000 tokens would be more expensive than sending 1 because of data costs\n            try:\n                gas += get_erc20_contract(self.w3,\n                                          payment_token).functions.transfer(funder, 1).estimateGas({'from':\n                                                                                                    payment_token})\n            except ValueError as exc:\n                raise InvalidERC20Token from exc\n\n        return gas"}
{"prompt": "def reg_on_abort(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"", "output": "\n        self.register_function(callable_object, *args, **kwargs)\n\n    def register_function(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"\n        self.registered_functions[callable_object] = args, kwargs\n\n    def register_method(self, callable_object, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"\n        self.registered_methods[callable_object] = args, kwargs\n\n    def register_functions(self, callable_objects, *args, **kwargs):\n        \"\"\" Register a function/method to be called when execution is aborted\"\"\"\n        for callable_object in callable_objects:\n            self.register_function(callable_object, *args, **kwargs)\n\n    def register_methods(self, callable_objects, ", "solution": "\n        persistent = kwargs.pop('persistent', False)\n        event = self._create_event(callable_object, 'abort', persistent, *args, **kwargs)\n        self.abort_callbacks.append(event)\n        return event"}
{"prompt": "def wa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wolfram|Alpha. Returns the first\n\tresult, if available.\n\t\"\"\"", "output": "\n\ttry:\n\t\tresult = client.query(event, nick)\n\texcept client.errors.CommandError:\n\t\treturn\n\telse:\n\t\tclient.send_message(event, channel, result)\n\ndef waa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free-text query resolver by Wolfram|Alpha. Returns the first\n\tresult, if available, as a list of results.\n\t\"\"\"\n\ttry:\n\t\tresult = client.query(event, nick)\n\texcept client.errors.CommandError:\n\t\treturn\n\telse:\n\t\tclient.send_message(event, channel, result)\n\ndef waa(client, event, channel, nick, rest):\n\t\"\"\"\n\tA free", "solution": "\n\tclient = wolframalpha.Client(pmxbot.config['Wolfram|Alpha API key'])\n\tres = client.query(rest)\n\treturn next(res.results).text"}
{"prompt": "def update_(self, sct_dict, conf_arg=True):\n        \"\"\"Update values of configuration section with dict.\n\n        Args:\n            sct_dict (dict): dict indexed with option names. Undefined\n                options are discarded.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"", "output": "\n        conf_arg = self._check_conf_arg(conf_arg)\n        for section in sct_dict:\n            section = section.split('.')\n            section = section[:-1]\n            section = section[-1]\n            section = section.replace('_', '-')\n            section = section.replace('-', '_')\n            section = section.replace(' ', '_')\n            section = section.replace(':', '_')\n            section = section.replace('-', '_')\n            section = section.replace('_', '-')\n            section = section.replace('-', '_')\n            section = section.replace(' ', '_')\n            section = section.replace(':', '_')\n            section = section.replace('-', '_')\n", "solution": "\n        for opt, val in sct_dict.items():\n            if opt not in self.def_:\n                continue\n            if not conf_arg or self.def_[opt].conf_arg:\n                self[opt] = val"}
{"prompt": "def poke(self, context):\n        \"\"\"\n        poke matching files in a directory with self.regex\n\n        :return: Bool depending on the search criteria\n        \"\"\"", "output": "\n        search = self.regex.search(context.path)\n        if search:\n            context.path = self.regex.sub('', context.path)\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('', self.regex.sub('', context.path))\n            context.path = self.regex.sub('", "solution": "\n        sb = self.hook(self.hdfs_conn_id).get_conn()\n        self.log.info(\n            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern\n        )\n        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if\n                  f['file_type'] == 'f' and\n                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]\n        result = self.filter_for_ignored_ext(result, self.ignored_ext,\n                                             self.ignore_copying)\n        result = self.filter_for_filesize(result, self.file_size)\n        return bool(result)"}
{"prompt": "def breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds and returns breadcrumb trail structure for 'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        :rtype: list|str\n        \"\"\"", "output": "\n        sitetree_context = context.get('sitetree_context')\n        sitetree_context['breadcrumbs'] = self.build_breadcrumbs(\n            tree_alias, sitetree_context)\n        return sitetree_context['breadcrumbs']\n\n    def build_breadcrumbs(self, tree_alias, context):\n        \"\"\"Builds and returns breadcrumb trail structure for'sitetree_breadcrumbs' tag.\n\n        :param str|unicode tree_alias:\n        :param Context context:\n        :rtype: list|str\n        \"\"\"\n        sitetree_context = context.get('sitetree_context')\n        sitetree_context['breadcrumbs'] = []\n        sitetree_context['breadcrumbs'].append(\n            sitetree_context['sitetree'].breadcrumb(tree_alias))\n        return sitetree_", "solution": "\n        tree_alias, sitetree_items = self.init_tree(tree_alias, context)\n\n        if not sitetree_items:\n            return ''\n\n        current_item = self.get_tree_current_item(tree_alias)\n\n        breadcrumbs = []\n\n        if current_item is not None:\n\n            context_ = self.current_page_context\n            check_access = self.check_access\n            get_item_by_id = self.get_item_by_id\n\n            def climb(base_item):\n                "}
{"prompt": "def getCSV(self):\n        \"\"\"\n        Returns\n        -------\n        filename: str\n        \"\"\"", "output": "\n        return self.filename\n\n    def getCSVPath(self):\n        \"\"\"\n        Returns\n        -------\n        path: str\n        \"\"\"\n        return self.path\n\n    def getCSVPaths(self):\n        \"\"\"\n        Returns\n        -------\n        paths: list\n        \"\"\"\n        return self.paths\n\n    def getCSVPathsNames(self):\n        \"\"\"\n        Returns\n        -------\n        names: list\n        \"\"\"\n        return self.names\n\n    def getCSVPathsNamesPaths(self):\n        \"\"\"\n        Returns\n        -------\n        names: list\n        paths: list\n        \"\"\"\n        return self.names, self.paths\n\n    def getCSVPathsNamesPathsNames(self):\n        \"\"\"\n        Returns\n        -------\n        names: list\n        names: list\n        paths: list\n        paths: list\n", "solution": "\n        import getpass\n        import gspread\n\n        user = raw_input(\"Insert Google username:\")\n        password = getpass.getpass(prompt=\"Insert password:\")\n        name = raw_input(\"SpreadSheet filename on Drive:\")\n        sheet = raw_input(\"Sheet name (first sheet is default):\")\n\n        cl = gspread.login(user, password)\n        sh = cl.open(name)\n\n        if not (sheet.strip()):\n            ws = sh.sheet1\n            sheet = \"1\"\n        else:\n            ws = sh.worksheet(sheet)\n\n        filename = name + '-worksheet_' + sheet + '.csv'\n        with open(filename, 'wb') as f:\n            writer = UnicodeWriter(f)\n            writer.writerows(ws.get_all_values())\n\n        return filename"}
{"prompt": "def hunt_repeated_yaml_keys(data):\n    \"\"\"Parses yaml and returns a list of repeated variables and\n       the line on which they occur\n    \"\"\"", "output": "\n    yaml_keys = []\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_lines = []\n    yaml_line = None\n    yaml_key = None\n    yaml_value = None\n    yaml_", "solution": "\n    loader = yaml.Loader(data)\n\n    def compose_node(parent, index):\n        # the line number where the previous token has ended (plus empty lines)\n        line = loader.line\n        node = Composer.compose_node(loader, parent, index)\n        node.__line__ = line + 1\n        return node\n\n    def construct_mapping(node, deep=False):\n        mapping = dict()\n        errors = dict()\n        for key_node, value_node in node.value:\n            key = key_node.value\n            if key in mapping:\n                if key in errors:\n                    errors[key].append(key_node.__line__)\n                else:\n                    errors[key] = [mapping[key], key_node.__line__]\n\n            mapping[key] = key_node.__line__\n\n        return errors\n\n    loader.compose_node = compose_node\n    loader.construct_mapping = construct_mapping\n    data = loader.get_single_data()\n    return data"}
{"prompt": "def deserialize_dataframe(reader, data_type_id):\n    \"\"\"\n    Deserialize a dataframe.\n\n    Parameters\n    ----------\n    reader : file\n        File-like object to read from. Must be opened in binary mode.\n    data_type_id : dict\n        Serialization format of the raw data.\n        See the azureml.DataTypeIds class for constants.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe object.\n    \"\"\"", "output": "\n    header = reader.readline().strip().split()\n    header = [header.pop(0), *header]\n    header = [data_type_id.get(h, h) for h in header]\n    header = [float(x) for x in header]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x is not None]\n    header = [x for x in header if x ", "solution": "\n    _not_none('reader', reader)\n    _not_none_or_empty('data_type_id', data_type_id)\n\n    serializer = _SERIALIZERS.get(data_type_id)\n    if serializer is None:\n        raise UnsupportedDatasetTypeError(data_type_id)\n    return serializer[1](reader=reader)"}
{"prompt": "def asin(wave):\n    r\"\"\"\n    Return the arc sine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.asin\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"", "output": "\n    return asin_wave(wave)\n\n\ndef asin_wave(wave):\n    r\"\"\"\n    Return the arc sine of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n   .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n   .. Auto-generated exceptions documentation for\n   .. peng.wave_functions.asin_wave\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n", "solution": "\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"asin\", \"rad\", np.arcsin)"}
{"prompt": "def _on_parent_event_dependent(self):\n        \"\"\"\n        After parrent statement become event dependent\n        propagate event dependency flag to child statements\n        \"\"\"", "output": "\n        for stmt in self.statements:\n            stmt._on_parent_event_dependent()\n\n    def _on_parent_event_undependent(self):\n        \"\"\"\n        After parrent statement become event undependent\n        propagate event dependency flag to child statements\n        \"\"\"\n        for stmt in self.statements:\n            stmt._on_parent_event_undependent()\n\n    def _on_parent_event_undepended_all(self):\n        \"\"\"\n        After parrent statement become event undependent and all child statements\n        become event undependent and all child statements become event undependent and all child statements\n        become event undependent and all child statements become ", "solution": "\n        if not self._is_completly_event_dependent:\n            self._is_completly_event_dependent = True\n            for stm in self._iter_stms():\n                stm._on_parent_event_dependent()"}
{"prompt": "def allclose(a, b):\n    \"\"\"\n    Test that a and b are close and match in shape.\n\n    Parameters\n    ----------\n    a : ndarray\n        First array to check\n\n    b : ndarray\n        First array to check\n    \"\"\"", "output": "\n    if a.shape!= b.shape:\n        return False\n    if a.dtype!= b.dtype:\n        return False\n    if a.size == 0:\n        return True\n    return allclose_or_equiv(a, b, equal_nan=True)\n\n\ndef allclose_or_equiv(a, b, equal_nan=False):\n    \"\"\"\n    Test whether two arrays are element-wise equal up to and/or along a\n    given axis, using either `allclose` or `equal`.\n\n    Parameters\n    ----------\n    a, b : ndarray\n        Input arrays to compare.\n    axis : int, optional\n        Axis along which to compare.\n    equal_nan : bool, optional\n        Whether to compare NaN's as equal. Default is False.\n\n    Returns\n    -------\n    bool\n        Whether the arrays are equal.\n\n    See Also", "solution": "\n    from numpy import allclose\n    return (a.shape == b.shape) and allclose(a, b)"}
{"prompt": "def signFix(val, width):\n    \"\"\"\n    Convert negative int to positive int which has same bits set\n    \"\"\"", "output": "\n    val = abs(val)\n    sign = val < 0\n    if sign:\n        val = -val\n        width = -width\n    return val * width\n\n\ndef signExtend(val, width, sign):\n    \"\"\"\n    Extend a number with sign bit\n    \"\"\"\n    val = abs(val)\n    sign = val < 0\n    if sign:\n        val = -val\n        width = -width\n    return val + width * (sign)\n\n\ndef signExtendExtend(val, width, sign, extend):\n    \"\"\"\n    Extend a number with sign bit and extend bit\n    \"\"\"\n    val = abs(val)\n    sign = val < 0\n", "solution": "\n    if val > 0:\n        msb = 1 << (width - 1)\n        if val & msb:\n            val -= mask(width) + 1\n    return val"}
{"prompt": "def list(self):\n        \"\"\"\n        Get all current labels\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"", "output": "\n        return self._get('/labels')\n\n    def list_by_project(self, project_id):\n        \"\"\"\n        Get all labels for a project\n\n        :param project_id: The ID of the project to get labels for\n        :type project_id: str\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        return self._get('/labels/%s' % project_id)\n\n    def list_by_service(self, service_id):\n        \"\"\"\n        Get all labels for a service\n\n        :param service_id: The ID of the service to get labels for\n        :type service_id: str\n\n        :return: The Logentries API response\n        :rtype: list of dict\n\n", "solution": "\n        return self._post(\n            request='list',\n            uri=ApiUri.TAGS.value,\n        ).get('tags')"}
{"prompt": "def add_menu_action(self, menu, action, defer_shortcut=False):\n        \"\"\"Add action to menu as well as self\n        \n        So that when the menu bar is invisible, its actions are still available.\n        \n        If defer_shortcut is True, set the shortcut context to widget-only,\n        where it will avoid conflict with shortcuts already bound to the\n        widgets themselves.\n        \"\"\"", "output": "\n        \n        menu.add_action(action)\n        \n        if defer_shortcut:\n            action.shortcut = 'alt+' + action.shortcut\n            \n            \nclass MenuBar(Menu):\n    \"\"\"Menu bar that is always visible\"\"\"\n    \n    \nclass MenuBarWidget(MenuBar):\n    \"\"\"Menu bar that is always visible, with shortcuts bound to widgets\"\"\"\n    \n    \nclass MenuBarShortcut(MenuBar):\n    \"\"\"Menu bar that is always visible, with shortcuts bound to widgets\"\"\"\n    \n    \nclass MenuBarShortcutWidget(MenuBarShortcut):\n    \"\"\"Menu ", "solution": "\n        menu.addAction(action)\n        self.addAction(action)\n\n        if defer_shortcut:\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)"}
{"prompt": "def from_string(bnf: str, entry=None, *optional_inherit) -> Grammar:\n    \"\"\"\n    Create a Grammar from a string\n    \"\"\"", "output": "\n    return Grammar.from_string(\n        bnf, entry, *optional_inherit,\n        from_string=from_string,\n        from_string_error=from_string_error,\n        from_string_error_code=from_string_error_code,\n        from_string_error_code_line=from_string_error_code_line,\n        from_string_error_code_column=from_string_error_code_column,\n        from_string_error_code_lines=from_string_error_code_lines,\n        from_string_error_code_columns=from_string_error_code_columns,\n        from_string_error_code_line=from_string_error_code_line,\n        from_string_error_code_column=from_string_error_code_column,\n        from_string_", "solution": "\n    inherit = [Grammar] + list(optional_inherit)\n    scope = {'grammar': bnf, 'entry': entry}\n    return build_grammar(tuple(inherit), scope)"}
{"prompt": "def parse_statement(self):\n    \"\"\"Parse a single statement.\n\n    Returns:\n      Either a `BindingStatement`, `ImportStatement`, `IncludeStatement`, or\n      `None` if no more statements can be parsed (EOF reached).\n    \"\"\"", "output": "\n    statement = None\n    while True:\n      token = self.token()\n      if token == tokenize.COMMENT:\n        continue\n      elif token == tokenize.STRING:\n        statement = StringStatement(token, self.current_line, self.current_column)\n      elif token == tokenize.INDENT:\n        self.indent()\n      elif token == tokenize.DEDENT:\n        self.dedent()\n      elif token == tokenize.NEWLINE:\n        self.new_line()\n      elif token == tokenize.ENDMARKER:\n        break\n      else:\n        raise ParseError('Invalid token: %s' % token)\n    return statement\n\n  def parse_statements(self):\n    \"\"\"Parse a list of statements.\n\n    Returns:\n      A list of `BindingStatement", "solution": "\n    self._skip_whitespace_and_comments()\n    if self._current_token.kind == tokenize.ENDMARKER:\n      return None\n\n    # Save off location, but ignore char_num for any statement-level errors.\n    stmt_loc = self._current_location(ignore_char_num=True)\n    binding_key_or_keyword = self._parse_selector()\n    statement = None\n    if self._current_token.value != '=':\n      if binding_key_or_keyword == 'import':\n        module = self._parse_selector(scoped=False)\n        statement = ImportStatement(module, stmt_loc)\n      elif binding_key_or_keyword == 'include':\n        str_loc = self._current_location()\n        success, filename = self._maybe_parse_basic_type()\n        if not success or not isinstance(filename, str):\n          self._raise_syntax_error('Expected file path as string.', str_loc)\n        statement = IncludeStatement(filename, stmt_loc)\n      else:\n        self._raise_syntax_error(\"Expected '='.\")\n    else:  # We saw an '='.\n      self._advance_one_token()\n      value = self.parse_value()\n      scope, selector, arg_name = parse_binding_key(binding_key_or_keyword)\n      statement = BindingStatement(scope, selector, arg_name, value, stmt_loc)\n\n    assert statement, 'Internal parsing error.'\n\n    if (self._current_token.kind != tokenize.NEWLINE and\n        self._current_token.kind != tokenize.ENDMARKER):\n      self._raise_syntax_error('Expected newline.')\n    elif self._current_token.kind == tokenize.NEWLINE:\n      self._advance_one_token()\n\n    return statement"}
{"prompt": "def add_xpaths_to_stream_item(si):\n    \"\"\"Mutably tag tokens with xpath offsets.\n\n    Given some stream item, this will tag all tokens from all taggings\n    in the document that contain character offsets. Note that some\n    tokens may not have computable xpath offsets, so an xpath offset\n    for those tokens will not be set. (See the documentation and\n    comments for ``char_offsets_to_xpaths`` for what it means for a\n    token to have a computable xpath.)\n\n    If a token can have its xpath offset computed, it is added to its\n    set of offsets with a ``OffsetType.XPATH_CHARS`` key.\n    \"\"\"", "output": "\n    offsets = si.get_offsets()\n    for t in si.tokens:\n        offset = offsets[t.token_offset]\n        if offset is not None:\n            t.add_xpath_offset(offset)\n\n\ndef char_offsets_to_xpaths(si):\n    \"\"\"Mutably tag tokens with xpath offsets.\n\n    Given some stream item, this will tag all tokens from all taggings\n    in the document that contain character offsets. Note that some\n    tokens may not have computable xpath offsets, so an xpath offset\n    for those tokens will not be set. (See the documentation and\n    comments for ``char_offsets_to_xpaths`` for what it means for a\n    token to have a ", "solution": "\n    def sentences_to_xpaths(sentences):\n        tokens = sentences_to_char_tokens(sentences)\n        offsets = char_tokens_to_char_offsets(tokens)\n        return char_offsets_to_xpaths(html, offsets)\n\n    def xprange_to_offset(xprange):\n        return Offset(type=OffsetType.XPATH_CHARS,\n                      first=xprange.start_offset, length=0,\n                      xpath=xprange.start_xpath,\n                      content_form='clean_html', value=None,\n                      xpath_end=xprange.end_xpath,\n                      xpath_end_offset=xprange.end_offset)\n\n    html = unicode(si.body.clean_html, 'utf-8')\n    for sentences in si.body.sentences.itervalues():\n        tokens = sentences_to_char_tokens(sentences)\n        for token, xprange in izip(tokens, sentences_to_xpaths(sentences)):\n            if xprange is None:\n                continue\n            offset = xprange_to_offset(xprange)\n            token.offsets[OffsetType.XPATH_CHARS] = offset"}
{"prompt": "def fill_parameters(self, path, blocks, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill all blocks sequentially.\n        :type blocks: list of deepy.layers.Block\n        \"\"\"", "output": "\n        \n        # load parameters\n        parameters = load_parameters(path)\n        \n        # fill parameters\n        for block in blocks:\n            block.fill_parameters(parameters, exclude_free_params, check_parameters)\n            \n    def fill_parameters_from_file(self, path, blocks, exclude_free_params=False, check_parameters=False):\n        \"\"\"\n        Load parameters from file to fill all blocks sequentially.\n        :type blocks: list of deepy.layers.Block\n        \"\"\"\n        \n        # load parameters\n        parameters = load_parameters(path)\n        \n        # fill parameters\n        for block in blocks:\n            block.fill_parameters(parameters, exclude_free_params, check_parameters)\n            \n    def fill_parameters_from_dict(self, parameters, exclude", "solution": "\n        if not os.path.exists(path):\n            raise Exception(\"model {} does not exist\".format(path))\n        # Decide which parameters to load\n        normal_params = sum([nn.parameters for nn in blocks], [])\n        all_params = sum([nn.all_parameters for nn in blocks], [])\n        # Load parameters\n        if path.endswith(\".gz\"):\n            opener = gzip.open if path.lower().endswith('.gz') else open\n            handle = opener(path, 'rb')\n            saved_params = pickle.load(handle)\n            handle.close()\n            # Write parameters\n            if len(all_params) != len(saved_params):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(saved_params)))\n            for target, source in zip(all_params, saved_params):\n                if not exclude_free_params or target not in normal_params:\n                    target.set_value(source)\n        elif path.endswith(\".npz\"):\n            arrs = np.load(path)\n            # Write parameters\n            if len(all_params) != len(arrs.keys()):\n                logging.warning(\n                    \"parameters in the network: {}, parameters in the dumped model: {}\".format(len(all_params),\n                                                                                               len(arrs.keys())))\n            for target, idx in zip(all_params, range(len(arrs.keys()))):\n                if not exclude_free_params or target not in normal_params:\n                    source = arrs['arr_%d' % idx]\n                    target.set_value(source)\n        else:\n            raise Exception(\"File format of %s is not supported, use '.gz' or '.npz' or '.uncompressed.gz'\" % path)"}
{"prompt": "def get_endpoint_path(self, endpoint_id):\n    \"\"\"return the first fullpath to a folder in the endpoint based on\n       expanding the user's home from the globus config file. This\n       function is fragile but I don't see any other way to do it.\n    \n       Parameters\n       ==========\n       endpoint_id: the endpoint id to look up the path for\n\n    \"\"\"", "output": "\n    \n    home = os.path.expanduser('~')\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(home)\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(home)\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(home)\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(home)\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(home)\n    \n    home = os.path.expanduser(home)\n    \n    home = os.path.expandvars(", "solution": "\n    config = os.path.expanduser(\"~/.globusonline/lta/config-paths\")\n    if not os.path.exists(config):\n        bot.error('%s not found for a local Globus endpoint.')\n        sys.exit(1)\n\n    path = None\n\n    # Read in the config and get the root path\n\n    config = [x.split(',')[0] for x in read_file(config)]\n    for path in config:\n        if os.path.exists(path):\n            break\n\n    # If we don't have an existing path, exit\n\n    if path is None:\n        bot.error('No path was found for a local Globus endpoint.')\n        sys.exit(1)\n\n    return path"}
{"prompt": "def _request_activity_list(self, athlete):\n        \"\"\"Actually do the request for activity list\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete -- Full name of athlete\n        \"\"\"", "output": "\n        athlete_url = URL_ACTIVITY_LIST.format(athlete)\n        athlete_headers = {'User-Agent': USER_AGENT}\n        athlete_response = requests.get(athlete_url, headers=athlete_headers)\n        athlete_response_json = athlete_response.json()\n        athlete_response_json['athlete'] = athlete\n        return athlete_response_json\n\n    def _parse_activity_list(self, athlete_response_json):\n        \"\"\"Parse the activity list response\n        This call is slow and therefore this method is memory cached.\n\n        Keyword arguments:\n        athlete_response_json -- Response from athlete API call\n        \"\"\"\n        ", "solution": "\n        response = self._get_request(self._athlete_endpoint(athlete))\n        response_buffer = StringIO(response.text)\n\n        activity_list = pd.read_csv(\n            filepath_or_buffer=response_buffer,\n            parse_dates={'datetime': ['date', 'time']},\n            sep=',\\s*',\n            engine='python'\n        )\n        activity_list.rename(columns=lambda x: x.lower(), inplace=True)\n        activity_list.rename(\n            columns=lambda x: '_' + x if x[0].isdigit() else x, inplace=True)\n\n        activity_list['has_hr'] = activity_list.average_heart_rate.map(bool)\n        activity_list['has_spd'] = activity_list.average_speed.map(bool)\n        activity_list['has_pwr'] = activity_list.average_power.map(bool)\n        activity_list['has_cad'] = activity_list.average_heart_rate.map(bool)\n        activity_list['data'] = pd.Series(dtype=np.dtype(\"object\"))\n        return activity_list"}
{"prompt": "def reconnectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):\n    \"\"\"\n    Reconnect model signals to unit to run simulation with simulation model\n    but use original unit interfaces for communication\n\n    :param synthesisedUnitOrIntf: interface where should be signals\n        replaced from signals from modelCls\n    :param modelCls: simulation model form where signals\n        for synthesisedUnitOrIntf should be taken\n    \"\"\"", "output": "\n    synthesisedUnitOrIntf.reconnectSignals(modelCls)\n\n\ndef connectUnitSignalsToModel(synthesisedUnitOrIntf, modelCls):\n    \"\"\"\n    Connect model signals to unit to run simulation with simulation model\n    but use original unit interfaces for communication\n\n    :param synthesisedUnitOrIntf: interface where should be signals\n        replaced from signals from modelCls\n    :param modelCls: simulation model form where signals\n        for synthesisedUnitOrIntf should be taken\n    \"\"\"\n    synthesisedUnitOrIntf.connectSignals(modelCls)\n\n\ndef connectUnitSignalsTo", "solution": "\n    obj = synthesisedUnitOrIntf\n    subInterfaces = obj._interfaces\n\n\n    if subInterfaces:\n        for intf in subInterfaces:\n            # proxies are destroyed on original interfaces and only proxies on\n            # array items will remain\n            reconnectUnitSignalsToModel(intf, modelCls)\n    else:\n        # reconnect signal from model\n        s = synthesisedUnitOrIntf\n        s._sigInside = getattr(modelCls, s._sigInside.name)"}
{"prompt": "def lines_without_stdlib(self):\n        \"\"\"Filters code from standard library from self.lines.\"\"\"", "output": "\n        lines = list(self.lines)\n        lines.remove('#!')\n        lines.remove('#!python')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove('#!pythonw')\n        lines.remove", "solution": "\n        prev_line = None\n        current_module_path = inspect.getabsfile(inspect.currentframe())\n        for module_path, lineno, runtime in self.lines:\n            module_abspath = os.path.abspath(module_path)\n            if not prev_line:\n                prev_line = [module_abspath, lineno, runtime]\n            else:\n                if (not check_standard_dir(module_path) and\n                        module_abspath != current_module_path):\n                    yield prev_line\n                    prev_line = [module_abspath, lineno, runtime]\n                else:\n                    prev_line[2] += runtime\n        yield prev_line"}
{"prompt": "def from_ascii(path, seperator=None, names=True, skip_lines=0, skip_after=0, **kwargs):\n    \"\"\"\n    Create an in memory DataFrame from an ascii file (whitespace seperated by default).\n\n    >>> ds = vx.from_ascii(\"table.asc\")\n    >>> ds = vx.from_ascii(\"table.csv\", seperator=\",\", names=[\"x\", \"y\", \"z\"])\n\n    :param path: file path\n    :param seperator: value seperator, by default whitespace, use \",\" for comma seperated values.\n    :param names: If True, the first line is used for the column names, otherwise provide a list of strings with names\n    :param skip_lines: skip lines at the start of the file\n    :param skip_after: skip lines at the end of the file\n    :param kwargs:\n    :rtype: DataFrame\n    \"\"\"", "output": "file\n    :param kwargs: additional keyword arguments\n    :return: DataFrame\n    \"\"\"\n    lines = open(path).readlines()\n    header = lines.pop(0).split(seperator)\n    if skip_lines:\n        lines = lines[:skip_lines]\n    if skip_after:\n        lines = lines[skip_after:]\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(lambda x: x.strip(), lines)\n    lines = map(", "solution": "\n\n    import vaex.ext.readcol as rc\n    ds = vaex.dataframe.DataFrameArrays(path)\n    if names not in [True, False]:\n        namelist = names\n        names = False\n    else:\n        namelist = None\n    data = rc.readcol(path, fsep=seperator, asdict=namelist is None, names=names, skipline=skip_lines, skipafter=skip_after, **kwargs)\n    if namelist:\n        for name, array in zip(namelist, data.T):\n            ds.add_column(name, array)\n    else:\n        for name, array in data.items():\n            ds.add_column(name, array)\n    return ds"}
{"prompt": "def ndtr(x, name=\"ndtr\"):\n  \"\"\"Normal distribution function.\n\n  Returns the area under the Gaussian probability density function, integrated\n  from minus infinity to x:\n\n  ```\n                    1       / x\n     ndtr(x)  = ----------  |    exp(-0.5 t**2) dt\n                sqrt(2 pi)  /-inf\n\n              = 0.5 (1 + erf(x / sqrt(2)))\n              = 0.5 erfc(x / sqrt(2))\n  ```\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"ndtr\").\n\n  Returns:\n    ndtr: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"", "output": "Tensor` of type `float32`, `float64`.\n  \"\"\"\n  with ops.name_scope(name, \"ndtr\", [x]) as name:\n    x = ops.convert_to_tensor(x, name=\"x\")\n    return gen_math_ops.erfc_inv(0.5) * math_ops.erfc(x) / math_ops.sqrt(2.0)\n\n\n@tf_export(\"random.poisson\", v1=[])\n@dispatch.add_dispatch_support\ndef poisson(shape, rate=1., dtype=dtypes.float32, name=None):\n  \"\"\"Poisson distribution.\n\n  Samples from a Poisson distribution with rate parameter `rate` and shape\n  `shape`. The generated values follow a Poisson distribution with rate `rate`\n  ", "solution": "\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\n          \"x.dtype=%s is not handled, see docstring for supported types.\"\n          % x.dtype)\n    return _ndtr(x)"}
{"prompt": "def _all_dirs(base_path):\n        \"\"\"\n        Return all dirs in base_path, relative to base_path\n        \"\"\"", "output": "\n        dirs = []\n        for dirpath, dirnames, filenames in os.walk(base_path):\n            dirs.extend(dirnames)\n            dirs.extend(filenames)\n        return dirs\n\n    def _all_files(base_path):\n        \"\"\"\n        Return all files in base_path, relative to base_path\n        \"\"\"\n        files = []\n        for dirpath, dirnames, filenames in os.walk(base_path):\n            files.extend(filenames)\n            files.extend(dirnames)\n        return files\n\n    def _all_files(base_path, ext):\n        \"\"\"\n        Return all files in base_path, relative to base_path\n        \"\"\"\n        files = []\n        for dirpath, dirnames, filenames in os.walk(base_path):\n            files.extend(filenames)\n            files.extend(dirname", "solution": "\n        for root, dirs, files in os.walk(base_path, followlinks=True):\n            for dir in dirs:\n                yield os.path.relpath(os.path.join(root, dir), base_path)"}
{"prompt": "def dump_encoding( file, encoding_name, encoding_list ):\n  \"\"\"dump a given encoding\"\"\"", "output": "\n  file.write( \"# encoding: %s \\n\" % encoding_name.upper() )\n  for encoding in encoding_list:\n    file.write( \"# %s\\n\" % encoding )\n    file.write( \"# %s\\n\" % encoding.upper() )\n    file.write( \"# %s\\n\" % encoding.upper().replace( \"-\", \"_\" ) )\n    file.write( \"# %s\\n\" % encoding.upper().replace( \"_\", \"-\" ) )\n    file.write( \"# %s\\n\" % encoding.upper().replace( \"-\", \"_\" ) )\n    file.write( \"# %s\\n\" % encoding.upper().replace( \"_\", \"-\" ) )\n    file.write( \"# %s\\n\" % encoding.upper().replace( \"-\", \"", "solution": "\n\n  write = file.write\n  write( \"  /* the following are indices into the SID name table */\\n\" )\n  write( \"  static const unsigned short  \" + encoding_name +\n         \"[\" + repr( len( encoding_list ) ) + \"] =\\n\" )\n  write( \"  {\\n\" )\n\n  line  = \"    \"\n  comma = \"\"\n  col   = 0\n  for value in encoding_list:\n    line += comma\n    line += \"%3d\" % value\n    comma = \",\"\n    col  += 1\n    if col == 16:\n      col = 0\n      comma = \",\\n    \"\n\n  write( line + \"\\n  };\\n\\n\\n\" )"}
{"prompt": "def _set_kernel_manager(self, kernel_manager):\n        \"\"\" Disconnect from the current kernel manager (if any) and set a new\n            kernel manager.\n        \"\"\"", "output": "\n        if kernel_manager is not None:\n            kernel_manager.__exit__(self.kernel_manager_closed, None)\n            self.kernel_manager = kernel_manager\n            self.kernel_manager_closed = False\n\n    def start_channels(self, channels):\n        \"\"\" Start a number of channels connected to the current kernel.\n        \"\"\"\n        if self.kernel_manager is None:\n            raise RuntimeError(\"No kernel manager has been started yet.\")\n        self.kernel_manager.start_channels(channels)\n\n    def stop_channels(self):\n        \"\"\" Stop all channels connected to the current kernel.\n        \"\"\"\n        if self.kernel_manager is None:\n            raise RuntimeError(\"No kernel manager has been started yet.\")\n        self.kernel_manager.stop_channels()\n\n    def start_kernel(self, kernel):\n        \"\"\" Start a kernel with given name.\n", "solution": "\n        # Disconnect the old kernel manager, if necessary.\n        old_manager = self._kernel_manager\n        if old_manager is not None:\n            old_manager.started_kernel.disconnect(self._started_kernel)\n            old_manager.started_channels.disconnect(self._started_channels)\n            old_manager.stopped_channels.disconnect(self._stopped_channels)\n\n            # Disconnect the old kernel manager's channels.\n            old_manager.sub_channel.message_received.disconnect(self._dispatch)\n            old_manager.shell_channel.message_received.disconnect(self._dispatch)\n            old_manager.stdin_channel.message_received.disconnect(self._dispatch)\n            old_manager.hb_channel.kernel_died.disconnect(\n                self._handle_kernel_died)\n\n            # Handle the case where the old kernel manager is still listening.\n            if old_manager.channels_running:\n                self._stopped_channels()\n\n        # Set the new kernel manager.\n        self._kernel_manager = kernel_manager\n        if kernel_manager is None:\n            return\n\n        # Connect the new kernel manager.\n        kernel_manager.started_kernel.connect(self._started_kernel)\n        kernel_manager.started_channels.connect(self._started_channels)\n        kernel_manager.stopped_channels.connect(self._stopped_channels)\n\n        # Connect the new kernel manager's channels.\n        kernel_manager.sub_channel.message_received.connect(self._dispatch)\n        kernel_manager.shell_channel.message_received.connect(self._dispatch)\n        kernel_manager.stdin_channel.message_received.connect(self._dispatch)\n        kernel_manager.hb_channel.kernel_died.connect(self._handle_kernel_died)\n\n        # Handle the case where the kernel manager started channels before\n        # we connected.\n        if kernel_manager.channels_running:\n            self._started_channels()"}
{"prompt": "def subscribe_to_trades(self, pair, **kwargs):\n        \"\"\"Subscribe to the passed pair's trades channel.\n\n        :param pair: str, Symbol pair to request data for\n        :param kwargs:\n        :return:\n        \"\"\"", "output": "\n        symbol = pair.split('/')[0]\n        market = self.market(symbol)\n        marketId = market['id']\n        marketName = market['name']\n        marketType = self.safe_string(market, 'type')\n        marketType = self.safe_string(marketType, 'name')\n        marketType = self.safe_string(marketType,'symbol')\n        marketType = self.safe_string(marketType, 'id')\n        marketType = self.safe_string(marketType,'symbol')\n        marketType = self.safe_string(marketType, 'id')\n        marketType = self.safe_string(marketType,'symbol')\n        marketType = self.safe_string(marketType, 'id')\n        ", "solution": "\n        identifier = ('trades', pair)\n        self._subscribe('trades', identifier, symbol=pair, **kwargs)"}
{"prompt": "def interact(self, display_banner=None):\n        \"\"\"Closely emulate the interactive Python console.\"\"\"", "output": "\n        try:\n            import readline\n        except ImportError:\n            pass\n        else:\n            # GNU readline doesn't support Ctrl-D\n            def input(prompt):\n                if display_banner is None:\n                    display_banner = True\n                line = input(prompt)\n                if not line:\n                    return line\n                line = line[:-1]\n                if display_banner:\n                    sys.stdout.write('\\n')\n                sys.stdout.write(line)\n                sys.stdout.write('\\n')\n                sys.stdout.flush()\n                line = line[:-1]\n                return line\n            readline.parse_and_bind(\"tab: complete\")\n            readline.parse_and_bind(\"set editing-mode emacs\")\n            readline.parse_and_bind(\"set show-all-if-ambigu", "solution": "\n\n        # batch run -> do not interact\n        if self.exit_now:\n            return\n\n        if display_banner is None:\n            display_banner = self.display_banner\n\n        if isinstance(display_banner, basestring):\n            self.show_banner(display_banner)\n        elif display_banner:\n            self.show_banner()\n\n        more = False\n\n        if self.has_readline:\n            self.readline_startup_hook(self.pre_readline)\n            hlen_b4_cell = self.readline.get_current_history_length()\n        else:\n            hlen_b4_cell = 0\n        # exit_now is set by a call to %Exit or %Quit, through the\n        # ask_exit callback.\n\n        while not self.exit_now:\n            self.hooks.pre_prompt_hook()\n            if more:\n                try:\n                    prompt = self.prompt_manager.render('in2')\n                except:\n                    self.showtraceback()\n                if self.autoindent:\n                    self.rl_do_indent = True\n\n            else:\n                try:\n                    prompt = self.separate_in + self.prompt_manager.render('in')\n                except:\n                    self.showtraceback()\n            try:\n                line = self.raw_input(prompt)\n                if self.exit_now:\n                    # quick exit on sys.std[in|out] close\n                    break\n                if self.autoindent:\n                    self.rl_do_indent = False\n\n            except KeyboardInterrupt:\n                #double-guard against keyboardinterrupts during kbdint handling\n                try:\n                    self.write('\\nKeyboardInterrupt\\n')\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n                    more = False\n                except KeyboardInterrupt:\n                    pass\n            except EOFError:\n                if self.autoindent:\n                    self.rl_do_indent = False\n                    if self.has_readline:\n                        self.readline_startup_hook(None)\n                self.write('\\n')\n                self.exit()\n            except bdb.BdbQuit:\n                warn('The Python debugger has exited with a BdbQuit exception.\\n'\n                     'Because of how pdb handles the stack, it is impossible\\n'\n                     'for IPython to properly format this particular exception.\\n'\n                     'IPython will resume normal operation.')\n            except:\n                # exceptions here are VERY RARE, but they can be triggered\n                # asynchronously by signal handlers, for example.\n                self.showtraceback()\n            else:\n                self.input_splitter.push(line)\n                more = self.input_splitter.push_accepts_more()\n                if (self.SyntaxTB.last_syntax_error and\n                    self.autoedit_syntax):\n                    self.edit_syntax_error()\n                if not more:\n                    source_raw = self.input_splitter.source_raw_reset()[1]\n                    self.run_cell(source_raw, store_history=True)\n                    hlen_b4_cell = \\\n                        self._replace_rlhist_multiline(source_raw, hlen_b4_cell)\n\n        # Turn off the exit flag, so the mainloop can be restarted if desired\n        self.exit_now = False"}
{"prompt": "async def add(self, dialog: Dialog):\n        \"\"\"\n        Adds a new dialog to the set and returns the added dialog.\n        :param dialog: The dialog to add.\n        \"\"\"", "output": "\n        dialogs = self.dialogs\n        if dialog in dialogs:\n            return dialogs[dialog]\n        dialogs[dialog] = dialog\n        return dialog\n\n    async def remove(self, dialog: Dialog):\n        \"\"\"\n        Removes a dialog from the set and returns the removed dialog.\n        :param dialog: The dialog to remove.\n        \"\"\"\n        dialogs = self.dialogs\n        if dialog in dialogs:\n            del dialogs[dialog]\n            return dialogs[dialog]\n        return None\n\n    async def clear(self):\n        \"\"\"\n        Removes all dialogs from the set.\n        \"\"\"\n        self.dialogs = {}\n\n    async def update(self, dialog: Dialog):\n        \"\"\"\n        Updates a dialog in the set and returns the updated dialog.\n        :param dialog", "solution": "\n        if dialog is None or not isinstance(dialog, Dialog):\n            raise TypeError('DialogSet.add(): dialog cannot be None and must be a Dialog or derived class.')\n\n        if dialog.id in self._dialogs:\n            raise TypeError(\"DialogSet.add(): A dialog with an id of '%s' already added.\" % dialog.id)\n\n        # dialog.telemetry_client = this._telemetry_client;\n        self._dialogs[dialog.id] = dialog\n\n        return self"}
{"prompt": "def cycle(self):\n        \"\"\"\n        Request one batch of events from Skype, calling :meth:`onEvent` with each event in turn.\n\n        Subclasses may override this method to alter loop functionality.\n        \"\"\"", "output": "\n        self.skype.sendMessage(\"SENDMSG {0}\".format(\n            self.skype.messageToString(\n                self.skype.messageToSend(\n                    messageType=\"SENDMSG\",\n                    messageText=self.messageText,\n                    messageToSend=self.messageToSend,\n                    messageAttachments=self.messageAttachments,\n                    messageAttachmentsToSend=self.messageAttachmentsToSend,\n                    messageToReply=\"\",\n                    messageToReplyTo=\"\",\n                    messageToReplyToTo=\"\",\n                    messageToReplyToTo=\"\",\n                    messageToReplyToToTo=\"\",\n                    messageToReply", "solution": "\n        try:\n            events = self.getEvents()\n        except requests.ConnectionError:\n            return\n        for event in events:\n            self.onEvent(event)\n            if self.autoAck:\n                event.ack()"}
{"prompt": "def _maybe_validate_perm(perm, validate_args, name=None):\n  \"\"\"Checks that `perm` is valid.\"\"\"", "output": "\n  if validate_args:\n    perm = _validate_perm(perm, name)\n  return perm\n\n\ndef _validate_perm(perm, name=None):\n  \"\"\"Validates a permutation.\n\n  Args:\n    perm: A `Tensor` of type `int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `int64`.\n  \"\"\"\n  with ops.name_scope(name, \"Permute\", [perm]) as name:\n    perm = ops.convert_to_tensor(perm, name=\"perm\")\n    rank = array_ops.rank(perm)\n    if rank is None:\n      raise ValueError(\"`perm` must be at least rank one.\")\n    if rank < 2:\n      raise ValueError(\"`perm` must have ", "solution": "\n  with tf.name_scope(name or 'maybe_validate_perm'):\n    assertions = []\n    if not dtype_util.is_integer(perm.dtype):\n      raise TypeError('`perm` must be integer type')\n\n    msg = '`perm` must be a vector.'\n    if tensorshape_util.rank(perm.shape) is not None:\n      if tensorshape_util.rank(perm.shape) != 1:\n        raise ValueError(\n            msg[:-1] +\n            ', saw rank: {}.'.format(tensorshape_util.rank(perm.shape)))\n    elif validate_args:\n      assertions += [assert_util.assert_rank(perm, 1, message=msg)]\n\n    perm_ = tf.get_static_value(perm)\n    msg = '`perm` must be a valid permutation vector.'\n    if perm_ is not None:\n      if not np.all(np.arange(np.size(perm_)) == np.sort(perm_)):\n        raise ValueError(msg[:-1] + ', saw: {}.'.format(perm_))\n    elif validate_args:\n      assertions += [\n          assert_util.assert_equal(\n              tf.sort(perm), tf.range(tf.size(input=perm)), message=msg)\n      ]\n\n    return assertions"}
{"prompt": "def create_cookie(self, delete=None):\n        \"\"\"\n        Creates the value for ``Set-Cookie`` HTTP header.\n\n        :param bool delete:\n            If ``True`` the cookie value will be ``deleted`` and the\n            Expires value will be ``Thu, 01-Jan-1970 00:00:01 GMT``.\n\n        \"\"\"", "output": "\n        expires = datetime.utcnow() + timedelta(days=self._cookie.max_age)\n        if delete:\n            expires = datetime.utcnow() - timedelta(days=self._cookie.max_age)\n        cookie = Cookie(\n            version=self._cookie.version,\n            name=self._cookie.name,\n            value=self._cookie.value,\n            port=self._cookie.port,\n            domain=self._cookie.domain,\n            domain_specified=self._cookie.domain_specified,\n            domain_initial_dot=self._cookie.domain_initial_dot,\n            path=self._cookie.path,\n            path_specified=self._cookie.path_specified,\n            secure=self._cookie.secure,\n            expires=expires,\n            disc", "solution": "\n        value = 'deleted' if delete else self._serialize(self.data)\n        split_url = parse.urlsplit(self.adapter.url)\n        domain = split_url.netloc.split(':')[0]\n\n        # Work-around for issue #11, failure of WebKit-based browsers to accept\n        # cookies set as part of a redirect response in some circumstances.\n        if '.' not in domain:\n            template = '{name}={value}; Path={path}; HttpOnly{secure}{expires}'\n        else:\n            template = ('{name}={value}; Domain={domain}; Path={path}; '\n                        'HttpOnly{secure}{expires}')\n\n        return template.format(\n            name=self.name,\n            value=value,\n            domain=domain,\n            path=split_url.path,\n            secure='; Secure' if self.secure else '',\n            expires='; Expires=Thu, 01-Jan-1970 00:00:01 GMT' if delete else ''\n        )"}
{"prompt": "def derivative(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the numerical derivative of a waveform's dependent variable vector.\n\n    The method used is the `backwards differences\n    <https://en.wikipedia.org/wiki/\n    Finite_difference#Forward.2C_backward.2C_and_central_differences>`_ method\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.derivative\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"", "output": "or float\n\n    :return: Numerical derivative of waveform's dependent variable vector\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    :Example:\n\n   .. code-block:: python\n\n        >>> from peng.eng.waveform import Waveform\n        >>> from peng.eng.waveform import WaveformError\n        >>> from peng.eng.waveform import WaveformErrorType\n        >>> from peng.eng.waveform import WaveformErrorSubType\n        >>> from peng.eng.waveform import WaveformErrorSubType2\n        >>> from peng.eng.waveform import WaveformErrorSubType3\n        >>> from peng.eng.waveform import ", "solution": "\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    delta_indep = np.diff(ret._indep_vector)\n    delta_dep = np.diff(ret._dep_vector)\n    delta_indep = np.concatenate((np.array([delta_indep[0]]), delta_indep))\n    delta_dep = np.concatenate((np.array([delta_dep[0]]), delta_dep))\n    ret._dep_vector = np.divide(delta_dep, delta_indep)\n    ret.dep_name = \"derivative({0})\".format(ret._dep_name)\n    ret.dep_units = _build_units(ret.indep_units, ret.dep_units, \"/\")\n    return ret"}
{"prompt": "def publish_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"", "output": "\n        endpoint = self.endpoint\n        topic = self.topic\n        client = boto3.client('sns', region_name=endpoint.region_name)\n        client.publish(\n            TopicArn=topic,\n            Message=message,\n            Subject=self.subject,\n            MessageStructure='json'\n        )\n\n    def publish_message_to_target(self, target_arn, message):\n        \"\"\"\n        Publish a message to a topic or an endpoint.\n\n        :param target_arn: either a TopicArn or an EndpointArn\n        :type target_arn: str\n        :param message: the default message you want to send\n        :param message: str\n        \"\"\"\n        endpoint = self.endpoint\n        topic = self.topic\n        client = boto3.client", "solution": "\n\n        conn = self.get_conn()\n\n        messages = {\n            'default': message\n        }\n\n        return conn.publish(\n            TargetArn=target_arn,\n            Message=json.dumps(messages),\n            MessageStructure='json'\n        )"}
{"prompt": "def has_delete_permission(self, request, obj=None):\n        \"\"\"\n        Implement a lookup for object level permissions. Basically the same as\n        ModelAdmin.has_delete_permission, but also passes the obj parameter in.\n        \"\"\"", "output": "\n        if self.opts.auto_created:\n            return False\n        return request.user.has_perm(self.opts.app_label + '.delete_' + obj._meta.object_name)\n\n    def get_actions(self, request):\n        \"\"\"\n        Return a dictionary mapping action names to their display action\n        names, using the same keyword arguments as for the superclass method.\n        \"\"\"\n        actions = super(ModelAdmin, self).get_actions(request)\n        actions.update({\n            'delete_selected': self.delete_selected,\n            'delete_selected_confirm': self.delete_selected_confirm,\n            'delete_selected_all': self.delete_selected_all,\n            'delete_all': self.delete_all,\n            'delete_all_confirm': self.delete_all_confirm,\n            'delete_all_selected': self.delete_all_selected,\n", "solution": "\n        if settings.TREE_EDITOR_OBJECT_PERMISSIONS:\n            opts = self.opts\n            r = request.user.has_perm(opts.app_label + '.' + opts.get_delete_permission(), obj)\n        else:\n            r = True\n\n        return r and super(TreeEditor, self).has_delete_permission(request, obj)"}
{"prompt": "def _clean(self, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Remove all signals from this interface (used after unit is synthesized\n        and its parent is connecting its interface to this unit)\n        \"\"\"", "output": "\n        for signal in self.signals:\n            signal.disconnect()\n        self.signals = []\n        self.interface = None\n        if rmConnetions:\n            self.disconnect()\n        if lockNonExternal:\n            self.lockNonExternal()\n\n    def connect(self, interface, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Connect this interface to another interface\n        \"\"\"\n        self.interface = interface\n        self.disconnect()\n        self.connect(interface, rmConnetions, lockNonExternal)\n\n    def disconnect(self, rmConnetions=True, lockNonExternal=True):\n        \"\"\"\n        Disconnect this interface from another interface\n        \"\"\"\n        self.interface = None\n        if rmConnetions:\n            self.disconnect()", "solution": "\n\n        if self._interfaces:\n            for i in self._interfaces:\n                i._clean(rmConnetions=rmConnetions,\n                         lockNonExternal=lockNonExternal)\n        else:\n            self._sigInside = self._sig\n            del self._sig\n\n        if lockNonExternal and not self._isExtern:\n            self._isAccessible = False"}
{"prompt": "def bytesize(self, byteorder='@'):\n        \"\"\"Compute the byte size after serialization.\n\n        Args:\n            byteorder (str, optional): This is byte order of the serialized data. Use one\n                of the `byte order characters\n                <https://docs.python.org/3/library/struct.html#byte-order-size-and-alignment>`_:\n                ``@``, ``=``, ``<``, ``>``, and ``!``.\n                Default is ``@`` -- the native order.\n\n        Returns:\n            int: Size in number of bytes after serialization.\n        \"\"\"", "output": "\n        byteorders = {\n            '@': 'big',\n            '=': 'little',\n            '<': 'little',\n            '>': 'big',\n            '!': 'big',\n        }\n        byteorder = byteorder.lower()\n        if byteorder not in byteorders:\n            raise ValueError(\"Unknown byte order '%s'\" % byteorder)\n        byteorder = byteorders[byteorder]\n        size = struct.calcsize('!B')\n        size = size + struct.calcsize('!' + byteorder + 'H')\n        return size\n\n    def serialize(self, obj, protocol=3):\n        \"\"\"Serialize an object to a byte string.\n\n        Args:\n            obj (object): Object to serialize.\n            protocol (", "solution": "\n        # Use 8 bytes to store the seed integer\n        seed_size = struct.calcsize(byteorder+'q')\n        # Use 4 bytes to store the number of hash values\n        length_size = struct.calcsize(byteorder+'i')\n        # Use 4 bytes to store each hash value as we are using the lower 32 bit\n        hashvalue_size = struct.calcsize(byteorder+'I')\n        return seed_size + length_size + len(self) * hashvalue_size"}
{"prompt": "def get_conn(self):\n        \"\"\"\n        Returns a FTP connection object\n        \"\"\"", "output": "\n        return self.connection\n\n    def login(self, user, passwd):\n        \"\"\"\n        Log in to the FTP server\n        \"\"\"\n        self.connection.set_pasv(True)\n        self.connection.login(user, passwd)\n\n    def quit(self):\n        \"\"\"\n        Quit the FTP server\n        \"\"\"\n        self.connection.quit()\n\n    def cwd(self, path):\n        \"\"\"\n        Change to the specified path\n        \"\"\"\n        self.cwd(path)\n        self.cwd(path)\n\n    def cwd(self, path):\n        \"\"\"\n        Change to the specified path\n        \"\"\"\n        self.cwd(path)\n        self.cwd(path)\n\n    def mkd(self, path, permissions=0o777, parents=False):\n        \"\"\"\n        Create a directory", "solution": "\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn"}
{"prompt": "def commonprefix(items):\n    \"\"\"Get common prefix for completions\n\n    Return the longest common prefix of a list of strings, but with special\n    treatment of escape characters that might precede commands in IPython,\n    such as %magic functions. Used in tab completion.\n\n    For a more general function, see os.path.commonprefix\n    \"\"\"", "output": "\n    prefixes = [os.path.commonprefix([x.split(os.sep)[1:] for x in items]) for x in items]\n    return os.path.commonprefix(prefixes)\n\n\ndef expand_path(path):\n    \"\"\"Expand ~ and ~user references to their home directories\n\n    ~user and ~user are expanded to their home directories, and the resulting\n    path is normalized.\n\n    For example, given the following input:\n\n        ~user\n        ~\n        /home/user\n        /home/\n\n    ~user expands to /home/user, and ~user expands to /home/user. This\n    behavior is consistent with how IPython handles path completion.\n\n    ~user expands to /home/user, and ~", "solution": "\n    # the last item will always have the least leading % symbol\n    # min / max are first/last in alphabetical order\n    first_match  = ESCAPE_RE.match(min(items))\n    last_match  = ESCAPE_RE.match(max(items))\n    # common suffix is (common prefix of reversed items) reversed\n    if first_match and last_match:\n        prefix = os.path.commonprefix((first_match.group(0)[::-1], last_match.group(0)[::-1]))[::-1]\n    else:\n        prefix = ''\n\n    items = [s.lstrip(ESCAPE_CHARS) for s in items]\n    return prefix+os.path.commonprefix(items)"}
{"prompt": "def _graph_wrap(func, graph):\n    \"\"\"Constructs function encapsulated in the graph.\"\"\"", "output": "\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef _is_graph_container(container):\n    \"\"\"Returns whether the container is a graph container.\"\"\"\n    return hasattr(container, 'graph')\n\n\ndef _is_graph_node(container):\n    \"\"\"Returns whether the container is a graph node.\"\"\"\n    return hasattr(container, 'graph') and _is_graph_container(container.graph)\n\n\ndef _is_graph_edge(container):\n    \"\"\"Returns whether the container is a graph edge.\"\"\"\n    return hasattr(container, 'graph') and _is_graph_container(container.graph)\n\n\ndef _is_graph_function(container):\n    \"\"\"Returns whether the container is a graph function.\"\"\"\n    return hasattr(container, 'graph') and _is_graph_container(", "solution": "\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n    return _wrapped"}
{"prompt": "def avail_archs(self):\n        \"\"\" Initialize the dictionary of architectures for disassembling via capstone\"\"\"", "output": "\n        archs = {}\n        archs['x86'] = arch_disasm.x86\n        archs['x86_64'] = arch_disasm.x86_64\n        archs['mips'] = arch_disasm.mips\n        archs['mipsel'] = arch_disasm.mipsel\n        archs['mips64'] = arch_disasm.mips64\n        archs['mips64el'] = arch_disasm.mips64el\n        archs['mips64elp'] = arch_disasm.mips64elp\n        archs['mips64elp64'] = arch_disasm.", "solution": "\n\n        return {\n            ARM32:   (CS_ARCH_ARM,   CS_MODE_ARM),\n            ARM64:   (CS_ARCH_ARM64, CS_MODE_LITTLE_ENDIAN),\n            ARM_TB:  (CS_ARCH_ARM,   CS_MODE_THUMB),\n            MIPS32:  (CS_ARCH_MIPS,  CS_MODE_MIPS32),\n            MIPS64:  (CS_ARCH_MIPS,  CS_MODE_MIPS64),\n            SPARC32: (CS_ARCH_SPARC, CS_MODE_BIG_ENDIAN),\n            SPARC64: (CS_ARCH_SPARC, CS_MODE_V9),\n            SYSTEMZ: (CS_ARCH_SYSZ,  CS_MODE_BIG_ENDIAN),\n            X86_16:  (CS_ARCH_X86,   CS_MODE_16),\n            X86_32:  (CS_ARCH_X86,   CS_MODE_32),\n            X86_64:  (CS_ARCH_X86,   CS_MODE_64),\n        }"}
{"prompt": "def html_to_xhtml(html):\n    \"\"\"Convert all tags in an HTML tree to XHTML by moving them to the\n    XHTML namespace.\n    \"\"\"", "output": "\n    tree = html.etree\n    root = tree.root\n    namespace = root.nsmap\n    root.clear()\n    root.extend(tree.getiterator('*'))\n    root.extend(tree.getiterator('*|comment'))\n    root.extend(tree.getiterator('*|processing-instruction'))\n    root.extend(tree.getiterator('*|text'))\n    root.extend(tree.getiterator('*|comment|processing-instruction|text'))\n    root.extend(tree.getiterator('*|comment|processing-instruction|text|*'))\n    root.extend(tree.getiterator", "solution": "\n    try:\n        html = html.getroot()\n    except AttributeError:\n        pass\n    prefix = \"{%s}\" % XHTML_NAMESPACE\n    for el in html.iter(etree.Element):\n        tag = el.tag\n        if tag[0] != '{':\n            el.tag = prefix + tag"}
{"prompt": "def p_gate_op_2(self, program):\n        \"\"\"\n        gate_op : id id_list ';'\n        \"\"\"", "output": "\n        program.add_gate(self.id_list_to_gate(program.id_list), program.id_list)\n\n    def p_gate_op_3(self, program):\n        \"\"\"\n        gate_op : id id_list '!' id_list ';'\n        \"\"\"\n        program.add_gate(self.id_list_to_gate(program.id_list), program.id_list)\n\n    def p_gate_op_4(self, program):\n        \"\"\"\n        gate_op : id id_list '!' id_list '!' id_list ';'\n        \"\"\"\n        program.add_gate(self.id_list_to_gate(program.id_list), program.id_list)\n\n    def p_gate_op_5(self, program):\n        \"\"\"\n        g", "solution": "\n        program[0] = node.CustomUnitary([program[1], program[2]])\n        # To verify:\n        # 1. id is declared as a gate in global scope\n        # 2. everything in the id_list is declared as a bit in local scope\n        self.verify_as_gate(program[1], program[2])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def insert_and_get(self, **fields):\n        \"\"\"Creates a new record in the database and then gets\n        the entire row.\n\n        This allows specifying custom conflict behavior using .on_conflict().\n        If no special behavior was specified, this uses the normal Django create(..)\n\n        Arguments:\n            fields:\n                The fields of the row to create.\n\n        Returns:\n            The model instance representing the row that was created.\n        \"\"\"", "output": "\n        record = self.model(**fields)\n        record.save()\n        return record\n\n    def bulk_create(self, records):\n        \"\"\"Creates a new record in the database and bulk inserts them.\n\n        Arguments:\n            records:\n                A list of model instances to create.\n\n        Returns:\n            A list of model instances that were created.\n        \"\"\"\n        records = map(lambda x: self.model(x), records)\n        records = map(lambda x: x.save(), records)\n        return records\n\n    def bulk_update(self, records):\n        \"\"\"Updates multiple records in the database and bulk inserts them.\n\n        Arguments:\n            records:\n                A list of model instances to update.\n\n        Returns:\n            A list of model instances that were ", "solution": "\n\n        if not self.conflict_target and not self.conflict_action:\n            # no special action required, use the standard Django create(..)\n            return super().create(**fields)\n\n        compiler = self._build_insert_compiler([fields])\n        rows = compiler.execute_sql(return_id=False)\n\n        columns = rows[0]\n\n        # get a list of columns that are officially part of the model and preserve the fact that the attribute name\n        # might be different than the database column name\n        model_columns = {}\n        for field in self.model._meta.local_concrete_fields:\n            model_columns[field.column] = field.attname\n\n        # strip out any columns/fields returned by the db that\n        # are not present in the model\n        model_init_fields = {}\n        for column_name, column_value in columns.items():\n            try:\n                model_init_fields[model_columns[column_name]] = column_value\n            except KeyError:\n                pass\n\n        return self.model(**model_init_fields)"}
{"prompt": "def strip_email_quotes(text):\n    \"\"\"Strip leading email quotation characters ('>').\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines of the input text.\n\n    Parameters\n    ----------\n    text : str\n\n    Examples\n    --------\n\n    Simple uses::\n\n        In [2]: strip_email_quotes('> > text')\n        Out[2]: 'text'\n\n        In [3]: strip_email_quotes('> > text\\\\n> > more')\n        Out[3]: 'text\\\\nmore'\n\n    Note how only the common prefix that appears in all lines is stripped::\n\n        In [4]: strip_email_quotes('> > text\\\\n> > more\\\\n> more...')\n        Out[4]: '> text\\\\n> more\\\\nmore...'\n\n    So if any line has no quote marks ('>') , then none are stripped from any\n    of them ::\n    \n        In [5]: strip_email_quotes('> > text\\\\n> > more\\\\nlast different')\n        Out[5]: '> > text\\\\n> > more\\\\nlast different'\n    \"\"\"", "output": "ped::\n\n        In [4]: strip_email_quotes('> > text\\\\n> > more')\n        Out[4]: 'text\\\\nmore'\n\n    \"\"\"\n    return re.sub(r'(>\\s)+', '', text)\n\n\ndef strip_email_quotes_multiline(text):\n    \"\"\"Strip leading email quotation characters ('>').\n\n    Removes any combination of leading '>' interspersed with whitespace that\n    appears *identically* in all lines of the input text.\n\n    Parameters\n    ----------\n    text : str\n\n    Examples\n    --------\n\n    Simple uses::\n\n        In [2]: strip_email_quotes_multiline('> > text')", "solution": "\n    lines = text.splitlines()\n    matches = set()\n    for line in lines:\n        prefix = re.match(r'^(\\s*>[ >]*)', line)\n        if prefix:\n            matches.add(prefix.group(1))\n        else:\n            break\n    else:\n        prefix = long_substr(list(matches))\n        if prefix:\n            strip = len(prefix)\n            text = '\\n'.join([ ln[strip:] for ln in lines])\n    return text"}
{"prompt": "def delete_vm_image(self, vm_image_name, delete_vhd=False):\n        \"\"\"\n        Deletes the specified VM Image from the image repository that is\n        associated with the specified subscription.\n\n        vm_image_name:\n            The name of the image.\n        delete_vhd:\n            Deletes the underlying vhd blob in Azure storage.\n        \"\"\"", "output": "\n        subscription_id = self.get_subscription_id()\n        resource_group = self.get_resource_group()\n        storage_account_name = self.get_storage_account_name()\n        storage_account_key = self.get_storage_account_key()\n        storage_account_url = self.get_storage_account_url()\n        storage_account_version = self.get_storage_account_version()\n        storage_account_account_type = self.get_storage_account_account_type()\n        storage_account_account_region = self.get_storage_account_account_region()\n        storage_account_account_key = self.get_storage_account_account_key()\n        storage_account_account_url = self.get_storage_account_account_url()\n        storage_account_account_version = self.get_storage_account_account_version()\n        storage_account_account_type = self.get_storage_account_account_type()\n", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        path = self._get_vm_image_path(vm_image_name)\n        if delete_vhd:\n            path += '?comp=media'\n        return self._perform_delete(path, as_async=True)"}
{"prompt": "def _add_column_kwargs(self, kwargs, column):\n        \"\"\"Add keyword arguments to kwargs (in-place) based on the passed in\n        `Column <sqlalchemy.schema.Column>`.\n        \"\"\"", "output": "\n\n        # this method is called when an instance is being constructed\n        # from a plain dict, so we can't use self.__dict__.update(kwargs)\n        # because that would cause infinite recursion on the base class\n        # _add_column_kwargs calls itself.\n\n        # we only want to copy the kwargs that are actually passed in\n        # to the column\n        column_kwargs = {}\n        for attr in column._all_columns:\n            if attr in kwargs:\n                column_kwargs[attr] = kwargs[attr]\n\n        # copy the other kwargs into the new column\n        column_kwargs.update(kwargs)\n\n        # add the column to the list of column kwargs\n        column._all_columns.append(column_kwargs)\n\n        # update the kwargs dict with the ", "solution": "\n        if column.nullable:\n            kwargs[\"allow_none\"] = True\n        kwargs[\"required\"] = not column.nullable and not _has_default(column)\n\n        if hasattr(column.type, \"enums\"):\n            kwargs[\"validate\"].append(validate.OneOf(choices=column.type.enums))\n\n        # Add a length validator if a max length is set on the column\n        # Skip UUID columns\n        # (see https://github.com/marshmallow-code/marshmallow-sqlalchemy/issues/54)\n        if hasattr(column.type, \"length\"):\n            try:\n                python_type = column.type.python_type\n            except (AttributeError, NotImplementedError):\n                python_type = None\n            if not python_type or not issubclass(python_type, uuid.UUID):\n                kwargs[\"validate\"].append(validate.Length(max=column.type.length))\n\n        if hasattr(column.type, \"scale\"):\n            kwargs[\"places\"] = getattr(column.type, \"scale\", None)"}
{"prompt": "def check_for_old_config(ipython_dir=None):\n    \"\"\"Check for old config files, and present a warning if they exist.\n\n    A link to the docs of the new config is included in the message.\n\n    This should mitigate confusion with the transition to the new\n    config system in 0.11.\n    \"\"\"", "output": "\n    old_config_files = glob.glob(os.path.join(ipython_dir, 'etc', 'ipython-config.*'))\n    old_config_files.sort()\n    old_config_files = [f for f in old_config_files if os.path.isfile(f)]\n    old_config_files = [f for f in old_config_files if not f.endswith('.ipynb')]\n    if len(old_config_files) == 0:\n        return\n    msg = \"Found old config files:\\n%s\" % '\\n'.join(old_config_files)\n    warnings.warn(msg)\n\n\ndef check_dependencies(ipython_dir=None):\n    \"\"\"Check for dependencies and present a warning if they exist.\n\n    IPython is required to run this function, but optional dependencies\n    are not. These dependencies are installed as part of setuptools and\n    are installed ", "solution": "\n    if ipython_dir is None:\n        ipython_dir = get_ipython_dir()\n\n    old_configs = ['ipy_user_conf.py', 'ipythonrc', 'ipython_config.py']\n    warned = False\n    for cfg in old_configs:\n        f = os.path.join(ipython_dir, cfg)\n        if os.path.exists(f):\n            if filehash(f) == old_config_md5.get(cfg, ''):\n                os.unlink(f)\n            else:\n                warnings.warn(\"Found old IPython config file %r (modified by user)\"%f)\n                warned = True\n\n    if warned:\n        warnings.warn("}
{"prompt": "def getField(self, fld_name):\n        \"\"\" Return :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n\n        Returns:\n            str: String value (scaled if numeric) for the field.\n        \"\"\"", "output": "\n        fld_val = self.meters[fld_name]\n        fld_val = fld_val.formatted()\n        fld_val = fld_val.scaled()\n        return fld_val\n\n    def getFields(self, fld_name):\n        \"\"\" Return :class:`~ekmmeters.Field` content, scaled and formatted.\n\n        Args:\n            fld_name (str): A :class:`~ekmmeters.Field` value which is on your meter.\n\n        Returns:\n            list: List of string values (scaled if numeric) for the field.\n        \"\"\"\n        fld_val = self.meters[fld_name]\n        fld_val = fld_val.formatted()\n        fld_val = fld_val.scaled", "solution": "\n        result = \"\"\n        if fld_name in self.m_req:\n            result = self.m_req[fld_name][MeterData.StringValue]\n        else:\n            ekm_log(\"Requested nonexistent field: \" + fld_name)\n\n        return result"}
{"prompt": "def _mk_index(self):\n        \"\"\"create the index for this set\"\"\"", "output": "\n        self.index = self.index.droplevel(self.index_col)\n        self.index = self.index.rename_axis(self.index_col)\n        self.index = self.index.reorder_levels(self.index_col,\n                                               self.index_order)\n\n    def _make_meta(self):\n        \"\"\"create the meta for this set\"\"\"\n        self.meta = self.meta.droplevel(self.meta_col)\n        self.meta = self.meta.rename_axis(self.meta_col)\n        self.meta = self.meta.reorder_levels(self.meta_col,\n                                               self.meta_order)\n\n    def _make_meta2(self):\n        \"\"\"create the meta for this set\"\"\"\n        self.meta2 = self.meta.droplevel(self.meta_col2)\n        self.meta2 = self.meta2.rename_axis(self.meta_col2)\n        self.meta2 = self.", "solution": "\n        index = defaultdict(list)\n        for line_no, line in enumerate(self._stripped_lines):\n            if line:\n                index[line].append(line_no)\n        return index"}
{"prompt": "def _find_blob_start(self):\n        \"\"\"Find first blob from selection.\n        \"\"\"", "output": "\n        selection = self.selection\n        blob = selection[0]\n        blob_start = blob.start\n        blob_end = blob.end\n        return blob_start, blob_end\n\n    def _find_blob_end(self):\n        \"\"\"Find last blob from selection.\n        \"\"\"\n        selection = self.selection\n        blob = selection[-1]\n        blob_start = blob.start\n        blob_end = blob.end\n        return blob_start, blob_end\n\n    def _find_blob_range(self, blob_start, blob_end):\n        \"\"\"Find blob range from blob start and blob end.\n        \"\"\"\n        selection = self.selection\n        blob = selection[blob_", "solution": "\n\n        # Convert input frequencies into what their corresponding channel number would be.\n        self._setup_chans()\n\n        # Check which is the blob time offset\n        blob_time_start = self.t_start\n\n        # Check which is the blob frequency offset (in channels)\n        blob_freq_start = self.chan_start_idx\n\n        blob_start = blob_time_start * self.n_channels_in_file + blob_freq_start\n\n        return blob_start"}
{"prompt": "def update(self, ava):\n        \"\"\"\n        Implements the dict.update() method\n        \"\"\"", "output": "\n        for (key, value) in ava.items():\n            self.__dict__[key] = value\n\n    def popitem(self):\n        \"\"\"\n        Implements the dict.popitem() method\n        \"\"\"\n        return self.pop(self.key())\n\n    def keys(self):\n        \"\"\"\n        Implements the dict.keys() method\n        \"\"\"\n        return list(self.__dict__.keys())\n\n    def values(self):\n        \"\"\"\n        Implements the dict.values() method\n        \"\"\"\n        return list(self.__dict__.values())\n\n    def items(self):\n        \"\"\"\n        Implements the dict.items() method\n        \"\"\"\n        return list(self.__dict__.items())\n\n    def iterkeys(self):\n        \"\"\"\n        Implements the dict.iterkeys() method\n        \"\"\"\n        return iter(self.keys())\n\n    def itervalues(self):\n", "solution": "\n        for key, val in ava.items():\n            self[key] = val"}
{"prompt": "def multiqc(institute_id, case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"", "output": "\n    case = Case(case_name)\n    case.load()\n    case.multiqc_report = MultiqcReport(case.multiqc_report_id)\n    case.multiqc_report.load()\n    return case\n\n\ndef multiqc_report_by_case(case_name):\n    \"\"\"Load multiqc report for the case.\"\"\"\n    case = Case(case_name)\n    case.load()\n    case.multiqc_report = MultiqcReport(case.multiqc_report_id)\n    case.multiqc_report.load()\n    return case\n\n\ndef multiqc_report_by_institute(institute_id):\n    \"\"\"Load multiqc report for the ", "solution": "\n    data = controllers.multiqc(store, institute_id, case_name)\n    if data['case'].get('multiqc') is None:\n        return abort(404)\n    out_dir = os.path.abspath(os.path.dirname(data['case']['multiqc']))\n    filename = os.path.basename(data['case']['multiqc'])\n    return send_from_directory(out_dir, filename)"}
{"prompt": "def get_user_token(user, purpose, minutes_valid):\n    \"\"\"Return login token info for given user.\"\"\"", "output": "\n    token = user.get_token(purpose, minutes_valid)\n    return token\n\n\ndef get_user_email(user, purpose, minutes_valid):\n    \"\"\"Return email address for given user.\"\"\"\n    email = user.get_email(purpose, minutes_valid)\n    return email\n\n\ndef get_user_emails(user, purpose, minutes_valid):\n    \"\"\"Return list of email addresses for given user.\"\"\"\n    emails = user.get_emails(purpose, minutes_valid)\n    return emails\n\n\ndef get_user_purpose(user, purpose):\n    \"\"\"Return purpose for given user.\"\"\"\n    purpose = user.get_purpose(purpose)\n    return purpose\n\n\ndef get_user_", "solution": "\n    token = ''.join(\n        dumps([\n            user.get_username(),\n            get_auth_hash(user, purpose),\n        ]).encode('base64').split('\\n')\n    )\n    return {\n        'id': get_meteor_id(user),\n        'token': token,\n        'tokenExpires': calc_expiry_time(minutes_valid),\n    }"}
{"prompt": "def write_meta(self):\n        \"\"\"ucds, descriptions and units are written as attributes in the hdf5 file, instead of a seperate file as\n         the default :func:`Dataset.write_meta`.\n         \"\"\"", "output": "\n        self.ucds.write_meta()\n        self.descriptions.write_meta()\n        self.units.write_meta()\n\n    def write_meta_file(self, filename):\n        \"\"\"Write the meta file to a file.\n        \"\"\"\n        meta_file = h5py.File(filename, \"w\")\n        meta_file[\"ucds\"] = self.ucds\n        meta_file[\"descriptions\"] = self.descriptions\n        meta_file[\"units\"] = self.units\n        meta_file.close()\n\n    def write_meta_files(self, filenames):\n        \"\"\"Write the meta files to a list.\n        \"\"\"\n        meta_files = []\n        for filename in filenames:\n            meta_files.append(self.write_meta_file(filename))\n        return meta_files\n\n    def write_", "solution": "\n        with h5py.File(self.filename, \"r+\") as h5file_output:\n            h5table_root = h5file_output[self.h5table_root_name]\n            if self.description is not None:\n                h5table_root.attrs[\"description\"] = self.description\n            h5columns = h5table_root if self._version == 1 else h5table_root['columns']\n            for column_name in self.columns.keys():\n                h5dataset = None\n                if column_name in h5columns:\n                    h5dataset = h5columns[column_name]\n                else:\n                    for group in h5columns.values():\n                        if 'type' in group.attrs:\n                            if group.attrs['type'] in ['csr_matrix']: \n                                for name, column in group.items():\n                                    if name == column_name:\n                                        h5dataset = column\n                if h5dataset is None:\n                    raise ValueError('column {} not found'.format(column_name))\n                for name, values in [(\"ucd\", self.ucds), (\"unit\", self.units), (\"description\", self.descriptions)]:\n                    if column_name in values:\n                        value = ensure_string(values[column_name], cast=True)\n                        h5dataset.attrs[name] = value\n                    else:\n                        if name in h5columns.attrs:\n                            del h5dataset.attrs[name]"}
{"prompt": "def rename(self, columns=None):\n        \"\"\"\n        Change names of columns in the frame.\n\n        Dict key is an index or name of the column whose name is to be set.\n        Dict value is the new name of the column.\n\n        :param columns: dict-like transformations to apply to the column names\n        \"\"\"", "output": "\n        if columns is None:\n            columns = {}\n        columns = columns.copy()\n        columns.update(self.columns)\n        columns.update(self._index_names)\n        columns.update(self._column_names)\n        columns.update(self._metadata._columns)\n        columns.update(self._metadata._columns_with_index)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        columns.update(self._metadata._columns_with_index_names)\n        ", "solution": "\n        assert_is_type(columns, None, dict)\n        new_names = self.names\n        ncols = self.ncols\n\n        for col, name in columns.items():\n            col_index = None\n            if is_type(col, int) and (-ncols <= col < ncols):\n                col_index = (col + ncols) % ncols  # handle negative indices\n            elif is_type(col, str) and col in self.names:\n                col_index = self.names.index(col)  # lookup the name\n\n            if col_index is not None:\n                new_names[col_index] = name\n\n        return self.set_names(new_names)"}
{"prompt": "def get_root_schema_dict(record):\n    \"\"\"Return a root jsonschema for a given record\n\n    A root schema includes the $schema attribute and all sub-record\n    schemas and definitions.\n    \"\"\"", "output": "\n    root_schema = {}\n    root_schema['$schema'] = record.get_schema()\n    for sub_record in record.get_children():\n        root_schema[sub_record.get_name()] = sub_record.get_schema()\n    return root_schema\n\n\ndef validate_record(record, schema):\n    \"\"\"Validate an individual record\n\n    Validate an individual record against the given schema. Raise an\n    exception if the record does not conform to the schema.\n    \"\"\"\n    validate_schema(schema, record)\n    validate_children(record, schema)\n\n\ndef validate_schema(schema, record):\n    \"\"\"Validate an individual record's schema\n\n    Validate an individual record's schema against the ", "solution": "\n    state = SchemaGeneratorState()\n    schema = get_schema_dict(record, state)\n    del state.record_schemas[record._schema_name]\n    if state.record_schemas:\n        schema['definitions'] = dict()\n        for name, sub_schema in state.record_schemas.iteritems():\n            schema['definitions'][name] = sub_schema\n    return schema"}
{"prompt": "def _check_multi_statement_line(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"", "output": "\n        statements = line[1:].strip().split()\n        if len(statements) > 1:\n            error(node, 'Multiple statements not allowed at line %s' % line)\n\n    def _check_multi_statement_block(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"\n        statements = line[1:].strip().split()\n        if len(statements) > 1:\n            error(node, 'Multiple statements not allowed at line %s' % line)\n\n    def _check_multi_statement_list(self, node, line):\n        \"\"\"Check for lines containing multiple statements.\"\"\"\n        statements = line[1:].strip().split()\n        if len(statements) > 1:\n            error(node, 'Multiple statements not allowed at line %s' % line)\n\n    def _check_multi_statement_list_item(self, node, line):\n        \"\"\"Check for lines containing multiple ", "solution": "\n        # Do not warn about multiple nested context managers\n        # in with statements.\n        if isinstance(node, nodes.With):\n            return\n        # For try... except... finally..., the two nodes\n        # appear to be on the same line due to how the AST is built.\n        if isinstance(node, nodes.TryExcept) and isinstance(\n            node.parent, nodes.TryFinally\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.If)\n            and not node.parent.orelse\n            and self.config.single_line_if_stmt\n        ):\n            return\n        if (\n            isinstance(node.parent, nodes.ClassDef)\n            and len(node.parent.body) == 1\n            and self.config.single_line_class_stmt\n        ):\n            return\n        self.add_message(\"multiple-statements\", node=node)\n        self._visited_lines[line] = 2"}
{"prompt": "def _validate(value, optdict, name=\"\"):\n    \"\"\"return a validated value for an option according to its type\n\n    optional argument name is only used for error message formatting\n    \"\"\"", "output": "\n    if value is None:\n        return None\n    if type(value) in (str, unicode):\n        return value\n    if type(value) in (list, tuple):\n        return type(value)(_validate(value, optdict, name))\n    raise TypeError, \"invalid type %r for option %r\" % (type(value), name)\n\nclass Option(object):\n    \"\"\"\n    Base class for all option types.\n    \"\"\"\n    def __init__(self, name, default=None, help=None, type=None, choices=None, required=False):\n        self.name = name\n        self.default = default\n        self.help = help\n        self.type = type\n        self.choices = choices\n        self.required = required\n\n    def configure(self, parser, namespace, values=None):\n        \"\"\"\n        Configure an option from the command-line or config file.\n        \"\"\"\n        if values is None:\n            values = {}\n        if self.", "solution": "\n    try:\n        _type = optdict[\"type\"]\n    except KeyError:\n        # FIXME\n        return value\n    return _call_validator(_type, optdict, name, value)"}
{"prompt": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"", "output": "\n        extra_options = extra_options or {}\n        extra_options['headers'] = headers or {}\n        extra_options['headers']['Content-Type'] = 'application/json'\n        extra_options['headers']['Accept'] = 'application/json'\n        extra_options['headers']['Authorization'] = 'Bearer'+ self.token\n        extra_options['headers']['X-Auth-Token'] = self.token\n        extra_options['headers']['X-Auth-Project-Id'] = self.project_id\n        extra_options['headers']['X-Auth-Domain-Id'] = self.domain_id\n        extra_options['headers']['X-Auth-Domain-Name'] = self.domain_name\n        extra_options['headers']['X-Auth-Project-Name'] = self.project_name\n        extra_options['headers']['X-", "solution": "\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)"}
{"prompt": "def print_annotation(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"", "output": "\n        annotations = []\n        for key, value in self.annotations.items():\n            annotations.append(f\"{key}: {value}\")\n        print(\", \".join(annotations))\n\n    def print_annotations(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"\n        annotations = []\n        for key, value in self.annotations.items():\n            annotations.append(f\"{key}: {value}\")\n        print(\", \".join(annotations))\n\n    def print_annotations_as_json(self):\n        \"\"\"Print annotation \"key: value\" pairs to standard output.\"\"\"\n        annotations = []\n        for key, value in self.annotations.items():\n            annotations.append(f\"{key}: {value}\")\n        print(\", \".join(annotations), file=sys.stdout)\n\n    def print_annotations_as_csv(self):\n        \"\"\"Print annotation \"key: ", "solution": "\n        for path, ann in self.annotation.items():\n            print(\"{}: {}\".format(path, ann['value']))"}
{"prompt": "def info(self, **kwargs):\n        \"\"\"\n        Get the detailed information about a particular credit record. This is \n        currently only supported with the new credit model found in TV. These \n        ids can be found from any TV credit response as well as the tv_credits \n        and combined_credits methods for people.\n\n        The episodes object returns a list of episodes and are generally going \n        to be guest stars. The season array will return a list of season \n        numbers.  Season credits are credits that were marked with the \n        \"add to every season\" option in the editing interface and are \n        assumed to be \"season regulars\".\n\n        Args:\n            language: (optional) ISO 639-1 code.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\n            season: (optional) season number\n            episode: (optional) episode number\n            credit_type: (optional) credit type\n            credit_id: (optional) credit id\n            credit_name: (optional) credit name\n            credit_number: (optional) credit number\n            credit_description: (optional) credit description\n            credit_type_description: (optional) credit type description\n            credit_type_code: (optional) credit type code\n            credit_type_label: (optional) credit type label\n            credit_type_code_description: (optional) credit type code description\n            credit_type_label_description: (optional) credit type label ", "solution": "\n        path = self._get_credit_id_path('info')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def parse_options():\n    \"\"\"\n    Commandline options arguments parsing.\n\n    :return: parsed commandline arguments.\n    :rtype: optparse.Values.\n    \"\"\"", "output": "\n    parser = optparse.OptionParser()\n    parser.add_option(\n        '-s', '--server',\n        dest='server',\n        default='localhost',\n        help='The server address to connect to.')\n    parser.add_option(\n        '-p', '--port',\n        dest='port',\n        default=9090,\n        help='The port to connect to.')\n    parser.add_option(\n        '-u', '--username',\n        dest='username',\n        default='',\n        help='The username to connect with.')\n    parser.add_option(\n        '-d', '--database',\n        dest='database',\n        default='',\n        help='The database to connect with.')\n    parser.add_option(\n        '-t', '--table", "solution": "\n\n    version = \"%%prog {version}\".format(version=__version__)\n    parser = OptionParser(version=version)\n    parser.add_option(\n        \"-s\", \"--server\", action=\"store\", dest=\"server\",\n        type=\"string\", default=\"\", metavar=\"SERVER\",\n        help=\"server name, IP address or unix socket path\"\n    )\n    parser.add_option(\n        \"-p\", \"--port\", action=\"store\", type=\"int\", dest=\"port\",\n        default=9001, metavar=\"PORT\", help=\"port number\"\n    )\n    parser.add_option(\n        \"-P\", \"--programs\", action=\"store\", dest=\"programs\", type=\"string\", default=\"\",\n        metavar=\"PROGRAMS\", help=\"comma separated programs list, or empty for all programs in supervisord response\"\n    )\n    parser.add_option(\n        \"-u\", \"--username\", action=\"store\", dest=\"username\", type=\"string\", default=\"\",\n        metavar=\"USERNAME\", help=\"supervisord user\"\n    )\n    parser.add_option(\n        \"-S\", \"--password\", action=\"store\", dest=\"password\", type=\"string\", default=\"\",\n        metavar=\"PASSWORD\", help=\"supervisord user password\"\n    )\n    parser.add_option(\n        \"-q\", \"--quiet\", metavar=\"QUIET\", action=\"store_true\", default=False, dest=\"quiet\", help=\"be quiet\"\n    )\n    parser.add_option(\n        \"--stopped-state\", action=\"store\", dest=\"stopped_state\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_OK,\n        metavar=\"STOPPED_STATE\", help=\"stopped state\"\n    )\n    parser.add_option(\n        \"--network-errors-exit-code\", action=\"store\", dest=\"network_errors_exit_code\", type=\"choice\", choices=EXIT_CODES.keys(), default=EXIT_CODE_UNKNOWN,\n        metavar=\"NETWORK_ERRORS_EXIT_CODE\", help=\"network errors exit code\"\n    )\n\n    options = parser.parse_args(sys.argv)[0]\n    STATE2TEMPLATE[STATE_STOPPED] = options.stopped_state  # update stopped state value from command line argument\n\n    # check mandatory command line options supplied\n    if not options.server:\n        parser.error(\"Required server address option missing\")\n    if options.username and not options.password:\n        parser.error(\"Required supervisord user password\")\n\n    return options"}
{"prompt": "def _create_hstore_unique(self, model, field, keys):\n        \"\"\"Creates a UNIQUE constraint for the specified hstore keys.\"\"\"", "output": "\n        unique = field.unique\n        constraint = UniqueConstraint(*keys, name=unique)\n        constraint.columns = [model._meta.pk.column]\n        constraint.table = model._meta.db_table\n        constraint.unique = True\n        return constraint\n\n    def _create_hstore_index(self, model, field, keys):\n        \"\"\"Creates an index on the specified hstore keys.\"\"\"\n        index_name = field.index_name\n        index_type = field.index_type\n        unique = field.unique\n        constraint = IndexConstraint(*keys, name=index_name,\n                                     unique=unique, type=index_type)\n        constraint.columns = [model._meta.pk.column]\n        constraint.table = model._meta.db_", "solution": "\n\n        name = self._unique_constraint_name(\n            model._meta.db_table, field, keys)\n        columns = [\n            '(%s->\\'%s\\')' % (field.column, key)\n            for key in keys\n        ]\n        sql = self.sql_hstore_unique_create.format(\n            name=self.quote_name(name),\n            table=self.quote_name(model._meta.db_table),\n            columns=','.join(columns)\n        )\n        self.execute(sql)"}
{"prompt": "def create_trace(\n        turn_activity: Activity,\n        name: str,\n        value: object = None,\n        value_type: str = None,\n        label: str = None,\n    ) -> Activity:\n        \"\"\"Creates a trace activity based on this activity.\n\n        :param turn_activity:\n        :type turn_activity: Activity\n        :param name: The value to assign to the trace activity's <see cref=\"Activity.name\"/> property.\n        :type name: str\n        :param value: The value to assign to the trace activity's <see cref=\"Activity.value\"/> property., defaults to None\n        :param value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property, defaults to None\n        :param value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :param label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"", "output": "None\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :type label: str\n        :rtype: Activity\n        \"\"\"\n        activity = Activity(\n            name=name,\n            value=value,\n            value_type=value_type,\n            label=label,\n        )\n        activity._activity_type = turn_activity._activity_type\n        activity._activity_subtype = turn_activity._activity_subtype\n        activity._activity_subtype_version = turn_activity._activity_subtype_version\n        activity._activity_subtype_version_number = turn_activity._activity_subtype_version_number\n        activity._activity_subtype_version_number = turn_", "solution": "\n\n        from_property = (\n            ChannelAccount(\n                id=turn_activity.recipient.id, name=turn_activity.recipient.name\n            )\n            if turn_activity.recipient is not None\n            else ChannelAccount()\n        )\n        if value_type is None and value is not None:\n            value_type = type(value).__name__\n\n        reply = Activity(\n            type=ActivityTypes.trace,\n            timestamp=datetime.utcnow(),\n            from_property=from_property,\n            recipient=ChannelAccount(\n                id=turn_activity.from_property.id, name=turn_activity.from_property.name\n            ),\n            reply_to_id=turn_activity.id,\n            service_url=turn_activity.service_url,\n            channel_id=turn_activity.channel_id,\n            conversation=ConversationAccount(\n                is_group=turn_activity.conversation.is_group,\n                id=turn_activity.conversation.id,\n                name=turn_activity.conversation.name,\n            ),\n            name=name,\n            label=label,\n            value_type=value_type,\n            value=value,\n        )\n        return reply"}
{"prompt": "def voucher_code(request):\n    \"\"\" A view *just* for entering a voucher form. \"\"\"", "output": "\n    form = VoucherForm(request.POST or None)\n    if form.is_valid():\n        form.save()\n        return HttpResponseRedirect(reverse('voucher:voucher_list'))\n    return render(request, 'voucher/voucher_form.html', {'form': form})\n\n\n@login_required\ndef voucher_list(request):\n    \"\"\" A view for listing vouchers. \"\"\"\n    context = {\n        'vouchers': Voucher.objects.all(),\n    }\n    return render(request, 'voucher/voucher_list.html', context)\n\n\n@login_required\ndef voucher_detail(request, pk):\n    \"\"\" A view for viewing a voucher. \"\"\"\n    voucher = Voucher.objects.get(pk=pk)\n", "solution": "\n\n    VOUCHERS_FORM_PREFIX = \"vouchers\"\n\n    # Handle the voucher form *before* listing products.\n    # Products can change as vouchers are entered.\n    v = _handle_voucher(request, VOUCHERS_FORM_PREFIX)\n    voucher_form, voucher_handled = v\n\n    if voucher_handled:\n        messages.success(request, \"Your voucher code was accepted.\")\n        return redirect(\"dashboard\")\n\n    data = {\n        \"voucher_form\": voucher_form,\n    }\n\n    return render(request, \"registrasion/voucher_code.html\", data)"}
{"prompt": "def create(cls, name_value, name_type):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"", "output": "\n        name_obj = cls(name_value, name_type)\n        return name_obj\n\n    @classmethod\n    def create(cls, name, name_value, name_type):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"\n        name_obj = cls(name, name_type, name_value, None)\n        return name_obj\n\n    @classmethod\n    def create(cls, name, name_value, name_type, name_source):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"\n        name_obj = cls(name, name_type, name_value, name_source)\n        return name_obj\n\n    @classmethod\n    def create(cls, name, name_value, name_type, name_source, name_source_name):\n        \"\"\"\n            Returns a Name object, populated with the given value and type\n        \"\"\"", "solution": "\n        if isinstance(name_value, Name.NameValue):\n            value = name_value\n        elif isinstance(name_value, str):\n            value = cls.NameValue(name_value)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_value'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_value', type(Name.NameValue),\n                                       type(name_value)))\n\n        if isinstance(name_type, Name.NameType):\n            n_type = name_type\n        elif isinstance(name_type, Enum):\n            n_type = cls.NameType(name_type)\n        else:\n            name = 'Name'\n            msg = exceptions.ErrorStrings.BAD_EXP_RECV\n            member = 'name_type'\n            raise TypeError(msg.format('{0}.{1}'.format(name, member),\n                                       'name_type', type(Name.NameType),\n                                       type(name_type)))\n\n        return Name(name_value=value,\n                    name_type=n_type)"}
{"prompt": "def loop_gtk(kernel):\n    \"\"\"Start the kernel, coordinating with the GTK event loop\"\"\"", "output": "\n    kernel.start()\n    kernel.gtk_eventloop.start()\n\n\ndef loop_qt4(kernel):\n    \"\"\"Start the kernel, coordinating with the Qt event loop\"\"\"\n    kernel.start()\n    kernel.qt4_eventloop.start()\n\n\ndef loop_qt5(kernel):\n    \"\"\"Start the kernel, coordinating with the Qt event loop\"\"\"\n    kernel.start()\n    kernel.qt5_eventloop.start()\n\n\ndef loop_qt6(kernel):\n    \"\"\"Start the kernel, coordinating with the Qt event loop\"\"\"\n    kernel.start()\n    kernel.qt6_eventloop.start()\n\n\ndef loop_qt7(kernel):\n    \"\"\"Start the kernel, coordinating with the Qt event ", "solution": "\n    from .gui.gtkembed import GTKEmbed\n\n    gtk_kernel = GTKEmbed(kernel)\n    gtk_kernel.start()"}
{"prompt": "def single_request_timeout(self, value):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"", "output": "\n        self._timeout = value\n\n    @property\n    def timeout(self):\n        \"\"\"The timeout (seconds) for a single HTTP REST API request.\"\"\"\n        return self._timeout\n\n    @property\n    def max_retries(self):\n        \"\"\"The maximum number of times to retry a failed request.\"\"\"\n        return self._max_retries\n\n    @property\n    def max_retry_delay(self):\n        \"\"\"The maximum delay between retries in seconds.\"\"\"\n        return self._max_retry_delay\n\n    @property\n    def max_retry_wait(self):\n        \"\"\"The maximum wait time in seconds between retries in seconds.\"\"\"\n        return self._max_retry_wait\n\n    @property\n    def max_retry_interval(self):\n        \"\"\"The maximum interval between retries in seconds.\"\"\"\n        return self._max_retry_interval\n\n    @property\n    def max_retry_total(self):", "solution": "\n        check_type(value, int)\n        assert value is None or value > 0\n        self._single_request_timeout = value"}
{"prompt": "def search(self, pattern=\"*\", raw=True, search_raw=True,\n                                                        output=False):\n        \"\"\"Search the database using unix glob-style matching (wildcards\n        * and ?).\n\n        Parameters\n        ----------\n        pattern : str\n          The wildcarded pattern to match when searching\n        search_raw : bool\n          If True, search the raw input, otherwise, the parsed input\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"", "output": "\n        pattern = pattern.lower()\n        pattern = pattern.replace(\"*\", \".*\")\n        pattern = pattern.replace(\"?\", \".?\")\n        pattern = pattern.replace(\"\\\\*\", \".*\")\n        pattern = pattern.replace(\"\\\\?\", \".?\")\n        pattern = pattern.replace(\"\\\\*\", \".*\")\n        pattern = pattern.replace(\"\\\\?\", \".?\")\n\n        ranges = []\n        range = self.get_range(pattern)\n        if raw:\n            ranges.append(range)\n        else:\n            ranges.append(range[0])\n            for row in range:\n                ranges.append(row)\n        if search_raw:\n            return ranges\n        else:\n            return ranges\n\n    def get_range(self, pattern):\n", "solution": "\n        tosearch = \"source_raw\" if search_raw else \"source\"\n        if output:\n            tosearch = \"history.\" + tosearch\n        self.writeout_cache()\n        return self._run_sql(\"WHERE %s GLOB ?\" % tosearch, (pattern,),\n                                    raw=raw, output=output)"}
{"prompt": "async def get_bots(self, limit, offset):\n        \"\"\"Gets an object of bots on DBL\"\"\"", "output": "\n        bots = []\n        async for bot in self.bot.get_bots(limit, offset):\n            bots.append(bot)\n        return bots\n\n    async def add_bot(self, bot):\n        \"\"\"Adds a bot to the DBL\"\"\"\n        await self.bot.add_bot(bot)\n        await self.bot.save()\n\n    async def remove_bot(self, bot):\n        \"\"\"Removes a bot from the DBL\"\"\"\n        await self.bot.remove_bot(bot)\n        await self.bot.save()\n\n    async def add_user(self, user):\n        \"\"\"Adds a user to the DBL\"\"\"\n        await self.bot.add_user(user)\n        await self.bot.save()\n\n    async def remove_user(self, user):\n        \"\"\"Removes a user from the DBL\"\"\"\n        await self.bot.remove_", "solution": "\n        if limit > 500:\n            limit = 50\n        return await self.request('GET', '{}/bots?limit={}&offset={}'.format(self.BASE, limit, offset))"}
{"prompt": "def group_transactions(self):\n        # type: () -> List[List[Transaction]]\n        \"\"\"\n        Groups transactions in the bundle by address.\n        \"\"\"", "output": "\n        transactions = []  # type: List[List[Transaction]]\n        for transaction in self.transactions:\n            transactions.append(\n                [\n                    Transaction(\n                        transaction=transaction,\n                        address=self.address,\n                        amount=self.amount,\n                        fee=self.fee,\n                        confirmations=self.confirmations,\n                        timestamp=self.timestamp,\n                        block_hash=self.block_hash,\n                        block_number=self.block_number,\n                        transaction_id=self.transaction_id,\n                    )\n                ]\n            )\n        return transactions\n\n    def to_dict(self) -> Dict:\n        \"\"\"\n        Serializes the bundle to a dictionary.\n        \"\"\"\n        return {\n            \"transactions\": self.transactions,\n            \"address\": self.address,\n", "solution": "\n        groups = []\n\n        if self:\n            last_txn = self.tail_transaction\n            current_group = [last_txn]\n            for current_txn in self.transactions[1:]:\n                # Transactions are grouped by address, so as long as the\n                # address stays consistent from one transaction to\n                # another, we are still in the same group.\n                if current_txn.address == last_txn.address:\n                    current_group.append(current_txn)\n                else:\n                    groups.append(current_group)\n                    current_group = [current_txn]\n\n                last_txn = current_txn\n\n            if current_group:\n                groups.append(current_group)\n\n        return groups"}
{"prompt": "def authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"", "output": "\n        callback_uri = callback_uri or self.config.callback\n        callback_uri = urlparse.urlparse(callback_uri)\n        query = urlparse.parse_qs(callback_uri.query)\n        query.setlist(ax_attrs, self.config.ax_attrs)\n        query.setdefault(\"openid.mode\", \"check_authentication\")\n        query.setdefault(\"openid.ns\", self.config.ns_uri)\n        query.setdefault(\"openid.claimed_id\",\n                          self.config.identity_url)\n        query.setdefault(\"openid.identity\",\n                          self.config.identity_url)\n        query.setdefault(\"openid.return_to\",\n                          self.config.return_to)\n        query.setdefault(\"openid.mode\", \"check_authentication\")\n        query.set", "solution": "\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))"}
{"prompt": "def write_byte_data(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"", "output": "\n        addr = self._check_addr(addr)\n        cmd_byte = cmd << 4\n        cmd_byte |= val\n        self.write_byte_data(addr, cmd_byte)\n\n    def write_byte_data_byte(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"\n        addr = self._check_addr(addr)\n        cmd_byte = cmd << 4\n        cmd_byte |= val\n        self.write_byte_data(addr, cmd_byte)\n\n    def write_byte_data_bytes(self, addr, cmd, val):\n        \"\"\"Write a byte of data to the specified cmd register of the device.\n        \"\"\"\n        addr = self._check_addr(addr)\n        cmd_byte = cmd << 4\n        ", "solution": "\n        assert self._device is not None, 'Bus must be opened before operations are made against it!'\n        # Construct a string of data to send with the command register and byte value.\n        data = bytearray(2)\n        data[0] = cmd & 0xFF\n        data[1] = val & 0xFF\n        # Send the data to the device.\n        self._select_device(addr)\n        self._device.write(data)"}
{"prompt": "def remove_client(self, client):\n        # type: (object) -> None\n        \"\"\"Remove the client from the users of the socket.\n\n        If there are no more clients for the socket, it\n        will close automatically.\n        \"\"\"", "output": "\n        if client in self.clients:\n            self.clients.remove(client)\n            self.clients_lock.acquire()\n            try:\n                self.clients_lock.release()\n            except AttributeError:\n                pass\n\n    def close_socket(self, sock):\n        # type: (socket.socket) -> None\n        \"\"\"Close the socket connection.\"\"\"\n        sock.close()\n\n    def close_all(self):\n        # type: () -> None\n        \"\"\"Close all sockets.\"\"\"\n        for sock in self.sockets:\n            self.close_socket(sock)\n\n    def wait(self):\n        # type: () -> None\n        \"\"\"Wait until all sockets are closed.\"\"\"\n        self.close_all()\n        self.socks_lock.acquire()\n        try:\n            while self.sockets:\n                self.socks_lock.release()\n                self.socks_lock.acquire()\n", "solution": "\n\n        try:\n            self._clients.remove(id(client))\n        except ValueError:\n            pass\n\n        if len(self._clients) < 1:\n            self.close()"}
{"prompt": "def send(self, data):\n        \"\"\"\n        Send encoded instructions to Guacamole guacd server.\n        \"\"\"", "output": "\n        self.connection.sendall(data)\n\n    def receive(self):\n        \"\"\"\n        Receive encoded instructions from Guacamole guacd server.\n        \"\"\"\n        return self.connection.recv(1024)\n\n    def flush(self):\n        \"\"\"\n        Flush any pending data on the connection.\n        \"\"\"\n        self.connection.flush()\n\n    def close(self):\n        \"\"\"\n        Close the connection.\n        \"\"\"\n        self.connection.close()\n\n    def wait(self):\n        \"\"\"\n        Wait for instructions to arrive on the connection.\n        \"\"\"\n        self.connection.wait()\n\n    def quit(self):\n        \"\"\"\n        Quit the connection.\n        \"\"\"\n        self.connection.quit()\n\n    def waitfor(self, condition, timeout=None):\n        \"\"\"\n        Wait for instructions to ", "solution": "\n        self.logger.debug('Sending data: %s' % data)\n        self.client.sendall(data.encode())"}
{"prompt": "def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"", "output": "\n        return H2OFrame.strsplit(self, pattern)\n\n    def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"\n        return H2OFrame.strsplit(self, pattern)\n\n    def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target column on the given regular expression pattern.\n\n        :param str pattern: The split pattern.\n        :returns: H2OFrame containing columns of the split strings.\n        \"\"\"\n        return H2OFrame.strsplit(self, pattern)\n\n    def strsplit(self, pattern):\n        \"\"\"\n        Split the strings in the target ", "solution": "\n        fr = H2OFrame._expr(expr=ExprNode(\"strsplit\", self, pattern))\n        fr._ex._cache.nrows = self.nrow\n        return fr"}
{"prompt": "def report(self, output_file=sys.stdout):\n        \"\"\"Report gathered analysis data in human readable form.\"\"\"", "output": "\n        header = ['Name', 'Type', 'Value', 'Unit']\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('-' * 80)\n        header.append('", "solution": "\n        if self.verbose > 1:\n            with pprint_nosort():\n                pprint.pprint(self.results)\n\n        if self.verbose > 0:\n            print('Runtime (per repetition): {:.2g} s'.format(\n                self.results['Runtime (per repetition) [s]']),\n                file=output_file)\n        if self.verbose > 0:\n            print('Iterations per repetition: {!s}'.format(\n                self.results['Iterations per repetition']),\n                file=output_file)\n        print('Runtime (per cacheline update): {:.2f} cy/CL'.format(\n            self.results['Runtime (per cacheline update) [cy/CL]']),\n            file=output_file)\n        print('MEM volume (per repetition): {:.0f} Byte'.format(\n            self.results['MEM volume (per repetition) [B]']),\n            file=output_file)\n        print('Performance: {:.2f} MFLOP/s'.format(self.results['Performance [MFLOP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} MLUP/s'.format(self.results['Performance [MLUP/s]']),\n              file=output_file)\n        print('Performance: {:.2f} It/s'.format(self.results['Performance [MIt/s]']),\n              file=output_file)\n        if self.verbose > 0:\n            print('MEM bandwidth: {:.2f} MByte/s'.format(self.results['MEM BW [MByte/s]']),\n                  file=output_file)\n        print('', file=output_file)\n\n        if not self.no_phenoecm:\n            print(\"Data Transfers:\")\n            print(\"{:^8} |\".format(\"cache\"), end='')\n            for metrics in self.results['data transfers'].values():\n                for metric_name in sorted(metrics):\n                    print(\" {:^14}\".format(metric_name), end='')\n                print()\n                break\n            for cache, metrics in sorted(self.results['data transfers'].items()):\n                print(\"{!s:^8} |\".format(cache), end='')\n                for k, v in sorted(metrics.items()):\n                    print(\" {!s:^14}\".format(v), end='')\n                print()\n            print()\n\n            print('Phenomenological ECM model: {{ {T_OL:.1f} || {T_nOL:.1f} | {T_L1L2:.1f} | '\n                  '{T_L2L3:.1f} | {T_L3MEM:.1f} }} cy/CL'.format(\n                **{k: float(v) for k, v in self.results['ECM'].items()}),\n                file=output_file)\n            print('T_OL assumes that two loads per cycle may be retiered, which is true for '\n                  '128bit SSE/half-AVX loads on SNB and IVY, and 256bit full-AVX loads on HSW, '\n                  'BDW, SKL and SKX, but it also depends on AGU availability.',\n                  file=output_file)"}
{"prompt": "def wait_for_kernel(self, timeout=None):\n        \"\"\"method to wait for a kernel to be ready\"\"\"", "output": "\n        kernel_id = self.kernel_id\n        if kernel_id is None:\n            raise NoSuchKernel(kernel_id=kernel_id)\n        kernel = self.clients.kernels.get(kernel_id)\n        if kernel.is_alive:\n            return kernel\n        else:\n            LOG.debug(\"Kernel %s is not ready.\", kernel_id)\n            raise KernelNotReady(kernel_id=kernel_id)\n\n    def start_kernel(self, kernel_id=None, path=None):\n        \"\"\"start a kernel\"\"\"\n        if kernel_id is None:\n            kernel_id = self.kernel_id\n        if kernel_id is None:\n            raise NoSuchKernel(kernel_id=kernel_id)\n        kernel = self.clients.kernels.get(kernel_id)\n        if kernel.is_alive:\n            ", "solution": "\n        tic = time.time()\n        self.km.hb_channel.unpause()\n        while True:\n            self.run_cell('1', False)\n            if self.km.hb_channel.is_beating():\n                # heart failure was not the reason this returned\n                break\n            else:\n                # heart failed\n                if timeout is not None and (time.time() - tic) > timeout:\n                    return False\n        return True"}
{"prompt": "def load(self, pkcs11dll_filename=None, *init_string):\n        \"\"\"\n        load a PKCS#11 library\n\n        :type pkcs11dll_filename: string\n        :param pkcs11dll_filename: the library name.\n          If this parameter is not set then the environment variable\n          `PYKCS11LIB` is used instead\n        :returns: a :class:`PyKCS11Lib` object\n        :raises: :class:`PyKCS11Error` (-1): when the load fails\n        \"\"\"", "output": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = self._pkcs11dll_filename\n        if pkcs11dll_filename is None:\n            raise PyKCS11Error(\n                \"PKCS#11 library could not be loaded: environment variable \"\n                \"PYKCS11LIB is not set\")\n        lib = ctypes.CDLL(pkcs11dll_filename)\n        lib.PKCS11_Create.restype = ctypes.c_void_p\n        lib.PKCS11_Create.argtypes = [ctypes.c_char_p]\n        lib.PKCS11_Create._argtypes = (ctypes.c_char_p,)\n        lib.PKCS11_Create._values = (None,)\n        ", "solution": "\n        if pkcs11dll_filename is None:\n            pkcs11dll_filename = os.getenv(\"PYKCS11LIB\")\n            if pkcs11dll_filename is None:\n                raise PyKCS11Error(-1, \"No PKCS11 library specified (set PYKCS11LIB env variable)\")\n        rv = self.lib.Load(pkcs11dll_filename)\n        if rv == 0:\n            raise PyKCS11Error(-1, pkcs11dll_filename)"}
{"prompt": "def load_publickey(type, buffer):\n    \"\"\"\n    Load a public key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n    :type buffer: A Python string object, either unicode or bytestring.\n    :return: The PKey object.\n    :rtype: :class:`PKey`\n    \"\"\"", "output": "\n    key = load_key(type, buffer)\n    return key\n\n\ndef load_privkey(type, buffer):\n    \"\"\"\n    Load a private key from a buffer.\n\n    :param type: The file type (one of :data:`FILETYPE_PEM`,\n        :data:`FILETYPE_ASN1`).\n    :param buffer: The buffer the key is stored in.\n    :type buffer: A Python string object, either unicode or bytestring.\n    :return: The PKey object.\n    :rtype: :class:`PKey`\n    \"\"\"\n    key = load_key(type, buffer)\n    return key\n\n\ndef load_x509(type, buffer):\n    \"\"\"\n    Load an X.509 certificate from a buffer.\n\n    :param type: The file type (one of :data:`", "solution": "\n    if isinstance(buffer, _text_type):\n        buffer = buffer.encode(\"ascii\")\n\n    bio = _new_mem_buf(buffer)\n\n    if type == FILETYPE_PEM:\n        evp_pkey = _lib.PEM_read_bio_PUBKEY(\n            bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n    elif type == FILETYPE_ASN1:\n        evp_pkey = _lib.d2i_PUBKEY_bio(bio, _ffi.NULL)\n    else:\n        raise ValueError(\"type argument must be FILETYPE_PEM or FILETYPE_ASN1\")\n\n    if evp_pkey == _ffi.NULL:\n        _raise_current_error()\n\n    pkey = PKey.__new__(PKey)\n    pkey._pkey = _ffi.gc(evp_pkey, _lib.EVP_PKEY_free)\n    pkey._only_public = True\n    return pkey"}
{"prompt": "def words(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"", "output": "\n        return self.words_list\n\n    def words_count(self):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0431\u043e\u0440\u0435\n        \"\"\"\n        return len(self.words)\n\n    def words_count_by_word(self, word):\n        \"\"\"\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0431\u043e\u0440\u0435, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u043c\u0441\u044f \u0432 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\n        \"\"\"\n        return self.words_list.count(word)\n\n    def words_count_by_word_and_pos(self, ", "solution": "\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield \"\".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))"}
{"prompt": "def parse_forensic_report(feedback_report, sample, msg_date,\n                          nameservers=None, dns_timeout=2.0,\n                          strip_attachment_payloads=False,\n                          parallel=False):\n    \"\"\"\n    Converts a DMARC forensic report and sample to a ``OrderedDict``\n\n    Args:\n        feedback_report (str): A message's feedback report as a string\n        sample (str): The RFC 822 headers or RFC 822 message sample\n        msg_date (str): The message's date header\n        nameservers (list): A list of one or more nameservers to use\n        (Cloudflare's public DNS resolvers by default)\n        dns_timeout (float): Sets the DNS timeout in seconds\n        strip_attachment_payloads (bool): Remove attachment payloads from\n        forensic report results\n        parallel (bool): Parallel processing\n\n    Returns:\n        OrderedDict: A parsed report and sample\n    \"\"\"", "output": "\n        strip_attachment_payloads (bool): If True, strips attachment\n        payloads from the message\n        parallel (bool): If True, run in parallel on multiple threads\n\n    Returns:\n        OrderedDict: A dictionary containing the parsed forensic report\n    \"\"\"\n    parsed_forensic_report = OrderedDict()\n    parsed_forensic_report['feedback_report'] = feedback_report\n    parsed_forensic_report['sample'] = sample\n    parsed_forensic_report['msg_date'] = msg_date\n\n    # Parse the nameservers\n    if nameservers is None:\n        nameservers = parse_dns_nameservers(sample.split(b'\\r\\n'))\n\n    # Parse the DNS ", "solution": "\n    delivery_results = [\"delivered\", \"spam\", \"policy\", \"reject\", \"other\"]\n\n    try:\n        parsed_report = OrderedDict()\n        report_values = feedback_report_regex.findall(feedback_report)\n        for report_value in report_values:\n            key = report_value[0].lower().replace(\"-\", \"_\")\n            parsed_report[key] = report_value[1]\n\n        if \"arrival_date\" not in parsed_report:\n            if msg_date is None:\n                raise InvalidForensicReport(\n                    \"Forensic sample is not a valid email\")\n            parsed_report[\"arrival_date\"] = msg_date.isoformat()\n\n        if \"version\" not in parsed_report:\n            parsed_report[\"version\"] = 1\n\n        if \"user_agent\" not in parsed_report:\n            parsed_report[\"user_agent\"] = None\n\n        if \"delivery_result\" not in parsed_report:\n            parsed_report[\"delivery_result\"] = None\n        else:\n            for delivery_result in delivery_results:\n                if delivery_result in parsed_report[\"delivery_result\"].lower():\n                    parsed_report[\"delivery_result\"] = delivery_result\n                    break\n        if parsed_report[\"delivery_result\"] not in delivery_results:\n            parsed_report[\"delivery_result\"] = \"other\"\n\n        arrival_utc = human_timestamp_to_datetime(\n            parsed_report[\"arrival_date\"], to_utc=True)\n        arrival_utc = arrival_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n        parsed_report[\"arrival_date_utc\"] = arrival_utc\n\n        ip_address = parsed_report[\"source_ip\"]\n        parsed_report_source = get_ip_address_info(ip_address,\n                                                   nameservers=nameservers,\n                                                   timeout=dns_timeout,\n                                                   parallel=parallel)\n        parsed_report[\"source\"] = parsed_report_source\n        del parsed_report[\"source_ip\"]\n\n        if \"identity_alignment\" not in parsed_report:\n            parsed_report[\"authentication_mechanisms\"] = []\n        elif parsed_report[\"identity_alignment\"] == \"none\":\n            parsed_report[\"authentication_mechanisms\"] = []\n            del parsed_report[\"identity_alignment\"]\n        else:\n            auth_mechanisms = parsed_report[\"identity_alignment\"]\n            auth_mechanisms = auth_mechanisms.split(\",\")\n            parsed_report[\"authentication_mechanisms\"] = auth_mechanisms\n            del parsed_report[\"identity_alignment\"]\n\n        if \"auth_failure\" not in parsed_report:\n            parsed_report[\"auth_failure\"] = \"dmarc\"\n        auth_failure = parsed_report[\"auth_failure\"].split(\",\")\n        parsed_report[\"auth_failure\"] = auth_failure\n\n        optional_fields = [\"original_envelope_id\", \"dkim_domain\",\n                           \"original_mail_from\", \"original_rcpt_to\"]\n        for optional_field in optional_fields:\n            if optional_field not in parsed_report:\n                parsed_report[optional_field] = None\n\n        parsed_sample = parse_email(\n            sample,\n            strip_attachment_payloads=strip_attachment_payloads)\n\n        if \"reported_domain\" not in parsed_report:\n            parsed_report[\"reported_domain\"] = parsed_sample[\"from\"][\"domain\"]\n\n        sample_headers_only = False\n        number_of_attachments = len(parsed_sample[\"attachments\"])\n        if number_of_attachments < 1 and parsed_sample[\"body\"] is None:\n            sample_headers_only = True\n        if sample_headers_only and parsed_sample[\"has_defects\"]:\n            del parsed_sample[\"defects\"]\n            del parsed_sample[\"defects_categories\"]\n            del parsed_sample[\"has_defects\"]\n        parsed_report[\"sample_headers_only\"] = sample_headers_only\n        parsed_report[\"sample\"] = sample\n        parsed_report[\"parsed_sample\"] = parsed_sample\n\n        return parsed_report\n\n    except KeyError as error:\n        raise InvalidForensicReport(\"Missing value: {0}\".format(\n            error.__str__()))\n\n    except Exception as error:\n        raise InvalidForensicReport(\n            \"Unexpected error: {0}\".format(error.__str__()))"}
{"prompt": "def map_peaks_to_image(peaks, r=4, vox_dims=(2, 2, 2), dims=(91, 109, 91),\n                       header=None):\n    \"\"\" Take a set of discrete foci (i.e., 2-D array of xyz coordinates)\n    and generate a corresponding image, convolving each focus with a\n    hard sphere of radius r.\"\"\"", "output": "\n    peaks = peaks.astype(np.int)\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    peaks = peaks.transpose()\n    peaks = peaks.reshape((-1, 3))\n    ", "solution": "\n    data = np.zeros(dims)\n    for p in peaks:\n        valid = get_sphere(p, r, vox_dims, dims)\n        valid = valid[:, ::-1]\n        data[tuple(valid.T)] = 1\n    return nifti1.Nifti1Image(data, None, header=header)"}
{"prompt": "def iter_parse(fiql_str):\n    \"\"\"Iterate through the FIQL string. Yield a tuple containing the\n    following FIQL components for each iteration:\n\n      - preamble: Any operator or opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n      - comparison: The comparison portion of a FIQL constraint or ``None``\n        if yielding the last portion of the string.\n      - argument: The argument portion of a FIQL constraint or ``None`` if\n        yielding the last portion of the string.\n\n    For usage see :func:`parse_str_to_expression`.\n\n    Args:\n        fiql_str (string): The FIQL formatted string we want to parse.\n\n    Yields:\n        tuple: Preamble, selector, comparison, argument.\n    \"\"\"", "output": "each iteration, yield a tuple containing the following components:\n\n      - operator: The operator portion of the FIQL string.\n      - opening_paranthesis: Any opening/closing paranthesis preceding a\n        constraint or at the very end of the FIQL string.\n      - selector: The selector portion of the FIQL string.\n      - comparison: The comparison portion of the FIQL string.\n      - argument: The argument portion of the FIQL string.\n\n    \"\"\"\n    preamble, selector, comparison, argument = fiql_str.partition(' ')\n    operator = selector.strip()\n    opening_paranthesis = selector.strip()\n    if operator == '(':\n        yield (\n            operator,", "solution": "\n    while len(fiql_str):\n        constraint_match = CONSTRAINT_COMP.split(fiql_str, 1)\n        if len(constraint_match) < 2:\n            yield (constraint_match[0], None, None, None)\n            break\n        yield (\n            constraint_match[0],\n            unquote_plus(constraint_match[1]),\n            constraint_match[4],\n            unquote_plus(constraint_match[6]) \\\n                    if constraint_match[6] else None\n        )\n        fiql_str = constraint_match[8]"}
{"prompt": "def shape(self):\n        \"\"\"Returns the shape of the data.\"\"\"", "output": "\n        return self._shape\n\n    def size(self):\n        \"\"\"Returns the size of the data.\"\"\"\n        return self._size\n\n    def dtype(self):\n        \"\"\"Returns the data type.\"\"\"\n        return self._dtype\n\n    def ndim(self):\n        \"\"\"Returns the number of dimensions.\"\"\"\n        return self._ndim\n\n    def sizes(self):\n        \"\"\"Returns the sizes of the data.\"\"\"\n        return self._sizes\n\n    def itemsize(self):\n        \"\"\"Returns the data item size.\"\"\"\n        return self._itemsize\n\n    def nbytes(self):\n        \"\"\"Returns the number of bytes occupied by the data.\"\"\"\n        return self._nbytes\n\n    def nbyteswap(self):\n        \"\"\"Returns the number of bytes occupied by swapping the data.\"\"\"\n        return self._nbyteswap\n\n    def contiguous(self):\n        \"\"\"Returns whether the data is contiguous.\"\"\"\n        return self._contiguous\n\n    def flags(self):\n        \"\"\"Returns the flag", "solution": "\n        # TODO cache\n        first = self.first().shape\n        shape = self._rdd.map(lambda x: x.shape[0]).sum()\n        return (shape,) + first[1:]"}
{"prompt": "def _wait_job_completion(self):\n        \"\"\"Wait for the cache to be empty before resizing the pool.\"\"\"", "output": "\n        LOG.debug('Waiting for cache to become empty.')\n        LOG.debug('Cache size: %s', self.cache_size)\n        LOG.debug('Pool size: %s', self.pool_size)\n        LOG.debug('Waiting for cache to become empty.')\n        LOG.debug('Cache size: %s', self.cache_size)\n        LOG.debug('Pool size: %s', self.pool_size)\n        LOG.debug('Waiting for cache to become empty.')\n        LOG.debug('Cache size: %s', self.cache_size)\n        LOG.debug('Pool size: %s', self.pool_size)\n        LOG.debug('Waiting for cache to become empty.')\n        LOG.debug('Cache size: %s', self.cache_size)\n        LOG.debug('Pool size: %s', self.pool_size)", "solution": "\n        # Issue a warning to the user about the bad effect of this usage.\n        if len(self._pending_work_items) > 0:\n            warnings.warn(\"Trying to resize an executor with running jobs: \"\n                          \"waiting for jobs completion before resizing.\",\n                          UserWarning)\n            mp.util.debug(\"Executor {} waiting for jobs completion before\"\n                          \" resizing\".format(self.executor_id))\n        # Wait for the completion of the jobs\n        while len(self._pending_work_items) > 0:\n            time.sleep(1e-3)"}
{"prompt": "def energy(self, state=None):\n        \"\"\"Calculates the length of the route.\"\"\"", "output": "\n        route = []\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(0)\n        route.append(", "solution": "\n        state = self.state if state is None else state\n        route = state\n        e = 0\n        if self.distance_matrix:\n            for i in range(len(route)):\n                e += self.distance_matrix[\"{},{}\".format(route[i-1], route[i])]\n        else:\n            for i in range(len(route)):\n                e += distance(self.cities[route[i-1]], self.cities[route[i]])\n        return e"}
{"prompt": "def recommendations(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"", "output": "\n        params = {'language': kwargs.get('language', None), 'page': kwargs.get('page', 1)}\n        return self._get('recommendations', params=params)\n\n    def recommended_movies(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO 639-1 code.\n            page: (optional) Minimum value of 1.  Expected value is an integer.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        params = {'language': kwargs.get('language', None), 'page': kwargs.get('page', 1)}\n        return self._get('recommended_movies', params=params)\n\n    def recommended_movies_detail(self, **kwargs):\n        \"\"\"\n        Get a list of recommended movies for a movie.\n\n        Args:\n            language: (optional) ISO ", "solution": "\n        path = self._get_id_path('recommendations')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def get_clinvar_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"", "output": "\n        clinvar_subm_id = None\n        try:\n            clinvar_subm_id = submission_id.split(\"SUB\")[0]\n        except:\n            pass\n        return clinvar_subm_id\n\n    def get_clinvar_submission_id(self, submission_id):\n        \"\"\"Returns the official Clinvar submission ID for a submission object\n\n            Args:\n                submission_id(str): submission_id(str) : id of the submission\n\n            Returns:\n                clinvar_subm_id(str): a string with a format: SUB[0-9]. It is obtained from clinvar portal when starting a new submission\n\n        \"\"\"\n        clinvar_subm_id = None\n        try:\n            clinvar_subm_id = submission_id.split(\"SUB\")[0]\n        except:", "solution": "\n        submission_obj = self.clinvar_submission_collection.find_one({'_id': ObjectId(submission_id)})\n        clinvar_subm_id = submission_obj.get('clinvar_subm_id') # This key does not exist if it was not previously provided by user\n        return clinvar_subm_id"}
{"prompt": "def dict_factory(self, cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        row_dict = dict(row)\n        row_dict[\"id\"] = row_dict.pop(\"id\")\n        row_dict[\"created_at\"] = row_dict.pop(\"created_at\")\n        row_dict[\"updated_at\"] = row_dict.pop(\"updated_at\")\n        row_dict[\"deleted_at\"] = row_dict.pop(\"deleted_at\")\n        row_dict[\"deleted\"] = row_dict.pop(\"deleted\")\n        row_dict[\"status\"] = row_dict.pop(\"status\")\n        row_dict[\"type\"] = row_dict.pop(\"type\")\n        row_dict[\"value\"] = row_dict.pop(\"value\")\n        row_dict[\"value_type\"] = row_dict.pop(\"value_type\")\n        row_dict[\"value_id\"] = row_dict", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp:\n                d[col[0]] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":  # or name == Field.Meter_Time:\n                continue\n            if name not in self.m_all_fields:\n                continue\n            if (str(val) != \"None\") and ((val > 0) or (val < 0)):\n                d[name] = str(val)\n        return d"}
{"prompt": "def streams(self, url, **params):\n        \"\"\"Attempts to find a plugin and extract streams from the *url*.\n\n        *params* are passed to :func:`Plugin.streams`.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"", "output": "\n        plugin = self.plugins.get(url)\n        if plugin is None:\n            raise NoPluginError(url)\n        return plugin.streams(**params)\n\n    def stream(self, url):\n        \"\"\"Attempts to find a plugin and extract stream from the *url*.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"\n        plugin = self.plugins.get(url)\n        if plugin is None:\n            raise NoPluginError(url)\n        return plugin.stream()\n\n    def stream_thumbnail(self, url):\n        \"\"\"Attempts to find a plugin and extract stream thumbnail from\n        the *url*.\n\n        Raises :exc:`NoPluginError` if no plugin is found.\n        \"\"\"\n        plugin = self.plugins.get(url)", "solution": "\n\n        plugin = self.resolve_url(url)\n        return plugin.streams(**params)"}
{"prompt": "def map(self, func):\n        \"\"\"\n        Process all data with given function.\n        The scheme of function should be x,y -> x,y.\n        \"\"\"", "output": "\n        func = self.to_numpy(func)\n        x, y = self.to_numpy(x)\n        x, y = self.to_numpy(y)\n        x, y = func(x, y)\n        return x, y\n\n    def apply_func(self, func, *args, **kwargs):\n        \"\"\"\n        Apply function on all data.\n        \"\"\"\n        x, y = self.to_numpy(x)\n        x, y = self.to_numpy(y)\n        x, y = func(x, y)\n        return x, y\n\n    def apply_func2(self, func, *args, **kwargs):\n        \"\"\"\n        Apply function on all data.\n        \"\"\"\n        x, y = self.to_numpy(x)\n        x, y = self.to_numpy(y)\n        x, y = func(x, y)\n        return x, y\n\n    def apply_func3(self, func, *args, **kwargs):\n        \"\"\"\n        Apply function on all data.\n", "solution": "\n        if self._train_set:\n            self._train_set = map(func, self._train_set)\n        if self._valid_set:\n            self._valid_set = map(func, self._valid_set)\n        if self._test_set:\n            self._test_set = map(func, self._test_set)"}
{"prompt": "def positions_func(inputs, pad=0):\n    \"\"\"\n    A layer filling i-th column of a 2D tensor with\n    1+ln(1+i) when it contains a meaningful symbol\n    and with 0 when it contains PAD\n    \"\"\"", "output": "\n    pos = tf.cast(tf.where(tf.equal(inputs, 1)), 1, tf.int32)\n    pos = tf.cast(tf.where(tf.equal(inputs, 0)), 0, tf.int32)\n    pos = tf.pad(pos, [[0, 0], [pad, pad], [pad, pad]])\n    return pos\n\n\ndef positional_encoding(\n    d_model, max_len=500, dtype=tf.float32, scale_grad_by_freq=False\n):\n    \"\"\"\n    Implements sinusoidal positional encoding.\n    Padding symbols are ignored, but it is necessary to specify whether we\n    should add position information at padding_idx (via padding) or not (via not\n    padding).\n    \"\"\"\n    freq_table = tf.get_variable(\n        \"", "solution": "\n    position_inputs = kb.cumsum(kb.ones_like(inputs, dtype=\"float32\"), axis=1)\n    position_inputs *= kb.cast(kb.not_equal(inputs, pad), \"float32\")\n    return kb.log(1.0 + position_inputs)"}
{"prompt": "def _with_loc(f: W) -> W:\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"", "output": "\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        loc = f.func_code.co_firstlineno\n        lineno = f.func_code.co_firstlineno + 1\n        function = f.func_name\n        code = f.func_code\n        filename = code.co_filename\n        name = code.co_name\n        co = code.co_code\n        co = co.replace(\n            \"co_firstlineno\", f\"{lineno}:\"\n        ).replace(\n            \"co_filename\", f\"{filename}:{name}:\"\n        ).replace(\n            \"co_name\", f\"{name}:\"\n        )\n        co = co.replace(\n            \"co_code\", f\"{filename}:{name}:{lineno}:{function}\"\n        )\n        co = co", "solution": "\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)"}
{"prompt": "def p_creg_decl(self, program):\n        \"\"\"\n           creg_decl : CREG indexed_id\n        \"\"\"", "output": "\n        program.add_declaration(\"creg_decl\", CREG, indexed_id=self.get_id())\n\n    def p_creg_decl_def(self, program):\n        \"\"\"\n           creg_decl_def : creg_decl ID\n        \"\"\"\n        program.add_declaration(\"creg_decl_def\", CREG, indexed_id=self.get_id())\n\n    def p_creg_decl_def_id(self, program):\n        \"\"\"\n           creg_decl_def_id : creg_decl_def ID\n        \"\"\"\n        program.add_declaration(\"creg_decl_def_id\", CREG, indexed_id=self.get_id())\n\n    def p_cre", "solution": "\n        program[0] = node.Creg([program[2]])\n        if program[2].name in self.external_functions:\n            raise QasmError(\"CREG names cannot be reserved words. \"\n                            + \"Received '\" + program[2].name + \"'\")\n        if program[2].index == 0:\n            raise QasmError(\"CREG size must be positive\")\n        self.update_symtab(program[0])"}
{"prompt": "def order_verification(self, institute, case, user, link, variant):\n        \"\"\"Create an event for a variant verification for a variant\n        and an event for a variant verification for a case\n\n        Arguments:\n            institute (dict): A Institute object\n            case (dict): Case object\n            user (dict): A User object\n            link (str): The url to be used in the event\n            variant (dict): A variant object\n\n        Returns:\n            updated_variant(dict)\n        \"\"\"", "output": "\n        updated_variant = {}\n        updated_variant['link'] = link\n        updated_variant['user'] = user\n        updated_variant['case'] = case\n        updated_variant['institute'] = institute\n        updated_variant['variant'] = variant\n        updated_variant['status'] ='verified'\n        updated_variant['verified'] = True\n        updated_variant['verified_at'] = datetime.utcnow()\n        updated_variant['verified_by'] = user['username']\n        updated_variant['verified_by'] = user['username']\n        updated_variant['verified_by'] = user['username']\n        updated_variant['verified_by'] = user", "solution": "\n        LOG.info(\"Creating event for ordering validation for variant\" \\\n                    \" {0}\".format(variant['display_name']))\n\n        updated_variant = self.variant_collection.find_one_and_update(\n            {'_id': variant['_id']},\n            {'$set': {'sanger_ordered': True}},\n            return_document=pymongo.ReturnDocument.AFTER\n        )\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='variant',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n\n        LOG.info(\"Creating event for ordering sanger for case\" \\\n                    \" {0}\".format(case['display_name']))\n\n        self.create_event(\n            institute=institute,\n            case=case,\n            user=user,\n            link=link,\n            category='case',\n            verb='sanger',\n            variant=variant,\n            subject=variant['display_name'],\n        )\n        return updated_variant"}
{"prompt": "def update_product_set(\n        self,\n        product_set,\n        location=None,\n        product_set_id=None,\n        update_mask=None,\n        project_id=None,\n        retry=None,\n        timeout=None,\n        metadata=None,\n    ):\n        \"\"\"\n        For the documentation see:\n        :class:`~airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator`\n        \"\"\"", "output": "\n        body = {\n            \"location\": location,\n            \"productSet\": product_set,\n            \"updateMask\": update_mask,\n        }\n        if product_set_id is not None:\n            body[\"productSetId\"] = product_set_id\n        if project_id is not None:\n            body[\"projectId\"] = project_id\n        if retry is not None:\n            body[\"retry\"] = retry\n        if timeout is not None:\n            body[\"timeout\"] = timeout\n        if metadata is not None:\n            body[\"metadata\"] = metadata\n\n        return self.execute_with_retries(\n            method=\"POST\",\n            endpoint=\"vision/v1beta1/productSets\",\n            body=body,\n            headers=self._gcp_vision_headers,", "solution": "\n        client = self.get_conn()\n        product_set = self.product_set_name_determiner.get_entity_with_name(\n            product_set, product_set_id, location, project_id\n        )\n        self.log.info('Updating ProductSet: %s', product_set.name)\n        response = client.update_product_set(\n            product_set=product_set, update_mask=update_mask, retry=retry, timeout=timeout, metadata=metadata\n        )\n        self.log.info('ProductSet updated: %s', response.name if response else '')\n        self.log.debug('ProductSet updated:\\n%s', response)\n        return MessageToDict(response)"}
{"prompt": "def get_mic(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Calculates a cryptographic message integrity code (MIC) over an application message, and\n        returns that MIC in a token. This is in contrast to :meth:`wrap` which calculates a MIC\n        over a message, optionally encrypts it and returns the original message and the MIC packed\n        into a single token. The peer application can then verify the MIC to ensure the associated\n        message has not been changed in transit.\n\n        :param message: The message to calculate a MIC for\n        :type message: bytes\n        :param qop_req: The quality of protection required. It is recommended to not change this\n            from the default as most GSSAPI implementations do not support it.\n        :returns: A MIC for the message calculated using this security context's cryptographic keys\n        :rtype: bytes\n        \"\"\"", "output": "\n        :type qop_req: integer\n        :return: The MIC\n        :rtype: bytes\n        \"\"\"\n        token = token()\n        token.qop = qop_req\n        token.mic = calculate_mic(message, token.qop)\n        return token, token.mic\n\n    def wrap(self, message, qop_req=C.GSS_C_QOP_DEFAULT):\n        \"\"\"\n        Wraps a message into an application token, and returns the original message and the MIC\n        packed into a single token. The peer application can then verify the MIC to ensure the associated\n        message has not been changed in transit.\n\n        :param message: The message to wrap\n        :type message: bytes\n        :param qop_req: The quality of ", "solution": "\n        if not (self.flags & C.GSS_C_INTEG_FLAG):\n            raise GSSException(\"No integrity protection negotiated.\")\n        if not (self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)):\n            raise GSSException(\"Protection not yet ready.\")\n\n        minor_status = ffi.new('OM_uint32[1]')\n        output_token_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer = ffi.new('gss_buffer_desc[1]')\n        message_buffer[0].length = len(message)\n        c_str_message = ffi.new('char[]', message)\n        message_buffer[0].value = c_str_message\n        retval = C.gss_get_mic(\n            minor_status,\n            self._ctx[0],\n            ffi.cast('gss_qop_t', qop_req),\n            message_buffer,\n            output_token_buffer\n        )\n        try:\n            if GSS_ERROR(retval):\n                if minor_status[0] and self.mech_type:\n                    raise _exception_for_status(retval, minor_status[0], self.mech_type)\n                else:\n                    raise _exception_for_status(retval, minor_status[0])\n\n            output_token = _buf_to_str(output_token_buffer[0])\n            return output_token\n        finally:\n            if output_token_buffer[0].length != 0:\n                C.gss_release_buffer(minor_status, output_token_buffer)"}
{"prompt": "def extractHolidayDate(self, setting_holiday):\n        \"\"\" Read a single holiday date from meter buffer.\n\n        Args:\n            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)\n\n        Returns:\n            tuple: Holiday tuple, elements are strings.\n\n            =============== ======================\n            Holiday         Holiday 0-19 as string\n            Day             Day 1-31 as string\n            Month           Monty 1-12 as string\n            =============== ======================\n\n        \"\"\"", "output": "\n        holiday = self.meter_buffer.readInt()\n        day = self.meter_buffer.readInt()\n        month = self.meter_buffer.readInt()\n        return holiday, day, month\n\n    def extractHolidayDates(self, setting_holiday):\n        \"\"\" Read multiple holiday dates from meter buffer.\n\n        Args:\n            setting_holiday (int):  Holiday from 0-19 or in range(Extents.Holidays)\n\n        Returns:\n            list: List of tuples of strings.\n\n            =============== ======================\n            Holiday         Holiday 0-19 as string\n            Day             Day 1-31 as string\n            Month           Monty 1-12 as string", "solution": "\n        ret = namedtuple(\"result\", [\"Holiday\", \"Month\", \"Day\"])\n        setting_holiday += 1\n        ret.Holiday = str(setting_holiday)\n\n        if (setting_holiday < 1) or (setting_holiday > Extents.Holidays):\n            ekm_log(\"Out of bounds:  holiday \" + str(setting_holiday))\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n\n        idxday = \"Holiday_\" + str(setting_holiday) + \"_Day\"\n        idxmon = \"Holiday_\" + str(setting_holiday) + \"_Mon\"\n        if idxmon not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        if idxday not in self.m_hldy:\n            ret.Holiday = ret.Month = ret.Day = str(0)\n            return ret\n        ret.Day = self.m_hldy[idxday][MeterData.StringValue]\n        ret.Month = self.m_hldy[idxmon][MeterData.StringValue]\n        return ret"}
{"prompt": "def getMaxStmIdForStm(stm):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"", "output": "\n    maxId = 0\n    for stmId in stm.getAssignments():\n        if stmId > maxId:\n            maxId = stmId\n    return maxId\n\ndef getMaxStmIdForStmId(stmId):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"\n    maxId = 0\n    for stmId in stm.getAssignments():\n        if stmId > maxId:\n            maxId = stmId\n    return maxId\n\ndef getMaxStmIdForStmIds(stmIds):\n    \"\"\"\n    Get maximum _instId from all assigments in statement\n    \"\"\"\n    maxId = 0\n    for stmId in stmId", "solution": "\n    maxId = 0\n    if isinstance(stm, Assignment):\n        return stm._instId\n    elif isinstance(stm, WaitStm):\n        return maxId\n    else:\n        for _stm in stm._iter_stms():\n            maxId = max(maxId, getMaxStmIdForStm(_stm))\n        return maxId"}
{"prompt": "def delete(self, endpoint, headers):\n        \"\"\"\n        Method to delete an item or all items\n\n        headers['If-Match'] must contain the _etag identifier of the element to delete\n\n        :param endpoint: endpoint (API URL)\n        :type endpoint: str\n        :param headers: headers (example: Content-Type)\n        :type headers: dict\n        :return: response (deletion information)\n        :rtype: dict\n        \"\"\"", "output": "\n        endpoint = endpoint.rstrip('/')\n        headers['If-Match'] = headers['If-Match'] or ''\n        headers['If-None-Match'] = headers['If-None-Match'] or ''\n        headers['If-Modified-Since'] = headers['If-Modified-Since'] or ''\n        headers['If-Unmodified-Since'] = headers['If-Unmodified-Since'] or ''\n        headers['If-Match'] = headers['If-Match'] or ''\n        headers['If-None-Match'] = headers['If-None-Match'] or ''\n        headers['If-Modified-Since'] = headers['If-", "solution": "\n        response = self.get_response(method='DELETE', endpoint=endpoint, headers=headers)\n\n        logger.debug(\"delete, response: %s\", response)\n        if response.status_code != 204:  # pragma: no cover - should not happen ...\n            resp = self.decode(response=response)\n\n        resp = {\"_status\": \"OK\"}\n        return resp"}
{"prompt": "def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param safe_dag_id: a dag_id with only alphanumeric characters\n        :param safe_task_id: a task_id with only alphanumeric characters\n        :param random_uuid: a uuid\n        :return: ``str`` valid Pod name of appropriate length\n        \"\"\"", "output": "\n        safe_dag_id = safe_dag_id.replace('.', '__')\n        safe_task_id = safe_task_id.replace('.', '__')\n        safe_uuid = safe_uuid.replace('.', '__')\n        safe_pod_id = '{}_{}_{}'.format(safe_dag_id, safe_task_id, safe_uuid)\n        return safe_pod_id\n\n    @staticmethod\n    def _make_safe_pod_name(safe_dag_id, safe_task_id, safe_uuid):\n        \"\"\"\n        Kubernetes pod names must be <= 253 chars and must pass the following regex for\n        validation\n        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n\n        :param ", "solution": "\n        MAX_POD_ID_LEN = 253\n\n        safe_key = safe_dag_id + safe_task_id\n\n        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + \"-\" + safe_uuid\n\n        return safe_pod_id"}
{"prompt": "def run(self, func):\n        \"\"\"Run 'func' under os sandboxing\"\"\"", "output": "\n        func()\n\n    def sudo(self, func):\n        \"\"\"Run 'func' under os sudo\"\"\"\n        func()\n\n    def sudo_inc(self, func):\n        \"\"\"Run 'func' under os sudo with incremental sudo\"\"\"\n        func()\n\n    def sudo_inc_sudo(self, func):\n        \"\"\"Run 'func' under os sudo with incremental sudo\"\"\"\n        func()\n\n    def sudo_inc_sudo_inc(self, func):\n        \"\"\"Run 'func' under os sudo with incremental sudo\"\"\"\n        func()\n\n    def sudo_inc_sudo_inc_sudo(self, func):\n        \"\"\"Run 'func' under os sudo with incremental sudo\"\"\"\n", "solution": "\n        try:\n            self._copy(self)\n            if _file:\n                __builtin__.file = self._file\n            __builtin__.open = self._open\n            self._active = True\n            return func()\n        finally:\n            self._active = False\n            if _file:\n                __builtin__.file = _file\n            __builtin__.open = _open\n            self._copy(_os)"}
{"prompt": "def _isint(string):\n    \"\"\"\n    >>> _isint(\"123\")\n    True\n    >>> _isint(\"123.45\")\n    False\n    \"\"\"", "output": "\n    try:\n        int(string)\n        return True\n    except ValueError:\n        return False\n\n\ndef _isfloat(string):\n    \"\"\"\n    >>> _isfloat(\"123\")\n    True\n    >>> _isfloat(\"123.45\")\n    False\n    \"\"\"\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n\n\ndef _isstr(string):\n    \"\"\"\n    >>> _isstr(\"123\")\n    True\n    >>> _isstr(\"123.45\")\n    False\n    \"\"\"\n    try:\n        str(string)\n        return True\n    except ValueError:\n        return False\n\n\ndef _islist(string):\n    \"\"\"\n    >>> _islist(\"123\")\n    True\n    >>> _islist(\"123.45\")\n    False\n    \"\"\"\n    try:\n        list(string)\n", "solution": "\n    return type(string) is int or \\\n        (isinstance(string, _binary_type) or\n         isinstance(string, string_types)) and \\\n        _isconvertible(int, string)"}
{"prompt": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the Get request payload and decode it into its\n        constituent parts.\n\n        Args:\n            input_stream (stream): A data stream containing encoded object\n                data, supporting a read method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"", "output": "\n        object_type = enums.ObjectType.Get\n        object_class = enums.ObjectClass.Get\n        object_name = enums.ObjectName.Get\n        object_version = enums.ObjectVersion.Get\n        object_size = enums.ObjectSize.Get\n        object_data = enums.ObjectData.Get\n        object_data_length = enums.ObjectDataLength.Get\n        object_data_offset = enums.ObjectDataOffset.Get\n        object_data_length = enums.ObjectDataLength.Get\n        object_data_offset = enums.ObjectDataOffset.Get\n        object_data_length = enums.ObjectDataLength.Get\n        object_data_offset = enums.ObjectDataOffset.Get\n        object_data_length = enums.Object", "solution": "\n        super(GetRequestPayload, self).read(\n            input_stream,\n            kmip_version=kmip_version\n        )\n        local_stream = utils.BytearrayStream(input_stream.read(self.length))\n\n        if self.is_tag_next(enums.Tags.UNIQUE_IDENTIFIER, local_stream):\n            self._unique_identifier = primitives.TextString(\n                tag=enums.Tags.UNIQUE_IDENTIFIER\n            )\n            self._unique_identifier.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_FORMAT_TYPE, local_stream):\n            self._key_format_type = primitives.Enumeration(\n                enum=enums.KeyFormatType,\n                tag=enums.Tags.KEY_FORMAT_TYPE\n            )\n            self._key_format_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(enums.Tags.KEY_COMPRESSION_TYPE, local_stream):\n            self._key_compression_type = primitives.Enumeration(\n                enum=enums.KeyCompressionType,\n                tag=enums.Tags.KEY_COMPRESSION_TYPE\n            )\n            self._key_compression_type.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        if self.is_tag_next(\n                enums.Tags.KEY_WRAPPING_SPECIFICATION,\n                local_stream\n        ):\n            self._key_wrapping_specification = \\\n                objects.KeyWrappingSpecification()\n            self._key_wrapping_specification.read(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.is_oversized(local_stream)"}
{"prompt": "def get_latex_maybe_optional_arg(self, pos):\n        \"\"\"\n        Parses the latex content given to the constructor (and stored in `self.s`),\n        starting at position `pos`, to attempt to parse an optional argument.\n\n        Attempts to parse an optional argument. If this is successful, we return\n        a tuple `(node, pos, len)` if success where `node` is a\n        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.\n        \"\"\"", "output": "\n        node = LatexGroupNode(self.s, pos)\n        pos = node.pos\n        len = 0\n        pos = node.pos\n        len = self.parse_optional_arg(pos, len)\n        if len == 0:\n            return None\n        return node, pos, len\n\n    def parse_optional_arg(self, pos, len):\n        \"\"\"\n        Attempts to parse an optional argument. If this is successful, we return\n        a tuple `(node, pos, len)` if success where `node` is a\n        :py:class:`LatexGroupNode`.  Otherwise, this method returns None.\n        \"\"\"\n        node = LatexGroupNode(self.s, pos)\n        pos = node.pos\n        len = self.parse_arg(pos, len)\n        if len == 0:", "solution": "\n\n        tok = self.get_token(pos, brackets_are_chars=False, environments=False)\n        if (tok.tok == 'brace_open' and tok.arg == '['):\n            return self.get_latex_braced_group(pos, brace_type='[')\n\n        return None"}
{"prompt": "def _label__get(self):\n        \"\"\"\n        Get or set any <label> element associated with this element.\n        \"\"\"", "output": "\n        return self._label\n\n    @label.setter\n    def label(self, value):\n        \"\"\"\n        Set any <label> element associated with this element.\n        \"\"\"\n        self._label = value\n    label = property(label, doc=\"(DOM) The label of this element.\")\n\n    def _get_labels(self):\n        \"\"\"\n        Get all <label> elements associated with this element.\n        \"\"\"\n        return self._labels\n\n    def _set_labels(self, value):\n        \"\"\"\n        Set all <label> elements associated with this element.\n        \"\"\"\n        self._labels = value\n    _labels = property(labels, _set_labels, doc=\"All <label> elements associated with this element.\")\n\n    def _get_name(self):\n        \"\"\"\n        Get or set the name attribute of this element.\n        \"\"\"\n        return self._name\n\n    def _set_", "solution": "\n        id = self.get('id')\n        if not id:\n            return None\n        result = _label_xpath(self, id=id)\n        if not result:\n            return None\n        else:\n            return result[0]"}
{"prompt": "def _split_comma_separated(string):\n    \"\"\"Return a set of strings.\"\"\"", "output": "\n    return set(string.split(','))\n\n\ndef _split_comma_separated_list(string):\n    \"\"\"Return a list of strings.\"\"\"\n    return [s.strip() for s in string.split(',')]\n\n\ndef _split_comma_separated_list_list(string):\n    \"\"\"Return a list of lists of strings.\"\"\"\n    return [s.split(',') for s in string.split(',')]\n\n\ndef _split_quoted_string(string):\n    \"\"\"Split a string into a list of quoted strings.\"\"\"\n    string = _unicode(string)\n    string = _string_to_list(string)\n    string = [s.strip('\"') for s in string]\n    return string\n\n\ndef _split_quoted_string_as_list(string):\n    \"\"\"Split a string into a list of quoted strings.\"\"\"\n    string = _unicode(string)\n    ", "solution": "\n    return set(text.strip() for text in string.split(',') if text.strip())"}
{"prompt": "def apply_next_patch(self, force=False, quiet=False):\n        \"\"\" Apply next patch in series file \"\"\"", "output": "\n        patch = self.patch\n        patch_file = self.patch_file\n        patch_file_path = patch_file\n        patch_file_name = patch_file\n        patch_file_path = patch_file_name\n        patch_file_name = patch_file_name\n        patch_file_path = patch_file_path[:-4]\n        patch_file_name = patch_file_name[:-4]\n        patch_file_path = patch_file_path[:-4]\n        patch_file_name = patch_file_path[:-4]\n        patch_file_path = patch_file_path[:-4]\n        patch_file_name = patch_file_path[:-4]\n        patch_file_path = patch_file_path[:-4]\n        patch_file_name = patch_file_path[:-4]\n        patch_file_path = patch_file_path[:-4]\n        patch_file", "solution": "\n        self._check()\n        top = self.db.top_patch()\n        if not top:\n            patch = self.series.first_patch()\n        else:\n            patch = self.series.patch_after(top)\n\n        if not patch:\n            raise AllPatchesApplied(self.series, top)\n\n        self.applying(patch)\n\n        self._apply_patch(patch, force, quiet)\n\n        self.db.save()\n\n        self.applied(self.db.top_patch())"}
{"prompt": "def _data_sanity_checks(self, explore_iterable):\n        \"\"\"Checks if data values are  valid.\n\n        Checks if the data values are supported by the parameter and if the values are of the same\n        type as the default value.\n\n        \"\"\"", "output": "\n        # Check if all values are supported\n        valid = True\n        for value in explore_iterable:\n            try:\n                float(value)\n                valid = False\n            except ValueError:\n                valid = False\n                break\n        if not valid:\n            raise ExploreDataException(\n                \"Invalid data values provided. Please provide valid data values.\"\n            )\n\n        # Check if all values are of the same type\n        valid = True\n        for value in explore_iterable:\n            try:\n                float(value)\n                valid = False\n            except ValueError:\n                valid = False\n                break\n        if not valid:\n            raise ExploreDataException(\n                \"Invalid data values provided. Please provide valid data values.\"\n            )\n\n        # Check if all values are of the ", "solution": "\n        data_list = []\n\n        for val in explore_iterable:\n\n            if not self.f_supports(val):\n                raise TypeError('%s is of not supported type %s.' % (repr(val), str(type(val))))\n\n            if not self._values_of_same_type(val, self._default):\n                raise TypeError(\n                    'Data of `%s` is not of the same type as the original entry value, '\n                    'new type is %s vs old type %s.' %\n                    (self.v_full_name, str(type(val)), str(type(self._default))))\n\n            data_list.append(val)\n\n        if len(data_list) == 0:\n            raise ValueError('Cannot explore an empty list!')\n\n        return data_list"}
{"prompt": "def _cursor_position_changed(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"", "output": "\n        cursor_pos = self._cursor_pos\n        cursor_pos = cursor_pos - self._cursor_pos\n        if cursor_pos < 0:\n            self._cursor_pos = 0\n        elif cursor_pos > self._cursor_length:\n            self._cursor_pos = self._cursor_length\n        self._update_tip()\n\n    def _update_tip(self):\n        \"\"\" Updates the tip based on user cursor movement.\n        \"\"\"\n        cursor_pos = self._cursor_pos\n        cursor_pos = cursor_pos - self._cursor_pos\n        cursor_pos = min(cursor_pos, self._cursor_length)\n        cursor_pos = max(0, cursor_pos)\n        cursor_pos = min(cursor_pos, self._cursor_length)\n        cursor_pos = max(0, cursor_pos)\n        cursor_pos = min(cursor_pos, self._cursor_length)\n", "solution": "\n        cursor = self._text_edit.textCursor()\n        if cursor.position() <= self._start_position:\n            self.hide()\n        else:\n            position, commas = self._find_parenthesis(self._start_position + 1)\n            if position != -1:\n                self.hide()"}
{"prompt": "def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"", "output": "\n        resources = []\n        for resource in self.resources:\n            if resource not in resources:\n                resources.append(resource)\n        return resources\n\n    def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"\n        resources = []\n        for resource in self.resources:\n            if resource not in resources:\n                resources.append(resource)\n        return resources\n\n    def resources(self):\n        \"\"\"\n        Returns list of unique `FragmentResource`s by order of first appearance.\n        \"\"\"\n        resources = []\n        for resource in self.resources:\n            if resource not in resources:\n                resources.append(resource)\n        return resources\n\n    def resources(self):\n        \"\"\"\n        Returns list of unique `", "solution": "\n        seen = set()\n        # seen.add always returns None, so 'not seen.add(x)' is always True,\n        # but will only be called if the value is not already in seen (because\n        # 'and' short-circuits)\n        return [x for x in self._resources if x not in seen and not seen.add(x)]"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        node = f(*args, **kwargs)\n        node.production = Production(node.production, node.pos)\n        return node\n    return wrapper\n\n\ndef track_productions(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        node = f(*args, **kwargs)\n        node.productions = [Production(node.production, node.pos)\n                             for _ in range(len(node.productions))]\n        return node\n    return wrapper\n\n\ndef track_productions_with_name(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        node = f(*args, **kwargs)\n        ", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def s2p(self):\n        \"\"\"Return 2 proton separation energy\"\"\"", "output": "\n        return self.efermi_energy * self.efermi_energy\n\n    def s2p_energy(self):\n        \"\"\"Return 2 proton separation energy as energy\"\"\"\n        return self.efermi_energy\n\n    def s2p_energy_energy(self):\n        \"\"\"Return 2 proton separation energy as energy + energy\"\"\"\n        return self.efermi_energy + self.efermi_energy\n\n    def s2p_energy_energy_energy(self):\n        \"\"\"Return 2 proton separation energy as energy + energy + energy\"\"\"\n        return self.efermi_energy + self.efermi_energy + self.efermi_energy\n\n    def s2p_energy_energy_energy_energy(self):\n        \"\"\"Return 2 ", "solution": "\n        M_P = 7.28897050         # proton mass excess in MeV\n        f = lambda parent, daugther: -parent + daugther + 2 * M_P\n        return self.derived('s2p', (-2, 0), f)"}
{"prompt": "def clear_xcom_data(self, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"", "output": "\n        if session is None:\n            session = get_session()\n        session.execute('DELETE FROM xcom_data')\n\n    def clear_xcom_data_by_instance(self, instance_id, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"\n        if session is None:\n            session = get_session()\n        session.execute('DELETE FROM xcom_data WHERE instance_id=:instance_id',\n                       {'instance_id': instance_id})\n\n    def clear_xcom_data_by_task(self, task_id, session=None):\n        \"\"\"\n        Clears all XCom data from the database for the task instance\n        \"\"\"\n        if session is None:\n            session = get_session()\n        session.execute('DELETE FROM ", "solution": "\n        session.query(XCom).filter(\n            XCom.dag_id == self.dag_id,\n            XCom.task_id == self.task_id,\n            XCom.execution_date == self.execution_date\n        ).delete()\n        session.commit()"}
{"prompt": "def as_property_description(self):\n        \"\"\"\n        Get the property description.\n\n        Returns a dictionary describing the property.\n        \"\"\"", "output": "\n        prop = {}\n        prop['description'] = self.description\n        prop['required'] = self.required\n        prop['default'] = self.default\n        prop['enum'] = self.enum\n        prop['type'] = self.type\n        prop['enum_values'] = self.enum_values\n        prop['description_template'] = self.description_template\n        prop['description_template_format'] = self.description_template_format\n        prop['description_template_params'] = self.description_template_params\n        prop['format'] = self.format\n        prop['format_params'] = self.format_params\n        prop['minimum'] = self.minimum\n        prop['maximum'] = self.maximum\n        prop['minimum_value'] = self.minimum_value", "solution": "\n        description = deepcopy(self.metadata)\n\n        if 'links' not in description:\n            description['links'] = []\n\n        description['links'].append(\n            {\n                'rel': 'property',\n                'href': self.href_prefix + self.href,\n            }\n        )\n        return description"}
{"prompt": "def save(self, fname=None):\n        \"\"\"\n        Download the chart from the URL into a filename as a PNG\n\n        The filename defaults to the chart title (chtt) if any\n        \"\"\"", "output": "\n        title = fname if fname else self.title\n        title = title.strip()\n        title = title.title()\n        title = title.capitalize()\n        title = title.replace(' ', '_')\n        title = title.replace('-', '_')\n        title = title.replace('(', '_')\n        title = title.replace(')', '_')\n        title = title.replace('*', '_')\n        title = title.replace('&', '_')\n        title = title.replace('@', '_')\n        title = title.replace('#', '_')\n        title = title.replace('$', '_')\n        title = title.replace('%', '_')\n        title = title.replace('^', '_')\n        title = title.replace('_', '_')\n        title = title.replace('", "solution": "\n        if not fname:\n            fname = self.getname()\n        assert fname != None, 'You must specify a filename to save to'\n        if not fname.endswith('.png'):\n            fname += '.png'\n        try:\n            urlretrieve(self.url, fname)\n        except Exception:\n            raise IOError('Problem saving %s to file'%fname)\n        return fname"}
{"prompt": "def opt(parser: Union[Parser, Sequence[Input]]) -> OptionalParser:\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"", "output": "\n    if isinstance(parser, Parser):\n        return parser\n    if isinstance(parser, Literal):\n        return OptionalParser(parser)\n    if isinstance(parser, list):\n        return OptionalParser(parser[0])\n    if isinstance(parser, tuple):\n        return OptionalParser(parser[0])\n    if isinstance(parser, dict):\n        return OptionalParser(parser[\"op\"])\n    if isinstance(parser, str):\n        return OptionalParser(parser)\n    raise TypeError(\n        \"Expected a Parser or a Literal or a list of Parsers or Literals or a dict\"\n    )\n\n\ndef match(*parsers: Union[Parser, Sequence[Input]]) -> Match:\n    \"\"\"Match a list of parsers.\n\n    Matches parsers sequentially until one succeeds. Returns a Match object\n    containing all successful matches.\n\n", "solution": "\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return OptionalParser(parser)"}
{"prompt": "def log(self):\n        \"\"\"\n        Print on screen and on file the percentages for each status.\n        \"\"\"", "output": "\n        if self.verbose:\n            msg = \"[{:<10}] {:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}{:<10}", "solution": "\n\n        if (\n            PyFunceble.CONFIGURATION[\"show_percentage\"]\n            and PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0\n        ):\n            # * We are allowed to show the percentage on screen.\n            # and\n            # * The number of tested is greater than 0.\n\n            # We initiate the output file.\n            output = (\n                PyFunceble.OUTPUT_DIRECTORY\n                + PyFunceble.OUTPUTS[\"parent_directory\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"parent\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"directories\"][\"percentage\"]\n                + PyFunceble.OUTPUTS[\"logs\"][\"filenames\"][\"percentage\"]\n            )\n\n            # We delete the output file if it does exist.\n            File(output).delete()\n\n            # We calculate the percentage of each statuses.\n            self._calculate()\n\n            if not PyFunceble.CONFIGURATION[\"quiet\"]:\n                # The quiet mode is activated.\n\n                # We print a new line.\n                print(\"\\n\")\n\n                # We print the percentage header on file and screen.\n                Prints(None, \"Percentage\", output).header()\n\n                # We construct the different lines/data to print on screen and file.\n                lines_to_print = [\n                    [\n                        PyFunceble.STATUS[\"official\"][\"up\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"up\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"up\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"down\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"down\"]) + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"down\"],\n                    ],\n                    [\n                        PyFunceble.STATUS[\"official\"][\"invalid\"],\n                        str(PyFunceble.INTERN[\"counter\"][\"percentage\"][\"invalid\"])\n                        + \"%\",\n                        PyFunceble.INTERN[\"counter\"][\"number\"][\"invalid\"],\n                    ],\n                ]\n\n                if PyFunceble.CONFIGURATION[\"syntax\"]:\n                    # We are checking for syntax.\n\n                    # We update the denomination of the UP.\n                    lines_to_print[0][0] = PyFunceble.STATUS[\"official\"][\"valid\"]\n\n                    # And we unset the INACTIVE line.\n                    del lines_to_print[1]\n\n                for to_print in lines_to_print:\n                    # We loop throught the different line to print.\n                    # (one line for each status.)\n\n                    # And we print the current status line on file and screen.\n                    Prints(to_print, \"Percentage\", output).data()\n\n        elif PyFunceble.INTERN[\"counter\"][\"number\"][\"tested\"] > 0:\n            # * We are not allowed to show the percentage on screen.\n            # but\n            # * The number of tested is greater than 0.\n\n            # We run the calculation.\n            # Note: The following is needed, because all counter calculation are\n            # done by this class.\n            self._calculate()"}
{"prompt": "def grid(self, *args):\n        \"\"\"\n        Apply a grid to your chart\n        args are of the form::\n            <x axis step size>,\n            <y axis step size>,\n            <length of line segment>,\n            <length of blank segment>\n            <x offset>,\n            <y offset>\n        APIPARAM: chg\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        grids =  map(str,map(float,args))\n        self['chg'] = ','.join(grids).replace('None','')\n        return self"}
{"prompt": "def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"", "output": "\n        collection = mongo_collection\n        filter_doc = filter_doc or {}\n        filter_doc = self._filter_doc(filter_doc)\n        collection = collection or self.db\n        collection.delete_one(filter_doc, **kwargs)\n\n    def delete_many(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a set of documents in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_many\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the documents to delete.\n        :type filter_doc: dict\n        :param mongo_db: The ", "solution": "\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)"}
{"prompt": "def _check_relative_import(\n        self, modnode, importnode, importedmodnode, importedasname\n    ):\n        \"\"\"check relative import. node is either an Import or From node, modname\n        the imported module name.\n        \"\"\"", "output": "\n        level = 0\n        levelname = None\n        levelnode = None\n        levelnamenode = None\n        levelnameasname = None\n        levelnameasnamenode = None\n        levelnameasnameasname = None\n        levelnameasnameasnamenode = None\n        levelnameasnameasnameasname = None\n        levelnameasnameasnameasnameasname = None\n        levelnameasnameasnameasnameasnameasname = None\n        levelnameasnameasnameasnameasnameasnameasname = None\n        levelnameasnameasnameasnameasnameasnameasnameasname", "solution": "\n        if not self.linter.is_message_enabled(\"relative-import\"):\n            return None\n        if importedmodnode.file is None:\n            return False  # built-in module\n        if modnode is importedmodnode:\n            return False  # module importing itself\n        if modnode.absolute_import_activated() or getattr(importnode, \"level\", None):\n            return False\n        if importedmodnode.name != importedasname:\n            # this must be a relative import...\n            self.add_message(\n                \"relative-import\",\n                args=(importedasname, importedmodnode.name),\n                node=importnode,\n            )\n            return None\n        return None"}
{"prompt": "def download(self, bucket_name, object_name, filename=None):\n        \"\"\"\n        Get a file from Google Cloud Storage.\n\n        :param bucket_name: The bucket to fetch from.\n        :type bucket_name: str\n        :param object_name: The object to fetch.\n        :type object_name: str\n        :param filename: If set, a local file path where the file should be written to.\n        :type filename: str\n        \"\"\"", "output": "\n        client = self._meta.client\n        download_url = client.generate_download_url(bucket_name, object_name)\n        req = client.bucket_storage_download_get_request(\n            bucket_name, object_name, download_url, filename=filename\n        )\n        client._connection.request(req, self._on_download)\n\n    def _on_download(self, response):\n        \"\"\"\n        Callback for download requests.\n\n        :param response: The response from the request.\n        :type response: :class:`~google.cloud.storage.client.ClientRawResponse`\n        \"\"\"\n        self._finish_download(response)\n\n    def _finish_download(self, response):\n        \"\"\"\n        Finish downloading a file from Google Cloud Storage.\n\n        :param response: The response from the request.\n        :type response: :class:`~google.cloud.", "solution": "\n        client = self.get_conn()\n        bucket = client.get_bucket(bucket_name)\n        blob = bucket.blob(blob_name=object_name)\n\n        if filename:\n            blob.download_to_filename(filename)\n            self.log.info('File downloaded to %s', filename)\n\n        return blob.download_as_string()"}
{"prompt": "def parse(self, s):\n        \"\"\"Parses the input string, and returns a reference to the created AST's root\"\"\"", "output": "\n        tree = ast.parse(s)\n        self.visit(tree)\n        return tree\n\n    def visit(self, node):\n        \"\"\"Visit a node\"\"\"\n        method = 'visit_' + node.__class__.__name__\n        visitor = getattr(self, method, self.generic_visit)\n        return visitor(node)\n\n    def generic_visit(self, node):\n        \"\"\"Called if no explicit visitor function exists for a node\"\"\"\n        for field, value in ast.iter_fields(node):\n            if isinstance(value, list):\n                for item in value:\n                    self.visit(item)\n            elif isinstance(value, ast.AST):\n                self.visit(value)\n\n\nclass ASTVisitor(ast.NodeVisitor):\n    \"\"\"Visitor to visit an AST tree\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize a visitor object\"\"\"\n        super(AST", "solution": "\n        with self.lock:\n            try:\n                return self.parser.parse(s, lexer=self.lexer)\n            except InvalidIEMLObjectArgument as e:\n                raise CannotParse(s, str(e))\n            except CannotParse as e:\n                e.s = s\n                raise e"}
{"prompt": "def allocate_ids(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n        .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"", "output": "\n        partial_keys = [self._to_full_key(key) for key in partial_keys]\n        full_keys = []\n        for partial_key in partial_keys:\n            full_keys.append(self._client.allocate_ids(partial_key))\n        return full_keys\n\n    def allocate_ids_batch(self, partial_keys):\n        \"\"\"\n        Allocate IDs for incomplete keys.\n\n       .. seealso::\n            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n        :param partial_keys: a list of partial keys.\n        :type partial_keys: list\n        :return: a list of full keys.\n        :rtype: list\n        \"\"\"\n        partial_keys = [self._to_full_key(key) for key in partial_keys]\n        full_key", "solution": "\n        conn = self.get_conn()\n\n        resp = (conn\n                .projects()\n                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})\n                .execute(num_retries=self.num_retries))\n\n        return resp['keys']"}
{"prompt": "def _insert_plain_text_into_buffer(self, cursor, text):\n        \"\"\" Inserts text into the input buffer using the specified cursor (which\n            must be in the input buffer), ensuring that continuation prompts are\n            inserted as necessary.\n        \"\"\"", "output": "\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        cursor.forward_char()\n        cursor.backward_char()\n        ", "solution": "\n        lines = text.splitlines(True)\n        if lines:\n            cursor.beginEditBlock()\n            cursor.insertText(lines[0])\n            for line in lines[1:]:\n                if self._continuation_prompt_html is None:\n                    cursor.insertText(self._continuation_prompt)\n                else:\n                    self._continuation_prompt = \\\n                        self._insert_html_fetching_plain_text(\n                            cursor, self._continuation_prompt_html)\n                cursor.insertText(line)\n            cursor.endEditBlock()"}
{"prompt": "def get_total_contributors(self, repo):\n        \"\"\"\n        Retrieves the number of contributors to a repo in the organization.\n        Also adds to unique contributor list.\n        \"\"\"", "output": "\n        contributors = []\n        for contributor in self.get_contributors(repo):\n            contributors.append(contributor)\n            contributors.append(contributor.name)\n            contributors.append(contributor.login)\n            contributors.append(contributor.url)\n            contributors.append(contributor.avatar_url)\n            contributors.append(contributor.web_url)\n            contributors.append(contributor.description or \"\")\n            contributors.append(contributor.bio)\n            contributors.append(contributor.location)\n            contributors.append(contributor.public_repos)\n            contributors.append(contributor.private_repos)\n            contributors.append(contributor.organization)\n            contributors.append(contributor.organization_type)\n            contributors.append(contributor.url)\n            contributors.append(contributor", "solution": "\n        repo_contributors = 0\n        for contributor in repo.iter_contributors():\n            repo_contributors += 1\n            self.unique_contributors[contributor.id].append(repo.name)\n            self.contributors_json[repo.name].append(contributor.to_json())\n        return repo_contributors"}
{"prompt": "def _unreg_event(self, event_list, event):\n        \"\"\" Tries to remove a registered event without triggering it \"\"\"", "output": "\n        for event in event_list:\n            if event.name == event.name:\n                event_list.remove(event.name)\n                return\n        raise KeyError(\"Event not found\")\n\n    def _unreg_event_list(self, event_list):\n        \"\"\" Tries to remove all registered events without triggering them \"\"\"\n        for event in event_list:\n            self._unreg_event(self._event_list, event)\n\n    def _unreg_event_list_by_name(self, event_list):\n        \"\"\" Tries to remove all registered events by name without triggering them \"\"\"\n        for event in event_list:\n            if event.name == event.name:\n                event_list.remove(event.name)\n                return\n        raise KeyError(\"Event not found\")\n\n    def _unreg_event_list_by_type(self, event_", "solution": "\n        try:\n            self.log.debug(\"Removing event {0}({1},{2})\".format(event['function'].__name__, event['args'], event['kwargs']))\n        except AttributeError:\n            self.log.debug(\"Removing event {0}\".format(str(event)))\n\n        try:\n            event_list.remove(event)\n        except ValueError:\n            try:\n                self.log.warn(\"Unable to remove event {0}({1},{2}) , not found in list: {3}\".format(event['function'].__name__, event['args'], event['kwargs'], event_list))\n            except AttributeError:\n                self.log.debug(\"Unable to remove event {0}\".format(str(event)))\n            raise KeyError('Unable to unregister the specified event from the signals specified')"}
{"prompt": "def get_system_per_cpu_times():\n    \"\"\"Return system per-CPU times as a list of named tuples.\"\"\"", "output": "\n    cpu_times = []\n    for cpu in per_cpu_times():\n        user, system, idle = cpu\n        idle = max(idle, 0)\n        cpu_times.append(_make_cpu_time_summary(user, system, idle))\n    return cpu_times\n\n\ndef per_cpu_times():\n    \"\"\"Return system-wide CPU times as a list of named tuples.\"\"\"\n    cpu_times = cext.per_cpu_times()\n    for cpu in cpu_times:\n        user = cpu.user\n        system = cpu.system\n        idle = cpu.idle\n        yield user, system, idle\n\n\ndef boot_time():\n    \"\"\"The system boot time expressed in seconds since the epo", "solution": "\n    ret = []\n    for cpu_t in _psutil_mswindows.get_system_cpu_times():\n        user, system, idle = cpu_t\n        item = _cputimes_ntuple(user, system, idle)\n        ret.append(item)\n    return ret"}
{"prompt": "def queue_startfile(self, cmdfile):\n        \"\"\"Arrange for file of debugger commands to get read in the\n        process-command loop.\"\"\"", "output": "\n        self.cmdfile = cmdfile\n        self.cmdfile_lock = threading.Lock()\n        self.cmdfile_lock.acquire()\n        self.cmdfile_lock.notifyAll()\n        self.cmdfile_lock.release()\n\n    def queue_stopfile(self):\n        \"\"\"Stop the file of debugger commands.\"\"\"\n        self.cmdfile_lock.acquire()\n        self.cmdfile_lock.notifyAll()\n        self.cmdfile_lock.release()\n\n    def queue_run(self, cmd):\n        \"\"\"Run the command in the debugger command file.\"\"\"\n        self.cmdfile_lock.acquire()\n        self.cmdfile_lock.notifyAll()\n        self.cmdfile_lock.release()\n        self.cmdfile.write(cmd)\n        self.cmdfile.flush()\n        self.cmdfile.seek(0)\n\n    def queue_", "solution": "\n        expanded_cmdfile = os.path.expanduser(cmdfile)\n        is_readable = Mfile.readable(expanded_cmdfile)\n        if is_readable:\n            self.cmd_queue.append('source ' + expanded_cmdfile)\n        elif is_readable is None:\n            self.errmsg(\"source file '%s' doesn't exist\" % expanded_cmdfile)\n        else:\n            self.errmsg(\"source file '%s' is not readable\" %\n                        expanded_cmdfile)\n            pass\n        return"}
{"prompt": "def read_infile(infile: Union[Path, str], from_words=False,\n                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,\n                tag_column: int = TAG_COLUMN, max_sents: int = -1,\n                read_only_words: bool = False) -> List[Tuple[List, Union[List, None]]]:\n    \"\"\"Reads input file in CONLL-U format\n\n    Args:\n        infile: a path to a file\n        word_column: column containing words (default=1)\n        pos_column: column containing part-of-speech labels (default=3)\n        tag_column: column containing fine-grained tags (default=5)\n        max_sents: maximal number of sents to read\n        read_only_words: whether to read only words\n\n    Returns:\n        a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``\n        in case ``read_only_words = True``\n    \"\"\"", "output": "words: if True, only read words and ignore the tags\n\n    Returns:\n        a list of tuples (sentences, tags)\n    \"\"\"\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []\n    sentence = []\n    tag = []\n    sentences = []\n    tags = []", "solution": "\n    answer, curr_word_sent, curr_tag_sent = [], [], []\n    if from_words:\n        word_column, read_only_words = 0, True\n    with open(infile, \"r\", encoding=\"utf8\") as fin:\n        for line in fin:\n            line = line.strip()\n            if line.startswith(\"#\"):\n                continue\n            if line == \"\":\n                if len(curr_word_sent) > 0:\n                    if read_only_words:\n                        curr_tag_sent = None\n                    answer.append((curr_word_sent, curr_tag_sent))\n                curr_tag_sent, curr_word_sent = [], []\n                if len(answer) == max_sents:\n                    break\n                continue\n            splitted = line.split(\"\\t\")\n            index = splitted[0]\n            if not from_words and not index.isdigit():\n                continue\n            curr_word_sent.append(splitted[word_column])\n            if not read_only_words:\n                pos, tag = splitted[pos_column], splitted[tag_column]\n                tag = pos if tag == \"_\" else \"{},{}\".format(pos, tag)\n                curr_tag_sent.append(tag)\n        if len(curr_word_sent) > 0:\n            if read_only_words:\n                curr_tag_sent = None\n            answer.append((curr_word_sent, curr_tag_sent))\n    return answer"}
{"prompt": "def fuzzed(self):\n        \"\"\"\n        Get a printable fuzzed object\n        \"\"\"", "output": "\n        return FuzzedObject(self.obj, self.fuzzer_name)\n\n    def fuzz(self, fuzzer_name):\n        \"\"\"\n        Fuzz the object with the given fuzzer name\n        \"\"\"\n        Fuzzer.fuzz(self, fuzzer_name)\n        self.fuzzer_name = fuzzer_name\n        self.fuzz()\n\n    def fuzzer_name(self):\n        \"\"\"\n        Get the fuzzer name for this object\n        \"\"\"\n        return self.fuzzer_name\n\n    def fuzz(self):\n        \"\"\"\n        Fuzz the object by calling its fuzz method\n        \"\"\"\n        self.fuzzer_name = self.obj.fuzzer_name\n        self.obj.fuzz()\n\n    def fuzzer_name(self):\n        \"\"\"\n        Get the fuzzer name for this object\n        \"\"\"\n        return self.", "solution": "\n        try:\n            if self.config.strong_fuzz:\n                fuzzer = PJFMutators(self.config)\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                    else:\n                        return urllib.quote(fuzzer.fuzz(json.dumps(self.config.json)))\n                else:\n                    if type(self.config.json) in [list, dict]:\n                        return fuzzer.fuzz(json.dumps(self.config.json))\n                    else:\n                        return fuzzer.fuzz(self.config.json)\n            else:\n                if self.config.url_encode:\n                    if sys.version_info >= (3, 0):\n                        return urllib.parse.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                    else:\n                        return urllib.quote(self.get_fuzzed(self.config.indent, self.config.utf8))\n                else:\n                    return self.get_fuzzed(self.config.indent, self.config.utf8)\n        except Exception as e:\n            raise PJFBaseException(e.message if hasattr(e, \"message\") else str(e))"}
{"prompt": "def validate(self, request):\n        \"\"\"Validate JSON-RPC request.\n\n        :param request: RPC request object\n        :type request: dict\n\n        \"\"\"", "output": "\n        try:\n            jsonrpclib.loads(request.data)\n        except ValueError as e:\n            raise InvalidRequest(e.message)\n        except jsonrpclib.ProtocolError as e:\n            raise InvalidRequest(e.message)\n\n    def _handle_request(self, request, response):\n        \"\"\"Handle JSON-RPC request.\n\n        :param request: RPC request object\n        :type request: dict\n        :param response: RPC response object\n        :type response: dict\n\n        \"\"\"\n        try:\n            jsonrpclib.loads(request.data)\n        except ValueError as e:\n            raise InvalidRequest(e.message)\n        except jsonrpclib.ProtocolError as e:\n            raise InvalidRequest(e.message)\n\n        try:\n            method = getattr(self, request.method)\n        except AttributeError:\n            raise InvalidRequest(\"Method '%s' not supported.\" % request.method)\n\n        try:\n            ", "solution": "\n\n        try:\n            validate_version(request)\n            validate_method(request)\n            validate_params(request)\n            validate_id(request)\n        except (AssertionError, KeyError) as error:\n            invalid_request(error)"}
{"prompt": "def action_notify(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"", "output": "\n        for subscriber in self._subscribers:\n            subscriber.notify_change(action)\n\n    def notify_change(self, action):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        action -- the action whose status changed\n        \"\"\"\n        for subscriber in self._subscribers:\n            subscriber.notify_change(action)\n\n    def notify_exception(self, exception):\n        \"\"\"\n        Notify all subscribers of an exception status change.\n\n        exception -- the exception whose status changed\n        \"\"\"\n        for subscriber in self._subscribers:\n            subscriber.notify_exception(exception)\n\n    def notify_success(self, result):\n        \"\"\"\n        Notify all subscribers of an action status change.\n\n        result ", "solution": "\n        message = json.dumps({\n            'messageType': 'actionStatus',\n            'data': action.as_action_description(),\n        })\n\n        for subscriber in list(self.subscribers):\n            try:\n                subscriber.write_message(message)\n            except tornado.websocket.WebSocketClosedError:\n                pass"}
{"prompt": "def update_mongo_compound_variants(self, bulk):\n        \"\"\"Update the compound information for a bulk of variants in the database\n\n            Args:\n                bulk(dict): {'_id': scout.models.Variant}\n\n        \"\"\"", "output": "\n        variants = bulk['_id'].values()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id'].keys()\n        variants = bulk['_id", "solution": "\n        requests = []\n        for var_id in bulk:\n            var_obj = bulk[var_id]\n            if not var_obj.get('compounds'):\n                continue\n            # Add a request to update compounds\n            operation = pymongo.UpdateOne(\n                {'_id': var_obj['_id']},\n                {\n                    '$set': {\n                        'compounds': var_obj['compounds']\n                    }\n                })\n            requests.append(operation)\n\n        if not requests:\n            return\n\n        try:\n            self.variant_collection.bulk_write(requests, ordered=False)\n        except BulkWriteError as err:\n            LOG.warning(\"Updating compounds failed\")\n            raise err"}
{"prompt": "def module_list(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"", "output": "\n    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n\n\ndef module_list_from_folder(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"\n    return [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]\n\n\ndef module_list_from_files(path):\n    \"\"\"\n    Return the list containing the names of the modules available in the given\n    folder.\n    \"\"\"\n    return [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]\n\n\ndef module_path(name):\n    \"\"\"\n    Return the full path to the module with the given name.\n    \"\"\"\n    return os.path.join(os.path.dirname(__file__), name)\n\n\ndef module_list_from_path(path):\n    \"\"\"\n    Return the list containing the names of the modules available ", "solution": "\n    # sys.path has the cwd as an empty string, but isdir/listdir need it as '.'\n    if path == '':\n        path = '.'\n\n    if os.path.isdir(path):\n        folder_list = os.listdir(path)\n    elif path.endswith('.egg'):\n        try:\n            folder_list = [f for f in zipimporter(path)._files]\n        except:\n            folder_list = []\n    else:\n        folder_list = []\n\n    if not folder_list:\n        return []\n\n    # A few local constants to be used in loops below\n    isfile = os.path.isfile\n    pjoin = os.path.join\n    basename = os.path.basename\n\n    def is_importable_file(path):\n        "}
{"prompt": "def modify_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"", "output": "\n    conf = {}\n    conf[\"redbaron\"] = {\n        \"enabled\": True,\n        \"port\": 5000,\n        \"host\": \"0.0.0.0\",\n        \"debug\": True,\n    }\n    return conf\n\n\ndef modify_redbaron_conf():\n    \"\"\"\n    pip install redbaron\n    \"\"\"\n    conf = {}\n    conf[\"redbaron\"] = {\n        \"enabled\": True,\n        \"port\": 5000,\n        \"host\": \"0.0.0.0\",\n        \"debug\": True,\n    }\n    return conf\n\n\ndef modify_redbaron_conf_with_custom_host():\n    \"\"\"\n    pip install redbaron\n    \"\"\"\n    conf = {}\n    conf[\"redbaron\"] = {", "solution": "\n    import redbaron\n    import ubelt as ub\n    conf_path = 'docs/conf.py'\n\n    source = ub.readfrom(conf_path)\n    red = redbaron.RedBaron(source)\n\n    # Insert custom extensions\n    extra_extensions = [\n        '\"sphinxcontrib.napoleon\"'\n    ]\n\n    ext_node = red.find('name', value='extensions').parent\n    ext_node.value.value.extend(extra_extensions)\n\n    # Overwrite theme to read-the-docs\n    theme_node = red.find('name', value='html_theme').parent\n    theme_node.value.value = '\"sphinx_rtd_theme\"'\n\n    ub.writeto(conf_path, red.dumps())"}
{"prompt": "def show(self, *args, **kwargs):\n        \"\"\"\n        Shows the chart URL in a webbrowser\n\n        Other arguments passed to webbrowser.open\n        \"\"\"", "output": "\n        webbrowser.open(self.chart_url, *args, **kwargs)\n\n    def render(self, filename, **kwargs):\n        \"\"\"\n        Renders the chart as a PNG image\n\n        Other arguments passed to PIL.Image.save\n        \"\"\"\n        im = PIL.Image.frombuffer(\"P\", (self.width, self.height),\n                                 self.chart.render(), \"raw\", \"RGB\", 0, 1)\n        im.save(filename, **kwargs)\n\n    def render_to_file(self, filename, **kwargs):\n        \"\"\"\n        Renders the chart as a PNG image\n\n        Other arguments passed to PIL.Image.save\n        \"\"\"\n        im = PIL.Image.frombuffer(\"P\", (self.width, self.height),\n                                 self.chart.render(), \"raw\", \"RGB\", 0, 1)\n        im.save(filename, **kwargs)\n\n    def render_to_", "solution": "\n        from webbrowser import open as webopen\n        return webopen(str(self), *args, **kwargs)"}
{"prompt": "def getView(self,name):\n        \"\"\"\n        Returns the view with name ``name``\\ .\n        \n        Raises a :py:exc:`ValueError` if the view does not exist.\n        \"\"\"", "output": "\n        \n        if name in self.views:\n            return self.views[name]\n        else:\n            raise ValueError(\"View %s does not exist\" % name)\n        \n    def getViews(self,names):\n        \"\"\"\n        Returns a list of views with names ``names``\\.\n        \n        Raises a :py:exc:`ValueError` if the view does not exist.\n        \"\"\"\n        \n        views = []\n        for name in names:\n            view = self.getView(name)\n            if view is not None:\n                views.append(view)\n            else:\n                raise ValueError(\"View %s does not exist\" % name)\n        return views\n        \n    def registerView(self,view,name):\n        \"\"\"\n        Registers a view with name ``name``\\.\n        \n        Raises a :py:exc", "solution": "\n        if name not in self.views:\n            raise ValueError(\"Unknown world view\")\n        return self.views[name]"}
{"prompt": "def _process_execute_error(self, msg):\n        \"\"\" Process a reply for an execution request that resulted in an error.\n        \"\"\"", "output": "\n        LOG.error(_LE(\"Reply from executor for command '%(command)s' \"\n                      \"with error '%(error)s'\"),\n                  {'command': msg['command'], 'error': msg['error']})\n        error = msg['error']\n        error_code = error['error_code']\n        error_text = error['error_text']\n        error_traceback = error['traceback']\n        LOG.error(_LE(\"Error from executor for command '%(command)s': \"\n                      \"%(error)s\"),\n                  {'command': msg['command'], 'error': error_text})\n        LOG.error(_LE(\"Error from executor for command '%(command)", "solution": "\n        content = msg['content']\n        # If a SystemExit is passed along, this means exit() was called - also\n        # all the ipython %exit magic syntax of '-k' to be used to keep\n        # the kernel running\n        if content['ename']=='SystemExit':\n            keepkernel = content['evalue']=='-k' or content['evalue']=='True'\n            self._keep_kernel_on_exit = keepkernel\n            self.exit_requested.emit(self)\n        else:\n            traceback = ''.join(content['traceback'])\n            self._append_plain_text(traceback)"}
{"prompt": "def s_add(self, path, function, method=None, type_cast=None):\n        \"\"\"Function for registering a simple path.\n\n        Args:\n            path (str): Path to be matched.\n            function (function): Function to associate with this path.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DELETE. You may use whatever fits your situation though.\n                Defaults to None.\n            type_cast (dict, optional): Mapping between the param name and\n                one of `int`, `float` or `bool`. The value reflected by the\n                provided param name will than be casted to the given type.\n                Defaults to None.\n        \"\"\"", "output": "\n        if method is None:\n            method = self.method\n        if type_cast is None:\n            type_cast = self.type_cast\n        path_parts = [path]\n        if method not in [\"GET\", \"POST\", \"PUT\", \"DELETE\"]:\n            path_parts.append(method)\n        path_parts.append(type_cast.get(path, str))\n        self.paths.append(path_parts)\n        self.functions.append(function)\n\n    def match(self, path, method=None, type_cast=None):\n        \"\"\"Match a path against the registered paths.\n\n        Args:\n            path (str): Path to be matched.\n            method (str, optional): Usually used to define one of GET, POST,\n                PUT, DELETE. You may use whatever fits your situation though.\n", "solution": "\n        with self._lock:\n            try:\n                path = '^/{}'.format(path.lstrip('/'))\n                path = '{}/$'.format(path.rstrip('/'))\n                path = path.replace('<', '(?P<')\n                path = path.replace('>', '>[^/]*)')\n\n                self.add(path, function, method, type_cast)\n            except Exception:\n                pass"}
{"prompt": "def login(request, template_name='ci/login.html',\n          redirect_field_name=REDIRECT_FIELD_NAME,\n          authentication_form=AuthenticationForm):\n    \"\"\"\n    Displays the login form and handles the login action.\n    \"\"\"", "output": "\n    redirect_to = request.REQUEST.get(redirect_field_name, '')\n    if request.method == 'POST':\n        form = authentication_form(data=request.POST)\n        if form.is_valid():\n            user = form.save()\n            login(request, user)\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = authentication_form()\n    return render(request, template_name, {\n        'form': form,\n        'title': _('Log in'),\n    })\n\n\ndef logout(request, next_page=None):\n    \"\"\"\n    Logs out the user and displays 'You are logged out' message.\n    \"\"\"\n    auth.logout(request)\n    messages.success(request, _('You are logged out.'))\n    return HttpResponse", "solution": "\n    redirect_to = request.POST.get(redirect_field_name,\n                                   request.GET.get(redirect_field_name, ''))\n\n    if request.method == \"POST\":\n        form = authentication_form(request, data=request.POST)\n        if form.is_valid():\n\n            # Ensure the user-originating redirection url is safe.\n            if not is_safe_url(url=redirect_to, host=request.get_host()):\n                redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n            # Okay, security check complete. Get the user object from auth api.\n            user = form.get_user()\n            request.session['user_token'] = user[\"token\"]\n            request.session['user_email'] = user[\"email\"]\n            request.session['user_permissions'] = user[\"permissions\"]\n            request.session['user_id'] = user[\"id\"]\n            request.session['user_list'] = user[\"user_list\"]\n\n            if not settings.HIDE_DASHBOARDS:\n                # Set user dashboards because they are slow to change\n                dashboards = ciApi.get_user_dashboards(user[\"id\"])\n                dashboard_list = list(dashboards['results'])\n                if len(dashboard_list) > 0:\n                    request.session['user_dashboards'] = \\\n                        dashboard_list[0][\"dashboards\"]\n                    request.session['user_default_dashboard'] = \\\n                        dashboard_list[0][\"default_dashboard\"][\"id\"]\n                else:\n                    request.session['user_dashboards'] = []\n                    request.session['user_default_dashboard'] = None\n\n            # Get the user access tokens too and format for easy access\n            tokens = ciApi.get_user_service_tokens(\n                params={\"user_id\": user[\"id\"]})\n            token_list = list(tokens['results'])\n            user_tokens = {}\n            if len(token_list) > 0:\n                for token in token_list:\n                    user_tokens[token[\"service\"][\"name\"]] = {\n                        \"token\": token[\"token\"],\n                        \"url\": token[\"service\"][\"url\"] + \"/api/v1\"\n                    }\n            request.session['user_tokens'] = user_tokens\n\n            return HttpResponseRedirect(redirect_to)\n    else:\n        form = authentication_form(request)\n\n    current_site = get_current_site(request)\n\n    context = {\n        'form': form,\n        redirect_field_name: redirect_to,\n        'site': current_site,\n        'site_name': current_site.name,\n    }\n\n    return TemplateResponse(request, template_name, context)"}
{"prompt": "def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"", "output": "\n        task = IOWriteTask(fileobj, data, offset)\n        self.io_write_tasks.append(task)\n        return task\n\n    def get_io_read_task(self, fileobj, offset):\n        \"\"\"Get an IO read task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to read from\n\n        :type offset: integer\n        :param offset: The offset to read the data from in the file-like object\n\n        :returns: An IO task to be used to read data from the file-like object\n        \"\"\"\n        task = IOReadTask(fileobj, offset)\n        self.io_read_tasks.append(task)\n        return task\n\n    def submit(self, func, *args, **kwargs):\n        \"\"\"Sub", "solution": "\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            }\n        )"}
{"prompt": "def __notify_listeners(self, data, stats):\n        \"\"\" notify all listeners about aggregate data and stats \"\"\"", "output": "\n        for listener in self.listeners:\n            listener.aggregate(data, stats)\n\n    def notify(self, event, *args, **kwargs):\n        \"\"\" notify all listeners about event \"\"\"\n        for listener in self.listeners:\n            listener.notify(event, *args, **kwargs)\n\n    def notifyAll(self, event, *args, **kwargs):\n        \"\"\" notify all listeners about event \"\"\"\n        for listener in self.listeners:\n            listener.notifyAll(event, *args, **kwargs)\n\n    def notifyListeners(self, event, *args, **kwargs):\n        \"\"\" notify all listeners about event \"\"\"\n        for listener in self.listeners:\n            listener.notifyListeners(event, *args, **kwargs)\n\n    def notifyListenersAll(self, event, *args, **kwargs):\n        \"\"\" notify all listeners about event \"\"\"\n        for listener in self.listeners:\n            ", "solution": "\n        for listener in self.listeners:\n            listener.on_aggregated_data(data, stats)"}
{"prompt": "def utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"", "output": "\n    return unicode_str.encode('utf-8')\n\n\ndef to_utf8(unicode_str):\n    \"\"\"\n    Return a utf-8 encoded string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"\n    return unicode_str.encode('utf-8')\n\n\ndef to_str(unicode_str):\n    \"\"\"\n    Return a string from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"\n    return unicode_str.decode('utf-8')\n\n\ndef to_bytes(unicode_str):\n    \"\"\"\n    Return a bytes object from a valid unicode string.\n\n    :param unicode_str: Unicode string.\n\n    :return: str\n    \"\"\"\n    return unicode_str.encode('utf-8')\n\n\ndef to_bytes(unicode_str):\n    \"\"\"\n    Return a bytes object from a valid unicode string.\n\n    :param ", "solution": "\n    if six.PY2 and isinstance(unicode_str, __unicode__):\n        return unicode_str.encode('utf-8')\n\n    return unicode_str"}
{"prompt": "def csoftmax_for_slice(input):\n    \"\"\" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n    Returns:\n        output: A list of [csoftmax results, masks]\n    \"\"\"", "output": "\n    output, masks = [], []\n    for input, mask in zip(input,masks):\n        output, mask = _softmax_helper(input, mask, True)\n        output.append(output)\n    return output, masks\n\n\ndef csoftmax_for_slice(input, mask):\n    \"\"\" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n        mask: A list of [mask tensor, cumulative ", "solution": "\n\n    [ten, u] = input\n\n    shape_t = ten.shape\n    shape_u = u.shape\n\n    ten -= tf.reduce_mean(ten)\n    q = tf.exp(ten)\n    active = tf.ones_like(u, dtype=tf.int32)\n    mass = tf.constant(0, dtype=tf.float32)\n    found = tf.constant(True, dtype=tf.bool)\n\n    def loop(q_, mask, mass_, found_):\n        q_list = tf.dynamic_partition(q_, mask, 2)\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,\n        #  1 element if true\n\n        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])\n        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])\n\n        # condition verification and mask modification\n        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,\n                                                 2)  # 0 when u is bigger than p, 1 when u is less than p\n\n        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)\n        split_u = tf.dynamic_partition(u, less_mask, 2)\n\n        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])\n        mass_ += tf.reduce_sum(split_u[1])\n\n        mask = mask * (tf.ones_like(less_mask) - less_mask)\n\n        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),\n                         lambda: False,\n                         lambda: True)\n\n        alpha = tf.reshape(alpha, q_.shape)\n\n        return alpha, mask, mass_, found_\n\n    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,\n                                         body=loop,\n                                         loop_vars=(q, active, mass, found))\n\n    return [csoft, mask_]"}
{"prompt": "def lazy(function, *args, **kwargs):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n        args: Positional arguments, passed directly to ``function``.\n\n        kwargs: Keyword arguments, passed directly to ``function``.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function`` with\n            the specified arguments.\n\n    \"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        return function(*args, **kwargs)\n    return wrapper\n\n\ndef lazy(function):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function`` with\n            the specified arguments.\n\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        return function(*args, **kwargs)\n    return wrapper\n\n\ndef lazy(function):\n    \"\"\" Produces a callable so that functions can be lazily evaluated in\n    templates.\n\n    Arguments:\n\n        function (callable): The function to call at evaluation time.\n\n    Return:\n\n        callable: A callable that will evaluate a call to ``function", "solution": "\n\n    NOT_EVALUATED = object()\n    retval = [NOT_EVALUATED]\n\n    def evaluate():\n        if retval[0] is NOT_EVALUATED:\n            retval[0] = function(*args, **kwargs)\n        return retval[0]\n\n    return evaluate"}
{"prompt": "def maybe_broadcast_structure(from_structure: Any, to_structure: Any) -> Any:\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"", "output": "\n  if isinstance(from_structure, collections.abc.Sequence):\n    from_structure = collections.abc.cast(\n        collections.abc.Sequence, from_structure)\n  if isinstance(to_structure, collections.abc.Sequence):\n    to_structure = collections.abc.cast(\n        collections.abc.Sequence, to_structure)\n  if isinstance(from_structure, collections.abc.Mapping):\n    from_structure = collections.abc.cast(\n        collections.abc.Mapping, from_structure)\n  if isinstance(to_structure, collections.abc.Mapping):\n    to_structure = collections.abc.cast(\n        collections.abc.Mapping, to_structure)\n  if isinstance(from_structure, collections.abc.Sequence) and isinstance(\n      to_structure, collections.abc.Sequence):", "solution": "\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)"}
{"prompt": "def main():\n    \"\"\"Initialize and run command line interface.\"\"\"", "output": "\n    logging.basicConfig(level=logging.INFO)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('boto3').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('boto3').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('boto3').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.CRITICAL)\n    logging.getLogger('botocore').setLevel(logging.", "solution": "\n    # Create and populate parser\n    parser = create_parser()\n\n    # Parse given arguments\n    args = parser.parse_args()\n\n    # Checking arguments\n    check_arguments(args, parser)\n\n    # BUSINESS LOGIC IS FOLLOWING\n    run(parser, args)"}
{"prompt": "def from_tuplelist(tuple_list):\n        \"\"\"\n        Populates a Layout from a list containing virtual\n        qubits---(QuantumRegister, int) tuples---, or None.\n\n        Args:\n            tuple_list (list):\n                e.g.: [qr[0], None, qr[2], qr[3]]\n        Returns:\n            Layout: the corresponding Layout object\n        Raises:\n            LayoutError: If the elements are not (Register, integer) or None\n        \"\"\"", "output": "\n        layout = None\n        for qureg, idx in tuple_list:\n            if qureg is None:\n                raise LayoutError(\"The qubits cannot be None\")\n            if idx is None:\n                raise LayoutError(\"The index cannot be None\")\n            if idx < 0 or idx >= len(qureg):\n                raise LayoutError(\"The index must be between 0 and {}\".format(len(qureg)))\n            layout = Layout(qureg, idx)\n        return layout\n\n    def copy(self):\n        \"\"\"\n        Copies the Layout.\n\n        Returns:\n            Layout: the copied Layout\n        \"\"\"\n        return Layout(self.qureg, self.idx)\n\n    def copy_to_list(self):\n        \"\"\"\n        Copies the Layout to a list.\n\n        Returns:\n            list: the copied list\n", "solution": "\n        out = Layout()\n        for physical, virtual in enumerate(tuple_list):\n            if virtual is None:\n                continue\n            elif Layout.is_virtual(virtual):\n                if virtual in out._v2p:\n                    raise LayoutError('Duplicate values not permitted; Layout is bijective.')\n                out[virtual] = physical\n            else:\n                raise LayoutError(\"The list should contain elements of the form\"\n                                  \" (Register, integer) or None\")\n        return out"}
{"prompt": "def reject_connection(self, reason=None):\n        \"\"\"\n        Reject a pending connection.\n        \"\"\"", "output": "\n        if self.connection:\n            self.connection.close()\n            self.connection = None\n        if reason:\n            self._reject_connection(reason)\n\n    def connectionLost(self, reason):\n        \"\"\"\n        Called when the connection was lost as a result of a\n        disconnect.\n        \"\"\"\n        self._reject_connection(reason)\n\n    def dataReceived(self, data):\n        \"\"\"\n        Override this method to handle received data.\n        \"\"\"\n        pass\n\n    def connectionClosed(self, reason):\n        \"\"\"\n        Called when the connection was closed as a result of a\n        disconnect.\n        \"\"\"\n        self._reject_connection(reason)\n\n    def connectionRefused(self):\n        \"\"\"\n        Called when the connection was refused ", "solution": "\n        assert self.pending, \"Connection is not pending.\"\n        if reason is None:\n            reason = ConnectionRefusedError()\n        self._accept_d.errback(reason)"}
{"prompt": "def signature(frame):\n    \"\"\"return suitable frame signature to key display expressions off of.\"\"\"", "output": "\n    sig = frame.f_code.co_varnames[:frame.f_code.co_argcount]\n    sig = sig + ['%s=%s' % (a, frame.f_locals[a]) for a in frame.f_code.co_varnames[frame.f_code.co_argcount:]\n                 if a in frame.f_locals]\n    return ', '.join(sig)\n\n\ndef format_stack_trace(trace, depth=0, max_depth=None):\n    \"\"\"Return a formatted stack trace as a string.\"\"\"\n    stack = traceback.extract_stack(trace)\n    stack = stack[depth:]\n    if max_depth is not None and len(stack) > max_depth:\n        stack = stack[:max_depth]\n    stack = map(format_stack_frame, stack", "solution": "\n    if not frame: return None\n    code = frame.f_code\n    return (code.co_name, code.co_filename, code.co_firstlineno)"}
{"prompt": "async def parse_tag_results(soup):\n    \"\"\"\n    Parse a page of tag or trait results. Same format.\n\n    :param soup: BS4 Class Object\n    :return: A list of tags, Nothing else really useful there\n    \"\"\"", "output": "\n    tags = []\n    tag_results = soup.find_all(\"div\", class_=\"tag-result\")\n    for tag_result in tag_results:\n        tag = Tag()\n        tag.name = tag_result.find(\"div\", class_=\"tag-name\").text\n        tag.description = tag_result.find(\"div\", class_=\"tag-description\").text\n        tag.url = tag_result.find(\"div\", class_=\"tag-url\").text\n        tag.tag_id = tag_result.find(\"div\", class_=\"tag-id\").text\n        tag.tag_type = tag_result.find(\"div\", class_=\"tag-type\").text\n        tag.tag_level = tag_result.find(\"div\", class_=\"tag-level\").text\n        ", "solution": "\n    soup = soup.find_all('td', class_='tc3')\n    tags = []\n    for item in soup:\n        tags.append(item.a.string)\n    return tags"}
{"prompt": "def mean(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the mean for expression, possibly on a grid defined by binby.\n\n        Example:\n\n        >>> df.mean(\"x\")\n        -0.067131491264005971\n        >>> df.mean(\"(x**2+y**2)**0.5\", binby=\"E\", shape=4)\n        array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}\n        \"\"\"", "output": "selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :param edges: {edges}\n        :return: {expression}\n        \"\"\"\n        binby = binby.split(\",\")\n        binby = [b.strip() for b in binby]\n        binby = binby[::-1]\n        binby = binby[:len(binby)//2]\n        binby = binby[::-1]\n        binby = binby[:len(binby)//2]\n        binby = binby[::-1]\n        binby = binby[:len(binby)//2]\n        binby = binby[::-1]\n        binby = ", "solution": "\n        return self._compute_agg('mean', expression, binby, limits, shape, selection, delay, edges, progress)\n        logger.debug(\"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\", expression, binby, limits, shape, selection, delay)\n        expression = _ensure_strings_from_expressions(expression)\n        selection = _ensure_strings_from_expressions(selection)\n        binby = _ensure_strings_from_expressions(binby)\n\n        @delayed\n        def calculate(expression, limits):\n            task = tasks.TaskStatistic(self, binby, shape, limits, weight=expression, op=tasks.OP_ADD_WEIGHT_MOMENTS_01, selection=selection)\n            self.executor.schedule(task)\n            progressbar.add_task(task, \"mean for %s\" % expression)\n            return task\n\n        @delayed\n        def finish(*stats_args):\n            stats = np.array(stats_args)\n            counts = stats[..., 0]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                mean = stats[..., 1] / counts\n            return vaex.utils.unlistify(waslist, mean)\n        waslist, [expressions, ] = vaex.utils.listify(expression)\n        progressbar = vaex.utils.progressbars(progress)\n        limits = self.limits(binby, limits, delay=True)\n        stats = [calculate(expression, limits) for expression in expressions]\n        var = finish(*stats)\n        return self._delay(delay, var)"}
{"prompt": "def build_fake_data(num_examples=10):\n  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"", "output": "\n  images = np.random.rand(num_examples, 784)\n  labels = np.random.randint(0, 10, num_examples)\n  return images, labels\n\n\nclass DataSet(object):\n  \"\"\"A generic data set class meant to represent a minibatch of data.\n\n  Note that this class doesn't do any I/O or conversion to other common\n  formats.\n\n  TODO(touts): Add documentation\n  \"\"\"\n\n  def __init__(self, images, labels, fake_data=False):\n    \"\"\"Constructs a DataSet.\n\n    Args:\n      images: Images. Images could be strings or file objects.\n      labels: Labels. Labels could be strings or file objects.\n      fake_data: Bool. Whether to include fake images in the set.\n    \"\"\"\n    self._num_example", "solution": "\n\n  class Dummy(object):\n    pass\n\n  num_examples = 10\n  mnist_data = Dummy()\n  mnist_data.train = Dummy()\n  mnist_data.train.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.train.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.train.num_examples = num_examples\n  mnist_data.validation = Dummy()\n  mnist_data.validation.images = np.float32(np.random.randn(\n      num_examples, *IMAGE_SHAPE))\n  mnist_data.validation.labels = np.int32(np.random.permutation(\n      np.arange(num_examples)))\n  mnist_data.validation.num_examples = num_examples\n  return mnist_data"}
{"prompt": "def min(self, expression, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None, edges=False):\n        \"\"\"Calculate the minimum for given expressions, possibly on a grid defined by binby.\n\n\n        Example:\n\n        >>> df.min(\"x\")\n        array(-128.293991)\n        >>> df.min([\"x\", \"y\"])\n        array([-128.293991 ,  -71.5523682])\n        >>> df.min(\"x\", binby=\"x\", shape=5, limits=[-10, 10])\n        array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])\n\n        :param expression: {expression}\n        :param binby: {binby}\n        :param limits: {limits}\n        :param shape: {shape}\n        :param selection: {selection}\n        :param delay: {delay}\n        :param progress: {progress}\n        :return: {return_stat_scalar}, the last dimension is of shape (2)\n        \"\"\"", "output": "expression: Expression to evaluate\n        :param binby: Bin by expression\n        :param limits: Limits to evaluate on\n        :param shape: Shape of the grid\n        :param selection: Whether to select bins\n        :param delay: Whether to delay evaluation of bins\n        :param progress: Progress callback\n        :param edges: Whether to return edges (default False)\n        :return: Minimum value\n        \"\"\"\n        binby = binby if isinstance(binby, str) else [binby]\n        binby = binby[0] if isinstance(binby, str) else binby\n        binby = binby.split(\",\") if isinstance(binby, str) else binby\n        binby = bin", "solution": "\n        return self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress)\n        @delayed\n        def finish(result):\n            return result[..., 0]\n        return self._delay(delay, finish(self.minmax(expression, binby=binby, limits=limits, shape=shape, selection=selection, delay=delay, progress=progress)))"}
{"prompt": "def _create_value(self, *args, **kwargs):\n        \"\"\"\n        Lowest value generator.\n\n        Separated from __call__, because it seems that python\n        cache __call__ reference on module import\n        \"\"\"", "output": "\n        func = self.func\n        args, kwargs = args or (), kwargs or {}\n        cache = self.cache\n        cache_key = func.__name__\n        if cache_key in cache:\n            return cache[cache_key]\n        cache[cache_key] = result = func(*args, **kwargs)\n        return result\n\n    def _call_with_partial(self, *args, **kwargs):\n        \"\"\"\n        Call function with partial arguments.\n\n        Used for partial application and partial function application\n        \"\"\"\n        func = self.func\n        args, kwargs = args or (), kwargs or {}\n        cache = self.cache\n        cache_key = func.__name__\n        if cache_key in cache:\n            return cache[cache_key]\n        cache[cache_key] = result = func(*args, **kwargs)\n        partial = functools.partial\n        partial_cache = partial", "solution": "\n        if not len(args):\n            raise TypeError('Object instance is not provided')\n\n        if self.by_instance:\n            field_type = args[0]\n        else:\n            field_type = args[0].__class__\n\n        function = self.registry.get(field_type, self.default)\n\n        if function is None:\n            raise TypeError(\"no match %s\" % field_type)\n\n        return function(*args, **kwargs)"}
{"prompt": "def join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"", "output": "\n    line = next(iterator)\n    previous = next(iterator)\n    yield line\n    yield previous\n\n\ndef join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"\n    line = next(iterator)\n    previous = next(iterator)\n    yield line\n    yield previous\n\n\ndef join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"\n    line = next(iterator)\n    previous = next(iterator)\n    yield line\n    yield previous\n\n\ndef join_lines(iterator):\n    \"\"\"\n    Joins a line ending in '\\' with the previous line.\n    \"\"\"\n    line = next(iterator", "solution": "\n    lines = []\n    for line in iterator:\n        if not line.endswith('\\\\'):\n            if lines:\n                lines.append(line)\n                yield ''.join(lines)\n                lines = []\n            else:\n                yield line\n        else:\n            lines.append(line.strip('\\\\'))"}
{"prompt": "def handle(self):\n        \"\"\"\n        Handle the backend of the given status.\n        \"\"\"", "output": "\n        status = self.status\n        backend = self.backend\n        backend = backend.get_backend(status)\n        if backend is None:\n            self.status = status\n            self.backend = None\n            return\n        try:\n            backend.execute()\n        except BackendError as e:\n            self.status = status\n            self.backend = backend\n            self.error = str(e)\n            self.save()\n        except CommandExecutionError as e:\n            self.status = status\n            self.backend = backend\n            self.error = str(e)\n            self.save()\n\n    def refresh(self):\n        \"\"\"\n        Refresh the backend of the given status.\n        \"\"\"\n        status = self.status\n        backend = self.backend\n        backend = backend.get_backend(status)\n        if backend is None:\n            return\n        try:\n            ", "solution": "\n\n        # We initiate the source we are going to parse to the Generate class.\n        source = \"URL\"\n\n        if self.catched.lower() not in PyFunceble.STATUS[\"list\"][\"invalid\"]:\n            # The parsed status is not in the list of invalid.\n\n            # We generate the status file with the catched status.\n            Generate(self.catched, source).status_file()\n        else:\n            # The parsed status is in the list of invalid.\n\n            # We generate the status file with the parsed status.\n            Generate(self.catched, \"SYNTAX\").status_file()\n\n        # We return the parsed status.\n        return self.catched"}
{"prompt": "def _segmentPath(self, path):\n        \"\"\"Return list of valid *path* segments.\"\"\"", "output": "\n        segments = []\n        for seg in self._segments:\n            if seg not in path:\n                segments.append(seg)\n        return segments\n\n    def _pathToSegments(self, path):\n        \"\"\"Return list of segments formed by joining path elements.\"\"\"\n        segments = []\n        for seg in self._segments:\n            segments.append(path + seg)\n        return segments\n\n    def _pathToSegmentsNoJoin(self, path, join):\n        \"\"\"Return list of segments formed by joining path elements.\"\"\"\n        segments = []\n        for seg in self._segments:\n            segments.append(seg)\n            if seg!= join:\n                segments.append(path + seg)\n        return segments\n\n    def _pathToSegments", "solution": "\n        parts = []\n        model = self._filesystemWidget.model()\n\n        # Separate root path from remainder.\n        remainder = path\n\n        while True:\n            if remainder == model.root.path:\n                break\n\n            if remainder:\n                parts.append(remainder)\n\n            head, tail = os.path.split(remainder)\n            if head == remainder:\n                break\n\n            remainder = head\n\n        parts.append(model.root.path)\n        return parts"}
{"prompt": "def p_gate_op_5(self, program):\n        \"\"\"\n        gate_op : BARRIER id_list ';'\n        \"\"\"", "output": "\n        program.add_gate('barrier', program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list + program.get_gate('barrier').id_list ", "solution": "\n        program[0] = node.Barrier([program[2]])\n        self.verify_bit_list(program[2])\n        self.verify_distinct([program[2]])"}
{"prompt": "def luhn_check(card_number):\n    \"\"\" checks to make sure that the card passes a luhn mod-10 checksum \"\"\"", "output": "\n    card_number = str(card_number)\n    card_number = card_number.replace(\"-\", \"\")\n    card_number = card_number.replace(\"(\", \"\")\n    card_number = card_number.replace(\")\", \"\")\n    card_number = card_number.replace(\",\", \"\")\n    card_number = card_number.replace(\"0\", \"\")\n    card_number = card_number.replace(\"X\", \"\")\n    card_number = card_number.replace(\"1\", \"\")\n    card_number = card_number.replace(\"2\", \"\")\n    card_number = card_number.replace(\"3\", \"\")\n    card_number = card_number.replace(\"4\", \"\")\n    card_number = card_number.replace(\"5\", \"\")\n    card_number = card_number.replace(\"6\", \"\")\n    card_number = card_number.replace(\"7\", \"\")\n    card_", "solution": "\n    sum = 0\n    num_digits = len(card_number)\n    oddeven = num_digits & 1\n\n    for count in range(0, num_digits):\n        digit = int(card_number[count])\n\n        if not ((count & 1) ^ oddeven):\n            digit *= 2\n        if digit > 9:\n            digit -= 9\n\n        sum += digit\n\n    return (sum % 10) == 0"}
{"prompt": "def _make_empty_queue_for(k, element):\n  \"\"\"Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.\n\n  For example:\n\n  ```python\n    element = tf.constant([[0., 1., 2., 3., 4.],\n                           [5., 6., 7., 8., 9.]])\n\n    # A queue capable of holding 3 elements.\n    _make_empty_queue_for(3, element)\n    # => [[[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]],\n    #\n    #     [[ 0.,  0.,  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: A positive scalar integer, number of elements that each queue will hold.\n    element: A `tf.Tensor`, only its shape and dtype information are relevant.\n\n  Returns:\n    A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype\n    as `element`.\n  \"\"\"", "output": ",  0.,  0.,  0.],\n    #      [ 0.,  0.,  0.,  0.,  0.]]]\n  ```\n\n  Args:\n    k: Number of elements to hold in the queue.\n    element: Tensor to hold in the queue.\n\n  Returns:\n    A `tf.Tensor` suitable to hold `k` element-shaped tensors.\n  \"\"\"\n  shape = tf.shape(element)\n  batch_size = shape[0]\n  capacity = k * batch_size\n  dtype = tf.as_dtype(element.dtype)\n  queue = tf.FIFOQueue(capacity, dtype, shape[1:-1])\n  queue.enqueue(element)\n  return queue\n\n\ndef _make_queue_for_shard(queue):\n  \"\"\"Creates a `tf.QueueBase` suitable to hold ", "solution": "\n  queue_shape = tf.concat(\n      [[k], distribution_util.prefer_static_shape(element)], axis=0)\n  return tf.zeros(queue_shape, dtype=element.dtype.base_dtype)"}
{"prompt": "def join(self):\n        \"\"\"Waits until all there are no processes enqueued.\"\"\"", "output": "\n        self._ensure_started()\n        procs = self.queue.get(self._wait_timeout)\n        if procs is None:\n            return\n        for p in procs:\n            p.join()\n        self.queue.task_done()\n\n    def joinall(self):\n        \"\"\"Waits until all processes exit (including children).\"\"\"\n        self._ensure_started()\n        procs = self.queue.get(self._wait_timeout)\n        if procs is None:\n            return\n        for p in procs:\n            p.join()\n        self.queue.task_done()\n\n    def terminate(self):\n        \"\"\"Terminates the process and all sub-processes.\"\"\"\n        self._ensure_started()\n        self.queue.put((None, None))\n        self.join()\n\n    def wait(self):\n        \"\"\"Wait until all sub-processes have ex", "solution": "\n        while True:\n            with self._processes_lock:\n                if not self._processes and self._processing_semaphore.locked():\n                    return True\n            gevent.sleep(0.1)"}
{"prompt": "def phase_offsets(Idat,Qdat,Udat,Vdat,tsamp,chan_per_coarse,feedtype='l',**kwargs):\n    \"\"\"\n    Calculates phase difference between X and Y feeds given U and V (U and Q for circular basis)\n    data from a noise diode measurement on the target\n    \"\"\"", "output": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "solution": "\n    #Fold noise diode data and calculate ON OFF diferences for U and V\n    if feedtype=='l':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        V_OFF,V_ON = foldcal(Vdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Vdiff = V_ON-V_OFF\n        poffset = np.arctan2(-1*Vdiff,Udiff)\n\n    if feedtype=='c':\n        U_OFF,U_ON = foldcal(Udat,tsamp,**kwargs)\n        Q_OFF,Q_ON = foldcal(Qdat,tsamp,**kwargs)\n        Udiff = U_ON-U_OFF\n        Qdiff = Q_ON-Q_OFF\n        poffset = np.arctan2(Udiff,Qdiff)\n\n    coarse_p =  convert_to_coarse(poffset,chan_per_coarse)\n\n    #Correct for problems created by discontinuity in arctan\n    #Find whether phase offsets have increasing or decreasing slope\n    y = coarse_p[:6]\n    x = np.arange(y.size)\n    m = np.polyfit(x,y,1)[0]\n\n    for i in range(coarse_p.size-3):\n        if (m>0 and coarse_p[i+1]<coarse_p[i]) or (m<0 and coarse_p[i+1]>coarse_p[i]):\n            coarse_p[i+1] = 2*coarse_p[i+2]-coarse_p[i+3]    #Move problem point near the next\n\n    return coarse_p"}
{"prompt": "def filter_on(self, filt=None, analyte=None, samples=None, subset=None, show_status=False):\n        \"\"\"\n        Turns data filters on for particular analytes and samples.\n\n        Parameters\n        ----------\n        filt : optional, str or array_like\n            Name, partial name or list of names of filters. Supports\n            partial matching. i.e. if 'cluster' is specified, all\n            filters with 'cluster' in the name are activated.\n            Defaults to all filters.\n        analyte : optional, str or array_like\n            Name or list of names of analytes. Defaults to all analytes.\n        samples : optional, array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n\n        Returns\n        -------\n        None\n        \"\"\"", "output": "all\n            samples.\n        subset : optional, array_like or None\n            Which samples to apply this filter to. If None, applies to all\n            samples.\n        show_status : optional, bool\n            Show status of filter and analyte.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if filt is None:\n            filt = self.filters\n        if analyte is None:\n            analyte = self.analytes\n        if samples is None:\n            samples = self.samples\n        if subset is None:\n            subset = self.subset\n        if show_status is True:\n            self.show_status()\n        if show_status is False:\n            self.show_status()\n        if show_status is True:\n            self.show_status()\n", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            try:\n                self.data[s].filt.on(analyte, filt)\n            except:\n                warnings.warn(\"filt.on failure in sample \" + s)\n\n        if show_status:\n            self.filter_status(subset=subset)\n        return"}
{"prompt": "def _add_https(self, q):\n        \"\"\"for push, pull, and other api interactions, the user can optionally\n           define a custom registry. If the registry name doesn't include http\n           or https, add it.\n \n           Parameters\n           ==========\n           q: the parsed image query (names), including the original\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n\n        # If image uses http or https, add back\n        if not q['registry'].startswith('http'):\n\n            if q['original'].startswith('http:'):\n                q['registry'] = 'http://%s' % q['registry']\n\n            elif q['original'].startswith('https:'):\n                q['registry'] = 'https://%s' % q['registry']\n\n            # Otherwise, guess from the user's environment\n            else:\n\n                prefix = 'https://'\n\n                # The user can set an environment variable to specify nohttps\n                nohttps = os.environ.get('SREGISTRY_REGISTRY_NOHTTPS')\n                if nohttps != None:\n                    prefix = 'http://'\n                q['registry'] = '%s%s' %(prefix, q['registry'])\n\n        return q"}
{"prompt": "def clean_tempfiles():\n  \"\"\"Clean up temp files\"\"\"", "output": "\n  try:\n    shutil.rmtree(tempdir)\n  except OSError:\n    pass\n\n\ndef setup_env():\n  \"\"\"Setup the environment for running this script\"\"\"\n  global tempdir\n  tempdir = tempfile.mkdtemp()\n  atexit.register(clean_tempfiles)\n  setup_logging()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n  setup_db()\n", "solution": "\n  for fn in TEMP_FILES:\n    if os.path.exists(fn):\n      os.unlink(fn)"}
{"prompt": "def get_section_by_sis_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return section resource for given sis id.\n        \"\"\"", "output": "\n        return self._get('/sis/%s/section' % sis_section_id, params=params)\n\n    def get_section_by_name(self, sis_section_name, params={}):\n        \"\"\"\n        Return section resource for given section name.\n        \"\"\"\n        return self._get('/sis/%s/section/%s' % (sis_section_name, sis_section_name), params=params)\n\n    def get_section_by_id(self, sis_section_id, params={}):\n        \"\"\"\n        Return section resource for given section id.\n        \"\"\"\n        return self._get('/sis/%s/section/%s' % (sis_section_id, sis_section_id), params=params)\n\n    def get_sections(self, params={}):\n        \"\"\"\n        Return sections list.\n        \"\"\"\n        return self._get('/sis/sections', ", "solution": "\n        return self.get_section(\n            self._sis_id(sis_section_id, sis_field=\"section\"), params)"}
{"prompt": "async def get_tracks(self, *, limit=20, offset=0) -> List[Track]:\n        \"\"\"Get a list of the songs saved in the current Spotify user\u2019s \u2018Your Music\u2019 library.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optional[int]\n            The index of the first item to return. Default: 0\n        \"\"\"", "output": "\n        uri = \"https://api.spotify.com/v1/me/tracks?limit={}&offset={}\".format(limit, offset)\n        async with self.session.get(uri) as response:\n            return await response.json()\n\n    async def get_track(self, track_id: str) -> Track:\n        \"\"\"Get the track with the given track ID.\n\n        Parameters\n        ----------\n        track_id : str\n            The track ID to retrieve.\n        \"\"\"\n        uri = \"https://api.spotify.com/v1/tracks/{}\".format(track_id)\n        async with self.session.get(uri) as response:\n            return await response.json()\n\n    async def get_artists(self, *, limit=20, offset=0) -> List[Artist]:\n        \"\"\"Get a list of the artists saved in the current Spotify user\u2019s \u2018Your ", "solution": "\n        data = await self.user.http.saved_tracks(limit=limit, offset=offset)\n\n        return [Track(self.__client, item['track']) for item in data['items']]"}
{"prompt": "def get_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be read.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"", "output": "\n        return self._get_buffer_element()\n\n    def write_direct(self, val):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be written.\n\n        :param val: The value to write to the buffer.\n        \"\"\"\n        self._write_buffer_element(val)\n\n    def write_directs(self, vals):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be written.\n\n        :param vals: The values to write to the buffer.\n        \"\"\"\n        self._write_buffer_element(vals)\n\n    def write_directs_blocking(self, vals):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is data that can be written.\n\n        :param vals: The values to write to the ", "solution": "\n\n        read_idx = self.__get_idx()\n\n        if read_idx is QueueClosed:\n            return QueueClosed\n\n        # Once the guard is released, read_idx will be placed into write_queue.\n        return self.Guard(self.write_queue, self.arys, lambda: read_idx)"}
{"prompt": "def _get_format_from_style(self, token, style):\n        \"\"\" Returns a QTextCharFormat for token by reading a Pygments style.\n        \"\"\"", "output": "\n        style_def = style.get_style_def(token)\n        style_def = style_def[0]\n        style_def = style_def[1]\n        style_def = style_def[2]\n        style_def = style_def[3]\n        style_def = style_def[4]\n        style_def = style_def[5]\n        style_def = style_def[6]\n        style_def = style_def[7]\n        style_def = style_def[8]\n        style_def = style_def[9]\n        style_def = style_def[10]\n        style_def = style_def[11]\n        style_def = style_def[12]\n        style_def = ", "solution": "\n        result = QtGui.QTextCharFormat()\n        for key, value in style.style_for_token(token).items():\n            if value:\n                if key == 'color':\n                    result.setForeground(self._get_brush(value))\n                elif key == 'bgcolor':\n                    result.setBackground(self._get_brush(value))\n                elif key == 'bold':\n                    result.setFontWeight(QtGui.QFont.Bold)\n                elif key == 'italic':\n                    result.setFontItalic(True)\n                elif key == 'underline':\n                    result.setUnderlineStyle(\n                        QtGui.QTextCharFormat.SingleUnderline)\n                elif key == 'sans':\n                    result.setFontStyleHint(QtGui.QFont.SansSerif)\n                elif key == 'roman':\n                    result.setFontStyleHint(QtGui.QFont.Times)\n                elif key == 'mono':\n                    result.setFontStyleHint(QtGui.QFont.TypeWriter)\n        return result"}
{"prompt": "def p_closed_proposition_list(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list SLASH SLASH closed_proposition\n                                    | closed_proposition\"\"\"", "output": "\n        p[0] = p[1]\n\n    def p_closed_proposition_list_1(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list closed_proposition\n                                    | closed_proposition\"\"\"\n        p[0] = p[1]\n\n    def p_closed_proposition_list_2(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list closed_proposition closed_proposition\n                                    | closed_proposition\"\"\"\n        p[0] = p[1]\n\n    def p_closed_proposition_list_3(self, p):\n        \"\"\" closed_proposition_list :  closed_proposition_list closed_proposition closed_proposition closed_proposition\n", "solution": "\n        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[0] = p[1] + [p[4]]"}
{"prompt": "def docker(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"", "output": "\n        future = Future()\n        future.set_result(self.docker_call(method, *args, **kwargs))\n        return future\n\n    def docker_call(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"\n        future = Future()\n        future.set_result(self.docker_call(method, *args, **kwargs))\n        return future\n\n    def docker_call(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"\n        future = Future()\n        future.set_result(self.docker_call(method, *args, **kwargs))\n        return future\n\n    def docker_call(self, method, *args, **kwargs):\n        \"\"\"Call a docker method in a background thread\n\n        returns a Future\n        \"\"\"\n        ", "solution": "\n        return self.executor.submit(self._docker, method, *args, **kwargs)"}
{"prompt": "def restore_context(self) -> bool:\n        \"\"\"Rollback to previous saved position.\"\"\"", "output": "\n        if self.current_position is not None:\n            self.current_position = self.current_position - 1\n            return True\n        return False\n\n    def move_to_position(self, position: int, mode: int = 0) -> bool:\n        \"\"\"Move the cursor to the specified position.\"\"\"\n        if mode == 0:\n            self.current_position = position\n            return True\n        return False\n\n    def move_to_line(self, line: int, mode: int = 0) -> bool:\n        \"\"\"Move the cursor to the specified line.\"\"\"\n        if mode == 0:\n            self.current_position = line\n            return True\n        return False\n\n    def move_to_column(self, column: int, mode: int = 0) -> bool:\n        \"\"\"Move the cursor to the specified column.\"\"\"\n        if mode == 0:\n            self.current_position = column\n            return True\n        return False\n\n    def move_to_end(self, mode: int = 0", "solution": "\n        self._cursor.position = self._contexts.pop()\n        return False"}
{"prompt": "def wave_interp_option(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpOption` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the name of the argument the\n     contract is attached to\n\n    :rtype: None\n    \"\"\"", "output": "\n    obj = obj.lower()\n    if obj == 'wave':\n        return\n    if obj == 'wave-interp':\n        return\n    raise RuntimeError('Argument `*[argument_name]*\\` is not valid. The token `*[argument_name]*` is replaced by the name of the argument the contract is attached to')\n\n\ndef wave_interp_argument_name(obj):\n    r\"\"\"\n    Validate if an object is a :ref:`WaveInterpArgumentName` pseudo-type object.\n\n    :param obj: Object\n    :type  obj: any\n\n    :raises: RuntimeError (Argument \\`*[argument_name]*\\` is not valid). The\n     token \\*[argument_name]\\* is replaced by the ", "solution": "\n    exdesc = pexdoc.pcontracts.get_exdesc()\n    if not isinstance(obj, str):\n        raise ValueError(exdesc)\n    if obj.upper() in [\"CONTINUOUS\", \"STAIRCASE\"]:\n        return None\n    raise ValueError(exdesc)"}
{"prompt": "def run(self, dag):\n        \"\"\"\n        Pick a layout by assigning n circuit qubits to device qubits 0, .., n-1.\n\n        Args:\n            dag (DAGCircuit): DAG to find layout for.\n\n        Raises:\n            TranspilerError: if dag wider than self.coupling_map\n        \"\"\"", "output": "\n        coupling_map = self.coupling_map\n        coupling_map = coupling_map.copy()\n        coupling_map.update(dag._dag._coupling_map)\n        coupling_map = coupling_map.copy()\n        coupling_map.update(dag._dag._dag_coupling_map)\n        coupling_map = coupling_map.copy()\n        coupling_map.update(dag._dag._dag_dag_coupling_map)\n        coupling_map = coupling_map.copy()\n        coupling_map.update(dag._dag._dag_dag_dag_coupling_map)\n        coupling_map = coup", "solution": "\n        num_dag_qubits = sum([qreg.size for qreg in dag.qregs.values()])\n        if num_dag_qubits > self.coupling_map.size():\n            raise TranspilerError('Number of qubits greater than device.')\n        self.property_set['layout'] = Layout.generate_trivial_layout(*dag.qregs.values())"}
{"prompt": "def update(self,dt):\n        \"\"\"\n        Should be called regularly to move the actor.\n        \n        This method does nothing if the :py:attr:`enabled` property is set to False.\n        \n        This method is called automatically and should not be called manually.\n        \"\"\"", "output": "\n        if self.enabled:\n            self.position += self.velocity*dt\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= 360\n            self.rotation += self.rotationSpeed*dt\n            self.rotation %= ", "solution": "\n        if not self.enabled:\n            return\n        dy = self.speed * dt * self.move\n        x,y,z = self.actor._pos\n        newpos = x,dy+y,z\n        self.actor.pos = newpos"}
{"prompt": "def _start_webserver(self, authorize_url=None):\n\t\t\"\"\"\n\t\tStart the webserver that will receive the code\n\t\t\"\"\"", "output": "\n\t\tfrom twisted.web import server, static, http\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom twisted.web.static import File\n\t\tfrom twisted.internet import reactor\n\t\tfrom twisted.web.resource import Resource\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom twisted.internet import reactor\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom twisted.web.server import NOT_DONE_YET\n\t\tfrom ", "solution": "\n\t\tserver_address = (SERVER_URL, SERVER_PORT)\n\t\tself.server = HTTPServer(server_address, OAuth2UtilRequestHandler)\n\t\tself.server.response_code = None\n\t\tself.server.authorize_url = authorize_url\n\t\tt = Thread(target=self.server.serve_forever)\n\t\tt.daemon = True\n\t\tt.start()"}
{"prompt": "def transcode_to_stream(input_filename, date_format=None):\n    \"\"\"\n    Read a JSON or CSV file and convert it into a JSON stream, which will\n    be saved in an anonymous temp file.\n    \"\"\"", "output": "\n    date_format = date_format or '%Y-%m-%d'\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d' if date_format == '%Y-%m-%d' else date_format\n    date_format = '%Y-%m-%d", "solution": "\n    tmp = tempfile.TemporaryFile()\n    for entry in open_json_or_csv_somehow(input_filename,\n                                          date_format=date_format):\n        tmp.write(json.dumps(entry, ensure_ascii=False).encode('utf-8'))\n        tmp.write(b'\\n')\n    tmp.seek(0)\n    return tmp"}
{"prompt": "def visit_classdef(self, node):\n        \"\"\"visit an astroid.Class node\n\n         * set the locals_type and instance_attrs_type mappings\n         * set the implements list and build it\n         * optionally tag the node with a unique id\n        \"\"\"", "output": "\n        node.locals_type = node.instance_attrs_type = node.implements = []\n        node.unique_id = None\n        node.is_abstract = False\n        node.name = node.name = node.name.replace('_', '-')\n        node.doc = None\n        node.lineno = node.col_offset = 0\n        node.parent = node.root = None\n        node.igetattr = None\n        node.igetattr_args = None\n        node.igetattr_kwds = None\n        node.igetattr_defaults = None\n        node.igetattr_returns = None\n        node.igetattr_vararg = None\n        node.igetattr_kwarg = None\n        node.igetattr_defaults_type = None\n        node.i", "solution": "\n        if hasattr(node, \"locals_type\"):\n            return\n        node.locals_type = collections.defaultdict(list)\n        if self.tag:\n            node.uid = self.generate_id()\n        # resolve ancestors\n        for baseobj in node.ancestors(recurs=False):\n            specializations = getattr(baseobj, \"specializations\", [])\n            specializations.append(node)\n            baseobj.specializations = specializations\n        # resolve instance attributes\n        node.instance_attrs_type = collections.defaultdict(list)\n        for assignattrs in node.instance_attrs.values():\n            for assignattr in assignattrs:\n                self.handle_assignattr_type(assignattr, node)\n        # resolve implemented interface\n        try:\n            node.implements = list(interfaces(node, self.inherited_interfaces))\n        except astroid.InferenceError:\n            node.implements = ()"}
{"prompt": "def _create_input_transactions(self, addy):\n        # type: (Address) -> None\n        \"\"\"\n        Creates transactions for the specified input address.\n        \"\"\"", "output": "\n        txns = []\n        txns.extend(\n            Transaction(\n                from_addr=addy,\n                to_addr=self.address,\n                amount=int(self.config.dev.coinbase_size * 1e8),\n                fee_amount=int(self.config.dev.coinbase_size * 1e8),\n                confirmations=0,\n                data=bytes([]),\n                script=bytes([OP_RETURN]),\n            )\n        )\n        txns.extend(\n            Transaction(\n                from_addr=addy,\n                to_addr=self.address,\n                amount=int(self.config.dev.coinbase_size * 1e8),\n                fee_amount=int(self.config.dev.coinbase_size * 1e", "solution": "\n        self._transactions.append(ProposedTransaction(\n            address=addy,\n            tag=self.tag,\n\n            # Spend the entire address balance; if necessary, we will\n            # add a change transaction to the bundle.\n            value=-addy.balance,\n        ))\n\n        # Signatures require additional transactions to store, due to\n        # transaction length limit.\n        # Subtract 1 to account for the transaction we just added.\n        for _ in range(addy.security_level - 1):\n            self._transactions.append(ProposedTransaction(\n                address=addy,\n                tag=self.tag,\n\n                # Note zero value; this is a meta transaction.\n                value=0,\n            ))"}
{"prompt": "def _define(self):\n        \"\"\"Calculate a subcircuit that implements this initialization\n\n        Implements a recursive initialization algorithm, including optimizations,\n        from \"Synthesis of Quantum Logic Circuits\" Shende, Bullock, Markov\n        https://arxiv.org/abs/quant-ph/0406176v5\n\n        Additionally implements some extra optimizations: remove zero rotations and\n        double cnots.\n        \"\"\"", "output": "\n        # initialize subcircuit by applying all gates to all qubits\n        gates = [\n            Gate(self.num_qubits, gate)\n            for gate in self.gates\n        ]\n        gates.append(\n            Gate(self.num_qubits, GateType.CNOT)\n        )\n        gates.append(\n            Gate(self.num_qubits, GateType.CNOT)\n        )\n        gates.append(\n            Gate(self.num_qubits, GateType.CNOT)\n        )\n        gates.append(\n            Gate(self.num_qubits, GateType.CNOT)\n        )\n        gates.append(\n            Gate(self.num_qubits, GateType.CNOT)\n        )\n        ", "solution": "\n        # call to generate the circuit that takes the desired vector to zero\n        disentangling_circuit = self.gates_to_uncompute()\n\n        # invert the circuit to create the desired vector from zero (assuming\n        # the qubits are in the zero state)\n        initialize_instr = disentangling_circuit.to_instruction().inverse()\n\n        q = QuantumRegister(self.num_qubits, 'q')\n        initialize_circuit = QuantumCircuit(q, name='init_def')\n        for qubit in q:\n            initialize_circuit.append(Reset(), [qubit])\n        initialize_circuit.append(initialize_instr, q[:])\n\n        self.definition = initialize_circuit.data"}
{"prompt": "def generate_query(command):\n    \"\"\"Add header, checksum and footer to command data.\"\"\"", "output": "\n    header = command.header\n    header.update(\n        checksum=generate_checksum(header),\n        footer=generate_checksum(command.footer),\n    )\n    return command\n\n\ndef validate_command(command):\n    \"\"\"Validate command data.\"\"\"\n    validate_header(command.header)\n    validate_checksum(command.header)\n    validate_footer(command.footer)\n\n\ndef validate_header(header):\n    \"\"\"Validate header data.\"\"\"\n    validate_string(header.name, MAX_HEADER_NAME_LENGTH)\n    validate_string(header.value, MAX_HEADER_VALUE_LENGTH)\n\n\ndef validate_checksum(header):\n    \"\"\"Validate checksum data.\"\"\"\n    validate_string(header.checksum, MAX_HEADER_VALUE_LENGTH)\n\n\ndef ", "solution": "\n    data = bytearray(command)\n    c = checksum(data)\n    data.append(c >> 8)\n    data.append(c & 0xFF)\n    data.replace(b'\\xFE', b'\\xFE\\xF0')\n\n    data = bytearray.fromhex(\"FEFE\") + data + bytearray.fromhex(\"FE0D\")\n    return data"}
{"prompt": "def copy(self):\n        \"\"\"\n        :returns: datasketch.MinHash -- A copy of this MinHash by exporting its state.\n        \"\"\"", "output": "\n        state = copy.deepcopy(self)\n        state.hash = self.hash\n        state.hash_bytes = self.hash_bytes\n        state.hash_length = self.hash_length\n        state.hash_table = self.hash_table\n        state.hash_table_length = self.hash_table_length\n        state.hash_table_entries = self.hash_table_entries\n        state.hash_table_entries_length = self.hash_table_entries_length\n        state.hash_table_entries_capacity = self.hash_table_entries_capacity\n        state.hash_table_entries_used = self.hash_table_entries_used\n        state.hash_table_entries_free = self.hash_table_entries_free\n        state.hash_table_entries_used2_capacity = self.hash_table_entries_used2_capacity\n        state.hash_table_entries_", "solution": "\n        return MinHash(seed=self.seed, hashfunc=self.hashfunc,\n                hashvalues=self.digest(),\n                permutations=self.permutations)"}
{"prompt": "def publish(\n            self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Publish runbook draft.\n\n        :param resource_group_name: Name of an Azure Resource group.\n        :type resource_group_name: str\n        :param automation_account_name: The name of the automation account.\n        :type automation_account_name: str\n        :param runbook_name: The parameters supplied to the publish runbook\n         operation.\n        :type runbook_name: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns None or\n         ClientRawResponse<None> if raw==True\n        :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\n        \"\"\"", "output": "polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns RunbookDraft or\n         ClientRawResponse<RunbookDraft> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.automation.models.RunbookDraft]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.automation.models.", "solution": "\n        raw_result = self._publish_initial(\n            resource_group_name=resource_group_name,\n            automation_account_name=automation_account_name,\n            runbook_name=runbook_name,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            if raw:\n                client_raw_response = ClientRawResponse(None, response)\n                client_raw_response.add_headers({\n                    'location': 'str',\n                })\n                return client_raw_response\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _is_async(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async keyword.\"\"\"", "output": "\n    return o.async or o.async_\n\ndef _is_async_func(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async_func keyword.\"\"\"\n    return o.async_func or o.async_func_\n\ndef _is_async_generator(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async_generator keyword.\"\"\"\n    return o.async_generator or o.async_gen_func\n\ndef _is_async_await(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async_await keyword.\"\"\"\n    return o.async_await or o.async_await_func\n\ndef _is_async_with_await(o: IMeta) -> bool:\n    \"\"\"Return True if the meta contains :async_with_await keyword.\"\"\"\n    return o.async_with_await or o.async_", "solution": "\n    return (  # type: ignore\n        Maybe(o.meta)\n        .map(lambda m: m.entry(SYM_ASYNC_META_KEY, None))\n        .or_else_get(False)\n    )"}
{"prompt": "def add_value(self, name, value):\n        \"\"\"\n        Add a new value to the list.\n\n        :param str name: name of the value that is being parsed\n        :param str value: value that is being parsed\n        :raises ietfparse.errors.MalformedLinkValue:\n            if *strict mode* is enabled and a validation error\n            is detected\n\n        This method implements most of the validation mentioned in\n        sections 5.3 and 5.4 of :rfc:`5988`.  The ``_rfc_values``\n        dictionary contains the appropriate values for the attributes\n        that get special handling.  If *strict mode* is enabled, then\n        only values that are acceptable will be added to ``_values``.\n\n        \"\"\"", "output": "\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n        # pylint: disable=too-many-return-statements\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n        # pylint: disable=too-many-return-statements\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n        # pylint: disable=too-many-return-statements\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n        # pylint: disable=too-many-return-statements\n        # pylint: disable=too-many-branches\n", "solution": "\n        try:\n            if self._rfc_values[name] is None:\n                self._rfc_values[name] = value\n            elif self.strict:\n                if name in ('media', 'type'):\n                    raise errors.MalformedLinkValue(\n                        'More than one {} parameter present'.format(name))\n                return\n        except KeyError:\n            pass\n\n        if self.strict and name in ('title', 'title*'):\n            return\n\n        self._values.append((name, value))"}
{"prompt": "def sp_search_query(query):\n    \"\"\"Translate a Mopidy search query to a Spotify search query\"\"\"", "output": "\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    query = '\"' + query + '\"'\n    ", "solution": "\n\n    result = []\n\n    for (field, values) in query.items():\n        field = SEARCH_FIELD_MAP.get(field, field)\n        if field is None:\n            continue\n\n        for value in values:\n            if field == 'year':\n                value = _transform_year(value)\n                if value is not None:\n                    result.append('%s:%d' % (field, value))\n            elif field == 'any':\n                result.append('\"%s\"' % value)\n            else:\n                result.append('%s:\"%s\"' % (field, value))\n\n    return ' '.join(result)"}
{"prompt": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"", "output": "\n        cov = CoverageData()\n        cov.start()\n        cov.stop()\n        cov.save()\n\n        if fullpath:\n            filenames = sorted(self.filenames)\n        else:\n            filenames = [self.basename]\n\n        for filename in filenames:\n            linenos = self.stats[filename]\n            cov.data_file(filename, linenos)\n        return cov\n\n    def write_html(self, covdir, covhtml):\n        \"\"\"Write the coverage html file.\n\n        covdir is the directory where the html file will be written.\n        covhtml is the name of the html file.\n\n        \"\"\"\n        covhtml = os.path.join(covdir, covhtml)\n        covhtml = self._replace_newlines(covhtml)\n        covhtml", "solution": "\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ"}
{"prompt": "def read_hex_integer(self) -> bool:\n    \"\"\"\n    read a hexadecimal number\n    Read the following BNF rule else return False::\n\n        readHexInteger = [\n            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+\n        ]\n    \"\"\"", "output": "\n    match = self.read_hex_integer_re()\n    if not match:\n        return False\n    number = int(match.group(0), 16)\n    if number > 0xff:\n        return False\n    return number\n\n\ndef read_hex_integer_re(self) -> Optional[Match[str]]:\n    \"\"\"\n    read a hexadecimal number\n    Read the following BNF rule else return None::\n\n        readHexInteger = [\n            [ '0'..'9' | 'a'..'f' | 'A'..'F' ]+\n        ]\n    \"\"\"\n    match = self.re_hex_integer.match(self.data)\n    if not match:\n        return None\n    number = int(match.group(0), 16)\n    if number > 0xff", "solution": "\n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    c = self._stream.peek_char\n    if c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f'):\n        self._stream.incpos()\n        while not self.read_eof():\n            c = self._stream.peek_char\n            if not (c.isdigit() or ('a' <= c.lower() and c.lower() <= 'f')):\n                break\n            self._stream.incpos()\n        return self._stream.validate_context()\n    return self._stream.restore_context()"}
{"prompt": "def init_app(self, app):\n        \"\"\"\n        Initializes the Flask-Allows object against the provided application\n        \"\"\"", "output": "\n        self.app = app\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = AllowsMiddleware(self.app.wsgi_app, app)\n        self.app.wsgi_app = Allows", "solution": "\n        if not hasattr(app, \"extensions\"):  # pragma: no cover\n            app.extensions = {}\n        app.extensions[\"allows\"] = self\n\n        @app.before_request\n        def start_context(*a, **k):\n            self.overrides.push(Override())\n            self.additional.push(Additional())\n\n        @app.after_request\n        def cleanup(response):\n            self.clear_all_overrides()\n            self.clear_all_additional()\n            return response"}
{"prompt": "def raw_dict_factory(cursor, row):\n        \"\"\" Sqlite callback accepting the cursor and the original row as a tuple.\n\n        Simple return of JSON safe types, including raw read hex strings.\n\n        Args:\n            cursor (sqlite cursor):  Original cursory\n            row (sqlite row tuple): Original row.\n\n        Returns:\n            dict: modified row.\n        \"\"\"", "output": "\n        row_dict = dict(row)\n        row_dict['hex'] = row_dict['hex'].decode('hex')\n        return row_dict\n\n    conn.row_factory = raw_dict_factory\n\n    return conn\n\n\ndef connect_to_db(db_file):\n    \"\"\" Connect to SQLite DB\n\n    Args:\n        db_file (str): Path to SQLite DB\n\n    Returns:\n        sqlite3.Connection: SQLite connection\n    \"\"\"\n    conn = None\n    try:\n        conn = sqlite3.connect(db_file)\n    except sqlite3.Error as e:\n        print(e)\n    return conn\n\n\ndef close_db(conn):\n    \"\"\" Close SQLite connection\n\n    Args:\n        conn (sqlite3.Connection): SQLite connection\n    \"\"\"\n    if conn is not None:\n        ", "solution": "\n        d = {}\n        for idx, col in enumerate(cursor.description):\n            val = row[idx]\n            name = col[0]\n            if name == Field.Time_Stamp or name == Field.Meter_Address:\n                d[name] = str(val)\n                continue\n            if name == \"Raw_A\" or name == \"Raw_B\":\n                d[name] = str(val)\n                continue\n        return d"}
{"prompt": "def _onSelectItem(self, selection, previousSelection):\n        \"\"\"Handle selection of item in listing.\"\"\"", "output": "\n        selection = selection[0]\n        previousSelection = previousSelection[0]\n        selectionItem = selection[1]\n        previousSelectionItem = previousSelection[1]\n        selectionItemType = selectionItem[0]\n        previousSelectionItemType = previousSelectionItem[0]\n        selectionItemText = selectionItem[1]\n        previousSelectionItemText = previousSelectionItem[1]\n        selectionItemValue = selectionItem[2]\n        previousSelectionItemValue = previousSelectionItem[2]\n        selectionItemClass = selectionItem[3]\n        previousSelectionItemClass = previousSelectionItem[3]\n        selectionItemIsSelected = selectionItem[5]\n        previousSelectionItemIsSelected = previousSelectionItem[5]\n        selectionItem", "solution": "\n        self._acceptButton.setEnabled(True)\n        del self._selected[:]\n        item = self._filesystemWidget.model().item(selection)\n        self._selected.append(item.path)"}
{"prompt": "def _srvc_set_config(self, trajectory):\n        \"\"\"Sets a config value to the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"", "output": "\n        if trajectory is not None:\n            trajectory.config = self.config\n        else:\n            self.config = self.config\n        self.config.update(trajectory)\n\n    def _srvc_get_config(self, trajectory):\n        \"\"\"Gets a config value from the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"\n        if trajectory is not None:\n            return trajectory.config\n        else:\n            return self.config\n\n    def _srvc_set_config_value(self, trajectory, value):\n        \"\"\"Sets a config value to the Trajectory or changes it if the trajectory was loaded\n        a the settings no longer match\"\"\"\n        if trajectory is not None:\n            trajectory.config_value = value\n        else:\n            self.config_value = value\n        self.", "solution": "\n        def _set_config(name, value, comment):\n            if not trajectory.f_contains('config.'+name, shortcuts=False):\n                trajectory.f_add_config(Parameter, name, value, comment=comment)\n\n        for attr_name in HDF5StorageService.NAME_TABLE_MAPPING:\n            table_name = HDF5StorageService.NAME_TABLE_MAPPING[attr_name]\n            value = getattr(self, attr_name)\n            _set_config('hdf5.overview.' + table_name,\n                         value,\n                         comment='Whether or not to have an overview '\n                                 'table with that name')\n\n        _set_config('hdf5.purge_duplicate_comments',\n                    self._purge_duplicate_comments,\n                    comment='Whether comments of results and'\n                            ' derived parameters should only'\n                            ' be stored for the very first instance.'\n                            ' Works only if the summary tables are'\n                            ' active.')\n\n        _set_config('hdf5.results_per_run', self._results_per_run,\n                    comment='Expected number of results per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.derived_parameters_per_run',\n                    self._derived_parameters_per_run,\n                    comment='Expected number of derived parameters per run,'\n                            ' a good guess can increase storage performance')\n\n        _set_config('hdf5.complevel', self._complevel,\n                    comment='Compression Level (0 no compression '\n                            'to 9 highest compression)')\n\n        _set_config('hdf5.complib', self._complib,\n                    comment='Compression Algorithm')\n\n        _set_config('hdf5.encoding', self._encoding,\n                    comment='Encoding for unicode characters')\n\n        _set_config('hdf5.fletcher32', self._fletcher32,\n                    comment='Whether to use fletcher 32 checksum')\n\n        _set_config('hdf5.shuffle', self._shuffle,\n                    comment='Whether to use shuffle filtering.')\n\n        _set_config('hdf5.pandas_format', self._pandas_format,\n                    comment="}
{"prompt": "def draw(self):\n        \"\"\"\n        Draws all layers of this LayeredWidget.\n        \n        This should normally be unneccessary, since it is recommended that layers use Vertex Lists instead of OpenGL Immediate Mode.\n        \"\"\"", "output": "\n        \n        # Draw the layers\n        for layer in self.layers:\n            layer.draw()\n        \n        # Draw the background\n        self.background.draw()\n        \n        # Draw the text\n        self.text.draw()\n        \n        # Draw the buttons\n        self.buttons.draw()\n        \n        # Draw the button\n        self.button.draw()\n        \n        # Draw the text\n        self.text.draw()\n        \n        # Draw the button\n        self.button.draw()\n        \n        # Draw the text\n        self.text.draw()\n        \n        # Draw the button\n        self.button.draw()\n        \n        # Draw the text\n        self.text.draw()\n        \n        # Draw the button\n        self.button.", "solution": "\n        super(LayeredWidget,self).draw()\n        for layer,_ in self.layers:\n            layer._draw()"}
{"prompt": "def get_exitcodes_terminated_worker(processes):\n    \"\"\"Return a formated string with the exitcodes of terminated workers.\n\n    If necessary, wait (up to .25s) for the system to correctly set the\n    exitcode of one terminated worker.\n    \"\"\"", "output": "\n    exitcodes = []\n    for p in processes:\n        exitcodes.append(p.exitcode)\n        if p.exitcode is None:\n            time.sleep(0.5)\n    return''.join(map(str, exitcodes))\n\n\ndef wait_for_terminated_worker(processes, timeout=10):\n    \"\"\"Wait for the terminated worker to exit with given exit codes.\n\n    Wait up to timeout seconds for the terminated worker to exit with given\n    exit codes.\n    \"\"\"\n    start = time.time()\n    exitcodes = []\n    for p in processes:\n        exitcodes.append(p.exitcode)\n        if p.exitcode is None:\n            time.sleep(0.5)\n    if exitcodes:\n        exitcodes.sort()\n        exitcodes =''.join(map(", "solution": "\n    patience = 5\n\n    # Catch the exitcode of the terminated workers. There should at least be\n    # one. If not, wait a bit for the system to correctly set the exitcode of\n    # the terminated worker.\n    exitcodes = [p.exitcode for p in list(processes.values())\n                 if p.exitcode is not None]\n    while len(exitcodes) == 0 and patience > 0:\n        patience -= 1\n        exitcodes = [p.exitcode for p in list(processes.values())\n                     if p.exitcode is not None]\n        time.sleep(.05)\n\n    return _format_exitcodes(exitcodes)"}
{"prompt": "def eintr_retry(exc_type, f, *args, **kwargs):\n    \"\"\"Calls a function.  If an error of the given exception type with\n    interrupted system call (EINTR) occurs calls the function again.\n    \"\"\"", "output": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type, e:\n            if e.args[0] == errno.EINTR:\n                continue\n            raise\n\n\ndef retry_on_eintr(f):\n    \"\"\"Decorator to retry a function on EINTR errors.\"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except IOError as e:\n            if e.args[0] == errno.EINTR:\n                return f(*args, **kwargs)\n            raise\n    return wrapper\n\n\ndef retry_on_eintr_errback(f):\n    \"\"\"Decorator to retry a function on EINTR errors.\"\"\"\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except IO", "solution": "\n    while True:\n        try:\n            return f(*args, **kwargs)\n        except exc_type as exc:\n            if exc.errno != EINTR:\n                raise\n        else:\n            break"}
{"prompt": "def _colorify(self, data):\n        \"\"\"\n        Retun colored string.\n\n        :param data: The string to colorify.\n        :type data: str\n\n        :return: A colored string.\n        :rtype: str\n        \"\"\"", "output": "\n        color_map = {\n           'red': '\\033[91m',\n            'green': '\\033[92m',\n            'yellow': '\\033[93m',\n            'blue': '\\033[94m',\n           'magenta': '\\033[95m',\n            'cyan': '\\033[96m',\n            'white': '\\033[97m',\n           'reset': '\\033[0m',\n        }\n        return color_map[data]\n\n    def _colorify2(self, data):\n        \"\"\"\n        Retun colored string.\n\n        :param data: The string to colorify.\n        :type data: str\n\n        :return: A colored string.\n        :rtype: str\n        \"\"\"\n        color_map = {\n           'red': '\\033[91m',\n            'green': '\\033[92m',\n            'yellow': '\\033[", "solution": "\n\n        if self.template in [\"Generic\", \"Less\"]:\n            # The template is in the list of template that need the coloration.\n\n            if (\n                self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"up\"]\n                or self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"valid\"]\n            ):\n                # The status is in the list of up status.\n\n                # We print the data with a green background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.GREEN + data\n            elif self.data_to_print[1].lower() in PyFunceble.STATUS[\"list\"][\"down\"]:\n                # The status is in the list of down status.\n\n                # We print the data with a red background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.RED + data\n            else:\n                # The status is not in the list of up and down status.\n\n                # We print the data with a cyan background.\n                data = PyFunceble.Fore.BLACK + PyFunceble.Back.CYAN + data\n\n        # We return the data.\n        return data"}
{"prompt": "def __eof_qubit(rho):\n    \"\"\"\n    Compute the Entanglement of Formation of a 2-qubit density matrix.\n\n    Args:\n        rho ((array_like): (4,4) array_like, input density matrix.\n\n    Returns:\n        float: The entanglement of formation.\n    \"\"\"", "output": "\n    density_matrix = rho.dot(rho.T)\n    density_matrix = np.exp(density_matrix)\n    return np.sum(density_matrix)\n\n\ndef eof_qubit(rho):\n    \"\"\"\n    Compute the Entanglement of Formation of a 3-qubit density matrix.\n\n    Args:\n        rho ((array_like): (4,4) array_like, input density matrix.\n\n    Returns:\n        float: The entanglement of formation.\n    \"\"\"\n    density_matrix = rho.dot(rho.T)\n    density_matrix = np.exp(density_matrix)\n    return np.sum(density_matrix)\n\n\ndef eof_qubit_2qubit_2(rho):\n    \"\"\"\n    Compute the Entangle", "solution": "\n    c = concurrence(rho)\n    c = 0.5 + 0.5 * np.sqrt(1 - c * c)\n    return shannon_entropy([c, 1 - c])"}
{"prompt": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"", "output": "\n        accesses = {}\n        for var in self.variables:\n            accesses[var] = []\n            for access in self.accesses[var]:\n                accesses[var].append(access)\n            if sources:\n                accesses[var] = list(filter(lambda x: x not in accesses[var], accesses[var]))\n            if destinations:\n                accesses[var] = list(filter(lambda x: x in accesses[var], accesses[var]))\n        return accesses\n\n    def compile_sympy_expressions(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy expressions, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"", "solution": "\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses"}
{"prompt": "def deserialize(bstr):\n    \"\"\"\n    Attempts to deserialize a bytestring into an audiosegment.\n\n    :param bstr: The bytestring serialized via an audiosegment's serialize() method.\n    :returns: An AudioSegment object deserialized from `bstr`.\n    \"\"\"", "output": "\n    segment = AudioSegment.from_bytes(bstr)\n    return segment\n\n\ndef serialize(segment):\n    \"\"\"\n    Serializes an AudioSegment object into a bytestring.\n\n    :param segment: An AudioSegment object.\n    :returns: A bytestring serialized via an audiosegment's serialize() method.\n    \"\"\"\n    return segment.serialize()\n\n\ndef from_file(filename, offset=0, duration=None, channels=None, sample_rate=None, offsets=None, channel_ids=None, channel_names=None, channel_modes=None, channel_layouts=None, channel_layouts_names=None, channel_layouts_modes=None, channel_layouts_layouts=None, channel_layouts_layouts_names=None, channel_layouts_layouts_modes=None, channel_layouts_layouts_layouts_names=None, channel_", "solution": "\n    d = pickle.loads(bstr)\n    seg = pickle.loads(d['seg'])\n    return AudioSegment(seg, d['name'])"}
{"prompt": "def create_validator(data_struct_dict, name=None):\n    \"\"\"\n    create a Validator instance from data_struct_dict\n\n    :param data_struct_dict: a dict describe validator's fields, like the dict `to_dict()` method returned.\n    :param name: name of Validator class \n\n    :return: Validator instance\n    \"\"\"", "output": "\n    validator = Validator(data_struct_dict, name=name)\n    return validator\n\n\ndef to_dict(validator):\n    \"\"\"\n    convert a Validator instance to a dict that describe the fields of Validator\n\n    :param validator: Validator instance\n\n    :return: a dict describe the fields of Validator\n    \"\"\"\n    return validator.to_dict()\n\n\ndef from_dict(validator_dict):\n    \"\"\"\n    convert a dict that describe the fields of Validator to a Validator instance\n\n    :param validator_dict: a dict describe the fields of Validator\n\n    :return: a Validator instance\n    \"\"\"\n    return Validator.from_dict(validator_dict)\n\n\ndef to_json(validator):\n    \"\"\"\n    convert a Validator instance to a json ", "solution": "\n\n    if name is None:\n        name = 'FromDictValidator'\n    attrs = {}\n    for field_name, field_info in six.iteritems(data_struct_dict):\n        field_type = field_info['type']\n        if field_type == DictField.FIELD_TYPE_NAME and isinstance(field_info.get('validator'), dict):\n            field_info['validator'] = create_validator(field_info['validator'])\n        attrs[field_name] = create_field(field_info)\n    name = force_str(name)\n    return type(name, (Validator, ), attrs)"}
{"prompt": "def _store(self, lines, buffer=None, store='source'):\n        \"\"\"Store one or more lines of input.\n\n        If input lines are not newline-terminated, a newline is automatically\n        appended.\"\"\"", "output": "\n        lines = iter(lines)\n        buffer = '' if buffer is None else buffer\n        lines = iter(lines)\n        for line in lines:\n            if buffer:\n                buffer += line\n            else:\n                yield buffer\n                buffer = line\n        if buffer:\n            yield buffer\n\n    def _store_and_check(self, lines, buffer=None, store='source'):\n        \"\"\"Store one or more lines of input.\n\n        If input lines are not newline-terminated, a newline is automatically\n        appended.\"\"\"\n        lines = iter(lines)\n        buffer = '' if buffer is None else buffer\n        lines = iter(lines)\n        for line in lines:\n            if buffer:\n                buffer += line\n            else:\n                yield buffer\n                buffer = line\n            if line.endswith('\\n", "solution": "\n        \n        if buffer is None:\n            buffer = self._buffer\n            \n        if lines.endswith('\\n'):\n            buffer.append(lines)\n        else:\n            buffer.append(lines+'\\n')\n        setattr(self, store, self._set_source(buffer))"}
{"prompt": "def selection_redo(self, name=\"default\", executor=None):\n        \"\"\"Redo selection, for the name.\"\"\"", "output": "\n        selection = self.selection\n        selection.redo(name=name)\n        if executor:\n            executor._redo(name=name)\n\n    def selection_undo(self, name=\"default\", executor=None):\n        \"\"\"Undo selection, for the name.\"\"\"\n        selection = self.selection\n        selection.undo(name=name)\n        if executor:\n            executor._redo(name=name)\n\n    def selection_copy(self, name=\"default\", executor=None):\n        \"\"\"Copy selection, for the name.\"\"\"\n        selection = self.selection\n        selection.copy(name=name)\n        if executor:\n            executor._redo(name=name)\n\n    def selection_paste(self, name=\"default\", executor=None):\n        \"\"\"Paste selection, for the name.\"\"\"\n        selection = self.", "solution": "\n        logger.debug(\"redo\")\n        executor = executor or self.executor\n        assert self.selection_can_redo(name=name)\n        selection_history = self.selection_histories[name]\n        index = self.selection_history_indices[name]\n        next = selection_history[index + 1]\n        self.selection_history_indices[name] += 1\n        self.signal_selection_changed.emit(self)\n        logger.debug(\"redo: selection history is %r, index is %r\", selection_history, index)"}
{"prompt": "def fetch(self, category=CATEGORY_QUESTION, offset=DEFAULT_OFFSET):\n        \"\"\"Fetch questions from the Kitsune url.\n\n        :param category: the category of items to fetch\n        :offset: obtain questions after offset\n        :returns: a generator of questions\n        \"\"\"", "output": "\n        questions = []\n        page = 1\n        while True:\n            url = KITSUIKU_QUESTIONS_URL.format(category=category,\n                                                   offset=offset)\n            resp = self.session.get(url)\n            resp.raise_for_status()\n            questions += resp.json()['questions']\n            offset += DEFAULT_OFFSET\n            page = page + 1\n            if page > MAX_QUESTIONS_PER_PAGE:\n                break\n        return questions\n\n    def get_question(self, question_id):\n        \"\"\"Fetch a question from the Kitsune url.\n\n        :param question_id: the id of the question\n        :returns: a question object\n        \"\"\"\n        questions = []\n        for question in self.fetch(CATEGORY_QUESTION):\n", "solution": "\n        if not offset:\n            offset = DEFAULT_OFFSET\n\n        kwargs = {\"offset\": offset}\n        items = super().fetch(category, **kwargs)\n\n        return items"}
{"prompt": "def make_reports(self, stats, old_stats):\n        \"\"\"render registered reports\"\"\"", "output": "\n        stats[\"reports\"] = list(self.reports)\n        stats[\"old_reports\"] = list(self.old_reports)\n        stats[\"old_reports_stats\"] = list(self.old_reports_stats)\n        stats[\"old_reports_stats_old\"] = list(self.old_reports_stats_old)\n        stats[\"old_reports_stats_new\"] = list(self.old_reports_stats_new)\n        stats[\"old_reports_stats_old2\"] = list(self.old_reports_stats_old2)\n        stats[\"old_reports_stats_new2\"] = list(self.old_reports_stats_new2)\n        stats[\"old_reports_stats_old3\"] = list(self.old_reports_stats_old3)\n        stats[\"old_report", "solution": "\n        sect = Section(\"Report\", \"%s statements analysed.\" % (self.stats[\"statement\"]))\n        for checker in self.report_order():\n            for reportid, r_title, r_cb in self._reports[checker]:\n                if not self.report_is_enabled(reportid):\n                    continue\n                report_sect = Section(r_title)\n                try:\n                    r_cb(report_sect, stats, old_stats)\n                except EmptyReportError:\n                    continue\n                report_sect.report_id = reportid\n                sect.append(report_sect)\n        return sect"}
{"prompt": "def format_lines(statements, lines):\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statements` is [1,2,3,4,5,10,11,12,13,14] and\n    `lines` is [1,2,5,10,11,13,14] then the result will be \"1-2, 5-11, 13-14\".\n\n    \"\"\"", "output": "\n    statements = sorted(statements)\n    lines = sorted(lines)\n    gaps = lines[1:] - lines[:-1]\n    gaps = [min(gap, len(statements)) for gap in gaps]\n    statements = [statements[i:j] for i, j in zip(gaps[:-1], gaps[1:])]\n    lines.extend(gaps)\n    return ', '.join(statements)\n\n\ndef format_lines(statements, lines):\n    \"\"\"Nicely format a list of line numbers.\n\n    Format a list of line numbers for printing by coalescing groups of lines as\n    long as the lines represent consecutive statements.  This will coalesce\n    even if there are gaps between statements.\n\n    For example, if `statement", "solution": "\n    pairs = []\n    i = 0\n    j = 0\n    start = None\n    statements = sorted(statements)\n    lines = sorted(lines)\n    while i < len(statements) and j < len(lines):\n        if statements[i] == lines[j]:\n            if start == None:\n                start = lines[j]\n            end = lines[j]\n            j += 1\n        elif start:\n            pairs.append((start, end))\n            start = None\n        i += 1\n    if start:\n        pairs.append((start, end))\n    ret = ', '.join(map(nice_pair, pairs))\n    return ret"}
{"prompt": "def _check_function(self):\n        \"\"\" make some basic checks on the function to make sure it is valid\"\"\"", "output": "\n        # make sure that the function has a name\n        if not self.name:\n            raise AttributeError(\"Function name not set\")\n        # make sure that the function has a docstring\n        if not self.docstring:\n            raise AttributeError(\"Function docstring not set\")\n        # make sure that the function has a signature\n        if not self.signature:\n            raise AttributeError(\"Function signature not set\")\n        # make sure that the function has a docstring\n        if not self.docstring:\n            raise AttributeError(\"Function docstring not set\")\n        # make sure that the function has a signature\n        if not self.signature:\n            raise AttributeError(\"Function signature not set\")\n        # make sure that the function has a docstring\n        if not self.docstring:\n            raise AttributeError(\"Function docstring not set\")\n        # make sure that the function has a signature\n        if not self.", "solution": "\n        # note, callable is valid for Python 2 and Python 3.2 onwards but\n        # not inbetween\n        if not callable(self._function):\n            raise RuntimeError(\n                \"provided function '{0}' is not callable\".\n                format(str(self._function)))\n        from inspect import getargspec\n        arg_info = getargspec(self._function)\n        if len(arg_info.args) != 1:\n            print str(arg_info)\n            raise RuntimeError(\n                \"provided function should have one argument but found \"\n                \"{0}\".format(len(arg_info.args)))"}
{"prompt": "def span(self, index):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"", "output": "\n        return self.spans[index]\n\n    def spans(self):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"\n        return self.spans\n\n    def span(self, idx):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"\n        return self.spans[idx]\n\n    def spans(self):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"\n        return self.spans\n\n    def span(self, idx):\n        \"\"\"Give the range of possible values in a tuple\n        Useful for mnemonic and explanation\n        \"\"\"\n        return self.", "solution": "\n        lower = self.value0+sum(1<<x for x in self.extraTable[:index])\n        upper = lower+(1<<self.extraTable[index])\n        return lower, upper-1"}
{"prompt": "def paid_invoices_by_date(request, form):\n    \"\"\" Shows the number of paid invoices containing given products or\n    categories per day. \"\"\"", "output": "\n    paid_invoices = Invoice.objects.filter(\n        paid=True,\n        date_paid__isnull=False,\n        date_paid__gte=form.cleaned_data['date_from'],\n        date_paid__lte=form.cleaned_data['date_to'],\n    ).order_by('date_paid')\n    paid_invoices_count = paid_invoices.count()\n    paid_invoices_count_per_day = paid_invoices.aggregate(\n        paid_count=Count('date_paid'),\n    )['paid_count']\n    paid_invoices_count_per_day_percentage = paid_invoices_count / ", "solution": "\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )"}
{"prompt": "def get(m, k, default=None):\n    \"\"\"Return the value of k in m. Return default if k not found in m.\"\"\"", "output": "\n    return m.get(k, default)\n\n\ndef pop(m, k, default=None):\n    \"\"\"Remove and return item from dict m with key k. Raise KeyError if k not found.\"\"\"\n    return m.pop(k, default)\n\n\ndef popitem(d, default=None):\n    \"\"\"Remove and return an item from dict d and return its value. Raise KeyError if key not found.\"\"\"\n    return d.popitem()\n\n\ndef setdefault(m, k, default=None):\n    \"\"\"Set default value for key k in m and return its value.\"\"\"\n    return m.setdefault(k,default)\n\n\ndef update(m, **kw):\n    \"\"\"Update m with new key/value pairs from kw and return the result.\n\n    All keys in m that are not specified in ", "solution": "\n    if isinstance(m, IAssociative):\n        return m.entry(k, default=default)\n\n    try:\n        return m[k]\n    except (KeyError, IndexError, TypeError) as e:\n        logger.debug(\"Ignored %s: %s\", type(e).__name__, e)\n        return default"}
{"prompt": "def _send_coroutine():\n    \"\"\"\n    Creates a running coroutine to receive message instances and send\n    them in a futures executor.\n    \"\"\"", "output": "\n    coro = asyncio.get_event_loop().create_future()\n    asyncio.ensure_future(coro)\n    return coro\n\n\nclass Message(object):\n    \"\"\"\n    Represents a message sent from the client to the server.\n    \"\"\"\n    def __init__(self, message, connection):\n        \"\"\"\n        Initializes the message object.\n\n        :param message: The message sent from the client to the server.\n        :type message: :class:`Message`\n        :param connection: The connection object used to send messages to the server.\n        :type connection: :class:`Connection`\n        \"\"\"\n        self._message = message\n        self._connection = connection\n        self._message_id = message.message_id\n        self._message_type = message.message_type\n        self._message_data = message.message_data\n        self._message_data_type = message.message_data_type\n        self._", "solution": "\n    with PoolExecutor() as executor:\n        while True:\n            msg = yield\n            future = executor.submit(msg.send)\n            future.add_done_callback(_exception_handler)"}
{"prompt": "def cb_list_groups(self, *args, **kwargs):\n        \"\"\"List all the check groups that pylint knows about\n\n        These should be useful to know what check groups someone can disable\n        or enable.\n        \"\"\"", "output": "\n        return self.pylint_manager.list_groups()\n\n    def cb_list_rules(self, *args, **kwargs):\n        \"\"\"List all the check rules that pylint knows about\n\n        These should be useful to know what check rules someone can disable\n        or enable.\n        \"\"\"\n        return self.pylint_manager.list_rules()\n\n    def cb_list_messages(self, *args, **kwargs):\n        \"\"\"List all the messages that pylint knows about\n\n        These should be useful to know what messages someone can disable\n        or enable.\n        \"\"\"\n        return self.pylint_manager.list_messages()\n\n    def cb_list_options(self, *args, **kwargs):\n        \"\"\"List all the options that pylint knows about\n\n        These ", "solution": "\n        for check in self.linter.get_checker_names():\n            print(check)\n        sys.exit(0)"}
{"prompt": "def add_column(self, name, data):\n        \"\"\"Add a column to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"", "output": "\n        self._add_columns([name], data)\n\n    def add_columns(self, name, data):\n        \"\"\"Add multiple columns to the DataFrame\n\n        :param str name: name of column\n        :param data: numpy array with the data\n        \"\"\"\n        self._add_columns([name], data)\n\n    def _add_columns(self, names, data):\n        \"\"\"Add multiple columns to the DataFrame\n\n        :param names: list of column names\n        :param data: numpy array with the data\n        \"\"\"\n        for n in names:\n            self.columns[n] = data\n\n    def drop(self, name, axis=0):\n        \"\"\"Drop a column from the DataFrame\n\n        :param str name: name of column\n        :param int axis: axis to drop the column from\n        \"\"\"\n        self._drop_columns([name], axis)\n\n    def drops(self, names, axis=", "solution": "\n        # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type)\n        # self._length = len(data)\n        # if self._length_unfiltered is None:\n        #     self._length_unfiltered = len(data)\n        #     self._length_original = len(data)\n        #     self._index_end = self._length_unfiltered\n        super(DataFrameArrays, self).add_column(name, data)\n        self._length_unfiltered = int(round(self._length_original * self._active_fraction))"}
{"prompt": "def get_objects_from_form(variant_ids, form_fields, object_type):\n    \"\"\"Extract the objects to be saved in the clinvar database collection.\n       object_type param specifies if these objects are variant or casedata objects\n\n       Args:\n        variant_ids(list): list of database variant ids\n        form_fields(dict): it's the submission form dictionary. Keys have the same names as CLINVAR_HEADER and CASEDATA_HEADER\n        object_type(str): either 'variant' or 'case_data'\n\n       Returns:\n        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'\n    \"\"\"", "output": "\n    submission_objects = []\n    for variant_id in variant_ids:\n        submission_objects.append(get_object_from_form(variant_id, form_fields, object_type))\n    return submission_objects\n\n\ndef extract_clinvar_objects(submission_objects):\n    \"\"\"Extract the clinvar objects from the submission objects and return them in a list.\n\n       Args:\n        submission_objects(list): list of submission objects of either type 'variant' or 'casedata'\n\n       Returns:\n        clinvar_objects(list): list of clinvar objects of either type 'variant' or 'casedata'\n    \"\"\"\n    clinvar_objects = []\n    for submission_object in submission_objects:\n", "solution": "\n    submission_fields = []\n    if object_type == 'variant':\n        submission_fields = CLINVAR_HEADER\n    else: #collect casedata objects\n        submission_fields = CASEDATA_HEADER\n\n    # A list of objects (variants of casedata info) to be saved into clinvar database collection\n    submission_objects = []\n\n    # Loop over the form fields and collect the data:\n    for variant_id in variant_ids: # loop over the variants\n\n        subm_obj = {} # A new submission object for each\n\n        # Don't included casedata for a variant unless specified by user\n        if object_type == 'casedata' and 'casedata_'+variant_id not in form_fields:\n            continue\n\n        subm_obj['csv_type'] = object_type\n        subm_obj['case_id'] = form_fields.get('case_id')\n        subm_obj['category'] = form_fields.get('category@'+variant_id)\n\n        for key, values in submission_fields.items(): # loop over the form info fields\n            field_value = form_fields.get(key+'@'+variant_id)\n            if field_value and not field_value == '-':\n                if key == 'ref_seq': # split this field into\n                    refseq_raw = field_value.split('|')\n                    subm_obj['ref_seq'] = refseq_raw[0]\n                    subm_obj['hgvs'] = refseq_raw[1]\n                else:\n                    subm_obj[key] = field_value\n\n        # Create a unique ID for the database\n        # For casedata : = caseID_sampleID_variantID\n        # For variants : ID = caseID_variantID\n        if object_type == 'casedata':\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id + '_' + str(subm_obj['individual_id'])\n        else:\n            subm_obj['_id'] = str(subm_obj['case_id']) + '_' + variant_id\n\n        submission_objects.append(subm_obj)\n\n    return submission_objects"}
{"prompt": "def evaluate(self, x, y=None, batch_size=32):\n        \"\"\"\n        Evaluate a model on a given dataset in distributed mode.\n\n        # Arguments\n        x: Input data. A Numpy array or RDD of Sample.\n        y: Labels. A Numpy array. Default is None if x is already RDD of Sample.\n        batch_size: Number of samples per gradient update.\n        \"\"\"", "output": "\n        x, y = to_numpy_or_python(x)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)\n        x, y = to_numpy_or_python(y), to_numpy_or_python(y)", "solution": "\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            evaluation_data = to_sample_rdd(x, y)\n        elif isinstance(x, RDD) and not y:\n            evaluation_data = x\n        else:\n            raise TypeError(\"Unsupported evaluation data type: %s\" % type(x))\n        return callBigDlFunc(self.bigdl_type, \"evaluate\",\n                             self.value,\n                             evaluation_data,\n                             batch_size)"}
{"prompt": "def _loop_to_py_ast(ctx: GeneratorContext, node: Loop) -> GeneratedPyAST:\n    \"\"\"Return a Python AST Node for a `loop*` expression.\"\"\"", "output": "\n    loop_node = GeneratedPyAST(\n        node.lineno,\n        node.col_offset,\n        node.end_lineno,\n        node.end_col_offset,\n        node.type,\n        node.name,\n        node.doc,\n        node.is_async,\n        node.is_generator,\n        node.is_async_generator,\n        node.is_async_def,\n        node.is_async_for,\n        node.is_async_with,\n        node.is_async_withitems,\n        node.is_async_withitemgetter,\n        node.is_async_withattr,\n        node.is_async_withitems,\n        node.is_async_withitemgetter,\n        ", "solution": "\n    assert node.op == NodeOp.LOOP\n\n    with ctx.new_symbol_table(\"loop\"):\n        binding_names = []\n        init_bindings: List[ast.AST] = []\n        for binding in node.bindings:\n            init_node = binding.init\n            assert init_node is not None\n            init_ast = gen_py_ast(ctx, init_node)\n            init_bindings.extend(init_ast.dependencies)\n            binding_name = genname(munge(binding.name))\n            binding_names.append(binding_name)\n            init_bindings.append(\n                ast.Assign(\n                    targets=[ast.Name(id=binding_name, ctx=ast.Store())],\n                    value=init_ast.node,\n                )\n            )\n            ctx.symbol_table.new_symbol(\n                sym.symbol(binding.name), binding_name, LocalType.LOOP\n            )\n\n        loop_result_name = genname(\"loop\")\n        with ctx.new_recur_point(\n            node.loop_id, RecurType.LOOP, binding_names=binding_names\n        ):\n            loop_body_ast: List[ast.AST] = []\n            body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n            loop_body_ast.extend(body_ast.dependencies)\n            loop_body_ast.append(\n                ast.Assign(\n                    targets=[ast.Name(id=loop_result_name, ctx=ast.Store())],\n                    value=body_ast.node,\n                )\n            )\n            loop_body_ast.append(ast.Break())\n\n            return GeneratedPyAST(\n                node=_load_attr(loop_result_name),\n                dependencies=list(\n                    chain(\n                        [\n                            ast.Assign(\n                                targets=[\n                                    ast.Name(id=loop_result_name, ctx=ast.Store())\n                                ],\n                                value=ast.NameConstant(None),\n                            )\n                        ],\n                        init_bindings,\n                        [\n                            ast.While(\n                                test=ast.NameConstant(True),\n                                body=loop_body_ast,\n                                orelse=[],\n                            )\n                        ],\n                    )\n                ),\n            )"}
{"prompt": "def get_model(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"", "output": "\n        model_name = model_name.replace('.', '/')\n        if model_name in self.models:\n            return self.models[model_name]\n        else:\n            raise KeyError(\"Model %s not found in %s\" % (model_name, self.models))\n\n    def get_model_by_name(self, model_name):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"\n        model_name = model_name.replace('.', '/')\n        if model_name in self.models:\n            return self.models[model_name]\n        else:\n            raise KeyError(\"Model %s not found in %s\" % (model_name, self.models))\n\n    def list_models(self):\n        \"\"\"\n        TODO: Need to validate model name has 2x '.' chars\n        \"\"\"\n        models = []\n        for model in self.models.values():", "solution": "\n        klass = None\n        try:\n            module_name, class_name = model_name.rsplit('.', 1)\n            mod = __import__(module_name, fromlist=[class_name])\n            klass = getattr(mod, class_name)\n        except ImportError, e:\n            self.error('Cannot find app %s %s' % (model_name, e))\n\n        return klass"}
{"prompt": "def compare(self, cost_map):\n        \"\"\"\n        Compare to previous records and return whether the given cost is a new best.\n        :return: True if the given cost is a new best\n        \"\"\"", "output": "\n        best_cost = cost_map.best_cost\n        best_record = cost_map.best_record\n        best_record = best_record[0]\n        best_record = best_record[1]\n        best_record = best_record[2]\n        best_record = best_record[3]\n        best_record = best_record[4]\n        best_record = best_record[5]\n        best_record = best_record[6]\n        best_record = best_record[7]\n        best_record = best_record[8]\n        best_record = best_record[9]\n        best_record = best_record[10]\n        best_record = best_record[11]\n        best_record = best_record", "solution": "\n        cri_val = cost_map[self._criteria]\n        if self._best_criteria is None:\n            self._best_criteria = cri_val\n            return True\n        else:\n            if self._smaller_is_better and cri_val < self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            elif not self._smaller_is_better and cri_val > self._best_criteria:\n                self._best_criteria = cri_val\n                return True\n            else:\n                return False"}
{"prompt": "def execute(self):\n        \"\"\"\n        Execute R script\n        \"\"\"", "output": "\n        script = self.script\n        script = script.replace(\"\\\\\", \"/\")\n        script = script.replace(\"'\", \"\")\n        script = script.replace(\"\\n\", \"\")\n        script = script.replace(\"\\r\", \"\")\n        script = script.replace(\"\\t\", \"\")\n        script = script.replace(\"\\n\", \"\")\n        script = script.replace(\"\\r\", \"\")\n        script = script.replace(\"\\t\", \"\")\n        script = script.replace(\"\\n\", \"\")\n        script = script.replace(\"\\r\", \"\")\n        script = script.replace(\"\\t\", \"\")\n        script = script.replace(\"\\n\", \"\")\n        script = script.replace(\"\\r\", \"\")\n        script = script.replace(\"\\t\", \"\")\n        script = script.replace(\"\\n\", \"\")\n        ", "solution": "\n        rprocess = OrderedDict()\n        commands = OrderedDict([\n            (self.file, ['Rscript', self.file] + self.cmd),\n        ])\n        for cmd_name, cmd in commands.items():\n            rprocess[cmd_name] = self.run_command_under_r_root(cmd)\n        \n        return self.decode_cmd_out(completed_cmd=rprocess[self.file])"}
{"prompt": "def _duplicated_isinstance_types(node):\n        \"\"\"Get the duplicated types from the underlying isinstance calls.\n\n        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.\n        :returns: Dictionary of the comparison objects from the isinstance calls,\n                  to duplicate values from consecutive calls.\n        :rtype: dict\n        \"\"\"", "output": "\n        comparisons = {}\n        for call in node.calls:\n            if isinstance(call, astroid.BoolOp):\n                comparisons[call.op] = call\n            elif isinstance(call, astroid.UnaryOp):\n                comparisons[call.op] = call.operand\n            elif isinstance(call, astroid.BinOp):\n                comparisons[call.op] = call.left\n                comparisons[call.op] = call.right\n        return comparisons\n\n    def _duplicated_isinstance_calls(node):\n        \"\"\"Get the duplicated isinstance calls from the underlying isinstance calls.\n\n        :param astroid.BoolOp node: Node which should contain a bunch of isinstance calls.\n        :returns: Dictionary of the comparison objects from the ", "solution": "\n        duplicated_objects = set()\n        all_types = collections.defaultdict(set)\n\n        for call in node.values:\n            if not isinstance(call, astroid.Call) or len(call.args) != 2:\n                continue\n\n            inferred = utils.safe_infer(call.func)\n            if not inferred or not utils.is_builtin_object(inferred):\n                continue\n\n            if inferred.name != \"isinstance\":\n                continue\n\n            isinstance_object = call.args[0].as_string()\n            isinstance_types = call.args[1]\n\n            if isinstance_object in all_types:\n                duplicated_objects.add(isinstance_object)\n\n            if isinstance(isinstance_types, astroid.Tuple):\n                elems = [\n                    class_type.as_string() for class_type in isinstance_types.itered()\n                ]\n            else:\n                elems = [isinstance_types.as_string()]\n            all_types[isinstance_object].update(elems)\n\n        # Remove all keys which not duplicated\n        return {\n            key: value for key, value in all_types.items() if key in duplicated_objects\n        }"}
{"prompt": "def digest(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_DigestInit/C_Digest\n\n        :param data: the data to be digested\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the digesting mechanism to be used\n          (use `MechanismSHA1` for `CKM_SHA_1`)\n        :type mecha: :class:`Mechanism`\n        :return: the computed digest\n        :rtype: list of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          You can easly convert it to a binary string with:\n          ``bytes(ckbytelistDigest)``\n          or, for Python 2:\n          ``''.join(chr(i) for i in ckbytelistDigest)``\n\n        \"\"\"", "output": "\n        \"\"\"\n        digest = mecha.digest(data)\n        return digest\n\n    def verify(self, data, mecha=MechanismSHA1):\n        \"\"\"\n        C_VerifyInit/C_Verify\n\n        :param data: the data to be verified\n        :type data:  (binary) sring or list/tuple of bytes\n        :param mecha: the verification/verification mechanism to be used\n          (use `MechanismSHA1` for `CKM_VERIFY_01`)\n        :type mecha: :class:`Mechanism`\n        :return: the verified data\n        :rtype: (binary) sring or list/tuple of bytes\n\n        :note: the returned value is an istance of :class:`ckbytelist`.\n          ", "solution": "\n        digest = ckbytelist()\n        m = mecha.to_native()\n        data1 = ckbytelist(data)\n        rv = self.lib.C_DigestInit(self.session, m)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # first call get digest size\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        # second call get actual digest data\n        rv = self.lib.C_Digest(self.session, data1, digest)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)\n        return digest"}
{"prompt": "def file_hash(self, source, cu):\n        \"\"\"Compute a hash that changes if the file needs to be re-reported.\"\"\"", "output": "\n        hasher = hashlib.sha1()\n        hasher.update(source)\n        hasher.update(cu)\n        return hasher.hexdigest()\n\n    def report_file(self, source, cu, hash):\n        \"\"\"Report the file as changed.\"\"\"\n        self.report(\n            source=self.file_hash(source, cu),\n            cu=cu,\n            hash=hash,\n            changed=True,\n        )\n\n    def report_file_changed(self, source, cu, hash):\n        \"\"\"Report the file as changed.\"\"\"\n        self.report(\n            source=self.file_hash(source, cu),\n            cu=cu,\n            hash=hash,\n            changed=True,\n        )", "solution": "\n        m = Hasher()\n        m.update(source)\n        self.coverage.data.add_to_hash(cu.filename, m)\n        return m.digest()"}
{"prompt": "def voronoi(script, hole_num=50, target_layer=None, sample_layer=None, thickness=0.5, backward=True):\n    \"\"\" Turn a model into a surface with Voronoi style holes in it\n\n    References:\n    http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html\n    http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html\n\n    Requires FilterScript object\n\n    Args:\n        script: the FilterScript object to write the filter to. Does not\n            work with a script filename.\n\n    Layer stack:\n        No impacts\n\n    MeshLab versions:\n        2016.12\n        1.3.4BETA\n    \"\"\"", "output": "    Arguments:\n        hole_num: number of Voronoi holes to create\n        target_layer: layer to place the Voronoi holes in\n        sample_layer: layer to place the Voronoi holes on\n        thickness: thickness of the Voronoi holes (in units of pixels)\n        backward: whether to reverse the order of the Voronoi holes\n\n    Returns:\n        None\n\n    \"\"\"\n    hole_layer = script.add_layer()\n    hole_layer.set_name(\"Voronoi Holes\")\n    hole_layer.set_type(\"Voronoi\")\n    hole_layer.set_", "solution": "\n\n    if target_layer is None:\n        target_layer = script.current_layer()\n    if sample_layer is None:\n        # Current layer is currently not changed after poisson_disk is run\n        sampling.poisson_disk(script, sample_num=hole_num)\n        sample_layer = script.last_layer()\n\n    vert_color.voronoi(script, target_layer=target_layer, source_layer=sample_layer, backward=backward)\n    select.vert_quality(script, min_quality=0.0, max_quality=thickness)\n    if backward:\n        select.invert(script)\n    delete.selected(script)\n    smooth.laplacian(script, iterations=3)\n\n    return None"}
{"prompt": "def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"", "output": "\n        indices_dict = collections.OrderedDict(\n            sorted(indices.items()))\n        return indices_dict.keys()\n\n    def global_iterator_to_indices(self, global_iterator):\n        \"\"\"\n        Transform a global iterator integer to a dictionary of indices.\n\n        Inverse of indices_to_global_iterator().\n        \"\"\"\n        indices_dict = collections.OrderedDict()\n        for index in global_iterator:\n            indices_dict[index] = index\n        return indices_dict\n\n    def indices_to_global_iterator(self, indices):\n        \"\"\"\n        Transform a dictionary of indices to a global iterator integer.\n\n        Inverse of global_iterator_to_indices().\n        \"\"\"\n        indices_dict = collections.OrderedDict(\n            sorted(indices.items()))\n        return indice", "solution": "\n        global_iterator = self.subs_consts(self.global_iterator().subs(indices))\n        return global_iterator"}
{"prompt": "def _value(self, dtype=None, name=None, as_ref=False):  # pylint: disable=g-doc-args\n  \"\"\"Get the value returned by `tf.convert_to_tensor(distribution)`.\n\n  Note: this function may mutate the distribution instance state by caching\n  the concretized `Tensor` value.\n\n  Args:\n    dtype: Must return a `Tensor` with the given `dtype` if specified.\n    name: If the conversion function creates a new `Tensor`, it should use the\n      given `name` if specified.\n    as_ref: `as_ref` is true, the function must return a `Tensor` reference,\n      such as a `Variable`.\n  Returns:\n    concretized_distribution_value: `Tensor` identical to\n    `tf.convert_to_tensor(distribution)`.\n\n  #### Examples\n\n  ```python\n  tfd = tfp.distributions\n  x = tfd.Normal(0.5, 1).set_tensor_conversion(tfd.Distribution.mean)\n\n  x._value()\n  # ==> tf.convert_to_tensor(x) ==> 0.5\n\n  x._value() + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n\n  x + 2\n  # ==> tf.convert_to_tensor(x) + 2. ==> 2.5\n  ```\n\n  \"\"\"", "output": "\n      `distribution` except for its `dtype` and `name`.\n  \"\"\"\n  concretized_distribution_value = self._concretized_distribution_tensor\n  if dtype is not None:\n    concretized_distribution_value = concretized_distribution_value.astype(\n        dtype)\n  if name is not None:\n    concretized_distribution_value = concretized_distribution_value.op.name\n  if as_ref:\n    return concretized_distribution_value\n  return concretized_distribution_value\n\n  def _concretized_distribution_tensor(self):\n    \"\"\"Concretized `Tensor` of the distribution instance.\n\n    Returns:\n      concretized_distribution_value: `Tensor` ", "solution": "\n  # pylint: disable=protected-access\n  if as_ref:\n    raise NotImplementedError(\n        'Cannot convert a `Distribution` to a reference '\n        '(e.g., `tf.Variable`).')\n  if self._concrete_value is None:\n    if self._convert_to_tensor_fn is None:\n      raise NotImplementedError(\n          'Failed to convert object of type {} to Tensor. Contents: {}. '\n          'Call `distribution.set_tensor_conversion(lambda self: ...)` to '\n          'enable `tf.convert_to_tensor` capability. For example: '\n          '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`'\n          ' results in `tf.convert_to_tensor(x)` being identical to '\n          '`x.mean()`.'.format(type(self), self))\n    with self._name_scope('value'):\n      self._concrete_value = (self._convert_to_tensor_fn(self)\n                              if callable(self._convert_to_tensor_fn)\n                              else self._convert_to_tensor_fn)\n      if not tf.is_tensor(self._concrete_value):\n        self._concrete_value = tfd._convert_to_tensor(\n            value=self._concrete_value,\n            name=name or 'concrete_value',\n            dtype=dtype,\n            dtype_hint=self.dtype)\n  return self._concrete_value"}
{"prompt": "def url_equal(first, second, ignore_scheme=False, ignore_netloc=False, ignore_path=False, ignore_params=False,\n              ignore_query=False, ignore_fragment=False):\n    \"\"\"\n    Compare two URLs and return True if they are equal, some parts of the URLs can be ignored\n    :param first: URL\n    :param second: URL\n    :param ignore_scheme: ignore the scheme\n    :param ignore_netloc: ignore the netloc\n    :param ignore_path: ignore the path\n    :param ignore_params: ignore the params\n    :param ignore_query: ignore the query string\n    :param ignore_fragment: ignore the fragment\n    :return: result of comparison\n    \"\"\"", "output": "\n    parsed_first = urlparse(first)\n    parsed_second = urlparse(second)\n    parsed_first.scheme = parsed_second.scheme\n    parsed_first.netloc = parsed_second.netloc\n    parsed_first.path = parsed_second.path\n    parsed_first.params = parsed_second.params\n    parsed_first.query = parsed_second.query\n    parsed_first.fragment = parsed_second.fragment\n    parsed_first.fragment = parsed_first.fragment.lstrip('/')\n    parsed_first.fragment = parsed_first.fragment.rstrip('/')\n    parsed_first.fragment = parsed_first.fragment.lstrip('/')\n    parsed_first.fragment = parsed_first.fragment.rstrip('/')\n    parsed_first.fragment = parsed_first.", "solution": "\n    # <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n\n    firstp = urlparse(first)\n    secondp = urlparse(second)\n\n    return ((firstp.scheme == secondp.scheme or ignore_scheme) and\n            (firstp.netloc == secondp.netloc or ignore_netloc) and\n            (firstp.path == secondp.path or ignore_path) and\n            (firstp.params == secondp.params or ignore_params) and\n            (firstp.query == secondp.query or ignore_query) and\n            (firstp.fragment == secondp.fragment or ignore_fragment))"}
{"prompt": "def get_users_for_course(self, course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given course id.\n        \"\"\"", "output": "\n        endpoint = 'courses/%s/users' % course_id\n        return self.get(endpoint, params=params)\n\n    def get_users_for_group(self, group_id, params={}):\n        \"\"\"\n        Returns a list of users for the given group id.\n        \"\"\"\n        endpoint = 'groups/%s/users' % group_id\n        return self.get(endpoint, params=params)\n\n    def get_users_for_coursegroup(self, coursegroup_id, params={}):\n        \"\"\"\n        Returns a list of users for the given coursegroup id.\n        \"\"\"\n        endpoint = 'coursegroups/%s/users' % coursegroup_id\n        return self.get(endpoint, params=params)\n\n    def get_users_for_groupgroup(self, groupgroup_id, params={}):\n        \"\"\"\n        Returns a list of users for the given groupgroup id.\n        \"\"\"\n        endpoint = 'group", "solution": "\n        url = COURSES_API.format(course_id) + \"/users\"\n        data = self._get_paged_resource(url, params=params)\n        users = []\n        for datum in data:\n            users.append(CanvasUser(data=datum))\n        return users"}
{"prompt": "def make_signing_service(config, entity_id):\n    \"\"\"\n    Given configuration initiate a SigningService instance\n\n    :param config: The signing service configuration\n    :param entity_id: The entity identifier\n    :return: A SigningService instance\n    \"\"\"", "output": "\n    return SigningService(config=config, entity_id=entity_id)\n\n\ndef signing_service_from_config(\n    config, entity_id=None, service_name=None, service_version=None\n):\n    \"\"\"\n    Given configuration initiate a SigningService instance\n\n    :param config: The signing service configuration\n    :param entity_id: The entity identifier\n    :param service_name: The service name\n    :param service_version: The service version\n    :return: A SigningService instance\n    \"\"\"\n    return SigningService.from_config(\n        config=config,\n        entity_id=entity_id,\n        service_name=service_name,\n        service_version=service_version,\n    )\n\n\ndef signing_service_from_config_with_default_version(\n    config, entity", "solution": "\n\n    _args = dict([(k, v) for k, v in config.items() if k in KJ_SPECS])\n    _kj = init_key_jar(**_args)\n\n    if config['type'] == 'internal':\n        signer = InternalSigningService(entity_id, _kj)\n    elif config['type'] == 'web':\n        _kj.issuer_keys[config['iss']] = _kj.issuer_keys['']\n        del _kj.issuer_keys['']\n        signer = WebSigningServiceClient(config['iss'], config['url'],\n                                         entity_id, _kj)\n    else:\n        raise ValueError('Unknown signer type: {}'.format(config['type']))\n\n    return signer"}
{"prompt": "def delete(filename, delete_v1=True, delete_v2=True):\n    \"\"\"Remove tags from a file.\n\n    Keyword arguments:\n\n    * delete_v1 -- delete any ID3v1 tag\n    * delete_v2 -- delete any ID3v2 tag\n    \"\"\"", "output": "\n    tags = []\n    if delete_v1:\n        tags.append(\"ID3v1\")\n    if delete_v2:\n        tags.append(\"ID3v2\")\n    tags.extend(get_tags(filename))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    tags = list(set(tags))\n    tags = list(filter(None, tags))\n    ", "solution": "\n\n    f = open(filename, 'rb+')\n\n    if delete_v1:\n        try:\n            f.seek(-128, 2)\n        except IOError:\n            pass\n        else:\n            if f.read(3) == b'TAG':\n                f.seek(-128, 2)\n                f.truncate()\n\n    # technically an insize=0 tag is invalid, but we delete it anyway\n    # (primarily because we used to write it)\n    if delete_v2:\n        f.seek(0, 0)\n        idata = f.read(10)\n        try:\n            id3, vmaj, vrev, flags, insize = unpack('>3sBBB4s', idata)\n        except struct.error:\n            id3, insize = b'', -1\n        insize = BitPaddedInt(insize)\n        if id3 == b'ID3' and insize >= 0:\n            delete_bytes(f, insize + 10, 0)"}
{"prompt": "def state(self):\n        \"\"\"Return internal state, useful for testing.\"\"\"", "output": "\n        return self._state\n\n    def copy(self):\n        \"\"\"Return copy of internal state.\"\"\"\n        return self._state.copy()\n\n    def copy_state(self):\n        \"\"\"Return copy of internal state.\"\"\"\n        return self._state.copy()\n\n    def copy_with_state(self, state):\n        \"\"\"Return copy of internal state with new state.\"\"\"\n        return self._state.copy_with(state)\n\n    def copy_with_states(self, states):\n        \"\"\"Return copy of internal state with new states.\"\"\"\n        return self._state.copy_with_states(states)\n\n    def copy_states(self, states):\n        \"\"\"Return copy of internal states with new states.\"\"\"\n        return self._state.copy_states(states)\n\n    def pop(self, state=None):\n        \"\"\"Remove and return state from internal state.\"\"\"\n        if state is None:\n            return self._state.pop(self.", "solution": "\n        return {'c': self.c, 's0': self.s0, 's1': self.s1, 's2': self.s2}"}
{"prompt": "def get_mint_tree(tokens_stream):\n    \"\"\"\n    This function is wrapper to normal parsers (tag_parser, block_parser, etc.).\n    Returns mint tree.\n    \"\"\"", "output": "\n    tree = MintTree()\n    tree.root_node = MintTreeNode()\n    tree.root_node.is_root = True\n    tree.root_node.is_leaf = False\n    tree.root_node.is_leaf = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True\n    tree.root_node.is_root = True", "solution": "\n    smart_stack = RecursiveStack()\n    block_parser.parse(tokens_stream, smart_stack)\n    return MintTemplate(body=smart_stack.stack)"}
{"prompt": "def from_yaml(cls, defaults, **kwargs):\n        \"\"\"Creates a new instance of a rule by merging two dictionaries.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"", "output": "\n        rule = cls()\n        rule.update(defaults, **kwargs)\n        return rule\n\n    @classmethod\n    def from_file(cls, path, **kwargs):\n        \"\"\"Creates a new instance of a rule by reading from a file.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"\n        rule = cls()\n        rule.update(yaml.safe_load(open(path)), **kwargs)\n        return rule\n\n    @classmethod\n    def from_string(cls, string, **kwargs):\n        \"\"\"Creates a new instance of a rule by parsing a string.\n\n        This allows for independant configuration files to be merged\n        into the defaults.\"\"\"\n        rule = cls()\n        rule.update(yaml.safe_load(string), **kwargs)\n        return rule\n\n    @classmethod\n", "solution": "\n        # TODO: I hate myself for this. Fix it later mmkay?\n        if \"token\" not in defaults:\n            kwargs[\"token\"] = None\n\n        defaults = copy.deepcopy(defaults)\n        return cls(\n            defaults=defaults,\n            token=kwargs.pop(\"token\"),\n            directory=kwargs.pop(\"directory\"),\n            **kwargs\n        )"}
{"prompt": "def getPhysicalInterface(self, physicalInterfaceId, draft=False):\n        \"\"\"\n        Get a physical interface.\n        Parameters:\n          - physicalInterfaceId (string)\n          - draft (boolean)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        return RemoteObject._RemoteObjectBase_getPhysicalInterface(self, physicalInterfaceId, draft)\n    getPhysicalInterface.methods[\"getPhysicalInterface\"] = RemoteObject._RemoteObjectBase_getPhysicalInterface\nclass RemoteObject(object):\n    \"\"\"\n    RemoteObject is a base class for all remote object types.\n    \"\"\"\n    __swig_setmethods__ = {}\n    for _s in [RemoteObject]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))\n    __setattr__ = lambda self, name, value: _swig_setattr(self, RemoteObject, name, value)\n    __swig_getmethods__ = {}\n    for _s in [Remote", "solution": "\n        if draft:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"/draft\", physicalInterfaceId)\n        else:\n            req = ApiClient.onePhysicalInterfaceUrl % (self.host, \"\", physicalInterfaceId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"physical interface retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting a physical interface\", resp)\n        return resp.json()"}
{"prompt": "def get(cls):\n        \"\"\"\n        Get the status while testing for an IP or domain.\n\n        .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"", "output": "\n        if cls.to_test is None:\n            return None\n        return cls._status[cls.to_test]\n\n    @classmethod\n    def set(cls, value):\n        \"\"\"\n        Set the status of the test to :code:`value`.\n\n       .. note::\n            We consider that the domain or IP we are currently testing\n            is into :code:`PyFunceble.INTERN[\"to_test\"]`.\n        \"\"\"\n        cls._status[cls.to_test] = value\n\n    @classmethod\n    def clear_all(cls):\n        \"\"\"\n        Clear the status of all tests.\n        \"\"\"\n        cls._status = {}\n\n    @classmethod\n    def clear_status(cls):\n        \"\"\"\n        Clear the status of the test to :code:`PyFunceble.INTERN[\"to_test\"]", "solution": "\n\n        if \"to_test\" in PyFunceble.INTERN and PyFunceble.INTERN[\"to_test\"]:\n            expiration_date = ExpirationDate().get()\n\n            if expiration_date is False:\n                return cls.handle(status=\"invalid\")\n\n            if expiration_date == PyFunceble.STATUS[\"official\"][\"up\"]:\n                return expiration_date, \"WHOIS\"\n\n            return cls.handle(status=\"inactive\")\n\n        raise NotImplementedError(\"We expect `INTERN['to_test']` to be set.\")"}
{"prompt": "def execute(option):\n    \"\"\"A script that melody calls with each valid set of options. This\n    script runs the required code and returns the results.\"\"\"", "output": "\n    options = option.split(',')\n    notes = []\n    for option in options:\n        note = option.split('=')\n        notes.append(note[0])\n    notes.sort()\n    notes = notes[:]\n    notes.reverse()\n    notes.insert(0, 0)\n    notes.append(len(notes))\n    notes.reverse()\n    notes.insert(0, 0)\n    notes.append(len(notes))\n    notes.reverse()\n    notes.insert(0, 0)\n    notes.append(len(notes))\n    notes.reverse()\n    notes.insert(0, 0)\n    notes.append(len(notes))\n    notes.reverse()\n    notes.insert(0, ", "solution": "\n\n    namelist_option = []\n    makefile_option = []\n    flags = \"\"\n    for entry in option:\n        key = entry.keys()[0]\n        if key == \"Problem Size\":\n            namelist_option.append({\"SIZE\": entry[key]})\n        elif key == \"F90\":\n            makefile_option.append(entry)\n        else:\n            flags += entry[key] + \" \"\n    makefile_option.append({\"F90FLAGS\": flags})\n\n    namelist = create_input(namelist_option, \"namelist\",\n                            template_location=\"templates\")\n\n    makefile_include = create_input(makefile_option, \"Makefile.include\",\n                                    template_location=\"templates\")\n\n    benchmark_base = \"shallow\"\n\n    # save the input files in the appropriate place\n    location = benchmark_base + \"/original/namelist\"\n    my_file = open(location, 'w')\n    my_file.write(namelist)\n    my_file.flush()\n\n    location = benchmark_base + \"/common/Makefile.include\"\n    my_file = open(location, 'w')\n    my_file.write(makefile_include)\n    my_file.flush()\n\n    # compile shallow if required\n    base_path = benchmark_base + \"/original\"\n    import subprocess\n    make_process = subprocess.Popen([\"make\", \"clean\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    make_process = subprocess.Popen([\"make\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n\n    # run shallow\n    make_process = subprocess.Popen([\"./shallow_base\"], cwd=base_path,\n                                    stderr=subprocess.PIPE,\n                                    stdout=subprocess.PIPE)\n    if make_process.wait() != 0:\n        return False, []\n    # _ = make_process.stderr.read()\n    stdout = make_process.stdout.read()\n\n    # determine if the results are correct. We will need to look at\n    # the results from stdout but for the moment we assume they are\n    # correct\n\n    # extract the required outputs\n    for line in stdout.split(\"\\n\"):\n        if \"Time-stepping\" in line:\n            total_time = line.split()[2]\n\n    return True, total_time"}
{"prompt": "def _read_next(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the next full form from the input stream.\"\"\"", "output": "\n    form: LispReaderForm = LispReaderForm()\n    form.name = ctx.read_string()\n    form.symbol = ctx.read_string()\n    form.body = ctx.read_body()\n    return form\n\n\ndef _read_body(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the body of the reader.\"\"\"\n    form: LispReaderForm = LispReaderForm()\n    form.body = ctx.read_body()\n    return form\n\n\ndef _read_symbol(ctx: ReaderContext) -> LispReaderForm:  # noqa: C901\n    \"\"\"Read the symbol of the reader.\"\"\"\n    form: LispReaderForm = LispReader", "solution": "\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))"}
{"prompt": "def __tomo_linear_inv(freqs, ops, weights=None, trace=None):\n    \"\"\"\n    Reconstruct a matrix through linear inversion.\n\n    Args:\n        freqs (list[float]): list of observed frequences.\n        ops (list[np.array]): list of corresponding projectors.\n        weights (list[float] or array_like):\n            weights to be used for weighted fitting.\n        trace (float or None): trace of returned operator.\n\n    Returns:\n        numpy.array: A numpy array of the reconstructed operator.\n    \"\"\"", "output": "\n    freqs = np.asarray(freqs)\n    weights = np.asarray(weights)\n    trace = trace or np.ones_like(freqs)\n    freqs, ops, weights = _check_freqs(freqs, ops, weights)\n    freqs, ops, weights = _check_weights(freqs, ops, weights)\n    freqs, ops, weights = _check_trace(trace, freqs, ops, weights)\n    freqs, ops, weights = _check_weights(freqs, ops, weights)\n    freqs, ops, weights = _check_weights(freqs, ops, weights)\n    freqs, ops, weights = _check_weights(freqs, ops, weights)\n    freqs, ops, weights = _check_", "solution": "\n    # get weights matrix\n    if weights is not None:\n        W = np.array(weights)\n        if W.ndim == 1:\n            W = np.diag(W)\n\n    # Get basis S matrix\n    S = np.array([vectorize(m).conj()\n                  for m in ops]).reshape(len(ops), ops[0].size)\n    if weights is not None:\n        S = np.dot(W, S)  # W.S\n\n    # get frequencies vec\n    v = np.array(freqs)  # |f>\n    if weights is not None:\n        v = np.dot(W, freqs)  # W.|f>\n    Sdg = S.T.conj()  # S^*.W^*\n    inv = np.linalg.pinv(np.dot(Sdg, S))  # (S^*.W^*.W.S)^-1\n\n    # linear inversion of freqs\n    ret = devectorize(np.dot(inv, np.dot(Sdg, v)))\n    # renormalize to input trace value\n    if trace is not None:\n        ret = trace * ret / np.trace(ret)\n    return ret"}
{"prompt": "def check_upload_status(self, video_id):\n        \"\"\"\n        Checks the video upload status\n        Newly uploaded videos may be in the processing state\n\n        Authentication is required\n\n        Returns:\n            True if video is available\n            otherwise a dict containes upload_state and detailed message\n            i.e. {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n        \"\"\"", "output": "\n        video = self.videos.get(video_id)\n        if not video:\n            return {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n\n        if video.status == \"processing\":\n            return {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n\n        video.status = \"uploading\"\n        video.save()\n\n        return {\"upload_state\": \"processing\", \"detailed_message\": \"\"}\n\n    def upload_video(self, video_id, video_file):\n        \"\"\"\n        Uploads a video to the server\n\n        Authentication is required\n\n        Returns:\n            True if video is uploaded\n            otherwise a dict containes upload_state and detailed message\n            i.e. {\"upload_state\": \"processing\", \"detailed_message\": \"\"", "solution": "\n        # Raise ApiError if not authenticated\n        if not self.authenticated:\n            raise ApiError(_(\"Authentication is required\"))\n\n        entry = self.fetch_video(video_id)\n        upload_status = Api.yt_service.CheckUploadStatus(entry)\n\n        if upload_status is not None:\n            video_upload_state = upload_status[0]\n            detailed_message = upload_status[1]\n            return {\"upload_state\": video_upload_state, \"detailed_message\": detailed_message}\n        else:\n            return True"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        status_copy = copy.copy(self)\n        status_copy.expected = status_copy.farthest = status.expected\n        return status_copy\n\n    def assert_equal(self, other: 'Status') -> None:\n        \"\"\"Assert that two statuses are equal.\n\n        Assert that two statuses are equal if they represent the same parse\n        status. In particular, two statuses are equal if they have the\n        same expected and farthest values, and if they have the same distance\n        between expected and farthest values.\n\n        Args:\n            other: The other status to compare this one to.\n        \"\"\"\n        assert self.expected == other.expected\n        assert self.farthest == other.farthest\n        assert self.distance == ", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def stream(self, id, task, type, follow=False, offset=0, origin=\"start\", plain=False):\n        \"\"\" This endpoint streams a task's stderr/stdout logs.\n\n            https://www.nomadproject.io/api/client.html#stream-logs\n\n            arguments:\n              - id: (str) allocation_id required\n              - task: (str) name of the task inside the allocation to stream logs from\n              - type: (str) Specifies the stream to stream. Either \"stderr|stdout\"\n              - follow: (bool) default false\n              - offset: (int) default 0\n              - origin: (str) either start|end, default \"start\"\n              - plain: (bool) Return just the plain text without framing. default False\n            returns: (str) text\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.BadRequestNomadException\n        \"\"\"", "output": "bool) default false\n\n            returns 200 OK with JSON object containing the logs stream\n\n            returns 400 Bad Request if id or task are missing\n\n            returns 404 Not Found if task does not exist\n\n            returns 500 Internal Server Error if task does not exist\n\n        \"\"\"\n        endpoint = f\"/tasks/{id}/stream\"\n        body = {\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n        }\n        if plain:\n            body[\"plain\"] = True\n        if task:\n            body[\"task\"] = task\n        if id:\n            body[\"id\"] = id\n        return self.post(endpoint, json=body)\n\n", "solution": "\n        params = {\n            \"task\": task,\n            \"type\": type,\n            \"follow\": follow,\n            \"offset\": offset,\n            \"origin\": origin,\n            \"plain\": plain\n        }\n        return self.request(id, params=params, method=\"get\").text"}
{"prompt": "def _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):\n    \"\"\"\n    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)\n    and returns the result.\n    \"\"\"", "output": "\n    stft_mask = np.zeros_like(mask, dtype=bool)\n    stft_mask[frequencies <= times] = True\n    stft_mask[times <= frequencies] = True\n    stft_mask[frequencies > times] = True\n    stft_mask[times > frequencies] = True\n    return stft_mask\n\n\ndef _map_segmentation_mask_to_stft_domain(mask, times, frequencies, stft_times, stft_frequencies):\n    \"\"\"\n    Maps the given `mask`, which is in domain (`frequencies`, `times`) to the new domain (`stft_frequencies`, `stft_times`)\n    and returns the result.\n    \"\"\"\n    ", "solution": "\n    assert mask.shape == (frequencies.shape[0], times.shape[0]), \"Times is shape {} and frequencies is shape {}, but mask is shaped {}\".format(\n        times.shape, frequencies.shape, mask.shape\n    )\n    result = np.zeros((stft_frequencies.shape[0], stft_times.shape[0]))\n\n    if len(stft_times) > len(times):\n        all_j = [j for j in range(len(stft_times))]\n        idxs  = [int(i) for i in np.linspace(0, len(times) - 1, num=len(stft_times))]\n        all_i = [all_j[idx] for idx in idxs]\n    else:\n        all_i = [i for i in range(len(times))]\n        idxs  = [int(i) for i in np.linspace(0, len(stft_times) - 1, num=len(times))]\n        all_j = [all_i[idx] for idx in idxs]\n\n    for i, j in zip(all_i, all_j):\n        result[:, j] = np.interp(stft_frequencies, frequencies, mask[:, i])\n\n    return result"}
{"prompt": "def sync_labels(self, repo):\n        \"\"\"Creates a local map of github labels/milestones to asana tags.\"\"\"", "output": "\n        labels = set()\n        for repo_name in repo:\n            labels.update(self.repos[repo_name].labels)\n        labels.update(self.repos[repo].labels)\n        labels.update(self.repos[repo_name].milestones)\n        labels.update(self.repos[repo_name].labels)\n        labels.update(self.repos[repo_name].milestones)\n        labels.update(self.repos[repo_name].labels)\n        labels.update(self.repos[repo_name].milestones)\n        labels.update(self.repos[repo_name].labels)\n        labels.update(self.repos[repo_name].milestones)\n        labels.update(self.repos[repo_name].labels)\n        labels.update(self.repos[repo_name].mile", "solution": "\n\n        logging.info(\"syncing new github.com labels to tags\")\n\n        # create label tag map\n        ltm = self.app.data.get(\"label-tag-map\", {})\n\n        # loop over labels, if they don't have tags, make them\n        for label in repo.get_labels():\n            tag_id = ltm.get(label.name, None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=label.name,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % label.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", label.name, tag['id'])\n                ltm[label.name] = tag['id']\n\n        # loop over milestones, if they don't have tags, make them\n        for ms in repo.get_milestones(state=\"all\"):\n            tag_id = ltm.get(_ms_label(ms.id), None)\n            if tag_id is None:\n\n                tag = self.app.asana.tags.create(name=ms.title,\n                                      workspace=self.asana_ws_id,\n                                      notes=\"gh: %s\" % ms.url\n                                      )\n\n                logging.info(\"\\t%s => tag %d\", ms.title, tag['id'])\n                ltm[_ms_label(ms.id)] = tag['id']\n\n        self.app.data['label-tag-map'] = ltm\n        return ltm"}
{"prompt": "def recurse(self, k, wait=False, wait_index=None, timeout='5m'):\n        \"\"\"Recursively get the tree below the given key\"\"\"", "output": "\n        wait = wait and wait_index is not None\n        wait_index = wait_index or 0\n        wait_index += k\n        wait_index = self.wait_index(wait_index)\n        children = self.children(k)\n        children = [c for c in children if c]\n        if wait:\n            children = [c for c in children if c.wait(wait_index)]\n        return children\n\n    def wait(self, wait_index=None, timeout='5m'):\n        \"\"\"Wait for the tree below the given key\"\"\"\n        wait_index = wait_index or 0\n        wait_index += self.key\n        wait_index = self.wait_index(wait_index)\n        children = self.children(self.key)\n        children = [c ", "solution": "\n        k = k.lstrip('/')\n        url = '{}/{}'.format(self.endpoint, k)\n        params = {}\n        params['recurse'] = 'true'\n        if wait:\n            params['wait'] = timeout\n            if not wait_index:\n                params['index'] = self.index(k, recursive=True)\n            else:\n                params['index'] = wait_index\n        r = requests.get(url, params=params)\n        if r.status_code == 404:\n            raise KeyDoesNotExist(\"Key \" + k + \" does not exist\")\n        if r.status_code != 200:\n            raise KVStoreError('GET returned {}'.format(r.status_code))\n        entries = {} \n        for e in r.json():\n            if e['Value']:\n                entries[e['Key']] = base64.b64decode(e['Value'])\n            else:\n                entries[e['Key']] = ''\n        return entries"}
{"prompt": "def delayed(f):\n    \"\"\"Decorator to transparantly accept delayed computation.\n\n    Example:\n\n    >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,\n    >>>                   shape=4, delay=True)\n    >>> @vaex.delayed\n    >>> def total_sum(sums):\n    >>>     return sums.sum()\n    >>> sum_of_sums = total_sum(delayed_sum)\n    >>> ds.execute()\n    >>> sum_of_sums.get()\n    See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations\n    \"\"\"", "output": "\n    def wrapper(*args, **kwargs):\n        func = functools.partial(f, *args, **kwargs)\n        func = functools.partial(func, delay=True)\n        return func\n    return wrapper\n\n\n@vaex.autojit\ndef delayed_sum(ds, binby=None, limits=None, shape=None, delay=False):\n    \"\"\"Compute a sum of the elements of a dataset using a delayed computation.\n\n    Args:\n        ds (Dataset): A dataset to sum.\n        binby (Dataset): A dataset to binby.\n        limits (tuple): A tuple of limits for the bins.\n        shape (tuple): A tuple of shapes for the bins.\n        delay (bool): If True, compute a sum of the elements of the dataset using a delayed computation.", "solution": "\n\n    def wrapped(*args, **kwargs):\n        # print \"calling\", f, \"with\", kwargs\n        # key_values = kwargs.items()\n        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])\n        # key_promise = [(key, promisify(value)) for key, value in key_values]\n        arg_promises = list([promisify(value) for value in args])\n        kwarg_promises = list([promise for key, promise in key_promise])\n        promises = arg_promises + kwarg_promises\n        for promise in promises:\n            def echo_error(exc, promise=promise):\n                print(\"error with \", promise, \"exception is\", exc)\n                # raise exc\n\n            def echo(value, promise=promise):\n                print(\"done with \", repr(promise), \"value is\", value)\n            # promise.then(echo, echo_error)\n\n        # print promises\n        allarguments = aplus.listPromise(*promises)\n\n        def call(_):\n            kwargs_real = {key: promise.get() for key, promise in key_promise}\n            args_real = list([promise.get() for promise in arg_promises])\n            return f(*args_real, **kwargs_real)\n\n        def error(exc):\n            print(\"error\", exc)\n            raise exc\n        return allarguments.then(call, error)\n    return wrapped"}
{"prompt": "def read(cls, path, sc=None, min_partitions=1, bigdl_type=\"float\"):\n        \"\"\"\n        Read images as Image Frame\n        if sc is defined, Read image as DistributedImageFrame from local file system or HDFS\n        if sc is null, Read image as LocalImageFrame from local file system\n        :param path path to read images\n        if sc is defined, path can be local or HDFS. Wildcard character are supported.\n        if sc is null, path is local directory/image file/image file with wildcard character\n        :param sc SparkContext\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :return ImageFrame\n        \"\"\"", "output": "\n        if sc is None:\n            sc = SparkContext.getOrCreate()\n        if path.endswith(\"*\"):\n            path = sc.textFile(path).collect()\n        else:\n            path = sc.textFile(path)\n        frame_reader = ImageFrameReader(sc, path, min_partitions, bigdl_type)\n        return frame_reader.read()\n\n\nclass ImageFrameReader(object):\n    \"\"\"\n    ImageFrameReader reads image frames from HDFS or local file system\n    \"\"\"\n\n    def __init__(self, sc, path, min_partitions=1, bigdl_type=\"float\"):\n        \"\"\"\n        ImageFrameReader constructor\n        :param sc SparkContext\n        :param path path to read images\n        :param min_partitions A suggestion value of the minimal split", "solution": "\n        return ImageFrame(jvalue=callBigDlFunc(bigdl_type, \"read\", path, sc, min_partitions))"}
{"prompt": "def _is_package(path):\n    \"\"\"\n    \u5224\u65ad\u4f20\u5165\u7684\u8def\u5f84\u662f\u5426\u4e3a\u4e00\u4e2a Python \u6a21\u5757\u5305\n\n    :param str path: \u5f85\u5224\u65ad\u7684\u8def\u5f84\n    :return: \u8fd4\u56de\u662f\uff0c\u5219\u4f20\u5165 path \u4e3a\u4e00\u4e2a Python \u5305\uff0c\u5426\u5219\u4e0d\u662f\n    :rtype: bool\n    \"\"\"", "output": "\n    return path.endswith('.py') or path.endswith('.pyc')\n\n\ndef iter_modules(path, prefix=''):\n    \"\"\"\n    \u904d\u5386\u4f20\u5165\u7684\u8def\u5f84\uff0c\u8fd4\u56de Python \u5305\u7684\u540d\u79f0\u548c\u6a21\u5757\n\n    :param str path: \u5f85\u904d\u5386\u7684\u8def\u5f84\n    :param str prefix: \u524d\u7f00\n    :return: \u8fd4\u56de module \u548c name\n    :rtype: tuple\n    \"\"\"\n    modules = []\n    module_name = prefix\n    for name in os.listdir(path):\n        if _is_package(name):\n            module_name = prefix + name\n        else:\n            modules.append((module_name, name))\n    return modules\n\n\ndef iter_packages(path):\n    \"\"\"\n    \u904d\u5386\u4f20\u5165", "solution": "\n    def _exists(s):\n        return os.path.exists(os.path.join(path, s))\n\n    return (\n        os.path.isdir(path) and\n        (_exists('__init__.py') or _exists('__init__.pyc'))\n    )"}
{"prompt": "def get_settings(self, client_name=None):\n    \"\"\"get all settings, either for a particular client if a name is provided,\n       or across clients.\n\n       Parameters\n       ==========\n       client_name: the client name to return settings for (optional)\n\n    \"\"\"", "output": "\n    settings = {}\n    if client_name:\n        settings[client_name] = {}\n    for client in self.clients:\n        settings[client_name] = client.get_settings()\n    return settings\n\ndef reset_settings():\n    \"\"\"reset all settings to default values\"\"\"\n    for client in self.clients:\n        client.reset_settings()\n\ndef reset_clients():\n    \"\"\"reset all clients to default values\"\"\"\n    for client in self.clients:\n        client.reset_clients()\n\ndef reset_settings_for_client(client_name):\n    \"\"\"reset all settings for a single client\"\"\"\n    for client in self.clients:\n        client.reset_settings_for_client(client_name)\n\ndef reset_clients_for_client(client_name):\n    \"\"\"reset ", "solution": "\n    settings = read_client_secrets()\n    if client_name is not None and client_name in settings:\n        return settings[client_name]           \n    return settings"}
{"prompt": "def _evolve(self, state, qargs=None):\n        \"\"\"Evolve a quantum state by the operator.\n\n        Args:\n            state (QuantumState): The input statevector or density matrix.\n            qargs (list): a list of QuantumState subsystem positions to apply\n                           the operator on.\n\n        Returns:\n            QuantumState: the output quantum state.\n\n        Raises:\n            QiskitError: if the operator dimension does not match the\n            specified QuantumState subsystem dimensions.\n        \"\"\"", "output": "\n        dim = self.dim\n        if qargs is None:\n            qargs = []\n        if state.dim!= dim:\n            raise QiskitError(\n                \"The input state must be a {} quantum state, but is of \"\n                \"dimension {}.\".format(dim, state.dim)\n            )\n        op = operator.evolve(self, qargs)\n        return op.apply(self.to_instruction())\n\n    def to_instruction(self):\n        \"\"\"Convert the quantum state to its corresponding instruction.\n\n        Returns:\n            Instruction: The corresponding instruction.\n        \"\"\"\n        return Instruction(\n            Operator(self.op),\n            ParameterVector(self.params),\n            ClassicalRegister(self.clbit),\n            QuantumRegister(self.qbit", "solution": "\n        state = self._format_state(state)\n        if qargs is None:\n            if state.shape[0] != self._input_dim:\n                raise QiskitError(\n                    \"Operator input dimension is not equal to state dimension.\"\n                )\n            if state.ndim == 1:\n                # Return evolved statevector\n                return np.dot(self.data, state)\n            # Return evolved density matrix\n            return np.dot(\n                np.dot(self.data, state), np.transpose(np.conj(self.data)))\n        # Subsystem evolution\n        return self._evolve_subsystem(state, qargs)"}
{"prompt": "def read(self, input_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Read the data encoding the CreateKeyPair response payload and decode it\n        into its constituent parts.\n\n        Args:\n            input_buffer (stream): A data buffer containing encoded object\n                data, supporting a read method.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be decoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            InvalidKmipEncoding: Raised if the private key unique identifier or\n                the public key unique identifier is missing from the encoded\n                payload.\n        \"\"\"", "output": "er is invalid.\n            KmipVersionError: Raised if the KMIP version is invalid.\n\n        Returns:\n            KmipCreateKeyPairResponse: The KMIP CreateKeyPair response object.\n        \"\"\"\n        response_object = self._create_key_pair_response_object()\n        response_object.ParseFromString(input_buffer)\n        return response_object\n\n    def _create_key_pair_response_object(self):\n        \"\"\"\n        Create the KMIP CreateKeyPair response object.\n\n        Returns:\n            KmipCreateKeyPairResponse: The KMIP CreateKeyPair response object.\n        \"\"\"\n        response_object = KmipCreateKeyPairResponse()\n        response_object.kmi", "solution": "\n        super(CreateKeyPairResponsePayload, self).read(\n            input_buffer,\n            kmip_version=kmip_version\n        )\n        local_buffer = utils.BytearrayStream(input_buffer.read(self.length))\n\n        if self.is_tag_next(\n                enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._private_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PRIVATE_KEY_UNIQUE_IDENTIFIER\n            )\n            self._private_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"private key unique identifier.\"\n            )\n\n        if self.is_tag_next(\n                enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER,\n                local_buffer\n        ):\n            self._public_key_unique_identifier = primitives.TextString(\n                tag=enums.Tags.PUBLIC_KEY_UNIQUE_IDENTIFIER\n            )\n            self._public_key_unique_identifier.read(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n        else:\n            raise exceptions.InvalidKmipEncoding(\n                \"The CreateKeyPair response payload encoding is missing the \"\n                \"public key unique identifier.\"\n            )\n\n        if kmip_version < enums.KMIPVersion.KMIP_2_0:\n            if self.is_tag_next(\n                    enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._private_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PRIVATE_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._private_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n            if self.is_tag_next(\n                    enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE,\n                    local_buffer\n            ):\n                self._public_key_template_attribute = \\\n                    objects.TemplateAttribute(\n                        tag=enums.Tags.PUBLIC_KEY_TEMPLATE_ATTRIBUTE\n                    )\n                self._public_key_template_attribute.read(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        self.is_oversized(local_buffer)"}
{"prompt": "def validate(self):\n        \"\"\"\n        Error check the attributes of the ActivateRequestPayload object.\n        \"\"\"", "output": "\n        super(ActivateRequestPayload, self).validate()\n\n        # Validate required fields\n        self._check_type('account_sid', account_sid,'str')\n        self._check_type('sid', sid, self._sid())\n\n        # Validate attributes\n        self._check_type('data', data, dict)\n        self._check_type('friendly_name', friendly_name, str)\n        self._check_type('sid', sid, self._solution['sid'])\n\n    def fetch(self):\n        \"\"\"\n        Fetch a ActivateRequestPayload.\n        \"\"\"\n        params = values.of({})\n\n        payload = self._version.fetch(\n            'GET',\n            self._uri,\n            params=params,\n        )\n\n        return ActivateRequestPayload(\n            ", "solution": "\n        if self.unique_identifier is not None:\n            if not isinstance(self.unique_identifier,\n                              attributes.UniqueIdentifier):\n                msg = \"invalid unique identifier\"\n                raise TypeError(msg)\n        if self.compromise_occurrence_date is not None:\n            if not isinstance(self.compromise_occurrence_date,\n                              primitives.DateTime):\n                msg = \"invalid compromise time\"\n                raise TypeError(msg)\n        if not isinstance(self.revocation_reason, objects.RevocationReason):\n            msg = \"invalid revocation reason\"\n            raise TypeError(msg)"}
{"prompt": "def receive_ack_renewing(self, pkt):\n        \"\"\"Receive ACK in RENEWING state.\"\"\"", "output": "\n        pkt.check_error()\n        pkt.set_state(self.RENEWING)\n\n    def receive_ack_not_renewing(self, pkt):\n        \"\"\"Receive ACK in NOT_RENEWING state.\"\"\"\n        pkt.check_error()\n        pkt.set_state(self.NOT_RENEWING)\n\n    def receive_ack_not_found(self, pkt):\n        \"\"\"Receive ACK in NOT_FOUND state.\"\"\"\n        pkt.check_error()\n        pkt.set_state(self.NOT_FOUND)\n\n    def receive_ack_not_found(self, pkt):\n        \"\"\"Receive ACK in NOT_FOUND state.\"\"\"\n        pk", "solution": "\n        logger.debug(\"C3. Received ACK?, in RENEWING state.\")\n        if self.process_received_ack(pkt):\n            logger.debug(\"C3: T. Received ACK, in RENEWING state, \"\n                         \"raise BOUND.\")\n            raise self.BOUND()"}
{"prompt": "def _get_server_info_from_logs(self):\n        \"\"\"\n        Check server's output log, and determine its scheme / IP / port (helper method for `._launch_server()`).\n\n        This method is polled during process startup. It looks at the server output log and checks for a presence of\n        a particular string (\"INFO: Open H2O Flow in your web browser:\") which indicates that the server is\n        up-and-running. If the method detects this string, it extracts the server's scheme, ip and port and returns\n        them; otherwise it returns None.\n\n        :returns: (scheme, ip, port) tuple if the server has already started, None otherwise.\n        \"\"\"", "output": "\n        server_output = self._run_command(\"h2o-flow logs --server\")\n        if server_output and server_output.find(\"INFO: Open H2O Flow in your web browser:\") > -1:\n            server_output = server_output.split(\"INFO: Open H2O Flow in your web browser:\")[1].strip()\n            server_output = server_output.split(\":\")\n            if len(server_output) == 3:\n                return server_output\n        return None\n\n    def _launch_server(self, server_config, server_port):\n        \"\"\"\n        Launch the server and return its IP and port.\n\n        :param server_config: Configuration for the server to launch (a dict with keys \"ip\" and \"port\").\n        :param server_port: Port number for the server to launch", "solution": "\n        searchstr = \"INFO: Open H2O Flow in your web browser:\"\n        with open(self._stdout, \"rt\") as f:\n            for line in f:\n                if searchstr in line:\n                    url = line[line.index(searchstr) + len(searchstr):].strip().rstrip(\"/\")\n                    parts = url.split(\":\")\n                    assert len(parts) == 3 and (parts[0] == \"http\" or parts[1] == \"https\") and parts[2].isdigit(), \\\n                        \"Unexpected URL: %s\" % url\n                    return parts[0], parts[1][2:], int(parts[2])\n        return None"}
{"prompt": "def track_production(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"", "output": "\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        node = args[0]\n        node.production = Production(node.production, node.production.productions)\n        return f(*args, **kwargs)\n    return wrapper\n\n\n@track_production\ndef track_production_node(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        node = args[0]\n        node.production = Production(node.production, node.production.productions)\n        node.production.production_node = node\n        return f(*args, **kwargs)\n    return wrapper\n\n\n@track_production_node\ndef track_production_node_with_children(f):\n    \"\"\"\n    decorator for adding positional information to returning nodes\n", "solution": "\n    @wraps(f)\n    def wrapper(self, p):\n        r = f(self, p)\n        node = p[0]\n        if isinstance(node, Node) and len(p) > 1:\n            set_positional_info(node, p)\n        return r\n    \n    return wrapper"}
{"prompt": "def comparison_stats(df, els=['Mg', 'Sr', 'Ba', 'Al', 'Mn']):\n    \"\"\"\n    Compute comparison stats for test and LAtools data.\n    \n    Population-level similarity assessed by a Kolmogorov-Smirnov test.\n    \n    Individual similarity assessed by a pairwise Wilcoxon signed rank test.\n    \n    Trends in residuals assessed by regression analysis, where significance of\n    the slope and intercept is determined by t-tests (both relative to zero).\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        A dataframe containing reference ('X/Ca_r'), test user \n        ('X/Ca_t') and LAtools ('X123') data.\n    els : list\n        list of elements (names only) to plot.\n    \n    Returns\n    -------\n    pandas.DataFrame\n    \n    \"\"\"", "output": "ref') and test ('test') data.\n    \n    els : list, optional\n        List of elements to include in comparisons. Defaults to ['Mg', 'Sr', 'Ba', 'Al', 'Mn'].\n    \n    Returns\n    -------\n    pd.DataFrame\n        A dataframe containing comparison statistics.\n    \"\"\"\n    \n    stats = df.copy()\n    \n    # Compute population-level similarity assessed by Kolmogorov-Smirnov test\n    stats.loc[stats['SIM'] < 0, 'SIM'] = 0\n    stats.loc[stats['SIM'] > 0, 'SIM'] = 1\n    stats.loc[stats['SIM'] < 0, 'SIM'] = -1", "solution": "\n    \n    # get corresponding analyte and ratio names\n    As = []\n    Rs = []\n    analytes = [c for c in df.columns if ('_r' not in c) and ('_t' not in c)]\n    ratios = [c for c in df.columns if ('_r' in c)]\n\n    for e in els:\n        if e == 'Sr':\n            As.append('Sr88')\n        elif e == 'Mg':\n            As.append('Mg24')\n        else:\n            As.append([a for a in analytes if e in a][0])\n        Rs.append([r for r in ratios if e in r][0][:-2])\n        \n    yt_stats = []\n    yl_stats = []\n    \n    for i, (e, a) in enumerate(zip(Rs, As)):\n        if a == 'Ba138':\n            m = 1e3\n            u = '$\\mu$mol/mol'\n        else:\n            m = 1\n            u = 'mmol/mol'\n        \n        x = df.loc[:, e + '_r'].values * m\n        yt = df.loc[:, e + '_t'].values * m\n        yl = df.loc[:, a].values * m\n        \n        yt_stats.append(summary_stats(x, yt, e))\n        yl_stats.append(summary_stats(x, yl, e))\n    \n    yt_stats = pd.concat(yt_stats).T\n    yl_stats = pd.concat(yl_stats).T\n    \n    return pd.concat([yt_stats, yl_stats], keys=['Test User', 'LAtools']).T"}
{"prompt": "def get_pk_value_on_save(self, instance):\n        \"\"\"Generate ID if required.\"\"\"", "output": "\n        pk_value = instance.pk\n        if pk_value is None:\n            pk_value = self.get_pk_value_from_db(instance)\n        return pk_value\n\n    def get_pk_value_from_db(self, instance):\n        \"\"\"Generate ID if required.\"\"\"\n        pk_value = None\n        if instance.pk:\n            pk_value = instance.pk\n        return pk_value\n\n    def get_pk_value(self, instance):\n        \"\"\"Generate ID if required.\"\"\"\n        pk_value = self.get_pk_value_from_db(instance)\n        return pk_value\n\n    def get_pk_values(self, instance):\n        \"\"\"Generate ID if required.\"\"\"\n        pk_values = self.get_pk_value_from_db(instance)\n        return ", "solution": "\n        value = super(AleaIdField, self).get_pk_value_on_save(instance)\n        if not value:\n            value = self.get_seeded_value(instance)\n        return value"}
{"prompt": "def _send(self, data, content_type):\n        \"\"\"\n        Sends a GCM message with the given content type\n        \"\"\"", "output": "\n        message = {'data': data, 'to': self.to}\n        headers['Content-Type'] = content_type\n        headers['Authorization'] = 'key=' + self.api_key\n        headers['X-Google-Project-Id'] = self.project_id\n        headers['X-Google-Sender'] = self.sender\n        headers['X-Google-Message-Id'] = self.message_id\n        headers['X-Google-Target-Id'] = self.target_id\n        headers['X-Google-Message-Format'] = self.message_format\n        headers['X-Google-Priority'] = self.priority\n        headers['X-Google-Expires'] = self.expires\n        headers['X-Google-Message-Body'] = self.body\n        headers['X-Google-Message-Id'] = self.message_id\n        headers['X-Goog", "solution": "\n\n        headers = {\n            \"Content-Type\": content_type,\n            \"Authorization\": \"key=%s\" % (self.api_key),\n            \"Content-Length\": str(len(data))\n        }\n\n        request = Request(self.api_url, data, headers)\n        return urlopen(request).read().decode(self.encoding)"}
{"prompt": "def make_clean_visible(_html, tag_replacement_char=' '):\n    \"\"\"\n    Takes an HTML-like Unicode string as input and returns a UTF-8\n    encoded string with all tags replaced by whitespace. In particular,\n    all Unicode characters inside HTML are replaced with a single\n    whitespace character.\n\n    This does not detect comments, style, script, link.  It also does\n    do anything with HTML-escaped characters.  All of these are\n    handled by the clean_html pre-cursor step.\n\n    Pre-existing whitespace of any kind (newlines, tabs) is converted\n    to single spaces ' ', which has the same byte length (and\n    character length).\n\n    This is a simple state machine iterator without regexes\n    \"\"\"", "output": "wrapper around clean_html() that also strips\n    out all HTML tags.  It also converts any HTML entities to\n    their corresponding characters.\n\n    Input is expected to be UTF-8 encoded string.\n    \"\"\"\n    cleaned = _html.decode('utf-8')\n    cleaned = cleaned.translate(tag_replacement_char)\n    return cleaned\n\n\ndef clean_html(html):\n    \"\"\"\n    Cleans HTML by removing all tags and converting all entities to their\n    corresponding characters.\n\n    Input is expected to be UTF-8 encoded string.\n    \"\"\"\n    cleaned = clean_html(html)\n    cleaned = cleaned.decode('utf-8')\n    return cleaned\n\n\ndef clean_html(html, ", "solution": "\n    def non_tag_chars(html):\n        n = 0\n        while n < len(html):\n            angle = html.find('<', n)\n            if angle == -1:\n                yield html[n:]\n                n = len(html)\n                break\n            yield html[n:angle]\n            n = angle\n\n            while n < len(html):\n                nl = html.find('\\n', n)\n                angle = html.find('>', n)\n                if angle == -1:\n                    yield ' ' * (len(html) - n)\n                    n = len(html)\n                    break\n                elif nl == -1 or angle < nl:\n                    yield ' ' * (angle + 1 - n)\n                    n = angle + 1\n                    break\n                else:\n                    yield ' ' * (nl - n) + '\\n'\n                    n = nl + 1\n                    # do not break\n\n    if not isinstance(_html, unicode):\n        _html = unicode(_html, 'utf-8')\n\n    # Protect emails by substituting with unique key\n    _html = fix_emails(_html)\n\n    #Strip tags with previous logic\n    non_tag = ''.join(non_tag_chars(_html))\n\n    return non_tag.encode('utf-8')"}
{"prompt": "def match(self, request):\n        \"\"\"\n        Matches a given Request instance contract against the registered mocks.\n\n        If a mock passes all the matchers, its response will be returned.\n\n        Arguments:\n            request (pook.Request): Request contract to match.\n\n        Raises:\n            pook.PookNoMatches: if networking is disabled and no mock matches\n                with the given request contract.\n\n        Returns:\n            pook.Response: the mock response to be used by the interceptor.\n        \"\"\"", "output": "\n        networking = self.networking\n        if not networking:\n            raise pook.PookNoMatches(\n                \"No mocks registered for request {}\".format(request)\n            )\n\n        matches = []\n        for mock in networking.mocks:\n            match = mock.match(request)\n            if match:\n                matches.append(match)\n\n        if not matches:\n            raise pook.PookNoMatches(\n                \"No mocks registered for request {}\".format(request)\n            )\n\n        return matches[0]\n\n    def response(self, request, response):\n        \"\"\"\n        Intercept the given Request and return a Response instance.\n\n        Arguments:\n            request (pook.Request): Request contract to intercept.\n            ", "solution": "\n        # Trigger engine-level request filters\n        for test in self.filters:\n            if not test(request, self):\n                return False\n\n        # Trigger engine-level request mappers\n        for mapper in self.mappers:\n            request = mapper(request, self)\n            if not request:\n                raise ValueError('map function must return a request object')\n\n        # Store list of mock matching errors for further debugging\n        match_errors = []\n\n        # Try to match the request against registered mock definitions\n        for mock in self.mocks[:]:\n            try:\n                # Return the first matched HTTP request mock\n                matches, errors = mock.match(request.copy())\n                if len(errors):\n                    match_errors += errors\n                if matches:\n                    return mock\n            except PookExpiredMock:\n                # Remove the mock if already expired\n                self.mocks.remove(mock)\n\n        # Validate that we have a mock\n        if not self.should_use_network(request):\n            msg = 'pook error!\\n\\n'\n\n            msg += (\n                '=> Cannot match any mock for the '\n                'following request:\\n{}'.format(request)\n            )\n\n            # Compose unmatch error details, if debug mode is enabled\n            if self.debug:\n                err = '\\n\\n'.join([str(err) for err in match_errors])\n                if err:\n                    msg += '\\n\\n=> Detailed matching errors:\\n{}\\n'.format(err)\n\n            # Raise no matches exception\n            raise PookNoMatches(msg)\n\n        # Register unmatched request\n        self.unmatched_reqs.append(request)"}
{"prompt": "def discrete_best_alpha(data, alpharangemults=(0.9,1.1), n_alpha=201, approximate=True, verbose=True):\n    \"\"\"\n    Use the maximum L to determine the most likely value of alpha\n\n    *alpharangemults* [ 2-tuple ]\n        Pair of values indicating multiplicative factors above and below the\n        approximate alpha from the MLE alpha to use when determining the\n        \"exact\" alpha (by directly maximizing the likelihood function)\n    \"\"\"", "output": "\n    alpharangemults = alpharangemults if isinstance(alpharangemults, tuple) else (alpharangemults,)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = tuple(alpharangemults)\n    alpharangemults = ", "solution": "\n\n    xmins = np.unique(data)\n    if approximate:\n        alpha_of_xmin = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n    else:\n        alpha_approx = [ discrete_alpha_mle(data,xmin) for xmin in xmins ]\n        alpharanges = [(0.9*a,1.1*a) for a in alpha_approx]\n        alpha_of_xmin = [ most_likely_alpha(data,xmin,alpharange=ar,n_alpha=n_alpha) for xmin,ar in zip(xmins,alpharanges) ]\n    ksvalues = [ discrete_ksD(data, xmin, alpha) for xmin,alpha in zip(xmins,alpha_of_xmin) ]\n\n    best_index = argmin(ksvalues)\n    best_alpha = alpha_of_xmin[best_index]\n    best_xmin = xmins[best_index]\n    best_ks = ksvalues[best_index]\n    best_likelihood = discrete_likelihood(data, best_xmin, best_alpha)\n\n    if verbose:\n        print(\"alpha = %f   xmin = %f   ksD = %f   L = %f   (n<x) = %i  (n>=x) = %i\" % (\n                best_alpha, best_xmin, best_ks, best_likelihood,\n                (data<best_xmin).sum(), (data>=best_xmin).sum()))\n\n    return best_alpha,best_xmin,best_ks,best_likelihood"}
{"prompt": "def _convert_date(date_string, date_format):\n    \"\"\"\n    Convert a date in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"", "output": "\n    try:\n        return datetime.strptime(date_string, date_format).timestamp()\n    except ValueError:\n        return None\n\n\ndef _convert_time(time_string, time_format):\n    \"\"\"\n    Convert a time in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"\n    try:\n        return datetime.strptime(time_string, time_format).timestamp()\n    except ValueError:\n        return None\n\n\ndef _convert_datetime(date_string, time_string, time_format):\n    \"\"\"\n    Convert a datetime in a given format to epoch time. Mostly a wrapper for\n    datetime's strptime.\n    \"\"\"\n    try:\n        return datetime.strptime(date_string, time_format).timestamp()\n    except ValueError:", "solution": "\n    if date_format != 'epoch':\n        return datetime.strptime(date_string, date_format).timestamp()\n    else:\n        return float(date_string)"}
{"prompt": "def create_session(token=None):\n    \"\"\"\n    Create a github3.py session connected to GitHub.com\n\n    If token is not provided, will attempt to use the GITHUB_API_TOKEN\n    environment variable if present.\n    \"\"\"", "output": "\n    session = github3.login(token=token)\n    return session\n\n\ndef github_api_url(session):\n    \"\"\"\n    Construct a GitHub API URL\n\n    Uses the GitHub API URL for the current session.\n    \"\"\"\n    return session.api_base_url + \"/api/v3\"\n\n\ndef github_api_version(session):\n    \"\"\"\n    Construct a GitHub API version\n\n    Uses the GitHub API version for the current session.\n    \"\"\"\n    return session.api_version\n\n\ndef github_api_endpoint(session):\n    \"\"\"\n    Construct a GitHub API endpoint\n\n    Uses the GitHub API endpoint for the current session.\n    \"\"\"\n    return session.api_base_url\n\n\ndef ", "solution": "\n    if token is None:\n        token = os.environ.get('GITHUB_API_TOKEN', None)\n\n    gh_session = github3.login(token=token)\n\n    if gh_session is None:\n        raise RuntimeError('Invalid or missing GITHUB_API_TOKEN')\n\n    return gh_session"}
{"prompt": "def load(self, source):\n        \"\"\"\n        Opens the source file.\n        \"\"\"", "output": "\n        self.source = source\n        self.source_name = os.path.basename(source)\n        self.source_path = os.path.dirname(source)\n        self.source_root = os.path.dirname(self.source_path)\n        self.source_file = os.path.basename(self.source_path)\n        self.source_dir = self.source_path.split(os.sep)[:-1]\n        self.source_dir[-1] = self.source_dir[-1].replace(self.source_root, '')\n        self.source_dir = self.source_dir + os.sep\n        self.source_dir += self.source_file\n        self.source_dir += self.source_dir[-1]\n        self.source_dir += os.sep\n        self.source_dir += self.source_dir[-1]\n        self.source_dir += os.sep\n        self.source_dir += self.source_dir[-1]\n        self.source_dir += os.sep\n        self.source_dir += self.source_dir", "solution": "\n        self.source = open(self.source, 'rb')    \n        self.loaded = True"}
{"prompt": "def unregisterObserver(self, observer):\n        \"\"\" Remove an observer from the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"", "output": "\n        self._observers.remove(observer)\n\n    def registerObserver(self, observer):\n        \"\"\" Register an observer on the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"\n        self._observers.append(observer)\n\n    def notifyObservers(self, *args):\n        \"\"\" Notify all observers of the meter update() chain.\n\n        Args:\n            *args: Any number of arguments to be passed to all observers.\n        \"\"\"\n        for observer in self._observers:\n            observer.update(*args)\n\n    def notifyObserversOf(self, observer):\n        \"\"\" Notify all observers of the meter update() chain of observer.\n\n        Args:\n            observer (MeterObserver): Subclasse", "solution": "\n        if observer in self.m_observers:\n            self.m_observers.remove(observer)\n        pass"}
{"prompt": "def query(self, minhash, size):\n        \"\"\"\n        Giving the MinHash and size of the query set, retrieve\n        keys that references sets with containment with respect to\n        the query set greater than the threshold.\n\n        Args:\n            minhash (datasketch.MinHash): The MinHash of the query set.\n            size (int): The size (number of unique items) of the query set.\n\n        Returns:\n            `iterator` of keys.\n        \"\"\"", "output": "\n        hashset = minhash.get_hashset(size)\n        hashset.set_threshold(size)\n        hashset.set_query(size)\n        hashset.set_query(size - 1)\n        hashset.set_query(size - 2)\n        hashset.set_query(size - 3)\n        hashset.set_query(size - 4)\n        hashset.set_query(size - 5)\n        hashset.set_query(size - 6)\n        hashset.set_query(size - 7)\n        hashset.set_query(size - 8)\n        hashset.set_query(size - 9)\n        hashset.set_query(size - 10)\n        hashset.set_query(size - 11)\n        hashset.set_query(size - 12)\n        hashset.set_query(size - ", "solution": "\n        for i, index in enumerate(self.indexes):\n            u = self.uppers[i]\n            if u is None:\n                continue\n            b, r = self._get_optimal_param(u, size)\n            for key in index[r]._query_b(minhash, b):\n                yield key"}
{"prompt": "def get_security_repository(self):\n        \"\"\" Security repository \"\"\"", "output": "\n        security_service = SecurityService(self.config)\n        return security_service.get_security_repository()\n\n    def get_security_service(self):\n        \"\"\" Security service \"\"\"\n        security_service = SecurityService(self.config)\n        return security_service.get_security_service()\n\n    def get_security_service_credentials(self):\n        \"\"\" Security service credentials \"\"\"\n        security_service = SecurityService(self.config)\n        return security_service.get_security_service_credentials()\n\n    def get_security_service_credentials_credential(self):\n        \"\"\" Security service credentials credential \"\"\"\n        security_service = SecurityService(self.config)\n        return security_service.get_security_service_credentials_credential()\n\n    def get_security_service_credentials_credentials(self):\n        \"\"\" Security service credentials credential", "solution": "\n        from .repositories import SecurityRepository\n\n        if not self.security_repo:\n            self.security_repo = SecurityRepository(self.session)\n        return self.security_repo"}
{"prompt": "def Counter(a, b, delta):\n    \"\"\"Counter derivative\n    \"\"\"", "output": "\n    return -delta * a / b\n\n\ndef Cubic(a, b, c, delta):\n    \"\"\"Cubic derivative\n    \"\"\"\n    return delta * (b ** 3 - 4 * a * c) / (9 * b * c)\n\n\ndef CubicDerivative(a, b, c, delta):\n    \"\"\"Cubic derivative derivative\n    \"\"\"\n    return delta * (b ** 3 - 4 * a * c) / (9 * b * c)\n\n\ndef CubicInfinite(a, b, c, delta):\n    \"\"\"Cubic infinite derivative\n    \"\"\"\n    return delta * (b ** 3 - 4 * a * c) / (9 * b * c)\n\n\ndef CubicInfiniteDerivative(a, b, c, delta):\n", "solution": "\n    if b < a:\n        return None \n\n    return (b - a) / float(delta)"}
{"prompt": "def cli_command_restart(self, msg):\n        \"\"\"\\\n        restart the subprocess\n        i. we set our state to RESTARTING - on restarting we still send heartbeat\n        ii. we kill the subprocess\n        iii. we start again\n        iv. if its started we set our state to RUNNING, else we set it to WAITING\n\n        :param msg:\n        :return:\n        \"\"\"", "output": "\n        LOG.debug(\"restart command received\")\n        LOG.debug(\"msg: %s\", msg)\n        LOG.debug(\"self.state: %s\", str(self.state))\n        if self.state!= constants.RESTARTING:\n            LOG.error(\"state is not RESTARTING\")\n            return\n        self.state = constants.RESTARTING\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heartbeat.send_heartbeat()\n        self.heart", "solution": "\n        info = ''\n        if self.state == State.RUNNING and self.sprocess and self.sprocess.proc:\n            self.state = State.RESTARTING\n            self.sprocess.set_exit_callback(self.proc_exit_cb_restart)\n            self.sprocess.proc.kill()\n            info = 'killed'\n            # TODO: check if process is really dead etc.\n        return info"}
{"prompt": "def delete_dir(bucket_name, root_path,\n               aws_access_key_id=None, aws_secret_access_key=None,\n               aws_profile=None):\n    \"\"\"Delete all objects in the S3 bucket named ``bucket_name`` that are\n    found in the ``root_path`` directory.\n\n    Parameters\n    ----------\n    bucket_name : `str`\n        Name of an S3 bucket.\n    root_path : `str`\n        Directory in the S3 bucket that will be deleted.\n    aws_access_key_id : `str`\n        The access key for your AWS account. Also set\n        ``aws_secret_access_key``.\n    aws_secret_access_key : `str`\n        The secret key for your AWS account.\n    aws_profile : `str`, optional\n        Name of AWS profile in :file:`~/.aws/credentials`. Use this instead\n        of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based\n        credentials.\n\n    Raises\n    ------\n    ltdconveyor.s3.S3Error\n        Thrown by any unexpected faults from the S3 API.\n    \"\"\"", "output": "profile : `str`\n        The AWS profile to use when connecting to AWS.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        aws_profile=aws_profile\n    )\n    bucket = session.resource('s3').Bucket(bucket_name)\n    bucket.delete_objects(\n        Delete={'Objects': [{'Key': path} for path in root_path.glob('*')]}\n    )\n\n\ndef list_buckets(bucket_name, aws_access_key_id=None, aws_secret_access_key=None,\n                 aws_profile=None):\n    \"\"\"List all objects in the S3 ", "solution": "\n    logger = logging.getLogger(__name__)\n\n    session = boto3.session.Session(\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key)\n    s3 = session.resource('s3')\n    client = s3.meta.client\n\n    # Normalize directory path for searching patch prefixes of objects\n    if not root_path.endswith('/'):\n        root_path.rstrip('/')\n\n    paginator = client.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=bucket_name, Prefix=root_path)\n\n    keys = dict(Objects=[])\n    for item in pages.search('Contents'):\n        try:\n            keys['Objects'].append({'Key': item['Key']})\n        except TypeError:  # item is None; nothing to delete\n            continue\n        # Delete immediately when 1000 objects are listed\n        # the delete_objects method can only take a maximum of 1000 keys\n        if len(keys['Objects']) >= 1000:\n            try:\n                client.delete_objects(Bucket=bucket_name, Delete=keys)\n            except Exception:\n                message = 'Error deleting objects from %r' % root_path\n                logger.exception(message)\n                raise S3Error(message)\n            keys = dict(Objects=[])\n\n    # Delete remaining keys\n    if len(keys['Objects']) > 0:\n        try:\n            client.delete_objects(Bucket=bucket_name, Delete=keys)\n        except Exception:\n            message = 'Error deleting objects from %r' % root_path\n            logger.exception(message)\n            raise S3Error(message)"}
{"prompt": "def find_launcher_class(clsname, kind):\n    \"\"\"Return a launcher for a given clsname and kind.\n\n    Parameters\n    ==========\n    clsname : str\n        The full name of the launcher class, either with or without the\n        module path, or an abbreviation (MPI, SSH, SGE, PBS, LSF,\n        WindowsHPC).\n    kind : str\n        Either 'EngineSet' or 'Controller'.\n    \"\"\"", "output": "\n    clsname = clsname.lower()\n    if clsname =='mpi':\n        from mpi4py import MPI\n        return MPI\n    elif clsname =='ssh':\n        from ssh import SSH\n        return SSH\n    elif clsname =='sge':\n        from sge import SGE\n        return SGE\n    elif clsname == 'pbfs':\n        from pbfs import PBFS\n        return PBFS\n    elif clsname == 'lsf':\n        from lsf import LSF\n        return LSF\n    elif clsname == 'windowshpc':\n        from windowshpc import WindowsHPC\n        return WindowsHPC\n    else:\n        raise ValueError('Unknown launcher class %s' % clsname)\n\n\n", "solution": "\n    if '.' not in clsname:\n        # not a module, presume it's the raw name in apps.launcher\n        if kind and kind not in clsname:\n            # doesn't match necessary full class name, assume it's\n            # just 'PBS' or 'MPI' prefix:\n            clsname = clsname + kind + 'Launcher'\n        clsname = 'IPython.parallel.apps.launcher.'+clsname\n    klass = import_item(clsname)\n    return klass"}
{"prompt": "def get_end_of_day(timestamp):\n    \"\"\"\n    Given a date or a datetime, return a datetime at 23:59:59 on that day\n    \"\"\"", "output": "\n    now = datetime.now()\n    if isinstance(timestamp, datetime):\n        timestamp = timestamp.date()\n    timestamp = timestamp.time()\n    timestamp = timestamp.strftime('%H:%M:%S')\n    return now + timedelta(hours=23, minutes=59, seconds=59, microseconds=999999999)\n\n\ndef to_datetime(date):\n    \"\"\"\n    Convert a string or date to a datetime object\n    \"\"\"\n    if isinstance(date, str):\n        try:\n            return datetime.strptime(date, '%Y-%m-%d')\n        except ValueError:\n            return datetime.strptime(date, '%Y-%m-%dT%H:%M:%S')\n    elif isinstance(date, datetime):\n        return date\n    else:\n        raise TypeError('to_datetime() argument must be str or datetime')\n\n\ndef to_timestamp(date):\n    \"\"\"\n    Convert a string or ", "solution": "\n    return datetime.datetime(timestamp.year, timestamp.month, timestamp.day, 23, 59, 59)"}
{"prompt": "def find_modules(rootpath, skip):\r\n    \"\"\"\r\n    Look for every file in the directory tree and return a dict\r\n    Hacked from sphinx.autodoc\r\n    \"\"\"", "output": "\r\n    modules = {}\r\n    for dirpath, dirnames, filenames in os.walk(rootpath):\r\n        # ignore hidden directories\r\n        if '.svn' in dirnames:\r\n            dirnames.remove('.svn')\r\n        # ignore certain extensions\r\n        if skip is not None and any(filename.endswith(skip) for filename in filenames):\r\n            continue\r\n        for filename in filenames:\r\n            modname = filename[filename.rfind('.') + 1:]\r\n            modules[modname] = filename\r\n    return modules\r\n\r\n\r\ndef setup(app):\r\n    app.connect('autodoc-process-docstring', process_docstring)\r\n    app.connect('autodoc-skip-member', skip_member)\r\n    app.connect('autodoc-skip-member', skip_member)\r\n", "solution": "\r\n\r\n    INITPY = '__init__.py'\r\n\r\n    rootpath = os.path.normpath(os.path.abspath(rootpath))\r\n    if INITPY in os.listdir(rootpath):\r\n        root_package = rootpath.split(os.path.sep)[-1]\r\n        print \"Searching modules in\", rootpath\r\n    else:\r\n        print \"No modules in\", rootpath\r\n        return\r\n\r\n    def makename(package, module):\r\n        "}
{"prompt": "def institutes(self, institute_ids=None):\n        \"\"\"Fetch all institutes.\n        \n        Args:\n            institute_ids(list(str))\n        \n        Returns:\n            res(pymongo.Cursor)\n        \"\"\"", "output": "\n        \n        if institute_ids is None:\n            institute_ids = []\n        \n        institute_ids = institute_ids or []\n        \n        query = {'institute_id': {'$in': institute_ids}}\n        \n        institutes = self.db[self.collection_name].find(query)\n        \n        return institutes\n    \n    def institute(self, institute_id):\n        \"\"\"Fetch a single institute.\n        \n        Args:\n            institute_id(str): The institute ID\n        \n        Returns:\n            res(pymongo.Document)\n        \"\"\"\n        ", "solution": "\n        query = {}\n        if institute_ids:\n            query['_id'] = {'$in': institute_ids}\n        LOG.debug(\"Fetching all institutes\")\n        return self.institute_collection.find(query)"}
{"prompt": "def send(self, node, message):\n        \"\"\"\n        Send a message to a node. Returns False if the connection appears to be dead either before or after actually trying to send the message.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        :returns success\n        :rtype bool\n        \"\"\"", "output": "\n        try:\n            node.send(message)\n            return True\n        except socket.error as e:\n            if e.errno == errno.ECONNREFUSED:\n                return False\n            else:\n                raise\n\n    def recv(self, node, message):\n        \"\"\"\n        Receive a message from a node. Returns False if the connection appears to be dead either before or after actually trying to receive the message.\n\n        :param node: target node\n        :type node: Node\n        :param message: message\n        :param message: any\n        :returns success\n        :rtype bool\n        \"\"\"\n        try:\n            return node.recv(message)\n        except socket.error as e:\n            if e.errno == errno.ECONNREFUSED:\n                return False\n            else:\n                raise\n\n    def recv", "solution": "\n\n        if node not in self._connections or self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        self._connections[node].send(message)\n        if self._connections[node].state != CONNECTION_STATE.CONNECTED:\n            return False\n        return True"}
{"prompt": "def create_model(self, project_id, model):\n        \"\"\"\n        Create a Model. Blocks until finished.\n        \"\"\"", "output": "\n        project = self.manager.get_project(project_id)\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = project.id\n        project_name = project.name\n        project_id = ", "solution": "\n        if not model['name']:\n            raise ValueError(\"Model name must be provided and \"\n                             \"could not be an empty string\")\n        project = 'projects/{}'.format(project_id)\n\n        request = self._mlengine.projects().models().create(\n            parent=project, body=model)\n        return request.execute()"}
{"prompt": "def read_msg(self):\n        \"\"\"Read one message unit. It's possible however that\n        more than one message will be set in a receive, so we will\n        have to buffer that for the next read.\n        EOFError will be raised on EOF.\n        \"\"\"", "output": "\n        buf = ''\n        while 1:\n            try:\n                buf = self.sock.recv(1024)\n            except socket.error as e:\n                if e.errno!= errno.EWOULDBLOCK:\n                    raise\n                break\n            if not buf:\n                raise EOFError\n        return buf\n\n    def read_msgs(self):\n        \"\"\"Read multiple message units. It's possible however that\n        more than one message will be set in a receive, so we will\n        have to buffer that for the next read.\n        EOFError will be raised on EOF.\n        \"\"\"\n        buf = ''\n        while 1:\n            try:\n                buf = self.sock.recv(1024)\n            except socket.error as e:\n                if e.errno!= errno.E", "solution": "\n        if self.state == 'connected':\n            if 0 == len(self.buf):\n                self.buf = self.inout.recv(Mtcpfns.TCP_MAX_PACKET)\n                if 0 == (self.buf):\n                    self.state = 'disconnected'\n                    raise EOFError\n                pass\n            self.buf, data = Mtcpfns.unpack_msg(self.buf)\n            return data.decode('utf-8')\n        else:\n            raise IOError(\"read_msg called in state: %s.\" % self.state)"}
{"prompt": "def _wakeup(self):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"", "output": "\n        self.device.wakeup()\n\n    def _wakeup_and_set_standby(self):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"\n        self.device.wakeup()\n        self.device.set_standby()\n\n    def _wakeup_and_set_standby_and_set_mode(self, mode):\n        \"\"\"\n        issue wakeup command to device to take out of standby mode.\n        \"\"\"\n        self.device.wakeup()\n        self.device.set_standby()\n        self.device.set_mode(mode)\n\n    def _wakeup_and_set_standby_and_set_mode_and_set_standby(self, mode):\n        \"\"\"\n        issue wakeup command to ", "solution": "\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def get_home_dir(require_writable=False):\n    \"\"\"Return the 'home' directory, as a unicode string.\n\n    * First, check for frozen env in case of py2exe\n    * Otherwise, defer to os.path.expanduser('~')\n    \n    See stdlib docs for how this is determined.\n    $HOME is first priority on *ALL* platforms.\n    \n    Parameters\n    ----------\n    \n    require_writable : bool [default: False]\n        if True:\n            guarantees the return value is a writable directory, otherwise\n            raises HomeDirError\n        if False:\n            The path is resolved, but it is not guaranteed to exist or be writable.\n    \"\"\"", "output": "\n    home_dir = os.environ.get('HOME')\n    if home_dir:\n        return home_dir\n    home_dir = os.path.expanduser('~')\n    if require_writable:\n        if not os.path.isdir(home_dir):\n            raise HomeDirError('HOME environment variable is not set')\n        return home_dir\n    return home_dir\n\n\ndef get_user():\n    \"\"\"Return the 'user' name, as a unicode string.\n    \n    See stdlib docs for how this is determined.\n    $USER is first priority on *ALL* platforms.\n    \"\"\"\n    user = os.environ.get('USER')\n    if user:\n        return user\n    user = pwd.getpwuid(os.getuid())[0]\n    return user\n\n\ndef get_group():\n    \"\"\"Return the '", "solution": "\n\n    # first, check py2exe distribution root directory for _ipython.\n    # This overrides all. Normally does not exist.\n\n    if hasattr(sys, \"frozen\"): #Is frozen by py2exe\n        if '\\\\library.zip\\\\' in IPython.__file__.lower():#libraries compressed to zip-file\n            root, rest = IPython.__file__.lower().split('library.zip')\n        else:\n            root=os.path.join(os.path.split(IPython.__file__)[0],\"../../\")\n        root=os.path.abspath(root).rstrip('\\\\')\n        if _writable_dir(os.path.join(root, '_ipython')):\n            os.environ[\"IPYKITROOT\"] = root\n        return py3compat.cast_unicode(root, fs_encoding)\n    \n    homedir = os.path.expanduser('~')\n    # Next line will make things work even when /home/ is a symlink to\n    # /usr/home as it is on FreeBSD, for example\n    homedir = os.path.realpath(homedir)\n    \n    if not _writable_dir(homedir) and os.name == 'nt':\n        # expanduser failed, use the registry to get the 'My Documents' folder.\n        try:\n            import _winreg as wreg\n            key = wreg.OpenKey(\n                wreg.HKEY_CURRENT_USER,\n                \"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n            )\n            homedir = wreg.QueryValueEx(key,'Personal')[0]\n            key.Close()\n        except:\n            pass\n    \n    if (not require_writable) or _writable_dir(homedir):\n        return py3compat.cast_unicode(homedir, fs_encoding)\n    else:\n        raise HomeDirError('%s is not a writable dir, '\n                'set $HOME environment variable to override' % homedir)"}
{"prompt": "def run_eplus(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default sys.stdout\n    stderr: default sys.stderr\n    beat_freq: if not none, stdout will be used at least every beat_freq (in seconds)\n    \"\"\"", "output": "\n    epm = EPM(epm_or_idf_path)\n    weather = Weather(weather_data_or_epw_path)\n    simulation = Simulation(simulation_dir_path, epm, weather, stdout=stdout, stderr=stderr)\n    simulation.run()\n    if beat_freq is not None:\n        simulation.run_beat(beat_freq)\n\n\ndef run_eplus_with_weather(epm_or_idf_path, weather_data_or_epw_path, simulation_dir_path, stdout=None, stderr=None, beat_freq=None):\n    \"\"\"\n    Parameters\n    ----------\n    epm_or_idf_path:\n    weather_data_or_epw_path\n    simulation_dir_path\n    stdout: default ", "solution": "\n    # work with absolute paths\n    simulation_dir_path = os.path.abspath(simulation_dir_path)\n\n    # check dir path\n    if not os.path.isdir(simulation_dir_path):\n        raise NotADirectoryError(\"Simulation directory does not exist: '%s'.\" % simulation_dir_path)\n\n    # epm\n    if not isinstance(epm_or_idf_path, Epm):\n        # we don't copy file directly because we want to manage it's external files\n        # could be optimized (use _copy_without_read_only)\n        epm = Epm.from_idf(epm_or_idf_path)\n    else:\n        epm = epm_or_idf_path\n\n    # create idf\n    simulation_idf_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".idf\")\n    epm.to_idf(simulation_idf_path)\n\n    # weather data\n    simulation_epw_path = os.path.join(simulation_dir_path, CONF.default_model_name + \".epw\")\n    if isinstance(weather_data_or_epw_path, WeatherData):\n        weather_data_or_epw_path.to_epw(simulation_epw_path)\n    else:\n        # no need to load: we copy directly\n        _copy_without_read_only(weather_data_or_epw_path, simulation_epw_path)\n\n    # copy epw if needed (depends on os/eplus version)\n    temp_epw_path = get_simulated_epw_path()\n    if temp_epw_path is not None:\n        _copy_without_read_only(simulation_epw_path, temp_epw_path)\n\n    # prepare command\n    eplus_relative_cmd = get_simulation_base_command()\n    eplus_cmd = os.path.join(CONF.eplus_base_dir_path, eplus_relative_cmd)\n\n    # idf\n    idf_command_style = get_simulation_input_command_style(\"idf\")\n    if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        idf_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        idf_file_cmd = simulation_idf_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # epw\n    epw_command_style = get_simulation_input_command_style(\"epw\")\n    if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.simu_dir:\n        epw_file_cmd = os.path.join(simulation_dir_path, CONF.default_model_name)\n    elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES.file_path:\n        epw_file_cmd = simulation_epw_path\n    else:\n        raise AssertionError(\"should not be here\")\n\n    # command list\n    simulation_command_style = get_simulation_command_style()\n    if simulation_command_style == SIMULATION_COMMAND_STYLES.args:\n        cmd_l = [eplus_cmd, idf_file_cmd, epw_file_cmd]\n    elif simulation_command_style == SIMULATION_COMMAND_STYLES.kwargs:\n        cmd_l = [eplus_cmd, \"-w\", epw_file_cmd, \"-r\", idf_file_cmd]\n    else:\n        raise RuntimeError(\"should not be here\")\n\n    # launch calculation\n    run_subprocess(\n        cmd_l,\n        cwd=simulation_dir_path,\n        stdout=stdout,\n        stderr=stderr,\n        beat_freq=beat_freq\n    )\n\n    # if needed, we delete temp weather data (only on Windows, see above)\n    if (temp_epw_path is not None) and os.path.isfile(temp_epw_path):\n        os.remove(os.path.join(temp_epw_path))"}
{"prompt": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"", "output": "\n    attr = ast.Attribute(\n        name=name,\n        ctx=ast.Load(),\n        value=ast.Name(\n            id=ctx.value,\n            ctx=ast.Load(),\n            ctx=ast.Store(),\n        ),\n    )\n    attr.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.value.value.ctx = ast.Load()\n    attr.", "solution": "\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)"}
{"prompt": "def _prepare_query_values(self):\n        \"\"\"Extra prep on query values by converting\n        dictionaries into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"", "output": "\n        if self.query_values is None:\n            self.query_values = {}\n            for k, v in self.params.items():\n                if isinstance(v, dict):\n                    self.query_values[k] = _prepare_query_values(v)\n                elif isinstance(v, HStoreValue):\n                    self.query_values[k] = v\n                else:\n                    self.query_values[k] = v\n\n    def prepare_query_values(self):\n        \"\"\"Prepare the query values by converting dictionaries\n        into :see:HStoreValue expressions.\n\n        This allows putting expressions in a dictionary.\n        The :see:HStoreValue will take care of resolving\n        the expressions inside the dictionary.\"\"\"\n        self._prepare_query_values()\n        self.query_values = _prepare_query_values(self.query_values)\n\n    def ", "solution": "\n\n        new_query_values = []\n        for field, model, val in self.query.values:\n            if isinstance(val, dict):\n                val = HStoreValue(val)\n\n            new_query_values.append((\n                field,\n                model,\n                val\n            ))\n\n        self.query.values = new_query_values"}
{"prompt": "def get_handler(level: str, fmt: str) -> logging.Handler:\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"", "output": "\n    logger = logging.getLogger(BASILISP_LOGGER)\n    logger.setLevel(level)\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(fmt))\n    logger.addHandler(handler)\n    return logger\n\n\ndef setup_logging(level: str = \"INFO\") -> None:\n    \"\"\"Set up the default logging level for Basilisp.\"\"\"\n    logger = get_handler(level, BASILISP_LOGGER)\n    logger.setLevel(level)\n\n\ndef setup_logging(level: str = \"INFO\", fmt: str = \"%(message)s\") -> None:\n    \"\"\"Set up the default logging level for Basilisp.\"\"\"\n    logger = get_handler(level, BASILISP_LOGGER)\n    logger.setLevel(level)\n    handler = logging.StreamHandler()\n    handler.set", "solution": "\n    handler: logging.Handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler"}
{"prompt": "def _get_course_content_from_ecommerce(course_id, site_code=None):\n    \"\"\"\n    Get course information using the Ecommerce course api.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str): course key of the course\n        site_code (str): site code\n\n    Returns:\n        course information from Ecommerce\n    \"\"\"", "output": "\n    course = ecommerce_client.get_course(course_id)\n    if course is None:\n        return {}\n    course_data = course_to_dict(course)\n    if site_code:\n        course_data['site_code'] = site_code\n    return course_data\n\n\ndef _get_course_content_from_ecommerce_with_site(course_id, site_code=None):\n    \"\"\"\n    Get course information using the Ecommerce course api with site code.\n\n    In case of error returns empty response.\n    Arguments:\n        course_id (str): course key of the course\n        site_code (str): site code\n\n    Returns:\n        course information from Ecommerce\n    \"\"\"\n    course = ", "solution": "\n    api = get_ecommerce_client(site_code=site_code)\n    try:\n        api_response = api.courses(course_id).get()\n    except Exception:  # pylint: disable=broad-except\n        logger.exception(\n            'An error occurred while retrieving data for course run [%s] from the Catalog API.',\n            course_id,\n            exc_info=True\n        )\n        return {}\n\n    return {\n        'title': api_response.get('name'),\n        'verification_deadline': api_response.get('verification_deadline')\n    }"}
{"prompt": "def container_services(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`ContainerServicesOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicesOperations>`\n        \"\"\"", "output": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from.v2017_07_01.operations import ContainerServicesOperations as OperationClass\n            return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n\n    @property\n    def container_service_plans(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-07-01: :class:`ContainerServicePlansOperations<azure.mgmt.containerservice.v2017_07_01.operations.ContainerServicePlansOperations>`\n        \"\"\"", "solution": "\n        api_version = self._get_api_version('container_services')\n        if api_version == '2017-07-01':\n            from .v2017_07_01.operations import ContainerServicesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def doc_from_xml(document_element_name, inner_xml,\n                     xmlns='http://schemas.microsoft.com/windowsazure'):\n        \"\"\"Wraps the specified xml in an xml root element with default azure\n        namespaces\"\"\"", "output": "\n        xml_root = ET.Element('{%s}%s' % (xmlns, document_element_name),\n                              nsmap={'xsi': 'http://www.w3.org/2001/XMLSchema-instance'})\n        xml_root.append(ET.fromstring(inner_xml))\n        return xml_root\n\n    def _wrap_xml_with_default_namespaces(xml_root, xmlns='http://schemas.microsoft.com/windowsazure'):\n        \"\"\"Wraps the specified xml in an xml root element with default azure\n        namespaces\"\"\"\n        xml_root = ET.Element('{%s}%s' % (xmlns, xml_root.tag), nsmap={'xsi': 'http://www.w3.org/2001/XMLSchema-instance'})\n        xml_root.append(xml_root)\n        return xml_root\n\n    def _wrap_xml_", "solution": "\n        xml = ''.join(['<', document_element_name,\n                      ' xmlns=\"{0}\">'.format(xmlns)])\n        xml += inner_xml\n        xml += ''.join(['</', document_element_name, '>'])\n        return xml"}
{"prompt": "def lessThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"", "output": "\n        return cmp(self.compare(left, right), -1)\n\n    def lessThanOrEqual(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return cmp(self.compare(left, right), 1)\n\n    def greaterThan(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return cmp(self.compare(left, right), +1)\n\n    def greaterThanOrEqual(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return cmp(self.compare(left, right), -1)\n\n    def equal(self, left, right):\n        \"\"\"Return ordering of *left* vs *right*.\"\"\"\n        return cmp(self.compare(", "solution": "\n        sourceModel = self.sourceModel()\n        if sourceModel:\n            leftItem = sourceModel.item(left)\n            rightItem = sourceModel.item(right)\n\n            if (isinstance(leftItem, Directory)\n                and not isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.AscendingOrder\n\n            elif (not isinstance(leftItem, Directory)\n                and isinstance(rightItem, Directory)):\n                return self.sortOrder() == Qt.DescendingOrder\n\n        return super(FilesystemSortProxy, self).lessThan(left, right)"}
{"prompt": "def get_entry_properties_from_element(element, include_id, id_prefix_to_skip=None, use_title_as_id=False):\n        \"\"\" get properties from element tree element \"\"\"", "output": "\n        properties = {}\n        properties[\"id\"] = element.get(\"id\")\n        properties[\"title\"] = element.get(\"title\")\n        properties[\"type\"] = element.get(\"type\")\n        properties[\"class\"] = element.get(\"class\")\n        properties[\"style\"] = element.get(\"style\")\n        properties[\"content\"] = element.get(\"content\")\n        properties[\"children\"] = []\n        for child in element:\n            if child.tag == \"entry\":\n                properties[\"children\"].append(get_entry_properties_from_element(child, include_id, id_prefix_to_skip, use_title_as_id))\n            elif child.tag == \"content\":\n                properties[\"content\"] = child.text\n            elif child.tag == \"title\":", "solution": "\n        properties = {}\n\n        etag = element.attrib.get(_make_etree_ns_attr_name(_etree_entity_feed_namespaces['m'], 'etag'), None)\n        if etag is not None:\n            properties['etag'] = etag\n\n        updated = element.findtext('./atom:updated', '', _etree_entity_feed_namespaces)\n        if updated:\n            properties['updated'] = updated\n\n        author_name = element.findtext('./atom:author/atom:name', '', _etree_entity_feed_namespaces)\n        if author_name:\n            properties['author'] = author_name\n\n        if include_id:\n            if use_title_as_id:\n                title = element.findtext('./atom:title', '', _etree_entity_feed_namespaces)\n                if title:\n                    properties['name'] = title\n            else:\n                element_id = element.findtext('./atom:id', '', _etree_entity_feed_namespaces)\n                if element_id:\n                    properties['name'] = _get_readable_id(element_id, id_prefix_to_skip)\n\n        return properties"}
{"prompt": "def source_address(self):\n        \"\"\"Return the authorative source of the link.\"\"\"", "output": "\n        return self.link.get('source')\n\n    def title(self):\n        \"\"\"Return the title of the link.\"\"\"\n        return self.link.get('title')\n\n    def url(self):\n        \"\"\"Return the url of the link.\"\"\"\n        return self.link.get('url')\n\n    def icon(self):\n        \"\"\"Return the icon of the link.\"\"\"\n        return self.link.get('icon')\n\n    def source(self):\n        \"\"\"Return the source of the link.\"\"\"\n        return self.link.get('source')\n\n    def type(self):\n        \"\"\"Return the type of the link.\"\"\"\n        return self.link.get('type')\n\n    def icon(self):\n        \"\"\"Return the icon of the link.\"\"\"\n        return self.link.get('icon')\n\n    def author(self):\n        \"\"\"Return the author of the link.\"\"\"\n        return self.link.get('author')\n\n    def author_details(self):\n        \"\"\"Return the author details of the link.\"\"\"", "solution": "\n        # If link is a sender, source is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_sender:\n            return self._pn_link.source.address\n        else:\n            return self._pn_link.remote_source.address"}
{"prompt": "def _serialize(xp_ast):\n    \"\"\"Generate token strings which, when joined together, form a valid\n    XPath serialization of the AST.\"\"\"", "output": "\n    tokens = xp_ast.tokens\n    token_strs = []\n    for token in tokens:\n        token_strs.append(token.value)\n    return ''.join(token_strs)\n\n\ndef _parse_xpath_expression(xp_expr):\n    \"\"\"Parse an XPath expression into a list of tokens.\"\"\"\n    tokens = xp_expr.tokens\n    token_strs = []\n    for token in tokens:\n        token_strs.append(token.value)\n    return token_strs\n\n\ndef _parse_xpath_expression_fragment(xp_expr):\n    \"\"\"Parse an XPath expression fragment into a list of tokens.\"\"\"\n    tokens = xp_expr.tokens\n    token_strs = []\n    for token in tokens:\n        token_strs.append(token.value)\n    return token_strs", "solution": "\n\n    if hasattr(xp_ast, '_serialize'):\n        for tok in xp_ast._serialize():\n            yield(tok)\n    elif isinstance(xp_ast, str):\n        yield(repr(xp_ast))"}
{"prompt": "def check_for_prefix(self, bucket_name, prefix, delimiter):\n        \"\"\"\n        Checks that a prefix exists in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"", "output": "\n        bucket = self.conn.get_bucket(bucket_name)\n        iterator = bucket.list_prefixes()\n        prefixes = []\n        for prefix in iterator:\n            if prefix['Prefix'] == prefix:\n                prefixes.append(prefix['Prefix'])\n        if prefixes and prefix in prefixes:\n            return True\n        return False\n\n    def list_prefixes(self, bucket_name, prefix='', delimiter=''):\n        \"\"\"\n        Lists prefixes in a bucket\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        :param prefix: a key prefix\n        :type prefix: str\n        :param delimiter: the delimiter marks key hierarchy.\n        :type delimiter: str\n        \"\"\"\n        bucket = self.conn.get_bucket(", "solution": "\n        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix\n        prefix_split = re.split(r'(\\w+[{d}])$'.format(d=delimiter), prefix, 1)\n        previous_level = prefix_split[0]\n        plist = self.list_prefixes(bucket_name, previous_level, delimiter)\n        return False if plist is None else prefix in plist"}
{"prompt": "def group(self, name):\n        \"\"\"GROUP command.\n        \"\"\"", "output": "\n        self.parser.print_help()\n        self.parser.exit()\n\n    def help(self):\n        \"\"\"Help command.\n        \"\"\"\n        self.parser.print_help()\n        self.parser.exit()\n\n    def version(self):\n        \"\"\"Version command.\n        \"\"\"\n        self.parser.print_version()\n        self.parser.exit()\n\n    def quit(self):\n        \"\"\"Quit command.\n        \"\"\"\n        self.parser.exit()\n\n    def help(self):\n        \"\"\"Help command.\n        \"\"\"\n        self.parser.print_help()\n        self.parser.exit()\n\n    def version(self):\n        \"\"\"Version command.\n        \"\"\"\n        self.parser.print_version()\n        self.parser.exit()\n\n    def quit(self):\n        \"\"\"Quit command.\n        \"\"\"\n        self.parser.exit()\n\n    def help(self):\n        \"\"\"Help command.", "solution": "\n        args = name\n\n        code, message = self.command(\"GROUP\", args)\n        if code != 211:\n            raise NNTPReplyError(code, message)\n\n        parts = message.split(None, 4)\n        try:\n            total = int(parts[0])\n            first = int(parts[1])\n            last  = int(parts[2])\n            group = parts[3]\n        except (IndexError, ValueError):\n            raise NNTPDataError(\"Invalid GROUP status '%s'\" % message)\n\n        return total, first, last, group"}
{"prompt": "def get(self, thing_id='0', property_name=None):\n        \"\"\"\n        Handle a GET request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        \"\"\"", "output": "\n        thing_id = int(thing_id)\n        property_name = property_name or self._property_name\n        thing = self.things.get(thing_id)\n        if thing is None:\n            raise HTTPError(404)\n        property = thing.properties.get(property_name)\n        if property is None:\n            raise HTTPError(404)\n        return property\n\n    def put(self, thing_id='0', property_name=None, value=None):\n        \"\"\"\n        Handle a PUT request.\n\n        thing_id -- ID of the thing this request is for\n        property_name -- the name of the property from the URL path\n        value -- value of the property\n        \"\"\"\n        thing_id = int(thing_id)\n        property_name = property_name or self._property_name\n        thing", "solution": "\n        thing = self.get_thing(thing_id)\n        if thing is None:\n            self.set_status(404)\n            return\n\n        if thing.has_property(property_name):\n            self.set_header('Content-Type', 'application/json')\n            self.write(json.dumps({\n                property_name: thing.get_property(property_name),\n            }))\n        else:\n            self.set_status(404)"}
{"prompt": "def plexp_inv(P, xmin, alpha, guess=1.):\n    \"\"\"\n    Inverse CDF for a piecewise PDF as defined in eqn. 3.10\n    of Clauset et al.\n\n    (previous version was incorrect and lead to weird discontinuities in the\n    distribution function)\n    \"\"\"", "output": "\n    N = P.shape[0]\n    alpha = np.atleast_1d(alpha)\n    guess = np.atleast_1d(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(guess)\n    guess = guess/np.sum(", "solution": "\n    def equation(x,prob):\n        return plexp_cdf(x, xmin, alpha)-prob\n    # http://stackoverflow.com/questions/19840425/scipy-optimize-faster-root-finding-over-2d-grid\n    def solver(y, x0=guess):\n        return scipy.optimize.fsolve(equation, guess, args=(y,))\n    f = np.vectorize(solver)\n    return f(P)"}
{"prompt": "def start(self, context):\n\t\t\"\"\"Construct the SQLAlchemy engine and session factory.\"\"\"", "output": "\n\t\tengine = create_engine(self.url)\n\t\tSession = sessionmaker(bind=engine)\n\t\tself.session = Session()\n\n\tdef end(self, context):\n\t\t\"\"\"Close the SQLAlchemy engine and session factory.\"\"\"\n\t\tself.session.close()\n\t\tengine.dispose()\n\n\tdef add_task(self, task):\n\t\t\"\"\"Add a task to the queue.\"\"\"\n\t\tself.session.add(task)\n\t\tself.session.commit()\n\n\tdef add_tasks(self, tasks):\n\t\t\"\"\"Add a list of tasks to the queue.\"\"\"\n\t\tfor task in tasks:\n\t\t\tself.session.add(task)\n\t\tself.session.commit()\n\n\tdef add_tasks_async(self, tasks):\n\t\t\"\"\"Add a list of tasks to the queue.\"\"\"\n\t\twith self.session as session:\n\t\t\tfor task in task", "solution": "\n\t\t\n\t\tif __debug__:\n\t\t\tlog.info(\"Connecting SQLAlchemy database layer.\", extra=dict(\n\t\t\t\t\turi = redact_uri(self.uri),\n\t\t\t\t\tconfig = self.config,\n\t\t\t\t\talias = self.alias,\n\t\t\t\t))\n\t\t\n\t\t# Construct the engine.\n\t\tengine = self.engine = create_engine(self.uri, **self.config)\n\t\t\n\t\t# Construct the session factory.\n\t\tself.Session = scoped_session(sessionmaker(bind=engine))\n\t\t\n\t\t# Test the connection.\n\t\tengine.connect().close()\n\t\t\n\t\t# Assign the engine to our database alias.\n\t\tcontext.db[self.alias] = engine"}
{"prompt": "def _matches_filepath_pattern(self, filepath):\n        \"\"\"Given a filepath, and a list of regex patterns, this function returns true\n        if filepath matches any one of those patterns\"\"\"", "output": "\n        for pattern in self.patterns:\n            if fnmatch.fnmatch(filepath, pattern):\n                return True\n        return False\n\n    def _matches_regex_pattern(self, regex):\n        \"\"\"Given a regex, this function returns true if the regex matches\n        any filename in the list of filenames\"\"\"\n        for filename in self.filenames:\n            if regex.match(filename):\n                return True\n        return False\n\n    def _matches_filename(self, filename):\n        \"\"\"Given a filename, this function returns true if the filename matches\n        any filename in the list of filenames\"\"\"\n        for filename in self.filenames:\n            if fnmatch.fnmatch(filename, filename):\n                return True\n        return False\n\n    def _matches_file(self, filename):\n        \"\"\"Given a filename, this function returns true if the filename matches\n        any ", "solution": "\n        if not self.only_blame_patterns:\n            return True\n\n        for pattern in self.only_blame_patterns:\n            if pattern.match(filepath):\n                return True\n        return False"}
{"prompt": "def iter_auth_hashes(user, purpose, minutes_valid):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"", "output": "\n    now = datetime.datetime.utcnow()\n    token_expiration = now + datetime.timedelta(minutes=minutes_valid)\n    token = generate_auth_token(user, purpose, token_expiration)\n    return token\n\n\ndef iter_auth_hashes_for_purpose(user, purpose):\n    \"\"\"\n    Generate auth tokens tied to user and specified purpose.\n\n    The hash expires at midnight on the minute of now + minutes_valid, such\n    that when minutes_valid=1 you get *at least* 1 minute to use the token.\n    \"\"\"\n    now = datetime.datetime.utcnow()\n    token_expiration = now + datetime.timedelta(minutes=1)\n    token = generate_auth_token(user, purpose, token_expiration)", "solution": "\n    now = timezone.now().replace(microsecond=0, second=0)\n    for minute in range(minutes_valid + 1):\n        yield hashlib.sha1(\n            '%s:%s:%s:%s:%s' % (\n                now - datetime.timedelta(minutes=minute),\n                user.password,\n                purpose,\n                user.pk,\n                settings.SECRET_KEY,\n            ),\n        ).hexdigest()"}
{"prompt": "def _control_key_down(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is down.\n\n        Parameters:\n        -----------\n        include_command : bool, optional (default True)\n            Whether to treat the Command key as a (mutually exclusive) synonym\n            for Control when in Mac OS.\n        \"\"\"", "output": "\n        if include_command:\n            return (self.modifiers & modifiers)!= 0\n        else:\n            return (self.modifiers & modifiers) == 0\n\n    def _control_key_up(self, modifiers):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Control\n        key is up.\n\n        Parameters:\n        -----------\n        modifiers : KeyboardModifiers\n            Keyboard modifiers object.\n        \"\"\"\n        return (self._control_key_down(modifiers, include_command=True)\n                and self.modifiers & modifiers == 0)\n\n    def _shift_key_down(self, modifiers, include_command=False):\n        \"\"\" Given a KeyboardModifiers flags object, return whether the Shift\n        key is down.\n\n        Parameters:\n        -----------\n        include_", "solution": "\n        # Note that on Mac OS, ControlModifier corresponds to the Command key\n        # while MetaModifier corresponds to the Control key.\n        if sys.platform == 'darwin':\n            down = include_command and (modifiers & QtCore.Qt.ControlModifier)\n            return bool(down) ^ bool(modifiers & QtCore.Qt.MetaModifier)\n        else:\n            return bool(modifiers & QtCore.Qt.ControlModifier)"}
{"prompt": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"", "output": "\n        algorithm = kwargs.pop('algorithm', None)\n        validate_algorithm_compatibility(algorithm, cls)\n        validate_algorithm_compatibility(cls.algorithm, algorithm)\n        validate_algorithm_compatibility(cls.algorithm_version, cls.version)\n        validate_algorithm_compatibility(cls.name, cls.name)\n        validate_algorithm_compatibility(cls.description, cls.description)\n        validate_algorithm_compatibility(cls.license, cls.license)\n        validate_algorithm_compatibility(cls.license, cls.license_url)\n        validate_algorithm_compatibility(cls.license, cls.license_url)\n        validate_algorithm_compatibility(cls.license, cls.license_url)\n        validate_algorithm_compatibility(cls.license, cls.license_url)", "solution": "\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data"}
{"prompt": "def timestamp_YmdHMS(value):\n    \"\"\"Convert timestamp string to time in seconds since epoch.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.\n\n    Args:\n        value: A timestamp string in the format '%Y%m%d%H%M%S'.\n\n    Returns:\n        The time in seconds since epoch as an integer.\n\n    Raises:\n        ValueError: If timestamp is invalid.\n\n    Note: The timezone is assumed to be UTC/GMT.\n    \"\"\"", "output": "\n    match = re.match(r'([0-9]{4})([0-9]{2})([0-9]{2})', value)\n    if not match:\n        raise ValueError('Invalid timestamp format: %s' % value)\n    year = int(match.group(1))\n    month = int(match.group(2))\n    day = int(match.group(3))\n    hour = int(match.group(4))\n    minute = int(match.group(5))\n    second = int(match.group(6))\n    if year < 1970:\n        year += 2000\n    return datetime.datetime(year, month, day, hour, minute, second,\n                             tzinfo=pytz.utc).timestamp()\n\n\ndef timestamp_to_datetime(value):\n    \"\"\"Convert timestamp string to datetime.\n\n    Timestamps strings like '20130618120000' are able to be converted by this\n    function.", "solution": "\n    i = int(value)\n    S = i\n    M = S//100\n    H = M//100\n    d = H//100\n    m = d//100\n    Y = m//100\n    return int(calendar.timegm((\n        Y % 10000, m % 100, d % 100, H % 100, M % 100, S % 100, 0, 0, 0)\n    ))"}
{"prompt": "def report_by_type_stats(sect, stats, _):\n    \"\"\"make a report of\n\n    * percentage of different types documented\n    * percentage of different types with a bad name\n    \"\"\"", "output": "\n    stats[\"bad_name\"] = 0\n    stats[\"bad_type\"] = 0\n    stats[\"good_name\"] = 0\n    stats[\"good_type\"] = 0\n    stats[\"bad_name\"] += len(sect[\"bad_name\"])\n    stats[\"bad_type\"] += len(sect[\"bad_type\"])\n    stats[\"good_name\"] += len(sect[\"good_name\"])\n    stats[\"good_type\"] += len(sect[\"good_type\"])\n    stats[\"bad_name\"] += len(sect[\"bad_name\"]) - len(sect[\"bad_type\"])\n    stats[\"bad_type\"] += len(sect[\"bad_type\"]) - len(sect[\"bad", "solution": "\n    # percentage of different types documented and/or with a bad name\n    nice_stats = {}\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        try:\n            total = stats[node_type]\n        except KeyError:\n            raise exceptions.EmptyReportError()\n        nice_stats[node_type] = {}\n        if total != 0:\n            try:\n                documented = total - stats[\"undocumented_\" + node_type]\n                percent = (documented * 100.0) / total\n                nice_stats[node_type][\"percent_documented\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_documented\"] = \"NC\"\n            try:\n                percent = (stats[\"badname_\" + node_type] * 100.0) / total\n                nice_stats[node_type][\"percent_badname\"] = \"%.2f\" % percent\n            except KeyError:\n                nice_stats[node_type][\"percent_badname\"] = \"NC\"\n    lines = (\"type\", \"number\", \"old number\", \"difference\", \"%documented\", \"%badname\")\n    for node_type in (\"module\", \"class\", \"method\", \"function\"):\n        new = stats[node_type]\n        lines += (\n            node_type,\n            str(new),\n            \"NC\",\n            \"NC\",\n            nice_stats[node_type].get(\"percent_documented\", \"0\"),\n            nice_stats[node_type].get(\"percent_badname\", \"0\"),\n        )\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))"}
{"prompt": "def find_safe(ns_qualified_sym: sym.Symbol) -> \"Var\":\n        \"\"\"Return the Var currently bound to the name in the namespace specified\n        by `ns_qualified_sym`. If no Var is bound to that name, raise an exception.\n\n        This is a utility method to return useful debugging information when code\n        refers to an invalid symbol at runtime.\"\"\"", "output": "\n        var_ns = ns_qualified_sym.namespace\n        var_sym = var_ns.lookup(ns_qualified_sym.name)\n        if var_sym is None:\n            raise NameError(f\"Namespace {ns_qualified_sym.namespace!r} has no \"\n                            f\"variable {ns_qualified_sym.name!r}\")\n        return var_sym\n\n    def print_var(self, var_sym: sym.Symbol, indent: int = 0) -> None:\n        \"\"\"Print a Var object to the given indent level.\"\"\"\n        var_str = var_sym.pretty_name()\n        var_str = var_str.replace(\"_\", \" \")\n        var_str = var_str.replace(\"-\", \" \")\n        var_str = var_str.replace(\"*\", ", "solution": "\n        v = Var.find(ns_qualified_sym)\n        if v is None:\n            raise RuntimeException(\n                f\"Unable to resolve symbol {ns_qualified_sym} in this context\"\n            )\n        return v"}
{"prompt": "def start(self):\n        \"\"\"Start the logger background synchronization service. This allows you to not need to\n        worry about syncing with ConnectorDB - you just insert into the Logger, and the Logger\n        will by synced every syncperiod.\"\"\"", "output": "\n        self.logger.info('Starting logger background synchronization service')\n        self.logger.info('Logger background synchronization service started')\n        self.logger.info('Logger background synchronization service is running')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB')\n        self.logger.info('Logger background synchronization service is synchronizing with the ConnectorDB", "solution": "\n\n        with self.synclock:\n            if self.syncthread is not None:\n                logging.warn(\n                    \"Logger: Start called on a syncer that is already running\")\n                return\n\n        self.sync()  # Attempt a sync right away\n        self.__setsync()"}
{"prompt": "def make_path_relative(path, rel_to):\n    \"\"\"\n    Make a filename relative, where the filename path, and it is\n    relative to rel_to\n\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/usr/share/another-place/src/Directory')\n        '../../../something/a-file.pth'\n        >>> make_path_relative('/usr/share/something/a-file.pth',\n        ...                    '/home/user/src/Directory')\n        '../../../usr/share/something/a-file.pth'\n        >>> make_path_relative('/usr/share/a-file.pth', '/usr/share/')\n        'a-file.pth'\n    \"\"\"", "output": "\n    path_parts = path.split(os.sep)\n    rel_to_parts = rel_to.split(os.sep)\n    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:\n        path_parts.pop(0)\n        rel_to_parts.pop(0)\n    full_parts = ['..'] * len(rel_to_parts) + path_parts\n    return os.sep.join(full_parts)\n\n\ndef copy_file(src, dest, symlink=True):\n    \"\"\"Copy a file; return False if it already exists.\n\n    If the destination file exists and is a regular file (i.e. a directory),\n    then do nothing.  If the destination file exists and is a symlink pointing\n    to a regular file, ", "solution": "\n    path_filename = os.path.basename(path)\n    path = os.path.dirname(path)\n    path = os.path.normpath(os.path.abspath(path))\n    rel_to = os.path.normpath(os.path.abspath(rel_to))\n    path_parts = path.strip(os.path.sep).split(os.path.sep)\n    rel_to_parts = rel_to.strip(os.path.sep).split(os.path.sep)\n    while path_parts and rel_to_parts and path_parts[0] == rel_to_parts[0]:\n        path_parts.pop(0)\n        rel_to_parts.pop(0)\n    full_parts = ['..'] * len(rel_to_parts) + path_parts + [path_filename]\n    if full_parts == ['']:\n        return '.' + os.path.sep\n    return os.path.sep.join(full_parts)"}
{"prompt": "def _rotations_to_disentangle(local_param):\n        \"\"\"\n        Static internal method to work out Ry and Rz rotation angles used\n        to disentangle the LSB qubit.\n        These rotations make up the block diagonal matrix U (i.e. multiplexor)\n        that disentangles the LSB.\n\n        [[Ry(theta_1).Rz(phi_1)  0   .   .   0],\n         [0         Ry(theta_2).Rz(phi_2) .  0],\n                                    .\n                                        .\n          0         0           Ry(theta_2^n).Rz(phi_2^n)]]\n        \"\"\"", "output": "\n        theta1, theta2, phi1, phi2 = local_param\n        theta1 = theta1 * np.pi / 180\n        theta2 = theta2 * np.pi / 180\n        phi1 = phi1 * np.pi / 180\n        phi2 = phi2 * np.pi / 180\n        Rx = np.cos(theta1) * np.sin(phi1)\n        Ry = np.sin(theta1) * np.sin(phi1)\n        Rz = np.cos(theta2) * np.sin(phi2)\n        return np.array([Rx, Ry, Rz])\n\n    def _rotations_to_disentangle_matrix(local_param):\n        \"\"\"\n        Static internal method to work out a rotation matrix that disentangles\n        the LSB.\n\n        ", "solution": "\n        remaining_vector = []\n        thetas = []\n        phis = []\n\n        param_len = len(local_param)\n\n        for i in range(param_len // 2):\n            # Ry and Rz rotations to move bloch vector from 0 to \"imaginary\"\n            # qubit\n            # (imagine a qubit state signified by the amplitudes at index 2*i\n            # and 2*(i+1), corresponding to the select qubits of the\n            # multiplexor being in state |i>)\n            (remains,\n             add_theta,\n             add_phi) = Initialize._bloch_angles(local_param[2 * i: 2 * (i + 1)])\n\n            remaining_vector.append(remains)\n\n            # rotations for all imaginary qubits of the full vector\n            # to move from where it is to zero, hence the negative sign\n            thetas.append(-add_theta)\n            phis.append(-add_phi)\n\n        return remaining_vector, thetas, phis"}
{"prompt": "def crcMeterRead(self, raw_read, def_buf):\n        \"\"\" Internal read CRC wrapper.\n\n        Args:\n            raw_read (str): Bytes with implicit string cast from serial read\n            def_buf (SerialBlock): Populated read buffer.\n\n        Returns:\n            bool:  True if passed CRC equals calculated CRC.\n        \"\"\"", "output": "\n        crc = crc32(def_buf.buf, raw_read)\n        if crc!= def_buf.crc:\n            def_buf.crc = crc\n            return True\n        return False\n\n    def crcMeterWrite(self, raw_write, def_buf):\n        \"\"\" Internal write CRC wrapper.\n\n        Args:\n            raw_write (str): Bytes with implicit string cast from serial write\n            def_buf (SerialBlock): Populated write buffer.\n\n        Returns:\n            bool:  True if passed CRC equals calculated CRC.\n        \"\"\"\n        crc = crc32(def_buf.buf, raw_write)\n        if crc!= def_buf.crc:\n            def_buf.crc = crc\n            return True\n        return False\n\n", "solution": "\n        try:\n            if len(raw_read) == 0:\n                ekm_log(\"(\" + self.m_context + \") Empty return read.\")\n                return False\n            sent_crc = self.calc_crc16(raw_read[1:-2])\n            logstr = \"(\" + self.m_context + \")CRC sent = \" + str(def_buf[\"crc16\"][MeterData.StringValue])\n            logstr += \" CRC calc = \" + sent_crc\n            ekm_log(logstr)\n            if int(def_buf[\"crc16\"][MeterData.StringValue], 16) == int(sent_crc, 16):\n                return True\n\n        # A cross simple test lines on a USB serial adapter, these occur every\n        # 1000 to 2000 reads, and they show up here as a bad unpack or\n        # a bad crc type call.  In either case, we suppress them a log will\n        # become quite large.  ekmcrc errors come through as type errors.\n        # Failures of int type conversion in 16 bit conversion occur as value\n        # errors.\n        except struct.error:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except TypeError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        except ValueError:\n            ekm_log(str(sys.exc_info()))\n            for frame in traceback.extract_tb(sys.exc_info()[2]):\n                fname, lineno, fn, text = frame\n                ekm_log(\"Error in %s on line %d\" % (fname, lineno))\n            return False\n\n        return False"}
{"prompt": "def is_bare_exception(self, node):\n        \"\"\"\n        Checks if the node is a bare exception name from an except block.\n\n        \"\"\"", "output": "\n        return node.name in self.bare_exceptions\n\n    def extract_exception(self, node):\n        \"\"\"\n        Extracts the exception name from the given node.\n\n        \"\"\"\n        return node.name\n\n    def extract_exception_only(self, node):\n        \"\"\"\n        Extracts the exception name from the given node.\n\n        \"\"\"\n        return node.name\n\n    def extract_node(self, node):\n        \"\"\"\n        Extracts the node name from the given node.\n\n        \"\"\"\n        return node.name\n\n    def extract_node_only(self, node):\n        \"\"\"\n        Extracts the node name from the given node.\n\n        \"\"\"\n        return node.name\n\n    def extract_nodes(self, node):\n        \"\"\"\n        Extracts the node name from the given node.\n\n        \"\"\"\n        return node.name\n\n    def extract_nodes_with_lineno(self, node):\n", "solution": "\n        return isinstance(node, Name) and node.id in self.current_except_names"}
{"prompt": "def add_gene_links(gene_obj, build=37):\n    \"\"\"Update a gene object with links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"", "output": "\n    gene_obj['links'] = []\n    for link in gene_obj['links']:\n        if link['build'] == build:\n            link['gene'] = gene_obj\n        else:\n            link['gene'] = None\n    return gene_obj\n\n\ndef remove_gene_links(gene_obj, build=37):\n    \"\"\"Remove a gene object from its links\n\n    Args:\n        gene_obj(dict)\n        build(int)\n\n    Returns:\n        gene_obj(dict): gene_obj updated with many links\n    \"\"\"\n    gene_obj['links'] = [link for link in gene_obj['links']\n                         if link['build']!= build]\n    return gene_obj\n\n\ndef remove_gene_links_by_gene(gene_", "solution": "\n    try:\n        build = int(build)\n    except ValueError:\n        build = 37\n    # Add links that use the hgnc_id\n    hgnc_id = gene_obj['hgnc_id']\n\n    gene_obj['hgnc_link'] = genenames(hgnc_id)\n    gene_obj['omim_link'] = omim(hgnc_id)\n    # Add links that use ensembl_id\n    if not 'ensembl_id' in gene_obj:\n        ensembl_id = gene_obj.get('common',{}).get('ensembl_id')\n    else:\n        ensembl_id = gene_obj['ensembl_id']\n    ensembl_37_link = ensembl(ensembl_id, build=37)\n    ensembl_38_link = ensembl(ensembl_id, build=38)\n    gene_obj['ensembl_37_link'] = ensembl_37_link\n    gene_obj['ensembl_38_link'] = ensembl_38_link\n    gene_obj['ensembl_link'] = ensembl_37_link\n    if build == 38:\n        gene_obj['ensembl_link'] = ensembl_38_link\n    gene_obj['hpa_link'] = hpa(ensembl_id)\n    gene_obj['string_link'] = string(ensembl_id)\n    gene_obj['reactome_link'] = reactome(ensembl_id)\n    gene_obj['clingen_link'] = clingen(hgnc_id)\n    gene_obj['expression_atlas_link'] = expression_atlas(ensembl_id)\n    gene_obj['exac_link'] = exac(ensembl_id)\n    # Add links that use entrez_id\n    gene_obj['entrez_link'] = entrez(gene_obj.get('entrez_id'))\n    # Add links that use omim id\n    gene_obj['omim_link'] = omim(gene_obj.get('omim_id'))\n    # Add links that use hgnc_symbol\n    gene_obj['ppaint_link'] = ppaint(gene_obj['hgnc_symbol'])\n    # Add links that use vega_id\n    gene_obj['vega_link'] = vega(gene_obj.get('vega_id'))\n    # Add links that use ucsc link\n    gene_obj['ucsc_link'] = ucsc(gene_obj.get('ucsc_id'))"}
{"prompt": "def cqt_frequencies(n_bins, fmin, bins_per_octave=12, tuning=0.0):\n    \"\"\"Compute the center frequencies of Constant-Q bins.\n\n    Examples\n    --------\n    >>> # Get the CQT frequencies for 24 notes, starting at C2\n    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))\n    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,\n            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])\n\n    Parameters\n    ----------\n    n_bins  : int > 0 [scalar]\n        Number of constant-Q bins\n\n    fmin    : float > 0 [scalar]\n        Minimum frequency\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)`\n        Deviation from A440 tuning in fractional bins (cents)\n\n    Returns\n    -------\n    frequencies : np.ndarray [shape=(n_bins,)]\n        Center frequency for each CQT bin\n    \"\"\"", "output": "486,  117.954,  124.977,\n             131.951,  138.946,  146.936,  154.926,  163.923,  172.921,\n             181.905,  190.891,  209.882,  228.878,  247.876,  266.874,\n             283.872,  302.868,  321.856,  340.852,  360.847,  380.844,\n             400.842,  419.839,  439.836,  460.834,  480.831,  508.8", "solution": "\n\n    correction = 2.0**(float(tuning) / bins_per_octave)\n    frequencies = 2.0**(np.arange(0, n_bins, dtype=float) / bins_per_octave)\n\n    return correction * fmin * frequencies"}
{"prompt": "def export_keying_material(self, label, olen, context=None):\n        \"\"\"\n        Obtain keying material for application use.\n\n        :param: label - a disambiguating label string as described in RFC 5705\n        :param: olen - the length of the exported key material in bytes\n        :param: context - a per-association context value\n        :return: the exported key material bytes or None\n        \"\"\"", "output": "\n        context = context or {}\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        context = context.copy()\n        ", "solution": "\n        outp = _no_zero_allocator(\"unsigned char[]\", olen)\n        context_buf = _ffi.NULL\n        context_len = 0\n        use_context = 0\n        if context is not None:\n            context_buf = context\n            context_len = len(context)\n            use_context = 1\n        success = _lib.SSL_export_keying_material(self._ssl, outp, olen,\n                                                  label, len(label),\n                                                  context_buf, context_len,\n                                                  use_context)\n        _openssl_assert(success == 1)\n        return _ffi.buffer(outp, olen)[:]"}
{"prompt": "def register_function(scope=None, as_property=False, name=None):\n    \"\"\"Decorator to register a new function with vaex.\n\n    Example:\n\n    >>> import vaex\n    >>> df = vaex.example()\n    >>> @vaex.register_function()\n    >>> def invert(x):\n    >>>     return 1/x\n    >>> df.x.invert()\n\n\n    >>> import numpy as np\n    >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))\n    >>> @vaex.register_function(as_property=True, scope='dt')\n    >>> def dt_relative_day(x):\n    >>>     return vaex.functions.dt_dayofyear(x)/365.\n    >>> df.departure.dt.relative_day\n    \"\"\"", "output": "x.dt.relative_day()\n    array([ 0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,\n            0.       ,  0.       ,  0.       ,  0.       ,", "solution": "\n    prefix = ''\n    if scope:\n        prefix = scope + \"_\"\n        if scope not in scopes:\n            raise KeyError(\"unknown scope\")\n    def wrapper(f, name=name):\n        name = name or f.__name__\n        # remove possible prefix\n        if name.startswith(prefix):\n            name = name[len(prefix):]\n        full_name = prefix + name\n        if scope:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.expression.ds.func, full_name)\n                    args = (self.expression, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            if as_property:\n                setattr(scopes[scope], name, property(closure()))\n            else:\n                setattr(scopes[scope], name, closure())\n        else:\n            def closure(name=name, full_name=full_name, function=f):\n                def wrapper(self, *args, **kwargs):\n                    lazy_func = getattr(self.ds.func, full_name)\n                    args = (self, ) + args\n                    return lazy_func(*args, **kwargs)\n                return functools.wraps(function)(wrapper)\n            setattr(vaex.expression.Expression, name, closure())\n        vaex.expression.expression_namespace[prefix + name] = f\n        return f  # we leave the original function as is\n    return wrapper"}
{"prompt": "def iter_hierarchy(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over parent-child relations\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.\n        :param int sleep: The amount of time to sleep between pages. Defaults to 0 seconds.\n        :rtype: iter[tuple[str,str]]\n        \"\"\"", "output": "\n        page = 1\n        page_size = 5000\n        pages = []\n        while page <= size:\n            pages.append(page)\n            page += 1\n            page_size = (page * page_size) // size\n            page = self.ebi_client.iter_pages(\n                ontology,\n                page_size=page_size,\n                page=page,\n                sleep=sleep,\n            )\n        return pages\n\n    def iter_relations(self, ontology, size=None, sleep=None):\n        \"\"\"Iterates over relations in an ontology\n\n        :param str ontology: The name of the ontology\n        :param int size: The size of each page. Defaults to 500, which is the maximum allowed by the EBI.\n        :param int sleep: The amount of ", "solution": "\n        for term in self.iter_terms(ontology=ontology, size=size, sleep=sleep):\n            try:\n                hierarchy_children_link = term['_links'][HIERARCHICAL_CHILDREN]['href']\n            except KeyError:  # there's no children for this one\n                continue\n\n            response = requests.get(hierarchy_children_link).json()\n\n            for child_term in response['_embedded']['terms']:\n                yield term['label'], child_term['label']"}
{"prompt": "def closeAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"", "output": "\n        self.sendCommand(\"C_CloseAllSessions\", slot)\n\n    def closeAllSessionsSlot(self, slot):\n        \"\"\"\n        C_CloseAllSessionsSlot\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"\n        self.sendCommand(\"C_CloseAllSessionsSlot\", slot)\n\n    def closeAllSessionsSlotAndCloseAllSessions(self, slot):\n        \"\"\"\n        C_CloseAllSessionsSlotAndCloseAllSessions\n\n        :param slot: slot number\n        :type slot: integer\n        \"\"\"\n        self.sendCommand(\"C_CloseAllSessionsSlotAndCloseAllSessions\", slot)\n\n", "solution": "\n        rv = self.lib.C_CloseAllSessions(slot)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def load_panel_app(adapter, panel_id=None, institute='cust000'):\n    \"\"\"Load PanelApp panels into scout database\n    \n    If no panel_id load all PanelApp panels \n    \n    Args:\n        adapter(scout.adapter.MongoAdapter)\n        panel_id(str): The panel app panel id\n    \"\"\"", "output": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "solution": "\n    base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/'\n    \n    hgnc_map = adapter.genes_by_alias()\n    \n    if panel_id:\n        panel_ids = [panel_id]\n\n    if not panel_id:\n        \n        LOG.info(\"Fetching all panel app panels\")\n        data = get_request(base_url.format('list_panels'))\n    \n        json_lines = json.loads(data)\n        \n        panel_ids = [panel_info['Panel_Id'] for panel_info in json_lines['result']]\n    \n    for panel_id in panel_ids:\n        panel_data = get_request(base_url.format('get_panel') + panel_id)\n        \n        parsed_panel = parse_panel_app_panel(\n            panel_info = json.loads(panel_data)['result'], \n            hgnc_map=hgnc_map,\n            institute=institute\n        )\n        parsed_panel['panel_id'] = panel_id\n        \n        if len(parsed_panel['genes']) == 0:\n            LOG.warning(\"Panel {} is missing genes. Skipping.\".format(parsed_panel['display_name']))\n            continue\n        \n        try:\n            adapter.load_panel(parsed_panel=parsed_panel)\n        except Exception as err:\n            raise err"}
{"prompt": "def reads_py(s, **kwargs):\n    \"\"\"Read a .py notebook from a string and return the NotebookNode object.\"\"\"", "output": "\n    nb = nbformat.read(\n        s,\n        as_version=4,\n        nbconvert_version=4,\n        nbconvert_to=\"python\",\n        config_overrides=kwargs.get(\"config_overrides\", {}),\n    )\n    return nb\n\n\ndef reads_ipynb(s, **kwargs):\n    \"\"\"Read an.ipynb notebook from a string and return the NotebookNode object.\"\"\"\n    nb = nbformat.read(\n        s,\n        as_version=4,\n        nbconvert_version=4,\n        nbconvert_to=\"python\",\n        config_overrides=kwargs.get(\"config_overrides\", {}),\n    )\n    return nb\n\n\ndef reads_markdown(s, **kwargs):\n    \"\"\"Read a markdown cell from a string and ", "solution": "\n    nbf, nbm, s = parse_py(s, **kwargs)\n    if nbf == 2:\n        nb = v2.to_notebook_py(s, **kwargs)\n    elif nbf == 3:\n        nb = v3.to_notebook_py(s, **kwargs)\n    else:\n        raise NBFormatError('Unsupported PY nbformat version: %i' % nbf)\n    return nb"}
{"prompt": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"", "output": "\n    head, tail = os.path.split(path)\n    if not head or not os.path.isdir(head):\n        os.makedirs(head, mode)\n    for part in tail.split('/'):\n        mkdirs(os.path.join(head, part), mode)\n\n\ndef touch(path, times=None):\n    \"\"\"\n    Creates a file at path with optional modification times. If times\n    is omitted, the current time is used.\n\n    :param path: The file to create\n    :type path: str\n    :param times: The modification times to set for the file\n    :type times: int or list of ints\n    \"\"\"\n    st = os.stat(path)\n    os.utime(path, (st.st_atime, st.st_mtime))\n\n\ndef copyfile(src, dst):\n    \"\"\"\n    Copy src to dst", "solution": "\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)"}
{"prompt": "def _resolve_graph(self, distribution_names=None, leaf_name='x'):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        appropriate `str`.)\n      leaf_name: `str` used when no maker depends on a particular\n        `model` element.\n\n    Returns:\n      graph: `tuple` of `(str tuple)` pairs representing the name of each\n        distribution (maker) and the names of its dependencies.\n\n    #### Example\n\n    ```python\n    d = tfd.JointDistributionSequential([\n                     tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),\n        lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),\n                     tfd.Normal(loc=0, scale=2.),\n        lambda n, g: tfd.Normal(loc=n, scale=g),\n    ])\n    d._resolve_graph()\n    # ==> (\n    #       ('e', ()),\n    #       ('g', ('e',)),\n    #       ('n', ()),\n    #       ('x', ('n', 'g')),\n    #     )\n    ```\n\n    \"\"\"", "output": "corresponding to each `model`\n        element.\n    \"\"\"\n    graph = []\n    for distribution_name in distribution_names:\n      graph.append(\n          (distribution_name, [leaf_name] if leaf_name else []))\n    return graph\n\n  def _resolve_distribution(self, distribution_names):\n    \"\"\"Creates a `tuple` of `tuple`s of dependencies.\n\n    This function is **experimental**. That said, we encourage its use\n    and ask that you report problems to `tfprobability@tensorflow.org`.\n\n    Args:\n      distribution_names: `list` of `str` or `None` names corresponding to each\n        of `model` elements. (`None`s are expanding into the\n        ", "solution": "\n    # This function additionally depends on:\n    #   self._dist_fn_args\n    #   self._dist_fn_wrapped\n    # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better,\n    # ignore args prefixed with `_`.\n    if distribution_names is None or any(self._dist_fn_args):\n      distribution_names = _resolve_distribution_names(\n          self._dist_fn_args, distribution_names, leaf_name)\n    if len(set(distribution_names)) != len(distribution_names):\n      raise ValueError('Distribution names must be unique: {}'.format(\n          distribution_names))\n    if len(distribution_names) != len(self._dist_fn_wrapped):\n      raise ValueError('Distribution names must be 1:1 with `rvs`.')\n    return tuple(zip(distribution_names,\n                     tuple(() if a is None else a for a in self._dist_fn_args)))"}
{"prompt": "def cli_encrypt(context, key):\n    \"\"\"\n    Encrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to encrypt to disk before attempting to\n    upload, allowing uploads retries and segmented encrypted objects.\n\n    See :py:mod:`swiftly.cli.encrypt` for context usage information.\n\n    See :py:class:`CLIEncrypt` for more information.\n    \"\"\"", "output": "\n    context.io_manager.encrypt(key)\n\n\ndef cli_decrypt(context, key):\n    \"\"\"\n    Decrypts context.io_manager's stdin and sends that to\n    context.io_manager's stdout.\n\n    This can be useful to decrypt from disk before attempting to\n    download, allowing downloads retries and segmented encrypted objects.\n\n    See :py:mod:`swiftly.cli.decrypt` for context usage information.\n\n    See :py:class:`CLIDecrypt` for more information.\n    \"\"\"\n    context.io_manager.decrypt(key)\n\n\ndef cli_upload_object(context, object_name, object_size, object_data):\n    \"\"\"\n    Uploads object_name with object_size bytes to swift ", "solution": "\n    with context.io_manager.with_stdout() as stdout:\n        with context.io_manager.with_stdin() as stdin:\n            for chunk in aes_encrypt(key, stdin, preamble=AES256CBC):\n                stdout.write(chunk)\n            stdout.flush()"}
{"prompt": "def _confirm_exit(self):\n        \"\"\"confirm shutdown on ^C\n        \n        A second ^C, or answering 'y' within 5s will cause shutdown,\n        otherwise original SIGINT handler will be restored.\n        \n        This doesn't work on Windows.\n        \"\"\"", "output": "\n        if self.sigint_handler is None:\n            return\n        \n        signal.signal(signal.SIGINT, self.sigint_handler)\n        \n        signal.alarm(1)\n        \n        try:\n            signal.alarm(0)\n            \n            while True:\n                signal.pause()\n                \n                if self._exit_code is None:\n                    break\n                \n                if self._exit_code == 0:\n                    break\n                \n                time.sleep(0.1)\n        finally:\n            signal.alarm(0)\n            \n            signal.signal(signal.SIGINT, signal.SIG_DFL)\n            \n            self.sigint_handler = None\n            \n    def _run_command(self, command, cwd=None, env=None, shell=False,\n                     stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                     universal_newlines=False, buf", "solution": "\n        # FIXME: remove this delay when pyzmq dependency is >= 2.1.11\n        time.sleep(0.1)\n        sys.stdout.write(\"Shutdown Notebook Server (y/[n])? \")\n        sys.stdout.flush()\n        r,w,x = select.select([sys.stdin], [], [], 5)\n        if r:\n            line = sys.stdin.readline()\n            if line.lower().startswith('y'):\n                self.log.critical(\"Shutdown confirmed\")\n                ioloop.IOLoop.instance().stop()\n                return\n        else:\n            print \"No answer for 5s:\",\n        print \"resuming operation...\"\n        # no answer, or answer is no:\n        # set it back to original SIGINT handler\n        # use IOLoop.add_callback because signal.signal must be called\n        # from main thread\n        ioloop.IOLoop.instance().add_callback(self._restore_sigint_handler)"}
{"prompt": "def send_raw(self, stream, msg_list, flags=0, copy=True, ident=None):\n        \"\"\"Send a raw message via ident path.\n\n        This method is used to send a already serialized message.\n\n        Parameters\n        ----------\n        stream : ZMQStream or Socket\n            The ZMQ stream or socket to use for sending the message.\n        msg_list : list\n            The serialized list of messages to send. This only includes the\n            [p_header,p_parent,p_content,buffer1,buffer2,...] portion of\n            the message.\n        ident : ident or list\n            A single ident or a list of idents to use in sending.\n        \"\"\"", "output": "\n        ident = ident or []\n        if isinstance(ident, list):\n            ident = ident[0]\n        msg_list = msg_list[:1] + msg_list[1:]\n        msg_list = msg_list + [ident]\n        msg_list = msg_list + [b\"\\0\"] * (4 - len(ident))\n        msg_list = msg_list + [b\"\\0\"] * (4 - len(msg_list))\n        msg_list = msg_list + ident\n        msg_list = msg_list + [b\"\\0\"] * (4 - len(msg_list))\n        msg_list = msg_list + [b\"\\0\"] * (4 - len(msg_list))\n        msg_list = msg_list + [b\"\\0\"] * (4 - len(msg_", "solution": "\n        to_send = []\n        if isinstance(ident, bytes):\n            ident = [ident]\n        if ident is not None:\n            to_send.extend(ident)\n\n        to_send.append(DELIM)\n        to_send.append(self.sign(msg_list))\n        to_send.extend(msg_list)\n        stream.send_multipart(msg_list, flags, copy=copy)"}
{"prompt": "def resolve_expression(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"", "output": "\n        return self.resolve_expression_or_raise(*args, **kwargs)\n\n    def resolve_expression_or_raise(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"\n        expression = super().resolve_expression_or_raise(*args, **kwargs)\n        if isinstance(expression, HStoreColumn):\n            return expression\n        else:\n            raise TypeError(\n                f\"Expected HStoreColumn but got {type(expression)}\"\n            )\n\n    def resolve_expression_or_raise_by_name(self, *args, **kwargs) -> HStoreColumn:\n        \"\"\"Resolves the expression into a :see:HStoreColumn expression.\"\"\"\n        expression = super().resolve_expression_or_raise_by_name(*args, **kwargs)\n        if isinstance(expression, HStoreColumn", "solution": "\n\n        original_expression = super().resolve_expression(*args, **kwargs)\n        expression = HStoreColumn(\n            original_expression.alias,\n            original_expression.target,\n            self.key\n        )\n        return expression"}
{"prompt": "def insert_default_options():\n    \"\"\"insert default options to sys.argv\n    \"\"\"", "output": "\n    sys.argv = [sys.argv[0]] + list(default_options)\n\ndef parse_args(argv):\n    \"\"\"parse command line arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog='pytest-pytest-runner',\n        description='pytest runner for the pytest framework',\n        epilog='pytest-pytest-runner is an extension to the pytest framework '\n                'that allows running pytest tests from command line. '\n                'It also allows running pytest tests from within '\n                'the pytest framework itself.',\n    )\n    parser.add_argument(\n        '--config-file',\n        metavar='FILE',\n        dest='config_file',\n        action='store',\n        help='pytest configuration file',", "solution": "\n    options = get_default_options()\n    options.reverse()\n    for arg in options:\n        sys.argv.insert(1, arg)"}
{"prompt": "def get_components(self):\n        \"\"\"\n        Returns the components of this name, as a sequence of 2-tuples.\n\n        :return: The components of this name.\n        :rtype: :py:class:`list` of ``name, value`` tuples.\n        \"\"\"", "output": "\n        return list(self.components)\n\n    def has_component(self, name):\n        \"\"\"\n        Returns ``True`` if this name has component ``name``.\n\n        :param name: the name of the component.\n        :type name: str\n        :return: ``True`` if this name has component ``name``.\n        :rtype: bool\n        \"\"\"\n        return name in self.components\n\n    def has_components(self, *names):\n        \"\"\"\n        Returns ``True`` if this name has components ``names``.\n\n        :param names: names of the components.\n        :type names: str\n        :return: ``True`` if this name has components ``names``.\n        :rtype: bool\n        \"\"\"\n        for name in names:\n            if not self.has_component(name):\n                return False\n        return True\n\n    def has_components_of(self, cls):\n        \"\"\"\n        Returns ``True``", "solution": "\n        result = []\n        for i in range(_lib.X509_NAME_entry_count(self._name)):\n            ent = _lib.X509_NAME_get_entry(self._name, i)\n\n            fname = _lib.X509_NAME_ENTRY_get_object(ent)\n            fval = _lib.X509_NAME_ENTRY_get_data(ent)\n\n            nid = _lib.OBJ_obj2nid(fname)\n            name = _lib.OBJ_nid2sn(nid)\n\n            # ffi.string does not handle strings containing NULL bytes\n            # (which may have been generated by old, broken software)\n            value = _ffi.buffer(_lib.ASN1_STRING_data(fval),\n                                _lib.ASN1_STRING_length(fval))[:]\n            result.append((_ffi.string(name), value))\n\n        return result"}
{"prompt": "def fetch_items(self, category, **kwargs):\n        \"\"\"Fetch the pages\n\n        :param category: the category of items to fetch\n        :param kwargs: backend arguments\n\n        :returns: a generator of items\n        \"\"\"", "output": "\n        page_size = kwargs.get('page_size', 20)\n        page_size = min(page_size, self._max_pages)\n        page_size = max(page_size, 1)\n        pages = range(page_size)\n        pages_iter = iter(pages)\n        pages_iter = itertools.islice(pages_iter, page_size)\n        pages_iter = itertools.chain(pages_iter, pages_iter)\n        pages_iter = itertools.islice(pages_iter, page_size)\n        pages_iter = itertools.chain(pages_iter, pages_iter)\n        pages_iter = itertools.islice(pages_iter, page_size)\n        pages_iter = itertools.chain(pages_iter, pages_iter)\n", "solution": "\n        from_date = kwargs['from_date']\n        reviews_api = kwargs['reviews_api']\n\n        mediawiki_version = self.client.get_version()\n        logger.info(\"MediaWiki version: %s\", mediawiki_version)\n\n        if reviews_api:\n            if ((mediawiki_version[0] == 1 and mediawiki_version[1] >= 27) or mediawiki_version[0] > 1):\n                fetcher = self.__fetch_1_27(from_date)\n            else:\n                logger.warning(\"Reviews API only available in MediaWiki >= 1.27\")\n                logger.warning(\"Using the Pages API instead\")\n                fetcher = self.__fetch_pre1_27(from_date)\n        else:\n            fetcher = self.__fetch_pre1_27(from_date)\n\n        for page_reviews in fetcher:\n            yield page_reviews"}
{"prompt": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"", "output": "\n    session = Session()\n    try:\n        yield session\n        session.close()\n    except:\n        session.rollback()\n        raise\n\n\ndef session_scope(session=None):\n    \"\"\"\n    Contextmanager that will create a session and return it in a scope\n    around a series of operations.\n    \"\"\"\n    if session is None:\n        session = create_session()\n    session = session\n    try:\n        yield session\n        session.close()\n    except:\n        session.rollback()\n        raise\n\n\ndef sessionmaker(bind=None, expire_on_commit=False, autocommit=True,\n                class_=Session, **kwargs):\n    \"\"\"\n    Create a new :class:`.Session` instance with bound engines or connections.\n\n    The :class:`.Session` instance will be ", "solution": "\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()"}
{"prompt": "def disable_report(self, reportid):\n        \"\"\"disable the report of the given id\"\"\"", "output": "\n        self.reports[reportid] = False\n\n    def enable_report(self, reportid):\n        \"\"\"enable the report of the given id\"\"\"\n        self.reports[reportid] = True\n\n    def report_status(self, reportid, status):\n        \"\"\"report the status of the given id\"\"\"\n        self.reports[reportid] = status\n\n    def report_error(self, reportid, error):\n        \"\"\"report the error of the given id\"\"\"\n        self.reports[reportid] = error\n\n    def report_warning(self, reportid, warning):\n        \"\"\"report the warning of the given id\"\"\"\n        self.reports[reportid] = warning\n\n    def report_info(self, reportid, info):\n        \"\"\"report the info", "solution": "\n        reportid = reportid.upper()\n        self._reports_state[reportid] = False"}
{"prompt": "def chrono(ctx, app_id, sentence_file,\n           json_flag, sentence, doc_time, request_id):\n    # type: (Context, unicode, Optional[IO], bool, unicode, unicode, unicode) -> None  # NOQA\n    \"\"\"Extract expression expressing date and time and normalize its value \"\"\"", "output": "\n    date_time = datetime.strptime(sentence, '%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.strftime('%Y-%m-%dT%H:%M:%S')\n    date_time = date_time.", "solution": "\n\n    app_id = clean_app_id(app_id)\n    sentence = clean_sentence(sentence, sentence_file)\n\n    api = GoolabsAPI(app_id)\n    ret = api.chrono(\n        sentence=sentence,\n        doc_time=doc_time,\n        request_id=request_id,\n    )\n\n    if json_flag:\n        click.echo(format_json(api.response.json()))\n        return\n\n    for pair in ret['datetime_list']:\n        click.echo(u'{0}: {1}'.format(text(pair[0]), pair[1]))"}
{"prompt": "def _get_unpacking_extra_info(node, infered):\n    \"\"\"return extra information to add to the message for unpacking-non-sequence\n    and unbalanced-tuple-unpacking errors\n    \"\"\"", "output": "\n    extra = []\n    unpacking_error = False\n    unpacking_nonsequence = False\n    unpacking_unbalanced_tuple = False\n    unpacking_sequence_error = False\n    unpacking_sequence_nonsequence = False\n    unpacking_sequence_unbalanced_tuple = False\n    unpacking_sequence_sequence_error = False\n    unpacking_sequence_sequence_nonsequence = False\n    unpacking_sequence_sequence_unbalanced_tuple = False\n    unpacking_sequence_sequence_sequence_error = False\n    unpacking_sequence_sequence_sequence_nonsequence = False\n    unpacking_sequence_sequence_sequence_unbalanced_tuple = False\n    unpacking_sequence_sequence_sequence_sequence_error = False\n    unpacking_sequence_sequence_sequence_sequence_", "solution": "\n    more = \"\"\n    infered_module = infered.root().name\n    if node.root().name == infered_module:\n        if node.lineno == infered.lineno:\n            more = \" %s\" % infered.as_string()\n        elif infered.lineno:\n            more = \" defined at line %s\" % infered.lineno\n    elif infered.lineno:\n        more = \" defined at line %s of %s\" % (infered.lineno, infered_module)\n    return more"}
{"prompt": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"", "output": "\n        godot_options = info.options[\"godot_options\"]\n        godot_options.set_option(godot_options.OPTION_GODOT_OPTIONS_SHOW_OPTIONS, True)\n        godot_options.set_option(godot_options.OPTION_GODOT_OPTIONS_SHOW_OPTIONS_HELP, \"Show this help menu.\")\n        godot_options.set_option(godot_options.OPTION_GODOT_OPTIONS_SHOW_OPTIONS_HELP_HELP, \"Show this help menu.\")\n        godot_options.set_option(godot_options.OPTION_GODOT_OPTIONS_SHOW_OPTIONS_HELP_HELP_HELP, \"Show this ", "solution": "\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )"}
{"prompt": "def getMappingsOnDeviceType(self, typeId, draft=False):\n        \"\"\"\n        Get all the mappings for a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        path ='mappings/deviceType/%s/mappings' % (typeId)\n        if draft:\n            path += '?draft=true'\n        return self._get(path)\n\n    def getMappingsOnDeviceTypes(self, typeIds, draft=False):\n        \"\"\"\n        Get all the mappings for a device type.\n        Parameters:\n            - typeIds (list of string) - the device types\n            - draft (boolean) - draft or active\n        Throws APIException on failure.\n        \"\"\"\n        path ='mappings/deviceType/%s/mappings' % (','.join(typeIds))\n        if draft:\n            path += '?draft=true'\n        return self._get(path)\n\n    def getMappingsOnDeviceType(self, type", "solution": "\n        if draft:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"/draft\", typeId)\n        else:\n            req = ApiClient.allDeviceTypeMappingsUrl % (self.host, \"\", typeId)\n\n        resp = requests.get(req, auth=self.credentials, verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"All device type mappings retrieved\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error getting all device type mappings\", resp)\n        return resp.json()"}
{"prompt": "def video_list(request, username=None):\n    \"\"\"\n    list of videos of a user\n    if username does not set, shows the currently logged in user\n    \"\"\"", "output": "\n    if username is None:\n        username = request.user.username\n    videos = Video.objects.filter(owner=username)\n    return render(request, 'videos/video_list.html', {'videos': videos})\n\n\ndef video(request, video_id):\n    \"\"\"\n    video details of a video\n    \"\"\"\n    video = Video.objects.get(id=video_id)\n    return render(request, 'videos/video.html', {'video': video})\n\n\ndef video_detail(request, video_id):\n    \"\"\"\n    video details of a video\n    \"\"\"\n    video = Video.objects.get(id=video_id)\n    return render(request, 'videos/video_detail.html', {'video': video})\n\n\ndef video_detail_video(request, video_id):\n    \"\"\"\n", "solution": "\n\n    # If user is not authenticated and username is None, raise an error\n    if username is None and not request.user.is_authenticated():\n        from django.http import Http404\n        raise Http404\n\n    from django.contrib.auth.models import User\n    user = User.objects.get(username=username) if username else request.user\n\n    # loop through the videos of the user\n    videos = Video.objects.filter(user=user).all()\n    video_params = []\n    for video in videos:\n        video_params.append(_video_params(request, video.video_id))\n\n    return render_to_response(\n        \"django_youtube/videos.html\",\n        {\"video_params\": video_params},\n        context_instance=RequestContext(request)\n    )"}
{"prompt": "def weekday(year_or_num, month=None, day=None, full=False):\n    \"\"\"Simple tag - returns the weekday of the given (year, month, day) or of given (weekday_number).\n\n    Usage (in template):\n\n    {% weekday 2014 3 3 %}\n\n    Result: Mon\n\n    Return abbreviation by default. To return full name: pass full=True\n\n    {% weekday 2014 3 3 full=True %}\n\n    Result: Monday\n\n\n    When only number of weekday is given then 0 is considered as \"Monday\"\n\n    {% weekday 0 full=True %}\n\n    Result: Monday\n\n    \"\"\"", "output": "\n    fn = 'weekday' if full else 'weekdayname'\n    return fn[month](year_or_num, day, full)\n\n\ndef month(year_or_num, month=None, day=None, full=False):\n    \"\"\"Simple tag - returns the month of the given (year, month, day) or of given (month).\n\n    Usage (in template):\n\n    {% month 2014 %}\n\n    Result: February\n\n    Return abbreviation by default. To return full name: pass full=True\n\n    {% month 2014 full=True %}\n\n    Result: February\n\n    \"\"\"\n    fn ='month' if full else'monthname'\n    return fn[month](year_or_num, day, full)\n\n\ndef year(year_or_num, month", "solution": "\n    if any([month, day]) and not all([month, day]):\n        raise TemplateSyntaxError(\"weekday accepts 1 or 3 arguments plus optional 'full' argument\")\n\n    try:\n        if all([year_or_num, month, day]):\n            weekday_num = date(*map(int, (year_or_num, month, day))).weekday()\n        else:\n            weekday_num = year_or_num\n        if full:\n            return WEEKDAYS[weekday_num]\n        else:\n            return WEEKDAYS_ABBR[weekday_num]\n    except Exception:\n        return"}
{"prompt": "def clinvar_submissions(store, user_id, institute_id):\n    \"\"\"Get all Clinvar submissions for a user and an institute\"\"\"", "output": "\n    submissions = store.get_submissions(user_id, institute_id)\n    return submissions\n\n\ndef clinvar_submissions_count(store, user_id, institute_id):\n    \"\"\"Get the number of Clinvar submissions for a user and an institute\"\"\"\n    return store.get_submissions_count(user_id, institute_id)\n\n\ndef clinvar_submissions_count(store, user_id, institute_id, clinvar_id):\n    \"\"\"Get the number of Clinvar submissions for a user and an institute\"\"\"\n    return store.get_submissions_count(user_id, institute_id, clinvar_id)\n\n\ndef clinvar_submissions_count(store, user_id, institute_id, clin", "solution": "\n    submissions = list(store.clinvar_submissions(user_id, institute_id))\n    return submissions"}
{"prompt": "def get_table(self, arch, pattern, colored=False, verbose=False):\n        \"\"\"\n        This function is used in sys command (when user want to find a specific syscall)\n\n        :param Architecture for syscall table;\n        :param Searching pattern;\n        :param Flag for verbose output\n        :return Return a printable table of matched syscalls\n        \"\"\"", "output": "\n        syscalls = []\n        syscalls_pattern = pattern.replace('*', '.*')\n        syscalls_pattern = syscalls_pattern.replace('?', '.*')\n        syscalls_pattern = syscalls_pattern.replace('[', '.*')\n        syscalls_pattern = syscalls_pattern.replace(']', '.*')\n        syscalls_pattern = syscalls_pattern.replace('{', '.*')\n        syscalls_pattern = syscalls_pattern.replace('}', '.*')\n        syscalls_pattern = syscalls_pattern.replace('[', '.*')\n        syscalls_pattern = syscalls_pattern.replace(']', '.*", "solution": "\n\n        rawtable = self.search(arch, pattern)\n        if len(rawtable) == 0:\n            return None\n\n        used_hd = self.__fetch_used_headers(rawtable, verbose)\n        table   = [self.__make_colored_row(used_hd, 'yellow,bold', upper=True) if colored else used_hd]\n\n        for command in rawtable:\n            cur_tb_field = []\n            for hd in used_hd:\n                value = command[hd]\n                cur_tb_field.append(self.__make_colored_field(value, hd, verbose=verbose))\n            table.append(cur_tb_field)\n        return DoubleTable(table)"}
{"prompt": "def str_variant(store, institute_id, case_name, variant_id):\n    \"\"\"Pre-process an STR variant entry for detail page.\n\n    Adds information to display variant\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        institute_id(str)\n        case_name(str)\n        variant_id(str)\n\n    Returns:\n        detailed_information(dict): {\n            'institute': <institute_obj>,\n            'case': <case_obj>,\n            'variant': <variant_obj>,\n            'overlapping_snvs': <overlapping_snvs>,\n            'manual_rank_options': MANUAL_RANK_OPTIONS,\n            'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n        }\n        \"\"\"", "output": "RANK_OPTIONS,\n           'manual_rank_option_values': MANUAL_RANK_OPTION_VALUES,\n           'manual_rank_option_values_str': MANUAL_RANK_OPTION_VALUE_STR,\n           'manual_rank_option_values_int': MANUAL_RANK_OPTION_VALUE_INT,\n           'manual_rank_option_values_float': MANUAL_RANK_OPTION_VALUE_FLOAT,\n           'manual_rank_option_values_bool': MANUAL_RANK_OPTION_VALUE_BOOL,\n           'manual_rank_option_values_date': MANUAL_RANK_OPTION_VALUE_DATE,\n           'manual_rank_option_values_datetime': MANUAL_RAN", "solution": "\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    variant_obj =  store.variant(variant_id)\n\n    # fill in information for pilup view\n    variant_case(store, case_obj, variant_obj)\n\n    variant_obj['callers'] = callers(variant_obj, category='str')\n\n    # variant_obj['str_ru']\n    # variant_obj['str_repid']\n    # variant_obj['str_ref']\n\n    variant_obj['comments'] = store.events(institute_obj, case=case_obj,\n                                           variant_id=variant_obj['variant_id'], comments=True)\n\n    return {\n        'institute': institute_obj,\n        'case': case_obj,\n        'variant': variant_obj,\n        'overlapping_snvs': overlapping_snvs,\n        'manual_rank_options': MANUAL_RANK_OPTIONS,\n        'dismiss_variant_options': DISMISS_VARIANT_OPTIONS\n    }"}
{"prompt": "def index(context, update):\n    \"\"\"Create indexes for the database\"\"\"", "output": "\n    context.configure(\n        connection=context.engine.connect(),\n        target_metadata=target_metadata,\n        include_object=include_object,\n        include_column=include_column,\n        indexes=[\n            Index('idx_%s_%s' % (context.target_metadata, context.schema),\n                  context.target_metadata, unique=True)\n        ]\n    )\n\n\ndef upgrade():\n    \"\"\"Upgrade database schema and/or data, creating a new revision.\"\"\"\n    context = migration.context()\n    version_table = get_table('version', context)\n    version_table.create()\n    version_table.insert(\n        dict(\n            id=0,\n            project_id=context.project_id,\n            version_id=0,\n            instance", "solution": "\n    LOG.info(\"Running scout index\")\n    adapter = context.obj['adapter']\n    \n    if update:\n        adapter.update_indexes()\n    else:\n        adapter.load_indexes()"}
{"prompt": "def remove_pid_file(self):\n        \"\"\"Remove the pid file.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"", "output": "\n        self.pid_file = None\n\n    def register_signal_handler(self, signum, action):\n        \"\"\"Register a signal handler.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"\n        self.signal_handlers.append((signum, action))\n\n    def register_signal_handler_blocking(self, signum, action):\n        \"\"\"Register a signal handler blocking.\n\n        This should be called at shutdown by registering a callback with\n        :func:`reactor.addSystemEventTrigger`. This needs to return\n        ``None``.\n        \"\"\"\n        self.signal_handlers_blocking.append((signum, action))\n\n    def register_signal_handler_non", "solution": "\n        pid_file = os.path.join(self.profile_dir.pid_dir, self.name + u'.pid')\n        if os.path.isfile(pid_file):\n            try:\n                self.log.info(\"Removing pid file: %s\" % pid_file)\n                os.remove(pid_file)\n            except:\n                self.log.warn(\"Error removing the pid file: %s\" % pid_file)"}
{"prompt": "def getoutput_pexpect(self, cmd):\n        \"\"\"Run a command and return its stdout/stderr as a string.\n\n        Parameters\n        ----------\n        cmd : str\n          A command to be executed in the system shell.\n\n        Returns\n        -------\n        output : str\n          A string containing the combination of stdout and stderr from the\n        subprocess, in whatever order the subprocess originally wrote to its\n        file descriptors (so the order of the information in this string is the\n        correct order as would be seen if running the command in a terminal).\n        \"\"\"", "output": "\n        child = pexpect.spawn(cmd, logfile=sys.stdout)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)\n        child.expect(self.prompt)\n        child.send(self.command)", "solution": "\n        try:\n            return pexpect.run(self.sh, args=['-c', cmd]).replace('\\r\\n', '\\n')\n        except KeyboardInterrupt:\n            print('^C', file=sys.stderr, end='')"}
{"prompt": "def get_signature(self, value):\n        \"\"\"Returns the signature for the given value\"\"\"", "output": "\n        signature = Signature()\n        signature.signature = value.signature\n        signature.name = value.name\n        signature.type = value.type\n        signature.value = value.value\n        return signature\n\n    def get_value(self, value):\n        \"\"\"Returns the value for the given signature\"\"\"\n        return value\n\n    def get_values(self, value):\n        \"\"\"Returns the values for a given signature\"\"\"\n        return value\n\n    def get_type(self, value):\n        \"\"\"Returns the type for the given value\"\"\"\n        return value\n\n    def get_types(self, value):\n        \"\"\"Returns the types for a given value\"\"\"\n        return value\n\n    def has_signature(self, value):\n        \"\"\"Checks if the given value has a signature\"\"\"\n        return value.signature is not None\n\n    def has_signature_type(self, value):\n        \"\"\"Checks if the given value has a signature type\"\"\"\n        return value.", "solution": "\n        value = want_bytes(value)\n        key = self.derive_key()\n        sig = self.algorithm.get_signature(key, value)\n        return base64_encode(sig)"}
{"prompt": "def generate_config_file(self):\n        \"\"\"generate default config file from Configurables\"\"\"", "output": "\n        config_file = os.path.join(self.config_dir, \"config.yaml\")\n        config_data = yaml.safe_dump(self.config_data)\n        with open(config_file, \"w\") as f:\n            f.write(config_data)\n        return config_file\n\n    def write_config_file(self):\n        \"\"\"write config file to disk\"\"\"\n        config_file = os.path.join(self.config_dir, \"config.yaml\")\n        with open(config_file, \"w\") as f:\n            f.write(self.config_data)\n\n    def write_config_file_if_changed(self):\n        \"\"\"write config file to disk if config file has changed\"\"\"\n        config_file = os.path.join(self.config_dir, \"config.yaml\")\n        config_data = yaml.safe_dump(self.config_data)\n        with open(config_file, \"r\") as f:\n            config_data = f.read()\n        if config_data!= self.config_data:\n            self.write_", "solution": "\n        lines = [\"# Configuration file for %s.\"%self.name]\n        lines.append('')\n        lines.append('c = get_config()')\n        lines.append('')\n        for cls in self.classes:\n            lines.append(cls.class_config_section())\n        return '\\n'.join(lines)"}
{"prompt": "def _republish_displaypub(self, content, eid):\n        \"\"\"republish individual displaypub content dicts\"\"\"", "output": "\n        for pub in self.displaypubs:\n            if pub['eid'] == eid:\n                pub['content'] = content\n                return\n        self.displaypubs.append({'eid': eid, 'content': content})\n\n    def publish_displaypub(self, eid, content):\n        \"\"\"publish individual displaypub content dicts\"\"\"\n        pub = self.displaypubs[eid]\n        pub['content'] = content\n\n    def publish_displaypubs(self, eids):\n        \"\"\"publish individual displaypub content dicts\"\"\"\n        for eid in eids:\n            self.publish_displaypub(eid, {})\n\n    def publish_displaypubs_json(self, eids):\n        \"\"\"publish individual display", "solution": "\n        try:\n            ip = get_ipython()\n        except NameError:\n            # displaypub is meaningless outside IPython\n            return\n        md = content['metadata'] or {}\n        md['engine'] = eid\n        ip.display_pub.publish(content['source'], content['data'], md)"}
{"prompt": "def __make_points_for_label(self, ts, data, label, prefix, gun_stats):\n        \"\"\"x\n        Make a set of points for `this` label\n\n        overall_quantiles, overall_meta, net_codes, proto_codes, histograms\n        \"\"\"", "output": "\n        quantiles = ts\n        meta = ts\n        net_codes = ts\n        proto_codes = ts\n        histograms = ts\n\n        quantiles = quantiles.copy()\n        quantiles.index = pd.to_datetime(quantiles.index)\n        quantiles.index = quantiles.index.tz_localize('UTC')\n        quantiles.index = quantiles.index.tz_convert('US/Eastern')\n\n        quantiles = quantiles.resample('1min').mean()\n\n        quantiles = quantiles.dropna()\n        quantiles = quantiles.dropna(how='all')\n        quantiles = quantiles.dropna(axis=1, how='all')\n        quantiles = quantiles.", "solution": "\n        label_points = list()\n\n        label_points.extend(\n            (\n                # overall quantiles for label\n                self.__make_points(\n                    prefix + \"overall_quantiles\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_quantile_fields(data)\n                ),\n                # overall meta (gun status) for label\n                self.__make_points(\n                    prefix + \"overall_meta\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_overall_meta_fields(data, gun_stats)\n                ),\n                # net codes for label\n                self.__make_points(\n                    prefix + \"net_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_netcodes_fields(data)\n                ),\n                # proto codes for label\n                self.__make_points(\n                    prefix + \"proto_codes\",\n                    {\"label\": label},\n                    ts,\n                    self.__make_protocodes_fields(data)\n                )\n            )\n        )\n        # histograms, one row for each bin\n        if self.histograms:\n            for bin_, count in zip(data[\"interval_real\"][\"hist\"][\"bins\"],\n                                   data[\"interval_real\"][\"hist\"][\"data\"]):\n                label_points.append(\n                    self.__make_points(\n                        prefix + \"histograms\",\n                        {\"label\": label},\n                        ts,\n                        {\"bin\": bin_, \"count\": count}\n                    )\n                )\n        return label_points"}
{"prompt": "def app_url_defaults(self, f):\n        \"\"\"Same as :meth:`url_defaults` but application wide.\n        \"\"\"", "output": "\n        def wrapper(*args, **kwargs):\n            kwargs.update({'app': self.app})\n            return f(*args, **kwargs)\n        return wrapper\n\n    def url_defaults(self, f):\n        \"\"\"Same as :meth:`url_defaults` but application wide.\n        \"\"\"\n        def wrapper(*args, **kwargs):\n            kwargs.update({'app': self.app,'request': args[0]})\n            return f(*args, **kwargs)\n        return wrapper\n\n    def url_value_preprocessor(self, f):\n        \"\"\"Same as :meth:`url_value_preprocessor` but application wide.\n        \"\"\"\n        def wrapper(*args, **kwargs):\n            kwargs.update({'app': self.app})\n            return f(*args, **kwargs)\n        return wrapper\n\n    def url_value_post_processor(self, f):\n        \"\"\"Same as :meth:`url_value_post_processor` but application wide.", "solution": "\n        self.record_once(lambda s: s.app.url_default_functions\n            .setdefault(None, []).append(f))\n        return f"}
{"prompt": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"", "output": "\n        writer = csv.writer(tmp_file, dialect='excel')\n        writer.writerow(table.fields)\n        rows = []\n        for rec in table.rows:\n            row = []\n            for field in table.fields:\n                row.append(rec[field])\n            rows.append(row)\n        writer.writerows(rows)\n\n    def bulk_load(self, tmp_file, table, **args):\n        \"\"\"\n        Loads a tab-delimited file into a database table\n        \"\"\"\n        reader = csv.reader(tmp_file, dialect='excel')\n        reader.next()\n        table.rows = [list(rec) for rec in reader]\n        table.create()\n\n    def bulk_load_from_file(self, tmp_file, table, **args):\n        \"\"\"\n        Load", "solution": "\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)"}
{"prompt": "def  make_html_items( self, items ):\n        \"\"\" convert a field's content into some valid HTML \"\"\"", "output": "\n        html = []\n        for field in items:\n            html.append( self.make_html_field( field ) )\n        return html\n\n    def make_html_field( self, field ):\n        \"\"\" convert a field into a HTML field \"\"\"\n        html = []\n        html.append( self.make_html_field( field.name ) )\n        html.append( self.make_html_field( field.value ) )\n        return html\n\n    def make_html_field( self, field ):\n        \"\"\" convert a field into a HTML field \"\"\"\n        html = []\n        html.append( self.make_html_field( field.name ) )\n        html.append( self.make_html_field( field.value ) )\n        html.append( self.make_html_", "solution": "\n        lines = []\n        for item in items:\n            if item.lines:\n                lines.append( self.make_html_code( item.lines ) )\n            else:\n                lines.append( self.make_html_para( item.words ) )\n\n        return string.join( lines, '\\n' )"}
{"prompt": "def p_relate_statement_2(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name ACROSS rel_id DOT phrase\"\"\"", "output": "\n        p[0] = p[2]\n\n    def p_relate_statement_3(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name DOT phrase\"\"\"\n        p[0] = p[2]\n\n    def p_relate_statement_4(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name DOT rel_id DOT phrase\"\"\"\n        p[0] = p[2]\n\n    def p_relate_statement_5(self, p):\n        \"\"\"statement : RELATE instance_name TO instance_name DOT rel_id DOT rel_id DOT phrase\"\"\"\n        p[0] = p[2]\n\n    def p_relate_statement_6(self, p):\n        \"\"\"statement : RELATE instance_", "solution": "\n        p[0] = RelateNode(from_variable_name=p[2],\n                          to_variable_name=p[4],\n                          rel_id=p[6],\n                          phrase=p[8])"}
{"prompt": "def load_exons(self, exons, genes=None, build='37'):\n        \"\"\"Create exon objects and insert them into the database\n        \n        Args:\n            exons(iterable(dict))\n        \"\"\"", "output": "\n        \n        genes = genes or []\n        \n        exons = Exons(exons)\n        \n        for gene in genes:\n            \n            exon = Exon()\n            \n            exon.set_gene(gene)\n            \n            exon.set_start(build['start'])\n            exon.set_end(build['end'])\n            \n            exons.append(exon)\n            \n        return exons\n    \n    \nclass Exons(object):\n    \"\"\"\n    Exons object\n    \"\"\"\n    \n    def __init__(self, exons):\n        \"\"\"\n        Args:\n            exons(iterable(dict))\n        \"\"\"\n        \n        self.exons = exon", "solution": "\n        genes = genes or self.ensembl_genes(build)\n        for exon in exons:\n            exon_obj = build_exon(exon, genes)\n            if not exon_obj:\n                continue\n            \n            res = self.exon_collection.insert_one(exon_obj)"}
{"prompt": "def enrich(self, gmt):\n        \"\"\"use local mode\n         \n        p = p-value computed using the Fisher exact test (Hypergeometric test)  \n\n        Not implemented here:\n\n            combine score = log(p)\u00b7z\n\n        see here: http://amp.pharm.mssm.edu/Enrichr/help#background&q=4\n        \n        columns contain:\n            \n            Term Overlap P-value Adjusted_P-value Genes\n\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        if isscalar(self.background):\n            if isinstance(self.background, int) or self.background.isdigit():\n                self._bg = int(self.background)\n            elif isinstance(self.background, str):\n                # self.background = set(reduce(lambda x,y: x+y, gmt.values(),[]))\n                self._bg = self.get_background()\n                self._logger.info(\"Background: found %s genes\"%(len(self._bg)))\n            else:\n                raise Exception(\"Unsupported background data type\")\n        else:\n            # handle array object: nd.array, list, tuple, set, Series\n            try:\n                it = iter(self.background)\n                self._bg = set(self.background)\n            except TypeError:\n                self._logger.error(\"Unsupported background data type\")\n        # statistical testing\n        hgtest = list(calc_pvalues(query=self._gls, gene_sets=gmt, \n                                   background=self._bg))\n        if len(hgtest) > 0:\n            terms, pvals, olsz, gsetsz, genes = hgtest\n            fdrs, rej = multiple_testing_correction(ps = pvals, \n                                                    alpha=self.cutoff,\n                                                    method='benjamini-hochberg')\n            # save to a dataframe\n            odict = OrderedDict()\n            odict['Term'] = terms\n            odict['Overlap'] = list(map(lambda h,g: \"%s/%s\"%(h, g), olsz, gsetsz))\n            odict['P-value'] = pvals\n            odict['Adjusted P-value'] = fdrs\n            # odict['Reject (FDR< %s)'%self.cutoff ] = rej\n            odict['Genes'] = [\";\".join(g) for g in genes]\n            res = pd.DataFrame(odict)\n            return res\n        return"}
{"prompt": "def populate_from_sequence(seq: list, r: ref(Edge), sr: state.StateRegister):\n    \"\"\" function that connect each other one sequence of MatchExpr. \"\"\"", "output": "\n    edge = Edge()\n    edge.set_from_sequence(seq)\n    edge.set_r(r)\n    edge.set_sr(sr)\n    edge.set_rs(sr)\n    edge.set_rt(sr)\n    edge.set_rd(sr)\n    edge.set_rs(sr)\n    edge.set_rt(sr)\n    edge.set_rd(sr)\n    edge.set_rs(sr)\n    edge.set_rt(sr)\n    edge.set_rd(sr)\n    edge.set_rs(sr)\n    edge.set_rt(sr)\n    edge.set_rd(sr)\n    edge.set_r", "solution": "\n    base_state = r\n    # we need to detect the last state of the sequence\n    idxlast = len(seq) - 1\n    idx = 0\n    for m in seq:\n        # alternatives are represented by builtin list\n        if isinstance(m, list):\n            # so recursively connect all states of each alternative sequences.\n            for item in m:\n                populate_from_sequence(item, r, sr)\n        elif isinstance(m, MatchExpr):\n            # from the current state, have we a existing edge for this event?\n            eX = r().get_next_edge(m)\n            if eX is None:\n                sX = None\n                if idx != idxlast:\n                    sX = state.State(sr)\n                    sX.matchDefault(base_state().s)\n                else:\n                    # last state of sequence return to the base\n                    sX = base_state().s\n                eX = Edge(sX)\n                r().next_edge[id(sX)] = eX\n                m.attach(r().s, sX, sr)\n            r = ref(eX)\n        idx += 1"}
{"prompt": "def mr_reader(job, input_stream, loads=core.loads):\n    \"\"\" Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects\n\n    Can be used as job.reader in luigi.hadoop.JobTask\n    \"\"\"", "output": "\n    reader = core.JsonReader(input_stream)\n    records = reader.read()\n    for record in records:\n        yield loads(record)\n\n\nclass JsonToPyschema(luigi.Task):\n    \"\"\" Converts a file object with json serialised pyschema records\n        to a stream of pyschema objects\n\n    Can be used as job.reader in luigi.hadoop.JobTask\n    \"\"\"\n    schema = luigi.Parameter()\n\n    def output(self):\n        return luigi.LocalTarget(self.schema)\n\n    def run(self):\n        reader = mr_reader(self.input(), self.schema)\n        records = reader.read()\n        for record in records:\n            yield record\n\n\nclass JsonTo", "solution": "\n    for line in input_stream:\n        yield loads(line),"}
{"prompt": "def add_from_raw_data(self, raw_data, data_type_id, name, description):\n        \"\"\"\n        Upload already serialized raw data as a new dataset.\n\n        Parameters\n        ----------\n        raw_data: bytes\n            Dataset contents to upload.\n        data_type_id : str\n            Serialization format of the raw data.\n            Supported formats are:\n                'PlainText'\n                'GenericCSV'\n                'GenericTSV'\n                'GenericCSVNoHeader'\n                'GenericTSVNoHeader'\n                'ARFF'\n            See the azureml.DataTypeIds class for constants.\n        name : str\n            Name for the new dataset.\n        description : str\n            Description for the new dataset.\n\n        Returns\n        -------\n        SourceDataset\n            Dataset that was just created.\n            Use open(), read_as_binary(), read_as_text() or to_dataframe() on\n            the dataset object to get its contents as a stream, bytes, str or\n            pandas DataFrame.\n        \"\"\"", "output": "\n        -------\n        Dataset\n            Dataset object.\n        \"\"\"\n        dataset = Dataset.create(\n            name=name,\n            description=description,\n            tags={\"data_type\": data_type_id},\n            properties={\"rawData\": raw_data},\n        )\n        return dataset\n\n    def register_model(self, model_name):\n        \"\"\"\n        Register a model with the Azure Machine Learning service.\n\n        Parameters\n        ----------\n        model_name : str\n            Name of the model.\n\n        Returns\n        -------\n        Model\n            Model object.\n        \"\"\"\n        registered_model = self.register_model(model_name=model_name)\n        return registered_model\n\n    def register_model(self, model_name):\n        \"\"\"\n        Register a model with the Azure Machine Learning service.\n\n        Parameters\n", "solution": "\n        _not_none('raw_data', raw_data)\n        _not_none_or_empty('data_type_id', data_type_id)\n        _not_none_or_empty('name', name)\n        _not_none_or_empty('description', description)\n\n        return self._upload(raw_data, data_type_id, name, description)"}
{"prompt": "def write_bit(self, value, bit_num, address):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write to.\n        :type bit_num: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"", "output": "\n        self._write_bit(address, value, bit_num)\n\n    def write_bits(self, value, bit_num, address):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write to.\n        :type bit_num: int\n        :param address: The address to write to.\n        :type address: int\n        \"\"\"\n        self._write_bits(address, value, bit_num)\n\n    def _write_bit(self, address, value, bit_num):\n        \"\"\"Writes the value given to the bit in the address specified.\n\n        :param address: The address to write to.\n        :type address: int\n        :param value: The value to write.\n        :type value: int\n        :param bit_num: The bit number to write ", "solution": "\n        bit_mask = get_bit_mask(bit_num)\n        old_byte = self.read(address)\n         # generate the new byte\n        if value:\n            new_byte = old_byte | bit_mask\n        else:\n            new_byte = old_byte & ~bit_mask\n        self.write(new_byte, address)"}
{"prompt": "def filter(self, *filters, **kwargs):\n        \"\"\"\n        Returns this Query instance with the query args combined with\n        existing set with AND.\n\n        kwargs are simply passed to a new Filter object and combined to any\n        other filters with AND.\n\n        By default, everything is combined using AND. If you provide\n        multiple filters in a single filter call, those are ANDed\n        together. If you provide multiple filters in multiple filter\n        calls, those are ANDed together.\n\n        If you want something different, use the F class which supports\n        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\n        filter once with the resulting Filter instance.\n        \"\"\"", "output": "\n        combined = And(self, *filters)\n        return self.filter(combined, **kwargs)\n\n    def exclude(self, *filters, **kwargs):\n        \"\"\"\n        Returns this Query instance with the query args excluded from\n        final results.\n\n        kwargs are simply passed to a new Filter object and combined to\n        any existing filters with OR.\n\n        By default, everything is combined using OR. If you provide multiple\n        filters in a single filter call, those are ORed together. If you provide\n        multiple filters in multiple filter calls, those are ANDed together.\n\n        If you want something different, use the F class which supports\n        ``&`` (and), ``|`` (or) and ``~`` (not) operators. Then call\n", "solution": "\n        f = list(filters)\n\n        if kwargs:\n            f += [Filter(**kwargs)]\n\n        return self._clone(filters=f)"}
{"prompt": "def topological_nodes(self):\n        \"\"\"\n        Yield nodes in topological order.\n\n        Returns:\n            generator(DAGNode): node in topological order\n        \"\"\"", "output": "\n        visited = set()\n        stack = [self]\n        while stack:\n            node = stack.pop()\n            if node in visited:\n                continue\n            visited.add(node)\n            yield node\n            for child in node.children:\n                if child not in visited:\n                    stack.append(child)\n\n\nclass DAGNode(object):\n    \"\"\"\n    Base class for all DAG nodes.\n\n    DAG nodes are used to represent a DAG structure, i.e. a tree of nodes,\n    each of which can have children and a parent. DAG nodes are immutable,\n    i.e. they cannot be modified after construction.\n    \"\"\"\n\n    __slots__ = ('children', 'parent', 'key')\n\n    def __init__(self, key=None, ", "solution": "\n        return nx.lexicographical_topological_sort(self._multi_graph,\n                                                   key=lambda x: str(x.qargs))"}
{"prompt": "def peng_power(snum):\n    r\"\"\"\n    Return engineering suffix and its floating point equivalent of a number.\n\n    :py:func:`peng.peng` lists the correspondence between suffix and floating\n    point exponent.\n\n    :param snum: Number\n    :type  snum: :ref:`EngineeringNotationNumber`\n\n    :rtype: named tuple in which the first item is the engineering suffix and\n            the second item is the floating point equivalent of the suffix\n            when the number is represented in engineering notation.\n\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.functions.peng_power\n\n    :raises: RuntimeError (Argument \\`snum\\` is not valid)\n\n    .. [[[end]]]\n\n    For example:\n\n        >>> import peng\n        >>> peng.peng_power(peng.peng(1235.6789E3, 3, False))\n        EngPower(suffix='M', exp=1000000.0)\n    \"\"\"", "output": "\n       :ref:`EngineeringNotationNumber`\n    \"\"\"\n    suffix = snum.suffix\n    exp = snum.exponent\n    if exp == 0:\n        return EngineeringNotationNumber(suffix, 0), 0\n    elif exp == 1:\n        return EngineeringNotationNumber(suffix, 1), 1\n    elif exp == 2:\n        return EngineeringNotationNumber(suffix, 2), 2\n    elif exp == 3:\n        return EngineeringNotationNumber(suffix, 3), 3\n    elif exp == 4:\n        return EngineeringNotationNumber(suffix, 4), 4\n    elif exp == 5:\n        return EngineeringNotationNumber(suffix, 5), 5\n    elif ", "solution": "\n    suffix = \" \" if snum[-1].isdigit() else snum[-1]\n    return EngPower(suffix, _SUFFIX_POWER_DICT[suffix])"}
{"prompt": "def get_next_proto_negotiated(self):\n        \"\"\"\n        Get the protocol that was negotiated by NPN.\n\n        :returns: A bytestring of the protocol name.  If no protocol has been\n            negotiated yet, returns an empty string.\n\n        .. versionadded:: 0.15\n        \"\"\"", "output": "\n        return self._negotiated_protocol\n\n    @negotiated_protocol.setter\n    def negotiated_protocol(self, value):\n        \"\"\"\n        Set the protocol that was negotiated by NPN.\n\n        :param value: A bytestring of the protocol name.\n        \"\"\"\n        self._negotiated_protocol = value\n\n    @property\n    def negotiated_protocols(self):\n        \"\"\"\n        Return a list of protocols negotiated by NPN.\n\n        :returns: A list of the protocol names.  If no protocols have been\n            negotiated yet, returns an empty list.\n        \"\"\"\n        return self._negotiated_protocols\n\n    @property\n    def negotiated_protocols_supported(self):\n        \"\"\"\n        Return a list of protocols supported by the NPN.\n\n", "solution": "\n        _warn_npn()\n        data = _ffi.new(\"unsigned char **\")\n        data_len = _ffi.new(\"unsigned int *\")\n\n        _lib.SSL_get0_next_proto_negotiated(self._ssl, data, data_len)\n\n        return _ffi.buffer(data[0], data_len[0])[:]"}
{"prompt": "def update_virtual_meta(self):\n        \"\"\"Will read back the virtual column etc, written by :func:`DataFrame.write_virtual_meta`. This will be done when opening a DataFrame.\"\"\"", "output": "\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_columns = None\n        self._virtual_", "solution": "\n        import astropy.units\n        try:\n            path = os.path.join(self.get_private_dir(create=False), \"virtual_meta.yaml\")\n            if os.path.exists(path):\n                meta_info = vaex.utils.read_json_or_yaml(path)\n                if 'virtual_columns' not in meta_info:\n                    return\n                self.virtual_columns.update(meta_info[\"virtual_columns\"])\n                self.variables.update(meta_info[\"variables\"])\n                self.ucds.update(meta_info[\"ucds\"])\n                self.descriptions.update(meta_info[\"descriptions\"])\n                units = {key: astropy.units.Unit(value) for key, value in meta_info[\"units\"].items()}\n                self.units.update(units)\n        except:\n            logger.exception(\"non fatal error\")"}
{"prompt": "def delete(self, teamId):\n        \"\"\"Delete a team.\n\n        Args:\n            teamId(basestring): The ID of the team to be deleted.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"", "output": "\n        body = {\n            'teamId': teamId\n        }\n        uri = '/teams/{}'.format(teamId)\n        return self._requester.delete(uri, method='DELETE', body=body)\n\n    def list(self, teamId=None, page=None, pageSize=None):\n        \"\"\"List teams.\n\n        Args:\n            teamId(basestring): The ID of the team to be listed.\n            page(basestring): The page number of the results to be returned.\n            pageSize(basestring): The number of results to be returned per page.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        uri = '/teams'\n        if teamId:\n", "solution": "\n        check_type(teamId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '/' + teamId)"}
{"prompt": "def per_triangle(script, sidedim=0, textdim=1024, border=2, method=1):\n    \"\"\"Trivial Per-Triangle parameterization\n\n    \"\"\"", "output": "\n    script = script.replace(' ', '')\n    script = script.replace('\\n', '')\n    script = script.replace('\\r', '')\n    script = script.replace('\\t', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')\n    script = script.replace(' ', '')", "solution": "\n    filter_xml = ''.join([\n        '  <filter name=\"Parametrization: Trivial Per-Triangle \">\\n',\n        '    <Param name=\"sidedim\"',\n        'value=\"%d\"' % sidedim,\n        'description=\"Quads per line\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Indicates how many triangles have to be put on each line (every quad contains two triangles). Leave 0 for automatic calculation\"',\n        '/>\\n',\n        '    <Param name=\"textdim\"',\n        'value=\"%d\"' % textdim,\n        'description=\"Texture Dimension (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Gives an indication on how big the texture is\"',\n        '/>\\n',\n        '    <Param name=\"border\"',\n        'value=\"%d\"' % border,\n        'description=\"Inter-Triangle border (px)\"',\n        'type=\"RichInt\"',\n        'tooltip=\"Specifies how many pixels to be left between triangles in parametrization domain\"',\n        '/>\\n',\n        '    <Param name=\"method\"',\n        'value=\"%d\"' % method,\n        'description=\"Method\"',\n        'enum_val0=\"Basic\"',\n        'enum_val1=\"Space-optimizing\"',\n        'enum_cardinality=\"2\"',\n        'type=\"RichEnum\"',\n        'tooltip=\"Choose space optimizing to map smaller faces into smaller triangles in parametrizazion domain\"'\n        '/>\\n',\n        '  </filter>\\n'])\n    util.write_filter(script, filter_xml)\n    return None"}
{"prompt": "def add_virtual_columns_cartesian_velocities_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", vx=\"vx\", vy=\"vy\", vz=\"vz\", vr=\"vr\", vlong=\"vlong\", vlat=\"vlat\", distance=None):\n        \"\"\"Concert velocities from a cartesian to a spherical coordinate system\n\n        TODO: errors\n\n        :param x: name of x column (input)\n        :param y:         y\n        :param z:         z\n        :param vx:       vx\n        :param vy:       vy\n        :param vz:       vz\n        :param vr: name of the column for the radial velocity in the r direction (output)\n        :param vlong: name of the column for the velocity component in the longitude direction  (output)\n        :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)\n        :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance\n        :return:\n        \"\"\"", "output": "        :param vlong:  name of the column for the longitude velocity in the r direction (output)\n        :param vlat:   name of the column for the latitude velocity in the r direction (output)\n        :param distance: distance in meters (optional)\n        :return: None\n        \"\"\"\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n", "solution": "\n        # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm\n        if distance is None:\n            distance = \"sqrt({x}**2+{y}**2+{z}**2)\".format(**locals())\n        self.add_virtual_column(vr, \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\".format(**locals()))\n        self.add_virtual_column(vlong, \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)\".format(**locals()))\n        self.add_virtual_column(vlat, \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )\".format(**locals()))"}
{"prompt": "def check_token(token):\n    \"\"\" Verify http header token authentification \"\"\"", "output": "\n    auth = request.headers.get('Authorization')\n    if not auth:\n        return False\n    scheme, token = auth.split()\n    if scheme!= 'Basic':\n        return False\n    try:\n        base64.b64decode(token)\n    except TypeError:\n        return False\n    return True\n\n\ndef authenticate(token=None):\n    \"\"\" Authenticate using token or headers. \"\"\"\n    if request.method == 'GET':\n        token = request.headers.get('X-Auth-Token')\n        if not token:\n            token = request.headers.get('Authorization')\n        if not token:\n            token = request.headers.get('X-Auth-Token')\n        if token and check_token(token):\n            return token\n    return None\n\n\ndef authorize(token=None):\n    \"\"\" Authorize using token or ", "solution": "\n    user = models.User.objects(api_key=token).first()\n    return user or None"}
{"prompt": "def validate_value(self, value):\n        \"\"\"\n        Validate new property value before setting it.\n\n        value -- New value\n        \"\"\"", "output": "\n        pass\n\n    def value_to_string(self, value):\n        \"\"\"\n        Convert value to a string for display purposes.\n\n        value -- Value to convert\n        \"\"\"\n        pass\n\n    def value_to_python(self, value):\n        \"\"\"\n        Convert value to a python object for use as a Python object.\n\n        value -- Value to convert\n        \"\"\"\n        pass\n\n    def value_to_json(self, value):\n        \"\"\"\n        Convert value to a JSON object for use as a Python object.\n\n        value -- Value to convert\n        \"\"\"\n        pass\n\n    def value_to_xml(self, value):\n        \"\"\"\n        Convert value to a XML object for use as a Python object.\n\n        value -- Value to convert\n        \"\"\"\n        pass\n\n    def value_to_date(self, value):\n", "solution": "\n        if 'readOnly' in self.metadata and self.metadata['readOnly']:\n            raise PropertyError('Read-only property')\n\n        try:\n            validate(value, self.metadata)\n        except ValidationError:\n            raise PropertyError('Invalid property value')"}
{"prompt": "def pack_metadata_statement(self, metadata, receiver='', iss='', lifetime=0,\n                                sign_alg=''):\n        \"\"\"\n        Given a MetadataStatement instance create a signed JWT.\n\n        :param metadata: Original metadata statement as a MetadataStatement\n            instance\n        :param receiver: Receiver (audience) of the JWT\n        :param iss: Issuer ID if different from default\n        :param lifetime: jWT signature life time\n        :param sign_alg: JWT signature algorithm\n        :return: A JWT\n        \"\"\"", "output": "\n        header = {\n            'typ': 'JWT',\n            'alg': sign_alg,\n            'kid': metadata.kid,\n            'kty': metadata.kty,\n            'crv': metadata.crv,\n            'x5t': metadata.x5t,\n            'x5c': [metadata.x5c],\n            'x5t': metadata.x5t,\n            'x5t': metadata.x5t,\n            'x5t': metadata.x5t,\n            'x5t': metadata.x5t,\n            'x5t': metadata.x5t,\n            'x5t': metadata.x5t,\n            'x5t': metadata.", "solution": "\n\n        return self.self_signer.sign(metadata, receiver=receiver, iss=iss,\n                                     lifetime=lifetime, sign_alg=sign_alg)"}
{"prompt": "def contact(self, id):\n        \"\"\"\n        Retrieve all details for a specific contact, including fields such as birthday and mood.\n\n        Args:\n            id (str): user identifier to lookup\n\n        Returns:\n            SkypeContact: resulting contact object\n        \"\"\"", "output": "\n        contact = SkypeContact(id)\n        contact._load(self._session)\n        return contact\n\n    def contacts(self, **kwargs):\n        \"\"\"\n        Retrieve all contacts.\n\n        Args:\n            **kwargs (dict): optional filters to apply to contacts\n\n        Returns:\n            list: resulting contact list\n        \"\"\"\n        contacts = []\n        for contact in self._session.query(SkypeContact).filter_by(**kwargs):\n            contacts.append(contact)\n        return contacts\n\n    def contacts_all(self, **kwargs):\n        \"\"\"\n        Retrieve all contacts.\n\n        Args:\n            **kwargs (dict): optional filters to apply to contacts\n\n        Returns:\n            list: resulting contact list\n        \"\"\"\n        contacts = []\n        for contact ", "solution": "\n        try:\n            json = self.skype.conn(\"POST\", \"{0}/users/batch/profiles\".format(SkypeConnection.API_USER),\n                                   json={\"usernames\": [id]}, auth=SkypeConnection.Auth.SkypeToken).json()\n            contact = SkypeContact.fromRaw(self.skype, json[0])\n            if contact.id not in self.contactIds:\n                self.contactIds.append(contact.id)\n            return self.merge(contact)\n        except SkypeApiException as e:\n            if len(e.args) >= 2 and getattr(e.args[1], \"status_code\", None) == 403:\n                # Not a contact, so no permission to retrieve information.\n                return None\n            raise"}
{"prompt": "def str_repeat(x, repeats):\n    \"\"\"Duplicate each string in a column.\n\n    :param int repeats: number of times each string sample is to be duplicated.\n    :returns: an expression containing the duplicated strings\n\n    Example:\n\n    >>> import vaex\n    >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']\n    >>> df = vaex.from_arrays(text=text)\n    >>> df\n      #  text\n      0  Something\n      1  very pretty\n      2  is coming\n      3  our\n      4  way.\n\n    >>> df.text.str.repeat(3)\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: str (expression)\n    ---------------------------------\n    0        SomethingSomethingSomething\n    1  very prettyvery prettyvery pretty\n    2        is comingis comingis coming\n    3                          ourourour\n    4                       way.way.way.\n    \"\"\"", "output": ": object\n\n    >>> df.text.str.repeat(3).str\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: object\n\n    >>> df.text.str.repeat(3).str.len()\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: int64\n\n    >>> df.text.str.repeat(3).str.len().str\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: int64\n\n    >>> df.text.str.repeat(3).str.len().str.len()\n    Expression = str_repeat(text, 3)\n    Length: 5 dtype: int64\n\n    >>> df.text.", "solution": "\n    sl = _to_string_sequence(x).repeat(repeats)\n    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)"}
{"prompt": "def deprecated(*args):\n    \"\"\" Deprecation warning decorator. Takes optional deprecation message,\n    otherwise will use a generic warning. \"\"\"", "output": "\n    def deprecated_func(fn):\n        def new_func(*args, **kwargs):\n            warnings.warn(\"Call to deprecated function %s.\" % fn.__name__,\n                          category=DeprecationWarning, stacklevel=2)\n            return fn(*args, **kwargs)\n        return new_func\n    if len(args) == 1 and callable(args[0]):\n        return deprecated_func(args[0])\n    else:\n        return deprecated_func(*args)\n\n\ndef requires_resource(resource):\n    \"\"\"Decorator to require the named resource in use\n\n    If the named resource is already being used by another process, an\n    error is raised.\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if resource in threading._dangling_threads:\n                raise RuntimeError(\n                    \"Cannot use thread %r while it is in ", "solution": "\n    def wrap(func):\n        def wrapped_func(*args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapped_func\n\n    if len(args) == 1 and callable(args[0]):\n        msg = \"Function '%s' will be deprecated in future versions of \" \\\n            \"Neurosynth.\" % args[0].__name__\n        return wrap(args[0])\n    else:\n        msg = args[0]\n        return wrap"}
{"prompt": "def get_last_activities(self, n):\n        \"\"\"Get all activity data for the last activity\n\n        Keyword arguments:\n        \"\"\"", "output": "\n        activities = list(self.activities.values())\n        activities.sort(key=lambda x: x.last_activity_time)\n        return activities[:n]\n\n    def get_last_activities_count(self):\n        \"\"\"Get the number of activities in the database\n\n        Keyword arguments:\n        \"\"\"\n        return len(self.activities)\n\n    def get_last_activity_time(self, n):\n        \"\"\"Get the last activity time\n\n        Keyword arguments:\n        \"\"\"\n        activities = list(self.activities.values())\n        activities.sort(key=lambda x: x.last_activity_time)\n        return activities[n].last_activity_time\n\n    def get_last_activity_time_str(self):\n        \"\"\"Get the last activity time as a string\n\n        Keyword arguments:\n        \"\"\"\n        return str(self.get_last_activity_time())", "solution": "\n        filenames = self.get_activity_list().iloc[-n:].filename.tolist()\n        last_activities = [self.get_activity(f) for f in filenames]\n        return last_activities"}
{"prompt": "def user(context, user_id, update_role, add_institute, remove_admin, remove_institute):\n    \"\"\"\n    Update a user in the database\n    \"\"\"", "output": "\n    user = context.session.query(models.User).filter_by(id=user_id).first()\n    if user:\n        user.name = context.session.query(models.User).filter_by(id=user_id).first().name\n        user.email = context.session.query(models.User).filter_by(id=user_id).first().email\n        user.institute = context.session.query(models.User).filter_by(id=user_id).first().institute\n        user.admin = context.session.query(models.User).filter_by(id=user_id).first().admin\n        user.institute_id = context.session.query(models.User).filter_by(id=user_id).first().institute_id\n        user.admin_id = context.session.query(models.User).filter_by(id=user_id).first().admin_id\n        ", "solution": "\n    adapter = context.obj['adapter']\n\n    user_obj = adapter.user(user_id)\n\n    if not user_obj:\n        LOG.warning(\"User %s could not be found\", user_id)\n        context.abort()\n\n    existing_roles = set(user_obj.get('roles',[]))\n    if update_role:\n        if not update_role in user_obj['roles']:\n            existing_roles = set(user_obj['roles'])\n            existing_roles.add(update_role)\n            LOG.info(\"Adding role %s to user\", update_role)\n        else:\n            LOG.warning(\"User already have role %s\", update_role)\n\n    if remove_admin:\n        try:\n            existing_roles.remove('admin')\n            LOG.info(\"Removing admin rights from user %s\", user_id)\n        except KeyError as err:\n            LOG.info(\"User %s does not have admin rights\", user_id)\n\n    user_obj['roles'] = list(existing_roles)\n\n    existing_institutes = set(user_obj.get('institutes',[]))\n    for institute_id in add_institute:\n        institute_obj = adapter.institute(institute_id)\n        if not institute_obj:\n            LOG.warning(\"Institute %s could not be found\", institute_id)\n        else:\n            existing_institutes.add(institute_id)\n            LOG.info(\"Adding institute %s to user\", institute_id)\n\n    for institute_id in remove_institute:\n        try:\n            existing_institutes.remove(institute_id)\n            LOG.info(\"Removing institute %s from user\", institute_id)\n        except KeyError as err:\n            LOG.info(\"User does not have access to institute %s\", institute_id)\n\n    user_obj['institutes'] = list(existing_institutes)\n\n    updated_user = adapter.update_user(user_obj)"}
{"prompt": "def _update_base(self):\n        \"\"\"update the base, including the URL for GitLab and the API endpoint.\n        \"\"\"", "output": "\n        # GitLab API endpoint\n        self.gitlab_api_endpoint = GitLabAPIEndpoint(self.gitlab_api_endpoint)\n        self.gitlab_api_endpoint.update_base()\n\n        # GitLab URL\n        self.gitlab_url = self.gitlab_api_endpoint.url\n\n        # GitLab API URL\n        self.gitlab_api_url = self.gitlab_api_endpoint.api_url\n\n        # GitLab API URL\n        self.gitlab_api_url = self.gitlab_api_endpoint.api_url\n\n        # GitLab API URL\n        self.gitlab_api_url = self.gitlab_api_endpoint.api_url\n\n        # Git", "solution": "\n        self.base = self._get_and_update_setting('SREGISTRY_GITLAB_BASE',\n                                                 \"https://gitlab.com/\")\n        self.api_base = \"%s/api/v4\" % self.base.strip('/')\n        self.artifacts = self._get_and_update_setting('SREGISTRY_GITLAB_FOLDER',\n                                                      'build')\n\n        self.job = self._get_and_update_setting('SREGISTRY_GITLAB_JOB', 'build')\n\n        bot.debug('      Api: %s' % self.api_base)\n        bot.debug('Artifacts: %s' % self.artifacts)\n        bot.debug('      Job: %s' % self.job)"}
{"prompt": "def send(self, message):\n        \"\"\" Send a message object\n\n            :type message: data.OutgoingMessage\n            :param message: The message to send\n            :rtype: data.OutgoingMessage\n            :returns: The sent message with populated fields\n            :raises AssertionError: wrong provider name encountered (returned by the router, or provided to OutgoingMessage)\n            :raises MessageSendError: generic errors\n            :raises AuthError: provider authentication failed\n            :raises LimitsError: sending limits exceeded\n            :raises CreditError: not enough money on the account\n        \"\"\"", "output": "\n        message = message._replace(\n            provider=self.provider,\n            amount=message.amount,\n            currency=message.currency,\n            memo=message.memo,\n            date=message.date,\n            time=message.time,\n            status=message.status,\n            currencyIso=message.currencyIso,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currencyIso3,\n            currencyIso3=message.currency", "solution": "\n        # Which provider to use?\n        provider_name = self._default_provider  # default\n        if message.provider is not None:\n            assert message.provider in self._providers, \\\n                'Unknown provider specified in OutgoingMessage.provideer: {}'.format(provider_name)\n            provider = self.get_provider(message.provider)\n        else:\n            # Apply routing\n            if message.routing_values is not None: # Use the default provider when no routing values are given\n                # Routing values are present\n                provider_name = self.router(message, *message.routing_values) or self._default_provider\n                assert provider_name in self._providers, \\\n                    'Routing function returned an unknown provider name: {}'.format(provider_name)\n            provider = self.get_provider(provider_name)\n\n        # Set message provider name\n        message.provider = provider.name\n\n        # Send the message using the provider\n        message = provider.send(message)\n\n        # Emit the send event\n        self.onSend(message)\n\n        # Finish\n        return message"}
{"prompt": "def string_arg(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"", "output": "\n    if isinstance(obj, LiteralObject):\n        return obj.value\n    elif isinstance(obj, str):\n        return obj\n    elif isinstance(obj, int):\n        return str(obj)\n    elif isinstance(obj, float):\n        return str(obj)\n    elif isinstance(obj, bool):\n        return str(obj)\n    elif isinstance(obj, dict):\n        return obj\n    elif isinstance(obj, list):\n        return [string_arg(ctx, x) for x in obj]\n    else:\n        raise TypeError(\n            \"Cannot handle object of type {}\".format(type(obj))\n        )\n\n\ndef literal_arg(ctx, obj):\n    \"\"\"\n    Handles LiteralObjects as well as computable arguments\n    \"\"\"\n    if isinstance(obj, LiteralObject):\n        return obj.value\n    elif isinstance(obj, str):\n        return literal_eval(obj)\n    elif isinstance(obj, int):\n        return literal_eval(obj)\n", "solution": "\n    if hasattr(obj, 'compute'):\n        obj = next(obj.compute(ctx), False)\n    return to_string(obj)"}
{"prompt": "def on_kill(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"", "output": "\n        self.athena.cancel()\n\n    def cancel(self):\n        \"\"\"\n        Cancel the submitted athena query\n        \"\"\"\n        self.athena.cancel()\n\n    def cancelled(self):\n        \"\"\"\n        Check if the athena query has been cancelled\n        \"\"\"\n        return self.athena.cancelled()\n\n    def cancelled(self):\n        \"\"\"\n        Check if the athena query has been cancelled\n        \"\"\"\n        return self.athena.cancelled()\n\n    def cancelled(self):\n        \"\"\"\n        Check if the athena query has been cancelled\n        \"\"\"\n        return self.athena.cancelled()\n\n    def cancelled(self):\n        \"\"\"\n        Check if the athena query ", "solution": "\n        if self.query_execution_id:\n            self.log.info('\u26b0\ufe0f\u26b0\ufe0f\u26b0\ufe0f Received a kill Signal. Time to Die')\n            self.log.info(\n                'Stopping Query with executionId - %s', self.query_execution_id\n            )\n            response = self.hook.stop_query(self.query_execution_id)\n            http_status_code = None\n            try:\n                http_status_code = response['ResponseMetadata']['HTTPStatusCode']\n            except Exception as ex:\n                self.log.error('Exception while cancelling query', ex)\n            finally:\n                if http_status_code is None or http_status_code != 200:\n                    self.log.error('Unable to request query cancel on athena. Exiting')\n                else:\n                    self.log.info(\n                        'Polling Athena for query with id %s to reach final state', self.query_execution_id\n                    )\n                    self.hook.poll_query_status(self.query_execution_id)"}
{"prompt": "def get_fernet():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet object\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"", "output": "\n    try:\n        from cryptography.fernet import Fernet\n    except ImportError:\n        raise AirflowException(\"Fernet cryptography library is not installed\")\n\n    return Fernet(fernet_key)\n\n\ndef get_fernet_key():\n    \"\"\"\n    Deferred load of Fernet key.\n\n    This function could fail either because Cryptography is not installed\n    or because the Fernet key is invalid.\n\n    :return: Fernet key\n    :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet\n    \"\"\"\n    try:\n        from cryptography.fernet import Fernet\n    except ImportError:\n        raise AirflowException(\"Fernet cryptography library is not installed\")\n\n    return Fernet.", "solution": "\n    global _fernet\n    log = LoggingMixin().log\n\n    if _fernet:\n        return _fernet\n    try:\n        from cryptography.fernet import Fernet, MultiFernet, InvalidToken\n        global InvalidFernetToken\n        InvalidFernetToken = InvalidToken\n\n    except BuiltinImportError:\n        log.warning(\n            \"cryptography not found - values will not be stored encrypted.\"\n        )\n        _fernet = NullFernet()\n        return _fernet\n\n    try:\n        fernet_key = configuration.conf.get('core', 'FERNET_KEY')\n        if not fernet_key:\n            log.warning(\n                \"empty cryptography key - values will not be stored encrypted.\"\n            )\n            _fernet = NullFernet()\n        else:\n            _fernet = MultiFernet([\n                Fernet(fernet_part.encode('utf-8'))\n                for fernet_part in fernet_key.split(',')\n            ])\n            _fernet.is_encrypted = True\n    except (ValueError, TypeError) as ve:\n        raise AirflowException(\"Could not create Fernet object: {}\".format(ve))\n\n    return _fernet"}
{"prompt": "def update_board(self, query_params=None):\n        \"\"\"\n        Update this board's information. Returns a new board.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n\n        query_params = self._common_args(query_params)\n\n        path_params = {}\n\n        query_params = self._common_args(query_params)\n        path_params = self._common_args(path_params)\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.\\\n            select_header_accept(['application/json'])\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.\\\n            select_header_content_type(['application/json'])\n\n        # Authentication setting\n        auth_settings = []\n\n        return self.api_client.call_api('/boards/{boardId", "solution": "\n        board_json = self.fetch_json(\n            uri_path=self.base_uri,\n            http_method='PUT',\n            query_params=query_params or {}\n        )\n\n        return self.create_board(board_json)"}
{"prompt": "def compose_u3(theta1, phi1, lambda1, theta2, phi2, lambda2):\n        \"\"\"Return a triple theta, phi, lambda for the product.\n\n        u3(theta, phi, lambda)\n           = u3(theta1, phi1, lambda1).u3(theta2, phi2, lambda2)\n           = Rz(phi1).Ry(theta1).Rz(lambda1+phi2).Ry(theta2).Rz(lambda2)\n           = Rz(phi1).Rz(phi').Ry(theta').Rz(lambda').Rz(lambda2)\n           = u3(theta', phi1 + phi', lambda2 + lambda')\n\n        Return theta, phi, lambda.\n        \"\"\"", "output": "        \"\"\"\n        theta1, phi1, lambda1 = _check_thetas(theta1, phi1, lambda1)\n        theta2, phi2, lambda2 = _check_thetas(theta2, phi2, lambda2)\n        phi1, phi2 = _check_phi(phi1, phi2)\n        lambda1, lambda2 = _check_lambda(lambda1, lambda2)\n        theta1, theta2 = _check_theta(theta1, theta2)\n        phi1, phi2 = _check_phi(phi1, phi2)\n        lambda1, lambda2 = _check_lambda(lambda1, lambda2)\n        theta1, theta2 = _check_theta(theta1, theta2)\n        phi1, phi2 = _", "solution": "\n        # Careful with the factor of two in yzy_to_zyz\n        thetap, phip, lambdap = Optimize1qGates.yzy_to_zyz((lambda1 + phi2), theta1, theta2)\n        (theta, phi, lamb) = (thetap, phi1 + phip, lambda2 + lambdap)\n\n        return (theta, phi, lamb)"}
{"prompt": "def filter_trim(self, start=1, end=1, filt=True):\n        \"\"\"\n        Remove points from the start and end of filter regions.\n        \n        Parameters\n        ----------\n        start, end : int\n            The number of points to remove from the start and end of\n            the specified filter.\n        filt : valid filter string or bool\n            Which filter to trim. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        \n        if filt:\n            filt = str(filt)\n            \n            if filt.lower() == 'all':\n                filt = '*'\n            else:\n                filt = filt.lower()\n                \n            if filt == 'all':\n                filt = '*'\n                \n            self.filters['start'] = start\n            self.filters['end'] = end\n            self.filters['filter'] = filt\n            \n        else:\n            self.filters['start'] = start\n            self.filters['end'] = end\n            \n    def filter_trim(self, start=1, end=1, filt=True):\n        \"\"\"\n        Remove points from the start and end of filter regions.\n        \n        Parameters\n        ----------\n        ", "solution": "\n        params = locals()\n        del(params['self'])\n            \n        f = self.filt.grab_filt(filt)\n        nf = filters.trim(f, start, end)\n        \n        self.filt.add('trimmed_filter',\n                    nf,\n                    'Trimmed Filter ({:.0f} start, {:.0f} end)'.format(start, end),\n                    params, setn=self.filt.maxset + 1)"}
{"prompt": "def verify_reg_list(self, obj, object_type):\n        \"\"\"Verify a list of registers.\"\"\"", "output": "\n        reg_list = obj.get_reg_list()\n        self.assertEqual(len(reg_list), object_type.num_registers)\n        for reg in reg_list:\n            self.assertIsInstance(reg, Reg)\n            self.assertEqual(reg.get_name(), object_type.get_reg_name(reg))\n\n    def test_reg_list(self):\n        \"\"\"Test the get_reg_list method.\"\"\"\n        object_type = RegType()\n        reg_list = object_type.get_reg_list()\n        self.assertEqual(len(reg_list), object_type.num_registers)\n        for reg in reg_list:\n            self.assertIsInstance(reg, Reg)\n            self.assertEqual(reg.get_name(), object_type.get_reg_name(reg))\n\n    def test_reg_list_empty(self):\n        \"\"\"Test the get_reg_list method with an empty list.\"\"\"", "solution": "\n        # We expect the object to be a bitlist or an idlist, we don't care.\n        # We will iterate it and ensure everything in it is declared as a bit,\n        # and throw if not.\n        for children in obj.children:\n            self.verify_reg(children, object_type)"}
{"prompt": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"", "output": "\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slope is not None else self.slope\n        slope = slope if slo", "solution": "\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model"}
{"prompt": "def to_keep(datetimes,\n            years=0, months=0, weeks=0, days=0,\n            hours=0, minutes=0, seconds=0,\n            firstweekday=SATURDAY, now=None):\n    \"\"\"\n    Return a set of datetimes that should be kept, out of ``datetimes``.\n\n    Keeps up to ``years``, ``months``, ``weeks``, ``days``,\n    ``hours``, ``minutes``, and ``seconds`` in the past.\n\n    When keeping weeks, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes after this point will always be\n    kept.\n    \"\"\"", "output": "before that date are considered\n    to be in the past.\n\n    When keeping days, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes before that date are considered\n    to be in the past.\n\n    When keeping hours, it prefers to keep ``firstweekday``, which\n    defaults to Saturday.\n\n    If ``now`` is None, it will base its calculations on\n    ``datetime.datetime.now()``. Datetimes before that date are considered\n    to be in the past.\n\n    When keeping minutes, it prefers to keep ``firstweekday``, which", "solution": "\n    datetimes = set(datetimes)\n    return (filters.Years.filter(datetimes, number=years, now=now) |\n            filters.Months.filter(datetimes, number=months, now=now) |\n            filters.Weeks.filter(datetimes, number=weeks,\n                                 firstweekday=firstweekday, now=now) |\n            filters.Days.filter(datetimes, number=days, now=now) |\n            filters.Hours.filter(datetimes, number=hours, now=now) |\n            filters.Minutes.filter(datetimes, number=minutes, now=now) |\n            filters.Seconds.filter(datetimes, number=seconds, now=now))"}
{"prompt": "def set(self, client_id, code, request, *args, **kwargs):\n        \"\"\"Creates Grant object with the given params\n\n        :param client_id: ID of the client\n        :param code:\n        :param request: OAuthlib request object\n        \"\"\"", "output": "\n        code = code.replace('+','')\n        code = code.replace(' ', '+')\n        code = code.replace('%', '.')\n        code = code.replace('_', '.')\n        code = code.replace('~', '')\n        code = code.replace('(', '')\n        code = code.replace(')', '')\n        code = code.replace(',', '')\n        code = code.replace(';', '')\n        code = code.replace(':', '')\n        code = code.replace(';', '')\n        code = code.replace('\\'', '')\n        code = code.replace('\"', '')\n        code = code.replace('=', '')\n        code = code.replace('>', '')\n        code = code.replace('<', '')\n        code = code.replace('>', '')\n        code = code.replace('", "solution": "\n        expires = datetime.utcnow() + timedelta(seconds=100)\n        grant = self.model(\n            client_id=request.client.client_id,\n            code=code['code'],\n            redirect_uri=request.redirect_uri,\n            scope=' '.join(request.scopes),\n            user=self.current_user(),\n            expires=expires\n        )\n        self.session.add(grant)\n\n        self.session.commit()"}
{"prompt": "def parse_hpo_diseases(hpo_lines):\n    \"\"\"Parse hpo disease phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            diseases(dict): A dictionary with mim numbers as keys\n    \"\"\"", "output": "\n    diseases = {}\n    for line in hpo_lines:\n        line = line.strip()\n        if line == \"\":\n            continue\n        diseases[line] = line\n    return diseases\n\n\ndef parse_hpo_disease_phenotypes(hpo_lines):\n    \"\"\"Parse hpo disease phenotypes\n    \n        Args:\n            hpo_lines(iterable(str))\n        \n        Returns:\n            disease_phenotypes(dict): A dictionary with mim numbers as keys\n    \"\"\"\n    disease_phenotypes = {}\n    for line in hpo_lines:\n        line = line.strip()\n        if line == \"\":\n            continue\n        disease_ph", "solution": "\n    diseases = {}\n    LOG.info(\"Parsing hpo diseases...\")\n    for index, line in enumerate(hpo_lines):\n        # First line is a header\n        if index == 0:\n            continue\n        # Skip empty lines\n        if not len(line) > 3:\n            continue\n        # Parse the info\n        disease_info = parse_hpo_disease(line)\n        # Skip the line if there where no info\n        if not disease_info:\n            continue\n        disease_nr = disease_info['disease_nr']\n        hgnc_symbol = disease_info['hgnc_symbol']\n        hpo_term = disease_info['hpo_term']\n        source = disease_info['source']\n        disease_id = \"{0}:{1}\".format(source, disease_nr)\n        \n        if disease_id not in diseases:\n            diseases[disease_id] = {\n                'disease_nr': disease_nr,\n                'source': source,\n                'hgnc_symbols': set(),\n                'hpo_terms': set(),\n            }\n\n        if hgnc_symbol:\n            diseases[disease_id]['hgnc_symbols'].add(hgnc_symbol)\n        if hpo_term:\n            diseases[disease_id]['hpo_terms'].add(hpo_term)\n\n    LOG.info(\"Parsing done.\")\n    return diseases"}
{"prompt": "def _check_currency_format(self, format=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            format (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"", "output": "\n        if format is None:\n            format = self.currency_format\n        return format\n\n    def _check_currency_symbol(self, symbol=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            symbol (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"\n        if symbol is None:\n            symbol = self.currency_symbol\n        return symbol\n\n    def _check_currency_symbol_position(self, symbol=None):\n        \"\"\"\n        Summary.\n\n        Args:\n            symbol (TYPE, optional): Description\n\n        Returns:\n            name (TYPE): Description\n        \"\"\"\n        if symbol is None:\n            symbol = self.currency_symbol_position\n        return symbol\n\n    def _check_currency_symbol_position_name(self, symbol=None):\n        \"\"\"\n        Summary.\n\n        Args:\n", "solution": "\n        defaults = self.settings['currency']['format']\n        if hasattr(format, '__call__'):\n            format = format()\n        if is_str(format) and re.match('%v', format):\n\n            # Create and return positive, negative and zero formats:\n            return {\n                'pos': format,\n                'neg': format.replace(\"-\", \"\").replace(\"%v\", \"-%v\"),\n                'zero': format\n            }\n        elif not format or not format['por'] or not re.match('%v',\n                                                             format['pos']):\n            self.settings['currency']['format'] = {\n                'pos': defaults,\n                'neg': defaults.replace(\"%v\", \"-%v\"),\n                'zero': defaults\n            }\n            return self.settings\n\n        return format"}
{"prompt": "def setMeterPassword(self, new_pwd, pwd=\"00000000\"):\n        \"\"\" Serial Call to set meter password.  USE WITH CAUTION.\n\n        Args:\n            new_pwd (str): 8 digit numeric password to set\n            pwd (str): Old 8 digit numeric password.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"", "output": "\n        self.sendCommand(\"setMeterPassword\", new_pwd, pwd)\n        return True\n\n    def openMeter(self, meter):\n        \"\"\" Serial Call to open a meter.  USE WITH CAUTION.\n\n        Args:\n            meter (str): Meter name.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"\n        self.sendCommand(\"openMeter\", meter)\n        return True\n\n    def closeMeter(self, meter):\n        \"\"\" Serial Call to close a meter.  USE WITH CAUTION.\n\n        Args:\n            meter (str): Meter name.\n\n        Returns:\n            bool: True on completion with ACK.\n        \"\"\"\n        self.sendCommand(\"closeMeter\", meter)", "solution": "\n        result = False\n        self.setContext(\"setMeterPassword\")\n        try:\n            if len(new_pwd) != 8 or len(pwd) != 8:\n                self.writeCmdMsg(\"Passwords must be exactly eight characters.\")\n                self.setContext(\"\")\n                return result\n\n            if not self.request(False):\n                self.writeCmdMsg(\"Pre command read failed: check serial line.\")\n            else:\n                if not self.serialCmdPwdAuth(pwd):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_pwd = binascii.hexlify(new_pwd.zfill(8))\n                    req_str = \"015731023030323028\" + req_pwd + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setMeterPassword): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"", "output": "\n        msg = str(self.msg).strip()\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n    def getMessageWithArgs(self, *args):\n        \"\"\"\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg).strip()\n        if self.args:\n            msg = msg % args\n        return msg\n\n    def makeMessage(self, args, kwargs):\n        \"\"\"\n        Make a message from this LogRecord.\n\n        The message will be formatted with self.msg % args, using the\n        arguments given as keyword arguments.\n        \"\"\"\n        return self.msg % kwargs\n\n    def makeMessageWithArgs(self, *args, **kwargs):\n        \"\"\"\n        Make a message from this LogRecord.\n\n        The message ", "solution": "\n        msg = self.msg\n        if self.args:\n            msg = msg.format(*self.args)\n        return maybe_encode(msg)"}
{"prompt": "def add_observer(self, callback, ntype, sender):\n        \"\"\"Add an observer callback to this notification center.\n\n        The given callback will be called upon posting of notifications of\n        the given type/sender and will receive any additional arguments passed\n        to post_notification.\n\n        Parameters\n        ----------\n        callback : callable\n            The callable that will be called by :meth:`post_notification`\n            as ``callback(ntype, sender, *args, **kwargs)\n        ntype : hashable\n            The notification type. If None, all notifications from sender\n            will be posted.\n        sender : hashable\n            The notification sender. If None, all notifications of ntype\n            will be posted.\n        \"\"\"", "output": "\n        if ntype is None:\n            ntype = self.ntype\n        if sender is None:\n            sender = self.sender\n        if ntype not in self._observers:\n            self._observers[ntype] = []\n        self._observers[ntype].append(callback)\n\n    def remove_observer(self, callback, ntype, sender):\n        \"\"\"Remove an observer callback to this notification center.\n\n        The given callback will be called upon posting of notifications of\n        the given type/sender and will no longer receive any additional\n        arguments passed to post_notification.\n\n        Parameters\n        ----------\n        callback : callable\n            The callable that will be called by :meth:`post_notification`\n            as ``callback(ntype, sender, *args, **kwargs)\n        ntype : hashable\n            The notification type", "solution": "\n        assert(callback != None)\n        self.registered_types.add(ntype)\n        self.registered_senders.add(sender)\n        self.observers.setdefault((ntype,sender), set()).add(callback)"}
{"prompt": "def cli_run():\n    \"\"\"docstring for argparse\"\"\"", "output": "\n    parser = argparse.ArgumentParser(\n        prog='cli',\n        description='CLI for running a trained model',\n        epilog='''\n        Examples:\n            $ python -m cli train --model models/model.pt --gpu 0 --lr 0.001 --epochs 100\n            $ python -m cli train --model models/model.pt --gpu 0 --lr 0.001 --epochs 100 --lr_decay 0.1 --lr_steps 10 --lr_gamma\n            $ python -m cli train --model models/model.pt --gpu 0 --lr 0.001 --epochs 100 --lr_decay 0.1 --lr_steps 10 ", "solution": "\n    parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow')\n    parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+')\n    parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda')\n    args = parser.parse_args()\n    main(args)"}
{"prompt": "def send_email(self, to, kind, **kwargs):\n        \"\"\" Sends an e-mail to the given address.\n\n        to: The address\n        kind: the ID for an e-mail kind; it should point to a subdirectory of\n            self.template_prefix containing subject.txt and message.html, which\n            are django templates for the subject and HTML message respectively.\n\n        context: a context for rendering the e-mail.\n\n        \"\"\"", "output": "\n        subject = render_to_string(self.subject, kwargs)\n        message = render_to_string(self.message, kwargs)\n        from_email = self.from_email\n        from_name = get_display_name(from_email)\n        send_mail(subject, message, from_email, [to], fail_silently=False)\n\n\nclass EmailBackend(BaseEmailBackend):\n    \"\"\"\n    A backend that sends emails via django.core.mail.EmailBackend.\n    \"\"\"\n    def open(self):\n        \"\"\"\n        Override to implement actions needed after the backend is open.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Override to implement actions needed after the backend is closed.\n        \"\"\"\n        pass\n\n    def send_messages(self, email_messages):\n", "solution": "\n\n        return __send_email__(self.template_prefix, to, kind, **kwargs)"}
{"prompt": "def list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"", "output": "\n    if len(lst) == 0:\n        return content\n    else:\n        return indent + list_to_str(lst[1:], content + lst[0], indent)\n\n\ndef list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"\n    if len(lst) == 0:\n        return content\n    else:\n        return indent + list_to_str(lst[1:], content + lst[0], indent)\n\n\ndef list_to_str(lst: list, content: str, indent: int=1):\n    \"\"\"recurs into list for string computing \"\"\"\n    if len(lst) == 0:\n        return content\n    else:\n        return indent + list_to_str(lst[1:], content + lst[0], indent)\n\n\ndef list_to_str(lst: list, content", "solution": "\n    for i in lst:\n        if isinstance(i, indentable):\n            content = i.to_str(content, indent)\n        elif isinstance(i, list):\n            content = list_to_str(i, content, indent)\n        elif isinstance(i, str):\n            content = catend(content, i, indent)\n    return content"}
{"prompt": "def add_btn_cancel(self,label_cancel):\n        \"\"\"\n        Adds a cancel button to let the user cancel whatever choice they were given.\n        \n        This widget can be triggered by setting the label ``label_cancel`` to a string.\n        \n        This widget will be positioned slightly below the main label and to the right\n        of the confirm button.\n        \"\"\"", "output": "\n        \n        self.confirm_button = Button(\n            parent = self.main_label,\n            text = \"Cancel\",\n            command = lambda: self.confirm_cancel()\n        )\n        self.confirm_button.pack(side = \"left\", padx = 5, pady = 5)\n        \n        self.confirm_button.bind(\"<Return>\", self.confirm_cancel)\n        \n        self.confirm_button.bind(\"<Escape>\", self.confirm_cancel)\n        \n        self.confirm_button.bind(\"<Button-1>\", self.confirm_cancel)\n        \n        self.confirm_button.bind(\"<ButtonRelease-1>\", self.confirm_cancel)\n        \n        self.confirm_button.bind(\"<ButtonRelease-2>\", self.confirm_cancel)\n        \n        self.confirm_button.bind(\"<ButtonRelease", "solution": "\n        # Cancel Button\n        self.wbtn_cancel = button.Button(\"btn_cancel\",self,self.window,self.peng,\n                        pos=lambda sw,sh, bw,bh: (sw/2+4,sh/2-bh/2-bh*2),\n                        size=[0,0],\n                        label=label_cancel,\n                        borderstyle=self.borderstyle\n                        )\n        self.wbtn_cancel.size = lambda sw,sh: (self.wbtn_cancel._label.font_size*8,self.wbtn_cancel._label.font_size*2)\n        self.addWidget(self.wbtn_cancel)\n        \n        def f():\n            self.doAction(\"cancel\")\n            self.exitDialog()\n        self.wbtn_cancel.addAction(\"click\",f)"}
{"prompt": "def _link_field_to_dict(field):\n        \"\"\" Utility for ripping apart github's Link header field.\n        It's kind of ugly.\n        \"\"\"", "output": "\n        field_name = field.split(\";\")[0]\n        field_value = field.split(\";\")[1]\n        field_value = field_value.strip()\n        field_value = field_value.split(\", \")\n        field_value = [x.strip() for x in field_value]\n        field_value = [x.replace(\"\\\"\", \"\") for x in field_value]\n        field_value = [x.replace(\"'\", \"\") for x in field_value]\n        field_value = [x.replace(\",\", \"\") for x in field_value]\n        field_value = [x.replace(\"(\", \"\") for x in field_value]\n        field_value = [x.replace(\")\", \"\") for x in field_value]\n        field_value = [x.replace(\"'\", \"\") for x in field_value]\n        field_value = [x.replace(\",\", \"\") for x in field_", "solution": "\n\n        if not field:\n            return dict()\n\n        return dict([\n            (\n                part.split('; ')[1][5:-1],\n                part.split('; ')[0][1:-1],\n            ) for part in field.split(', ')\n        ])"}
{"prompt": "def _get_protobuf_kind(kind):\n    \"\"\"Converts py_zipkin's Kind to Protobuf's Kind.\n\n    :param kind: py_zipkin's Kind.\n    :type kind: py_zipkin.Kind\n    :return: correcponding protobuf's kind value.\n    :rtype: zipkin_pb2.Span.Kind\n    \"\"\"", "output": "\n    if kind == py_zipkin.Kind.SERVER:\n        return zipkin_pb2.Span.SPAN_KIND_RPC_SERVER\n    elif kind == py_zipkin.Kind.CLIENT:\n        return zipkin_pb2.Span.SPAN_KIND_CLIENT\n    elif kind == py_zipkin.Kind.SERVER_ASYNC:\n        return zipkin_pb2.Span.SPAN_KIND_RPC_SERVER_ASYNC\n    elif kind == py_zipkin.Kind.CLIENT_ASYNC:\n        return zipkin_pb2.Span.SPAN_KIND_CLIENT_ASYNC\n    else:\n        raise ValueError('Unknown kind: %s' % kind)\n\n\ndef _is_server_span(span):\n", "solution": "\n    if kind == Kind.CLIENT:\n        return zipkin_pb2.Span.CLIENT\n    elif kind == Kind.SERVER:\n        return zipkin_pb2.Span.SERVER\n    elif kind == Kind.PRODUCER:\n        return zipkin_pb2.Span.PRODUCER\n    elif kind == Kind.CONSUMER:\n        return zipkin_pb2.Span.CONSUMER\n    return None"}
{"prompt": "def _assert_recur_is_tail(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"", "output": "\n    if node.is_synthetic:\n        return\n    if node.is_do:\n        if node.then_node is not None:\n            _assert_recur_is_tail(node.then_node)\n        if node.else_node is not None:\n            _assert_recur_is_tail(node.else_node)\n\n\ndef _assert_recur_is_head(node: Node) -> None:  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the head position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` ", "solution": "\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)"}
{"prompt": "def remove_namespaces(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"", "output": "\n    for elem in root.iter():\n        elem.tag = elem.tag.split('}')[-1]\n        elem.clear()\n        elem.tail = ''\n    return root\n\n\ndef strip_namespaces(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"\n    for elem in root.iter():\n        elem.tag = elem.tag.split('}')[-1]\n        elem.clear()\n        elem.tail = ''\n    return root\n\n\ndef strip_namespaces_ns(root):\n    \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\"\n    for elem in root.iter():\n        elem.tag = elem.tag.split('}')[-1]\n        elem.clear()\n        elem.tail", "solution": "\n    for elem in root.getiterator():\n        if not hasattr(elem.tag, 'find'):\n            continue\n\n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i + 1:]\n\n    objectify.deannotate(root, cleanup_namespaces=True)"}
{"prompt": "def _block_tuple(iterator, dtypes, bsize=-1):\n    \"\"\"Pack rdd of tuples as tuples of arrays or scipy.sparse matrices.\"\"\"", "output": "\n    dtypes = tuple(dtypes)\n    if bsize < 0:\n        bsize = len(iterator)\n    shape = ()\n    for t in iterator:\n        shape += t.shape\n    shape = tuple(shape)\n    dtypes = tuple(dtypes)\n    iterator = iter(iterator)\n    arrays = []\n    spmatrices = []\n    for t in iterator:\n        arrays.append(np.empty(shape, dtype=dtypes))\n        spmatrices.append(np.empty((bsize,) + shape, dtype=dtypes))\n    arrays = tuple(arrays)\n    spmatrices = tuple(spmatrices)\n    return arrays, spmatrices\n\n\ndef _check_iterable_dtypes(dtypes):\n    \"\"\"Check that all dtypes are iterable.\"\"\"", "solution": "\n    i = 0\n    blocked_tuple = None\n    for tuple_i in iterator:\n        if blocked_tuple is None:\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n\n        if (bsize > 0) and (i >= bsize):\n            yield tuple(_pack_accumulated(x, dtype)\n                        for x, dtype in zip(blocked_tuple, dtypes))\n            blocked_tuple = tuple([] for _ in range(len(tuple_i)))\n            i = 0\n\n        for x_j, x in zip(tuple_i, blocked_tuple):\n            x.append(x_j)\n        i += 1\n    if i > 0:\n        yield tuple(_pack_accumulated(x, dtype)\n                    for x, dtype in zip(blocked_tuple, dtypes))"}
{"prompt": "def ddos_custom_policies(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-11-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_11_01.operations.DdosCustomPoliciesOperations>`\n           * 2018-12-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2018_12_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-02-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_02_01.operations.DdosCustomPoliciesOperations>`\n        \"\"\"", "output": "dosCustomPoliciesOperations>`\n           * 2019-04-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_04_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-06-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_06_01.operations.DdosCustomPoliciesOperations>`\n           * 2019-07-01: :class:`DdosCustomPoliciesOperations<azure.mgmt.network.v2019_07_01.operations.DdosCustomPolici", "solution": "\n        api_version = self._get_api_version('ddos_custom_policies')\n        if api_version == '2018-11-01':\n            from .v2018_11_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2018-12-01':\n            from .v2018_12_01.operations import DdosCustomPoliciesOperations as OperationClass\n        elif api_version == '2019-02-01':\n            from .v2019_02_01.operations import DdosCustomPoliciesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def update(self):\n        \"\"\"Update repository from its remote.\n\n        Calling this method, the repository will be synchronized with\n        the remote repository using 'fetch' command for 'heads' refs.\n        Any commit stored in the local copy will be removed; refs\n        will be overwritten.\n\n        :raises RepositoryError: when an error occurs updating the\n            repository\n        \"\"\"", "output": "\n        self.log.debug('Updating repository from remote repository...')\n        self.log.debug('Fetching remote heads...')\n        heads = self.fetch()\n        self.log.debug('Updating repository from remote heads...')\n        self.log.debug('Updating repository from local copy...')\n        self.update()\n        self.log.debug('Updating repository from local copy...')\n        self.log.debug('Updating repository from remote heads...')\n        self.update()\n        self.log.debug('Updating repository from remote heads...')\n        self.update()\n        self.log.debug('Updating repository from remote heads...')\n        self.update()\n        self.log.debug('Updating repository from remote heads...')\n        self.update()\n        self.log.debug('", "solution": "\n        cmd_update = ['git', 'fetch', 'origin', '+refs/heads/*:refs/heads/*', '--prune']\n        self._exec(cmd_update, cwd=self.dirpath, env=self.gitenv)\n\n        logger.debug(\"Git %s repository updated into %s\",\n                     self.uri, self.dirpath)"}
{"prompt": "def setZeroResettableKWH(self, password=\"00000000\"):\n        \"\"\" Serial call to zero resettable kWh registers.\n\n        Args:\n            password (str): Optional password.\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"", "output": "\n        command = \"AT+CREG?\\r\"\n        response =  self.send_command(command)\n        if response == \"OK\":\n            command = \"AT+CREG?\\r\"\n            response =  self.send_command(command)\n            if response == \"OK\":\n                command = \"AT+CREG?\\r\"\n                response =  self.send_command(command)\n                if response == \"OK\":\n                    command = \"AT+CREG?\\r\"\n                    response =  self.send_command(command)\n                    if response == \"OK\":\n                        command = \"AT+CREG?\\r\"\n                        response =  self.send_command(command)\n                        if response == \"OK\":\n                            command", "solution": "\n        result = False\n        self.setContext(\"setZeroResettableKWH\")\n        try:\n            if not self.requestA():\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_str = \"0157310230304433282903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success: 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result"}
{"prompt": "def add_heart_failure_handler(self, handler):\n        \"\"\"add a new handler for heart failure\"\"\"", "output": "\n        self.heart_failure_handlers.append(handler)\n\n    def add_heart_failure_handler_once(self, handler):\n        \"\"\"add a new handler for heart failure once\"\"\"\n        self.heart_failure_handlers.append(handler)\n\n    def add_heart_failure_handler_many(self, handlers):\n        \"\"\"add a new handler for heart failure many times\"\"\"\n        self.heart_failure_handlers.extend(handlers)\n\n    def add_heart_failure_handler_many(self, handlers):\n        \"\"\"add a new handler for heart failure many times\"\"\"\n        self.heart_failure_handlers.extend(handlers)\n\n    def add_heart_failure_handler_many(self, handlers):\n        \"\"\"add a new handler for heart failure many time", "solution": "\n        self.log.debug(\"heartbeat::new heart failure handler: %s\", handler)\n        self._failure_handlers.add(handler)"}
{"prompt": "def like_hashtag(self, hashtag, amount=None):\n    \"\"\" Likes last medias from hashtag \"\"\"", "output": "\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    hashtag = hashtag.lower()\n    hashtag = hashtag.strip()\n    ", "solution": "\n    self.logger.info(\"Going to like media with hashtag #%s.\" % hashtag)\n    medias = self.get_total_hashtag_medias(hashtag, amount)\n    return self.like_medias(medias)"}
{"prompt": "def __coord_fft_hz(n, sr=22050, **_kwargs):\n    \"\"\"Get the frequencies for FFT bins\"\"\"", "output": "\n    freqs = np.linspace(sr / 2, sr / 2, n)\n    return freqs\n\n\ndef freq_range(freqs, sr=22050, **_kwargs):\n    \"\"\"Get the frequency range for FFT bins\"\"\"\n    freqs = np.linspace(sr / 2, sr / 2, len(freqs))\n    return freqs\n\n\ndef freq_ranges(freqs, sr=22050, **_kwargs):\n    \"\"\"Get the frequency ranges for FFT bins\"\"\"\n    freqs = np.linspace(sr / 2, sr / 2, len(freqs))\n    return freqs\n\n\ndef freq_ranges_hz(freqs, sr=22050, **_kwargs):\n    \"\"\"Get the frequency ranges for FFT bin", "solution": "\n    n_fft = 2 * (n - 1)\n    # The following code centers the FFT bins at their frequencies\n    # and clips to the non-negative frequency range [0, nyquist]\n    basis = core.fft_frequencies(sr=sr, n_fft=n_fft)\n    fmax = basis[-1]\n    basis -= 0.5 * (basis[1] - basis[0])\n    basis = np.append(np.maximum(0, basis), [fmax])\n    return basis"}
{"prompt": "def main(ctx, log_level, keeper_hostname, username, password):\n    \"\"\"ltd is a command-line client for LSST the Docs.\n\n    Use ltd to upload new site builds, and to work with the LTD Keeper API.\n    \"\"\"", "output": "\n    logging.basicConfig(level=log_level)\n    logging.getLogger('requests').setLevel(logging.WARNING)\n    logging.getLogger('urllib3').setLevel(logging.WARNING)\n\n    keeper_client = keeper.KeeperClient(\n        keeper_hostname=keeper_hostname,\n        username=username,\n        password=password,\n    )\n\n    site = keeper_client.site(ctx.site_id)\n    site.builds.list()\n\n    build_id = ctx.build_id\n    build_url = site.builds.build_url(build_id)\n    build_data = site.builds.build_data(build_id)\n    build_data = build_data.copy()\n    build_data['url'] = build_url\n\n    build_data['status'] = build_data['", "solution": "\n    ch = logging.StreamHandler()\n    formatter = logging.Formatter(\n        '%(asctime)s %(levelname)8s %(name)s | %(message)s')\n    ch.setFormatter(formatter)\n\n    logger = logging.getLogger('ltdconveyor')\n    logger.addHandler(ch)\n    logger.setLevel(log_level.upper())\n\n    # Subcommands should use the click.pass_obj decorator to get this\n    # ctx.obj object as the first argument.\n    ctx.obj = {\n        'keeper_hostname': keeper_hostname,\n        'username': username,\n        'password': password,\n        'token': None\n    }"}
{"prompt": "def _fetch_from_archive(self, method, args):\n        \"\"\"Fetch data from the archive\n\n        :param method: the name of the command to execute\n        :param args: the arguments required by the command\n        \"\"\"", "output": "\n        archive_path = self.archive_path\n        archive_name = os.path.basename(archive_path)\n        archive_contents = open(archive_path, \"rb\").read()\n        archive_checksum = hashlib.md5(archive_contents).hexdigest()\n\n        archive_checksum_path = os.path.join(\n            archive_path, archive_checksum\n        )\n\n        if not os.path.exists(archive_path):\n            raise CommandError(\n                \"The archive file %s does not exist\" % archive_path\n            )\n\n        if not os.path.exists(archive_checksum_path):\n            raise CommandError(\n                \"The archive file %s does not have the correct checksum\" % archive_path\n            )\n\n        archive_checksum = open(archive_checksum_path, \"rb\").read()\n\n        if method!= \"fetch\":\n            archive_checksum = ", "solution": "\n        if not self.archive:\n            raise ArchiveError(cause=\"Archive not provided\")\n\n        data = self.archive.retrieve(method, args, None)\n\n        if isinstance(data, nntplib.NNTPTemporaryError):\n            raise data\n\n        return data"}
{"prompt": "def attach_dynamic_tree_items(tree_alias, src_tree_items):\n        \"\"\"Attaches dynamic sitetrees items registered with `register_dynamic_trees()`\n        to an initial (source) items list.\n\n        :param str|unicode tree_alias:\n        :param list src_tree_items:\n        :rtype: list\n        \"\"\"", "output": "\n        tree_items = []\n        for tree_item in src_tree_items:\n            tree_items.append(tree_item.tree_alias)\n            tree_items.append(tree_item.tree_item)\n            tree_items.append(tree_item.tree_item_id)\n            tree_items.append(tree_item.tree_item_type)\n            tree_items.append(tree_item.tree_item_name)\n            tree_items.append(tree_item.tree_item_parent)\n            tree_items.append(tree_item.tree_item_children)\n            tree_items.append(tree_item.tree_item_children_count)\n            tree_items.append(tree_item.tree_item_children_type)\n            tree_items.append(tree_item.tree_item_children_name)\n            tree_items.append(tree_item.tree_item_children_count)", "solution": "\n        if not _DYNAMIC_TREES:\n            return src_tree_items\n\n        # This guarantees that a dynamic source stays intact,\n        # no matter how dynamic sitetrees are attached.\n        trees = deepcopy(_DYNAMIC_TREES)\n\n        items = []\n        if not src_tree_items:\n            if _IDX_ORPHAN_TREES in trees and tree_alias in trees[_IDX_ORPHAN_TREES]:\n                for tree in trees[_IDX_ORPHAN_TREES][tree_alias]:\n                    items.extend(tree.dynamic_items)\n        else:\n\n            # TODO Seems to be underoptimized %)\n\n            # Tree item attachment by alias.\n            for static_item in list(src_tree_items):\n                items.append(static_item)\n                if not static_item.alias:\n                    continue\n\n                idx = _IDX_TPL % (tree_alias, static_item.alias)\n                if idx not in trees:\n                    continue\n\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    for dyn_item in tree.dynamic_items:\n                        if dyn_item.parent is None:\n                            dyn_item.parent = static_item\n                        # Unique IDs are required for the same trees attached\n                        # to different parents.\n                        dyn_item.id = generate_id_for(dyn_item)\n                        items.append(dyn_item)\n\n            # Tree root attachment.\n            idx = _IDX_TPL % (tree_alias, None)\n            if idx in _DYNAMIC_TREES:\n                trees = deepcopy(_DYNAMIC_TREES)\n                for tree in trees[idx]:\n                    tree.alias = tree_alias\n                    items.extend(tree.dynamic_items)\n\n        return items"}
{"prompt": "def snip_print(str,width = 75,print_full = 0,header = ''):\n    \"\"\"Print a string snipping the midsection to fit in width.\n\n    print_full: mode control:\n      - 0: only snip long strings\n      - 1: send to page() directly.\n      - 2: snip long strings and ask for full length viewing with page()\n    Return 1 if snipping was necessary, 0 otherwise.\"\"\"", "output": "\n    \n    str_len = len(str)\n    \n    if str_len < width:\n        return 0\n    \n    if print_full:\n        page(header,str_len)\n    \n    mid = width // 2\n    \n    str_len = str_len - mid\n    \n    if str_len < mid:\n        return 0\n    \n    page(header,str_len)\n    \n    page(str[:mid],mid)\n    \n    page(str[mid:],str_len)\n    \n    return 1\n\ndef snip_print(str,width = 75,print_full = 0,header = '',print_page = 0):\n    \"\"\"Print a string snipping the midsection to fi", "solution": "\n\n    if print_full == 1:\n        page(header+str)\n        return 0\n\n    print header,\n    if len(str) < width:\n        print str\n        snip = 0\n    else:\n        whalf = int((width -5)/2)\n        print str[:whalf] + ' <...> ' + str[-whalf:]\n        snip = 1\n    if snip and print_full == 2:\n        if raw_input(header+' Snipped. View (y/n)? [N]').lower() == 'y':\n            page(str)\n    return snip"}
{"prompt": "def _choi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Choi representation to the Chi representation.\"\"\"", "output": "\n    chi_data = data.copy()\n    chi_data.flat[::input_dim] = np.exp(chi_data.flat[::input_dim])\n    chi_data = chi_data.sum(axis=0, keepdims=True)\n    chi_data = chi_data / chi_data.sum(axis=0, keepdims=True)\n    return chi_data\n\n\ndef _chi_to_chi(data, input_dim, output_dim):\n    \"\"\"Transform Chi representation to the Choi representation.\"\"\"\n    chi_data = data.copy()\n    chi_data.flat[::input_dim] = np.exp(chi_data.flat[::input_dim])\n    chi_data = chi_data.sum(axis=0, keepdims=True)\n    chi_data = chi_data / ch", "solution": "\n    num_qubits = int(np.log2(input_dim))\n    return _transform_to_pauli(data, num_qubits)"}
{"prompt": "def rst_msg(self, text, opts={}):\n        \"\"\"Convert ReStructuredText and run through msg()\"\"\"", "output": "\n        opts['text'] = text\n        return self.msg(text, **opts)\n\n    def msg(self, text, opts={}):\n        \"\"\"Convert plain text and run through msg()\"\"\"\n        opts['text'] = text\n        return self.msg_plain(text, **opts)\n\n    def msg_plain(self, text, opts={}):\n        \"\"\"Convert plain text and run through msg()\"\"\"\n        opts['text'] = text\n        opts['format'] = 'rst'\n        return self.convert(text, **opts)\n\n    def convert(self, text, opts={}):\n        \"\"\"Convert plain text and run through msg()\"\"\"\n        opts['text'] = text\n        opts['format'] = 'txt'\n        return self.convert(text, **opts)\n\n", "solution": "\n        text = Mformat.rst_text(text,\n                                'plain' == self.debugger.settings['highlight'],\n                                self.debugger.settings['width'])\n        return self.msg(text)"}
{"prompt": "def measure_topology(fbasename=None, log=None, ml_version=ml_version):\n    \"\"\"Measures mesh topology\n\n    Args:\n        fbasename (str): input filename.\n        log (str): filename to log output\n\n    Returns:\n        dict: dictionary with the following keys:\n            vert_num (int): number of vertices\n            edge_num (int): number of edges\n            face_num (int): number of faces\n            unref_vert_num (int): number or unreferenced vertices\n            boundry_edge_num (int): number of boundary edges\n            part_num (int): number of parts (components) in the mesh.\n            manifold (bool): True if mesh is two-manifold, otherwise false.\n            non_manifold_edge (int): number of non_manifold edges.\n            non_manifold_vert (int): number of non-manifold verices\n            genus (int or str): genus of the mesh, either a number or\n                'undefined' if the mesh is non-manifold.\n            holes (int or str): number of holes in the mesh, either a number\n                or 'undefined' if the mesh is non-manifold.\n\n    \"\"\"", "output": "old (bool): True if mesh is one-manifold, otherwise false.\n            face_num (int): number of faces in the mesh.\n            face_vert_num (int): number of vertices in each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex id of each face.\n            face_vert_id (int): vertex ", "solution": "\n    ml_script1_file = 'TEMP3D_measure_topology.mlx'\n    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)\n    compute.measure_topology(ml_script1)\n    ml_script1.save_to_file(ml_script1_file)\n    ml_script1.run_script(log=log, script_file=ml_script1_file)\n    topology = ml_script1.topology\n    return topology"}
{"prompt": "def import_submodule(mod, subname, fullname):\n    \"\"\"m = import_submodule(mod, subname, fullname)\"\"\"", "output": "\n    mod = sys.modules[fullname]\n    parts = fullname.split('.')\n    parts.pop()\n    part = '.'.join(parts)\n    obj = mod.__dict__.get(part)\n    if obj is None:\n        raise ImportError('No module named %s' % fullname)\n    return obj\n\n\ndef import_string(import_str):\n    \"\"\"Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import failed.\"\"\"\n    mod_str, _sep, class_str = import_str.rpartition('.')\n    try:\n        return import_module(mod_str), getattr(import_module(mod_str), class_str)\n    except ImportError, e:\n        raise ImportError('", "solution": "\n    # Require:\n    # if mod == None: subname == fullname\n    # else: mod.__name__ + \".\" + subname == fullname\n\n    global found_now\n    if fullname in found_now and fullname in sys.modules:\n        m = sys.modules[fullname]\n    else:\n        print 'Reloading', fullname\n        found_now[fullname] = 1\n        oldm = sys.modules.get(fullname, None)\n\n        if mod is None:\n            path = None\n        elif hasattr(mod, '__path__'):\n            path = mod.__path__\n        else:\n            return None\n\n        try:\n            # This appears to be necessary on Python 3, because imp.find_module()\n            # tries to import standard libraries (like io) itself, and we don't\n            # want them to be processed by our deep_import_hook.\n            with replace_import_hook(original_import):\n                fp, filename, stuff = imp.find_module(subname, path)\n        except ImportError:\n            return None\n\n        try:\n            m = imp.load_module(fullname, fp, filename, stuff)\n        except:\n            # load_module probably removed name from modules because of\n            # the error.  Put back the original module object.\n            if oldm:\n                sys.modules[fullname] = oldm\n            raise\n        finally:\n            if fp: fp.close()\n\n        add_submodule(mod, m, fullname, subname)\n\n    return m"}
{"prompt": "def velocity_from_bundle(self, bundle):\n        \"\"\"[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value.\"\"\"", "output": "\n        warnings.warn(\n            \"velocity_from_bundle() is deprecated and will be removed in a future version. \"\n            \"Use velocity_from_bundles() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.velocity_from_bundles(bundle)\n\n    def velocity_from_bundles(self, bundles):\n        \"\"\"[DEPRECATED] Return velocity, given the `coefficient_bundle()` return value.\"\"\"\n        warnings.warn(\n            \"velocity_from_bundles() is deprecated and will be removed in a future version. \"\n            \"Use velocity_from_bundle() instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        velocity = Vector3D()\n        for bundle in bundle", "solution": "\n\n        coefficients, days_per_set, T, twot1 = bundle\n        coefficient_count = coefficients.shape[2]\n\n        # Chebyshev derivative:\n\n        dT = np.empty_like(T)\n        dT[0] = 0.0\n        dT[1] = 1.0\n        dT[2] = twot1 + twot1\n        for i in range(3, coefficient_count):\n            dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]\n        dT *= 2.0\n        dT /= days_per_set\n\n        return (dT.T * coefficients).sum(axis=2)"}
{"prompt": "def update(self):\n        \"\"\"Update cameras and motion settings with latest from API.\"\"\"", "output": "\n        # Update camera settings from API\n        camera_settings = self.api.camera_settings()\n        camera_settings.update(\n            **self._camera_settings_from_api(\n                camera_settings.camera_settings()\n            )\n        )\n\n        # Update motion settings from API\n        motion_settings = self.api.motion_settings()\n        motion_settings.update(\n            **self._motion_settings_from_api(\n                motion_settings.motion_settings()\n            )\n        )\n\n        # Update camera and motion settings with latest from API\n        self._camera_settings_from_api(camera_settings)\n        self._motion_settings_from_api(motion_settings)\n\n        # Update camera and motion settings with latest from API\n        self._", "solution": "\n        cameras = self._api.camera_list()\n        self._cameras_by_id = {v.camera_id: v for i, v in enumerate(cameras)}\n\n        motion_settings = []\n        for camera_id in self._cameras_by_id.keys():\n            motion_setting = self._api.camera_event_motion_enum(camera_id)\n            motion_settings.append(motion_setting)\n\n        self._motion_settings_by_id = {\n            v.camera_id: v for i, v in enumerate(motion_settings)}"}
{"prompt": "def get_refs(self, location):\n        \"\"\"Return map of named refs (branches or tags) to commit hashes.\"\"\"", "output": "\n        refs = {}\n        for ref in self._refs:\n            refs[ref] = self._get_ref(ref)\n        return refs\n\n    def get_ref(self, name):\n        \"\"\"Get a named reference by full name.\n\n        Args:\n            name (str): The full name of the reference.\n\n        Returns:\n            str: The commit hash for the reference.\n        \"\"\"\n        return self._refs.get(name)\n\n    def head(self, ref):\n        \"\"\"Get the commit hash for the specified reference.\n\n        Args:\n            ref (str): The full name of the reference.\n\n        Returns:\n            str: The commit hash for the reference.\n        \"\"\"\n        return self._get_ref(ref)\n\n    def branch(self, name):\n        \"\"\"Get the commit hash for the specified branch.\n\n        Args:\n            name (str): The ", "solution": "\n        output = call_subprocess([self.cmd, 'show-ref'],\n                                 show_stdout=False, cwd=location)\n        rv = {}\n        for line in output.strip().splitlines():\n            commit, ref = line.split(' ', 1)\n            ref = ref.strip()\n            ref_name = None\n            if ref.startswith('refs/remotes/'):\n                ref_name = ref[len('refs/remotes/'):]\n            elif ref.startswith('refs/heads/'):\n                ref_name = ref[len('refs/heads/'):]\n            elif ref.startswith('refs/tags/'):\n                ref_name = ref[len('refs/tags/'):]\n            if ref_name is not None:\n                rv[ref_name] = commit.strip()\n        return rv"}
{"prompt": "def main():\n    \"\"\"The main function. Instantiates a GameState object and then\n    enters a REPL-like main loop, waiting for input, updating the state\n    based on the input, then outputting the new state.\"\"\"", "output": "\n    game = GameState()\n    game.start()\n    game.run()\n\n\nif __name__ == '__main__':\n    main()\n\n<|/ file ext=.py |>\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nA simple game that plays against a computer.\n\nThis game is designed to play against a computer by simulating the\ngame by playing against the computer's opponent.\n\nThe game is designed to play against a computer by simulating the\ngame by playing against the computer's opponent.\n\nThe game is designed to play against a computer by simulating the\ngame by playing against the computer", "solution": "\n\n    state = GameState()\n    print(state)\n    while state.running:\n        input = get_single_char()\n\n        state, should_advance = state.handle_input(input)\n        if should_advance:\n            state = state.advance_robots()\n            state = state.check_game_end()\n\n        print(state)\n\n    print(state.message)"}
{"prompt": "def delete_policy_name(self, name):\n        \"\"\"**Description**\n            Delete the policy with the given name.\n\n        **Arguments**\n            - name: the name of the policy to delete\n\n        **Success Return Value**\n            The JSON object representing the now-deleted policy.\n\n        **Example**\n            `examples/delete_policy.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_policy.py>`_\n\n        \"\"\"", "output": "\n        uri = self._build_uri(\n            \"/policies/{name}\"\n           .format(name=self._encode_name(name))\n        )\n        return self._delete(uri, expect_errors=True)\n\n    def list_policies(self):\n        \"\"\"**Description**\n            List the policies available in the SDC instance.\n\n        **Arguments**\n            - None\n\n        **Success Return Value**\n            A list of policies available in the SDC instance.\n\n        **Example**\n            `examples/list_policies.py <https://github.com/draios/python-sdc-client/blob/master/examples/list_policies.py>`_\n\n        \"\"\"\n        uri = self._build_uri(\"/policies\")\n        return self._get(uri, expect_errors=True)\n\n    def list_", "solution": "\n        res = requests.get(self.url + '/api/policies', headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        # Find the policy with the given name and delete it\n        for policy in res.json()[\"policies\"]:\n            if policy[\"name\"] == name:\n                return self.delete_policy_id(policy[\"id\"])\n\n        return [False, \"No policy with name {}\".format(name)]"}
{"prompt": "def share_vm_image(self, vm_image_name, permission):\n        \"\"\"\n        Share an already replicated OS image. This operation is only for\n        publishers. You have to be registered as image publisher with Windows\n        Azure to be able to call this.\n\n        vm_image_name:\n            The name of the virtual machine image to share\n        permission:\n            The sharing permission: public, msdn, or private\n        \"\"\"", "output": "\n        share = Share()\n        share.set_name('OSImage')\n        share.set_permission(permission)\n        share.set_vm_image_name(vm_image_name)\n        share.set_vm_image_location(self.vm_image_location)\n        share.set_vm_image_publisher(self.vm_image_publisher)\n        share.set_vm_image_offer(self.vm_image_offer)\n        share.set_vm_image_sku(self.vm_image_sku)\n        share.set_vm_image_version(self.vm_image_version)\n        share.set_vm_image_os_disk(self.vm_image_os_disk)\n        share.set_vm_image_os_disk_size(self.vm_image_", "solution": "\n        _validate_not_none('vm_image_name', vm_image_name)\n        _validate_not_none('permission', permission)\n\n        path = self._get_sharing_path_using_vm_image_name(vm_image_name)\n        query = '&permission=' + permission\n        path = path + '?' + query.lstrip('&')\n\n        return self._perform_put(\n            path, None, as_async=True, x_ms_version='2015-04-01'\n        )"}
{"prompt": "def get_stats(self, username='', password='', organization='llnl', force=True):\n        \"\"\"\n        Retrieves the traffic for the users of the given organization.\n        Requires organization admin credentials token to access the data.\n        \"\"\"", "output": "\n        organization = organization.lower()\n        org = Organization.objects.get(name=organization)\n        orgadmin = org.get_admin_user()\n        orgadmin = orgadmin.user if orgadmin else None\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = orgadmin.user if orgadmin else None\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = orgadmin or get_system_user()\n        orgadmin = org", "solution": "\n        date = str(datetime.date.today())\n        referrers_file_path =  ('../github_stats_output/referrers.csv')\n        views_file_path =  ('../github_stats_output/views.csv')\n        clones_file_path =  ('../github_stats_output/clones.csv')\n        if force or not os.path.isfile(file_path):\n            my_github.login(username, password)\n            calls_beginning = self.logged_in_gh.ratelimit_remaining + 1\n            print 'Rate Limit: ' + str(calls_beginning)\n            my_github.get_org(organization)\n            my_github.get_traffic()\n            views_row_count = my_github.check_data_redundancy(file_path=views_file_path,\n                dict_to_check=self.views)\n            clones_row_count = my_github.check_data_redundancy(file_path=clones_file_path,\n                dict_to_check=self.clones)\n            my_github.write_to_file(referrers_file_path=referrers_file_path,\n                views_file_path=views_file_path,\n                clones_file_path=clones_file_path,\n                views_row_count=views_row_count,\n                clones_row_count=clones_row_count)\n            my_github.write_json(dict_to_write=self.referrers_json,\n                path_ending_type='traffic_popular_referrers')\n            my_github.write_json(dict_to_write=self.views_json,\n                path_ending_type='traffic_views')\n            my_github.write_json(dict_to_write=self.clones_json,\n                path_ending_type='traffic_clones')\n            my_github.write_json(dict_to_write=self.releases_json,\n                path_ending_type='releases')\n            calls_remaining = self.logged_in_gh.ratelimit_remaining\n            calls_used = calls_beginning - calls_remaining\n            print ('Rate Limit Remaining: ' + str(calls_remaining) + '\\nUsed '\n                + str(calls_used) + ' API calls.')"}
{"prompt": "def save_setting(self, setting_name, value):\n        \"\"\"Saves the setting value into the database.\"\"\"", "output": "\n        setting = Setting(setting_name, value)\n        self.session.add(setting)\n        self.session.commit()\n\n    def get_setting(self, setting_name):\n        \"\"\"Returns the setting value from the database.\"\"\"\n        setting = self.session.query(Setting).filter_by(setting_name=setting_name).first()\n        return setting.value\n\n    def delete_setting(self, setting_name):\n        \"\"\"Deletes the setting from the database.\"\"\"\n        setting = self.session.query(Setting).filter_by(setting_name=setting_name).first()\n        self.session.delete(setting)\n        self.session.commit()\n\n    def get_settings(self):\n        \"\"\"Returns a list of all settings.\"\"\"\n        return self.session.query(Setting).all()\n\n    def get_settings_by_name(self, setting_name):\n        \"\"\"Returns a list of all settings with the given name.\"\"\"\n        return self.session.query(Setting).filter_by(setting_name=", "solution": "\n        setting = self.get_setting(setting_name)\n        if setting is None:\n            setting = models.DashboardWidgetSettings.objects.create(\n                widget_name=self.get_name(),\n                setting_name=setting_name,\n                value=value)\n        setting.value = value\n        setting.save()\n        return setting"}
{"prompt": "def get_extended_key_usage_from_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the extendedKeyUsage\n    extension.\n    \"\"\"", "output": "\n    extended_key_usage = certificate.get_extension_for_oid(\n        EXTENSION_OID_COMMON_EXTENSION_KEY_USAGE)\n    if extended_key_usage is None:\n        return None\n    return extended_key_usage.get_value()\n\n\ndef get_extended_key_usage_from_x509_certificate(certificate):\n    \"\"\"\n    Given an X.509 certificate, extract and return the extendedKeyUsage\n    extension.\n    \"\"\"\n    extended_key_usage = certificate.get_extension_for_oid(\n        EXTENSION_OID_COMMON_EXTENSION_KEY_USAGE)\n    if extended_key_usage is None:\n        return None\n    return extended_key_usage.get_value()\n\n\ndef get_extended_key_usage_from_x509_crl", "solution": "\n    try:\n        return certificate.extensions.get_extension_for_oid(\n            x509.oid.ExtensionOID.EXTENDED_KEY_USAGE\n        ).value\n    except x509.ExtensionNotFound:\n        return None"}
{"prompt": "def join_html_attrs(attrs):\n    \"\"\"Joins the map structure into HTML attributes.\n\n    The return value is a 2-tuple ``(template, ordered_values)``. It should be\n    passed into :class:`markupsafe.Markup` to prevent XSS attacked.\n\n    e.g.::\n\n        >>> join_html_attrs({'href': '/', 'data-active': 'true'})\n        ('data-active=\"{0}\" href=\"{1}\"', ['true', '/'])\n    \"\"\"", "output": "\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    attr = next(attrs)\n    attrs = iter(attrs)\n    ", "solution": "\n    attrs = collections.OrderedDict(freeze_dict(attrs or {}))\n    template = ' '.join('%s=\"{%d}\"' % (k, i) for i, k in enumerate(attrs))\n    return template, list(attrs.values())"}
{"prompt": "def restore_sys_module_state(self):\n        \"\"\"Restore the state of the sys module.\"\"\"", "output": "\n        sys.path[:] = self.sys_path\n        sys.meta_path[:] = self.sys_meta_path\n        sys.path_importer_cache[:] = self.sys_path_importer_cache\n        sys.path_importer_cache_updated.clear()\n\n    def clear_sys_path_hooks(self):\n        \"\"\"Clear the path hooks.\"\"\"\n        sys.path_hooks.clear()\n        sys.path_importer_cache.clear()\n        sys.path_importer_cache_updated.clear()\n\n    def clear_sys_path_importer_cache(self):\n        \"\"\"Clear the path importer cache.\"\"\"\n        sys.path_importer_cache.clear()\n\n    def clear_sys_path_importer_cache_updated(self):\n        \"\"\"Clear the path importer cache updated.\"\"\"\n", "solution": "\n        try:\n            for k, v in self._orig_sys_module_state.iteritems():\n                setattr(sys, k, v)\n        except AttributeError:\n            pass\n        # Reset what what done in self.init_sys_modules\n        if self._orig_sys_modules_main_mod is not None:\n            sys.modules[self._orig_sys_modules_main_name] = self._orig_sys_modules_main_mod"}
{"prompt": "def dequantize_flow(dx, dy, max_val=0.02, denorm=True):\n    \"\"\"Recover from quantized flow.\n\n    Args:\n        dx (ndarray): Quantized dx.\n        dy (ndarray): Quantized dy.\n        max_val (float): Maximum value used when quantizing.\n        denorm (bool): Whether to multiply flow values with width/height.\n\n    Returns:\n        ndarray: Dequantized flow.\n    \"\"\"", "output": "\n    flow_range = np.abs(dx.astype(np.float32) - dy.astype(np.float32))\n    flow_range = flow_range / max_val\n    flow_range = flow_range.astype(np.float32)\n    flow_range = np.minimum(flow_range, 1.0)\n    flow_range = np.maximum(flow_range, 0.0)\n    flow_range = flow_range / 255.\n    if denorm:\n        flow_range = flow_range * max_val\n    return flow_range\n\n\ndef dequantize_flow_to_rgb(flow, pixel_range):\n    \"\"\"Recover from quantized flow to rgb.\n\n    Args:\n        flow (ndarray): Quantized flow.\n        pixel_range (tuple[float]): Range of pixel values.\n\n", "solution": "\n    assert dx.shape == dy.shape\n    assert dx.ndim == 2 or (dx.ndim == 3 and dx.shape[-1] == 1)\n\n    dx, dy = [dequantize(d, -max_val, max_val, 255) for d in [dx, dy]]\n\n    if denorm:\n        dx *= dx.shape[1]\n        dy *= dx.shape[0]\n    flow = np.dstack((dx, dy))\n    return flow"}
{"prompt": "def addCases(self, tupesValStmnts):\n        \"\"\"\n        Add multiple case statements from iterable of tuleles\n        (caseVal, statements)\n        \"\"\"", "output": "\n        for case in tupesValStmnts:\n            self.cases.append(case)\n\n    def addCase(self, caseVal, statements):\n        \"\"\"\n        Add case statement to tupeles\n        \"\"\"\n        self.cases.append(Case(caseVal, statements))\n\n    def addCases(self, tupesValStmnts):\n        \"\"\"\n        Add multiple case statements from iterable of tuleles\n        (caseVal, statements)\n        \"\"\"\n        for case in tupesValStmnts:\n            self.cases.append(case)\n\n    def addCase(self, caseVal, statements):\n        \"\"\"\n        Add case statement to tupeles\n        \"\"\"\n        self.", "solution": "\n        s = self\n        for val, statements in tupesValStmnts:\n            s = s.Case(val, statements)\n        return s"}
{"prompt": "def create_endpoint(port=None, service_name=None, host=None, use_defaults=True):\n    \"\"\"Creates a new Endpoint object.\n\n    :param port: TCP/UDP port. Defaults to 0.\n    :type port: int\n    :param service_name: service name as a str. Defaults to 'unknown'.\n    :type service_name: str\n    :param host: ipv4 or ipv6 address of the host. Defaults to the\n    current host ip.\n    :type host: str\n    :param use_defaults: whether to use defaults.\n    :type use_defaults: bool\n    :returns: zipkin Endpoint object\n    \"\"\"", "output": "\n    endpoint = Endpoint(\n        host=host,\n        port=port,\n        service_name=service_name,\n        use_defaults=use_defaults,\n    )\n    return endpoint\n\n\ndef endpoint_from_dict(endpoint_dict):\n    \"\"\"Creates a new Endpoint object from dict.\n\n    :param endpoint_dict: dict with endpoint info\n    :type endpoint_dict: dict\n    :returns: zipkin Endpoint object\n    \"\"\"\n    host = endpoint_dict.get('host')\n    port = endpoint_dict.get('port')\n    service_name = endpoint_dict.get('service_name')\n    use_defaults = endpoint_dict.get('use_defaults')\n    endpoint = Endpoint(\n        host=host,\n        port=port,\n        service_name=service_name,", "solution": "\n    if use_defaults:\n        if port is None:\n            port = 0\n        if service_name is None:\n            service_name = 'unknown'\n        if host is None:\n            try:\n                host = socket.gethostbyname(socket.gethostname())\n            except socket.gaierror:\n                host = '127.0.0.1'\n\n    ipv4 = None\n    ipv6 = None\n\n    if host:\n        # Check ipv4 or ipv6.\n        try:\n            socket.inet_pton(socket.AF_INET, host)\n            ipv4 = host\n        except socket.error:\n            # If it's not an ipv4 address, maybe it's ipv6.\n            try:\n                socket.inet_pton(socket.AF_INET6, host)\n                ipv6 = host\n            except socket.error:\n                # If it's neither ipv4 or ipv6, leave both ip addresses unset.\n                pass\n\n    return Endpoint(\n        ipv4=ipv4,\n        ipv6=ipv6,\n        port=port,\n        service_name=service_name,\n    )"}
{"prompt": "def main():\n    \"\"\"\n    Parse argv for options and arguments, and start schema generation.\n    \"\"\"", "output": "\n    options = parse_options()\n    arguments = parse_arguments(options)\n    generate_schema(options)\n\n\nif __name__ == \"__main__\":\n    main()\n\n<|/ file source=github |>\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nSchema generation script.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom schema_generator import generate_schema\n\n\ndef parse_options():\n    \"\"\"\n    Parse command line options.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Schema generation script.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        \"-o\", \"--output\",\n        help=\"Output file name.\",\n        dest=\"output\",\n        default=\"schema.json\")\n    parser.add_argument(\n        \"-s\", \"--schema\",\n        help=\"Schema", "solution": "\n    parser = optparse.OptionParser(usage=\"%prog [options] <model_path> [another_model_path...]\",\n                                   version=xtuml.version.complete_string,\n                                   formatter=optparse.TitledHelpFormatter())\n                                   \n    parser.set_description(__doc__.strip())\n    \n    parser.add_option(\"-c\", \"--component\", dest=\"component\", metavar=\"NAME\",\n                      help=\"export sql schema for the component named NAME\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-d\", \"--derived-attributes\", dest=\"derived\",\n                      help=\"include derived attributes in the schema\",\n                      action=\"store_true\", default=False)\n    \n    parser.add_option(\"-o\", \"--output\", dest='output', metavar=\"PATH\",\n                      help=\"save sql schema to PATH (required)\",\n                      action=\"store\", default=None)\n    \n    parser.add_option(\"-v\", \"--verbosity\", dest='verbosity', action=\"count\", \n                      help=\"increase debug logging level\", default=2)\n\n    \n    (opts, args) = parser.parse_args()\n    if len(args) == 0 or opts.output is None:\n        parser.print_help()\n        sys.exit(1)\n\n    levels = {\n              0: logging.ERROR,\n              1: logging.WARNING,\n              2: logging.INFO,\n              3: logging.DEBUG,\n    }\n    logging.basicConfig(level=levels.get(opts.verbosity, logging.DEBUG))\n\n    loader = ooaofooa.Loader()\n    for filename in args:\n        loader.filename_input(filename)\n\n    c = loader.build_component(opts.component, opts.derived)\n    xtuml.persist_database(c, opts.output)"}
{"prompt": "def disconnect(self, mol):\n        \"\"\"Break covalent bonds between metals and organic atoms under certain conditions.\n\n        The algorithm works as follows:\n\n        - Disconnect N, O, F from any metal.\n        - Disconnect other non-metals from transition metals + Al (but not Hg, Ga, Ge, In, Sn, As, Tl, Pb, Bi, Po).\n        - For every bond broken, adjust the charges of the begin and end atoms accordingly.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: The molecule with metals disconnected.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"", "output": "connected.\n        :rtype: rdkit.Chem.rdchem.Mol\n        \"\"\"\n        disconnected_mols = []\n        for bond in mol.GetBonds():\n            bond_type = bond.GetBondType()\n            if bond_type == rdkit.Chem.rdchem.BondType.SINGLE:\n                bond_atom1 = bond.GetBeginAtom()\n                bond_atom2 = bond.GetEndAtom()\n                bond_atom1.SetChiralTag(1)\n                bond_atom2.SetChiralTag(1)\n                bond_atom1.SetImplicitValence(self.charge_at_atom(bond_atom", "solution": "\n        log.debug('Running MetalDisconnector')\n        # Remove bonds that match SMARTS\n        for smarts in [self._metal_nof, self._metal_non]:\n            pairs = mol.GetSubstructMatches(smarts)\n            rwmol = Chem.RWMol(mol)\n            orders = []\n            for i, j in pairs:\n                # TODO: Could get the valence contributions of the bond instead of GetBondTypeAsDouble?\n                orders.append(int(mol.GetBondBetweenAtoms(i, j).GetBondTypeAsDouble()))\n                rwmol.RemoveBond(i, j)\n            # Adjust neighbouring charges accordingly\n            mol = rwmol.GetMol()\n            for n, (i, j) in enumerate(pairs):\n                chg = orders[n]\n                atom1 = mol.GetAtomWithIdx(i)\n                atom1.SetFormalCharge(atom1.GetFormalCharge() + chg)\n                atom2 = mol.GetAtomWithIdx(j)\n                atom2.SetFormalCharge(atom2.GetFormalCharge() - chg)\n                log.info('Removed covalent bond between %s and %s', atom1.GetSymbol(), atom2.GetSymbol())\n        Chem.SanitizeMol(mol)\n        return mol"}
{"prompt": "def accept_alert(self, text=None, wait=None):\n        \"\"\"\n        Execute the wrapped code, accepting an alert.\n\n        Args:\n            text (str | RegexObject, optional): Text to match against the text in the modal.\n            wait (int | float, optional): Maximum time to wait for the modal to appear after\n                executing the wrapped code.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"", "output": "\n        wait = wait or self.wait\n        modal = ModalDialog(self.modal_name, text)\n        modal.wait_for(wait)\n        modal.accept()\n\n    def dismiss_alert(self):\n        \"\"\"\n        Execute the wrapped code, dismissing the alert.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been found.\n        \"\"\"\n        modal = ModalDialog(self.modal_name)\n        modal.dismiss()\n\n    def wait_for(self, wait):\n        \"\"\"\n        Wait for the modal to appear.\n\n        Args:\n            wait (int | float): Maximum time to wait for the modal to appear.\n\n        Raises:\n            ModalNotFound: If a modal dialog hasn't been ", "solution": "\n\n        wait = wait or capybara.default_max_wait_time\n        with self.driver.accept_modal(\"alert\", text=text, wait=wait):\n            yield"}
{"prompt": "def build_type(s_dt):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.\n    \"\"\"", "output": "\n    s_types = []\n    for s_type in s_dt.types:\n        s_types.append(build_type(s_type))\n    return s_types\n\n\ndef build_type_from_type(s_type):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_EDT, S_SDT and S_UDT.\n    \"\"\"\n    s_types = []\n    for s_type in s_type.types:\n        s_types.append(build_type_from_type(s_type))\n    return s_types\n\n\ndef build_type_from_type_and_subtype(s_type, s_subtype):\n    \"\"\"\n    Build a partial xsd tree out of a S_DT and its sub types S_CDT, S_ED", "solution": "\n    s_cdt = nav_one(s_dt).S_CDT[17]()\n    if s_cdt:\n        return build_core_type(s_cdt)\n    \n    s_edt = nav_one(s_dt).S_EDT[17]()\n    if s_edt:\n        return build_enum_type(s_edt)\n    \n    s_udt = nav_one(s_dt).S_UDT[17]()\n    if s_udt:\n        return build_user_type(s_udt)"}
{"prompt": "def _get_obj_count_difference(objs1, objs2):\n    \"\"\"Returns count difference in two collections of Python objects.\"\"\"", "output": "\n    counts1 = collections.Counter(objs1)\n    counts2 = collections.Counter(objs2)\n    return counts1 - counts2\n\n\ndef _is_list_like(obj):\n    \"\"\"Returns True if obj is a list-like object.\"\"\"\n    return hasattr(obj, '__iter__') and not isinstance(obj, str)\n\n\ndef _is_dict_like(obj):\n    \"\"\"Returns True if obj is a dict-like object.\"\"\"\n    return hasattr(obj, 'items') and not isinstance(obj, str)\n\n\ndef _is_set_like(obj):\n    \"\"\"Returns True if obj is a set-like object.\"\"\"\n    return hasattr(obj, 'add') and not isinstance(obj, str)\n\n\ndef _is_dict_or_set_like(obj):\n    \"\"\"Returns True if obj is a dict-like object or set-like object", "solution": "\n    clean_obj_list1 = _process_in_memory_objects(objs1)\n    clean_obj_list2 = _process_in_memory_objects(objs2)\n    obj_count_1 = _get_object_count_by_type(clean_obj_list1)\n    obj_count_2 = _get_object_count_by_type(clean_obj_list2)\n    return obj_count_1 - obj_count_2"}
{"prompt": "def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"", "output": "\n        return self.api_options()\n\n    def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"\n        return self.api_options()\n\n    def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"\n        return self.api_options()\n\n    def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"\n        return self.api_options()\n\n    def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number\n        \"\"\"\n        return self.api_options()\n\n    def api_options(self):\n        \"\"\"\n        current spelling settings\n        :return: api options as number", "solution": "\n        options = 0\n        if self._ignore_uppercase:\n            options |= 1\n        if self._ignore_digits:\n            options |= 2\n        if self._ignore_urls:\n            options |= 4\n        if self._find_repeat_words:\n            options |= 8\n        if self._ignore_latin:\n            options |= 16\n        if self._flag_latin:\n            options |= 128\n        if self._by_words:\n            options |= 256\n        if self._ignore_capitalization:\n            options |= 512\n        if self._ignore_roman_numerals:\n            options |= 2048\n        return options"}
{"prompt": "def get_all_intervals(self):\n        \"\"\"Returns the true list of intervals including the empty intervals.\"\"\"", "output": "\n        intervals = []\n        for interval in self.intervals:\n            if interval.is_empty:\n                intervals.append(interval)\n        return intervals\n\n    def get_interval(self, interval_number):\n        \"\"\"Returns the interval with the given number.\"\"\"\n        interval = self.intervals[interval_number]\n        return interval\n\n    def get_interval_number(self, interval):\n        \"\"\"Returns the interval number of the given interval.\"\"\"\n        return self.intervals.index(interval)\n\n    def get_interval_numbers(self):\n        \"\"\"Returns the list of interval numbers.\"\"\"\n        return self.intervals\n\n    def get_interval_numbers_sorted(self):\n        \"\"\"Returns the list of interval numbers in sorted order.\"\"\"\n        return self.intervals\n\n    def get_interval_numbers_sorted_by_start(self):\n        \"\"\"Returns the list of interval numbers in ", "solution": "\n        ints = sorted(self.get_intervals(True))\n        if self.tier_type == 'IntervalTier':\n            if not ints:\n                ints.append((self.xmin, self.xmax, ''))\n            else:\n                if ints[0][0] > self.xmin:\n                    ints.insert(0, (self.xmin, ints[0][0], ''))\n                if ints[-1][1] < self.xmax:\n                    ints.append((ints[-1][1], self.xmax, ''))\n                p = ints[-1]\n                for index, i in reversed(list(enumerate(ints[:-1], 1))):\n                    if p[0] - i[1] != 0:\n                        ints.insert(index, (i[1], p[0], ''))\n                    p = i\n        return ints"}
{"prompt": "def info_update(self, obj_id, data):\n\t\t\"\"\"Update metadata with of a specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"", "output": "\n\t\t\n\t\tobj = self.objs[obj_id]\n\t\t\n\t\tif obj.type not in ['image', 'video', 'audio', 'document']:\n\t\t\traise ValueError(\"Invalid object type: %s\" % obj.type)\n\t\t\n\t\tobj.update(data)\n\t\t\n\tdef info_get(self, obj_id):\n\t\t\"\"\"Get metadata of specified object.\n\t\t\tSee http://msdn.microsoft.com/en-us/library/live/hh243648.aspx\n\t\t\t\tfor the list of RW keys for each object type.\"\"\"\n\t\t\n\t\tobj = self.objs[obj_id]\n\t\t\n\t\tif obj.type not in ['image', 'video', 'audio', 'document']:\n\t\t\traise ValueError(\"Invalid object type: %s\" % obj.type)\n\t\t\n\t\treturn ", "solution": "\n\t\treturn self(obj_id, method='put', data=data, auth_header=True)"}
{"prompt": "def hide_error_message(self):\n        \"\"\"\n        This function hides the error message when all values are correct.\n        \"\"\"", "output": "\n        self.error = False\n        self.error_list = []\n        self.error_list_text = []\n        self.error_list_text_text = []\n        self.error_list_text_text2 = []\n        self.error_list_text_text3 = []\n        self.error_list_text_text4 = []\n        self.error_list_text_text5 = []\n        self.error_list_text_text6 = []\n        self.error_list_text_text7 = []\n        self.error_list_text_text8 = []\n        self.error_list_text_text9 = []\n        self.error_list_text_text10 = []\n        self.error_list_text_text11 = []\n        self.error_list_text_text12 = []\n        self.error_list_text_text13 = []\n        self.error_list_text_text14 = []\n        self.error_", "solution": "\n        self.ui.error_label.setScaledContents(False)  # Warning image hiden.\n        self.ui.error_text_label.hide()"}
{"prompt": "def conditional_entropy(X, Y, base=2):\n    \"\"\"Calculates the conditional entropy, H(X|Y), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    Y: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    base: integer (default: 2)\n        The base in which to calculate conditional entropy\n\n    Returns\n    ----------\n    conditional_entropy: float\n        The conditional entropy calculated according to the equation H(X|Y) = H(X,Y) - H(Y)\n\n    \"\"\"", "output": "\n    conditional_entropy = base * np.log(2 * np.pi)\n    conditional_entropy -= np.log(np.sum(np.exp(- X * np.log(X)) +\n                                        np.exp(- Y * np.log(Y))))\n    return conditional_entropy\n\n\ndef conditional_entropy_from_logits(X, Y):\n    \"\"\"Calculates the conditional entropy from logits (i.e. the conditional entropy\n    is the negative log likelihood), in the given base\n\n    Parameters\n    ----------\n    X: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n    Y: array-like (# samples)\n        An array of values for which to compute the conditional entropy\n\n    Returns\n    ----------\n    condition", "solution": "\n    return joint_entropy(X, Y, base=base) - entropy(Y, base=base)"}
{"prompt": "def update_policy(self, id, policy):\n        \"\"\" Create policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - name\n                - policy\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"", "output": "\n        body = {\n            \"name\": policy.name,\n            \"policy\": policy.policy\n        }\n        body = json.dumps(body)\n        req = requests.put(\n            url=self._build_url(id),\n            data=body,\n            headers=self._build_headers()\n        )\n        return self._handle_response(req, 201)\n\n    def update_policy(self, id, policy):\n        \"\"\" Update policy.\n\n            https://www.nomadproject.io/api/acl-policies.html\n\n            arguments:\n                - name\n                - policy\n            returns: request.Response\n\n            raises:\n              - nomad.api.exceptions.BaseNomadException\n              - nomad.api.exceptions.URLNotFoundNomadException\n        \"\"\"\n        body = {", "solution": "\n        return self.request(\"policy\", id, json=policy, method=\"post\")"}
{"prompt": "def mark(self, value=1):\n        \"\"\"Record an event with the derive.\n\n        :param value: counter value to record\n        \"\"\"", "output": "\n        self._marks[self._counter] = value\n        self._counter += 1\n\n    def marks(self):\n        \"\"\"Get the current counter value for each event.\"\"\"\n        return self._marks\n\n    def marks_sum(self):\n        \"\"\"Get the sum of all marks.\"\"\"\n        return sum(self._marks.values())\n\n    def marks_mean(self):\n        \"\"\"Get the mean of all marks.\"\"\"\n        return sum(self._marks.values()) / len(self._marks)\n\n    def marks_min(self):\n        \"\"\"Get the min of all marks.\"\"\"\n        return min(self._marks.values())\n\n    def marks_max(self):\n        \"\"\"Get the max of all marks.\"\"\"\n        return max(self._marks.values())\n\n    def marks_std(self):\n        \"\"\"Get the std of all marks.\"\"\"\n        return math.", "solution": "\n        last = self.last.get_and_set(value)\n        if last <= value:\n            value = value - last\n        super(Derive, self).mark(value)"}
{"prompt": "def __fetch_pull_requests(self, from_date, to_date):\n        \"\"\"Fetch the pull requests\"\"\"", "output": "\n        from_date = from_date.strftime(\"%Y-%m-%d\")\n        to_date = to_date.strftime(\"%Y-%m-%d\")\n        query = {\n            \"from\": from_date,\n            \"to\": to_date,\n            \"state\": \"open\",\n            \"sort\": \"created\",\n            \"direction\": \"desc\",\n            \"limit\": 100,\n        }\n        page = self.api.fetch(\n            endpoint=\"pull_requests\",\n            method=\"GET\",\n            params=query,\n        )\n        return page\n\n    def get_pull_request_by_id(self, pull_request_id):\n        \"\"\"Fetch the pull request by id\"\"\"\n        query = {\n            \"id\": pull_request_id,\n            \"state\": \"open\",\n            \"sort\": \"", "solution": "\n\n        raw_pulls = self.client.pulls(from_date=from_date)\n        for raw_pull in raw_pulls:\n            pull = json.loads(raw_pull)\n\n            if str_to_datetime(pull['updated_at']) > to_date:\n                return\n\n            self.__init_extra_pull_fields(pull)\n            for field in TARGET_PULL_FIELDS:\n\n                if not pull[field]:\n                    continue\n\n                if field == 'user':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'merged_by':\n                    pull[field + '_data'] = self.__get_user(pull[field]['login'])\n                elif field == 'review_comments':\n                    pull[field + '_data'] = self.__get_pull_review_comments(pull['number'])\n                elif field == 'requested_reviewers':\n                    pull[field + '_data'] = self.__get_pull_requested_reviewers(pull['number'])\n                elif field == 'commits':\n                    pull[field + '_data'] = self.__get_pull_commits(pull['number'])\n\n            yield pull"}
{"prompt": "def connect(src, *destinations, exclude: set=None, fit=False):\n    \"\"\"\n    Connect src (signals/interfaces/values) to all destinations\n\n    :param exclude: interfaces on any level on src or destinations\n        which should be excluded from connection process\n    :param fit: auto fit source width to destination width\n    \"\"\"", "output": "\n    src = src.copy()\n    dst = src.copy()\n    dst['width'] = src.width\n    dst['height'] = src.height\n    dst['fit'] = fit\n    dst['exclude'] = exclude\n    dst['width'] = dst['width'] or src.width\n    dst['height'] = dst['height'] or src.height\n    dst['fit'] = dst['fit'] or fit\n    dst['exclude'] = dst['exclude'] or exclude\n    dst['width'] = dst['width'] or src.width\n    dst['height'] = dst['height'] or src.height\n    dst['fit'] = dst['fit'] or ", "solution": "\n    assignemnts = []\n\n    if isinstance(src, HObjList):\n        for dst in destinations:\n            assert len(src) == len(dst), (src, dst)\n        _destinations = [iter(d) for d in destinations]\n        for _src in src:\n            dsts = [next(d) for d in _destinations]\n            assignemnts.append(connect(_src, *dsts, exclude=exclude, fit=fit))\n    else:\n        for dst in destinations:\n            assignemnts.append(_connect(src, dst, exclude, fit))\n\n    return assignemnts"}
{"prompt": "def addLogicalInterfaceToDeviceType(self, typeId, logicalInterfaceId):\n        \"\"\"\n        Adds a logical interface to a device type.\n        Parameters:\n            - typeId (string) - the device type\n            - logicalInterfaceId (string) - the id returned by the platform on creation of the logical interface\n            - description (string) - optional (not used)\n        Throws APIException on failure.\n        \"\"\"", "output": "\n        uri = '/deviceType/%s/logicalInterface/%s' % (typeId, logicalInterfaceId)\n        return self._post(uri, {})\n\n    def addLogicalInterfaceToDeviceTypes(self, typeIds, logicalInterfaceIds):\n        \"\"\"\n        Adds a set of logical interfaces to a device type.\n        Parameters:\n            - typeIds (array of string) - the device type\n            - logicalInterfaceIds (array of string) - the id returned by the platform on creation of the logical interface\n            - description (string) - optional (not used)\n        Throws APIException on failure.\n        \"\"\"\n        uri = '/deviceType/%s/logicalInterface' % ','.join(typeIds)\n        uri += '/", "solution": "\n        req = ApiClient.allDeviceTypeLogicalInterfacesUrl % (self.host, \"/draft\", typeId)\n        body = {\"id\" : logicalInterfaceId}\n#       body = {\"name\" : \"required but not used!!!\", \"id\" : logicalInterfaceId, \"schemaId\" : schemaId}\n#       if description:\n#           body[\"description\"] = description\n        resp = requests.post(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"}, data=json.dumps(body),\n                        verify=self.verify)\n        if resp.status_code == 201:\n            self.logger.debug(\"Logical interface added to a device type\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error adding logical interface to a device type\", resp)\n        return resp.json()"}
{"prompt": "def repository(self, owner, repository):\n        \"\"\"Fetch information about a repository.\"\"\"", "output": "\n        endpoint = '/repos/{}/{}'.format(owner, repository)\n        resp = self.get(endpoint)\n        return resp\n\n    def search(self, owner, repository, query):\n        \"\"\"Search for repositories.\"\"\"\n        endpoint = '/repos/{}/{}/search'.format(owner, repository)\n        resp = self.get(endpoint, params={'q': query})\n        return resp\n\n    def search_repositories(self, owner, repository, query):\n        \"\"\"Search for repositories.\"\"\"\n        endpoint = '/repos/{}/{}/search'.format(owner, repository)\n        resp = self.get(endpoint, params={'q': query})\n        return resp\n\n    def search_users(self, owner, query):\n        \"\"\"Search for user", "solution": "\n\n        url = urijoin(self.base_url, self.RREPOSITORY, owner, repository)\n\n        logger.debug(\"DockerHub client requests: %s\", url)\n\n        response = self.fetch(url)\n\n        return response.text"}
{"prompt": "def target_address(self):\n        \"\"\"Return the authorative target of the link.\"\"\"", "output": "\n        return self.link.get('address')\n\n    def target_type(self):\n        \"\"\"Return the type of the link.\"\"\"\n        return self.link.get('type')\n\n    def target_id(self):\n        \"\"\"Return the ID of the link.\"\"\"\n        return self.link.get('id')\n\n    def supported_features(self):\n        \"\"\"Flag supported features.\"\"\"\n        return self.link.get('type')\n\n    def name(self):\n        \"\"\"Return the name of the link.\"\"\"\n        return self.link.get('name')\n\n    def capabilities(self):\n        \"\"\"Flag supported features.\"\"\"\n        return self.link.get('type')\n\n    def to_dict(self):\n        \"\"\"Return parsed link details.\"\"\"\n        details = self.link.get('details')\n        if details:\n            details = details.strip()\n        return details\n\n    def to_dicts(self):\n        \"\"\"Return parsed link details.\"\"\"\n", "solution": "\n        # If link is a receiver, target is determined by the local\n        # value, else use the remote.\n        if self._pn_link.is_receiver:\n            return self._pn_link.target.address\n        else:\n            return self._pn_link.remote_target.address"}
{"prompt": "def recv_unsub(self, id_=None):\n        \"\"\"DDP unsub handler.\"\"\"", "output": "\n        if id_ is None:\n            id_ = self.id_\n        self.id_ = id_\n        self.recv_unsub_ack()\n\n    def recv_unsub_ack(self):\n        \"\"\"DDP unsub ack handler.\"\"\"\n        self.recv_unsub_resp()\n\n    def recv_unsub_resp(self):\n        \"\"\"DDP unsub response handler.\"\"\"\n        self.recv_unsub_resp_ack()\n\n    def recv_unsub_resp_ack(self):\n        \"\"\"DDP unsub response ack handler.\"\"\"\n        self.recv_unsub_resp_nack()\n\n    def recv_unsub_resp_nack(self):\n        \"\"\"DDP unsub response nack handler.\"\"\"\n        self.recv_unsub", "solution": "\n        if id_:\n            self.api.unsub(id_)\n        else:\n            self.reply('nosub')"}
{"prompt": "def score(infile, outfile, classifier, xgb_autotune, apply_weights, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test):\n    \"\"\"\n    Conduct semi-supervised learning and error-rate estimation for MS1, MS2 and transition-level data. \n    \"\"\"", "output": "\n    Evaluate an XGB model using cross-validation.\n\n    Parameters\n    ----------\n    infile : str\n        Path to the input file.\n    outfile : str\n        Path to the output file.\n    classifier : xgboost.XGBClassifier\n        Model to evaluate.\n    xgb_autotune : bool\n        Whether to use autotuning or not.\n    apply_weights : bool\n        Whether to use weights or not.\n    xeval_fraction : float\n        Fraction of data to use for cross-validation.\n    xeval_num_iter : int\n        Number of cross-validation iterations.\n    ss_initial_fdr : float\n        Initial FDR value.\n    ss_iteration_fd", "solution": "\n\n    if outfile is None:\n        outfile = infile\n    else:\n        outfile = outfile\n\n    # Prepare XGBoost-specific parameters\n    xgb_hyperparams = {'autotune': xgb_autotune, 'autotune_num_rounds': 10, 'num_boost_round': 100, 'early_stopping_rounds': 10, 'test_size': 0.33}\n\n    xgb_params = {'eta': 0.3, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': 1, 'alpha': 0, 'scale_pos_weight': 1, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    xgb_params_space = {'eta': hp.uniform('eta', 0.0, 0.3), 'gamma': hp.uniform('gamma', 0.0, 0.5), 'max_depth': hp.quniform('max_depth', 2, 8, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': 1, 'colsample_bytree': 1, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'lambda': hp.uniform('lambda', 0.0, 1.0), 'alpha': hp.uniform('alpha', 0.0, 1.0), 'scale_pos_weight': 1.0, 'silent': 1, 'objective': 'binary:logitraw', 'nthread': 1, 'eval_metric': 'auc'}\n\n    if not apply_weights:\n        PyProphetLearner(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test).run()\n    else:\n        PyProphetWeightApplier(infile, outfile, classifier, xgb_hyperparams, xgb_params, xgb_params_space, xeval_fraction, xeval_num_iter, ss_initial_fdr, ss_iteration_fdr, ss_num_iter, ss_main_score, group_id, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps, level, ipf_max_peakgroup_rank, ipf_max_peakgroup_pep, ipf_max_transition_isotope_overlap, ipf_min_transition_sn, tric_chromprob, threads, test, apply_weights).run()"}
{"prompt": "def cleanup(controller, engines):\n    \"\"\"Cleanup routine to shut down all subprocesses we opened.\"\"\"", "output": "\n    for engine in engines:\n        engine.shutdown()\n\n\ndef start_engines(engines, controller, options):\n    \"\"\"Start all engines.\"\"\"\n    cleanup(engines, engines)\n    engine = engines[0]\n    engine.start()\n    controller.add_engine(engine)\n\n\ndef stop_engines(engines, controller, options):\n    \"\"\"Stop all engines.\"\"\"\n    cleanup(engines, engines)\n    engine = engines[0]\n    engine.stop()\n    controller.remove_engine(engine)\n\n\ndef start_engine(engine, controller, options):\n    \"\"\"Start a engine.\"\"\"\n    engine.start()\n    controller.add_engine(engine)\n\n\ndef stop_engine", "solution": "\n    import signal, time\n    \n    print('Starting cleanup')\n    print('Stopping engines...')\n    for e in engines:\n        e.send_signal(signal.SIGINT)\n    print('Stopping controller...')\n    # so it can shut down its queues\n    controller.send_signal(signal.SIGINT)\n    time.sleep(0.1)\n    print('Killing controller...')\n    controller.kill()\n    print('Cleanup done')"}
{"prompt": "def reset(self):\n        \"\"\"Reset the input buffer and associated state.\"\"\"", "output": "\n        self.buffer = []\n        self.state = None\n\n    def append(self, item):\n        \"\"\"Append a chunk of text to the input buffer.\"\"\"\n        self.buffer.append(item)\n\n    def flush(self):\n        \"\"\"Flush the input buffer.\"\"\"\n        self.buffer.extend(self.state)\n\n    def peek(self, n=1):\n        \"\"\"Return the next n characters of the input buffer.\"\"\"\n        return self.buffer[:n]\n\n    def read(self, n=1):\n        \"\"\"Read up to n characters from the input buffer.\"\"\"\n        return self.buffer[:n]\n\n    def readline(self, length=None):\n        \"\"\"Read a line from the input buffer.\"\"\"\n        if length is None:\n            length = self.getsize()\n        assert length >= 0\n        assert length <= len(self.buffer)\n        chunk = self.buffer[:length", "solution": "\n        super(IPythonInputSplitter, self).reset()\n        self._buffer_raw[:] = []\n        self.source_raw = ''\n        self.cell_magic_parts = []\n        self.processing_cell_magic = False"}
{"prompt": "def for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"", "output": "\n    type_module.register_type(type_name, func)\n\n\ndef for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"\n    type_module.register_type(type_name, func)\n\n\ndef for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type object itself.\n    \"\"\"\n    type_module.register_type(type_name, func)\n\n\ndef for_type_by_name(type_module, type_name, func):\n    \"\"\"\n    Add a pretty printer for a type specified by the module and name of a type\n    rather than the type ", "solution": "\n    key = (type_module, type_name)\n    oldfunc = _deferred_type_pprinters.get(key, None)\n    if func is not None:\n        # To support easy restoration of old pprinters, we need to ignore Nones.\n        _deferred_type_pprinters[key] = func\n    return oldfunc"}
{"prompt": "def domain(self, domain=None, last_domain=None):\n        \"\"\"\n        Manage the case that we want to test only a domain.\n\n        :param domain: The domain or IP to test.\n        :type domain: str\n\n        :param last_domain:\n            The last domain to test if we are testing a file.\n        :type last_domain: str\n\n        :param return_status: Tell us if we need to return the status.\n        :type return_status: bool\n        \"\"\"", "output": "\n        domain = self.normalize_domain(domain)\n        if last_domain is not None:\n            last_domain = self.normalize_domain(last_domain)\n        else:\n            last_domain = None\n        status = self.test_domain(domain, last_domain)\n        if return_status:\n            return status\n        else:\n            return status and self.test_file(domain)\n\n    def normalize_domain(self, domain):\n        \"\"\"\n        Normalize the domain to lower case.\n\n        :param domain: The domain to normalize.\n        :type domain: str\n\n        :return: The normalized domain.\n        :rtype: str\n        \"\"\"\n        domain = domain.lower()\n        if domain.startswith('.'):\n            domain = domain[1:]\n        return domain\n\n    def normalize_filename(self, filename):\n        \"\"\"\n        Normalize the filename to lower case.\n\n", "solution": "\n\n        # We print the header.\n        self._print_header()\n\n        if domain:\n            # A domain is given.\n\n            # We format and set the domain we are testing and treating.\n            PyFunceble.INTERN[\"to_test\"] = self._format_domain(domain)\n        else:\n            # A domain is not given.\n\n            # We set the domain we are testing and treating to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # The domain is given (Not None).\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # We test and get the status of the domain.\n                status, _ = self.status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_domain, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the domain and the status.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the tested domain and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None"}
{"prompt": "def varimp(self, use_pandas=False):\n        \"\"\"\n        Pretty print the variable importances, or return them in a list/pandas DataFrame.\n\n        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n\n        :returns: A dictionary of lists or Pandas DataFrame instances.\n        \"\"\"", "output": "\n        imp = dict()\n        for var in self.variables:\n            imp[var] = list()\n            for imp_var in self.variables[var]:\n                imp[var].append(imp_var.get_imp(use_pandas=use_pandas))\n        return imp\n\n    def print_variables(self, use_pandas=False):\n        \"\"\"\n        Pretty print the variable importances, or return them in a list/pandas DataFrame.\n\n        :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n\n        :returns: A dictionary of lists or Pandas DataFrame instances.\n        \"\"\"\n        imp = dict()\n        for var in self.variables:\n            imp[var] = list()", "solution": "\n        return {model.model_id: model.varimp(use_pandas) for model in self.models}"}
{"prompt": "def statementsAreSame(statements: List[HdlStatement]) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"", "output": "\n    return all(statementsAreSame(statement) for statement in statements)\n\n\ndef statementsAreSame(statement: HdlStatement) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"\n    return statement.name == statement.name and statement.type == statement.type\n\n\ndef statementsAreSame(statement: HdlStatement, other: HdlStatement) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"\n    return statement.name == other.name and statement.type == other.type\n\n\ndef statementsAreSame(statement: HdlStatement, other: HdlStatement, ignoreCase: bool = True) -> bool:\n    \"\"\"\n    :return: True if all statements are same\n    \"\"\"", "solution": "\n    iterator = iter(statements)\n    try:\n        first = next(iterator)\n    except StopIteration:\n        return True\n\n    return all(first.isSame(rest) for rest in iterator)"}
{"prompt": "def get_meta_netnode():\n    \"\"\"\n    Get the netnode used to store settings metadata in the current IDB.\n    Note that this implicitly uses the open IDB via the idc iterface.\n    \"\"\"", "output": "\n    return idb.netnode('settings')\n\ndef get_meta_settings():\n    \"\"\"\n    Get the settings metadata stored in the current IDB.\n    \"\"\"\n    return idb.get_settings()\n\ndef get_meta_setting(name):\n    \"\"\"\n    Get the value of a setting in the settings metadata stored in the current IDB.\n    \"\"\"\n    return idb.get_setting(name)\n\ndef set_meta_setting(name, value):\n    \"\"\"\n    Set the value of a setting in the settings metadata stored in the current IDB.\n    \"\"\"\n    idb.set_setting(name, value)\n\ndef del_meta_setting(name):\n    \"\"\"\n    Delete a setting in the settings metadata stored in the current IDB.\n    \"\"\"\n    idb.del_setting(name)\n\ndef del_meta_net", "solution": "\n    node_name = \"$ {org:s}.{application:s}\".format(\n        org=IDA_SETTINGS_ORGANIZATION,\n        application=IDA_SETTINGS_APPLICATION)\n    return netnode.Netnode(node_name)"}
{"prompt": "def setPin(self, old_pin, new_pin):\n        \"\"\"\n        C_SetPIN\n\n        :param old_pin: old PIN\n        :param new_pin: new PIN\n        \"\"\"", "output": "\n        self._pins[old_pin] = new_pin\n\n    def GetPin(self, old_pin):\n        \"\"\"\n        C_GetPIN\n\n        :param old_pin: old PIN\n        \"\"\"\n        return self._pins[old_pin]\n\n    def GetPins(self):\n        \"\"\"\n        C_GetPINS\n\n        \"\"\"\n        return self._pins\n\n    def GetPinsAsString(self):\n        \"\"\"\n        C_GetPINSAsString\n\n        \"\"\"\n        return \",\".join([\"%d:%d\" % (old_pin, self._pins[old_pin]) for old_pin in self._pins])\n\n    def GetPinsAsStrings(self):\n        \"\"\"\n        C_GetPINSAsStrings\n\n        \"\"\"\n        return \",\".join([\"", "solution": "\n        old_pin1 = ckbytelist(old_pin)\n        new_pin1 = ckbytelist(new_pin)\n        rv = self.lib.C_SetPIN(self.session, old_pin1, new_pin1)\n        if rv != CKR_OK:\n            raise PyKCS11Error(rv)"}
{"prompt": "def divide(df, new_column, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"", "output": "\n    warnings.warn(\n        \"`divide` is deprecated. Use `formula` instead.\",\n        FutureWarning,\n    )\n    return df.divide(new_column, column_1, column_2)\n\n\ndef divide_many(df, new_columns, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"\n    warnings.warn(\n        \"`divide_many` is deprecated. Use `formula` instead.\",\n        FutureWarning,\n    )\n    return df.divide_many(new_columns, column_1, column_2)\n\n\ndef divide_rows(df, new_columns, column_1, column_2):\n    \"\"\"\n    DEPRECATED -  use `formula` instead\n    \"\"\"\n", "solution": "\n    return _basic_math_operation(df, new_column, column_1, column_2, op='truediv')"}
{"prompt": "def _setup_template_file(self, template_file_path):\n        \"\"\" Setup self.template\n\n        Parameters\n        ----------\n        template_file_path: str\n            Document template file path.\n        \"\"\"", "output": "\n        self.template = Template(filename=template_file_path)\n\n    def _setup_template_context(self, template_context):\n        \"\"\" Setup self.template_context\n\n        Parameters\n        ----------\n        template_context: dict\n            Template context dictionary.\n        \"\"\"\n        self.template_context = template_context\n\n    def _setup_template_contexts(self, template_contexts):\n        \"\"\" Setup self.template_contexts\n\n        Parameters\n        ----------\n        template_contexts: list\n            List of template context dictionaries.\n        \"\"\"\n        self.template_contexts = template_contexts\n\n    def _setup_template_context(self, template_context, template_context):\n        \"\"\" Setup self.template_context\n\n        Parameters\n        ----------\n        template_context: dict\n            Template context dictionary.\n        template_context: dict\n            Template context dictionary.", "solution": "\n        try:\n            template_file = template_file_path\n            template_env = get_environment_for(template_file_path)\n            template = template_env.get_template(os.path.basename(template_file))\n        except:\n            raise\n        else:\n            self._template_file = template_file\n            self._template_env = template_env\n            self.template = template"}
{"prompt": "def pxconfig(self, line):\n        \"\"\"configure default targets/blocking for %px magics\"\"\"", "output": "\n        line = line.replace('%px', 'pxconfig')\n        line = line.replace('%pxconfig', 'pxconfig')\n        return line\n\n    def pxconfigblock(self, line):\n        \"\"\"configure default targets/blocking for %pxconfig magics\"\"\"\n        line = line.replace('%pxconfig', 'pxconfig')\n        line = line.replace('%px', 'pxconfig')\n        return line\n\n    def pxconfigblocks(self, line):\n        \"\"\"configure default targets/blocking for %pxconfig magics\"\"\"\n        line = line.replace('%pxconfig', 'pxconfig')\n        line = line.replace('%px', 'pxconfig')\n        return line\n\n    def pxconfigblocks_all(self, line):\n        \"\"\"configure default targets/block", "solution": "\n        args = magic_arguments.parse_argstring(self.pxconfig, line)\n        if args.targets:\n            self.view.targets = self._eval_target_str(args.targets)\n        if args.block is not None:\n            self.view.block = args.block\n        if args.set_verbose is not None:\n            self.verbose = args.set_verbose"}
{"prompt": "def l(*members, meta=None) -> List:\n    \"\"\"Creates a new list from members.\"\"\"", "output": "\n    members = iter(members)\n    if meta is None:\n        meta = {}\n    meta.setdefault('type', list)\n    meta.setdefault('ordered', False)\n    meta.setdefault('default', ())\n    meta.setdefault('help', 'List members.')\n    meta.setdefault('formatter_class', argparse.ArgumentDefaultsHelpFormatter)\n    meta.setdefault('conflict_handler','resolve')\n    meta.setdefault('add_help', False)\n    meta.setdefault('help', None)\n    meta.setdefault('names', None)\n    meta.setdefault('types', None)\n    meta.setdefault('help', None)\n    meta.setdefault('type', list)\n    meta.setdefault('conf", "solution": "\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )"}
{"prompt": "def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n    \"\"\"wrapper\"\"\"", "output": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "solution": "\n    #'http://video.fc2.com/en/content/20151021bTVKnbEw'\n    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'\n    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'\n    hostname = urlparse(url).hostname\n    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):\n        return False\n    upid = match1(url, r'.+/content/(\\w+)')\n\n    fc2video_download_by_upid(upid, output_dir, merge, info_only)"}
{"prompt": "def tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"", "output": "\n    binary = bin(deci_num).rjust(len, '0')\n    return binary\n\n\ndef tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"\n    binary = bin(deci_num).rjust(len, '0')\n    return binary\n\n\ndef tobin(deci_num, len=32):\n    \"\"\"\n    Given a decimal number, returns a string bitfield of length = len\n    Example: given deci_num = 1 and len = 10, it return 0000000001\n    \"\"\"\n    binary = bin(deci_num).rjust(len", "solution": "\n    bitstr = \"\".join(map(lambda y: str((deci_num >> y) & 1), range(len - 1, -1, -1)))\n    return bitstr"}
{"prompt": "def export_mt_variants(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of lines to include in the document\n    \"\"\"", "output": "\n    document_lines = []\n    document_lines.append(\n        f'Sample: {sample_id}\\n\\n'\n        f'Variants:\\n'\n    )\n    for variant in variants:\n        document_lines.append(\n            f'\\t{variant[\"position\"]} {variant[\"variant\"]}\\n'\n        )\n    return document_lines\n\n\ndef export_mt_variants_as_excel(variants, sample_id):\n    \"\"\"Export mitochondrial variants for a case to create a MT excel report\n\n    Args:\n        variants(list): all MT variants for a case, sorted by position\n        sample_id(str) : the id of a sample within the case\n\n    Returns:\n        document_lines(list): list of ", "solution": "\n    document_lines = []\n    for variant in variants:\n        line = []\n        position = variant.get('position')\n        change = '>'.join([variant.get('reference'),variant.get('alternative')])\n        line.append(position)\n        line.append(change)\n        line.append(str(position)+change)\n        genes = []\n        prot_effect = []\n        for gene in variant.get('genes'):\n            genes.append(gene.get('hgnc_symbol',''))\n            for transcript in gene.get('transcripts'):\n                if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):\n                    prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))\n        line.append(','.join(prot_effect))\n        line.append(','.join(genes))\n        ref_ad = ''\n        alt_ad = ''\n        for sample in variant['samples']:\n            if sample.get('sample_id') == sample_id:\n                ref_ad = sample['allele_depths'][0]\n                alt_ad = sample['allele_depths'][1]\n        line.append(ref_ad)\n        line.append(alt_ad)\n        document_lines.append(line)\n    return document_lines"}
{"prompt": "def write_sky_params_to_file(self):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"", "output": "\n        sky_params = self.sky_params\n        sky_params.write_sky_params_to_file()\n\n    def write_sky_params_to_file(self, file_name):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"\n        sky_params = self.sky_params\n        sky_params.write_sky_params_to_file(file_name)\n\n    def write_sky_params_to_file(self, file_name):\n        \"\"\"Writes the params to file that skytool_Free needs to generate the sky radiance distribution.\"\"\"\n        sky_params = self.sky_params\n        sky_params.write_sky_params_to_file(file_name)\n\n    def write_", "solution": "\n\n        inp_file = self.sky_file + '_params.txt'\n        lg.info('Writing Inputs to file : ' + inp_file)\n\n        f = open(inp_file, 'w')\n\n        f.write('verbose= ' + str(self.verbose) + '\\n')\n        f.write('band_count= ' + str(self.num_bands) + '\\n')\n        f.write('band_centres_data= ')\n        f.write(\",\".join([str(wave) for wave in self.wavelengths]) + '\\n')\n        f.write('partition= ' + self.partition + '\\n')\n        f.write('vn= ' + str(self.vn) + '\\n')\n        f.write('hn= ' + str(self.hn) + '\\n')\n        f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.write('theta_points= ')\n        f.write(\",\".join([str(theta) for theta in self.theta_points]) + '\\n')\n        f.write('type= ' + self.sky_type + '\\n')\n        f.write('azimuth= ' + str(self.sky_azimuth) + '\\n')\n        f.write('zenith= ' + str(self.sky_zenith) + '\\n')\n        f.write('sky_save_fp= ' + inp_file.strip('_params.txt') + '\\n')\n        f.write('sky_image_save_fp= ' + self.sky_file + '.ppm' + '\\n')\n        f.write('sky_image_size= 256' + '\\n')\n        if self.sky_type == 'hlideal':\n            f.write('C= ' + str(self.sky_c) + '\\n')\n            f.write('rdif= ' + str(self.sky_r_dif) + '\\n')\n        f.flush()\n        f.close()"}
{"prompt": "def defrag(filt, threshold=3, mode='include'):\n    \"\"\"\n    'Defragment' a filter.\n\n    Parameters\n    ----------\n    filt : boolean array\n        A filter\n    threshold : int\n        Consecutive values equal to or below this threshold\n        length are considered fragments, and will be removed.\n    mode : str\n        Wheter to change False fragments to True ('include')\n        or True fragments to False ('exclude')\n\n    Returns\n    -------\n    defragmented filter : boolean array\n    \"\"\"", "output": "\n    filt = filt[filt > threshold]\n    if mode == 'include':\n        filt[filt > threshold] = False\n    elif mode == 'exclude':\n        filt[filt > threshold] = True\n    return filt\n\n\ndef deconvolve(filt, convolve, mode='include'):\n    \"\"\"\n    'Deconvolve' a filter.\n\n    Parameters\n    ----------\n    filt : boolean array\n        A filter\n    convolve : function\n        Convolution function\n    mode : str\n        Wheter to change False fragments to True ('include')\n        or True fragments to False ('exclude')\n\n    Returns\n    -------\n", "solution": "\n    if bool_2_indices(filt) is None:\n        return filt\n\n    if mode == 'include':\n        inds = bool_2_indices(~filt) + 1\n        rep = True\n    if mode == 'exclude':\n        inds = bool_2_indices(filt) + 1\n        rep = False\n\n    rem = (np.diff(inds) <= threshold)[:, 0]\n\n    cfilt = filt.copy()\n    if any(rem):\n        for lo, hi in inds[rem]:\n            cfilt[lo:hi] = rep\n\n    return cfilt"}
{"prompt": "def _build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku):\n    \"\"\"Build and return Sailthru purchase item object\"\"\"", "output": "\n    item_data = {\n        'course_id': course_id,\n        'course_url': course_url,\n        'cost_in_cents': cost_in_cents,\n       'mode': mode,\n        'course_data': course_data,\n       'sku': sku,\n    }\n    return PurchaseItem(**item_data)\n\n\ndef _build_purchase_item_list(course_id, course_url, cost_in_cents, mode, course_data, sku, items):\n    \"\"\"Build and return list of Sailthru purchase item objects\"\"\"\n    item_list = []\n    for item_data in items:\n        item_list.append(_build_purchase_item(course_id, course_url, cost_in_cents, mode, course_data, sku", "solution": "\n\n    # build item description\n    item = {\n        'id': \"{}-{}\".format(course_id, mode),\n        'url': course_url,\n        'price': cost_in_cents,\n        'qty': 1,\n    }\n\n    # get title from course info if we don't already have it from Sailthru\n    if 'title' in course_data:\n        item['title'] = course_data['title']\n    else:\n        # can't find, just invent title\n        item['title'] = 'Course {} mode: {}'.format(course_id, mode)\n\n    if 'tags' in course_data:\n        item['tags'] = course_data['tags']\n\n    # add vars to item\n    item['vars'] = dict(course_data.get('vars', {}), mode=mode, course_run_id=course_id)\n\n    item['vars']['purchase_sku'] = sku\n\n    return item"}
{"prompt": "def _kl_normal_normal(n_a, n_b, name=None):\n  \"\"\"Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n  Args:\n    n_a: instance of a Normal distribution object.\n    n_b: instance of a Normal distribution object.\n    name: (optional) Name to use for created operations.\n      default is \"kl_normal_normal\".\n\n  Returns:\n    Batchwise KL(n_a || n_b)\n  \"\"\"", "output": "\n  with ops.name_scope(name, \"kl_normal_normal\", [n_a, n_b]) as name:\n    n_a_mean = math_ops.reduce_mean(n_a.log_prob(None), name=name)\n    n_b_mean = math_ops.reduce_mean(n_b.log_prob(None), name=name)\n    n_a_log_std = math_ops.reduce_mean(n_a.log_prob(None) + n_a.log_std,\n                                      name=name)\n    n_b_log_std = math_ops.reduce_mean(n_b.log_prob(None) + n_b.log_std,\n                                      name=name)\n    kl = n_a_log_std - n_b_log_std + n_a_log_std / n_b_mean - n_b_mean / n_a_mean\n    return kl\n\n\ndef _k", "solution": "\n  with tf.name_scope(name or \"kl_normal_normal\"):\n    one = tf.constant(1, dtype=n_a.dtype)\n    two = tf.constant(2, dtype=n_a.dtype)\n    half = tf.constant(0.5, dtype=n_a.dtype)\n    s_a_squared = tf.square(n_a.scale)\n    s_b_squared = tf.square(n_b.scale)\n    ratio = s_a_squared / s_b_squared\n    return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half *\n            (ratio - one - tf.math.log(ratio)))"}
{"prompt": "def scan(self, *key_ranges, **kwargs):\n        \"\"\"Scan for FCs in the given id ranges.\n\n        :param key_ranges:\n          ``key_ranges`` should be a list of pairs of ranges. The first\n          value is the lower bound id and the second value is the\n          upper bound id. Use ``()`` in either position to leave it\n          unbounded. If no ``key_ranges`` are given, then all FCs in\n          the store are returned.\n        :param [str] feature_names:\n          A list of feature names to retrieve. When ``None``, all\n          features are retrieved. Wildcards are allowed.\n        :rtype: Iterable of ``(content_id, FC)``\n        \"\"\"", "output": "\n        ranges = key_ranges or []\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        ranges = ranges + [('*', FC.ALL)]\n        range", "solution": "\n        for hit in self._scan(*key_ranges, **kwargs):\n            yield did(hit['_id']), self.fc_from_dict(hit['_source']['fc'])"}
{"prompt": "def encode_span(self, span):\n        \"\"\"Encodes a single span to protobuf.\"\"\"", "output": "\n        span = Span(\n            start=span.start,\n            end=span.end,\n            label=span.label,\n            score=span.score,\n            sentence=span.sentence,\n            word=span.word,\n            tag=span.tag,\n            head=span.head,\n            deprel=span.deprel,\n            deprel_type=span.deprel_type,\n            head_types=span.head_types,\n            deps=span.deps,\n            ents=span.ents,\n            ents_type=span.ents_type,\n            ents_heads=span.ents_heads,\n            ents_deps=span.ents_deps,", "solution": "\n        if not protobuf.installed():\n            raise ZipkinError(\n                'protobuf encoding requires installing the protobuf\\'s extra '\n                'requirements. Use py-zipkin[protobuf] in your requirements.txt.'\n            )\n\n        pb_span = protobuf.create_protobuf_span(span)\n        return protobuf.encode_pb_list([pb_span])"}
{"prompt": "def verilogTypeOfSig(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"", "output": "\n    if signalItem.type =='reg':\n        return'reg'\n    elif signalItem.type == 'wire':\n        return 'wire'\n    else:\n        raise Exception('Unknown signal type:'+ signalItem.type)\n\n\ndef verilogTypeOfSignal(signalItem):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"\n    if signalItem.type =='reg':\n        return'reg'\n    elif signalItem.type == 'wire':\n        return 'wire'\n    else:\n        raise Exception('Unknown signal type:'+ signalItem.type)\n\n\ndef verilogTypeOfSignals(signals):\n    \"\"\"\n    Check if is register or wire\n    \"\"\"\n    for signal in signals:\n        if verilogTypeOfSignal(signal) =='reg", "solution": "\n    driver_cnt = len(signalItem.drivers)\n    if signalItem._const or driver_cnt > 1 or\\\n       arr_any(signalItem.drivers, _isEventDependentDriver):\n        return SIGNAL_TYPE.REG\n    else:\n        if driver_cnt == 1:\n            d = signalItem.drivers[0]\n            if not isinstance(d, (Assignment, PortItem)):\n                return SIGNAL_TYPE.REG\n\n        return SIGNAL_TYPE.WIRE"}
{"prompt": "def check_for_bucket(self, bucket_name):\n        \"\"\"\n        Check if bucket_name exists.\n\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"", "output": "\n        bucket_name = self._normalize_name(bucket_name)\n        bucket = self.s3.get_bucket(bucket_name)\n        if bucket is None:\n            raise NotFoundException(\n                \"Bucket {} does not exist\".format(bucket_name)\n            )\n\n        return bucket\n\n    def _normalize_name(self, name):\n        \"\"\"\n        Normalize bucket name to lowercase and remove trailing slashes.\n\n        :param name: the name of the bucket\n        :type name: str\n        \"\"\"\n        name = name.lower()\n        if name.endswith(\"/\"):\n            name = name[:-1]\n        return name\n\n    def list_buckets(self, prefix=None, delimiter=None):\n        \"\"\"\n        List all buckets.\n\n        :param prefix: prefix to filter buckets with\n        :type prefix: str\n        :param delimiter: delimiter to filter ", "solution": "\n        try:\n            self.get_conn().head_bucket(Bucket=bucket_name)\n            return True\n        except ClientError as e:\n            self.log.info(e.response[\"Error\"][\"Message\"])\n            return False"}
{"prompt": "def get_event_hub(self, hub_name):\n        \"\"\"\n        Retrieves an existing event hub.\n\n        hub_name:\n            Name of the event hub.\n        \"\"\"", "output": "\n        event_hub = self._get_event_hub_by_name(hub_name)\n        if event_hub is None:\n            raise CLIError(\n                \"Event hub '{0}' does not exist\".format(hub_name)\n            )\n        return event_hub\n\n    def get_event_hubs(self, hub_names=None, resource_group_name=None):\n        \"\"\"\n        Retrieves all event hubs.\n\n        hub_names:\n            Names of event hubs to retrieve.\n        resource_group_name:\n            Resource group of the event hubs to retrieve.\n        \"\"\"\n        event_hubs = []\n        if hub_names:\n            event_hubs = self._get_event_hubs(hub_names, resource_group_name)\n        else:\n            event_hubs = self._get_event_hubs_by_resource_group(resource_group_name)\n", "solution": "\n        _validate_not_none('hub_name', hub_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(hub_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_event_hub(response)"}
{"prompt": "def get_plot_data(self):\n        \"\"\" Generates the JSON report to plot the gene boxes\n\n        Following the convention of the reports platform, this method returns\n        a list of JSON/dict objects with the information about each entry in\n        the abricate file. The information contained in this JSON is::\n\n            {contig_id: <str>,\n             seqRange: [<int>, <int>],\n             gene: <str>,\n             accession: <str>,\n             coverage: <float>,\n             identity: <float>\n             }\n\n        Note that the `seqRange` entry contains the position in the\n        corresponding contig, not the absolute position in the whole assembly.\n\n        Returns\n        -------\n        json_dic : list\n            List of JSON/dict objects with the report data.\n        \"\"\"", "output": ": list\n            List of dictionaries containing information about the gene\n            boxes\n        \"\"\"\n        json_dic = list()\n        for contig in self.contigs:\n            json_dic.append({\n                'contig_id': contig.contig_id,\n               'seqRange': contig.seqRange,\n                'gene': contig.gene,\n                'accession': contig.accession,\n                'coverage': contig.coverage,\n                'identity': contig.identity,\n            })\n        return json_dic\n\n    def get_plot_data(self):\n        \"\"\" Generates the JSON report to plot the gene boxes\n\n        Following the convention of the reports platform, this method returns\n        a list of JSON/dict objects with the information about each entry ", "solution": "\n\n        json_dic = {\"plotData\": []}\n        sample_dic = {}\n        sample_assembly_map = {}\n\n        for entry in self.storage.values():\n\n            sample_id = re.match(\"(.*)_abr\", entry[\"log_file\"]).groups()[0]\n            if sample_id not in sample_dic:\n                sample_dic[sample_id] = {}\n\n            # Get contig ID using the same regex as in `assembly_report.py`\n            # template\n            contig_id = self._get_contig_id(entry[\"reference\"])\n            # Get database\n            database = entry[\"database\"]\n            if database not in sample_dic[sample_id]:\n                sample_dic[sample_id][database] = []\n\n            # Update the sample-assembly correspondence dict\n            if sample_id not in sample_assembly_map:\n                sample_assembly_map[sample_id] = entry[\"infile\"]\n\n            sample_dic[sample_id][database].append(\n                {\"contig\": contig_id,\n                 \"seqRange\": entry[\"seq_range\"],\n                 \"gene\": entry[\"gene\"].replace(\"'\", \"\"),\n                 \"accession\": entry[\"accession\"],\n                 \"coverage\": entry[\"coverage\"],\n                 \"identity\": entry[\"identity\"],\n                 },\n            )\n\n        for sample, data in sample_dic.items():\n            json_dic[\"plotData\"].append(\n                {\n                    \"sample\": sample,\n                    \"data\": {\"abricateXrange\": data},\n                    \"assemblyFile\": sample_assembly_map[sample]\n                }\n            )\n\n        return json_dic"}
{"prompt": "def usi(self):\n        \"\"\"\n        Gets an USI string for the move.\n        For example a move from 7A to 8A would be `7a8a` or `7a8a+` if it is\n        a promotion.\n        \"\"\"", "output": "\n        move = self.move\n        move = move.upper()\n        move = move.lstrip(\"+\")\n        move = move.rstrip(\"+\")\n        move = move.lstrip(\"(\")\n        move = move.rstrip(\")\")\n        move = move.lstrip(\"(\")\n        move = move.rstrip(\")\")\n        move = move.lstrip(\"(\")\n        move = move.rstrip(\")\")\n        move = move.lstrip(\"(\")\n        move = move.rstrip(\")\")\n        move = move.lstrip(\"(\")\n        move = move.", "solution": "\n        if self:\n            if self.drop_piece_type:\n                return '{0}*{1}'.format(PIECE_SYMBOLS[self.drop_piece_type].upper(), SQUARE_NAMES[self.to_square])\n            else:\n                return SQUARE_NAMES[self.from_square] + SQUARE_NAMES[self.to_square] + \\\n                       ('+' if self.promotion else '')\n        else:\n            return '0000'"}
{"prompt": "def load_institute(adapter, internal_id, display_name, sanger_recipients=None):\n    \"\"\"Load a institute into the database\n\n        Args:\n            adapter(MongoAdapter)\n            internal_id(str)\n            display_name(str)\n            sanger_recipients(list(email))\n    \"\"\"", "output": "\n    institute = Institute(internal_id=internal_id, display_name=display_name,\n                          sanger_recipients=sanger_recipients)\n    adapter.insert_one(institute)\n\n\ndef load_organization(adapter, internal_id, display_name, sanger_recipients=None):\n    \"\"\"Load an organization into the database\n\n        Args:\n            adapter(MongoAdapter)\n            internal_id(str)\n            display_name(str)\n            sanger_recipients(list(email))\n    \"\"\"\n    organization = Organization(internal_id=internal_id, display_name=display_name,\n                              sanger_recipients=sanger_recipients)\n    adapter.insert_one(organization)", "solution": "\n\n    institute_obj = build_institute(\n        internal_id=internal_id,\n        display_name=display_name,\n        sanger_recipients=sanger_recipients\n    )\n    log.info(\"Loading institute {0} with display name {1}\" \\\n             \" into database\".format(internal_id, display_name))\n\n    adapter.add_institute(institute_obj)"}
{"prompt": "def compute_nc(X, G):\n    \"\"\"Computes the novelty curve from the self-similarity matrix X and\n        the gaussian kernel G.\"\"\"", "output": "\n    \n    # Compute the novelty curve from the self-similarity matrix X\n    novelty = compute_nc_from_X(X)\n    \n    # Compute the novelty curve from the gaussian kernel G\n    novelty = compute_nc_from_G(G, novelty)\n    \n    return novelty\n</cell>\n<text>\nThe novelty curve is computed using the following formula:\n\n$$ nc = \\frac{n}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{n}{2}\\left(\\frac{G}{2}\\right)^{2}\\right) $$\n\nwhere $G$ is the gaussian kernel defined ", "solution": "\n    N = X.shape[0]\n    M = G.shape[0]\n    nc = np.zeros(N)\n\n    for i in range(M // 2, N - M // 2 + 1):\n        nc[i] = np.sum(X[i - M // 2:i + M // 2, i - M // 2:i + M // 2] * G)\n\n    # Normalize\n    nc += nc.min()\n    nc /= nc.max()\n    return nc"}
{"prompt": "def _get_boot_time():\n    \"\"\"Return system boot time (epoch in seconds)\"\"\"", "output": "\n    return int(time.time())\n\n\ndef _get_boot_time_str():\n    \"\"\"Return system boot time formatted as string\"\"\"\n    return time.strftime(\"%H:%M:%S\", time.gmtime(_get_boot_time()))\n\n\ndef _get_boot_time_epoch():\n    \"\"\"Return system boot time formatted as integer\"\"\"\n    return _get_boot_time() / 1000.0\n\n\ndef _get_cpu_count():\n    \"\"\"Return number of CPUs in the system\"\"\"\n    try:\n        return len(os.sched_getaffinity(0))\n    except AttributeError:\n        return len(psutil.cpu_count())\n\n\ndef _get_cpu_count_logical():\n    \"\"\"Return number of logical CPUs in the system\"\"\"\n    try:\n        return len(psutil.cpu_count_logical())\n    except AttributeError:", "solution": "\n    f = open('/proc/stat', 'r')\n    try:\n        for line in f:\n            if line.startswith('btime'):\n                return float(line.strip().split()[1])\n        raise RuntimeError(\"line not found\")\n    finally:\n        f.close()"}
{"prompt": "def get_plugin_of_type(self, plugin_class):\n        \"\"\"\n        Retrieve a plugin of desired class, KeyError raised otherwise\n        \"\"\"", "output": "\n        plugin = None\n        for plugin in self.plugins:\n            if plugin.plugin_type == plugin_class:\n                return plugin\n        raise KeyError('Plugin of class %s not found' % plugin_class)\n\n    def get_plugin_by_name(self, name):\n        \"\"\"\n        Retrieve a plugin by name, KeyError raised otherwise\n        \"\"\"\n        for plugin in self.plugins:\n            if plugin.name == name:\n                return plugin\n        raise KeyError('Plugin with name %s not found' % name)\n\n    def register_plugin(self, plugin):\n        \"\"\"\n        Register a plugin to be instantiated when this plugin is instantiated\n        \"\"\"\n        self.plugins.append(plugin)\n\n    def register_plugin_from_file(self, plugin_file):\n        \"\"\"\n        Register a plugin to be instantiated ", "solution": "\n        logger.debug(\"Searching for plugin: %s\", plugin_class)\n        matches = [plugin for plugin in self.plugins.values() if isinstance(plugin, plugin_class)]\n        if matches:\n            if len(matches) > 1:\n                logger.debug(\n                    \"More then one plugin of type %s found. Using first one.\",\n                    plugin_class)\n            return matches[-1]\n        else:\n            raise KeyError(\"Requested plugin type not found: %s\" % plugin_class)"}
{"prompt": "def calculate_linear_predictor(model_matrix, model_coefficients, offset=None,\n                               name=None):\n  \"\"\"Computes `model_matrix @ model_coefficients + offset`.\"\"\"", "output": "\n  model_matrix = tf.convert_to_tensor(model_matrix)\n  model_coefficients = tf.convert_to_tensor(model_coefficients)\n  offset = tf.convert_to_tensor(offset)\n  assert model_matrix.shape.ndims == 2\n  assert model_coefficients.shape.ndims == 2\n  assert model_coefficients.shape[0] == model_matrix.shape[0]\n  assert model_coefficients.shape[0] == model_matrix.shape[1]\n  assert model_coefficients.shape[1] == model_matrix.shape[2]\n  assert model_coefficients.shape[2] == model_matrix.shape[3]\n  assert model_coefficients.shape[3] == 1\n  assert model_coefficients.shape[4] == 1\n  assert model_coefficients.shape[5] == model_matrix.shape[3]\n  assert model_coefficients.shape[6] == 1", "solution": "\n  with tf.compat.v1.name_scope(name, 'calculate_linear_predictor',\n                               [model_matrix, model_coefficients, offset]):\n    predicted_linear_response = tf.linalg.matvec(model_matrix,\n                                                 model_coefficients)\n    if offset is not None:\n      predicted_linear_response += offset\n    return predicted_linear_response"}
{"prompt": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"", "output": "\n    hero_slider = context.scene.hero_slider\n    hero_slider.set_value(context.scene.hero_slider_value)\n    hero_slider.set_min_value(context.scene.hero_slider_min_value)\n    hero_slider.set_max_value(context.scene.hero_slider_max_value)\n    hero_slider.set_step(0.1)\n    hero_slider.set_value(context.scene.hero_slider_value)\n    hero_slider.set_min_value(context.scene.hero_slider_min_value)\n    hero_slider.set_max_value(context.scene.hero_slider_max_value)\n    hero_slider.set_step(0.", "solution": "\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }"}
{"prompt": "def get_color(self, color, intensity=0):\n        \"\"\" Returns a QColor for a given color code, or None if one cannot be\n            constructed.\n        \"\"\"", "output": "\n        color = QColor(*color)\n        if color.rgba() == (0, 0, 0, 0):\n            return None\n        return QColor(*color.rgba()[:3] + (intensity,))\n\n    def get_color_from_hex(self, hex):\n        \"\"\" Returns a QColor from a hex string or a QColor if one cannot be\n            constructed.\n        \"\"\"\n        color = QColor(*QColor(*hex).rgba())\n        if color.rgba() == (0, 0, 0, 0):\n            return None\n        return color\n\n    def get_color_from_rgb(self, r, g, b):\n        \"\"\" Returns a QColor from a RGB tuple or a QColor if one cannot be\n            constructed.\n        \"\"\"\n        color = QColor(*QColor(*(r, g, b)).rgba", "solution": "\n        if color is None:\n            return None\n\n        # Adjust for intensity, if possible.\n        if color < 8 and intensity > 0:\n            color += 8\n\n        constructor = self.color_map.get(color, None)\n        if isinstance(constructor, basestring):\n            # If this is an X11 color name, we just hope there is a close SVG\n            # color name. We could use QColor's static method\n            # 'setAllowX11ColorNames()', but this is global and only available\n            # on X11. It seems cleaner to aim for uniformity of behavior.\n            return QtGui.QColor(constructor)\n\n        elif isinstance(constructor, (tuple, list)):\n            return QtGui.QColor(*constructor)\n\n        return None"}
{"prompt": "def update_z(self, z, indices=None):\n        \"\"\"\n        Update partial or entire z.\n\n        Args:\n            z (numpy.ndarray or list): to-be-updated z\n            indices (numpy.ndarray or list or optional): to-be-updated qubit indices\n\n        Returns:\n            Pauli: self\n\n        Raises:\n            QiskitError: when updating whole z, the number of qubits must be the same.\n        \"\"\"", "output": "\n        if indices is None:\n            indices = np.arange(self.num_qubits)\n        indices = np.asarray(indices)\n        if z.ndim!= 2:\n            raise QiskitError('update_z only accepts 2D arrays')\n        if z.shape!= (self.num_qubits, self.num_qubits):\n            raise QiskitError('update_z only accepts 2D arrays')\n        if indices.shape!= (self.num_qubits,):\n            raise QiskitError('update_z only accepts 2D arrays')\n        if indices.size!= z.size:\n            raise QiskitError('update_z only accepts 2D arrays')\n        self._update_z(z, indices)\n        return self\n\n    def _update_z(self, z, indices):\n        \"\"\"\n        Update ", "solution": "\n        z = _make_np_bool(z)\n        if indices is None:\n            if len(self._z) != len(z):\n                raise QiskitError(\"During updating whole z, you can not \"\n                                  \"change the number of qubits.\")\n            self._z = z\n        else:\n            if not isinstance(indices, list) and not isinstance(indices, np.ndarray):\n                indices = [indices]\n            for p, idx in enumerate(indices):\n                self._z[idx] = z[p]\n\n        return self"}
{"prompt": "def is_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"", "output": "\n        if atol is None:\n            atol = self.atol\n        if rtol is None:\n            rtol = self.rtol\n        return abs(self.trace() - atol) < rtol\n\n    def trace(self):\n        \"\"\"Return the trace.\"\"\"\n        return self.trace\n\n    def trace_ref(self):\n        \"\"\"Return the trace of the reference operator.\"\"\"\n        return self.trace_ref\n\n    def trace_ref_cptp(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-positive trace-preserving.\"\"\"\n        if atol is None:\n            atol = self.atol\n        if rtol is None:\n            rtol = self.rtol\n        return abs(self.trace_ref() - atol) < rtol\n\n    def trace_ref_cptp2(self, atol=None, rtol=None):\n        \"\"\"Return True if completely-", "solution": "\n        if self._data[1] is not None:\n            return False\n        if atol is None:\n            atol = self._atol\n        if rtol is None:\n            rtol = self._rtol\n        accum = 0j\n        for op in self._data[0]:\n            accum += np.dot(np.transpose(np.conj(op)), op)\n        return is_identity_matrix(accum, rtol=rtol, atol=atol)"}
{"prompt": "def delete_clinvar_object(self, object_id, object_type, submission_id):\n        \"\"\"Remove a variant object from clinvar database and update the relative submission object\n\n            Args:\n                object_id(str) : the id of an object to remove from clinvar_collection database collection (a variant of a case)\n                object_type(str) : either 'variant_data' or 'case_data'. It's a key in the clinvar_submission object.\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"", "output": "\n        clinvar_submission = self.clinvar_submission\n        clinvar_submission.delete_objects(object_id, object_type, submission_id)\n        updated_submission = clinvar_submission.get_objects(object_id, object_type, submission_id)\n        return updated_submission\n\n    def delete_clinvar_submission(self, submission_id):\n        \"\"\"Remove a clinvar submission from clinvar_collection database collection\n\n            Args:\n                submission_id(str): the _id key of a clinvar submission\n\n            Returns:\n                updated_submission(obj): an updated clinvar submission\n        \"\"\"\n        clinvar_submission = self.clinvar_submission\n        clinvar_submission.delete_objects(submission_id)\n        updated_submission = clinvar_submission.get_object", "solution": "\n\n        LOG.info(\"Deleting clinvar object %s (%s)\", object_id, object_type)\n\n        # If it's a variant object to be removed:\n        #   remove reference to it in the submission object 'variant_data' list field\n        #   remove the variant object from clinvar collection\n        #   remove casedata object from clinvar collection\n        #   remove reference to it in the submission object 'caset_data' list field\n\n        # if it's a casedata object to be removed:\n        #   remove reference to it in the submission object 'caset_data' list field\n        #   remove casedata object from clinvar collection\n\n        result = ''\n\n        if object_type == 'variant_data':\n            # pull out a variant from submission object\n            self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'variant_data': object_id} })\n\n            variant_object = self.clinvar_collection.find_one( {'_id': object_id} )\n            linking_id = variant_object.get(\"linking_id\") #it's the original ID of the variant in scout, it's linking clinvar variants and casedata objects together\n\n            # remove any object with that linking_id from clinvar_collection. This removes variant and casedata\n            result = self.clinvar_collection.delete_many( {'linking_id': linking_id } )\n\n        else: # remove case_data but keep variant in submission\n            # delete the object itself from clinvar_collection\n             result = self.clinvar_collection.delete_one( {'_id': object_id } )\n\n        # in any case remove reference to it in the submission object 'caset_data' list field\n        self.clinvar_submission_collection.find_one_and_update( {'_id': ObjectId(submission_id)}, {'$pull': {'case_data': object_id} })\n\n        updated_submission = self.clinvar_submission_collection.find_one_and_update( {'_id':submission_id}, { '$set' : {'updated_at': datetime.now()} }, return_document=pymongo.ReturnDocument.AFTER )\n\n        return updated_submission"}
{"prompt": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"", "output": "\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBitAddr()\n        bitAddr += self._getBit", "solution": "\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd"}
{"prompt": "def select(selector, obj):\n    \"\"\"Appy selector to obj and return matching nodes.\n\n    If only one node is found, return it, otherwise return a list of matches.\n    Returns False on syntax error. None if no results found.\n    \"\"\"", "output": "\n    root = obj.root\n    matches = []\n    for node in root.iter(selector):\n        matches.append(node)\n        if len(matches) == 1:\n            return matches[0]\n    return matches\n\n\ndef find(selector, obj):\n    \"\"\"Find all nodes matching selector in obj and return them.\n\n    If only one node is found, return it, otherwise return a list of matches.\n    Returns False on syntax error. None if no results found.\n    \"\"\"\n    root = obj.root\n    matches = []\n    for node in root.iter(selector):\n        matches.append(node)\n        if len(matches) == 1:\n            return matches[0]\n    return matches\n\n\ndef findall(selector, obj):\n    \"\"\"Find all nodes matching selector in obj and return them.\n\n    If only one node is found, ", "solution": "\n\n    parser = Parser(obj)\n    try:\n        return parser.parse(selector)\n    except SelectorSyntaxError as e:\n        log.exception(e)\n        return False"}
{"prompt": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"", "output": "\n    host_re = re.compile(r\"^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]*[a-zA-Z0-9])\\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\\-]*[A-Za-z0-9])$\")\n    host = host_re.match(host).groups()\n    port = None\n    if len(host) == 2:\n        host, port = host\n    return host, port\n\n\ndef validate_host(host, port):\n    \"\"\"Validates that a host is valid and matches a given port.\n    \"\"\"\n    host, port = parse_host(host)\n    if not host:\n        raise InvalidHost(host, port)\n    if not port:\n        raise InvalidHost(host, port)\n    return host, port\n\n\ndef validate_host_port(host, port", "solution": "\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)"}
{"prompt": "def receive_nak_rebinding(self, pkt):\n        \"\"\"Receive NAK in REBINDING state.\"\"\"", "output": "\n        pkt.check_error()\n        self.logger.debug(\"Received NAK in REBINDING state\")\n        self.state = STATE_REBINDING\n\n    def receive_nak_unbinding(self, pkt):\n        \"\"\"Receive NAK in UNBINDING state.\"\"\"\n        pkt.check_error()\n        self.logger.debug(\"Received NAK in UNBINDING state\")\n        self.state = STATE_UNBINDING\n\n    def receive_nak_unbind_response(self, pkt):\n        \"\"\"Receive NAK in UNBINDING state.\"\"\"\n        pkt.check_error()\n        self.logger.debug(\"Received NAK in UNBINDING state\")\n        self.state = ", "solution": "\n        logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")\n        if self.process_received_nak(pkt):\n            logger.debug(\"C3.1: T. Received NAK, in RENEWING state, \"\n                         \"raise INIT.\")\n            raise self.INIT()"}
{"prompt": "def diseases(context):\n    \"\"\"Show all diseases in the database\"\"\"", "output": "\n    diseases = Disease.query.all()\n    context.diseases = diseases\n    context.diseases_count = diseases.count()\n    context.diseases_pages = paginate(diseases, per_page=10)\n\n\n@blueprint.route('/diseases/<disease_id>')\ndef disease(disease_id):\n    \"\"\"Show a specific disease\"\"\"\n    disease = Disease.query.get_or_404(disease_id)\n    context.disease = disease\n    context.disease_pages = paginate(disease, per_page=10)\n\n\n@blueprint.route('/diseases/<disease", "solution": "\n    LOG.info(\"Running scout view diseases\")\n    adapter = context.obj['adapter']\n\n    disease_objs = adapter.disease_terms()\n\n    nr_diseases = disease_objs.count()\n    if nr_diseases == 0:\n        click.echo(\"No diseases found\")\n    else:\n        click.echo(\"Disease\")\n        for disease_obj in adapter.disease_terms():\n            click.echo(\"{0}\".format(disease_obj['_id']))\n        LOG.info(\"{0} diseases found\".format(nr_diseases))"}
{"prompt": "def infix_to_postfix(nodes, *, recurse_types=None):\n  \"\"\"Convert a list of nodes in infix order to a list of nodes in postfix order.\n\n  E.G. with normal algebraic precedence, 3 + 4 * 5 -> 3 4 5 * +\n  \"\"\"", "output": "\n  result = []\n  stack = [nodes]\n  while stack:\n    node = stack.pop()\n    if node.is_postfix:\n      result.append(node)\n      continue\n    if node.is_infix:\n      stack.append(node.children)\n      continue\n    if recurse_types:\n      node = recurse_types(node)\n    if node.is_infix_op:\n      stack.append(node.children)\n  return result\n\n\ndef postfix_to_infix(nodes, *, recurse_types=None):\n  \"\"\"Convert a list of nodes in postfix order to a list of nodes in infix order.\n\n  E.G. with normal algebraic precedence, 3 4 5 * + -> 3 ", "solution": "\n  output = []\n  operators = []\n\n  for node in nodes:\n    if isinstance(node, OperatorNode):\n      # Drain out all operators whose precedence is gte the node's...\n      cmp_operator = node.operator\n      while operators:\n        current_operator = operators[-1].operator\n        if current_operator.precedence > cmp_operator.precedence or \\\n           current_operator.precedence == cmp_operator.precedence and current_operator.association == Association.left:\n          output.append(operators.pop())\n        else:\n          break\n      operators.append(node)\n    else:\n      if recurse_types is not None and node.node_type in recurse_types:\n        output.extend(infix_to_postfix(node.children, recurse_types=recurse_types))\n      else:\n        output.append(node)\n\n  return output + list(reversed(operators))"}
{"prompt": "def get(cls, key, default=None):\n        \"\"\"\n        Resembles the :meth:`dict.get` method.\n\n        :returns:\n            A configuration dictionary for specified provider.\n\n        \"\"\"", "output": "\n        provider = cls.get_provider(key)\n        if provider is None:\n            return default\n        return provider.get_configuration()\n\n    @classmethod\n    def get_provider(cls, key):\n        \"\"\"\n        Looks up provider by key.\n\n        :param key:\n            Configuration key for provider.\n\n        :returns:\n            Provider instance or None.\n\n        \"\"\"\n        provider = cls.provider_map.get(key)\n        if provider is None:\n            return None\n        return provider\n\n    @classmethod\n    def register(cls, provider):\n        \"\"\"\n        Register provider with :class:`Config`.\n\n        :param provider:\n            Provider instance.\n\n        \"\"\"\n        cls.provider_map[provider.name] = provider\n\n    @classmethod\n    def register_all(cls):\n        \"\"\"\n        Register all ", "solution": "\n\n        # Query datastore.\n        result = cls.query(cls.provider_name == key).get()\n\n        if result:\n            result_dict = result.to_dict()\n\n            # Use NDBOpenIDStore by default\n            result_dict['store'] = NDBOpenIDStore\n\n            # Convert coma-separated values to list. Currently only scope is\n            # csv.\n            for i in ('scope', ):\n                prop = result_dict.get(i)\n                if prop:\n                    result_dict[i] = [s.strip() for s in prop.split(',')]\n                else:\n                    result_dict[i] = None\n\n            return result_dict\n        else:\n            return default"}
{"prompt": "def dump(obj, file, reducers=None, protocol=None):\n    \"\"\"Replacement for pickle.dump() using _LokyPickler.\"\"\"", "output": "\n    reducers = reducers or {}\n    protocol = protocol or pickle.HIGHEST_PROTOCOL\n    Pickler(file, protocol).dump(obj, reducers)\n\n\ndef load(file):\n    \"\"\"Replacement for pickle.load() using _LokyPickler.\"\"\"\n    Pickler(file).load()\n\n\ndef loads(string, encoding='ascii'):\n    \"\"\"Replacement for pickle.loads() using _LokyPickler.\"\"\"\n    return loads(string, encoding=encoding, object_hook=_decode_bytes)\n\n\ndef dumps(obj, protocol=None):\n    \"\"\"Replacement for pickle.dumps() using _LokyPickler.\"\"\"\n    protocol = protocol or pickle.HIGHEST", "solution": "\n    global _LokyPickler\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)"}
{"prompt": "def rule(cls, rulename=None, erase=False):\n    \"\"\"Attach a method to a parsing class and register it as a parser rule.\n\n       The method is registered with its name unless rulename is provided.\n    \"\"\"", "output": "\n    rule = cls._rules.get(rulename, None)\n    if rule is None:\n        rule = cls(rulename)\n        cls._register_rule(rule)\n    if erase:\n        cls._erase_rule(rule)\n    return rule\n\ndef rules(cls):\n    \"\"\"Return a list of all registered parser rules.\"\"\"\n    return list(cls._rules.keys())\n\ndef rule(cls):\n    \"\"\"Return the first registered parser rule or None.\"\"\"\n    return cls._rules.values()[0]\n\ndef register(cls, rule):\n    \"\"\"Register a parser rule and return the new rule.\"\"\"\n    cls._rules[rule.name] = rule\n    return rule\n\ndef unregister(cls, rule):\n    \"\"\"Unregister a ", "solution": "\n    if not hasattr(cls, '_rules'):\n        raise TypeError(\n            \"%s didn't seems to be a BasicParser subsclasse\" % cls.__name__)\n    class_hook_list = cls._hooks\n    class_rule_list = cls._rules\n\n    def wrapper(f):\n        nonlocal rulename\n        add_method(cls)(f)\n        if rulename is None:\n            rulename = f.__name__\n        if not erase and (rulename in class_hook_list or rulename in class_rule_list):\n            raise TypeError(\"%s is already define has rule or hook\" % rulename)\n        if '.' not in rulename:\n            rulename = cls.__module__ + '.' + cls.__name__ + '.' + rulename\n        set_one(class_rule_list, rulename, f)\n        return f\n    return wrapper"}
{"prompt": "def download_url(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"", "output": "\n    filename = filename or os.path.basename(url)\n    download_file(url, root, filename, md5)\n\n\ndef download_file(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    filename = filename or os.path.basename(url)\n    download_file(url, root, filename, md5)\n\n\ndef download_file(url, root, filename=None, md5=None):\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n", "solution": "\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print('Using downloaded and verified file: ' + fpath)\n    else:\n        try:\n            print('Downloading ' + url + ' to ' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == 'https':\n                url = url.replace('https:', 'http:')\n                print('Failed download. Trying https -> http instead.'\n                      ' Downloading ' + url + ' to ' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )"}
{"prompt": "def _in_buffer(self, position=None):\n        \"\"\" Returns whether the current cursor (or, if specified, a position) is\n            inside the editing region.\n        \"\"\"", "output": "\n        if position is None:\n            position = self.position\n        row, col = position\n        row, col = min(max(row, 0), self.rows), min(max(col, self.cols), 1)\n        row, col = max(0, row), max(0, col)\n        return self.buffer.text_buffer.in_region(row, col)\n\n    def in_viewport(self, position=None):\n        \"\"\" Returns whether the current cursor (or, if specified, a position) is\n            inside the editing region.\n        \"\"\"\n        if position is None:\n            position = self.position\n        row, col = position\n        row, col = min(max(row, 0), self.rows), min(max(col, self.cols), 1)\n        row, col = max(0, row), max(0, col)\n        return self.buffer.text_buffer.in_view", "solution": "\n        cursor = self._control.textCursor()\n        if position is None:\n            position = cursor.position()\n        else:\n            cursor.setPosition(position)\n        line = cursor.blockNumber()\n        prompt_line = self._get_prompt_cursor().blockNumber()\n        if line == prompt_line:\n            return position >= self._prompt_pos\n        elif line > prompt_line:\n            cursor.movePosition(QtGui.QTextCursor.StartOfBlock)\n            prompt_pos = cursor.position() + len(self._continuation_prompt)\n            return position >= prompt_pos\n        return False"}
{"prompt": "def split_type(self, typename):\n        \"\"\"\n        Given a potentially complex type, split it into its base type and specializers\n        \"\"\"", "output": "\n        typename = typename.strip()\n        typename = typename.lstrip('*')\n        typename = typename.rstrip('*')\n        typename = typename.lstrip(' ')\n        typename = typename.rstrip(' ')\n        typename = typename.lstrip(' ')\n        typename = typename.rstrip(' ')\n        typename = typename.lstrip(' ')\n        typename = typename.rstrip(' ')\n        typename = typename.lstrip(' ')\n        typename = typename.rstrip(' ')\n        typename = typename.lstrip(' ')\n        typename = typename.rstrip(' ')\n        typename = typename.lstrip(' ')\n        typename = ", "solution": "\n\n        name = self._canonicalize_type(typename)\n        if '(' not in name:\n            return name, False, []\n\n        base, sub = name.split('(')\n        if len(sub) == 0 or sub[-1] != ')':\n            raise ArgumentError(\"syntax error in complex type, no matching ) found\", passed_type=typename, basetype=base, subtype_string=sub)\n\n        sub = sub[:-1]\n\n        subs = sub.split(',')\n        return base, True, subs"}
{"prompt": "def get_response_code(url, timeout=10):\n    \"\"\"\n    Visit the URL and return the HTTP response code in 'int'\n    \"\"\"", "output": "\n    try:\n        req = requests.get(url, timeout=timeout)\n        return req.status_code\n    except requests.exceptions.RequestException:\n        return None\n\n\ndef download_file(url, filename):\n    \"\"\"\n    Download the file at the given URL and save it at filename\n    \"\"\"\n    req = requests.get(url, stream=True)\n    with open(filename, 'wb') as f:\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n\n\ndef download_file_from_google_drive(id, destination):\n    \"\"\"\n    Download a file from Google Drive.\n    \"\"\"\n    URL = \"https://docs.google.com/uc?export=download\"\n    session = requests.Session()\n\n    response = session.get(URL, params={'id': id}, stream=True)\n", "solution": "\n    try:    \n        req = urllib2.urlopen(url, timeout=timeout)\n    except HTTPError, e:\n        return e.getcode()\n    except Exception, _:\n        fail(\"Couldn't reach the URL '%s'\" % url)\n    else:\n        return req.getcode()"}
{"prompt": "def asAMP(cls):\n        \"\"\"\n        Returns the exception's name in an AMP Command friendly format.\n\n        For example, given a class named ``ExampleExceptionClass``, returns\n        ``\"EXAMPLE_EXCEPTION_CLASS\"``.\n        \"\"\"", "output": "\n        return cls.__name__.upper()\n\n    @classmethod\n    def fromException(cls, exception):\n        \"\"\"\n        Converts an exception into a :class:`CommandError` instance.\n\n        :param exception: The exception to convert.\n        \"\"\"\n        return cls(\n            command=exception.command,\n            code=exception.code,\n            args=exception.args,\n            kwargs=exception.kwargs,\n            exception=exception,\n        )\n\n    def toAMP(self):\n        \"\"\"\n        Converts the :class:`CommandError` instance into an AMP Command.\n\n        :returns: A AMP Command.\n        \"\"\"\n        return AMPCommand(\n            command=self.command,\n            code=self.code,\n            args=self.args,\n            kwargs=self.kwargs,\n            exception=self.exception,\n", "solution": "\n        parts = groupByUpperCase(cls.__name__)\n        return cls, \"_\".join(part.upper() for part in parts)"}
{"prompt": "def to_grayscale(img):\n    \"\"\"Convert PIL image to numpy grayscale array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (gray, alpha): both numpy arrays.\n    \"\"\"", "output": "\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n    gray = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n                                cv2.THRESH_BINARY, 11, 2)\n    return gray, 255\n\n\ndef to_rgb(img):\n    \"\"\"Convert PIL image to numpy RGB array and numpy alpha array.\n\n    Args:\n      img (PIL.Image): PIL Image object.\n\n    Returns:\n      (gray, alpha): both numpy arrays.\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    gray = cv2.GaussianBlur(gray, (", "solution": "\n    gray = numpy.asarray(ImageOps.grayscale(img)).astype(numpy.float)\n\n    imbands = img.getbands()\n    alpha = None\n    if 'A' in imbands:\n        alpha = numpy.asarray(img.split()[-1]).astype(numpy.float)\n\n    return gray, alpha"}
{"prompt": "def _handle_options(self, options):\n        \"\"\"\n        Handle the data from the options.\n\n        :param options: The list of options from the rule.\n        :type options: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"", "output": "\n        domains = []\n        for option in options:\n            domains += option.domains\n        return domains\n\n    def _handle_domains(self, domains):\n        \"\"\"\n        Handle the data from the domains.\n\n        :param domains: The list of domains to return globally.\n        :type domains: list\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"\n        domains = []\n        for domain in domains:\n            domains += domain.domains\n        return domains\n\n    def _handle_wildcard(self, wildcard):\n        \"\"\"\n        Handle the data from the wildcard.\n\n        :param wildcard: The wildcard to return globally.\n        :type wildcard: str\n\n        :return: The list of domains to return globally.\n        :rtype: list\n        \"\"\"", "solution": "\n\n        # We initiate a variable which will save our result\n        result = []\n\n        # We initiate the regex which will be used to extract the domain listed\n        # under the option domain=\n        regex_domain_option = r\"domain=(.*)\"\n\n        for option in options:\n            # We loop through the list of option.\n            try:\n                # We try to extract the list of domains from the currently read\n                # option.\n                domains = Regex(\n                    option, regex_domain_option, return_data=True, rematch=True, group=0\n                ).match()[-1]\n\n                if domains:\n                    # We could extract something.\n\n                    if self.aggressive:  # pragma: no cover\n                        result.extend(\n                            [\n                                x\n                                for x in domains.split(\"|\")\n                                if x and not x.startswith(\"~\")\n                            ]\n                        )\n                    else:\n                        # We return True.\n                        return True\n            except TypeError:\n                pass\n\n        # We return the result.\n        return result"}
{"prompt": "def to_fmt(self) -> fmt.indentable:\n    \"\"\"\n    Return an Fmt representation for pretty-printing\n    \"\"\"", "output": "\n    return fmt.indentable(self.to_str())\n\ndef to_str(self) -> str:\n    \"\"\"\n    Return an str representation for pretty-printing\n    \"\"\"\n    return str(self.to_fmt())\n\ndef to_json(self) -> str:\n    \"\"\"\n    Return an json representation for pretty-printing\n    \"\"\"\n    return json.dumps(self.to_fmt())\n\ndef to_yaml(self) -> str:\n    \"\"\"\n    Return an yaml representation for pretty-printing\n    \"\"\"\n    return yaml.dump(self.to_fmt())\n\ndef to_jsonl(self) -> str:\n    \"\"\"\n    Return an jsonl representation for pretty-printing\n    \"\"\"\n    return json.dumps(self.to_fmt(), indent=2)\n\ndef to_yamll(self) -> str:\n    \"\"\"\n    Return an ", "solution": "\n    qual = \"scope\"\n    txt = fmt.sep(\" \", [qual])\n    name = self.show_name()\n    if name != \"\":\n        txt.lsdata.append(name)\n    if len(self._hsig) > 0 or len(self.mapTypeTranslate) > 0:\n        lsb = []\n        if len(self.mapTypeTranslate) > 0:\n            lsb.append(\"translate:\\n\")\n            lsb.append(fmt.end(\"\\n\", self.mapTypeTranslate.to_fmt()))\n        for k in sorted(self._hsig.keys()):\n            s = self._hsig[k]\n            lsb.append(fmt.end(\"\\n\", [s.to_fmt()]))\n        block = fmt.block(\":\\n\", \"\", fmt.tab(lsb))\n        txt.lsdata.append(block)\n    return txt"}
{"prompt": "def when_children_replaced ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being replaced on a\n            specified object.\n        \"\"\"", "output": "\n        listeners = object.children\n        if listeners is None:\n            listeners = object.children = {}\n        listeners[listener] = remove\n\n    def when_children_removed ( self, object, listener, remove ):\n        \"\"\" Sets up or removes a listener for children being removed from a specified\n            object.\n        \"\"\"\n        listeners = object.children\n        if listeners is None:\n            listeners = object.children = {}\n        listeners[listener] = remove\n\n    def when_children_renamed (\n        self, object, listener, old_name, new_name, remove ):\n        \"\"\" Sets up or removes a listener for children being renamed from a\n            specified object.\n        \"\"\"\n        listeners = object.children\n        if listeners is None:\n", "solution": "\n        object.on_trait_change( listener, \"subgraphs\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"clusters\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"nodes\", remove = remove,\n                                dispatch = \"fast_ui\" )\n        object.on_trait_change( listener, \"edges\", remove = remove,\n                                dispatch = \"fast_ui\" )"}
{"prompt": "def tokenize_line(line):\n    \"\"\"\n    Tokenize a line:\n    * split tokens on whitespace\n    * treat quoted strings as a single token\n    * drop comments\n    * handle escaped spaces and comment delimiters\n    \"\"\"", "output": "\n    tokens = line.strip().split()\n    quoted = False\n    token_start = 0\n    token_end = 0\n    token_content = []\n    for token in tokens:\n        if token == '\"':\n            quoted = True\n        elif token == \"'\":\n            quoted = False\n        elif quoted:\n            token_content.append(token)\n        else:\n            token_content.append(token)\n        token_end += 1\n        if token_end >= len(tokens):\n            token_start = 0\n            token_end = 0\n    if quoted:\n        token_content.append(tokens[token_start])\n    return token_start, token_end, token_content\n\n\ndef tokenize_lines(lines):\n    \"\"\"\n    Tokenize a list of lines:\n    * ", "solution": "\n    ret = []\n    escape = False\n    quote = False\n    tokbuf = \"\"\n    ll = list(line)\n    while len(ll) > 0:\n        c = ll.pop(0)\n        if c.isspace():\n            if not quote and not escape:\n                # end of token\n                if len(tokbuf) > 0:\n                    ret.append(tokbuf)\n\n                tokbuf = \"\"\n            elif quote:\n                # in quotes\n                tokbuf += c\n            elif escape:\n                # escaped space\n                tokbuf += c\n                escape = False\n            else:\n                tokbuf = \"\"\n\n            continue\n\n        if c == '\\\\':\n            escape = True\n            continue\n        elif c == '\"':\n            if not escape:\n                if quote:\n                    # end of quote\n                    ret.append(tokbuf)\n                    tokbuf = \"\"\n                    quote = False\n                    continue\n                else:\n                    # beginning of quote\n                    quote = True\n                    continue\n        elif c == ';':\n            if not escape:\n                # comment \n                ret.append(tokbuf)\n                tokbuf = \"\"\n                break\n            \n        # normal character\n        tokbuf += c\n        escape = False\n\n    if len(tokbuf.strip(\" \").strip(\"\\n\")) > 0:\n        ret.append(tokbuf)\n\n    return ret"}
{"prompt": "def tasks(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`TasksOperations<azure.mgmt.containerregistry.v2018_09_01.operations.TasksOperations>`\n        \"\"\"", "output": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from.v2018_09_01.operations import TasksOperations as OperationClass\n            return OperationClass(self._client, self._config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n\n    @property\n    def images(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-09-01: :class:`ImagesOperations<azure.mgmt.containerregistry.v2018_09_01.operations.ImagesOperations>`\n        \"\"\"\n        api_version = self._get_api_version", "solution": "\n        api_version = self._get_api_version('tasks')\n        if api_version == '2018-09-01':\n            from .v2018_09_01.operations import TasksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"prompt": "def f_supports(self, data):\n        \"\"\"Checks if input data is supported by the parameter.\"\"\"", "output": "\n        \n        # Check if data is supported\n        if data.ndim == 1:\n            if data.shape[0] == 1:\n                return True\n            else:\n                return False\n        else:\n            if data.shape[0] == 1:\n                return True\n            else:\n                return False\n    \n    def f_predict(self, data, **kwargs):\n        \"\"\"Predicts the output given the input.\"\"\"\n        \n        # Check if data is supported\n        if data.ndim == 1:\n            if data.shape[0] == 1:\n                return data\n            else:\n                return np.full(data.shape[0], fill_value=np.nan, dtype=float)\n        else:\n            if data.shape[0] == 1:\n                return data\n            else:\n                return np.full(data.shape[0], fill_value=np.nan, dtype=float)\n    \n    def f_train(self, data, **kwargs):\n        \"\"\"Trains the model given the input.\"\"\"\n        ", "solution": "\n        dtype = type(data)\n        if dtype is tuple or dtype is list and len(data) == 0:\n            return True  #  ArrayParameter does support empty tuples\n        elif dtype is np.ndarray and data.size == 0 and data.ndim == 1:\n                return True  #  ArrayParameter supports empty numpy arrays\n        else:\n            return super(ArrayParameter, self).f_supports(data)"}
{"prompt": "def moments_of_masked_time_series(time_series_tensor, broadcast_mask):\n  \"\"\"Compute mean and variance, accounting for a mask.\n\n  Args:\n    time_series_tensor: float `Tensor` time series of shape\n      `concat([batch_shape, [num_timesteps]])`.\n    broadcast_mask: bool `Tensor` of the same shape as `time_series`.\n  Returns:\n    mean: float `Tensor` of shape `batch_shape`.\n    variance: float `Tensor` of shape `batch_shape`.\n  \"\"\"", "output": "\n  batch_shape = time_series_tensor.shape.as_list()\n  num_timesteps = batch_shape[-1]\n  num_timeseries_nonzero = math_ops.reduce_sum(\n      math_ops.to_float(math_ops.greater(broadcast_mask, 0)))\n  num_timeseries_nonzero = math_ops.minimum(num_timeseries_nonzero, num_timesteps)\n  num_timeseries_nonzero = array_ops.reshape(num_timeseries_nonzero,\n                                             [num_timeseries_nonzero])\n  num_timeseries_nonzero = array_ops.tile(num_timeseries_nonzero,\n                                           array_ops.shape(time_series_tensor)[0])\n  num_timeseries_nonzero = array_ops.reshape(num_timeseries_nonzero,\n                                             [num_time", "solution": "\n  num_unmasked_entries = tf.cast(\n      tf.reduce_sum(input_tensor=tf.cast(~broadcast_mask, tf.int32), axis=-1),\n      time_series_tensor.dtype)\n\n  # Manually compute mean and variance, excluding masked entries.\n  mean = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      time_series_tensor), axis=-1) / num_unmasked_entries)\n  variance = (tf.reduce_sum(input_tensor=tf.where(\n      broadcast_mask,\n      tf.zeros_like(time_series_tensor),\n      (time_series_tensor - mean[..., tf.newaxis]) ** 2), axis=-1)\n              / num_unmasked_entries)\n  return mean, variance"}
{"prompt": "def flush(self, full=False, all=False, force=False):\n        \"\"\"\n        Call ``.save()`` on all modified entities in the session. Use when you\n        want to flush changes to Redis, but don't want to lose your local\n        session cache.\n\n        See the ``.commit()`` method for arguments and their meanings.\n        \"\"\"", "output": "\n        flushed = super(Session, self).flush(full=full, all=all, force=force)\n        if flushed:\n            self.commit()\n        return flushed\n\n    def commit(self):\n        \"\"\"\n        Commit all pending changes to the database.\n        \"\"\"\n        conn = self.connection\n        conn.send_command('SAVEPOINT default')\n        conn.send_command('RELEASE SAVEPOINT default')\n        conn.send_command('SAVEPOINT main')\n        conn.send_command('RELEASE SAVEPOINT main')\n\n    def rollback(self):\n        \"\"\"\n        Roll back the current transaction to the last saved point.\n        \"\"\"\n        conn = self.connection\n        conn.", "solution": "\n        self._init()\n\n        return self.save(*self.known.values(), full=full, all=all, force=force)"}
{"prompt": "def format_price_commas(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with commas\n    \"\"\"", "output": "\n    return str(round(price, 2))\n\n\ndef format_price_dollars(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of dollars) with dollars\n    \"\"\"\n    return str(round(price, 2))\n\n\ndef format_price_euros(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of euros) with euros\n    \"\"\"\n    return str(round(price, 2))\n\n\ndef format_price_pounds(price):\n    \"\"\"\n    Formats prices, rounding (i.e. to the nearest whole number of pounds) with pounds\n    \"\"\"\n    return str(round(price, 2))\n\n\n", "solution": "\n    if price is None:\n        return None\n    if price >= 0:\n        return jinja2.Markup('&pound;{:,.2f}'.format(price))\n    else:\n        return jinja2.Markup('-&pound;{:,.2f}'.format(-price))"}
{"prompt": "def _distribute_jobs(self):\n        \"\"\"\n        Distributes jobs from the active job set to any waiting get_job\n        callbacks.\n        \"\"\"", "output": "\n        for job in self.active_jobs:\n            job.distribute()\n\n    def wait_for_jobs(self, jobs, timeout=None):\n        \"\"\"\n        Wait for jobs to complete.\n\n        Blocks until the jobs complete.\n\n        Raises JobTimeoutError if the timeout is exceeded.\n        \"\"\"\n        start = time.time()\n        while True:\n            jobs = self.active_jobs\n            if jobs:\n                job = jobs.popleft()\n                if job.state in Job.STATE_COMPLETE:\n                    return job\n            if timeout and time.time() - start > timeout:\n                raise JobTimeoutError('Timed out waiting for jobs')\n            time.sleep(0.01)\n\n    def wait_for_job(self, job, timeout=None):\n        \"\"\"\n", "solution": "\n\n        while (self._active_js.job_available()\n                and len(self._ready_callbacks) > 0):\n            job = self._active_js.get_job()\n            self._job_sources[job] = self._active_js\n            callback = self._ready_callbacks.popleft()\n            callback(job)"}
{"prompt": "def finalize(self, success_or_fail, message=''):\n        \"\"\"\n        :param success_or_fail: string that is 'success' or 'fail'\n        :param message:\n        \"\"\"", "output": "\n        if success_or_fail =='success':\n            self.success = True\n        elif success_or_fail == 'fail':\n            self.success = False\n        else:\n            raise AssertionError('Invalid success_or_fail value')\n        self.message = message\n\n    def to_dict(self):\n        \"\"\"\n        :return: dict representation of the object\n        \"\"\"\n        return {\n           'success': self.success,\n           'message': self.message\n        }\n\n\nclass JobStatusEnum(object):\n    \"\"\"\n    Enum class for job statuses\n    \"\"\"\n    RUNNING = 'running'\n    SUCCEEDED ='succeeded'\n    FAILED = 'failed'\n    SKIPPED ='skipped'\n    SKIPPING ='skipping'\n    ", "solution": "\n        self.logit.debug('String OutputPorts: %s' % self.__string_output_ports)\n        if self.__string_output_ports:\n            with open(os.path.join(self.output_path, 'ports.json'), 'w') as opf:\n                json.dump(self.__string_output_ports, opf, indent=4)\n            self.logit.debug('Ports.json written to %s' % os.path.join(self.output_path, 'ports.json'))\n\n        with open(os.path.join(self.base_path, 'status.json'), 'w') as sf:\n            json.dump({'status': success_or_fail, 'reason': message}, sf, indent=4)"}
{"prompt": "def get_users_for_sis_course_id(self, sis_course_id, params={}):\n        \"\"\"\n        Returns a list of users for the given sis course id.\n        \"\"\"", "output": "\n        params = params or {}\n        params['sis_course_id'] = sis_course_id\n        return self._call_api(\n            '/users', 'GET', params=params\n        )\n\n    def get_users_for_sis_course_id_with_users(self, sis_course_id, users, params={}):\n        \"\"\"\n        Returns a list of users for the given sis course id.\n        \"\"\"\n        params = params or {}\n        params['sis_course_id'] = sis_course_id\n        params['users'] = users\n        return self._call_api(\n            '/users', 'GET', params=params\n        )\n\n    def get_users_for_sis_course_id_with_users_and_sis_course_id(self, sis_course_id, users, si", "solution": "\n        return self.get_users_for_course(\n            self._sis_id(sis_course_id, sis_field=\"course\"), params)"}
{"prompt": "def sync_folder(self, path, bucket):\n        \"\"\"Syncs a local directory with an S3 bucket.\n     \n        Currently does not delete files from S3 that are not in the local directory.\n\n        path: The path to the directory to sync to S3\n        bucket: The name of the bucket on S3\n        \"\"\"", "output": "\n        \n        bucket = self.bucket(bucket)\n        \n        prefix = self.prefix(path)\n        \n        bucket.put_prefix(prefix)\n        \n        prefix = self.prefix(path)\n        \n        bucket.delete_prefix(prefix)\n        \n    def sync_file(self, path, bucket):\n        \"\"\"Syncs a local file with an S3 bucket.\n     \n        Currently does not delete files from S3 that are not in the local file.\n\n        path: The path to the file to sync to S3\n        bucket: The name of the bucket on S3\n        \"\"\"\n        \n        bucket = self.bucket(bucket)\n        \n        prefix = self.prefix(path)\n        \n        bucket.put_object(Key", "solution": "\n        bucket = self.conn.get_bucket(bucket)\n        local_files = self._get_local_files(path)\n        s3_files = self._get_s3_files(bucket)\n        for filename, hash in local_files.iteritems():\n            s3_key = s3_files[filename]\n            if s3_key is None:\n                s3_key = Key(bucket)\n                s3_key.key = filename\n                s3_key.etag = '\"!\"'\n            \n            if s3_key.etag[1:-1] != hash[0]:\n                s3_key.set_contents_from_filename(join(path, filename), md5=hash)"}
{"prompt": "def new(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"", "output": "\n        return self._create(*args, **kwargs)\n\n    def _create(self, *args, **kw):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"\n        kw['cls'] = self.__class__\n        kw['args'] = self.args\n        kw['kwargs'] = self.kwargs\n        return self._create_instance(*args, **kw)\n\n    def _create_instance(self, *args, **kwargs):\n        \"\"\"\n        Create and return a new instance.\n        \"\"\"\n        raise NotImplementedError('subclasses of BaseModel may require a _create() method')\n\n    def db_table(self, table_name, schema=None):\n        \"\"\"\n        Returns a quoted table name in a database neutral way.\n        \"\"\"\n        if schema is None:\n            schema = self.schema\n        kw = {'schema': schema}\n        return quote_name(table_name), kw\n\n    def ", "solution": "\n        inst = self.clazz()\n        self.storage.append(inst)\n        \n        # set all attributes with an initial default value\n        referential_attributes = dict()\n        for name, ty in self.attributes:\n            if name not in self.referential_attributes:\n                value = self.default_value(ty)\n                setattr(inst, name, value)\n            \n        # set all positional arguments\n        for attr, value in zip(self.attributes, args):\n            name, ty = attr\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n            \n        # set all named arguments\n        for name, value in kwargs.items():\n            if name not in self.referential_attributes:\n                setattr(inst, name, value)\n            else:\n                referential_attributes[name] = value\n        \n        if not referential_attributes:\n            return inst\n        \n        # batch relate referential attributes \n        for link in self.links.values():\n            if set(link.key_map.values()) - set(referential_attributes.keys()):\n                continue\n             \n            kwargs = dict()\n            for key, value in link.key_map.items():\n                kwargs[key] = referential_attributes[value]\n            \n            if not kwargs:\n                continue\n            \n            for other_inst in link.to_metaclass.query(kwargs):\n                relate(other_inst, inst, link.rel_id, link.phrase)\n        \n        for name, value in referential_attributes.items():\n            if getattr(inst, name) != value:\n                logger.warning('unable to assign %s to %s', name, inst)\n                \n        return inst"}
{"prompt": "def plotBrightLimitInV(gBright, pdf=False, png=False):\n  \"\"\"\n  Plot the bright limit of Gaia in V as a function of (V-I).\n\n  Parameters\n  ----------\n\n  gBright - The bright limit of Gaia in G\n  \"\"\"", "output": "\n\n  fig, ax = plt.subplots(figsize=(10, 6))\n  ax.plot(gBright, label='Gaia Bright')\n  ax.plot(gBright*0.9, label='Gaia Bright (95% CI)')\n  ax.set_xlabel('V (V)')\n  ax.set_ylabel('Gaia Bright (V)')\n  ax.legend()\n\n  if pdf:\n    plt.savefig('plots/bright_limit_in_V.pdf', bbox_inches='tight')\n  if png:\n    plt.savefig('plots/bright_limit_in_V.png', bbox_inches='tight')\n\ndef plotBrightLimitInI(gBright, pdf=False, png=False):\n  \"\"\"\n  Plot the bright limit of Gaia in ", "solution": "\n  vmini=np.linspace(0.0,6.0,1001)\n  gminv=gminvFromVmini(vmini)\n  vBright=gBright-gminv\n\n  fig=plt.figure(figsize=(10,6.5))\n  plt.plot(vmini,vBright,'b-')\n  plt.xlabel('$(V-I)$')\n  plt.ylabel('Bright limit of Gaia in $V$')\n  plt.xlim(0,6)\n  plt.ylim(5,11)\n  plt.grid(which='both')\n  plt.title(\"Bright limit in $G$: {0}\".format(gBright))\n\n  if (pdf):\n    plt.savefig('VBandBrightLimit.pdf')\n  elif (png):\n    plt.savefig('VBandBrightLimit.png')\n  else:\n    plt.show()"}
{"prompt": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"", "output": "\n        tablename = tablename.lower()\n        tablename = tablename.rstrip('s')\n        tablename = tablename.rstrip('n')\n        tablename = tablename.rstrip('l')\n        tablename = tablename.rstrip('c')\n        tablename = tablename.rstrip('d')\n        tablename = tablename.rstrip('t')\n        tablename = tablename.rstrip('e')\n        tablename = tablename.rstrip('r')\n        tablename = tablename.rstrip('t')\n        tablename = tablename.rstrip('e')\n        tablename = tablename.rstrip('r')\n        tablename = tablename.rstrip('t')\n        tablename = tablename.", "solution": "\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table"}
{"prompt": "def _align(self, axes, key_shape=None):\n        \"\"\"\n        Align local bolt array so that axes for iteration are in the keys.\n\n        This operation is applied before most functional operators.\n        It ensures that the specified axes are valid, and might transpose/reshape\n        the underlying array so that the functional operators can be applied\n        over the correct records.\n\n        Parameters\n        ----------\n        axes: tuple[int]\n            One or more axes that will be iterated over by a functional operator\n\n        Returns\n        -------\n        BoltArrayLocal\n        \"\"\"", "output": "\n        axes = tuple(axes)\n        axes = tuple(\n            sorted(\n                axes,\n                key=lambda ax: ax if ax is not None else 0,\n            )\n        )\n        axes = tuple(\n            ax if ax is not None else 0 for ax in axes\n        )\n        axes = tuple(\n            ax if ax is not None else 0 for ax in axes\n        )  # ensure axes are valid\n        axes = tuple(\n            ax if ax is not None else 0 for ax in axes\n        )  # ensure axes are valid\n        axes = tuple(\n            ax if ax is not None else 0 for ax in axes\n        )  # ensure axes are valid\n        axes = tuple(\n            ax if ax is not None else 0 for ", "solution": "\n\n        # ensure that the key axes are valid for an ndarray of this shape\n        inshape(self.shape, axes)\n\n        # compute the set of dimensions/axes that will be used to reshape\n        remaining = [dim for dim in range(len(self.shape)) if dim not in axes]\n        key_shape = key_shape if key_shape else [self.shape[axis] for axis in axes]\n        remaining_shape = [self.shape[axis] for axis in remaining]\n        linearized_shape = [prod(key_shape)] + remaining_shape\n\n        # compute the transpose permutation\n        transpose_order = axes + remaining\n\n        # transpose the array so that the keys being mapped over come first, then linearize keys\n        reshaped = self.transpose(*transpose_order).reshape(*linearized_shape)\n\n        return reshaped"}
{"prompt": "def info(self):\n        \"\"\" Returns a description of the trait.\"\"\"", "output": "\n        return self.description\n\n    def accept(self, visitor, **kwargs):\n        \"\"\" Accepts the visitor and returns a new trait instance.\"\"\"\n        return visitor.visit_trait(self)\n\n    def copy(self):\n        \"\"\" Returns a copy of this trait. \"\"\"\n        return Trait(self.name, self.description, self.default)\n\n    def copy_named(self, name=None):\n        \"\"\" Returns a copy of this trait with a new name. \"\"\"\n        return Trait(self.name, name or self.name, None)\n\n    def copy_named_default(self, name=None):\n        \"\"\" Returns a copy of this trait with a new default value. \"\"\"\n        return Trait(self.name, name or self.name, self.default)\n\n    def copy_named_description(self, name=None):\n        \"\"\" Returns a copy of this trait with a new description. \"\"\"\n        return Trait(self.", "solution": "\n        result = 'any of ' + repr(self.values)\n        if self._allow_none:\n            return result + ' or None'\n        return result"}
{"prompt": "def update_affinity_group(self, affinity_group_name, label,\n                              description=None):\n        \"\"\"\n        Updates the label and/or the description for an affinity group for the\n        specified subscription.\n\n        affinity_group_name:\n            The name of the affinity group.\n        label:\n            A name for the affinity group. The name can be up to 100 characters\n            in length.\n        description:\n            A description for the affinity group. The description can be up to\n            1024 characters in length.\n        \"\"\"", "output": "\n        body = {\n            'label': label,\n            'description': description\n        }\n        uri = '/subscriptions/{0}/providers/Microsoft.Network/affinity/groups/{1}'\n        uri = uri.format(self._subscription_id, affinity_group_name)\n        self.client.put(uri, json=body)\n\n    def list_affinity_groups(self, filter=None):\n        \"\"\"\n        Lists all affinity groups for the specified subscription.\n\n        Filters:\n            None.\n        \"\"\"\n        uri = '/subscriptions/{0}/providers/Microsoft.Network/affinity/groups'\n        uri = uri.format(self._subscription_id)\n        if filter is not None:\n            uri = uri + '?$filter=label%20", "solution": "\n        _validate_not_none('affinity_group_name', affinity_group_name)\n        _validate_not_none('label', label)\n        return self._perform_put(\n            '/' + self.subscription_id + '/affinitygroups/' +\n            _str(affinity_group_name),\n            _XmlSerializer.update_affinity_group_to_xml(label, description))"}
{"prompt": "def HWProcess(cls, proc, ctx):\n        \"\"\"\n        Serialize HWProcess instance\n\n        :param scope: name scope to prevent name collisions\n        \"\"\"", "output": "\n        proc_name = proc.name\n        proc_type = proc.type\n        proc_id = proc.id\n        proc_name = proc_name.replace(' ', '_')\n        proc_type = proc_type.replace(' ', '_')\n        proc_id = proc_id.replace(' ', '_')\n        proc_name = proc_name.replace('-', '_')\n        proc_type = proc_type.replace('-', '_')\n        proc_id = proc_id.replace('-', '_')\n        proc_name = proc_name.replace('.', '_')\n        proc_type = proc_type.replace('.', '_')\n        proc_id = proc_id.replace('.', '_')\n        proc_name = proc_name.replace(':', '_')\n        proc_type = proc_type.replace(':', '_')", "solution": "\n        body = proc.statements\n        childCtx = ctx.withIndent()\n        statemets = [cls.asHdl(s, childCtx) for s in body]\n        proc.name = ctx.scope.checkedName(proc.name, proc)\n\n        return cls.methodTmpl.render(\n            indent=getIndent(ctx.indent),\n            name=proc.name,\n            statements=statemets\n        )"}
{"prompt": "def _get_tol(tol, dtype, validate_args):\n  \"\"\"Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.\"\"\"", "output": "\n  if tol is None:\n    tol = _EPSILON\n  if validate_args:\n    dtype = dtype if dtype in (np.float32, np.float64) else np.float32\n  return _convert_to_tensor_v2_with_dispatch(\n      tol, dtype, validate_args)\n\n\ndef clip_by_value(tensor, clip_value_min, clip_value_max, name=None):\n  \"\"\"Clips elements of `tensor` by the values of `clip_value_min` and\n  `clip_value_max`.\n\n  Given a tensor `tensor` of arbitrary shape, `clip_value_min` and `clip_value_max`\n  are scalar `Tensor`s of the same type and shape, and `tensor` is\n  clipped such that the values of the smaller tensor are strictly ", "solution": "\n  if tol is None:\n    return tf.convert_to_tensor(value=0, dtype=dtype)\n\n  tol = tf.convert_to_tensor(value=tol, dtype=dtype)\n  if validate_args:\n    tol = distribution_util.with_dependencies([\n        assert_util.assert_non_negative(\n            tol, message=\"Argument 'tol' must be non-negative\")\n    ], tol)\n  return tol"}
{"prompt": "def get_item_objects(self, query_params=None):\n        \"\"\"\n        Get the items for this checklist. Returns a list of ChecklistItem objects.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        path_params = {}\n\n        query_params = self._common_query_params(query_params)\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.\\\n            select_header_accept(['application/json'])\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.\\\n            select_header_content_type(['application/json'])\n\n        # Authentication setting\n        auth_settings = []\n\n        return self.api_client.call_api('/checklists', 'GET',\n                                        path_params,\n                                        query_params,\n                                        header_params,\n                                        body=", "solution": "\n        card = self.get_card()\n        checklistitems_list = []\n        for checklistitem_json in self.get_items(query_params):\n            checklistitems_list.append(self.create_checklist_item(card.id, self.id, checklistitem_json))\n\n        return checklistitems_list"}
{"prompt": "def onTWriteCallback__init(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"", "output": "\n        self.sim = sim\n        self.sim.onTWriteCallback = self\n        self.sim.onTWriteCallback__init(self)\n\n    def onTWriteCallback__onTWrite(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"\n        self.sim = sim\n        self.sim.onTWriteCallback = self\n        self.sim.onTWriteCallback__onTWrite(self)\n\n    def onTWriteCallback__onTWriteDone(self, sim):\n        \"\"\"\n        Process for injecting of this callback loop into simulator\n        \"\"\"\n        self.sim = sim\n        self.sim.onTWriteCallback = self\n        self.sim.onTWriteCallback__on", "solution": "\n        yield from self.onTWriteCallback(sim)\n        self.intf.t._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)\n        self.intf.o._sigInside.registerWriteCallback(\n            self.onTWriteCallback,\n            self.getEnable)"}
{"prompt": "def GetParam(tag, param, default=__SENTINEL):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"", "output": "\r\n    return GetParam(tag, param, default, GetParam.Params)\r\n\r\ndef GetParam(tag, param, default=__SENTINEL, params=GetParam.Params):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"\r\n    tag = GetParam.Tags.get(tag, tag)\r\n    if tag in params:\r\n        return params[tag]\r\n    else:\r\n        return default\r\n\r\ndef GetParams(tag, params=GetParam.Params):\r\n    \"\"\" Convenience function for accessing tag parameters\"\"\"\r\n    return GetParam(tag, params, GetParam.Params, GetParam.Tags)\r\n\r\ndef GetParams(tag, params=GetParam.Params, tags=GetParam.Tags):", "solution": "\r\n    if tag.HasParam(param):\r\n        return tag.GetParam(param)\r\n    else:\r\n        if default == __SENTINEL:\r\n            raise KeyError\r\n        else:\r\n            return default"}
{"prompt": "def _connection_failed(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after connection failure detected.\"\"\"", "output": "\n        self._connection_lost = True\n        self._connection_error = error\n\n    def _connection_lost(self, error=\"Error not specified!\"):\n        \"\"\"Clean up after connection lost detected.\"\"\"\n        self._connection_lost = True\n        self._connection_error = error\n\n    def _connection_made(self, transport):\n        \"\"\"Handle a connection made event.\n\n        We need to handle a connection made event because we may not\n        have received a response from the server yet.\n        \"\"\"\n        self._transport = transport\n        self._transport.close()\n        self._transport = None\n\n    def _handle_read(self):\n        \"\"\"Handle a read event.\n\n        We need to handle a read event because we may not\n        have received a response from the server yet.\n", "solution": "\n        if not self._error:\n            LOG.error(\"Connection failed: %s\", str(error))\n            self._error = error"}
{"prompt": "def sequence_detection_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if the security context can use out-of-sequence message detection for messages\n        protected by :meth:`get_mic` and :meth:`wrap`. False if OOS detection cannot be used.\n        \"\"\"", "output": "\n        return self._negotiated_sequence_detection\n\n    @property\n    def negotiated_sequence_detection(self):\n        \"\"\"\n        Boolean value indicating whether out-of-sequence message detection is enabled for\n        messages protected by :meth:`get_mic` and :meth:`wrap`.\n        \"\"\"\n        return self._negotiated_sequence_detection\n\n    @property\n    def negotiated_sequence_detection_negotiated(self):\n        \"\"\"\n        Boolean value indicating whether out-of-sequence message detection is enabled for\n        messages protected by :meth:`get_mic` and :meth:`wrap`.\n        \"\"\"\n        return self._negotiated_sequence_detection_negotiated\n\n    @property\n    def negotiated_sequence_detection_negotiated_negotiated(self):\n        \"\"\"\n", "solution": "\n        return (\n            self.flags & C.GSS_C_SEQUENCE_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )"}
{"prompt": "def domain_to_fqdn(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"", "output": "\n    domain = domain.lower()\n    domain = '.'.join(domain.split('.')[:-1])\n    if proto is None:\n        proto = 'http'\n    return '%s%s.%s' % (proto, domain, domain)\n\n\ndef domain_to_url(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"\n    domain = domain.lower()\n    domain = '.'.join(domain.split('.')[:-1])\n    if proto is None:\n        proto = 'http'\n    return '%s//%s' % (proto, domain)\n\n\ndef domain_to_url(domain, proto=None):\n    \"\"\" returns a fully qualified app domain name \"\"\"\n    domain = domain.lower()\n    domain = '.'.join(domain.split('.')[:-1])\n    if proto is None:\n        proto = 'http'", "solution": "\n    from .generic import get_site_proto\n    proto = proto or get_site_proto()\n    fdqn = '{proto}://{domain}'.format(proto=proto, domain=domain)\n    return fdqn"}
{"prompt": "def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns a\n        serializable object for ``o``, or calls the base implementation (to\n        raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could implement\n        default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                return JSONEncoder.default(self, o)\n        \"\"\"", "output": "\n        raise TypeError(repr(o) + \" is not JSON serializable\")\n\n    def encode(self, o):\n        \"\"\"Return a JSON string representation of a Python data structure.\n\n        >>> from simplejson import JSONEncoder\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\": [\"bar\", \"baz\"]}'\n\n        \"\"\"\n        # This is for extremely simple cases and benchmarks.\n        if isinstance(o, basestring):\n            if isinstance(o, str):\n                _encoding = self.encoding\n                if (_encoding is not None\n                        and not (_encoding == 'utf-8')):\n                    o = o.decode(_encoding)\n            if self.ensure_ascii:\n                return encode_basestring_ascii(o)\n            else:\n                return encode_basestring(o)\n        # This doesn't pass the ", "solution": "\n        if isinstance(o, datetime):\n            return http_date(o)\n        if isinstance(o, uuid.UUID):\n            return str(o)\n        if hasattr(o, '__html__'):\n            return text_type(o.__html__())\n        return _json.JSONEncoder.default(self, o)"}
{"prompt": "def merge(\n            self, reservation_order_id, sources=None, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Merges two `Reservation`s.\n\n        Merge the specified `Reservation`s into a new `Reservation`. The two\n        `Reservation`s being merged must have same properties.\n\n        :param reservation_order_id: Order Id of the reservation\n        :type reservation_order_id: str\n        :param sources: Format of the resource id should be\n         /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}\n        :type sources: list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns list or\n         ClientRawResponse<list> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]\n        :raises:\n         :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`\n        \"\"\"", "output": "list[str]\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns Reservation or\n         ClientRawResponse<Reservation> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.reservations.models.Reservation]", "solution": "\n        raw_result = self._merge_initial(\n            reservation_order_id=reservation_order_id,\n            sources=sources,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            deserialized = self._deserialize('[ReservationResponse]', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)"}
{"prompt": "def _get_concatenation(extractors, text, *, ignore_whitespace=True):\n  \"\"\"Returns a concatenation ParseNode whose children are the nodes returned by each of the\n  methods in the extractors enumerable.\n\n  If ignore_whitespace is True, whitespace will be ignored and then attached to the child it\n  preceeded.\n  \"\"\"", "output": "\n  concatenation = ParseNode(_EMPTY_NODE, ignore_whitespace)\n  for extractor in extractors:\n    concatenation = concatenation.copy_with(extractor(text))\n  return concatenation\n\n\ndef extract_entities(text, *, ignore_whitespace=True, strip_whitespace=True):\n  \"\"\"Extracts named entities from a piece of text.\"\"\"\n  concatenation = _get_concatenation(\n      [extract_entities_chars, extract_entities_words], text, ignore_whitespace=ignore_whitespace,\n      strip_whitespace=strip_whitespace)\n  return concatenation.value\n\n\ndef extract_entities_chars(text, *, ignore_whitespace=True, strip_whitespace=True):\n  \"\"\"Extracts named entities from a piece of text using character extractors.\"\"\"\n  ", "solution": "\n  ignored_ws, use_text = _split_ignored(text, ignore_whitespace)\n\n  extractor, *remaining = extractors\n\n  child = _call_extractor(extractor, use_text)\n  child.add_ignored(ignored_ws)\n\n  # TODO: Should I set node.position = -len(text) for the case that ignored whitespace will cause\n  #       the first child's position to not be the whitespace, and therefore the concatenation's\n  #       position will be the first non-whitespace? I think not, but I'm adding this note in\n  #       case that causes an issue I'm not seeing at the moment.\n  node = ParseNode(ParseNodeType.concatenation, children=[child])\n\n  if remaining:\n    # child.consumed will include ignored whitespace, so we base the text we pass on on text rather\n    # than use_text.\n    return node.merged(_get_concatenation(remaining,\n                                          text[child.consumed:],\n                                          ignore_whitespace=ignore_whitespace))\n  else:\n    return node"}
{"prompt": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"", "output": "\n        rotation = Rotation.from_angle(angle_degrees)\n        rotation.rotate_columns(x, y, xnew, ynew, propagate_uncertainties)\n        return rotation\n\n    def add_virtual_columns_translation(self, x, y, xnew, ynew, propagate_uncertainties=False):\n        \"\"\"Translation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"\n        translation = Translation.from_angle(angle_degrees)\n        translation.translate_columns(x, y, xnew, ynew, propagate_", "solution": "\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])"}
{"prompt": "def _call(self, resource, params):\n        \"\"\"Call to get a resource.\n\n        :param method: resource to get\n        :param params: dict with the HTTP parameters needed to get\n            the given resource\n        \"\"\"", "output": "\n        LOG.debug('Calling %(method)s %(url)s with params %(params)s',\n                  {'method': resource, 'url': self._build_url(resource),\n                   'params': params})\n        req = webob.Request.blank(self._build_url(resource))\n        req.method = 'GET'\n        req.headers['Accept'] = 'application/json'\n        req.headers['Content-Type'] = 'application/json'\n        req.headers['X-Auth-Token'] = self.auth_token\n        req.body = json.dumps(params)\n        res = req.get_response(self._app)\n        LOG.debug('Response body: %s', res.body)\n        res_dict = jsonutils.loads(res.body)\n        LOG.debug('Response dict: %s', res_dict)\n        return res_dict\n\n", "solution": "\n        url = self.URL % {'base': self.base_url, 'resource': resource}\n\n        if self.api_token:\n            params[self.PKEY] = self.api_token\n\n        logger.debug(\"Redmine client requests: %s params: %s\",\n                     resource, str(params))\n\n        r = self.fetch(url, payload=params, verify=False)\n\n        return r.text"}
{"prompt": "def maintained_selection():\n    \"\"\"Maintain selection during context\n\n    Example:\n        >>> with maintained_selection():\n        ...     # Modify selection\n        ...     cmds.select('node', replace=True)\n        >>> # Selection restored\n\n    \"\"\"", "output": "\n    selection = cmds.ls(sl=True)\n    selection = set(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection)\n    selection = set(selection)\n    selection = sorted(selection", "solution": "\n\n    previous_selection = cmds.ls(selection=True)\n    try:\n        yield\n    finally:\n        if previous_selection:\n            cmds.select(previous_selection,\n                        replace=True,\n                        noExpand=True)\n        else:\n            cmds.select(deselect=True,\n                        noExpand=True)"}
{"prompt": "def get_marginal_distribution(self, index_points=None):\n    \"\"\"Compute the marginal of this GP over function values at `index_points`.\n\n    Args:\n      index_points: `float` `Tensor` representing finite (batch of) vector(s) of\n        points in the index set over which the GP is defined. Shape has the form\n        `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature\n        dimensions and must equal `kernel.feature_ndims` and `e` is the number\n        (size) of index points in each batch. Ultimately this distribution\n        corresponds to a `e`-dimensional multivariate normal. The batch shape\n        must be broadcastable with `kernel.batch_shape` and any batch dims\n        yielded by `mean_fn`.\n\n    Returns:\n      marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,\n        according to whether `index_points` consists of one or many index\n        points, respectively.\n    \"\"\"", "output": "of `index_points` is assumed to be `[b1,..., B]`.\n\n    Returns:\n      `Tensor` of shape `[b1,..., B]` where `b1 =..., bB =..., e =..., f1 =...,\n      fF =...` represents the marginal distribution of the GP at `index_points`.\n    \"\"\"\n    index_points = tf.convert_to_tensor(index_points)\n    index_points = tf.nest.flatten(index_points)\n    index_points = tf.nest.map_structure(\n        lambda i: tf.cast(i, self.dtype), index_points)\n    index_points = tf.nest.map_structure(\n        lambda i: tf.reshape(i, [-1]), index_points)\n    index_point", "solution": "\n    with self._name_scope('get_marginal_distribution'):\n      # TODO(cgs): consider caching the result here, keyed on `index_points`.\n      index_points = self._get_index_points(index_points)\n      covariance = self._compute_covariance(index_points)\n      loc = self._mean_fn(index_points)\n      # If we're sure the number of index points is 1, we can just construct a\n      # scalar Normal. This has computational benefits and supports things like\n      # CDF that aren't otherwise straightforward to provide.\n      if self._is_univariate_marginal(index_points):\n        scale = tf.sqrt(covariance)\n        # `loc` has a trailing 1 in the shape; squeeze it.\n        loc = tf.squeeze(loc, axis=-1)\n        return normal.Normal(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')\n      else:\n        scale = tf.linalg.LinearOperatorLowerTriangular(\n            tf.linalg.cholesky(_add_diagonal_shift(covariance, self.jitter)),\n            is_non_singular=True,\n            name='GaussianProcessScaleLinearOperator')\n        return mvn_linear_operator.MultivariateNormalLinearOperator(\n            loc=loc,\n            scale=scale,\n            validate_args=self._validate_args,\n            allow_nan_stats=self._allow_nan_stats,\n            name='marginal_distribution')"}
{"prompt": "def _reset(self, index, total, percentage_step, length):\n        \"\"\"Resets to the progressbar to start a new one\"\"\"", "output": "\n        self.progressbar.reset()\n        self.progressbar.update(index, total, percentage_step, length)\n\n    def _update_percentage(self, index, total, percentage_step, length):\n        \"\"\"Updates the percentage of the progressbar\"\"\"\n        self.progressbar.update(index, total, percentage_step, length)\n\n    def _update_total(self, index, total, percentage_step, length):\n        \"\"\"Updates the total of the progressbar\"\"\"\n        self.progressbar.update(index, total, percentage_step, length)\n\n    def _update_length(self, index, total, percentage_step, length):\n        \"\"\"Updates the length of the progressbar\"\"\"\n        self.progressbar.update(index, total, percentage_step, length)\n\n    def _update_percentage_step(self, index, total, percentage_step, length):\n        \"\"\"Updates the percentage step of the ", "solution": "\n        self._start_time = datetime.datetime.now()\n        self._start_index = index\n        self._current_index = index\n        self._percentage_step = percentage_step\n        self._total = float(total)\n        self._total_minus_one = total - 1\n        self._length = length\n        self._norm_factor = total * percentage_step / 100.0\n        self._current_interval = int((index + 1.0) / self._norm_factor)"}
{"prompt": "def _swap_ops_from_edge(edge, layout):\n    \"\"\"Generate list of ops to implement a SWAP gate along a coupling edge.\"\"\"", "output": "\n    edge = layout.edge(edge)\n    edge_ops = [op.name for op in edge]\n    edge_ops_map = {op: op for op in edge_ops}\n    edge_ops_map.update({\n        'Gx': 'Gx',\n        'Gy': 'Gy',\n        'Gz': 'Gz',\n        'Gxx': 'Gxx',\n        'Gxy': 'Gxy',\n        'Gxz': 'Gxz',\n        'Gyy': 'Gyy',\n        'Gyz': 'Gyz',\n        'Gzz': 'Gzz',\n        'Gxxz': 'Gxxz',\n        'Gxyz", "solution": "\n\n    device_qreg = QuantumRegister(len(layout.get_physical_bits()), 'q')\n    qreg_edge = [(device_qreg, i) for i in edge]\n\n    # TODO shouldn't be making other nodes not by the DAG!!\n    return [\n        DAGNode({'op': SwapGate(), 'qargs': qreg_edge, 'cargs': [], 'type': 'op'})\n    ]"}
{"prompt": "def frames(self, key=None, timeoutSecs=60, **kwargs):\n    if not (key is None or isinstance(key, (basestring, Key))):\n        raise Exception(\"frames: key should be string or Key type %s %s\" % (type(key), key))\n\n    params_dict = {\n        'find_compatible_models': 0,\n        'row_offset': 0, # is offset working yet?\n        'row_count': 5,\n    }\n    \"\"\"\n    Return a single Frame or all of the Frames in the h2o cluster.  The\n    frames are contained in a list called \"frames\" at the top level of the\n    result.  Currently the list is unordered.\n    TODO:\n    When find_compatible_models is implemented then the top level \n    dict will also contain a \"models\" list.\n    \"\"\"", "output": "level frame\n    will be the one with the highest model id.  This is currently not done\n    in h2o.get_frames.\n    \"\"\"\n    frames = []\n    frame_id = 0\n    frame_count = 0\n    frame_timeoutSecs = timeoutSecs\n    frame_timeout = 0\n    frame_ids = []\n    frame_model_ids = []\n    frame_models = []\n    frame_model_id = 0\n    frame_model_count = 0\n    frame_model_timeoutSecs = timeoutSecs\n    frame_model_timeout = 0\n    frame_model_frame_ids = []\n    frame_model_frames = []\n    frame_model_frame_id = 0\n    frame_model_frame_count = 0\n    frame_model_frame_timeout", "solution": "\n    h2o_methods.check_params_update_kwargs(params_dict, kwargs, 'frames', False)\n    \n    # key can be type Key? (from h2o_xl) str(key) should return\n    if key:\n        if isinstance(key, Key):\n            keyStr = key.frame\n        else:\n            keyStr = key\n        result = self.do_json_request('3/Frames.json/' + keyStr, timeout=timeoutSecs, params=params_dict)\n    else:\n        result = self.do_json_request('3/Frames.json', timeout=timeoutSecs, params=params_dict)\n    return result"}
{"prompt": "def queryGitHubFromFile(self, filePath, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub GraphQL query from a file.\n\n        Can only be used with GraphQL queries.\n        For REST queries, see the 'queryGitHub' method.\n\n        Args:\n            filePath (str): A relative or absolute path to a file containing\n                a GraphQL query.\n                File may use comments and multi-line formatting.\n                .. _GitHub GraphQL Explorer:\n                   https://developer.github.com/v4/explorer/\n            gitvars (Optional[Dict]): All query variables.\n                Defaults to empty.\n                GraphQL Only.\n            verbosity (Optional[int]): Changes output verbosity levels.\n                If < 0, all extra printouts are suppressed.\n                If == 0, normal print statements are displayed.\n                If > 0, additional status print statements are displayed.\n                Defaults to 0.\n            **kwargs: Keyword arguments for the 'queryGitHub' method.\n\n        Returns:\n            Dict: A JSON style dictionary.\n\n        \"\"\"", "output": "sity (int): Verbosity level. 0 is no output, 1 is all\n                output and 2 is only errors.\n                Defaults to 0.\n                GraphQL Only.\n            **kwargs (dict): Any additional keyword arguments to pass to the\n                GitHub GraphQL client.\n\n        Returns:\n            dict: A dictionary containing the results of the query.\n        \"\"\"\n        client = GitHubGraphQLClient(\n            gitvars=gitvars, verbosity=verbosity, **kwargs,\n        )\n        return client.queryGitHub(filePath)\n\n    def queryGitHubFromFiles(self, filePaths, gitvars={}, verbosity=0, **kwargs):\n        \"\"\"Submit a GitHub GraphQL query from a list of files.\n\n        Can ", "solution": "\n        gitquery = self._readGQL(filePath, verbose=(verbosity >= 0))\n        return self.queryGitHub(gitquery, gitvars=gitvars, verbosity=verbosity, **kwargs)"}
{"prompt": "def get_revision(self, location):\n        \"\"\"\n        Return the maximum revision for all files under a given location\n        \"\"\"", "output": "\n        location = self._rev_parse(location)\n        location = self._rev_parse_with_max_revs(location)\n        return location\n\n    def get_revision_range(self, location):\n        \"\"\"\n        Return the revision range for files under a given location\n        \"\"\"\n        location = self._rev_parse(location)\n        location = self._rev_parse_with_max_revs(location)\n        return location\n\n    def get_all_revisions(self, location):\n        \"\"\"\n        Return all revisions for files under a given location\n        \"\"\"\n        location = self._rev_parse(location)\n        location = self._rev_parse_with_max_revs(location)\n        return location\n\n    def get_all_revisions_range(self, location):\n        \"\"\"\n        Return all revision range", "solution": "\n        # Note: taken from setuptools.command.egg_info\n        revision = 0\n\n        for base, dirs, files in os.walk(location):\n            if self.dirname not in dirs:\n                dirs[:] = []\n                continue    # no sense walking uncontrolled subdirs\n            dirs.remove(self.dirname)\n            entries_fn = os.path.join(base, self.dirname, 'entries')\n            if not os.path.exists(entries_fn):\n                # FIXME: should we warn?\n                continue\n\n            dirurl, localrev = self._get_svn_url_rev(base)\n\n            if base == location:\n                base_url = dirurl + '/'   # save the root url\n            elif not dirurl or not dirurl.startswith(base_url):\n                dirs[:] = []\n                continue    # not part of the same svn tree, skip it\n            revision = max(revision, localrev)\n        return revision"}
{"prompt": "def scientific_notation_elements(self, value, locale):\n        \"\"\" Returns normalized scientific notation components of a value.\"\"\"", "output": "\n        parts = value.split('.')\n        if len(parts) == 1:\n            return value\n        parts[0] = parts[0].replace('e', 'e')\n        parts[0] = parts[0].replace('E', 'e')\n        parts[0] = parts[0].replace('E+', 'e')\n        parts[0] = parts[0].replace('E-1', 'e')\n        parts[0] = parts[0].replace('E-2', 'e')\n        parts[0] = parts[0].replace('E-3', 'e')\n        parts[0] = parts[0].replace('E-', 'e')\n        parts[0] = parts[0].replace('+', 'p')\n        parts[0] = parts[0].replace('-', 'n')\n        parts[0] = part", "solution": "\n        # Normalize value to only have one lead digit.\n        exp = value.adjusted()\n        value = value * get_decimal_quantum(exp)\n        assert value.adjusted() == 0\n\n        # Shift exponent and value by the minimum number of leading digits\n        # imposed by the rendering pattern. And always make that number\n        # greater or equal to 1.\n        lead_shift = max([1, min(self.int_prec)]) - 1\n        exp = exp - lead_shift\n        value = value * get_decimal_quantum(-lead_shift)\n\n        # Get exponent sign symbol.\n        exp_sign = ''\n        if exp < 0:\n            exp_sign = babel.numbers.get_minus_sign_symbol(locale)\n        elif self.exp_plus:\n            exp_sign = babel.numbers.get_plus_sign_symbol(locale)\n\n        # Normalize exponent value now that we have the sign.\n        exp = abs(exp)\n\n        return value, exp, exp_sign"}
{"prompt": "def difflag1(self):\n        \"\"\"\n        Conduct a diff-1 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"", "output": "\n        return self.difflag(lag=1)\n\n    def difflag2(self):\n        \"\"\"\n        Conduct a diff-2 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"\n        return self.difflag(lag=2)\n\n    def difflag3(self):\n        \"\"\"\n        Conduct a diff-3 transform on a numeric frame column.\n\n        :returns: an H2OFrame where each element is equal to the corresponding element in the source\n            frame minus the previous-row element in the same frame.\n        \"\"\"\n        return self.difflag(lag=3)\n\n    def difflag4(self):\n        \"\"\"", "solution": "\n        if self.ncols > 1:\n            raise H2OValueError(\"Only single-column frames supported\")\n        if self.types[self.columns[0]] not in {\"real\", \"int\", \"bool\"}:\n            raise H2OValueError(\"Numeric column expected\")\n        fr = H2OFrame._expr(expr=ExprNode(\"difflag1\", self), cache=self._ex._cache)\n        return fr"}
{"prompt": "async def _run_spider_hook(self, hook_func):\n        \"\"\"\n        Run hook before/after spider start crawling\n        :param hook_func: aws function\n        :return:\n        \"\"\"", "output": "\n        hook_func()\n\n    async def start_crawling(self, crawler_settings: CrawlerSettings, *, spider: Spider):\n        \"\"\"\n        Start crawling\n        :param crawler_settings: crawler's settings\n        :param spider: spider object\n        :return:\n        \"\"\"\n        spider_name = spider.name\n        spider_version = spider.version\n        spider_type = spider.__class__.__name__\n        spider_settings = crawler_settings.copy()\n        spider_settings.setdefault('USER_AGENT', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36')\n        spider_settings.setdefault('SPIDER_", "solution": "\n        if callable(hook_func):\n            try:\n                aws_hook_func = hook_func(weakref.proxy(self))\n                if isawaitable(aws_hook_func):\n                    await aws_hook_func\n            except Exception as e:\n                self.logger.error(f'<Hook {hook_func.__name__}: {e}')"}
{"prompt": "def save(self, name=None, output='png', dirc=None):\n        \"\"\"Saves Bloch sphere to file of type ``format`` in directory ``dirc``.\n        Args:\n            name (str):\n                Name of saved image. Must include path and format as well.\n                i.e. '/Users/Paul/Desktop/bloch.png'\n                This overrides the 'format' and 'dirc' arguments.\n            output (str):\n                Format of output image.\n            dirc (str):\n                Directory for output images. Defaults to current working directory.\n        \"\"\"", "output": "\n        if dirc is None:\n            dirc = self.dirc\n        if name is None:\n            name = self.name\n        if output is None:\n            output = self.output\n        if output not in self.formats:\n            raise ValueError('Output format not recognized: {}'.format(output))\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n\n        self.render()\n        if dirc:\n            if not os.path.isdir(os.getcwd() + \"/\" + str(dirc)):\n                os.makedirs(os.getcwd() + \"/\" + str(dirc))\n        if name is None:\n            if dirc:\n                self.fig.savefig(os.getcwd() + \"/\" + str(dirc) + '/bloch_' +\n                                 str(self.savenum) + '.' + output)\n            else:\n                self.fig.savefig(os.getcwd() + '/bloch_' + str(self.savenum) +\n                                 '.' + output)\n        else:\n            self.fig.savefig(name)\n        self.savenum += 1\n        if self.fig:\n            plt.close(self.fig)"}
{"prompt": "def rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some counts about it.\n     \"\"\"", "output": "\n     rejester = rejester.WorkUnit(work_unit)\n     rejester.fetch()\n     rejester.save()\n     return rejester\n\ndef rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some counts about it.\n     \"\"\"\n     rejester = rejester.WorkUnit(work_unit)\n     rejester.fetch()\n     rejester.save()\n     return rejester\n\ndef rejester_run(work_unit):\n     \"\"\"get a rejester.WorkUnit with KBA s3 path, fetch it, and save\n     some ", "solution": "\n     #fname = 'verify-chunks-%d-%d' % (os.getpid(), time.time())\n     fname = work_unit.key.strip().split('/')[-1]\n     \n     output_dir_path = work_unit.data.get('output_dir_path', '/mnt')\n     u = uuid.uuid3(uuid.UUID(int=0), work_unit.key.strip())\n     path1 = u.hex[0]\n     path2 = u.hex[1]\n     fpath = os.path.join(output_dir_path, path1, path2, fname)\n     if not os.path.exists(os.path.dirname(fpath)):\n          os.makedirs(os.path.dirname(fpath))\n\n     output = gzip.open(fpath + '-out.gz', 'wb')\n\n     expected_si_count = int(fname.split('-')[1])\n\n     max_tries = 20\n     tries = 0\n     while tries < max_tries:\n          try:\n               exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, stream_ids = \\\n                   attempt_fetch(work_unit, fpath)\n               if si_count != expected_si_count:\n                    print 'retrying because si_count = %d != %d expected_si_count' % (si_count, expected_si_count)\n                    sys.stdout.flush()\n                    tries += 1\n                    continue\n               else:\n                    print 'succeeded in reading si_count = %d' % (si_count,)\n                    sys.stdout.flush()\n               output.write( '%s\\t%d\\t%d\\t%d\\t%d\\t%s\\t%s\\n' % (\n                         exc, si_count, serif_count, clean_visible_bytes, clean_visible_count, \n                         work_unit.key.strip(), ','.join(['%s|%s' % tup for tup in stream_ids])) )\n               break\n          except Exception, exc:\n               print 'broken?'\n               print traceback.format_exc(exc)\n               sys.stdout.flush()\n               tries += 1\n               output.write(traceback.format_exc(exc))\n\n     output.close()"}
{"prompt": "def get_arguments(self):\n        \"\"\"\n        Extracts the specific arguments of this CLI\n        \"\"\"", "output": "\n        parser = argparse.ArgumentParser(\n            prog='python -m pysnptools.snpreader -h | --help',\n            description='Reads SNPs from a given file or STDIN',\n            epilog='Example: python -m pysnptools.snpreader -i <input> -o <output> --snp_type=<type> --snp_filter=<filter> --snp_min_count=<min_count> --snp_max_count=<max_count> --snp_min_length=<min_length> --snp_max_length=<max_len> --snp_min_coverage=<min_coverage> --snp_max_coverage=<max_coverage", "solution": "\n\n        AlarmModify.get_arguments(self)\n        self._alarm_id = self.args.alarm_id if self.args.alarm_id is not None else None\n        self.get_api_parameters()"}
{"prompt": "def html_tags_for(self, asset_type, *args, **kwargs):\n        \"\"\"Return html tags for urls of asset_type\n        \"\"\"", "output": "\n        asset_type = asset_type.lower()\n        if asset_type in self.tags:\n            return self.tags[asset_type]\n        else:\n            return ''\n\n    def url_for_asset(self, asset_type, *args, **kwargs):\n        \"\"\"Return url for asset_type\n        \"\"\"\n        asset_type = asset_type.lower()\n        if asset_type in self.assets:\n            return self.assets[asset_type]\n        else:\n            return ''\n\n    def url_for_asset_type(self, asset_type, *args, **kwargs):\n        \"\"\"Return url for asset_type\n        \"\"\"\n        asset_type = asset_type.lower()\n        if asset_type in self.assets:\n            return self.assets[asset_type]\n        else:\n            return ''\n\n    def url_for_asset_type_and_id(self, asset_type, *args, **kwargs):\n        \"\"\"Return url for asset_type and ", "solution": "\n        html = []\n        for ref in self.depends:\n            html.append(self._ref(ref).html_tags_for(asset_type, *args, **kwargs))\n        if asset_type in self.typed_bundles:\n            html.append(render_asset_html_tags(asset_type, self.urls_for_self(asset_type, *args, **kwargs)))\n        return \"\\n\".join(html)"}
{"prompt": "def read_key(self, key, bucket_name=None):\n        \"\"\"\n        Reads a key from S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"", "output": "\n        bucket = self.s3_client.get_bucket(bucket_name)\n        object = bucket.get_key(key)\n        return object\n\n    def write_key(self, key, bucket_name=None):\n        \"\"\"\n        Writes a key into S3\n\n        :param key: S3 key that will point to the file\n        :type key: str\n        :param bucket_name: Name of the bucket in which the file is stored\n        :type bucket_name: str\n        \"\"\"\n        bucket = self.s3_client.get_bucket(bucket_name)\n        object = bucket.new_key(key)\n        object.content_type = 'application/octet-stream'\n        object.content_encoding = 'binary'\n        object.content_language = 'en'\n        object.cache_control = 'no-cache'\n        object.content_disposition = 'attachment", "solution": "\n\n        obj = self.get_key(key, bucket_name)\n        return obj.get()['Body'].read().decode('utf-8')"}
{"prompt": "def _map_type_to_dict(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding dictionary in root.\n\n        \"\"\"", "output": "\n        type_map = {'RESULT':'result'}\n        return type_map.get(type_name, None)\n\n    def _map_type_to_type(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding instance type (e.g. 'Result').\n\n        \"\"\"\n        type_map = {'RESULT': 'Result'}\n        return type_map.get(type_name, None)\n\n    def _map_type_to_type_name(self, type_name):\n        \"\"\" Maps a an instance type representation string (e.g. 'RESULT')\n        to the corresponding instance type (e.g. 'Result').\n\n        \"\"\"\n        type_map = {'RESULT': 'Result'}\n        return type_map.get(type_name, None)\n\n    def _map_type_to_type_names(self, ", "solution": "\n        root = self._root_instance\n\n        if type_name == RESULT:\n            return root._results\n        elif type_name == PARAMETER:\n            return root._parameters\n        elif type_name == DERIVED_PARAMETER:\n            return root._derived_parameters\n        elif type_name == CONFIG:\n            return root._config\n        elif type_name == LEAF:\n            return root._other_leaves\n        else:\n            raise RuntimeError('You shall not pass!')"}
{"prompt": "def create(self, public=False, **kwargs):\n        \"\"\"Creates the device. Attempts to create private devices by default,\n        but if public is set to true, creates public devices.\n\n        You can also set other default properties by passing in the relevant information.\n        For example, setting a device with the given nickname and description::\n\n            dev.create(nickname=\"mydevice\", description=\"This is an example\")\n\n        Furthermore, ConnectorDB supports creation of a device's streams immediately,\n        which can considerably speed up device setup::\n\n            dev.create(streams={\n                \"stream1\": {\"schema\": '{\\\"type\\\":\\\"number\\\"}'}\n            })\n\n        Note that the schema must be encoded as a string when creating in this format.\n        \"\"\"", "output": "must be valid JSON, i.e. it must contain an \"type\" key.\n        \"\"\"\n        properties = kwargs.copy()\n        properties.setdefault('nickname', \"\")\n        properties.setdefault('description', \"\")\n        properties.setdefault('streams', {})\n        properties.setdefault('streams_public', False)\n        properties.setdefault('streams_private', False)\n        properties.setdefault('streams_private_invites', False)\n        properties.setdefault('streams_private_invites_invites', False)\n        properties.setdefault('streams_private_invites_invites_private', False)\n        properties.setdefault('streams_private_invites_invites_private_invites', False)\n        properties.setdefault('", "solution": "\n        kwargs[\"public\"] = public\n        self.metadata = self.db.create(self.path, kwargs).json()"}
{"prompt": "def _check_init(self, node):\n        \"\"\"check that the __init__ method call super or ancestors'__init__\n        method\n        \"\"\"", "output": "\n        call = node.call\n        if call.func.id!='super':\n            return\n        ancestors = node.ancestors\n        if len(ancestors) == 2:\n            ancestor = ancestors[0]\n            if ancestor.func.id!='super':\n                return\n            ancestor = ancestors[1]\n            if ancestor.func.id!='super':\n                return\n        if call.args[0] =='self':\n            return\n        if call.args[0] == 'cls':\n            return\n        if call.args[0] =='self':\n            return\n        if call.args[0] == 'cls':\n            return\n        if call.args[0] =='self':\n            return\n        if call.args[0] == 'cls':\n            return\n        if call.args[0] =='self':\n            return\n        if call.", "solution": "\n        if not self.linter.is_message_enabled(\n            \"super-init-not-called\"\n        ) and not self.linter.is_message_enabled(\"non-parent-init-called\"):\n            return\n        klass_node = node.parent.frame()\n        to_call = _ancestors_to_call(klass_node)\n        not_called_yet = dict(to_call)\n        for stmt in node.nodes_of_class(astroid.Call):\n            expr = stmt.func\n            if not isinstance(expr, astroid.Attribute) or expr.attrname != \"__init__\":\n                continue\n            # skip the test if using super\n            if (\n                isinstance(expr.expr, astroid.Call)\n                and isinstance(expr.expr.func, astroid.Name)\n                and expr.expr.func.name == \"super\"\n            ):\n                return\n            try:\n                for klass in expr.expr.infer():\n                    if klass is astroid.Uninferable:\n                        continue\n                    # The infered klass can be super(), which was\n                    # assigned to a variable and the `__init__`\n                    # was called later.\n                    #\n                    # base = super()\n                    # base.__init__(...)\n\n                    if (\n                        isinstance(klass, astroid.Instance)\n                        and isinstance(klass._proxied, astroid.ClassDef)\n                        and is_builtin_object(klass._proxied)\n                        and klass._proxied.name == \"super\"\n                    ):\n                        return\n                    if isinstance(klass, objects.Super):\n                        return\n                    try:\n                        del not_called_yet[klass]\n                    except KeyError:\n                        if klass not in to_call:\n                            self.add_message(\n                                \"non-parent-init-called\", node=expr, args=klass.name\n                            )\n            except astroid.InferenceError:\n                continue\n        for klass, method in not_called_yet.items():\n            cls = node_frame_class(method)\n            if klass.name == \"object\" or (cls and cls.name == \"object\"):\n                continue\n            self.add_message(\"super-init-not-called\", args=klass.name, node=node)"}
{"prompt": "def analyse(self, traj, network, current_subrun, subrun_list, network_dict):\n        \"\"\"Calculates average Fano Factor of a network.\n\n        :param traj:\n\n            Trajectory container\n\n            Expects:\n\n            `results.monitors.spikes_e`: Data from SpikeMonitor for excitatory neurons\n\n            Adds:\n\n            `results.statistics.mean_fano_factor`: Average Fano Factor\n\n        :param network:\n\n            The BRIAN network\n\n        :param current_subrun:\n\n            BrianParameter\n\n        :param subrun_list:\n\n            Upcoming subruns, analysis is only performed if subruns is empty,\n            aka the final subrun has finished.\n\n        :param network_dict:\n\n            Dictionary of items shared among componetns\n\n        \"\"\"", "output": "\n            otherwise the analysis is performed in all subruns\n\n        :param network_dict:\n\n            Dictionary containing all networks parameters\n\n        :return:\n\n            Dictionary containing all networks parameters\n\n        \"\"\"\n        network_dict['mean_fano_factor'] = []\n\n        network_dict['mean_fano_factor'].append(\n            np.mean(\n                [\n                    np.mean(\n                        [\n                            np.mean(\n                                [\n                                    np.mean(\n                                        [\n                                            np.mean(\n                                                [\n                                                    np.mean(\n                                                        [\n                                                            np.mean(\n                                                                [\n                                                                    np.mean(\n                                                                        [\n                                                                            np.mean(\n                                                                                [\n                                                                                    np.mean(\n                                                                                        [\n                                                                                            np.mean(\n                                                                                                [", "solution": "\n        #Check if we finished all subruns\n        if len(subrun_list)==0:\n            spikes_e = traj.results.monitors.spikes_e\n\n            time_window = traj.parameters.analysis.statistics.time_window\n            start_time = traj.parameters.simulation.durations.initial_run\n            end_time = start_time+traj.parameters.simulation.durations.measurement_run\n            neuron_ids = traj.parameters.analysis.statistics.neuron_ids\n\n            mean_ff = self._compute_mean_fano_factor(\n                neuron_ids, spikes_e, time_window, start_time, end_time)\n\n            traj.f_add_result('statistics.mean_fano_factor', mean_ff, comment='Average Fano '\n                                                                      'Factor over all '\n                                                                      'exc neurons')\n\n            print('R_ee: %f, Mean FF: %f' % (traj.R_ee, mean_ff))"}
{"prompt": "def _get_properties(config):\n    \"\"\"Returns a tuple of property classes and names.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"", "output": "\n    properties = []\n    for name, value in config.items():\n        if isinstance(value, str):\n            properties.append(_PropertyClass(name, value))\n        elif isinstance(value, dict):\n            properties.append(_PropertyClass(name, value))\n        else:\n            raise TypeError(\"Invalid property type %r\" % type(value))\n    return tuple(properties)\n\n\nclass Property(object):\n    \"\"\"Base class for all property classes.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"\n\n    def __init__(self, name, doc=None, getter=None, setter=None):\n        \"\"\"Initializes a property.\n\n        Args:\n          name: Name of the property.\n          doc: Optional docstring for the property.\n          getter: Optional getter function ", "solution": "\n    property_classes = {BUILTIN_PROPERTY}\n    property_names = set()  # Not returning 'property', it has its own check.\n    if config is not None:\n        property_classes.update(config.property_classes)\n        property_names.update(\n            (prop.rsplit(\".\", 1)[-1] for prop in config.property_classes)\n        )\n    return property_classes, property_names"}
{"prompt": "def _call(self, utterances_batch: list, utterances_ids: Optional[list]=None) -> list:\n        \"\"\"\n        Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent passes incoming utterances batch through skills filter,\n        agent skills, skills processor. Batch of dialog IDs can be provided, in\n        other case utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        \"\"\"", "output": "\n        responses = []\n        for utterance in utterances_batch:\n            utterance_ids = utterance.ids if utterances_ids is None else utterance_ids\n            response = self._process_utterance(utterance, utterance_ids)\n            responses.append(response)\n        return responses\n\n    def _process_utterance(self, utterance: DialogueStateTracker, utterance_ids: list) -> Response:\n        \"\"\"\n        Processes single utterance and returns a corresponding response.\n\n        Args:\n            utterance: Incoming utterance.\n            utterance_ids: List of dialog IDs corresponding to incoming utterance.\n\n        Returns:\n            response: Response corresponding to incoming utterance.\n        \"\"\"\n        utterance_ids = utterance_ids if utterance_ids is not None else utterance.dialog_ids\n", "solution": "\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n        batch_history = [self.history[utt_id] for utt_id in ids]\n        responses = []\n\n        filtered = self.skills_filter(utterances_batch, batch_history)\n\n        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):\n            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]\n\n            if skill_i_utt_indexes:\n                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]\n                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]\n                res = [(None, 0.)] * batch_size\n                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)\n\n                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):\n                    res[i] = (predicted, confidence)\n\n                responses.append(res)\n\n        responses = self.skills_processor(utterances_batch, batch_history, *responses)\n\n        return responses"}
{"prompt": "def _add_label_from_dict(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a dictionary.\n        \"\"\"", "output": "\n        if query_params is None:\n            query_params = {}\n        query_params['label'] = self.label\n        return query_params\n\n    def _add_label_to_dict(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a dictionary.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n        query_params['label'] = self.label\n        return query_params\n\n    def _add_label_from_list(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a list.\n        \"\"\"\n        if query_params is None:\n            query_params = {}\n        query_params['label'] = self.label\n        return query_params\n\n    def _add_label_to_list(self, query_params=None):\n        \"\"\"\n        Add a label to this card, from a list.\n", "solution": "\n        return self.fetch_json(\n            uri_path=self.base_uri + '/labels',\n            http_method='POST',\n            query_params=query_params or {}\n        )"}
{"prompt": "def diff_lorenz(value_array, sigma, beta, rho):\n    \"\"\"The Lorenz attractor differential equation\n\n    :param value_array: 3d array containing the x,y, and z component values.\n    :param sigma: Constant attractor parameter\n    :param beta: FConstant attractor parameter\n    :param rho: Constant attractor parameter\n\n    :return: 3d array of the Lorenz system evaluated at `value_array`\n\n    \"\"\"", "output": "\n    return value_array * np.exp(-(sigma * value_array) ** 2 / (2 * beta * rho))\n\n\ndef diff_lorenz_attractor_derivative(value_array, sigma, beta, rho):\n    \"\"\"The Lorenz attractor derivative of the Lorenz attractor\n\n    :param value_array: 3d array containing the x,y, and z component values.\n    :param sigma: Constant attractor parameter\n    :param beta: FConstant attractor parameter\n    :param rho: Constant attractor parameter\n\n    :return: 3d array of the Lorenz attractor derivative evaluated at `value_array`\n\n    \"\"\"\n    return value_array * (np.exp(-(sigma * ", "solution": "\n    diff_array = np.zeros(3)\n    diff_array[0] = sigma * (value_array[1]-value_array[0])\n    diff_array[1] = value_array[0] * (rho - value_array[2]) - value_array[1]\n    diff_array[2] = value_array[0] * value_array[1] - beta * value_array[2]\n\n    return diff_array"}
{"prompt": "def _onDeviceCommand(self, client, userdata, pahoMessage):\n        \"\"\"\n        Internal callback for gateway command messages, parses source device from topic string and\n        passes the information on to the registered device command callback\n        \"\"\"", "output": "\n        topic = pahoMessage.getTopic()\n        topic = topic.lower()\n        if topic == \"device\":\n            deviceId = pahoMessage.getPayload()\n            device = self.devices[deviceId]\n            device.onDeviceCommand(client, userdata, pahoMessage)\n        elif topic == \"register\":\n            deviceId = pahoMessage.getPayload()\n            device = self.devices[deviceId]\n            device.onRegister(client, userdata, pahoMessage)\n        elif topic == \"unregister\":\n            deviceId = pahoMessage.getPayload()\n            device = self.devices[deviceId]\n            device.onUnregister(client, userdata, pahoMessage", "solution": "\n        try:\n            command = Command(pahoMessage, self._messageCodecs)\n        except InvalidEventException as e:\n            self.logger.critical(str(e))\n        else:\n            self.logger.debug(\"Received gateway command '%s'\" % (command.command))\n            if self.deviceCommandCallback:\n                self.deviceCommandCallback(command)"}
{"prompt": "def wait_next_block_factory(app, timeout=None):\n    \"\"\"Creates a `wait_next_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a new block to appear.\n\n    :param app: the app-instance the function should work for\n    :param timeout: timeout in seconds\n    \"\"\"", "output": "\n    def wait_next_block(block):\n        if timeout is None:\n            return\n        block.wait(timeout)\n    return wait_next_block\n\n\ndef wait_next_block_factory(app, timeout=None, timeout2=None):\n    \"\"\"Creates a `wait_next_block` function, that\n    will wait `timeout` seconds (`None` = indefinitely)\n    for a new block to appear.\n\n    :param app: the app-instance the function should work for\n    :param timeout: timeout in seconds\n    :param timeout2: timeout in seconds\n    \"\"\"\n    def wait_next_block(block):\n        if timeout is None:\n            return\n        if timeout2 is None:\n            block.wait(timeout)\n        else:\n            block.wait(timeout ", "solution": "\n\n    chain = app.services.chain\n\n    # setup new block callbacks and events\n    new_block_evt = gevent.event.Event()\n\n    def _on_new_block(app):\n        log.DEV('new block mined')\n        new_block_evt.set()\n    chain.on_new_head_cbs.append(_on_new_block)\n\n    def wait_next_block():\n        bn = chain.chain.head.number\n        chain.consensus_manager.log('waiting for new block', block=bn)\n        new_block_evt.wait(timeout)\n        new_block_evt.clear()\n        if chain.chain.head.number > bn:\n            chain.consensus_manager.log('new block event', block=chain.chain.head.number)\n        elif chain.chain.head.number == bn:\n            chain.consensus_manager.log('wait_next_block timed out', block=bn)\n\n    return wait_next_block"}
{"prompt": "def _set_affiliation(self, v, load=False):\n        \"\"\"\n    Setter method for affiliation, mapped from YANG variable /universe/individual/affiliation (identityref)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_affiliation is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_affiliation() directly.\n    \"\"\"", "output": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=RestrictedClassType(base_type=six.text_type, restriction_type=\"dict_key\", restriction_arg={'NONE': {}, 'PUBLIC': {}, 'PRIVATE': {}, 'GROUP': {}, 'OTHER': {}}, int_size=16), is_leaf=True, yang_name=\"affiliation\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, namespace='http://openconfig.net/yang/platform/universe', defining_module='openconfig-platform-universe', yang_type='identityref', is_config=False)\n    except (TypeError, ValueError):\n", "solution": "\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=RestrictedClassType(\n                    base_type=unicode,\n                    restriction_type=\"dict_key\",\n                    restriction_arg={\n                        u\"napalm-star-wars:EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"EMPIRE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"napalm-star-wars:REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                        u\"REBEL_ALLIANCE\": {\n                            \"@namespace\": u\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                            \"@module\": u\"napalm-star-wars\",\n                        },\n                    },\n                ),\n                is_leaf=True,\n                yang_name=\"affiliation\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                namespace=\"https://napalm-yang.readthedocs.io/napalm-star-wars\",\n                defining_module=\"napalm-star-wars\",\n                yang_type=\"identityref\",\n                is_config=True,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": "}
{"prompt": "def run(self, cmd, start_opts=None, globals_=None, locals_=None):\n        \"\"\" Run debugger on string `cmd' using builtin function eval\n        and if that builtin exec.  Arguments `globals_' and `locals_'\n        are the dictionaries to use for local and global variables. By\n        default, the value of globals is globals(), the current global\n        variables. If `locals_' is not given, it becomes a copy of\n        `globals_'.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n        . Also see `run_eval' if what you want to run is an\n        run_eval'able expression have that result returned and\n        `run_call' if you want to debug function run_call.\n        \"\"\"", "output": "the global and local variables are updated.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n       . Also the global and local variables are updated.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are in\n        Debugger.settings which changed after an instance is created\n       . Also the global and local variables are updated.\n\n        Debugger.core.start settings are passed via optional\n        dictionary `start_opts'. Overall debugger settings are ", "solution": "\n        if globals_ is None:\n            globals_ = globals()\n        if locals_ is None:\n            locals_ = globals_\n        if not isinstance(cmd, types.CodeType):\n            self.eval_string = cmd\n            cmd = cmd+'\\n'\n            pass\n        retval = None\n        self.core.start(start_opts)\n        try:\n            retval = eval(cmd, globals_, locals_)\n        except SyntaxError:\n            try:\n                exec(cmd, globals_, locals_)\n            except DebuggerQuit:\n                pass\n            except DebuggerQuit:\n                pass\n            pass\n        except DebuggerQuit:\n            pass\n        finally:\n            self.core.stop()\n        return retval"}
{"prompt": "def make_links_absolute(self, base_url=None, resolve_base_href=True,\n                            handle_failures=None):\n        \"\"\"\n        Make all links in the document absolute, given the\n        ``base_url`` for the document (the full URL where the document\n        came from), or if no ``base_url`` is given, then the ``.base_url``\n        of the document.\n\n        If ``resolve_base_href`` is true, then any ``<base href>``\n        tags in the document are used *and* removed from the document.\n        If it is false then any such tag is ignored.\n\n        If ``handle_failures`` is None (default), a failure to process\n        a URL will abort the processing.  If set to 'ignore', errors\n        are ignored.  If set to 'discard', failing URLs will be removed.\n        \"\"\"", "output": "d, otherwise they are logged and a warning is\n        emitted.\n\n        \"\"\"\n        base_url = self.resolve_base_href(base_url, resolve_base_href)\n        base_url = self.resolve_url(base_url, resolve_base_href)\n        base_url = self.resolve_url(base_url, resolve_base_href, True)\n        base_url = self.resolve_url(base_url, resolve_base_href, False)\n        base_url = self.resolve_url(base_url, resolve_base_href, True)\n        base_url = self.resolve_url(base_url, resolve_base_href, False)\n        base_url = self.resolve_url(base_url, resolve_base_href, True)\n        base_url = self.resolve_url(base_url, resolve_base_href, False)\n        base_url = self.resolve_url(base_url, resolve_base_", "solution": "\n        if base_url is None:\n            base_url = self.base_url\n            if base_url is None:\n                raise TypeError(\n                    \"No base_url given, and the document has no base_url\")\n        if resolve_base_href:\n            self.resolve_base_href()\n\n        if handle_failures == 'ignore':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return href\n        elif handle_failures == 'discard':\n            def link_repl(href):\n                try:\n                    return urljoin(base_url, href)\n                except ValueError:\n                    return None\n        elif handle_failures is None:\n            def link_repl(href):\n                return urljoin(base_url, href)\n        else:\n            raise ValueError(\n                \"unexpected value for handle_failures: %r\" % handle_failures)\n\n        self.rewrite_links(link_repl)"}
{"prompt": "def signin_card(card: SigninCard) -> Attachment:\n        \"\"\"\n        Returns an attachment for a signin card. For channels that don't natively support signin cards an alternative\n        message will be rendered. Will raise a TypeError if 'card' argument is not a SigninCard.\n        :param card:\n        :return:\n        \"\"\"", "output": "\n        attachment = Attachment()\n        attachment.title = card.title\n        attachment.text = card.text\n        attachment.url = card.url\n        attachment.image_url = card.image_url\n        attachment.thumb_url = card.thumb_url\n        attachment.thumb_width = card.thumb_width\n        attachment.thumb_height = card.thumb_height\n        attachment.thumb_alt_text = card.thumb_alt_text\n        attachment.thumb_color = card.thumb_color\n        attachment.thumb_position = card.thumb_position\n        attachment.thumb_scale = card.thumb_scale\n        attachment.thumb_position = card.thumb_position\n        ", "solution": "\n        if not isinstance(card, SigninCard):\n            raise TypeError('CardFactory.signin_card(): `card` argument is not an instance of an SigninCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.signin_card,\n                          content=card)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Clear GDoc Spreadsheet by sending empty csv file.\n        \"\"\"", "output": "\n        self.clear()\n        self.write_csv([])\n\n    def clear(self):\n        \"\"\"\n        Clear GDoc Spreadsheet by sending empty csv file.\n        \"\"\"\n        self.clear()\n        self.write_csv([])\n\n    def write_csv(self, csv_list):\n        \"\"\"\n        Write csv list to GDoc Spreadsheet.\n        \"\"\"\n        csv_file = open(self.csv_path, \"w\")\n        csv_file.write(csv_list)\n        csv_file.close()\n\n    def write_csv_to_file(self, csv_list):\n        \"\"\"\n        Write csv list to GDoc Spreadsheet.\n        \"\"\"\n        csv_file = open(self.csv_path, \"w\")\n        csv_file.write(csv_list)\n        csv_file.close()\n\n    def write_csv(self, csv_", "solution": "\n        empty_file_path = os.path.join(self.temp_path, 'empty.csv')\n        try:\n            empty_file = open(empty_file_path, 'w')\n            empty_file.write(',')\n            empty_file.close()\n        except IOError as e:\n            raise PODocsError(e)\n\n        self._upload_file_to_gdoc(empty_file_path, content_type='text/csv')\n\n        os.remove(empty_file_path)"}
{"prompt": "def add_item_to_basket(self, item, variant=VARIANT.MEDIUM, quantity=1):\n        \"\"\"\n        Add an item to the current basket.\n\n        :param Item item: Item from menu.\n        :param int variant: Item SKU id. Ignored if the item is a side.\n        :param int quantity: The quantity of item to be added.\n        :return: A response having added an item to the current basket.\n        :rtype: requests.Response\n        \"\"\"", "output": "\n        basket_id = self.session.basket_id\n        basket_data = self.session.basket_data\n        basket_items = self.session.basket_items\n        basket_items[basket_id] = basket_items[basket_id] + quantity\n        basket_data[basket_id] = basket_data[basket_id] + quantity\n        self.session.basket_data = basket_data\n        self.session.basket_items = basket_items\n        basket_id = basket_id + 1\n        basket_data = basket_data + quantity\n        basket_items = bas", "solution": "\n        item_type = item.type\n\n        if item_type == 'Pizza':\n            return self.add_pizza_to_basket(item, variant, quantity)\n        elif item_type == 'Side':\n            return self.add_side_to_basket(item, quantity)\n        return None"}
{"prompt": "def validate_expression(self, expression):\n        \"\"\"Validate an expression (may throw Exceptions)\"\"\"", "output": "\n        try:\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()\n            expression = expression.strip()", "solution": "\n        # return self.evaluate(expression, 0, 2)\n        vars = set(self.get_column_names()) | set(self.variables.keys())\n        funcs = set(expression_namespace.keys())\n        return vaex.expresso.validate_expression(expression, vars, funcs)"}
{"prompt": "def plot(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"", "output": "\n        if fig is None:\n            fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(self.x, self.y, 'o', markersize=2)\n        ax.plot(self.x, self.y_hat, '-', color='red')\n        ax.set_xlabel('$x$')\n        ax.set_ylabel('$y$')\n        ax.set_title('Model prediction')\n        return fig\n\n    def plot_prediction(self, fig=None):\n        \"\"\"Plot visualization of model prediction.\"\"\"\n        if fig is None:\n            fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(self.x, self.y_hat, '-', color='red')\n        ax.set_xlabel('$x$')\n        ax.set_ylabel('$y$')\n        ax.set_title('Model prediction')\n        return fig\n\n    def plot_prediction_error(self, fig=None):\n        \"\"\"Plot visualiz", "solution": "\n        if not fig:\n            fig = plt.gcf()\n\n        fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n        ax = fig.add_subplot(1, 1, 1)\n\n        sorted_overlapping_ports = sorted(\n            [(p, self.results['port cycles'][p]) for p in self.machine['overlapping ports']],\n            key=lambda x: x[1])\n\n        yticks_labels = []\n        yticks = []\n        xticks_labels = []\n        xticks = []\n\n        # Plot configuration\n        height = 0.9\n\n        i = 0\n        # T_OL\n        colors = ([(254. / 255, 177. / 255., 178. / 255.)] +\n                  [(255. / 255., 255. / 255., 255. / 255.)] * (len(sorted_overlapping_ports) - 1))\n        for p, c in sorted_overlapping_ports:\n            ax.barh(i, c, height, align='center', color=colors.pop(),\n                    edgecolor=(0.5, 0.5, 0.5), linestyle='dashed')\n            if i == len(sorted_overlapping_ports) - 1:\n                ax.text(c / 2.0, i, '$T_\\mathrm{OL}$', ha='center', va='center')\n            yticks_labels.append(p)\n            yticks.append(i)\n            i += 1\n        xticks.append(sorted_overlapping_ports[-1][1])\n        xticks_labels.append('{:.1f}'.format(sorted_overlapping_ports[-1][1]))\n\n        # T_nOL + memory transfers\n        y = 0\n        colors = [(187. / 255., 255 / 255., 188. / 255.)] * (len(self.results['cycles'])) + \\\n                 [(119. / 255, 194. / 255., 255. / 255.)]\n        for k, v in [('nOL', self.results['T_nOL'])] + self.results['cycles']:\n            ax.barh(i, v, height, y, align='center', color=colors.pop())\n            ax.text(y + v / 2.0, i, '$T_\\mathrm{' + k + '}$', ha='center', va='center')\n            xticks.append(y + v)\n            xticks_labels.append('{:.1f}'.format(y + v))\n            y += v\n        yticks_labels.append('LD')\n        yticks.append(i)\n\n        ax.tick_params(axis='y', which='both', left='off', right='off')\n        ax.tick_params(axis='x', which='both', top='off')\n        ax.set_xlabel('t [cy]')\n        ax.set_ylabel('execution port')\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticks_labels)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xticks_labels, rotation='vertical')\n        ax.xaxis.grid(alpha=0.7, linestyle='--')\n        fig.savefig(self._args.ecm_plot)"}
{"prompt": "def reduceCnf(cnf):\n    \"\"\"\n    I just found a remarkably large bug in my SAT solver and found an\n    interesting solution.\n    Remove all b | -b\n    (-b | b) & (b | -a) & (-b | a) & (a | -a)\n    becomes\n    (b | -a) & (-b | a)\n\n    Remove all (-e) & (-e)\n    (-e | a) & (-e | a) & (-e | a) & (-e | a)\n    becomes\n    (-e | a)\n    (-b | b | c) becomes nothing, not (c)\n    \"\"\"", "output": "\n    cnf = cnf.copy()\n    cnf.pop('b')\n    cnf.pop('-b')\n    cnf.pop('e')\n    cnf.pop('-e')\n    cnf.pop('a')\n    cnf.pop('a')\n    cnf.pop('a')\n    return cnf\n\n\ndef reduceCnf2(cnf):\n    \"\"\"\n    Just a remarkably large bug in my SAT solver and found an\n    interesting solution.\n    Remove all b | -b\n    (-b | b) & (b | -a) & (-b | a) & (a | -a)\n    becomes\n    (b | -a) & (-b | a)\n\n", "solution": "\n    output = Cnf()\n    for x in cnf.dis:\n        dont_add = False\n        for y in x:\n            for z in x:\n                if z == -y:\n                    dont_add = True\n                    break\n            if dont_add: break\n        if dont_add: continue\n        # TODO: Is this necessary anymore? Probably not. Do statistical analysis.\n        if x not in output.dis:\n            output.dis |= frozenset([x])\n    return output"}
{"prompt": "def edit(self,index=None):\n        \"\"\"Edit a block.\n\n        If no number is given, use the last block executed.\n\n        This edits the in-memory copy of the demo, it does NOT modify the\n        original source file.  If you want to do that, simply open the file in\n        an editor and use reload() when you make changes to the file.  This\n        method is meant to let you change a block during a demonstration for\n        explanatory purposes, without damaging your original script.\"\"\"", "output": "\n        if index is None:\n            index = self.blocks.index(self.blocks[-1])\n        self.blocks[index].edit()\n\n    def execute(self):\n        \"\"\"Execute the block.\"\"\"\n        self.blocks[-1].execute()\n\n    def undo(self):\n        \"\"\"Undo the last block.\"\"\"\n        self.blocks[-1].undo()\n\n    def redo(self):\n        \"\"\"Redo the last block.\"\"\"\n        self.blocks[-2].redo()\n\n    def undo_block_under_cursor(self):\n        \"\"\"Undo the last block under the cursor.\"\"\"\n        self.blocks[-1].undo_block_under_cursor()\n\n    def redo_block_under_cursor(self):\n        \"\"\"Redo the last block under the cursor.\"\"\"\n        self.blocks[-2].redo_block_und", "solution": "\n\n        index = self._get_index(index)\n        if index is None:\n            return\n        # decrease the index by one (unless we're at the very beginning), so\n        # that the default demo.edit() call opens up the sblock we've last run\n        if index>0:\n            index -= 1\n\n        filename = self.shell.mktempfile(self.src_blocks[index])\n        self.shell.hooks.editor(filename,1)\n        new_block = file_read(filename)\n        # update the source and colored block\n        self.src_blocks[index] = new_block\n        self.src_blocks_colored[index] = self.ip_colorize(new_block)\n        self.block_index = index\n        # call to run with the newly edited index\n        self()"}
{"prompt": "def enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"", "output": "\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    return enums\n\n\ndef enum_value(*sequential, **named):\n    \"\"\"\n    Build an enum value statement\n    \"\"\"\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    return enums.values()\n\n\ndef enum_name(*sequential, **named):\n    \"\"\"\n    Build an enum name statement\n    \"\"\"\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    return enums.keys()\n\n\ndef enum(*sequential, **named):\n    \"\"\"\n    Build an enum statement\n    \"\"\"\n    enums = dict(zip(sequential, range(len(", "solution": "\n    #: build enums from parameter\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    enums['map'] = copy.copy(enums)\n    #: build reverse mapping\n    enums['rmap'] = {}\n    for key, value in enums.items():\n        if type(value) is int:\n            enums['rmap'][value] = key\n    return type('Enum', (), enums)"}
{"prompt": "def check_part(state, name, part_msg, missing_msg=None, expand_msg=None):\n    \"\"\"Return child state with name part as its ast tree\"\"\"", "output": "\n    node = state.md.states[name]\n    parts = []\n    parts.append(ast.Str(text=part_msg))\n    parts.append(ast.Str(text=expand_msg))\n    parts.append(ast.Str(text=missing_msg))\n    parts.append(node)\n    return ast.Tuple(elts=parts)\n\n\ndef check_state(state, name, msg, missing_msg=None, expand_msg=None):\n    \"\"\"Return child state with name as its ast tree\"\"\"\n    node = state.md.states[name]\n    parts = []\n    parts.append(ast.Str(text=msg))\n    parts.append(ast.Str(text=missing_msg))\n    parts.append(node)\n    return ast.Tuple(elts=parts)\n\n\n", "solution": "\n\n    if missing_msg is None:\n        missing_msg = \"Are you sure you defined the {{part}}? \"\n    if expand_msg is None:\n        expand_msg = \"Did you correctly specify the {{part}}? \"\n\n    if not part_msg:\n        part_msg = name\n    append_message = {\"msg\": expand_msg, \"kwargs\": {\"part\": part_msg}}\n\n    has_part(state, name, missing_msg, append_message[\"kwargs\"])\n\n    stu_part = state.student_parts[name]\n    sol_part = state.solution_parts[name]\n\n    assert_ast(state, sol_part, append_message[\"kwargs\"])\n\n    return part_to_child(stu_part, sol_part, append_message, state)"}
{"prompt": "def list_datasets(self, get_global_public):\n        \"\"\"\n        Lists datasets in resources. Setting 'get_global_public' to 'True'\n        will retrieve all public datasets in cloud. 'False' will get user's\n        public datasets.\n\n        Arguments:\n            get_global_public (bool): True if user wants all public datasets in\n                                      cloud. False if user wants only their\n                                      public datasets.\n\n        Returns:\n            dict: Returns datasets in JSON format\n\n        \"\"\"", "output": "\n        datasets = []\n        dataset_ids = []\n        dataset_names = []\n        dataset_descriptions = []\n        dataset_types = []\n        dataset_visibility = []\n        dataset_owners = []\n        dataset_owners_types = []\n        dataset_owners_descriptions = []\n        dataset_owners_visibility = []\n        dataset_owners_owners = []\n        dataset_owners_owners_types = []\n        dataset_owners_owners_descriptions = []\n        dataset_owners_owners_visibility = []\n        dataset_owners_owners_owners_types = []\n        dataset_owners_owners_owners_descriptions = []\n        dataset_owners_owners_owners_visibility = []\n        dataset_owners_owner", "solution": "\n        appending = \"\"\n        if get_global_public:\n            appending = \"public\"\n        url = self.url() + \"/resource/{}dataset/\".format(appending)\n        req = self.remote_utils.get_url(url)\n\n        if req.status_code is not 200:\n            raise RemoteDataNotFoundError('Could not find {}'.format(req.text))\n        else:\n            return req.json()"}
{"prompt": "def is_known_type(self, type_name):\n        \"\"\"Check if type is known to the type system.\n\n        Returns:\n            bool: True if the type is a known instantiated simple type, False otherwise\n        \"\"\"", "output": "\n        return type_name in self.known_types\n\n    def get_type(self, type_name):\n        \"\"\"Get the type of a simple type.\n\n        Args:\n            type_name (str): The name of the type\n\n        Returns:\n            str: The type name\n        \"\"\"\n        return self.known_types[type_name]\n\n    def get_type_names(self):\n        \"\"\"Get all known simple types.\n\n        Returns:\n            list: The list of type names\n        \"\"\"\n        return self.known_types.keys()\n\n    def get_type_names_without_types(self):\n        \"\"\"Get all known simple types without types.\n\n        Returns:\n            list: The list of type names without types\n        \"\"\"\n        return list(self.known_types.keys())\n\n    def get_type_names_without_types_without_types(self):\n        \"\"\"Get all known simple types without ", "solution": "\n\n        type_name = str(type_name)\n        if type_name in self.known_types:\n            return True\n\n        return False"}
{"prompt": "async def fetch_nearby(lat: float, long: float, limit: int = 10) -> Optional[List[Dict]]:\n    \"\"\"\n    Gets wikipedia articles near a given set of coordinates.\n    :raise ApiError: When there was an error connecting to the API.\n\n    todo cache\n    \"\"\"", "output": "\n    params = {\"lat\": lat, \"long\": long, \"limit\": limit}\n    async with aiohttp.ClientSession() as session:\n        async with session.get(\n            f\"{API_BASE_URL}/search?{urlencode(params)}\", timeout=DEFAULT_TIMEOUT\n        ) as response:\n            if response.status!= 200:\n                raise ApiError(response.status, await response.text())\n            json = await response.json()\n            return json.get(\"query\", [])\n\n\nclass Wikipedia:\n    \"\"\"\n    Wikipedia API wrapper.\n    \"\"\"\n\n    def __init__(self, api_key: str, api_url: str, timeout: int = DEFAULT_TIMEOUT):\n        \"\"\"\n        Initializes the Wikipedia API wrapper.\n\n        :param api_key: API key for Wikipedia API.", "solution": "\n    request_url = f\"https://en.wikipedia.org/w/api.php?action=query\" \\\n                  f\"&list=geosearch\" \\\n                  f\"&gscoord={lat}%7C{long}\" \\\n                  f\"&gsradius=10000\" \\\n                  f\"&gslimit={limit}\" \\\n                  f\"&format=json\"\n\n    async with ClientSession() as session:\n        try:\n            async with session.get(request_url) as request:\n                if request.status == 404:\n                    return None\n                data = (await request.json())[\"query\"][\"geosearch\"]\n\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n        except JSONDecodeError as dec_err:\n            logger.error(f\"Could not decode data: {dec_err}\")\n            raise ApiError(f\"Could not decode data: {dec_err}\")\n        except KeyError:\n            return None\n        else:\n            for location in data:\n                location.pop(\"ns\")\n                location.pop(\"primary\")\n            return data"}
{"prompt": "def create_symmetric_key(self, algorithm, length):\n        \"\"\"\n        Create a symmetric key.\n\n        Args:\n            algorithm(CryptographicAlgorithm): An enumeration specifying the\n                algorithm for which the created key will be compliant.\n            length(int): The length of the key to be created. This value must\n                be compliant with the constraints of the provided algorithm.\n\n        Returns:\n            dict: A dictionary containing the key data, with the following\n                key/value fields:\n                * value - the bytes of the key\n                * format - a KeyFormatType enumeration for the bytes format\n\n        Raises:\n            InvalidField: Raised when the algorithm is unsupported or the\n                length is incompatible with the algorithm.\n            CryptographicFailure: Raised when the key generation process\n                fails.\n\n        Example:\n            >>> engine = CryptographyEngine()\n            >>> key = engine.create_symmetric_key(\n            ...     CryptographicAlgorithm.AES, 256)\n        \"\"\"", "output": "\n            CryptographicError: Raised when the key creation fails.\n        \"\"\"\n        algorithm = algorithm.value\n        length = length.value\n\n        if algorithm not in self._supported_algorithms:\n            raise InvalidField(\"Unsupported algorithm: {0}\".format(algorithm))\n\n        if length < self._min_key_length:\n            raise InvalidField(\"Length of key must be at least {0} \"\n                               \"bytes\".format(self._min_key_length))\n\n        algorithm_key = self._key_factory.create_key(algorithm)\n        algorithm_key.set_key_length(length)\n        return algorithm_key\n\n    def import_key(self, algorithm, key, key_material):\n        \"\"\"\n        Import a symmetric key from a key material.\n\n        Args:\n            algorithm(Cr", "solution": "\n        if algorithm not in self._symmetric_key_algorithms.keys():\n            raise exceptions.InvalidField(\n                \"The cryptographic algorithm {0} is not a supported symmetric \"\n                \"key algorithm.\".format(algorithm)\n            )\n\n        cryptography_algorithm = self._symmetric_key_algorithms.get(algorithm)\n\n        if length not in cryptography_algorithm.key_sizes:\n            raise exceptions.InvalidField(\n                \"The cryptographic length ({0}) is not valid for \"\n                \"the cryptographic algorithm ({1}).\".format(\n                    length, algorithm.name\n                )\n            )\n\n        self.logger.info(\n            \"Generating a {0} symmetric key with length: {1}\".format(\n                algorithm.name, length\n            )\n        )\n\n        key_bytes = os.urandom(length // 8)\n        try:\n            cryptography_algorithm(key_bytes)\n        except Exception as e:\n            self.logger.exception(e)\n            raise exceptions.CryptographicFailure(\n                \"Invalid bytes for the provided cryptographic algorithm.\")\n\n        return {'value': key_bytes, 'format': enums.KeyFormatType.RAW}"}
{"prompt": "def _get_organisations(self):\n        \"\"\"\n        :returns: list of organisations, sorted alphabetically\n        :rtype: list(list(str))\n        \"\"\"", "output": "\n        organisations = []\n        for organisation in self.organisations:\n            organisations.append(organisation.name)\n        organisations = sorted(organisations)\n        return organisations\n\n    def _get_organisations_without_duplicates(self):\n        \"\"\"\n        :returns: list of organisations, without duplicates\n        :rtype: list(list(str))\n        \"\"\"\n        organisations = []\n        for organisation in self.organisations:\n            if organisation not in organisations:\n                organisations.append(organisation.name)\n        organisations = sorted(organisations)\n        return organisations\n\n    def _get_organisations_without_duplicates_without_duplicates(self):\n        \"\"\"\n        :returns: list of organisations, without duplicates, without duplicates\n        :rtype: ", "solution": "\n        organisations = []\n        for child in self.vcard.getChildren():\n            if child.name == \"ORG\":\n                organisations.append(child.value)\n        return sorted(organisations)"}
{"prompt": "def clear(self):\n        \"\"\"\n        Completely resets the database. This means that all information in\n        the local cache and on disc will be erased.\n        \"\"\"", "output": "\n        self.db.clear()\n        self.db._clear_cache()\n        self.db._clear_cache()\n\n    def commit(self):\n        \"\"\"\n        Commits all changes to the database. This will discard all changes\n        to the database and start a new one.\n        \"\"\"\n        self.db.commit()\n\n    def rollback(self):\n        \"\"\"\n        Rolls back the changes to the database. This will discard all changes\n        to the database and start a new one.\n        \"\"\"\n        self.db.rollback()\n\n    def cursor(self):\n        \"\"\"\n        Returns a cursor over the database connection.\n        \"\"\"\n        return self.db.cursor()\n\n    def commit(self):\n        \"\"\"\n        Commits all changes to the database. This will discard all changes\n        to the database ", "solution": "\n        if not os.path.isdir(self.fdir):\n            os.makedirs(self.fdir, exist_ok=True)\n            return\n\n        for f in os.listdir(self.fdir):\n            del self[f]"}
{"prompt": "def reset_selective(self, regex=None):\n        \"\"\"Clear selective variables from internal namespaces based on a\n        specified regular expression.\n\n        Parameters\n        ----------\n        regex : string or compiled pattern, optional\n            A regular expression pattern that will be used in searching\n            variable names in the users namespaces.\n        \"\"\"", "output": "\n        if regex is None:\n            self.selective = {}\n        else:\n            self.selective = selective.selective(regex)\n\n    def selective_update(self, selective):\n        \"\"\"Update selective variables from internal namespaces based on a\n        selective dictionary.\n\n        Parameters\n        ----------\n        selective : dictionary\n            A selective dictionary that will be used in searching\n            variable names in the users namespaces.\n        \"\"\"\n        self.selective = selective\n\n    def selective_clear(self):\n        \"\"\"Clear selective variables from internal namespaces.\"\"\"\n        self.selective = {}\n\n    def selective_add(self, selective):\n        \"\"\"Add selective variables to internal namespaces.\n\n        Parameters", "solution": "\n        if regex is not None:\n            try:\n                m = re.compile(regex)\n            except TypeError:\n                raise TypeError('regex must be a string or compiled pattern')\n            # Search for keys in each namespace that match the given regex\n            # If a match is found, delete the key/value pair.\n            for ns in self.all_ns_refs:\n                for var in ns:\n                    if m.search(var):\n                        del ns[var]"}
{"prompt": "def load_from_path(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"", "output": "\n    spec = load_spec(path)\n    if spec.is_dir:\n        spec = load_from_dir(path)\n    return spec\n\n\ndef load_from_dir(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"\n    specs = []\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            path = os.path.join(root, file)\n            spec = load_spec(path)\n            specs.append(spec)\n    return specs\n\n\ndef load_spec(path):\n    \"\"\"\n    Load a spec from a given path, discovering specs if a directory is given.\n\n    \"\"\"\n    spec = load_spec_from_file(path)\n    if spec.is_dir:\n        spec = load_", "solution": "\n\n    if os.path.isdir(path):\n        paths = discover(path)\n    else:\n        paths = [path]\n\n    for path in paths:\n        name = os.path.basename(os.path.splitext(path)[0])\n        imp.load_source(name, path)"}
{"prompt": "def _has_role(self, role_name_or_list):\n        \"\"\"\n        Whether the user has this role name\n        \"\"\"", "output": "\n        if isinstance(role_name_or_list, basestring):\n            role_name_or_list = [role_name_or_list]\n        for role_name in role_name_or_list:\n            if role_name in self.roles:\n                return True\n        return False\n\n    def _has_permission(self, permission_name_or_list):\n        \"\"\"\n        Whether the user has this permission name\n        \"\"\"\n        if isinstance(permission_name_or_list, basestring):\n            permission_name_or_list = [permission_name_or_list]\n        for permission_name in permission_name_or_list:\n            if permission_name in self.permissions:\n                return True\n        return False\n\n    def _has_permissions(self, permission_name_or_list):\n        \"\"\"\n        Whether the user has this permissions name\n        \"\"\"\n        if isinstance(permission_name_or_list, basestring):\n            permission_name_or_list = [", "solution": "\n        if not isinstance(role_name_or_list, list):\n            role_name_or_list = [role_name_or_list]\n        return any(\n            [r.name in role_name_or_list for r in self.get_user_roles()])"}
{"prompt": "def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"", "output": "\n        future = executor.submit(task, tag=tag)\n        return future\n\n    def submit(self, executor, task, tag=None, callback=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :type callback: s3transfer.futures.Callback\n        :param callback: A callback to invoke when the task is complete\n        \"\"\"\n        future = executor.submit(task, ", "solution": "\n        logger.debug(\n            \"Submitting task %s to executor %s for transfer request: %s.\" % (\n                task, executor, self.transfer_id)\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future))\n        return future"}
{"prompt": "def _get_streams(self):\n        \"\"\"\n        Find the streams for OlympicChannel\n        :return:\n        \"\"\"", "output": "\n        stream_url = self.config['stream_url']\n        stream_data = requests.get(stream_url)\n        stream_json = stream_data.json()\n        stream_data = stream_json['streams']\n        stream_data = sorted(stream_data, key=lambda x: x['stream_id'])\n        return stream_data\n\n    def _update_streams(self):\n        \"\"\"\n        Update the streams for OlympicChannel\n        :return:\n        \"\"\"\n        stream_url = self.config['stream_url']\n        stream_data = requests.get(stream_url)\n        stream_json = stream_data.json()\n        stream_data = stream_json['streams']\n        stream_data = sorted(stream_data, key=lambda x: x['stream_id'])\n        for stream in stream_data:\n            self.stream_manager.update_stream(stream['stream", "solution": "\n        match = self._url_re.match(self.url)\n        type_of_stream = match.group('type')\n        lang = re.search(r\"/../\", self.url).group(0)\n\n        if type_of_stream == 'tv':\n            path = re.search(r\"tv/.*-\\d/$\", self.url).group(0)\n\n            return self._get_live_streams(lang, path)\n        elif type_of_stream == 'playback':\n            path = re.search(r\"/playback/.*/$\", self.url).group(0)\n            return self._get_vod_streams()"}
{"prompt": "def compute_ffmc2d(X):\n    \"\"\"Computes the 2D-Fourier Magnitude Coefficients.\"\"\"", "output": "\n    N, D = X.shape\n    \n    # Compute the FFT of the data\n    fft = np.fft.fft2(X)\n    \n    # Compute the 2D FFT of the FFT\n    fft2 = np.fft.fftshift(fft)\n    \n    # Compute the 2D FFT of the FFT\n    fft3 = np.fft.fftshift(fft2)\n    \n    # Compute the 2D FFT of the FFT\n    fft4 = np.fft.fftshift(fft3)\n    \n    # Compute the 2D FFT of the FFT\n    fft5 = np.fft.fftshift(fft4)\n    \n    # Compute the 2D FFT of the FFT\n    fft6 = np.fft.", "solution": "\n    # 2d-fft\n    fft2 = scipy.fftpack.fft2(X)\n\n    # Magnitude\n    fft2m = magnitude(fft2)\n\n    # FFTshift and flatten\n    fftshift = scipy.fftpack.fftshift(fft2m).flatten()\n\n    #cmap = plt.cm.get_cmap('hot')\n    #plt.imshow(np.log1p(scipy.fftpack.fftshift(fft2m)).T, interpolation=\"nearest\",\n    #    aspect=\"auto\", cmap=cmap)\n    #plt.show()\n\n    # Take out redundant components\n    return fftshift[:fftshift.shape[0] // 2 + 1]"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        prompt = '> '\n        ", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def get_service_certificate(self, service_name, thumbalgorithm, thumbprint):\n        \"\"\"\n        Returns the public data for the specified X.509 certificate associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        thumbalgorithm:\n            The algorithm for the certificate's thumbprint.\n        thumbprint:\n            The hexadecimal representation of the thumbprint.\n        \"\"\"", "output": "\n        service = self.services.get(service_name)\n        if not service:\n            raise ServiceNotFound(service_name)\n\n        cert = service['certificates'][0]\n        public_data = cert['publicData']\n        public_data['thumbprint'] = thumbprint\n        public_data['thumbprintAlgorithm'] = thumbalgorithm\n        return public_data\n\n    def get_service_certificates(self, service_name):\n        \"\"\"\n        Returns the public data for the specified X.509 certificates associated\n        with a hosted service.\n\n        service_name:\n            Name of the hosted service.\n        \"\"\"\n        service = self.services.get(service_name)\n        if not service:\n            raise ServiceNotFound(service_name)\n\n        certs = service['certificates']\n", "solution": "\n        _validate_not_none('service_name', service_name)\n        _validate_not_none('thumbalgorithm', thumbalgorithm)\n        _validate_not_none('thumbprint', thumbprint)\n        return self._perform_get(\n            '/' + self.subscription_id + '/services/hostedservices/' +\n            _str(service_name) + '/certificates/' +\n            _str(thumbalgorithm) + '-' + _str(thumbprint) + '',\n            Certificate)"}
{"prompt": "def get_default_args(func):\n    \"\"\"\n    returns a dictionary of arg_name:default_values for the input function\n    \"\"\"", "output": "\n    func_name = func.__name__\n    func_args = func.__code__.co_varnames\n    func_defaults = func.__defaults__\n    func_kwdefaults = func.__kwdefaults__\n    func_annotations = func.__annotations__\n    func_kwannotations = func.__kwannotations__\n    func_closure = func.__closure__\n    func_code = func.__code__\n    func_defaults = func.__defaults__\n    func_kwdefaults = func.__kwdefaults__\n    func_annotations = func.__annotations__\n    func_kwannotations = func.__kwannotations__\n    func_closure = func.__closure__\n    func_code = func.", "solution": "\n    args, _, _, defaults, *rest = inspect.getfullargspec(func)\n    return dict(zip(reversed(args), reversed(defaults)))"}
{"prompt": "def index():\n    \"\"\"Display a list of all user institutes.\"\"\"", "output": "\n    institutes = Institute.objects.all()\n    return render(request, 'institutes/index.html', {'institutes': institutes})\n\n\n@login_required\ndef institute(request, institute_id):\n    \"\"\"Display a specific institute.\"\"\"\n    institute = Institute.objects.get(id=institute_id)\n    institutes = Institute.objects.all()\n    return render(request, 'institutes/institute.html', {'institute': institute, 'institutes': institutes})\n\n\n@login_required\ndef institutes(request):\n    \"\"\"Display a list of all institutes.\"\"\"\n    inst", "solution": "\n    institute_objs = user_institutes(store, current_user)\n    institutes_count = ((institute_obj, store.cases(collaborator=institute_obj['_id']).count())\n                        for institute_obj in institute_objs if institute_obj)\n    return dict(institutes=institutes_count)"}
{"prompt": "def _plotting(self, rank_metric, results, graph_num, outdir, \n                  format, figsize, pheno_pos='', pheno_neg=''):\n        \"\"\" Plotting API.\n            :param rank_metric: sorted pd.Series with rankings values.\n            :param results: self.results\n            :param data: preprocessed expression table\n\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        \n        # no values need to be returned\n        if self._outdir is None: return\n        #Plotting\n        top_term = self.res2d.index[:graph_num]\n        # multi-threading\n        pool = Pool(self._processes)\n        for gs in top_term:\n            hit = results.get(gs)['hits_indices']\n            NES = 'nes' if self.module != 'ssgsea' else 'es'\n            term = gs.replace('/','_').replace(\":\",\"_\")\n            outfile = '{0}/{1}.{2}.{3}'.format(self.outdir, term, self.module, self.format)\n            # gseaplot(rank_metric=rank_metric, term=term, hits_indices=hit,\n            #           nes=results.get(gs)[NES], pval=results.get(gs)['pval'], \n            #           fdr=results.get(gs)['fdr'], RES=results.get(gs)['RES'],\n            #           pheno_pos=pheno_pos, pheno_neg=pheno_neg, figsize=figsize,\n            #           ofname=outfile)\n            pool.apply_async(gseaplot, args=(rank_metric, term, hit, results.get(gs)[NES],\n                                              results.get(gs)['pval'],results.get(gs)['fdr'],\n                                              results.get(gs)['RES'],\n                                              pheno_pos, pheno_neg, \n                                              figsize, 'seismic', outfile))\n            if self.module == 'gsea':\n                outfile2 = \"{0}/{1}.heatmap.{2}\".format(self.outdir, term, self.format)\n                # heatmap(df=self.heatmat.iloc[hit, :], title=term, ofname=outfile2, \n                #         z_score=0, figsize=(self._width, len(hit)/2))\n                pool.apply_async(heatmap, args=(self.heatmat.iloc[hit, :], 0, term, \n                                               (self._width, len(hit)/2+2), 'RdBu_r',\n                                                True, True, outfile2))\n        pool.close()\n        pool.join()"}
{"prompt": "def get(self, requestId):\n        \"\"\"\n        Gets details of a device management request.\n        It accepts requestId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"", "output": "\n        uri = '/deviceManagement/requests/' + requestId\n        return self._get(uri)\n\n    def list(self, requestId=None, pageSize=None, pageNumber=None):\n        \"\"\"\n        Lists device management requests.\n        It accepts requestId (string) as parameters\n        In case of failure it throws APIException\n        \"\"\"\n        uri = '/deviceManagement/requests'\n        if requestId:\n            uri += '/' + requestId\n        return self._get(uri, pageSize, pageNumber)\n\n    def listByDeviceId(self, deviceId):\n        \"\"\"\n        Lists device management requests by device id.\n        It accepts deviceId (string) as parameters\n        In case of failure it throws API", "solution": "\n        url = MgmtRequests.mgmtSingleRequest % (requestId)\n        r = self._apiClient.get(url)\n\n        if r.status_code == 200:\n            return r.json()\n        else:\n            raise ApiException(r)"}
{"prompt": "def load(self, reload=False):\n        \"\"\"\n        Load the config and defaults from files.\n        \"\"\"", "output": "\n        self.config = {}\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.config_path))\n        self.config.update(load_config(self.", "solution": "\n        if reload or not self._loaded:\n            # load defaults\n            if self._defaults_file and type(self._defaults_file) == str:\n                self._defaults_file = File(self._defaults_file, parent=self._parent)\n            defaults = {}\n            if self._defaults_file:\n                defaults = yaml.safe_load(self._defaults_file.read().replace('\\t', '    '))\n\n            # load data\n            data = {}\n            if self.exists:\n                data = yaml.safe_load(self.read().replace('\\t', '    '))\n\n            # initialise with the loaded data\n            self._defaults = defaults\n            self._data = copy.deepcopy(self._defaults)\n            self.update(data=data)\n\n            # if specified, apply environment variables\n            if self._apply_env:\n                self.update(ConfigEnv(self._env_prefix))\n\n            self._loaded = True\n\n        return self"}
{"prompt": "def walk_files(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr``.\n    \"\"\"", "output": "\n    for root, dirs, files in os.walk(mgr):\n        for f in files:\n            yield os.path.join(root, f)\n\n\ndef walk_files_recursive(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr`` and recurse into subdirectories.\n    \"\"\"\n    for root, dirs, files in os.walk(mgr):\n        for f in files:\n            yield os.path.join(root, f)\n\n\ndef walk_files_recursive_dirs(mgr):\n    \"\"\"\n    Iterate over all files visible to ``mgr`` and recurse into subdirectories.\n    \"\"\"\n    for root, dirs, files in os.walk(mgr):\n        for f in files:\n            yield os.path.join(root, f)\n\n\ndef walk_file", "solution": "\n    for dir_, subdirs, files in walk_files(mgr):\n        for file_ in files:\n            yield file_"}
{"prompt": "def data_types(self):\n        \"\"\"Return a list of data types.\"\"\"", "output": "\n        return ['uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n\n    def data_type(self, dtype):\n        \"\"\"Return a numpy data type from a data type.\"\"\"\n        if dtype == 'uint8':\n            return np.uint8\n        elif dtype == 'uint16':\n            return np.uint16\n        elif dtype == 'uint32':\n            return np.uint32\n        elif dtype == 'uint64':\n            return np.uint64\n        elif dtype == 'int8':\n            return np.int8\n        elif dtype == 'int16':\n            return np.int16\n        elif dtype == 'int32':\n            return np.int32\n        elif dtype == 'int64':\n            return np.int64\n        elif dtype == 'float32':\n            return np.float32\n", "solution": "\n        data = self.gencloud.project_data(self.id)\n        return sorted(set(d.type for d in data))"}
{"prompt": "def ProductsForm(category, products):\n    \"\"\" Produces an appropriate _ProductsForm subclass for the given render\n    type. \"\"\"", "output": "\n    products_form = ProductsFormClass(category, products)\n    products_form.fields['price'].widget.attrs.update({'class': 'price'})\n    return products_form\n\n\nclass ProductsFormClass(forms.ModelForm):\n    \"\"\" A ModelForm for editing Products. \"\"\"\n    price = forms.DecimalField(max_digits=10, decimal_places=2)\n\n    class Meta:\n        model = Product\n        fields = ('name', 'price', 'description')\n\n    def __init__(self, *args, **kwargs):\n        super(ProductsFormClass, self).__init__(*args, **kwargs)\n        self.fields['price'].widget.attrs.update({'class': 'price'})\n\n    def clean(self):\n        cleaned_data = super(ProductsFormClass, self).clean()\n        price = cleaned_data.get('price')\n        if price:\n            try:", "solution": "\n\n    # Each Category.RENDER_TYPE value has a subclass here.\n    cat = inventory.Category\n    RENDER_TYPES = {\n        cat.RENDER_TYPE_QUANTITY: _QuantityBoxProductsForm,\n        cat.RENDER_TYPE_RADIO: _RadioButtonProductsForm,\n        cat.RENDER_TYPE_ITEM_QUANTITY: _ItemQuantityProductsForm,\n        cat.RENDER_TYPE_CHECKBOX: _CheckboxProductsForm,\n    }\n\n    # Produce a subclass of _ProductsForm which we can alter the base_fields on\n    class ProductsForm(RENDER_TYPES[category.render_type]):\n        pass\n\n    products = list(products)\n    products.sort(key=lambda prod: prod.order)\n\n    ProductsForm.set_fields(category, products)\n\n    if category.render_type == inventory.Category.RENDER_TYPE_ITEM_QUANTITY:\n        ProductsForm = forms.formset_factory(\n            ProductsForm,\n            formset=_ItemQuantityProductsFormSet,\n        )\n\n    return ProductsForm"}
{"prompt": "def chop(array, epsilon=1e-10):\n    \"\"\"\n    Truncate small values of a complex array.\n\n    Args:\n        array (array_like): array to truncte small values.\n        epsilon (float): threshold.\n\n    Returns:\n        np.array: A new operator with small values set to zero.\n    \"\"\"", "output": "\n    array = np.atleast_1d(array)\n    array_real = np.real(array)\n    array_imag = np.imag(array)\n    array_real[array_real < epsilon] = 0.0\n    array_imag[array_imag < epsilon] = 0.0\n    return array_real + array_imag * 1j\n\n\ndef complex_to_real(complex_array):\n    \"\"\"\n    Convert complex array into real array.\n\n    Args:\n        complex_array (array_like): complex array to convert.\n\n    Returns:\n        np.array: real array of the complex array.\n    \"\"\"\n    complex_array = np.atleast_1d(complex_array)\n    complex_array_real = np.real(complex_array)\n    complex_array_imag = np.imag(", "solution": "\n    ret = np.array(array)\n\n    if np.isrealobj(ret):\n        ret[abs(ret) < epsilon] = 0.0\n    else:\n        ret.real[abs(ret.real) < epsilon] = 0.0\n        ret.imag[abs(ret.imag) < epsilon] = 0.0\n    return ret"}
{"prompt": "def make_clean_html(raw, stream_item=None, encoding=None):\n    \"\"\"Get a clean text representation of presumed HTML.\n\n    Treat `raw` as though it is HTML, even if we have no idea what it\n    really is, and attempt to get a properly formatted HTML document\n    with all HTML-escaped characters converted to their unicode.\n\n    This is called below by the `clean_html` transform stage, which\n    interprets MIME-type.  If `character_encoding` is not provided,\n    and `stream_item` is provided, then this falles back to\n    :attr:`streamcorpus.StreamItem.body.encoding`.\n\n    :param str raw: raw text to clean up\n    :param stream_item: optional stream item with encoding metadata\n    :type stream_item: :class:`streamcorpus.StreamItem`\n    :returns: UTF-8-encoded byte string of cleaned HTML text\n    :returntype: :class:`str`\n\n    \"\"\"", "output": ": stream item to clean up\n    :param str encoding: the character encoding of `raw`\n    :return: a clean text representation of `raw`\n    :rtype: str\n    \"\"\"\n    clean = raw.decode(encoding or 'utf-8')\n    clean = clean.encode('ascii', 'ignore')\n    clean = clean.decode(stream_item.body.encoding or 'utf-8')\n    return clean\n\n\ndef clean_html(raw, stream_item=None, encoding=None):\n    \"\"\"Clean up presumed HTML.\n\n    Treat `raw` as though it is HTML, even if we have no idea what it\n    really is, and attempt to get a properly formatted HTML document\n    with all HTML-escaped characters converted to their ", "solution": "\n    # Fix emails by protecting the <,> from HTML\n    raw = fix_emails(raw)\n    raw_decoded = nice_decode(raw, stream_item=stream_item, encoding=encoding)\n    if raw_decoded is None:\n        # give up on decoding it... maybe this should use force_unicode\n        raw_decoded = raw\n\n    # default attempt uses vanilla lxml.html\n    try:\n        root = lxml.html.document_fromstring(raw_decoded)\n    except ValueError, exc:\n        if 'with encoding declaration' in str(exc):\n            root = lxml.html.document_fromstring(raw)\n        else:\n            raise\n\n    # While we have the document parsed as a DOM, let's strip attributes.\n    # (The HTML cleaner seems to only support whitelisting attributes.\n    # As of now, we just want to blacklist a few.)\n    lxml.etree.strip_attributes(root, 'class', 'id')\n\n    # if that worked, then we will be able to generate a\n    # valid HTML string\n    fixed_html = lxml.html.tostring(root, encoding=unicode)\n\n    # remove any ^M characters\n    fixed_html = string.replace(fixed_html, '\\r', ' ')\n\n    # We drop utf8 characters that are above 0xFFFF as\n    # Lingpipe seems to be doing the wrong thing with them.\n    fixed_html = drop_invalid_and_upper_utf8_chars(fixed_html)\n\n    # construct a Cleaner that removes any ``<script>`` tags,\n    # Javascript, like an ``onclick`` attribute, comments, style\n    # tags or attributes, ``<link>`` tags\n    cleaner = lxml.html.clean.Cleaner(\n        scripts=True, javascript=True,\n        comments=True,\n        # do not remove <html> <head> <title> etc\n        page_structure=False,\n        remove_tags=['base'],\n        style=True, links=True)\n\n    # now get the really sanitized HTML\n    _clean_html = cleaner.clean_html(fixed_html)\n\n    # generate pretty HTML in utf-8\n    _clean_html = lxml.html.tostring(\n        lxml.html.document_fromstring(_clean_html),\n        method='html', encoding='utf-8',\n        pretty_print=True,\n        # include_meta_content_type=True\n        )\n\n    return uniform_html(_clean_html)"}
{"prompt": "def func(self):\n        \"\"\"Alternative naming, you can use `node.func.name` instead of `node.f_func`\"\"\"", "output": "\n        return self.name\n\n    @property\n    def lineno(self):\n        \"\"\"Line number of the node in the source code\"\"\"\n        return self.lineno\n\n    @property\n    def col_offset(self):\n        \"\"\"Column number of the node in the source code\"\"\"\n        return self.col_offset\n\n    @property\n    def end_lineno(self):\n        \"\"\"Line number of the end of the node\"\"\"\n        return self.end_lineno\n\n    @property\n    def end_col_offset(self):\n        \"\"\"Column number of the end of the node\"\"\"\n        return self.end_col_offset\n\n    @property\n    def end_pos(self):\n        \"\"\"Position of the end of the node\"\"\"\n        return self.end_pos\n\n    @property\n    def end_col_offset(self):\n        \"\"\"Position of the end of the node\"\"\"\n        return self.end_col_offset\n\n    @property\n    def end_lineno(self):", "solution": "\n        if self._func is None:\n            self._func = NNTreeNodeFunc(self)\n        return self._func"}
{"prompt": "def load_java_messages_to_ignore():\n    \"\"\"\n    Load in pickle file that contains dict structure with bad java messages to ignore per unit test\n    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.\n\n    :return:\n    \"\"\"", "output": "\n    g_ok_java_messages = load_pickle(\"g_ok_java_messages\")\n\n\ndef load_java_messages_for_report(unittest_report):\n    \"\"\"\n    Load in pickle file that contains dict structure with bad java messages to ignore per unit test\n    or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.\n\n    :param unittest_report:\n    :return:\n    \"\"\"\n    g_ok_java_messages = load_pickle(\"g_ok_java_messages\")\n    g_ok_java_messages[unittest_report.test_name] = load_pickle(\n        \"g_ok_java_messages_for_report_{}\".format(unittest_report.test_name)", "solution": "\n    global g_ok_java_messages\n    global g_java_message_pickle_filename\n\n    if os.path.isfile(g_java_message_pickle_filename):\n        with open(g_java_message_pickle_filename,'rb') as tfile:\n            g_ok_java_messages = pickle.load(tfile)\n    else:\n        g_ok_java_messages[\"general\"] = []"}
{"prompt": "def rowCount(self, parent):\n        \"\"\"Return number of children *parent* index has.\"\"\"", "output": "\n        return parent.rowCount()\n\n    def columnCount(self, parent):\n        \"\"\"Return number of children *parent* index has.\"\"\"\n        return parent.columnCount()\n\n    def data(self, index, role=Qt.DisplayRole):\n        \"\"\"\n        Return the data at the given index.\n\n        role = Qt.DisplayRole - return data as text\n        role = Qt.EditRole - return data as formatted text\n        role = Qt.DecorationRole - return data as an icon\n        \"\"\"\n        if not index.isValid():\n            return None\n\n        column = index.column()\n        row = index.row()\n\n        if role == Qt.DisplayRole:\n            child = self.itemAt(index)\n            return child.data(", "solution": "\n        if parent.column() > 0:\n            return 0\n\n        if parent.isValid():\n            item = parent.internalPointer()\n        else:\n            item = self.root\n\n        return len(item.children)"}
{"prompt": "def sh(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (integer) and OUTPUT (string)\n    \"\"\"", "output": "\n    process = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)\n    output, returncode = process.communicate()\n    if returncode:\n        raise CalledProcessError(returncode, cmd)\n    return returncode, output\n\n\ndef shp(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (integer) and OUTPUT (string)\n    \"\"\"\n    process = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)\n    output, returncode = process.communicate()\n    if returncode:\n        raise CalledProcessError(returncode, cmd)\n    return returncode, output\n\n\ndef shpd(cmd, escape=True):\n    \"\"\" Executes the given command.\n    returns a 2-tuple with returncode (", "solution": "\n\n    if escape:\n        cmd = quote(cmd)\n\n    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n    output, unused_err = process.communicate()\n    retcode = process.poll()\n\n    return (retcode, output)"}
{"prompt": "def abort(self, jobs=None, targets=None, block=None):\n        \"\"\"Abort specific jobs from the execution queues of target(s).\n\n        This is a mechanism to prevent jobs that have already been submitted\n        from executing.\n\n        Parameters\n        ----------\n\n        jobs : msg_id, list of msg_ids, or AsyncResult\n            The jobs to be aborted\n            \n            If unspecified/None: abort all outstanding jobs.\n\n        \"\"\"", "output": "\n        if jobs is None:\n            jobs = self.jobs\n        elif isinstance(jobs, AsyncResult):\n            jobs = jobs.jobs\n        else:\n            jobs = [jobs]\n        \n        if targets is None:\n            targets = self.targets\n        elif isinstance(targets, AsyncResult):\n            targets = targets.targets\n        else:\n            targets = [targets]\n        \n        aborted = []\n        for job in jobs:\n            if job.is_aborted:\n                continue\n            \n            job.abort()\n            aborted.append(job)\n        \n        for target in targets:\n            if target.is_aborted:\n                continue\n            \n            target.abort()\n            aborted.append(target)\n", "solution": "\n        block = self.block if block is None else block\n        jobs = jobs if jobs is not None else list(self.outstanding)\n        targets = self._build_targets(targets)[0]\n        \n        msg_ids = []\n        if isinstance(jobs, (basestring,AsyncResult)):\n            jobs = [jobs]\n        bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)\n        if bad_ids:\n            raise TypeError(\"Invalid msg_id type %r, expected str or AsyncResult\"%bad_ids[0])\n        for j in jobs:\n            if isinstance(j, AsyncResult):\n                msg_ids.extend(j.msg_ids)\n            else:\n                msg_ids.append(j)\n        content = dict(msg_ids=msg_ids)\n        for t in targets:\n            self.session.send(self._control_socket, 'abort_request',\n                    content=content, ident=t)\n        error = False\n        if block:\n            self._flush_ignored_control()\n            for i in range(len(targets)):\n                idents,msg = self.session.recv(self._control_socket,0)\n                if self.debug:\n                    pprint(msg)\n                if msg['content']['status'] != 'ok':\n                    error = self._unwrap_exception(msg['content'])\n        else:\n            self._ignored_control_replies += len(targets)\n        if error:\n            raise error"}
{"prompt": "def format_execution_time(self, start=None, end=None):\n        \"\"\"\n        Format the calculated time into a human readable format.\n\n        :param start: A starting time.\n        :type start: int|str\n\n        :param stop: A ending time.\n        :type stop: int|str\n\n        :return: A human readable date.\n        :rtype: str\n        \"\"\"", "output": "\n        start = start or 0\n        stop = stop or self.execution_count\n\n        elapsed_time = (self.execution_count - start) * 1000\n        elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n\n        if end is not None:\n            elapsed_time = (self.execution_count - start) * 1000\n            elapsed_time_str += \" [%dms]\" % elapsed_time\n\n        return \"%s [%dms]\" % (elapsed_time_str, elapsed_time)\n\n    def log_stats(self, stats):\n        \"\"\"\n        Log the stats collected from the worker.\n\n        :param stats: Stats collected from the worker.\n        :type stats: dict\n        \"\"\"\n        stats[\"start_time\"] = self.start_time\n        stats[\"end_time\"] = self.end_time\n        ", "solution": "\n\n        # We return the formatted execution time.\n        return \":\".join(list(self._calculate(start, end).values()))"}
{"prompt": "def _all_load_skeleton(self, traj_node, hdf5_group):\n        \"\"\"Reloads skeleton data of a tree node\"\"\"", "output": "\n        load_skeleton = traj_node.load_skeleton\n        load_skeleton(hdf5_group)\n\n    def _all_load_nodes(self, traj_node, hdf5_group):\n        \"\"\"Reloads nodes of a tree node\"\"\"\n        load_nodes = traj_node.load_nodes\n        load_nodes(hdf5_group)\n\n    def _all_load_links(self, traj_node, hdf5_group):\n        \"\"\"Reloads links of a tree node\"\"\"\n        load_links = traj_node.load_links\n        load_links(hdf5_group)\n\n    def _all_load_constraints(self, traj_node, hdf5_group):\n        \"\"\"Reloads constraints of a tree node\"\"\"\n        load_constraints = traj_node.load_constraints\n        ", "solution": "\n        if traj_node.v_annotations.f_is_empty():\n            self._ann_load_annotations(traj_node, hdf5_group)\n        if traj_node.v_comment == '':\n            comment = self._all_get_from_attrs(hdf5_group, HDF5StorageService.COMMENT)\n            if comment is None:\n                comment = ''\n            traj_node.v_comment = comment"}
{"prompt": "def overlapping(self, variant_obj):\n        \"\"\"Return overlapping variants.\n\n        Look at the genes that a variant overlaps to.\n        Then return all variants that overlap these genes.\n\n        If variant_obj is sv it will return the overlapping snvs and oposite\n        There is a problem when SVs are huge since there are to many overlapping variants.\n\n        Args:\n            variant_obj(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"", "output": "\n        overlaps = []\n        for gene in variant_obj['genes']:\n            overlap = set(variant_obj['genes']).intersection(set(gene))\n            overlaps.append(overlap)\n        return overlaps\n\n    def filter(self, variant_obj, filters):\n        \"\"\"Filter variants by filters.\n\n        Args:\n            variant_obj(dict)\n            filters(dict)\n\n        Returns:\n            variants(iterable(dict))\n        \"\"\"\n        filtered = []\n        for gene in variant_obj['genes']:\n            for filter in filters:\n                if filter in gene:\n                    filtered.append(variant_obj)\n                    break\n        return filtered\n\n    def filter_by_chrom(self, variant_obj, chrom):\n        \"\"\"Filter ", "solution": "\n        #This is the category of the variants that we want to collect\n        category = 'snv' if variant_obj['category'] == 'sv' else 'sv'\n\n        query = {\n            '$and': [\n                {'case_id': variant_obj['case_id']},\n                {'category': category},\n                {'hgnc_ids' : { '$in' : variant_obj['hgnc_ids']}}\n            ]\n        }\n\n        sort_key = [('rank_score', pymongo.DESCENDING)]\n        # We collect the 30 most severe overlapping variants\n        variants = self.variant_collection.find(query).sort(sort_key).limit(30)\n\n        return variants"}
{"prompt": "def validate(self):\n        \"\"\"\n        Verify that the contents of the SecretData object are valid.\n\n        Raises:\n            TypeError: if the types of any SecretData attributes are invalid.\n        \"\"\"", "output": "\n        # Validate required properties are set\n        if self.kind is None:\n            raise TypeError(\"SecretData must have a 'kind' value\")\n\n        # Validate properties inherited from Secret\n        if self.metadata is None:\n            raise TypeError(\"SecretData must have a'metadata' value\")\n        if self.data is None:\n            raise TypeError(\"SecretData must have a 'data' value\")\n\n        # Validate unknown properties raise errors\n        # XXX: should we raise a specific type error for unknown properties?\n        pass\n\n    @cached_property\n    def metadata(self):\n        \"\"\"\n        Gets a dict with metadata about the SecretData object.\n\n        Returns:\n            dict: a dict mapping str to str\n        \"\"\"\n        return typing.cast(\n            dict,\n            self._properties.get(\"metadata\", ", "solution": "\n        if not isinstance(self.value, bytes):\n            raise TypeError(\"secret value must be bytes\")\n        elif not isinstance(self.data_type, enums.SecretDataType):\n            raise TypeError(\"secret data type must be a SecretDataType \"\n                            \"enumeration\")\n\n        mask_count = len(self.cryptographic_usage_masks)\n        for i in range(mask_count):\n            mask = self.cryptographic_usage_masks[i]\n            if not isinstance(mask, enums.CryptographicUsageMask):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\n                    \"secret data mask {0} must be a CryptographicUsageMask \"\n                    \"enumeration\".format(position))\n\n        name_count = len(self.names)\n        for i in range(name_count):\n            name = self.names[i]\n            if not isinstance(name, six.string_types):\n                position = \"({0} in list)\".format(i)\n                raise TypeError(\"secret data name {0} must be a string\".format(\n                    position))"}
{"prompt": "def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"", "output": "\n        matrix = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n        matrix[0, 0] = 1.0\n        matrix[1, 1] = 1.0\n        matrix[2, 2] = 1.0\n        return matrix\n\n    def to_vector(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        return np.array([1.0, 0.0, 0.0])\n\n    def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        return np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\n\n    def to_vector(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        return np.array([1.0, 0.0, 0.0])\n\n    def to_matrix(self):\n        \"\"\"Return a Numpy.array for the U3 gate.\"\"\"\n        return np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])", "solution": "\n        theta, phi, lam = self.params\n        return numpy.array(\n            [[\n                numpy.cos(theta / 2),\n                -numpy.exp(1j * lam) * numpy.sin(theta / 2)\n            ],\n             [\n                 numpy.exp(1j * phi) * numpy.sin(theta / 2),\n                 numpy.exp(1j * (phi + lam)) * numpy.cos(theta / 2)\n             ]],\n            dtype=complex)"}
{"prompt": "def _merge_with_other_stm(self, other: \"IfContainer\") -> None:\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"", "output": "\n        self.stms.append(other.stms)\n        self.else_stms.extend(other.else_stms)\n\n    def merge_with_stmt(self, stmt: \"Statement\") -> None:\n        \"\"\"\n        Merge statement into this statement\n        \"\"\"\n        self.stms.append(stmt.stms)\n\n    def merge_with_else_stmt(self, stmt: \"Statement\") -> None:\n        \"\"\"\n        Merge else statement into this statement\n        \"\"\"\n        self.else_stms.append(stmt.stms)\n\n    def merge_with_else_stmts(self, stmts: \"Sequence[Statement]\") -> None:\n        \"\"\"\n        Merge else statements into the statement\n        \"\"\"\n        self.else_stms.extend(stmts)\n\n    def merge_with_stmts(self, stmts: \"Sequence[Statement]\"", "solution": "\n        merge = self._merge_statement_lists\n        newCases = []\n        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):\n            newCases.append((c, merge(caseA, caseB)))\n\n        self.cases = newCases\n\n        if self.default is not None:\n            self.default = merge(self.default, other.default)\n\n        self._on_merge(other)"}
{"prompt": "def  main( argv ):\n    \"\"\"main program loop\"\"\"", "output": "\n    \n    # parse command line arguments\n    opts, args = getopt.getopt( argv[1:], 'h', ['help', 'help-full', 'help-short','version','version-full','version-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-short-full', 'help-short-short', 'help-", "solution": "\n\n    global output_dir\n\n    try:\n        opts, args = getopt.getopt( sys.argv[1:], \\\n                                    \"hb\",         \\\n                                    [\"help\", \"backup\"] )\n    except getopt.GetoptError:\n        usage()\n        sys.exit( 2 )\n\n    if args == []:\n        usage()\n        sys.exit( 1 )\n\n    # process options\n    #\n    output_dir = None\n    do_backup  = None\n\n    for opt in opts:\n        if opt[0] in ( \"-h\", \"--help\" ):\n            usage()\n            sys.exit( 0 )\n\n        if opt[0] in ( \"-b\", \"--backup\" ):\n            do_backup = 1\n\n    # create context and processor\n    source_processor = SourceProcessor()\n\n    # retrieve the list of files to process\n    file_list = make_file_list( args )\n    for filename in file_list:\n        source_processor.parse_file( filename )\n\n        for block in source_processor.blocks:\n            beautify_block( block )\n\n        new_name = filename + \".new\"\n        ok       = None\n\n        try:\n            file = open( new_name, \"wt\" )\n            for block in source_processor.blocks:\n                for line in block.lines:\n                    file.write( line )\n                    file.write( \"\\n\" )\n            file.close()\n        except:\n            ok = 0"}
{"prompt": "def checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"", "output": "\n    filename = frame.f_code.co_filename\n    if filename == __file__:\n        return False\n    if filename.endswith('.pyc'):\n        filename = filename[:-1]\n    if filename.endswith('$py.class'):\n        filename = filename[:-len('$py.class')]\n    if filename.endswith('$py.class'):\n        filename = filename[:-len('$py.class')]\n    if filename.endswith('$py.class'):\n        filename = filename[:-len('$py.class')]\n    if filename.endswith('$py.class'):\n        filename = filename[:-len('$py.class')]\n    if filename.endswith('$py.class'):\n        filename = filename[:-len('$py.class')]\n    if filename.endswith('$py.class'):\n        ", "solution": "\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True"}
{"prompt": "def gene_variants(self, query=None,\n                   category='snv', variant_type=['clinical'],\n                   nr_of_variants=50, skip=0):\n        \"\"\"Return all variants seen in a given gene.\n\n        If skip not equal to 0 skip the first n variants.\n\n        Arguments:\n            query(dict): A dictionary with querys for the database, including\n            variant_type: 'clinical', 'research'\n            category(str): 'sv', 'str', 'snv' or 'cancer'\n            nr_of_variants(int): if -1 return all variants\n            skip(int): How many variants to skip\n        \"\"\"", "output": "\n        variant_types = ['clinical','research','sv','str','snv', 'cancer']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ['all']\n        variant_types = variant_types + ", "solution": "\n\n        mongo_variant_query = self.build_variant_query(query=query,\n                                   category=category, variant_type=variant_type)\n\n        sorting = [('rank_score', pymongo.DESCENDING)]\n\n        if nr_of_variants == -1:\n            nr_of_variants = 0 # This will return all variants\n        else:\n            nr_of_variants = skip + nr_of_variants\n\n        result = self.variant_collection.find(\n            mongo_variant_query\n            ).sort(sorting).skip(skip).limit(nr_of_variants)\n\n        return result"}
{"prompt": "def filter_exclude_downhole(self, threshold, filt=True, samples=None, subset=None):\n        \"\"\"\n        Exclude all points down-hole (after) the first excluded data.\n\n        Parameters\n        ----------\n        threhold : int\n            The minimum number of contiguous excluded data points\n            that must exist before downhole exclusion occurs.\n        file : valid filter string or bool\n            Which filter to consider. If True, applies to currently active\n            filters.\n        \"\"\"", "output": "\n        if filt:\n            filt = self.filter\n        if filt is None:\n            filt = self.filter\n        if filt == 'all':\n            filt = self.filter\n        if filt == 'downhole':\n            filt = self.filter\n        if filt == 'downhole+':\n            filt = self.filter\n        if filt == 'downhole-':\n            filt = self.filter\n        if filt == 'downhole++':\n            filt = self.filter\n        if filt == 'downhole+-':\n            filt = self.filter\n        if filt == 'downhole-+':\n            filt = self.filter\n        if ", "solution": "\n        if samples is not None:\n            subset = self.make_subset(samples)\n\n        samples = self._get_samples(subset)\n\n        for s in samples:\n            self.data[s].filter_exclude_downhole(threshold, filt)"}
{"prompt": "def match(self, path):\n        \"\"\"Return route handler with arguments if path matches this route.\n\n        Arguments:\n          path (str): Request path\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the path.\n        \"\"\"", "output": "\n        handler = self.handlers.get(path)\n        if handler is None:\n            return None\n        args, kwargs = handler.match()\n        return handler, args, kwargs\n\n    def match_path(self, path, **args):\n        \"\"\"Match request path to route handler.\n\n        Arguments:\n          path (str): Request path\n          **args (dict): Keyword arguments\n\n        Returns:\n          tuple or None: A tuple of three items:\n\n            1. Route handler (callable)\n            2. Positional arguments (list)\n            3. Keyword arguments (dict)\n\n          ``None`` if the route does not match the path.\n        \"\"\"\n        handler = self.handlers.get(path)\n        if handler is None:\n            return None\n        args, kwargs = handler.match()\n        return handler, args, kwargs\n\n    def match", "solution": "\n        match = self._re.search(path)\n        if match is None:\n            return None\n        kwargs_indexes = match.re.groupindex.values()\n        args_indexes = [i for i in range(1, match.re.groups + 1)\n                          if i not in kwargs_indexes]\n        args = [match.group(i) for i in args_indexes]\n        kwargs = {}\n        for name, index in match.re.groupindex.items():\n            kwargs[name] = match.group(index)\n        return self._callback, args, kwargs"}
{"prompt": "def get_int(errmsg, arg, default=1, cmdname=None):\n    \"\"\"If arg is an int, use that otherwise take default.\"\"\"", "output": "\n    try:\n        return int(arg)\n    except ValueError:\n        if cmdname is None:\n            errmsg += \" must be an integer.\"\n        else:\n            errmsg += \" must be an integer, but got %s.\" % type(arg).__name__\n        return default\n\n\ndef validate_arg(arg, cmdname=None):\n    \"\"\"Validate an argument and raise a ValueError if invalid.\"\"\"\n    try:\n        return arg\n    except ValueError:\n        if cmdname is None:\n            errmsg = \"%s must be an integer.\" % arg\n        else:\n            errmsg = \"%s must be an integer, but got %s.\" % (arg, type(arg).__name__)\n        raise argparse.ArgumentTypeError(errmsg)\n\n\ndef validate_arg_int(arg, cmdname=None):\n    \"\"\"Validate an argument and raise a ValueError if ", "solution": "\n    if arg:\n        try:\n            # eval() is used so we will allow arithmetic expressions,\n            # variables etc.\n            default = int(eval(arg))\n        except (SyntaxError, NameError, ValueError):\n            if cmdname:\n                errmsg(\"Command '%s' expects an integer; got: %s.\" %\n                       (cmdname, str(arg)))\n            else:\n                errmsg('Expecting an integer, got: %s.' % str(arg))\n                pass\n            raise ValueError\n    return default"}
{"prompt": "def get(*args, **kwargs):\n    \"\"\"Get users.\"\"\"", "output": "\n    pass\n\n\ndef set(*args, **kwargs):\n    \"\"\"Set users.\"\"\"\n    pass\n\n\ndef delete(*args, **kwargs):\n    \"\"\"Delete users.\"\"\"\n    pass\n\n\ndef exists(*args, **kwargs):\n    \"\"\"Check if users are exists.\"\"\"\n    pass\n\n\ndef add(*args, **kwargs):\n    \"\"\"Add users.\"\"\"\n    pass\n\n\ndef remove(*args, **kwargs):\n    \"\"\"Remove users.\"\"\"\n    pass\n\n\ndef clear(*args, **kwargs):\n    \"\"\"Clear users.\"\"\"\n    pass\n\n\ndef list(*args, **kwargs):\n    \"\"\"List users.\"\"\"\n    pass\n\n\ndef exists(*args, **kwargs):\n    \"\"\"Check if users are exists.\"\"\"\n    pass\n\n\ndef set(*args, **kwargs):\n    \"\"\"Set users.\"\"\"\n    pass\n\n\ndef delete(*args, **kwargs):\n    \"\"\"Delete users.\"\"\"\n    pass\n\n\ndef add(*args, **kwargs):\n    \"\"\"Add users.\"\"\"\n    pass\n\n\n", "solution": "\n    from invenio.modules.oauth2server.models import Client\n    q = Client.query\n    return q.count(), q.all()"}
{"prompt": "def verify(\n            cls,\n            timestamp: int,\n            message_hash: SHA512Hash,\n            signature: bytes,\n    ) -> bool:\n        \"\"\"\n        Verify a given NIST message hash and signature for a beacon value.\n\n        :param timestamp: The timestamp of the record being verified.\n        :param message_hash:\n            The hash that was carried out over the message.\n            This is an object belonging to the `Crypto.Hash` module.\n        :param signature: The signature that needs to be validated.\n        :return: True if verification is correct. False otherwise.\n        \"\"\"", "output": "\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = cls.hash_object(signature)\n        message_hash = cls.hash_object(message_hash)\n        signature = ", "solution": "\n\n        # Determine verifier type to use based on timestamp.\n        if timestamp < 1496176860:\n            verifier = cls._VERIFIER_20130905\n        elif timestamp < 1502202360:\n            verifier = None\n        else:\n            verifier = cls._VERIFIER_20170808\n\n        # If a verifier exists to handle this problem, use it directly.\n        # Else, we cannot verify the record and must mark it invalid.\n        if verifier:\n            result = verifier.verify(\n                message_hash,\n                signature,\n            )\n        else:\n            result = False\n\n        # Convert 1 to 'True', 'False' otherwise\n        if isinstance(result, int):\n            result = True if result == 1 else False\n\n        return result"}
{"prompt": "def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from function bodies.\"\"\"", "output": "\n        body = self.visit(node.body)\n        if body is not None:\n            node.body = body\n        return node\n\n    def visit_ClassDef(self, node: ast.ClassDef) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from class bodies.\"\"\"\n        body = self.visit(node.body)\n        if body is not None:\n            node.body = body\n        return node\n\n    def visit_Return(self, node: ast.Return) -> Optional[ast.AST]:\n        \"\"\"Eliminate dead code from return statements.\"\"\"\n        expr = self.visit(node.value)\n        if expr is not None:\n            node.value = expr\n        return node\n\n    def visit_Delete(self, node: ast.Delete) -> Optional[ast.AST]:\n        \"\"\"Eliminate ", "solution": "\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )"}
{"prompt": "def register_success(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        succeeded\n        \"\"\"", "output": "\n        self.bot.register_success(nick, message, channel, cmd_channel)\n\n    def register_failure(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        failed\n        \"\"\"\n        self.bot.register_failure(nick, message, channel, cmd_channel)\n\n    def register_timeout(self, nick, message, channel, cmd_channel):\n        \"\"\"\\\n        Received registration acknowledgement from the BotnetBot, as well as the\n        name of the command channel, so join up and indicate that registration\n        failed\n        \"\"\"\n        self.bot.register_timeout(nick, message, channel, cmd_channel)\n\n", "solution": "\n        # the boss will tell what channel to join\n        self.channel = cmd_channel\n        self.conn.join(self.channel)\n        \n        # indicate that registered so we'll stop trying\n        self.registered.set()"}
{"prompt": "def csv_to_po(trans_csv_path, meta_csv_path, locale_root,\n              po_files_path, header=None):\n    \"\"\"\n    Converts GDocs spreadsheet generated csv file into po file.\n    :param trans_csv_path: path to temporary file with translations\n    :param meta_csv_path: path to temporary file with meta information\n    :param locale_root: path to locale root folder containing directories\n                        with languages\n    :param po_files_path: path from lang directory to po file\n    \"\"\"", "output": "\n    trans_csv_path = os.path.abspath(trans_csv_path)\n    meta_csv_path = os.path.abspath(meta_csv_path)\n    locale_root = os.path.abspath(locale_root)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po_files_path = os.path.abspath(po_files_path)\n    po", "solution": "\n    pattern = \"^\\w+.*po$\"\n    for root, dirs, files in os.walk(locale_root):\n        for f in filter(lambda x: re.match(pattern, x), files):\n            os.remove(os.path.join(root, f))\n\n    # read title row and prepare descriptors for po files in each lang\n    trans_reader = UnicodeReader(trans_csv_path)\n    meta_reader = UnicodeReader(meta_csv_path)\n    try:\n        title_row = trans_reader.next()\n    except StopIteration:\n        # empty file\n        return\n\n    trans_languages = _prepare_locale_dirs(title_row[3:], locale_root)\n\n    po_files = {}\n\n    meta_reader.next()\n    # go through every row in downloaded csv file\n    for trans_row, meta_row in izip_longest(trans_reader, meta_reader):\n        filename = trans_row[0].rstrip()\n        metadata = meta_row[0].rstrip() if meta_row else METADATA_EMPTY\n        comment = trans_row[1]\n        msgid = trans_row[2]\n\n        if filename not in po_files:\n            _prepare_polib_files(po_files, filename, trans_languages,\n                                 locale_root, po_files_path, header)\n\n        _write_entries(po_files[filename], trans_languages, msgid,\n                       trans_row[3:], metadata, comment)\n    for filename in po_files:\n        for lang in po_files[filename]:\n            po_files[filename][lang].save()\n\n    trans_reader.close()\n    meta_reader.close()"}
{"prompt": "def _add_header(self):\n        \"\"\"Add email header info.\"\"\"", "output": "\n        from_addr = self.from_addr\n        from_addr = self.from_addr or self.from_addrs[0]\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_addrs else None\n        from_addr = self.from_addrs[0] if self.from_", "solution": "\n        self.message[\"From\"] = self.from_\n        self.message[\"Subject\"] = self.subject\n        if self.to:\n            self.message[\"To\"] = self.list_to_string(self.to)\n        if self.cc:\n            self.message[\"Cc\"] = self.list_to_string(self.cc)\n        if self.bcc:\n            self.message[\"Bcc\"] = self.list_to_string(self.bcc)"}
{"prompt": "def url_dequery(url):\n    \"\"\"Return a URL with the query component removed.\n\n    :param url: URL to dequery.\n    :type url: str\n    :rtype: str\n    \"\"\"", "output": "\n    return url[:url.rfind('?')]\n\n\ndef url_unquote(url):\n    \"\"\"Return a URL with the query component unquoted.\n\n    :param url: URL to unquote.\n    :type url: str\n    :rtype: str\n    \"\"\"\n    return url[url.find('?') + 1:]\n\n\ndef url_quote(url):\n    \"\"\"Return a URL with the query component quoted.\n\n    :param url: URL to quote.\n    :type url: str\n    :rtype: str\n    \"\"\"\n    return '?' + url\n\n\ndef url_unquote(url):\n    \"\"\"Return a URL with the query component unquoted.\n\n    :param url: URL to unquote.\n    :type url: str\n    :rtype: str\n    \"\"\"\n    return url[:url.find('?')]\n\n\ndef url_quote(", "solution": "\n    url = urlparse.urlparse(url)\n    return urlparse.urlunparse((url.scheme,\n                                url.netloc,\n                                url.path,\n                                url.params,\n                                '',\n                                url.fragment))"}
{"prompt": "def rendered_content(self):\n        \"\"\"Returns a 'minified' version of the javascript content\"\"\"", "output": "\n        html = self.content\n        html = re.sub(\n            r'<script[^>]*>.*?</script>',\n            '<script id=\"__js__\" class=\"__js__\">%s</script>' % html,\n            html,\n        )\n        html = re.sub(\n            r'<style[^>]*>.*?</style>',\n            '<style id=\"__css__\" class=\"__css__\">%s</style>' % html,\n            html,\n        )\n        return html\n\n    def render(self, context):\n        \"\"\"Render the template with the context dict.\"\"\"\n        context = context.flatten()\n        context.update(self.kwargs)\n        html = self.content\n        html = re.sub(\n            r'", "solution": "\n        template = self.resolve_template(self.template_name)\n        if django.VERSION[1] < 8:\n            if template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        else:\n            if template.template.name.endswith('.min'):\n                return super(MinifiedJsTemplateResponse, self).rendered_content\n        # if no minified template exists, minify the response\n        content = super(MinifiedJsTemplateResponse, self).rendered_content\n        content = jsmin.jsmin(content)\n        return content"}
{"prompt": "def get_except_handler_name(self, node):\n        \"\"\"\n        Helper to get the exception name from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"", "output": "\n        handler_name = node.name\n        if node.type is not None:\n            handler_name = node.type.fullname\n        return handler_name\n\n    def get_except_handler_args(self, node):\n        \"\"\"\n        Helper to get the exception args from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"\n        handler_name = get_except_handler_name(self, node)\n        handler_args = []\n        if node.type is not None:\n            handler_args.append(node.type.fullname)\n        return handler_args\n\n    def get_except_handler_body(self, node):\n        \"\"\"\n        Helper to get the exception body from an ExceptHandler node in both py2 and py3.\n\n        \"\"\"\n        handler_name = get_except_handler_name(self, node)\n        handler_body = None\n        if node.type is not None:", "solution": "\n        name = node.name\n        if not name:\n            return None\n\n        if version_info < (3,):\n            return name.id\n        return name"}
{"prompt": "def _get_live_streams(self, lang, path):\n        \"\"\"\n        Get the live stream in a particular language\n        :param lang:\n        :param path:\n        :return:\n        \"\"\"", "output": "\n        live = []\n        live_path = '%s/live/%s.m3u8' % (path, lang)\n        if os.path.exists(live_path):\n            live = read_m3u8(live_path)\n        return live\n\n    def _get_live_streams_info(self, lang, path):\n        \"\"\"\n        Get the live stream info in a particular language\n        :param lang:\n        :param path:\n        :return:\n        \"\"\"\n        live = []\n        live_path = '%s/live/%s.m3u8' % (path, lang)\n        if os.path.exists(live_path):\n            live = read_m3u8(live_path)\n        return live\n\n    def _get_live_streams_info_by_id(self, lang, path):\n        \"\"\"\n        Get the live stream info in a particular language\n        :param lang:\n        :param path:", "solution": "\n        res = self.session.http.get(self._live_api_url.format(lang, path))\n        live_res = self.session.http.json(res)['default']['uid']\n        post_data = '{\"channel_url\":\"/api/channels/%s/\"}' % live_res\n        try:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['stream_url']\n        except BaseException:\n            stream_data = self.session.http.json(self.session.http.post(self._stream_get_url, data=post_data))['channel_url']\n        return HLSStream.parse_variant_playlist(self.session, stream_data)"}
{"prompt": "def rotatePoint(self, pointX, pointY):\n        \"\"\"\n        Rotates a point relative to the mesh origin by the angle specified in the angle property.\n        Uses the angle formed between the segment linking the point of interest to the origin and\n        the parallel intersecting the origin. This angle is called beta in the code.\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "    \n        if(self.angle == 0 or self.angle == None):\n            return(pointX, pointY)\n              \n        # 1. Compute the segment length\n        length = math.sqrt((pointX - self.xll) ** 2 + (pointY - self.yll) ** 2)\n        \n        # 2. Compute beta\n        beta = math.acos((pointX - self.xll) / length) \n        if(pointY < self.yll):\n            beta = math.pi * 2 - beta\n           \n        # 3. Compute offsets\n        offsetX = math.cos(beta) * length - math.cos(self._angle_rd + beta) * length\n        offsetY = math.sin(self._angle_rd + beta) * length - math.sin(beta) * length \n        return (pointX - offsetX, pointY + offsetY)"}
{"prompt": "def get_term_by_sis_id(self, sis_term_id):\n        \"\"\"\n        Return a term resource for the passed SIS ID.\n        \"\"\"", "output": "\n        return self._get_term_by_id(sis_term_id)\n\n    def get_term_by_name(self, name):\n        \"\"\"\n        Return a term resource for the passed name.\n        \"\"\"\n        return self._get_term_by_name(name)\n\n    def get_term_by_id(self, term_id):\n        \"\"\"\n        Return a term resource for the passed ID.\n        \"\"\"\n        return self._get_term_by_id(term_id)\n\n    def get_term_by_name(self, name):\n        \"\"\"\n        Return a term resource for the passed name.\n        \"\"\"\n        return self._get_term_by_name(name)\n\n    def get_term_by_id(self, term_id):\n        \"\"\"\n        Return a term resource for the passed ID.\n        \"\"\"\n        return self._get_term_by_id(term_id)\n\n    def get_term_by_name(self, name):\n        \"\"\"\n        Return a term resource for the passed name.\n", "solution": "\n        for term in self.get_all_terms():\n            if term.sis_term_id == sis_term_id:\n                return term"}
{"prompt": "def set_hvac_mode(self, index, hvac_mode):\n        \"\"\" possible hvac modes are auto, auxHeatOnly, cool, heat, off \"\"\"", "output": "\n        if hvac_mode not in self.hvac_modes:\n            raise KeyError(f\"Invalid hvac mode {hvac_mode}\")\n        self._hvac_modes[index] = hvac_mode\n\n    def set_hvac_mode_from_name(self, hvac_mode_name):\n        \"\"\" Set hvac mode from its name \"\"\"\n        hvac_mode = self.hvac_modes[hvac_mode_name]\n        self.set_hvac_mode(index, hvac_mode)\n\n    def set_hvac_mode_from_index(self, index):\n        \"\"\" Set hvac mode from its index \"\"\"\n        self.set_hvac_mode(index, self.hvac_modes[index", "solution": "\n        body = {\"selection\": {\"selectionType\": \"thermostats\",\n                              \"selectionMatch\": self.thermostats[index]['identifier']},\n                              \"thermostat\": {\n                                  \"settings\": {\n                                      \"hvacMode\": hvac_mode\n                                  }\n                              }}\n        log_msg_action = \"set HVAC mode\"\n        return self.make_request(body, log_msg_action)"}
{"prompt": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        \"\"\"check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        \"\"\"", "output": "\n        bad = False\n        escaped = False\n        escape_count = 0\n        bad_escape = False\n        bad_escape_count = 0\n        bad_escape_char = None\n        bad_escape_char_count = 0\n        bad_escape_char_is_quote = False\n        bad_escape_char_is_escape = False\n        bad_escape_char_is_unescape = False\n        bad_escape_char_is_unescaped_quote = False\n        bad_escape_char_is_unescaped_escape = False\n        bad_escape_char_is_unescaped_unescape = False\n        bad_escape_char_is_unescaped_unescaped_quote = False\n        bad_escape_char_is_unescaped_unescaped_escape = False\n        bad_escape_", "solution": "\n        # Walk through the string; if we see a backslash then escape the next\n        # character, and skip over it.  If we see a non-escaped character,\n        # alert, and continue.\n        #\n        # Accept a backslash when it escapes a backslash, or a quote, or\n        # end-of-line, or one of the letters that introduce a special escape\n        # sequence <http://docs.python.org/reference/lexical_analysis.html>\n        #\n        # TODO(mbp): Maybe give a separate warning about the rarely-used\n        # \\a \\b \\v \\f?\n        #\n        # TODO(mbp): We could give the column of the problem character, but\n        # add_message doesn't seem to have a way to pass it through at present.\n        i = 0\n        while True:\n            i = string_body.find(\"\\\\\", i)\n            if i == -1:\n                break\n            # There must be a next character; having a backslash at the end\n            # of the string would be a SyntaxError.\n            next_char = string_body[i + 1]\n            match = string_body[i : i + 2]\n            if next_char in self.UNICODE_ESCAPE_CHARACTERS:\n                if \"u\" in prefix:\n                    pass\n                elif (_PY3K or self._unicode_literals) and \"b\" not in prefix:\n                    pass  # unicode by default\n                else:\n                    self.add_message(\n                        \"anomalous-unicode-escape-in-string\",\n                        line=start_row,\n                        args=(match,),\n                    )\n            elif next_char not in self.ESCAPE_CHARACTERS:\n                self.add_message(\n                    \"anomalous-backslash-in-string\", line=start_row, args=(match,)\n                )\n            # Whether it was a valid escape or not, backslash followed by\n            # another character can always be consumed whole: the second\n            # character can never be the start of a new backslash escape.\n            i += 2"}
{"prompt": "def processFlat(self):\n        \"\"\"Main process.for flat segmentation.\n        Returns\n        -------\n        est_idxs : np.array(N)\n            Estimated times for the segment boundaries in frame indeces.\n        est_labels : np.array(N-1)\n            Estimated labels for the segments.\n        \"\"\"", "output": "\n        \n        # Read in flat segmentation data\n        flat_data = np.load(self.flat_file_name)\n        \n        # Extract times and labels\n        flat_ts = flat_data['times']\n        flat_labels = flat_data['labels']\n        \n        # Extract indices of flat segments\n        flat_idxs = np.where(flat_labels == 0)[0]\n        \n        # Extract indices of non-flat segments\n        nonflat_idxs = np.where(flat_labels!= 0)[0]\n        \n        # Extract indices of non-flat segments within flat segments\n        nonflat_within_idxs = np.where(flat_ts[flat_idxs] > 0)[0]\n        \n        # ", "solution": "\n        self.config[\"hier\"] = False\n        est_idxs, est_labels, F = self.process()\n        assert est_idxs[0] == 0 and est_idxs[-1] == F.shape[1] - 1\n        return self._postprocess(est_idxs, est_labels)"}
{"prompt": "def finish_registration(self, heart):\n        \"\"\"Second half of engine registration, called after our HeartMonitor\n        has received a beat from the Engine's Heart.\"\"\"", "output": "\n        self.heartbeat_count += 1\n        if self.heartbeat_count >= 2:\n            self.heartbeat_count = 0\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbeat_time = heart\n            self.heartbeat_count += 1\n            self.heartbe", "solution": "\n        try:\n            (eid,queue,reg,purge) = self.incoming_registrations.pop(heart)\n        except KeyError:\n            self.log.error(\"registration::tried to finish nonexistant registration\", exc_info=True)\n            return\n        self.log.info(\"registration::finished registering engine %i:%r\", eid, queue)\n        if purge is not None:\n            purge.stop()\n        control = queue\n        self.ids.add(eid)\n        self.keytable[eid] = queue\n        self.engines[eid] = EngineConnector(id=eid, queue=queue, registration=reg,\n                                    control=control, heartbeat=heart)\n        self.by_ident[queue] = eid\n        self.queues[eid] = list()\n        self.tasks[eid] = list()\n        self.completed[eid] = list()\n        self.hearts[heart] = eid\n        content = dict(id=eid, queue=self.engines[eid].queue.decode('ascii'))\n        if self.notifier:\n            self.session.send(self.notifier, \"registration_notification\", content=content)\n        self.log.info(\"engine::Engine Connected: %i\", eid)"}
{"prompt": "def get_ticker_price(self, ticker,\n                         startDate=None, endDate=None,\n                         fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Price for a stock ticker.\n           On average, each feed contains 3 data sources.\n\n            Supported tickers + Available Day Ranges are here:\n            https://apimedia.tiingo.com/docs/tiingo/daily/supported_tickers.zip\n\n            Args:\n                ticker (string): Unique identifier for stock ticker\n                startDate (string): Start of ticker range in YYYY-MM-DD format\n                endDate (string): End of ticker range in YYYY-MM-DD format\n                fmt (string): 'csv' or 'json'\n                frequency (string): Resample frequency\n        \"\"\"", "output": "\n                fmt (string): Format of returned data\n                frequency (string): Frequency of returned data\n\n            Returns:\n                dict: Dictionary of ticker data\n        \"\"\"\n        ticker_url = self.ticker_url(ticker)\n        ticker_data = self.fetch_json(ticker_url, params=dict(\n            frequency=frequency,\n            startDate=startDate,\n            endDate=endDate,\n            fmt=fmt\n        ))\n        return ticker_data\n\n    def get_ticker_volume(self, ticker,\n                          startDate=None, endDate=None,\n                          fmt='json', frequency='daily'):\n        \"\"\"By default, return latest EOD Composite Volume for a stock tick", "solution": "\n        url = self._get_url(ticker, frequency)\n        params = {\n            'format': fmt if fmt != \"object\" else 'json',  # conversion local\n            'resampleFreq': frequency\n        }\n\n        if startDate:\n            params['startDate'] = startDate\n        if endDate:\n            params['endDate'] = endDate\n\n        # TODO: evaluate whether to stream CSV to cache on disk, or\n        # load as array in memory, or just pass plain text\n        response = self._request('GET', url, params=params)\n        if fmt == \"json\":\n            return response.json()\n        elif fmt == \"object\":\n            data = response.json()\n            return [dict_to_object(item, \"TickerPrice\") for item in data]\n        else:\n            return response.content.decode(\"utf-8\")"}
{"prompt": "def _plant_trie(strings: _List[str]) -> dict:\n    \"\"\"Create a Trie out of a list of words and return an atomic regex pattern.\n\n    The corresponding Regex should match much faster than a simple Regex union.\n    \"\"\"", "output": "\n    trie = Trie()\n    for word in strings:\n        trie.add(word)\n    return trie\n\n\ndef _is_number(s: str) -> bool:\n    \"\"\"Check if a string is a number.\"\"\"\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef _is_alpha(s: str) -> bool:\n    \"\"\"Check if a string is a letter.\"\"\"\n    return all(c.isalpha() for c in s)\n\n\ndef _is_digit(s: str) -> bool:\n    \"\"\"Check if a string is a digit.\"\"\"\n    return all(c.isdigit() for c in s)\n\n\ndef _is_space(s: str) -> bool:\n    \"\"\"Check if a string is a space.\"\"\"\n    return all(c.isspace() for c in s)\n\n\ndef _is_punctuation(s: str) -> bool:\n", "solution": "\n    # plant the trie\n    trie = {}\n    for string in strings:\n        d = trie\n        for char in string:\n            d[char] = char in d and d[char] or {}\n            d = d[char]\n        d[''] = None  # EOS\n    return trie"}
{"prompt": "def normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n    return_offset : bool, optional\n        return the log of the scale factor for each row\n\n    Returns\n    -------\n    tuple\n        normalized profile (fresh np object) and offset (if return_offset==True)\n    \"\"\"", "output": "\n    \n    if log:\n        profile = np.log(in_profile)\n    else:\n        profile = in_profile\n    \n    profile_norm = (profile - profile.min()) / (profile.max() - profile.min())\n    \n    if return_offset:\n        offset = profile_norm.sum(axis=0) / profile_norm.shape[0]\n        return profile_norm, offset\n    else:\n        return profile_norm\n\n\ndef normalize_profile(in_profile, log=False, return_offset = True):\n    \"\"\"return a normalized version of a profile matrix\n\n    Parameters\n    ----------\n    in_profile : np.array\n        shape Lxq, will be normalized to one across each row\n    log : bool, optional\n        treat the input as log probabilities\n", "solution": "\n    if log:\n        tmp_prefactor = in_profile.max(axis=1)\n        tmp_prof = np.exp(in_profile.T - tmp_prefactor).T\n    else:\n        tmp_prefactor = 0.0\n        tmp_prof = in_profile\n\n    norm_vector = tmp_prof.sum(axis=1)\n    return (np.copy(np.einsum('ai,a->ai',tmp_prof,1.0/norm_vector)),\n            (np.log(norm_vector) + tmp_prefactor) if return_offset else None)"}
{"prompt": "def delete_dashboard(self, dashboard):\n        \"\"\"**Description**\n            Deletes a dashboard.\n\n        **Arguments**\n            - **dashboard**: the dashboard object as returned by :func:`~SdcClient.get_dashboards`.\n\n        **Success Return Value**\n            `None`.\n\n        **Example**\n            `examples/delete_dashboard.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboard.py>`_\n        \"\"\"", "output": "\n        dashboard_id = dashboard.id\n        self.delete_dashboard(dashboard_id)\n\n    def delete_dashboards(self, dashboards):\n        \"\"\"**Description**\n            Deletes a list of dashboards.\n\n        **Arguments**\n            - **dashboards**: the list of dashboards returned by :func:`~SdcClient.get_dashboards`.\n\n        **Success Return Value**\n            `None`.\n\n        **Example**\n            `examples/delete_dashboards.py <https://github.com/draios/python-sdc-client/blob/master/examples/delete_dashboards.py>`_\n        \"\"\"\n        dashboard_ids = []\n        for dashboard in dashboards:\n            dashboard_ids.append(dashboard.id)\n        self.delete_dashboards(dashboard_ids)\n\n    def delete_dashboard", "solution": "\n        if 'id' not in dashboard:\n            return [False, \"Invalid dashboard format\"]\n\n        res = requests.delete(self.url + self._dashboards_api_endpoint + '/' + str(dashboard['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n\n        return [True, None]"}
{"prompt": "def _make_masked_initializer(mask, initializer):\n  \"\"\"Returns a masked version of the given initializer.\"\"\"", "output": "\n  masked_initializer = tf.compat.v1.initializers.initializer(\n      tf.compat.v1.initializers.random_uniform_initializer(\n          minval=-1., maxval=1., seed=tf.compat.v1.random.get_seed(None)))\n  return masked_initializer(mask, dtype=initializer.dtype)\n\n\ndef masked_conv2d_backprop_filter(\n    inputs, filters, kernel_size, strides, padding, data_format='NHWC',\n    dilation_rate=None, name=None):\n  \"\"\"Computes gradients of the convolutional filter with respect to inputs.\n\n  See [ConvNets: Mathematically Building Convolutional Networks for\n  Fast Stylization](https://arxiv.org/abs/1603.07285).\n\n  Args:\n    inputs: Tensor of rank 4 of shape\n", "solution": "\n  initializer = tf.keras.initializers.get(initializer)\n  def masked_initializer(shape, dtype=None, partition_info=None):\n    # If no `partition_info` is given, then don't pass it to `initializer`, as\n    # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which\n    # don't accept a `partition_info` argument).\n    if partition_info is None:\n      x = initializer(shape, dtype)\n    else:\n      x = initializer(shape, dtype, partition_info)\n    return tf.cast(mask, x.dtype) * x\n  return masked_initializer"}
{"prompt": "def wipe(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"", "output": "\n    ctx.run(\"mongo --quiet --eval 'db.dropDatabase(\\'admin\\')'\")\n\n\ndef drop(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"\n    ctx.run(\"mongo --quiet --eval 'db.dropDatabase(\\'admin\\')'\")\n\n\ndef drop_all(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"\n    ctx.run(\"mongo --quiet --eval 'db.dropDatabase(\\'admin\\')'\")\n\n\ndef drop_collection(ctx):\n    \"\"\"Drop the mongo database given.\"\"\"\n    ctx.run(\"mongo --quiet --eval 'db.dropDatabase(\\'admin\\')'\")\n\n\n", "solution": "\n    LOG.info(\"Running scout wipe\")\n    db_name = ctx.obj['mongodb']\n    LOG.info(\"Dropping database %s\", db_name)\n    try:\n        ctx.obj['client'].drop_database(db_name)\n    except Exception as err:\n        LOG.warning(err)\n        ctx.abort()\n    LOG.info(\"Dropped whole database\")"}
{"prompt": "def new_contact(cls, address_book, supported_private_objects, version,\n            localize_dates):\n        \"\"\"Use this to create a new and empty contact.\"\"\"", "output": "\n        contact = cls(\n            address_book=address_book,\n            supported_private_objects=supported_private_objects,\n            version=version,\n            localize_dates=localize_dates)\n        return contact\n\n    def new_contact_from_dict(cls, contact_dict, address_book,\n                               supported_private_objects, version,\n                               localize_dates):\n        \"\"\"Use this to create a new and empty contact from a dictionary.\"\"\"\n        contact = cls(\n            address_book=address_book,\n            supported_private_objects=supported_private_objects,\n            version=version,\n            localize_dates=localize_dates)\n        contact.from_dict(contact_dict)\n        return contact\n\n    def new_contact_from_dict(cls, ", "solution": "\n        return cls(address_book, None, supported_private_objects, version,\n                localize_dates)"}
{"prompt": "def sample_variants(self, variants, sample_name, category = 'snv'):\n        \"\"\"Given a list of variants get variant objects found in a specific patient\n\n        Args:\n            variants(list): a list of variant ids\n            sample_name(str): a sample display name\n            category(str): 'snv', 'sv' ..\n\n        Returns:\n            result(iterable(Variant))\n        \"\"\"", "output": "\n        \n        variant_ids = []\n        for v in variants:\n            variant_ids.append(v.id)\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_ids))\n        \n        variant_ids = list(set(variant_id", "solution": "\n        LOG.info('Retrieving variants for subject : {0}'.format(sample_name))\n        has_allele = re.compile('1|2') # a non wild-type allele is called at least once in this sample\n\n        query = {\n            '$and': [\n                {'_id' : { '$in' : variants}},\n                {'category' : category},\n                {'samples': {\n                    '$elemMatch': { 'display_name' : sample_name, 'genotype_call': { '$regex' : has_allele } }\n                }}\n            ]\n        }\n\n        result = self.variant_collection.find(query)\n        return result"}
{"prompt": "def _register_by_id_managed_msg(self, msgid, line, is_disabled=True):\n        \"\"\"If the msgid is a numeric one, then register it to inform the user\n        it could furnish instead a symbolic msgid.\"\"\"", "output": "\n        if msgid is None:\n            return\n        try:\n            msgid = int(msgid)\n        except ValueError:\n            msgid = msgid.decode('utf-8')\n        msg_type = self.msg_types.get(msgid)\n        if msg_type is None:\n            return\n        msg_type.register_by_id(line, is_disabled=is_disabled)\n\n    def register_by_id(self, msgid, line, is_disabled=True):\n        \"\"\"Register a message by its numeric id.\"\"\"\n        self._register_by_id_managed_msg(msgid, line, is_disabled=is_disabled)\n\n    def register_by_msgid(self, msgid, line, is_disabled=True):\n        \"\"\"Register a message by its symbolic msgid.\"\"\"\n        self._register_by_id_managed_msg(msgid, line", "solution": "\n        try:\n            message_definitions = self.msgs_store.get_message_definitions(msgid)\n            for message_definition in message_definitions:\n                if msgid == message_definition.msgid:\n                    MessagesHandlerMixIn.__by_id_managed_msgs.append(\n                        (\n                            self.current_name,\n                            message_definition.msgid,\n                            message_definition.symbol,\n                            line,\n                            is_disabled,\n                        )\n                    )\n        except UnknownMessageError:\n            pass"}
{"prompt": "def pretty(obj, verbose=False, max_width=79, newline='\\n'):\n    \"\"\"\n    Pretty print the object's representation.\n    \"\"\"", "output": "\n    indent ='' * 4\n    space ='' * max_width\n    newline_indent = indent + newline\n    newline = '\\n' + newline_indent\n    space = space * max_width\n    \n    if isinstance(obj, basestring):\n        return indent + obj + newline\n    elif isinstance(obj, dict):\n        pairs = []\n        for key, value in obj.iteritems():\n            pairs.append((key, pretty(value, verbose, max_width, newline)))\n        return indent + '{' + ', '.join(pairs) + '}' + newline\n    elif isinstance(obj, list):\n        pairs = []\n        for index, value in enumerate(obj):\n            pairs.append((index, pretty(value, verbose, max_width, newline)))\n        return indent + '[' + ', '.join(pairs) + ']' + newline\n    else:\n        return ", "solution": "\n    stream = StringIO()\n    printer = RepresentationPrinter(stream, verbose, max_width, newline)\n    printer.pretty(obj)\n    printer.flush()\n    return stream.getvalue()"}
{"prompt": "def __match_intervals(intervals_from, intervals_to, strict=True):  # pragma: no cover\n    \"\"\"Numba-accelerated interval matching algorithm.\n\n    \"\"\"", "output": "\n    intervals_from = list(intervals_from)\n    intervals_to = list(intervals_to)\n    intervals_from.sort()\n    intervals_to.sort()\n    intervals_from = [interval for interval in intervals_from if interval[0] < interval[1]]\n    intervals_to = [interval for interval in intervals_to if interval[0] > interval[1]]\n    intervals_from = [interval for interval in intervals_from if interval[1] > interval[0]]\n    intervals_to = [interval for interval in intervals_to if interval[1] < interval[0]]\n    intervals_from = [interval for interval in intervals_from if interval[0] < interval[1]]\n    ", "solution": "\n    # sort index of the interval starts\n    start_index = np.argsort(intervals_to[:, 0])\n\n    # sort index of the interval ends\n    end_index = np.argsort(intervals_to[:, 1])\n\n    # and sorted values of starts\n    start_sorted = intervals_to[start_index, 0]\n    # and ends\n    end_sorted = intervals_to[end_index, 1]\n\n    search_ends = np.searchsorted(start_sorted, intervals_from[:, 1], side='right')\n    search_starts = np.searchsorted(end_sorted, intervals_from[:, 0], side='left')\n\n    output = np.empty(len(intervals_from), dtype=numba.uint32)\n    for i in range(len(intervals_from)):\n        query = intervals_from[i]\n\n        # Find the intervals that start after our query ends\n        after_query = search_ends[i]\n        # And the intervals that end after our query begins\n        before_query = search_starts[i]\n\n        # Candidates for overlapping have to (end after we start) and (begin before we end)\n        candidates = set(start_index[:after_query]) & set(end_index[before_query:])\n\n        # Proceed as before\n        if len(candidates) > 0:\n            output[i] = __match_interval_overlaps(query, intervals_to, candidates)\n        elif strict:\n            # Numba only lets us use compile-time constants in exception messages\n            raise ParameterError\n        else:\n            # Find the closest interval\n            # (start_index[after_query] - query[1]) is the distance to the next interval\n            # (query[0] - end_index[before_query])\n            dist_before = np.inf\n            dist_after = np.inf\n            if search_starts[i] > 0:\n                dist_before = query[0] - end_sorted[search_starts[i]-1]\n            if search_ends[i] + 1 < len(intervals_to):\n                dist_after = start_sorted[search_ends[i]+1] - query[1]\n            if dist_before < dist_after:\n                output[i] = end_index[search_starts[i]-1]\n            else:\n                output[i] = start_index[search_ends[i]+1]\n    return output"}
{"prompt": "def on_key_press(self,symbol,modifiers):\n        \"\"\"\n        Keyboard event handler handling only the escape key.\n        \n        If an escape key press is detected, mouse exclusivity is toggled via :py:meth:`PengWindow.toggle_exclusivity()`\\ .\n        \"\"\"", "output": "\n        if symbol == K_BACKSPACE:\n            self.toggle_exclusivity()\n        elif symbol == K_ESCAPE:\n            self.close()\n        elif symbol == K_RETURN:\n            self.close()\n        elif symbol == K_SPACE:\n            self.close()\n        elif symbol == K_UP:\n            self.move('up')\n        elif symbol == K_DOWN:\n            self.move('down')\n        elif symbol == K_LEFT:\n            self.move('left')\n        elif symbol == K_RIGHT:\n            self.move('right')\n        elif symbol == K_ESCAPE:\n            self.close()\n        elif symbol == K_TAB:\n            self.move('tab')\n        elif symbol == K_BACK", "solution": "\n        if symbol == key.ESCAPE:\n            self.world.peng.window.toggle_exclusivity()\n            return pyglet.event.EVENT_HANDLED"}
{"prompt": "def pprint_simple_array(val, displaywidth, msg_nocr, msg, lineprefix=''):\n    \"\"\"Try to pretty print a simple case where a list is not nested.\n    Return True if we can do it and False if not. \"\"\"", "output": "\n    if isinstance(val, list):\n        val = list(val)\n    width = max(map(len, val))\n    if width > displaywidth:\n        return False\n    if msg_nocr:\n        line = [''* displaywidth]\n    else:\n        line = [' '] * (displaywidth - len(line))\n    line.extend(['[', lineprefix])\n    if display:\n        line.extend([' ', repr(val), ']'])\n    else:\n        line.extend([' ', repr(val[0]), repr(val[1]), '...'])\n    line.extend([' ', lineprefix])\n    line.extend([''] * (displaywidth - len(line)))\n    return line\n\n\ndef pprint_dict(val, displaywidth, ", "solution": "\n\n    if type(val) != list:\n        return False\n\n    numeric = True\n    for i in range(len(val)):\n        if not (type(val[i]) in [bool, float, int]):\n            numeric = False\n            if not (type(val[i]) in [bool, float, int, bytes]):\n                return False\n            pass\n        pass\n    mess = columnize([repr(v) for v in val],\n                     opts={\"arrange_array\": True,\n                           \"lineprefix\": lineprefix,\n                           \"displaywidth\": int(displaywidth)-3,\n                           'ljust': not numeric})\n    msg_nocr(mess)\n    return True"}
{"prompt": "def size(self, source):\n    \"\"\"Get the size component of the given s3url. If it is a\n       directory, combine the sizes of all the files under\n       that directory. Subdirectories will not be counted unless\n       --recursive option is set.\n    \"\"\"", "output": "\n    size = 0\n    dirname = os.path.dirname(source)\n    if dirname:\n        size = sum(size(self.join(dirname, f)) for f in os.listdir(dirname))\n    return size\n\ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\ndef sizeof_formatted(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi", "solution": "\n    result = []\n    for src in self.source_expand(source):\n      size = 0\n      for f in self.s3walk(src):\n        size += f['size']\n      result.append((src, size))\n\n    return result"}
{"prompt": "def parse_compounds(compound_info, case_id, variant_type):\n    \"\"\"Get a list with compounds objects for this variant.\n\n        Arguments:\n            compound_info(str): A Variant dictionary\n            case_id (str): unique family id\n            variant_type(str): 'research' or 'clinical'\n\n        Returns:\n            compounds(list(dict)): A list of compounds\n    \"\"\"", "output": "\n    compounds = []\n    for compound_info in compound_info:\n        compound_id = compound_info.get('compound_id')\n        compound_name = compound_info.get('compound_name')\n        compound_type = compound_info.get('compound_type')\n        compound_description = compound_info.get('compound_description')\n        compound_status = compound_info.get('compound_status')\n        compound_status = compound_status.lower()\n        compound_status = compound_status.replace(' ', '_')\n        compound_status = compound_status.replace('_', '-')\n        compound_status = compound_status.replace('-', '_')\n        compound_status = compound_status.replace('_', '-')\n        compound_status = compound_status.replace('-', '_')\n        compound_status = compound_status.replace('-', '_')\n        ", "solution": "\n    # We need the case to construct the correct id\n    compounds = []\n    if compound_info:\n        for family_info in compound_info.split(','):\n            splitted_entry = family_info.split(':')\n            # This is the family id\n            if splitted_entry[0] == case_id:\n                for compound in splitted_entry[1].split('|'):\n                    splitted_compound = compound.split('>')\n                    compound_obj = {}\n                    compound_name = splitted_compound[0]\n                    compound_obj['variant'] = generate_md5_key(compound_name.split('_') +\n                                                               [variant_type, case_id])\n\n                    try:\n                        compound_score = float(splitted_compound[1])\n                    except (TypeError, IndexError):\n                        compound_score = 0.0\n\n                    compound_obj['score'] = compound_score\n                    compound_obj['display_name'] = compound_name\n\n                    compounds.append(compound_obj)\n\n    return compounds"}
{"prompt": "def any_slug_field(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField\r\n    >>> result = any_field(models.SlugField())\r\n    >>> type(result)\r\n    <type 'str'>\r\n    >>> from django.core.validators import slug_re\r\n    >>> re.match(slug_re, result) is not None\r\n    True\r\n    \"\"\"", "output": "\r\n    return random_string(length=field.max_length, **kwargs)\r\n\r\n\r\ndef any_slug_field_with_default(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField if SlugField has default\r\n    >>> result = any_slug_field(models.SlugField(default='foo'))\r\n    >>> type(result)\r\n    <type'str'>\r\n    >>> result = any_slug_field_with_default(models.SlugField(default='foo'))\r\n    >>> type(result)\r\n    <type'str'>\r\n    \"\"\"\r\n    return random_string(length=field.max_length, **kwargs)\r\n\r\n\r\ndef any_slug_field_with_default_and_blank(field, **kwargs):\r\n    \"\"\"\r\n    Return random value for SlugField if SlugField has default and blank\r\n", "solution": "\r\n    letters = ascii_letters + digits + '_-'\r\n    return xunit.any_string(letters = letters, max_length = field.max_length)"}
{"prompt": "def factorize(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n            .C : updated values for C.\n            .U : updated values for U.\n            .R : updated values for R.\n        \"\"\"", "output": "\n        self.C = self.C * self.factor\n        self.U = self.U * self.factor\n        self.R = self.R * self.factor\n\n    def factorize2(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n           .C : updated values for C.\n           .U : updated values for U.\n           .R : updated values for R.\n        \"\"\"\n        self.C = self.C * self.factor\n        self.U = self.U * self.factor\n        self.R = self.R * self.factor\n\n    def factorize3(self):\n        \"\"\" Factorize s.t. CUR = data\n\n            Updated Values\n            --------------\n           .C : updated values for C.\n           .U : updated values for U.\n           .R : ", "solution": "\n\n        [prow, pcol] = self.sample_probability()\n\n        self._rid = self.sample(self._rrank, prow)\n        self._cid = self.sample(self._crank, pcol)\n\n        self._cmdinit()\n\n        self.computeUCR()"}
{"prompt": "def _deduplicate_indexed_slices(values, indices):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"", "output": "\n    unique_indices = tf.unique(indices)\n    unique_indices_int64 = tf.cast(unique_indices, tf.int64)\n    unique_indices_int64_flat = tf.reshape(unique_indices_int64, [-1])\n    unique_indices_flat = tf.gather(unique_indices_int64_flat, tf.range(tf.size(values)))\n    unique_indices_flat = tf.reshape(unique_indices_flat, tf.shape(values))\n    summed_values = tf.reduce_sum(tf.gather(values, unique_indices_flat), axis=-1)\n    return summed_values, unique_indices_flat\n\n\ndef _is_index_in_bounds(indices, bounds):\n  \"\"\"Returns whether `indices` is in bounds.\n  Args:\n    indices: A `Tensor` with ", "solution": "\n    unique_indices, new_index_positions = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values,\n                                            new_index_positions,\n                                            tf.shape(unique_indices)[0])\n    return (summed_values, unique_indices)"}
{"prompt": "def token_validate_with_login(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            request_token: The token you generated for the user to approve.\n            username: The user's username on TMDb.\n            password: The user's password on TMDb.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"", "output": "\n        body = {\n            \"username\": username,\n            \"password\": password,\n            \"request_token\": request_token,\n        }\n        return self.post(\n            \"/auth/login\", json=body, headers={\"Content-Type\": \"application/json\"}\n        )\n\n    def token_validate_with_email(self, **kwargs):\n        \"\"\"\n        Authenticate a user with a TMDb username and password.  The user\n        must have a verified email address and be registered on TMDb.\n\n        Args:\n            email: The user's email address on TMDb.\n\n        Returns:\n            A dict respresentation of the JSON returned from the API.\n        \"\"\"\n        body = {\n            \"email\": email,\n            \"request_token\": request_token,\n", "solution": "\n        path = self._get_path('token_validate_with_login')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response"}
{"prompt": "def setLang(self,lang):\n        \"\"\"\n        Sets the default language for all domains.\n        \n        For recommendations regarding the format of the language code, see\n        :py:class:`TranslationManager`\\ .\n        \n        Note that the ``lang`` parameter of both :py:meth:`translate()` and\n        :py:meth:`translate_lazy()` will override this setting.\n        \n        Also note that the code won't be checked for existence or plausibility.\n        This may cause the fallback strings to be displayed instead if the language\n        does not exist.\n        \n        Calling this method will cause the ``setlang`` action and the\n        :peng3d:event`peng3d:i18n.set_lang` event to be triggered. Note that both\n        action and event will be triggered even if the language did not actually change.\n        \n        This method also automatically updates the :confval:`i18n.lang` config value.\n        \"\"\"", "output": "3d.domain.domain.Domain` to be executed.\n        \n        :param lang: The language code (ISO 639-1 code).\n        \"\"\"\n        self.lang = lang\n        \n    def translate(self,text,lang=None):\n        \"\"\"\n        Translates the given text into the given language.\n        \n        Note that this method will also perform a fallback translation.\n        \n        Calling this method will cause the ``translate`` action and the\n        :peng3d:event`peng3d.domain.domain.Domain` to be executed.\n        \n        :param text: The text to translate.\n        :param lang: The language code (ISO 639-1 code).\n        \"\"\"\n        if lang is None:\n            lang = self.", "solution": "\n        self.lang = lang\n        self.peng.cfg[\"i18n.lang\"] = lang\n        \n        if lang not in self.cache:\n            self.cache[lang]={}\n        \n        self.doAction(\"setlang\")\n        self.peng.sendEvent(\"peng3d:i18n.set_lang\",{\"lang\":self.lang,\"i18n\":self})"}
{"prompt": "def end_group(self, dedent=0, close=''):\n        \"\"\"End a group. See `begin_group` for more details.\"\"\"", "output": "\n        self.indent = self.indents.pop()\n        self.dedent = self.dedents.pop()\n        self.close = close\n\n    def flush(self):\n        \"\"\"Flush any buffered output to the output stream.\"\"\"\n        if self.buffer:\n            self.buffer.seek(0)\n            self.buffer.truncate()\n            self.buffer.write(self.output)\n            self.output = ''\n\n    def write_line(self, line):\n        \"\"\"Write a line of text to the output stream.\"\"\"\n        self.output += line\n\n    def write_error(self, message):\n        \"\"\"Write an error message to the output stream.\"\"\"\n        self.write_line(message)\n        self.write_line('\\n')\n        self.flush()\n\n    def write_warning(self, message):\n        \"\"\"Write a warning message to the output stream.\"\"\"\n        self.", "solution": "\n        self.indentation -= dedent\n        group = self.group_stack.pop()\n        if not group.breakables:\n            self.group_queue.remove(group)\n        if close:\n            self.text(close)"}
{"prompt": "def revoke_token(self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"Revoke an access or refresh token.\n        \"\"\"", "output": "\n        token_type = token_hint.token_type\n        access_token = token_hint.access_token\n        refresh_token = token_hint.refresh_token\n        token_type_hint = self.token_provider_class(\n            request=request,\n            token_type=token_type,\n            access_token=access_token,\n            refresh_token=refresh_token,\n            **kwargs\n        )\n        token_type_hint.delete()\n        return token_type_hint\n\n    def refresh_token(\n        self, token, token_type_hint, request, *args, **kwargs):\n        \"\"\"Refresh an access or refresh token.\n        \"\"\"\n        token_type = token_hint.token_type\n        access_token = token_hint.access_token\n        refresh_token = token_hint.refresh_token\n        token_type_hint = self.token_provider_", "solution": "\n        if token_type_hint:\n            tok = self._tokengetter(**{token_type_hint: token})\n        else:\n            tok = self._tokengetter(access_token=token)\n            if not tok:\n                tok = self._tokengetter(refresh_token=token)\n\n        if tok:\n            request.client_id = tok.client_id\n            request.user = tok.user\n            tok.delete()\n            return True\n\n        msg = 'Invalid token supplied.'\n        log.debug(msg)\n        request.error_message = msg\n        return False"}
{"prompt": "def is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"", "output": "\n    return isinstance(node.parent, astroid.node_classes.Call)\n\n\ndef is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"\n    return isinstance(node.parent, astroid.node_classes.AnnAssign)\n\n\ndef is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"\n    return isinstance(node.parent, astroid.node_classes.AnnAssign)\n\n\ndef is_postponed_evaluation_enabled(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the postponed evalu", "solution": "\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )"}
{"prompt": "def _log(cls, level, msg, **kwargs):\n        \"\"\"\n        Logs a message with pre-formatted prefix.\n\n        :param int level:\n            Logging level as specified in the\n            `login module <http://docs.python.org/2/library/logging.html>`_ of\n            Python standard library.\n\n        :param str msg:\n            The actual message.\n\n        \"\"\"", "output": "\n        prefix = kwargs.pop('prefix', '[%(levelname)s] ')\n        message = prefix + msg\n        kwargs['level'] = level\n        kwargs['message'] = message\n        kwargs['asctime'] = time.asctime(time.localtime())\n        kwargs['filename'] = kwargs.get('filename', sys._getframe().f_code.co_filename)\n        kwargs['funcName'] = kwargs.get('funcName', sys._getframe().f_code.co_name)\n        kwargs['lineno'] = kwargs.get('lineno', sys._getframe().f_lineno)\n        kwargs['file'] = kwargs.get('file', sys.stderr)\n        kwargs['exc_info'] = kwargs.get('exc_info', None)\n        kwargs['stack_info'] = kwargs.get('stack_info', None)\n        kwargs['extra'] = kwargs.get('extra', None)\n        kwargs['extra'] = ", "solution": "\n\n        logger = getattr(cls, '_logger', None) or authomatic.core._logger\n        logger.log(\n            level, ': '.join(\n                ('authomatic', cls.__name__, msg)), **kwargs)"}
{"prompt": "def print_list_lines(self, filename, first, last):\n        \"\"\"The printing (as opposed to the parsing part of a 'list'\n        command.\"\"\"", "output": "\n        lines = list()\n        for line in self.list_lines(filename):\n            lines.append(line)\n            if first and line[0] == first:\n                first = None\n            if last and line[-1] == last:\n                last = None\n        return lines\n\n    def list_lines(self, filename):\n        \"\"\"Yield all lines in the given file.\"\"\"\n        try:\n            with open(filename) as fp:\n                for line in fp:\n                    yield line\n        except IOError:\n            pass\n\n    def list_files(self, directory):\n        \"\"\"Yield all files in the given directory.\"\"\"\n        for root, dirs, files in os.walk(directory):\n            for name in files:\n                yield os.path.join(root, name)\n\n    def list_directories(self, directory):\n        \"\"\"Yield all directorie", "solution": "\n        try:\n            Colors = self.color_scheme_table.active_colors\n            ColorsNormal = Colors.Normal\n            tpl_line = '%%s%s%%s %s%%s' % (Colors.lineno, ColorsNormal)\n            tpl_line_em = '%%s%s%%s %s%%s%s' % (Colors.linenoEm, Colors.line, ColorsNormal)\n            src = []\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno)\n                if not line:\n                    break\n\n                if lineno == self.curframe.f_lineno:\n                    line = self.__format_line(tpl_line_em, filename, lineno, line, arrow = True)\n                else:\n                    line = self.__format_line(tpl_line, filename, lineno, line, arrow = False)\n\n                src.append(line)\n                self.lineno = lineno\n\n            print >>io.stdout, ''.join(src)\n\n        except KeyboardInterrupt:\n            pass"}
{"prompt": "def _propagate_mean(mean, linop, dist):\n  \"\"\"Propagate a mean through linear Gaussian transformation.\"\"\"", "output": "\n  mean, linop, dist = dist.unstack(mean), linop.unstack(linop), dist.unstack(dist)\n  mean = mean.stack()\n  linop = linop.stack()\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)\n  dist = dist.stack()\n  dist = dist.unstack(dist)", "solution": "\n  return linop.matmul(mean) + dist.mean()[..., tf.newaxis]"}
{"prompt": "def get_cdata(self, *args):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"", "output": "\n        if len(args) == 1:\n            args = args[0]\n        if isinstance(args, tuple):\n            args = list(args)\n        if args:\n            buf = self._cffi_backend.buffer(*args)\n            if buf.ndim == 0:\n                return buf.data\n            else:\n                return buf.data[0], buf.data[1], buf.data[2], buf.data[3]\n        else:\n            return None\n\n    def get_cdata_as(self, obj):\n        \"\"\"\n        all args-->_cffi_backend.buffer\n        Returns-->cdata (if a SINGLE argument was provided)\n                  LIST of cdata (if a args was a tuple or list)\n        \"\"\"\n        if isinstance(obj, tuple):\n            obj = list(obj)\n        if obj:\n            buf = self._cffi_backend.buffer(*obj)", "solution": "\n        res = tuple([\n            self.from_buffer(x) for x in args\n        ])\n\n        if len(res) == 0:\n            return None\n        elif len(res) == 1:\n            return res[0]\n        else:\n            return res"}
{"prompt": "def sourceWatchdog(self):\n        \"\"\"Watchdog timer function. \n\n        Recreates sources which have not generated events in 10*interval if\n        they have watchdog set to true in their configuration\n        \"\"\"", "output": "\n        \n        # Only recreate sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*interval\n        \n        # Find all sources which have not generated events in 10*", "solution": "\n        for i, source in enumerate(self.sources):\n            if not source.config.get('watchdog', False):\n                continue \n            sn = repr(source)\n            last = self.lastEvents.get(source, None)\n            if last:\n                try:\n                    if last < (time.time()-(source.inter*10)):\n                        log.msg(\"Trying to restart stale source %s: %ss\" % (\n                            sn, int(time.time() - last)\n                        ))\n\n                        s = self.sources.pop(i)\n                        try:\n                            s.t.stop()\n                        except Exception as e:\n                            log.msg(\"Could not stop timer for %s: %s\" % (\n                                sn, e))\n\n                        config = copy.deepcopy(s.config)\n\n                        del self.lastEvents[source]\n                        del s, source\n\n                        source = self.createSource(config)\n\n                        reactor.callLater(0, self._startSource, source)\n                except Exception as e:\n                    log.msg(\"Could not reset source %s: %s\" % (\n                        sn, e))"}
{"prompt": "def get_key_for_purpose_and_type(self, purpose, key_type):\n        \"\"\"\n        Gets a list of keys that match the purpose and key_type, and returns the first key in that list\n        Note, if there are many keys that match the criteria, the one you get back will be random from that list\n        :returns: A key object that matches the criteria\n        \"\"\"", "output": "\n        key_objects = []\n        for key_object in self.get_all_key_objects():\n            if key_object.get_purpose() == purpose and key_object.get_type() == key_type:\n                key_objects.append(key_object)\n        if len(key_objects) == 0:\n            return None\n        else:\n            return random.choice(key_objects)\n\n    def get_key_by_purpose(self, purpose):\n        \"\"\"\n        Gets a key object by its purpose\n        :param purpose: The purpose of the key object\n        :returns: A key object by its purpose\n        \"\"\"\n        key_objects = self.get_all_key_objects()\n        for key_object in key_objects:\n            if key_object.get_purpose() == purpose:\n                return key_object\n        return None\n\n    def get_key_by_type(self, key_type):", "solution": "\n        key = [key for key in self.keys.values() if key.purpose == purpose and key.key_type == key_type]\n        try:\n            return key[0]\n        except IndexError:\n            return None"}
{"prompt": "def _trj_fill_run_table(self, traj, start, stop):\n        \"\"\"Fills the `run` overview table with information.\n\n        Will also update new information.\n\n        \"\"\"", "output": "\n        # Update run table\n        run = traj.run(start, stop)\n        run._update_table()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # Update run info\n        run._update_run_info()\n\n        # ", "solution": "\n\n        def _make_row(info_dict):\n            row = (info_dict['idx'],\n                   info_dict['name'],\n                   info_dict['time'],\n                   info_dict['timestamp'],\n                   info_dict['finish_timestamp'],\n                   info_dict['runtime'],\n                   info_dict['parameter_summary'],\n                   info_dict['short_environment_hexsha'],\n                   info_dict['completed'])\n            return row\n\n        runtable = getattr(self._overview_group, 'runs')\n\n        rows = []\n        updated_run_information = traj._updated_run_information\n        for idx in range(start, stop):\n            info_dict = traj._run_information[traj._single_run_ids[idx]]\n            rows.append(_make_row(info_dict))\n            updated_run_information.discard(idx)\n\n        if rows:\n            runtable.append(rows)\n            runtable.flush()\n\n        # Store all runs that are updated and that have not been stored yet\n        rows = []\n        indices = []\n        for idx in updated_run_information:\n            info_dict = traj.f_get_run_information(idx, copy=False)\n            rows.append(_make_row(info_dict))\n            indices.append(idx)\n\n        if rows:\n            runtable.modify_coordinates(indices, rows)\n\n        traj._updated_run_information = set()"}
{"prompt": "def to_service(self, service, version):\n        \"\"\"Sets the service name and version the request should target\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n\n        Returns:\n            The request builder instance in order to chain calls\n        \"\"\"", "output": "\n        return ServiceRequestBuilder(service, version)\n\n    def build(self, service, version):\n        \"\"\"Builds the request and returns the response\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n\n        Returns:\n            The response builder instance in order to chain calls\n        \"\"\"\n        return ServiceResponseBuilder(service, version)\n\n    def build(self, service, version, **kwargs):\n        \"\"\"Builds the request and returns the response\n\n        Args:\n            service (str): The name of the service as displayed in the services.json file\n            version (str): The version of the service as displayed in the services.json file\n            kwargs (dict): Any additional keyword arguments to pass to the ", "solution": "\n        service_url = self._service_locator.get_service_url(service, version)\n        return self.__copy_and_set('service_url', self.__strip_trailing_slashes(service_url))"}
{"prompt": "def _cmd(self, cmd, *args, **kw):\n        \"\"\"\n        write a single command, with variable number of arguments. after the\n        command, the device must return ACK\n        \"\"\"", "output": "\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01\n        kw['timeout'] = 0.01", "solution": "\n        ok = kw.setdefault('ok', False)\n\n        self._wakeup()\n        if args:\n            cmd = \"%s %s\" % (cmd, ' '.join(str(a) for a in args))\n        for i in xrange(3):\n            log.info(\"send: \" + cmd)\n            self.port.write(cmd + '\\n')\n            if ok:\n                ack = self.port.read(len(self.OK))  # read OK\n                log_raw('read', ack)\n                if ack == self.OK:\n                    return\n            else:\n                ack = self.port.read(len(self.ACK))  # read ACK\n                log_raw('read', ack)\n                if ack == self.ACK:\n                    return\n        raise NoDeviceException('Can not access weather station')"}
{"prompt": "def prefilter_lines(self, lines, continue_prompt=False):\n        \"\"\"Prefilter multiple input lines of text.\n\n        This is the main entry point for prefiltering multiple lines of\n        input.  This simply calls :meth:`prefilter_line` for each line of\n        input.\n\n        This covers cases where there are multiple lines in the user entry,\n        which is the case when the user goes back to a multiline history\n        entry and presses enter.\n        \"\"\"", "output": "\n        prompt = '(y/n) '\n        prompt = prompt.ljust(len(self.prompt)+1)\n        prompt = prompt.strip()\n        prompt = '>'+ prompt\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        prompt = prompt.strip()\n        ", "solution": "\n        llines = lines.rstrip('\\n').split('\\n')\n        # We can get multiple lines in one shot, where multiline input 'blends'\n        # into one line, in cases like recalling from the readline history\n        # buffer.  We need to make sure that in such cases, we correctly\n        # communicate downstream which line is first and which are continuation\n        # ones.\n        if len(llines) > 1:\n            out = '\\n'.join([self.prefilter_line(line, lnum>0)\n                             for lnum, line in enumerate(llines) ])\n        else:\n            out = self.prefilter_line(llines[0], continue_prompt)\n\n        return out"}
{"prompt": "def app_template_filter(self, name=None):\n        \"\"\"Register a custom template filter, available application wide.  Like\n        :meth:`Flask.template_filter` but for a blueprint.\n\n        :param name: the optional name of the filter, otherwise the\n                     function name will be used.\n        \"\"\"", "output": "\n        if name is None:\n            name = self.options.get('template_filter')\n        self.filters[name] = _TemplateFilter(self, name, self)\n\n    def template_test(self, name=None):\n        \"\"\"Register a custom template test, available application wide.  Like\n        :meth:`Flask.template_test` but for a blueprint.\n\n        :param name: the optional name of the test, otherwise the\n                     function name will be used.\n        \"\"\"\n        if name is None:\n            name = self.options.get('template_test')\n        self.tests[name] = _TemplateTest(self, name, self)\n\n    def template_global(self, name=None):\n        \"\"\"Register a custom template global, available application wide.  Like\n        :meth:`Flask.template_global` but for a blueprint.\n\n        :param name: the optional name of the global, otherwise the\n                     function name will be ", "solution": "\n        def decorator(f):\n            self.add_app_template_filter(f, name=name)\n            return f\n        return decorator"}
{"prompt": "def _replace_at_index(x, index, replacement):\n  \"\"\"Replaces an element at supplied index.\"\"\"", "output": "\n  assert isinstance(x, np.ndarray)\n  assert isinstance(index, int)\n  assert isinstance(replacement, np.ndarray)\n  assert index.shape == replacement.shape\n  assert index.dtype == replacement.dtype\n  assert index.ndim == 1\n  assert index.size == x.size\n  assert replacement.size == x.size\n  assert index.dtype == replacement.dtype\n  assert index.ndim == 1\n  assert index.size == x.size\n  assert index.dtype == x.dtype\n  assert index.shape == replacement.shape\n  assert index.dtype == x.dtype\n  assert index.shape == replacement.shape\n  assert index.dtype == x.dtype\n  assert index.shape == replacement.shape\n  assert ", "solution": "\n  x_new = tf.concat([x[:index], tf.expand_dims(replacement, axis=0),\n                     x[(index + 1):]], axis=0)\n  return x_new"}
{"prompt": "def converted(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"", "output": "\n        converted = {}\n        for key, value in self.items():\n            if isinstance(value, dict):\n                converted[key] = converted(value)\n            elif isinstance(value, list):\n                converted[key] = converted(value)\n            else:\n                converted[key] = value\n        return converted\n\n    def to_dict(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"\n        converted = self.converted()\n        return converted\n\n    def to_json(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"\n        converted = self.converted()\n        return json.dumps(converted)\n\n    def to_dicts(self):\n        \"\"\"\n        :rtype: {str: object}\n        \"\"\"\n        converted = self.converted", "solution": "\n        if self._converted is None:\n            self._converted = self.converter(self.name, self.value)\n        return self._converted"}
{"prompt": "def categories(self):\n        \"\"\"\n        Return the list of levels for an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"", "output": "\n        return self._categories\n\n    @categories.setter\n    def categories(self, categories):\n        \"\"\"\n        Set the list of levels for an enum (categorical) column.\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"\n        self._categories = categories\n\n    @property\n    def ordered(self):\n        \"\"\"\n        Return whether the column is ordered (True) or not (False).\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"\n        return self._ordered\n\n    @ordered.setter\n    def ordered(self, ordered):\n        \"\"\"\n        Set the column is ordered (True) or not (False).\n\n        This function can only be applied to single-column categorical frame.\n        \"\"\"\n        self._ordered = ordered\n\n    @property", "solution": "\n        if self.ncols != 1:\n            raise H2OValueError(\"This operation only applies to a single factor column\")\n        if self.types[self.names[0]] != \"enum\":\n            raise H2OValueError(\"Input is not a factor. This operation only applies to a single factor column\")\n        return self.levels()[0]"}
{"prompt": "def mixin(cls, mixin_cls):\n        \"\"\"Decorator for mixing in additional functionality into field type\n\n        Example:\n\n        >>> @Integer.mixin\n        ... class IntegerPostgresExtensions:\n        ...     postgres_type = 'INT'\n        ...\n        ...     def postgres_dump(self, obj):\n        ...         self.dump(obj) + \"::integer\"\n\n        Is roughly equivalent to:\n\n        >>> Integer.postgres_type = 'INT'\n        ...\n        ... def postgres_dump(self, obj):\n        ...     self.dump(obj) + \"::integer\"\n        ...\n        ... Integer.postgres_dump = postgres_dump\n\n        \"\"\"", "output": "\n        setattr(cls, 'postgres_dump', cls.postgres_dump)\n        setattr(cls, 'postgres_dumps', cls.postgres_dumps)\n        setattr(cls, 'postgres_dumps_as_string', cls.postgres_dumps_as_string)\n        setattr(cls, 'postgres_dumps_as_json', cls.postgres_dumps_as_json)\n        setattr(cls, 'postgres_dumps_as_xml', cls.postgres_dumps_as_xml)\n        setattr(cls, 'postgres_dumps_as_yaml', cls.postgres_dumps_as_yaml)\n        setattr(cls, 'postgres_dumps_as_jsonb', cls.postgres_dumps_as_jsonb)\n        set", "solution": "\n        for item_name in dir(mixin_cls):\n            if item_name.startswith(\"__\"):\n                # don't copy magic properties\n                continue\n            item = getattr(mixin_cls, item_name)\n\n            if isinstance(item, types.MethodType):\n                # unbound method will cause problems\n                # so get the underlying function instead\n                item = item.im_func\n\n            setattr(cls, item_name, item)\n        return mixin_cls"}
{"prompt": "def get_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the annotations within the times.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_ref_annotation_data_between_times` for the format.\n\n        :param str id_tier: Name of the tier.\n        :param int start: Start time of the annotation.\n        :param int end: End time of the annotation.\n        :returns: List of annotations within that time.\n        :raises KeyError: If the tier is non existent.\n        \"\"\"", "output": "\n        annotations = self.annotations.filter(tier=id_tier)\n        annotations = annotations.filter(time__range=(start, end))\n        return annotations\n\n    def get_ref_annotation_data_between_times(self, id_tier, start, end):\n        \"\"\"Gives the annotations within the times and references.\n        When the tier contains reference annotations this will be returned,\n        check :func:`get_annotation_data_between_times` for the format.\n\n        :param str id_tier: Name of the tier.\n        :param int start: Start time of the annotation.\n        :param int end: End time of the annotation.\n        :returns: List of annotations within that time and the references.\n        :raises KeyError: If the tier is non exist", "solution": "\n        if self.tiers[id_tier][1]:\n            return self.get_ref_annotation_data_between_times(\n                id_tier, start, end)\n        anns = ((self.timeslots[a[0]], self.timeslots[a[1]], a[2])\n                for a in self.tiers[id_tier][0].values())\n        return sorted(a for a in anns if a[1] >= start and a[0] <= end)"}
{"prompt": "def __deftype_method_recur_to_py_ast(\n    ctx: GeneratorContext, node: Recur\n) -> GeneratedPyAST:\n    \"\"\"Return a Python AST node for `recur` occurring inside a `deftype*` method.\"\"\"", "output": "\n    deftype = node.deftype\n    method_name = node.name\n    method_node = ctx.find_node(deftype, method_name)\n    method_node = ctx.find_node(deftype, method_name, method_node)\n    method_node = ctx.find_node(deftype, method_name, method_node, is_method=True)\n    method_node = ctx.find_node(deftype, method_name, method_node, is_method=False)\n    method_node = ctx.find_node(deftype, method_name, method_node, is_method=False)\n    method_node = ctx.find_node(deftype, method_name, method_node, is_method=False)\n    method_node = ctx.find_node(deftype, method_name, method_node, is_method=False)\n    method_node = ctx.find_node(deftype, method_name, ", "solution": "\n    assert node.op == NodeOp.RECUR\n    recur_nodes: List[ast.AST] = []\n    recur_deps: List[ast.AST] = []\n    for expr in node.exprs:\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_nodes.append(expr_ast.node)\n        recur_deps.extend(expr_ast.dependencies)\n\n    this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n    assert this_entry is not None, \"Field type local must have this\"\n\n    return GeneratedPyAST(\n        node=ast.Call(\n            func=_TRAMPOLINE_ARGS_FN_NAME,\n            args=list(\n                chain(\n                    [\n                        ast.NameConstant(ctx.recur_point.is_variadic),\n                        ast.Name(id=this_entry.munged, ctx=ast.Load()),\n                    ],\n                    recur_nodes,\n                )\n            ),\n            keywords=[],\n        ),\n        dependencies=recur_deps,\n    )"}
{"prompt": "def _random_adjspecies_pair():\n    \"\"\"Return an ordered 2-tuple containing a species and a describer.\"\"\"", "output": "\n    species, describer = _random_species_pair()\n    adjspecies, adjdescriber = _random_adjspecies_pair()\n    return species, describer, adjspecies, adjdescriber\n\n\ndef _random_species_pair():\n    \"\"\"Return a 2-tuple containing a species and a describer.\"\"\"\n    species, describer = _random_species_pair()\n    adjspecies, adjdescriber = _random_adjspecies_pair()\n    return species, describer, adjspecies, adjdescriber\n\n\ndef _random_adjspecies_pair():\n    \"\"\"Return a 2-tuple containing a species and a describer.\"\"\"\n    species, describer = _random_species_pair()\n", "solution": "\n    describer, desc_position = random_describer()\n    if desc_position == 'prefix':\n        return (describer, random_species())\n    elif desc_position == 'suffix':\n        return (random_species(), describer)"}
{"prompt": "def _run_cmd_line_code(self):\n        \"\"\"Run code or file specified at the command-line\"\"\"", "output": "\n        cmd_line = self.cmd_line\n        if cmd_line:\n            cmd_line = self.cmd_line[0]\n            cmd_line = self.cmd_line[1:]\n            cmd_line = cmd_line.strip()\n            if cmd_line:\n                cmd_line = self.cmd_line[0]\n                cmd_line = self.cmd_line[1:]\n                cmd_line = cmd_line.strip()\n                if cmd_line:\n                    cmd_line = self.cmd_line[0]\n                    cmd_line = self.cmd_line[1:]\n                    cmd_line = cmd_line.strip()\n                    if cmd_line:\n                        cmd_line = self.cmd_line[0]\n                        cmd_line = self.cmd_line[1:]\n                        cmd_line = cmd_line.strip()\n                        if cmd_line:\n                            cmd_line = self.cmd_line[0]\n", "solution": "\n        if self.code_to_run:\n            line = self.code_to_run\n            try:\n                self.log.info(\"Running code given at command line (c=): %s\" %\n                              line)\n                self.shell.run_cell(line, store_history=False)\n            except:\n                self.log.warn(\"Error in executing line in user namespace: %s\" %\n                              line)\n                self.shell.showtraceback()\n\n        # Like Python itself, ignore the second if the first of these is present\n        elif self.file_to_run:\n            fname = self.file_to_run\n            try:\n                self._exec_file(fname)\n            except:\n                self.log.warn(\"Error in executing file in user namespace: %s\" %\n                              fname)\n                self.shell.showtraceback()"}
{"prompt": "def _first_glimpse_sensor(self, x_t):\n        \"\"\"\n        Compute first glimpse position using down-sampled image.\n        \"\"\"", "output": "\n        downsample_factor = self.downsample_factor\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        downsample_factor = int(downsample_factor)\n        ", "solution": "\n        downsampled_img = theano.tensor.signal.downsample.max_pool_2d(x_t, (4,4))\n        downsampled_img = downsampled_img.flatten()\n        first_l = T.dot(downsampled_img, self.W_f)\n        if self.disable_reinforce:\n            wf_grad = self.W_f\n            if self.random_glimpse:\n                first_l = self.srng.uniform((2,), low=-1.7, high=1.7)\n        else:\n            sampled_l_t = self._sample_gaussian(first_l, self.cov)\n            sampled_pdf = self._multi_gaussian_pdf(disconnected_grad(sampled_l_t), first_l)\n            wf_grad = T.grad(T.log(sampled_pdf), self.W_f)\n            first_l = sampled_l_t\n        return first_l, wf_grad"}
{"prompt": "def makeReturnFormat(self):\n        \"\"\" Strip reserved and CRC for m_req :class:`~ekmmeters.SerialBlock`. \"\"\"", "output": "\n        req = self.m_req\n        req.format = req.FORMAT_RESERVED\n        req.format = req.FORMAT_CRC\n        return req\n\n    def makeRequest(self, req):\n        \"\"\" Make request from request format and request data. \"\"\"\n        req = self.makeReturnFormat()\n        req.data = req.data[:req.size]\n        req.size = req.size\n        return req\n\n    def makeResponse(self, resp):\n        \"\"\" Make response from response format and response data. \"\"\"\n        resp = self.makeReturnFormat()\n        resp.data = resp.data[:resp.size]\n        resp.size = resp.size\n        return resp\n\n    def makeResponseFormat(self):\n        \"\"\" Strip reserve", "solution": "\n        for fld in self.m_blk_a:\n            compare_fld = fld.upper()\n            if not \"RESERVED\" in compare_fld and not \"CRC\" in compare_fld:\n                self.m_req[fld] = self.m_blk_a[fld]\n        pass"}
{"prompt": "def add_attachment(self, filename, open_file):\n        \"\"\"\n        Adds an attachment to this card.\n        \"\"\"", "output": "\n        self.attachments.append({\n            'filename': filename,\n            'open_file': open_file\n        })\n\n    def add_attachments(self, attachments):\n        \"\"\"\n        Adds attachments to this card.\n        \"\"\"\n        self.attachments.extend(attachments)\n\n    def to_dict(self):\n        \"\"\"\n        Converts this card to a dictionary.\n        \"\"\"\n        return {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'type': self.type,\n            'attachments': self.attachments\n        }\n\n    @classmethod\n    def from_dict(cls, data):\n        \"\"\"\n        Converts a dictionary to a card instance.\n        \"\"\"\n        card = cls(\n            id=data['id'],\n            name=data['name'],\n            description=data['description'],", "solution": "\n        fields = {\n            'api_key': self.client.api_key,\n            'token': self.client.user_auth_token\n        }\n\n        content_type, body = self.encode_multipart_formdata(\n            fields=fields,\n            filename=filename,\n            file_values=open_file\n        )\n\n        return self.fetch_json(\n            uri_path=self.base_uri + '/attachments',\n            http_method='POST',\n            body=body,\n            headers={'Content-Type': content_type},\n        )"}
{"prompt": "def configure(self, options, conf):\n        \"\"\"\n        Configure plugin.\n        \"\"\"", "output": "\n        conf.register_opts(plugin.OPTS)\n        conf.register_opts(plugin.EXTRA_OPTS)\n        conf.register_opts(plugin.EXTRA_OPTS_V2)\n        conf.register_opts(plugin.EXTRA_OPTS_V3)\n        conf.register_opts(plugin.EXTRA_OPTS_V4)\n        conf.register_opts(plugin.EXTRA_OPTS_V5)\n        conf.register_opts(plugin.EXTRA_OPTS_V6)\n        conf.register_opts(plugin.EXTRA_OPTS_V7)\n        conf.register_opts(plugin.EXTRA_OPTS_V8)\n        conf.register_opts(plugin.EXTR", "solution": "\n        try:\n            self.status.pop('active')\n        except KeyError:\n            pass\n        super(Coverage, self).configure(options, conf)\n        if conf.worker:\n            return\n        if self.enabled:\n            try:\n                import coverage\n            except ImportError:\n                log.error(\"Coverage not available: \"\n                          \"unable to import coverage module\")\n                self.enabled = False\n                return\n        self.conf = conf\n        self.coverErase = options.cover_erase\n        self.coverTests = options.cover_tests\n        self.coverPackages = []\n        if options.cover_packages:\n            for pkgs in [tolist(x) for x in options.cover_packages]:\n                self.coverPackages.extend(pkgs)\n        self.coverInclusive = options.cover_inclusive\n        if self.coverPackages:\n            log.info(\"Coverage report will include only packages: %s\",\n                     self.coverPackages)\n        self.coverHtmlDir = None\n        if options.cover_html:\n            self.coverHtmlDir = options.cover_html_dir\n            log.debug('Will put HTML coverage report in %s', self.coverHtmlDir)\n        self.coverBranches = options.cover_branches\n        self.coverXmlFile = None\n        if options.cover_min_percentage:\n            self.coverMinPercentage = int(options.cover_min_percentage.rstrip('%'))\n        if options.cover_xml:\n            self.coverXmlFile = options.cover_xml_file\n            log.debug('Will put XML coverage report in %s', self.coverXmlFile)\n        if self.enabled:\n            self.status['active'] = True\n            self.coverInstance = coverage.coverage(auto_data=False,\n                branch=self.coverBranches, data_suffix=None)"}
{"prompt": "def _select_next_server(self):\n        \"\"\"\n        Looks up in the server pool for an available server\n        and attempts to connect.\n        \"\"\"", "output": "\n        server = self.server_pool.get()\n        if server is None:\n            raise socket.error(\"No free server found\")\n        try:\n            server.connect()\n        except socket.error as e:\n            if e.args[0] not in (errno.EINPROGRESS, errno.EALREADY,\n                                  errno.EWOULDBLOCK):\n                raise\n            server.close()\n            server = self.server_pool.get()\n        return server\n\n    def _select_next_client(self):\n        \"\"\"\n        Looks up in the client pool for an available client\n        and attempts to connect.\n        \"\"\"\n        client = self.client_pool.get()\n        if client is None:\n            raise socket.error(\"No free client found\")\n        try:\n            client.connect()\n", "solution": "\n\n        while True:\n            if len(self._server_pool) == 0:\n                self._current_server = None\n                raise ErrNoServers\n\n            now = time.monotonic()\n            s = self._server_pool.pop(0)\n            if self.options[\"max_reconnect_attempts\"] > 0:\n                if s.reconnects > self.options[\"max_reconnect_attempts\"]:\n                    # Discard server since already tried to reconnect too many times\n                    continue\n\n            # Not yet exceeded max_reconnect_attempts so can still use\n            # this server in the future.\n            self._server_pool.append(s)\n            if s.last_attempt is not None and now < s.last_attempt + self.options[\"reconnect_time_wait\"]:\n                # Backoff connecting to server if we attempted recently.\n                yield from asyncio.sleep(self.options[\"reconnect_time_wait\"], loop=self._loop)\n            try:\n                s.last_attempt = time.monotonic()\n                r, w = yield from asyncio.open_connection(\n                    s.uri.hostname,\n                    s.uri.port,\n                    loop=self._loop,\n                    limit=DEFAULT_BUFFER_SIZE)\n                self._current_server = s\n\n                # We keep a reference to the initial transport we used when\n                # establishing the connection in case we later upgrade to TLS\n                # after getting the first INFO message. This is in order to\n                # prevent the GC closing the socket after we send CONNECT\n                # and replace the transport.\n                #\n                # See https://github.com/nats-io/asyncio-nats/issues/43\n                self._bare_io_reader = self._io_reader = r\n                self._bare_io_writer = self._io_writer = w\n                break\n            except Exception as e:\n                s.last_attempt = time.monotonic()\n                s.reconnects += 1\n\n                self._err = e\n                if self._error_cb is not None:\n                    yield from self._error_cb(e)\n                continue"}
{"prompt": "def time_remaining_est(self, completed_iter):\n        \"\"\"Estimate the remaining time left.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            est_time: Estimated time remaining.\n        \"\"\"", "output": "\n        completed_iter = int(completed_iter)\n        estimated_time = self.estimated_time\n        estimated_time += completed_iter * self._estimated_time_multiplier\n        return estimated_time\n\n    def estimate_time_left(self, completed_iter):\n        \"\"\"Estimate the time left until completion.\n\n        Parameters:\n            completed_iter (int): Number of iterations completed.\n\n        Returns:\n            time_left: Estimated time left.\n        \"\"\"\n        completed_iter = int(completed_iter)\n        time_left = self.estimated_time\n        time_left -= completed_iter * self._estimated_time_multiplier\n        return time_left\n\n    def estimate_time_left(self, completed_iter):", "solution": "\n        if completed_iter:\n            t_r_est = (time.time() - self.t_start) / \\\n                completed_iter*(self.iter-completed_iter)\n        else:\n            t_r_est = 0\n        date_time = datetime.datetime(1, 1, 1) + datetime.timedelta(seconds=t_r_est)\n        time_string = \"%02d:%02d:%02d:%02d\" % \\\n            (date_time.day - 1, date_time.hour, date_time.minute, date_time.second)\n\n        return time_string"}
{"prompt": "def bkg_subtract(self, analytes=None, errtype='stderr', focus_stage='despiked'):\n        \"\"\"\n        Subtract calculated background from data.\n\n        Must run bkg_calc first!\n\n        Parameters\n        ----------\n        analytes : str or iterable\n            Which analyte(s) to subtract.\n        errtype : str\n            Which type of error to propagate. default is 'stderr'.\n        focus_stage : str\n            Which stage of analysis to apply processing to. \n            Defaults to 'despiked' if present, or 'rawdata' if not. \n            Can be one of:\n            * 'rawdata': raw data, loaded from csv file.\n            * 'despiked': despiked data.\n            * 'signal'/'background': isolated signal and background data.\n              Created by self.separate, after signal and background\n              regions have been identified by self.autorange.\n            * 'bkgsub': background subtracted data, created by \n              self.bkg_correct\n            * 'ratios': element ratio data, created by self.ratio.\n            * 'calibrated': ratio data calibrated to standards, created by self.calibrate.\n        \"\"\"", "output": "\n            * 'despiked': despiked data, loaded from csv file.\n            * 'rawdata_err': raw data with errors removed, loaded from csv file.\n            * 'despiked_err': despiked data with errors removed, loaded from csv file.\n            * 'rawdata_err_bkg': raw data with errors removed, loaded from csv file, \n                subtracted from background data.\n            * 'despiked_err_bkg': despiked data with errors removed, loaded from csv file, \n                subtracted from background data.\n            * 'rawdata_err_bkg_err': raw data with errors removed, loaded from csv file, ", "solution": "\n        if analytes is None:\n            analytes = self.analytes\n        elif isinstance(analytes, str):\n            analytes = [analytes]\n\n        if focus_stage == 'despiked':\n            if 'despiked' not in self.stages_complete:\n                focus_stage = 'rawdata'\n\n        # make uncertainty-aware background interpolators\n        bkg_interps = {}\n        for a in analytes:\n            bkg_interps[a] = un_interp1d(x=self.bkg['calc']['uTime'],\n                                         y=un.uarray(self.bkg['calc'][a]['mean'],\n                                                     self.bkg['calc'][a][errtype]))\n        self.bkg_interps = bkg_interps\n\n        # apply background corrections\n        with self.pbar.set(total=len(self.data), desc='Background Subtraction') as prog:\n            for d in self.data.values():\n                # [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), None, focus_stage=focus_stage) for a in analytes]\n                [d.bkg_subtract(a, bkg_interps[a].new(d.uTime), ~d.sig, focus_stage=focus_stage) for a in analytes]\n                d.setfocus('bkgsub')\n\n                prog.update()\n\n        self.stages_complete.update(['bkgsub'])\n        self.focus_stage = 'bkgsub'\n        return"}
{"prompt": "def find_bp(self, filename, lineno, frame):\n        \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n        Called only if we know there is a bpt at this\n        location.  Returns breakpoint that was triggered and a flag\n        that indicates if it is ok to delete a temporary breakpoint.\n\n        \"\"\"", "output": "\n        lineno = max(0, lineno)\n        filename = os.path.basename(filename)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line:\n            line = line.strip()\n            match = self.bpfile_re.match(line)\n            if match:\n                bpfile = self.bpfiles[match.group('bpfile')]\n                bpinfo = self.bpinfos[match.group('bpinfo')]\n                bptype = match.group('bptype')\n                bpnumber = int(match.group('bpnumber'))\n                bpstate = self.bpstates[match.group('bpstate')]\n                return bpinfo, bptype, bpnumber, bpstate\n        return None\n\n    def clear_cache(self):\n        \"\"\"Clear the BPT cache.\"\"\"\n        self.", "solution": "\n        possibles = self.bplist[filename, lineno]\n        for i in range(0, len(possibles)):\n            b = possibles[i]\n            if not b.enabled:\n                continue\n            if not checkfuncname(b, frame):\n                continue\n            # Count every hit when bp is enabled\n            b.hits += 1\n            if not b.condition:\n                # If unconditional, and ignoring, go on to next, else\n                # break\n                if b.ignore > 0:\n                    b.ignore = b.ignore -1\n                    continue\n                else:\n                    # breakpoint and marker that's ok to delete if\n                    # temporary\n                    return (b, True)\n            else:\n                # Conditional bp.\n                # Ignore count applies only to those bpt hits where the\n                # condition evaluates to true.\n                try:\n                    val = eval(b.condition, frame.f_globals, frame.f_locals)\n                    if val:\n                        if b.ignore > 0:\n                            b.ignore = b.ignore -1\n                            # continue\n                        else:\n                            return (b, True)\n                    # else:\n                    #   continue\n                except:\n                    # if eval fails, most conservative thing is to\n                    # stop on breakpoint regardless of ignore count.\n                    # Don't delete temporary, as another hint to user.\n                    return (b, False)\n                pass\n            pass\n        return (None, None)"}
{"prompt": "def _to_dataframe(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"", "output": "\n        dataset = Dataset(self.path)\n        dataset.load()\n        return dataset\n\n    def _to_dataframes(self):\n        \"\"\"Read and return the dataset contents as a pandas DataFrame.\"\"\"\n        datasets = []\n        for dataset in self.datasets:\n            datasets.append(dataset._to_dataframe())\n        return datasets\n\n    def to_csv(self, filename, index=False, sep=\",\", na_rep=\"NaN\", float_format=None):\n        \"\"\"\n        Write the dataset to a comma-separated values (csv) file.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the file to write to.\n        index : bool, optional\n            Write the index to the file.\n        sep : str, optional\n            Separator to use when writing the file.\n        na_re", "solution": "\n        #TODO: figure out why passing in the opened stream directly gives invalid data\n        data = self.read_as_binary()\n        reader = BytesIO(data)\n        return deserialize_dataframe(reader, self.data_type_id)"}
{"prompt": "def create_tfs_git_client(url, token=None):\n    \"\"\"\n    Creates a TFS Git Client to pull Git repo info\n    \"\"\"", "output": "\n    git_client = GitClient(url, token=token)\n    return git_client\n\n\ndef git_repo_exists(repo_url):\n    \"\"\"\n    Checks if a repo exists on TFS\n    \"\"\"\n    git_client = create_tfs_git_client(repo_url)\n    return git_client.exists()\n\n\ndef git_repo_exists(repo_url, repo_name):\n    \"\"\"\n    Checks if a repo exists on TFS\n    \"\"\"\n    git_client = create_tfs_git_client(repo_url)\n    return git_client.exists(repo_name)\n\n\ndef git_repo_exists(repo_url, repo_name, repo_branch):\n    \"\"\"\n    Checks if a repo exists on TFS\n", "solution": "\n    if token is None:\n        token = os.environ.get('TFS_API_TOKEN', None)\n\n    tfs_connection = create_tfs_connection(url, token)\n    tfs_git_client = tfs_connection.get_client('vsts.git.v4_1.git_client.GitClient')\n\n    if tfs_git_client is None:\n        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'\n        raise RuntimeError(msg, url)\n\n    return tfs_git_client"}
{"prompt": "def canFetchMore(self, index):\n        \"\"\"Return if more data available for *index*.\"\"\"", "output": "\n        return self._canFetchMore(index)\n\n    def fetchMore(self, index):\n        \"\"\"Fetch more data for *index*.\"\"\"\n        self._fetchMore(index)\n\n    def canFetchLess(self, index):\n        \"\"\"Return if less data available for *index*.\"\"\"\n        return self._canFetchLess(index)\n\n    def fetchLess(self, index):\n        \"\"\"Fetch less data for *index*.\"\"\"\n        self._fetchLess(index)\n\n    def canFetchAll(self, index):\n        \"\"\"Return if all data available for *index*.\"\"\"\n        return self._canFetchAll(index)\n\n    def fetchAll(self, index):\n        \"\"\"Fetch all data for *index*.\"\"\"\n        self._fetchAll(index)\n\n    def canFetch(self, index):\n        \"\"\"Return if data ", "solution": "\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        return item.canFetchMore()"}
{"prompt": "def info_formatter(info):\n    \"\"\"Produce a sequence of formatted lines from info.\n\n    `info` is a sequence of pairs (label, data).  The produced lines are\n    nicely formatted, ready to print.\n\n    \"\"\"", "output": "\n    lines = info.split('\\n')\n    width = max(map(len, lines))\n    lines = [line + '\\n' for line in lines]\n    lines = [line + (width - len(line) % width) *'' + '-'\n            for line in lines]\n    return '\\n'.join(lines)\n\n\ndef format_summary(info, formatter=info_formatter):\n    \"\"\"Produce a summary line from info tuples.\n\n    Given a list of tuples, produces a single string containing the\n    formatted summary line for the items in the list.  Each tuple\n    is formatted as \"<label>: <data>\".\n\n    \"\"\"\n    summary = []\n    for label, data in info:\n        summary.append(formatter((label, data)))\n    summary = '\\n'.join(summary)\n    return ", "solution": "\n    label_len = max([len(l) for l, _d in info])\n    for label, data in info:\n        if data == []:\n            data = \"-none-\"\n        if isinstance(data, (list, tuple)):\n            prefix = \"%*s:\" % (label_len, label)\n            for e in data:\n                yield \"%*s %s\" % (label_len+1, prefix, e)\n                prefix = \"\"\n        else:\n            yield \"%*s: %s\" % (label_len, label, data)"}
{"prompt": "async def handle_request(self, request: Request\n                             ) -> typing.Tuple[AsyncGeneratorType, Response]:\n        \"\"\"\n        Wrap request with middleware.\n        :param request:\n        :return:\n        \"\"\"", "output": "\n        async with self.middleware_stack:\n            async for request in self.middleware_stack.async_handle_request(request):\n                yield request\n\n    async def handle_response(self, request: Request) -> Response:\n        \"\"\"\n        Wrap response with middleware.\n        :param request:\n        :return:\n        \"\"\"\n        async with self.middleware_stack:\n            async for response in self.middleware_stack.async_handle_response(request):\n                return response\n\n    async def handle_exception(self, request: Request) -> Response:\n        \"\"\"\n        Wrap exception with middleware.\n        :param request:\n        :return:\n        \"\"\"\n        async with self.middleware_stack:\n            async for response in self.middleware_stack.async_handle_exception(request):\n                return response\n\n    async def ", "solution": "\n        callback_result, response = None, None\n\n        await self._run_request_middleware(request)\n        try:\n            callback_result, response = await request.fetch_callback(self.sem)\n        except NotImplementedParseError as e:\n            self.logger.error(e)\n        except NothingMatchedError as e:\n            self.logger.error(f'<Item: {str(e).lower()}>')\n        except Exception as e:\n            self.logger.error(f'<Callback[{request.callback.__name__}]: {e}')\n        await self._run_response_middleware(request, response)\n        await self._process_response(request=request, response=response)\n        return callback_result, response"}
{"prompt": "def get_connection(host='localhost', port=27017, username=None, password=None,\n                   uri=None, mongodb=None, authdb=None, timeout=20, *args, **kwargs):\n    \"\"\"Get a client to the mongo database\n\n        host(str): Host of database\n        port(int): Port of database\n        username(str)\n        password(str)\n        uri(str)\n        authdb (str): database to use for authentication\n        timeout(int): How long should the client try to connect\n\n    \"\"\"", "output": "\n    kwargs.setdefault('tz_aware', False)\n    kwargs.setdefault('connect', True)\n    kwargs.setdefault('document_class', Document)\n    kwargs.setdefault('tz_aware', False)\n    kwargs.setdefault('slave_okay', False)\n    kwargs.setdefault('read_preference', ReadPreference.SECONDARY)\n    kwargs.setdefault('read_preference', ReadPreference.PRIMARY)\n    kwargs.setdefault('max_pool_size', 100)\n    kwargs.setdefault('pool_size', 100)\n    kwargs.setdefault('max_retries', 5)\n    kwargs.setdefault('max_pool_timeout', 10)\n    kwargs.setdefault('pool_size', 100)\n    kwargs.setdefault('max_pool_size', ", "solution": "\n    authdb = authdb or mongodb\n    if uri is None:\n        if username and password:\n            uri = (\"mongodb://{}:{}@{}:{}/{}\"\n                   .format(quote_plus(username), quote_plus(password), host, port, authdb))\n            log_uri = (\"mongodb://{}:****@{}:{}/{}\"\n                   .format(quote_plus(username), host, port, authdb))\n        else:\n            log_uri = uri = \"mongodb://%s:%s\" % (host, port)\n            \n\n    LOG.info(\"Try to connect to %s\" % log_uri)\n    try:\n        client = MongoClient(uri, serverSelectionTimeoutMS=timeout)\n    except ServerSelectionTimeoutError as err:\n        LOG.warning(\"Connection Refused\")\n        raise ConnectionFailure\n\n    LOG.info(\"Connection established\")\n    return client"}
{"prompt": "def _get_job(self, project_id, job_id):\n        \"\"\"\n        Gets a MLEngine job based on the job name.\n\n        :return: MLEngine job object if succeed.\n        :rtype: dict\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"", "output": "\n        job_resource = self.service.projects().jobs().get(\n            projectId=project_id, jobId=job_id\n        ).execute()\n        return job_resource\n\n    def _get_jobs(self, project_id):\n        \"\"\"\n        Gets MLEngine jobs based on the job name.\n\n        :return: MLEngine jobs list if succeed.\n        :rtype: list\n\n        Raises:\n            googleapiclient.errors.HttpError: if HTTP error is returned from server\n        \"\"\"\n        job_resource = self.service.projects().jobs().list(\n            projectId=project_id\n        ).execute()\n        return job_resource.get('jobs', [])\n\n    def _get_model(self, project_id, model_id):\n        \"\"\"\n        Gets a MLEngine model based on the model name.\n\n", "solution": "\n        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n        request = self._mlengine.projects().jobs().get(name=job_name)\n        while True:\n            try:\n                return request.execute()\n            except HttpError as e:\n                if e.resp.status == 429:\n                    # polling after 30 seconds when quota failure occurs\n                    time.sleep(30)\n                else:\n                    self.log.error('Failed to get MLEngine job: {}'.format(e))\n                    raise"}
{"prompt": "def _closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list: list[datetime.date]\n    :param before_target: closest before or after the target\n    :type before_target: bool or None\n    :returns: The closest date\n    :rtype: datetime.date or None\n    \"\"\"", "output": "\n    closest_date = None\n    closest_date_diff = None\n    for date in date_list:\n        if closest_date is None:\n            closest_date = date\n        elif abs(date - closest_date) < abs(closest_date_diff - abs(closest_date_diff)):\n            closest_date = date\n            closest_date_diff = abs(date - closest_date)\n    return closest_date\n\n\ndef _find_closest_date(target_dt, date_list, before_target=None):\n    \"\"\"\n    This function finds the date in a list closest to the target date.\n    An optional parameter can be given to get the closest before or after.\n\n    :param target_dt: The target date\n    :type target_dt: datetime.date\n    :param date_list: The list of dates to search\n    :type date_list", "solution": "\n    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max\n    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max\n    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt\n    if before_target is None:\n        return min(date_list, key=fnone).date()\n    if before_target:\n        return min(date_list, key=fb).date()\n    else:\n        return min(date_list, key=fa).date()"}
{"prompt": "def _has_parent_of_type(node, node_type, statement):\n    \"\"\"Check if the given node has a parent of the given type.\"\"\"", "output": "\n    parent = node.parent\n    while parent is not None:\n        if parent.type == node_type:\n            return True\n        parent = parent.parent\n    return False\n\n\ndef _is_subtype(node_type, nodes):\n    \"\"\"Check if the given node is a subtype of the given node type.\"\"\"\n    for node in nodes:\n        if node.type == node_type:\n            return True\n    return False\n\n\ndef _is_supertype(node_type, nodes):\n    \"\"\"Check if the given node is a supertype of the given node type.\"\"\"\n    for node in nodes:\n        if node.type!= node_type:\n            return True\n    return False\n\n\ndef _is_same_subtype(node_type, nodes):\n    \"\"\"Check if the given node is a subtype of the given node type.\"\"\"\n    for node in nodes:\n        if node.type == ", "solution": "\n    parent = node.parent\n    while not isinstance(parent, node_type) and statement.parent_of(parent):\n        parent = parent.parent\n    return isinstance(parent, node_type)"}
{"prompt": "def _wrap_command(cmds, cls, strict=True):\n    \"\"\"Wrap a setup command\n\n    Parameters\n    ----------\n    cmds: list(str)\n        The names of the other commands to run prior to the command.\n    strict: boolean, optional\n        Whether to raise errors when a pre-command fails.\n    \"\"\"", "output": "\n    cmds = [cmd] if isinstance(cmd, str) else cmds\n    cmds = [cls.command_prefix + cmd for cmd in cmds]\n    cmds = [cls.command_prefix + cmd + cls.command_postfix for cmd in cmds]\n    cmds = [cls.command_prefix + cmd + cls.command_postfix + '--strict' for cmd in cmds]\n    cmds = [cls.command_prefix + cmd + cls.command_postfix + '--noinput' for cmd in cmds]\n    cmds = [cls.command_prefix + cmd + cls.command_postfix + '--quiet' for cmd in cmds]\n    cmds = [cls.command_prefix + cmd + cls.command_postfix + '--quiet-fail' for cmd in ", "solution": "\n    class WrappedCommand(cls):\n\n        def run(self):\n            if not getattr(self, 'uninstall', None):\n                try:\n                    [self.run_command(cmd) for cmd in cmds]\n                except Exception:\n                    if strict:\n                        raise\n                    else:\n                        pass\n            # update package data\n            update_package_data(self.distribution)\n\n            result = cls.run(self)\n            return result\n    return WrappedCommand"}
{"prompt": "def as_tuple(self):\n        \"\"\"\n        :rtype: (str, object)\n        \"\"\"", "output": "\n        return self.name, self.value\n\n    def __repr__(self):\n        return \"<%s %r>\" % (self.__class__.__name__, self.name)\n\n\nclass Function(object):\n    \"\"\"\n    A function object represents a callable object in Python.\n\n    Functions are first-class objects in Python.  They can be passed around\n    and called (although they are not really callable).  A function can\n    contain code to be executed when the function is called, as well as be\n    passed around as a bound method.\n\n    Instances of :class:`Function` can be passed around and called as function\n    objects.  This is useful when writing classes that need to store state\n    or perform operations on arguments.  Functions are also useful when\n    writing generic functions that don't care about the specific arguments for\n", "solution": "\n        if self._as_tuple is None:\n            self._as_tuple = self.converted.items()[0]\n        return self._as_tuple"}
{"prompt": "def add_virtual_columns_cartesian_to_spherical(self, x=\"x\", y=\"y\", z=\"z\", alpha=\"l\", delta=\"b\", distance=\"distance\", radians=False, center=None, center_name=\"solar_position\"):\n        \"\"\"Convert cartesian to spherical coordinates.\n\n\n\n        :param x:\n        :param y:\n        :param z:\n        :param alpha:\n        :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).\n        :param distance:\n        :param radians:\n        :param center:\n        :param center_name:\n        :return:\n        \"\"\"", "output": "\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "solution": "\n        transform = \"\" if radians else \"*180./pi\"\n\n        if center is not None:\n            self.add_variable(center_name, center)\n        if center is not None and center[0] != 0:\n            x = \"({x} - {center_name}[0])\".format(**locals())\n        if center is not None and center[1] != 0:\n            y = \"({y} - {center_name}[1])\".format(**locals())\n        if center is not None and center[2] != 0:\n            z = \"({z} - {center_name}[2])\".format(**locals())\n        self.add_virtual_column(distance, \"sqrt({x}**2 + {y}**2 + {z}**2)\".format(**locals()))\n        # self.add_virtual_column(alpha, \"((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}\".format(**locals()))\n        self.add_virtual_column(alpha, \"arctan2({y}, {x}){transform}\".format(**locals()))\n        self.add_virtual_column(delta, \"(-arccos({z}/{distance})+pi/2){transform}\".format(**locals()))"}
{"prompt": "def unix_ts(dtval):\n    \"\"\"Convert datetime into a unix timestamp.\n    This is the equivalent to Python 3's int(datetime.timestamp()).\n\n    :param dt: datetime to convert\n    \"\"\"", "output": "\n    return int(time.mktime(dtval.timetuple()))\n\n\ndef unix_to_ts(unixtime):\n    \"\"\"Convert unix timestamp into a datetime.\n    This is the equivalent to Python 3's datetime.fromtimestamp(unixtime).\n\n    :param unixtime: unix timestamp to convert\n    \"\"\"\n    return datetime.fromtimestamp(unixtime)\n\n\ndef unix_to_epoch(unixtime):\n    \"\"\"Convert unix timestamp into a datetime.\n    This is the equivalent to Python 3's datetime.fromtimestamp(unixtime).\n\n    :param unixtime: unix timestamp to convert\n    \"\"\"\n    return datetime.fromtimestamp(unixtime).timestamp()\n\n\ndef unix_to_epoch_t", "solution": "\n    epoch = datetime(1970, 1, 1, 0, 0, tzinfo=tzutc())\n    delta = (dtval - epoch)\n    return delta.days * 24 * 3600 + delta.seconds"}
{"prompt": "def path_dispatch_kwarg(mname, path_default, returns_model):\n    \"\"\"\n    Parameterized decorator for methods that accept path as a second\n    argument.\n    \"\"\"", "output": "\n    def dispatch_method(self, path, *args, **kwargs):\n        path_args = []\n        path_kwargs = {}\n        path_defaults = []\n        path_defaults_kwargs = {}\n        path_defaults_defaults = []\n        path_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults = []\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_defaults_kwargs = {}\n        path_defaults_defaults_", "solution": "\n    def _wrapper(self, path=path_default, **kwargs):\n        prefix, mgr, mgr_path = _resolve_path(path, self.managers)\n        result = getattr(mgr, mname)(path=mgr_path, **kwargs)\n        if returns_model and prefix:\n            return _apply_prefix(prefix, result)\n        else:\n            return result\n    return _wrapper"}
{"prompt": "def get_label ( self, object ):\n        \"\"\" Gets the label to display for a specified object.\n        \"\"\"", "output": "\n        return object.label\n\n    def get_description ( self, object ):\n        \"\"\" Gets the description to display for a specified object.\n        \"\"\"\n        return object.description\n\n    def get_icon ( self, object ):\n        \"\"\" Gets the icon to display for a specified object.\n        \"\"\"\n        return object.icon\n\n    def get_icon_url ( self, object ):\n        \"\"\" Gets the icon to display for a specified object.\n        \"\"\"\n        return object.icon_url\n\n    def get_id ( self, object ):\n        \"\"\" Gets the id to display for a specified object.\n        \"\"\"\n        return object.id\n\n    def get_image ( self, object ):\n        \"\"\" Gets the image to use for a specified object.\n        \"\"\"\n        return object.image\n\n    def get_image_url ( self, object ):\n        \"\"\" Gets the image to use for a specified ", "solution": "\n        label = self.label\n        if label[:1] == '=':\n            return label[1:]\n\n        label = xgetattr( object, label, '' )\n\n        if self.formatter is None:\n            return label\n\n        return self.formatter( object, label )"}
{"prompt": "def create(self, request):\n        \"\"\"\n        Log in django staff user\n        \"\"\"", "output": "\n        email = request.POST.get('email')\n        password = request.POST.get('password')\n        user = authenticate(email=email, password=password)\n        if user is not None:\n            if user.is_active:\n                login(request, user)\n                return HttpResponseRedirect(reverse('admin:index'))\n            else:\n                messages.error(request, \"Your account is disabled.\")\n                return HttpResponseRedirect(reverse('admin:index'))\n        else:\n            messages.error(request, \"Invalid email or password.\")\n            return HttpResponseRedirect(reverse('admin:index'))\n\nclass LogoutView(View):\n    \"\"\"\n    Logout django staff user\n    \"\"\"\n    def get(self, request):\n        \"\"\"\n        Logout django staff ", "solution": "\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n        # from django.utils.decorators import method_decorator\n        # from django.views.decorators.debug import sensitive_post_parameters\n        # sensitive_post_parameters_m = method_decorator(sensitive_post_parameters())\n\n        login_form = AuthenticationForm(request, data=request.data)\n\n        if not login_form.is_valid():\n            raise serializers.ValidationError(login_form.errors)\n\n        auth_login(request, login_form.get_user())\n\n        serializer = UserSerializer(request.user)\n        return Response(serializer.data, status=status.HTTP_200_OK)"}
{"prompt": "def remove(self, recursive=True, ignore_error=True):\n        \"\"\"\n        Remove the directory.\n        \"\"\"", "output": "\n        shutil.rmtree(self.path, ignore_error=ignore_error)\n\n    def listdir(self, recursive=True, ignore_error=True):\n        \"\"\"\n        List the contents of the directory.\n        \"\"\"\n        return os.listdir(self.path, ignore_error=ignore_error)\n\n    def makedirs(self, mode=0o777, recursive=True, ignore_error=True):\n        \"\"\"\n        Make directories.\n        \"\"\"\n        if not os.path.isdir(self.path):\n            try:\n                os.makedirs(self.path, mode=mode, exist_ok=recursive)\n            except OSError as exc:\n                if ignore_error:\n                    return\n                raise OSError(str(exc))\n\n    def removedirs(self, ignore_error=True):\n        \"\"\"\n        Remove directories.\n        \"\"\"\n        shutil.rmtree(self.path, ignore_error=ignore_error)\n\n    def ", "solution": "\n        try:\n            if recursive or self._cleanup == 'recursive':\n                shutil.rmtree(self.path)\n            else:\n                os.rmdir(self.path)\n        except Exception as e:\n            if not ignore_error:\n                raise e"}
{"prompt": "def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"", "output": "\n        other = status.farthest\n        if other is None:\n            other = status.expected\n        if other is None:\n            return self\n        if other is self:\n            return self\n        other.farthest = self\n        other.expected = self\n        return self\n\n    def assert_equal(self, other: 'Status') -> None:\n        \"\"\"Assert that two statuses are equal.\"\"\"\n        assert self.expected == other.expected\n        assert self.farthest == other.farthest\n\n    def __repr__(self) -> str:\n        return 'Status(expected={}, farthest={})'.format(self.expected, self.farthest)\n\n    def __str__(self) -> str:\n        return '{} -> {}'.format(self.expected, self.farthest)\n\n\nclass Input:\n    \"\"\"An input to a test.\n\n    Instances of this class represent a ", "solution": "\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self"}
{"prompt": "def activity(self, value):\n        \"\"\"\n        Used to set TurnContext._activity when a context object is created. Only takes instances of Activities.\n        :param value:\n        :return:\n        \"\"\"", "output": "\n        self._activity = value\n\n    @property\n    def activity_id(self):\n        \"\"\"\n        Used to set TurnContext._activity_id when a context object is created. Only takes instances of Activities.\n        :return:\n        \"\"\"\n        return self._activity_id\n\n    @property\n    def activity_type(self):\n        \"\"\"\n        Used to set TurnContext._activity_type when a context object is created. Only takes instances of Activities.\n        :return:\n        \"\"\"\n        return self._activity_type\n\n    @property\n    def activity_title(self):\n        \"\"\"\n        Used to set TurnContext._activity_title when a context object is created. Only takes instances of Activities.\n        :return:\n        \"\"\"\n        return self._activity_title\n\n    @property\n    def activity_subtitle(self):\n        \"\"\"\n        Used to set ", "solution": "\n        if not isinstance(value, Activity):\n            raise TypeError('TurnContext: cannot set `activity` to a type other than Activity.')\n        else:\n            self._activity = value"}
{"prompt": "def _post_cutout_no_chunking_blosc(self, token, channel,\n                                       x_start, y_start, z_start,\n                                       data, resolution):\n        \"\"\"\n        Accepts data in zyx. !!!\n        \"\"\"", "output": "\n        cutout = Cutout(resolution, x_start, y_start, z_start)\n        cutout.cutout_data(token, channel, data)\n        cutout.cutout_metadata(token)\n\n    def _post_cutout_no_chunking_zlib(self, token, channel,\n                                       x_start, y_start, z_start,\n                                       data, resolution):\n        \"\"\"\n        Accepts data in zyx.!!!\n        \"\"\"\n        cutout = Cutout(resolution, x_start, y_start, z_start)\n        cutout.cutout_data(token, channel, data)\n        cutout.cutout_metadata(token)\n\n    def _post_cutout_no_chunking_zstd(self, token, channel,\n                                       x_start, y_start, ", "solution": "\n        data = numpy.expand_dims(data, axis=0)\n        blosc_data = blosc.pack_array(data)\n        url = self.url(\"{}/{}/blosc/{}/{},{}/{},{}/{},{}/0,0/\".format(\n            token, channel,\n            resolution,\n            x_start, x_start + data.shape[3],\n            y_start, y_start + data.shape[2],\n            z_start, z_start + data.shape[1]\n        ))\n\n        req = self.remote_utils.post_url(url, data=blosc_data, headers={\n            'Content-Type': 'application/octet-stream'\n        })\n\n        if req.status_code is not 200:\n            raise RemoteDataUploadError(req.text)\n        else:\n            return True"}
{"prompt": "def current(self):\n        \"\"\" bool: Whether this window is the window in which commands are being executed. \"\"\"", "output": "\n        return self._current\n\n    @current.setter\n    def current(self, value):\n        \"\"\" bool: Whether this window is the window in which commands are being executed. \"\"\"\n        self._current = value\n\n    @property\n    def parent(self):\n        \"\"\" QWidget: The parent widget of this window. \"\"\"\n        return self._parent\n\n    @parent.setter\n    def parent(self, value):\n        \"\"\" QWidget: The parent widget of this window. \"\"\"\n        self._parent = value\n\n    @property\n    def title(self):\n        \"\"\" str: The title of this window. \"\"\"\n        return self._title\n\n    @title.setter\n    def title(self, value):\n        \"\"\" str: The title of this window. \"\"\"\n        self._title = value\n\n    @property\n    def icon(self):\n        \"\"\" QIcon: The icon of this window. \"\"\"\n        return self._icon\n\n    @", "solution": "\n        try:\n            return self.driver.current_window_handle == self.handle\n        except self.driver.no_such_window_error:\n            return False"}
{"prompt": "def autodetect_files(self):\n        \"\"\" Attempt to detect requirements files in the current working directory \"\"\"", "output": "\n        requirements_path = os.path.join(self.working_directory,'requirements.txt')\n        if os.path.exists(requirements_path):\n            requirements = open(requirements_path, 'r').readlines()\n            return requirements\n        else:\n            return []\n\n    def autodetect_packages(self):\n        \"\"\" Attempt to detect packages in the current working directory \"\"\"\n        packages_path = os.path.join(self.working_directory, 'packages')\n        if os.path.exists(packages_path):\n            packages = os.listdir(packages_path)\n            return packages\n        else:\n            return []\n\n    def autodetect_package_versions(self):\n        \"\"\" Attempt to detect package versions in the current working directory \"\"\"\n        packages_path = os.path.join(self.working_directory, 'packages')\n        package_versions = {}\n        for package in self.autodetect_packages():\n            ", "solution": "\n        if self._is_valid_requirements_file('requirements.txt'):\n            self.filenames.append('requirements.txt')\n\n        if self._is_valid_requirements_file('requirements.pip'):  # pragma: nocover\n            self.filenames.append('requirements.pip')\n\n        if os.path.isdir('requirements'):\n            for filename in os.listdir('requirements'):\n                file_path = os.path.join('requirements', filename)\n                if self._is_valid_requirements_file(file_path):\n                    self.filenames.append(file_path)\n        self._check_inclusions_recursively()"}
{"prompt": "def format_extension(self):\n        \"\"\"The format extension of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extension\n            '.js'\n        \"\"\"", "output": "\n        return self.format_extension\n\n    @property\n    def format_extensions(self):\n        \"\"\"The format extensions of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_extensions\n            ['.js']\n\n            >>> attrs = AssetAttributes(environment, 'js/lib/external.min.js.coffee')\n            >>> attrs.format_extensions\n            ['.js']\n        \"\"\"\n        return self.format_extensions\n\n    @property\n    def format_prefix(self):\n        \"\"\"The format prefix of asset.\n        Example::\n\n            >>> attrs = AssetAttributes(environment, 'js/models.js.coffee')\n            >>> attrs.format_prefix\n            'js'\n\n            >>> attrs = AssetAttribute", "solution": "\n        for extension in reversed(self.extensions):\n            compiler = self.environment.compilers.get(extension)\n            if not compiler and self.environment.mimetypes.get(extension):\n                return extension"}
